{"id": "4238063", "url": "https://en.wikipedia.org/wiki?curid=4238063", "title": "1983 world oil market chronology", "text": "1983 world oil market chronology\n\n\"Oil glut takes hold. Demand falls as a result of conservation, use of other fuels and recession. OPEC agrees to limit overall output to 17.5 MMB/D. OPEC agrees to individual output quotas and cuts prices by $5 to $29 per barrel.\"\n|-\n"}
{"id": "47667935", "url": "https://en.wikipedia.org/wiki?curid=47667935", "title": "Adebowale Adefuye", "text": "Adebowale Adefuye\n\nAdebowale Ibidapo Adefuye (January 1947 – August 27, 2015) was a Nigerian historian and diplomat.\n\nBorn in Ijebu-Igbo, Adefuye attended the University of Ibadan, first graduating in 1969. He obtained a Ph.D in history from the same institution in 1973. During his academic career, Adefuye was named a Fulbright Scholar and used the funds to do research at Columbia University, the University of North Florida, and the University of Florida. Adefuye taught at the University of Lagos, heading the school's history department from 1985 to 1987.\n\nHe was named the Ambassador to Jamaica in 1987, serving until 1991. During that period, Adefuye also concurrently served as the ambassador to Belize and Haiti. He was then the Deputy High Commissioner to the United Kingdom. Adefuye left that post to serve as the deputy director of the Commonwealth of Nations for fourteen years. After leaving the Commonwealth, he became an advisor to the Economic Community of West African States in 2008. President Goodluck Jonathan appointed Adefuye the ambassador to the United States in 2010. During his tenure, Adefuye continually advocated for the United States to provide more military aid to Nigeria to effectively counter the forces of Boko Haram. He was recalled in 2015 after Muhammadu Buhari was sworn in as president of Nigeria.\n\nHe died in Washington, D.C. on August 27, 2015, of a heart attack.\n"}
{"id": "31098262", "url": "https://en.wikipedia.org/wiki?curid=31098262", "title": "Amber Coast", "text": "Amber Coast\n\nThe Amber Coast is the name given to a coastal strip of the Baltic Sea in the northwest of Kaliningrad (Russia, Kaliningrad Oblast, Sambia Peninsula, formerly northern East Prussia in Germany). In this area amber (Baltic amber) has been excavated since the mid-19th century and up to today in open-pit mining. Two deposits – Palmnikenskoe and Primorskoe, containing 80% of world amber reserves, were found near Yantarny on the Western coast of the Sambia Peninsula in 1948-1951’s.\n\nScientists believe that amber was deposited during the Upper Eocene and Lower Oligocene in a delta of a prehistoric river, in a shallow part of a marine basin. In addition to the coast near Kaliningrad, amber is also found elsewhere in the Baltic Sea region. The deposits are found mostly in the \"blue earth glauconite\", a layer 1 to 17.5 meters thick found 25 to 40 meters from the surface. In addition to the Sambia region, amber is gathered in noticeable amounts at German, Polish and Lithuanian Baltic beaches (areas of the Bay of Gdańsk as well as the Vistula Lagoon), the western coast of Denmark and the Frisian Islands. Small amounts of Baltic amber can even be found outside the Baltic region, for example on the coastline of the south east of England.\n\nHowever, about 90% to 98% of all output of amber has been produced in the Sambia region (now a Russian exclave, formerly in Eastern Prussia and the Polish-Lithuanian Commonwealth). The Sambian amber-producing region is a square of about 30–40 km (20–25 miles), although geologists estimate there are deposits beyond the region of the main excavations. A potential nearby source of amber is the Courish Lagoon. Amber excavation is overseen by the Russian Amber Company (\"Ruskij Jantar\").\n\nThe Amber Coast is mentioned as early as by Tacitus in his work \"Germania\".\n\nAnother coastal strip referred to as “amber coast” is the Costa de Ambar (also known as “Costambar”) in the west of Puerto Plata (Hispaniola, Dominican Republic). In this area there are a number of small shaft mines, from which is excavated the so-called \"Dominican amber\". The Dominican amber production site is the world's second-largest, although compared to the Baltic region it is \"a distant second\".\n\n"}
{"id": "10790422", "url": "https://en.wikipedia.org/wiki?curid=10790422", "title": "America at a Crossroads", "text": "America at a Crossroads\n\nAmerica at a Crossroads is a documentary miniseries concerning the issues facing the United States of America as related to the War on Terrorism. It aired originally on PBS television.\n\nThe miniseries initially consisted of 11 independently produced \"aired\" episodes, and premiered April 15–20, 2007 on PBS. Its executive producers are Jeff Bieber and Dalton Delan; series producer is Leo Eaton and it is presented by Robert MacNeil. Its music score is composed by Canadian musician Mark Korven.\n\n\"America at a Crossroads\" explores the challenges confronting the post-9/11 world — including the war on terrorism; the conflicts in Iraq and Afghanistan; the experience of American troops serving abroad; the struggle for balance within the Muslim world; and global perspectives on America's role overseas.\n\nAimed at creating a national dialogue surrounding the crucial issues explored in the series, an extensive media and outreach campaign in more than 25 communities accompanies the series. The campaign features screening events with the filmmakers and their subjects in discussions with United States military personnel, leading policy experts, leaders of the Islamic community, scholars from across the country as well as members of the public.\n\n\n\n\nOne film, \"The Case for War: In Defense of Freedom\", in which Richard Perle presented his view of the challenges facing the U.S. generated considerable controversy. Some critics complained that the film failed to adequately challenge Perle's views. \"The New York Times\" called the film \"a fascinating study in rationalization, a lighter, less repentant version of \"The Fog of War\", Errol Morris's documentary about Robert S. McNamara\". One critic complained that PBS's endorsement of the neoconservative viewpoint in this film was so misguided, that PBS viewers should express their disgust by \"either cutting off donations or at least demanding back a percentage of what they've already given\". Conversely, the producers of an unaired segment, \"Islam vs. Islamists\", claimed they had been victims of liberal bias and subsequently aired their documentary on the Fox News Channel under the provocative title \"\".\n\n\n"}
{"id": "52685", "url": "https://en.wikipedia.org/wiki?curid=52685", "title": "Ancient Roman architecture", "text": "Ancient Roman architecture\n\nAncient Roman architecture adopted the external language of classical Greek architecture for the purposes of the ancient Romans , but differed from Greek buildings, becoming a new architectural style. The two styles are often considered one body of classical architecture. Roman architecture flourished in the Roman Republic and even more so under the Empire, when the great majority of surviving buildings were constructed. It used new materials, particularly concrete, and newer technologies such as the arch and the dome to make buildings that were typically strong and well-engineered. Large numbers remain in some form across the empire, sometimes complete and still in use.\n\nRoman Architecture covers the period from the establishment of the Roman Republic in 509 BC to about the 4th century AD, after which it becomes reclassified as Late Antique or Byzantine architecture. Almost no substantial examples survive from before about 100 BC, and most of the major survivals are from the later empire, after about 100 AD. Roman architectural style continued to influence building in the former empire for many centuries, and the style used in Western Europe beginning about 1000 is called Romanesque architecture to reflect this dependence on basic Roman forms.\n\nThe Romans only began to achieve significant originality in architecture around the beginning of the Imperial period, after they had combined aspects of their original Etruscan architecture with others taken from Greece , including most elements of the style we now call classical architecture. They moved from trabeated construction mostly based on columns and lintels to one based on massive walls, punctuated by arches, and later domes, both of which greatly developed under the Romans. The classical orders now became largely decorative rather than structural, except in colonnades. Stylistic developments included the Tuscan and Composite orders; the first being a shortened, simplified variant on the Doric order and the Composite being a tall order with the floral decoration of the Corinthian and the scrolls of the Ionic. The period from roughly 40 BC to about 230 AD saw most of the greatest achievements, before the Crisis of the Third Century and later troubles reduced the wealth and organizing power of the central government.\n\nThe Romans produced massive public buildings and works of civil engineering, and were responsible for significant developments in housing and public hygiene, for example their public and private baths and latrines, under-floor heating in the form of the hypocaust, mica glazing (examples in Ostia Antica), and piped hot and cold water (examples in Pompeii and Ostia).\n\nDespite the technical developments of the Romans, which took their buildings far away from the basic Greek conception where columns were needed to support heavy beams and roofs, they were very reluctant to abandon the classical orders in formal public buildings, even though these had become essentially decorative . \nHowever, they did not feel entirely restricted by Greek aesthetic concerns, and treated the orders with considerable freedom.\n\nInnovation started in the 3rd or 2nd century BC with the development of Roman concrete as a readily available adjunct to, or substitute for, stone and brick. More daring buildings soon followed, with great pillars supporting broad arches and domes. The freedom of concrete also inspired the colonnade screen, a row of purely decorative columns in front of a load-bearing wall. In smaller-scale architecture, concrete's strength freed the floor plan from rectangular cells to a more free-flowing environment.\n\nFactors such as wealth and high population densities in cities forced the ancient Romans to discover new architectural solutions of their own. The use of vaults and arches, together with a sound knowledge of building materials, enabled them to achieve unprecedented successes in the construction of imposing infrastructure for public use. Examples include the aqueducts of Rome, the Baths of Diocletian and the Baths of Caracalla, the basilicas and Colosseum. These were reproduced at a smaller scale in most important towns and cities in the Empire. Some surviving structures are almost complete, such as the town walls of Lugo in Hispania Tarraconensis, now northern Spain. The administrative structure and wealth of the empire made possible very large projects even in locations remote from the main centres, as did the use of slave labour, both skilled and unskilled.\n\nEspecially under the empire, architecture often served a political function, demonstrating the power of the Roman state in general, and of specific individuals responsible for building. Roman architecture perhaps reached its peak in the reign of Hadrian, whose many achievements include rebuilding the Pantheon in its current form and leaving his mark on the landscape of northern Britain with Hadrian's Wall.\n\nWhile borrowing much from the preceding Etruscan architecture, such as the use of hydraulics and the construction of arches, Roman prestige architecture remained firmly under the spell of Ancient Greek architecture and the classical orders. \nThis came initially from Magna Graecia, the Greek colonies in southern Italy, and indirectly from Greek influence on the Etruscans, but after the Roman conquest of Greece directly from the best classical and Hellenistic examples in the Greek world. \nThe influence is evident in many ways; for example, in the introduction and use of the Triclinium in Roman villas as a place and manner of dining. Roman builders employed Greeks in many capacities, especially in the great boom in construction in the early Empire.\n\nThe Roman Architectural Revolution, also known as the \"Concrete Revolution\", was the widespread use in Roman architecture of the previously little-used architectural forms of the arch, vault, and dome. For the first time in history, their potential was fully exploited in the construction of a wide range of civil engineering structures, public buildings, and military facilities. These included amphitheatres, aqueducts, baths, bridges, circuses, dams, domes, harbours, temples, and theatres.\n\nA crucial factor in this development, which saw a trend toward monumental architecture, was the invention of Roman concrete (\"opus caementicium\"), which led to the liberation of shapes from the dictates of the traditional materials of stone and brick.\n\nThese enabled the building of the many aqueducts throughout the empire, such as the Aqueduct of Segovia, the Pont du Gard, and the eleven aqueducts of Rome. The same concepts produced numerous bridges, some of which are still in daily use, for example the Puente Romano at Mérida in Spain, and the Pont Julien and the bridge at Vaison-la-Romaine, both in Provence, France.\n\nThe dome permitted construction of vaulted ceilings without crossbeams and made possible large covered public space such as public baths and basilicas, such as Hadrian's Pantheon, the Baths of Diocletian and the Baths of Caracalla, all in Rome.\n\nThe Romans first adopted the arch from the Etruscans, and implemented it in their own building. The use of arches that spring directly from the tops of columns was a Roman development, seen from the 1st century AD, that was very widely adopted in medieval Western, Byzantine and Islamic architecture.\n\nThe Romans were the first builders in the history of architecture to realize the potential of domes for the creation of large and well-defined interior spaces. Domes were introduced in a number of Roman building types such as temples, thermae, palaces, mausolea and later also churches. Half-domes also became a favoured architectural element and were adopted as apses in Christian sacred architecture.\n\nMonumental domes began to appear in the 1st century BC in Rome and the provinces around the Mediterranean Sea. Along with vaults, they gradually replaced the traditional post and lintel construction which makes use of the column and architrave. The construction of domes was greatly facilitated by the invention of concrete, a process which has been termed the Roman Architectural Revolution. Their enormous dimensions remained unsurpassed until the introduction of structural steel frames in the late 19th century (see List of the world's largest domes).\n\nRoman architecture supplied the basic vocabulary of Pre-Romanesque and Romanesque architecture, and spread across Christian Europe well beyond the old frontiers of the empire, to Ireland and Scandinavia for example. In the East, Byzantine architecture developed new styles of churches, but most other buildings remained very close to Late Roman forms. The same can be said in turn of Islamic architecture, where Roman forms long continued, especially in private buildings such as houses and the Turkish bath, and civil engineering such as fortifications and bridges.\n\nIn Europe the Italian Renaissance saw a conscious revival of correct classical styles, initially purely based on Roman examples. Vitruvius was respectfully reinterpreted by a series of architectural writers, and the Tuscan and Composite orders formalized for the first time, to give five rather than three orders. After the flamboyance of Baroque architecture, the Neoclassical architecture of the 18th century revived purer versions of classical style, and for the first time added direct influence from the Greek world.\n\nNumerous local classical styles developed, such as Palladian architecture, Georgian architecture and Regency architecture in the English-speaking world, Federal architecture in the United States, and later Stripped Classicism and PWA Moderne.\n\nRoman influences may be found around us today, in banks, government buildings, great houses, and even small houses, perhaps in the form of a porch with Doric columns and a pediment or in a fireplace or a mosaic shower floor derived from a Roman original, often from Pompeii or Herculaneum. The mighty pillars, domes and arches of Rome echo in the New World too, where in Washington DC we see them in the Capitol Building, the White House, the Lincoln Memorial and other government buildings. All across the US the seats of regional government were normally built in the grand traditions of Rome, with vast flights of stone steps sweeping up to towering pillared porticoes, with huge domes gilded or decorated inside with the same or similar themes that were popular in Rome.\n\nIn Britain, a similar enthusiasm has seen the construction of thousands of neo-Classical buildings over the last five centuries, both civic and domestic, and many of the grandest country houses and mansions are purely Classical in style, an obvious example being Buckingham Palace.\n\nMarble is not found especially close to Rome, and was only rarely used there before Augustus, who famously boasted that he had found Rome made of brick and left it made of marble, though this was mainly as a facing for brick or concrete. The Temple of Hercules Victor of the late 2nd century BC is the earliest surviving exception in Rome. From Augustus' reign the quarries at Carrara were extensively developed for the capital, and other sources around the empire exploited, especially the prestigious Greek marbles like Parian. Travertine limestone was found much closer, around Tivoli, and was used from the end of the Republic; the Colosseum is mainly built of this stone, which has good load-bearing capacity, with a brick core. Other more or less local stones were used around the empire.\n\nThe Romans were extremely fond of luxury imported coloured marbles with fancy veining, and the interiors of the most important buildings were very often faced with slabs of these, which have usually now been removed even where the building survives. Imports from Greece for this purpose began in the 2nd century BC.\n\nThe Romans made fired clay bricks from about the beginning of the Empire, replacing earlier sun-dried mud-brick. Roman brick was almost invariably of a lesser height than modern brick, but was made in a variety of different shapes and sizes. Shapes included square, rectangular, triangular and round, and the largest bricks found have measured over three feet in length. Ancient Roman bricks had a general size of 1½ Roman feet by 1 Roman foot, but common variations up to 15 inches existed. Other brick sizes in ancient Rome included 24\" x 12\" x 4\", and 15\" x 8\" x 10\". Ancient Roman bricks found in France measured 8\" x 8\" x 3\". The Constantine Basilica in Trier is constructed from Roman bricks 15\" square by 1½\" thick. There is often little obvious difference (particularly when only fragments survive) between Roman bricks used for walls on the one hand, and tiles used for roofing or flooring on the other, so archaeologists sometimes prefer to employ the generic term ceramic building material (or CBM).\n\nThe Romans perfected brick-making during the first century of their empire and used it ubiquitously, in public and private construction alike. The Romans took their brickmaking skills everywhere they went, introducing the craft to the local populations. The Roman legions, which operated their own kilns, introduced bricks to many parts of the empire; bricks are often stamped with the mark of the legion that supervised their production. The use of bricks in southern and western Germany, for example, can be traced back to traditions already described by the Roman architect Vitruvius. In the British Isles, the introduction of Roman brick by the ancient Romans was followed by a 600–700 year gap in major brick production.\n\nConcrete quickly supplanted brick as the primary building material, and more daring buildings soon followed, with great pillars supporting broad arches and domes rather than dense lines of columns suspending flat architraves. The freedom of concrete also inspired the colonnade screen, a row of purely decorative columns in front of a load-bearing wall. In smaller-scale architecture, concrete's strength freed the floor plan from rectangular cells to a more free-flowing environment. Most of these developments are described by Vitruvius, writing in the first century AD in his work De Architectura.\n\nAlthough concrete had been used on a minor scale in Mesopotamia, Roman architects perfected Roman concrete and used it in buildings where it could stand on its own and support a great deal of weight. The first use of concrete by the Romans was in the town of Cosa sometime after 273 BC. Ancient Roman concrete was a mixture of lime mortar, aggregate, pozzolana, water, and stones, and was stronger than previously-used concretes. The ancient builders placed these ingredients in wooden frames where they hardened and bonded to a facing of stones or (more frequently) bricks. The aggregates used were often much larger than in modern concrete, amounting to rubble.\n\nWhen the framework was removed, the new wall was very strong, with a rough surface of bricks or stones. This surface could be smoothed and faced with an attractive stucco or thin panels of marble or other coloured stones called revetment. Concrete construction proved to be more flexible and less costly than building solid stone buildings. The materials were readily available and not difficult to transport. The wooden frames could be used more than once, allowing builders to work quickly and efficiently. Concrete is arguably the Roman contribution most relevant to modern architecture.\n\nThe ancient Romans employed regular orthogonal structures on which they molded their colonies. They probably were inspired by Greek and Hellenic examples, as well as by regularly planned cities that were built by the Etruscans in Italy. (see Marzabotto)\n\nThe Romans used a consolidated scheme for city planning, developed for military defense and civil convenience. The basic plan consisted of a central forum with city services, surrounded by a compact, rectilinear grid of streets, and wrapped in a wall for defense. To reduce travel times, two diagonal streets crossed the square grid, passing through the central square. A river usually flowed through the city, providing water, transport, and sewage disposal. Hundreds of towns and cities were built by the Romans throughout their empire. Many European towns, such as Turin, preserve the remains of these schemes, which show the very logical way the Romans designed their cities. They would lay out the streets at right angles, in the form of a square grid. All roads were equal in width and length, except for two, which were slightly wider than the others. One of these ran east–west, the other, north–south, and they intersected in the middle to form the center of the grid. All roads were made of carefully fitted flag stones and filled in with smaller, hard-packed rocks and pebbles. Bridges were constructed where needed. Each square marked off by four roads was called an \"insula,\" the Roman equivalent of a modern city block.\n\nEach insula was square, with the land within it divided. As the city developed, each insula would eventually be filled with buildings of various shapes and sizes and crisscrossed with back roads and alleys. Most insulae were given to the first settlers of a Roman city, but each person had to pay to construct his own house.\n\nThe city was surrounded by a wall to protect it from invaders and to mark the city limits. Areas outside city limits were left open as farmland. At the end of each main road was a large gateway with watchtowers. A portcullis covered the opening when the city was under siege, and additional watchtowers were constructed along the city walls. An aqueduct was built outside the city walls.\n\nThe development of Greek and Roman urbanization is relatively well-known, as there are relatively many written sources, and there has been much attention to the subject, since the Romans and Greeks are generally regarded as the main ancestors of modern Western culture. It should not be forgotten, though, that the Etruscans had many considerable towns and there were also other cultures with more or less urban settlements in Europe, primarily of Celtic origin.\n\nThe amphitheatre was, with the triumphal arch and basilica, the only major new type of building developed by the Romans. Some of the most impressive secular buildings are the amphitheatres, over 200 being known and many of which are well preserved, such as that at Arles, as well as its progenitor, the Colosseum in Rome. They were used for gladiatorial contests, public displays, public meetings and bullfights, the tradition of which still survives in Spain. Their typical shape, functions and name distinguish them from Roman theatres, which are more or less semicircular in shape; from the circuses (akin to hippodromes) whose much longer circuits were designed mainly for horse or chariot racing events; and from the smaller stadia, which were primarily designed for athletics and footraces.\n\nThe earliest Roman amphitheatres date from the middle of the first century BC, but most were built under Imperial rule, from the Augustan period (27 BC–14 AD) onwards. Imperial amphitheatres were built throughout the Roman empire; the largest could accommodate 40,000–60,000 spectators, and the most elaborate featured multi-storeyed, arcaded façades and were elaborately decorated with marble, stucco and statuary. After the end of gladiatorial games in the 5th century and of animal killings in the 6th, most amphitheatres fell into disrepair, and their materials were mined or recycled. Some were razed, and others converted into fortifications. A few continued as convenient open meeting places; in some of these, churches were sited.\n\nArchitecturally, they are typically an example of the Roman use of the classical orders to decorate large concrete walls pierced at intervals, where the columns have nothing to support. Aesthetically, however, the formula is successful.\n\nThe Roman basilica was a large public building where business or legal matters could be transacted. They were normally where the magistrates held court, and used for other official ceremonies, having many of the functions of the modern town hall. The first basilicas had no religious function at all. As early as the time of Augustus, a public basilica for transacting business had been part of any settlement that considered itself a city, used in the same way as the late medieval covered market houses of northern Europe, where the meeting room, for lack of urban space, was set \"above\" the arcades, however. Although their form was variable, basilicas often contained interior colonnades that divided the space, giving aisles or arcaded spaces on one or both sides, with an apse at one end (or less often at each end), where the magistrates sat, often on a slightly raised dais. The central aisle tended to be wide and was higher than the flanking aisles, so that light could penetrate through the clerestory windows.\n\nThe oldest known basilica, the Basilica Porcia, was built in Rome in 184 BC by Cato the Elder during the time he was Censor. Other early examples include the basilica at Pompeii (late 2nd century BC). After Christianity became the official religion, the basilica shape was found appropriate for the first large public churches, with the attraction of avoiding reminiscences of the Greco-Roman temple form.\n\nThe Roman circus was a large open-air venue used for public events in the ancient Roman Empire. The circuses were similar to the ancient Greek hippodromes, although circuses served varying purposes and differed in design and construction. Along with theatres and amphitheatres, Circuses were one of the main entertainment sites of the time. Circuses were venues for chariot races, horse races, and performances that commemorated important events of the empire were performed there. For events that involved re-enactments of naval battles, the circus was flooded with water.\n\nThe performance space of the Roman circus was normally, despite its name, an oblong rectangle of two linear sections of race track, separated by a median strip running along the length of about two thirds the track, joined at one end with a semicircular section and at the other end with an undivided section of track closed (in most cases) by a distinctive starting gate known as the carceres, thereby creating a circuit for the races.\n\nA forum was a central public open space in a Roman municipium, or any civitas, primarily used as a marketplace, along with the buildings used for shops and the stoas used for open stalls. Other large public buildings were often sited at the edges or close by. Many forums were constructed at remote locations along a road by the magistrate responsible for the road, in which case the forum was the only settlement at the site and had its own name, such as Forum Popili or Forum Livi.\n\nDuring the years of the Republic, Augustus claimed he \"found the city in brick and left it in marble\". While chances are high that this was an exaggeration, there is something to be said for the influx of marble use in Roman Forum from 63 BC onwards. During Augustus reign, the Forum was described to have been \"a larger, freer space than was the Forum of imperial times.\" The Forum began to take on even more changes upon the arrival of Julius Casear who drew out extensive plans for the market hub. While Casear's death came prematurely, the ideas himself, as well as Augustus had in regards to the Forum proved to be the most influential for years to come. According to Walter Dennison's The Roman Forum As Cicero Saw It, the author writes that \"the diverting of public business to the larger and splendid imperial fora erected in the vicinity resulted in leaving the general design of the Forum Romanum\".\n\nEvery city had at least one forum of varying size. In addition to its standard function as a marketplace, a forum was a gathering place of great social significance, and often the scene of diverse activities, including political discussions and debates, rendezvous, meetings, etc. Much the best known example is the Roman Forum, the earliest of several in Rome.\n\nIn new Roman towns the forum was usually located at, or just off, the intersection of the main north-south and east-west streets (the cardo and decumanus). All forums would have a Temple of Jupiter at the north end, and would also contain other temples, as well as the basilica; a public weights and measures table, so customers at the market could ensure they were not being sold short measures; and would often have the baths nearby.\n\nA horreum was a type of public warehouse used during the ancient Roman period. Although the Latin term is often used to refer to granaries, Roman horrea were used to store many other types of consumables; the giant Horrea Galbae in Rome were used not only to store grain but also olive oil, wine, foodstuffs, clothing and even marble. By the end of the imperial period, the city of Rome had nearly 300 horrea to supply its demands. The biggest were enormous, even by modern standards; the Horrea Galbae contained 140 rooms on the ground floor alone, covering an area of some 225,000 square feet (21,000 m²).\n\nThe first horrea were built in Rome towards the end of the 2nd century BC, with the first known public horreum being constructed by the ill-fated tribune, Gaius Gracchus in 123 BC. The word came to be applied to any place designated for the preservation of goods; thus it was often used refer to cellars (\"horrea subterranea\"), but it could also be applied to a place where artworks were stored, or even to a library. Some public horrea functioned somewhat like banks, where valuables could be stored, but the most important class of horrea were those where foodstuffs such as grain and olive oil were stored and distributed by the state.\n\nThe word itself is thought to have linguist roots tied to the word hordeum which in Latin means 'barley'. In the John's Hopkin's University Press, The Classical Weekly states that \"Pliny the Elder does indeed make a distinction between the two words. He describes the horreum as a structure made of brick, the walls of which were not less than three feet thick; it had no windows or openings for ventilation\". Furthermore, the storehouses would also host oil and wine and also utilize large jars that could serve as cache's for large amounts of products.These storehouses were also used to house keep large sums of money and were used much like personal storage units today are. Romans were \"These horrea were divided and subdivided, so that one could hire only so much space as one wanted, a whole room (cella), a closet (armarium), or only a chest or strong box (arca, arcula, locus, loculus).\"\n\nMulti-story apartment blocks called insulae catered to a range of residential needs. The cheapest rooms were at the top owing to the inability to escape in the event of a fire and the lack of piped water. Windows were mostly small, facing the street, with iron security bars. Insulae were often dangerous, unhealthy, and prone to fires because of overcrowding and haphazard cooking arrangements. There are examples in the Roman port town of Ostia, that date back to the reign of Trajan, but they seem to have been found only in Rome and a few other places. Elsewhere writers report them as something remarkable, but Livy and Vituvius refer to them in Rome. External walls were in \"Opus Reticulatum\" and interiors in \"Opus Incertum\", which would then be plastered and sometimes painted.\n\nTo lighten up the small dark rooms, tenants able to afford a degree of painted colourful murals on the walls. Examples have been found of jungle scenes with wild animals and exotic plants. Imitation windows (trompe l'oeil) were sometimes painted to make the rooms seem less confined.\n\nAncient Rome had elaborate and luxurious houses owned by the elite. The average house, or in cities apartment, of a commoner or plebe did not contain many luxuries. The domus, or single-family residence, was only for the well-off in Rome, with most having a layout of the closed unit, consisting of one or two rooms. Between 312 and 315 A.D. Rome had 1781 domus and 44,850 of insulae.\n\nInsulae have been the subject of great debate for historians of Roman culture, defining the various meanings of the word. Insula was a word used to describe apartment buildings, or the apartments themselves, meaning apartment, or inhabitable room, demonstrating just how small apartments for Plebes were. Urban divisions were originally street blocks, and later began to divide into smaller divisions, the word insula referring to both blocks and smaller divisions. The insula contained cenacula, tabernae, storage rooms under the stairs, and lower floor shops. Another type of housing unit for Plebes was a cenaculum, an apartment, divided into three individual rooms: cubiculum, exedra, and medianum. Common Roman apartments were mainly masses of smaller and larger structures, many with narrow balconies that present mysteries as to their use, having no doors to access them, and they lacked the excessive decoration and display of wealth that aristocrats’ houses contained. Luxury in houses was not common, as the life of the average person did not consist of being in their houses, as they instead would go to public baths, and engage in other communal activities.\n\nMany lighthouses were built around the Mediterranean and the coasts of the empire, including the Tower of Hercules at A Coruña in northern Spain, a structure which survives to this day. A smaller lighthouse at Dover, England also exists as a ruin about half the height of the original. The light would have been provided by a fire at the top of the structure.\n\nAll Roman cities had at least one thermae, a popular facility for public bathing, exercising and socializing. Exercise might include wrestling and weight-lifting, as well as swimming. Bathing was an important part of the Roman day, where some hours might be spent, at a very low cost subsidized by the government. Wealthier Romans were often accompanied by one or more slaves, who performed any required tasks such as fetching refreshment, guarding valuables, providing towels, and at the end of the session, applying olive oil to their masters' bodies which was then scraped off with a strigil, a scraper made of wood or bone. Romans did not wash with soap and water as we do now.\n\nRoman bath-houses were also provided for private villas, town houses and forts. They were normally supplied with water from an adjacent river or stream, or by aqueduct. The design of thermae is discussed by Vitruvius in De Architectura.\n\nRoman temples were among the most important and richest buildings in Roman culture, though only a few survive in any sort of complete state. Their construction and maintenance was a major part of ancient Roman religion, and all towns of any importance had at least one main temple, as well as smaller shrines. The main room \"(cella)\" housed the cult image of the deity to whom the temple was dedicated, and often a small altar for incense or libations. Behind the cella was a room or rooms used by temple attendants for storage of equipment and offerings.\n\nSome remains of many Roman temples survive, above all in Rome itself, but the relatively few near-complete examples were nearly all converted to Christian churches (and sometimes subsequently to mosques), usually a considerable time after the initial triumph of Christianity under Constantine. The decline of Roman religion was relatively slow, and the temples themselves were not appropriated by the government until a decree of the Emperor Honorius in 415. Some of the oldest surviving temples include the Temple of Hercules Victor (mid 2nd century BC) and Temple of Portunus (120-80 BC), both standing within the Forum Boarium.\n\nThe form of the Roman temple was mainly derived from the Etruscan model, but using Greek styles . Roman temples emphasised the front of the building, which followed Greek temple models and typically consisted of wide steps leading to a portico with columns, a pronaos, and usually a triangular pediment above, which was filled with statuary in the most grand examples; this was as often in terracotta as stone, and no examples have survived except as fragments. However, unlike the Greek models, which generally gave equal treatment to all sides of the temple, which could be viewed and approached from all directions, the sides and rear of Roman temples might be largely undecorated (as in the Pantheon, Rome and Vic), inaccessible by steps (as in the Maison Carrée and Vic), and even back on to other buildings. As in the Maison Carrée, columns at the side might be half-columns, emerging from (\"engaged with\" in architectural terminology) the wall. The platform on which the temple sat was typically raised higher in Roman examples than Greek, with up ten or twelve or more steps rather than the three typical in Greek temples; the Temple of Claudius was raised twenty steps. These steps were normally only at the front, and typically not the whole width of that.\n\nThe Greek classical orders in all their details were closely followed in the façades of temples, as in other prestigious buildings. However the idealized proportions between the different elements set out by the only significant Roman writer on architecture to survive, Vitruvius, and subsequent Italian Renaissance writers, do not reflect actual Roman practice, which could be very variable, though always aiming at balance and harmony. Following a Hellenistic trend, the Corinthian order and its variant the Composite order were most common in surviving Roman temples, but for small temples like that at Alcántara, a simple Tuscan order could be used.\n\nThere was considerable local variation in style, as Roman architects often tried to incorporate elements the population expected in its sacred architecture. This was especially the case in Egypt and the Near East, where different traditions of large stone temples were already millennia old. The Romano-Celtic temple was a simple style for small temples found in the Western Empire, and by far the most common type in Roman Britain. It often lacked any of the distinctive classical features, and may have had considerable continuity with pre-Roman temples of the Celtic religion.\n\nRoman theatres were built in all areas of the empire from Spain, to the Middle East. Because of the Romans' ability to influence local architecture, we see numerous theatres around the world with uniquely Roman attributes.\n\nThese buildings were semi-circular and possessed certain inherent architectural structures, with minor differences depending on the region in which they were constructed. The \"scaenae frons\" was a high back wall of the stage floor, supported by columns. The \"proscaenium\" was a wall that supported the front edge of the stage with ornately decorated niches off to the sides. The Hellenistic influence is seen through the use of the \"proscaenium\". The Roman theatre also had a \"podium\", which sometimes supported the columns of the \"scaenae frons\". The \"scaenae\" was originally not part of the building itself, constructed only to provide sufficient background for the actors. Eventually, it became a part of the edifice itself, made out of concrete. The theatre itself was divided into the stage (orchestra) and the seating section (auditorium). \"Vomitoria\" or entrances and exits were made available to the audience.\n\nA Roman villa was a country house built for the upper class, while a domus was a wealthy family's house in a town. The Empire contained many kinds of villas, not all of them lavishly appointed with mosaic floors and frescoes. In the provinces, any country house with some decorative features in the Roman style may be called a \"villa\" by modern scholars. Some were pleasure palaces such as those— like Hadrian's Villa at Tivoli— that were situated in the cool hills within easy reach of Rome or— like the Villa of the Papyri at Herculaneum— on picturesque sites overlooking the Bay of Naples. Some villas were more like the country houses of England or Poland, the visible seat of power of a local magnate, such as the famous palace rediscovered at Fishbourne in Sussex.\n\nSuburban villas on the edge of cities were also known, such as the Middle and Late Republican villas that encroached on the Campus Martius, at that time on the edge of Rome, and which can be also seen outside the city walls of Pompeii, including the Villa of the Mysteries, famous for its frescos. These early suburban villas, such as the one at Rome's Auditorium site or at Grottarossa in Rome, demonstrate the antiquity and heritage of the \"villa suburbana\" in Central Italy. It is possible that these early, suburban villas were also in fact the seats of power (maybe even palaces) of regional strongmen or heads of important families (\"gentes\").\n\nA third type of villa provided the organizational center of the large farming estates called latifundia; such villas might be lacking in luxuries. By the 4th century, \"villa\" could simply mean an agricultural estate or holding: Jerome translated the Gospel of Mark (xiv, 32) \"chorion\", describing the olive grove of Gethsemane, with \"villa\", without an inference that there were any dwellings there at all (\"Catholic Encyclopedia\" \"Gethsemane\").\n\nWith the colossal Diocletian's Palace, built in the countryside but later turned into a fortified city, a form of residential castle emerges, that anticipates the Middle Ages.\n\nThe initial invention of the watermill appears to have occurred in the hellenized eastern Mediterranean in the wake of the conquests of Alexander the Great and the rise of Hellenistic science and technology. In the subsequent Roman era, the use of water-power was diversified and different types of watermills were introduced. These include all three variants of the vertical water wheel as well as the horizontal water wheel. Apart from its main use in grinding flour, water-power was also applied to pounding grain, crushing ore, sawing stones and possibly fulling and bellows for iron furnaces.\n\nIn architecture, a monolith is a structure which has been excavated as a unit from a surrounding matrix or outcropping of rock. Monoliths are found in all types of Roman buildings. They were either: quarried without being moved; or quarried and moved; or quarried, moved and lifted clear off the ground into their position (e.g. architraves); or quarried, moved and erected in an upright position (e.g. columns).\n\nTransporting was done by land or water (or a combination of both), in the later case often by special-built ships such as obelisk carriers. For lifting operations, ancient cranes were employed since ca. 515 BC, such as in the construction of Trajan's Column.\n\nAn obelisk is a tall, four-sided, narrow tapering monument which ends in a pyramid-like shape at the top. These were originally called \"tekhenu\" by the builders, the ancient Egyptians. The Greeks who saw them used the Greek 'obeliskos' to describe them, and this word passed into Latin and then English. The Romans commissioned obelisks in an ancient Egyptian style. Examples include:\n\nRoman gardens were influenced by Egyptian, Persian, and Greek gardening techniques . In Ancient Latium, a garden was part of every farm. According to Cato the Elder, every garden should be close to the house and should have flower beds and ornamental trees. Horace wrote that during his time flower gardens became a national indulgence.\n\nGardens were not reserved for the extremely wealthy. Excavations in Pompeii show that gardens attaching to residences were scaled down to meet the space constraints of the home of the average Roman. Modified versions of Roman garden designs were adopted in Roman settlements in Africa, Gaul, and Britannia. As town houses were replaced by tall \"insula\" (apartment buildings), these urban gardens were replaced by window boxes or roof gardens. \n\nA triumphal arch is a monumental structure in the shape of an archway with one or more arched passageways, often designed to span a road. The origins of the Roman triumphal arch are unclear. There were precursors to the triumphal arch within the Roman world; in Italy, the Etruscans used elaborately decorated single bay arches as gates or portals to their cities. Surviving examples of Etruscan arches can still be seen at Perugia and Volterra. The two key elements of the triumphal arch – a round-topped arch and a square entablature – had long been in use as separate architectural elements in ancient Greece.\n\nThe innovation of the Romans was to use these elements in a single free-standing structure. The columns became purely decorative elements on the outer face of arch, while the entablature, liberated from its role as a building support, became the frame for the civic and religious messages that the arch builders wished to convey. Little is known about how the Romans viewed triumphal arches. Pliny the Elder, writing in the first century AD, was the only ancient author to discuss them. He wrote that they were intended to \"elevate above the ordinary world\" an image of an honoured person usually depicted in the form of a statue with a quadriga.\n\nThe first recorded Roman triumphal arches were set up in the time of the Roman Republic. Generals who were granted a triumph were termed \"triumphators\" and would erect \"fornices\" or honorific arches bearing statues to commemorate their victories.\nRoman triumphal practices changed significantly at the start of the imperial period when the first Roman Emperor Augustus decreed that only emperors would be granted triumphs. The triumphal arch changed from being a personal monument to being an essentially propagandistic one, serving to announce and promote the presence of the ruler and the laws of the state. Arches were not necessarily built as entrances, but – unlike many modern triumphal arches – they were often erected across roads and were intended to be passed through, not round.\n\nMost Roman triumphal arches were built during the imperial period. By the fourth century AD there were 36 such arches in Rome, of which three have survived – the Arch of Titus (AD 81), the Arch of Septimius Severus (203-205) and the Arch of Constantine (312). Numerous arches were built elsewhere in the Roman Empire. The single arch was the most common, but many triple arches were also built, of which the Triumphal Arch of Orange (\"circa\" AD 21) is the earliest surviving example. From the 2nd century AD, many examples of the \"arcus quadrifrons\" – a square triumphal arch erected over a crossroads, with arched openings on all four sides – were built, especially in North Africa. Arch-building in Rome and Italy diminished after the time of Trajan (AD 98-117) but remained widespread in the provinces during the 2nd and 3rd centuries AD; they were often erected to commemorate imperial visits.\nThe ornamentation of an arch was intended to serve as a constant visual reminder of the triumph and \"triumphator\". The façade was ornamented with marble columns, and the piers and attics with decorative cornices. Sculpted panels depicted victories and achievements, the deeds of the \"triumphator\", the captured weapons of the enemy or the triumphal procession itself. The spandrels usually depicted flying Victories, while the attic was often inscribed with a dedicatory inscription naming and praising the \"triumphator\". The piers and internal passageways were also decorated with reliefs and free-standing sculptures. The vault was ornamented with coffers. Some triumphal arches were surmounted by a statue or a \"currus triumphalis\", a group of statues depicting the emperor or general in a quadriga.\n\nInscriptions on Roman triumphal arches were works of art in themselves, with very finely cut, sometimes gilded letters. The form of each letter and the spacing between them was carefully designed for maximum clarity and simplicity, without any decorative flourishes, emphasizing the Roman taste for restraint and order. This conception of what later became the art of typography remains of fundamental importance down to the present day.\n\nRoman roads were vital to the maintenance and development of the Roman state, and were built from about 500 BC through the expansion and consolidation of the Roman Republic and the Roman Empire. They provided efficient means for the overland movement of armies, officials and civilians, and the inland carriage of official communications and trade goods. At the peak of Rome's development, no fewer than 29 great military highways radiated from the capital, and the Late Empire's 113 provinces were interconnected by 372 great road links.\nRoman road builders aimed at a regulation width (see Laws and standards above), but actual widths have been measured at between 3.6 ft (1.1 m) and more than . Today, the concrete has worn from the spaces around the stones, giving the impression of a very bumpy road, but the original practice was to produce a surface that was no doubt much closer to being flat.\n\nThe Romans constructed numerous aqueducts in order to bring water from distant sources into their cities and towns, supplying public baths, latrines, fountains and private households. Waste water was removed by complex sewage systems and released into nearby bodies of water, keeping the towns clean and free from effluent. Aqueducts also provided water for mining operations, milling, farms and gardens.\n\nAqueducts moved water through gravity alone, being constructed along a slight downward gradient within conduits of stone, brick or concrete. Most were buried beneath the ground, and followed its contours; obstructing peaks were circumvented or, less often, tunnelled through. Where valleys or lowlands intervened, the conduit was carried on bridgework, or its contents fed into high-pressure lead, ceramic or stone pipes and siphoned across. Most aqueduct systems included sedimentation tanks, sluices and distribution tanks to regulate the supply at need.\n\nRome's first aqueduct supplied a water-fountain sited at the city's cattle market. By the third century AD, the city had eleven aqueducts, sustaining a population of over a million in a water-extravagant economy; most of the water supplied the city's many public baths. Cities and municipalities throughout the Roman Empire emulated this model, and funded aqueducts as objects of public interest and civic pride, \"an expensive yet necessary luxury to which all could, and did, aspire.\"\n\nMost Roman aqueducts proved reliable, and durable; some were maintained into the early modern era, and a few are still partly in use. Methods of aqueduct surveying and construction are noted by Vitruvius in his work \"De Architectura\" (1st century BC). The general Frontinus gives more detail in his official report on the problems, uses and abuses of Imperial Rome's public water supply. Notable examples of aqueduct architecture include the supporting piers of the Aqueduct of Segovia, and the aqueduct-fed cisterns of Constantinople.\n\nRoman bridges, built by ancient Romans, were the first large and lasting bridges built. Roman bridges were built with stone and had the arch as the basic structure (see arch bridge). Most utilized concrete as well, which the Romans were the first to use for bridges.\n\nRoman arch bridges were usually semicircular, although a few were segmental (such as Alconétar Bridge). A segmental arch is an arch that is less than a semicircle. The advantages of the segmental arch bridge were that it allowed great amounts of flood water to pass under it, which would prevent the bridge from being swept away during floods and the bridge itself could be more lightweight. Generally, Roman bridges featured wedge-shaped primary arch stones (voussoirs) of the same in size and shape. The Romans built both single spans and lengthy multiple arch aqueducts, such as the Pont du Gard and Segovia Aqueduct. Their bridges featured from an early time onwards flood openings in the piers, e.g. in the Pons Fabricius in Rome (62 BC), one of the world's oldest major bridges still standing.\nRoman engineers were the first and until the industrial revolution the only ones to construct bridges with concrete, which they called Opus caementicium. The outside was usually covered with brick or ashlar, as in the Alcántara bridge.\n\nThe Romans also introduced segmental arch bridges into bridge construction. The 330 m long Limyra Bridge in southwestern Turkey features 26 segmental arches with an average span-to-rise ratio of 5.3:1, giving the bridge an unusually flat profile unsurpassed for more than a millennium. Trajan's bridge over the Danube featured open-spandrel segmental arches made of wood (standing on 40 m high concrete piers). This was to be the longest arch bridge for a thousand years both in terms of overall and individual span length, while the longest extant Roman bridge is the 790 m long Puente Romano at Mérida.\n\nRoman canals were typically multi-purpose structures, intended for irrigation, drainage, land reclamation, flood control and navigation where feasible. Some navigational canals were recorded by ancient geographers and are still traceable by modern archaeology. Channels which served the needs of urban water supply are covered at the List of aqueducts in the Roman Empire.\n\nFreshwater reservoirs were commonly set up at the termini of aqueducts and their branch lines, supplying urban households, agricultural estates, imperial palaces, thermae or naval bases of the Roman navy.\n\nRoman dam construction began in earnest in the early imperial period. For the most part, it concentrated on the semi-arid fringe of the empire, namely the provinces of North Africa, the Near East, and Hispania. The relative abundance of Spanish dams below is due partly to more intensive field work there; for Italy only the Subiaco Dams, created by emperor Nero (54–68 AD) for recreational purposes, are attested. These dams are noteworthy, though, for their extraordinary height, which remained unsurpassed anywhere in the world until the Late Middle Ages.\n\nThe most frequent dam types were earth- or rock-filled embankment dams and masonry gravity dams. These served a wide array of purposes, such as irrigation, flood control, river diversion, soil-retention, or a combination of these functions. The impermeability of Roman dams was increased by the introduction of waterproof hydraulic mortar and especially \"opus caementicium\" in the Concrete Revolution. These materials also allowed for bigger structures to be built, like the Lake Homs Dam, possibly the largest water barrier today, and the sturdy Harbaqa Dam, both of which consist of a concrete core.\n\nRoman builders were the first to realize the stabilizing effect of arches and buttresses, which they integrated into their dam designs. Previously unknown dam types introduced by the Romans include arch-gravity dams, arch dams,; buttress dams, and multiple-arch buttress dams.\n\nThe Romans generally fortified cities, rather than fortresses, but there are some fortified camps, such as the Saxon Shore forts like Porchester Castle in England. City walls were already significant in Etruscan architecture, and in the struggle for control of Italy under the early Republic many more were built, using different techniques. These included tightly-fitting massive irregular polygonal blocks, shaped to fit exactly in a way reminiscent of later Inca work. The Romans called a simple rampart wall an agger; at this date great height was not necessary. The Servian Wall around Rome was an ambitious project of the early 4th century BC. The wall was up to 10 metres (32.8 ft) in height in places, 3.6 metres (12 ft) wide at its base, 11 km (7 mi) long, and is believed to have had 16 main gates, though many of these are mentioned only from writings, with no other known remains. Some of it had a \"fossa\" or ditch in front, and an agger behind, and it was enough to deter Hannibal. Later the Aurelian Wall replaced it, enclosing an expanded city, and using more sophisticated designs, with small forts at intervals.\n\nThe Romans walled major cities and towns in areas they saw as vulnerable, and parts of many walls remain incorporated in later defensive fortifications, as at Córdoba (2nd century BC), Chester (earth and wood in the 70s AD, stone from c. 100), and York (from 70s AD). Strategic walls across open country were far rarer, and Hadrian's Wall (from 122) and the Antonine Wall (from 142, abandoned only 8 years after completion) are the most significant examples, both on the Pictish frontier of Roman Britain...\n\nOn his return from campaigns in Greece, the general Sulla brought back what is probably the most well-known element of the early imperial period, the mosaic, a decoration made of colourful chips of stone inserted into cement. This tiling method took the empire by storm in the late first century and the second century and in the Roman home joined the well known mural in decorating floors, walls, and grottoes with geometric and pictorial designs.\n\nThere were two main techniques in Greco-Roman mosaic: \"opus vermiculatum\" used tiny \"tesserae\", typically cubes of 4 millimeters or less, and was produced in workshops in relatively small panels which were transported to the site glued to some temporary support. The tiny \"tesserae\" allowed very fine detail, and an approach to the illusionism of painting. Often small panels called \"emblemata\" were inserted into walls or as the highlights of larger floor-mosaics in coarser work. The normal technique, however, was \"opus tessellatum\", using larger tesserae, which were laid on site. There was a distinct native Italian style using black on a white background, which was no doubt cheaper than fully coloured work.\n\nA specific genre of Roman mosaic obtained the name \"asaroton\" (Greek \"unswept floor\"). It represented an optical illusion of the leftovers from a feast on the floor of reach houses.\n\nA hypocaust was an ancient Roman system of underfloor heating, used to heat houses with hot air. The Roman architect Vitruvius, writing about the end of the 1st century B.C., attributes their invention to Sergius Orata. Many remains of Roman hypocausts have survived throughout Europe, western Asia, and northern Africa. The hypocaust was an invention which improved the hygiene and living conditions of citizens, and was a forerunner of modern central heating.\n\nHypocausts were used for heating hot baths (thermae), houses and other buildings, whether public or private. The floor was raised above the ground by pillars, called pilae stacks, with a layer of tiles, then a layer of concrete, then another of tiles on top; and spaces were left inside the walls so that hot air and smoke from the furnace would pass through these enclosed areas and out of flues in the roof, thereby heating but not polluting the interior of the room.\n\nIn Sicily truss roofs presumably appeared as early as 550 BC.Their potential was fully realized in the Roman period, which saw trussed roofs over 30 wide spanning the rectangular spaces of monumental public buildings such as temples, basilicas, and later churches. Such spans were three times as wide as the widest prop-and-lintel roofs and only surpassed by the largest Roman domes.\n\nThe largest truss roof by span of ancient Rome covered the Aula Regia (throne room) built for emperor Domitian (81–96 AD) on the Palatine Hill, Rome. The timber truss roof had a width of 31.67 m, slightly surpassing the postulated limit of 30 m for Roman roof constructions. Tie-beam trusses allowed for much larger spans than the older prop-and-lintel system and even concrete vaulting. Nine out of the ten largest rectangular spaces in Roman architecture were bridged this way, the only exception being the groin vaulted Basilica of Maxentius.\n\nThe spiral stair is a type of stairway which, due to its complex helical structure, was introduced relatively late into architecture. Although the oldest example dates back to the 5th century BC, it was only in the wake of the influential design of Trajan's Column that this space-saving new type permanently caught hold in Roman architecture.\n\nApart from the triumphal columns in the imperial cities of Rome and Constantinople, other types of buildings such as temples, thermae, basilicas and tombs were also fitted with spiral stairways. Their notable absence in the towers of the Aurelian Wall indicates that although used in medieval castles, they did not yet figure prominently in Roman military engineering. By late antiquity, separate stair towers were constructed adjacent to the main buildings, as in the Basilica of San Vitale.\n\nThe construction of spiral stairs passed on both to Christian and Islamic architecture.\n\n\n\n\n\n\n"}
{"id": "5359467", "url": "https://en.wikipedia.org/wiki?curid=5359467", "title": "Charles Dawson", "text": "Charles Dawson\n\nCharles Dawson (11 July 1864 – 10 August 1916) was a British amateur archaeologist who was initially credited with, and is now blamed for, discoveries that turned out to be imaginative frauds, climaxing with that of the Piltdown Man (\"Eoanthropus dawsoni\"), which he presented in 1912. Dawson was often present at finds in the archaeological digs, or was the finder himself.\n\nBorn the eldest of three sons, Dawson moved with his family from Preston, Lancashire to Hastings, Sussex when he was still very young. Charles initially studied as a lawyer following his father and pursued a hobby of collecting and studying fossils.\n\nHe made a number of seemingly important fossil finds. Amongst these were teeth from a previously unknown species of mammal, later named \"Plagiaulax dawsoni\" in his honour; three new species of dinosaur, one later named \"Iguanodon dawsoni\"; and a new form of fossil plant, \"Salaginella dawsoni\". The British Museum conferred upon him the title of Honorary Collector. For these important finds he was elected a fellow of the Geological Society and a few years later after another find, to the Society of Antiquaries of London in 1895. Dawson died prematurely from septicaemia in 1916 at Lewes, Sussex.\n\nIn 1889 Dawson was a co-founder of the Hastings and St Leonards Museum Association, one of the first voluntary museum friends groups established in Britain. Dawson worked on a voluntary basis as a member of the Museum Committee, in charge of the acquisition of artefacts and documents. His interest in archaeology developed and he had an uncanny knack of making spectacular discoveries, \"The Sussex Daily News\" named him the \"Wizard of Sussex\" for his success.\n\nIn 1893 Dawson investigated a curious flint mine full of prehistoric, Roman and mediaeval artefacts at Lavant, near Chichester and probed in inner depths of two tunnels beneath Hastings Castle. In the same year, he presented the British Museum with a Roman statuette from Beauport Park, which was made, uniquely for the period, of cast iron. Other discoveries followed, including a strange form of hafted Neolithic stone axe and a well-preserved ancient timber boat.\nHe studied ancient quarries, re-analysed the Bayeux Tapestry and, in 1909, produced what was then the definitive study of Hastings Castle. He later found faked evidence for the final phases of Roman occupation in Britain at Pevensey Castle in Sussex. Investigating unusual elements of the natural world, Dawson presented a petrified toad inside a flint nodule, discovered a large supply of natural gas at Heathfield in East Sussex, reported on a sea-serpent in the English Channel, observed a new species of human and found a strange goldfish/carp hybrid. It was even reported that he was experimenting with phosphorescent bullets as a deterrent to Zeppelin attacks on London.\n\nIn recognition of his many discoveries, Dawson was elected a fellow of the Society of Antiquaries of London in 1895. At the age of 31, and without a university degree to his name, he was now Charles Dawson F.G.S., F.S.A. His most famous discovery was in 1912 with the discovery of the Piltdown Man which was billed as the \"missing link\" between humans and other great apes.\n\nQuestions about the Piltdown find were raised from the beginning, first by Arthur Keith, but also by paleontologists and anatomists from the Smithsonian Institution and from Europe. Those disputing the find were attacked in very personal terms. Challenges to Piltdown Man arose again in the 1920s, but were again dismissed. In 1949, further questions were raised about the Piltdown Man and its authenticity, which led to the conclusive demonstration that Piltdown was a hoax, in 1953. Since then, a number of Dawson's other finds have also been shown to be forged or planted.\n\nIn 2003, Miles Russell of Bournemouth University published the results of his investigation into Dawson's antiquarian collection and concluded that at least 38 specimens were clear fakes. Russell has noted that Dawson’s whole academic career appears to have been \"one built upon deceit, sleight of hand, fraud and deception, the ultimate gain being international recognition\".\n\nIn 2016, a team of British researchers used DNA studies to provide added evidence for the provenance of Piltdown Man. They concluded that Dawson likely acted alone, and was the only person in the group of suspected perpetrators with sufficient scientific knowledge to have carried off the bluff.\n\nCharles Dawson never received a knighthood, though many others associated with the Piltdown \"find\" did, and was never elected to the Royal Society. Following his death in 1916, no further finds were made at Piltdown.\n\n\n"}
{"id": "1926861", "url": "https://en.wikipedia.org/wiki?curid=1926861", "title": "Class of 9/11", "text": "Class of 9/11\n\nThe \"Class of 9/11\" is a term coined by National Public Radio for American high school graduating classes of 2005. These students were freshmen when the September 11 attacks occurred in 2001, and have had to cope with the many aspects of the aftermath during teenage life and high school. The Class of 2002 can also be considered the Class of 9/11.\n\nThis term is now used mainly for the 2005 graduating class of West Point, which contained 911 students. According to the Associated Press report the graduating cadets were told they were \"a special group forged by historic events\".\n\nOn September 12, 2006, the first member of the class was killed. 2nd Lt. Emily Perez, a Medical Service Corps officer with the 204th Support Battalion, 2nd Brigade, 4th Infantry Division, was leading a platoon when a roadside bomb exploded, killing her. Perez, who had been the highest-ranking black and Hispanic woman in the Academy's history, was the first female West Point graduate to die in Iraq.\n"}
{"id": "4032900", "url": "https://en.wikipedia.org/wiki?curid=4032900", "title": "Claude Arpi", "text": "Claude Arpi\n\nClaude Arpi is French-born author, journalist, historian and tibetologist born in 1949 in Angoulême who lives in Auroville, India. He is the author of \"\" (Har-Anand Publications, New Delhi, 1999), and several articles on Tibet, China, India and Indo-French relations.\n\nClaude Arpi is the director of the Pavilion of Tibetan Culture at Auroville. The 14th Dalai Lama inaugurated the Pavilion, with Claude Arpi in attendance, on 20 January 2009.\n\n\n"}
{"id": "17977012", "url": "https://en.wikipedia.org/wiki?curid=17977012", "title": "Comoedia Lydiae", "text": "Comoedia Lydiae\n\nThe Comoedia Lydiae (or Lidia) is a medieval Latin elegiac comedy from the late twelfth century. The \"argument\" at the beginning of the play refers to it as the \"Lidiades\" (line 3, a play on \"Heroides\"), which the manuscripts gloss as \"comedia de Lidia facta\" (a comedy made about Lidia) and which its English translator gives as \"Adventures of Lidia\".\n\n\"Lidia\" was long ascribed to Matthieu de Vendôme, but in 1924 Edmond Faral, in his study of Latin \"fabliaux\", discounted this hypothesis. More recently, scholars have argued in favour of the authorship of the cleric Arnulf of Orléans, which now seems secure. The play was probably composed sometime shortly after 1175.\n\nCompared with the other elegiac comedies, \"Lidia\" is not as dependent on Ovid. It is dark and cynical in its view of human nature, even misogynistic. Lidia, the title character, is portrayed as a complete brute, sexually mischievous, faithless, cruel, and completely self-centred. Arnulf is explicit when he claims that Lidia is just a typical woman (line 37).\n\nIn style, \"Lidia\" is highly rhetorical. Bruno Roy called it \"the apotheosis of the pun\". Lidia's name is often punned with \"ludus\" (game) and \"ludere\" (play), often with connotations of deception or sexual activity. Women are the \"virus\" that destroys \"virum\" (man, virility). Lidia would be unsatisfied even with ten (\"decem\") men, a pun on her husband's name, Decius. The puns, though fashionable in the late twelfth century, make elegance in translation very difficult.\n\n\"Lidia\" is preserved in two fourteenth-century manuscripts. One of them may have been copied by the hand of Giovanni Boccaccio. Regardless, he certainly borrowed the tale for his \"Decameron\", 7.9. His major alteration was the name of Lidia's husband, changed from Decius to Nicostrato. Geoffrey Chaucer also borrowed aspects of \"Lidia\" for \"The Merchant's Tale\", one of \"The Canterbury Tales\".\n\nThe comedy is divided into three parts: a short \"argument\" explaining the nature and purpose of the work, a brief prologue laying out the characters and the situation, and the story itself. In the argument Arnulf claims that he is writing to improve upon his previous comedy about \"the sportive knight\", \"Miles gloriosus\". He has depicted \"all female wiles worthy of note\" so that you \"may flee forewarned: after all, you too may have a Lidia in your life\" (lines 5–6). A moralistic or didactic purpose was often given in the Middle Ages to justify the production of eroticised or sexualised literature.\n\nThe prologue begins with a pun on one of the main characters, Pyrrhus, the loyal knight of Lidia's husband, the duke Decius, and the Latin word for pear tree, \"pirus\". The pun is accommodated in English by use of \"Pearus\" for \"Pyrrhus\". A pear was a common phallic symbol from antiquity to the Middle Ages. The dramatist is poking fun at Pyrrhus when he refers to \"the pears fallen from the pear tree\" (line 8). The references to the \"jealous one\" in the prologue are probably a reference to Matthieu de Vendôme and his rivalry with Arnulf.\n\nThe tale begins by describing Lidia's dissatisfaction with her marriage. She is enamored of Pearus and whenever he passes she pretends to faint, his name gets stuck in her throat (which, given its phallic symbolism, is an innuendo for oral sex), and when she lies in her bed alone she is pleased that Decius is away. She then concocts a plan to test Pearus. She sends her elderly messenger Lusca (the one-eyed) to tell Pearus how she dies for him, would willingly give herself to him, and is unfaithful to her husband. Shocked, Pearus rationalises that it is a test of his loyalty planned by his master, Decius, and proclaims that just as Lidia is loyal to the duke, so is Pearus.\n\nWhat follows is a diatribe from Lusca on the evil of women, the promiscuity of Lidia, and the decline of the state of marriage. She decides, however, that her interests are best served by Lidia's continued infidelity, since a disloyal wife is freer with her husband's wealth. When Lusca approaches Pearus a second time, the knight is moved by the story of Hippolytus to test Lusca's allegation that Decius is a fool whom Lidia controls and deceives at will. He devises three tests for Lidia: she must kill the duke's prized falcon to prove she can deceive him, she must pluck five hairs from his beard, and she must extract one of his teeth. Each of these tests is a test of virility, since the falcon, the beard, and the tooth could all be symbols for male sexuality in the Middle Ages.\n\nIn the following scene, Lusca relays Pearus' challenge to Lidia. Lidia, dressed \"sumptuously\", then brazenly enters the noisy hall where Decius is holding court, makes an impassioned speech accusing Decius of preferring the hunting grounds to her bedchamber, and grabbing the falcon from its perch, wrings its neck in front of all. Then, laughing, she nuzzles up to Decius and plucks five hairs from his beard, claiming that they were white, making him appear older than he was.\n\nThe ruse to take Decius' tooth takes days of planning. Lidia eventually has the youthful cupbearers turn their heads to the side as they serve the wine, in the belief that they have bad breath. Then, at the banquet, she loudly proclaims that they turn aside because Decius has bad breath. Pearus is then summoned to help remove the duke's offending bad tooth. Amazed, Pearus then concedes to Lidia's newest wish: to be caught \"in flagrante delicto\" by the duke.\n\nThe plan is simple. Lidia feigns illness and the four named characters make a trip to a garden to help relieve her. When they arrive at a pear tree, Decius sends Pearus up it to fetch some fruit. While in the tree the knight, feigning modesty, pretends that he can see the duke and Lidia in the act of intercourse. Lidia explains that it is an illusion caused by the height. Decius and Pearus promptly switch places to test the illusion. While Pearus and Lidia have sex, the duke believes he is being tricked by the pear tree. When he climbs down, he orders the tree cut down, at Lidia's request, so that it will not deceive others.\n"}
{"id": "1771929", "url": "https://en.wikipedia.org/wiki?curid=1771929", "title": "Curt Weibull", "text": "Curt Weibull\n\nCurt Weibull (19 August 1886 – 10 November 1991) was a Swedish historian, educator and author.\n\nCurt Hugo Johannes Weibull was born in Lund, Sweden. He was a member of the noted Swedish . He was the son of history professor (1835–1902) and the brother of Lauritz Weibull, Alexander Weibull, Julius Oscar Elof Weibull and Carl Gustaf Weibull. He and his brothers attended the University of Lund.\n\nCurt Weibull was a professor of history at Gothenburg University from 1927–1953 and its president from 1936 to 1946. In 1928 he and his brother, Lauritz Weibull, founded the periodical \"Scandia\". Together they are known for having introduced a critical theory of history in Swedish historical research, inspired by German historian, Leopold von Ranke. Weibull was an important mentor to historian Erik Lönnroth, who further developed the methods to evaluate sources.\n\nHis most important and acclaimed work is a criticism regarding the interpretation and the ahistoricism of the \"Gesta Danorum\" by the 12th century Danish historian Saxo Grammaticus. This piece was called: \"Saxo. Kritiska undersökningar i Danmarks historia från Sven Estridsens död till Canute VI\" (Saxo. Critical studies in Denmark's history from Sven Estriden's death to Canute VI), and was rather controversial at the time, as it revealed the vague foundations of Denmark's older history of the time.\n\nIn 1991, when he was 105, his last work was published: an article in a book celebrating the 100th anniversary of Gothenburg University. That probably makes him the oldest historian in the world to have a new study published while still alive. An anecdote tells that when a Danish historian was counter-criticizing parts of Weibull's Ph.D. thesis (from 1916) on Saxo Grammaticus in his own thesis (believing Weibull to be dead since this was after his 100th birthday) Weibull appeared on the public disputation angrily defending his work. In the late 1970s, while holding a lecture about his life and his research to younger students, he had cheekily remarked about a Danish professor who had criticized his own thesis when it appeared: \"I haven't replied in depth to the criticism of the professor. But it's not too late, now is it?\"\n\nHe was the father of the Swedish historian and Liberal politician, (1924–1998) and was buried in in Lund.\n\n\n\nThis article is fully or partially based on material from Nordisk familjebok, 1904–1926 and from Nationalencyklopedin online edition.\n\n\n"}
{"id": "4463454", "url": "https://en.wikipedia.org/wiki?curid=4463454", "title": "Dauvit Broun", "text": "Dauvit Broun\n\nDauvit Broun, FRSE, FBA () (born 1961) is a Scottish historian and academic. He is the Professor of Scottish History at the University of Glasgow. A specialist in medieval Scottish and Celtic studies, he concentrates primarily on early medieval Scotland, and has written abundantly on the topic of early Scottish king-lists, as well as on literacy, charter-writing, national identity, and on the text known as \"de Situ Albanie\". He is editor of the \"New Edinburgh History of Scotland\" series, the pre-1603 editor of the \"Scottish Historical Review\", convener of the Scottish History Society, and the Principal Investigator of the Arts and Humanities Research Council-funded project 'The Paradox of Medieval Scotland, 1093-1286'.\n\nDauvit was elected a Fellow of the Royal Society of Edinburgh in 2013. In July 2017, Broun was elected a Fellow of the British Academy (FBA), the United Kingdom's national academy for the humanities and social sciences.\n\n"}
{"id": "4978737", "url": "https://en.wikipedia.org/wiki?curid=4978737", "title": "Edwards Professor of Medieval History", "text": "Edwards Professor of Medieval History\n\nThe Edwards Professor of Medieval History is a professorship at the University of Glasgow, in Scotland.\n\nFounded in 1955 as the Chair of Medieval History the name was changed in 1989 to commemorate the Glasgow scholar and antiquarian John Edwards (1846-1937).\n\n\n\n"}
{"id": "54638395", "url": "https://en.wikipedia.org/wiki?curid=54638395", "title": "Ethiopian historiography", "text": "Ethiopian historiography\n\nEthiopian historiography embodies the ancient, medieval, early modern and modern disciplines of recording the history of Ethiopia, including both native and foreign sources. The roots of Ethiopian historical writing can be traced back to the ancient Kingdom of Aksum (c. 100 – c. 940 AD). These early texts were written in either the Ethiopian Ge'ez script or the Greek alphabet, and included a variety of mediums such as manuscripts and epigraphic inscriptions on monumental stelae and obelisks documenting contemporary events. The writing of history became an established genre in Ethiopian literature during the early Solomonic dynasty (1270–1974). In this period, written histories were usually in the form of royal biographies and dynastic chronicles, supplemented by hagiographic literature and universal histories in the form of annals. Christian mythology became a linchpin of medieval Ethiopian historiography due to works such as the Orthodox \"Kebra Nagast\". This reinforced the genealogical traditions of Ethiopia's Solomonic dynasty rulers, which asserted that they were descendants of Solomon, the legendary King of Israel.\n\nEthiopian historiographic literature has been traditionally dominated by Christian theology and the chronology of the Bible. There was also considerable influence from Muslim, pagan and foreign elements from within the Horn of Africa and beyond. Diplomatic ties with Christendom were established in the Roman era under Ethiopia's first Christian king, Ezana of Axum, in the 4th century AD, and were renewed in the Late Middle Ages with embassies traveling to and from medieval Europe. Building on the legacy of ancient Greek and Roman historical writings about Ethiopia, medieval European chroniclers made attempts to describe Ethiopia, its people, and religious faith in connection to the mythical Prester John, who was viewed as a potential ally against Islamic powers. Ethiopian history and its peoples were also mentioned in works of medieval Islamic historiography and even Chinese encyclopedias, travel literature, and official histories.\n\nDuring the 16th century and onset of the early modern period, military alliances with the Portuguese Empire were made, the Jesuit Catholic missionaries arrived, and prolonged warfare with Islamic foes including the Adal Sultanate and Ottoman Empire, as well as with the polytheistic Oromo people, threatened the security of the Ethiopian Empire. These contacts and conflicts inspired works of ethnography, by authors such as the monk and historian Bahrey, which were embedded into the existing historiographic tradition and encouraged a broader view in historical chronicles for Ethiopia's place in the world. The Jesuit missionaries Pedro Páez (1564–1622) and Manuel de Almeida (1580–1646) also composed a history of Ethiopia, but it remained in manuscript form among Jesuit priests of Portuguese India and was not published in the West until modern times.\n\nModern Ethiopian historiography was developed locally by native Ethiopians as well as by foreign historians, most notably Hiob Ludolf (1624–1704), the German orientalist who British historian Edward Ullendorff (1920–2011) considered the founder of Ethiopian Studies. The late 19th and early 20th centuries marked a period where Western historiographic methods were introduced and synthesized with traditionalist practices, embodied by works such as those by Heruy Wolde Selassie (1878–1938). The discipline has since developed new approaches in studying the nation's past and offered criticism of some traditional Semitic-dominated views that have been prevalent, sometimes at the expense of Ethiopia's traditional ties with the Middle East. Marxist historiography and African studies have also played significant roles in developing the discipline. Since the 20th century, historians have given greater consideration to issues of class, gender, and ethnicity. Traditions pertaining mainly to other Afroasiatic-speaking populations have also been accorded more importance, with literary, linguistic, and archaeological analyses reshaping the perception of their roles in historical Ethiopian society. Historiography of the 20th century focused largely on the Abyssinian Crisis of 1935 and the Second Italo-Ethiopian War, whereas the Ethiopian victory over the Kingdom of Italy in the 1896 Battle of Adwa played a major role in the historiographic literature of these two countries immediately following the First Italo-Ethiopian War.\n\nWriting was introduced to Ethiopia as far back as the 5th century BC with the ancient South Arabian script. This South Semitic script served as the basis for the creation of Ethiopia's Ge'ez script, the oldest evidence for it found in Matara, Eritrea, and dated to the 2nd century AD. However, the 1st-century AD Roman \"Periplus of the Erythraean Sea\" asserts that the local ruler of Adulis could speak and write in Greek. This embrace of Hellenism could also be found in the coinage of Aksumite currency, in which legends were usually written in Greek, much like ancient Greek coinage.\n\nThe roots of the historiographic tradition in Ethiopia date back to the Aksumite period (c. 100 – c. 940 AD) and are found in epigraphic texts commissioned by monarchs to recount the deeds of their reign and royal house. Written in a false autobiographical style, in either the native Ge'ez script, the Greek alphabet, or both, they are preserved on stelae, thrones, and obelisks found in a wide geographical span that includes Sudan, Eritrea, and Ethiopia. In commemorating the contemporary ruler or aristocrats and elite members of society, these documents record various historical events such as military campaigns, diplomatic missions, and acts of philanthropy. For instance, 4th-century stelae erected by Ezana of Axum memorialize his achievements in battle and expansion of the realm in the Horn of Africa, while the \"Monumentum Adulitanum\" inscribed on a throne in Adulis, Eritrea, contains descriptions of Kaleb of Axum's conquests in the Red Sea region during the 6th century, including parts of the Arabian peninsula. It is clear that such texts influenced the epigraphy of later Aksumite rulers who still considered their lost Arabian territories as part of their realm.\n\nIn Roman historiography, the ecclesiastical history of Tyrannius Rufinus, a Latin translation and extension of the work of Eusebius dated circa 402, offers an account of the Christian conversion of Ethiopia (labeled as \"India ulterior\") by the missionary Frumentius of Tyre. The text explains that Frumentius, in order to complete this task, was ordained a bishop by Athanasius of Alexandria (298–373), most likely after 346 during the latter's third tenure as Bishop of Alexandria. The mission certainly took place before 357, when Athanasius was deposed, replaced by George of Cappadocia, and forced into flight, during which he wrote an apologetic letter to Roman emperor Constantius II (r. 337–361) that coincidentally preserved an Imperial Roman letter to the royal court of Aksum. In this letter, Constantius II addresses two \"tyrants\" of Ethiopia, Aizanas and Sazanas, who are undoubtedly Ezana and his brother Saiazana, or Sazanan, a military commander. The letter also hints that the ruler of Aksum was already a Christian monarch. From the early inscriptions of Ezana's reign it is clear that he was once a polytheist, who erected bronze, silver, and gold statues to Ares, Greek god of war. But the dual Greek and Sabaean-style Ge'ez inscriptions on the Ezana Stone, commemorating Ezana's conquests of the Kingdom of Kush (located in Nubia, i.e. modern Sudan), mention his conversion to Christianity. This claim is supported by the Christian symbol of the cross decorating virtually all Aksumite coins minted after Ezana's reign, along with the sudden discontinuation of polytheistic writings.\nCosmas Indicopleustes, a 6th-century Eastern Roman monk and former merchant who wrote the \"Christian Topography\" (describing the Indian Ocean trade leading all the way to China), visited the Aksumite port city of Adulis and included eyewitness accounts of it in his book. He copied a Greek inscription detailing the reign of an early 3rd-century polytheistic ruler of Aksum who sent a naval fleet across the Red Sea to conquer the Sabaeans in what is now Yemen, along with other parts of western Arabia. Ancient Sabaean texts from Yemen confirm that this was the Aksumite ruler Gadara, who made alliances with Sabaean kings, leading to eventual Axumite control over western Yemen that would last until the Himyarite ruler Shammar Yahri'sh (r. c. 265 – c. 287) expelled the Aksumites from southwestern Arabia. It is only from Sabaean and Himyarite inscriptions that we know the names of several Aksumite kings and princes after Gadara, including the monarchs `DBH and DTWNS. Inscriptions of king Ezana mention stone-carved thrones near the Church of Our Lady Mary of Zion in Axum (the platforms of which still exist), and Cosmas described a white-marble throne and stele in Adulis that were both covered in Greek inscriptions.\n\nAside from epigraphy, Aksumite historiography also includes the manuscript textual tradition. Some of the earliest Ethiopian illuminated manuscripts include translations of the Bible into Ge'ez, such as the Garima Gospels that were written between the 4th and 7th centuries and imitated the Byzantine style of manuscript art. The Aksum Collection containing a Ge'ez codex that provides chronologies for the diocese and episcopal sees of the Coptic Orthodox Church of Alexandria in Roman Egypt was compiled between the 5th and 7th centuries. These texts reveal how the Aksumites viewed history through the narrow lens of Christian chronology, but their early historiography was perhaps also influenced by non-Christian works, such as those from the Kingdom of Kush, the Ptolemaic dynasty of Hellenistic Egypt, and the Yemenite Jews of the Himyarite Kingdom.\n\nThe power of the Aksumite Kingdom declined after the 6th century due to the rise of other regional states in the Horn of Africa. Modern scholars continue to debate the identity and provenance of the legendary or semi-legendary figure Gudit (fl. 10th century), a queen who is traditionally believed to have overthrown the Kingdom of Aksum. The legend is found in the 13th-century chronicle of the monk Tekle Haymanot, who compiled historical writings gathered from various Ethiopian churches and monasteries. The chronicle alleges that, after being exiled from Axum, she married a Jewish king of Syria and converted to Judaism. The Scottish travel writer James Bruce (1730–1794) was incredulous about the tale and believed she was simply a Jewish queen. Carlo Conti Rossini (1872–1949) hypothesized that she was an ethnic Sidamo from Damot, whereas Steven Kaplan argues she was a non-Christian invader and historian Knud Tage Andersen contends she was a regular member of the Aksumite royal house who shrewdly seized the throne. The latter is more in line with another legend that claims Dil Na'od, the last king of Aksum, kept his daughter Mesobe Werq in isolation out of fear of a prophecy that her son would overthrow him, yet she eloped with the nobleman Mara Takla Haymanot from Lasta who eventually killed the Aksumite king in a duel, took the throne and founded the Zagwe dynasty. The latter remains one of the most poorly understood periods of Ethiopia's recorded history. What is known is that the early Zagwe kings were polytheistic, eventually converted to Christianity, and ruled over the northern Highlands of Ethiopia, while Islamic sultanates inhabited the coastal Ethiopian Lowlands.\n\nWhen the forces of Yekuno Amlak (r. 1270–1285) toppled the Zagwe dynasty in 1270 he became the first Emperor of Ethiopia, establishing a line of rulers in the Solomonic dynasty that would last into the 20th century. By this time the Greek language, once pivotal for translation in Ethiopian literature, had become marginalized and mixed with Coptic and Arabic translations. This contributed to a process by which medieval Ethiopian historians created a new historiographic tradition largely divorced from the ancient Aksumite textual corpus. The Solomonic kings professed a direct link to the kings of Aksum and a lineage traced back to Solomon and the Queen of Sheba in the Hebrew Bible. These genealogical traditions formed the basis of the \"Kebra Nagast\", a seminal work of Ethiopian literature and Ge'ez-language text originally compiled in Copto-Arabic sometime between the 10th and 13th centuries. Its current form dates to the 14th century, by which point it included detailed mythological and historical narratives relating to Ethiopia along with theological discourses on themes in the Old and New Testament. De Lorenzi compares the tome's mixture of Christian mythology with historical events to the legend of King Arthur that was greatly embellished by the Welsh cleric Geoffrey of Monmouth in his chronicle \"Historia Regum Britanniae\" of 1136. Although the \"Kebra Nagast\" indicates that the emperors of Rome or Constantinople and Ethiopia were descended from the Israelite king Solomon, there is an emphatically anti-Jewish sentiment expressed in several passages of the book.\n\nThe most common form of written history sponsored by the Solomonic royal court was the biography of contemporary rulers, who were often lauded by their biographers along with the Solomonic dynasty. The royal biographical genre was established during the reign of Amda Seyon I (r. 1314–1344), whose biography not only recounted the diplomatic exchanges and military conflicts with the rival Islamic powers of the Ifat Sultanate and Adal Sultanate, but also depicted the Ethiopian ruler as the Christian savior of his nation. The origins of the dynastic history (\"tarika nagast\") are perhaps found in the biographical chronicle of Baeda Maryam I (r. 1468–1478), which provides a narrative of his life and that of his children and was most likely written by the preceptor of the royal court. Teshale Tibebu asserts that Ethiopian court historians were \"professional flatterers\" of their ruling monarchs, akin to their Byzantine Greek and Imperial Chinese counterparts. For instance, the anonymously written biography of the emperor Gelawdewos (r. 1540–1549) speaks glowingly of the ruler, albeit in an elegiac tone, while attempting to place him and his deeds within a greater moral and historical context.\n\nThere are also hagiographies of previous Zagwe dynastic rulers composed during the Solomonic period. For instance, during the reign of Zara Yaqob (1434–1468) a chronicle focusing on Gebre Mesqel Lalibela (r. 1185–1225) portrayed him as a Christian saint who performed miracles. Conveniently for the legitimacy of the Solomonic dynasty, the chronicle stated that Lalibela did not desire for his heirs to inherit his throne.\n\nIn Greek historiography, Herodotus (484–425 BC) had written brief descriptions of ancient Ethiopians, who were also mentioned in the New Testament. Although the Byzantine Empire maintained regular relations with Ethiopia during the Early Middle Ages, the Early Muslim conquests of the 7th century severed the connection between Ethiopia and the rest of Christendom. Records of these contacts encouraged medieval Europeans to discover if Ethiopia was still Christian or had converted to Islam, an idea bolstered by the presence of Ethiopian pilgrims in the Holy Land and Jerusalem during the Crusades. During the High Middle Ages, the Mongol conquests of Genghis Khan (r. 1206–1227) led Europeans to speculate about the existence of a priestly, legendary warrior king named Prester John, who was thought to inhabit distant lands in Asia associated with Nestorian Christians and might help to defeat rival Islamic powers. The travel literature of Marco Polo and Odoric of Pordenone regarding their separate journeys to Yuan-dynasty China during the 13th and 14th centuries, respectively, and fruitless searches in southern India, helped to dispel the notion that Prester John's kingdom existed in Asia. An Ethiopian diplomatic mission sent to Christian Europe by Ethiopian emperor Wedem Arad (r. 1299–1314) in 1306 allowed the chance for the priest and cartographer Giovanni da Carignano (1250–1329) to question the embassy as they stayed in Genoa, after which he published a work describing the geographic location of their kingdom and the nature of their Christian faith.\n\nIn his 1324 \"Book of Marvels\" the Dominican missionary Jordanus, bishop of the Roman Catholic Diocese of Quilon along the Malabar Coast of India, was the first known author to suggest that Ethiopia was the location of Prester John's kingdom. The Florentine merchant Antonio Bartoli visited Ethiopia from the 1390s until about 1404 when he returned to Europe with Ethiopian diplomats. This was followed by the lengthy stay of Pietro Rombuldo in Ethiopia from 1404 to 1444 and Ethiopian diplomats attending the ecumenical Council of Florence in 1441, where they expressed some vexation with the European attendees who insisted on addressing their emperor as Prester John. Thanks to the legacy of European medieval historiography, this belief persisted beyond the Late Middle Ages. For instance, the Portuguese missionary Francisco Álvares set out for Ethiopia in 1520 believing that he was to visit the homeland of Prester John.\n\nEthiopia is mentioned in some works of Islamic historiography, usually in relation to the spread of Islam. Islamic sources state that in 615 the Aksumite king Armah (r. 614–631) provided refuge for the exiled followers of Muhammad in Axum, an event known as the First Hejira (i.e. Migration to Abyssinia). In his \"History\", the scholar ibn Wadîh al-Ya'qûbî (d. 897) of the Abbasid Caliphate identified Abyssinia (\"al-Habasha\") as being located to the north of the territory of the Berber (Somali) as well as the land of the Zanj (the \"Blacks\"). The Mamluk-Egyptian historian Shihab al-Umari (1300–1349) wrote that the historical state of Bale, neighboring the Hadiya Sultanate of southern Ethiopia, was part of an Islamic Zeila confederacy, although it fell under the control of the Ethiopian Empire in the 1330s, during the reign of Amda Seyon I. Al-Maqrizi (1364–1422), another Mamluk-Egyptian historian, wrote that the Ifat sultan Sa'ad ad-Din II (r. 1387–1415) won a crushing victory against the Christian Amhara in Bale, despite the latter's numerical superiority. He described other allegedly significant victories won by the Adal sultan Jamal ad-Din II (d. 1433) in Bale and Dawaro, where the Muslim leader was said to have taken enough war booty to provide his poorer subjects with multiple slaves. Historian Ulrich Braukämper states that these works of Islamic historiography, while demonstrating the influence and military presence of the Adal sultanate in southern Ethiopia, tend to overemphasize the importance of military victories that at best led to temporary territorial control in regions such as Bale. In his \"Description of Africa\" (1555), the historian Leo Africanus (c. 1494–1554) of Al-Andalus described Abassia (Abyssinia) as the realm of the \"Prete Ianni\" (i.e. Prester John), unto whom the Abassins (Abyssinians) were subject. He also identified Abassins as one of five main population groups on the continent alongside Africans (Moors), Egyptians, Arabians and Cafri (Cafates).\n\nContacts between the Ethiopian Empire and Imperial China seem to have been very limited, if not mostly indirect. There were some attempts in Chinese historiographic and encyclopedic literature to describe parts of Ethiopia or outside areas that it once controlled. Zhang Xiang, a scholar of Africa–China relations, asserts that the country of \"Dou le\" described in the \"Xiyu juan\" (i.e. Western Regions) chapter of the \"Book of Later Han\" was that of the Aksumite port city of Adulis. It was from this city that an envoy was sent to Luoyang, the capital of China's Han dynasty, in roughly 100 AD. The 11th-century \"New Book of Tang\" and 14th-century \"Wenxian Tongkao\" describe the country of Nubia (previously controlled by the Aksumite Kingdom) as a land of deserts south of the Byzantine Empire that was infested with malaria, where the natives of the local \"Mo-lin\" territory had black skin and consumed foods such as Persian dates. In his English translation of this document, Friedrich Hirth identified \"Mo-lin\" (\"Molin\") with the kingdom of 'Alwa and neighboring \"Lao-p'o-sa\" with the kingdom of Maqurra, both in Nubia. \n\nThe \"Wenxian Tongkao\" describes the main religions of Nubia, including the \"Da Qin\" religion (i.e. Christianity, particularly Nestorian Christianity associated with the Eastern Roman Empire) and the day of rest occurring every seven days for those following the faith of the \"Da shi\" (i.e. the Muslim Arabs). These passages are ultimately derived from the \"Jingxingji\" of Du Huan (fl. 8th century), a travel writer during the Chinese Tang dynasty (618–907) who was captured by Abbasid forces in the 751 Battle of Talas, after which he visited parts of West Asia and northeast Africa. Historian Wolbert Smidt identified the territory of \"Molin\" in Du's \"Jingxingji\" (preserved in part by the \"Tongdian\" of Du You) as the Christian kingdom of Muqurra in Nubia. He also associated the territory of \"Laobosa\" (\"Lao-p'o-sa\") depicted therein with Abyssinia, thereby making this the first Chinese text to describe Ethiopia. When Du Huan left the region to return home, he did so through the Aksumite port of Adulis. Trade activity between Ethiopia and China during the latter's Song dynasty (960–1279) seems to be confirmed by Song-Chinese coinage found in the medieval village of Harla, near Dire Dawa, Ethiopia. The Chinese Ming dynasty (1368–1644) sent diplomats to Ethiopia, which was also frequented by Chinese merchants. Although only private and indirect trade was conducted with African countries during the early Manchu-led Qing dynasty (1644–1911), the Chinese were able to refer to Chinese-written travel literature and histories about East Africa before diplomatic relations were restored with African countries in the 19th century.\n\nDuring the 16th century the Ethiopian biographical tradition became far more complex, intertextual, and broader in its view of the world given Ethiopia's direct involvement in the conflicts between the Ottoman and Portuguese empires in the Red Sea region. The annals of Dawit II (r. 1508–1540) describe the defensive war he waged against the Adal sultan Ahmad ibn Ibrahim al-Ghazi (r. 1527–1543), in an episodic format quite different from the earlier chronicling tradition. The chronicle of Gelawdewos, perhaps written by the Ethiopian Orthodox Church abbot Enbaqom (1470–1560), is far more detailed than any previous Ethiopian work of history. It explains the Ethiopian emperor's military alliance with Cristóvão da Gama (1516–1542), son of the Portuguese explorer Vasco da Gama, against the Adal Sultan al-Ghazi and his Ottoman allies, and later against the Ottoman governor of Yemen, Özdemir Pasha (d. 1560).\n\nThe biography of Galawdewos' brother and successor Menas of Ethiopia (r. 1559–1563) is divided into two parts, one dedicated to his life before taking the throne and the other to his troubled reign fighting against rebels. His chronicle was completed by the biographers of his successor Sarsa Dengel (r. 1563–1597). The latter's chronicle can be considered an epic cycle for its preface describing events in previous eras mixed with biblical allusions. It also describes conflicts against rebel nobility allied with the Ottomans as well as a military campaign against Ethiopian Jews.\n\nBy the 16th century Ethiopian works began to discuss the profound impact of foreign peoples in their own regional history. The chronicle of Gelawdewos explained the friction between the Ethiopian Orthodox Church and the Catholic missionaries from Spain and Portugal, after the arrival of the Jesuits in 1555. With the persuasion of Jesuits in his realm, emperor Susenyos I (r. 1607–1632) became the only Ethiopian ruler to convert from Orthodox Christianity to Catholicism, perhaps earlier than the accepted date of 1625, after which his attempts to convert his subjects and undermine the Orthodox church led to internal revolts. In 1593 the Ethiopian monk, historian, and ethnographer Bahrey published a work of ethnography that provided reasoning for the military success of the polytheistic Oromo people who fought against the Ethiopian Empire. Ethiopian histories of this period also included details of foreign Muslims, Jews, Christians (including those from Western Europe), Safavid Iranians, and even figures of the fallen Byzantine Empire.\nPedro Paez (1564–1622), a Spanish Jesuit at the court of Susenyos I, translated portions of the chronicles of Ethiopian emperors stretching back to the reign of Amda Seyon I in the 14th century AD, as well as the reign of king Kaleb of Axum in the 6th century AD. Some of these fragments were preserved in the \"Historia de Ethiopia\" by the Portuguese Jesuit Manuel de Almeida (1580–1646), but Paez's original manuscript was largely rewritten to remove polemical passages against the rival Dominican Order. Paez also translated a chapter from an Ethiopian hagiography that covered the life and works of the 13th-century ruler Gebre Mesqel Lalibela. The \"Historia de Ethiopia\", which arrived in Goa, India, by the end of 1624, was not published in Europe until the modern era and only remained in circulation among members of the Society of Jesus in Portuguese India, although Almeida's map of Ethiopia was published by Baltasar Teles in 1660. In his treatise \"A Voyage to Abyssinia\", the Portuguese Jesuit missionary Jerónimo Lobo (1595–1678) described Abyssinia and its denizens, indicating that the native inhabitants were of two distinct physical types: the Abyssinians proper, who were narrow-featured, near olive-skinned, and had long hair which they wore in various styles, and the Soudans, who were platyrrhine (i.e. flat-nosed), black-skinned, and had woolly hair. Following the abdication of Susenyos I, his son and successor Fasilides (r. 1632–1667) had the Jesuits expelled from Ethiopia.\n\nAt least as far back as the reign of Susenyos I the Ethiopian royal court employed an official court historian known as a \"sahafe te'ezaz\", who was usually also a senior scholar within the Ethiopian Orthodox Church. Susenyos I had his confessor Meherka Dengel and counselor Takla Sellase (d. 1638), nicknamed \"Tino\", compose his biography. Biographies were written for the emperors Yohannes I (r. 1667–1682), Iyasu I (1682–1706), and Bakaffa (r. 1721–1730), the latter employing four separate court historians: Sinoda, Demetros, Arse, and Hawaryat Krestos. The reigns of the emperors Iyasu II (r. 1730–1755) and Iyoas I (r. 1755–1769) were included in general dynastic histories, while the last known royal biography in chronicle format prior to the 19th century was written by the church scholar Gabru and covered the first reign of Tekle Giyorgis I (r. 1779–1784), the text ending abruptly just before his deposition.\n\nThe chaotic period known as the Era of the Princes (Ethiopian: \"Zemene Mesafint\") from the mid-18th to mid-19th centuries witnessed political fragmentation, civil war, loss of central authority, and, as a result of these, a complete shift away from the royal biography in favor of dynastic histories. A new genre of dynastic history, known as the \"Short Chronicle\" according to Lorenzi, was established by a church scholar named Takla Haymanot, whose work combined universal history with Solomonic dynastic history. The \"Short Chronicle\" genre of historiography continued well into the 20th century. Ge'ez became an extinct language by the 17th century, but it wasn't until the reign of Tewodros II (r. 1855–1868) that royal chronicles were written in the vernacular Semitic language of Amharic.\n\nAnother genre of history writing produced during the Era of the Princes was the terse Ethiopian annal known as \"ya'alam tarik\". These works attempted to list major world events from the time of the biblical Genesis until their present time in a universal history. For instance, the translated work of John of Nikiû explaining human history until the Muslim conquest of Egypt in 642 became a canonical text in Ethiopian historiography. There are also chronological and genealogical lists of rulers and Orthodox Church patriarchs that include some elements of historical narrative.\n\nVarious biographies of Ethiopian emperors have been compiled in the modern era. In 1975 the Oxford-educated historian Zewde Gebre-Sellassie (1926–2008) published a biography on the Emperor Yohannes II (r. 1699–1769), with whom he was distantly related. In 1973 and 1974, the Emperor Haile Selassie (r. 1930–1974) published his autobiography \"My Life and Ethiopia's Progress\"; in 1976 it was translated from Amharic into English and annotated by Edward Ullendorff in an Oxford University Press publication. Hanna Rubinkowska maintains that Emperor Selassie was an active proponent of \"historiographic manipulation\", especially when it came to concealing historical materials that seemingly contested or contradicted dynastic propaganda and official history. For instance, he removed certain chronicles and historical works from the public eye and placed them in his private library, such as \"aleqa\" Gabra Igziabiher Elyas' (1892–1969) biographical chronicle covering the reigns of Selassie's predecessors Iyasu V (r. 1913–1916), a late convert to Islam, and the Empress Zewditu (r. 1916–1930). The latter work was edited, translated into English and republished by Rudolf K. Molvaer in 1994.\n\nEdward Ullendorff considered the German orientalist Hiob Ludolf (1624–1704) to be the founder of Ethiopian studies in Europe, thanks to his efforts in documenting the history of Ethiopia and the Ge'ez language, as well as Amharic. The Ethiopian monk Abba Gorgoryos (1595–1658), while lobbying the \"Propaganda Fide\" in Rome to become bishop of Ethiopia following his Catholic conversion and expulsion of the Jesuits by Ethiopian emperor Fasilides, collaborated with Ludolf – who never actually visited Ethiopia – and provided him with critical information for composing his \"Historia Aethiopica\" and its \"Commentaries\". The ethnically-Ethiopian Portuguese cleric António d'Andrade (1610–1670) aided them as a translator, since Abba Gorgoryos was not a fluent speaker of either Latin or Italian. After Ludolf, the 18th-century Scottish travel writer James Bruce, who visited Ethiopia, and German orientalist August Dillmann (1823–1894) are also considered pioneers in the field of early Ethiopian studies. After spending time at the Ethiopian royal court, Bruce was the first to systematically collect and deposit Ethiopian historical documents into libraries of Europe, in addition to composing a history of Ethiopia based on native Ethiopian sources. Dillmann cataloged a variety of Ethiopian manuscripts, including historical chronicles, and in 1865 published the \"Lexicon Linguae Aethiopicae\", the first such lexicon to be published on languages of Ethiopia since Ludolf's work.\nEthiopian historians such as Taddesse Tamrat (1935–2013) and Sergew Hable Sellassie have argued that modern Ethiopian studies were an invention of the 17th century and originated in Europe. Tamrat considered Carlo Conti Rossini's 1928 \"Storia d'Etiopia\" a groundbreaking work in Ethiopian studies. The philosopher Messay Kebede likewise acknowledged the genuine contributions of Western scholars to the understanding of Ethiopia's past. But he also criticized the perceived scientific and institutional bias that he found to be pervasive in Ethiopian-, African-, and Western-made historiographies on Ethiopia. Specifically, Kebede took umbrage at E. A. Wallis Budge's translation of the \"Kebra Nagast\", arguing that Budge had assigned a South Arabian origin to the Queen of Sheba although the \"Kebra Nagast\" itself did not indicate such a provenience for this fabled ruler. According to Kebede, a South Arabian extraction was contradicted by biblical exegetes and testimonies from ancient historians, which instead indicated that the Queen was of African origin. Additionally, he chided Budge and Ullendorff for their postulation that the Aksumite civilization was founded by Semitic immigrants from South Arabia. Kebede argued that there is little physical difference between the Semitic-speaking populations in Ethiopia and neighboring Cushitic-speaking groups to validate the notion that the former groups were essentially descendants of South Arabian settlers, with a separate ancestral origin from other local Afroasiatic-speaking populations. He also observed that these Afroasiatic-speaking populations were heterogeneous, having interbred with each other and also assimilated alien elements of both uncertain extraction and negroid origin.\n\nDuring the late 19th and early 20th centuries, Ethiopian vernacular historiography became more heavily influenced by Western methods of historiography, but De Lorenzi contends that these were \"indigenized\" to suit the cultural sensibilities of traditionalist historians. Gabra Heywat Baykadan, a foreign-educated historian and reformist intellectual during the reign of Menelik II (r. 1889–1913), was unique among his peers for breaking almost entirely from the traditionalist approach to writing vernacular history and systematically adopting Western theoretical methods. Heruy Wolde Selassie (1878-1938), \"blattengeta\" and foreign minister of Ethiopia, used English scholarship and nominally adopted modern Western methods in writing vernacular history, but he was a firmly traditionalist historian. His innovative works include a 1922 historical dictionary that offered a prosopographic study of Ethiopia's historical figures and contemporary notables, a history of Ethiopian foreign relations, historiographic travel literature, and a traditionalist historical treatise combining narrative histories for the Zagwe and Solomonic dynasties with other parts on church history and biographies of church leaders.\n\nTakla Sadeq Makuriya (1913–2000), historian and former head of the National Archives and Library of Ethiopia, wrote various works in Amharic as well as foreign languages, including a four-volume Amharic-language series on the history of Ethiopia from ancient times until the reign of Selassie, published in the 1950s. During the 1980s he published a three-volume tome exploring the reigns of 19th-century Ethiopian rulers and the theme of national unity. He also produced two English chapters on the history of the Horn of Africa for UNESCO's \"General History of Africa\" and several French-language works on Ethiopia's church history and royal genealogies. Some volumes from his vernacular survey on general Ethiopian history have been edited and circulated as school textbooks in Ethiopian classrooms by the Ministry of Education. Kebede Michael (1916–1998), a playwright, historian, editor, and director of archaeology at the National Library, wrote works of world history, histories of Western civilization, and histories of Ethiopia, which, unlike his previous works, formed the central focus of his 1955 world history written in Amharic.\n\nThe decisive victory of the Ethiopian Empire over the Kingdom of Italy in the 1896 Battle of Adwa, during the First Italo-Ethiopian War, made a profound impact on the historiography of Italy and Ethiopia. It was not lost to the collective memory of Italians, since the Italian capture of Adwa, Tigray Region, Ethiopia in 1935, during the Second Italo-Ethiopian War, was hailed as an act that avenged their previous humiliation and defeat. Historiography about Ethiopia throughout much of the 20th century focused primarily on this second invasion and the Abyssinian Crisis that preceded it, in which Ethiopia was depicted as being relegated to the role of a pawn in European diplomacy. The Ethiopian courtier (i.e. \"blatta\") and historian Marse Hazan Walda Qirqos (1899–1978) was commissioned by the Selassie regime to compile a documentary history of the Italian occupation entitled \"A Short History of the Five Years of Hardship\", composed concurrently with the submission of historical evidence to the United Nations War Crimes Commission for Fascist Italy's war crimes. Coauthored by Berhanu Denque, this work was one of the first vernacular Amharic histories to cover the Italian colonial period, documenting contemporary newspaper articles and propaganda pieces, events such as the 1936 fall of Addis Ababa and the 1941 British-Ethiopian reconquest of the country, and speeches by key figures such as Emperor Selassie and Rodolfo Graziani (1882–1955), Viceroy of Italian East Africa.\n\nModern historians have taken new approaches to analyzing both traditional and modern Ethiopian historiography. For instance, Donald Crummey (1941–2013) investigated instances in Ethiopian historiography dealing with class, ethnicity, and gender. He also criticized earlier approaches made by Sylvia Pankhurst (1882–1960) and Richard Pankhurst (1927–2017), who focused primarily on the Ethiopian ruling class while ignoring marginalized peoples and minority groups in Ethiopian historical works. Following the 1974 Ethiopian Revolution and overthrow of the Solomonic dynasty with the deposition of Haile Selassie, the historical materialism of Marxist historiography came to dominate the academic landscape and understanding of Northeast African history. In her 2001 article \"Women in Ethiopian History: A Bibliographic Review\", Belete Bizuneh remarks that the impact of social history on African historiography in the 20th century generated an unprecedented focus on the roles of women and gender in historical societies, but that Ethiopian historiography seems to have fallen outside the orbit of these historiographic trends.\n\nBy relying on the written works of both Christian and Muslim authors, oral traditions, and modern methods of anthropology, archaeology, and linguistics, Mohammed Hassen, Associate Professor of History at Georgia State University, asserts that the largely non-Christian Oromo people have interacted and lived among the Semitic-speaking Christian Amhara people since at least the 14th century, not the 16th century as is commonly accepted in both traditional and recent Ethiopian historiography. His work also stresses Ethiopia's need to properly integrate its Oromo population and the fact that the Cushitic-speaking Oromo, despite their traditional reputation as invaders, were significantly involved in maintaining the cultural, political, and military institutions of the Christian state.\n\nIn his 1992 review of Naguib Mahfouz's \"The Search\" (1964), the Ethiopian scholar Mulugeta Gudeta observed that Ethiopian and Egyptian societies bore striking historical resemblances. According to Haggai Erlich, these parallels culminated in the establishment of the Egyptian \"abun\" ecclesiastical office, which exemplified Ethiopia's traditional connection to Egypt and the Middle East. In the earlier part of the 20th century, Egyptian nationalists also propounded the idea of forming a \"Unity of the Nile Valley\", a territorial union that would include Ethiopia. This objective gradually ebbed due to political tension over control of the Nile waters. Consequently, after the 1950s, Egyptian scholars adopted a more distant if not apathetic approach to Ethiopian affairs and academic studies. For instance, the Fifth Nile 2002 Conference held in Addis Ababa in 1997 was attended by hundreds of scholars and officials, among whom were 163 Ethiopians and 16 Egyptians. By contrast, there were no Egyptian attendees at the Fourteenth International Conference of Ethiopian Studies later held in Addis Ababa in 2000, similar to all previous ICES conferences since the 1960s.\n\nErlich argues that, in deference to their training as Africanists, native and foreign Ethiopianists of the post-1950 generation focused more on historiographic matters pertaining to Ethiopia's place within the African continent. This trend had the effect of marginalizing Ethiopia's traditional bonds with the Middle East in historiographic works. In Bahru Zewde's retrospective on Ethiopian historiography published in 2000, he highlighted Ethiopia's ancient tradition of historiography, observing that it dates from at least the fourteenth century and distinguishes the territory from most other areas in Africa. He also noted a shift in emphasis in Ethiopian studies away from the field's traditional fixation on Ethiopia's northern Semitic-speaking groups, with an increasing focus on the territory's other Afroasiatic-speaking communities. Zewde suggested that this development was made possible by a greater critical usage of oral traditions. He offered no survey of Ethiopia's role in Middle Eastern studies and made no mention of Egyptian-Ethiopian historical relations. Zewde also observed that historiographic studies in Africa were centered on methods and schools that were primarily developed in Nigeria and Tanzania, and concluded that \"the integration of Ethiopian historiography into the African mainstream, a perennial concern, is still far from being achieved to a satisfactory degree.\"\n\n\n"}
{"id": "33407176", "url": "https://en.wikipedia.org/wiki?curid=33407176", "title": "Francesco Danieli", "text": "Francesco Danieli\n\nFrancesco Danieli (1981 in Lecce) is an Italian historian and iconologist.\n\nBorn in 1981, Francesco Danieli is an Italian historian and iconologist. He is known in the international scene, especially for his work on art and faith in Tridentine Italy and his study on Philip Neri and his aesthetic experience.\nDanieli studied Philosophy, Theology, Archaeology, Art and History in Naples and Rome. He is Director of \"Gli Argonauti\", series of publications by Edizioni Universitarie Romane (Rome). He has been involved in various international research projects and conferences. His extensive publication record includes books, essays, reviews, and newspaper columns.\n\n"}
{"id": "227189", "url": "https://en.wikipedia.org/wiki?curid=227189", "title": "Friedrich Christoph Dahlmann", "text": "Friedrich Christoph Dahlmann\n\nFriedrich Christoph Dahlmann (13 May 1785, Wismar5 December 1860, Bonn) was a German historian and politician.\n\nHe came of an old Hanseatic family of Wismar, then controlled by Sweden. His father, who was burgomaster of the town, intended him to study theology, but Friedrich preferred classical philology, which he studied from 1802 to 1806 at the University of Copenhagen, University of Halle, and then again at Copenhagen. After finishing his studies, he translated some of the Greek tragic poets, and The Clouds of Aristophanes. But he was also interested in modern literature and philosophy; and the troubles of the times, of which he had personal experience, aroused in him a strong feeling of German patriotism, though throughout his life he was always proud of his connection with Scandinavia, and Gustavus Adolphus was his particular hero.\n\nIn 1809, on the outbreak of war in Austria, Dahlmann, together with the poet Heinrich von Kleist, whom he had met in Dresden, went to Bohemia, and was afterwards with the Imperial army, up till the Battle of Aspern, with the somewhat vague object of trying to convert the Austrian war into a German one. This hope was shattered by the defeat of Wagram.\n\nHe now decided to try his fortunes in Denmark, where he had influential relations. After taking his doctor's degree at Wittenberg (1810) he qualified at Copenhagen in 1811, with an essay on the origins of the ancient theatre, as a lecturer on ancient literature and history, on which he delivered lectures in Latin. His influential friends soon brought him further advancement. As early as 1812 he was summoned to Kiel, as successor to the historian Dietrich Hermann Hegewisch (1746–1812). This appointment proved in two respects a decisive moment in his career; on the one hand it made him give his whole attention to a subject for which he was admirably suited, but to which he had so far given only a secondary interest; and on the other hand, it threw him into politics.\n\nIn 1815 he obtained, in addition to his professorate, the position of secretary to the perpetual deputation of the estates of Schleswig-Holstein. In this capacity he began, by means of memoirs or of articles in the Kieler Bldlter, which he founded himself, to appear as an able and zealous champion of the half-forgotten rights of the Elbe duchies, as against Denmark, and of their close connexion with Germany. It was he upon whom the Danes afterwards threw the blame of having invented the Schleswig-Holstein question; certainly his activities form an important link in the chain of events which eventually led to the solution of 1864. So far as this interest affected himself, the chief profit lay in the fact that it deepened his conception of the state, and directed it to more practical ends. Whereas at that time mere speculation dominated both. the French Liberalism of the school of Rotteck, and Karl Ludwig von Haller's Romanticist doctrine of the Christian state, Dahlmann took as his premises the circumstances as he found them, and evolved the new out of the old by a quiet process of development. Moreover, in the inevitable conflict with the Danish crown his upright point of view and his German patriotism were further confirmed.\n\nAfter transferring to Göttingen around 1829 he had the opportunity of working in the same spirit. As confidant of the duke of Cambridge, he was allowed to take a share in framing the Hanoverian constitution of 1833, which remodelled the old aristocratic government in a direction which had become inevitable since the July revolution in Paris; and when in 1837 the new king Ernst August declared the constitution invalid, Dahlmann inspired the famous protest of the seven professors of Göttingen. Though deprived of his position and banished, he had the satisfaction of knowing that German national feeling received a boost from his courageous action, while public subscriptions saved him from poverty.\n\nAfter several years in Leipzig and Jena, King Frederick William IV of Prussia appointed him in October 1842 to a professorship at the University of Bonn. The years that followed were those of his greatest fame. His \"Politik\" (1835) had already made him a name as a writer; he now published his \"Dänische Geschichte\" (1840–1843), a historical work of the first rank; and this was soon followed by histories of the English and French revolutions, which, though of less scientific value, exercised a decisive influence upon public opinion by their open advocacy of the system of constitutional monarchy. As a teacher too he was much beloved. Though no orator, and in spite of a personality not particularly amiable or winning, he produced a profound impression upon young men by the pregnancy of his expression, a consistent logical method of thought based on Immanuel Kant and by the manliness of his character.\n\nWhen the revolution of 1848 broke out, the \"father of German nationality,\" as the provisional government at Milan called him, found himself the centre of universal interest. Both Mecklenburg and Prussia offered him in vain the post of envoy to the diet of the confederation. Naturally, too, he was elected to the national assembly at Frankfurt, and took a leading part in the constitutional committees appointed first by the diet, then by the parliament. His objective was to make Germany as far as possible a united constitutional monarchy, with the exclusion of the whole of Austria, or at least, of its non-German parts. Prussia was to provide the emperor, but at the same time—and in this lay the doctrinaire weakness of the system—was to give up its separate existence, consecrated by history, in the same way as the other states. When, therefore, Frederick William IV, without showing any anxiety to bind himself by the conditions laid down at Frankfurt, concluded with Denmark the seven months truce of Malmö (26 August 1848), Dahlmann proposed that the national parliament should refuse to recognize the truce, with the express intention of clearing up once for all the relations of the parliament with the court of Berlin. The motion was passed by a small majority (September 5); but the members of Dahlmann's party were just those who voted against it, and it was they who on September 18 reversed the previous vote and passed a resolution accepting the truce, after Dahlmann had failed to form a ministry on the basis of the resolution of the 5th, owing to his objection to the Radicals.\n\nDahlmann afterwards described this as the decisive turning-point in the fate of the parliament. He did not immediately give up hope. Though he took little active part in parliamentary debates, he was very active on commissions and in party conferences, and it was largely owing to him that a German constitution was at last evolved, and that Frederick William IV was elected hereditary emperor (28 March 1849). He was accordingly one of the deputation which offered the crown to the king in Berlin. The king's refusal came as less of a surprise to him than to most of his colleagues. He counted on being able to compel recognition of the constitution by the moral pressure of the consent of the people. It was only when the attitude of the Radicals made it clear to him that this course would lead to a revolution, that he decided, after a long struggle, to retire from the national parliament (21 May).\n\nHe remained one of the chief promoters of the well-known conference of the imperial party at Gotha, the proceedings of which were not, however, satisfactory to him; and he took part in the sessions of the first Prussian chamber (1849–1850) and of the parliament of Erfurt (1850). But finally, convinced that for the moment all efforts towards the unity of Germany were unavailing, he retired from political life, though often pressed to stand for election, and again took up his work of teaching at Bonn. His last years were, however, saddened by illness, bereavement and continual friction with his colleagues. His death followed an apoplectic fit.\n\nDahkmann's chief works included:\n"}
{"id": "3586970", "url": "https://en.wikipedia.org/wiki?curid=3586970", "title": "Golden age hip hop", "text": "Golden age hip hop\n\nGolden age hip hop is a name given to mainstream hip hop music created in the mid/late 1980s and early 1990s, particularly by artists and musicians originating from the New York metropolitan area. It is characterized by its diversity, quality, innovation and influence on hip hop after the genre's emergence and establishment in the previous decade. There were various types of subject matter, while the music was experimental and the sampling from old records was eclectic.\n\nThe artists most often associated with the period are LL Cool J, Slick Rick, Ultramagnetic MC's, the Jungle Brothers, Run–D.M.C., Public Enemy, Beastie Boys, KRS-One, Eric B. & Rakim, De La Soul, Big Daddy Kane, EPMD, and A Tribe Called Quest. Releases by these acts co-existed in this period with, and were as commercially viable as, those of early gangsta rap artists such as Ice-T, Geto Boys and N.W.A, the sex raps of 2 Live Crew and Too Short, and party-oriented music by acts such as Kid 'n Play, The Fat Boys, DJ Jazzy Jeff & The Fresh Prince and MC Hammer.\n\nThe golden age is noted for its innovation – a time \"when it seemed that every new single reinvented the genre,\" according to \"Rolling Stone\". Referring to \"hip-hop in its golden age\", \"Spin\"<nowiki>'</nowiki>s editor-in-chief Sia Michel said, \"there were so many important, groundbreaking albums coming out right about that time\",\nand MTV's Sway Calloway added: \"The thing that made that era so great is that nothing was contrived. Everything was still being discovered and everything was still innovative and new\". Writer William Jelani Cobb said, \"what made the era they inaugurated worthy of the term golden was the sheer number of stylistic innovations that came into existence... in these golden years, a critical mass of mic prodigies were literally creating themselves and their art form at the same time\".\n\nThe term \"Golden age hip hop\" frames the late 1980s in mainstream hip hop, said to be characterized by its diversity, quality, innovation and influence, and associated with Public Enemy, KRS-One and his Boogie Down Productions, Eric B. & Rakim, Ultramagnetic MCs, De La Soul, A Tribe Called Quest, and the Jungle Brothers due to their themes of Afrocentricity and political militancy, their experimental music, and their eclectic sampling. This same period is sometimes referred to as \"mid-school\" or a \"middle school\" in hip hop, the phrase covering acts such as Gang Starr, The UMC's, Main Source, Lord Finesse, EPMD, Just Ice, Stetsasonic, True Mathematics, and Mantronix.\n\nThe innovations of Run-D.M.C., LL Cool J, and new school producers such as Larry Smith, and Rick Rubin of Def Jam Recordings, were quickly advanced on by the Beastie Boys, Marley Marl and his Juice Crew MCs, Boogie Down Productions, Public Enemy, and Eric B. & Rakim. Hip hop production became denser, rhymes and beats faster, as the drum machine was augmented with the sampler technology. Rakim took lyrics about the art of rapping to new heights, while KRS-One and Chuck D pushed \"message rap\" towards black activism. Native Tongues artists' inclusive, sample-crowded music accompanied their positivity, Afrocentricity and playful energy. With the eventual commercial dominance of West Coast gangsta rap, particularly the emergence of the relaxed sounds of G-funk by the early nineties, the East Coast new school/golden age can be said to have ended, with hardcore rappers such as the Wu-Tang Clan and gangsta rappers such as Nas and The Notorious B.I.G. coming to dominate the East Coast scene.\n\nDuring the golden age of hip hop, samples were heavily used. The ability to sample different beats, riffs and patterns from a wide variety of sources gave birth to a new breed of producers and DJs who did not necessarily need formal musical training or instruments, just a good ear for sound collages. These samples were derived from a number of genres, ranging from jazz, funk and soul to rock and roll. For example, \"Paul's Boutique\", the Beastie Boys' second studio album, drew from over 200 individual samples, 24 of which were featured on the last track of the album. Samples and sound bites were not limited to just music. RZA of the Wu-Tang Clan, a hip hop collective formed in the 1990s, sampled sound clips from his own collection of 1970s kung-fu films to bolster and frame the group's gritty lyrical content. Many of the sample-laden albums released during this time would not be able to receive legal clearance today.\n\nThe era also provided some of the greatest advances in rapping technique. Kool G Rap, referring to the golden age in the book \"How to Rap\" said, \"that era bred rappers like a Big Daddy Kane, a KRS-One, a Rakim, a Chuck D... their rapping capability and ability – these dudes were phenomenal\". Many of hip hop's biggest artists were also at their creative peak. Allmusic said the golden age \"witnessed the best recordings from some of the biggest rappers in the genre's history... overwhelmingly based in New York City, golden age rap is characterized by skeletal beats, samples cribbed from hard rock or soul tracks, and tough dis raps... rhymers like PE's Chuck D, Big Daddy Kane, KRS-One, Rakim, and LL Cool J basically invented the complex wordplay and lyrical kung-fu of later hip-hop\".\n\nIn addition to lyrical self-glorification, hip hop was also used as a form of social protest. Lyrical content from the era often drew attention to a variety of social issues including Afrocentric living, drug use, crime and violence, religion, culture, the state of the American economy, and the modern man's struggle. Conscious and political hip hop tracks of the time were a response to the effects of American capitalism and former President Reagan's conservative political economy. According to Rose Tricia, \"In rap, relationships between black cultural practice, social and economic conditions, technology, sexual and racial politics, and the institution policing of the popular terrain are complex and in constant motion. Even though hip hop was used as a mechanism for different social issues it was still very complex with issues within the movement itself.\n\nThere was also often an emphasis on black nationalism. Hip hop scholar Michael Eric Dyson stated, \"during the golden age of hip hop, from 1987 to 1993, Afrocentric and black nationalist rap were prominent\", and critic Scott Thill described the time as \"the golden age of hip hop, the late '80s and early '90s when the form most capably fused the militancy of its Black Panther and Watts Prophets forebears with the wide-open cultural experimentalism of De La Soul and others\". Stylistic variety was also prominent. MSNBC said that in the golden age, \"rappers had an individual sound that was dictated by their region and their communities, not by a marketing strategist\" and the \"Village Voice\" referred to the golden age's \"eclecticism\".\n\nAllMusic writes, \"Hip-hop's golden age is bookended by the commercial breakthrough of Run–D.M.C. in 1986 and the explosion of gangsta rap with NWA in the late 80s and Dr. Dre and Snoop Doggy Dogg in 1993.\" However, the specific time period that the golden age covers varies among different sources. The \"New York Times\" also defines hip-hop's golden age as the \"late 1980's and early 90's\". Ed Simmons of The Chemical Brothers said, \"there was that golden age of hip-hop in the early 90s when the Jungle Brothers made \"Straight Out the Jungle\" and De La Soul made \"Three Feet High and Rising\"\" (though these records were in fact made in 1988 and 1989 respectively). MSNBC called the 1980s the \"Golden Age\" of hip-hop music. The Guardian states, \"The golden age of hip-hop, from 1986 to 1993, gave the world an amazing number of great records,\" and also describes the period in November 1993, when A Tribe Called Quest and Wu-Tang Clan released albums, as \"The Next Golden age\"\n\nThe golden age is described by scholar Mickey Hess as \"circa 1986-1994.\" Music critic Tony Green, in the book \"Classic Material\", refers to the two-year period 1993–1994 as \"a second Golden Age\" that saw influential, high-quality albums using elements of past classicism – drum machines (Roland TR-808), drum samplers (Akai MPC60, E-mu SP-1200), turntable scratches, references to old school hip hop hits, and \"tongue-twisting triplet verbalisms\" – while making clear that new directions were being taken. Green lists as examples the Wu-Tang Clan's \"Enter the Wu-Tang (36 Chambers)\", Nas' \"Illmatic\", De La Soul's 1993 release \"Buhloone Mindstate\", Snoop Doggy Dogg's \"Doggystyle\", A Tribe Called Quest's third album \"Midnight Marauders\" and the Outkast debut \"Southernplayalisticadillacmuzik\".\n\nAccording to copyright, music, and pop culture scholars Kembrew Mcleod and Peter DiCola, the golden age of hip-hop sampling spans from 1987-1992. Artists and record labels were not yet aware of the permanence of hip-hop culture in mainstream media, and did not yet accept it as a legitimate institution. They believe the ruling made in \"Grand Upright Music, Ltd. v. Warner Bros. Records Inc.\" marked the end of the golden age of hip hop and its sampling practices.\n\nNotable hip hop producer and innovator, Marley Marl, formed the Juice Crew hip hop collective. Marl also founded Cold Chillin' Records and assembled various hip hop acts, including MC Shan, Big Daddy Kane, Biz Markie, Roxanne Shanté, Kool G Rap & DJ Polo, and Masta Ace. His Juice Crew collective was an important force in ushering the \"golden age\" era of hip hop, with advances in lyrical technique, distinctive personalities of emerging artists like Biz Markie and Big Daddy Kane, and attaining crossover commercial success for hip hop music. Marley Marl's first production was an \"answer record\" to \"Sucker MCs\" in 1983 entitled \"Sucker DJs\" by Dimples D. Soon after came 14-year-old Roxanne Shanté's answer to UTFO's \"Roxanne Roxanne\", \"Roxanne's Revenge\" (1985), sparking off the huge wave of answer records known as the Roxanne Wars. More disses (insults intended to show disrespect) from Shanté followed: \"Bite This\" (1985), \"Queen of Rox\" (1985), introducing Biz Markie on \"Def Fresh Crew\" (1986), \"Payback\" (1987), and \"Have a Nice Day\" (1987).\n\nShante's \"Have a Nice Day\" had aimed some barbs at the principal two members of a new group from the Bronx called Boogie Down Productions (BDP): \"Now KRS-ONE you should go on vacation with that name soundin' like a wack radio station, and as for Scott La Rock, you should be ashamed, when T La Rock said \"It's Yours\", he didn't mean his name\". Boogie Down Productions had manufactured a disagreement with the Juice Crew's MC Shan, releasing \"South Bronx\" and \"The Bridge is Over\" in reply to his \"The Bridge\" and \"Kill That Noise\" respectively. KRS-One considered Run-D.M.C. the epitome of rap music in 1984 and had begun to rap following their lead. He has also said that BDP's approach reflected a feeling that the early innovators like Run-D.M.C. and LL Cool J were by 1986 tainted by commercial success and out of touch with the streets.\n\nBoogie Down's first album \"Criminal Minded\" (1987) admitted a reggae influence and had KRS-One imitating the Beatles' \"Hey Jude\" on the title track. It also contained two tales of grim street life, yet played for callous laughs: \"The P Is Free\", in which KRS speaks of throwing out his girl who wants crack cocaine in exchange for sex, and \"9mm Goes Bang\", in which he shoots a drug dealer then cheerfully sings \"la la la la la la\". Songs like these presaged the rise of an underground that matched violent lyrics to the hardcore drum machine tracks of the new school. The cover of \"Criminal Minded\" was a further reflection of a move towards this sort of radical image, depicting the group in a half-light, holding firearms. The next album \"By All Means Necessary\" (1988) left that element behind for political radicalism following the murder of Scott La Rock, with its title and cover alluding to Malcolm X. KRS-One became involved with the Stop the Violence Movement at this time. Boogie Down Productions, along with Run-D.M.C. and Public Enemy, associated the new school as rap music with a strong message.\n\nEric B. & Rakim appeared with the Marley Marl produced \"Eric B. Is President\" and \"My Melody\" on Zakia Records in 1986. Both tracks appeared on \"Paid in Full\" (1987). Just as Boogie Down Productions had, the pair reflected changes in street life on their debut's cover, which depicted the two wearing large gold chains and surrounded by money. Like \"Criminal Minded\", the sampling prevalent in the album cemented James Brown's status as a hip hop source, while Rakim's allusions showed the growing influence of mystic Islam-offshoot The Nation of Gods and Earths in hip-hop. The music was minimalist, austerely so, with many writers noting that coupled with Rakim's precise, logical style, the effect was almost one of scientific rigour. The group followed \"Paid in Full\" with \"Follow The Leader\" (1988) (on which they were open-minded enough to sample The Eagles), \"Let The Rhythm Hit 'Em\" (1990) and \"Don't Sweat The Technique\" (1992).\n\nRakim is generally regarded as the most cutting-edge of the MCs of the new school era.\nJess Harvell in \"Pitchfork\" in 2005 wrote that \"Rakim's innovation was applying a patina of intellectual detachment to rap's most sacred cause: talking shit about how you're a better rapper than everyone else.\" Christgau in the \"Village Voice\" in 1990 wrote of Rakim's style as \"calm, confident, clear. On their third album, as on their phase-shifting 1986 debut,\" he continues, \"Eric B.'s samples truly are beats, designed to accentuate the natural music of an idealized black man's voice.\" Looking back at the late eighties in \"Rolling Stone\" in 1997, Ed Moralez describes Rakim as \"the new-school MC of the moment, using a smooth baritone to become the jazz soloist of mystic Afrocentric rap.\"\n\nPublic Enemy, having been reluctantly convinced to sign to a record label, released \"Yo! Bum Rush the Show\" on Def Jam in 1987. It debuted the Public Enemy logo, a circle of hatted b-boy in a sniper's cross-hairs, was replete with battle rhymes (\"Miuzi Weighs a Ton\", \"Public Enemy #1\"), social-political fare (\"Rightstarter (Message to a Black Man)\" and anti-crack messages (\"Megablast\"). The album was a critical and commercial success, particularly in Europe, unusually so for a hip hop album at that time. \"Bumrush the Show\" had been recorded on the heels of Run-D.M.C.'s \"Raising Hell\", but was held back by Def Jam in order for them to concentrate on releasing and promoting the Beastie Boys' \"License to Ill\". Chuck D of Public Enemy felt that by the time their first record was released, Boogie Down Productions and Rakim had already changed the landscape for how an MC could rap. Public Enemy were already recording their second album \"It Takes a Nation of Millions to Hold Us Back\" (Def Jam, 1988) when \"Bumrush\" hit stores.\n\nThe underground sound, centered on urban violence, that was to become gangsta rap, existed on the East Coast soon after Run–D.M.C. had inaugurated the new school of hip hop. Philadelphia's Schoolly D self-released \"Gangsta Boogie\" in 1984, and \"P.S.K. What Does It Mean?\"/\"Gucci Time\" in 1985, leading to \"Saturday Night\" (Schoolly D, 1986, Jive, 1987). The West Coast, which became the home of gangsta rap, had Toddy Tee's influential \"Batteram\" mixtape in 1985, and Ice-T's \"Six in the Morning\" in 1986 before N.W.A's first records, leading to the hugely successful \"Straight Outta Compton\" in 1988.\n\nDevelopments in the New York new school continuum in this climate were represented by the Native Tongues groups—The Jungle Brothers, De La Soul, A Tribe Called Quest, Queen Latifah and Monie Love—along with fellow travellers like Leaders of the New School, KMD and Brand Nubian. They moved away from aggressive, macho posturing, towards ambiguity, fun and Afrocentricity. Their music was sample-crowded, more open and accessible than their new school predecessors. De La Soul's debut sampled everyone from The Turtles to Steely Dan, while A Tribe Called Quest matched tough beats to mellow jazz samples and playful, thoughtful raps.\n\nThis lawsuit was known for effectively ending the \"Wild West\" period for sampling during the golden age of hip hop. In 1991, Gilbert O'Sullivan's song publisher sued Warner Brothers Records over the use of the original in Biz Markie's song \"Alone Again.\" No copyright case precedents were cited in the ruling of the final verdict, and the presiding judge's opinion was prefaced with the words \"Thou Shalt not Steal.\"\n\nThe sixties pop band The Turtles filed a lawsuit in 1989 against hip hop group De La Soul for the uncleared use of a sampled element derived from their original 1968 track \"You Showed Me.\" The lawsuit was settled out of court for a reported $1.7 million, though group members later claimed that the actual payout was significantly less.\n\nAccording to a number of sources, such as; \"Rolling Stone\", \"The Village Voice\", \"Pittsburgh Post-Gazette\", Allmusic, \"The Age\", MSNBC, and author William Jelani Cobb, the following were key artists in the golden age of hip hop:\n\nA\n\nB\n\nC\n\nD\n\nE\n\nF\n\n\nG\n\nH\n\nI\n\nJ\n\n\nK\n\nL\n\nM\n\nN\n\nO\n\nP\n\nQ\n\nR\n\nS\n\nT\n\nU\nX\n\nY\n"}
{"id": "55504987", "url": "https://en.wikipedia.org/wiki?curid=55504987", "title": "Grahame Clark Medal", "text": "Grahame Clark Medal\n\nThe Grahame Clark Medal is awarded by the British Academy every two years \"for academic achievement involving recent contributions to the study of prehistoric archaeology\". It was endowed in 1992 by Sir Grahame Clark, an eminent prehistorian and archaeologist, and first awarded in 1993.\n\n\n"}
{"id": "2647027", "url": "https://en.wikipedia.org/wiki?curid=2647027", "title": "Heinrich Brunner", "text": "Heinrich Brunner\n\nHeinrich Brunner (; 21 June 1840 – 11 August 1915) was a German historian born at Wels in Upper Austria. After studying at the universities of Vienna, Göttingen and Berlin, he became professor at the University of Lemberg in 1866, and in quick succession held similar positions at Prague, Strasbourg and Berlin. \n\nFrom 1872 Brunner devoted himself especially to studying the early laws and institutions of the Franks and kindred peoples of Western Europe, and on these subjects his researches have been of supreme value. He also became a leading authority on modern German law. He became a member of the Berlin Academy of Sciences in 1884, and in 1886, after the death of Georg Waitz, undertook the supervision of the \"Leges\" section of the \"Monumenta Germaniae Historica\".\n\n\nHe is also the author of the German versions of The Sources of English Law.\n\n"}
{"id": "5327591", "url": "https://en.wikipedia.org/wiki?curid=5327591", "title": "History of Bosnia and Herzegovina (1941–45)", "text": "History of Bosnia and Herzegovina (1941–45)\n\nAfter the Kingdom of Yugoslavia was invaded by the Axis powers during World War II, all of Bosnia was ceded to the newly created Independent State of Croatia. Axis rule in Bosnia led to widespread persecution and mass-killings of native undesirables and anti-fascists. Many Serbs themselves took up arms and joined the Chetniks, a Serb nationalist and royalist resistance movement that conducted ineffective guerrilla warfare against the occupying Nazi forces, only to collaborate with them later in the war against the communist partisans. The Chetniks were also known to persecute and murder non-Serbs and communist sympathizers. On 12 October 1941 a group of 108 notable Muslim citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the persecution of Serbs organized by Ustaše, made distinction between Muslims who participated in such persecutions and whole Muslim population, presented information about the persecutions of Muslims by Serbs and requested security for all citizens of the country, regardless of their identity.\n\nStarting in 1941, Yugoslav communists under the leadership of Josip Broz Tito organized their own multi-ethnic resistance group, the partisans, who fought against both Axis and Chetnik forces. On 29 November 1943 the Anti-Fascist Council of National Liberation of Yugoslavia with Tito at its helm held a founding conference in Jajce where Bosnia and Herzegovina was reestablished as a republic within the Yugoslavian federation in its Ottoman borders. Military success eventually prompted the Allies to support the Partisans, and the end of the war resulted in the establishment of the Socialist Federal Republic of Yugoslavia, with the constitution of 1946 officially making Bosnia and Herzegovina one of six constituent republics in the new state.\n\nDuring the war, and following the massive deterioration of internal security under the incompetent Ustaše regime, the Nazis tried to create a quisling Waffen-SS unit in Bosnia called the 13th Waffen Mountain Division of the SS Handschar (1st Croatian) in February 1943. Imam Halim Malkoć was the only Muslim to earn the German Iron Cross during World War II. \n\n"}
{"id": "2402376", "url": "https://en.wikipedia.org/wiki?curid=2402376", "title": "Indigenous peoples of the Americas", "text": "Indigenous peoples of the Americas\n\nThe indigenous peoples of the Americas are the pre-Columbian peoples of the Americas and their descendants.\n\nAlthough some indigenous peoples of the Americas were traditionally hunter-gatherers—and many, especially in the Amazon basin, still are—many groups practiced aquaculture and agriculture. The impact of their agricultural endowment to the world is a testament to their time and work in reshaping and cultivating the flora indigenous to the Americas. Although some societies depended heavily on agriculture, others practiced a mix of farming, hunting and gathering. In some regions the indigenous peoples created monumental architecture, large-scale organized cities, city-states, chiefdoms, states, kingdoms and empires. Among which are the Aztec, Inca and Maya states that until the 16th century were among the most politically and socially advanced nations in the world. They had a vast knowledge of engineering, architecture, mathematics, astronomy, writing, physics, medicine, planting and irrigation, geology, mining, sculpture and goldsmithing.\n\nMany parts of the Americas are still populated by indigenous peoples; some countries have sizable populations, especially Belize, Bolivia, Canada, Chile, Ecuador, Greenland, Guatemala, Guyana, Mexico, Panama, Peru, and the United States. At least a thousand different indigenous languages are spoken in the Americas. Some, such as the Quechuan languages, Aymara, Guaraní, Mayan languages and Nahuatl, count their speakers in millions. Many also maintain aspects of indigenous cultural practices to varying degrees, including religion, social organization and subsistence practices. Like most cultures, over time, cultures specific to many indigenous peoples have evolved to incorporate traditional aspects but also cater to modern needs. Some indigenous peoples still live in relative isolation from Western culture, and a few are still counted as uncontacted peoples.\n\n or (\"indigenous peoples\") is a common term in Spanish-speaking countries, and or (lit. \"native peoples\") may also be heard, while (aborigine) is used in Argentina, and (aboriginal peoples) is common in Chile. The term \"Amerindian\" (short for \"'Indians of the Americas\") is used in Quebec, the Guianas, and the English-speaking Caribbean. In Brazil, or are common if formal-sounding designations, while is still the more often-heard term (the noun for the Indian nationality being ), and and being rarely used in Amerindian-specific contexts (e.g. is usually understood as the ethnonym for Indigenous Australians). Indigenous peoples are commonly known in Canada as Aboriginal peoples, which includes not only First Nations and Arctic Inuit, but also the minority population of First Nations-European mixed-race Métis people who identify culturally and ethnically with indigenous peoplehood. This is contrasted, for instance, to the American Indian-European mixed-race mestizos of Hispanic America ( in Brazil) who, with their larger population (in most Latin American countries constituting either outright majorities, pluralities, or at the least large minorities), identify largely as a new ethnic group distinct from both Europeans and Indigenous Americans, but still considering themselves a subset of the European-derived Hispanic or Brazilian peoplehood in culture and ethnicity (cf. ).\n\nIndigenous peoples of the United States are commonly known as Native Americans or American Indians, and Alaska Natives.\n\nApplication of the term \"Indian\" originated with Christopher Columbus, who, in his search for India, thought that he had arrived in the East Indies. Eventually, those islands came to be known as the \"West Indies\", a name still used. This led to the blanket term \"Indies\" and \"Indians\" (Spanish , Portuguese ) for the indigenous inhabitants, which implied some kind of racial or cultural unity among the indigenous peoples of the Americas. This unifying concept, codified in law, religion, and politics, was not originally accepted by the myriad groups of indigenous peoples themselves, but has since been embraced, or tolerated, by many over the last two centuries. Even though the term \"Indian\" generally does not include the culturally and linguistically distinct indigenous peoples of the Arctic regions of the Americas—such as the Aleuts, Inuit, or Yupik peoples, who entered the continent as a second more recent wave of migration several thousand years before, and have much more recent genetic and cultural commonalities with the aboriginal peoples of the Asiatic Arctic Russian Far East—these groups are nonetheless considered \"indigenous peoples of the Americas\". The Portuguese and Spanish equivalents to Indian, nevertheless, could be used to mean any hunter-gatherer or full-blooded Indigenous person, particularly to continents other than Europe or Africa—for example, .\n\nThe specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are the subject of ongoing research and discussion. According to archaeological and genetic evidence, North and South America were the last continents in the world to gain human habitation. During the Wisconsin glaciation, 50–17,000 years ago, falling sea levels allowed people to move across the land bridge of Beringia that joined Siberia to northwest North America (Alaska). Alaska was a glacial refugium because it had low snowfall, allowing a small population to exist. The Laurentide Ice Sheet covered most of North America, blocking nomadic inhabitants and confining them to Alaska (East Beringia) for thousands of years.\n\nIndigenous genetic studies suggest that the first inhabitants of the Americas share a single ancestral population, one that developed in isolation, conjectured to be Beringia. The isolation of these peoples in Beringia might have lasted 10–20,000 years. Around 16,500 years ago, the glaciers began melting, allowing people to move south and east into Canada and beyond. These people are believed to have followed herds of now-extinct Pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran Ice Sheets.\n\nAnother route proposed involves migration – either on foot or using primitive boats – along the Pacific Northwest coast to the south, including as far as South America. Archeological evidence of the latter would have been covered by the sea level rise of more than 120 meters since the last ice age.\n\nThe time range of 40,000–16,500 years ago is debatable and probably will remain so for years to come. The few agreements achieved to date include:\n\n\nStone tools, particularly projectile points and scrapers, are the primary evidence of the earliest human activity in the Americas. Archaeologists and anthropologists have studied differences among these crafted lithic flaked tools to classify cultural periods. The Clovis culture, the earliest definitively-dated Paleo-Indians in the Americas, appears around 11,500 RCBP (radiocarbon years Before Present), equivalent to 13,500 to 13,000 calendar years ago.\n\nIn 2014, the autosomal DNA was sequenced of a 12,500+-year-old infant from Montana, whose remains were found in close association with several Clovis artifacts. These are the Anzick-1 remains from the Anzick Clovis burial in Montana. The data indicate that the individual was closely related to present North American Native American populations. But, the DNA was ancestral to present-day South American and Central American Native American populations. The implication is that there was an early divergence between North American indigenous peoples and those of Central and South America. Ruled out were hypotheses which posit that invasions subsequent to the Clovis culture overwhelmed or assimilated previous migrants into the Americas. After study, the remains were returned to Montana for burial by Native Americans.\n\nSimilarly, the skeleton of a teenage girl (named 'Naia' after a water nymph from Greek mythology) was found in 2007 in the underwater caves called \"sistema Sac Actun\" in Mexico's eastern Yucatán Peninsula. DNA was extracted and dated. The skeleton was found to be 13,000 years old, and it is considered the oldest genetically intact human skeleton ever found in the Americas. The DNA indicates she was from a lineage derived from Asian origins and also represented in the DNA of the modern native population.\n\nThe remains of two infants found at the Upward Sun River site have been dated to 11,500 years ago. They show that all Native Americans descended from a single founding population that initially split from East Asians around 36,000 years ago. They also show that the basal northern and southern Native American branches, to which all other indigenous Americans belong, diverged around 16,000 years ago.\n\nThe Pre-Columbian era refers to all period subdivisions in the history and prehistory of the Americas before the appearance of significant European and African influences on the American continents, spanning the time of the original arrival in the Upper Paleolithic to European colonization during the early modern period.\n\nWhile technically referring to the era before Christopher Columbus' voyages of 1492 to 1504, in practice the term usually includes the history of American indigenous cultures until Europeans either conquered or significantly influenced them. \"Pre-Columbian\" is used especially often in the context of discussing the pre-contact Mesoamerican indigenous societies: Olmec; Toltec; Teotihuacano' Zapotec; Mixtec; Aztec and Maya civilizations; and the complex cultures of the Andes: Inca Empire, Moche culture, Muisca Confederation, and Cañari.\nThe Norte Chico civilization (in present-day Peru) is one of the defining six original civilizations of the world, arising independently around the same time as that of Egypt. Many later pre-Columbian civilizations achieved great complexity, with hallmarks that included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first significant European and African arrivals (ca. late 15th–early 16th centuries), and are known only through oral history and through archaeological investigations. Others were contemporary with the contact and colonization period, and were documented in historical accounts of the time. A few, such as the Mayan, Olmec, Mixtec, and Nahua peoples, had their own written languages and records. However, the European colonists of the time worked to eliminate non-Christian beliefs, and burned many pre-Columbian written records. Only a few documents remained hidden and survived, leaving contemporary historians with glimpses of ancient culture and knowledge.\n\nAccording to both indigenous American and European accounts and documents, American civilizations before and at the time of European encounter had achieved great complexity and many accomplishments. For instance, the Aztecs built one of the largest cities in the world, Tenochtitlan, the ancient site of Mexico City, with an estimated population of 200,000. American civilizations also displayed impressive accomplishments in astronomy and mathematics. The domestication of maize or corn required thousands of years of selective breeding, and continued cultivation of multiple varieties was done with planning and selection, generally by women.\n\nInuit, Yupik, Aleut, and American Indian creation myths tell of a variety of origins of their respective peoples. Some were \"always there\" or were created by gods or animals, some migrated from a specified compass point, and others came from \"across the ocean\".\n\nThe European colonization of the Americas fundamentally changed the lives and cultures of the native peoples of the continents. Although the exact pre-colonization population-count of the Americas is unknown, scholars estimate that Native American populations diminished by between 80% and 90% within the first centuries of contact with Europeans. The majority of these losses are attributed to the introduction of Afro-Eurasian diseases into the Americas. Epidemics ravaged the Americas with diseases such as smallpox, measles, and cholera, which the early colonists and African slaves brought from Europe. The disease spread was slow initially, as Europeans were poor vectors for transferring the disease due to their natural exposure. This changed with the mass importation of Western and Central Africans slaves, who like the Native Americans lacked any resistances to the diseases of Europe and Northern Africa. These two groups were able to maintain a population large enough for diseases such as smallpox to spread rapidly amongst themselves. In 1520, contact with an African who had been infected with smallpox had arrived in Yucatán. By 1558, the disease had spread throughout South America and had arrived at the Plata basin. Colonist violence towards indigenous peoples exacerbated the loss of lives. European colonists perpetrated massacres on the indigenous groups and enslaved them. According to the U.S. Bureau of the Census (1894), the North American Indian Wars of the 19th century cost the lives of about 19,000 Europeans and 30,000 Native Americans.\n\nThe first indigenous group encountered by Columbus, the 250,000 Taínos of Hispaniola, represented the dominant culture in the Greater Antilles and the Bahamas. Within thirty years about 70% of the Taínos had died. They had no immunity to European diseases, so outbreaks of measles and smallpox ravaged their population. One such outbreak occurred in an African slave camp, where smallpox spread to the nearby Taínos populations and reduced their numbers by 50%. Increasing punishment of the Taínos for revolting against forced labor, despite measures put in place by the encomienda, which included religious education and protection from warring tribes, eventually led to the last great Taíno rebellion (1511–1529).\n\nFollowing years of mistreatment, the Taínos began to adopt suicidal behaviors, with women aborting or killing their infants and men jumping from cliffs or ingesting untreated cassava, a violent poison. Eventually, a Taíno Cacique named Enriquillo managed to hold out in the Baoruco Mountain Range for thirteen years, causing serious damage to the Spanish, Carib-held plantations and their Indian auxiliaries. Hearing of the seriousness of the revolt,\nEmperor Charles V (also King of Spain) sent captain Francisco Barrionuevo to negotiate a peace treaty with the ever-increasing number of rebels. Two months later, after consultation with the Audencia of Santo Domingo, Enriquillo was offered any part of the island to live in peace.\n\nThe Laws of Burgos, 1512–1513, were the first codified set of laws governing the behavior of Spanish settlers in America, particularly with regard to native Indians. The laws forbade the maltreatment of natives and endorsed their conversion to Catholicism. The Spanish crown found it difficult to enforce these laws in distant colonies.\n\nVarious theories for the decline of the Native American populations emphasize epidemic diseases, conflicts with Europeans, and conflicts among warring tribes. Among the various contributing factors, epidemic disease was the overwhelming cause of the population decline of the American natives. After initial contact with Europeans and Africans, Old World diseases caused the deaths of 90 to 95% of the native population of the New World in the following 150 years. Smallpox killed from one third to half of the native population of Hispaniola in 1518. By killing the Incan ruler Huayna Capac, smallpox caused the Inca Civil War of 1529–1532. Smallpox was only the first epidemic. Typhus (probably) in 1546, influenza and smallpox together in 1558, smallpox again in 1589, diphtheria in 1614, measles in 1618—all ravaged the remains of Inca culture.\n\nSmallpox killed millions of native inhabitants of Mexico. Unintentionally introduced at Veracruz with the arrival of Pánfilo de Narváez on April 23, 1520, smallpox ravaged Mexico in the 1520s, possibly killing over 150,000 in Tenochtitlán (the heartland of the Aztec Empire) alone, and aiding in the victory of Hernán Cortés over the Aztec Empire at Tenochtitlan (present-day Mexico City) in 1521.\n\nThere are many factors as to why Native Americans suffered such immense losses from Afro-Eurasian diseases. After the land bridge separated the human populations of the Old World and the New World, the Native Americans lost many of the immunities their ancestors possessed. In addition, Europeans acquired many diseases, like cow pox, from domestication of animals that the Native Americans did not have access to. While Europeans adapted to these diseases, there was no way for Native Americans to acquire those diseases and build up resistances to them. Finally, many of the European diseases that were brought over to the Americas were diseases, like yellow fever, that were relatively manageable if infected as a child, but were deadly if infected as an adult. Children could survive the disease and that individual would have immunity to the disease for the rest of their life. Upon contact with the adult populations of Native Americans, these childhood diseases were very fatal.\n\nColonization of the Caribbean led to the destruction of the Arawaks of the Lesser Antilles. Their culture was destroyed by 1650. Only 500 had survived by the year 1550, though the bloodlines continued through to the modern populace. In Amazonia, indigenous societies weathered, and continue to suffer, centuries of colonization and genocide.\nContact with European diseases such as smallpox and measles killed between 50 and 67 per cent of the aboriginal population of North America in the first hundred years after the arrival of Europeans. Some 90 per cent of the native population near Massachusetts Bay Colony died of smallpox in an epidemic in 1617–1619. In 1633, in Fort Orange (New Netherland), the Native Americans there were exposed to smallpox because of contact with Europeans. As it had done elsewhere, the virus wiped out entire population-groups of Native Americans. It reached Lake Ontario in 1636, and the lands of the Iroquois by 1679. During the 1770s smallpox killed at least 30% of the West Coast Native Americans. The 1775–82 North American smallpox epidemic and the 1837 Great Plains smallpox epidemic brought devastation and drastic population depletion among the Plains Indians. In 1832 the federal government of the United States established a smallpox vaccination program for Native Americans (\"The Indian Vaccination Act of 1832\").\n\nThe indigenous peoples in Brazil declined from a pre-Columbian high of an estimated three million to some 300,000 in 1997.\n\nThe Spanish Empire and other Europeans re-introduced horses to the Americas. Some of these animals escaped and began to breed and increase their numbers in the wild.\nThe re-introduction of the horse, extinct in the Americas for over 7500 years, had a profound impact on Native American culture in the Great Plains of North America and in Patagonia in South America. By domesticating horses, some tribes had great success: horses enabled them to expand their territories, exchange more goods with neighboring tribes, and more easily capture game, especially bison.\n\nIn the course of thousands of years, American indigenous peoples domesticated, bred and cultivated a large array of plant species. These species now constitute between 50% and 60% of all crops in cultivation worldwide. In certain cases, the indigenous peoples developed entirely new species and strains through artificial selection, as with the domestication and breeding of maize from wild \"teosinte\" grasses in the valleys of southern Mexico. Numerous such agricultural products retain their native names in the English and Spanish lexicons.\n\nThe South American highlands became a center of early agriculture. Genetic testing of the wide variety of cultivars and wild species suggests that the potato has a single origin in the area of southern Peru, from a species in the \"Solanum brevicaule\" complex. Over 99% of all modern cultivated potatoes worldwide are descendants of a subspecies indigenous to south-central Chile, \"Solanum tuberosum ssp. tuberosum\", where it was cultivated as long as 10,000 years ago. According to Linda Newson, \"It is clear that in pre-Columbian times some groups struggled to survive and often suffered food shortages and famines, while others enjoyed a varied and substantial diet.\"\nPersistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico.\n\nNatives of North America began practicing farming approximately 4,000 years ago, late in the Archaic period of North American cultures. Technology had advanced to the point where pottery had started to become common and the small-scale felling of trees had become feasible. Concurrently, the Archaic Indians began using fire in a controlled manner. They carried out intentional burning of vegetation to mimic the effects of natural fires that tended to clear forest understories. It made travel easier and facilitated the growth of herbs and berry-producing plants, which were important both for food and for medicines.\n\nIn the Mississippi River valley, Europeans noted that Native Americans managed groves of nut- and fruit-trees not far from villages and towns and their gardens and agricultural fields. They would have used prescribed burning further away, in forest and prairie areas.\n\nMany crops first domesticated by indigenous Americans are now produced and used globally, most notably maize or \"corn\", arguably the most important crop in the world. Other significant crops include cassava; chia; squash (pumpkins, zucchini, marrow, acorn squash, butternut squash); the pinto bean, \"Phaseolus\" beans including most common beans, tepary beans and lima beans; tomatoes; potatoes; avocados; peanuts; cocoa beans (used to make chocolate); vanilla; strawberries; pineapples; peppers (species and varieties of \"Capsicum\", including bell peppers, jalapeños, paprika and chili peppers); sunflower seeds; rubber; brazilwood; chicle; tobacco; coca; manioc and some species of cotton.\n\nStudies of contemporary indigenous environmental management — including of agro-forestry practices among Itza Maya in Guatemala and of hunting and fishing among the Menominee of Wisconsin — suggest that longstanding \"sacred values\" may represent a summary of sustainable millennial traditions.\n\nIndigenous Americans also domesticated some animals, such as llamas, alpacas, and guinea-pigs.\n\nCultural practices in the Americas seem to have been shared mostly within geographical zones where distinct ethnic groups adopting shared cultural traits, similar technologies, and social organizations. An example of such a cultural area is Mesoamerica, where millennia of coexistence and shared development among the peoples of the region produced a fairly homogeneous culture with complex agricultural and social patterns. Another well-known example is the North American plains where until the 19th century several peoples shared the traits of nomadic hunter-gatherers based primarily on buffalo hunting.\n\nThe languages of the North American Indians have been classified into 56 groups or stock tongues, in which the spoken languages of the tribes may be said to centre. In connection with speech, reference may be made to gesture language which was highly developed in parts of this area.\nOf equal interest is the picture writing especially well developed among the Chippewas and Delawares.\n\nThe development of writing is counted among the many achievements and innovations of pre-Columbian American cultures. Independent from the development of writing in other areas of the world, the Mesoamerican region produced several indigenous writing systems beginning in the 1st millennium BCE. What may be the earliest-known example in the Americas of an extensive text thought to be writing is by the Cascajal Block. The Olmec hieroglyphs tablet has been indirectly dated from ceramic shards found in the same context to approximately 900 BCE, around the time that Olmec occupation of San Lorenzo Tenochtitlán began to wane.\n\nThe Maya writing system was a combination of phonetic syllabic symbols and logograms—that is, it was a logosyllabic writing system. It is the only pre-Columbian writing system known to represent completely the spoken language of its community. In total, the script has more than one thousand different glyphs, although a few are variations of the same sign or meaning, and many appear only rarely or are confined to particular localities. At any one time, no more than about five hundred glyphs were in use, some two hundred of which (including variations) had a phonetic or syllabic interpretation.\n\nThe Zapotec writing system is one of the earliest writing systems in the Americas. The oldest example of the Zapotec script is a monument discovered in San José Mogote, dating from around from 600 BCE. Zapotec writing was logographic and presumably syllabic. The remains of the Zapotec writing system are present in the monumental architecture. There are only a few extant inscriptions, making study of this writing system difficult.\n\nAztec codices (singular \"codex\") are books written by pre-Columbian and colonial-era Aztecs. These codices provide some of the best primary sources for Aztec culture. The pre-Columbian codices differ from European codices in that they are largely pictorial; they were not meant to symbolize spoken or written narratives. The colonial era codices contain not only Aztec pictograms, but also Classical Nahuatl (in the Latin alphabet), Spanish, and occasionally Latin.\n\nSpanish mendicants in the sixteenth century taught indigenous scribes in their communities to write their languages in Latin letters, and there are a large number of local-level documents in Nahuatl, Zapotec, Mixtec, and Yucatec Maya from the colonial era, many of which were part of lawsuits and other legal matters. Although Spaniards initially taught indigenous scribes alphabetic writing, the tradition became self-perpetuating at the local level. The Spanish crown gathered such documentation, and contemporary Spanish translations were made for legal cases. Scholars have translated and analyzed these documents in what is called the New Philology to write histories of indigenous peoples from indigenous viewpoints.\n\nThe Wiigwaasabak, birch bark scrolls on which the Ojibwa (Anishinaabe) people wrote complex geometrical patterns and shapes, can also be considered a form of writing, as can Mi'kmaq hieroglyphics.\n\nAboriginal syllabic writing, or simply syllabics, is a family of abugidas used to write some Aboriginal Canadian languages of the Algonquian, Inuit, and Athabaskan language families.\n\nNative American music in North America is almost entirely monophonic, but there are notable exceptions. Traditional Native American music often centers around drumming. Rattles, clapper sticks, and rasps were also popular percussive instruments. Flutes were made of rivercane, cedar, and other woods. The tuning of these flutes is not precise and depends on the length of the wood used and the hand span of the intended player, but the finger holes are most often around a whole step apart and, at least in Northern California, a flute was not used if it turned out to have an interval close to a half step. The Apache fiddle is a single stringed instrument.\n\nThe music of the indigenous peoples of Central Mexico and Central America was often pentatonic. Before the arrival of the Spaniards and other Europeans, music was inseparable from religious festivities and included a large variety of percussion and wind instruments such as drums, flutes, sea snail shells (used as a trumpet) and \"rain\" tubes. No remnants of pre-Columbian stringed instruments were found until archaeologists discovered a jar in Guatemala, attributed to the Maya of the Late Classic Era (600–900 CE), which depicts a stringed musical instrument which has since been reproduced. This instrument is one of the very few stringed instruments known in the Americas prior to the introduction of European musical instruments; when played, it produces a sound that mimics a jaguar's growl.\n\nVisual arts by indigenous peoples of the Americas comprise a major category in the world art collection. Contributions include pottery, paintings, jewellery, weavings, sculptures, basketry, , and beadwork. Because too many artists were posing as Native Americans and Alaska Natives in order to profit from the cachet of Indigenous art in the United States, the U.S. passed the Indian Arts and Crafts Act of 1990, requiring artists to prove that they are enrolled in a state or federally recognized tribe. To support the ongoing practice of American Indian, Alaska Native, and Native Hawaiian arts and cultures in the United States, the Ford Foundation, arts advocates and American Indian tribes created an endowment seed fund and established a national Native Arts and Cultures Foundation in 2007.\n\nThe following table provides estimates for each country in the Americas of the populations of indigenous people and those with partial indigenous ancestry, each expressed as a percentage of the overall population. The total percentage obtained by adding both of these categories is also given.\n\n\"Note:\" these categories are inconsistently defined and measured differently from country to country. Some figures are based on the results of population-wide genetic surveys while others are based on self-identification or observational estimation.\n\nAboriginal peoples in Canada comprise the First Nations, Inuit and Métis; the descriptors \"Indian\" and \"Eskimo\" are falling into disuse. \"Indian\" is a name that originated from foreigners. In Canada, it is quite frowned upon to use the name \"Indian\" in casual conversation. \"Eskimo\" is considered derogatory in many other places because it was given by non-Inuit people and was said to mean \"eater of raw meat.\" \nHundreds of Aboriginal nations evolved trade, spiritual and social hierarchies. The Métis ethnicity developed a culture from the mid-17th century after generations of First Nations and native Inuit married European settlers. They were small farmers, hunters and trappers, and usually Catholic and French-speaking. The Inuit had more limited interaction with European settlers during that early period. Various laws, treaties, and legislation have been enacted between European-Canadians and First Nations across Canada. Aboriginal Right to Self-Government provides the opportunity for First Nations to manage their own historical, cultural, political, health care and economic control within their communities.\nAlthough not without conflict, European/Canadian early interactions in the east with First Nations and Inuit populations were relatively peaceful compared to the later experience of native peoples in the United States. Combined with a late economic development in many regions, this relatively peaceful history resulted in Indigenous peoples having a fairly strong influence on the early national culture, while preserving their own identity. From the late 18th century, European Canadians encouraged Aboriginals to assimilate into the mainstream European-influenced culture, which they referred to as Canadian culture. The government attempted forced integration in the late 19th and early 20th centuries. National Aboriginal Day recognises the cultures and contributions of Aboriginal peoples of Canada. There are currently over 600 recognized First Nations governments or bands encompassing 1,172,790 people spread across Canada, with distinctive Aboriginal cultures, languages, art, and music.\n\nIndigenous peoples in what is now the contiguous United States, including their descendants, were commonly called \"American Indians\", or simply \"Indians\" domestically. Since the late 20th century, when some insisted on using \"Native American\", as their preferred term, the United States Census Bureau and other parts of government have also adopted it. In Alaska, indigenous peoples belong to 11 cultures with 11 languages. These include the St. Lawrence Island Yupik, Iñupiat, Athabaskan, Yup'ik, Cup'ik, Unangax, Alutiiq, Eyak, Haida, Tsimshian, and Tlingit, and are collectively called Alaska Natives. They include Native American peoples as well as Inuit, who are distinct but occupy areas of the region. The United States has authority with Indigenous Polynesian peoples, which include Hawaiians, Marshallese, Samoan, Tahitian, and Tongan; politically they are classified as Pacific Islands American. They are geographically, genetically, and culturally distinct from indigenous peoples of the mainland continents of the Americas.\nNative Americans in the United States make up 0.97% to 2% of the population. In the 2010 census, 2.9 million people identified as Native American, Native Hawaiian, and Alaska Native alone. A total of 5.2 million people identified as Native Americans, either alone or in combination with one or more ethnicity or other races. 1.8 million are enrolled tribal members. Tribes have established their own criteria for membership, which are often based on blood quantum, lineal descent, or residency. A minority of Native Americans live in land units called Indian reservations. Some California and Southwestern tribes, such as the Kumeyaay, Cocopa, Pascua Yaqui and Apache, span both sides of the US–Mexican border. By treaty, Haudenosaunee people have the legal right to freely cross the US–Canada border. Athabascan, Tlingit, Haida, Tsimshian, Iñupiat, Blackfeet, Nakota, Cree, Anishinaabe, Huron, Lenape, Mi'kmaq, Penobscot, and Haudenosaunee, among others, live in both Canada and the United States. The international border cut through their common cultural territory.\n\nThe territory of modern-day Mexico was home to numerous indigenous civilizations prior to the arrival of the Spanish \"conquistadores\": The Olmecs, who flourished from between 1200 BCE to about 400 BCE in the coastal regions of the Gulf of Mexico; the Zapotecs and the Mixtecs, who held sway in the mountains of Oaxaca and the Isthmus of Tehuantepec; the Maya in the Yucatán (and into neighbouring areas of contemporary Central America); the Purépecha in present-day Michoacán and surrounding areas, and the Aztecs/Mexica, who, from their central capital at Tenochtitlan, dominated much of the centre and south of the country (and the non-Aztec inhabitants of those areas) when Hernán Cortés first landed at Veracruz.\n\nIn contrast to what was the general rule in the rest of North America, the history of the colony of New Spain was one of racial intermingling (\"mestizaje\"). \"Mestizos\", which in Mexico designate people who do not identify culturally with any indigenous grouping, quickly came to account for a majority of the colony's population; but 6% of the Mexican population identify as speakers of one of the indigenous languages. The CDI identifies 62 indigenous groups in Mexico, each with a unique language.\n\nIn the states of Chiapas and Oaxaca and in the interior of the Yucatán Peninsula the majority of the population is indigenous. Large indigenous minorities, including Aztecs or Nahua, Purépechas, Mazahua, Otomi, and Mixtecs are also present in the central regions of Mexico. In Northern Mexico indigenous people are a small minority.\nThe General Law of Linguistic Rights of the Indigenous Peoples grants all indigenous languages spoken in Mexico, regardless of the number of speakers, the same validity as Spanish in all territories in which they are spoken, and indigenous peoples are entitled to request some public services and documents in their native languages. Along with Spanish, the law has granted them—more than 60 languages—the status of \"national languages\". The law includes all indigenous languages of the Americas regardless of origin; that is, it includes the indigenous languages of ethnic groups non-native to the territory. The National Commission for the Development of Indigenous Peoples recognizes the language of the Kickapoo, who immigrated from the United States, and recognizes the languages of the Guatemalan indigenous refugees. The Mexican government has promoted and established bilingual primary and secondary education in some indigenous rural communities. Nonetheless, of the indigenous peoples in Mexico, only about 67% of them (or 5.4% of the country's population) speak an indigenous language and about a sixth do not speak Spanish (1.2% of the country's population).\n\nThe indigenous peoples in Mexico have the right of free determination under the second article of the constitution. According to this article the indigenous peoples are granted:\namongst other rights.\n\nMestizos (mixed European-Indigenous) number about 34% of the population; unmixed Maya make up another 10.6% (Ketchi, Mopan, and Yucatec). The Garifuna, who came to Belize in the 19th century from Saint Vincent and the Grenadines, have mixed African, Carib, and Arawak ancestry make up another 6% of the population.\n\nThere are over 114,000 inhabitants of Native American origins, representing 2.4% of the population. Most of them live in secluded reservations, distributed among eight ethnic groups: Quitirrisí (In the Central Valley), Matambú or Chorotega (Guanacaste), Maleku (Northern Alajuela), Bribri (Southern Atlantic), Cabécar (Cordillera de Talamanca), Guaymí (Southern Costa Rica, along the Panamá border), Boruca (Southern Costa Rica) and Ngäbe (Southern Costa Rica).\n\nThese native groups are characterized for their work in wood, like masks, drums and other artistic figures, as well as fabrics made of cotton.\n\nTheir subsistence is based on agriculture, having corn, beans and plantains as the main crops.\n\nMuch of El Salvador was home to the Pipil, the Lenca, Xinca, and Kakawira. The Pipil lived in western El Salvador, spoke Nawat, and had many settlements there, most noticeably Cuzcatlan. The Pipil had no precious mineral resources, but they did have rich and fertile land that was good for farming. The Spaniards were disappointed not to find gold or jewels in El Salvador as they had in other lands like Guatemala or Mexico, but upon learning of the fertile land in El Salvador, they attempted to conquer it. Noted Meso-American indigenous warriors to rise militarily against the Spanish included Princes Atonal and Atlacatl of the Pipil people in central El Salvador and Princess Antu Silan Ulap of the Lenca people in eastern El Salvador, who saw the Spanish not as gods but as barbaric invaders. After fierce battles, the Pipil successfully fought off the Spanish army led by Pedro de Alvarado along with their Mexican Indian allies (the Tlaxcalas), sending them back to Guatemala. After many other attacks with an army reinforced with Guatemalan Indian allies, the Spanish were able to conquer Cuzcatlan. After further attacks, the Spanish also conquered the Lenca people. Eventually, the Spaniards intermarried with Pipil and Lenca women, resulting in the Mestizo population which would become the majority of the Salvadoran people. Today many Pipil and other indigenous populations live in the many small towns of El Salvador like Izalco, Panchimalco, Sacacoyo, and Nahuizalco.\n\nGuatemala has one of the largest Indigenous populations in Central America, with approximately 39.3% of the population considering themselves Indigenous. The Indigenous demographic portion of Guatemala’s population consists of majority Mayan groups and one Non-Mayan group. The Mayan portion, can be broken down into 23 groups namely K’iche 11.3%, Kaqchikel 7.4%, Mam 5.5%, Q’eqchi’ 7.6% and Other 7.5%. The Non-Mayan group consists of the Xinca who are another set of Indigenous people making up 0.5% of the population.\n\nThe Mayan tribes cover a vast geographic area throughout Central America and expanding beyond Guatemala into other countries. One could find vast groups of Mayan people in Boca Costa, in the Southern portions of Guatemala, as well as the Western Highlands living together in close communities. Within these communities and outside of them, around 23 Indigenous languages or Amerindian Languages are spoken as a first language. Of these 23 languages, they only received official recognition by the Government in 2003 under the Law of National Languages. The Law on National Languages recognizes 23 Indigenous languages including Xinca, enforcing that public and government institutions not only translate but also provide services in said languages. It would provide services in Cakchiquel, Garifuna, Kekchi, Mam, Quiche and Xinca. \n The Law of National Languages has been an effort to grant and protect Indigenous people rights not afforded to them previously. Along with the Law of National Languages passed in 2003, in 1996 the Guatemalan Constitutional Court had ratified the ILO Convention 169 on Indigenous and Tribal Peoples. The ILO Convention 169 on Indigenous and Tribal Peoples, is also known as Convention 169 . Which is the only International Law regarding Indigenous peoples that Independent countries can adopt. The Convention, establishes that Governments like Guatemala’s must consult with Indigenous groups prior to any projects occurring on tribal lands.\n\nAbout five percent of the population are of full-blooded indigenous descent, but upwards to eighty percent more or the majority of Hondurans are mestizo or part-indigenous with European admixture, and about ten percent are of indigenous or African descent. The main concentration of indigenous in Honduras are in the rural westernmost areas facing Guatemala and to the Caribbean Sea coastline, as well on the Nicaraguan border. The majority of indigenous people are Lencas, Miskitos to the east, Mayans, Pech, Sumos, and Tolupan.\n\nAbout 5% of the Nicaraguan population are indigenous. The largest indigenous group in Nicaragua is the Miskito people. Their territory extended from Cape Camarón, Honduras, to Rio Grande, Nicaragua along the Mosquito Coast. There is a native Miskito language, but large numbers speak Miskito Coast Creole, Spanish, Rama and other languages. Their use of Creole English came about through frequent contact with the British, who colonized the area. Many Miskitos are Christians. Traditional Miskito society was highly structured, politically and otherwise. It had a king, but he did not have total power. Instead, the power was split between himself, a Miskito Governor, a Miskito General, and by the 1750s, a Miskito Admiral. Historical information on Miskito kings is often obscured by the fact that many of the kings were semi-mythical.\n\nAnother major indigenous culture in eastern Nicaragua are the Mayangna (or Sumu) people, counting some 10,000 people. A smaller indigenous culture in southeastern Nicaragua are the Rama.\n\nOther indigenous groups in Nicaragua are located in the central, northern, and Pacific areas and they are self-identified as follows: Chorotega, Cacaopera (or Matagalpa), Xiu-Subtiaba, and Nahua.\n\nIn 2005, Argentina's indigenous population (known as \"pueblos originarios\") numbered about 600,329 (1.6% of total population); this figure includes 457,363 people who self-identified as belonging to an indigenous ethnic group and 142,966 who identified themselves as first-generation descendants of an indigenous people. The ten most populous indigenous peoples are the Mapuche (113,680 people), the Kolla (70,505), the Toba (69,452), the Guaraní (68,454), the Wichi (40,036), the Diaguita–Calchaquí (31,753), the Mocoví (15,837), the Huarpe (14,633), the Comechingón (10,863) and the Tehuelche (10,590). Minor but important peoples are the Quechua (6,739), the Charrúa (4,511), the Pilagá (4,465), the Chané (4,376), and the Chorote (2,613). The Selknam (Ona) people are now virtually extinct in its pure form. The languages of the Diaguita, Tehuelche, and Selknam nations have become extinct or virtually extinct: the Cacán language (spoken by Diaguitas) in the 18th century and the Selknam language in the 20th century; one Tehuelche language (Southern Tehuelche) is still spoken by a handful of elderly people.\n\nIn Bolivia, the 2001 census reported that 62% of residents over the age of 15 identify as belonging to an indigenous people. Some 3.7% report growing up with an indigenous mother tongue but do not identify as indigenous. When both of these categories are totaled, and children under 15, some 66.4% of Bolivia's population was recorded as indigenous in the 2001 Census.\n\nThe largest indigenous ethnic groups are: Quechua, about 2.5 million people; Aymara, 2.0 million; Chiquitano, 181,000; Guaraní, 126,000; and Mojeño, 69,000. Some 124,000 belong to smaller indigenous groups. The Constitution of Bolivia, enacted in 2009, recognizes 36 cultures, each with its own language, as part of a pluri-national state. Some groups, including CONAMAQ (the National Council of Ayllus and Markas of Qullasuyu), draw ethnic boundaries within the Quechua- and Aymara-speaking population, resulting in a total of 50 indigenous peoples native to Bolivia.\n\nLarge numbers of Bolivian highland peasants retained indigenous language, culture, customs, and communal organization throughout the Spanish conquest and the post-independence period. They mobilized to resist various attempts at the dissolution of communal landholdings and used legal recognition of \"empowered caciques\" to further communal organization. Indigenous revolts took place frequently until 1953. While the National Revolutionary Movement government begun in 1952 discouraged people identifying as indigenous (reclassifying rural people as \"campesinos\", or peasants), renewed ethnic and class militancy re-emerged in the Katarista movement beginning in the 1970s. Many lowland indigenous peoples, mostly in the east, entered national politics through the 1990 March for Territory and Dignity organized by the CIDOB confederation. That march successfully pressured the national government to sign the ILO Convention 169 and to begin the still-ongoing process of recognizing and giving official title to indigenous territories. The 1994 Law of Popular Participation granted \"grassroots territorial organizations;\" these are recognized by the state and have certain rights to govern local areas.\n\nSome radio and television programs are produced in the Quechua and Aymara languages. The constitutional reform in 1997 recognized Bolivia as a multi-lingual, pluri-ethnic society and introduced education reform. In 2005, for the first time in the country's history, an indigenous Aymara, Evo Morales, was elected as President.\n\nMorales began work on his \"indigenous autonomy\" policy, which he launched in the eastern lowlands department on August 3, 2009. Bolivia was the first nation in the history of South America to affirm the right of indigenous people to self-government. Speaking in Santa Cruz Department, the President called it \"a historic day for the peasant and indigenous movement\", saying that, though he might make errors, he would \"never betray the fight started by our ancestors and the fight of the Bolivian people.\" A vote on further autonomy for jurisdictions took place in December 2009, at the same time as general elections to office. The issue divided the country.\n\nAt that time, indigenous peoples voted overwhelmingly for more autonomy: five departments that had not already done so voted for it; as did Gran Chaco Province in Taríja, for regional autonomy; and 11 of 12 municipalities that had referendums on this issue.\n\nIndigenous peoples of Brazil make up 0.4% of Brazil's population, or about 817,000 people, but millions of Brazilians are mestizo or have some indigenous ancestry. Indigenous peoples are found in the entire territory of Brazil, although in the 21st century, the majority of them live in indigenous territories in the North and Center-Western part of the country. On January 18, 2007, Fundação Nacional do Índio (FUNAI) reported that it had confirmed the presence of 67 different uncontacted tribes in Brazil, up from 40 in 2005. Brazil is now the nation that has the largest number of uncontacted tribes, and the island of New Guinea is second.\n\n\"The Washington Post\" reported in 2007, \"As has been proved in the past when uncontacted tribes are introduced to other populations and the microbes they carry, maladies as simple as the common cold can be deadly. In the 1970s, 185 members of the Panara tribe died within two years of discovery after contracting such diseases as flu and chickenpox, leaving only 69 survivors.\"\n\nAccording to the 2012 Census, 10% of the Chilean population, including the Rapa Nui (a Polynesian people) of Easter Island, was indigenous, although most show varying degrees of mixed heritage. Many are descendants of the Mapuche, and live in Santiago, Araucanía and Los Lagos Region. The Mapuche successfully fought off defeat in the first 300–350 years of Spanish rule during the Arauco War. Relations with the new Chilean Republic were good until the Chilean state decided to occupy their lands. During the Occupation of Araucanía the Mapuche surrendered to the country's army in the 1880s. Their land was opened to settlement by Chileans and Europeans. Conflict over Mapuche land rights continues to the present.\n\nOther groups include the Aymara, the majority of whom live in Bolivia and Peru, with smaller numbers in the Arica-Parinacota and Tarapacá regions, and the Atacama people (\"Atacameños\"), who reside mainly in El Loa.\n\nA minority today within Colombia's overwhelmingly Mestizo and White Colombian population, Colombia's indigenous peoples consist of around 85 distinct cultures and more than 1,378,884 people. A variety of collective rights for indigenous peoples are recognized in the 1991 Constitution.\n\nOne of the influences is the Muisca culture, a subset of the larger Chibcha ethnic group, famous for their use of gold, which led to the legend of \"El Dorado\". At the time of the Spanish conquest, the Muisca were the largest native civilization geographically between the Incas and the Aztecs empires.\n\nEcuador was the site of many indigenous cultures, and civilizations of different proportions. An early sedentary culture, known as the Valdivia culture, developed in the coastal region, while the Caras and the Quitus unified to form an elaborate civilization that ended at the birth of the Capital Quito. The Cañaris near Cuenca were the most advanced, and most feared by the Inca, due to their fierce resistance to the Incan expansion. Their architecture remains were later destroyed by Spaniards and the Incas.\nApproximately 96.4% of Ecuador's Indigenous population are Highland Quichuas living in the valleys of the Sierra region. Primarily consisting of the descendants of peoples conquered by the Incas, they are Kichwa speakers and include the Caranqui, the Otavalos, the Cayambe, the Quitu-Caras, the Panzaleo, the Chimbuelo, the Salasacan, the Tugua, the Puruhá, the Cañari, and the Saraguro. Linguistic evidence suggests that the Salascan and the Saraguro may have been the descendants of Bolivian ethnic groups transplanted to Ecuador as \"mitimaes\".\n\nCoastal groups, including the Awá, Chachi, and the Tsáchila, make up 0.24% percent of the indigenous population, while the remaining 3.35 percent live in the Oriente and consist of the Oriente Kichwa (the Canelo and the Quijos), the Shuar, the Huaorani, the Siona-Secoya, the Cofán, and the Achuar.\n\nIn 1986, indigenous people formed the first \"truly\" national political organization. The Confederation of Indigenous Nationalities of Ecuador (CONAIE) has been the primary political institution of the Indigenous since then and is now the second largest political party in the nation. It has been influential in national politics, contributing to the ouster of presidents Abdalá Bucaram in 1997 and Jamil Mahuad in 2000.\n\nIndigenous population in Peru make up around 25% approximately. Native Peruvian traditions and customs have shaped the way Peruvians live and see themselves today. Cultural citizenship—or what Renato Rosaldo has called, \"the right to be different and to belong, in a democratic, participatory sense\" (1996:243)—is not yet very well developed in Peru. This is perhaps no more apparent than in the country's Amazonian regions where indigenous societies continue to struggle against state-sponsored economic abuses, cultural discrimination, and pervasive violence.\n\nMost Venezuelans have some indigenous heritage and are pardo, even if they identify as white. But those who identify as indigenous, from being raised in those cultures, make up only around 2% of the total population. The indigenous peoples speak around 29 different languages and many more dialects. As some of the ethnic groups are very small, their native languages are in danger of becoming extinct in the next decades. The most important indigenous groups are the Ye'kuana, the Wayuu, the Pemon and the Warao. The most advanced native people to have lived within the boundaries of present-day Venezuela is thought to have been the Timoto-cuicas, who lived mainly in the Venezuelan Andes. Historians estimate that there were between 350 thousand and 500 thousand indigenous inhabitants at the time of Spanish colonization. The most densely populated area was the Andean region (Timoto-cuicas), thanks to their advanced agricultural techniques and ability to produce a surplus of food.\n\nThe 1999 constitution of Venezuela gives the indigenous special rights, although the vast majority of them still live in very critical conditions of poverty. The government provides primary education in their languages in public schools to some of the largest groups, in efforts to continue the languages.\n\nIndigenous peoples make up the majority of the population in Bolivia and Peru, and are a significant element in most other former Spanish colonies. Exceptions to this include Uruguay (Native Charrúa). According to the 2011 Census, 2.4% of Uruguayans reported having indigenous ancestry. Some governments recognize some of the major Native American languages as official languages: Quechua in Peru and Bolivia; Aymara also in Peru and Bolivia, Guarani in Paraguay, and Greenlandic in Greenland.\n\nIn Cuba, the population of Amerindians includes 0.1 of the population and 0.2 part Native which is also part of the population. Many are from the Taíno people or Arawak people. When the Spanish Empire was in control of the island they used the Natives as slaves and many died from diseases, hence decreasing the population. Presently 0.3 of the population of Cuba consists of part Native and full-blooded Amerindians.\n\nDominica is home to the Carib Territory, one of the last indigenous communities in the Caribbean. The Carib Territory is home to an estimated 3,000 Kalinago or Carib people.\n\nThe Native American name controversy relates to the dispute over acceptable ways to refer to the indigenous peoples of the Americas and to broad subsets thereof, such as those living in a specific country or sharing certain cultural attributes. Early settlers often adopted terms that some tribes used for each other, not realizing these were derogatory terms used by enemies. When discussing broader subsets of peoples, naming may be based on shared language, region, or historical relationship. Many English exonyms have been used to refer to the indigenous peoples of the Americas. Some of these names were based on foreign-language terms used by earlier explorers and colonists, while others resulted from the colonists' attempts to translate or transliterate endonyms from the native languages. Other terms arose during periods of conflict between the colonizers and indigenous peoples.\n\nSince the late 20th century, indigenous peoples in the Americas have been more vocal about how they want to be addressed, pushing to suppress use of terms widely considered to be obsolete, inaccurate, or racist. During the latter half of the 20th century and the rise of the Indian rights movement, the United States government responded by proposing the use of the term \"Native American,\" to recognize the primacy of indigenous peoples' tenure in the nation. As may be expected among people of different cultures, not all Native Americans or American Indians agree on its use. No single group naming convention has been accepted by all indigenous peoples. They prefer to be addressed as people of their tribe or nations.\n\nSince the late 20th century, indigenous peoples in the Americas have become more politically active in asserting their treaty rights and expanding their influence. Some have organized in order to achieve some sort of self-determination and preservation of their culturess. Organizations such as the Coordinator of Indigenous Organizations of the Amazon River Basin and the Indian Council of South America are examples of movements that are overcoming national borders to reunited indigenous populations, for instance those across the Amazon Basin. Similar movements for indigenous rights can also be seen in Canada and the United States, with movements like the International Indian Treaty Council and the accession of native Indian group into the Unrepresented Nations and Peoples Organization.\n\nThere has been a recognition of indigenous movements on an international scale. The membership of the United Nations voted to adopt the Declaration on the Rights of Indigenous Peoples, despite dissent from some of the stronger countries of the Americas.\n\nIn Colombia, various indigenous groups have protested the denial of their rights. People organized a march in Cali in October 2008 to demand the government live up to promises to protect indigenous lands, defend the indigenous against violence, and reconsider the free trade pact with the United States.\n\nThe first indigenous candidate to be democratically elected as head of a country in Latin America was Benito Juárez, a Zapotec Mexican; he was elected President of Mexico in 1858.\n\nEvo Morales (Aymara people) was the first indigenous candidate elected as president of Bolivia, in 2006, and the first in South America. He has been twice re-elected. His election encouraged the indigenous movement across Latin America.\n\nRepresentatives from indigenous and rural organizations from major South American countries, including Bolivia, Ecuador, Colombia, Chile and Brazil, started a forum in support of Morales' legal process of change. The meeting condemned plans by the European \"foreign power elite\" to destabilize the country. The forum also expressed solidarity with Morales and his economic and social changes in the interest of historically marginalized majorities. It questioned US interference through diplomats and NGOs. The forum was suspicious of plots against Bolivia and other countries that elected leftist leaders, including Cuba, Venezuela, Ecuador, Paraguay and Nicaragua.\n\nThe forum rejected the supposed violent method used by regional civic leaders from the called \"Crescent departments\" in Bolivia to impose autonomous statutes, applauded the decision to expel the US ambassador to Bolivia, and reaffirmed the sovereignty and independence of the presidency. Amongst others, representatives of CONAIE, the National Indigenous Organization of Colombia, the Chilean Council of All Lands, and the Brazilian Landless Movement participated in the forum.\n\nGenetic history of indigenous peoples of the Americas primarily focuses on Human Y-chromosome DNA haplogroups and Human mitochondrial DNA haplogroups. \"Y-DNA\" is passed solely along the patrilineal line, from father to son, while \"mtDNA\" is passed down the matrilineal line, from mother to offspring of both sexes. Neither recombines, and thus Y-DNA and mtDNA change only by chance mutation at each generation with no intermixture between parents' genetic material. Autosomal \"atDNA\" markers are also used, but differ from mtDNA or Y-DNA in that they overlap significantly. AtDNA is generally used to measure the average continent-of-ancestry genetic admixture in the entire human genome and related isolated populations.\n\nScientific evidence links indigenous Americans to Asian peoples, specifically Siberian populations, such as the Ket, Selkup, Chukchi and Koryak peoples. Indigenous peoples of the Americas have been linked to North Asian populations by the distribution of blood types, and in genetic composition as reflected by molecular data, such as DNA. There is general agreement among anthropologists that the source populations for the migration into the Americas originated from an area somewhere east of the Yenisei River. The common occurrence of the mtDNA Haplogroups A, B, C, and D among eastern Asian and Native American populations has long been recognized. As a whole, the greatest frequency of the four Native American associated haplogroups occurs in the Altai–Baikal region of southern Siberia. Some subclades of C and D closer to the Native American subclades occur among Mongolian, Amur, Japanese, Korean, and Ainu populations.\n\nGenetic studies of mitochondrial DNA (mtDNA) of Amerindians and some Siberian and Central Asian peoples also revealed that the gene pool of the Turkic-speaking peoples of Siberia such as Altaians, Khakas, Shors and Soyots, living between the Altai and Lake Baikal along the Sayan mountains, are genetically close to Amerindians. This view is shared by other researchers who argue that \"the ancestors of the American Indians were the first to separate from the great Asian population in the Middle Paleolithic.\" 2012 research found evidence for a recent common ancestry between Native Americans and indigenous Altaians based on mitochondrial DNA and Y-Chromosome analysis. The paternal lineages of Altaians mostly belong to the subclades of haplogroup P-M45 (R1a 38–93%; Q1a 4-32%).\n\nThe genetic pattern indicates indigenous peoples of the Americas experienced two very distinctive genetic episodes; first with the initial peopling of the Americas, and secondly with European colonization of the Americas. The former is the determinant factor for the number of gene lineages, zygosity mutations, and founding haplotypes present in today's indigenous peoples of the Americas populations.\n\nHuman settlement of the New World occurred in stages from the Bering sea coast line, with a possible initial layover of 10,000 to 20,000 years in Beringia for the small founding population. The micro-satellite diversity and distributions of the Y lineage specific to South America indicates that certain indigenous peoples of the Americas populations have been isolated since the initial colonization of the region. The Na-Dené, Inuit and Indigenous Alaskan populations exhibit haplogroup Q (Y-DNA) mutations, however are distinct from other indigenous peoples of the Americas with various mtDNA and atDNA mutations. This suggests that the earliest migrants into the northern extremes of North America and Greenland derived from later migrant populations.\nA 2013 study in \"Nature\" reported that DNA found in the 24,000-year-old remains of a young boy from the archaeological Mal'ta-Buret' culture suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have \"had a more north-easterly distribution 24,000 years ago than commonly thought\". \"We estimate that 14 to 38 percent of Native American ancestry may originate through gene flow from this ancient population\", the authors wrote. Professor Kelly Graf said,\nOur findings are significant at two levels. First, it shows that Upper Paleolithic Siberians came from a cosmopolitan population of early modern humans that spread out of Africa to Europe and Central and South Asia. Second, Paleoindian skeletons like Buhl Woman with phenotypic traits atypical of modern-day indigenous Americans can be explained as having a direct historical connection to Upper Paleolithic Siberia.\n\nA route through Beringia is seen as more likely than the Solutrean hypothesis. Kashani et al. 2012 state that \"The similarities in ages and geographical distributions for C4c and the previously analyzed X2a lineage provide support to the scenario of a dual origin for Paleo-Indians. Taking into account that C4c is deeply rooted in the Asian portion of the mtDNA phylogeny and is indubitably of Asian origin, the finding that C4c and X2a are characterized by parallel genetic histories definitively dismisses the controversial hypothesis of an Atlantic glacial entry route into North America.\"\n\n\n\n\n"}
{"id": "142331", "url": "https://en.wikipedia.org/wiki?curid=142331", "title": "Indo-Pakistani wars and conflicts", "text": "Indo-Pakistani wars and conflicts\n\nSince the partition of British India in 1947 and creation of modern states of India and Pakistan, the two South Asian countries have been involved in four wars, including one undeclared war, and many border skirmishes and military stand-offs. Most of these wars and conflict have ended with defeat or disaster for Pakistan.\n\nThe Kashmir issue has been the main cause of all major conflicts between the two countries with the exception of the Indo-Pakistani War of 1971 where conflict originated due to turmoil in erstwhile East Pakistan (now Bangladesh).\n\nThe Partition of British India came about in the aftermath of World War II, when both Great Britain and British India were dealing with the economic stresses caused by the war and its demobilisation. It was the intention of those who wished for a Muslim state to come from British India to have a clean partition between independent and equal \"Pakistan\" and \"Hindustan\" once independence came.\n\nThe partition itself, according to leading politicians such as Mohammed Ali Jinnah, leader of the All India Muslim League, and Jawaharlal Nehru, leader of the Indian National Congress, should have resulted in peaceful relations. As the Hindu and Muslim populations were scattered unevenly in the whole country, the partition of British India into India and Pakistan in 1947 was not possible along religious lines. Nearly one third of the Muslim population of British India remained in India. Inter-communal violence between Hindus, Sikhs and Muslims resulted in between 500,000 and 1 million casualties.\n\nPrincely-ruled territories, such as Kashmir and Hyderabad, were also involved in the Partition. Rulers of these territories had the choice of joining India or Pakistan.\n\nThe war, also called the First Kashmir War, started in October 1947 when Pakistan feared that the Maharaja of the princely state of Kashmir and Jammu would accede to India. Following partition, princely states were left to choose whether to join India or Pakistan or to remain independent. Jammu and Kashmir, the largest of the princely states, had a majority Muslim population and significant fraction of Hindu population, all ruled by the Hindu Maharaja Hari Singh. Tribal Islamic forces with support from the army of Pakistan attacked and occupied parts of the princely state forcing the Maharaja to sign the Instrument of Accession of the princely state to the Dominion of India to receive Indian military aid. The UN Security Council passed Resolution 47 on 22 April 1948. The fronts solidified gradually along what came to be known as the Line of Control. A formal cease-fire was declared at 23:59 on the night of 1 January 1949. India gained control of about two-thirds of the state (Kashmir valley, Jammu and Ladakh) whereas Pakistan gained roughly a third of The Pakistan controlled areas are collectively referred to as Pakistan administered Kashmir.\n\nThis war started following Pakistan's Operation Gibraltar, which was designed to infiltrate forces into Jammu and Kashmir to precipitate an insurgency against rule by India. India retaliated by launching a full-scale military attack on West Pakistan. The seventeen-day war caused thousands of casualties on both sides and witnessed the largest engagement of armored vehicles and the largest tank battle since World War II. The hostilities between the two countries ended after a ceasefire was declared following diplomatic intervention by the Soviet Union and USA and the subsequent issuance of the Tashkent Declaration. India had the upper hand over Pakistan when the ceasefire was declared.\n\nThis war was unique in the way that it did not involve the issue of Kashmir, but was rather precipitated by the crisis created by the political battle brewing in erstwhile East Pakistan between Sheikh Mujibur Rahman, Leader of East Pakistan, and Yahya Khan and Zulfikar Ali Bhutto, leaders of West Pakistan. This would culminate in the declaration of Independence of Bangladesh from the state system of Pakistan. Following Operation Searchlight and the 1971 Bangladesh atrocities, about 10 million Bengalis in East Pakistan took refuge in neighbouring India.\nIndia intervened in the ongoing Bangladesh liberation movement. After a large scale pre-emptive strike by Pakistan, full-scale hostilities between the two countries commenced.\n\nPakistan attacked at several places along India's western border with Pakistan, but the Indian Army successfully held their positions. The Indian Army quickly responded to the Pakistan Army's movements in the west and made some initial gains, including capturing around of Pakistan territory (land gained by India in Pakistani Kashmir, Pakistani Punjab and Sindh sectors but gifted it back to Pakistan in the Simla Agreement of 1972, as a gesture of goodwill). Within two weeks of intense fighting, Pakistani forces in East Pakistan surrendered to the joint command of Indian and Bangladeshi forces following which the People's Republic of Bangladesh was created. This war saw the highest number of casualties in any of the India-Pakistan conflicts, as well as the largest number of prisoners of war since the Second World War after the surrender of more than 90,000 Pakistani military and civilians. In the words of one Pakistani author, \"Pakistan lost half its navy, a quarter of its air force and a third of its army\".\n\nCommonly known as the Kargil War, this conflict between the two countries was mostly limited. During early 1999, Pakistani troops infiltrated across the Line of Control (LoC) and occupied Indian territory mostly in the Kargil district. India responded by launching a major military and diplomatic offensive to drive out the Pakistani infiltrators. Two months into the conflict, Indian troops had slowly retaken most of the ridges that were encroached by the infiltrators. according to official count, an estimated 75%–80% of the intruded area and nearly all high ground was back under Indian control.\nFearing large-scale escalation in military conflict, the international community, led by the United States, increased diplomatic pressure on Pakistan to withdraw forces from remaining Indian territory. \nFaced with the possibility of international isolation, the already fragile Pakistani economy was weakened further. The morale of Pakistani forces after the withdrawal declined as many units of the Northern Light Infantry suffered heavy casualties. The government refused to accept the dead bodies of many officers, an issue that provoked outrage and protests in the Northern Areas. Pakistan initially did not acknowledge many of its casualties, but Nawaz Sharif later said that over 4,000 Pakistani troops were killed in the operation and that Pakistan had lost the conflict. By the end of July 1999, organized hostilities in the Kargil district had ceased.\nThe war was a major military defeat for the Pakistani Army.\n\nApart from the aforementioned wars, there have been skirmishes between the two nations from time to time. Some have bordered on all-out war, while others were limited in scope. The countries were expected to fight each other in 1955 after warlike posturing on both sides, but full-scale war did not break out.\n\n\n\n\n\nThe nuclear conflict between both countries is of passive strategic nature with nuclear doctrine of Pakistan stating a first strike policy, although the strike would only be initiated if and only if, the Pakistan Armed Forces are unable to halt an invasion (as for example in 1971 war) or a nuclear strike is launched against Pakistan, whereas India has a declared policy of no first use.\n\n\n\n\nThese wars have provided source material for both Indian and Pakistani film and television dramatists, who have adapted events of the war for the purposes of drama and to please target audiences in their nations.\n\n\n\n\n"}
{"id": "4083754", "url": "https://en.wikipedia.org/wiki?curid=4083754", "title": "International Tracing Service", "text": "International Tracing Service\n\nThe International Tracing Service (ITS), in German Internationaler Suchdienst, in French Service International de Recherches in Bad Arolsen, Germany, is an internationally governed centre for documentation, information and research on Nazi persecution, forced labour and the Holocaust in Nazi Germany and its occupied regions. The archive contains about 30 million documents from concentration camps, details of forced labour, and files on displaced persons. ITS preserves the original documents and clarifies the fate of those persecuted by the Nazis. The archives have been accessible to researchers since 2007.\n\nIn 1943, the international section of the British Red Cross was asked by the Headquarters of the Allied Forces to set up a registration and tracing service for missing persons. The organization was formalized under the Supreme Headquarters Allied Expeditionary Forces and named the \"Central Tracing Bureau\" on February 15, 1944. As the war unfolded, the bureau was moved from London to Versailles, then to Frankfurt am Main, and finally to Bad Arolsen, which was considered a central location among the areas of Allied occupation and had an intact infrastructure unaffected by war.\n\nOn July 1, 1947, the International Refugee Organization took over administration of the bureau, and on January 1, 1948 the name was changed to its current International Tracing Service. In April 1951, administrative responsibilities for the service were placed under the Allied High Commission for Germany. When the status of occupation of Germany was repealed in 1954, the ICRC took over the administration of the ITS. The Bonn Agreement of 1955 (which stated that no data that could harm the former Nazi victims or their families should be published) and their amendment protocols dating from 2006 provided the legal foundation of the International Tracing Service. The daily operations were managed by a director appointed by the ICRC, who had to be a Swiss citizen. After some discussion, in 1990 the Federal Republic of Germany renewed its continuing commitment to funding the operations of the ITS. The documents in the ITS archives were opened to public access on November 28, 2007.\n\nTracing missing persons, clarifying people's fates, providing family members with information, also for compensation and pension matters, have been the principal tasks of the ITS since its beginning. Since the opening of the archives, new tasks such as research and education and the archival description of the documents gain more importance in relation to the tasks of tracing and clarifying fates. Since these new activities are not part of its humanitarian mission, the ICRC withdrew from the management of the ITS in December 2012. The Bonn Agreement was replaced on December 9, 2011, when the eleven member states of the International Commission signed two new agreements in Berlin on the future tasks and management of the ITS.\n\nITS was founded as an organization dedicated to finding missing persons, typically lost to family and friends as a result of war, persecution or forced labour during World War II. The service operates under the legal authority of the Berlin Agreements from December 2011 and is funded by the government of Germany. The German Federal Archives are the institutional partner for the ITS since January 2013.\n\nThe organization is governed by an International Commission with representatives from Belgium, France, Germany, Greece, Israel, Italy, Luxembourg, Netherlands, Poland, United Kingdom, and the United States. The Commission draws up the guidelines for the work to be carried out by the ITS and monitors these in the interests of the former victims of persecution.\n\nThe director of the ITS is appointed by the International Commission and is accountable directly to the Commission. Since January 2016, Floriane Azoulay is the director. There are about 240 staff employed by the ITS. The institution is funded by the German Federal Government Commissioner for Culture and the Media (BKM).\n\nOn 28 November 2007 the ITS archives were made broadly available to the general public. The ITS records may be consulted in person, or by mail, telephone, fax or e-mail; addresses and contact numbers are available on the ITS website. Inquiries can be submitted to the ITS using the online form on the organization's website. The archives are also open for research.\n\nThe mailing address is: \n\nAfter the end of the Second World War the main task of the ITS was initially to conduct a search for the survivors of Nazi persecution and their family-members. Today, this accounts for no more than about three percent of its work, which is why the name of the organisation is no longer really up-to-date. However, a large number of new obligations have been taken on over the course of the decades.\n\nThese include certification of the forms persecution took, confirmation for pension and compensation payments, allowing victims and their family members to inspect copies of the original documents and enabling the following generations to find out what happened to their forebears.\n\nMore than 70 years after the end of World War II, the ITS receives more than 1,000 inquiries every month from all around the world. Most of them now come from younger generations who are seeking information about the fate of their family members. In 2015, the ITS received around 15,500 requests regarding the fate of 21,909 persons from survivors, family members or researchers.\n\nDuring the compensation phase of Eastern European forced labourers through the \"Remembrance, Responsibility and Future\" Foundation between 2000 and 2007, around 950,000 enquiries were sent to the Tracing Service. As a result of this flood of enquiries, the ITS was tremendously over-extended. Consequently, this created a gigantic backlog, which temporarily did considerable damage to the standing of the institution. Especially enquiries, which had no direct bearing on the foundation, remained unprocessed.\n\nITS's total inventory comprises 26,000 linear metres of original documents from the Nazi era and post-war period, 232,710 meters of microfilm and more than 106,870 microfiches. Work is under way to digitize the files, both for purposes of easier search and for preserving the historical record.\n\nThe inventory is split up into three main areas: incarceration, forced labour and displaced persons. The variety of documents is enormous. They include list material and individual documents, such as registration cards, transport lists, records of deaths, questionnaires, labour passports, health insurance and social insurance documents. Among the documents are also examples of prominent victims of Nazi persecution, i.e. Anne Frank and Elie Wiesel.\n\nIn addition to this there are smaller sections associated with the work of a tracing service: the alphabetical-phonetic Central Name Index, the child search archives and the correspondence files. The Central Name Index represents the key to the documents. With 50 million references on the fate of over 17.5 million people, it is based on an alphabetic-phonetic filing system that was developed especially for ITS.\n\nMaking the inventory researchable for all historical issues is an urgent responsibilities after opening the archives. To date, the arrangement of the documents having been collected over a period of six decades was subject to the requirements of a tracing service, which brought families together and clarified the fates of individuals. The Central Name Index was the key to the documents, while the documents were arranged according to victim groups.\n\nThis principle no longer is sufficient, since historians ask not only for names, but also for topics, events, locations or nationalities. The goal is to compile finding aids that can be accessed and published online and are based on international archival standards. The first series of inventories could be published on the Internet (for the time being in the German language only). The documents were indexed according to their origin and content. In view of the volume of the documents to be described, this process will take some years' time.\n\nThe International Commission at its May 2007 meeting approved the US Holocaust Memorial Museum's proposal to permit advance distribution of the material, as it is digitized, to the designated repository institutions prior to the completion of the agreement ratification process officially opening the material. In August 2007, the USHMM received the first installment of records and in November 2007, received the Central Name Index. Materials will continue to be received as they are digitized.\n\nOne institution is designated for each of the 11 countries to receive a copy of the archive. The following locations have been designated by their respective countries.\n\nArchives on the fate of prisoners of war exist in Geneva at the ICRC, Central Tracing Agency. Inquiries are dealt with.\n\nOther archives deal with missing Germans on occasion of flight and expulsion and with missing German Wehrmacht soldiers. German Red Cross searches for German Missing persons except those who were prosecuted by Nazi regime. Kirchlicher Suchdienst has knowledge on population of former eastern regions of Germany. Deutsche Dienststelle (WASt) has the archives of Wehrmacht soldiers killed in action. German War Graves Commission has an online-inventory of war graves.\n\nThe ITS had been criticized before 2008 for refusing to open its archives to the public. The ITS, backed by the German government, had cited German archival law to support their position. The laws mandate a 100-year-gap between releasing records in order to protect privacy. However, their critics argued that the ITS as such is not subject to German law. One accusation raised against Germany and the ITS by critics was that the archive was kept closed out of a desire to repress information about the Holocaust.\n\nCritics cited the fact that all eleven governments sitting on the International Commission of the ITS endorsed the Stockholm International Forum Declaration of January 2000, which included a call for the opening of various Holocaust-era archives. However, since the Declaration was made, there had been little practical change in the operations of the ITS, despite repeated negotiations between the ITS, ICRC, and various Jewish and Holocaust survivor advocacy groups. A critical press release from the United States Holocaust Memorial Museum written in March 2006 charged that \"'In practice, however, the ITS and the ICRC have consistently refused to cooperate with the International Commission board and have kept the archive closed.'\" In early 2006, several newspaper articles also raised questions about the quality of the ITS' management and the underlying reasons for the existing backlog.\n\nIn May 2006, the International Commission for the ITS decided to open the archives and documents for researchers use, and to transfer, upon request, one copy of the ITS archives and documents to each one of its member states. This took place once all 11 countries ratified the new ITS Protocol. On November, 28th, 2007 it was announced that Greece, as the last of the member countries, filed its ratification papers with the German Foreign Ministry. It has now been announced that the documents in the archive are open to public access.\n\nAssociated Press (AP) reporters who were given access to ITS files found a carton of documents related to an escapee program run by the Truman Administration. The AP reporters used these files and declassified US documents to describe how the United States asked the ITS to run background checks on escapees from Eastern Europe. The Central Intelligence Agency reviewed their histories and then recruited some of them to return to their countries of origin, to spy for the United States. The program did not return very much useful intelligence, because these recruits, motivated to impress their handlers, supplied information that was not reliable, and because by 1952, the Soviets had largely exposed these efforts. Many recruits disappeared, presumed dead.\n\nA group of students participated from 2013 to 2014 in the project \"DENK MAL – Erinnerung im öffentlichen Raum\" at the school. The students, including the author Tariq Abo Gamra, erected a plaque at the entrance of the school in remembrance of the murdered and prosecuted Jewish students in Nazi Germany. A commemoration ceremony took place on the 10th of November 2014. The project received letters from German Chancellor Angela Merkel and the German President Joachim Gauck congratulating them. The project was supported by the International Tracing Service.\n\n\n"}
{"id": "1448101", "url": "https://en.wikipedia.org/wiki?curid=1448101", "title": "Jan Romein", "text": "Jan Romein\n\nJan Marius Romein (30 October 1893 – 16 July 1962) was a Dutch historian, journalist, literary scholar and professor of history at the University of Amsterdam. A Marxist and a student of Huizinga, Romein is remembered for his popularizing books of Dutch national history, jointly authored with his wife Annie Romein-Verschoor. His work has been translated into English, German, French, Italian, Indonesian and Japanese.\n\nBorn in Rotterdam, Romein married the writer and historian Annie Romein-Verschoor (1895–1978) on 14 August 1920.\n\nRomein began writing while studying humanities at the University of Leiden (1914–1920). Of his professors the historian Johan Huizinga inspired him the most. During his studies and impressed by the First World War and the Russian Revolution he became interested in Marxism. He translated Franz Mehring's biography on Karl Marx into Dutch (1921; with an introductory essay). After a stay of seven months in Denmark, where Romein's friend and former fellow student Hans Kramers had become the assistant of the physicist Niels Bohr, the couple moved to Amsterdam in 1921. Romein became an editor of the daily \"De Tribune\" of the young Communistische Partij Holland (CPH, Communist Party of Holland). In addition, he worked as a freelance writer and translator. Already in 1916-1918 he published a Dutch translation of Romain Rolland's \"Jean Christophe\" (10 vols., with an introductory essay). In 1924 he received his doctoral degree, with the highest distinction, at the University of Leiden with the dissertation \"Dostoyevsky in the Eyes of Western Critics\". In 1927 he left the communist party, but he remained interested in Marxism and in the political development of the Soviet Union and of Asia. After publishing a book on the history of the Eastern Roman Empire (\"Byzantium\", 1928) he translated and edited the \"Harmsworth's Universal History of the World\" into Dutch, in co-operation with other historians (1929–1932, 9 vols.), with three added chapters written by himself. His first book publication in the field of Dutch history was a pioneering study on the history of Dutch historical writing during the Middle Ages (1932). His most famous books include a history of \"The Low Countries\" (1934) and a four-volume work with 36 short biographies of important Dutch (1938–1940), both in cooperation with his wife and fellow historian Annie Romein-Verschoor. In 1939, Romein was appointed professor of history at the University of Amsterdam. He survived World War II after being held hostage as a prisoner for three months by the German police in the notorious Amersfoort police detention camp, and returned to writing and teaching. In 2011 Jan Romein and his wife were posthumously awarded the title \"Righteous among the Nations\" by Yad Vashem in Jerusalem, for offering a hiding place to a persecuted Jewish fellow-citizen during the German occupation.\n\nIn 1937, he published an essay on technology called \"The dialectics of progress\" (in Dutch: \"De dialectiek van de vooruitgang\") in which he describes a phenomenon called the \"Law of the handicap of a head start\" (\"Wet van de remmende voorsprong\"), as part of the series \"The unfinished past\" (in Dutch: \"Het onvoltooid verleden\"). This article was also published in German as \"Dialektik des Fortschritts\" in: \"Mass und Wert. Zweimonatsschrift für freie deutsche Kultur\" (eds. Thomas Mann and Konrad Falke), vol. 2 (Zurich, Switzerland, 1939).\n\nIn 1946, Annie Romein received a copy of Anne Frank's diary which she tried to have published. When she was unsuccessful, she gave the diary to her husband, who wrote the first article about the diary and its writer, for the newspaper \"Het Parool\". Interest raised by his article led to the diary being published the following year. Romein was interested in the (auto)biographical approach to history. During the years of German occupation he wrote a book on this, which was published in 1946 (German translation 1948) and is still regarded as an informative and original contribution to the historiography of the genre. Also in 1946, he introduced the theory of history as a subject in the academic curriculum.\nSoon after the beginning of the Cold War his marxist conceptions, though undogmatic, had placed him in relative isolation, and in 1949 he was denied entry to the United States for an intended speaking engagement at an international scholarly conference in Princeton. Instead, he was welcomed as a guest professor in the young republic of Indonesia during the 1951–1952 academic year. He devoted the latter part of his life to writing a history of Europe during the 25-year period from 1889 to 1914: \"The Watershed of Two Eras. Europe in 1900\", which was published posthumously in 1967 (English edition 1978). It is an attempt at writing \"integral history\" of the transitional decades from the 19th to the 20th century, during which Europe's supremacy in the world started waning. The book devotes chapters to all aspects of European history during this period: political, economic, social, cultural, scientific, literature, art, emancipation movements etc. Due to a chronic illness, which became manifest in 1959, he limited his professorship at the University of Amsterdam to solely Theoretical History. He died in Amsterdam in 1962.\n\nRomein held Marxist views and was active within the Communist Party of Holland (CPH) from 1917 onwards, initially as a secretary for the communist parliamentarian Willem van Ravesteyn, then as a high-ranking editor of the party's daily \"De Tribune\". When internal struggles led to a Moscow-backed \"palace revolution\" in 1925, Romein sided with Van Ravesteyn and David Wijnkoop, and as a result was forced out of the \"Tribune\"'s editorial board. Expulsion from the party for continued support of Wijnkoop followed in 1927, ending Romein's direct involvement in politics.\n\nRomein's Marxism made him a controversial figure and affected his career when in 1938 he was a candidate for a professorship at the Municipal University of Amsterdam, where professors were appointed by the municipal council. On the one hand, the liberals and right-wing factions disliked Romein for his continued support of the Soviet Union; on the other hand, the communist faction headed by Romein's old friend Wijnkoop were upset about his openly criticizing of Soviet show trials, two years earlier. This criticism, however, won him some sympathy within SDAP faction. A majority that included Wijnkoop's communists voted against Romein's appointment. A year later, when a different professorship was vacant, Romein was appointed professor of Dutch history, this time with support from Wijnkoop and despite a vehement campaign against his candidature.\n\nIn the Cold War years of the early 1950s, Romein and his collaborators became increasingly isolated. He condemned the Soviet crackdown on the 1956 Hungarian uprising, in a pamphlet that simultaneously denounced the French and British involvement in Egypt during the Suez Crisis.\n\nIn English:\n\nBooks in Dutch, German, Italian and Bahasa:\n\n"}
{"id": "588263", "url": "https://en.wikipedia.org/wiki?curid=588263", "title": "Johann Friedrich Böhmer", "text": "Johann Friedrich Böhmer\n\nJohann Friedrich Böhmer (22 April 179522 October 1863) was a German historian. His historical work was chiefly concerned with collecting and tabulating charters and other imperial documents of the Middle Ages.\n\nBöhmer was born in Frankfurt as the son of the Palatine official Karl Ludwig Böhmer. Educated at the universities of Heidelberg and Göttingen, he showed an interest in art and visited Italy; but returning to Frankfurt he turned his attention to the study of history, and became secretary of the \"Gesellschaft für ältere deutsche Geschichtskunde\". He was also archivist and then librarian of the city of Frankfurt.\n\nBöhmer had a great dislike of Prussia and the Protestant faith, and a corresponding affection for Austria and the Roman Catholic Church, to which, however, he did not belong. His critical sense was, perhaps, somewhat warped; but his researches are of great value to students. He died unmarried.\n\nBöhmer's historical work was chiefly concerned with collecting and tabulating charters and other imperial documents of the Middle Ages. First appeared an abstract, the \"Regesta chronologico-diplomatica regum atque imperatorum Romanorum 911-1313\" (Frankfurt, 1831), which was followed by the \"Regesta chronologico-diplomatica Karolorum. Die Urkunden sämtlicher Karolinger in kurzen Auszügen\" (Frankfurt, 1833), and a series of \"Regesta imperii\". For the period 1314-1347 (Frankfurt, 1839) the \"Regesta\" was followed by three, and for the period 1246-1313 (Frankfurt, 1844) by two supplementary volumes. The remaining period of the \"Regesta\", as edited by Böhmer, is 1198-1254 (Stuttgart, 1849). These collections contain introductions and explanatory passages by the author.\n\nVery valuable also is the \"Fontes rerum Germanicarum\" (Stuttgart, 1843–1868), a collection of original authorities for German history during the 13th and 14th centuries. The fourth and last volume of this work was edited by A. Huber after the author's death. Other collections edited by Böhmer are: \"Die Reichsgesetze 900-1400\" (Frankfurt, 1832); \"Wittelsbachische Regesten von der Erwerbung des Herzogtums Bayern bis zu 1340\" (Stuttgart, 1854); and \"Codex diplomaticus Moeno-Francofurtanus\". \"Urkundenbuch der Reichsstadt Frankfurt\" (Frankfurt, 1836; new edition by F Law, 1901).\n\nOther volumes and editions of the \"Regesta imperii\", edited by Julius von Ficker, Engelbert Mühlbacher, Eduard Winkelmann and others, are largely based on Böhmer's work. Böhmer left a great amount of unpublished material, and after his death two other works were published from his papers: \"Acta imperii selecta\", edited by J. Ficker (Innsbruck, 1870); and \"Regesta archiepiscoporum Maguntinensium\", edited by C Will (Innsbruck, 1877–1886).\n"}
{"id": "38011003", "url": "https://en.wikipedia.org/wiki?curid=38011003", "title": "John-Paul Himka", "text": "John-Paul Himka\n\nJohn-Paul Himka (born May 18, 1949 in Detroit, Michigan) is an American-Canadian historian and retired professor of history of the University of Alberta in Edmonton. Himka received his BA in Byzantine-Slavonic Studies and Ph.D. in History from University of Michigan in 1971 and 1977 respectively. The title of his Ph.D. dissertation was \"Polish and Ukrainian Socialism: Austria, 1867-1890\". As a historian Himka was a Marxist in the 1970s-80s, but became influenced by postmodernism in the 1990s. In 2012 he defined his methodology in history as \"eclectic\".\n\nHimka is of mixed ethnic background, Ukrainian (on father's side) and Italian (on mother's). Initially he wanted to become a Greek Catholic priest and studied at St. Basil Seminary in Stamford, Connecticut. However, due to the radicalization of his political views to the left by the end of the 1960s he did not pursue that vocation.\n\nSince 1977, he taught at University of Alberta, Department of History and Classics. He became full Professor in 1992 and retired from the university in 2014. Himka is the recipient of several awards and fellowships, most notably the Rutherford Award for Excellence in Undergraduate Teaching in 2006, the Philip Lawson Award for Excellence in Teaching, and the J. Gordin Kaplan Award for Research Excellence. He served as co-editor of the \"Encyclopedia of Ukraine\" for three volumes devoted to history.\n\nHimka, who traveled to Ukraine to conduct research since 1976, began to work with academics at Lviv University's Department of History. Initially Himka focused on Galicia's social history in the 19th and 20th centuries. The 1988 1000th anniversary of the Christianization of Rus' kindled his interest in the history of Greek Catholic Church and the influence of the church in terms of the development of Ukrainian nationalism. In 2002 he researched socialism in Habsburg Galicia, a formerly autonomous region in Western Ukraine, sacred culture of the Eastern Slavs (on iconography in particular) and the Holocaust in Ukraine. Since the late 1990s his contention with what he calls Ukrainian \"nationalist historical myths\" became subject of increasing, sometimes heated, debates both in Ukraine and Ukrainian Diaspora (especially in North America). Himka challenged the interpretation of Holodomor as a genocide and the view that Ukrainian nationalism and nationalists played no or almost no role in the Holocaust in Ukraine. He also opposed official glorification of such nationalistic heroes as Roman Shukhevych and Stepan Bandera in Ukraine during presidency of Viktor Yushchenko.\n\nIn his 1996 article, \"Krakivski visti and the Jews, 1943: A Contribution to the History of Ukrainian-Jewish Relations during the Second World War\", published in the \"Journal of Ukrainian Studies\", based on earlier Ukrainian-language versions presented in 1991 in Kyiv and 1993 in Jerusalem at Ukrainian-Jewish relations conferences, Himka wrote that the history of Ukrainian-Jewish relations during WWII remained surprisingly under investigated. Himka cited the Raul Hinberg's \"monumental study\", \"The Destruction of the European Jews\" as the source used by historians. In response to this lacuna, Himka presented his detailed study of the publishing of a series of antisemitic articles in 1943 in the \"flagship of Ukrainian journalism under Nazi occupation,\" Krakov's daily newspaper, the \"Krakivs'ki Visti\". The primary sources for his study included the articles as well as the records of the \"Krakivs'ki Visti\" maintained by Ukrainian-Canadian, Michael Chomiak, who was born in the Ukraine in the 1910s as Mykhailo Khomiak and changed his name to Michael Chomiak when he emigrated to Canada at the end of WWII. The Provincial Archives of Alberta acquired Chomiak's records in 1985 following Chomiak's death. He was the chief editor of \"Krakivs'ki Visti\" from 1940 to 1945. Himka is his son-in-law. Himka described how \"Krakivs'ki Visti\" \"played an important and, generally, positive role in Ukrainian life,\" \"serving as a buffer between the German occupation authorities and the population of the Generalgouvernement.\" In response to a May 1943 order by the German press chief, the newspaper published antisemitic articles from May 25 through July which were received negatively by the Ukrainian intelligentsia in general.\n\nHimka completed a series of three major studies on the history of Ukrainian Galicia in the 19th century. The first, \"Socialism in Galicia: the emergence of Polish social democracy and Ukrainian radicalism (1860-1890)\" was published in 1983. The second, \"Galician Villagers and the Ukrainian National Movement in the Nineteenth Century\" was published in 1988. The third, \"Religion and Nationality in Western Ukraine: The Greek Catholic Church and the Ruthenian National Movement in Galicia, 1867-1900\", which is \"devoted to the interrelations between church and state\", was published in 1999. In a book review in the \"Harvard Ukrainian Studies\", Larry Wolff described \"Religion and Nationality in Western Ukraine\" subtle, sophisticated and insightful account of an \"important and profoundly complex historical problem.\" Wolff writes that Himka's research \"makes the case for a contingent and evolutionary perspective on nationality in which several different forms and inflections of national identity jostle one another in cultural competition, enhanced or diminished by various historical forces, including religion, without any predetermined outcome. \"engaged with the all-important issue of national identity, makes a brilliant contribution, not just to the history of Ukrainian nationality, but also to the general theoretical understanding of modern nationalism.\" Himke employs \"concepts of nationality and nationalism developed by Ernest Gellner, E.J. Hobsbawm, and Miroslav Hroch. Himka observed that the \"Greek Catholic case in Galicia\" is \"a stunningly transparent instance of how much agency and choice can be involved in the construction of nationality.\"\n\nIn his 2005 article, \"War Criminality: A Blank Spot in the Collective Memory of the Ukrainian Diaspora War Criminality\" he examined material that emerged from an important Ukrainian-Jewish relations conference held in 1983, that happened to be held on the 50th anniversary of the Soviet famine of 1932–33. as well as \"current electronic media and recent years of \"The Ukrainian Weekly\", supplemented with a retrospective sampling of articles from Svoboda.\" At the 1983 conference, Professor Yaroslav Bilinksy \"denied \"a causal connection between alleged collaboration of Jewish-born Communists in the collectivization of agriculture and the Great Famine and any proven collaboration of Ukrainian-born extremists in the Holocaust.\"\n\nHis 2009 book, \"Last Judgment Iconography in the Carpathians\", was the result of ten years of research \"throughout the region of the Carpathian Mountains, where he \"found a distinctive and transnational blending of Gothic, Byzantine, and Novgorodian art.\"\n\nIn his chapter \"Ethnicity and the Reporting of Mass Murder: Krakivs′ki visti, the NKVD Murders of 1941, and the Vinnytsia Exhumation\", Himka examined how the \"Krakivs'ki Visti\", an \"important [Ukrainian] nationalist newspaper\" \"reported on two cases of mass violence by the Soviets, the 1941 NKVD prisoner massacres and the 1943 Vinnytsia massacre. Himka wrote that \"Krakivs'ki Visti\" \"ethnicized both perpetrators and victims, ascribing primarily Jewish identity to the former and depicting the latter as almost exclusively Ukrainian.\"\n\nJohn-Paul Himka is married to Chrystia Chomiak, Mykhailo Chomiak's (c.1910s - 1984) daughter. Himka learned of Chomiak's role as editor of the \"Krakivs'ki Visti\" in Chomiak's personal papers after he died in 1984. They have two children.\n\nHe was awarded the 2001-2002 Killam Annual Professorship\n\n\n\n\n"}
{"id": "1186118", "url": "https://en.wikipedia.org/wiki?curid=1186118", "title": "Jōji", "text": "Jōji\n\nDuring the Meiji period, an Imperial decree dated March 3, 1911 established that the legitimate reigning monarchs of this period were the direct descendants of Emperor Go-Daigo through Emperor Go-Murakami, whose had been established in exile in Yoshino, near Nara.\n\nUntil the end of the Edo period, the militarily superior pretender-Emperors supported by the Ashikaga shogunate had been mistakenly incorporated in Imperial chronologies despite the undisputed fact that the Imperial Regalia were not in their possession.\n\nThis illegitimate had been established in Kyoto by Ashikaga Takauji.\n\n\nIn this time frame, Shōhei (1346–1370) was a Southern Court equivalent \"nengō,\"\n\n\n\n"}
{"id": "11400449", "url": "https://en.wikipedia.org/wiki?curid=11400449", "title": "Karl Benrath", "text": "Karl Benrath\n\nKarl Benrath (10 August 1845 in Düren – 21 July 1924 in Königsberg) was a German church historian.\n\nBenrath was educated in Bonn, Berlin and Heidelberg. In 1871 went on a scientific tour of several years to Italy and England. From 1879 he was professor at Bonn, and from 1890 professor of church history at Königsberg.\n\nHis most important works describe 16th. century reformation movement in Northern Italy.\n\n"}
{"id": "11643673", "url": "https://en.wikipedia.org/wiki?curid=11643673", "title": "Keiun", "text": "Keiun\n\n, also known as Kyōun, was a following \"Taihō\" and preceding \"Wadō\". The period spanned the years from May 704 through January 708. The reigning emperors were and .\n\n\n\n\n"}
{"id": "20623953", "url": "https://en.wikipedia.org/wiki?curid=20623953", "title": "Leibniz Institute of European History", "text": "Leibniz Institute of European History\n\nThe Leibniz Institute of European History (IEG) in Mainz, Germany, is an independent, public research institute that carries out and promotes historical research on the foundations of Europe in the early and late Modern period. Though autonomous in nature, the IEG has close connections to the Johannes Gutenberg University Mainz. In 2012, it joined the Leibniz Association.\n\nFounded in 1950 on the initiative of Raymond Schmittlein, the head of the Direction Générale des Affaires Culturelles of the French military government, the new institution had the aim of helping to overcome the longstanding nationalist and confessional divides between the European states and their populations through “non-prejudiced” historical research and, in so doing, to support Franco-German reconciliation in particular. Specifically, it was intended that research conducted at the Institute would assist a revision (“detoxification”) of the history (text)books, and eventually enable the establishment of a “European history book”.\n\nThis idea had surfaced in the late-1940s during dialogues between German and French historians in Speyer, which Schmittlein had set up in 1948/49. It became mixed with concepts of a Christian, “Occidental” history which were prevalent among a group of German historians, which included the medieval historian Fritz Kern (1884–1950) based in Bonn. He had headed the German delegation in 1948. The Catholic theologian and church historian Joseph Lortz (1887–1975) had also participated in these dialogues.\n\nThe first plans for the foundation of an “Institute for cultural and religious history” were drafted by Kern, who – as the first director of the Institute – in addition to the didactic goal also wanted to realize a multi-volume history of the world (“Historia Mundi”) that would be based on religion and would adopt a universal-historical perspective. Lortz served as an additional founding director. In 1950, he took up an extraordinary professorship for western religious history which was created especially for him in the philosophical faculty of Mainz University. These founding aims and history explain the structure of the research institute that was established in 1950 under the title “Institut für Europäische Geschichte” (IEG) (its charter came into effect on 19 April 1951), with its department of “Western Religious History” (Abteilung fuer Abendlaendische Religionsgeschichte) and its department of “Universal History” (Abteilung fuer Universalgeschichte). Both departments were headed up by a director, as they still are (currently Irene Dingel and Johannes Paulmann). Today, the directors also hold professorships at Johannes Gutenberg University Mainz.\n\nThe IEG's research on the historical foundations of Europe considers both integrating and antagonistic movements and forces shaping the geographic continent as well as the cultural context of Europe over centuries and setting up its distinct characteristics in contrast to the other continents. Research at the IEG thus targets pan-European and partly European communicative connections originating in bilateral and multilateral transfer processes. Their protagonists did not necessarily have to be aware of their taking part in \"Europe-wide\" interrelations. The religious and confessional developments of these transfer processes are one important focus of research at the IEG.\n\nAlso, research on the foundations of Europe maps the history of conscious reflections on Europe and Europeans, and analyses attempts at political unification, existing political plans for Europe, ideal conceptions and utopian visions of Europe – always including anti-European ideas in the picture. An integral element of this approach is the history of historiography on Europe.\n\nThis conceptual formulation includes a reflexion of theory and methods of historical research on Europe. The Leibniz Institute of European History questions the focus of interest of 'European' approaches and reflects the underlying ideological propositions in historical research on the history of Europe.\n\nThe Institute’s charter defines the primary goals of the IEG as follows: \"Research on the religious and intellectual traditions of Europe, their development and crises, and particularly on religious differences, their effects and the possibilities of overcoming these differences\", and \"Europe-focused fundamental research which assists the historical understanding of the process of the coalescence of Europe and the individual historical paths of the European states and peoples\".\nIn accordance with its charter, IEG pursues these goals through:\n\nThe Leibniz Institute of European History (IEG) researches the historical foundations of Europe in the modern era. Its research projects are developed in an interdisciplinary way by the sections \"Abendländische Religionsgeschichte\" and \"Universalgeschichte\". They span the historical periods from the beginning of the early modern period to contemporary history.\nThe central theme of the current research programme at the IEG is \"negotiating difference\" – the ways in which difference is established, confronted and enabled in its religious, cultural, political and social dimensions.\n\nTogether with the Johannes Gutenberg University Mainz, the IEG runs the graduate project \"The Christian Churches and the Challenge of 'Europe'”, which is part-funded by the German Research Foundation (Deutsche Forschungsgemeinschaft – DFG).\n\nSince 1950, the IEG has awarded doctoral and postdoctoral fellowships for research stays in Mainz. It also welcomes international researchers as fellows in residence with external funding. The IEG states that its research fellowship programme combines research, training and international networking. Fellows can pursue their individual research projects. They discuss problems and methods of European historical research in an interdisciplinary and international surrounding. According to their special fields and interests, they are invited to participate in the academic activities of the Institute. In cooperation with its international partners, the Institute supports the exchange of fellows to embed them in a network of European historical research.\n\nThe Leibniz Institute of European History (IEG) awards research scholarships and fellowships to young academics (doctoral and postdoctoral researchers) from Germany and abroad. Funding is provided for both doctoral and postdoctoral research dealing with the religious, political, social and cultural history of Europe from the early modern period to 1989/90. Comparative, transnational and transfer-historical research projects are particularly welcome, as are projects which deal with topics of intellectual, church or theological history.\nIn 2013, the IEG established the Senior Research Fellowship Programme for established researchers. It enables the directors to invite respected academics from other European countries and outside Europe to Mainz to pursue their own research and to exchange ideas with the academics working at the IEG.\nFor decades, the IEG has been receiving other scholarship holders and fellows funded by organizations abroad and in Germany (for example, the Alexander von Humboldt Foundation and the DAAD) as visiting academics. Additionally, the Institute participates in the Leibniz-DAAD Research Fellowship Programme. These visiting researchers, who are usually young academics, usually stay at the Institute where they become part of the international community of IEG fellows and scholarship holders.\n\nThe IEG works to promote innovative Open Access publications, the use of digital tools in historical research, and the expansion of digital research infrastructures. Far beyond the confines of academia, there exists a wide interest in research findings and resources published in Open Access – for instance, the wide range of digitized historical maps or European History Online (EGO), published in German and English, which provides an overview of 500 years of modern European history across boundaries of geography, discipline, and methodology. Core questions of European history are also addressed in the Jahrbuch für Europäische Geschichte/European History Yearbook, which has been published in an Open Access format since 2014 and contains articles in English and German. \n\nAnother Open Access publication, \"On site, in time - Negotiating differences in Europe\", looks at events that took place in European locations and cast new light on the historical development of Europe since 1500. Approximately 60 articles illustrate the manifold conflict-laden approaches to difference and inequality have been.\n\nThe Leibniz Institute of European History (IEG) is part of the new Mainz Centre for Digitality in the Humanities and Cultural Studies (mainzed). mainzed is a joint initiative of six scientific institutions to promote digital methodology in the humanities and cultural sciences in Mainz, Germany.\n\nThe library offers approximately 90,000 printed titles and 900,000 licensed online resources on the history of Europe from the mid-15th century. There is an emphasis on general European history and international history, as well as on church history and the history of theology since the period of Humanism and the Reformation.\n\nThe library holds numerous international learned journals and periodicals, with more than 500 running subscriptions (see the Overview of Journals ZDB). Additionally, a large number of both special and general bibliographical aids are available. The library’s entire collection is contained in the Institute’s Online Catalogue (OPAC). The catalogue also contains the new acquisitions of the current year and a large array of online resources and databases funded by the German Research Foundation. In the context of the HeBIS union of libraries, the Institute’s library is part of the local library system (Lokales Bibliothekssystem – LBS) of Rhine-Hesse (organization and technical support is provided by Mainz University Library).\n\nThe IEG edits a monograph serial and a serial for conference reports, the latter being published online or in print. It runs a server for historical digital maps (IEG-MAPS) and publishes European peace treaties of the early modern period.\n\n\n"}
{"id": "2602118", "url": "https://en.wikipedia.org/wiki?curid=2602118", "title": "Majiabang culture", "text": "Majiabang culture\n\nThe Majiabang culture was a Chinese Neolithic culture that existed at the mouth of the Yangtze River, primarily around Lake Tai near Shanghai and north of Hangzhou Bay. The culture spread throughout southern Jiangsu and northern Zhejiang from around 5000 BC to 3300 BC. The later part of the period is now considered a separate cultural phase, referred to as the Songze culture.\n\nInitially, archaeologists had considered the Majiabang sites and sites in northern Jiangsu to be part of the same culture, naming it the . Archaeologists later realized that the northern Jiangsu sites were of the Dawenkou culture and renamed the southern Jiangsu sites Majiabang culture. Some scholars state that the Hemudu culture co-existed with the Majiabang culture as two separate and distinct cultures, with cultural transmissions between the two. Other scholars group Hemudu in with Majiabang subtraditions.\n\nMajiabang people cultivated rice. At Caoxieshan and Chuodun, sites of the Majiabang culture, archaeologists excavated paddy fields, indicating the centrality of rice to the economy. In addition faunal remains excavated from Majiabang archaeological sites indicated that people had domesticated pigs. However, the remains of sika and roe deer have been found, showing that people were not totally reliant on agricultural production. Archaeological sites also bear evidence that Majiabang people produced jade ornaments.\n\nIn the lower stratum of the Songze excavation site in Shanghai's modern day Qingpu District, archaeologists found the prone skeleton of one of the area's earliest inhabitantsa 25-30-year-old male with an almost complete skull dated to the Majiabang era.\n\n"}
{"id": "5944672", "url": "https://en.wikipedia.org/wiki?curid=5944672", "title": "Mihály Horváth", "text": "Mihály Horváth\n\nMihály Horváth (20 October 1809, Szentes – 19 August 1878, Karlsbad) was a Hungarian Roman Catholic bishop, historian, and politician. He was an exponent of Hungarian nationalism with an emphasis on its historical culture.\n\n\n"}
{"id": "23240311", "url": "https://en.wikipedia.org/wiki?curid=23240311", "title": "Morten Ringard", "text": "Morten Ringard\n\nMorten Richard Ringard (18 May 1908 – 1973) was a Norwegian educator and writer.\n\nHe was born in Holmestrand. He spent much of his career in Flekkefjord, from 1942 as a school director. In the same year he wrote a book on the city history, \"Flekkefjords historie\". In 1952 he wrote the 200-year history of Holmestrand in \"Byen under fjellet\". He biographed Hans Seland, Gustaf Fröding, Herman Bang and Metternich. Poetry collections include \"Nattens Sang\" (1931). He was honored with a Festschrift in 1968.\n"}
{"id": "188399", "url": "https://en.wikipedia.org/wiki?curid=188399", "title": "Mourning", "text": "Mourning\n\nMourning is, in the simplest sense, grief over someone's death. The word is also used to describe a cultural complex of behaviours in which the bereaved participate or are expected to participate. Customs vary between cultures and evolve over time, though many core behaviors remain constant.\n\nWearing black clothes is one practice followed in many countries, though other forms of dress are seen. Those most affected by the loss of a loved one often observe a period of grieving, marked by withdrawal from social events and quiet, respectful behavior. People may follow religious traditions for such occasions.\n\nMourning may apply to the death of, or anniversary of the death of, an important individual like a local leader, monarch, religious figure,family etc. State mourning may occur on such an occasion. In recent years, some traditions have given way to less strict practices, though many customs and traditions continue to be followed.\n\nElisabeth Kübler-Ross has developed a well known model, not yet scientifically proven, the subject of many highly contested transpositions and adaptations. It is a theoretical cycle consisting of five steps:\n\nThe above five phases can be linear but often a mourner can flip back before starting to move forward. A good way to pass through grief is to understand what has happened and share the feelings and emotions with relatives or people who also are grieving. These steps do not necessarily follow each other. It is not an inevitable process.\n\nMourning is a personal and collective response which can vary depending on feelings and contexts. It starts with denial and ends with acceptance.\n\nGrief can be defined as the period following the death of someone close. This is both psychological and social:\n\nDeath can be a release in the case of a tyrannical person or when death terminates a long, painful illness. However, this release may add remorse and guilt for the mourner.\n\nIn Ethiopia, an \"Edir\" (var. \"eddir\", \"idir\") is a traditional community organization whose members assist each other during the mourning process. Members make monthly financial contributions forming the Edir's fund. They are entitled to receive a certain sum of money from this fund to help cover funeral and other expenses associated with deaths. Additionally, Edir members comfort the mourners: female members take turns doing housework, such as preparing food for the mourning family, while male members usually take the responsibility to arrange the funeral and erect a temporary tent to shelter guests who come to visit the mourning family. Edir members are required to stay with the mourning family and comfort them for three full days.\n\nWhite is the traditional color of mourning in Chinese culture, with white clothes and hats formerly having been associated with death. In imperial China, Confucian mourning obligations required even the emperor to retire from public affairs upon the death of a parent. The traditional period of mourning was nominally 3 years, but usually 25–27 lunar months in practice and even shorter in the case of necessary officers. (The emperor, for example, typically remained in seclusion only 27 days.)\n\nThe Japanese term for mourning dress is \"mofuku\" (喪服). The term refers to either primarily black Western-style formal wear or to black traditional Japanese clothing worn at funerals and Buddhist memorial services. Other colors, particularly reds and bright shades, are considered inappropriate for mourning dress. If wearing Western clothes, women may wear a single strand of white pearls. Japanese-style mourning dress for women consists of a five-crested plain black silk kimono, black obi and black accessories worn over white undergarments, black sandals and white split-toe socks. Men's mourning dress consists of clothing worn on extremely formal occasions: a plain black silk five-crested kimono and black and white or gray and white striped hakama trousers over white undergarments, black crested haori jacket with a white closure, white or black zori and white tabi. It is customary for Japanese-style mourning dress to be worn only by the immediate family and very close friends of the deceased; other attendees wear Western-style mourning dress or subdued Western or Japanese formal clothes.\n\nIn Thailand people will wear black when attending a funeral. Black is considered the mourning color, although historically it was white. Widows may wear purple when mourning the death of their spouse.\n\nThe Filipino practices for mourning have influences from Chinese, Japanese and folk Catholic beliefs. People may wear white or black. The color red is frowned upon in the time of mourning, it is believed that those who wear red within 9–40 days will die or suffer illness. The consumption of chicken during the wake and funeral is believed to bring death among the relatives. There is an initial nine-day mourning practice called \"Pasiyam\", a novena is to be prayed by those who are mourning. During those days the spirit of the deceased is believed to be roaming. 40 days is a Catholic practice of commemorating the dead after 40 days from their death date. A Mass and a small feast are held to commemorate the dead during the 40-day period, the 40th day as their judgment day. The immediate family wear black. When the one-year period is over, the first death anniversary will signal the end of mourning celebrated by a feast.\n\nIn the Assyrian tradition, just after a person passes way, the mourning family host guests in an open house style. Only bitter coffee and tea are served, showcasing the sorrowful state of the family. On the funeral day, a memorial mass is held in the church. At the graveyard, the people gather and burn incense around the grave as clergy chant hymns in the Syriac language. The closest female relatives traditionally bewail or lament in a public display of grief, as the casket descends. A few others may sing a dirge or a sentimental threnody. During all these occasions, everyone is expected to dress in complete black. Following the burial, everyone would return to the church hall for afternoon lunch and eulogy. At the hall, the closest relatives sit on a long table facing the guests (akin to a bridegroom’s table at a wedding) as many people walk by before leaving shaking hands, offering their condolences. On the third day, mourners would customarily visit the grave site with a pastor to burn incense, symbolising Jesus’ triumph over death on the third day, and 40 days after the funeral (representing Jesus ascending to heaven), and in conclusion one year to the date. Mourners also wear only black until the 40 day mark and would typically not dance or celebrate any major events for one year.\n\nThe custom of wearing unadorned black clothing for mourning dates back at least to the Roman Empire, when the \"toga pulla\", made of dark-colored wool, was worn during mourning.\n\nThrough the Middle Ages and Renaissance, distinctive mourning was worn for general as well as personal loss; after the St. Bartholomew's Day Massacre of Huguenots in France, Elizabeth I of England and her court are said to have dressed in full mourning to receive the French Ambassador.\n\nWomen in mourning and widows wore distinctive black caps and veils, generally in a conservative version of the current fashion.\n\nIn areas of Russia, the Czech Republic, Slovakia, Greece, Mexico, Portugal, and Spain, widows wear black for the rest of their lives. The immediate family members of the deceased wear black for an extended time. Since the 1870s, mourning practices for some cultures, even those who have emigrated to the United States, are to wear black for at least two years, though lifelong black for widows remains in Europe.\n\nIn Belgium the Court went in public mourning after publication in the Moniteur Belge. In 1924 the court went in mourning after the death of Marie-Adélaïde, Grand Duchess of Luxembourg, (10 days); The duke of Montpensier ( 5 days) and a full month for the death of Princess Louise of Belgium.\n\nThe color of deepest mourning among medieval European queens was white. In 1393, Parisians were treated to the unusual spectacle of a royal funeral carried out in white, for Leo V, King of Armenia, who died in exile. This royal tradition survived in Spain until the end of the 15th century. In 1934, Queen Wilhelmina of the Netherlands reintroduced white mourning after the death of her husband Prince Henry. It has remained a tradition in the Dutch royal family. In 2004, the four daughters of Queen Juliana of the Netherlands all wore white to their mother's funeral. In 1993, the Spanish-born Queen Fabiola introduced it in Belgium for the funeral of her husband, King Baudouin of Belgium. The custom for the Queens of France to wear [white mourning] was the origin of the White Wardrobe created in 1938 by Norman Hartnell for Queen Elizabeth (later called Queen Mother). She was required to make a State visit to France while in mourning for her mother.\n\nToday, no special dress or behaviour is obligatory for those in mourning in the general population, although ethnic and religious faiths have specific rituals, and black is typically worn at funerals. Traditionally, however, there were strict social rules to be observed.\n\nBy the 19th century, mourning behaviour in England had developed into a complex set of rules, particularly among the upper classes. For women, the customs involved wearing heavy, concealing, black clothing, and the use of heavy veils of black crêpe. The entire ensemble was colloquially known as \"widow's weeds\" (from the Old English , meaning \"garment\").\n\nSpecial caps and bonnets, usually in black or other dark colours, went with these ensembles. There was mourning jewellery, often made of jet. Jewellery was also occasionally made from the hair of the deceased. The wealthy would wear cameos or lockets designed to hold a lock of the deceased's hair or some similar relic.\n\nWidows were expected to wear special clothes to indicate that they were in mourning for up to four years after the death, although a widow could choose to wear such attire for the rest of her life. To change the costume earlier was considered disrespectful to the deceased and, if the widow was still young and attractive, suggestive of potential sexual promiscuity. Those subject to the rules were slowly allowed to re-introduce conventional clothing at specific times; such stages were known by such terms as \"full mourning\", \"half mourning\", and similar descriptions. For half mourning, muted colours such as lilac, grey and lavender could be introduced.\nFriends, acquaintances, and employees wore mourning to a greater or lesser degree depending on their relationship to the deceased. Mourning was worn for six months for a sibling. Parents would wear mourning for a child for \"as long as they feel so disposed\". A widow was supposed to wear mourning for two years and was not supposed to enter society for twelve months. No lady or gentleman in mourning was supposed to attend social events while in deep mourning. In general, servants wore black armbands when there had been a death in the household. However, amongst polite company the wearing of a simple black armband was seen as appropriate only for military men, or others compelled to wear uniform in the course of their duties—a black arm band instead of proper mourning clothes was seen as a degradation of proper etiquette and to be avoided. In general, men were expected to wear mourning suits (not to be confused with morning suits) of black frock coats with matching trousers and waistcoats. In the later interbellum period between World War I and World War II, as the frock coat became increasingly rare, the mourning suit consisted of a black morning coat with black trousers and waistcoat, essentially a black version of the morning suit worn to weddings and other occasions, which would normally include coloured waistcoats and striped or checked trousers.\n\nFormal mourning culminated during the reign of Queen Victoria, whose long and conspicuous grief over the death of her husband, Prince Albert, may have had much to do with it. Although fashions began to be more functional and less restrictive for the succeeding Edwardians, appropriate dress for men and women—including that for the period of mourning—was still strictly prescribed and rigidly adhered to.\n\nThe rules were gradually relaxed, and it became acceptable practise for both sexes to dress in dark colours for up to a year after a death in the family. By the late 20th century, this no longer applied, and black had been widely adopted by women in cities as a fashionable colour.\n\nMourning generally followed English forms into the 20th century. Black dress is still considered proper etiquette for attendance at funerals, but extended periods of wearing black dress is no longer expected. However, attendance at social functions such as weddings when a family is in deep mourning is frowned upon. Men who share their father's given name and use a suffix such as \"Junior\" retain the suffix at least until the father's funeral is complete.\n\nIn the antebellum South, with social mores that imitated those of England, mourning was just as strictly observed by the upper classes.\n\nIn the 19th century, mourning could be quite expensive, as it required a whole new set of clothes and accessories or, at the very least, overdying existing garments and taking them out of daily use. For a poorer family, this was a strain on the resources.\n\nAt the end of \"The Wonderful Wizard of Oz\", Dorothy explains to Glinda that she must return home because her aunt and uncle cannot afford to go into mourning for her because it was too expensive.\n\nIn Tonga, family members of deceased persons wear black for an extended time, with large plain Taʻovala. Often, black bunting is hung from homes and buildings. In the case of the death of royalty, the entire country adopts mourning dress and black and purple bunting is displayed from most buildings.\n\nState mourning or, in the case of a monarchy, court mourning refers to displays of mourning behavior on the death of a public figure or member of a royal family.\n\nThe degree and duration of public mourning is generally decreed by a protocol officer. It was not unusual for the British court to declare that all citizens should wear full mourning for a specified period after the death of the monarch or that the members of the court should wear full- or half-mourning for an extended time. On the death of Queen Victoria (January 22, 1901), the \"Canada Gazette\" published an \"extra\" edition announcing that court mourning would continue until January 24, 1902. It directed the public to wear deep mourning until March 6, 1901 and half-mourning until April 17, 1901.\n\nThe black-and-white costumes designed by Cecil Beaton for the Royal Ascot sequence in \"My Fair Lady\" were inspired by the \"Black Ascot\" of 1910, when the court was in mourning for Victoria's son, Edward VII.\n\nAll over the world, states usually declare a period of 'official mourning' after the death of a Head of State. The signs may vary but usually include the lowering or posting half-mast of flags on public buildings. (In contrast, the Royal Standard of the United Kingdom is not flown at half-mast, because there is always a monarch on the throne.)\n\nIn January 2006, on the death of Jaber Al-Ahmad Al-Jaber Al-Sabah, the Emir of Kuwait, a mourning period of 40 days was declared. In Tonga, the official mourning lasts for a year; only afterwards is the royal heir crowned the new king.\n\nOn the other hand, the principle of continuity of the State must be respected. The principle is reflected in the French saying \"Le Roi est mort, vive le Roi!\" [The king is dead, long live the king!]. Regardless of the formalities of mourning, power must be handed on; if the succession is uncontested, that is best done immediately. Yet, a short interruption of work in the civil service may result from one or more days of closing the offices, especially on the day of the State funeral.\n\nThere are five grades of mourning obligations in the Confucian Code. A man is expected to honor most of those descended from his great-great-grandfather, and most of their wives. One's father (and mother) would merit 27 months. One's grandfather on the male side, as well as one's wife, would be grade two, or twelve months of austerities. A paternal uncle is grade three at nine months. Grade four is reserved for one's father's first cousin, maternal grandparents, siblings and sister's children (five months). First cousins once removed, second cousins and a man's wife's parents were to get grade five (three months).\n\nOrthodox Christians usually hold the funeral either the day after death or on the third day, and always during the daytime. In traditional Orthodox communities the body of the departed would be washed and prepared for burial by family or friends, and then placed in the coffin in the home. A house in mourning would be recognizable by the lid of the coffin, with a cross on it, and often adorned with flowers, set on the porch by the front door.\n\nSpecial prayers are held on the third, seventh or ninth (number varies in different national churches), and 40th days after death; the third, sixth and ninth or twelfth month; and annually thereafter in a memorial service, for up to three generations. Kolyva is ceremoniously used to honor the dead.\n\nSometimes men in mourning will not shave for the 40 days. Forty seems to have recurring pre-Judaic origins e.g. in the Rites of Persephone. In Greece and other Orthodox countries, it is not uncommon for widows to remain in mourning dress for the rest of their lives.\n\nWhen an Orthodox bishop dies, a successor is not elected until after the 40 days of mourning are completed, during which period his diocese is said to be \"widowed\".\n\nThe 40th day has great significance in Orthodox religion. That is the period during which soul of deceased wanders on earth. On the 40th day ascension of his soul occurs. Therefore, it's the most important day in mourning period, when special prayers should be held on grave site of deceased. \n\nAs in the Roman Catholic rites, there can be symbolic mourning. During Holy Week, some temples in the Church of Cyprus draw black curtains across the icons. The services of Good Friday and Holy Saturday morning are patterned in part on the Orthodox Christian burial service, and funeral lamentations.\n\nThe European social forms are, in general, forms of Christian religious expression transferred to the greater community.\n\nIn the Roman Catholic Church, the Mass of Paul VI, adopted in 1969, allows several options for the liturgical color used in Masses for the Dead. Before the liturgical reform, black was the ordinary color for funeral Masses; in the revised use, several options are available, though black is the norm. According to the General Instruction of the Roman Missal (§346d-e), black vestments is to be worn at Offices and Masses for the dead; an insult was given for several countries to use violet or white vestments, and in some of those nations those colours have largely supplanted the use of black vestments. \n\nChristian churches often go into mourning symbolically during the period of Lent to commemorate the sacrifice and death of Jesus. Customs vary among the denominations and include the covering or removal of statuary, icons and paintings, and use of special liturgical colors, such as violet/purple, during Lent and Holy Week.\n\nIn more formal congregations, parishioners also dress according to specific forms during Holy Week, particularly on Maundy Thursday and Good Friday, when it is common to wear black or sombre dress or, as mentioned, the liturgical color purple.\n\nDeath is not seen as the final \"end\", but is seen as a turning point in the seemingly endless journey of the indestructible \"atman\" or soul through innumerable bodies of animals and people. Hence, Hinduism prohibits excessive mourning or lamentation upon death, as this can hinder the passage of the departed soul towards its journey ahead: \"As mourners will not help the dead in this world, therefore (the relatives) should not weep, but perform the obsequies to the best of their power.\"\n\nHindu mourning is described in dharma shastras. It begins immediately after the cremation of the body and ends on the morning of the thirteenth day. Traditionally the body is cremated within 24 hours after death; however, cremations are not held after sunset or before sunrise. Immediately after the death, an oil lamp is lit near the deceased, and this lamp is kept burning for three days.\n\nHinduism associates death with ritual impurity for the immediate blood family of the deceased, hence during these mourning days, the immediate family must not perform any religious ceremonies (except funerals), must not visit temples or other sacred places, must not serve the sages (holy men), must not give alms, must not read or recite from the sacred scriptures, nor can they attend social functions such as marriages, parties, etc. The family of the deceased is not expected to serve any visiting guests food or drink. It is customary that the visiting guests do not eat or drink in the house where the death has occurred. The family in mourning are required to bathe twice a day, eat a single simple vegetarian meal, and try to cope with their loss.\n\nOn the day on which the death has occurred, the family do not cook; hence usually close family and friends will provide food for the mourning family. White clothing (the color of purity) is the color of mourning, and many will wear white during the mourning period.\n\nThe male members of the family do not cut their hair or shave, and the female members of the family do not wash their hair until the 10th day after the death. On the morning of the 10th day, all male members of the family shave and cut their hair, and female members wash their hair. This day is called \"Dasai\" or \"Daswan\". After \"Daswan\", some vedic rituals are started. If the deceased was young and unmarried, the \"Narayan Bali\" is performed by the Pandits. The Mantras of \"Bhairon Paath\" are recited. This ritual is performed through the person who has given the Mukhagni (Ritual of giving fire to the dead body).\n\nOn the morning of the 13th day, a Śrāddha ceremony is performed. The main ceremony involves a fire sacrifice, in which offerings are given to the ancestors and to gods, to ensure the deceased has a peaceful afterlife. Pind Sammelan is performed to ensure the involvement of the departed soul with that of God. Typically after the ceremony, the family cleans and washes all the idols in the family shrine; and flowers, fruits, water and purified food are offered to the gods. Then, the family is ready to break the period of mourning and return to daily life.\n\nIn Shi’ah Islam, examples of mourning practices are held annually in the month of Muharram, i.e. the first month of Islamic Lunar calendar. This mourning is held in the commemoration of Imam Al Husayn ibn Ali, who was martyred along with his 72 companions by Yazid bin Muawiyah. Shi’ah Muslims wear black clothes and take out processions on road to mourn on the tragedy of Karbala. \n\nMourning is observed in Islam by increased devotion, receiving visitors and condolences, and avoiding decorative clothing and jewelry.\n\nLoved ones and relatives are to observe a three-day mourning period. Widows observe an extended mourning period (Iddah), four months and ten days long, in accordance with the Qur'an 2:234. During this time, she is not to remarry, move from her home, or wear decorative clothing or jewelry.\n\nGrief at the death of a beloved person is normal, and weeping for the dead is allowed in Islam. What is prohibited is to express grief by wailing (\"bewailing\" refers to mourning in a loud voice), shrieking, tearing hair or clothes, breaking things, scratching faces, or uttering phrases that make a Muslim lose faith.\n\nThe Qur'an prohibits widows from engaging themselves for four lunar months and ten days after the death of their husbands. According to Qur'an:\nIslamic scholars consider this directive a balance between mourning a husband's death and protection of the widow from censure that she became interested in remarrying too soon after her husband’s death. This is also to ascertain whether or not she is pregnant.\n\nJudaism looks upon mourning as a process by which the stricken can re-enter into society, and so provides a series of customs that make this process gradual. The first stage, observed as all the stages are by immediate relatives (parents, spouse, siblings and children) is the Shiva (literally meaning seven), which consists of the first seven days after the funeral. The second stage is the Shloshim (thirty), referring to the thirty days following the death. The period of mourning after the death of a parent lasts one year. Each stage places lighter demands and restrictions than the previous one in order to reintegrate the bereaved into normal life.\n\nThe most known and central stage is Shiva, which is a Jewish mourning practice in which people adjust their behaviour as an expression of their bereavement for the week immediately after the burial. In the West, typically, mirrors are covered and a small tear is made in an item of clothing to indicate a lack of interest in personal vanity. The bereaved dress simply and sit on the floor, short stools or boxes rather than chairs when receiving the condolences of visitors. In some cases relatives or friends take care of the bereaved's house chores, as cooking and cleaning. English speakers use the expression \"to sit shiva\".\n\nDuring the Shloshim the mourners are no longer expected to sit on the floor or be taken care of (cooking/cleaning). However, some customs still apply. There is a prohibition on getting married or attending any sort of celebrations and men refrain from shaving or cutting their hair.\n\nRestrictions during the year of mourning include not wearing new clothes, not listening to music and not attending celebrations. In addition, the sons of the deceased recite the Kaddish prayer for the first eleven months of the year.\n\n\n"}
{"id": "42402138", "url": "https://en.wikipedia.org/wiki?curid=42402138", "title": "Narragansett Runestone", "text": "Narragansett Runestone\n\nThe Narragansett Runestone, also known as the Quidnessett Rock, is a 2.5 tonne slab of metasandstone located in Rhode Island, United States. It is inscribed with two rows of symbols, which some have indicated resemble ancient Runic characters.\n\nThe stone was stolen in 2012. On April 26, 2013, the Rhode Island Attorney General announced that the rock was recovered after an individual came forward with information. The rock was moved to the University of Rhode Island School of Oceanography for testing, but this proved to be impossible.\n\nIn January 2014, plans were announced to move the runestone to Goddard Memorial State Park in East Greenwich. In October 2015, the runestone was placed for long-term public viewing in Wickford, a village of North Kingstown Rhode Island.\n\nThe Narragansett runestone was first reported to the Rhode Island Historical Preservation and Heritage Commission (HPHC) in the 1980s. The New England Antiquities Research Association (NEARA) ran several studies and published a number of papers in the 1980s and 1990s about the rock. According to NEARA, the stone was discovered by a quahogger in December 1984 while digging in the mud flats of Narragansett Bay.\n\nThe HPHC was unable to find any information about the stone in any previous inventories of Narragansett Bay. They found that as early as 1939, the runestone was located upland and may have been buried. Recently, the inscriptions on the stone were visible only for a short period of time between the shifting tides, due to dramatic erosion of the shoreline at Pojac Point and the fact that the stone was positioned 20 feet from the extreme low tide line mark.\n\nIn 2014, Everett Brown of Providence reported that he and his brother Warren had carved the runes on Quidnessett Rock in summer 1964. He said that he had forgotten about the incident until the stone was removed and recovered in 2013. His account has been disputed by other local people, who state that they saw the stone before 1964, and have challenged other elements of his statements.\n\nThe stone is referenced in episode 11 of season 1 of \"America Unearthed.\"\n\nThe state Coastal Resources Management Council reported that the runestone had been removed from the tidal waters off Pojac Point between July and August 2012. In May 2013, the state Attorney General’s Environmental Unit and DEM’s Criminal Investigation Unit announced that they had recovered the stone.\n\nTown historian and independent columnist G. Timothy Cranston said that a Pojac Point resident had removed the stone, as he was tired of having tourists scouring the neighborhood and shoreline looking for the stone. He said that the resident – who was not named – was ordered by state officials to retrieve the stone after having sunk it in deeper waters off the coast. After the stone was recovered, in October 2015 it was placed for long-term public viewing in Wickford, a village of the Town of North Kingstown, Rhode Island.\n\n"}
{"id": "203959", "url": "https://en.wikipedia.org/wiki?curid=203959", "title": "New Zealand Wars", "text": "New Zealand Wars\n\nThe New Zealand Wars were a series of armed conflicts that took place in New Zealand from 1845 to 1872 between the New Zealand government and the Māori. Until at least the 1980s, European New Zealanders referred to them as the Māori wars; the historian James Belich was one of the first to refer to them as the \"New Zealand wars\", in his 1987 book \"The New Zealand wars and the Victorian interpretation of racial conflict.\"\n\nThough the wars were initially localised conflicts triggered by tensions over disputed land purchases, they escalated dramatically from 1860 as the government became convinced it was facing united Māori resistance to further land sales and a refusal to acknowledge Crown sovereignty. The colonial government summoned thousands of British troops to mount major campaigns to overpower the Kīngitanga (Māori King) movement and also acquire farming and residential land for British settlers. Later campaigns were aimed at quashing the so-called Hauhau movement, an extremist part of the Pai Mārire religion, which was strongly opposed to the alienation of Māori land and eager to strengthen Māori identity.\n\nAt the peak of hostilities in the 1860s, 18,000 British troops, supported by artillery, cavalry and local militia, battled about 4,000 Māori warriors in what became a gross imbalance of manpower and weaponry. Although outnumbered, the Māori were able to withstand their enemy with techniques that included anti-artillery bunkers and the use of carefully placed pā, or fortified villages, that allowed them to block their enemy's advance and often inflict heavy losses, yet quickly abandon their positions without significant loss. Guerilla-style tactics were used by both sides in later campaigns, often fought in dense bush. Over the course of the Taranaki and Waikato campaigns, the lives of about 1,800 Māori and 800 Europeans were lost, and total Māori losses over the course of all the wars may have exceeded 2,100.\n\nViolence over land ownership broke out first in the Wairau Valley in the South Island in June 1843, but rising tensions in Taranaki eventually led to the involvement of British military forces at Waitara in March 1860. The war between the government and Kīngitanga Māori spread to other areas of the North Island, with the biggest single campaign being the invasion of the Waikato in 1863–1864, before hostilities concluded with the pursuits of warlord Riwha Tītokowaru in Taranaki (1868–1869) and guerrilla fighter Te Kooti Arikirangi Te Turuki on the east coast (1868–1872).\n\nAlthough Māori were initially fought by British forces, the New Zealand government developed its own military force, including local militia, rifle volunteer groups, the specialist Forest Rangers and kūpapa (pro-government Māori). The government also responded with legislation to imprison Māori opponents and confiscate expansive areas of the North Island for sale to settlers, with the funds used to cover war expenses—punitive measures that on the east and west coasts provoked an intensification of Māori resistance and aggression.\n\nThe 1840 Treaty of Waitangi guaranteed that individual Māori \"iwi\" (tribes) should have undisturbed possession of their lands, forests, fisheries and other \"taonga\" (treasures) in return for becoming British subjects, selling land to the government only and surrendering sovereignty to the British Government. Historians, however, have debated whether Māori signatories fully understood this last point, due to the possible mistranslation of the word \"sovereignty\" in the treaty copies. The majority of Māori wanted to sign in order to consolidate peace and in hopes of ending the long intertribal Musket Wars (1807–1842). They also wished to acquire the technological culture of the British.\n\nAll pre-treaty colonial land-sale deals had taken place directly between two parties. In the early period of contact, Māori had generally sought trade with Europeans. The British and the French had established mission stations, and missionaries had received land from iwi for houses, schools, churches, and farms.\n\nTraders, Sydney businessmen and the New Zealand Company had bought large tracts of land before 1840, and the British government at Westminster became concerned about protecting Māori from exploitation. As part of the Treaty of Waitangi, colonial authorities decreed that Māori could sell land only to the Crown (the Right of Pre-emption). But as the New Zealand colonial government, pressured by immigrant European settlers, tried to speed up land sales to provide farmland, it met resistance from the Kīngitanga (Māori King) movement that emerged in the 1850s and opposed further European encroachment.\n\nGovernor Thomas Gore Browne's provocative purchase of a disputed block of land at Waitara in 1859 set the government on a collision course with the Kīngitanga movement, and the government interpreted the Kīngitanga response as a challenge to the Crown's authority. Governor Gore Browne succeeded in bringing 3500 Imperial troops from the Australian colonies to quash this perceived challenge, and within four years a total of 9,000 British troops had arrived in New Zealand, assisted by more than 4,000 colonial and \"kūpapa\" (pro-government Māori) fighters as the government sought a decisive victory over the \"rebel\" Māori.\n\nThe use of a punitive land confiscation policy from 1865, depriving \"rebel\" Māori of the means of living, fuelled further Māori anger and resentment, fanning the flames of conflict in Taranaki (1863–1866) and on the east coast (1865–1866).\n\nThe various conflicts of the New Zealand wars span a considerable period, and the causes and outcomes differ widely. The earliest conflicts in the 1840s happened at a time when Māori were still the predominant power, but by the 1860s settler numbers and resources were much greater. From about 1862 British troops began arriving in much greater number, summoned by Governor George Grey for his Waikato invasion, and in March 1864 total troop numbers peaked at about 14,000 (9,000 Imperial troops, more than 4,000 colonial and a few hundred \"kūpapa\").\n\nThe first armed conflict between Māori and the European settlers took place on 17 June 1843 in the Wairau Valley, in the north of the South Island. The clash was sparked when settlers led by a representative of the New Zealand Company—which held a false title deed to a block of land—attempted to clear Māori off the land ready for surveying. The party also attempted to arrest Ngāti Toa chiefs Te Rauparaha and Te Rangihaeata. Fighting broke out and 22 Europeans were killed, as well as four to six Māori. Several Europeans were slain after being captured. In early 1844, the new Governor, Robert FitzRoy, investigated the incident and declared the settlers were at fault. The Wairau Affray—described as the Wairau Massacre in early texts—was the only armed conflict of the New Zealand Wars to take place in the South Island.\n\nThe Flagstaff War took place in the far north of New Zealand, around the Bay of Islands, between March 1845 and January 1846. In 1845 George Grey arrived in New Zealand to take up his appointment as governor. At this time Hōne Heke challenged the authority of the British, beginning by cutting down the flagstaff on Flagstaff Hill at Kororāreka. The flagstaff had previously flown the colours of United Tribes of New Zealand but now carried the Union Jack and therefore symbolised the grievances of Heke and his ally Te Ruki Kawiti, as to changes that had followed the signing of the Treaty of Waitangi.\n\nThere were many causes of the Flagstaff War and Heke had a number of grievances in relation to the Treaty of Waitangi. While land acquisition by the Church Missionary Society (CMS) had been controversial, the rebellion led by Heke was directed against the colonial forces with the CMS missionaries trying to persuade Heke to end the fighting. Despite the fact that Tāmati Wāka Nene and most of Ngāpuhi sided with the government, the small and ineptly led British had been beaten at Battle of Ohaeawai. Grey, armed with the financial support and far more troops armed with 32-pounder cannons that had been denied to FitzRoy, attacked and occupied Kawiti's fortress at Ruapekapeka, forcing Kawiti to retreat. Heke's confidence waned after he was wounded in battle with Tāmati Wāka Nene and his warriors, and by the realisation that the British had far more resources than he could muster, including some Pākehā Māori, who supported the colonial forces.\n\nAfter the Battle of Ruapekapeka, Heke and Kawiti were ready for peace. It was Tāmati Wāka Nene they approached to act as the intermediary to negotiate with Governor Grey, who accepted the advice of Nene that Heke and Kawiti should not be punished for their rebellion. The fighting in the north ended and there was no punitive confiscation of Ngāpuhi land.\n\nThe Hutt Valley campaign of 1846 came as a sequel to the Wairau Affray. The causes were similar—dubious land purchases by the New Zealand Company and the desire of the settlers to move on to land before disputes over titles were resolved—and the two conflicts shared many of the same protagonists. The campaign's most notable clashes were the Māori dawn raid on an imperial stockade at Boulcott's Farm on 16 May 1846 in which eight British soldiers and an estimated two Māori died, and the Battle of Battle Hill from 6–13 August as British troops, local militia and \"kūpapa\" pursued a Ngāti Toa force led by chief Te Rangihaeata through steep and dense bushland. Ngāti Toa chief Te Rauparaha was also taken into custody during the campaign; he was detained without charge in Auckland for two years.\n\nThe bloodshed heightened settlers' fears in nearby Wanganui, which was given a strong military force to guard against attack. In April 1847 an accidental shooting of a minor Wanganui Māori chief led to a bloody revenge attack on a settler family; when the perpetrators were captured and hanged, a major raid was launched on the town as a reprisal, with homes plundered and burned and livestock stolen. Māori besieged the town before mounting a frontal attack in July 1847. A peace settlement was reached in early 1848.\n\nThe catalyst for the First Taranaki War was the disputed sale to the Crown of a 240 hectare block of land at Waitara, despite a veto by the paramount chief of Te Āti Awa tribe, Wiremu Kīngi, and a \"solemn contract\" by local Māori not to sell. Governor Browne accepted the purchase with full knowledge of the circumstances and tried to occupy the land, anticipating it would lead to armed conflict, and a demonstration of the substantive sovereignty the British believed they had gained in the 1840 Treaty of Waitangi. Hostilities began on 17 March 1860. The war was fought by more than 3,500 imperial troops brought in from Australia, as well as volunteer soldiers and militia, against Māori forces that fluctuated between a few hundred and about 1,500. After a series of battles and actions the war ended in a ceasefire, with neither side explicitly accepting the peace terms of the other. Total losses among the imperial, volunteer and militia troops are estimated to have been 238, while Māori casualties totalled about 200. Though there were claims by the British that they had won the war, there were widely held views at the time they had suffered an unfavourable and humiliating result. Historians have also been divided on the result. Historian James Belich has claimed that Māori succeeded in thwarting the British bid to impose sovereignty over them, and had therefore been victorious. However, he also states that the Māori victory was a hollow one, leading to the invasion of the Waikato.\n\n Governor Thomas Gore-Browne began making arrangements for a Waikato campaign to destroy the \"Kīngitanga\" stronghold at the close of the First Taranaki War. Preparations were suspended in December 1861 when he was replaced by Sir George Grey, but Grey revived plans for an invasion in June 1863. He persuaded the Colonial Office in London to send more than 10,000 Imperial troops to New Zealand and General Sir Duncan Cameron was appointed to lead the campaign. Cameron used soldiers to build the 18 km-long Great South Road to the border of Kīngitanga territory and on 9 July 1863 Grey ordered all Māori living between Auckland and the Waikato take an oath of allegiance to Queen Victoria or be expelled south of the Waikato River; when his ultimatum was rejected the vanguard of the army crossed the frontier into Kīngitanga territory and established a forward camp. A long series of bush raids on his supply lines forced Cameron to build an extensive network of forts and redoubts through the area. In a continual buildup of force, Cameron eventually had 14,000 British and colonial soldiers at his disposal as well as steamers and armoured vessels for use on the Waikato River. They fought a combined Māori contingent of about 4,000.\n\nCameron and his Kīngitanga foe engaged in several major battles including the Battle of Rangiriri and a three-day siege at Orakau, capturing the Kīngitanga capital of Ngaruawahia in December 1863, before completing their Waikato conquest in April 1864. The Waikato campaign cost the lives of 700 British and colonial soldiers and about 1,000 Māori.\n\nThe \"Kīngitanga\" Māori retreated into the rugged interior of the North Island and in 1865 the New Zealand Government confiscated about 12,000 km of Māori land (4% of New Zealand's land area) for white settlement—an action that quickly provoked the Second Taranaki War.\n\nBetween 1863 and 1866 there was a resumption of hostilities between Māori and the New Zealand Government in Taranaki, which is sometimes referred to as the Second Taranaki War. The conflict, which overlapped the wars in Waikato and Tauranga, was fuelled by a combination of factors: lingering Māori resentment over the sale of land at Waitara in 1860 and government delays in resolving the issue; a large-scale land confiscation policy launched by the government in late 1863; and the rise of the so-called Hauhau movement, an extremist part of the Pai Marire syncretic religion, which was strongly opposed to the alienation of Māori land and eager to strengthen Māori identity. The Hauhau movement became a unifying factor for Taranaki Māori in the absence of individual Māori commanders.\n\nThe style of warfare after 1863 differed markedly from that of the 1860–1861 conflict, in which Māori had taken set positions and challenged the army to an open contest. From 1863 the army, working with greater numbers of troops and heavy artillery, systematically took possession of Māori land by driving off the inhabitants, adopting a \"scorched earth\" strategy of laying waste to Māori villages and cultivations, with attacks on villages, whether warlike or otherwise. Historian Brian Dalton noted: \"The aim was no longer to conquer territory, but to inflict the utmost 'punishment' on the enemy; inevitably there was a great deal of brutality, much burning of undefended villages and indiscriminate looting, in which loyal Maoris often suffered.\" As the troops advanced, the Government built an expanding line of redoubts, behind which settlers built homes and developed farms. The effect was a creeping confiscation of almost of land, with little distinction between the land of loyal or rebel Māori owners. The outcome of the armed conflict in Taranaki between 1860 and 1869 was a series of enforced confiscations of Taranaki tribal land from Māori blanketed as being in rebellion against the Government.\n\nEast coast hostilities erupted in April 1865 and, as in the Second Taranaki War, sprang from Māori resentment of punitive government land confiscations coupled with the embrace of radical Pai Marire expression. The religion arrived on the east coast from Taranaki in early 1865. The subsequent ritual killing of missionary Carl Volkner by Pai Mārire (or Hauhau) followers at Opotiki on 2 March 1865 sparked settler fears of an outbreak of violence and later that year the New Zealand government launched a lengthy expedition to hunt for Volkner's killers and neutralise the movement's influence. Rising tensions between Pai Mārire followers and conservative Māori led to a number of wars between and within Māori \"iwi\", with \"kūpapa\" armed by the government in a bid to exterminate the movement.\n\nMajor conflicts within the campaign included the cavalry and artillery attack on Te Tarata pā near Opotiki in October 1865 in which about 35 Māori were killed, and the seven-day siege of Waerenga-a-Hika in November 1865. The government confiscated northern parts of Urewera land in January 1866 in a bid to break down supposed Māori support for Volkner's killers and confiscated additional land in Hawke's Bay a year later after a rout of a Māori party it deemed a threat to the settlement of Napier.\n\nWar flared again in Taranaki in June 1868 as Riwha Titokowaru, chief of the Ngāti Ruanui's Ngaruahine \"hapu\" (sub-tribe), responded to the continued surveying and settlement of confiscated land with well-planned and effective attacks on settlers and government troops in an effort to block the occupation of Māori land. Coinciding with a violent raid on a European settlement on the East Coast by Te Kooti, the attacks shattered what European colonists regarded as a new era of peace and prosperity, creating fears of a \"general uprising of hostile Māoris\".\n\nTitokowaru, who had fought in the Second Taranaki War, was the most skilful West Coast Māori warrior. He also assumed the roles of a priest and prophet of the extremist Hauhau movement of the Pai Mārire religion, reviving ancient rites of cannibalism and propitiation of Māori gods with the human heart torn from the first slain in a battle. Although Titokowaru's forces were numerically small and initially outnumbered in battle 12 to one by government troops, the ferocity of their attacks provoked fear among settlers and prompted the resignation and desertion of many militia volunteers, ultimately leading to the withdrawal of most government military forces from South Taranaki and giving Titokowaru control of almost all territory between New Plymouth and Wanganui. Although Titokowaru provided the strategy and leadership that had been missing among tribes that had fought in the Second Taranaki War and his forces never lost a battle during their intensive campaign, they mysteriously abandoned a strong position at Tauranga-ika Pā and Titokowaru's army immediately began to disperse. Kimble Bent, who lived as a slave with Titokowaru's \"hapu\" after deserting from the 57th Regiment, told Cowan 50 years later the chief had lost his \"mana tapu\", or sacred power, after committing adultery with the wife of another chief.\n\nOnce Titokowaru was defeated and the East Coast threat minimised, the alienation of Māori land, as well as the political subjugation of Māori, continued at an even more rapid pace.\n\nTe Kooti's War was fought in the East Coast region and across the heavily forested central North Island and Bay of Plenty between government military forces and followers of spiritual leader Te Kooti Arikirangi Te Turuki. The conflict was sparked by Te Kooti's return to New Zealand after two years of internment on the Chatham Islands, from where he had escaped with almost 200 Māori prisoners of war and their families. Te Kooti, who had been held without trial on the island for two years, asked that he and his followers be left in peace, but within two weeks they were being pursued by a force of militia, government troops and Māori volunteers. The pursuit turned into a four-year guerrilla war, involving more than 30 expeditions by colonial and Māori troops against Te Kooti's dwindling number of warriors.\n\nAlthough initially fighting defensively against pursuing government forces, Te Kooti went on the offensive from November 1868, starting with the so-called Poverty Bay massacre, a well-organised lightning strike against selected European settlers and Māori opponents in the Matawhero district, in which 51 men, women and children were slaughtered and their homes set alight. The attack prompted another vigorous pursuit by government forces, which included a siege at Ngatapa pā that came to a bloody end: although Te Kooti escaped the siege, Māori forces loyal to the government caught and executed more than 130 of his supporters, as well as prisoners he had earlier seized. Dissatisfied with the Māori King Movement's reluctance to continue its fight against European invasion and confiscation, Te Kooti offered Māori an Old Testament vision of salvation from oppression and a return to a promised land. Wounded three times in battle, he gained a reputation for being immune to death and uttered prophecies that had the appearance of being fulfilled. In early 1870 Te Kooti gained refuge from Tūhoe tribes, which consequently suffered a series of damaging raids in which crops and villages were destroyed, after other Māori \"iwi\" were lured by the promise of a ₤5,000 reward for Te Kooti's capture. Te Kooti was finally granted sanctuary by the Māori king in 1872 and moved to the King Country, where he continued to develop rituals, texts and prayers of his Ringatū faith. He was formally pardoned by the government in February 1883 and died in 1893.\n\nA 2013 Waitangi Tribunal report said the action of Crown forces on the East Coast from 1865 to 1869—the East Coast Wars and the start of Te Kooti's War—resulted in the deaths of proportionately more Māori than in any other district during the New Zealand wars. It condemned the \"illegal imprisonment\" on the Chatham Islands of a quarter of the East Coast region's adult male population and said the loss in war of an estimated 43 percent of the male population, many through acts of \"lawless brutality\", was a stain on New Zealand's history and character.\n\nThe New Zealand campaigns involved Māori warriors from a range of \"iwi\", most of which were allied with the Kīngitanga movement, fighting a mix of Imperial troops, local militia groups, the specialist Forest Rangers and kūpapa, or \"loyalist\" Māori.\n\nIn 1855 just 1,250 Imperial troops, from two under-strength British regiments, were in New Zealand. Although both were scheduled to depart at the end of the year, Browne succeeded in retaining one of them for use in New Plymouth, where settlers feared the spread of intertribal violence. At the outbreak of Taranaki hostilities in 1860, reinforcements were brought from Auckland to boost the New Plymouth garrison, raising the total force of regulars to 450 and for many months the total number of Māori under arms exceeded the number of troops in Taranaki. In mid-April the arrival of three warships and about 400 soldiers from Australia marked the beginning of the escalation of imperial troop numbers.\n\nThe buildup increased rapidly under Grey's term as governor: when the second round of hostilities broke out in Taranaki in May 1863 he applied to the Secretary of State in London for the immediate dispatch of three more regiments and also wrote to the Australian governors asking for whatever British troops that could be made available. Lieutenant-General Duncan Cameron, the Commander-in-Chief of the British troops in New Zealand, began the Waikato invasion in July with fewer than 4,000 effective troops in Auckland at his disposal, but the continuous arrival of regiments from overseas rapidly swelled the force.\n\nThe Colonial Defence Force, a cavalry unit of about 100 men, was formed by Colonel Marmaduke Nixon in May 1863 and served in Waikato and militia forces were also used throughout the New Zealand wars. The Militia Ordinance 1845 provided for the compulsory training or service within 40 km of their town by all able-bodied European men aged between 18 and 60; the Auckland Militia and Volunteers reached a peak of about 1650 on active service in the early stages of the Waikato campaign; and the last force—the Taranaki Militia—was released from service in 1872.\n\nA special 65-man bush-scouring corps, the Forest Rangers, composed of local farmers who were familiar with the bush, had proven guerrilla techniques and were capable of \"roughing it\", was formed in August 1863; the Forest Rangers split into two separate companies in November, with the second led by Gustavus von Tempsky and both served in Waikato and Taranaki. Other rangers corps during the New Zealand wars included the Taranaki Bush Rangers, Patea Rangers, Opotiki Volunteer Rangers, Wanganui Bush Rangers and Wellington Rangers. From September 1863 the first contingents of what was planned as 5,000 military settlers—recruited on the goldfields of Australia and Otago with promises of free grants of land confiscated from \"rebel\" Māori—also began service in the Waikato. By the end of October the number of military settlers, known as the Waikato Militia, had reached more than 2,600 and total troop numbers peaked at about 14,000 in March 1864 (9,000 Imperial troops, more than 4,000 colonial and a few hundred \"kūpapa\").\n\nIn November 1864 Premier Frederick Weld introduced a policy of \"self-reliance\" for New Zealand, which included the gradual but complete withdrawal of Imperial troops, who would be replaced by a colonial force of 1,500. The move came at a time of rising conflict between Grey, who sought more extensive military operations to \"pacify\" the west coast of the North Island between Taranaki and Wanganui, and Cameron, who regarded such a campaign as unnecessary, impractical and contrary to Imperial policy. Grey blocked Cameron's attempts to dispatch the first regiments from New Zealand in May 1865 and the first regiment finally embarked in January 1866. By May 1867 only the 2/18th Regiment remained in the country, their departure delayed by political pressure over the \"peril\" still facing settlers; the last soldiers finally left in February 1870.\n\nAbout 15 of the 26 major North Island tribal groups sent contingents to join the Waikato campaign, although sometimes they represented a single \"hapu\", or clan, within the tribe. Continual presence on battlefields remained difficult for most, however, because of the constant need for tribal labour in their home community, so there was a constant turnover of small tribal groups. At Meremere, Paterangi, Hangatiki and Maungatatauri, between August 1863 and June 1864 Māori maintained forces of between 1,000 and 2,000 men, but troops were forced to disperse after each campaign because of labour and domestic needs at home. Belich has estimated that the total Māori mobilisation was at least 4,000 warriors, representing one-third of the total manpower available.\n\nAlthough they were not part of a structured command system, Māori generally followed a consistent strategic plan, uniting to build skilfully engineered defensive lines up to long. Māori united under proven military commanders including Rewi Maniapoto and Tikaokao of Ngāti Maniapoto and Wiremu Tamihana of Ngāti Hauā.\n\nCampaigners on both sides of the New Zealand wars had developed distinctive war strategies and tactics. The British set out to fight a European-style war, based on engaging with the opposing forces, besieging and then capturing fortified positions. The British Army were professional soldiers who had experience fighting in various parts of the Empire, many from India and Afghanistan, and were led by officers who were themselves trained by men who had fought at Waterloo.\n\nMany of the Māori fighters had been raised during the Musket Wars, the decades-long bitter intertribal fighting during which warriors had perfected the art of building defensive fortifications around a \"pā\". During the Flagstaff War Kawiti and Heke appear to have followed a strategy of drawing the colonial forces into attacking a fortified pā, from which the warriors could fight from a strong defensive position that was secure from cannon fire.\n\nThe word \"pā\" means a fortified strong point near a Māori village or community. They were built with a view to defence, but primarily they were built to safely store food. Puketapu Pā and then Ohaeawai Pā were the first of the so-called \"gunfighter \"pā\"\", built to engage enemies armed with muskets and cannons. A strong, wooden palisade was fronted with woven flax leaves (\"Phormium tenax\") whose tough, stringy foliage absorbed much of the force of the ammunition. The palisade was lifted a few centimetres from the ground so muskets could be fired from underneath rather than over the top. Sometimes there were gaps in the palisade, which led to killing traps. There were trenches and rifle pits to protect the occupants and, later, very effective artillery shelters. They were usually built so that they were almost impossible to surround completely, but usually presented at least one exposed face to invite attack from that direction. They were cheap and easily built—the L-Pa at Waitara was constructed by 80 men overnight—and they were completely expendable. The British repeatedly mounted often lengthy expeditions to besiege a \"pā\", which would absorb their bombardment and possibly one or two attacks, and then be abandoned by the Māori. Shortly afterwards, a new \"pā\" would appear in another inaccessible site. \"Pā\" like these were built in the dozens, particularly during the First Taranaki War, where they eventually formed a cordon surrounding New Plymouth, and in the Waikato campaign.\nFor a long time, the modern \"pā\" effectively neutralised the overwhelming disparity in numbers and armaments. At Ohaeawai Pā in 1845, at Rangiriri in 1863 and again at Gate Pā in 1864, British and colonial forces discovered that frontal attacks on a defended \"pā\" were extremely costly. At Gate Pā, during the 1864 Tauranga Campaign, Māori withstood a day-long bombardment in their underground shelters and trenches. The palisade destroyed, the British troops rushed the \"pā\" whereupon Māori fired on them from hidden trenches, killing 38 and injuring many more in the most costly battle for the Pākehā of the New Zealand Wars. The troops retired and Māori abandoned the \"pā\".\n\nBritish troops soon realised an easy way to neutralise a \"pā\". Although cheap and easy to build, a gunfighter \"pā\" required a significant input of labour and resources. The destruction of the Māori economic base in the area around the \"pā\" made it difficult for the \"hapu\" to support the fighting men. This was the reasoning behind the bush-scouring expeditions of Chute and McDonnell in the Second Taranaki War.\n\nThe biggest problem for the Māori was that their society was ill-adapted to support a sustained campaign. A long campaign would disrupt food supplies and epidemics resulted in significant numbers of deaths among the Māori. While the British could defeat Māori in battle, the defeats were often not decisive. For example, the capture of Ruapekapeka Pā can be considered a British tactical victory, but it was purpose-built as a target for the British, and its loss was not damaging; Heke and Kawiti managed to escape with their forces intact. However the British force consisted of professional soldiers supported by an economic system capable of sustaining them in the field almost indefinitely, in contrast the Māori warrior was a part-time fighter who also needed to work on producing food.\n\nThe main weapon used by the British forces in the 1860s was the Pattern 1853 Enfield. Properly described as a rifled musket, it was loaded down the barrel like a conventional musket but the barrel was rifled. While muskets were accurate to about 60–80 m, an 1853 Enfield was accurate to about 300 m to 400 m in the hands of an experienced soldier; at 100 m an experienced soldier could easily hit a human target. The rifle was 1.44 m long, weighed 4 kg and had a 53 cm socket bayonet. This rifle was also commonly used in the American Civil War by both sides.\n\nThe Calisher and Terry carbine (short rifle) was ordered by the New Zealand Government from Calisher and Terry, Birmingham gunsmiths in 1861 after earlier fighting against Māori showed the need for a carbine suited to fighting in heavy bush. This was the favoured weapon of the New Zealand Forest Rangers because of its shortness, its lightness, and its ability to be reloaded while the marksman lay down—unlike the Enfield, which required the soldier to stand to load the powder—and could be loaded on the run. This feature led to a decisive victory for the New Zealand forces at Orakau: several groups of soldiers harried the fleeing Māori but only the Forest Rangers, equipped with carbines were able to follow them 10 km to the Puniu River shooting as they went.\n\nRevolvers were mainly used by officers, but were a general issue for the Forest Rangers. The most common revolver appears to have been the five-shot Beaumont–Adams .44 percussion revolver. Other revolvers in use were the Colt Navy .36 1851 model with open top frame. The Colt was favoured by the Forest Rangers because it was light and accurate being a single-action revolver. Von Tempsky's second company of the Forest Rangers also used the Bowie knife.\n\nLarge areas of land were confiscated from the Māori by the government under the New Zealand Settlements Act in 1863, purportedly as punishment for rebellion. In reality, land was confiscated from both \"loyal\" and \"rebel\" tribes alike. More than of land was confiscated. Although about half of this was subsequently paid for or returned to Māori control, it was often not returned to its original owners. The confiscations had a lasting impact on the social and economic development of the affected tribes.\n\nThe legacy of the New Zealand Wars continues, but these days the battles are mostly fought in courtrooms and around the negotiation table. Numerous reports by the Waitangi Tribunal have criticised Crown actions during the wars, and also found that the Māori, too, had breached the treaty. As part of the negotiated out-of-court settlements of tribes' historical claims (Treaty of Waitangi claims and settlements), as of 2011 the Crown is making formal apologies to tribes.\n\n\n\n\n"}
{"id": "12040469", "url": "https://en.wikipedia.org/wiki?curid=12040469", "title": "Nuclear weapons and Israel", "text": "Nuclear weapons and Israel\n\nThe State of Israel is widely believed to possess nuclear weapons. Current estimates put the size of the Israeli nuclear arsenal at between 80 and 400 nuclear warheads, and the country is believed to possess the ability to deliver them in a variety of methods including: aircraft; submarine-launched cruise missiles; and the Jericho series of intermediate to intercontinental range ballistic missiles. Its first deliverable nuclear weapon is thought to have been created in late 1966 or early 1967; which would make it the sixth country in the world to have developed them.\n\nHowever, Israel maintains a policy of deliberate ambiguity, never officially denying nor admitting to having nuclear weapons, instead repeating over the years that \"Israel will not be the first country to introduce nuclear weapons to the Middle East\". Israel has also declined to sign the Treaty on the Non-Proliferation of Nuclear Weapons (NPT) despite international pressure to do so, saying that would be contrary to its national security interests. \n\nAdditionally, Israel developed the Begin Doctrine of counter-proliferation and preventive strikes, denying other regional actors the ability to acquire their own nuclear weapons. The Israeli Air Force conducted Operation Opera and Operation Orchard, destroying the Iraqi and \nSyrian nuclear reactors in 1981 and 2007, respectively, and the Stuxnet malware that severely damaged Iranian nuclear facilities in 2010 is widely thought to have been developed by Israel. As of 2018, Israel remains the only country in the Middle East believed to possess nuclear weapons. The Samson Option refers to Israel's deterrence strategy of massive retaliation with nuclear weapons as a \"last resort\" against a country whose military has invaded and/or destroyed much of Israel.\n\nIsrael started investigating the nuclear field soon after it declared independence in 1948, and with French co-operation secretly began building the Shimon Peres Negev Nuclear Research Center, a facility near Dimona housing a nuclear reactor and reprocessing plant in the late 1950s. The first extensive details of the weapons program came in October 5, 1986, with news coverage of information provided by Mordechai Vanunu, a technician formerly employed at the center. Vanunu was later captured by the Mossad and brought back to Israel, where he was sentenced to 18 years in prison for treason and espionage.\n\nIsrael's first Prime Minister David Ben-Gurion was \"nearly obsessed\" with obtaining nuclear weapons to prevent the Holocaust from recurring. He stated, \"What Einstein, Oppenheimer, and Teller, the three of them are Jews, made for the United States, could also be done by scientists in Israel, for their own people\". Ben-Gurion decided to recruit Jewish scientists from abroad even before the end of the 1948 Arab–Israeli War that established Israel's independence. He and others, such as head of the Weizmann Institute of Science and defense ministry scientist Ernst David Bergmann, believed and hoped that Jewish scientists such as Oppenheimer and Teller would help Israel.\n\nIn 1949 a unit of the Israel Defense Forces Science Corps, known by the Hebrew acronym HEMED GIMMEL, began a two-year geological survey of the Negev. While a preliminary study was initially prompted by rumors of petroleum fields, one objective of the longer two year survey was to find sources of uranium; some small recoverable amounts were found in phosphate deposits. That year Hemed Gimmel funded six Israeli physics graduate students to study overseas, including one to go to the University of Chicago and study under Enrico Fermi, who had overseen the world's first artificial and self-sustaining nuclear chain reaction. In early 1952 Hemed Gimmel was moved from the IDF to the Ministry of Defense and was reorganized as the Division of Research and Infrastructure (EMET). That June, Bergmann was appointed by Ben-Gurion to be the first chairman of the Israel Atomic Energy Commission (IAEC).\n\nHemed Gimmel was renamed Machon 4 during the transfer, and was used by Bergmann as the \"chief laboratory\" of the IAEC; by 1953, Machon 4, working with the Department of Isotope Research at the Weizmann Institute, developed the capability to extract uranium from the phosphate in the Negev and a new technique to produce indigenous heavy water. The techniques were two years more advanced than American efforts. Bergmann, who was interested in increasing nuclear cooperation with the French, sold both patents to the Commissariat à l'énergie atomique (CEA) for 60 million francs. Although they were never commercialized, it was a consequential step for future French-Israeli cooperation. In addition, Israeli scientists probably helped construct the G-1 plutonium production reactor and UP-1 reprocessing plant at Marcoule. France and Israel had close relations in many areas. France was principal arms supplier for the young Jewish state, and as instability spread through French colonies in North Africa, Israel provided valuable intelligence obtained from contacts with Sephardi Jews in those countries. At the same time Israeli scientists were also observing France's own nuclear program, and were the only foreign scientists allowed to roam \"at will\" at the nuclear facility at Marcoule. In addition to the relationships between Israeli and French Jewish and non-Jewish researchers, the French believed that cooperation with Israel could give them access to international Jewish nuclear scientists.\n\nAfter U.S. President Dwight Eisenhower announced the Atoms for Peace initiative, Israel became the second country to sign on (following Turkey), and signed a peaceful nuclear cooperation agreement with the United States on July 12, 1955. This culminated in a public signing ceremony on March 20, 1957, to construct a \"small swimming-pool research reactor in Nachal Soreq\", which would be used to shroud the construction of a much larger facility with the French at Dimona.\n\nIn 1986 Francis Perrin, French high-commissioner for atomic energy from 1951 to 1970 stated publicly that in 1949 Israeli scientists were invited to the Saclay Nuclear Research Centre, this cooperation leading to a joint effort including sharing of knowledge between French and Israeli scientists especially those with knowledge from the Manhattan Project. According to Lieutenant Colonel Warner D. Farr in a report to the USAF Counterproliferation Center while France was previously a leader in nuclear research \"Israel and France were at a similar level of expertise after the war, and Israeli scientists could make significant contributions to the French effort. Progress in nuclear science and technology in France and Israel remained closely linked throughout the early fifties.\" Furthermore, according to Farr, \"There were several Israeli observers at the French nuclear tests and the Israelis had 'unrestricted access to French nuclear test explosion data.'\"\n\nThe French justified their decision to provide Israel a nuclear reactor by claiming it was not without precedent. In September 1955 Canada publicly announced that it would help the Indian government build a heavy-water research reactor, the CIRUS reactor, for \"peaceful purposes\". When Egyptian President Gamal Abdel Nasser nationalized the Suez Canal, France proposed Israel attack Egypt and invade the Sinai as a pretext for France and Britain to invade Egypt posing as \"peacekeepers\" with the true intent of seizing the Suez Canal (see Suez Crisis). In exchange, France would provide the nuclear reactor as the basis for the Israeli nuclear weapons program. Shimon Peres, sensing the opportunity on the nuclear reactor, accepted. On September 17, 1956, Peres and Bergmann reached a tentative agreement in Paris for the CEA to sell Israel a small research reactor. This was reaffirmed by Peres at the Protocol of Sèvres conference in late October for the sale of a reactor to be built near Dimona and for a supply of uranium fuel.\n\nIsrael benefited from an unusually pro-Israel French government during this time. After the Suez Crisis led to the threat of Soviet intervention and the British and French were being forced to withdraw under pressure from the U.S., Ben-Gurion sent Peres and Golda Meir to France. During their discussions the groundwork was laid for France to build a larger nuclear reactor and chemical reprocessing plant, and Prime Minister Guy Mollet, ashamed at having abandoned his commitment to fellow socialists in Israel, supposedly told an aide, \"I owe the bomb to them,\" while General Paul Ely, Chief of the Defence Staff, said that \"We must give them this to guarantee their security, it is vital.\" Mollet's successor Maurice Bourgès-Maunoury stated \"I gave you [Israelis] the bomb in order to prevent another Holocaust from befalling the Jewish people and so that Israel could face its enemies in the Middle East.\"\n\nThe French–Israeli relationship was finalized on October 3, 1957, in two agreements whose contents remain secret: One political that declared the project to be for peaceful purposes and specified other legal obligations, and one technical that described a 24 megawatt EL-102 reactor. The one to actually be built was to be two to three times as large and be able to produce 22 kilograms of plutonium a year. When the reactor arrived in Israel, Prime Minister Ben-Gurion declared that its purpose was to provide a pumping station to desalinate a billion gallons of seawater annually and turn the desert into an \"agricultural paradise\". Six of seven members of the Israel Atomic Energy Commission promptly resigned, protesting that the reactor was the precursor to \"political adventurism which will unite the world against us\".\n\nBefore construction began it was determined that the scope of the project would be too large for the EMET and IAEC team, so Shimon Peres recruited Colonel Manes Pratt, then Israeli military attaché in Burma, to be the project leader. Building began in late 1957 or early 1958, bringing hundreds of French engineers and technicians to the Beersheba and Dimona area. In addition, thousands of newly immigrated Sephardi Jews were recruited to do digging; to circumvent strict labor laws, they were hired in increments of 59 days, separated by one day off.\n\nBy the late 1950s Shimon Peres had established and appointed a new intelligence service assigned to search the globe and clandestinely secure technology, materials and equipment needed for the program, by any means necessary. The new service would eventually be named LEKEM (pronounced LAKAM, the Hebrew acronym for ‘Science liaison Bureau’). Peres appointed IDF Internal Security Chief, Benjamin Blumberg, to the task. As head of the LEKEM, Blumberg would rise to become a key figure in Israel’s intelligence community, coordinating agents worldwide and securing the crucial components for the program.\n\nWhen Charles de Gaulle became French President in late 1958 he wanted to end French–Israeli nuclear cooperation, and said that he would not supply Israel with uranium unless the plant was opened to international inspectors, declared peaceful, and no plutonium was reprocessed. Through an extended series of negotiations, Shimon Peres finally reached a compromise with Foreign Minister Maurice Couve de Murville over two years later, in which French companies would be able to continue to fulfill their contract obligations and Israel would declare the project peaceful. Due to this, French assistance did not end until 1966. However the supply of uranium fuel was stopped earlier, in 1963. Despite this, a French uranium company based in Gabon may have sold Israel uranium in 1965. The US government launched an investigation but was unable to determine if such a sale had taken place.\n\nTop secret British documents<ref name=\"JIC/1103/61\"></ref><ref name=\"JIC/519/61\"></ref> obtained by BBC \"Newsnight\" show that Britain made hundreds of secret shipments of restricted materials to Israel in the 1950s and 1960s. These included specialist chemicals for reprocessing and samples of fissile material—uranium-235 in 1959, and plutonium in 1966, as well as highly enriched lithium-6, which is used to boost fission bombs and fuel hydrogen bombs. The investigation also showed that Britain shipped 20 tons of heavy water directly to Israel in 1959 and 1960 to start up the Dimona reactor. The transaction was made through a Norwegian front company called Noratom, which took a 2% commission on the transaction. Britain was challenged about the heavy water deal at the International Atomic Energy Agency after it was exposed on Newsnight in 2005. British Foreign Minister Kim Howells claimed this was a sale to Norway. But a former British intelligence officer who investigated the deal at the time confirmed that this was really a sale to Israel and the Noratom contract was just a charade. The Foreign Office finally admitted in March 2006 that Britain knew the destination was Israel all along. Israel admits running the Dimona reactor with Norway's heavy water since 1963. French engineers who helped build Dimona say the Israelis were expert operators, so only a relatively small portion of the water was lost during the years since the reactor was first put into operation.\n\nIn 1961, the Israeli Prime Minister David Ben-Gurion informed the Canadian Prime Minister John Diefenbaker that a pilot plutonium-separation plant would be built at Dimona. British intelligence concluded from this and other information that this \"can only mean that Israel intends to produce nuclear weapons\". The nuclear reactor at Dimona went critical in 1962. After Israel's rupture with France, the Israeli government reportedly reached out to Argentina. The Argentine government agreed to sell Israel yellowcake (uranium oxide). Between 1963 and 1966, about 90 tons of yellowcake were allegedly shipped to Israel from Argentina in secret. By 1965 the Israeli reprocessing plant was completed and ready to convert the reactor's fuel rods into weapons grade plutonium.\n\nThe exact costs for the construction of the Israeli nuclear program are unknown, though Peres later said that the reactor cost $80 million in 1960, half of which was raised by foreign Jewish donors, including many American Jews. Some of these donors were given a tour of the Dimona complex in 1968.\n\nIsrael is believed to have begun full-scale production of nuclear weapons following the 1967 Six-Day War, although it had built its first operational nuclear weapon by December 1966. A CIA report from early 1967 stated that Israel had the materials to construct a bomb in six to eight weeks and some authors suggest that Israel had two crude bombs ready for use during the war. According to US journalist Seymour Hersh, everything was ready for production at this time save an official order to do so. Israel crossed the nuclear threshold on the eve of the Six-Day War in May 1967. \"[Prime Minister Levi] Eshkol, according to a number of Israeli sources, secretly ordered the Dimona [nuclear reactor] scientists to assemble two crude nuclear devices. He placed them under the command of Brigadier General Yitzhak Yaakov, the chief of research and development in Israel's Defense Ministry. One official said the operation was referred to as Spider because the nuclear devices were inelegant contraptions with appendages sticking out. The crude atomic bombs were readied for deployment on trucks that could race to the Egyptian border for detonation in the event Arab forces overwhelmed Israeli defenses.\"\n\nAnother CIA report from 1968 states that \"Israel might undertake a nuclear weapons program in the next several years.\" Moshe Dayan, then Defense Minister, believed that nuclear weapons were cheaper and more practical than indefinitely growing Israel's conventional forces. He convinced the Labor Party's economic boss Pinchas Sapir of the value of commencing the program by giving him a tour of the Dimona site in early 1968, and soon after Dayan decided that he had the authority to order the start of full production of four to five nuclear warheads a year. Hersh stated that it is widely believed that the words \"Never Again\" were welded, in English and Hebrew, onto the first warhead.\n\nIn order to produce plutonium the Israelis needed a large supply of uranium ore. In 1968, the Mossad purchased 200 tons from Union Minière du Haut Katanga, a Belgian mining company, on the pretense of buying it for an Italian chemical company in Milan. Once the uranium was shipped from Antwerp it was transferred to an Israeli freighter at sea and brought to Israel. The orchestrated disappearance of the uranium, named Operation Plumbat, became the subject of the 1978 book \"The Plumbat Affair\".\n\nEstimates as to how many warheads Israel has built since the late 1960s have varied, mainly based on the amount of fissile material that could have been produced and on the revelations of Israeli nuclear technician Mordechai Vanunu.\nBy 1969, U.S. Defense Secretary Melvin Laird believed that Israel might have a nuclear weapon that year. Later that year, U.S. President Richard Nixon in a meeting with Israeli Prime Minister Golda Meir pressed Israel to \"make no visible introduction of nuclear weapons or undertake a nuclear test program\", so maintaining a policy of nuclear ambiguity. Before the Yom Kippur War, Peres nonetheless wanted Israel to publicly demonstrate its nuclear capability to discourage an Arab attack, and fear of Israeli nuclear weapons may have discouraged Arab military strategy during the war from being as aggressive as it could have been.\n\nThe CIA believed that Israel's first bombs may have been made with highly enriched uranium stolen in the mid-1960s from the U.S. Navy nuclear fuel plant operated by the Nuclear Materials and Equipment Corporation, where sloppy material accounting would have masked the theft.\n\nBy 1974, the U.S. intelligence community believed Israel had stockpiled a small number of fission weapons, and by 1979 were perhaps in a position to test a more advanced small tactical nuclear weapon or thermonuclear weapon trigger design.\n\nThe CIA believed that the number of Israeli nuclear weapons stayed from 10 to 20 from 1974 until the early 1980s. Vanunu's information in October 1986 said that based on a reactor operating at 150 megawatts and a production of 40 kg of plutonium per year, Israel had 100 to 200 nuclear devices. Vanunu revealed that between 1980 and 1986 Israel attained the ability to build thermonuclear weapons. By the mid 2000s estimates of Israel's arsenal ranged from 75 to 400 nuclear warheads.\n\nSeveral reports have surfaced claiming that Israel has some uranium enrichment capability at Dimona. Vanunu asserted that gas centrifuges were operating in Machon 8, and that a laser enrichment plant was being operated in Machon 9 (Israel holds a 1973 patent on laser isotope separation). According to Vanunu, the production-scale plant has been operating since 1979–80. If highly enriched uranium is being produced in substantial quantities, then Israel's nuclear arsenal could be much larger than estimated solely from plutonium production.\n\nIn 1991 alone, as the Soviet Union dissolved, nearly 20 top Jewish Soviet scientists reportedly emigrated to Israel, some of whom had been involved in operating nuclear power plants and planning for the next generation of Soviet reactors. In September 1992, German intelligence was quoted in the press as estimating that 40 top Jewish Soviet nuclear scientists had emigrated to Israel since 1989.\n\nIn a 2010 interview, Uzi Eilam, former head of the Israeli Atomic Energy Commission, told the Israeli daily \"Maariv\" that the nuclear reactor in Dimona had been through extensive improvements and renovations and is now functioning as new, with no safety problems or hazard to the surrounding environment or the region.\n\nAccording to Lieutenant Colonel Warner D. Farr in a report to the US Air Force Counterproliferation Center, much lateral proliferation happened between pre-nuclear Israel and France, stating \"the French nuclear test in 1960 made two nuclear powers, not one—such was the depth of collaboration\" and that \"the Israelis had unrestricted access to French nuclear test explosion data,\" minimizing the need for early Israeli testing, although this cooperation cooled following the success of the French tests.\n\nIn June 1976, a West Germany army magazine, \"Wehrtechnik\" (\"military technology\"), claimed that Western intelligence reports documented Israel conducting an underground test in the Negev in 1963. The book \"Nuclear Weapons in the Middle East: Dimensions and Responsibilities\" by Taysir Nashif cites other reports that on November 2, 1966, the country may have carried out a non-nuclear test, speculated to be zero yield or implosion in nature in the Israeli Negev desert.\n\nOn September 22, 1979, Israel may have been involved in a possible nuclear bomb test, also known as the Vela Incident, in the southern Indian Ocean. A committee was set up under then-U.S. president Jimmy Carter, headed by Professor Jack Ruina of MIT. Most of the committee's members assumed that South African navy vessels had sailed out of Simonstown port, near Cape Town, to a secret location in the Indian Ocean, where they conducted the nuclear test. The committee defined the nuclear device tested as compact and especially clean, emitting little radioactive fallout, making it very nearly impossible to pinpoint. Another committee assessment concluded a cannon had fired a nuclear artillery shell, and the detected test was focused on a small tactical nuclear weapon. After renouncing their nuclear weapons program South Africa was revealed to only have six large, primitive, aircraft-deliverable atomic bombs with a seventh being built, but no sophisticated miniaturized devices of the artillery shell size.\n\nIn what they called the \"Last Secret of the Six-Day War\", \"The New York Times\" reported that in the days before the 1967 Six-Day War Israel planned to insert a team of paratroopers by helicopter into the Sinai set up and remote detonate a nuclear bomb on command from the prime minister and military command on a mountaintop as a warning to belligerent surrounding states. The greatly outnumbered Jewish state in a surprising turn of events effectively eliminated the Egyptian Air Force and occupied the Sinai winning the war before the test could even be set up. Retired Israeli brigadier general Itzhak Yaakov referred to this operation as the Israeli Samson Option.\n\nPioneering American nuclear weapons designer Theodore Taylor commented on the uncertainties involved in the process of boosting small fission weapons and the thermonuclear designs seen in the Vanunu leaked photographs. He stated that these designs required more than theoretical analysis for full confidence in the weapons' performance. Taylor therefore concluded that Israel had \"unequivocally\" tested an advanced series miniaturized nuclear device.\n\nThe Israeli nuclear program was first revealed publicly on December 13, 1960, in a small \"Time\" article, which said that a non-Communist non-NATO country had made an \"atomic development\". On December 16, the \"Daily Express\" revealed this country to be Israel, and on December 18, US Atomic Energy Commission chairman John McCone appeared on \"Meet the Press\" to officially confirm the Israeli construction of a nuclear reactor and announce his resignation. The following day \"The New York Times\", with the help of McCone, revealed that France was assisting Israel.\n\nThe news led Ben-Gurion to make the only statement by an Israeli Prime Minister about Dimona. On December 21 he announced to the Knesset that the government was building a 24 megawatt reactor \"which will serve the needs of industry, agriculture, health, and science\", and that it \"is designed exclusively for peaceful purposes\". Bergmann, who was chairman of the Israel Atomic Energy Commission from 1954 to 1966, however said that \"There is no distinction between nuclear energy for peaceful purposes or warlike ones\" and that \"We shall never again be led as lambs to the slaughter\".\n\nThe first public revelation of Israel's nuclear capability (as opposed to development program) came from NBC News, which reported in January 1969 that Israel decided \"to embark on a crash course program to produce a nuclear weapon\" two years previously, and that they possessed or would soon be in possession of such a device. This was initially dismissed by Israeli and U.S. officials, as well as in an article in \"The New York Times\". Just one year later on July 18, \"The New York Times\" made public for the first time that the U.S. government believed Israel to possess nuclear weapons or to have the \"capacity to assemble atomic bombs on short notice\". Israel reportedly assembled 13 bombs during the Yom Kippur War as a last defense against total defeat, and kept them usable after the war.\n\nThe first extensive details of the weapons program came in the London-based \"Sunday Times\" on October 5, 1986, which printed information provided by Mordechai Vanunu, a technician formerly employed at the Negev Nuclear Research Center near Dimona. For publication of state secrets Vanunu was kidnapped by the Mossad in Rome, brought back to Israel, and sentenced to 18 years in prison for treason and espionage. Although there had been much speculation prior to Vanunu's revelations that the Dimona site was creating nuclear weapons, Vanunu's information indicated that Israel had also built thermonuclear weapons.\n\nTheodore Taylor, a former U.S. weapon designer leading the field in small, efficient nuclear weapons, reviewed the 1986 leaks and photographs of the Israeli nuclear program by Mordechai Vanunu in detail. Taylor concluded that Israel's thermonuclear weapon designs appeared to be \"less complex than those of other nations,\" and as of 1986 \"not capable of producing yields in the megaton or higher range.\" Nevertheless, \"they may produce at least several times the yield of fission weapons with the same quantity of plutonium or highly enriched uranium.\" In other words, Israel could \"boost\" the yield of its nuclear fission weapons. According to Taylor, the uncertainties involved in the process of boosting required more than theoretical analysis for full confidence in the weapons' performance. Taylor therefore concluded that Israel had \"unequivocally\" tested a miniaturized nuclear device. The Institute for Defense Analyses (IDA) concluded after reviewing the evidence given by Vanunu that as of 1987, \"the Israelis are roughly where the U.S. was in the fission weapon field in about 1955 to 1960.\" and would require supercomputers or parallel computing clusters to refine their hydrogen bomb designs for improved yields without testing, though noting they were already then developing the computer code base required. Israel was first permitted to import US built supercomputers beginning in November 1995.\n\nAccording to a 2013 report by the \"Bulletin of the Atomic Scientists\", which cited US Defense Intelligence Agency figures, Israel began the production of nuclear weapons in 1967, when it produced its first two nuclear bombs. According to the report's calculations, Israel produced nuclear weapons at an average rate of two per year, and stopped production in 2004. The report stated that Israel has 80 nuclear warheads and has enough fissile material to produce 190 more. In 2014, former US president Jimmy Carter stated that \"Israel has, what, 300 or more, nobody knows exactly how many\" nuclear weapons.\n\nIn 2010, \"The Guardian\" released South African government documents that it alleged confirmed the existence of Israel's nuclear arsenal. According to the newspaper, the documents are minutes taken by the South African side of alleged meetings between senior officials from the two countries in 1975. \"The Guardian\" alleged that these documents reveal that Israel had offered to sell South Africa nuclear weapons that year. The documents appeared to confirm information disclosed by a former South African naval commander Dieter Gerhardt – jailed in 1983 for spying for the Soviet Union, who said there was an agreement between Israel and South Africa involving an offer by Israel to arm eight Jericho missiles with atomic bombs. Waldo Stumpf—who led a project to dismantle South Africa's nuclear weapons program—doubted Israel or South Africa would have contemplated a deal seriously, saying that Israel could not have offered to sell nuclear warheads to his country due to the serious international complications that such a deal could entail. Shimon Peres, former Israeli President and then Defense Minister, has rejected the newspaper's claim that the negotiations took place. He also asserted that \"The Guardian\"’s conclusions were \"based on the selective interpretation of South African documents and not on concrete facts\".\n\nAvner Cohen, author of \"Israel and the Bomb\" and \"The Worst-Kept Secret: Israel's Bargain with the Bomb\", said \"Nothing in the documents suggests there was an actual offer by Israel to sell nuclear weapons to the regime in Pretoria.\"\n\nThe United States was concerned over possible Israeli nuclear proliferation. US intelligence began to notice the Dimona reactor shortly after construction began, when American U-2 spy planes overflew the reactor, leading to a diplomatic clash. In 1960, the outgoing Eisenhower administration asked the Israeli government for an explanation for the mysterious construction near Dimona. Israel's response was that the site was a future textile factory, but that no inspection would be allowed. When Ben-Gurion visited Washington in 1960, he held a series of meetings with State Department officials, and was bluntly told that for Israel to possess nuclear weapons would affect the balance of power in the region. After John F. Kennedy took office as US President in 1961, he put continuous pressure on Israel to open the plant to American inspection. Reportedly, every high-level meeting and communication between the US and Israeli governments contained a demand for an inspection of Dimona. To increase pressure, Kennedy denied Ben-Gurion a meeting at the White House – when they met in May 1961, it was at the Waldorf Astoria Hotel in New York. The meeting itself was dominated by this issue. Ben-Gurion was evasive on the issue for two years, in the face of persistent US demands for an inspection. Finally, in a personal letter dated May 18, 1963, Kennedy threatened Israel with total isolation unless inspectors were allowed into Dimona. However, Ben-Gurion resigned as Prime Minister shortly afterward. His successor, Levi Eshkol, received a similar letter from Kennedy.\n\nIsrael eventually accepted an inspection, and Kennedy made two concessions – the US would sell Israel Hawk anti-aircraft missiles after having refused to sell Israel any major weapon systems for years. In addition, the US government agreed to the Israeli demand that the inspections would be carried out by an all-American team which would schedule its visits weeks in advance, rather than the IAEA.\n\nIn 1964, the US government tried to prevent Argentina's sale of yellowcake to Israel, with no success.\n\nAllegedly, because Israel knew the schedule of the inspectors' visits, it was able to disguise the true purpose of the reactor. The inspectors eventually reported that their inspections were useless, due to Israeli restrictions on what parts of the facility they could investigate. According to British writer and intelligence expert Gordon Thomas, former Mossad agent Rafi Eitan told him how the inspectors were fooled:\n\nA bogus control center was built over the real one at Dimona, complete with fake control panels and computer-lined gauges that gave a credible impression of measuring the output of a reactor engaged in an irrigation scheme to turn the Negev into a lush pastureland. The area containing the \"heavy\" water smuggled from France and Norway was placed off-limits to the inspectors \"for safety reasons\". The sheer volume of heavy water would have been proof the reactor was being readied for a very different purpose.\n\nIn 1968, the CIA stated in a top-secret National Intelligence Estimate that Israel had nuclear weapons. This assessment was given to President Lyndon B. Johnson. The basis for this claim was the CIA's belief, although never proven, that the uranium that went missing in the Apollo Affair had been diverted to Israel (Seymour Hersh claims that during the plant decommissioning nearly all of the missing uranium was recovered trapped in the facility pipes or accounted for.), as well as evidence gathered from NSA electronic eavesdropping on Israeli communications, which proved that the Israeli Air Force had engaged in practice bombing runs that only made sense for the delivery of nuclear weapons.\n\nIn 1969, the US government terminated the inspections. That same year, Richard Nixon became President. According to US government documents declassified in 2007, the Nixon administration was concerned with Israel's nuclear program, worrying that it could set off a regional nuclear arms race, with the Soviet Union possibly granting the Arab states a nuclear guarantee. In a memorandum dated July 19, 1969, National Security Adviser Henry Kissinger warned that \"The Israelis, who are one of the few peoples whose survival is genuinely threatened, are probably more likely than almost any other country to actually use their nuclear weapons.\" However, Kissinger warned that attempting to force Israel to disarm could have consequences, writing that \"Israel will not take us seriously on the nuclear issue unless they believe we are prepared to withhold something they very much need\" (Kissinger was referring to a pending sale of F-4 Phantom fighter jets to Israel). Kissinger wrote that \"if we withhold the Phantoms and they make this fact public in the United States, enormous political pressure will be mounted on us. We will be in an indefensible position if we cannot state why we are withholding the planes. Yet if we explain our position publicly, we will be the ones to make Israel’s possession of nuclear weapons public with all the international consequences this entails.\" Among the suggestions Kissinger presented to Nixon was the idea of the United States adopting a policy of \"nuclear ambiguity\", or pretending not to know about Israel's nuclear program.\n\nAccording to Israeli historian Avner Cohen, author of \"Israel and the Bomb\", historical evidence indicates that when Nixon met with Israeli Prime Minister Golda Meir at the White House in September 1969, they reached a secret understanding, where Israel would keep its nuclear program secret and refrain from carrying out nuclear tests, and the United States would tolerate Israel's possession of nuclear weapons and not press it to sign the Nuclear Non-Proliferation Treaty.\n\nThe State of Israel has never made public any details of its nuclear capability or arsenal. The following is a history of estimates by many different sources on the size and strength of Israel's nuclear arsenal. Estimates may vary due to the amount of material Israel has on store versus assembled weapons, and estimates as to how much material the weapons actually use, as well as the overall time in which the reactor was operated.\n\nIsraeli military forces possess land, air, and sea-based methods for deploying their nuclear weapons, thus forming a nuclear triad that is mainly medium to long ranged, the backbone of which is submarine-launched cruise missiles and medium and intercontinental ballistic missiles, with Israeli Air Force long range strike aircraft on call to perform nuclear interdiction and strategic strikes. During 2008 the Jericho III ICBM became operational, giving Israel extremely long range nuclear strike abilities.\n\nIsrael is believed to have nuclear second strike abilities in the form of its submarine fleet and its nuclear-capable ballistic missiles that are understood to be buried deeply enough that they would survive a pre-emptive nuclear strike. Ernst David Bergmann was the first to seriously begin thinking about ballistic missile capability and Israel test-fired its first Shavit II sounding rocket in July 1961. In 1963 Israel put a large-scale project into motion, to jointly develop and build 25 short-range missiles with the French aerospace company Dassault. The Israeli project, codenamed Project 700, also included the construction of a missile field at Hirbat Zacharia, a site west of Jerusalem. The missiles that were first developed with France became the Jericho I system, first operational in 1971. It is possible that the Jericho I was removed from operational service during the 1990s. In the mid-1980s the Jericho II medium-range missile, which is believed to have a range of 2800–5000 km, entered service. It is believed that Jericho II is capable of delivering nuclear weapons with a superior degree of accuracy. The Shavit three stages solid fuel space launch vehicle produced by Israel to launch many of its satellites into low earth orbit since 1988 is a civilian version of the Jericho II. The Jericho III ICBM, became operational in January 2008 and some reports speculate that the missile may be able to carry MIRVed warheads. The maximum range estimation of the Jericho III is 11,500 km with a payload of 1000–1300 kg (up to six small nuclear warheads of 100 kt each or one 1 megaton nuclear warhead), and its accuracy is considered high.\n\nIn January 2008 Israel carried out the successful test launch of a long-range, ballistic missile capable of carrying a nuclear warhead from the reported launch site at the Palmachim Airbase south of Tel Aviv. Israeli radio identified the missile as a Jericho III and the Hebrew YNet news Web site quoted unnamed defence officials as saying the test had been \"dramatic\" and that the new missile can reach \"extremely long distances\", without elaborating. Soon after the successful test launch, Isaac Ben-Israel, a retired army general and Tel Aviv University professor, told Israeli Channel 2 TV:\nThe test came two days after Ehud Olmert, then Israel's Prime Minister, warned that \"all options were on the table to prevent Tehran from acquiring nuclear weapons\" and a few months later Israel bombed a suspected Syrian nuclear facility built with extensive help from North Korea. At the same time, regional defense experts said that by the beginning of 2008, Israel had already launched a programme to extend the range of its existing Jericho II ground attack missiles. The Jericho-II B missile is capable of sending a one ton nuclear payload 5,000 kilometers. The range of Israels' Jericho II B missiles is reportedly capable of being modified to carry nuclear warheads no heavier than 500 kg over 7,800 km, making it an ICBM. It is estimated that Israel has between 50 and 100 Jericho II B missiles based at facilities built in the 1980s. The number of Jericho III missiles that Israel possesses is unknown.\n\nIsrael's fighter aircraft have been cited as possible nuclear delivery systems. The Israeli Aerial refueling fleet of modified Boeing 707s and the use of external and conformal fuel tanks gives Israeli F-15, F-15I and F-16 fighter bombers strategic reach, as demonstrated in Operation Wooden Leg.\n\nThe Israeli Navy operates a fleet of five modern German-built \"Dolphin\"-class submarines with a further three planned, and various reports indicate that these submarines are equipped with Popeye Turbo cruise missiles that can deliver nuclear and conventional warheads with extremely high accuracy.\n\nThe proven effectiveness of cruise missiles of its own production may have been behind Israel's recent acquisition of these submarines which are equipped with torpedo tubes suitable for launching long-range (1500–2400 km) nuclear-capable cruise missiles that would offer Israel a second strike capability. Israel is reported to possess a 200 kg nuclear warhead, containing 6 kg of plutonium, that could be mounted on cruise missiles. The missiles were reportedly test launched in the Indian Ocean near Sri Lanka in June 2000, and are reported to have hit their target at a range of 1500 km.\n\nIn June 2002, former State Department and Pentagon officials confirmed that the US Navy observed Israeli missile tests in the Indian Ocean in 2000, and that the Dolphin-class vessels have been fitted with nuclear-capable cruise missiles of a new design. It is believed by some to be a version of Rafael Armament Development Authority's Popeye turbo cruise missile while some believe that the missile may be a version of the Gabriel 4LR that is produced by Israel Aircraft Industries. However, others claim that such a range implies an entirely new type of missile. During the second half of the 1990s, Israel asked the United States to sell it 50 Tomahawk land-attack cruise missiles to enhance its deep-strike capabilities. Washington rejected Israel's request in March 1998, since such a sale would have violated the Missile Technology Control Regime guidelines, which prohibit the transfer of missiles with a range exceeding 300 km. Shortly after the rejection, an Israeli official told Defense News, \"History has taught us that we cannot wait indefinitely for Washington to satisfy our military requirements. If this weapon system is denied to us, we will have little choice but to activate our own defense industry in pursuit of this needed capability.\" In July 1998, the Air Intelligence Center warned the US Congress that Israel was developing a new type of cruise missile.\n\nAccording to Israeli defense sources, in June 2009 Israeli Dolphin-class submarine sailed from the Mediterranean to the Red Sea via the Suez Canal during a drill that showed that Israel can access the Indian Ocean, and the Persian Gulf, far more easily than before. IDF sources said the decision to allow navy vessels to sail through the canal was made recently and was a definite \"change of policy\" within the service. Israeli officials said the submarine was surfaced when it passed through the canal. In the event of a conflict with Iran, and if Israel decided to involve its Dolphin-class submarines, the quickest route would be to send them through the Suez Canal.\n\nThe Israeli fleet was expanded after Israel signed a 1.3 billion euro contract to purchase two additional submarines from ThyssenKrupp's subsidiary Howaldtswerke-Deutsche Werft in 2006. These two U212s are to be delivered to the Israeli navy in 2011 and are \"Dolphin II\" class submarines. The submarines are believed to be capable of launching cruise missiles carrying nuclear warheads, despite statements by the German government in 2006, in confirming the sale of the two vessels, that they were not equipped to carry nuclear weapons. The two new boats are an upgraded version of the old Dolphins, and equipped with an air-independent propulsion system, that allow them to remain submerged for longer periods of time than the three nuclear arms-capable submarines that have been in Israel's fleet since 1999. In October 2009 it was reported that the Israeli navy sought to buy a sixth Dolphin class submarine.\n\nOn June 4, 2012, \"Der Spiegel\" published an investigative article stating that Israel has armed its newest submarines with nuclear missiles. Numerous Israeli and German officials were quoted testifying to the nuclear capabilities of the submarines and the placement of nuclear missiles aboard the ships. In response to the article, officials from both Germany and Israel refused to comment. Several papers have stated the implications of Israel attaining these nuclear weapon carrying submarines are increased due to the threat of attacks upon Iran by Israel.\n\nIt has been reported that Israel has several other nuclear weapons capabilities:\n\nIsrael's deliberately ambiguous policy to confirm or deny its own possession of nuclear weapons, or to give any indication regarding their potential use, make it necessary to gather details from other sources, including diplomatic and intelligence sources and 'unauthorized' statements by its political and military leaders. Alternatively, with the Begin Doctrine, Israel is very clear and decisive regarding the country's policy on potential developments of nuclear capability by any other regional adversaries, which it will not allow.\n\nAlthough Israel has officially acknowledged the existence of Dimona since Ben-Gurion's speech to the Knesset in December 1960, Israel has never officially acknowledged its construction or possession of nuclear weapons. In addition to this policy, on May 18, 1966, Prime Minister Levi Eshkol told the Knesset that \"Israel has no atomic weapons and will not be the first to introduce them into our region,\" a policy first articulated by Shimon Peres to U.S. President John F. Kennedy in April 1963. In the late 1960s, Israeli Ambassador to the US Yitzhak Rabin informed the United States State Department that its understanding of \"introducing\" such weapons meant that they would be tested and publicly declared, while merely possessing the weapons did not constitute \"introducing\" them. Avner Cohen defines this initial posture as \"nuclear ambiguity\", but he defines the stage after it became clear by 1970 that Israel possessed nuclear weapons as a policy of \"amimut\", or \"nuclear opacity\".\n\nIn 1998, former Prime Minister Shimon Peres said that Israel \"built a nuclear option, not in order to have a Hiroshima but an Oslo\". The \"nuclear option\" may refer to a nuclear weapon or to the nuclear reactor near Dimona, which Israel claims is used for scientific research. Peres, in his capacity as the Director General of the Ministry of Defense in the early 1950s, was responsible for building Israel's nuclear capability.\n\nIn a December 2006 interview, Israeli Prime Minister Ehud Olmert stated that Iran aspires \"to have a nuclear weapon as America, France, Israel and Russia\". Olmert's office later said that the quote was taken out of context; in other parts of the interview, Olmert refused to confirm or deny Israel's nuclear weapon status.\n\nIsrael's nuclear doctrine is shaped by its lack of strategic depth: a subsonic fighter jet could cross the from the Jordan River to the Mediterranean Sea in just 4 minutes. It additionally relies on a reservist-based military which magnifies civilian and military losses in its small population. Israel tries to compensate for these weaknesses by emphasising intelligence, maneuverability and firepower.\n\nAs a result, its strategy is based on the premise that it cannot afford to lose a single war, and thus must prevent them by maintaining deterrence, including the option of preemption. If these steps are insufficient, it seeks to prevent escalation and determine a quick and decisive war outside of its borders.\n\nStrategically, Israel's long-range missiles, nuclear-capable aircraft, and possibly its submarines present an effective second strike deterrence against unconventional and conventional attack, and if Israel's defences fail and its population centers are threatened, the Samson Option, an all-out attack against an adversary, would be employed. Its nuclear arsenal can also be used tactically to destroy military units on the battlefield.\n\nAlthough nuclear weapons are viewed as the ultimate guarantor of Israeli security, as early as the 1960s the country has avoided building its military around them, instead pursuing absolute conventional superiority so as to forestall a last-resort nuclear engagement.\n\nAccording to historian Avner Cohen, Israel first articulated an official policy on the use of nuclear weapons in 1966, which revolved around four \"red lines\" that could lead to a nuclear response:\n\nSeymour Hersh alleges weapons were deployed on several occasions. On October 8, 1973, just after the start of the Yom Kippur War, Golda Meir and her closest aides decided to put eight nuclear armed F-4s at Tel Nof Airbase on 24-hour alert and as many nuclear missile launchers at Sedot Mikha Airbase operational as possible. Seymour Hersh adds that the initial target list that night \"included the Egyptian and Syrian military headquarters near Cairo and Damascus\". This nuclear alert was meant not only as a means of precaution, but to push the Soviets to restrain the Arab offensive and to convince the US to begin sending supplies. One later report said that a Soviet intelligence officer did warn the Egyptian chief of staff, and colleagues of US National Security Advisor Henry Kissinger said that the threat of a nuclear exchange caused him to urge for a massive Israeli resupply. Hersh points out that before Israel obtained its own satellite capability, it engaged in espionage against the United States to obtain nuclear targeting information on Soviet targets.\n\nAfter Iraq attacked Israel with Scud missiles during the 1991 Gulf War, Israel went on full-scale nuclear alert and mobile nuclear missile launchers were deployed. In the buildup to the United States 2003 invasion of Iraq, there were concerns that Iraq would launch an unconventional weapons attack on Israel. After discussions with President George W. Bush, the then Israeli Prime Minister Ariel Sharon warned \"If our citizens are attacked seriously — by a weapon of mass destruction, chemical, biological or by some mega-terror attack act — and suffer casualties, then Israel will respond.\" Israeli officials interpreted President Bush's stance as allowing a nuclear Israeli retaliation on Iraq, but only if Iraq struck before the U.S. military invasion.\n\nIsraeli military and nuclear doctrine increasingly focused on preemptive war against any possible attack with conventional, chemical, biological or nuclear weapons, or even a potential conventional attack on Israel's weapons of mass destruction.\n\nLouis René Beres, who contributed to Project Daniel, urges that Israel continue and improve these policies, in concert with the increasingly preemptive nuclear policies of the United States, as revealed in the Doctrine for Joint Nuclear Operations.\n\nAlone or with other nations, Israel has used diplomatic and military efforts as well as covert action to prevent other Middle Eastern countries from acquiring nuclear weapons.\n\nMossad agents triggered explosions in April 1979 at a French production plant near Toulon, damaging two reactor cores destined for Iraqi reactors. Mossad agents may also have been behind the assassinations of an Egyptian nuclear engineer in Paris as well as two Iraqi engineers, all working for the Iraqi nuclear program.\n\nOn June 7, 1981, Israel launched an air strike destroying the breeder reactor at Osirak, Iraq, in Operation Opera.\n\nMossad may have also assassinated professor Gerald Bull, an artillery expert, who was leading the Project Babylon supergun for Saddam Hussein in the 1980s, which was capable of delivering a tactical nuclear payload.\n\nOn September 6, 2007, Israel launched an air strike dubbed Operation Orchard against a target in the Deir ez-Zor region of Syria. While Israel refused to comment, unnamed US officials said Israel had shared intelligence with them that North Korea was cooperating with Syria on some sort of nuclear facility. Both Syria and North Korea denied the allegation and Syria filed a formal complaint with the United Nations. The International Atomic Energy Agency concluded in May 2011 that the destroyed facility was \"very likely\" an undeclared nuclear reactor. Journalist Seymour Hersh speculated that the Syrian air strike might have been intended as a trial run for striking alleged Iranian nuclear weapons facilities.\n\nOn January 7, 2007, \"The Sunday Times\" reported that Israel had drawn up plans to destroy three Iranian nuclear facilities. Israel swiftly denied the specific allegation and analysts expressed doubts about its reliability. Also in 2007 Israel pressed for United Nations economic sanctions against Iran, and repeatedly threatened to launch a military strike on Iran if the United States did not do so first.\n\nIsrael is widely believed to be behind the assassination of a number of Iranian nuclear scientists. The death of the Iranian physicist Ardeshir Hassanpour, who may have been involved in the nuclear program, has been claimed by the intelligence group Stratfor to have also been a Mossad assassination.\n\nThe 2010 Stuxnet malware is widely believed to have been developed by Israel and the United States. It spread worldwide, but appears to have been designed to target the Natanz Enrichment Plant, where it reportedly destroyed up to 1,000 centrifuges.\n\nIsrael was originally expected to sign the 1968 Nuclear Non-Proliferation Treaty (NPT) and on June 12, 1968, Israel voted in favor of the treaty in the UN General Assembly.\n\nHowever, when the invasion of Czechoslovakia in August by the Soviet Union delayed ratification around the world, Israel's internal division and hesitation over the treaty became public. The Johnson administration attempted to use the sale of 50 F-4 Phantoms to pressure Israel to sign the treaty that fall, culminating in a personal letter from Lyndon Johnson to Israeli Prime Minister Levi Eshkol. But by November Johnson had backed away from tying the F-4 sale with the NPT after a stalemate in negotiations, and Israel would neither sign nor ratify the treaty. After the series of negotiations, U.S. assistant secretary of defense for international security Paul Warnke was convinced that Israel already possessed nuclear weapons. In 2007 Israel sought an exemption to non-proliferation rules in order to import atomic material legally.\n\nIn 1996, the United Nations General Assembly passed a resolution calling for the establishment of a nuclear-weapon-free zone in the region of the Middle East. Arab nations and annual conferences of the International Atomic Energy Agency (IAEA) repeatedly have called for application of IAEA safeguards and the creation of a nuclear-free Middle East. Arab nations have accused the United States of practicing a double standard in criticizing Iran's nuclear program while ignoring Israel's possession of nuclear weapons. According to a statement by the Arab League, Arab states will withdraw from the NPT if Israel acknowledges having nuclear weapons but refuses to open its facilities to international inspection and destroy its arsenal.\n\nIn a statement to the May 2009 preparatory meeting for the 2010 NPT Review Conference, the US delegation reiterated the longstanding US support for \"universal adherence to the NPT\", but uncharacteristically named Israel among the four countries that have not done so. An unnamed Israeli official dismissed the suggestion that it would join the NPT and questioned the effectiveness of the treaty. \"The Washington Times\" reported that this statement threatened to derail the 40-year-old secret agreement between the US and Israel to shield Israel's nuclear weapons program from international scrutiny. According to Avner Cohen, by not stating that Israel has atomic weapons, the US avoids having to sanction the country for violating American non-proliferation law. Cohen, author of \"Israel and the Bomb\", argued that acknowledging its nuclear program would allow Israel to take part constructively in efforts to control nuclear weapons.\n\nThe Final Document of the 2010 NPT Review Conference called for a conference in 2012 to implement a resolution of the 1995 NPT Review Conference that called for the establishment of a Middle East Zone free of weapons of mass destruction. The United States joined the international consensus for Final Document, but criticized the section on the Middle East resolution for singling out Israel as the only state in the region that is not party to the NPT, while at the same time ignoring Iran's \"longstanding violation of the NPT and UN Security Council Resolutions.\"\n\nAdam Raz, The Struggle for the Bomb, Carmel Publishing House, Jerusalem 2015 [Heb.]\n\n"}
{"id": "55136466", "url": "https://en.wikipedia.org/wiki?curid=55136466", "title": "Peter Johnson (railway historian)", "text": "Peter Johnson (railway historian)\n\nPeter Johnson is a British railway historian and author who specialises in books and articles on narrow-gauge railways. He is particularly associated with the Ffestiniog Railway.\n\nJohnson is a retired a local government officer.\n\nJohnson was a director of the Ffestiniog Railway Society from 1991 to 2003 and editor (originally jointly with Norman Gurley and Dan Wilson) of the Festiniog Railway Society Magazine from 1974 to 2003. In 2003 he was appointed the Festiniog Railway Companys official photographer.\n\nIn June 1995, Johnson started writing a monthly column for \"Steam Railway\" magazine, as of September 2017 he was the magazine's longest serving continuous contributor. Since 2006 he has written obituaries of prominent railway people for The Guardian newspaper.\n\n\n"}
{"id": "4722244", "url": "https://en.wikipedia.org/wiki?curid=4722244", "title": "Physical training uniform", "text": "Physical training uniform\n\nA physical training uniform is a military uniform used during exercise, calisthenics, drills, and in some cases, very casual periods of time (off-duty time during Initial Entry Training in the U.S. Army, for example). The United States Army, Marine Corps, Navy, Air Force, and Coast Guard require use of a physical training (PT) uniform during unit exercise (including formation runs, calisthenics, and conditioning exercises). Some military units produce unique T-shirts with their unit insignia and motto, and for special events, this shirt is part of the uniform. Occasionally, exercise will be conducted in that branch's utility uniforms, normally with the blouse removed and the undershirt exposed (also known as \"boots and utes\" ).\n\nThe Army Physical Fitness Uniform (APFU) consists of:\nFor unit runs, \"esprit de corps\" or special occasions, Commanders may have soldiers wear unique T-shirts with the distinctive unit insignia and unit colors.\n\nAll items worn by Marines conducting PT as a group are subject to uniformity, at the commander's discretion. The Marine Corps PTUs consist of:\n\nThe Navy PTU's nylon moisture wicking and odor resistant Navy blue shorts come in six and eight inch lengths, providing standard appearance among different height sailors. It also has side pockets with a hidden ID card pocket inside the waistband, which is something that all of the branches' PTUs have in common. As with the Army uniform, optional compression shorts may be worn under the PTU shorts provided they are black or navy blue in color.\n\nThe Navy PTUs consist of:\n\nIn July 2018, Chief of Naval Operations John M. Richardson announced changes to the Navy PTU that will include the uniform having a moisture-wicking navy blue short-sleeve shirt and shorts with a zipper pocket on the back right side. The uniform is planned to sport gold non-reflective eagle logos with the statement \"Forged By The Sea\" on the back of shirt, and the uniform will no longer have long-sleeve shirts. The uniform will be released in October 2018 with a retirement date for the gold-shirt uniform pending. Also, a secondary navy blue uniform planned to resemble the gold-shirt uniform is in development and is planned to be released in 2019.\n\nThe Air Force Physical Training Uniform (AFPTU) consists of:\n\nThe Coast Guard PTUs consist of:\n"}
{"id": "1179913", "url": "https://en.wikipedia.org/wiki?curid=1179913", "title": "Society of Archivists", "text": "Society of Archivists\n\nThe Society of Archivists (SoA), which was in existence from 1947 to 2010, was the principal professional body for archivists, archive conservators and records managers in the United Kingdom and Ireland. In 2010 the Society amalgamated with the National Council on Archives (NCA) and the Association of Chief Archivists in Local Government (ACALG) to become the Archives and Records Association (United Kingdom and Ireland), otherwise known as the ARA. Of the three bodies which merged, the Society was by far the largest, and many of its structures and activities were inherited by the new body with little obvious change.\n\nThe Society was founded in 1947 as the Society of Local Archivists, but changed its name to the Society of Archivists in 1954 due to its membership expanding beyond archivists working in local government.\n\nThe Society was constituted as a registered charity with the express aim to:\n\nIn 2010, the Society had over 2,000 members, mainly made up of professional archivists, records managers and archive conservators from all the different types of organisations that employ such professionals. Through an affiliate membership scheme, those who worked at a paraprofessional level or who were generally interested in the work of the Society and its members were also able to join. There was a student membership option for those who were studying for a professional qualification in archives, records management or archive conservation.\n\nThe Society was governed by a Council (partially elected every two years), along with a number of committees and sub-committees relating to specific areas of business. The membership was divided geographically into eleven regions spanning the UK and Ireland, each with its own regional committee. Seven special interest groups also existed, representing members’ different employment backgrounds and/or concerns.\n\nThe Society had an office in Taunton, Somerset, which dealt with day-to-day administration. In 2010, this was taken over by the Archives and Records Association.\n\nThe Society published a biannual publication entitled the \"Journal of the Society of Archivists\", covering mainly professional issues. After the creation of the Archives and Records Association in 2010, the title remained \"Journal of the Society of Archivists\" until 2012 (volume 33); after which, from 2013, the journal was retitled the \"Archives and Records: The Journal of the Archives and Records Association\".\n\nThe Society also published a monthly newsletter, \"ARC\" (Archives, Records Management, Conservation); and a fortnightly recruitment supplement called \"ARC Recruitment\". Periodically more in depth guides on particular types of records or professional issues were published.\n\nIn terms of professional training for archives and records management, the Society accredited externally provided postgraduate diploma/degree programmes. For some twenty years the Society itself ran a distance-learning Diploma in Archives Administration, but this was discontinued in 2001.\n\nFor archive conservation, the Society ran a Certificate in Archive Conservation as an in-service scheme restricted to Society members only. It was also involved in the Professional Accreditation of Conservator-Restorers (PAC-R) scheme.\n\nOther training events for members were organised by the Regional committees or Special Interest Groups of the Society and co-ordinated by the Society’s Training Officer.\n\nProfessional members were encouraged to participate in the Society’s Registration Scheme, which acted as a formal process of continued professional development. A register was first established in 1987, initially with a clause allowing professional members to be accepted automatically if they had been working in a recognised post for at least two years. This clause was closed in 1996, and full completion of the scheme made the only way to become a Registered Member of the Society of Archivists (RMSA).\n\n"}
{"id": "53768465", "url": "https://en.wikipedia.org/wiki?curid=53768465", "title": "Stolpersteine in Prague-Žižkov", "text": "Stolpersteine in Prague-Žižkov\n\nThe Stolpersteine in Prague-Žižkov lists the Stolpersteine in a cadastral district Žižkov of Prague. The district has been split off. Since 2002 it belongs mainly to Praha 3, but smaller parts belong to Praha 8 and Praha 10. The district is named after Hussite military leader Jan Žižka. Stolpersteine is the German name for stumbling blocks collocated all over Europe by German artist Gunter Demnig. They remember the fate of the Nazi victims being murdered, deported, exiled or driven to suicide.\n\nGenerally, the stumbling blocks are posed in front of the building where the victims had their last self chosen residence. The name of the Stolpersteine in Czech is: \"Kameny zmizelých\", stones of the disappeared.\n\nAccording to the website of Gunter Demnig the Stolpersteine of Prague were posed on 8 October 2008, 7 November 2009, 12 June 2010, 13 to 15 July 2011 and on 17 July 2013 by the artist himself. A further collocation occurred on 28 October 2012, but is not mentioned on Demnig's page.\n\nThe Czech Stolperstein project was initiated in 2008 by the \"Česká unie židovské mládeže\" (Czech Union of Jewish Youth) and was realized with the patronage of the Mayor of Prague.\n\n\n"}
{"id": "53548027", "url": "https://en.wikipedia.org/wiki?curid=53548027", "title": "Stolpersteine in Ratenice", "text": "Stolpersteine in Ratenice\n\nThe Stolpersteine in Ratenice lists the Stolpersteine in Ratenice in the Central Bohemian Region (Czech: Středočeský kraj), the central part of Bohemia. Stolpersteine is the German name for stumbling blocks collocated all over Europe by German artist Gunter Demnig. They remember the fate of the Nazi victims being murdered, deported, exiled or driven to suicide.\n\nGenerally, the stumbling blocks are posed in front of the building where the victims had their last self chosen residence. The name of the Stolpersteine in Czech is: \"Kameny zmizelých\", stones of the disappeareds.\n\nThe Stolpersteine of Ratenice were collocated by the artist himself on 29 October 2012.\n\nThe Czech Stolperstein project was initiated in 2008 by the \"Česká unie židovské mládeže\" (Czech Union of Jewish Youth) and was realized with the patronage of the Mayor of Prague.\n\n\n"}
{"id": "5682878", "url": "https://en.wikipedia.org/wiki?curid=5682878", "title": "Tadeusz Korzon", "text": "Tadeusz Korzon\n\nTadeusz Korzon (1839—1918) was a historian specializing in the history of Poland.\n\nKorzon was born to Polish parents in Lithuania, and as a youth he studied law at Moscow University.\n\nHe took part in the January Uprising (1863-1865) of Poles against the Russian occupation (partitions of Poland) by organising pro-Polish patriotic demonstrations. After the defeat of the Uprising he was sentenced to death, later changed to exile to Orenburg until 1867. Afterwards from 1869 he lived in Warsaw, where he became of the teachers in the Flying University. From 1897 he was the head librarian of the Biblioteka Zamojskich. From 1903 member of the Polska Akademia Umiejętności.\n\n\nHis earliest work was related to the French and English system of punishments and was published in 1861. His main work was published by the Krakow academy of sciences \"Wewnętrne dzieje Polski za Stanislawa-Augusta, 1764—94\" (1882 - 1886) (4 tomes) and gave many new statistical, administrative and economic details about the internal life of Poland in the 18th century. One of his last works was the \"Dzieje wojen i wojskowości w Polsce\" (3 tomes) published in 1912.\n\n\n"}
{"id": "14416469", "url": "https://en.wikipedia.org/wiki?curid=14416469", "title": "Tangaroa Expedition", "text": "Tangaroa Expedition\n\nThe Tangaroa Expedition of 2006 closely resembled the Kon-Tiki expedition sailing a balsa raft from Peru to Polynesia. Tangaroa outperformed Kon-Tiki by having an improved sail rig and by actively using guaras (centerboards). As such, the expedition represents a scientific continuation of Thor Heyerdahl's experiments in recreated maritime technology.\n\nThe raft was named after the Māori sea-god Tangaroa. Based on records of ancient vessels, the raft used a relatively sophisticated square sail that allowed sailing into the wind, or tacking. It was high by wide. The raft also included a set of modern navigation and communication equipment, including solar panels, portable computers, and desalination equipment. The crew posted to their website.\n\nTangaroa's six-man crew was led by Norwegian Torgeir Higraff and included Olav Heyerdahl, grandson of Thor Heyerdahl, Bjarne Krekvik (captain), Øyvin Lauten (executive officer), Swedish Anders Berg (photographer) and Peruvian Roberto Sala. Tangaroa was launched on the same day that Kon-Tiki had been—April 28—and it reached its destination on July 7, which was 30 days faster than Heyerdahl's Kon-Tiki which had taken 101 days for the voyage. Tangaroa's speed was credited to the proper use of the quara centerboards in navigation. Heyerdahl had not known how to correctly use them.\n\nA documentary, \"The Tangaroa Expedition\" (\"Ekspedisionen Tangaroa\"), was produced by Videomaker (Norwegian), 2007, shot by photographers Anders Berg and Jenssen.\n\n"}
{"id": "27981198", "url": "https://en.wikipedia.org/wiki?curid=27981198", "title": "Tegin", "text": "Tegin\n\nTegin (aka tigin, tiğin, Pinyin: \"teqin, tiin\" , erroneously \"tèlè\" ) is a Turkic title, commonly attachable to the names of the junior members of the Khan family. However, Ligeti cast doubts on the Turkic provenance by pointing to the non-Turkic plural form \"tegit\".\n\nHistory records many people carrying the title Tegin, from those noted incidentally to those heading their own states. The best known are Kul Tigin (, erroneously ), noted for the stele in his memory in the Orkhon inscriptions; Alp-Tegin, founder of the Ghazni state, which grew into the Ghaznavid Empire; Arslan Tegin and Bughra Tegin, both instrumental in the creation of the Kara-Khanid Kaganate. The Chinese \"History of the Northern Dynasties\" states that the Hephthalite emperor of the Gandhara state was from a ruling clan of the neighboring Tegin state. With time, the title \"tegin\" became a popular personal name, and now perseveres both as personal and family name, predominantly in the South Asia and Middle East areas.\n"}
{"id": "56335790", "url": "https://en.wikipedia.org/wiki?curid=56335790", "title": "The Asatir", "text": "The Asatir\n\nThe Asaṭīr (), also known as \"The Samaritan Book of the \"Secrets of Moses,\"\" is a collection of Samaritan Biblical legends, parallel to the Jewish Midrash, and which draws heavily upon oral traditions known among Jews in the 2nd and 3rd centuries CE. Moses Gaster places its compilation about the middle or end of the third century BCE, and rendered a translation of the work in 1927 with the Royal Asiatic Society in London. Others have said that its language style resembles more the Arabic language used by the scholar Ab Ḥisda [Isda] of Tyre (Abū'l-Ḥasan aṣ-Ṣūrī) in his poems of the eleventh century CE, and place its composition in the second-half of the tenth-century. The book's title, \"Asatir\" (or \"Astir\"), was thought by Gaster to mean \"secrets,\" from which name, he applied to the book its newer title, \"The Secrets of Moses.\" Even so, such an interpretation has nothing to do with the contents of the book, nor with its subject. A more precise translation of the Arabic title of the work, \"al-Asāṭīr\", would be \"legends\" or \"tales,\" as in the Koranic expression \"asāṭīr al-Awwalīn\" (\"the Legends of the Ancients\"). \n\nThe book's narrative covers the whole of the Pentateuch, starting with Adam, the first man, and concluding with the death of Moses, adding thereto anecdotal material not available in the Hebrew Bible. It deals mainly with the succession of personages from Adam to Moses, some 26 generations. The whole book is written around the story of their lives, as handed down by oral traditions. \n\nThe book, preserved by the Samaritan community of Nablus, compiled on parchment in late Samaritan Aramaic mixed with an antiquated Arabic vernacular, and divided into twelve chapters, was discovered by Gaster in 1907. Its antiquity is attested to by the fact that it was written when the vestiges of a \"peculiar Samaritan Hebrew-Aramaic\" was still in practice, and Arabic had only begun to supersede it. Since there is no evidence that Moses actually conveyed the oral traditions contained therein, various Samaritan writers merely refer to its author as \"the Master of the Asatir,\" or the \"Author of the Asatir\" (\"Baal Asatir\"), leaving it undecided as to whether Moses had actually conveyed its legends. The book is therefore largely ascribed as being pseudepigraphic. The account tells of the Pharaoh at the time of Moses being from the progeny of Japheth, rather than of Ham. The Pharaoh at the time of Joseph, the same account says, was from the progeny of Ishmael.\n\nThe epithet used to describe Nimrod in Genesis 10:8, namely, \"Gibbor\", is rendered by the author of \"The Asatir\" as \"giant,\" rather than \"mighty one.\" According to \"The Asatir\", there were several kings, one in succession after the other, whose names were Nimrod. Seth is said to have built the city of Antokia (Antioch), one of the cities inhabited before the Great Deluge, while Noah, after the Great Deluge, is said to have been buried in the Tomb of the Patriarchs in Hebron, along with Adam, the first man. The alleged burial-place of Adam mirrors that of Jewish tradition. \n\nAlthough in today's Modern Age it is near futile to trace the migration patterns of Noah's progeny because of mass-migrations of peoples, \"The Asatir\" describes the descendants of two of the sons of Shem, viz. Laud (Ld) and Aram, as having settled in a region of Afghanistan formerly known as Khorasan (Charassan), but known by the Arabic-speaking peoples of Afrikia (North Africa) as simply \"the isle\" (Arabic: \"Al-gezirah\"). Elam and Ashur are said to have settled in places north of Ur of the Chaldees. \n\nThe first half of the 11th chapter contains a description of the borders of the Land of Israel, in which some of the place names mentioned are no longer identifiable. Some suggest that the author's familiarity with the geography of northern Erez Israel and Syria leads to the conclusion that he may have lived in this region, where large Samaritan communities then flourished in Acre, Tyre, and Damascus.\nWhile the author of \"The Asatir\" and Jewish traditions are in general agreement, there are differences in minor details. For example, according to \"Seder Olam Rabba\", there were 340 years from the Great Deluge in the time of Noah (dated at 1656 \"anno mundi\") to the Division of the earth (dated at 1996 \"anno mundi\") when his sons were sent into their respective countries at the confounding of the languages, only ten years before the death of Noah, when he was aged 940. The Samaritan tradition, as conveyed by \"The Asatir\", avers differently, that Noah divided the earth among his three sons and their descendants some twenty years before his death, when he was aged 930.\n\nToday, there exists an English translation of the work, made by Moses Gaster. A partial Hebrew translation was later published by Z. Ben-Ḥayyim, in 'The Book of Asatir', \"Tarbiẕ\" 14 (1943), 104–252, 174–190; \"Tarbiẕ\" 15 (1944), 71–87. An Arabic translation was also made of the text, as also a Samaritan modern Hebrew translation, called \"Pitron\". An English translation of the Samaritan modern Hebrew translation, \"Pitron\", was made by Gaster.\n\n\n\n"}
{"id": "4897127", "url": "https://en.wikipedia.org/wiki?curid=4897127", "title": "The Journal of Hellenic Studies", "text": "The Journal of Hellenic Studies\n\nThe Journal of Hellenic Studies is a peer-reviewed academic journal covering research in Hellenic studies. It also publishes reviews of recent books of importance to Hellenic studies. It was established in 1880 and is published annually by Cambridge University Press on behalf of the Society for the Promotion of Hellenic Studies. The editor-in-chief is Roger Brock (University of Edinburgh).\n\nThe following persons have been editors-in-chief of the journal:\n\n"}
{"id": "16680380", "url": "https://en.wikipedia.org/wiki?curid=16680380", "title": "Tovma Artsruni", "text": "Tovma Artsruni\n\nTovma Artsruni (; also known in English-language historiography as Thomas Artsruni; precise birth date and date of death unknown) was a ninth-century to tenth-century Armenian historian and author of the \"History of the House of Artsrunik\" (). Contrary to the given title, the four-volume work not only relates the history of Artsruni royal family, of which he was a member of, and its origins near Lake Van but also comprehensively covers the history of Armenia.\n\nTovma began writing \"History\" sometime in the 870s. Much like other histories composed by Armenian historians, the first volume starts at the beginning of the Armenian nation and ends in the middle of the fifth century. However, Tovma's most valuable contributions are found in the second and third volumes which accurately detail Armenian life under the rule of the Arab Caliphates and in particular the 851 Arab military expedition led by the Turkic general Bugha al-Kabir, its subsequent consequences, and the establishment of the independent Bagratid state north of Lake Van. Tovma was a relative of the king of Vaspurakan Gagik I, and wrote a detailed account in \"History\" about the famous palace and church Gagik constructed on Akhtamar Island.\n\nThe precise date that Tovma completed his work is unknown although some historians have determined that it was composed sometime after 905. Tovma's work ends with an incomplete 29th chapter yet several unknown authors (referred to as \"Anonymous\"; ) took it upon themselves to continue \"History\" down to the 1370s and added an appendix and colophon. Tovma's \"History\" was first published in 1852 in Constantinople in Armenian and was subsequently translated into French by Marie-Félicité Brosset in 1862.\n\n"}
{"id": "1257677", "url": "https://en.wikipedia.org/wiki?curid=1257677", "title": "Type site", "text": "Type site\n\nIn archaeology a type site (also known as a type-site or typesite) is a site that is considered the model of a particular archaeological culture. For example, the type site of the Pre-Pottery Neolithic A culture is Jericho, in the West Bank. A type site is also often the eponym (the site after which the culture is named). For example, the type site of the pre-Celtic/Celtic Bronze Age Hallstatt culture is the lakeside village of Hallstatt, Austria.\n\nIn geology the term is used similarly for a site considered to be typical of a particular rock formation etc.\n\nA type site contains artifacts, in an assemblage, that are typical of that culture. Type sites are often the first or foundational site discovered about the culture they represent. The use of this term is therefore similar to that of the \"specimen type\" in biology (see biological types) or \"locus typicus\" (type locality) in geology.\n\n\n\n\n\n\n\n"}
{"id": "54098971", "url": "https://en.wikipedia.org/wiki?curid=54098971", "title": "Unit 180", "text": "Unit 180\n\nUnit 180 is a North Korean cyberwarfare cell, a component of the Reconnaissance General Bureau.\n\nKim Heung-kwang, a former computer science professor in North Korea, stated that Unit 180 is likely involved in illicit operations to obtain cash for the regime, such as a counterattack on Bangladesh Bank and the WannaCry ransomware attack.\n"}
