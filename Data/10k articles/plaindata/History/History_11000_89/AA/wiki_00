{"id": "57452118", "url": "https://en.wikipedia.org/wiki?curid=57452118", "title": "A. W. Ecclestone", "text": "A. W. Ecclestone\n\nArthur William \"Billy\" Ecclestone (7 January 1901 – 1984) was an English architect and the chief surveyor for the Norfolk brewers Lacons in the first half of the twentieth century. In that capacity, he was responsible for the design of a number of their public houses, two of which are now listed buildings with Historic England. He was also a local councillor, justice of the peace, and historian of Great Yarmouth.\n\nArthur Ecclestone was born in Great Yarmouth on 7 January 1901 to Arthur James Eccleston and his wife Alice Mary Ecclestone. He was christened at St Nicholas's parish church, Great Yarmouth.\n\nEcclestone was a member of the Society of Architects and from 1925 a licentiate of the Royal Institute of British Architects. From the 1920s to the 1960s he worked for the Lacons brewery in Great Yarmouth for whom he was the chief surveyor and responsible for their pub designs.\n\nHis designs include the Clipper Schooner (1938) in Great Yarmouth with a decorative tiled panel showing a sailing ship that the \"Tile Gazetteer\" described as typical of Ecclestone's practice in his modern pub designs; the Iron Duke in Yarmouth (late 1930s, completed 1948); and the Never Turn Back in Caister-on-Sea (1957) which he designed in the Art Deco and Streamline Moderne styles as a memorial to the nine lifeboatmen who died in the Caister lifeboat disaster of 1901. Both the Iron Duke and the Never Turn Back are grade II listed with Historic England.\n\nHe also designed the Winter Gardens at Gorleston in 1929 and the Links Hotel in Gorleston, which was demolished in 1999.\n\nOutside work, Ecclestone was a justice of the peace and local councillor.\n\nEcclestone died in 1984.\n\nEcclestone was a historian of Great Yarmouth and published a number of articles and books on the town:\n\n\n"}
{"id": "2073078", "url": "https://en.wikipedia.org/wiki?curid=2073078", "title": "Aegidius Tschudi", "text": "Aegidius Tschudi\n\nAegidius (or Giles or Glig) Tschudi (5 February 150528 February 1572) was a Swiss statesman and historian, an eminent member of the Tschudi family of Glarus, Switzerland. His best known work is the Chronicon Helveticum, a history of the early Swiss Confederation.\n\nHaving served his native land in various offices, in 1558 he became the chief magistrate or \"Landarnmann\", and in 1559 was ennobled by the Emperor Ferdinand, to whom he had been sent as ambassador.\n\nOriginally inclined to moderation, he became later in life more and more devoted to the cause of the Counter-Reformation. It is, however, as the historian of the Swiss Confederation that he is best known. He collected material for three major works, which have never wholly lost their value, though his researches have been largely corrected. In 1538 his book on Rhaetia, written in 1528, was published in Latin and in German: \"De prisca ac vera Alpina Rhætia\", or \"Die uralt warhafftig Alpisch Rhætia\".\n\nTschudi's chief works were not published until long after his death. The \"Beschreibung Galliae Comatae\" appeared under Gallati's editorship in 1758, and is mainly devoted to a topographical, historical and antiquarian description of ancient Helvetia and Rhaetia, the latter part being his early work on Rhaetia revised and greatly enlarged. This book was designed practically as an introduction to his magnum opus, the \"Chronicon Helveticum\", part of which (from 1001 to 1470) was published by J. R. Iselin in two stately folios (1734–1736); the rest consists only of rough materials. There exist two rather antiquated biographies of Tschudi by I. Fuchs (2 vols, St Gall, 1805) and C. Vogel (Zürich, 1856).\n\nTschudi worked from both documents and legends to portray the ancient traditions of Swiss defense of liberty, giving roles not only to William Tell but to the heroic moment of the foundation of the Confederacy, when Werner Stauffacher representing Schwyz, Walter Fürst of Uri and Arnold of Melchtal for Unterwalden meet at the Rutli, a meadow above Lake Lucerne, and take an oath to defend Swiss freedom. Tschudi’s influential text dates that event to 8 November 1307.\n\nDown to the latter part of the 19th century Swiss historical writers had largely based their works on his investigations and manuscripts. The historical reputation of Tschudi has suffered after later research. His statements and documents relating to Roman times and the early history of Glarus and his own family had long roused suspicion. Detailed examination has proved that he not merely claimed to have copied Roman inscriptions that had never existed, and amended others in an arbitrary fashion, but that he deliberately forged documents to push back the origin of his family to the 10th century. He thus also entirely misrepresented the early history of Glarus, which is that of a democratic community, and not (as he pretended) that of a preserve of several aristocratic families. Tschudi's historical credit is thus low, and no document printed or historical statement made by him can be accepted without careful verification and examination.\n\nFor a summary of these discoveries see Georg von Wyss in the \"Jahrbuch\" of the Historical Society of Glarus (1895), vol. xxx., in No. i (1894), of the \"Anzeiger f. schweizerische Geschichte\", and in his \"Geschichte d. Historiographie in d. Schweiz\" (1895), pp. 196, 201, 202. The original articles by Vogelin (Roman inscriptions) appeared in vols xi., xiv. and xv. (1886–1890) of the \"Jahrbuch f. schweizer Geschichte\", and that by Schulte (Glarus) in vol. xviii. (1893) of the same periodical. For the defence, see a weak pamphlet, \"Schulte u. Tschudi\" (Coire, 1898), by P. C. von Planta.\n\n"}
{"id": "51240288", "url": "https://en.wikipedia.org/wiki?curid=51240288", "title": "Andrzej Feliks Grabski", "text": "Andrzej Feliks Grabski\n\nAndrzej Feliks Grabski (13 September 1934 in Warsaw – 26 June 2000 in Łodź) was a Polish historian and medievalist.\n\nHe was son of Andrzej Kazimierz Grabski. His grandfather was Władysław Grabski. He graduated at the University of Łódź in 1955.\n\n\n"}
{"id": "48388315", "url": "https://en.wikipedia.org/wiki?curid=48388315", "title": "Antoni Palau i Dulcet", "text": "Antoni Palau i Dulcet\n\nHe investigated the introduction of press in Spain in the \"De los orígenes de la imprenta y su introducción en España\" (\" From the origins of printing and its introduction in Spain \"), 1952.\n\nHe is best known as the author of an indispensable \"Manual del librero hispano-americano: inventario bibliográfico de la producción científica y literaria de España y de la América Latina desde la invención de la imprenta hasta nuestros días, con el valor comercial de todos los artículos descritos (1923-1945), en siete volúmenes\" (\" Handbook of the Spanish-American bookseller: bibliographic survey of scientific and literary production of Spain and Latin America since the invention of printing to our days, with the market value of all the items described \" (1923-1945), in seven volumes\").\n\nIn its second edition it consisted of 28 volumes published between 1948 and 1977 and seven volumes of indices between 1981 and 1987 by Agustín Palau Claveras with: Alphabetical title-materials, corrections and additions connections, a total of 35 volumes.\nHe also prepared bibliographies of Cervantes and Quevedo and an interesting autobiography, \"Memòries d'un llibreter català\" (\" Memoirs of a Catalan bookseller \") (1935)., which demonstrates his humanist spirit and Cultural altruistic desire.\n\nHe was named favorite son of Montblanc in 1949. On 31 July 1949, the mayor of the \"Vila Ducal\" , Jose Maria Abelló Barrios handed him the silver medal.\n\n\n\"Antoni Palau i Dulcet i Josep Porter i Rovira: dos montblanquins apassionats pels llibres\". Montblanc: Centre d'Estudis de la Conca de Barberà. Col·lecció biografies 2. 2007. \n\n"}
{"id": "419509", "url": "https://en.wikipedia.org/wiki?curid=419509", "title": "Archaeology and the Book of Mormon", "text": "Archaeology and the Book of Mormon\n\nSince the publication of the Book of Mormon in 1830, Mormon archaeologists have attempted to find archaeological evidence to support it. Members of The Church of Jesus Christ of Latter-day Saints (LDS Church) and other denominations of the Latter Day Saint movement generally believe that the Book of Mormon describes ancient historical events in the Americas, but historians and archaeologists do not regard it as a work of ancient American history.\n\nThe Book of Mormon describes God's dealings with three civilizations in the Americas over the course of several hundred years. The book primarily deals with the Nephites and the Lamanites, who - it states - existed in the Americas from about 600 BC to about AD 400. It also deals with the rise and fall of the Jaredite nation, which the Book of Mormon says came from the Old World shortly after the confounding of the languages at the Tower of Babel.\n\nSome early-20th century Mormon researchers claimed various archaeological findings such as place names, and ruins of the Inca, Maya, Olmec, and other ancient American and Old World civilizations as giving credence to the Book of Mormon record. Others disagree with these conclusions, arguing that the Book of Mormon mentions several animals, plants, and technologies that are not substantiated by the archaeological record of the period 3100 BC to 400 AD in the Americas.\n\nInsights into pre-Columbian civilizations, technologies, movements, and history have been established. These include the Formative Mesoamerican civilizations such as the (Pre-Classic) Olmec, Maya, and Zapotec, which flourished during the approximate period the events related in the Book of Mormon are said to have occurred.\n\nSome contemporary Book of Mormon scholars suggest that the Jaredites may have been the Olmec and that part of the Maya may have been the Nephites and Lamanites. Other Book of Mormon scholars disagree and point to some nineteenth-century archaeological finds (e.g., earth and timber fortifications and towns, the use of a plaster-like cement, ancient roads, metal points and implements, copper breastplates, head-plates, textiles, pearls, native North American inscriptions, North American elephant remains etc.) are not interpreted by as proving the historicity or divinity of the Book of Mormon. Numerous observers have suggested that the Book of Mormon appears to be a work of fiction that parallels others within the 19th-century \"mound-builder\" genre that was pervasive at the time.\n\nDuring the early 1980s, rumors circulated in Mormon culture that the Book of Mormon was being used by the Smithsonian to guide primary archaeological research. These rumors were brought to the attention of Smithsonian directors who, by 1982, sent a form letter to inquiring parties stating that the Smithsonian did not use the Book of Mormon to guide any research, and included a list of specific reasons Smithsonian archaeologists considered the Book of Mormon historically unlikely. In 1998, the Smithsonian revised the form letter and stated that Book of Mormon had not been used by the Smithsonian in any form of archaeological research. Mormon scholars speculated that this was because the earlier version of the letter contradicts some aspects of research published by Smithsonian staff members. Non-Mormon scholars note that the Smithsonian has not retracted any of its previous statements and feel that the response was toned down to avoid negative public relations with Mormons. Terryl Givens speculates that the change in the statement was an effort to avoid controversy.\n\nThe Institute for Religious Research posted on their website a 1998 letter from National Geographic Society stated that they were unaware of any archaeological evidence that would support the Book of Mormon.\n\nCritics of the Book of Mormon have argued that there are words and phrases in the book that are anachronistic with archaeological findings. These relate to artifacts, animal, plant, or technology that critics believe did not exist in the Americas during the Book of Mormon time period (before 600 BC to about 400 AD). The list below summarizes a few of the anachronistic criticisms in the Book of Mormon, as well as perspectives and rebuttals by Mormon apologists.\n\nHorses are mentioned eleven times in the Book of Mormon, yet critics argue that horses were extinct in the Western Hemisphere over 10,000 years ago and did not reappear there until the Spaniards brought them from Europe. Horses were re-introduced to the Americas (Caribbean) by Christopher Columbus in 1493 and to the American continent by Cortés in 1519. Mormon archaeologist John L. Sorenson claims that there is fossil evidence that some New World horses may have survived the Pleistocene–Holocene transition, though these findings are disputed by other Book of Mormon scholars. Alternately, Mormon apologist Robert R. Bennett suggests that the word \"horse\" in the Book of Mormon may have referred to a different animal, such as a tapir.\n\nElephants are mentioned twice in the earliest Book of Mormon record, the Book of Ether. Critics argue that the archaeological record suggests that all elephant-like creatures became extinct in the New World around 10,000 BC. The source of this extinction is speculated to be the result of human predation, a significant climate change, or a combination of both factors. A small population of mammoths survived on St. Paul Island, Alaska, up until 3700 BC,\n\nApologists deal with the \"elephant\" in much the same way as they treat the \"horse\" anachronism, countering with the following arguments:\n\nThere are six references to \"cattle\" made in the Book of Mormon, including verbiage suggesting they were domesticated. While the Book of Mormon may follow the common biblical precedent of referring to all domesticated animals as \"cattle\", there is no evidence that Old World cattle (members of the genus \"Bos\") inhabited the New World prior to European contact in the 16th century AD. Further, there is currently no archeological evidence of American bison having been domesticated. It is widely accepted that the only large mammal to be domesticated in the Americas was the llama and that no species of goats, deer, or sheep were domesticated before the arrival of the Europeans to the continent.\n\nMormon apologists argue the following to deal with this anachronism:\n\n\"Sheep\" are mentioned in the Book of Mormon as being raised in the Americas by the Jaredites between 2500 BC and 600 BC. Another verse mentions \"lamb-skin\" worn by armies of robbers (circa AD 21). However, domesticated sheep are known to have been first introduced to the Americas during the second voyage of Columbus in 1493.\n\nMormon apologists argue the following to deal with this anachronism:\n\n\"Goats\" are mentioned three times in the Book of Mormon placing them among the Nephites and the Jaredites (i.e., between 2500 BC and 400 AD). In two of the verses, \"goats\" are distinguished from \"wild goats\", indicating that there were at least two varieties, one of them possibly domesticated.\n\nDomesticated goats are known to have been introduced on the American continent by Europeans in the 15th century, 1000 years after the conclusion of the Book of Mormon, and nearly 2000 years after goats are last mentioned in the Book of Mormon. The aggressive mountain goat is indigenous to North America. There is no evidence that it was ever domesticated.\n\nMormon apologists argue the following to deal with this anachronism:\n\nThe Book of Mormon suggests that \"swine\" existed and were domesticated among the Jaredites. There have not been any remains, references, artwork, tools, or any other evidence suggesting that swine were ever present in the pre-Columbian New World.\n\nMormon apologists argue the following to deal with this anachronism:\n\n\"Barley\" is mentioned three times in the Book of Mormon narrative in portions that have been dated by Mormons to the 1st and 2nd century BC. \"Wheat\" is mentioned once in the Book of Mormon narrative dating to the same time period. The introduction of domesticated modern barley and wheat to the New World was made by Europeans after 1492.\n\nMormon apologists argue the following to deal with this anachronism:\n\nAdditionally, Bennett also notes that the Norse, after reaching North America, claimed to have found what they called \"self-sown wheat\".\n\nCritics reject the notion that \"Hordeum pusillum\" was the \"barley\" referred to in the Book of Mormon. They also note that the earliest mention of barley in the Book of Mormon dates to 121 BC, which is several hundred years prior to the date given for the recent discovery of domesticated \"Hordeum pusillum\" in North America.\n\nThe Book of Mormon mentions the use of \"silk\" six times. \"Silk\" is commonly understood to mean the material that is created from the cocoon of the Asian moth \"Bombyx mori\".\n\nApologists argue the following to deal with this anachronism:\n\nThe Book of Mormon contains two accounts of \"chariots\" being used in the New World.\n\nCritics argue that there is no archaeological evidence to support the use of wheeled vehicles in Mesoamerica, especially since many parts of ancient Mesoamerica were not suitable for wheeled transport. Clark Wissler, the Curator of Ethnography at the American Museum of Natural History in New York City, noted: \"we see that the prevailing mode of land transport in the New World was by human carrier. The wheel was unknown in pre-Columbian times.\"\n\nA comparison of the South American Inca civilization to Mesoamerican civilizations shows the same lack of wheeled vehicles. Although the Incas used a vast network of paved roads, these roads are so rough, steep, and narrow that they appear to be unsuitable for wheeled use. Bridges that the Inca people built, and even continue to use and maintain today in some remote areas, are straw-rope bridges so narrow (about 2–3 feet wide) that no wheeled vehicle can fit. Inca roads were used mainly by chaski message runners and llama caravans.\n\nSome Mormon apologists argue the following to deal with this anachronism:\n\n\"Steel\" and \"iron\" are mentioned several times in the Book of Mormon. No evidence has been found in the Americas of iron being hardened to make \"steel\" in ancient times.\n\nBetween 2004 and 2007, a Purdue University archaeologist, Kevin J. Vaughn, discovered a 2000-year-old hematite mine near Nazca, Peru. Although hematite is today mined as an iron ore, Vaughn believes that the hematite was then being mined for use as red pigment. There are also numerous excavations that included iron minerals. He noted:\n\nEven though ancient Andean people smelted some metals, such as copper, they never smelted iron like they did in the Old World ... Metals were used for a variety of tools in the Old World, such as weapons, while in the Americas, metals were used as prestige goods for the wealthy elite.\n\nApologists counter that the word \"steel\" in the Book of Mormon likely refers to a hardened metal other than iron. This argument follows from the fact that the Book of Mormon refers to certain Old World articles made of \"steel\". Similar \"steel\" articles mentioned in the King James Version of the Bible (KJV) are actually hardened copper. It has been demonstrated that much of the terminology of the Book of Mormon parallels the language of the KJV. Ancient mound-building cultures of North America are known to have mined and worked copper, silver, gold, and meteoric iron, although few instances of metallic blades or of deliberately alloyed (or \"hardened\") copper have been uncovered from ancient North America. Examples of ancient copper knife blades have been found on Isle Royale and around Lake Superior.\n\nThe Book of Mormon makes numerous references to \"swords\" and their use in battle. When the remnants of the Jaredites' final battle were discovered, the Book of Mormon narrative states that \"the blades thereof were cankered with rust.\"\n\nApologists argue that most references to swords do not speak of the material they were made of, and that they may refer to a number of weapons such as the macuahuitl, a \"sword\" made of obsidian blades that was used by the Aztecs. It was very sharp and could decapitate a man or horse. However, this does not address the mention of the Jaredite swords, because obsidian cannot rust.\n\n\"Cimiters\" are mentioned about ten times in the Book of Mormon as existing hundreds of years before the term \"scimitar\" was coined. The word \"cimiter\" is considered an anachronism since the word was never used by the Hebrews (from which the Book of Mormon peoples came) or any other civilization prior to 450 AD.\n\nApologists argue the following to deal with this anachronism:\n\nThe Book of Mormon describes in detail a system of weights and measures used by the Nephite society. However, the archaeological record shows that the overall use of metal in ancient America appears to have been extremely limited. A more common exchange medium in Mesoamerica was cacao beans.\n\n The Book of Mormon describes more than one literate people inhabiting ancient America. The Nephite people are described as possessing a language and writing with roots in Hebrew and Egyptian, and writing the original text of the Book of Mormon in this unknown language, called Reformed Egyptian. A transcript of some of the characters of this language has been preserved in the \"Anthon Transcript\".\n\nArchaeological evidence shows that the only people known to have developed written languages in America were the Olmecs and Maya, whose written languages have no resemblance to Hebrew or Egyptian hieroglyphs. Additionally, professional linguists and Egyptologists do not consider the Anthon Transcript to contain any legitimate ancient writing. Edward H. Ashment called the characters of the transcript \"hieroglyphics of the Micmac Indians of northeastern North America\".\n\nThe Smithsonian Institution has noted, \"Reports of findings of ancient Egyptian Hebrew, and other Old World writings in the New World in pre-Columbian contexts have frequently appeared in newspapers, magazines, and sensational books. None of these claims has stood up to examination by reputable scholars. No inscriptions using Old World forms of writing have been shown to have occurred in any part of the Americas before 1492 except for a few Norse rune stones which have been found in Greenland.\"<ref name=\"irr.org/smith\"> (hosted on the Institute for Religious Research website)</ref>\n\nLinguistic studies on the evolution of the spoken languages of the Americas agree with the widely held model that \"Homo sapiens\" arrived in America between 15,000 and 10,000 BC. According to the Book of Mormon, immigrants arrived on the American continent about 2500 BC (the presumed time period of the biblical Tower of Babel).\nMormon apologists argue the following to deal with this anachronism:\n\nAll chronologic dates given in the Book of Mormon are stated in terms of the Nephite calendar. The system of dates used by the rebellious Lamanites is not stated, though the Book of Mormon indicates that Lamanite converts strictly observed the Hebrew calendar. The highest numbered month mentioned in the Book of Mormon is the eleventh, and the highest numbered day is the twelfth, but the total number of months in a year and the number of days in a month is not explicitly stated. Even so, it appears that Book of Mormon peoples observed lunar cycles, \"months\", and that the Nephites observed the Israelite Sabbath at the end of a seven-day week.\n\nMost North American tribes relied upon a calendar of 13 months, relating to the annual number of lunar cycles. Seasonal rounds and ceremonies were performed each moon. Months were counted in the days between phase cycles of the moon. Calendar systems in use in North America during this historical period relied on this simple system.\n\nOne of the more distinctive features shared among pre-Columbian Mesoamerican civilizations is the use of an extensive system of inter-related calendars. The epigraphic and archaeological record for this practice dates back at least 2,500 years, by which time it appears to have been well-established. The most widespread and significant of these calendars was the 260-day calendar, formed by combining 20 named days with 13 numerals in successive sequence (13 × 20 = 260). Another system of perhaps equal antiquity is the 365-day calendar, approximating the solar year, formed from 18 \"months\" × 20 named days + 5 additional days. These systems and others are found in societies of that era such as the Olmec, Zapotec, Mixe-Zoque, Mixtec, and Maya (whose system of Maya calendars are widely regarded as the most intricate and complex among them) reflected the vigesimal (base 20) numeral system and other numbers, such as 13 and 9.\n\nIn the early 1840s, John Lloyd Stephens' two-volume work \"Incidents of Travel in Central America, Chiapas, and Yucatan\" was seen by some church members as an essential guide to the ruins of Book of Mormon cities. In the fall of 1842, an article appearing in the church's \"Times and Seasons\" alleged that the ruins of Quirigua, discovered by Stephens, may be the very ruins of Zarahemla or some other Book of Mormon city. Other articles followed, including one published shortly after the death of Joseph Smith. Every Latter Day Saint was encouraged to read Stephens' book and to regard the stone ruins described in it as relating to the Book of Mormon. It is now believed that these Central American ruins date more recent than Book of Mormon times.\n\nIn recent years, there have been differing views among Book of Mormon scholars, particularly between the scholars and the \"hobbyists\".\n\nFrom the mid-1950s onwards, New World Archaeological Foundation (NWAF), based out of Brigham Young University, has sponsored archaeological excavations in Mesoamerica, with a focus on the Mesoamerican time period known as the Preclassic (earlier than \"c.\" AD 200). The results of these and other investigations, while producing valuable archaeological data, have not led to any widespread acceptance by non-Mormon archaeologists of the Book of Mormon account. Citing the lack of specific New World geographic locations to search, Michael D. Coe, a prominent Mesoamerican archaeologist and Professor Emeritus of Anthropology at Yale University, wrote, \n\nAs far as I know there is not one professionally trained archaeologist, who is not a Mormon, who sees any scientific justification for believing the historicity of the Book of Mormon, and I would like to state that there are quite a few Mormon archaeologists who join this group.\n\nIn 1955, Thomas Ferguson, the founder of the NWAF, received five years of funding from the LDS Church and began to dig throughout Mesoamerica for evidence of the veracity of the Book of Mormon claims. In a 1961 newsletter, Ferguson predicted that although nothing had been found, the Book of Mormon cities would be found within 10 years. In 1972, Christian scholar Hal Hougey wrote Ferguson questioning the progress given the stated timetable in which the cities would be found. Replying to Hougey, as well as other secular and non-secular requests, Ferguson wrote in a letter dated 5 June 1972: \"Ten years have passed ... I had sincerely hoped that Book-of-Mormon cities would be positively identified within 10 years—and time has proved me wrong in my anticipation.\"\n\nDuring the period of 1959 to 1961, NWAF colleague Dee Green was editor of the \"BYU Archaeological Society Newsletter\" and had an article from it published in the summer 1969 edition of \"\" in which he acknowledged that the NWAF findings did not back up the veracity of the Book of Mormon claims. After this article and another six years of fruitless search, Ferguson published a 29-page paper in 1975 where he concluded, \"I'm afraid that up to this point, I must agree with Dee Green, who has told us that to date there is no Book-of-Mormon geography\".\n\nIn 1976, referring to his own paper, Ferguson wrote a letter in which he stated\n\nThe real implication of the paper is that you can't set the Book-of-Mormon geography down anywhere—because it is fictional and will never meet the requirements of the dirt-archeology. I should say—what is in the ground will never conform to what is in the book.\"\n\nFerguson's archaeological efforts failed to garner complete support from all prominent Mormon scholars. Author and Mormon Professor of Biblical and Mormon scripture Hugh Nibley published the following critical remarks:\n\nBook of Mormon archaeologists have often been disappointed in the past because they have consistently looked for the wrong things ... Blinded by the gold of the pharaohs and the mighty ruins of Babylon, Book of Mormon students have declared themselves \"not interested\" in the drab and commonplace remains of our lowly Indians. But in all the Book of Mormon we look in vain for anything that promises majestic ruins.\n\nThough the NWAF failed to establish Book of Mormon archaeology, the archaeological investigations of NWAF-sponsored projects were a success for ancient American archaeology in general which has been recognized and appreciated by non-Mormon archaeologists. Currently BYU maintains 86 documents on the work of the NWAF at the BYU NWAF website; these documents are used outside both BYU and the LDS Church by researchers.\n\nAs noted above, there is a general consensus among archaeologists that the archaeological record does not substantiate the Book of Mormon account, and in some ways directly contradicts it.\n\nAn example of the mainstream archaeological opinion of Mormon archaeology is summarized by historian and journalist Hampton Sides:\n\nYale's Michael Coe likes to talk about what he calls \"the fallacy of misplaced concreteness,\" the tendency among Mormon theorists like Sorenson to keep the discussion trained on all sorts of extraneous subtopics ... while avoiding what is most obvious: that Joseph Smith probably meant \"horse\" when he wrote down the word \"horse\".\n\nSome Mormon archaeologists and researchers have focused on the Arabian peninsula in the Middle East where they believe the Book of Mormon narrative describes actual locations. These alleged connections include the following:\n\nArchaeological studies in the New World that tie Book of Mormon places and peoples to real world locations and civilizations are incredibly difficult since there are generally no landmarks defined in the Book of Mormon that can unambiguously identify real world locations. Generally non-Mormon archaeologists do not consider there to be any authentic Book of Mormon archaeological sites. Various apologists have claimed that events in the Book of Mormon took place in a variety of locations including North America, South America, Central America, and even the Malay Peninsula. These finds are divided into competing models, most notably the Hemispheric Geography Model, the Mesoamerican Limited Geography Model, and the Finger Lakes Limited Geography Model.\n\nThe Hemispheric Geography Model posits that the events of the Book of Mormon took place over the entirety of the North and South American continents. By corollary many Mormons believe that the three groups mentioned in the Book of Mormon (Jaredites, Nephites, and Lamanites) exclusively populated an empty North and South American Continent, and that Native Americans were all of Israeli descent.\n\nSpeculations from various church leaders has shifted slightly over time, with Joseph Smith and early Mormon leaders taking a traditional stance. This model was also implicitly endorsed in the introduction to the Book of Mormon, which noted that Lamanites are the \"principal ancestors of the American Indians.\" More recently, the church has not taken as strong position on the absolute origin of Native American peoples.\n\nSome Mormon apologists believe the following archaeological finds support this theory:\n\n\n\nThe Mesoamerican Limited Geography Model posits that the events of the Book of Mormon occurred in a geographically \"limited\" region in Mesoamerica only hundreds of miles in dimension and that other people were present in the New World at the time of Lehi's arrival. This model has been proposed and advocated by various Mormon apologists in the 20th century (both RLDS and LDS). Geographically limited settings for the Book of Mormon have been suggested by LDS church leaders as well, and this view has been published in the official church magazine, \"Ensign\".\n\nMormon apologists believe the following archaeological evidence supports the Mesoamerican Geography Model:\n\n\nSome Mormon apologists hold that the events of the Book of Mormon occurred in a small region in and around the Finger Lakes region of New York. Part of the basis of this theory lies on statements made by Joseph Smith and other church leaders.\n\nMormon apologists believe the archaeological evidence below supports claims that authentic Book of Mormon sites exist in the Finger Lakes region of New York:\n\nMormon scholars have estimated that at various periods in Book of Mormon history, the populations of civilizations discussed in the book ranged between 300,000 and 1.5 million people. The size of the late Jaredite civilization was even larger. According to the Book of Mormon, the final war that destroyed the Jaredites resulted in the deaths of at least two million men.\n\nFrom Book of Mormon population estimates, it is evident that the civilizations described are comparable in size to the civilizations of ancient Egypt, ancient Greece, ancient Rome, and the Maya. Such civilizations left numerous artifacts in the form of hewn stone ruins, tombs, temples, pyramids, roads, arches, walls, frescos, statues, vases, and coins. The archaeological problem posed by the earth-, timber-, and metal-working societies described in the Book of Mormon was summarized by Hugh Nibley, a prominent BYU professor:\n\nWe should not be surprised at the lack of ruins in America in general. Actually the scarcity of identifiable remains in the Old World is even more impressive. In view of the nature of their civilization one should not be puzzled if the Nephites had left us no ruins at all. People underestimate the capacity of things to disappear, and do not realize that the ancients almost never built of stone. Many a great civilization which has left a notable mark in history and literature has left behind not a single recognizable trace of itself. We must stop looking for the wrong things. \n\nThe National Geographic Society has noted, \"Reports of findings of ancient Egyptian Hebrew, and other Old World writings in the New World in pre-Columbian contexts have frequently appeared in newspapers, magazines, and sensational books. None of these claims has stood up to examination by reputable scholars. No inscriptions using Old World forms of writing have been shown to have occurred in any part of the Americas before 1492 except for a few Norse rune stones which have been found in Greenland.\"\n\nLosses of ancient writings occurred in the Old World, including as a result of deliberate or accidental fires, wars, earthquakes, and floods. Similar losses occurred in the New World. Much of the literature of the pre-Columbian Maya was destroyed during the Spanish conquest in the 16th century. On this point, Michael Coe noted:\n\nNonetheless, our knowledge of ancient Maya thought must represent only a tiny fraction of the whole picture, for of the thousands of books in which the full extent of their learning and ritual was recorded, only four have survived to modern times (as though all that posterity knew of ourselves were to be based upon three prayer books and \"Pilgrim's Progress\").\n\nThe Maya civilization also left behind a vast corpus of inscriptions (upwards of ten thousand are known) written in the Maya script, the earliest of which date from around the 3rd century BC with the majority written in the Classic Period (c. 250–900 AD). Mayanist scholarship is now able to decipher a large number of these inscriptions. These inscriptions are mainly concerned with the activities of Mayan rulers and the commemoration of significant events, with the oldest known Long Count date corresponding to December 7, 36 BC, being recorded on Chiapa de Corzo \"Stela 2\" in central Chiapas. None of these inscriptions make contact with events, places, rulers, or timeline of Book of Mormon.\n\nOne Mormon researcher has referred to ancient Mesoamerican accounts that appear to parallel events recorded in the Book of Mormon.\n\nThere is no archaeological evidence of the Jaredite people described in the Book of Mormon that is accepted by mainstream archaeologists. Nevertheless, some Mormon scholars believe that the Jaredites were the Olmec civilization, though archaeological evidence supporting this theory is disputed and circumstantial.\n\nUnlike the Jaredites of the Book of Mormon, whose society predominantly situated in lands north of a \"narrow neck\" of land, Olmec civilization spread to both the east and west sides of a broad, lateral Central American isthmus (the Isthmus of Tehuantepec).\n\nThe Jaredite civilization in the American covenant land is said to have been completely destroyed as the result of a civil war near the time that Lehi's party is said to have arrived in the New World (approximately 590 BC). Olmec civilization, on the other hand, flourished in Mesoamerica during the Preclassic period, dating from 1200 BC to about 400 BC. The Olmec civilization suddenly disintegrated for unknown reasons, although archaeological evidence clearly indicates a definite Olmec influence within the Maya civilization that followed. Although the Olmec civilization ended, there are indications that some of the Olmec people survived and interacted with other cultures.\n\nWhile making allowance for the likelihood that Book of Mormon peoples migrated to Mexico and Central America, Joseph Smith nevertheless placed the arrival of the Jaredites in \"the lake country of America\", i.e., the region of the Great Lakes).\n\nNo Central or South American civilization is recognized to correlate with the Nephites of the Book of Mormon. The Book of Mormon makes no mention of Lamanites or Nephites erecting impressive works of hewn stone as did the Maya or various South American peoples. Some believe that Nephites lived in the Great Lakes region. Numerous aboriginal fortresses of earth and timber were known to have existed in this region.\n\nThere are ten instances in the Book of Mormon in which cities are described as having defensive fortifications. For example, Alma 52:2 describes how the Lamanites \"sought protection in their fortifications\" in the city of Mulek.\n\nOne archaeologist has noted the existence of ancient Mesoamerican defensive fortifications. According to one article in an LDS Church magazine, military fortifying berms are found in the Yucatán Peninsula. Proponents of the Heartland Model have found it ironic that such great lengths would be taken to find \"Moroniesque\" aboriginal defensive works so far away from Cumorah, when such works are known to have existed in New York.\n\nIn the early 1950s, M. Wells Jakeman of the BYU Department of Archaeology suggested that a complicated scene carved on Stela 5 in Izapa was a depiction of a Book of Mormon event called \"Lehi's dream\", which features a vision of the tree of life. This interpretation is disputed by other Mormon and non-Mormon scholars. Julia Guernsey Kappelman, author of a definitive work on Izapan culture, finds that Jakeman's research \"belies an obvious religious agenda that ignored Izapa Stela 5's heritage\".\n\nSorenson claims that one artifact, La Venta Stela 3, depicts a person with Semitic features (\"striking beard and beaked nose\"). Mormon researchers such as Robin Heyworth have claimed that Copan Stela B depicts elephants; others such as Alfred M Tozzer and Glover M Allen claim it depicts macaws.\n\n\n\n\n"}
{"id": "4533592", "url": "https://en.wikipedia.org/wiki?curid=4533592", "title": "Archaeology of Northern Europe", "text": "Archaeology of Northern Europe\n\nThe archaeology of Northern Europe studies the prehistory of Scandinavia and the adjacent North European Plain, \nroughly corresponding to the territories of modern Sweden, Norway, Denmark, northern Germany, Poland and the Netherlands.\n\nThe region entered the Mesolithic around the 7th millennium BCE. The transition to the Neolithic is characterized by the Funnelbeaker culture in the 4th millennium BCE. The Chalcolithic is marked by the arrival of the Corded Ware culture, possibly the first influence in the region of Indo-European expansion. The Nordic Bronze Age proper begins roughly one millennium later, around 1500 BCE. The end of the Bronze Age is characterized by cultural contact with the Central European La Tène culture (Celts), contributing to the development of the Iron Age by the 4th century BCE, presumably the locus of Common Germanic culture. \nNorthern Europe enters the protohistorical period in the early centuries CE, with the adoption of writing and ethnographic accounts by Roman authors.\n\nThe following is a refined listing of Northern European archaeological periods, expanded from the basic three-age system with finer subdivisions and extension into the modern historical period.\n\nDuring the 6th millennium BCE, the climate of Scandinavia was generally warmer and more humid than today. The bearers of the Nøstvet and Lihult cultures and the Kongemose culture were mesolithic hunter-gatherers. The Kongemose culture was replaced by the Ertebølle culture, adapting to the climatic changes and gradually adopting the Neolithic Revolution, transitioning to the megalithic Funnelbeaker culture.\n\nThe Pezmog 4 archaeological site along the Vychegda River (Komi Republic) was discovered in 1994. Pottery of early comb ware type appears there already at the beginning of the 6th millennium BC.\n\nPit–Comb Ware culture appeared in northern Europe as early 4200 BC, and continued until c. 2000 BC. Some scholars argue that it is associated with the area of the Uralic languages.\n\nDuring the 4th millennium BCE, the Funnelbeaker culture expanded into Sweden up to Uppland. The Nøstvet and Lihult cultures were succeeded by the Pitted Ware culture\n\nEarly Indo-European presence likely dates to the late 3rd millennium BCE, introducing the Nordic Bronze Age (Battle-Axe culture).\n\nThe tripartite division of the Nordic Iron Age into \"Pre-Roman Iron Age\", \"Roman Iron Age\" and \"Germanic Iron Age\" is due to Swedish archaeologist Oscar Montelius.\n\nThe Pre-Roman Iron Age (5th/4th–1st centuries BCE) was the earliest part of the Iron Age in Scandinavia and North European Plain.\nSucceeding the Nordic Bronze Age, the Iron Age developed in contact with the \nHallstatt culture in Central Europe.\nArchaeologists first made the decision to divide the Iron Age of Northern Europe into distinct pre-Roman and Roman Iron Ages after Emil Vedel unearthed a number of Iron Age artifacts in 1866 on the island of Bornholm. They did not exhibit the same permeating Roman influence seen in most other artifacts from the early centuries CE, indicating that parts of northern Europe had not yet come into contact with the Romans at the beginning of the Iron Age.\n\nOut of the Late Bronze Age Urnfield culture of the 12th century BCE developed the Early Iron Age Hallstatt culture of Central Europe from the eighth to sixth centuries BCE, which was followed by the La Tène culture of Central Europe (450 BCE to 1st century BCE). Albeit the metal iron came into wider use by metalsmiths in the Mediterranean as far back as 1300 BCE due to the Late Bronze Age collapse, the Pre-Roman Iron Age of Northern Europe started only as early as the 5th/4th to the 1st century BCE.\n\nThe Iron Age in northern Europe is markedly distinct from the Celtic La Tène culture south of it. The old long-range trading networks south-north between the Mediterranean cultures and Northern Europe had broken down at the end of the Nordic Bronze Age and caused a rapid and deep cultural change in Scandinavia. Bronze, which was an imported metal, suddenly became very scarce and iron, which was a local natural resource, slowly became more abundant, as the techniques for extracting, smelting and smithing it were acquired from their Central European Celtic neighbours. Iron was extracted from bog iron in peat bogs and the first iron objects to be fabricated were needles and edged tools such as swords and sickles. The rise of iron use in Scandinavia was slow, bog ore was only abundant in southwestern Jutland and it was not until 200–100 BCE, that the iron-working techniques were generally mastered and a productive smithing industry had evolved in the larger settlements. Iron products were also known in Scandinavia during the Bronze Age, but they were a scarce imported material. Similarly, imported bronze continued to be used during the Iron Age in Scandinavia, but it was now much scarcer and mostly used for decoration.\n\nFunerary practices continued the Bronze Age tradition of burning corpses and placing the remains in urns, a characteristic of the Urnfield culture. During the previous centuries, influences from the Central European La Tène culture spread to Scandinavia from north-western Germany, and there are finds from this period from all the provinces of southern Scandinavia. Archaeologists have found swords, shield bosses, spearheads, scissors, sickles, pincers, knives, needles, buckles, kettles, etc. from this time. Bronze continued to be used for torcs and kettles, the style of which were continuous from the Bronze Age. Some of the most prominent finds from the pre-Roman Iron Age in northern Europe are the Gundestrup cauldron and the Dejbjerg wagons, two four-wheeled wagons of wood with bronze parts.\n\nThe cultural change that ended the Nordic Bronze Age was affected by the expansion of Hallstatt culture from the south and accompanied by a changing climate, which caused a dramatic change in the flora and fauna. In Scandinavia, this period is often called the \"Findless Age\" due to the lack of archaeological finds. While the archaeological record from Scandinavia are consistent with an initial decline in population, the southern parts of the culture, the Jastorf culture, was in expansion southwards. It consequently appears that the climate change played an important role in this southward expansion into continental Europe. There are differing schools of thought on the interpretation of the geographical spread of cultural innovation during this time; whether the new material culture reflects a possibly warlike movement of Germanic peoples (\"demic diffusion\") southwards or whether innovations found at the Pre-Roman Iron Age sites represents a more peaceful trans-cultural diffusion. The current view in the Netherlands hold that Iron Age innovations, starting with Hallstatt (800 BCE), did not involve intrusions and featured a local development from Bronze Age culture. Another Iron Age nucleus considered to represent a local development is the Wessenstedt culture (800–600 BCE).\n\nThe bearers of this northern Iron Age culture were likely speakers of Germanic languages. The stage of development of this Germanic is not known, although Proto-Germanic has been proposed. The late phase of this period sees the beginnings of the Migration Period, starting with the invasions of the Teutons and the Cimbri until their defeat at the Battle of Aquae Sextiae in 102 BCE, presaging the more turbulent Roman Iron Age and Migration Period.\n\nThe Roman Iron Age (1–400 CE) is a part of the Iron Age. The name comes from the hold that the Roman Empire had begun to exert on the Germanic tribes of Northern Europe.\n\nIn Scandinavia, there was a great import of goods, such as coins (more than 7,000), vessels, bronze images, glass beakers, enameled buckles, weapons, etc. Moreover, the style of metal objects and clay vessels was markedly Roman. Objects such as shears and pawns appear for the first time. In the 3rd and 4th centuries, some elements are imported from Germanic tribes that had settled north of the Black Sea, such as the runes.\n\nThere are also many bog bodies from this time in Denmark, Schleswig and southern Sweden. Together with the bodies, there are weapons, household wares and clothes of wool. Great ships made for rowing have been found from the 4th century in Nydam Mose in southern Denmark.\n\nThe prime burial tradition was cremation, but the third century and thereafter saw an increase in inhumation.\n\nThrough the 5th and 6th centuries, gold and silver become more and more common. This time saw the ransack of the Roman Empire by Germanic tribes, from which many Scandinavians returned with gold and silver. A new Iron Age had begun in Northern Europe, the Germanic Iron Age.\n\nThe Germanic Iron Age is divided into the early Germanic Iron Age (EGIA) and the late Germanic Iron Age (LGIA). In Sweden, the LGIA (550–800) is usually called the Vendel era, in Norway, the Merovinger (Merovingian) Age.\n\nThe Germanic Iron Age begins with the fall of the Roman Empire and the rise of the Celtic and Germanic kingdoms in Western Europe. It is followed, in Northern Europe and Scandinavia, by the Viking Age.\n\nDuring the decline of the Roman Empire, an abundance of gold flowed into Scandinavia; there are excellent works in gold from this period. Gold was used to make scabbard mountings and bracteates.\n\nAfter the Western Roman Empire fell, gold became scarce and Scandinavians began to make objects of gilded bronze, with decorative figures of interlacing animals. In the EGIA, the decorations tended to be representational—the animal figures are rather faithful anatomically; in the LGIA, they tended to be more abstract or symbolic—intricate interlaced shapes and limbs.\n\nThe LGIA in the 8th century blends into the Viking Age and the proto-historical period, with legendary or semi-legendary oral tradition recorded a few centuries later in the \"Gesta Danorum\", heroic legend and sagas, and an incipient tradition of primary written documents in the form of runestones.\n\n\n"}
{"id": "38667895", "url": "https://en.wikipedia.org/wiki?curid=38667895", "title": "Austromarxism", "text": "Austromarxism\n\nAustro-Marxism was a Marxist theoretical current, led by Victor Adler, Otto Bauer, Karl Renner and Max Adler, members of the Social Democratic Workers' Party of Austria in Austria-Hungary and the First Austrian Republic (1918–1934). It is known for its theory of nationality and nationalism, and its attempt to conciliate it with socialism in the imperial context. Hence, Otto Bauer thought of the \"personal principle\" as a way of gathering the geographically divided members of the same nation. In \"Social Democracy and the Nationalities Question\" (1907), he wrote that \"The personal principle wants to organize nations not in territorial bodies but in simple association of persons\", thus radically disjoining the nation from the territory and making of the nation a non-territorial association.\n\nBeginning in 1904, the Austro-Marxist group organized around magazines such as the \"Blätter zur Theorie und Politik des wissenschaftlichen Sozialismus\" and the \"Marx-Studien\". Far from being a homogeneous movement, it was a home for such different thinkers and politicians as the Neokantian Max Adler and the orthodox Marxist Rudolf Hilferding.\n\nIn 1921 the Austro-Marxists formed the International Working Union of Socialist Parties (also known as 2½ International or the Vienna International), hoping to unite the 2nd and 3rd Internationals, something which eventually failed. \n\nAustro-Marxism inspired later movements such as Eurocommunism and the New Left, all searching for a democratic socialist middle ground between communism and social democracy and a way to eventually unite the two movements.\n\nAustro-Marxism also instituted economic and social reforms such as healthcare, municipal housing, and educational system in Vienna, which later inspired the Scandinavian social democratic parties and the British Labour Party.\n\nAustro-Marxism was also the first movement in Europe to mount an armed resistance to a fascist government, although eventually defeated in 1934.\n\nThe Austro-Marxist principle of national personal autonomy was later adopted by various parties, among them the Bund (General Jewish Labour Union), left-wing Zionists (Hashomer Hatzair) in favour of a binational solution in Palestine, the Jewish Folkspartei between the two world wars and the Democratic Union of Hungarians in Romania after 1989.\n\nAustro-Marxism can be seen as the predecessor of Eurocommunism. Both ideologies are conceived as alternatives to Marxism-Leninism.\n\n\n\n"}
{"id": "56359907", "url": "https://en.wikipedia.org/wiki?curid=56359907", "title": "C. H. S. Fifoot", "text": "C. H. S. Fifoot\n\nCecil Herbert Stuart Fifoot, FBA (1899 – 31 January 1975) was a British legal scholar.\n\nFifoot was born in Penarth, near Cardiff, the son of Sydney Fifoot, manager of the Great Western Colliery Company. He was educated at Berkhamsted School, and was then commissioned into the Royal Field Artillery in 1917, aged 18. He was sent to France, where he was injured in July 1918, leaving him partially deaf. He then attended Exeter College, Oxford, where he took a First in Jurisprudence.\n\nHe was called to the bar by the Middle Temple in 1922, and began to practice in South Wales. In 1924, however, he gave up legal practice and accepted a tutorial fellowship at Hertford College, Oxford, where he remained a fellow until 1959. He was elected an honorary fellow in 1962.\n\nHe was University Lecturer in Law from 1930 until 1945, when he was appointed All Souls Reader in English Law. He was Reader in Common Law to the Inns of Court from 1954 to 1967. He was Bursar of Hertford College from 1926 to 1934 and Dean from 1940 to 1944. He was also the University's Senior Proctor from 1935 to 1936.\n\nHis only son, Richard Fifoot, was Bodley's Librarian from 1979 to 1981.\n"}
{"id": "29458388", "url": "https://en.wikipedia.org/wiki?curid=29458388", "title": "Center for Hellenic Studies in Greece, Harvard University", "text": "Center for Hellenic Studies in Greece, Harvard University\n\nThe Center for Hellenic Studies in Greece (CHS GR) is a Harvard research Center based in Nafplio, Greece. A twin institution to its counterpart Harvard University’s Center for Hellenic Studies in Washington, D.C., CHS GR is housed in the Iatrou building, formerly home to the municipal town hall of Nafplio, at Philhellene and King Othon Street. A joint American and Greek Executive Board supervise the Center in Greece. The Director of the Center for Hellenic Studies in Greece is Professor Ioannis Petropoulos of the Democritus University of Thrace.\n\nThe Center for Hellenic Studies in Greece inaugurated in 2008 by way of a joint decision between the Provost and the Dean of the Harvard Faculty of Arts and Sciences. CHS GR utilizes the knowledge and expertise of Harvard's faculty and research centers and is a part of Harvard’s continuing effort to expand its international presence. The Center serves as a nexus for the network of international Centers operated by Harvard in Europe and around the globe, and it was organized and developed by being accessible, and free of charge, to the general public.\n\nFrom a geographical standpoint Nafplio's role in the shaping of modern Greece and its enduring cultural significance make it an opportune setting for Harvard students as well as for visitors from Greece and across the globe. Nafplio neighbors with many archaeological sites in Greece such as Mycenae, Mystras, Epidaurus, Olympia, Tiryns, etc. Nafplio was the first capital of modern Greece after independence and a port during the Mycenean period.\n\nThe Center for Hellenic Studies’s facility in Nafplio has been equipped with a Digital Library which provides access to Harvard Library's electronic resources and a number of additional databases. Access is open to all researchers and visitors of the Center, regardless of affiliation.\n\nThe Center for Hellenic Studies provides training and professional development to students, visitors, and researchers in all fields of study through a wide variety of programs, which include internships, workshops and seminars, research initiatives, and study abroad:\n\n\nAs part of its mission of outreach, the Center for Hellenic Studies in Greece offers a regular program of activities that address not only Harvard and other academic institutions but also the local community, including the Argolis region and the Peloponnese. Such is the Events Series, which is organized with the support of Municipalities of the Argolis, Nafplio, Argos-Mycenae, Ermionida, Epidaurus, as well as other institutions such as the Society for the Promotion of Education and Learning (SPEL), the University of Peloponnese, the Harvard Club of Greece, the Archaeological Museum of Thessaloniki, etc. A number of other activities such as educational programs, exhibitions, events and workshops are organized in collaboration with institutions of the city such as the Municipal Organization of Culture, Environment, Sports, and Tourism (DOPPAT), Municipalities of the Argolis as well as the regional government of the Peloponnese, the Athletic Federation of Greece (SEGAS), the Department of Theatre Studies, University of Peloponnese, the Fougaro Cultural Complex, etc.\n\nCHS in Greece has established long collaborations with institutions such as SPEL under the direction of George Babiniotis, the International Olympic Academy, the Fulbright Foundation in Greece, the Michael Marks Charitable Trust, the University of Peloponnese, in order to provide a regular program of events and activities addressed to the local and global educational and research community.\n\nCHS GR also hosts and supports a number of visiting study abroad programs and collaborates with a number of educational institutions. To name a few: the Summer Study Abroad Program of the Jackson School of International Studies, University of Washington, Seattle; the Summer Program in Greece of Duke University, Department of Philosophy and Education Office for Undergraduates; the Study Abroad Program in Greece “Food and Culture in Greece” of Louisiana University; the Summer Program in Greece of the College of William and Mary, Department of Classical Studies.\n\nAll these activities constitute an offering back to the local community of the Peloponnese and the global community.\n\n"}
{"id": "58824526", "url": "https://en.wikipedia.org/wiki?curid=58824526", "title": "Christa Ehrmann-Hämmerle", "text": "Christa Ehrmann-Hämmerle\n\nChrista Ehrmann-Hämmerle (born Schaffhausen, October 31, 1957 as Christa Hämmerle) is a Swiss-born Austrian historian. She is Associate Professor of Modern History at the University of Vienna. Her work focuses on military history, particularly World War 1, as well as women and gender history of the 19th and 20th centuries. Since 2011, she has been a spokeswoman for the Military History Working Group. She is co-founder and co-editor of the scientific journal \"L'Homme - Europäische Zeitschrift für Feministische Geschichtswissenschaft\".\n\nShe was awarded the Käthe Leichter Prize for Women's Research, Gender Studies and Gender Equality in 1999.\n\n"}
{"id": "17835873", "url": "https://en.wikipedia.org/wiki?curid=17835873", "title": "Daniel Walker Howe", "text": "Daniel Walker Howe\n\nDaniel Walker Howe (born January 10, 1937 in Ogden, Utah) is an American historian who specializes in the early national period of U.S. history, with a particular interest in its intellectual and religious dimensions. He was Rhodes Professor of American History at Oxford University in England (from 1992 to 2002 then Emeritus) and Professor of History Emeritus at the University of California, Los Angeles. He won the 2008 Pulitzer Prize for History for \"\" (2007), his most famous book. He was president of the Society for Historians of the Early American Republic in 2001, and is a Fellow of both the American Academy of Arts and Sciences and the Royal Historical Society. He received an honorary Doctor of Humanities degree from Weber State University in 2014.\n\nHowe was born in Ogden, Utah and graduated from East High School in Denver. He received his Bachelor of Arts at Harvard University in 1959, magna cum laude in American history and literature, and his Ph.D. in history at University of California, Berkeley in 1966. Howe's connection with Oxford University began when he matriculated at Magdalen College to read modern history in 1960, receiving his M.A. in 1965.\n\nHowe has taught at Yale University (1966–73), UCLA (1973-92), where he chaired the history department, and Oxford (1992-2002). In 2011 he spent a semester as a visiting professor at Wofford College in Spartanburg, South Carolina.\n\nIn 1989–1990 Howe was Harold Vyvyan Harmsworth Professor of American History at Oxford and a fellow of The Queen's College, Oxford. In 1992 he became a permanent member of the Oxford history faculty and a fellow of St Catherine's College, Oxford until his retirement in 2002. Brasenose College, Oxford elected him an Honorary Member of its Senior Common Room.\n\nHe currently resides in Sherman Oaks, California, and is married with three grown children and six grandchildren as of February 2015.\n\n\n"}
{"id": "45223610", "url": "https://en.wikipedia.org/wiki?curid=45223610", "title": "David Masnata y de Quesada", "text": "David Masnata y de Quesada\n\nDavid Masnata y de Quesada, Marquis of Santa Ana y Santa María, was a Cuban lawyer, professor, historian, and founder of the Instituto Cubano de Genealogía y Heráldica.\n\nMasnata was born on September 11, 1926. He was the first child of Emilio Masnata y Azcue and Aurora de Quesada y Miranda, grandson of Gonzalo de Quesada, an associate of Cuban revolutionary hero José Martí, and great-grandson of Dr. Ramón L. Miranda, the family physician of José Martí.\n\nMasnata’s interest in genealogy and history developed during his early years. He founded the Instituto Cubano de Genealogía y Heráldica (Cuban Institute of Genealogy and Heraldry) in 1950 and served as secretary until 1961. He was also a member of the Sociedad Cubana de Estudios Históricos y Internacionales (Cuban Society of Historical and International Studies) from 1956 to 1961 and was an honorary member of the Academia Mejicana de Genealogía y Heráldica (Mexican Academy of Genealogy and Heraldry) from 1950 to 1988.\n\nFollowing Castro’s communist takeover of Cuba, Masnata moved to New York in 1961 and left behind a large collection of Cuban books and research papers on Cuban and Spanish history, genealogy, and heraldry. While in exile, Masnata compiled a new collection of approximately three thousand books and thousands of records and documents on Spanish and Cuban genealogy. He became a proficient genealogist, and traced his ancestors from the sixteenth century to the early twentieth century.\n\nAt the age of twenty-five, Masnata received his law degree from the University of Havana and practiced law in Cuba until 1961. From 1951 to 1961 he was a member of the Colegio de Abogados de la Habana (Bar Association of Havana). He received another degree from New York University in 1967 and entered the Bar Association of New York City in 1969. From 1970 to 1971 Masnata served as a professor at Columbia University. Masnata was acknowledged by the Supreme Court of the United States in 1973 and licensed to practice in Washington, DC, and New York. In 1979, Masnata was licensed to practice law in Spain and became a member of the Ilustre Colegio de Abogados de Madrid.\n\nHe received the Prix de Liechtenstein in 1985 from H.R.H. Frank Josef II, Reigning Prince of Liechtenstein, for his work \"La Casa Real de la Cerd\" \"(The Royal House of La Cerd)\".\n\nDavid Masnata died in New York on October 5, 1988.\n\n\n"}
{"id": "25821907", "url": "https://en.wikipedia.org/wiki?curid=25821907", "title": "Disjecta membra", "text": "Disjecta membra\n\n, also written , is Latin for \"scattered fragments\" (also scattered limbs, members, or remains) and is used to refer to surviving fragments of ancient poetry, manuscripts, and other literary or cultural objects, including even fragments of ancient pottery. It is derived from , a phrase used by Horace, a Roman poet.\n\nFragments of ancient writing, especially ancient Latin poetry found in other works are commonly referred to as \"disjecta membra\". The terms \"disiecta membra\" and \"disjecta membra\" are paraphrased from the Roman lyric poet Horace (65 BC – 8 BC), who wrote of in his \"Satires\", 1.4.62, referring to the \"limbs of a dismembered poet\". In full, the term originally appeared as , in reference to the earlier Roman poet Ennius.\n\nAlthough Horace's intended meaning remains the subject of speculation and debate, the passage is often taken to imply that if a line from poetry were torn apart and rearranged, the dismembered parts of the poet would still be recognisable. In this sense, in the study of literature, \"disjecta membra\" is often used to describe the piecing together of ancient fragments of an identifiable literary source. Similarly, isolated leaves or parts of leaves from ancient or medieval manuscripts may also be termed \"disjecta membra\". Scholars have been able to identify fragments now held in different libraries that originally belonged to the same manuscript. \n\nScholars have long referred to sherds of ancient Greek pottery as . They have studied fragments of ancient Greek pottery in institutional collections, and have attributed many such pieces to the artists who made them. In a number of instances, they have been able to identify fragments now in different collections that belong to the same vase.\n\n"}
{"id": "37051893", "url": "https://en.wikipedia.org/wiki?curid=37051893", "title": "Early skyscrapers", "text": "Early skyscrapers\n\nThe early skyscrapers were a range of tall, commercial buildings built between 1884 and 1939, predominantly in the American cities of New York City and Chicago. Cities in the United States were traditionally made up of low-rise buildings, but significant economic growth after the Civil War and increasingly intensive use of urban land encouraged the development of taller buildings beginning in the 1870s. Technological improvements enabled the construction of fireproofed iron-framed structures with deep foundations, equipped with new inventions such as the elevator and electric lighting. These made it both technically and commercially viable to build a new class of taller buildings, the first of which, Chicago's tall Home Insurance Building, opened in 1885. Their numbers grew rapidly, and by 1888 they were being labelled \"skyscrapers\".\n\nChicago initially led the way in skyscraper design, with many constructed in the center of the financial district during the late 1880s and early 1890s. Sometimes termed the products of the Chicago school of architecture, these skyscrapers attempted to balance aesthetic concerns with practical commercial design, producing large, square \"palazzo\"-styled buildings hosting shops and restaurants on the ground level and containing rentable offices on the upper floors. In contrast, New York's skyscrapers were frequently narrower towers which, more eclectic in style, were often criticized for their lack of elegance. In 1892, Chicago banned the construction of new skyscrapers taller than , leaving the development of taller buildings to New York.\n\nA new wave of skyscraper construction emerged in the first decade of the 20th century. The demand for new office space to hold America's expanding workforce of white-collar staff continued to grow. Engineering developments made it easier to build and live in yet taller buildings. Chicago built new skyscrapers in its existing style, while New York experimented further with tower design. Iconic buildings such as the Flatiron were followed by the tall Singer Tower, the Metropolitan Life Insurance Company Tower and the Woolworth Building. Though these skyscrapers were commercial successes, criticism mounted as they broke up the ordered city skyline and plunged neighboring streets and buildings into perpetual shadow. Combined with an economic downturn, this led to the introduction of zoning restraints in New York in 1916.\n\nIn the interwar years, skyscrapers spread to nearly all major U.S. cities, while a handful were built in other Western countries. The economic boom of the 1920s and extensive real estate speculation encouraged a wave of new skyscraper projects in New York and Chicago. New York City's 1916 Zoning Resolution helped shape the Art Deco or \"set-back\" style of skyscrapers, leading to structures that focused on volume and striking silhouettes, often richly decorated. Skyscraper heights continued to grow, with the Chrysler and the Empire State Building each claiming new records, reaching and respectively. With the onset of the Great Depression, the real estate market collapsed, and new builds stuttered to a halt. Popular and academic culture embraced the skyscraper through films, photography, literature, and ballet, seeing the buildings as either positive symbols of modernity and science, or alternatively examples of the ills of modern life and society. Skyscraper projects after World War II typically rejected the designs of the early skyscrapers, instead embracing the international style; many older skyscrapers were redesigned to suit contemporary tastes or even demolished—such as the Singer Tower, once the world's tallest skyscraper.\n\nEarly skyscrapers emerged in the U.S. as a result of economic growth, the financial organization of American businesses, and the intensive use of land. New York was one of the centers of early skyscraper construction and had a history as a key seaport located on the small island of Manhattan, on the east coast of the U.S. As a consequence of its colonial history and city planning, New York real estate was broken up into many small parcels of land, with few large sites. During the first half of the 19th century it became the national center of American finance, and the banks in the financial district of Wall Street competed fiercely with English institutions for international dominance.\n\nThe Great Fire of 1835 destroyed most of the old financial buildings, and in their place a wide variety of new buildings were erected and demolished in quick succession during the 1840s and 1850s; traveler Philip Hone suggested that the entire city was being rebuilt every decade. Most buildings adopted the Italian Renaissance inspired \"palazzo\"-style of architecture popular in England, and rose no more than five or six stories. New York did not have any restrictions on the height of buildings, but in practice low-rise buildings were the norm, at least until 1865, with the tallest buildings being the city's churches. New York's population tripled between 1840 and 1870, and property values soared, increasing by more than 90 percent between 1860 and 1875.\n\nFurther west, the city of Chicago became the other major site in the development of early skyscrapers. In contrast to New York, Chicago emerged as a major metropolis only in the mid-19th century, growing from a village of around fifty inhabitants in 1830, to a city of 30,000 in 1850 and nearly 300,000 by 1870. Chicago became the railroad hub for the American West and the primary trading city for the emerging territories, famous for its commercial culture. It saw itself as different from the cities on the east coast and was immensely proud of its status as a growing, vibrant center.\n\nBy the 1870s, Chicago had become the main financial center for the West, but in October 1871 the Great Chicago Fire destroyed the majority of the wooden structures within the city. The city was rebuilt on large plots of land in a new grid network and followed new city ordinances that prohibited construction in wood. These factors encouraged the building of taller properties in new innovative designs, which, like New York, saw a range of businesses and services being packed into single buildings.\n\nThe construction of taller buildings during the 1870s was hindered by the financial Panic of 1873 and the ensuing economic depression, which lasted until around 1879. Construction slowed, and property values slumped. By 1880, however, the recovery was well underway, with new construction in New York returning to the pace of 1871, and the economic upturn making the construction of taller buildings an attractive financial option again, establishing many of the preconditions for the development of the skyscraper.\n\nThe emergence of skyscrapers was made possible by technological improvements during the middle of the 19th century. One of these developments was the iron framed building. Masonry buildings supported their internal floors through their walls, but the taller the building, the thicker the walls had to become, particularly at the base. In the 1860s, French engineers experimented with using built-up plate girders made of wrought iron to construct buildings supported by internal metal frames. These frames were stronger than traditional masonry and permitted much thinner walls. The methodology was extensively described in engineering journals and was initially used to build warehouses.\n\nUsing these metal frames for taller buildings, however, meant exposing them to increased wind pressure. As a consequence, protective wind bracing had to be introduced, enabled by the work of Augustin-Jean Fresnel who produced equations for calculating the loads and moments on larger buildings. Metal-framed buildings were also vulnerable to fire and required special fireproofing. French engineers had made advances in this area in the early 19th century, and major breakthroughs came with the work of architect Peter Wight in the 1860s. Spurred on by the catastrophic fires in Chicago in 1871 and Boston in 1872, his findings were turned into a wide variety of patented fireproofing products during the 1870s.\n\nTaller, heavier buildings such as skyscrapers also required stronger foundations than smaller buildings. Earlier buildings had typically rested their foundations on rubble, which was in turn laid down on the soft top layer of the ground called the overburden. As buildings became taller and heavier, the overburden could not support their weight, and foundations increasingly needed to rest directly on the bedrock below. In both New York and Chicago this required digging down a considerable distance through soft soil and often below the water table, risking the hole filling up with water before the foundations were complete. The deeper the foundations needed to be, the greater the challenge. Special water-tight boxes called caissons were invented to deal with this problem in England in 1830 and adopted in the U.S. during the 1850s and 1860s.\n\nThe development of the elevator was also essential to the emergence of the early skyscrapers, as office buildings taller than around six stories would have been impractical without them. Powered elevators were first installed in England during the 1830s and spread to U.S. factories and hotels by the 1840s. Elevators using hoist ropes, however, could only function effectively in low-rise buildings, and this limitation encouraged the introduction of the hydraulic elevator in 1870, even though early models contained dangerous design flaws. By 1876 these problems had been resolved, providing a solution for servicing the early skyscrapers.\n\nNew environmental technologies in heating, lighting, ventilation and sanitation were also critical to creating taller buildings that were attractive to work in. Central heating could not be easily extended to serve larger buildings; in the 1850s, a system using low-pressure steam and steam-operated fans became adopted in the construction of the later skyscrapers. Many U.S. buildings were lit by gas, but this carried safety risks and was difficult to install in taller buildings. As an alternative, electric lights were installed from 1878 onwards, powered by basement generators. Ventilation was also a challenge, as smoke drifting into offices from the streets and the fumes from the gas lighting made air quality a major health issue. A steam-driven, forced-draft ventilation system was invented in 1860 and became widely used in taller buildings by the 1870s, overcoming much of the problem. Improvements in iron piping permitted running hot and cold water and sanitation facilities to be installed throughout taller buildings for the first time.\n\nThere is academic disagreement over which building should be considered the first skyscraper. The term was first used in the 1780s to describe a particularly tall horse, before later being applied to the sail at the top of a ship's mast, tall hats and bonnets, tall men, and a ball that was hit high into the air. In the 1880s it began to be applied to buildings, first in 1883 to describe large public monuments and then in 1889 as a label for tall office blocks, coming into widespread use over the next decade. Identifying the first \"true skyscraper\" is not straightforward, and various candidates exist depending on the criteria applied. George Post's New York Equitable Life Building of 1870, for example, was the first tall office building to use the elevator, while his Produce Exchange building of 1884 made substantial structural advances in metal frame design. The Home Insurance Building in Chicago, opened in 1885, is, however, most often labelled the first skyscraper because of its innovative use of structural steel in a metal frame design.\n\nThe Home Insurance Building was a tall, 10-story skyscraper designed by William Le Baron Jenney, who had been trained as an engineer in France and was a leading architect in Chicago. Jenney's design was unusual in that it incorporated structural steel into the building's internal metal frame alongside the traditional wrought iron. This frame took the weight of the floors of the building and helped to support the weight of the external walls as well, proving an important step towards creating the genuine non-structural curtain walls that became a feature of later skyscrapers. The design was not perfect – some of the weight was still carried by masonry walls, and the metal frame was bolted, rather than riveted, together – but it was clearly a significant advance in tall building construction.\n\nThe approach quickly caught on in Chicago. In 1889 the Tacoma Building replaced the bolted metal design with a stronger riveted approach, and Chicago's Chamber of Commerce Building introduced interior light courts to the structural design of skyscrapers. The 1890 Rand McNally Building became the first entirely self-supporting, steel-framed skyscraper. Some buildings, such as The Rookery and the Monadnock Building, combined elements of both the newer and older styles, but generally Chicago rapidly adopted steel structures as a flexible and effective way to produce a range of tall buildings. Structural engineers specializing in the steel frame design began to establish practices in Chicago.\n\nThere was a boom in skyscraper construction in Chicago from 1888 onwards. By 1893, Chicago had built 12 skyscrapers between 16 and 20 stories tall, tightly clustered in the center of the financial district. Chicago's skyscrapers, however, were constrained by the contemporary limits of steel-frame design and the muddy sub-soil in the city, which together limited most of their skyscrapers to around 16 or 17 stories. Chicago's skyscrapers rapidly became tourist destinations for the views of the wider city they provided from their upper floors and as attractive sites in their own right. Tourists were advised to hire cabs for street tours of the skyscrapers – by lying back in the cab, they would be able to safely take in the tops of the tall buildings.\n\nThe Masonic Temple was the most prominent of these skyscrapers. Built by the Freemasons of Chicago in 1892, at a time when the Masons was a fast-growing social community, the lavish tall skyscraper had 19 stories, the bottom ten holding shops and the higher levels containing the Masons' private suites and meeting halls, some able to hold up to 1,300 people. At the top was a roof garden and observation gallery. The Freemasons were competing with their local rivals the Odd Fellows, who intended to build a much higher skyscraper, tall, that they announced would be the tallest building in the world. Newspapers picked up the story, circulating facts about the size of the Temple and making comparison to historical buildings such as the Capitol or the Statue of Liberty. The Odd Fellows project failed, but the Masonic Temple capitalized on the publicity, being declared the \"tallest commercial building in the world\".\n\nIn comparison, New York trailed behind Chicago, having only four buildings over 16 stories tall by 1893. Part of the delay was caused by the slowness of the city authorities to authorize metal-frame construction techniques; it was not until 1889 that they relented and allowed Bradford Gilbert to construct the Tower Building, an 11-story iron-framed skyscraper. This encouraged the building of more skyscrapers in New York, although the city remained cautious about the technology for some years. Finally, in 1895 a breakthrough was made with the construction of the American Surety Building, a twenty-story, high-steel development that broke Chicago's height record. From then on, New York thoroughly embraced skeleton frame construction. In particular, New York newspaper companies adopted the skyscraper, building several along Park Row, sometimes termed \"Newspaper Row\", in the 1880s and 1890s. A few early skyscrapers were also constructed in Baltimore, Boston, Pittsburgh, St. Louis, Rochester, and Buffalo, such as the Wainwright Building, Wilder Building, and Guaranty Building. Early examples on the West Coast include the Old Chronicle Building and the Call Building in San Francisco, as well as the Luzon Building in Tacoma, Washington.\n\nThe architects of early skyscrapers faced a number of challenges. The most fashionable architectural style in the late 19th century was the French Beaux-Arts movement, sometimes termed the Italian Renaissance style, which applied Classical aesthetic principles to modern buildings. American architects trained in the Beaux-Arts style at the Parisian Académie des Beaux-Arts began to return home in the 1880s, bringing these ideals and standards with them. Applying contemporary Beaux-Arts standards to early skyscrapers, however, was not straightforward. The buildings that the Beaux-Arts movement influenced were typically much shorter and broader than any skyscraper, and it was difficult to accurately reproduce the style in a tall, narrow building.\n\nSkyscrapers were also primarily commercial buildings, and economics as well as aesthetics had to play an important part in their design. The architectural writer Barr Ferree noted in 1893 that \"current American architecture is not a matter of art, but of business. A building must pay, or there will be no investor ready with the money to meet its cost. This is at once the curse and the glory of American architecture.\" George Hill echoed the theme, condemning unnecessary features on the basis that \"every cubic foot that is used for purely ornamental purposes beyond that needed to express its use and to make it harmonize with others of its class, is a waste\".\n\nBy the 1890s, Chicago architects were producing a solution to this problem, creating a new architectural style, often termed the \"Chicago school of architecture\". The school included architects such as Louis Sullivan, William Le Baron Jenney and John Root, whose designs combined architectural aesthetic theory with practical commercial sense. They favoured placing rich, ornate designs on the outside of skyscrapers at the ground level and simpler, plainer ornamentation on the upper levels, with strong vertical lines. The roofs of their skyscrapers typically formed a comprehensible outline and structure when seen at a distance as part of the city skyline. The intent was to draw the observer's eye upwards, celebrating what Sullivan termed the \"lofty\" nature of the skyscraper, but not wasting resources on intricate detailing unlikely to appeal to a busy businessman. At the same time, the more lavish ground floor designs would make the building stand out to passers-by and pull in the necessary business for a successful commercial building.\nThis community also saw close collaboration between architects, specialist structural engineers, and building contractors emerge on the new skyscraper projects. Historically the industry had been dominated by individuals and small firms who combined the roles of architect and engineer, but this broke down in Chicago during the period, being replaced by a partnership between specialist architects who focused on the appearance of the skyscraper, and specialist engineers who focused on the structures that enabled it to be built. Chicago architectural firms grew to be large, hierarchical and with numerous specialist staff; the D. H. Burnham & Company, for example, felt like a small factory to visitors, and ultimately expanded to employ 180 staff.\n\nThe resulting Chicago school produced large, solid-looking skyscrapers, built with a common appearance and to a common height. The result was usually a box-like \"palazzo\", illuminated with a large light court, ideally, if space allowed, in the center. The outside of the building was commonly divided into three parts: a base, middle section and the roof line. This tripartite design was intended both to emulate classical columns, and reflect the functions of the different parts of the skyscraper. The central court could form a simple courtyard, but many companies preferred to roof over the courtyard with glass to produce an atrium for shops and restaurants. Rents for these shops were up to five or six times that for office space, and made an important difference to the income from a property. Chicago skyscraper windows were also a feature of the style; these were large, fixed windows flanked by smaller sash windows on either side, which provided access to sunlight and adequate ventilation. Sometimes these protruded from the building to form a slight bay.\n\nAlthough the exterior of the Chicago skyscrapers buildings were relatively plain, the entrance ways and lobbies were fitted out in a grand style. The Unity Building, for example, was reported as including \"Numidian, Alps, Green and Sienna marbles ... an artistic screen of glass and bronze ... a marble balcony\" alongside \"Corinthian columns with finely carved capitals, gold-leaf and silver chandeliers, and silver-plated latticework\" on the elevators. The aim was to project a sense of prosperity and solid financial credentials, which in turn would attract tenants willing to pay high rents. For the tenants, such surroundings were good for their own business credibility and affirmed their own social status as professionals.\n\nNew York faced similar architectural challenges, but in comparison to Chicago, skyscraper architects worked less closely with engineers and other specialists, and instead held strong backgrounds in the Beaux Arts movement and perceived their role to be primarily artistic rather than a partnership with the mechanical arts. Their practices tended to be smaller, resembling \"atelier\" style workshops. Structural engineers in New York took longer to build up a strong professional role there, a trend reflected in the lower engineering quality of many early skyscrapers in the city. The New York style emphasized stunning height and a somewhat eclectic use of architectural features from other periods, creating an energetic, flamboyant appearance. Towers were common, making best use of the relatively small plots of land in New York. Some New York skyscrapers emulated the tripartite style of Chicago, but others broke their exterior down into many different layers, each with its own style. Proponents argued that this reintroduced a sense of human proportion to these tall buildings; critics felt that the results were confusing and ungainly.\n\nEarly skyscrapers were mainly made up of small office cubicles, commonly only across, which were placed adjacent to one another along long corridors, following a pattern first invented in the Oriel Chambers building in England in 1864. This allowed the average small company to rent a small amount of space using one or two offices, but held out the option for future expansion by renting additional office cubicles if required at a later date. A skyscraper office relied on natural sunlight from the windows but if necessary was dimly lit from electric desk-top lamps. By the standards of the day, these offices were very modern, with radiators, air vents, and the latest fixtures and fittings, and modern communication systems, including telephone and pneumatic tubes were often installed. As a result, many businesses chose to move out of their older, low-rise offices in Chicago to take up tenancy in the new skyscrapers, which were felt to be more convenient and healthier.\n\nThe first skyscrapers were mainly occupied by male workers, but this changed during the 1890s, with female employees becoming more common. The percentage of female clerical workers in Chicago, for example, increased from 11 percent in 1880 to 21 percent by 1890, reaching 30 percent by 1900. Various moral concerns were raised about men and women mixing in such offices, which were characterized as being masculine spaces, full of tobacco smoke and profanity and therefore unsuitable for women. The new female workers typically worked as typists or stenographers, using the recently invented typewriter, which grew in number in U.S. offices from 146 in 1879 to 65,000 by 1890.\n\nSkyscrapers provided a wide range of in-house services for their tenants, including shops, restaurants, barbers, tobacconists, newsagents, tailors, professional specialists and libraries. Skyscrapers also employed a substantial number of service staff to maintain and support them; a building such as the Chicago Board of Trade spent 20 percent of its rental revenue on service staff, employing 41 people, including janitors, elevator operators, engineers, and an electrician. With this collection of services and facilities, skyscrapers of the period were often referred to as small cities in their own right.\n\nOpposition to Chicago's skyscrapers began to grow during the late 1880s and early 1890s. Even before the development of the skyscraper, some criticized large buildings in Chicago for dominating churches and private houses, and this sentiment strengthened. Critics complained that the concentration of tall buildings in the center of the city was causing huge congestion, and each new skyscraper was also burning additional coal to power its facilities, together consuming a total of over one million tons each year, leaving smoke and stagnant air hanging over Chicago. Many were concerned over the risk of a major fire breaking out and spreading, uncontrolled, from building to building.\n\nChicago was not alone in having concerns over the growth of the skyscraper. In Boston, the Fiske and the Ames Buildings were built in the late 1880s, and tall respectively, but protests by local civic campaigners and the real estate industry resulted in the city passing a law to limit new buildings to a maximum of , effectively banning the construction of skyscrapers. The cities of Philadelphia, Los Angeles and Washington D.C. similarly introduced height restriction laws to limit skyscraper construction.\n\nThe decisive factor in favor of change in Chicago, however, was the economic slowdown in the early 1890s, which gave way to the financial panic of 1893. The recession, combined with the frantic building of the previous few years, meant that Chicago suddenly had a significant surplus of office space, threatening occupancy levels and rents. Regulation was introduced by the city council to control the problem in 1892, with support from the real estate industry who hoped to slow the construction of additional office space and shore up their diminishing profit margins. The height of new buildings was limited to , with lower height levels on narrower streets, effectively curtailing the construction of any taller skyscrapers.\n\nThe early years of the 20th century saw a range of technically sophisticated, architecturally confident skyscrapers built in New York; academics Sarah Landau and Carl Condit term this \"the first great age\" of skyscraper building. Some were relatively conservative buildings in a classical style, such as the Mutual Life, Atlantic Mutual, and Broad Exchange Buildings, all designed by Clinton and Russell. Others broke new ground, including the Flatiron Building which opened in 1903 near Madison Square. The Chicago firm of Daniel Hudsdon Burnham designed the high, 21 story structure; the unusually shaped, narrow building needed particularly strong wind bracing, while the facade was richly textured and incorporated stylistic features more common in Chicago. A critical and popular success, the Flatiron was likened to the Parthenon of Ancient Greece and became a New York icon.\n\nThe construction of the Singer Tower was announced by the company in 1906, who intended to produce the tallest skyscraper in the world. The company already had several low-rise buildings in New York that the tower would be incorporated into and planned to rent out the bottom half of the tower to tenants to subsidize their use of the upper half. The skyscraper was designed by Ernest Flagg, a Beaux-Arts advocate and noted critic of existing skyscrapers, who justified taking on the project as a way of generating support for skyscraper reform. The design was technically challenging: the tall, narrow tower needed special wind bracing, and the deep bedrock on the site required particularly deep foundations. The tower was faced in dark brick and followed the Beaux-Arts style used by the rest of the complex, with a galleried lobby fitted out in Italian marble. When it opened in 1908, it had 47 stories and was tall; visitors paid $0.50 ($12 in 2010 terms) each to use the observation area at the top of the building.\n\nThe Metropolitan Life Insurance Company Tower was opened in 1909, the culmination of a long building project by N. LeBrun and Sons to hold Metropolitan Life's growing headquarters staff, 2,800 strong by 1909. At high and with 50 stories, it became the world's new tallest building. Metropolitan Life intended the skyscraper to promote the company's image, and the building was surrounded by publicity. The tower was featured on the front of prominent magazines such as \"Scientific American\", as well as on the sides of corn flake boxes, coffee packets, and cars. The tower was loosely modeled on the Venician St Mark's Campanile, and featured extensive Early Renaissance-style detailing, with the more modern additions of huge clock faces, electric floodlights for night-time illumination, and an observation deck at the top. Contemporary architects Everett Waid and Harvey Corbett described how the building had \"the latest ideas in ventilation, air conditioning, sound deadening, artificial lighting, intercommunicating pneumatic tubes, telephones, call bells, unit operating clock systems [and] special elevator and escalator installations\". The design won critical acclaim within the American architectural profession.\n\nThe construction of the Woolworth Building was announced in 1910 by Frank Woolworth, who wanted to create the largest income-producing property in the U.S. The project grew, and Woolworth finally opted for a 55-story, high skyscraper, the latest tallest building in the world, at a cost of $13.5 million ($5.1 billion in 2010 terms). Architect Cass Gilbert designs included a very high proportion of usable – and thus rentable – floor space, with a great deal of light and a flexible floor plan that could be subdivided for different tenants. Up-to-date fittings were installed to encourage a high-class of tenants, including the world's fastest elevators, safety features, and a swimming pool. Gilbert adopted the Beaux-Arts style, using accented terracotta and glass to emphasis vertical lines, elegantly echoing the structural frame underneath and incorporating 15th and 16th century Flamboyant Gothic-styled features. It was capped by a gilded tower that blended into the sky behind it to produce an illusion of even greater height. The building was illuminated with floodlights at night, topped with red and white flashing lights. It was famously dubbed the \"Cathedral of Commerce\", rather to Gilbert's displeasure as he had attempted to avoid copying ecclesiastical architecture.\n\nMeanwhile, Chicago's skyscraper industry also boomed during the decade before World War I. The city's elevated train network was operating by 1910, allowing more workers to come into the centre. In 1910 alone of new offices were opened and by the end of the decade, Chicago had the second largest number of headquarters offices in the U.S. Chicago architectural firms such as Daniel H. Burnham and then Graham, Anderson, Probst & White continued to design skyscrapers in the \"palazzo\" style made popular in the previous decade. Chicago had hosted the World's Columbian Exposition in 1893, a massive international event which had excited interest in the themes of classical architecture and well-designed city landscapes. Chicago also had extensive discussions in 1909 about the potential for designing large parts of the city, the Burnham Plan of Chicago.\n\nThe resulting skyscrapers reflected these debates: the Railway Exchange, the Peoples Gas and the Illinois Continental and Commercial Bank Buildings were each substantial, quarter-block wide \"palazzo\" cubes of common height, their facades divided into a classical tripartite design, and sporting classical columns and other features. Despite the apparent uniformity of design, individual buildings varied considerably in the detail of their designs in effort to express their particular identities, the Peoples Gas Building using texture and the Railway Exchange Building's white terracotta, for example.\n\nThe process of building skyscrapers became more sophisticated, starting with the acquisition of the real estate needed for the site. Brokers working on commission would secretly acquire the individual lots of land required for a project, operating under a variety of names to avoid having the price increase once a planned build became known. The properties at the front of the site would typically be bought first, so if news broke of the skyscraper then those owning property at the back of the plot would have little choice but to sell anyway. The funding for skyscraper developments was normally lent by banks, insurance companies, or raised through bonds sold through a specialised bond house, with the latter becoming increasingly popular after World War I.\n\nEfforts were made to improve the processes for erecting skyscrapers, largely through the work of general contractors such as Louis Horowitz and Frank Gilbreth, who in turn drew on recent work by efficiency specialist Frederick Taylor. Time schedules were devised for all the work to be undertaken, with costs carefully monitored and reports produced each day. The results were demonstrated on the Woolworth Building construction project, where of steel were assembled in only six days, a record for the period. Improved windbracing techniques were introduced. The use of pneumatic caissons in skyscraper foundations grew more advanced; in the construction of the 1908 Manhattan Municipal Building they were successfully sunk below the surface, with specially conditioned workers operating in shifts with constant medical support.\n\nNew technologies were also introduced within the buildings. Fast Otis elevators, powered by electricity rather than steam-driven hydraulics, began to be installed in skyscrapers, with Ellithorpe safety air cushions protecting the passengers in the case of failure. Lighting improved, with the recommended levels in 1916 being around twice the level of the 1890s. Nonetheless, skyscrapers still relied primarily on natural sunlight, which required installing large windows and having tall ceilings to allow the sunlight to penetrate the back of the offices; an office deeper than was not considered a practical design.\n\nOne of the reasons for the increase in the numbers of skyscrapers during the period was the growth in demand for office workers. In part this demand was fuelled by many U.S. firms becoming larger and more complex, and white-collar sectors such as insurance and banking grew in scale. It was also driven by changing technology. The typewriter was joined in the office by the adding machine, the telephone and filing cabinets, all adding to the demand for office space and requiring increasingly specialised workers.\n\nTenants and rental income were essential to the financial success of any skyscraper, as even the largest skyscrapers and those founded by prominent companies rented out much of their office space. Owners could charge significantly more for office space close to the main windows, making it most efficient to build skyscrapers with as much premium office space as possible, even if this cost slightly more to construct in the first instance. As a result, a standard pattern for office units in both New York and Chicago emerged, with either a single rectangular office adjoining an exterior wall, or a \"T\" shaped design, with a reception room giving way to two windowed offices, separated by glass partitions. Skyscrapers usually took on large numbers of relatively small companies as their tenants. A skyscraper such as the Woolworth Tower had around 600 different tenants in 1913, for example, while a typical tenant might rent four or five office units in a skyscraper.\n\nSkyscrapers, particularly those in New York, attracted considerable comment, much of it negative. On his return to New York, writer Henry James condemned the buildings in \"The American Scene\" as simply \"giants of the mere market\", \"mercenary monsters\" doomed to be torn down in turn as other, even larger, buildings took their place. In Chicago the combination of the environmental pollution and skyscrapers meant that, as Charles Warner complained, \"one can scarcely see across the streets on a damp day, and the huge buildings loom up in the black sky in ghostly dimness\".\n\nWider artistic sentiments varied. Many, like, Alfred Stieglitz harboured mixed feelings over New York's skyscrapers, reflected in his famous 1903 portrait of the Flatiron building, and his 1910 work \"Old and New New York\" that contrasts the growing steel frame of the emerging Vanderbilt Hotel with the old low-rise blocks of the street below. Poets also wrote about the issues, the early Modernist Sadakichi Hartmann describing how \"from the city's stir and madd'ning roar\" the Flatiron's \"monstrous shape soars in massive flight\". Artists such as Alvin Coburn and John Marin experimented with producing portraits of New York's skyscrapers, capturing the positive and negative aspects of the modern structures. In 1908 artist Harry Pettit produced a romantic interpretation of a future New York, filled with giant skyscrapers supporting aerial bridges and receiving dirigibles from around the globe.\n\nAmongst the architectural community, the Exposition in Chicago inspired many in America to champion planning cities that had a unified design, in which each building had unique features but elegantly complemented its neighbours, typically by being built to a common height: \"horizontal visual unity\". In the aftermath of the Exposition, many of these advocates joined with the Beaux-Arts movement to form the City Beautiful movement, proposing low-rise cities with wide boulevards, built in a classical style. These critics condemned New York's skyscrapers, Montgomery Schuyler describing how they had produced a \"horribly jagged sierra\" of a city skyline and complaining that no modern skyscraper had turned out to be an architectural success. Charles Lamb argued that the skyscraper impinged on the rights of the rest of the city by destroying the collective appearance of an urban area. Some architects, such as Charles McKim and Stanford White refused to work on such projects altogether.\n\nCalls were made in New York for legislation to prevent the growth of skyscrapers, citing their effect on the city skylines, but these attempts faced legal obstacles. The United States Constitution did not give local government authority to prevent the buildings on the grounds of aesthetics alone, and any such law in New York was considered likely to face judicial challenge.\n\nOthers argued that legal reform of skyscraper construction was needed for public health and safety reasons, an area in which U.S. law was normally more supportive of state intervention. The risk of fire was one concern; while skyscrapers might be relatively fireproof, their height made conventional fire-fighting difficult. Architect Charles Bragdon considered a large scale, devastating high-rise fire to be inevitable if regulation was not forthcoming. The Baltimore fire of 1904 was widely cited as an example of this risk and, although Baltimore had few high rise buildings, city lawyers argued that the taller buildings had caused the flames to spread through the city during the blaze. Baltimore subsequently passed a law in 1904, banning buildings higher than . Other campaigners argued that skyscrapers were compromising access to light and air, noting that when tall buildings rose straight up from the sidewalk, they cast long shadows across the street, preventing healthy sunlight from reaching smaller buildings below.\nEarly attempts by Ernest Flagg to limit the height of New York's skyscrapers in 1896 failed; further unsuccessful attempts followed between 1906 and 1908, and legislation was turned down again in 1909, partially because of pressure from the real estate industry. After 1913, however, the property market in New York entered a recession, and vacancy levels in buildings began to rise. The campaign for change was helped by the construction of the Equitable Building in 1915 at the estimated cost of $29 million ($10.9 billion in 2010 terms), which rapidly became infamous as its vast height and bulk blocked views cast neighbours into permanent shade. The real estate industry finally ceased its objections to new legislation, and the 1916 Zoning Resolution was passed.\n\nThe details of the legislation were largely the work of architect George Ford. Ford had sympathies with the City Beautiful movement, disliked the unimaginative form of many New York skyscrapers, and had concerns over urban public health, but he also found tall buildings exciting and believed that horizontal visual unity produced boring architecture. As a consequence of Ford's influence, New York's laws were relatively complex and divided up the city into different zones; in each zone, a particular zoning envelope applied, controlling the height to which buildings could be built. Typically a building could only rise up from the sidewalk vertically by around , after which the building had to be set back at a set angle to allow sunlight to reach the street below. On a quarter of the site, however, the owner could build as high as they wished, without any further restrictions.\n\nA real estate boom occurred in the U.S. after the end of the First World War, with a particular surge in the construction of new skyscrapers between 1925 and 1931. In New York, a quarter of the financial district was rebuilt between 1928 and 1931, with of new office space added between 1925 and 1929 alone. In Chicago, limited wartime construction created supply shortages, and rent levels rose in response by around 100 percent between 1919 and 1924. This level of potential profits encouraged an explosion of new building projects in the city. The boom inflated prices in the real estate market, leading to speculative financing and building projects, including the introduction of 100 percent mortgages for new builds. An early edition of the \"Fortune\" magazine commented wryly that \"all a man needs to own a skyscraper is the money and the land. And he may be able to get along without the money.\"\n\nSkyscrapers continued to grow in height throughout the 1920s. In part this was the result of improvements in technology: steel-frame structures had become increasingly efficient, while improvements in elevator design made taller buildings easier to ascend. Commercial factors were at work too, as growing commercial demand pushed up rents, enabling taller projects, while the higher offices attracted more sunlight, permitting the charging of premium rents. The tallest buildings could also acquire publicity for their owners, in turn making it easier to find and keep the best tenants. Furthermore, the higher the cost of the underlying real estate, the taller a building needed to be to generate a suitable return on the investment, and the minimum sensible commercial height for a skyscraper project grew to between 40 and 45 stories. 70 story skyscrapers became relatively common, although an influential 1930 study demonstrated that the best rate of return on a skyscraper was to build it 63 stories high, returning an annual profit of 10.25 percent.\nSkyscrapers continued to spread both across the U.S. and internationally. New York and Chicago remained the center of skyscraper development, but most major cities in the U.S. had built skyscrapers by 1929, frequently as a result of competition between rival cities for status and investment. Cincinnati built the Cincinnati Towers in 1914, followed by the Carew Tower Complex in 1930. In Detroit, the General Motors Building opened in 1920 and the Fisher Building in 1928. Cleveland acquired the Union Trust Building in 1923 and the Terminal Tower in 1929; the latter, built by the Van Sweringen brothers, was, for a short period, the second tallest building in the world. Seattle had built Smith Tower in 1914, and the Los Angeles local government granted itself an exemption from city planning restrictions in order to build the Los Angeles City Hall in 1928. Also in 1928, the Industrial Trust Tower was built in Providence.\n\nSkyscrapers were also built in other developed countries, although reaching nowhere near the level of construction seen in the U.S. This was partially the result of a lack of funding but also because of local architectural preferences. European cities including London and Paris had laws to ban tall buildings, but elsewhere skyscrapers began to appear, including Toronto's Imperial Bank of Commerce Building, Antwerp's Boerentoren and Buenos Aires' Kavanagh Building. Many other European skyscrapers were proposed in a frenzy of excited planning, although few materialised. Soviet Russia began the construction of the Palace of the Soviets in the late 1930s, in the Socialist Classicism style, which would have become the tallest building in the world, but war intervened and the skyscraper was never completed. In the post-war years, this style resulted in the monumental Seven Sisters of Moscow building.\n\nThe technology used in constructing skyscrapers continued to develop. Time was increasingly a factor in the projects, and architects and their specialist teams developed faster ways to design and construct the buildings to minimise the interest payments during construction and hasten the arrival of rental income. By 1930, skyscrapers were being erected in just 12 months by teams of workers totalling 5,000 men, with four floors being assembled in a typical week. Building skyscraper towers involved some adaption of engineering techniques, as effectively two different buildings were being designed – the base and the tower – which needed to be efficiently linked using elevators and other service facilities. Most new offices settled around a standard size, wide by deep, depending on the height of the ceiling, with multiple small windows considered better than a few larger ones. The output of electric lighting continued to improve, although this began to give off excessive heat within the offices. Air conditioning was first installed in a few skyscrapers during the 1930s.\n\nDuring the 1920s and 1930s many skyscrapers were designed in an Art Deco style. This architectural approach typically combined what Carol Willis terms an \"aesthetic of simple, sculptural mass\" with the use of rich colour and ornamentation on the surfaces of the buildings. The aim was to call attention to the increasingly complex three-dimensional shape of the skyscraper, in contrast to earlier styles which could be critiqued, as historian Larry Ford suggests, as being merely \"short buildings made taller with additional stories\". Windows were de-emphasised in favour of creating a strong sense of shape and mass, the surrounding walls treated as textured fabric, dressing the building underneath. Skyscrapers of this period typically lost their ornamental horizontal divisions, being broken up by physical changes in their shape as one looked up the building, the whole forming a striking silhouette.\n\nIn New York, the 1916 act to allow light and air to reach the streets encouraged a stepped or ziggurat approach to skyscraper design; this \"set-back\" style often made unrestricted use of the 25 percent of the site allowed by law to complete with a very tall tower. This encouraged a diversity of buildings, while maintaining an element of harmony and consistency of style. The Paramount Building and 120 Wall Street, for example, were constructed in the set-back style without towers, partially because the limited size of the sites would have made the towers relatively narrow and – when packed with the necessary elevators and service facilities – economically unviable. Many other skyscrapers built on larger or more expensive plots opted for the tallest towers possible, including the $24 million ($3.8 billion in 2010 terms) Bank of Manhattan Trust Building and the City Bank-Farmers Trust Company Building. New York continued its lead in tall skyscrapers throughout the period; in 1920 it had ten times the number of tall buildings than its nearest rival Chicago.\n\nSome New York skyscrapers complemented traditional, cubicle offices linked by corridors, with larger, more open-plan spaces called \"general offices\". These maximised the number of workers that could be fitted into a given space and provided for greater flexibility. Expensive corporate suites were also created during the 1930s, especially on Wall Street, typically on the highest levels of the buildings. These were used by the corporate executives and were usually lavishly outfitted in a range of traditional and exotic styles. The lobbies of these skyscrapers remained grand affairs, although some banks now eschewed renting out space to shops and restaurants in favour of a more exclusive atmosphere. The largest skyscrapers held up to 16,000 workers, although between 5,000 and 10,000 was more common, and the buildings held a wide range of services to support them, including beauty salons, private luncheon clubs, chiropodists and gymnasiums. A skyscraper such as the Cities Service Building directly employed over 200 staff to manage and protect the property. The technology within the offices also grew still more sophisticated, with dictating, automatic typing and tabulating machines being used by teams of ever more specialised office workers.\n\nChicago altered its laws in the 1920s to allow towers to be built as part of its skyscrapers. In 1920, the maximum building height in Chicago was increased to , and unoccupied structures on a building, such as ornamental towers, were allowed to extend up to high. Additional changes came in 1923, with taller occupied towers being permitted for the first time, but subject to controls on overall volumes. The main building of a skyscraper could be up to tall, and a tower could be built on up to 25 percent of the lot, but the tower could not have a volume of more than a sixth of the main block. In practice, this meant that a tower could not be built more than around 20 stories tall in a typical Chicago skyscraper development.\n\nInitially, Chicago still preferred \"palazzo\"-styled buildings with large light courts in the centre, because they remained the most profitable designs. The Wrigley Building, built under the 1920 law, demonstrated the effect of two ornamental towers on top of a skyscraper. Under the revised law, the Straus Building and the Pittsfield Building took the \"palazzo\" design and added somewhat stunted towers on top in the early 1920s, producing profitable buildings.\n\nOne of the period's most famous buildings, the Tribune Tower, emerged from a competition held by the Tribune Company in 1922 to celebrate its 75th anniversary. The newspaper was one of the largest in the world and used the competition, in which members of the public were invited to influence the design of the skyscraper, to build a loyal following amongst its readership and generate free publicity. The final design was determined by a competition panel largely made up of the company's appointees, who chose John Howells and Raymond Hood's tower design. The resulting tower was a conservative, Gothic design and controversy about the decision broke out almost immediately: Louis Sullivan and many others criticized Howells and Hood's design as being derivative of the Woolworth Tower. Regardless of its critics, the Tribune received as many as 20,000 visitors to its observation gallery when it opened in 1925. The unbuilt second-place entry in the competition, a more simplified stepped-back design by Eliel Saarinen, also proved highly influential.\n\nThe popularity of the older style then began to wane in favor of a greater emphasis on towers. One common way of building these within Chicago's laws was to build a square main block with a central service core, and then simply place a tower on the top; the more massive the main block, the taller the tower could be. The Trustees System Service Building and the Foreman State National Bank Building form good examples of this approach. Alternatively, the front of the main block could be recessed, as at the Chicago Civic Opera Building or the LaSalle-Wacker Building, sacrificing volume but producing the visual effect of two high wings flanking a very tall tower. The distinctive New York \"setback\" style was not adopted in Chicago, the only example of this style being the Palmolive Building on North Michigan Avenue.\n\nThe boom in skyscraper construction began to falter following the Wall Street Crash of 1929, with fast economic growth giving way to the slump years of the Great Depression: construction of all sorts slowed considerably. The real estate market that lay behind new skyscraper builds collapsed, and the value of many properties dropped dramatically: the American Insurance Union tower cost $8 million ($1.2 billion in 2010 terms) to build in 1927, but was valued at only $3.5 million ($900 million) by 1933; the Bank of Manhattan Trust Building defaulted on its debts in 1935 and was put on sale for just $1.2 million ($240 m). The major bond house of S.W. Straus, behind many successful developments, defaulted on the $214 million ($47 billion) in bonds held by 60,000 investors; the Van Swerigan developer brothers went bankrupt. Vacancy rates began to increase as the recession bit, increasing from only one percent in central New York in early 1920 to 17 percent in 1931 and 25 percent by 1934.\n\nIn the face of the recession, some skyscrapers projects were canceled or scaled back. Plans by the Metropolitan Life company to build a 100-story skyscraper alongside their existing tower had been put forward in 1929, but were shelved in the face of the recession and public criticism of such expenditure in the economic climate. Instead the first phase of the project, known as the North Building ended up only 32 stories high in 1932 and the building, even at this height, was only fully completed in 1950. In many other cases, projects which had already been commissioned continued through to completion. This resulted in of new office space being added to New York between 1931–34 even after the start of the recession, adding to the problem of underoccupancy. Some of these buildings, however, became iconic structures, pushing the limits of skyscraper height sharply upwards.\n\nThe Chrysler Building was completed in 1930, just after the Depression began to affect the industry. Architect William Van Alen competed with the designers of the Bank of Manhattan Trust Building to produce the tallest building in the world and famously erected the Chrysler spire in a last-minute, secret move in order to acquire the title for his own 69-story, tall building. The exterior was built of white and grey brick, but metal was used extensively to ornament it further, including gargoyles, nickel-chrome eagles' heads, and a winged helmet of Mercury. The entrance used black granite to contrast with the nickel-chrome windows, and the foyer gave way to red marble and a mural ceiling. The design of each part of the building was individualist and distinct, with even each elevator different in design. A Chrysler showroom was placed on the first floor of the building and an observation deck and the exclusive \"Cloud Club\" on the upper levels.\nThe Empire State Building originated from a 1928 project to redevelop the Waldorf-Astoria Hotel into a 50-story mixed-use development; the purchase of the site for $14 million ($2.1 billion) set a record in New York for the year. John Raskob and Pierre du Pont entered into the project as financial backers, and concluded that the project would be more profitable if the site was used to build an extremely tall 80-story skyscraper instead. Although revised financial estimates suggested that the height should be cut back, the caché of having the world's tallest building was considerable, and instead an additional five stories were added to ensure the building, at , would be slightly taller than the Chrysler building. An observation deck was built to attract tourists, which proved to be a valuable source of revenue. The limestone, granite, and aluminium skyscraper was specially designed to be easily erected, with standardized paneling and structural fittings, and was completed in only 18 months, opening in 1931. Due to the recession, however, it was only 25 percent occupied throughout the 1930s and ran at a loss; critics dubbed it the \"Empty State Building\".\n\nThe Rockefeller Center had originally been intended by John D. Rockefeller, Jr. to be a new location for the Metropolitan Opera House, but the Stock Market Crash brought an end to the scheme. Rockefeller decided to develop a large office center instead, taking advantage of the low construction costs during the recession. At the center of the development was the RCA Building, heavily influenced by architect Raymond Hood. The long, slab-like RCA had two distinct axes – from one side, it appeared to be a narrow tower, from the other it rose like a sheer wall. Not only was the structure highly distinctive, but it was also economically very effective. The design maximized the available light to the offices and eliminated any darker internal rooms, as the core of the building was entirely taken up with elevators and other engineering services. It took until 1940 before all of the building had been filled by tenants.\n\nIn Chicago, the final pre-war skyscraper was built in 1934. The Field Building was commissioned during the final boom years of the inter-war economy, and the Home Insurance Building – the first skyscraper, built in 1884 – was demolished to make way for the \"wing and tower\"-style development.\n\nPublic interest in skyscrapers increased during the 1920s, particularly after the Tribune competition. The competition drawings were circulated on display, with 25,000 people coming to see them in Chicago in just one month. In the aftermath, images of skyscrapers flourished across American culture, commencing what historian Merrill Schleier dubbed a \"skyscraper mania\". The \"Titan City\" exhibition in 1925 celebrated existing skyscraper and featured futuristic murals by Harvey Corbett and Hugh Ferriss, depicting the skyscraper at the heart of the \"City of the Future\". Charles Sheeler and Paul Strand's 1921 short film \"Manhatta\" explored the form, ending with a sunset shot from the top of a skyscraper. Authors such as Janet Flanner, John Dos Passos and Mary Borden wrote novels with skyscrapers as important motifs or settings. John Carpenter produced a ballet on the theme. Paul Frankl designed a range of popular \"skyscraper furniture\".\n\nMuch of this commentary was positive, reflecting an optimism about technology and the direction of urban life in general. Skyscrapers were seen as an expression of rational engineering, the perfect buildings for mankind to live in, as celebrated in artist Louis Lozowick's lithographs. Some other proponents of skyscrapers likened them to medieval cathedrals, symbols of the modern age. Poems depicted skyscrapers as objects of sublime, rational beauty, Ferris describing them as \"buildings like crystals, walls of translucent glass, sheer glass blocks sheathing a steel grill\". At Chicago's Century of Progress exposition in 1933, skyscrapers and technology were portrayed as a solution for America's current and future problems. The French-Swiss architect Le Corbusier acclaimed New York in 1935 as being \"overwhelming, amazing, exciting, violently alive\", but went on to complain that there were still too few skyscrapers, and those that had been built were not yet tall enough. Lewis Hine, employed to record the building of the Empire State Building, portrayed the skyscraper construction teams as courageous heroes, creating a genre of photography that continued up until 1941.\nTheir critics expressed concerns about the effect of modern technology and urban living on the human condition, arguing that skyscrapers generated pollution and noise, and imposed a regimented and dehumanising lifestyle on the people that worked in them. Social commentator Lewis Mumford reflected the concerns of many in his critiques entitled \"Is the Skyscraper Tolerable?\" and \"The Intolerable City\". Political theorist Stefan Hirsch condemned the buildings as \"bandages covering the sky, stifling our breath\". Inventor Thomas Edison expressed fears that an uncontrolled expansion of skyscrapers would result in overcrowding and disaster. Artist Howard Cook's engravings critiqued the oppressive character of the new skyscrapers as they loomed over the traditional city. Berenice Abbott's photographic studies of New York in the 1930s explored the complex theme of urban change and the effect of skyscrapers on the older ways of life in the city, echoing Steigler's work in the first decade of the century.\n\nHollywood used skyscrapers extensively in popular films. \"The Skyscrapers of New York\" in 1906 became the first of many, and in the 1920s Harold Lloyd produced his five \"skyscraper films\", most prominently \"Safety Last!\", in which the hero dangles from a clock on the side of a Los Angeles building. In these early silent movies, skyscrapers were closely associated with masculine identity; Lloyd's climbing of skyscrapers closely associated with his characters' transformation from young men into mature adults, and the winning of the heroine. The 1933 film \"King Kong\" included another iconic use of the early skyscraper in its final scenes, as the giant ape scaled the Empire State Building shortly before his death; the scene can be interpreted as contrasting natural instinct with the insensitive rationality of the modern building and wider New York.\n\nSkyscraper development paused during the years of the Second World War. Once development began again in the 1950s and 1960s, the skyscraper entered a different phase of development, usually called the international or modern period. Some experimental designs in this style had been built in the U.S. using European architectural concepts in the early 1930s, most notably the Philadelphia Savings Fund Society and McGraw-Hill Buildings. Post-war, their modernist themes were used in a new generation of skyscrapers. These stood alone on individual plots in the fashion of the Rockefeller Centre's RCA building, rather than as part of a row of buildings, forming huge slabs and towers featuring huge glass façades, breaking with earlier skyscraper traditions. Inside, new technologies such as fluorescent lighting and widespread air-conditioning meant that many older architectural features such as light courts and operable windows were unnecessary.\n\nThe trend had substantial implications for many early skyscrapers. Some were redesigned to fit new tastes. Much of the ornate detail was removed from the Metropolitan Life Tower in the 1960s, for example, to fit contemporary, plainer fashions. Many older skyscrapers could not be adapted, however, as they lacked the physical depth to build larger modern offices or the space for new service facilities. Some were demolished to make way for new, larger structures. Amongst these was the Singer Tower, demolished in 1968 and replaced by the international style United States Steel Building. In the 21st century, buildings such as 90 West Street have been renovated as upmarket housing, partially because of their extensive windows and access to sunlight.\n\nCritical discussion of early skyscrapers began from the 1880s onwards in the architectural community and continued across a growing cultural and academic community in the inter-war period. In 1930, the terms the Chicago \"school\" and \"movement\" began to be used for the first time, popularised by the 1940s academics Sigfried Giedion and Carl Condit as a label for the early Chicago architects. They regarded skyscrapers as the early forerunners of modernism and as marking a clear break with earlier architectural forms in the U.S. This interpretation of Chicago's skyscrapers was later challenged by Robert Bruegmann and Daniel Bluestone, who argued that it underplayed the links of the movement to pre-existing Chicago culture. During the 1980s and 1990s, analysis of the early skyscrapers increasingly shifted away from the architects and architecture of the individual buildings to considerations of skyscrapers' roles in the wider urban context. Histories stressing the social, economic, and cultural dimension of skyscrapers began to be produced, with New York's skyscrapers receiving greater attention than before. The Skyscraper Museum – the first to address the theme – was founded in New York in 1997 by historian Carol Willis to preserve the history of the skyscraper.\n\n\nBibliography\n\n"}
{"id": "55611173", "url": "https://en.wikipedia.org/wiki?curid=55611173", "title": "Edward MacDonald", "text": "Edward MacDonald\n\nEdward MacDonald (Dr. Ed) is the Chair of the Faculty of Arts at the University of Prince Edward Island. He is an Associate Professor of history, teaching about Canadian political history, Atlantic Canada and Prince Edward Island.\n\n\n"}
{"id": "35926178", "url": "https://en.wikipedia.org/wiki?curid=35926178", "title": "Ernest William Hawkes", "text": "Ernest William Hawkes\n\nErnest William Hawkes (July 19, 1883 – March 13, 1957) was an American anthropologist best known for his work studying the indigenous peoples of Alaska and northern Canada. A native of Ashfield, Massachusetts, Hawkes was the brother of the well-known \"blind naturist\" Clarence Hawkes. E. W. Hawkes studied at Dakota Wesleyan University (1909) and Pennsylvania University (1913, 1915).\n\nOver the course of multiple trips to Alaska and northern Canada, Hawkes gathered data for his books. His 1914 publication \"Dance Festivals of the Alaskan Eskimo\" was based on the three years Hawkes spent in the Bering Strait District, including on the Diomede Islands and at St. Michael. It was while stationed at St. Michael as a government teacher over the winter of 1911-1912 that Hawkes observed the traditional Inuit \"Messenger Feast\", which he recounted in his 1913 \"Inviting In\". His 1916 \"The Labrador Eskimo\" was based on his experiences in summer 1914 with the Geological Survey of Canada in the Hudson Bay area.\n\nHawkes held a variety of university fellowships in Anthropology, including Columbia University (1913–1914), Harrison College (1914–1916), and later Glendale Community College in Glendale, California.\n\nHe died in Los Angeles in 1957.\n\n\n"}
{"id": "84591", "url": "https://en.wikipedia.org/wiki?curid=84591", "title": "Ernst Moritz Arndt", "text": "Ernst Moritz Arndt\n\nErnst Moritz Arndt (26 December 1769 – 29 January 1860) was a German nationalist historian, writer and poet. Early in his life, he fought for the abolition of serfdom, later against Napoleonic dominance over Germany. Arndt had to flee to Sweden for some time due to his anti-French positions. He is one of the main founders of German nationalism and the 19th century movement for German unification. After the Carlsbad Decrees, the forces of the restoration counted him as a demagogue.\n\nArndt played an important role for the early national and liberal Burschenschaft movement and for the unification movement, and his song \"Was ist des Deutschen Vaterland?\" acted as an unofficial German national anthem.\n\nLong after his death, his anti-French propaganda was used again, in both World Wars. This, together with some strongly antisemitic and anti-Polish statements, has led to a highly critical view of Arndt today.\n\nArndt was born at Gross Schoritz (now a part of Garz on the island of Rügen), then in Swedish Pomerania, as the son of a prosperous farmer and emancipated serf of the lord of the district, Count Putbus. His mother came of well-to-do German yeoman stock. In 1787 the family moved to Stralsund, where Arndt was able to attend the academy. After an interval of private study he went in 1791 to the University of Greifswald as a student of theology and history, and in 1793 moved to Jena, where he came under the influence of the German idealist philosopher Gottlieb Fichte.\n\nAfter the completion of his university studies he returned home, and for two years was a private tutor in the family of Ludwig Koscgarten (1758–1818), pastor of Wittow on Rügen, and having qualified for the ministry as a candidate of theology, he assisted in church services. At the age of 28 he renounced the ministry, and for 18 months led a life of traveling, visiting Austria, Hungary, Italy, France and Belgium. Turning homewards up the river Rhine, he was moved by the sight of the ruined castles along its banks to intense bitterness against France. The impressions of this journey he later described in \"Travels in parts of Germany, Hungary, Italy and France in 1798 and 1799\".\n\nIn 1800 he taught at the University of Greifswald as an independent lecturer (\"privatdocent\") in history, and the same year published \"Über die Freiheit der alten Republiken\". \"Germanien und Europa\" appeared in 1803, a \"fragmentary outburst,\" as he himself called it, on his views on French aggression. This was followed by one of his most influential books, \"Geschichte der Leibeigenschaft in Pommern und Rügen\" (Berlin, 1803), a history of serfdom in Pomerania and on Rügen, which was so convincing an indictment that King Gustav IV Adolf of Sweden in 1806 abolished serfdom.\n\nArndt had meanwhile risen from \"privatdocent\" to extraordinary professor, and in 1806 was appointed to the chair of history at the university. In this year he published the first part of his \"Geist der Zeit\", in which he flung down the gauntlet to Napoleon and called on countrymen to rise and shake off the French yoke. So great was the excitement it produced that Arndt was compelled to take refuge in Sweden to escape the vengeance of Napoleon.\nSettling in Stockholm, he obtained government employment, and devoted himself to the great cause which was nearest his heart. In pamphlets, poems and songs, he communicated his enthusiasm for German independence to his countrymen. Schill's heroic death at Stralsund prompted him to return to Germany, and in disguise he reached Berlin in December.\n\nIn 1810 he returned to Greifswald, but only for a few months. He again set out on his adventurous travels, lived in close contact, with notable men of his time, such as Gebhard Leberecht von Blücher, August von Gneisenau and Heinrich Friedrich Karl Stein, and in 1812 was summoned by the last named to St Petersburg to assist in the organization of the final struggle against France. Meanwhile, pamphlet after pamphlet, and his stirring patriotic songs, such as \"Was ist des Deutschen Vaterland?\", \"Der Gott, der Eisen wachsen ließ,\" and \"Was blasen Trompeten?\" became widely popular.\nWhen, after the peace, the University of Bonn was founded in 1818, Arndt was appointed to teach from his \"Geist der Zeit\", in which he criticized the particularist policies of the German principalities. The boldness of his demands for reform offended the Prussian government, and in the summer in 1819 he was arrested and his papers confiscated.\nAlthough speedily liberated, he was in the following year, at the behest of the Central Commission of Investigation at Mainz – established in accordance with the reactionary Carlsbad Decrees – arraigned before a special tribunal. Although not found guilty, he was forbidden to exercise the functions of his professorship, although he was allowed to retain the stipend. The next 20 years he passed in retirement and literary activity.\n\nIn 1840 he was reinstated in his professorship, and in 1841 was chosen rector of the university. The revolutionary outbreak of 1848 rekindled in the venerable patriot his old hopes and energies, and he took seat as one of the deputies to the National Assembly at Frankfurt. He participated in the deputation that offered the Imperial crown to Frederick William IV, and was indignant at the king's refusal to accept it, so he retired from public life.\nHe continued to lecture and to write with freshness and vigor, and on his 90th birthday received from all parts of Germany good wishes and tokens of affection. He died at Bonn in January 1860, and was buried in Bonn's \"Alter Friedhof\". There are monuments to his memory at Schoritz on Rügen, at the University of Greifswald, and in Bonn.\n\nArndt was married twice, first in 1800, his wife dying in the following year; a second time in 1817. His youngest son drowned in the Rhine in 1834.\n\nOriginally a supporter of the ideas of the French revolution, Arndt dissociated himself from them when the Reign of Terror of the Jacobins became apparent. When Napoleon began to conquer Europe, this renunciation was transformed into visceral dislike.\n\nLike Fichte and Jahn, Arndt began to envision the German nation as a society of ethnic homogeneity, drawing on the history of the German people, especially in the Middle Ages. His writings lack a specific political program, but instead cite external enemies. While \"freedom\" is often mentioned, the freedom Arndt envisioned was not that of a pluralistic society, but rather of a romanticized national community. The French are denigrated as weakened, womanish and morally depraved, while supposed German virtues are extolled.\n\nThese ideas led Arndt to generate anti-French propaganda during the Napoleonic conquest of the German states:\nArndt also was prejudiced against Poles and other Slavs, and published an anti-Polish pamphlet in 1831 in which he castigated Polish \"barbarity and wildness\". During the liberal Revolution of 1848, when the issue of reviving the Polish state was raised in Frankfurt, Arndt declared that \"tribes\" of Slavs and Wends \"have never done or been able to do anything lasting with respect to state, science, or art,\" and concluded: \"At the outset I assert with world history that pronounces judgment [that] the Poles and the whole Slavonic tribe are inferior to Germans.\"\n\nHe also warned of close contact with Judaism. He warned of the \"thousands [of Jews] which by the Russian tyranny will now come upon us even more abounding from Poland\" – \"the impure flood from the East\". Moreover, he warned of a Jewish intellectual conspiracy, claiming that Jews had \"usurped\" half of literature.\n\nArndt paired his antisemitism with his anti-French views, calling the French \"the Jewish people\" (\"das Judenvolk\"), or \"refined bad Jews\" (\"verfeinerte schlechte Juden\"). In 1815 he wrote of the French: \"Jews... I call them again, not only for their Jewish lists and their penny-pinching avarice, but even more because of their Jew-like sticking together.\"\n\nArndt's lyric poems are not all confined to politics. Many among the \"Gedichte\" are religious pieces.\nThis is a selection of his best-known poems and songs:\n\n\n\n\n\n\n"}
{"id": "3245019", "url": "https://en.wikipedia.org/wiki?curid=3245019", "title": "February 2003 Saddam Hussein interview", "text": "February 2003 Saddam Hussein interview\n\nThe Saddam interview refers to a famous television interview that occurred between President of Iraq Saddam Hussein and American news anchor Dan Rather on February 24, 2003, very shortly before the 2003 Invasion of Iraq. The interview was aired both in the United States and on all three Iraqi television networks. British politician Tony Benn had also interviewed Saddam earlier that month.\n\nRather and \"CBS Evening News\" executive producer Jim Murphy were driven around Baghdad for 45 minutes and switched cars on two separate occasions to keep Saddam's position secret. The interview was held at the Republican Palace. Neither Rather nor Murphy were allowed to bring their own tape recorders. Saddam supplied his own translator, and CBS approved the translation of the recording. Rather and Murphy were treated well in the course of the interview, with Saddam offering Rather coffee at one point.\n\nIn the 1980s, Rather had an on-air confrontation with then-president George H. W. Bush over the Iran-Contra Affair. His son, George W. Bush, responded similarly, and declined to give Rather an interview during his presidency. After the Saddam interview, the White House was interested in a rebuttal interview. CBS News would accept President Bush, Vice President Dick Cheney or Secretary of State Colin Powell for the interview, but the White House only offered other officials for the interview, such as Ari Fleischer and Dan Bartlett. The CBS network deemed these individuals inappropriate for the broadcast, and an American interview was never done.\n\nMemorable lines:\n\nSaddam invited President Bush to a live TV debate, to which Bush declined.\n\nOn one occasion Saddam interrupted his translator and corrected his use of the term \"Bush\", instead of \"Mr. Bush\", which Saddam explained was out of respect.\n\nSaddam Hussein floated an idea of holding a live uncensored unprepared debate televised worldwide where both American and Iraqi Presidents sitting in their respective countries would discuss their nation's problems. He also said Dan Rather could moderate the debate. This never happened.\n\n\n\n"}
{"id": "684707", "url": "https://en.wikipedia.org/wiki?curid=684707", "title": "Feng Youlan", "text": "Feng Youlan\n\nFeng Youlan (; 4 December 1895 – 26 November 1990) was a Chinese philosopher who was instrumental for reintroducing the study of Chinese philosophy in the modern era.\n\nFeng Youlan was born on 4 December 1895 in Tanghe County, Nanyang, Henan, China, to a middle-class family. His younger sister was Feng Yuanjun, who would become a famous Chinese writer. He studied philosophy in the China Public School in Shanghai, between 1912–1915, a preparatory school for college, then studied in Chunghua University, Wuhan (later merged into Central China Normal University) and Peking University between 1915 and 1918, where he was able to study Western philosophy and logic as well as Chinese philosophy.\n\nUpon his graduation in 1918, he traveled to the United States in 1919, where he studied at Columbia University on the Boxer Indemnity Scholarship Program. There he met, among many philosophers who were to influence his thought and career, John Dewey, the pragmatist, who became his teacher. Feng gained his PhD from Columbia in 1923. His PhD thesis was titled \"A Comparative Study of Life Ideals\".\n\nHe went on to teach at Chinese universities including Jinan University, Yenching University, and Tsinghua University in Beijing. From 1934 to 1938 (and again from 1946 to 1949) he was Chair of the Department of Philosophy at Tsinghua. It was while at Tsinghua that Feng published what was to be his best-known and most influential work, his \"History of Chinese Philosophy\" (1934, in two volumes). In it he presented and examined the history of Chinese philosophy from a viewpoint which was very much influenced by the Western philosophical fashions prevalent at the time, which resulted in what Peter J. King of Oxford describes as a distinctly positivist tinge to most of the philosophers he described. Nevertheless, the book became the standard work in its field, and had a huge effect in reigniting an interest in Chinese thought.\n\nIn 1935 Feng, on his way to a conference in Prague, stopped briefly in the Soviet Union and was impressed with the radical social changes and cultural ferment. His speeches extolling the utopian possibilities of communism, although also describing the mistakes he saw, drew attention from Chiang Kai-sheks's police. Feng was arrested and spent a short time in jail, but soon became a firm supporter of the government and its resistance to Japan. During the Sino-Japanese War he published works which supported the New Life Movement for revitalizing Confucian values.\n\nIn 1939, Feng brought out his \"Xin Lixue\" (\"New Rational Philosophy\", or \"Neo-Lixue\"). Lixue was a philosophical position of a small group of twelfth-century neo-Confucianists (including Cheng Hao, Cheng Yi, and Zhu Xi); Feng's book took certain metaphysical notions from their thought and from taoism (such as li and tao), analyzed and developed them in ways that owed much to the Western philosophical tradition, and produced a rationalistic neo-Confucian metaphysics. He also developed, in the same way, an account of the nature of morality and of the structure of human moral development.\n\nWhen the Second Sino-Japanese War broke out, the students and staff of Beijing's Tsinghua and Peking Universities, together with Tianjin's Nankai University, fled their campuses. They went first to Hengshan, where they set up the Changsha Temporary University, and then to Kunming, where they set up Southwest Associated University.\nWhen, in 1946 the three Universities returned to Beijing, Feng instead went to the U.S. again, this time to take up a post as visiting professor at the University of Pennsylvania. He spent the year 1948–1949 as a visiting professor at the University of Hawaii. He served as President of Tsinghua University from December 1948 to May 1949 because of Zhang Dongsun's refusal (it was known as National Tsinghua University until January 1949).\n\nWhile he was at Pennsylvania, news from China made it clear that the communists were on their way to seizing power. Feng's friends tried to persuade him to stay, but he was determined to return; his political views were broadly socialist, and he thus felt optimistic about China's future under its new government.\n\nOnce back home, Feng began to study Marxist–Leninist thought, but he soon found that the political situation fell short of his hopes; by the mid-1950s his philosophical approach was being attacked by the authorities. He was forced to repudiate much of his earlier work, and to rewrite the rest – including his \"History\" – in order to fit in with the ideas of the Cultural revolution.\n\nDespite all this, Feng refused to leave China, and after enduring much hardship he finally saw a relaxation of censorship, and was able to write with a certain degree of freedom. He died on 26 November 1990 in Beijing.\n\n\n\n\n"}
{"id": "24641191", "url": "https://en.wikipedia.org/wiki?curid=24641191", "title": "Free city (classical antiquity)", "text": "Free city (classical antiquity)\n\nA free city (; ) was a self-governed city during the Hellenistic and Roman Imperial eras. The status was given by the king or emperor, who nevertheless supervised the city's affairs through his \"epistates\" or \"curator\" (Greek: \"epimeletes\") respectively. Several autonomous cities had also the right to issue civic coinage bearing the name of the city.\n\nExamples of free cities include Amphipolis, which after 357 BC remained permanently a free and autonomous city inside the Macedonian kingdom; and probably also Cassandreia and Philippi. \n\nUnder Seleucid rule, numerous cities enjoyed autonomy and issued coins; some of them, like Seleucia and Tarsus continued to be free cities, even after the Roman conquest by Pompey. Nicopolis was also constituted a free city by Augustus, its founder. Thessalonica after the battle of Philippi, was made a free city in 42 BC, when it had sided with the victors. Athens, a free city with its own laws, appealed to Hadrian to devise new laws which he modelled on those given by Draco and Solon.\n\n\"Autonomi\" or rather \"Autonomoi\" was the name given by the Greeks to those states which were governed by their own laws, and were not subject to any foreign power. This name was also given to those cities subject to the Romans, which were permitted to enjoy their own laws, and elect their own magistrates. This permission was regarded as a great privilege, and mark of honour; and it is accordingly found recorded on coins and medals (e.g. Metropolis of the Antiochians autonomous).\n\n\n"}
{"id": "20604724", "url": "https://en.wikipedia.org/wiki?curid=20604724", "title": "Geir Lundestad", "text": "Geir Lundestad\n\nGeir Lundestad (born January 17, 1945) is a Norwegian historian, who until 2014 served as the director of the Norwegian Nobel Institute when Olav Njølstad took over. In this capacity, he also served as the secretary of the Norwegian Nobel Committee. However, he is not a member of the committee itself.\n\nBorn in Sulitjelma, Lundestad studied history at the University of Oslo and University of Tromsø, graduating in 1970 with a cand.philol. degree and in 1976 with a doctorate respectively. From 1974 to 1990, he held various positions as Lecturer and Professor at the University of Tromsø before beginning his positions with the Norwegian Nobel Institute and Committee. Subsequently, he has been associated with the University of Oslo as an Adjunct Professor of International History. Lundestad spent several years in the United States as a research fellow, at Harvard University, from 1978 to 1979 and again in 1983, and at the Woodrow Wilson Center in Washington, D.C., between 1988 and 1989. \n\nHe is a member of the Norwegian Academy of Science and Letters.\n\n\n"}
{"id": "141957", "url": "https://en.wikipedia.org/wiki?curid=141957", "title": "Golden age of American animation", "text": "Golden age of American animation\n\nThe golden age of American animation was a period in the history of U.S. animation that began with the advent of sound cartoons in 1928 and continued until around 1970 when theatrical animated shorts began losing to the new medium of television animation.\n\nMany popular characters emerged from this period, including Bugs Bunny, Mickey Mouse, Daffy Duck, Donald Duck, Goofy, Popeye, Tom and Jerry, Porky Pig, Betty Boop, Woody Woodpecker, Droopy, Mighty Mouse, Mr. Magoo, Pink Panther, the Fox and the Crow, George and Junior, Wile E. Coyote and the Road Runner, Barney Bear, and the first animated adaptation of Superman, Casper and Little Lulu, among others.\n\nFeature length animation also began during this period, most notably with Walt Disney's first films: \"Snow White and the Seven Dwarfs\", \"Pinocchio\", \"Fantasia\", \"Dumbo\" and \"Bambi\". Also began animation on television with the first animated series from 1949 to the early 1960s.\n\nAfter battleing Max Fleischer for 1st cartoon with sound (Walt lost) he found him self returning to Kansas City, Missouri from World War I, Walt Disney decided to become a newspaper cartoonist drawing political caricatures and comic strips. However nobody would hire Disney, so his older brother Roy, who was working as a banker at the time, got him a job at the \"Pesmen-Rubin Art Studio\" where he created advertisements for newspapers, magazines, and movie theaters. Here he met fellow cartoonist Ub Iwerks, the two quickly became friends and in January 1920, when their time at the studio expired they decided to open up their own advertising agency together called Iwerks-Disney Commercial Artists. The business however got off to a rough start and Walt temporarily left for the \"Kansas City Film and Ad Co.\" to raise money for the fleeting company and Iwerks soon followed as he was unable to run the business alone.\n\nWhile working here he made commercials for local theaters using crude cut-out animation. Disney became fascinated by the art and decided to become an animator. He then borrowed a camera from work and rented a book from the local library called \"Animated Cartoons: How They Are Made, Their Origin and Development\" by Edwin G. Lutz and decided that cel animation would produce better quality and decided to open up his own animation studio. Disney then teamed up with Fred Harman and made their first film, \"The Little Artist\" which was nothing more than an artist (Disney) taking a cigarette break at his work desk. Harman soon dropped out of the venture, but Disney was able to strike a deal with local theater owner Frank L. Newman and animated a cartoon all by himself entitled \"Newman Laugh-O-Grams\" screened in roughly February 1921. Walt then quit his job at the film and ad company and incorporated Laugh-O-Gram Films in May 1922, and hired former advertising colleagues as unpaid \"students\" of animation including Ub Iwerks and Fred Harman's brother, Hugh Harman.\n\nThroughout 1922 the company produced a series of \"modernized\" adaptations of fairy tales including \"Little Red Riding Hood\", \"The Four Musicians of Bremen\", \"Jack and the Beanstalk\", \"Jack the Giant Killer\", \"Goldielocks and the Three Bears\", \"Puss in Boots\", \"Cinderella\" and \"Tommy Tucker's Tooth\", the latter being mostly a live-action film about dental hygiene. None of these films turned a profit. The last film made by the company was a short called \"Alice's Wonderland\". Loosely inspired by Lewis Carroll's \"Alice's Adventures in Wonderland\"; the short featured a live-action five-year-old girl named Alice (Virginia Davis) who had adventures in a fully animated world. The film was never fully complete however as the studio went bankrupt in the summer of 1923.\n\nUpon the closure of Laugh-O-Grams, Walt worked as a freelance filmmaker before selling his camera for a one-way ticket to Los Angeles. Once arriving he moved in with his Uncle Robert and his brother Roy, who was recovering at a nearby government hospital from tuberculosis he had suffered during the war. After failing to get a job as a director of live-action films he sent the unfinished \"Alice's Wonderland\" reel to short-subjects distributor Margaret J. Winkler of \"Winkler Pictures\" in New York. Winkler was distributing both the Felix the Cat and Out of the Inkwell cartoons at the time, but the Fleischer brothers were about to leave to set up their own distribution company, \"Red Seal Films\", and Felix producer Pat Sullivan was constantly fighting with Winkler; therefore Winkler agreed to distribute Disney's \"Alice Comedies\" as sort of an insurance policy.\n\nOnce Walt received the notice on October 15, he convinced Roy to leave the hospital and help him set-up his business. The next day, on October 16, 1923, Disney Bros. Cartoon Studio opened its doors at a small rented office two blocks away from his uncle's house with Roy managing business and Walt handling creative affairs. He convinced Virginia Davis's parents which caused the first official \"Alice\" short, \"Alice's Day at Sea\", to be released on January 1, 1924; delayed by eleven days. Ub Iwerks was re-hired in February 1925 and the quality of animation on the \"Alice\" series improved; this prompted Hugh Harman, Rudolf Ising and Carman Maxwell to follow Disney west in June 1925. Around that time, Davis was replaced with Maggie Gay and the cartoons started to focus less on the live-action scenes and more the fully animated scenes, particularly those featuring Alice's pet sidekick Julius, who bore an uncanny resemblance to Felix the Cat. In February 1926, Disney built a larger studio at 2719 Hyperion Avenue and changed the name of the company to Walt Disney Cartoons.\n\nIn November 1923, Winkler married Charles Mintz and handed over the business to him when she fell pregnant a few months later. Mintz was often described as a cold, stern and ruthless chain-smoking tyrant; one employee remembered him as \"a grim-faced man, with a pair of cold eyes glittering behind the pince nez\" and \"never talked to the staff. He looked us over like an admiral surveying a row of stanchions.\" While Winkler had offered gentle critiques and encouragement, Mintz communicated to Disney in a harsh and cruel tone. In 1927, Mintz ordered Disney to stop producing \"Alice Comedies\" due to the costs of combining live-action and animation.\n\nMintz managed to gain a distribution deal with Universal Studios; however it was Mintz—not Disney—that signed the deal. Disney and lead animator Ub Iwerks created Oswald the Lucky Rabbit, who debuted in Trolley Troubles in 1927. The Oswald series was a success and became the first hit for the Disney studio.\n\nIn the spring of 1928, Disney traveled to New York to ask Mintz for a budget increase. His request was harshly denied by Mintz, who pointed out that in the contract Mintz had signed with Universal, it was Universal—not Disney—that owned the rights to the character. Mintz revealed to Disney that he had hired most of his staff away from the studio (except for Ub Iwerks, Les Clark and Wilfred Jackson who refused to leave) and threatened that unless he took a 20 per cent budget decrease, he would drop Disney and continue the Oswald series by himself. Walt refused, and Winkler Pictures dropped its distribution.\n\nWhile Disney was finishing the remaining cartoons for Mintz, Disney and his staff secretly came up with a new cartoon character to replace Oswald—Mickey Mouse.\n\nThe inspiration for Mickey has never been clear. Walt Disney said that he came up with the idea on the train ride back to Los Angeles shortly after the confrontation with Mintz, but other records say that he came up with the idea after he returned to the studio. Walt Disney once said that he was inspired by a pet mouse he once had at the old Laugh-O-Grams studio, but more commonly said that he chose a mouse because a mouse had never been the central character of a cartoon series before. Some animation historians claim that Ub Iwerks created Mickey and he should be the one to receive credit for the creation of Mickey Mouse.\n\nIn 1928, \"Plane Crazy\" became the first entry into the Mickey Mouse series; however, it was not released because of a poor reaction from test screenings and failed to gain a distributor. The second Mickey Mouse cartoon \"The Gallopin' Gaucho\" also failed to gain attention of the audience and a distributor. Disney knew what was missing: sound. Sound film had been captivating audiences since 1927 with \"The Jazz Singer\" and Walt decided that the next cartoon \"Steamboat Willie\" would have sound. \"Steamboat Willie\" was not the first sound cartoon, Max and Dave Fleischer had produced \"Song Car-Tunes\" since 1926. However, they failed to keep the sound synchronized with the animation and the main focus of the cartoons were the bouncing ball sing-a-longs. The \"Song Car-Tunes\" were not a success and some staff members doubted whether a cartoon with sound would be successful. So Disney arranged a special preview screening with the music and sound effects being played live behind stage through a microphone. The \"Steamboat Willie\" test screening was a success and managed to gain a distributor, \"Celebrity Pictures\" chief Pat Powers. However, the first attempt to synchronize the sound with the animation was a disaster with the timing being all wrong. In order to finance the second recording, Walt sold his car. This time he used a click track to keep his musicians on the beat (Disney later learned that it was easier to record the dialogue, music and sound effects first and animate to the sound). Little more than a month before \"Steamboat Willie\"'s premiere, Paul Terry released his sound cartoon \"Dinner Time\"; however it was not a financial success and Walt Disney described it as \"a bunch of racket\".\n\n\"Steamboat Willie\" was released on November 18, 1928, and was a huge success. Disney quickly gained huge dominance in the animation field using sound in his future cartoons by dubbing \"Plane Crazy\", \"The Gallopin' Gaucho\" and the nearly completed \"The Barn Dance\". Mickey Mouse's popularity put the animated character into the ranks of the most popular screen personalities in the world. Disney's biggest competitor, Pat Sullivan with his Felix the Cat, was eclipsed by Mickey's popularity and the studio closed in 1932.\nMerchandising based on Disney cartoons rescued a number of companies from bankruptcy during the depths of the Depression, and Disney took advantage of this popularity to move forward with further innovations in animation.\n\nIn 1929, he launched a new series entitled \"Silly Symphonies\" which was based around music with no recurring characters. However, they did not become as popular as the Mickey Mouse series.\n\nIn 1930, after a falling-out with Powers, Disney switched distributors to Columbia Pictures. However, Ub Iwerks left Disney after an offer from Powers to be in charge of his own studio.\n\nIn 1932, Mickey Mouse had become an international sensation, but the \"Silly Symphonies\" had not. Columbia Pictures backed out of its distribution of the series and Disney was lured to move the \"Silly Symphonies\" to United Artists by a budget increase.\n\nWalt Disney then worked with the Technicolor company to create the first full three-strip color cartoon, \"Flowers and Trees\". It was a huge success and also became the first cartoon to win an Academy Award. Shortly afterward, Disney negotiated an exclusive, but temporary deal with Technicolor so only he could use the three-strip process in animated films—no other studio was permitted to use it. However, he withheld making Mickey Mouse in color because he thought that Technicolor might boost the \"Silly Symphonies\" popularity.\n\nBy 1932 Walt Disney had realized the success of animated films depended upon telling emotionally gripping stories that would grab the audience and not let go. This realization led to an important innovation around 1932 and 1933: a \"story department,\" separate from the animators, with storyboard artists who would be dedicated to working on a \"story development\" phase of the production pipeline.\nIn turn, Disney's continued emphasis on story development and characterization resulted in another hit in 1933: \"Three Little Pigs\", which is seen as the first cartoon in which multiple characters displayed unique, individual personalities and is still considered to be the most successful animated short of all time, and also featured the hit song that became the anthem in fighting the Great Depression: \"Who's Afraid of the Big Bad Wolf\".\n\nIn the Mickey Mouse series, he continued to add personality to his characters; this resulted in the creation of new characters such as Pluto with The Chain Gang in 1930, Goofy with Mickey's Revue in 1932 and Donald Duck in 1934 with \"The Wise Little Hen\" (under the \"Silly Symphony\" series). When Disney's contract with Technicolor expired, the Mickey Mouse series was moved into Technicolor starting with The Band Concert in 1935. In addition, Mickey was partially redesigned for Technicolor later that year.\n\nIn 1937, Disney invented the multiplane camera, which gave an illusion of depth to the animated world. He first used this on the Silly Symphony cartoon \"The Old Mill\". Much of Disney's work was heavily influenced by European stories and myths, and the work of illustrators such as Doré and Busch.\n\nAlso in 1937, Disney changed distributors to RKO Radio Pictures and remained under distribution until the early 1950s.\n\nIn 1937, Walt Disney produced \"Snow White and the Seven Dwarfs\", the first American feature-length animated film. This was the culmination of four years of effort by Disney studios. Walt Disney was convinced that short cartoons would not keep his studio profitable in the long run, so he took what was seen as an enormous gamble. The critics predicted that \"Snow White\" would result in financial ruin for the studio. They said that the colors would be too bright for the audience and they would get sick of the gags and leave. However, the critics were proven wrong. \"Snow White\" was a worldwide box office success, and was universally acclaimed as a landmark in the development of animation as a serious art form.\n\nAfter the success of \"Snow White\", Disney went on to produce \"Pinocchio\", which was released in 1940. It was considered a stunning achievement both technically and artistically, costing twice as much as \"Snow White\". However \"Pinocchio\" was not a financial success, since World War II (which began in 1939) had cut off 40% of Disney's foreign release market and although it was a moderate success in the United States, the domestic gross alone was not enough to make back its revenue. However, the film did receive very positive reviews and has made millions from subsequent re-releases. Later that year, Disney produced \"Fantasia\". It originally started with the Mickey Mouse cartoon \"The Sorcerer's Apprentice\" in an attempt to recapture Mickey's popularity, which had sharply declined to Max Fleischer's \"Popeye\" and Disney's Donald Duck. In the \"Sorcerer's Apprentice\", Mickey Mouse was redesigned by Fred Moore. This redesign of Mickey is still in use today. The short featured no dialogue, only music which was conducted by Leopold Stokowski. When the budget for the short grew very expensive, Stokowski suggested to Disney that it could be a feature film with other pieces of classical music matched to animation. Disney agreed and production started. \"Fantasia\" would also become the first commercial film to be released in stereophonic sound. However, like \"Pinocchio\", \"Fantasia\" was not a financial success. \"Fantasia\" was also the first Disney film not to be received well, receiving mixed reviews from the critics. It was looked down upon by music critics and audiences, who felt that Walt Disney was striving for something beyond his reach by trying to introduce mainstream animation to abstract art, classical music, and \"elite\" subjects. However, the film would be reevaluated in later years and considered a significant achievement in the art of animation.\n\nIn 1941 in order to compensate for the relative poor box office of \"Pinocchio\" and \"Fantasia\", Disney produced a low-budget feature film, \"Dumbo\". Dumbo was a major hit and today is one of the most critically acclaimed animated movies ever made. Just a few days after rough animation was complete on \"Dumbo\", the Disney animators' strike broke out. This was caused by the Screen Cartoonists' Guild (which had been formed in 1938), who severed many ties between Walt Disney and his staff, while encouraging many members of the Disney studio to leave and seek greener pastures. Later that year, \"Dumbo\" became a big success, the first time since \"Snow White\". The critically acclaimed film brought in much-needed revenue and kept the studio afloat. A few months after \"Dumbo\" was released, the United States entered the war after Pearl Harbor was attacked. This caused the mobilization of all movie studios (including their cartoon divisions) to produce propaganda material to bolster public confidence and encourage support for the war effort. The war (along with the strike) shook Walt Disney's empire, as the US Army had seized Disney's studio as soon as the US entered World War II in December 1941. Due to this, Disney put the feature films \"Alice in Wonderland\", \"Peter Pan\", \"Wind in the Willows\", \"Song of the South\", \"Mickey and the Beanstalk\" and \"Bongo\" on hold until the war was over. The only feature film that was allowed to continue production was \"Bambi\", which was released in 1942. \"Bambi\" was ground-breaking in terms of animating animals realistically. However, due to the war \"Bambi\" failed at the box-office and received mixed reviews from the critics. This was to be short lived as it grossed a considerable amount of money in the 1947 re-release.\n\nDisney was now fully committed to the war and contributed by producing propaganda shorts and a feature film entitled \"Victory Through Air Power\". \"Victory Through Air Power\" did poor box office and the studio lost around $500,000 as a result. The required propaganda cartoon shorts were also not as popular as Disney's regular shorts, and by the time the Army ended its stay at Disney Studios when the war ended in 1945, Disney struggled to restart his studio, and had a low amount of cash on hand.\n\nFurther Disney feature films of the 1940s were modestly budgeted collections of animated short segments put together to make a feature film. These began with \"Saludos Amigos\" in 1942 and continued during the war with \"The Three Caballeros\" in 1944 and after the war with \"Make Mine Music\" in 1946, \"Fun and Fancy Free\" in 1947, \"Melody Time\" in 1948 and \"The Adventures of Ichabod and Mr. Toad\" in 1949. For the feature films \"Mickey and the Beanstalk\", \"Bongo\" and \"Wind in the Willows\", he condensed them into the package films \"Fun and Fancy Free\" and \"The Adventures of Ichabod and Mr. Toad\" since Walt feared that the low-budget animation would not become profitable. The most ambitious Disney film of this period was the 1946 film \"Song of the South\", a film blending live-action and animation which drew criticism in later years for accusations of racial stereotyping.\n\nIn 1950, Disney produced \"Cinderella\". \"Cinderella\" was an enormous success, becoming the highest-grossing film of 1950, and became Disney's most successful film since \"Snow White and the Seven Dwarfs\" and Disney's first single narrative feature film since \"Bambi\".\n\nDisney also started producing full live-action films beginning with \"Treasure Island\" in 1950. He also had been creating nature documentaries since \"Seal Island\" in 1948 and started broadcasting on television with his \"One Hour in Wonderland\" special in 1951. Due to this, Walt Disney was needed on several different units at one time and was spending less time in the animation department. However, he never lost interest in animation and was always present at story-meetings; there they needed him the most. In 1951, he released \"Alice in Wonderland\", a project he had been working on since the late 1930s and had shelved during the war. \"Alice in Wonderland\" was initially moderately successful and received mixed reviews from the critics (though it would be hailed as one of Disney's Greatest classics a few decades later, as well as make millions subsequent theatrical and home video releases). In 1953 he released \"Peter Pan\", which, like \"Alice in Wonderland\", had been in production since the late 1930s, early-1940s and was shelved during the war. However unlike \"Alice\", \"Peter Pan\" was a big success both critically and financially on its first release.\n\nWhen Disney's contract with RKO expired at the end of 1953, instead of renewing it as usual Disney was concerned about the instability of RKO (due to owner Howard Hughes' increasingly erratic control of the studio) and started distributing its own films through its newly created Buena Vista Distribution subsidiary. This allowed a higher budget for shorts and features than the last few years of cartoons made for RKO dictated, which made it possible to make some of the cartoons in the new CinemaScope format. However, the budget per short was nowhere near as high as it had been in the 1940s as Disney had been focusing more on live action, television, and feature animation and less on short animation. In 1953, shortly after the switch from RKO to Buena Vista, Disney released its final Mickey Mouse short, \"The Simple Things\". From there, the studio produced fewer animated shorts by the year until the animated shorts division was eventually closed in 1956. After that, any future short cartoon work was done through the feature animation division until 1969. The last Disney short of the golden age, It's Tough to Be a Bird, was released in 1969.\n\nIn 1955 he created \"Lady and the Tramp\", the first animated film in CinemaScope. Upon building Disneyland in 1955, Walt Disney regained a huge amount of popularity among the public, and turned his focus at producing his most ambitious movie: \"Sleeping Beauty\". Sleeping Beauty was filmed in Super Technirama 70 mm film and in stereophonic sound like \"Fantasia\". \"Sleeping Beauty\" also signaled a change in the style of drawing, with cartoony and angular characters; taking influence from \"United Productions of America\". Although \"Sleeping Beauty\" was the second-highest-grossing film of 1959 (just behind \"Ben-Hur\"), the film went over budget, costing 6 million dollars, and the film failed to make back its expenditure. The studio was in serious debt and had to cut the cost of animation. In 1960, this resulted in Disney switching to xerography, that replaced the traditional hand-inking. The first feature film that used Xerox cels was \"101 Dalmatians\" in 1961. It was a huge success; however, the Xerox resulted in films with a \"sketchier\" look and lacked the quality of the hand-inked films. According to Floyd Norman, who was working at Disney at the time, it felt like the end of an era.\n\nOn December 15, 1966, Walt Disney died of lung cancer. The last two films he was involved in was \"The Jungle Book\" and \"The Many Adventures of Winnie the Pooh\", since one of the shorts \"Winnie the Pooh and the Honey Tree\" was released during his lifetime, and he was involved in the production of \"Winnie the Pooh and the Blustery Day\". The Jungle Book was released a year after his death and \"Winnie the Pooh and the Blustery Day\" released two years later, while \"The Many Adventures of Winnie the Pooh\" was released in 1977. Winnie the Pooh and the Blustery Day also won the 1968 Academy Award for Animated Short Film. After Walt Disney's death, the animated films produced by the Disney company were only moderately successful. The animation department did not fully recover until the late 1980s and early 1990s with the Disney Renaissance.\n\nOne of Disney's main competitors was Max Fleischer, the head of Fleischer Studios, which produced cartoons for Paramount Pictures. Fleischer Studios was a family-owned business, operated by Max Fleischer and his younger brother Dave Fleischer, who supervised the production of the cartoons. The Fleischers scored successful hits with the \"Betty Boop\" cartoons and the \"Popeye the Sailor\" series. Popeye's popularity during the 1930s rivaled Mickey Mouse at times, and Popeye fan clubs sprang up across the country in imitation of Mickey's fan clubs; in 1935, polls showed that Popeye was even more popular than Mickey Mouse. However, during the early 1930s, stricter censorship rules enforced by the new Production Code in 1934 required animation producers to remove risqué humor. The Fleischers in particular had to tone down the content of their Betty Boop cartoons, which waned in popularity afterwards. The Fleischers also had produced a number of \"Color Classics\" cartoons during the 1930s which attempted to emulate Walt Disney's use of color, but the series was not a success.\n\nIn 1934 Max Fleischer became interested in producing an animated feature film shortly after Disney's announcement of \"Snow White\", however Paramount vetoed the idea. In 1936, Fleischer Studios produced the first of three two-reel \"Popeye\" Technicolor features: \"Popeye the Sailor Meets Sindbad the Sailor\" in 1936, \"Popeye the Sailor Meets Ali Baba's Forty Thieves\" in 1937, and \"Aladdin and His Wonderful Lamp\" in 1939.\n\nIn 1938 after Disney's success with \"Snow White and the Seven Dwarfs\", Paramount gave the Fleischers permission to produce a feature film and Fleischer studio relocated itself from New York to Miami, Florida in order to avoid organized unions, which became a threat to the studio after a five-month strike occurred among Fleischer Studio workers in late 1937. Here the Fleischers produced \"Gulliver's Travels\" which was released in 1939. It was a small success and encouraged the Fleischers to produce more.\n\nIn May 1941 the Fleischers gave Paramount full ownership of the studio as collateral to pay off their debts left from the loans they obtained from the studio to make unsuccessful cartoons like \"Stone Age\", \"Gabby\", and \"Color Classics\". However, they still maintained their positions as heads of their studio's production. Under Paramount rule, the Fleischers brought Popeye into the Navy and contributed to the war effort, and would gain more success by beginning a series of spectacular \"Superman\" cartoons (the first of which was nominated for an Oscar) that have become legendary in themselves.\n\nDespite the success Superman gave the studio, a major blow to the studio would occur when the married Dave started having an adulterous affair with the Miami secretary. This led to many disputes between the Fleischer Brothers until Max and Dave were no longer speaking to each other. In 1941 they released \"Mister Bug Goes to Town\", unfortunately it was released a few days before the attack on Pearl Harbor, which caused \"Mister Bug\" to fail at the box-office. Shortly after the film's poor box office, Dave Fleischer, still maintaining his position as co-chief of his studio, had left Fleischer Studios to run Columbia Pictures' Screen Gems cartoons. Due to this, Paramount Pictures expelled Dave and Max Fleischer from their positions as the head of the cartoon studio.\n\nIn a move that remains controversial to the present day (though it has not been closely examined by film historians), Paramount took over the Fleischer studio completely and brought it under the fold of their own studio, renaming it Famous Studios and continuing the work that the Fleischers began. Isadore Sparber, Seymour Kneitel and later Bill Tytla were promoted to directors and Sam Buchwald was promoted to executive producer. Paramount also discontinued the expensive \"Superman\" cartoons in 1943, in favor of adapting Little Lulu to theaters. The departure of the Fleischers had an immediate effect on the studio: the Paramount cartoons of the war years continued to be entertaining and popular and still retained most of the Fleischer style and gloss, however it began losing the popularity and distinct style of animation after WWII ended. \n\nFamous Studios continued to release Popeye shorts as well as creating an anthology short series called Noveltoons. Noveltoons introduced many notable characters such as Casper the Friendly Ghost (based on a children reading book), Little Audrey (who replaced the Little Lulu shorts), Herman and Katnip (A cat and mouse duel similar to Tom and Jerry), Baby Huey and many other lesser known characters. By 1967, Paramount's new owners Gulf+Western shut down the studio.\n\nIn 1929 former Disney animators Hugh Harman and Rudolf Ising made a cartoon entitled \"Bosko, the Talk-Ink Kid\", and tried to sell it to a distributor in 1930. Warner Bros. who had previously tried an unsuccessful attempt to set up a cartoon studio in New York in order to compete with Disney, agreed to distribute the series. Under producer Leon Schlesinger's guide Harman and Ising created \"Looney Tunes\" (the title being variation on Disney's \"Silly Symphonies\") starring their character Bosko. A second Harman-Ising series, \"Merrie Melodies\", followed in 1931. Both series showed the strong influence of the early Disney films.\n\nAfter disputes over money, Harman-Ising parted company with Schlesinger in 1933, taking Bosko with them to work with Metro Goldwyn Mayer. Schlesinger began his own cartoon operation under the new name Leon Schlesinger Productions, hiring Harman-Ising animator Friz Freleng and several others to run the studio. Animator Tom Palmer created a Bosko clone known as Buddy and answered to Disney's use of color in \"Silly Symphonies\" cartoons in 1934, and began making all future \"Merrie Melodies\" cartoons in color. However, since Disney had an exclusive deal with Technicolor, Schlesinger was forced to use Cinecolor and Two Strip Technicolor until 1935 when Disney's contract with Technicolor expired. In 1935, Schlesinger fired Tom Palmer and Buddy was retired.\n\nIn a 1935 \"Merrie Melodie\" directed by Friz Freleng entitled \"I Haven't Got a Hat\" was the first screen appearance of Porky Pig. Also in 1935, Schlesinger hired a new animation director who proceeded to revitalize the studio: Tex Avery. Schlesinger put Avery in charge of the low-budget Looney Tunes in a low run-down old building the animators named \"Termite Terrace\". Under Avery, Porky Pig would replace the Buddy series and become the first Warner Bros. cartoon character to achieve star power. Also at \"Termite Terrace\" animator Bob Clampett redesigned Porky from a fat, chubby pig to a more cute and childlike character.\n\nUnlike the other cartoon producers at the time, Avery had no intention of competing with Disney, but instead brought a new wacky, zany style of animation to the studio that would increase the Warner Bros. cartoons' popularity in the crowded marketplace. This was firmly established in 1937 when Tex Avery directed \"Porky's Duck Hunt\". During production of the short lead animator Bob Clampett elaborated the exit of the Duck character by having him jump up and down on his head, flip around and holler off into the sunset. This created the character of Daffy Duck. After Daffy was created, he would add even more success to Warner Bros. cartoons and replaced Porky Pig as the studio's most popular animated character, and Bob Clampett took over \"Termite Terrace\", while Tex Avery took over the \"Merrie Melodies\" department.\n\nThe 1940 Academy Award-nominated cartoon \"A Wild Hare\" (directed by Avery) marked Bugs Bunny's official debut. Bugs quickly replaced Daffy as the studio's top star. By 1942, Bugs had become the most popular cartoon character. Because of the success of Bugs, Daffy and Porky, the Schlesinger studio now had risen to new heights, and Bugs quickly became the star of the color \"Merrie Melodies\" cartoons, which had previously been used for one-shot character appearances. By 1942 Warners' shorts had now surpassed Disney's in sales and popularity.\n\nFrank Tashlin also worked with Avery in the Merrie Melodies department. He began at Warners in 1933 as an animator but was fired and joined Iwerks in 1934. Tashlin returned to Warners in 1936, taking over direction of Merrie Melodies department. He returned in 1943 directing Porky and Daffy cartoons. He left in 1946 to direct live-action films. Robert McKimson took over his old unit. \"Here today, gone tomorrow. Now you see him, now you don't. That was Frank Tashlin, who would be working at Leon Schlesinger's one day, and, suddenly, gone the next day.\" Clampett was fired that same year and Arthur Davis took over his unit for 3 years before management thought there should only be three directors.\n\nAfter several disputes with Schlesinger (suspended for 4 weeks with no pay), Avery left Warner Bros. in 1941 and went to work at MGM. Avery created Droopy in 1943 and many other characters during his 12-year career at MGM. Schlesinger sold his studio to Warner Bros. in August 1944. Edward Selzer was in turn named the new producer. By this time, Warner cartoons' top directors of the 1940s and 1950s were Friz Freleng, Chuck Jones, and Bob Clampett. Their cartoons are now considered classics of the medium. They directed some of the most beloved animated shorts of all time, including (for Clampett) \"Porky in Wackyland\", \"A Corny Concerto\", \"The Great Piggy Bank Robbery\", \"The Big Snooze\", (for Freleng) \"You Ought to Be in Pictures\", \"Little Red Riding Rabbit\", \"Birds Anonymous\", \"Knighty Knight Bugs\", (for Jones) \"Duck Amuck\", \"Duck Dodgers in the 24½th Century\", \"One Froggy Evening\", \"What's Opera, Doc?\", (for McKimson) \"Walky Talky Hawky\", \"Hillbilly Hare\", \"Devil May Hare\" and \"Stupor Duck\".\n\nRobert McKimson was promoted to director after Frank Tashlin left the studio to direct live-action films. Arthur Davis took over after Clampett was fired by Selzer. Clampett went to work on Beany and Cecil. McKimson created many recurring characters like Foghorn Leghorn, the Barnyard Dawg, Hippety Hopper, Speedy Gonzales, Taz and Sylvester Jr..\n\nAfter more than two decades at the top, Warner Bros. shut down the original \"Termite Terrace\" studio in 1963 and DePatie-Freleng Enterprises assumed production of the shorts, licensed by Warner Bros. After DePatie-Freleng ceased production of \"Looney Tunes\" in 1967, Bill Hendricks was put in charge of production of the newly renamed Warner Bros.-Seven Arts animation studio, and hired veterans such as Alex Lovy and LaVerne Harding from the Walter Lantz studio, Volus Jones and Ed Solomon from Disney, Jaime Diaz who later worked on \"The Fairly OddParents\" as director, and David Hanan, who previously worked on \"Roger Ramjet\". Hendricks brought only three of the original \"Looney Tunes\" veterans to the studio; Ted Bonniscken, Norman McCabe and Bob Givens. The studio's one shot cartoons from this era were critically panned and are widely considered to be the worst in the studio's history: \"Cool Cat\", \"Merlin the Magic Mouse\", \"Chimp and Zee\" and \"Norman Normal\", despite the later gaining a large cult following, were said to be witless, crudely animated as well as having poor writing and design because of the extremely low budgets the crew had to work with by this time. Alex Lovy left the studio in 1968 and Robert McKimson took over. McKimson mostly focused on the recurring characters Alex Lovy had created and two of his own creation, Bunny and Claude. The last of the original \"Looney Tunes\" shorts produced was \"Bugged by a Bee\" and the last \"Merrie Melodies\" short was \"Injun Trouble\", which shares its name with another \"Looney Tunes\" short from 1938. The Warner Bros.-Seven Arts studio finally shut down in 1969. A total of 1,039 Looney Tunes shorts had been created.\n\nA decade later, after the success of the film, \"The Bugs Bunny/Road Runner Movie\", which consisted predominantly of footage from the classic shorts by Jones, a new in-house studio to produce original animation opened its doors in 1980 named Warner Bros. Animation, which exists to this day.\n\nAt first, Mickey was drawn by Disney's long-time partner and friend Ub Iwerks, who was also a technical innovator in cartoons, and drew an average of 600 drawings for Disney on a daily basis; Disney was responsible for the ideas in the cartoons, and Iwerks was responsible for bringing them to life. However, Iwerks left the Disney studio in 1930 to form his own company, which was financially backed by Celebrity Pictures owner Pat Powers. After his departure, Disney eventually found a number of different animators to replace Iwerks. Iwerks would produce three cartoon series during the 1930s: \"Flip the Frog\" and \"Willie Whopper\" for Metro-Goldwyn-Mayer, and the \"ComiColor Cartoons\" for Pat Powers' Celebrity Productions. However, none of these cartoons could come close to matching the success of Disney or Fleischer cartoons, and in 1933, MGM, Iwerks' cartoon distributor since 1930, ended distribution of his cartoons in favor of distributing Harman and Ising cartoons, and Iwerks left after his contract expired in 1934. After his stay with MGM, Iwerks' cartoons were distributed by Celebrity Pictures, and Iwerks would answer to Disney's use of Technicolor and create the Comicolor series, which aired cartoons in two-strip Cinecolor. However, by 1936, the Iwerks Studio began to experience financial setbacks and closed after Pat Powers withdrew financial aid to the studio. Iwerks returned to Disney in 1940, where he worked as the head of the \"special effects development\" division until his death in 1971.\n\nAfter MGM dropped Iwerks, they hired Harman and Ising away from Leon Schlesinger and Warner Bros and appointed them heads of the studio. They began producing \"Bosko\" and \"Happy Harmonies\" cartoons which were emulative of Disney's \"Silly Symphonies\". However they failed to make a success in the theaters, and in 1937 the \"Bosko\" and \"Happy Harmonies\" series were discontinued and MGM replaced Harman and Ising with Fred Quimby. After Quimby took over, he kept a number of Harman and Ising's staff and scouted other animation studios for talent. He created an animated adaptation of the comic book series \"The Katzenjammer Kids\" which he re-titled \"The Captain & The Kids\". \"The Captain & The Kids\" series was unsuccessful. In 1939, however, Quimby gained success after rehiring Harman & Ising. After returning to MGM, Ising created MGM's first successful animated star, Barney Bear and Harman directed his masterpiece \"Peace on Earth\".\n\nIn 1939 William Hanna and Joseph Barbera started a partnership that would last for more than six decades, until Hanna's death in 2001. The duo's first cartoon together was \"Puss Gets the Boot\" (1940), featuring an unnamed mouse's attempts to outwit a housecat named Jasper. Though released without fanfare, the short was financially and critically successful, earning an Academy Award nomination for Best Short Subject (Cartoons) of 1940. On the strength of the Oscar nomination and public demand, Hanna and Barbera set themselves to producing a long-running series of cat-and-mouse cartoons, soon christening the characters \"Tom and Jerry\". \"Puss Gets the Boot\" did not win the 1940 Academy Award for Best Cartoon, but another MGM cartoon, Rudolf Ising's \"The Milky Way\" did, making \"MGM cartoon studio\" the first studio to wrestle the Cartoon Academy Award away from Walt Disney.\n\nAfter appearing in \"Puss Gets the Boot\", Tom and Jerry quickly became the stars of MGM cartoons. With Hanna-Barbera under their belts, \"MGM cartoon studio\" was finally able to compete with Disney in the field of animated cartoons. The shorts were successful at the box office, many licensed products (comic books, toys, etc.) were released to the market, and the series would earn twelve more Academy Award for Short Subjects (Cartoons) nominations, with seven of the Tom and Jerry shorts going on to win the Academy Award: \"The Yankee Doodle Mouse\" (1943), \"Mouse Trouble\" (1944), \"Quiet Please!\" (1945), \"The Cat Concerto\" (1946), \"The Little Orphan\" (1948), \"The Two Mouseketeers\" (1951), and \"Johann Mouse\" (1952). Tom and Jerry was eventually tied with \"Disney's Silly Symphonies\" as the most-awarded theatrical cartoon series. Hanna and Barbera also produced/directed for \"MGM cartoon studio\" half a dozen \"one-shot theatrical shorts\" besides \"Tom and Jerry series\", including \"Gallopin' Gals\" (1940), \"Officer Pooch\" (1941), \"War Dogs\" (1943) and \"Good Will to Men\" (1955). Fred Quimby retired in 1955, with Hanna and Barbera replacing him in charge of the remaining MGM cartoons (including the last seven Tex Avery's \"Droopy episodes\") until 1957, when the studio shut down the H-B unit, ending all the animation productions.\n\nKey to the successes of \"Tom and Jerry\" and other MGM cartoons was the work of Scott Bradley, who scored virtually all of the cartoons for the studio from 1934 to 1957. Bradley's scores made use of both classical and jazz sensibilities. In addition, he often used songs from the scores of MGM's feature films, the most frequent of them being \"The Trolley Song\" from \"Meet Me in St. Louis\" (1944) and \"Sing Before Breakfast\" from \"Broadway Melody of 1936\".\n\nMeanwhile, Tex Avery came to MGM in 1942 and revitalized their cartoon studio with the same spark that had infused the Warner animators. Tex Avery's wild surreal masterpieces of his MGM days set new standards for \"adult\" entertainment in Code-era cartoons. Tex Avery did not like to use recurring characters, but did stay faithful to a character throughout his career at MGM with \"Droopy\", who was created in \"Dumb-Hounded\" in 1943. Tex also created \"Screwy Squirrel\" in 1944, but Tex was less fond of him and discontinued the series after five cartoons. He also created the inspired Of Mice and Men duo \"George and Junior\" in 1946, but only four cartoons were produced. In 1953, Metro-Goldwyn-Mayer closed down the Tex Avery unit. Fred Quimby retired in 1955, with Hanna and Barbera replacing him in charge of the remaining MGM cartoons (including the last seven Tex Avery's \"Droopy episodes\") until 1957, when the studio shut down the H-B unit, ending all the animation productions.\n\nAfter Charles Mintz was fired from Universal he was still in charge of his own cartoon operation producing Krazy Kat cartoons for Columbia Pictures. He also created a new series featuring a boy named Scrappy, created by Dick Huemer in 1930. Scrappy was a big break for Mintz and was also his most successful creation, but his studio would suffer irreparable damage after Dick Huemer was fired from the Mintz Studio in 1933. In 1934 Mintz, like most other animation studios at the time, also attempted to answer Disney's use of Technicolor, and began making color cartoons through the Color Rhapsodies series; the series was originally in either Cinecolor or two-strip Technicolor, but moved to three-strip Technicolor after Disney's contract with Technicolor expired in 1935. However, the series failed to garner attention, and by 1939, Mintz was largely indebted to Columbia Pictures. As a result, Mintz sold his studio to Columbia. Columbia renamed the studio, which Mintz still managed, Screen Gems; Mintz died the following year.\n\nFrank Tashlin and John Hubley, were Disney animators who left during the strike, and obtained jobs at Screen Gems, where Tashlin served as head producer while Hubley acted as director for studio. Tashlin helped Screen Gems gain more success by introducing The Fox and the Crow, Screen Gems' biggest stars. Tashlin maintained his position until Columbia Pictures released him from the studio in favor of Dave Fleischer in 1942. The Screen Gems cartoons were only moderately successful and never gained the artistic talent of Disney, Warner Bros. or MGM. Columbia Pictures closed the studio in 1949 and started looking for a new cartoon production company.\n\nIn 1941 John Hubley left Screen Gems and formed a studio with former Disney animators Stephen Bosustow, David Hilberman, and Zachary Schwartz, who—like Hubley—had left Walt's nest during the animator's strike. The studio Hubley founded was a newer, smaller animation studio that focused on pursuing Hubley's own vision of trying out newer, more abstract and experimental styles of animation. Bosustow, Hilberman, and Schwartz named the new studio as Industrial Film and Poster Service, or IFPS. Artistically, the studio used a style of animation that has come to be known as \"limited animation\". The first short from the newly formed studio was \"Hell-Bent for Election\" (directed by Warners veteran Chuck Jones), a cartoon made for the re-election campaign of Franklin D. Roosevelt. Although this new film was a success, it did not break the boundaries that Hubley and his staffers had hoped. It wasn't until the third short, Bobe Cannon's \"Brotherhood of Man\", that the studio began producing shorts aggressively stylized in contrast to the films of the other studios. Cannon's film even preached a message that, at the time, was looked down upon—racial tolerance. By 1946, the studio was renamed as United Productions of America (UPA), and Hilberman and Schwartz had sold their shares of the studio stock to Bosustow.\n\nIn 1948 UPA also found a home for itself at Columbia Pictures and began producing theatrical cartoons for the general public, instead of just using propaganda and military training themes; UPA also earned itself two Academy Award nominations for new cartoons starring The Fox And The Crow during its first two years in production. From there, the UPA animators began producing a series of cartoons that immediately stood out among the crowded field of mirror-image, copycat cartoons of the other studios. The success of UPA's \"Mr. Magoo\" series made all of the other studios sit up and take notice, and when the UPA short \"Gerald McBoing-Boing\" won an Oscar, the effect on Hollywood was immediate and electrifying. The UPA style was markedly different from everything else being seen on movie screens, and audiences responded to the change that UPA offered from the repetition of usual cat-mouse battles. Mr Magoo would go on to be the studio's most successful cartoon character. However, UPA would suffer a major blow after John Hubley was fired from the studio during the McCarthy Era in 1952, due to suspicions of his having ties to Communism; Steve Bosustow took over, but was not as successful as Hubley, and the studio was eventually sold to Henry Saperstein.\n\nBy 1953 UPA had gained great influence within the industry. The Hollywood cartoon studios gradually moved away from the lush, realistic detail of the 1940s to a more simplistic, less realistic style of animation. By this time, even Disney was attempting to mimic UPA. 1953's \"Melody\" and \"Toot, Whistle, Plunk and Boom\" in particular were experiments in stylization that followed in the footsteps of the newly formed studio.\n\nIn 1959, UPA released \"1001 Arabian Nights\" starring Mr Magoo, and in 1962, UPA released \"Gay Purr-ee\" with the voice talents of Judy Garland. In 1964, UPA decided to abandon animation and simply become a distribution company.\n\nIn 1929 Walter Lantz replaced Charles Mintz as producer of Universal Studios cartoons. Lantz's main character at this time was Oswald the Lucky Rabbit, whose earlier cartoons had been produced by both Walt Disney and Charles Mintz. Lantz also started to experiment with color cartoons, and the first one, called Jolly Little Elves, was released in 1934. In 1935 Lantz made his studio independent from Universal Studios, and Universal Studios was now only the distributors of his cartoons, instead of the direct owners.\n\nIn the 1940s Oswald began to lose popularity. Lantz and his staff worked on several ideas for possible new cartoon characters (among them Meany, Miny, and Moe and Baby-Face Mouse). Eventually one of these characters clicked; his name was Andy Panda, who aired in Technicolor. However successful Andy was, it was not until the character's fifth cartoon, \"Knock Knock\" that a real breakthrough character was introduced. This was none other than Woody Woodpecker, who become Lantz's most successful creation.\n\nWalter Lantz Studio closed at the end of 1948 due to financial problems. It opened again in 1950 with a downsized staff, mainly because Lantz was able to sign a deal with Universal (by this time now known as Universal-International) for more Woody Woodpecker cartoons, starting with 1951's Puny Express. The character would continue to appear in theatrical shorts until 1972, when Lantz finally closed his studio. Luckily for Lantz Woody Woodpecker's survival was lengthened when he started appearing in \"The Woody Woodpecker Show\" from 1957 to 1958, from which it entered syndication until 1966. NBC revived the show twice—in 1970 and 1976, and finally in 1985 Lantz sold all of the Woody Woodpecker shorts to Universal, then part of MCA.\n\nIn 1928, producer Amadee J. Van Beuren formed a partnership with Paul Terry and formed the '\"Aesop's Fables Studio\" for the production of the \"Aesop's Film Fables\" cartoon series. In 1929, Terry left to start his own studio and was replaced by John Foster took over the animation department who renamed the studio Van Beuren Studios.\n\nVan Beuren continued the Aesop's Fables series, and unsuccessfully tried a cartoon adaptation of radio blackface comedians Amos 'n Andy. Other Van Beuren cartoons featured Tom and Jerry (not the cat and mouse, but a Mutt and Jeff-like human duo,) and Otto Soglow's comic strip character The Little King. Frank Tashlin and Joseph Barbera were among animators who worked briefly for the studio during its short life.\n\nIn 1934, as other studios were putting cartoons in Technicolor to answer to Disney's Silly Symphonies cartoon series, Van Beuren Studio abandoned its remaining cartoons and answered Disney's use of Technicolor by creating the Rainbow Parade series, which was all color. However, the series was not a success, and by 1936, RKO Pictures, the owner of the Van Beuren Studio, closed the studio as RKO chose to instead distribute Disney cartoons.\n\nAfter losing his \"Aesop's Film Fables\" series to Van Beuren Studios in 1929, Terry established a new studio called Terrytoons.\n\nTerrytoons produced 26 cartoons a year for E.W. Hammons' Educational Pictures, which in turn supplied short-subject product to the Fox Film Corporation, (later 20th Century-Fox.) Terry's cartoons of the 1930s were mainly black-and-white musical cartoons without recurring characters, except for Farmer Al Falfa, who had appeared in Terry's cartoons since the silent era. Educational foundered in the late 1930s, and Terry signed directly with Fox to distribute his pictures.\n\nThe 1940s brought Terry's most popular and successful characters, Mighty Mouse beginning in 1942, and Heckle and Jeckle, developed by combining what was originally a husband-and-wife pair of mischievous magpies from the 1946 Farmer Al Falfa cartoon \"The Talking Magpies\" with Terry's notion that twin brothers or look-alikes had comic possibilities.\n\nTerry sold his company and its backlog to CBS in 1955 and retired. CBS continued to operate the studio for nearly 15 years afterward; its output divided between theatrical short cartoons and television series, including Tom Terrific, Lariat Sam, and Deputy Dawg.\n\nWhile much of the focus in an animated cartoon is on the visuals, the vocal talents and symphonic scores that accompanied the images were also very important to the cartoons' success. As motion pictures drew audiences away from their radio sets, it also drew talented actors and vocal impressionists into film and animation. Mel Blanc gave voice to most of Warner Bros. more popular characters, including Bugs Bunny, Porky Pig (starting in 1937), and Daffy Duck. Other voices and personalities from vaudeville and radio contributed to the popularity of animated films in the Golden Era. Some of these (generally uncredited) actors included Cliff Edwards, also known as Ukulele Ike, Arthur Q. Bryan, Stan Freberg, Bea Benaderet, Bill Thompson, Grace Stafford, Jim Backus, June Foray, and Daws Butler.\n\nCartoons of this era also included scores played by studio orchestras. Carl Stalling at Schlesinger/Warner Bros. and Scott Bradley at MGM composed numerous cartoon soundtracks, creating original material as well as incorporating familiar classical and popular melodies. Many of the early cartoons, particularly those of Disney's \"Silly Symphonies\" series, were built around classical pieces. These cartoons sometimes featured star characters, but many had simple nature themes.\n\nFor a great part of the history of Hollywood animation, the production of animated films was an exclusive industry that did not branch off very often into other areas. The various animation studios worked almost exclusively on producing animated cartoons and animated titles for movies. Only occasionally was animation used for other aspects of the movie industry. The low-budget \"Superman\" serials of the 1940s used animated sequences of Superman flying and performing super-powered feats which were used in the place of live-action special effects, but this was not a common practice.\n\nThe exclusivity of animation also resulted in the birth of a sister industry that was used almost exclusively for motion picture special effects: stop motion animation. In spite of their similarities, the two genres of stop-motion and hand-drawn animation rarely came together during the Golden Age of Hollywood. Stop-motion animation made a name for itself with the 1933 box-office hit \"King Kong\", where animator Willis O'Brien defined many of the major stop motion techniques used for the next 50 years. The success of \"King Kong\" led to a number of other early special effects films, including \"Mighty Joe Young,\" which was also animated by O'Brien and helped to start the careers of several animators, including Ray Harryhausen, who came into his own in the 1950s. George Pal was the only stop-motion animator to produce a series of stop-motion animated cartoons for theatrical release, the \"Puppetoon\" series for Paramount, some of which were animated by Ray Harryhausen. Pal went on to produce several live-action special effects-laden feature films.\n\nStop motion animation reached the height of its popularity during the 1950s. The exploding popularity of science fiction films led to an exponential development in the field of special effects, and George Pal became the producer of several popular special effects-laden films. Meanwhile, Ray Harryhausen's work on such films as \"Earth vs. the Flying Saucers, The Seventh Voyage of Sinbad\", and \"The Beast from 20,000 Fathoms\" drew in large crowds and encouraged the development of \"realistic\" special effects in films. These effects used many of the same techniques as cel animation, but still the two media did not often come together. Stop motion developed to the point where Douglas Trumbull's effects in \"\" seemed lifelike to an unearthly degree.\n\nHollywood special effects continued to develop in a manner that largely avoided cel animation, though several memorable animated sequences were included in live-action feature films of the era. The most famous of these was a scene during the movie \"Anchors Aweigh\", in which actor Gene Kelly danced with an animated Jerry Mouse (of \"Tom and Jerry\" fame). But except for occasional sequences of this sort, the only real integration of cel animation into live-action films came in the development of animated credit and title sequences. Saul Bass' opening sequences for Alfred Hitchcock's films (including \"Vertigo\", \"North by Northwest\", and \"Psycho\") are highly praised, and inspired several imitators.\n\nThe major Hollywood studios contributed greatly to the war effort, and their cartoon studios pitched in as well with various contributions. At the Fleischer studios, Popeye the Sailor joined the Navy and began fighting Nazis and \"Japs\"; while the Warner Bros. studio produced a series of \"Private Snafu\" instructional film cartoons especially for viewing by enlisted soldiers.\n\nThe 1960s saw some creative sparks in the theatrical film medium, in particular from DePatie–Freleng Enterprises. Their first and most successful project was animating the opening titles for the 1964 film, \"The Pink Panther\", starring Peter Sellers. The film and its animated sequences were so successful that United Artists commissioned the studio to produce a Pink Panther cartoon series. The first short, \"The Pink Phink\", won the Academy Award for Best Animated Short Film of 1964. The studio also produced other successful cartoon series such as \"The Inspector\" and \"The Ant and the Aardvark\".\n\nMeanwhile, Chuck Jones, who had been fired from Warner Bros., moved to MGM to produce thirty-four theatrical \"Tom and Jerry\" cartoons in late 1963. These cartoons were animated in his distinctive style, but they never quite matched the popularity of the Hanna-Barbera originals of the 1940s and 1950s heyday. However, they were more successful than the Gene Deitch \"Tom and Jerry\" shorts, which were produced overseas during 1961 and 1962.\n\nFrom 1964 to 1967 DePatie–Freleng produced \"Looney Tunes\" and \"Merrie Melodies\" shorts under contract with Warner Bros. These cartoons can be recognized easily because they use the modern abstract WB logos instead of the famous bullseye WB shield concentric circles. The studio also subcontracted 11 Road Runner cartoons to Format Films. DePatie–Freleng ceased production of \"Looney Tunes\" and moved to the San Fernando Valley in 1967 to continue production of their \"Pink Panther\" cartoons. In 1967, WB would resume production on their own cartoons, before shutting down the theatrical cartoon department altogether in 1969. In 1981, the studio was purchased by Marvel Comics and was renamed Marvel Productions.\n\nIn 1946 the animation union of the time negotiated a pay increase of 25%, making the cartoons more expensive to produce on a general basis. After the 1948 verdict following the Hollywood Antitrust case, there was no longer a booking guarantee on the theatres for cartoons from any of the studios, making it a more risky business and because of this less resources were invested in the theatrical shorts, causing a gradual decline. By the beginning of the 1950s, the medium of television was beginning to gain more momentum, and the animation industry began to change as a result. At the head of this change were the tandem of William Hanna and Joseph Barbera, the creators of \"Tom and Jerry.\" The new Hanna-Barbera utilized the limited animation style that UPA had pioneered. With this limited animation, Hanna and Barbera created several characters including \"Huckleberry Hound\", \"The Flintstones\", \"Yogi Bear\" and \"Top Cat\". With television's growing popularity, which included the Saturday morning cartoons, a decline began in movie-going. To face the competition from TV, the theaters did what they could to reduce their own costs. One way of doing so was booking features only and avoiding the expenses of shorts, which were considered unnecessary and too expensive. Those few shorts that found their way to the theaters despite this are often viewed by critics as inferior to their predecessors.\n\nThis is a timeline of American animation studios' active production of regularly released cartoon shorts for theatrical exhibition. Some studios continue to release animated shorts to theaters on an infrequent basis. The colors correspond to the animation studio's associated theatrical distributor.\n\nThe 1988 film \" Who Framed Roger Rabbit\" honored the golden age of American animation and classical Hollywood cinema. The film featured cameos of various characters from multiple studios, such as Disney, Warner Bros., Fleischer Studios, and Universal, among others. The film also contains the only time in cinematic history that Disney's Mickey Mouse and WB's Bugs Bunny appear on-screen together. The 2017 game \" Cuphead \" features a golden age animation style inspired by Disney and Fleischer.\n\n\n"}
{"id": "16102957", "url": "https://en.wikipedia.org/wiki?curid=16102957", "title": "Heinrich Brück", "text": "Heinrich Brück\n\nHeinrich Brück (25 October 1831, Bingen – 4 November 1903) was a German Catholic church historian, and Bishop of Mainz.\n\nHe followed for some time the cooper's trade. After a course of studies under of a distinguished ecclesiastic, Dr. Joseph Hirschel, he entered the seminary at Mainz. He was ordained to the priesthood in 1855, exercised for some time the ministry, and made a postgraduate course at Munich under Ignaz von Döllinger, and at Rome. In 1867 he was appointed to the chair of ecclesiastical history in the seminary of Mainz.\n\nHe continued to teach until his elevation to the episcopate, with the exception of the years from 1878 to 1887, when seminary was closed by the order of the Government due to the Kulturkampf. In 1889 he became a canon of Mainz Cathedral; he received also several positions of trust in the administration of the diocese. In 1899 he was chosen Bishop of Mainz.\n\nPerhaps his best known work is his manual of church history, from \"Lehrbuch der Kirchengeschichte\" (Mainz, 1874; 8th ed., 1902). It has been translated into English, French, and Italian. The author showed himself possessed of extensive knowledge not only in history, but also in theology and canon law. A more special work is his \"Geschichte der katholischen Kirche in Deutschland im neunzehnten Jahrhundert\"—History of the catholic Church in Germany in the Nineteenth Century\", in five volumes (1887-1905).\n\nHe was also the author of an account of rationalistic movements in Catholic Germany (1865), a life of Dean Lennig (1870), and a work on secret societies in Spain (1881).\n\n"}
{"id": "18656400", "url": "https://en.wikipedia.org/wiki?curid=18656400", "title": "Ibn Faradi", "text": "Ibn Faradi\n\nAbū–l-Walīd ‛Abdallāh ibn ul-Faradi (9621012), best known as Ibn Faradi, was a Muladi historian. He was born at Córdoba and studied law and tradition. In 992, he made the pilgrimage and proceeded to Egypt and Kairawan, studying in these places. After his return, in 1009 he became qadi in Valencia. He and his fellow Muladis were killed at Córdoba when the Berbers took the city.\n\nIbn Faradi's chief work is the \"History of the Learned Men of Andalusia\", edited by F. Codera (Madrid, 18911892). He also wrote a history of the poets of Andalusia.\n"}
{"id": "55965112", "url": "https://en.wikipedia.org/wiki?curid=55965112", "title": "International Centre for Chinese Heritage and Archaeology", "text": "International Centre for Chinese Heritage and Archaeology\n\nThe International Centre for Chinese Heritage and Archaeology (ICCHA) () is the name of a collaborative centre between University College London and Peking University.\n\nThe International Centre for Chinese Heritage and Archaeology is a collaboration between the School for Archaeology and Museology of Peking University and the Institute of Archaeology, UCL, to promote the exchange of archaeologists between Europe and China. The ICCHA regularly hosts world-class conferences, invites visiting scholars, enables exchange and communication between Chinese and British archaeologist, seeking to bridge the gap in archaeological thought and theory.\n\nThe Centre owes much to the vision of Peter Ucko, former director of the Institute of Archaeology, UCL. True to his commitment to world archaeology, Ucko developed a keen interest in China, and sought to bring Chinese archaeology into the world archaeology. With the help of Wang Tao, then Lecturer in Chinese art and archaeology at SOAS and others, he developed links with archaeological departments in the People's Republic of China, and arranged for two joint posts in Chinese archaeology to be shared between UCL and SOAS, one of which was taken by Wang. The International Centre for Chinese Heritage and Archaeology (ICCHA) was officially launched in Beijing on 15 December 2003, with the full support of the State Administration of Cultural Heritage of the People's Republic of China, with joint offices in London and Beijing, forming the first such venture between China and the United Kingdom. With Ucko as its first director (2003-2007), the ICCHA soon yielded several collaborative projects in training and research, and resulted in a number of scholarships for Chinese students to be trained in archaeology at UCL. In 2006 Ucko and Wang travelled to ten Chinese cities, interviewing academic archaeologists about how they taught the subject, with the aim of publishing their findings, but Ucko's untimely death prevented this. \n\nThe ICCHA celebrated its 10th anniversary in 2013.\n\n\n\n"}
{"id": "309142", "url": "https://en.wikipedia.org/wiki?curid=309142", "title": "Irakli Tsereteli", "text": "Irakli Tsereteli\n\nIrakli Tsereteli (; , \"Irakliy Georgievich Tsereteli\"; 20 November 1881 – 20 May 1959) was a Georgian politician and a leading Social-Democratic spokesman during the era of the Russian Revolutions. He was born and brought up in Georgia when it was part of the Russian Empire, \n\nA member of the Menshevik faction of the Russian Social Democratic Labour Party, Tsereteli was elected to the Duma in 1907, where he gained fame for his oratory abilities. Shortly after entering the Duma, Tsereteli was arrested and charged with conspiracy to overthrow the Tsarist government, and exiled to Siberia. Returning in the aftermath of the 1917 February Revolution, he took up a leading position with the Petrograd Soviet and accepted a position in the Russian Provisional Government as Minister of Post and Telegraph, and briefly as Minister of the Interior. After the October Revolution and rise of the Bolsheviks, he returned to Georgia. Tsereteli worked as a diplomat at the Paris Peace Conference, where he lobbied for international recognition and assistance for the newly independent Democratic Republic of Georgia, which largely failed to materialize before the Red Army invaded in 1921. He spent the rest of his life in exile, mainly in France, working with socialist organisations and writing on socialism, and died in New York in 1959.\n\nA dedicated social democrat, Tsereteli was one of the leading figures of the movement in Russia. In 1915, during his Siberian exile, he formed what became known as Siberian Zimmerwaldism, and developed \"Revolutionary Defensism\", the concept of a defensive war, which Tsereteli argued was not being conducted at the time. Concerned that political fragmentation would lead to a civil war in Russia, Tsereteli strived to broker compromises between the various leftist factions in the Russian Revolution and was the force behind efforts to work together with the middle classes, to no avail. Renowned for his speaking ability, Tsereteli gained appreciation for his ability in this regard, giving impassioned speeches in the Duma and in the Petrograd Soviet. An avowed internationalist, Tsereteli grew increasingly distant from the Georgian Mensheviks who gradually adopted more nationalist tendencies. Uninterested in acquiring or maintaining political power, Tsereteli was much more concerned with inequality, and tried to solve that above all.\n\nTsereteli was born in Gorisa, Kutais Governorate, in the Russian Empire (now in Imereti, Georgia), to an Orthodox Christian family, the third child of Giorgi Tsereteli, a radical writer from the noble Tsereteli family, and Olympiada Nikoladze, the sister of the journalist Niko Nikoladze. Tsereteli had one sister, Eliko (1877–1950) and brother, Levan (1879–1918). Both Giorgi and Niko were members of the \"meore dasi\" (\"მეორე დასი\"; Georgian for \"second group\"), a group of Georgian populists and socialists, and they greatly influenced Irakli's outlook. Tsereteli grew up in nearby Kutaisi and spent the summers at his family's estate in Gorisa; From a young age he noticed the inequality between his family and their servants and the local peasants, and desired to fix the imbalance.\n\nWhen he was three, Tsereteli's mother died, so he and his siblings were sent to live with two aunts in Kutaisi, while Giorgi moved to Tiflis (now Tbilisi), the administrative centre of the Caucasus, occasionally visiting the children. Tsereteli would later move to Tiflis and attend a gymnasium. While there, he lived with his father, who had since married Anastasia Tumanova, an ethnic Armenian. Tsereteli's biographer W.H. Roobol suggests that due to Tsereteli's reserve towards Tumanova, Giorgi's influence over his son declined: \"In any event, Giorgi Tsereteli was unable to imbue his son Irakli with his patriotic ideals.\" Nikoladze's views, which were more cautioned against Georgian nationalism, also likely played a part in Tsereteli's shifting ideals. At the gymnasium Tsereteli distanced himself from religion, questioning death and its meaning, and was introduced to the writings of Charles Darwin, which also factored into his disinterest in religion. He completed his schooling in 1900, the same year as his father's death, and moved to Moscow to study law.\n\nSoon after arriving in Moscow Tsereteli became embroiled in the student protests that broke out that year; how involved he initially was is unclear, with the only certainty being he was not yet a Marxist. It was during these protests that Tsereteli first gained fame as a great speaker, and he eventually became a leading figure in the student movement. He was arrested in the spring of 1901 and after a brief detention was allowed to return to Georgia. Though he had been arrested, he was allowed to return to Moscow in the autumn of 1901 to write his exams. There had been relative quiet in the universities until that point, but it again erupted into protests; this time Tsereteli took a leading role, and was regarded as one of the most important figures of the student movement in Moscow. At a meeting of student protesters on 9 February 1902 Tsereteli was arrested; considered one of the most radical leaders, he was one of two students given a sentence of five years' exile in Siberia, the longest sentence given. Though the government quickly backtracked and offered him the chance to serve it in Georgia, Tsereteli refused, seeing it as a pardon and considering \"its acceptance as being in conflict with [his] views\". This refusal, which was publicised with other exiles, cited social democracy, and effectively confirmed Tsereteli's support for the ideology by this point. After declining the offer to return to Georgia, Tsereteli was sent to the village of Tulun, roughly 400 kilometres from Irkutsk, arriving in early 1902. However, by late summer he was permitted to move to Irkutsk. It was during this exile that Tsereteli became familiar with the Russian social democrats, particularly Marxism; Tsereteli read Vladimir Lenin's \"What Is To Be Done?\", though he disliked the view Lenin espoused.\n\nOn his release from prison Tsereteli returned to Georgia and aligned himself with the Social Democratic Labour Party's Georgian branch, later known as the Georgian Mensheviks. He also began working as an editor for his father's former publication, \"Kvali\" (\"კვალი\"; \"Trace\"), writing most of their leading articles. However, in January 1904 he was again arrested, and spent two months in the Metekhi prison in Tiflis; two months later \"Kvali\" was banned. Tsereteli was allowed to leave Georgia, likely due to the influence of his uncle, so he moved to Berlin to resume his law studies, spending 18 months in Europe. Suffering from a form of haemophilia, Tsereteli became seriously ill in the autumn of 1905, but was unable to quickly return home as the 1905 Revolution broke out in the Russian Empire. It was only in May 1906 that he returned to Georgia.\n\nTsereteli remained in Georgia throughout the summer of 1906 recovering from his illness, and was not politically active. Even so, he was invited to stand as the Social Democratic candidate for the Russian legislative elections in January 1907, representing the Kutais Governorate, his home region. He was encouraged to do so by a fellow Georgian Menshevik, Noe Zhordania; later political opponents who disagreed on nearly every topic, Zhordania would later recall in his memoirs \"that this was the only time that Irakli ever listened to me.\" All seven seats in Georgia were won by the Social Democrats.\n\nDespite being the youngest member of the Imperial Duma (at 25, the minimum age required for membership), Tsereteli took a leading role. He immediately gained recognition as a great orator. In particular he was noted for three speeches in which he outlined the Social Democrats' views and heavily criticized the government. The first speech, which opened with him stating that the \"government has fettered the nation in the chains of a state emergency, which imprisons its best sons, reduces the people to beggary and fritters away the pennies collected for the hungry and destitute. Today, there spoke to us the old feudal Russia, personified by the government,\" went on to call for the opposition not to work with the government regarding the agrarian reforms of Prime Minister Pyotr Stolypin, or indeed anything, stopping just short of calling for an armed insurrection. This gained Tsereteli immediate respect among his peers. He strived to unite the opposition parties, though he faced considerable opposition both from the Kadets, a liberal group who had previously opposed the government but were now more amicable to them, and the Bolsheviks, who worked to discredit the Mensheviks in the Duma. He sought out an alliance with the other leftist factions, namely the Socialist Revolutionary Party and the Trudoviks, a splinter group from the Socialist Revolutionaries.\n\nStolypin grew increasingly tired of the opposition from the Social Democrats, and feared that his reforms would not be passed. A conspiracy was created implicating the Social Democrats with trying to overthrow the government, giving Stolypin the ability to have them expelled from the Duma and freeing him to implement his policies, which severely reduced the number eligible voters. The Duma was dissolved on 2 June 1907 and shortly after midnight on 3 June several of the Social Democrats were arrested, including Tsereteli. He was convicted in November, and sentenced to five years' hard labour, though on account of his poor health it was commuted to time in prison. The first year of his prison term was spent in St. Petersburg, and in the winter of 1908–1909 Tsereteli was moved to Nikolayev in southern Ukraine; after four years in Nikolayev he was again moved, sent to the Alexandrovsky Central Prison in Irkutsk. In the autumn of 1913 Tsereteli was permitted to move to Usolye, a village about 70 kilometres from Irkutsk and easily accessible owing to its location on a branch line of the Trans-Siberian Railway.\n\nTsereteli would later reflect fondly on this period of exile: there were several other exiles in the region, and in the summers they would meet in Usolye, which had a favourable climate. On occasion Tsereteli was also able to visit Irkutsk, engaging in political talks. Both Bolsheviks and Mensheviks were involved in these discussions, and engaged with each other cordially, leading Tsereteli to believe the two factions could eventually reunite. This was in stark contrast to the situation outside Siberia, where the two factions had been increasingly distancing themselves.\n\nThe outbreak of the First World War in August 1914 was not of much interest to Tsereteli initially. However, much like the rest of the population in the region he regularly read updates in the newspapers, and tried to ascertain what type of opposition was occurring internationally; though most mentions of opposition movements was censored, Tsereteli concluded that something had to exist, and felt that the Second International, a Paris-based organization of socialist and labour parties, could play some role in ending the war. Tsereteli also engaged in discussion with other Social Democrats in the Irkutsk region on his views towards the war, and like them would have them published in a journal – \"Siberian Journal\" (\"Сибирский Журнал\", in Russian), later replaced by the \"Siberian Review\" (\"Сибирское Обозрение\") – that he edited. This group would later be referred to as the Siberian Zimmerwaldists, a reference to the 1915 Zimmerwald Conference of international socialist groups.\n\nAt its root, Siberian Zimmerwaldism was based on the ideals of a branch of socialists who were opposed to the war and wanted to restore the Second International, which had fractured upon the outbreak of the war as the various socialist groups differed on policy towards the war: many had abandoned the International in favour of defense of their countries (the so-called \"Defensists\"), while the \"Minority\" was split between the extreme left (led by Lenin), which advocated for class warfare, and the more mainstream view that sought to use the International; as such they were known as the \"Internationalists,\" of which the Siberian Zimmerwaldists were related to. Through his editorship of the journals, Tsereteli both became a mentor to other Siberian Zimmerwaldists and influenced the group's stance on the war, even though he only wrote three articles over the course of the war, making it difficult to fully determine his position.\n\nThe first of Tsereteli's wartime articles, titled \"The International and the War\" (\"Интернационал и Война\") looked at how the differing socialist groups reacted to the war. He agreed with the majority Internationalist view, which had stated that the war was not totally inevitable, and that the International had thus been trying to limit the threat of war. He further argued that the International was not strong enough to call a general strike, as the proletariat was not strong enough to overthrow capitalism, and it would only hurt the movement. Tsereteli also criticized the Defensists, stating that while there could be such a thing as a just defence, \"not one of the warring powers except Belgium [was] conducting a defensive war.\" That socialist leaders in Germany, France, and the United Kingdom had supported their respective governments in the war effort was also unacceptable to Tsereteli, though he explained that it \"could not distort the historical path of the proletariat\".\n\nThe second article Tsereteli wrote, \"Democracy in Russia at War\" (\"Демократия среди воюющей России\") was largely a response to the leading Russian \"Defensists\", namely Georgi Plekhanov and Alexander Potresov, and refuted their argument. He stated that all of the warring states were guilty and none could be victorious. His third article, \"For two years\" (\"За два года\"), looked at how the war had evolved, and how bourgeois nationalism had encompassed the conflict. He called the conflict an \"imperialist struggle over spheres of influence,\" largely conforming to the view of the International, though also stating his support for the idea of self-defense. Publication of more articles was halted by the authorities, but the articles Tsereteli did write had a considerable impact, and helped keep him relevant even while in exile.\n\nNews of the February Revolution, which began on 23 February 1917, arrived in Irkutsk on 2 March and reached Usolye that evening; Tsereteli left for Irkutsk the following morning. Several people, including Tsereteli, arrested the regional governor and declared Irkutsk a free city. A committee consisting of important social groups was formed to run the city, while a soviet (council) of soldiers was simultaneously created. Tsereteli took a leading role in this committee, though the work took a considerable toll on his health and after ten days he had to step down as he began to vomit blood. His family and friends suggested he return to Georgia, though Tsereteli instead decided to travel to Petrograd (the name St. Petersburg had adopted at the start of the war), arriving there on 19 or 20 March.\n\nTsereteli was the first of the major exiled politicians to arrive in Petrograd after the Revolution, and thus was welcomed by a large crowd at the train station. Immediately, Tsereteli went to the Petrograd Soviet and gave a speech in support of the revolution, but warned members that it was too early to implement socialist policies. At the time of his arrival, there was no clear leadership of the country, with both the Petrograd Soviet and the Provisional Government claiming authority. The Soviet, composed of representatives of workers and soldiers, enjoyed popular support, though it was not regarded as a government. In contrast, the Provisional Government claimed it was the legitimate successor to the Russian Empire, but did not have the support of the people. Each thus needed the other to legitimize their claim. This system, later dubbed \"dual power\", was highly inefficient, though neither side wanted to upset the balance lest they lose their power.\n\nDue to his former membership in the Duma, Tsereteli was appointed to the Soviet on 21 in an advisory role. At his first meeting he argued that Russia should strive to defend itself, calling defense \"one of the fundamentals of the revolution\". He stated that both the country and the revolution had to be defended from the German Empire, but also that the Soviet should pressure the Provisional Government to negotiate a peace, one that recognized self-determination and did not include annexation. This policy would soon be given the name \"Revolutionary Defensism\". Tsereteli led the Soviet side in negotiations with the Provisional Government to have the no-annexation policy adopted, in the process showing that he had effectively become a leader within the Soviet. Tsereteli was not seeking an increased role for himself, nor did he want the Soviet to become a power-base, but simply a representative body of the workers and soldiers.\n\nThe April Crisis – a series of demonstrations against Russia's continued participation in the war and a note to the Allied powers affirming that Russia was still interested in annexing Constantinople – nearly led to the downfall of the Provisional Government, and it survived mainly due to negotiations with the Soviet to form a coalition. The coalition was unpopular among many of the Mensheviks, Tsereteli included, but they realized that without the support of the Soviet the Provisional Government was unlikely to survive another threat like the April Crisis, thereby ending the Revolution, so they supported it. Though the socialists could have dominated the newly formed cabinet, Tsereteli cautioned that this would only hurt their cause, so they only took six of the fifteen cabinet posts.\n\nTsereteli was given the position of Minister of Post and Telegraph, an office created just so he could be in the cabinet. Reluctant to join the government, Tsereteli only did so in hopes of avoiding the dissolution of the Provisional Government and the outbreak of civil war. He did little in his role as minister, which he held until August 1917, and kept his focus on the Soviet, leaving the actual administration to others. In his memoirs, Tsereteli never mentioned his time as minister, and the only notable action he took in the position was an attempt to increase the pay of post office employees. Even so, Tsereteli's position in the cabinet was aimed at allowing him to serve as a liaison between the Provisional Government and Soviet. He also realized that, as a member of the cabinet, he could \"exercise real influence upon the government, since the government and the middle classes which back it are greatly impressed by the power of the Soviet\". Despite his relatively unimportant ministerial post, Tsereteli was regarded as a major figure by his peers: Viktor Chernov called him the \"Minister of General Affairs,\" while Nikolai Sukhanov referred to him as the \"Commissar of the Government in the Soviet\". Highly valued by the prime minister, Georgy Lvov, Tsereteli was part of the \"inner cabinet\" that held the real power in the Provisional Government. He would later express support for the cabinet, as long as it benefited the Revolution.\n\nLvov resigned as prime minister on 2 July 1917, after disagreements within the cabinet regarding the status of the Ukrainian People's Republic, which was in control of Ukraine. Tsereteli had travelled to Kiev with a party representing the Provisional Government to negotiate a means to ensure defense of Russia while respecting Ukrainian self-determination. The outcome saw the Ukrainians allow the Russians to continue to defend their territory, while granting increased autonomy, a move opposed by many in the cabinet. This came at the same time as the July Days, a major demonstration that broke out in Petrograd, and threatened the Provisional Government. The Provisional Government was able to withstand the threat, and Alexander Kerensky took over as prime minister. Though Tsereteli opposed Kerensky, seeing him as the force behind Lvov's resignation, he had little option but to consent to the move.\n\nTsereteli was appointed Minister of the Interior, serving for two weeks until a new cabinet could be formed. Despite his senior ranking in the Soviet, Tsereteli was passed over for the post of Prime Minister, ostensibly because of his position; the coalition wanted reform and felt that influence from the Soviet would prevent that. However, with Kerensky frequently absent, Tsereteli served as the \"de facto\" prime minister, and tried to implement some domestic reforms and restore order throughout the country. Upon his return, Kerenskey was given a mandate to form a new cabinet, though Tsereteli declined a position in it, wanting instead to focus his efforts in the Soviet. He used his influence to force Kerensky to release Leon Trotsky, imprisoned in the aftermath of the July Days; Tsereteli needed Trotsky and the Bolsheviks to support the socialist movement in the Soviet against the Kadets. This had the opposite effect, as Trotsky quickly maneuvered to orchestrate a Bolshevik takeover of the Soviet, expelling Tsereteli.\n\nRemoved from his post in the Soviet and suffering from tuberculosis, Tsereteli decided to move into semi-retirement. At the end of September 1917, he returned to Georgia, his first visit there in ten years. Roobol believed that Tsereteli only left because he was confident that the new Kerensky government was secure enough to last until the Constituent Assembly could meet. Though the Bolsheviks now had control of the Soviet, Tseretli was dismissive of them as a threat to the Provisional Government; while he expected them to try and seize power, he expected them to only last \"two or three weeks\".\n\nTsereteli stayed in Georgia for about a month, returning to Petrograd after the Bolsheviks seized control in the October Revolution. Seen as a threat due to his position as a leading Menshevik and a delegate for the upcoming Constituent Assembly, a warrant for Tsereteli's arrest was issued on 17 December. He defied the authorities and stayed in Petrograd for the only meeting of the Constituent Assembly, which took place on 5 January 1918. Speaking to the body, he attacked the Bolsheviks, accusing them of failing to do anything constructive, and stifling any criticism against their policies. The assembly was dissolved by the Bolshevik regime after its lone meeting. Now fearing arrest, Tserteli returned to Georgia, which had broken away from Russian control during the Revolutions.\n\nBack in Georgia, Tsereteli delivered a speech on 23 February 1918 at the Transcaucasian Centre of Soviets, reporting on the events in Russia. He warned the delegates of the problems dual power had caused, and that the soviet would have to surrender its power to a legislative body. This was established as the \"Seim,\" a \"de facto\" parliament created the same day. A member of this new body, Tsereteli took up a leading role in helping defend the Transcaucasus, which included Armenia, Azerbaijan, and Georgia, from the approaching forces of the Ottoman Empire. He strongly denounced the Treaty of Brest-Litovsk, which was signed between the Bolshevik government and the Central Powers to end Russia's involvement in the war, as it would have meant ceding important Transcaucasian territories to the Ottoman, such as the Black Sea port city of Batumi. In response to this the Transcaucasus declared war against the Ottoman Empire on 14 April.\n\nThe tripartite Transcaucasian Democratic Federative Republic was formed on 22 April, though due to the ongoing invasion by the Ottoman and the lack of unity among the three groups, it was immediately in a precarious position. The Georgians, afraid for their own country and future, began negotiating with Germany for protection against the Ottomans, which would come in the form of an independent state. On 26 May Tsereteli gave a speech to the Seim stating that from the start the Transcaucasian Republic had been unable to operate due to its people not being unified. On the same day, the Georgian leadership declared an independent state, the Democratic Republic of Georgia. This was followed two days later by Azerbaijan and, finally, Armenia, dissolving the Transcaucasian Republic.\n\nWithin the new Georgian state Tsereteli took up a seat in the Constituent Assembly, which was elected in February 1919. However, he did not play a major role in the Georgian government, instead helping out more in an advisory role. That he supported what was essentially a nationalist state contradicted his earlier internationalist stance, though Roobol suggested that Tsereteli \"wanted a state which would be more than a Georgian national state,\" and championed the causes of the ethnic minorities within Georgia. Even so, he was no longer able to exercise much political influence, and faded into the background.\n\nIn 1919, Tsereteli and Nikolay Chkheidze were asked to lead a Georgian delegation to the Paris Peace Conference; the two were asked to attend on account of their contacts in Europe, and that neither had a major role in the Georgian government. They faced considerable difficulties there, as many of the delegates were unfamiliar with the situation in Georgia, so both Tsereteli and Chkheidze gave several newspaper interviews expressing that Georgia was only interested in gaining \"de jure\" recognition of its independence. Tsereteli subsequently visited London to help their cause. While he did not make much of an impact with the British government, the Georgian government was \"de facto\" recognized on 10 January 1920, mainly because the British wanted allies in the region in the event the Bolsheviks allied with the Turks and took over the region.\n\nHis diplomatic efforts a success, Tsereteli returned to advocating socialism. In the summer of 1920 he represented the Georgian Social Democratic Party at a Labour and Socialist International conference in Switzerland and promoted the success of Georgia as a socialist state. He also proved instrumental in helping Karl Kautsky arrive in Georgia in August 1920 to research a book on the country. However, his health problems returned, and Tsereteli was ordered to rest in December of that year.\n\nTsereteli was recovering in France when he heard about the Red Army invasion of Georgia and subsequent Bolshevik takeover in February 1921. The news had a detrimental impact on his health, and he retired to a French village for the summer. He also worried about his Nikoladze aunts, as they had lost considerable amounts of money with the Bolshevik occupation. When his health improved in October, Tsereteli moved to Paris, joining the Georgian government-in-exile. In exile he lived frugally, and quickly grew tired of residing in France, enjoying any opportunity to travel. The suicide of Chkheidze in 1926 had a profound impact on Tsereteli, and it exacerbated his distaste for exile.\n\nAfter retiring from émigré political life in 1930, Tsereteli resumed his law studies, which he had never completed in his youth, finishing in 1932, and worked in Paris as a lawyer. He also helped edit Pavel Axelrod's works after the latter's death in 1928. Initially working with Fedor Dan, whom he had met during his Siberian exile, they clashed as Dan had become more pro-Bolshevik, and Dan ultimately left the project over their dispute. Tsereteli would later be aided in this work by his friend and fellow socialist Vladimir Voitinsky, and the project was published in Germany in 1932.\n\nHighly indignant about what he called the \"platonic attitute\" of the Western socialist parties towards Georgia and their inadequate support to the beleaguered country, Tsereteli continued to regard Bolshevism as the root of this evil, and believed that the Bolshevik regime would not survive long. He continued to attend International's conferences in Europe, trying to get the organization to adopt a stronger anti-Bolshevik stance, though had limited success. He attended the Conference of the Three Internationals, at which the issue of Georgia was a major issue. By 1928, as the inner conflicts of the Bolsheviks ended, it became apparent to Tsereteli that they would not so easily be removed from power, and his hopes of returning to Georgia faded.\n\nTsereteli gradually distanced himself from his fellow Georgian exiles, and opposed both the liberal nationalist Zurab Avalishvili and the social democrat Noe Zhordania; all three wrote extensively abroad on Georgian politics. Tsereteli accepted the principle of the fight for Georgia's independence, but rejected the view of Zhordania and other Georgian émigrés that the Bolshevik domination was effectively identical to Russian domination. Furthermore, he insisted on close cooperation between the Russian and Georgian socialists against the Bolsheviks, but did not agree with any cooperation with Georgian nationalists. This led to Tsereteli's isolation among fellow émigrés and he largely withdrew from political activity. Invited to join Voitinsky in the United States, Tsereteli waited until after the Second World War ended to do so, finally moving in 1948. Columbia University asked him to finish writing his memoirs, which he continued to work on until his death in 1959. In 1973, he was reburied at the Leuville Cemetery near Paris.\n\nThroughout his life Tsereteli remained a committed internationalist, adopting this view during his first exile in Siberia. He felt that if the population of the Russian Empire were united, and not divided along ethnic or national lines, socialist policies could be implemented. His views were heavily influenced by the writings of Pavel Axelrod, whom Tsereteli considered his most important teacher. After reading Lenin's \"What Is To Be Done?\" in 1902, he came to oppose Lenin's view. Tsereteli never deviated from his internationalist stance, which eventually led to conflict with other Georgian Mensheviks, who became far more nationalist throughout the 1920s.\n\nUpon the outbreak of the First World War, Tsereteli, still exiled in Siberia, formulated a policy that allowed for the continuation of the war, in contrast to the more mainstream socialist goals of pressuring governments to end the conflict. This policy, expressed in three articles written by Tsereteli, would become known as \"Siberian Zimmerwaldism,\" in reference to the Zimmerwald Conference of 1915 that first saw the socialist views of the war put forth. Siberian Zimmerwaldism allowed for, under certain circumstances, a defensive war, though Tsereteli argued that only Belgium fit these criteria, as the other warring states were fighting offensively. Though he edited the journal that published the Siberian Zimmerwaldist views, Tsereteli only wrote three articles during the war, making it difficult to fully comprehend his views at the time.\n\nDuring his political career, Tsereteli was highly regarded by his peers, though he has since faded into relative obscurity. His leading role in the Petrograd Soviet led Lenin to refer to Tsereteli as \"the conscience of the Revolution\". Lvov would later call him \"the only true statesman in the Soviet\". However, his refusal to perceive the Bolsheviks as a serious threat, even as late as October 1917, ultimately helped them lead the October Revolution. As Georgi Plekhanov, a contemporary Marxist and revolutionary, stated: \"Tsereteli and his friends without themselves knowing or desiring it, have been preparing the road for Lenin.\"\n\nThat Tsereteli quickly faded from prominence in histories of the era is not surprising; Rex A. Wade, one of the preeminent historians of the Russian Revolution, noted that because Tsereteli \"was not as flamboyant as Kerensky or as well known to foreigners as Miliukov, and therefore has not attracted as much attention as either in Western writings\". Roobol concluded that, \"it was [his] prestige rather than the force of his arguments which won over the doubters.\" Roobol also described Tsereteli's career in an apt, simple form: \"a rapid rise, a short period of generally recognized leadership and a rather more gradual slide into political isolation\".\n\n"}
{"id": "50658925", "url": "https://en.wikipedia.org/wiki?curid=50658925", "title": "Joel Veinberg", "text": "Joel Veinberg\n\nJoel Veinberg (Russian: \"Йоэль Пейсахович (Иоэл Песахович) Вейнберг\", Latvian: \"Joels Veinbergs\"; born 1922 in Riga, died in 2011 in Yerusalem), also known as Joel Weinberg, was a Latvian and Soviet orientalist and historian of Jewish origin.\n\nHe graduated from a high school with language of instruction in Hebrew in Latvia and started university studies in history. From 1941 to 1943 he was confined to the Riga Ghetto together with his family and was thereafter imprisoned by Nazi Germany until 1945 in concentration camps in Latvia and Germany (including Buchenwald). After the war Veinberg worked as a lecturer of history in the University of Latvia and later at the Daugavpils University.\n\nJoel Veinberg was one of the few Jewish historians in the USSR who could work in the \"inactual\" (from the point of view of the Soviet officialdom) fields such as ancient Jewish history and culture. He published numerous works on the Ancient Orient («Человек в культуре древнего Ближнего Востока», М., 1986 (\"Man and Culture in the Ancient Orient\", published in Moscow)), Second Temple period and especially the Persian period, as well as on the Bible (\"The Old Testament in the Light of Current Science\", in Latvian, Riga, 1966).\n\nSince 1994 he worked at the Ben-Gurion University of the Negev as professor at the department of Bible and Ancient Orient.\n\n\n"}
{"id": "11951591", "url": "https://en.wikipedia.org/wiki?curid=11951591", "title": "Josef Tomeš", "text": "Josef Tomeš\n\nJosef Tomeš (born January 18, 1954 in Prague) is Czech historian. He studied history and philosophy at Charles University in Prague. He is also chairman of Society of Viktor Dyk \"(Společnost Viktora Dyka)\".\n"}
{"id": "1104854", "url": "https://en.wikipedia.org/wiki?curid=1104854", "title": "Linnart Mäll", "text": "Linnart Mäll\n\nLinnart Mäll (7 June 1938 – 14 February 2010) was an Estonian historian, orientalist, translator and politician.\n\nBorn in Tallinn, Estonia, Mäll graduated from the University of Tartu in 1962 with a major in general history. He followed graduation with postgraduate studies at the Institute of Oriental Studies of the Russian Academy of Sciences at the USSR Academy of Sciences (1964–1966) and Department of History, University of Tartu (1966–1969); 1985 Cand. Hist. (Ph.D.) in history, Ph.D. thesis \"Ashtasāhasrikā Prajñāpāramitā \"as a Historical Source\"\".\n\nSince 1994 he was Head of the Centre for Oriental Studies, senior research fellow, Department of History, Faculty of Philosophy, University of Tartu. From 1969 to 1973 he served as lecturer of the Chair of General History at Tartu State University. Later he was dismissed for anti-communist views and subsequently worked for ten years as engineer of the Cabinet for Oriental Studies. He was partly rehabilitated in 1983 and promoted to head of the Laboratory for History and Semiotics (1983–1991). He later served as head of the Laboratory for Oriental Studies (1991–1994).\n\nHis main research fields included: Buddhist Mahāyāna texts, Buddhist mythology, classical Indian literature and culture, classical Chinese texts, Tibetan Buddhist texts and the history of small nations and peoples.\n\nHe was one of the first who applied the methods of semiotic analysis for investigation of Buddhist texts and other texts of classical Oriental thought. Mäll was one of the central figures of the branch of oriental studies in the Tartu-Moscow Semiotic School in 1960–70s. In the 1990s he worked on the elaboration of the conception of humanistic base texts; since 1998 the initiator and head of the research project \"Humanistic base texts in the history of mankind\"; and author of ten books and over one hundred academic articles.\n\nMäll was inspired to become a Buddhist and buddhologist by well-known Estonian theologian and philosopher Uku Masing in the early 1960s. He later studied under and worked together with several Buddhist and non-Buddhist teachers and scholars including Nikolai Konrad, Alexander Piatigorsky, Oktiabrina Volkova, Youri Parfionovich, Lev Menshikov and Lama Bidia Dandaron. He was a teacher and spiritual master for many Estonian Buddhists and orientalists of the younger generation. In the 1990s he established close ties with The Dalai Lama and served as the main organizer of both of the visits of His Holiness in Estonia (1991 and 2001). Mäll was the founder and director of the first Mahāyāna Institute (which existed from 1991 to 1994).\n\nHe was awarded the Order of the White Star, IV degree in 2001.\n\nMäll died of cancer in Tartu on 14 February 2010.\n\nHis membership in the academic associations is as follows:\n\n\n"}
{"id": "22986033", "url": "https://en.wikipedia.org/wiki?curid=22986033", "title": "List of ancient legal codes", "text": "List of ancient legal codes\n\nThe legal code was a common feature of the legal systems of the ancient Middle East. The Sumerian Code of Ur-Nammu (c. 2100-2050 BC), then the Babylonian Code of Hammurabi (c. 1760 BC), are amongst the earliest originating in the Fertile Crescent. In the Roman empire, a number of codifications were developed, such as the Twelve Tables of Roman law (first compiled in 450 BC) and the Corpus Juris Civilis of Justinian, also known as the Justinian Code (429 - 534 CE). In ancient China, the first comprehensive criminal code was the Tang Code, created in 624 CE in the Tang Dynasty.\n\nThe following is a list of ancient legal codes in chronological order:\n\n\n"}
{"id": "53738585", "url": "https://en.wikipedia.org/wiki?curid=53738585", "title": "List of geomagnetic reversals", "text": "List of geomagnetic reversals\n\nList of geomagnetic the beginning and end of geomagnetic reversals. The ages of the beginning and end of each period of normal (same as present) polarity are listed. \n\nSource for the last 83 million years: Cande and Kent, 1995 \n\nAges in million years before present (Ma).\n"}
{"id": "19770", "url": "https://en.wikipedia.org/wiki?curid=19770", "title": "Memetics", "text": "Memetics\n\nMemetics (also referred to colloquially as memeology) is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Critics regard memetics as a pseudoscience. Memetics describes how an idea can propagate successfully, but doesn't necessarily imply a concept is factual.\n\nThe term meme was coined in Richard Dawkins' 1976 book \"The Selfish Gene,\" but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a \"unit of culture\" (an idea, belief, pattern of behaviour, etc.) which is \"hosted\" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.\n\nThe Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The \"Journal of Memetics\" was published electronically from 1997 to 2005.\n\nIn his book \"The Selfish Gene\" (1976), the evolutionary biologist Richard Dawkins used the term \"meme\" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Bella Hiscock outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.\n\nDawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of \"memetics\" in \"The Selfish Gene\", but rather coined the term \"meme\" in a speculative spirit. Accordingly, different researchers came to define the term \"unit of information\" in different ways.\n\nThe modern memetics movement dates from the mid-1980s. A January 1983 \"Metamagical Themas\" column by Douglas Hofstadter, in \"Scientific American\", was influential – as was his 1985 book of the same name. \"Memeticist\" was coined as analogous to \"geneticist\" – originally in \"The Selfish Gene.\" Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as \"memetics\" by analogy with \"genetics\". Dawkins' \"The Selfish Gene\" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of \"Consciousness Explained\" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay \"Viruses of the Mind\", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's \"Snow Crash\").\n\nThe idea of \"language as a virus\" had already been introduced by William S. Burroughs as early as 1962 in his book \"The Ticket That Exploded\", and later in \"The Electronic Revolution\", published in 1970 in \"\". Douglas Rushkoff explored the same concept in \"Media Virus: Hidden Agendas in Popular Culture\" in 1995.\n\nHowever, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: \"Virus of the Mind: The New Science of the Meme\" by former Microsoft executive turned motivational speaker and professional poker-player, Richard Brodie, and \"Thought Contagion: How Belief Spreads Through Society\" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' \"The Selfish Gene\" until his book was very close to publication.\n\nAround the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – \"Evolutionary Models of Information Transmission\" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the \"Journal of Ideas\" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published \"The Meme Machine\", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.\n\nThe term \"meme\" derives from the Ancient Greek μιμητής (\"mimētḗs\"), meaning \"imitator, pretender\". The similar term \"mneme\" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work \"Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen\", translated into English in 1921 as \"The Mneme\" . Until Daniel Schacter published \"Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory\" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word \"meme\" independently of Semon, writing this:\n\"'Mimeme' comes from a suitable Greek root, but I want a monosyllable that sounds a bit like 'gene'. I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to 'memory', or to the French word même.\" \n\nIn 2005, the \"Journal of Memetics – Evolutionary Models of Information Transmission\" ceased publication and published a set of articles on the future of memetics. The website states that although \"there was to be a relaunch...after several years nothing has happened\". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words \"meme\" and \"memetics\" (without disowning the ideas in his book), adopting the self-description \"thought contagionist\". He died in 2005.\n\nSusan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.\nThat is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or \"memeplexes\". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.\n\nThe memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as \"a unit of cultural transmission\". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as \"a unit of cultural information that can be copied, located in the brain\". This thinking is more in line with Dawkins' second definition of the meme in his book \"The Extended Phenotype\". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.\n\nThese two schools became known as the \"internalists\" and the \"externalists.\" Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, \"i-memes\", in response to external \"e-memes\".\n\nAn advanced statement of the internalist school came in 2002 with the publication of \"The Electric Meme\", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of \"Darwinizing Culture: The Status of Memetics as a Science\", edited by Aunger and with a foreword by Dennett, in 2001.\n\nThis evolutionary model of cultural information transfer is based on the concept that units of information, or \"memes\", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.\n\nCritics contend that some proponents' assertions are \"untested, unsupported or incorrect.\" Luis Benitez-Bribiesca, a critic of memetics, calls it \"a pseudoscientific dogma\" and \"a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution\" among other things. As factual criticism, he refers to the lack of a \"code script\" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in \"Darwin's Dangerous Idea\") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of \"no code script\" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)\n\nAnother criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.\nMary Midgley criticises memetics for at least two reasons: \"One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in \"Darwin's Dangerous Idea\", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of \"meme\" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple.\"\n\nHenry Jenkins, Joshua Green, and Sam Ford, in their book \"Spreadable Media\" (2013), criticize Dawkins' idea of the meme, writing that \"while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture.\" The three authors also criticize other interpretations of memetics, especially those which describe memes as \"self-replicating\", because they ignore the fact that \"culture is a human product and replicates through human agency.\"\n\nLike other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is \"heuristically trivial\", being a mere redescription of what is already known without offering any useful novelty.\n\nDawkins in \"A Devil's Chaplain\" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.\n\nAnother definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, \"memeplexes\", and the \"deme\", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word \"meme\". For example, in the sense of computer simulation the term \"memetic algorithm\" is used to define a particular computational viewpoint.\n\nThe possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.\n\nVelikovsky (2013) proposed the \"holon\" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.\n\nProponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – \"Evolutionary Models of Information Transmission\" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. \nKeith Henson in \"Memetics and the Modular-Mind\" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.\n\nDiCarlo () has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In \"Problem Solving and Neurotransmission in the Upper Paleolithic\" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. \nBut the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for equilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).\n\nHouben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.\nThis renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).\n\nResearch methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.\n\nThe application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.\n\nAnother application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).\n\nBen Cullen, in his book \"Contagious Ideas\", brought the idea of the meme into the discipline of archaeology. He coined the term \"Cultural Virus Theory\", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.\n\nFrancis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls \"memetic selection criteria\". These criteria opened the way to a specialized field of \"applied memetics\" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.\n\nIn \"Selfish Sounds and Linguistic Evolution\", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.\n\nAustralian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.\n\nSwedish political scientist Mikael Sandberg argues against \"Lamarckian\" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active (\"Lamarckian\") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the \"Lamarckian\" IT strategy.\n\n\n\n"}
{"id": "8241961", "url": "https://en.wikipedia.org/wiki?curid=8241961", "title": "Muhammad Khwandamir", "text": "Muhammad Khwandamir\n\nGhiyāś ad-Dīn Muḥammad Khwāndamīr, Khvandamir, or Khondamir or Hondemir () (1475–1534) was a Persian Islamic scholar born in Herat, in 880 AH or 1475 CE, a grandson and successor to noted historian Mirkhond.\n\nHondamir, like his grandfather, belonged to the Herat literary circle of Timurid vizier Mir Ali-Shir Navai. In 1527 the Moghul conqueror Babur invited Hondamir to India, where he lived and died. Hondamir edited and completed 7th and 8th volums of the general history composed by his grandfather, and approximately in 1500 composed an extract from it \"Holaset el-ehbar\". Following the same pattern, in 1521 Hondamir composed for Ali-Shir Navai a three-volume general history entitled \"Habib al-Siyar\", and brought it to his time. Another notable work was \"Al-Destour Voser\". In the European languages, other works of Hondemir were not published.\n\n\n"}
{"id": "86449", "url": "https://en.wikipedia.org/wiki?curid=86449", "title": "Ogma", "text": "Ogma\n\nOgma (modern spelling: Oghma) is a character from Irish mythology and Scottish mythology. A member of the Tuatha Dé Danann, he is often considered a deity and may be related to the Gallic god Ogmios.\n\nHe fights in the first battle of Magh Tuiredh, when the Tuatha Dé take Ireland from the Fir Bolg. Under the reign of Bres, when the Tuatha Dé are reduced to servitude, Ogma is forced to carry firewood, but nonetheless is the only one of the Tuatha Dé who proves his athletic and martial prowess in contests before the king. When Bres is overthrown and Nuadu restored, Ogma is his champion. His position is threatened by the arrival of Lugh at the court, so Ogma challenges him by lifting a great flagstone, which normally required eighty oxen to move it, and hurling it out of Tara, but Lugh answers the challenge by hurling it back. When Nuadu hands command of the Battle of Mag Tuired to Lugh, Ogma becomes Lugh's champion, and promises to repel the Fomorian king, Indech, and his bodyguard, and to defeat a third of the enemy. During the battle he finds Orna, the sword of the Fomorian king Tethra, which recounts the deeds done with it when unsheathed. During the battle Ogma and Indech fall in single combat, although there is some confusion in the texts as in \"Cath Maige Tuired\" Ogma, Lugh and the Dagda pursue the Fomorians after the battle to recover the harp of Uaitne, the Dagda's harper.\n\nHe often appears as a triad with Lugh and the Dagda (The Dagda is his brother and Lugh is his half-brother), who are sometimes collectively known as the \"trí dée dána\" or three gods of skill, although that designation is elsewhere applied to other groups of characters. His father is Elatha and his mother is usually given as Ethliu, sometimes as Étaín. His sons include Delbaeth and Tuireann. He is said to have invented the Ogham alphabet, which is named after him.\n\nScholars of Celtic mythology have proposed that Ogma represents the vestiges of an ancient Celtic god. By virtue of his battle prowess and invention of Ogham, he is compared with Ogmios, a Gaulish deity associated with eloquence and equated with Herakles. J. A. MacCulloch compares Ogma's epithet \"grianainech\" (sun-face) with Lucian's description of the \"smiling face\" of Ogmios, and suggests Ogma's position as champion of the Tuatha Dé Danann may derive \"from the primitive custom of rousing the warriors' emotions by eloquent speeches before a battle\", although this is hardly supported by the texts. Scholars such Rudolf Thurneysen and Anton van Hamel dispute any link between Ogma and Ogmios.\n"}
{"id": "3345809", "url": "https://en.wikipedia.org/wiki?curid=3345809", "title": "Parikshit", "text": "Parikshit\n\nPariksit (Sanskrit: परिक्षित्, ) was a Kuru king who reigned during the Middle Vedic period (12th-9th centuries BCE). Along with his son and successor Janamejaya, he played a decisive role in the consolidation of the Kuru state, the arrangement of Vedic hymns into collections, and the development of the orthodox srauta ritual, transforming the Kuru realm into the dominant political and cultural center of northern Iron Age India.\n\nHe also appears as a figure in later legends and traditions. According to the \"Mahabharata\" and the Puranas, he succeeded his greatuncle Yudhishthira to the throne of Hastinapur.\n\n\"Listen to the good praise of the King belonging to all people, who, (like) a god, is above men,\n(listen to the praise) of Parikṣit! - ‘Parikṣit has just now made us peaceful dwelling; darkness\nhas just now run to its dwelling.’ The Kuru householder, preparing (grains) for milling, speaks\n(thus) with his wife. — ‘What shall I bring you, sour milk, the \"mantha\" [a barley/milk drink?' the wife keeps asking in the Realm of King Pariksit. — By itself, the ripe\nbarley bends heavily (\"iva\") over the deep track of the path. The dynasty thrives auspiciously in the\nRealm of King Parikṣit.”\nParikshit is eulogised in a hymn of the \"Atharvaveda\" (XX.127.7-10) as a great Kuru king (\"Kauravya\"), whose realm flowed with milk and honey and people lived happily in his kingdom. He is mentioned as the \"raja vishvajanina\" (universal king).\n\nFew other details about his reign are recorded in Vedic literature. According to the Mahabharata, Parikshit married princess Madravati of the Madra Kingdom, reigned for 24 years, and died at the age of 60, but this is a much later text and cannot be confirmed as historical fact.\n\nMichael Witzel dates the Pārikṣita Dynasty of the Kuru Kingdom to the 12th-11th centuries BC. H.C. Raychaudhuri dates Parikshit in ninth century BC. He was succeeded by his son Janamejaya.\n\nOnly one Parikshit is mentioned in Vedic literature; however, post-Vedic literature (Mahabharata and Puranas) seems to indicate the existence of two kings by this name, one who lived before the Kurukshetra War was an ancestor to the Pandavas, and one who lived later and was a descendant. Historian H. C. Raychaudhuri believes that the second Parikshit's description better corresponds to the Vedic king, whereas the information available about the first is scanty and inconsistent, but Raychaudhuri questions whether there were actually two distinct kings. He suggests that the doubling was eventually \"invented by genealogists to account for anachronisms\" in the later parts of the Mahabharata, as \"a bardic duplication of the same original individual regarding whose exact place in the Kuru genealogy no unanimous tradition had survived,\" and therefore there \"is an intrusion into the genealogical texts\" of the late, post-Vedic tradition, which also has two of Parikshit's son Janamejaya.\n\nThere is no unanimity regarding the father of Parikshit among epics and Puranas. He is depicted as the son of Avikshit, Anasva, Kuru or Abhimanyu, but is more popular as Abhimanyu's posthumous son.\n\nAccording to the \"Shatapatha Brahmana\" (XIII.5.4), Parikshita had four sons, Janamejaya, Bhimasena, Ugrasena and Śrutasena. All of them performed the \"Asvamedha Yajna\" (horse sacrifice).\n\nHis bodily existence ended due to the curse of a Brahmana, who used the Nāga king, Takshaka, the ruler of Taxila as the instrument of death. Parikshit was the husband of Queen Iravati and was succeeded by his son Janamejaya. According to the \"Mahabharata\", he ruled for 24 years and died at the age of sixty.\n\nA thesis based upon Ugrasravas’ narration suggests an alternate interpretation regarding Parikshit’s lineage. In this interpretation, Parikshit fathered a firstborn son with an unnamed putrika wife. Albeit the child was Parikshit’s firstborn, he was the son of a putrika and therefore could not succeed his father on the throne as he was to be the heir of his maternal grandfather. This son’s name was Sringin; his maternal grandfather was Samika. As this would leave Parikshit without an heir, he had another son, Janamejaya, with a second wife, Madravati. Sringin and Samika are seen again in the hunting story that results in Parikshit’s demise. Their relationship served an additional motive for Sringin to murder Parikshit.\n\nThe Bhagavata Purana (1.8.9) states that the son of Drona, Ashwatthama had prepared a Brahmastra (a powerful weapon summoned to Brahma) to kill King Parikshit while he was in his mother's (Uttarā) womb, as a revenge against the Pandavas for killing his relatives (especially his father) in the Kurukshetra war. Uttarā was terrified by the powerful rays of the weapon and worried about her child, she prayed to her uncle-in-law Krishna for help. Krishna pacified her and protected the child in the womb from the deadly weapon and thus saved his life. Parikshit was thus born to Uttara and later was throned as the heir to the Pandavas at Hastinapura.\n\nThere seem to be two Parikhits and two Janamejayas, former being referred to in Vedas and the latter in the Puranic literature. The following is about the Puranic king.\n\nOn hearing this, Parikshit's son Janamejaya II vowed to kill Takshaka within a week. He starts the Sarpamedha Yajna, which forced each and every snake of the entire universe to fall in the havan kund. However one snake got stuck around Surya's chariot and because of the force of Yajna the chariot was also getting pulled inside the hawankund. This could have ended up taking the Surya's chariot in the sacrificial altar and ending the regime of Sun from the universe. This resulted in plea from all the gods to stop the sacrifice. When Takshaka arrived then this Yajna was stopped from doing so by Astika Muni, as a result of which Takshaka lived. That day was Shukla Paksha Panchami in the month of Shravan and is since celebrated as the festival of Nag Panchami.\n\n\n"}
{"id": "1239866", "url": "https://en.wikipedia.org/wiki?curid=1239866", "title": "Population history of indigenous peoples of the Americas", "text": "Population history of indigenous peoples of the Americas\n\nThe population figure of indigenous peoples of the Americas before the 1492 voyage of Christopher Columbus have proven difficult to establish. Scholars rely on archaeological data and written records from settlers from Europe. Most scholars writing at the end of the 19th century estimated that the pre-Columbian population was as low as 10 million; by the end of the 20th century most scholars gravitated to a middle estimate of around 50 million, with some historians arguing for an estimate of 100 million or more. Contact with the Europeans led to the European colonization of the Americas, in which millions of immigrants from Europe eventually settled in the Americas.\n\nThe population of African and Eurasian peoples in the Americas grew steadily, while the indigenous population plummeted. Eurasian diseases such as influenza, bubonic plague and pneumonic plagues, yellow fever, smallpox, and malaria devastated the Native Americans, who did not have immunity to them. Conflict and outright warfare with Western European newcomers and other American tribes further reduced populations and disrupted traditional societies. The extent and causes of the decline have long been a subject of academic debate, along with its characterization as a genocide.\n\nGiven the fragmentary nature of the evidence, even semi-accurate pre-Columbian population figures are impossible to obtain. Scholars have varied widely on the estimated size of the indigenous populations prior to colonization and on the effects of European contact. Estimates are made by extrapolations from small bits of data. In 1976, geographer William Denevan used the existing estimates to derive a \"consensus count\" of about 54 million people. Nonetheless, more recent estimates still range widely.\n\nUsing an estimate of approximately 37 million people in Mexico, Central and South America in 1492 (including 6 million in the Aztec Empire, 5-10 million in the Mayan States, 11 million in what is now Brazil, and 12 million in the Inca Empire), the lowest estimates give a death toll due from disease of 90% by the end of the 17th century (nine million people in 1650). Latin America would match its 15th-century population early in the 19th century; it numbered 17 million in 1800, 30 million in 1850, 61 million in 1900, 105 million in 1930, 218 million in 1960, 361 million in 1980, and 563 million in 2005. In the last three decades of the 16th century, the population of present-day Mexico dropped to about one million people. The Maya population is today estimated at six million, which is about the same as at the end of the 15th century, according to some estimates. In what is now Brazil, the indigenous population declined from a pre-Columbian high of an estimated four million to some 300,000.\n\nWhile it is difficult to determine exactly how many Natives lived in North America before Columbus, estimates range from a low of 2.1 million to 7 million people to a high of 18 million.\n\nThe aboriginal population of Canada during the late 15th century is estimated to have been between 200,000 and two million, with a figure of 500,000 currently accepted by Canada's Royal Commission on Aboriginal Health. Repeated outbreaks of Old World infectious diseases such as influenza, measles and smallpox (to which they had no natural immunity), were the main cause of depopulation. This combined with other factors such as dispossession from European/Canadian settlements and numerous violent conflicts resulted in a forty- to eighty-percent aboriginal population decrease after contact. For example, during the late 1630s, smallpox killed over half of the Wyandot (Huron), who controlled most of the early North American fur trade in what became Canada. They were reduced to fewer than 10,000 people.\n\nHistorian David Henige has argued that many population figures are the result of arbitrary formulas selectively applied to numbers from unreliable historical sources. He believes this is a weakness unrecognized by several contributors to the field, and insists there is not sufficient evidence to produce population numbers that have any real meaning. He characterizes the modern trend of high estimates as \"pseudo-scientific number-crunching.\" Henige does not advocate a low population estimate, but argues that the scanty and unreliable nature of the evidence renders broad estimates inevitably suspect, saying \"high counters\" (as he calls them) have been particularly flagrant in their misuse of sources. Many population studies acknowledge the inherent difficulties in producing reliable statistics, given the scarcity of hard data.\n\nThe population debate has often had ideological underpinnings. Low estimates were sometimes reflective of European notions of cultural and racial superiority. Historian Francis Jennings argued, \"Scholarly wisdom long held that Indians were so inferior in mind and works that they could not possibly have created or sustained large populations.\"\n\nThe indigenous population of the Americas in 1492 was not necessarily at a high point and may actually have been in decline in some areas. Indigenous populations in most areas of the Americas reached a low point by the early 20th century. In most cases, populations have since begun to climb.\n\nGenetic diversity and population structure in the American land mass using DNA micro-satellite markers (genotype) sampled from North, Central, and South America have been analyzed against similar data available from other indigenous populations worldwide. The Amerindian populations show a lower genetic diversity than populations from other continental regions. Observed is both a decreasing genetic diversity as geographic distance from the Bering Strait occurs and a decreasing genetic similarity to Siberian populations from Alaska (genetic entry point). Also observed is evidence of a higher level of diversity and lower level of population structure in western South America compared to eastern South America. A relative lack of differentiation between Mesoamerican and Andean populations is a scenario that implies coastal routes were easier than inland routes for migrating peoples (Paleo-Indians) to traverse. The overall pattern that is emerging suggests that the Americas were recently colonized by a small number of individuals (effective size of about 70-250), and then they grew by a factor of 10 over 800 – 1000 years. The data also show that there have been genetic exchanges between Asia, the Arctic and Greenland since the initial peopling of the Americas. A new study in early 2018 suggests that the effective population size of the original founding population of Native Americans was about 250 people.\n\nAccording to Noble David Cook, a community of scholars have recently, albeit slowly, \"been quietly accumulating piece by piece data on early epidemics in the Americas and their relation to subjugation of native peoples.\" They now believe that widespread epidemic disease, to which the natives had no prior exposure or resistance, was the primary cause of the massive population decline of the Native Americans. Earlier explanations for the population decline of the American natives include the European immigrants' accounts of the brutal practices of the Spanish conquistadores, as recorded by the Spaniards themselves. This was applied through the encomienda, which was a system ostensibly set up to protect people from warring tribes as well as to teach them the Spanish language and the Catholic religion, but in practice was tantamount to serfdom and slavery. The most notable account was that of the Dominican friar Bartolomé de las Casas, whose writings vividly depict Spanish atrocities committed in particular against the Taínos. It took five years for the Taíno rebellion to be quelled by both the \"Real Audiencia\"—through diplomatic sabotage, and through the Indian auxiliaries fighting with the Spanish. After Emperor Charles V personally eradicated the notion of the encomienda system as a use for slave labour, there were not enough Spanish to have caused such a large population decline. The second European explanation was a perceived divine approval, in which God removed the natives as part of His \"divine plan\" to make way for a new Christian civilization. Many Native Americans viewed their troubles in terms of religious or supernatural causes within their own belief systems.\n\nSoon after Europeans and enslaved Africans arrived in the New World, bringing with them the infectious diseases of Europe and Africa, observers noted immense numbers of indigenous Americans began to die from these diseases. One reason this death toll was overlooked is that once introduced, the diseases raced ahead of European immigration in many areas. Disease killed a sizable portion of the populations before European written records were made. After the epidemics had already killed massive numbers of natives, many newer European immigrants assumed that there had always been relatively few indigenous peoples. The scope of the epidemics over the years was tremendous, killing millions of people—possibly in excess of 90% of the population in the hardest hit areas—and creating one of \"the greatest human catastrophe in history, far exceeding even the disaster of the Black Death of medieval Europe\", which had killed up to one-third of the people in Europe and Asia between 1347 and 1351.\n\nOne of the most devastating diseases was smallpox, but other deadly diseases included typhus, measles, influenza, bubonic plague, cholera, malaria, tuberculosis, mumps, yellow fever and pertussis, which were chronic in Eurasia.\n\nThis transfer of disease between the Old and New Worlds was later studied as part of what has been labeled the \"Columbian Exchange\".\n\nThe epidemics had very different effects in different regions of the Americas. The most vulnerable groups were those with a relatively small population and few built-up immunities. Many island-based groups were annihilated. The Caribs and Arawaks of the Caribbean nearly ceased to exist, as did the Beothuks of Newfoundland. While disease raged swiftly through the densely populated empires of Mesoamerica, the more scattered populations of North America saw a slower spread.\n\nViral and bacterial diseases that kill victims before the illnesses spread to others tend to flare up and then die out. A more resilient disease would establish an equilibrium; if its victims lived beyond infection, the disease would spread further. The evolutionary process selects against quick lethality, with the most immediately fatal diseases being the most short-lived. A similar evolutionary pressure acts upon victim populations, as those lacking genetic resistance to common diseases die and do not leave descendants, whereas those who are resistant procreate and pass resistant genes to their offspring. For example, in the first fifty years of the sixteenth century, an unusually strong strain of syphilis killed a high proportion of infected Europeans within a few months; over time, however, the disease has become much less virulent.\n\nThus both infectious diseases and populations tend to evolve towards an equilibrium in which the common diseases are non-symptomatic, mild or manageably chronic. When a population that has been relatively isolated is exposed to new diseases, it has no resistance to the new diseases (the population is \"biologically naive\"). These people die at a much higher rate, resulting in what is known as a \"virgin soil\" epidemic. Before the European arrival, the Americas had been isolated from the Eurasian-African landmass. The peoples of the Old World had had thousands of years for their populations to accommodate to their common diseases.\n\nThe fact that all members of an immunologically naive population are exposed to a new disease simultaneously increases the fatalities. In populations where the disease is endemic, generations of individuals acquired immunity; most adults had exposure to the disease at a young age. Because they were resistant to reinfection, they are able to care for individuals who caught the disease for the first time, including the next generation of children. With proper care, many of these \"childhood diseases\" are often survivable. In a naive population, all age groups are affected at once, leaving few or no healthy caregivers to nurse the sick. With no resistant individuals healthy enough to tend to the ill, a disease may have higher fatalities.\n\nThe natives of the Americas were faced with several new diseases at once creating a situation where some who successfully resisted one disease might die from another. Multiple simultaneous infections (e.g., smallpox and typhus at the same time) or in close succession (e.g., smallpox in an individual who was still weak from a recent bout of typhus) are more deadly than just the sum of the individual diseases. In this scenario, death rates can also be elevated by combinations of new and familiar diseases: smallpox in combination with American strains of yaws, for example.\n\nOther contributing factors:\n\n\nWhen Old World diseases were first carried to the Americas at the end of the fifteenth century, they spread throughout the southern and northern hemispheres, leaving the indigenous populations in near ruins. No evidence has been discovered that the earliest Spanish colonists and missionaries deliberately attempted to infect the American natives, and some effort was actually made to limit the devastating effects of disease before it killed off what remained of their forced slave labor under their encomienda system. The cattle introduced by the Spanish contaminated various water reserves which Native Americans dug in the fields to accumulate rain water. In response, the Franciscans and Dominicans created public fountains and aqueducts to guarantee access to drinking water. But when the Franciscans lost their privileges in 1572, many of these fountains were not guarded any more and deliberate well poisoning may have happened. Although no proof of such poisoning has been found, some historians believe the decrease of the population correlates with the end of religious orders' control of the water.\n\nIn the centuries that followed, accusations and discussions of biological warfare were common. Well-documented accounts of incidents involving both threats and acts of deliberate infection are very rare, but may have occurred more frequently than scholars have previously acknowledged. Many of the instances likely went unreported, and it is possible that documents relating to such acts were deliberately destroyed, or sanitized. By the middle of the 18th century, colonists had the knowledge and technology to attempt biological warfare with the smallpox virus. They well understood the concept of quarantine, and that contact with the sick could infect the healthy with smallpox, and those who survived the illness would not be infected again. Whether the threats were carried out, or how effective individual attempts were, is uncertain.\n\nOne such threat was delivered by fur trader James McDougall, who is quoted as saying to a gathering of local chiefs, \"You know the smallpox. Listen: I am the smallpox chief. In this bottle I have it confined. All I have to do is to pull the cork, send it forth among you, and you are dead men. But this is for my enemies and not my friends.\" Likewise, another fur trader threatened Pawnee Indians that if they didn't agree to certain conditions, \"he would let the smallpox out of a bottle and destroy them.\" The Reverend Isaac McCoy was quoted in his \"History of Baptist Indian Missions\" as saying that the white men had deliberately spread smallpox among the Indians of the southwest, including the Pawnee tribe, and the havoc it made was reported to General Clark and the Secretary of War. Artist and writer George Catlin observed that Native Americans were also suspicious of vaccination, \"They see white men urging the operation so earnestly they decide that it must be some new mode or trick of the pale face by which they hope to gain some new advantage over them.\" So great was the distrust of the settlers that the Mandan chief Four Bears denounced the white man, whom he had previously treated as brothers, for deliberately bringing the disease to his people.\n\nDuring the Seven Years' War, British militia took blankets from their smallpox hospital and gave them as gifts to two neutral Lenape Indian dignitaries during a peace settlement negotiation, according to the entry in the Captain's ledger, \"To convey the Smallpox to the Indians\". In the following weeks, the high commander of the British forces in North America conspired with his Colonel to \"Extirpate this Execreble Race\" of Native Americans, writing, \"Could it not be contrived to send the small pox among the disaffected tribes of Indians? We must on this occasion use every stratagem in our power to reduce them.\" His Colonel agreed to try. Most scholars have asserted that the 1837 Great Plains smallpox epidemic was \"started among the tribes of the upper Missouri River by failure to quarantine steam boats on the river\", and Captain Pratt of the \"St. Peter\" \"was guilty of contributing to the deaths of thousands of innocent people. The law calls his offense criminal negligence. Yet in light of all the deaths, the almost complete annihilation of the Mandans, and the terrible suffering the region endured, the label criminal negligence is benign, hardly befitting an action that had such horrendous consequences.\" However, some sources attribute the 1836–40 epidemic to the deliberate communication of smallpox to Native Americans, with historian Ann F. Ramenofsky writing, \"\"Variola Major\" can be transmitted through contaminated articles such as clothing or blankets. In the nineteenth century, the U. S. Army sent contaminated blankets to Native Americans, especially Plains groups, to control the Indian problem.\" Well into the 20th century, deliberate infection attacks continued as Brazilian settlers and miners transported infections intentionally to the native groups whose lands they coveted.\"\n\nAfter Edward Jenner's 1796 demonstration that the smallpox vaccination worked, the technique became better known and smallpox became less deadly in the United States and elsewhere. Many colonists and natives were vaccinated, although, in some cases, officials tried to vaccinate natives only to discover that the disease was too widespread to stop. At other times, trade demands led to broken quarantines. In other cases, natives refused vaccination because of suspicion of whites. The first international healthcare expedition in history was the Balmis expedition which had the aim of vaccinating indigenous peoples against smallpox all along the Spanish Empire in 1803. In 1831, government officials vaccinated the Yankton Sioux at Sioux Agency. The Santee Sioux refused vaccination and many died.\n\nWhile epidemic disease was a leading factor of the population decline of the American indigenous peoples after 1492, there were other contributing factors, all of them related to European contact and colonization. One of these factors was warfare. According to demographer Russell Thornton, although many lives were lost in wars over the centuries, and war sometimes contributed to the near extinction of certain tribes, warfare and death by other violent means was a comparatively minor cause of overall native population decline.\n\nFrom the U.S. Bureau of the Census in 1894: \"The Indian wars under the government of the United States have been more than 40 in number. They have cost the lives of about 19,000 white men, women and children, including those killed in individual combats, and the lives of about 30,000 Indians. The actual number of killed and wounded Indians must be very much higher than the given... Fifty percent additional would be a safe estimate...\"\n\nThere is some disagreement among scholars about how widespread warfare was in pre-Columbian America, but there is general agreement that war became deadlier after the arrival of the Europeans and their firearms. The South or Central American infrastructure allowed for thousands of European conquistadors and tens of thousands of their Indian auxiliaries to attack the dominant indigenous civilization. Empires such as the Incas depended on a highly centralized administration for the distribution of resources. Disruption caused by the war and the colonization hampered the traditional economy, and possibly led to shortages of food and materials. The Arauco War, Chichimeca War, Red Cloud's War, Seminole Wars, War of 1812, Pontiac's Rebellion, Beaver Wars, French-Indian War, American Civil War, American Revolution, Modoc War, Oka Crisis, Battle of Cut Knife, all represented either pyrrhic victories by colonial forces, outright defeat, military stalemates, or further alliance-politics. Across the western hemisphere, war with various Native American civilizations constituted alliances based out of both necessity or economic prosperity and, resulted in mass-scale intertribal warfare. European colonization in the North American continent also contributed to a number of wars between Native Americans, who fought over which of them should have first access to new technology and weaponry—like in the Beaver Wars.\n\nSome Spaniards objected to the encomienda system, notably Bartolomé de las Casas, who insisted that the Indians were humans with souls and rights. Due to many revolts and military encounters, Emperor Charles V helped relieve the strain on both the Indian laborers and the Spanish vanguards probing the Caribana for military and diplomatic purposes. Later on New Laws were promulgated in Spain in 1542 to protect isolated natives, but the abuses in the Americas were never entirely or permanently abolished. The Spanish also employed the pre-Columbian draft system called the \"mita\", and treated their subjects as something between slaves and serfs. Serfs stayed to work the land; slaves were exported to the mines, where large numbers of them died. In other areas the Spaniards replaced the ruling Aztecs and Incas and divided the conquered lands among themselves ruling as the new feudal lords with often, but unsuccessful lobbying to the viceroys of the Spanish crown to pay Tlaxcalan war demnities. The infamous Bandeirantes from São Paulo, adventurers mostly of mixed Portuguese and native ancestry, penetrated steadily westward in their search for Indian slaves. Serfdom existed as such in parts of Latin America well into the 19th century, past independence.\n\nFriar Bartolomé de las Casas and Antonius Flávio Chesta (Tony Chesta) and other dissenting Spaniards from the colonial period described the manner in which the natives were treated by colonials. This has helped to create an image of the Spanish conquistadores as cruel in the extreme.\n\nGreat revenues were drawn from Hispaniola so the advent of losing manpower didn't benefit the Spanish crown. At best, the reinforcement of vanguards sent by the Council of the Indies to explore the Caribana country and gather information on alliances or hostilities was the main goal of the local viceroys and their adelantados. Although mass killings and atrocities were not a significant factor in native depopulation, no mainstream scholar dismisses the sometimes humiliating circumstances now believed to be precipitated by civil disorder as well as Spanish cruelty.\n\n\nThe populations of many Native American peoples were reduced by the common practice of intermarrying with Europeans. Although many Indian cultures that once thrived are extinct today, their descendants exist today in some of the bloodlines of the current inhabitants of the Americas.\n\nOn 8 September 2000, the head of the United States Bureau of Indian Affairs (BIA) formally apologized for the agency's participation in the \"ethnic cleansing\" of Western tribes.\n\n\n\n\n\n"}
{"id": "158687", "url": "https://en.wikipedia.org/wiki?curid=158687", "title": "Pre-Columbian trans-oceanic contact theories", "text": "Pre-Columbian trans-oceanic contact theories\n\nPre-Columbian trans-oceanic contact theories relate to visits or interactions with the Americas and/or indigenous peoples of the Americas by people from Africa, Asia, Europe, or Oceania before Columbus's first voyage to the Caribbean in 1492. Such contact is generally accepted in prehistory, but has been hotly debated in the historic period.\n\nTwo historical cases of pre-Columbian contact are accepted amongst the scientific and scholarly mainstream. Successful explorations led to Norse settlement of Greenland and the L'Anse aux Meadows settlement in Newfoundland some 500 years before Columbus.\n\nThe scientific and scholarly responses to other post-prehistory, pre-Columbian contact claims have varied. Some such contact claims are examined in reputable peer-reviewed sources. Other contact claims, typically based on circumstantial and ambiguous interpretations of archaeological finds, cultural comparisons, comments in historical documents, and narrative accounts, have been dismissed as fringe science or pseudoarcheology.\n\nNorse journeys to Greenland and Canada are supported by historical and archaeological evidence. A Norse colony in Greenland was established in the late 10th century, and lasted until the mid 15th century, with court and parliament assemblies (\"þing\") taking place at Brattahlíð and a bishop at Garðar. The remains of a Norse settlement at L'Anse aux Meadows in Newfoundland, Canada, were discovered in 1960 and are dated to around the year 1000 (carbon dating estimate 990–1050 CE), L'Anse aux Meadows is the only site widely accepted as evidence of pre-Columbian trans-oceanic contact. It was named a World Heritage site by UNESCO in 1978. It is also notable for its possible connection with the attempted colony of Vinland established by Leif Erikson around the same period or, more broadly, with Norse exploration of the Americas.\n\nFew sources describing contact between indigenous peoples and Norse people exist. Contact between the Thule people (ancestors of the modern Inuit) and Norse between the 12th or 13th centuries is known. The Norse Greenlanders called these incoming settlers \"skrælingar\". Conflict between the Greenlanders and the \"skrælings\" is recorded in the \"Icelandic Annals\". The term skrælings is also used in the Vínland sagas, which relate to events during the 10th century, when describing trade and conflict with native peoples.\n\nThe sweet potato, which is native to the Americas, was widespread in Polynesia when Europeans first reached the Pacific. Sweet potato has been radiocarbon-dated in the Cook Islands to 1000 CE, and current thinking is that it was brought to central Polynesia c. 700 CE and spread across Polynesia from there. It has been suggested that it was brought by Polynesians who had traveled to South America and back, or that South Americans brought it to the Pacific. It is possible that the plant could successfully float across the ocean if discarded from the cargo of a boat. Phylogenetic analysis supports the hypothesis of at least two separate introductions of sweet potatoes from South America into Polynesia, including one before and one after European contact. (see also #Linguistics.)\n\nA team of academics headed by the University of York's Mummy Research Group and BioArch, while examining a Peruvian mummy at the Bolton Museum, found that it had been embalmed using a tree resin. Before this it was thought that Peruvian mummies were naturally preserved. The resin, found to be that of an \"Araucaria\" conifer related to the 'monkey puzzle tree', was from a variety found only in Oceania and probably New Guinea. \"Radiocarbon dating of both the resin and body by the University of Oxford's radiocarbon laboratory confirmed they were essentially contemporary, and date to around CE 1200.\"\n\nResearchers including Kathryn Klar and Terry Jones have proposed a theory of contact between Hawaiians and the Chumash people of Southern California between 400 and 800 CE. The sewn-plank canoes crafted by the Chumash and neighboring Tongva are unique among the indigenous peoples of North America, but similar in design to larger canoes used by Polynesians for deep-sea voyages. \"Tomolo'o\", the Chumash word for such a craft, may derive from \"kumula'au\", the Hawaiian term for the logs from which shipwrights carve planks to be sewn into canoes. The analogous Tongva term, \"tii'at\", is unrelated. If it occurred, this contact left no genetic legacy in California or Hawaii. This theory has attracted limited media attention within California, but most archaeologists of the Tongva and Chumash cultures reject it on the grounds that the independent development of the sewn-plank canoe over several centuries is well-represented in the material record.\n\nThe existence of chicken bones dating from 1321 to 1407 in Chile and thought to be genetically linked to South Pacific Island chicken landraces suggested further evidence of South Pacific contact with South America. The genetic link between the South American Mapuche (to whom the chickens were thought to originally belong) chicken bones and South Pacific Island species has been rejected by a more recent genetic study which concluded that \"The analysis of ancient and modern specimens reveals a unique Polynesian genetic signature\" and that \"a previously reported connection between pre-European South America and Polynesian chickens most likely resulted from contamination with modern DNA, and that this issue is likely to confound ancient DNA studies involving haplogroup E chicken sequences.\"\n\nIn recent years, evidence has emerged suggesting a possibility of pre-Columbian contact between the Mapuche people (Araucanians) of south-central Chile and Polynesians. Chicken bones found at the site El Arenal in the Arauco Peninsula, an area inhabited by Mapuche, support a pre-Columbian introduction of chicken to South America. The bones found in Chile were radiocarbon-dated to between 1304 and 1424, before the arrival of the Spanish. Chicken DNA sequences taken were matched to those of chickens in American Samoa and Tonga, and dissimilar to European chicken. However, a later report in the same journal looking at the same mtDNA concluded that the Chilean chicken specimen clusters with the same European/Indian subcontinental/Southeast Asian sequences, providing no support for a Polynesian introduction of chickens to South America.\n\nDutch linguists and specialists in Amerindian languages Willem Adelaar and Pieter Muysken have suggested that two lexical items may be shared by Polynesian languages and languages of South America. One is the name of the sweet potato, which was domesticated in the New World. Proto-Polynesian *\"kumala\" (compare Easter Island \"kumara\", Hawaiian \"ʻuala\", Māori \"kumāra\"; apparent cognates outside Eastern Polynesian may be borrowed from Eastern Polynesian languages, calling Proto-Polynesian status and age into question) may be connected with Quechua and Aymara \"k’umar ~ k’umara\". A possible second is the word for 'stone axe', Easter Island \"toki\", New Zealand Maori \"toki\" 'adze', Mapuche \"toki\", and further afield, Yurumanguí \"totoki\" 'axe'. According to Adelaar and Muysken, the similarity in the word for sweet potato \"constitutes near proof of incidental contact between inhabitants of the Andean region and the South Pacific\", though according to Adelaar and Muysken the word for axe is not as convincing. The authors argue that the presence of the word for sweet potato suggests sporadic contact between Polynesia and South America, but no migrations.\n\nIn December 2007, several human skulls were found in a museum in Concepción, Chile. These skulls originated from Mocha Island, an island just off the coast of Chile in the Pacific Ocean, formerly inhabited by the Mapuche. Craniometric analysis of the skulls, according to Lisa Matisoo-Smith of the University of Otago and José Miguel Ramírez Aliaga of the Universidad de Valparaíso, suggests that the skulls have \"Polynesian features\" – such as a pentagonal shape when viewed from behind, and rocker jaws.\n\nFrom 2007 to 2009, geneticist Erik Thorsby and colleagues have published two studies in \"Tissue Antigens\" that evidence an Amerindian genetic contribution to the Easter Island population, determining that it was probably introduced before European discovery of the island.\n\nIn 2014, geneticist Anna-Sapfo Malaspinas of The Center for GeoGenetics at the University of Copenhagen published a study in \"Current Biology\" that found human genetic evidence of contact between the populations of Easter Island and South America, approximately 600 years ago (i.e. 1400 CE ± 100 years).\n\nSome members of the now extinct Botocudo people, who lived in the interior of Brazil, were found in research published in 2013 to have been members of mtDNA haplogroup B4a1a1, which is normally found only among Polynesians and other subgroups of Austronesians. This was based on an analysis of fourteen skulls. Two belonged to B4a1a1 (while twelve belonged to subclades of mtDNA Haplogroup C1 common among Native Americans). The research team examined various scenarios, none of which they could say for certain were correct. They dismissed a scenario of direct contact in prehistory between Polynesia and Brazil as \"too unlikely to be seriously entertained.\" While B4a1a1 is also found among the Malagasy people of Madagascar (which experienced significant Austronesian settlement in prehistory), the authors described as \"fanciful\" suggestions that B4a1a1 among the Botocudo resulted from the African slave trade (which included Madagascar).\n\nA genetic study published in \"Nature\" in July 2015 stated that \"some Amazonian Native Americans descend partly from a ... founding population that carried ancestry more closely related to indigenous Australians, New Guineans and Andaman Islanders than to any present-day Eurasians or Native Americans\". The authors, who included David Reich, added: \"This signature is not present to the same extent, or at all, in present-day Northern and Central Americans or in a ~12,600-year-old Clovis-associated genome, suggesting a more diverse set of founding populations of the Americas than previously accepted.\" This appears to conflict with an article published roughly simultaneously in \"Science\" which adopts the previous consensus perspective. The ancestors of all Native Americans entered the Americas as a single migration wave from Siberia no earlier than ~23 ka, separate from the Inuit and diversified into \"northern\" and \"southern\" Native American branches ~13 ka. There is evidence of post-divergence gene flow between some Native Americans and groups related to East Asians/Inuit and Australo-Melanesians .\n\nSimilar cultures of peoples across the Bering Strait in both Siberia and Alaska suggest human travel between the two places ever since the strait was formed. After Paleo-Indians arrived during the Ice Age and began the settlement of the Americas, a second wave of people from Asia came to Alaska around 8000 BC. These \"Na-Dene\" peoples, who share many linguistic and genetic similarities not found in other parts of the Americas, populated the far north of the Americas and only made it as south as Oasisamerica. By 4000 BC, it is theorized, \"Eskimo\" peoples began coming to the Americas from Siberia. \"Eskimo\" tribes live today in both Asia and North America and there is much evidence they lived in Asia even in prehistoric times.\n\nBronze artifacts discovered in a 1,000-year-old house in Alaska suggest pre-Columbian trade. Bronze working had not been developed in Alaska at the time and suggest the bronze came from nearby Asia—possibly China, Korea, or Russia. Also inside the house were found the remains of obsidian artifacts, which have a chemical signature that indicates the obsidian is from the Anadyr River valley in Russia.\n\nA 2013 genetic study suggests the possibility of contact between Ecuador and East Asia. The study suggests that the contact could have been trans-oceanic or a late-stage coastal migration that did not leave genetic imprints in North America. This contact could explain the alleged similarity between the pottery of the Valdivia culture of Ecuador and the Jōmon culture of Northeast Asia.\n\nOther researchers have argued that the Olmec civilization came into existence with the help of Chinese refugees, particularly at the end of the Shang dynasty. In 1975, Betty Meggers of the Smithsonian Institution argued that the Olmec civilization originated due to Shang Chinese influences around 1200 BCE. In a 1996 book, Mike Xu, with the aid of Chen Hanping, claimed that celts from La Venta bear Chinese characters. These claims are unsupported by mainstream Mesoamerican researchers.\n\nOther claims have been made for early Chinese contact with North America.\n\nIn 1882 artifacts identified at the time as Chinese coins were discovered in British Columbia. A contemporary account states that:In the summer of 1882 a miner found on De Foe (Deorse?) creek, Cassiar district, Br. Columbia, thirty Chinese coins in the auriferous sand, twenty-five feet below the surface. They appeared to have been strung, but on taking them up the miner let them drop apart. The earth above and around them was as compact as any in the neighborhood. One of these coins I examined at the store of Chu Chong in Victoria. Neither in metal nor markings did it resemble the modern coins, but in its figures looked more like an Aztec calendar. So far as I can make out the markings, this is a Chinese chronological cycle of sixty years, invented by Emperor Huungti, 2637 BCE, and circulated in this form to make his people remember it. In 1885, a vase containing similar discs was also discovered, wrapped in the roots of a tree around 300 years old. Grant Keddie, Curator of Archeology at the Royal BC Museum, examined a photograph of a coin from Cassiar taken in the 1940s (whereabouts now unknown) and he believes that the character style and the evidence that it was machine-ground show it to be a 19th-century copy of a Ming Dynasty temple token.\n\nA group of Chinese Buddhist missionaries led by Hui Shen before 500 CE claimed to have visited a location called Fusang. Although Chinese mapmakers placed this territory on the Asian coast, others have suggested as early as the 1800s that Fusang might have been in North America, due to perceived similarities between portions of the California coast and Fusang as depicted by Asian sources.\n\nIn his book \"1421: The Year China Discovered the World\", the British author Gavin Menzies made the controversial claim that the fleet of Zheng He arrived in America in 1421. Professional historians contend that Zheng He reached the eastern coast of Africa, and dismiss Menzies's hypothesis as entirely without proof.\n\nIn 1973 and 1975 doughnut-shaped stones were discovered off the coast of California that resembled stone anchors used by Chinese fishermen. These (sometimes called the \"Palos Verdes stones\") were initially thought to be up to 1500 years old and proof of pre-Columbian contact by Chinese sailors. Later geological investigations showed them to be a local rock known as Monterey shale, and they are thought to have been used by Chinese settlers fishing off the coast in the nineteenth century.\n\nIn June 2016, Purdue University published the results of research on six metal and composite metal artifacts excavated from a late prehistoric archaeological context at Cape Espenberg on the northern coast of the Seward Peninsula in Alaska. Also part of the research team was Robert J. Speakman, of the Center for Applied Isotope Studies at the University of Georgia, and Victor Mair, of East Asian Languages and Civilizations at the University of Pennsylvania. The report is the first evidence that metal from Asia reached prehistoric North America before the contact with Europeans, stating that X-ray fluorescence identified two of these artifacts as smelted industrial alloys with large proportions of tin and lead. The presence of smelted alloys in a prehistoric Inuit context in northwest Alaska was demonstrated for the first time and indicates the movement of Eurasian metal across the Bering Strait into North America before sustained contact with Europeans.\n\nSmithsonian archaeologist Betty Meggers wrote that pottery associated with the Valdivia culture of coastal Ecuador dated to 3000–1500 BCE exhibited similarities to pottery produced during the Jōmon period in Japan, arguing that contact between the two cultures might explain the similarities. Chronological and other problems have led most archaeologists to dismiss this idea as implausible. The suggestion has been made that the resemblances (which are not complete) are simply due to the limited number of designs possible when incising clay.\n\nAlaskan anthropologist Nancy Yaw Davis claims that the Zuni people of New Mexico exhibit linguistic and cultural similarities to the Japanese. The Zuni language is a linguistic isolate, and Davis contends that the culture appears to differ from that of the surrounding natives in terms of blood type, endemic disease, and religion. Davis speculates that Buddhist priests or restless peasants from Japan may have crossed the Pacific in the 13th century, traveled to the American Southwest, and influenced Zuni society.\n\nIn the 1890s, lawyer and politician James Wickersham argued that pre-Columbian contact between Japanese sailors and Native Americans was highly probable, given that from the early 17th century to the mid-19th century several dozen Japanese ships were carried from Asia to North America along the powerful Kuroshio Currents. Such Japanese ships landed from the Aleutian Islands in the north to Mexico in the south, carrying a total of 293 persons in the 23 cases where head-counts were given in historical records. In most cases, the Japanese sailors gradually made their way home on merchant vessels. In 1834 a dismasted, rudderless Japanese ship crashed near Cape Flattery. Three survivors of the ship were enslaved by Makahs for a period before being rescued by members of the Hudson's Bay Company. They were never able to return to their homeland due to Japan's isolationist policy. Another Japanese ship crashed in about 1850 near the mouth of the Columbia River, Wickersham writes, and the sailors were assimilated into the local Native American population. While admitting there was no definitive proof of pre-Columbian contact between Japanese and North Americans, Wickersham thought it implausible that such contacts as outlined above would have started only after Europeans arrived in North America.\n\nIn 1879, Alexander Cunningham described the carvings on the Stupa of Bharhut from c. 200 BCE and described some fruit like detail as a custard-apple (\"Annona squamosa\"). He wasn't aware that botanists believed this plant to be indigenous only in the Americas, but others quickly pointed out the difficulty, as it was generally believed that the custard-apple had not been brought to India before Vasco da Gama's discovery of the sea route in 1498. This suggestion was generally disregarded but a 2009 study claimed to have found carbonized remains that date to 2000 BCE and appear like seeds of custard apple.\n\nGrafton Elliot Smith claimed that certain details in the Mayan stelae at Copán represented an Asian elephant. He wrote \"Elephants and Ethnologists\", a book on the topic in 1924. Contemporary archaeologists suggested that it was based on a tapir and his suggestions have generally been dismissed by subsequent research.\n\nSome carving details from around the 12th century in Karnataka that appeared like ears of maize (\"Zea mays\"), a crop from the New World, were interpreted by Carl Johannessen in 1989 as evidence of pre-Columbian contact. These suggestions were dismissed by multiple Indian researchers based on several lines of evidence. The object has been claimed by some to represent a Muktaphala, an imaginary fruit bedecked with pearls.\n\n Proposed claims for an African presence in Mesoamerica stem from attributes of the Olmec culture, the claimed transfer of African plants to the Americas, interpretations of European and Arabic historical accounts and certain genetic studies of Mexican populations. \nThe Olmec culture existed from roughly 1200 BCE to 400 BCE. The idea that the Olmecs are related to Africans was suggested by José Melgar, who discovered the first colossal head at Hueyapan (now Tres Zapotes) in 1862. More recently, Ivan Van Sertima has argued that these statues depict settlers or explorers from Africa, but his views have been the target of severe scholarly criticism.\n\nLeo Wiener's \"Africa and the Discovery of America\" suggests similarities between Mandinka and native Mesoamerican religious symbols such as the winged serpent and the sun disk, or Quetzalcoatl, and words that have Mande roots and share similar meanings across both cultures such as \"kore\", \"gadwal\", and \"qubila\" (in Arabic) or \"kofila\" (in Mandinka). \n\nNorth African sources describe what some consider to be visits to the New World by a Mali fleet in 1311.\nAccording to the abstract of Columbus's log made by Bartolomé de las Casas, the purpose of Columbus’s third voyage was to test both the claims of King John II of Portugal that \"canoes had been found which set out from the coast of Guinea [West Africa] and sailed to the west with merchandise\" as well as the claims of the native inhabitants of the Caribbean island of Hispaniola that \"from the south and the southeast had come black people whose spears were made of a metal called guanín...from which it was found that of 32 parts: 18 were gold, 6 were silver, and 8 copper.\" Another supporting claim was made by Washington Irving, in his “Life of Columbus”, who wrote that in 1503 when Columbus was on the Mosquito Coast “There was no pure gold to be met with here, all their ornaments were of guanin; but the natives assured the Adelantado that in proceeding along the coast, the ships would soon arrive at a country where gold was in great abundance.”\n\nBrazilian researcher Niede Guidon, who led the Pedra Furada sites excavations \"... said she believed that humans … might have come not overland from Asia but by boat from Africa\", with the journey taking place 100,000 years ago. Michael R. Waters, a geoarchaeologist at Texas A&M University noted the absence of genetic evidence in modern populations to support Guidon's claim.\n\nEarly Chinese accounts of Muslim expeditions state that Muslim sailors reached a region called Mulan Pi (\"magnolia skin\") (). Mulan Pi is mentioned in \"Lingwai Daida\" (1178) by Zhou Qufei and \"Zhufan Zhi\" (1225) by Chao Jukua, together referred to as the \"Sung Document\". Mulan Pi is normally identified as Spain of the Almoravid dynasty (Al-Murabitun), though some fringe theories hold that it is instead some part of the Americas.\n\nOne supporter of the interpretation of Mulan Pi as part of the Americas was historian Hui-lin Li in 1961, and while Joseph Needham was also open to the possibility, he doubted that Arab ships at the time would have been able to withstand a return journey over such a long distance across the Atlantic Ocean and points out that a return journey would have been impossible without knowledge of prevailing winds and currents.\n\nAccording to Muslim historian Abu al-Hasan 'Alī al-Mas'ūdī (871-957), Khashkhash Ibn Saeed Ibn Aswad () sailed over the Atlantic Ocean and discovered a previously unknown land ( \"\") in 889 and returned with a shipload of valuable treasures.\n\nUsing gold obtained by expansion of the African coastal trade down the west African coast, the Phoenician state of Carthage minted gold staters in 350 BCE bearing a pattern, in the reverse exergue of the coins, interpreted as a map of the Mediterranean with the Americas shown to the west across the Atlantic. Reports of the discovery of putative Carthaginian coins in North America are based on modern replicas, that may have been buried at sites from Massachusetts to Nebraska in order to confuse and mislead archaeological investigation.\n\nThe Bat Creek inscription and Los Lunas Decalogue Stone have led some to suggest the possibility that Jewish seafarers may have come to America after fleeing the Roman Empire at the time of the Jewish Revolt.\n\nScholar Cyrus H. Gordon believed that Phoenicians and other Semitic groups had crossed the Atlantic in antiquity, ultimately arriving in both North and South America. This opinion was based on his own work on the Bat Creek inscription. Similar ideas were also held by John Philip Cohane; Cohane even claimed that many geographical names in America have a Semitic origin.\n\nThe Solutrean hypothesis argues that Europeans migrated to the New World during the Paleolithic era, circa 16,000 to 13,000 BCE. This hypothesis proposes contact partly on the basis of perceived similarities between the flint tools of the Solutrean culture in modern-day France, Spain and Portugal (which thrived circa 20,000 to 15,000 BCE), and the Clovis culture of North America, which developed circa 9000 BCE.\nThe Solutrean hypothesis was proposed in the mid-1990s. It has little support amongst the scientific community, and genetic markers are inconsistent with the idea.\n\nEvidence of contacts with the civilizations of Classical Antiquity—primarily with the Roman Empire, but sometimes also with other cultures of the age—have been based on isolated archaeological finds in American sites that originated in the Old World. The Bay of Jars in Brazil has been yielding ancient clay storage jars that resemble Roman amphorae for over 150 years. It has been proposed that the origin of these jars is a Roman wreck, although it has been suggested that they could be 15th or 16th century Spanish olive oil jars.\n\nRomeo Hristov argues that a Roman ship, or the drifting of such a shipwreck to the American shores, is a possible explanation of archaeological finds (like the Tecaxic-Calixtlahuaca bearded head) from ancient Rome in America. Hristov claims that the possibility of such an event has been made more likely by the discovery of evidences of travels from Romans to Tenerife and Lanzarote in the Canaries, and of a Roman settlement (from the 1st century BCE to the 4th century CE) on Lanzarote island.\n\nIn 1950, an Italian botanist, Domenico Casella, suggested that a depiction of a pineapple was represented among wall paintings of Mediterranean fruits at Pompeii. According to Wilhelmina Feemster Jashemski, this interpretation has been challenged by other botanists, who identify it as a pine cone from the Umbrella pine tree, which is native to the Mediterranean area.\n\nA small terracotta head sculpture, with a beard and European-like features, was found in 1933 (in the Toluca Valley, 72 kilometres southwest of Mexico City) in a burial offering under three intact floors of a pre-colonial building dated to between 1476 and 1510. The artifact has been studied by Roman art authority Bernard Andreae, director emeritus of the German Institute of Archaeology in Rome, Italy, and Austrian anthropologist Robert von Heine-Geldern, both of whom stated that the style of the artifact was compatible with small Roman sculptures of the 2nd century. If genuine and if not placed there after 1492 (the pottery found with it dates to between 1476 and 1510) the find provides evidence for at least a one-time contact between the Old and New Worlds.\n\nAccording to ASU's Michael E. Smith, John Paddock, a leading Mesoamerican scholar, used to tell his classes in the years before he died that the artifact was planted as a joke by Hugo Moedano, a student who originally worked on the site. Despite speaking with individuals who knew the original discoverer (García Payón), and Moedano, Smith says he has been unable to confirm or reject this claim. Though he remains skeptical, Smith concedes he cannot rule out the possibility that the head was a genuinely buried Post-classic offering at Calixtlahuaca.\n\nHenry I Sinclair, Earl of Orkney and feudal baron of Roslin (c. 1345 – c. 1400) was a Scottish nobleman. He is best known today because of a modern legend that he took part in explorations of Greenland and North America almost 100 years before Christopher Columbus. In 1784, he was identified by Johann Reinhold Forster as possibly being the Prince Zichmni described in letters allegedly written around the year 1400 by the Zeno brothers of Venice, in which they describe a voyage throughout the North Atlantic under the command of Zichmni.\n\nHenry was the grandfather of William Sinclair, 1st Earl of Caithness, the builder of Rosslyn Chapel (near Edinburgh, Scotland). The authors Robert Lomas and Christopher Knight believe some carvings in the chapel to be ears of New World corn or maize. This crop was unknown in Europe at the time of the chapel's construction, and was not cultivated there until several hundred years later. Knight and Lomas view these carvings as evidence supporting the idea that Henry Sinclair travelled to the Americas well before Columbus. In their book they discuss meeting with the wife of the botanist Adrian Dyer, and that Dyer's wife told him that Dyer agreed that the image thought to be maize was accurate. In fact Dyer found only one identifiable plant among the botanical carvings and suggested that the \"maize\" and \"aloe\" were stylized wooden patterns, only coincidentally looking like real plants. Specialists in medieval architecture interpret these carvings as stylised depictions of wheat, strawberries or lilies.\nSome have conjectured that Columbus was able to persuade the Catholic Monarchs of Castile and Aragon to support his planned voyage only because they were aware of some recent earlier voyage across the Atlantic. Some suggest that Columbus himself visited Canada or Greenland before 1492, because according to Bartolomé de las Casas he wrote he had sailed 100 leagues past an island he called Thule in 1477. Whether he actually did this and what island he visited, if any, is uncertain. Columbus is thought to have visited Bristol in 1476. Bristol was also the port from which John Cabot sailed in 1497, crewed mostly by Bristol sailors. In a letter of late 1497 or early 1498 the English merchant John Day wrote to Columbus about Cabot's discoveries, saying that land found by Cabot was \"discovered in the past by the men from Bristol who found 'Brasil' as your lordship knows\". There may be records of expeditions from Bristol to find the \"isle of Brazil\" in 1480 and 1481. Trade between Bristol and Iceland is well documented from the mid 15th century.\n\nGonzalo Fernández de Oviedo y Valdés records several such legends in his \"General y natural historia de las Indias\" of 1526, which includes biographical information on Columbus. He discusses the then-current story of a Spanish caravel that was swept off its course while on its way to England, and wound up in a foreign land populated by naked tribesmen. The crew gathered supplies and made its way back to Europe, but the trip took several months and the captain and most of the men died before reaching land. The ship's pilot, a man called Alonso Sánchez, and very few others finally made it to Portugal, but all were very ill. Columbus was a good friend of the pilot, and took him to be treated in his own house, and the pilot described the land they had seen and marked it on a map before dying. People in Oviedo's time knew this story in several versions, but Oviedo regarded it as myth.\n\nIn 1925, Soren Larsen wrote a book claiming that a joint Danish-Portuguese expedition landed in Newfoundland or Labrador in 1473 and again in 1476. Larsen claimed that Didrik Pining and Hans Pothorst served as captains, while João Vaz Corte-Real and the possibly mythical John Scolvus served as navigators, accompanied by Álvaro Martins. Nothing beyond circumstantial evidence has been found to support Larsen's claims.\n\nThe legend of Saint Brendan, an Irish monk, involves a fantastical journey into the Atlantic Ocean in search of Paradise in the 6th century. Since the discovery of the New World, various authors have tried to link the Brendan legend with an early discovery of America. In 1977 The voyage was successfully recreated by Tim Severin using an ancient Irish Currach.\n\nAccording to a British myth, Madoc was a prince from Wales who explored the Americas as early as 1170. While most scholars consider this legend to be untrue, it was used as justification for British claims to the Americas, based on the notion of a Briton arriving before other European nationalities.\n\nBiologist and controversial amateur epigrapher Barry Fell claims that Irish Ogham writing has been found carved into stones in the Virginias. Linguist David H. Kelley has criticized some of Fell's work but nonetheless argued that genuine Celtic Ogham inscriptions have in fact been discovered in America. However, others have raised serious doubts about these claims.\n\nTraces of coca and nicotine found in some Egyptian mummies have led to speculation that Ancient Egyptians may have had contact with the New World. The initial discovery was made by a German toxicologist, Svetlana Balabanova, after examining the mummy of a priestess called Henut Taui. Follow-up tests of the hair shaft, performed to rule out contamination, gave the same results.\n\nA television show reported that examination of numerous Sudanese mummies undertaken by Balabanova mirrored what was found in the mummy of Henut Taui. Balabanova suggested that the tobacco may be accounted for since it may have also been known in China and Europe, as indicated by analysis run on human remains from those respective regions. Balabanova proposed that such plants native to the general area may have developed independently, but have since gone extinct. Other explanations include fraud, though curator Alfred Grimm of the Egyptian Museum in Munich disputes this. Skeptical of Balabanova's findings, Rosalie David, Keeper of Egyptology at the Manchester Museum, had similar tests performed on samples taken from the Manchester mummy collection and reported that two of the tissue samples and one hair sample did test positive for nicotine. Sources of nicotine other than tobacco and sources of cocaine in the Old World are discussed by the British biologist Duncan Edlin.\n\nMainstream scholars remain skeptical, and they do not see this as proof of ancient contact between Africa and the Americas, especially because there may be possible Old World sources. Two attempts to replicate Balabanova's finds of cocaine failed, suggesting \"that either Balabanova and her associates are misinterpreting their results or that the samples of mummies tested by them have been mysteriously exposed to cocaine.\"\n\nA re-examination in the 1970s of the mummy of Ramesses II revealed the presence of fragments of tobacco leaves in its abdomen. This became a popular topic in fringe literature and the media and was seen as proof of contact between Ancient Egypt and the New World. The investigator, Maurice Bucaille, noted that when the mummy was unwrapped in 1886 the abdomen was left open and that \"it was no longer possible to attach any importance to the presence inside the abdominal cavity of whatever material was found there, since the material could have come from the surrounding environment.\" Following the renewed discussion of tobacco sparked by Balabanova's research and its mention in a 2000 publication by Rosalie David, a study in the journal \"Antiquity\" suggested that reports of both tobacco and cocaine in mummies \"ignored their post-excavation histories\" and pointed out that the mummy of Ramesses II had been moved five times between 1883 and 1975.\n\nIn 2010 Sigríður Sunna Ebenesersdóttir published a genetic study showing that over 350 living Icelanders carried mitochondrial DNA of a new type that is similar to the type found only in Native American and East Asian populations. Using the deCODE genetics database, Sigríður Sunna determined that the DNA entered the Icelandic population not later than 1700, and likely several centuries earlier. However Sigríður Sunna also states that \"...while a Native American origin seems most likely for [this new haplogroup], an Asian or European origin cannot be ruled out\".\n\nIn 1009, legends report that Norse explorer Thorfinn Karlsefni abducted two children from Markland, an area on the North American mainland where Norse explorers visited but did not settle. The two children were then taken to Greenland, where they were baptized and taught to speak Norse.\n\nIn 1420, Danish geographer Claudius Clavus Swart wrote that he personally had seen \"pygmies\" from Greenland who were caught by Norsemen in a small skin boat. Their boat was hung in Nidaros Cathedral in Trondheim along with another, longer boat also taken from \"pygmies\". Clavus Swart's description fits the Inuit and two of their types of boats, the kayak and the umiak. Similarly, the Swedish clergyman Olaus Magnus wrote in 1505 that he saw in Oslo Cathedral two leather boats taken decades earlier. According to Olaus, the boats were captured from Greenland pirates by one of the Haakons, which would place the event in the 14th century.\n\nIn Ferdinand Columbus's biography of his father Christopher, he says that in 1477 his father saw in Galway, Ireland two dead bodies which had washed ashore in their boat. The bodies and boat were of exotic appearance, and have been suggested to have been Inuit who had drifted off course.\n\nIt has been suggested that the Norse took other indigenous peoples to Europe as slaves over the following centuries, because they are known to have taken Scottish and Irish slaves.\n\nThere is also evidence of Inuit coming to Europe under their own power or as captives after 1492. A substantial body of Greenland Inuit folklore first collected in the 19th century told of journeys by boat to Akilineq, here depicted as a rich country across the ocean.\n\nPre-Columbian contact between Alaska and Kamchatka via the subarctic Aleutian Islands would have been conceivable, but the two settlement waves on this archipelago started on the American side and its western continuation, the Commander Islands, remained uninhabited until after Russian explorers encountered the Aleut people in 1741. There is no genetic or linguistic evidence for earlier contact along this route.\n\nIn 1650, a British preacher in Norfolk, Thomas Thorowgood, published \"Jewes in America or Probabilities that the Americans are of that Race\", for the New England missionary society. Tudor Parfitt writes:The society was active in trying to convert the Indians but suspected that they might be Jews and realized they better be prepared for an arduous task. Thorowgood's tract argued that the native population of North America were descendants of the Ten Lost Tribes.\n\nIn 1652 Sir Hamon L'Estrange, an English author writing on history and theology, published \"Americans no Jews, or improbabilities that the Americans are of that Race\" in response to the tract by Thorowgood. In response to L'Estrange, Thorowgood published a second edition of his book in 1660 with a revised title and included a foreword written by John Eliot, a Puritan missionary who had translated the Bible into an Indian language.\n\nThe Book of Mormon, a sacred text of the Latter Day Saint movement published by founder and leader Joseph Smith Jr in 1830 at the age of twenty-four, states that some ancient inhabitants of the New World are descendants of Semitic peoples who sailed from the Old World. Mormon groups such as the Foundation for Ancient Research and Mormon Studies attempt to study and expand on these ideas. Scientific consensus rejects these claims.\n\nThe National Geographic Society, in a 1998 letter to the Institute for Religious Research, stated \"Archaeologists and other scholars have long probed the hemisphere's past and the society does not know of anything found so far that has substantiated the Book of Mormon.\"\n\nSome LDS scholars hold the view that archaeological study of Book of Mormon claims are not meant to vindicate the literary narrative. For example, Terryl Givens, professor of English at the University of Richmond, points out that there is a lack of historical accuracy in the Book of Mormon in relation to modern archaeological knowledge.\n\nIn the 1950s, Professor M. Wells Jakeman popularized a belief that the Izapa Stela 5 represents the Book of Mormon prophets Lehi and Nephi's tree of life vision, and was a validation of the historicity of the claims of pre-Columbian settlement in the Americas. His interpretations of the carving and its connection to pre-Columbian contact have been disputed. Since that time, scholarship on the Book of Mormon has concentrated on cultural parallels rather than \"smoking gun\" sources.\n\n"}
{"id": "21716662", "url": "https://en.wikipedia.org/wiki?curid=21716662", "title": "Reallexikon der Assyriologie", "text": "Reallexikon der Assyriologie\n\nThe Reallexikon der Assyriologie und vorderasiatischen Archäologie (RlA), formerly \"Reallexikon der Assyriologie\", is a multi-language (English, German, and French) encyclopedia on the Ancient Near East. It was founded by Bruno Meissner in 1922, reformed in 1966 by editor Ruth Opificius and publisher Wolfram von Soden. From 1972 to 2004 edited by Dietz-Otto Edzard, since 2005 by Michael P. Streck.\n\nA team of 585 different authors of many countries have been involved in the project and a total of 14 volumes and two issues of the last volume have been published until 2017. By the end of 2017 the last issues of the 15th volume will be published, finally bringing the original project into completion.\n\nTo be distinguished from the \"Akkadisches Handwörterbuch\" (AHw).\n\n"}
{"id": "320017", "url": "https://en.wikipedia.org/wiki?curid=320017", "title": "Robert Harris (novelist)", "text": "Robert Harris (novelist)\n\nRobert Dennis Harris (born 7 March 1957) is an English novelist. He is a former journalist and BBC television reporter. Although he began his career in non-fiction, his fame rests upon his works of historical fiction. Beginning with the best-seller \"Fatherland\", Harris focused on events surrounding the Second World War, followed by works set in ancient Rome. His most recent works centre on contemporary history. Harris was educated at Selwyn College, Cambridge, where he was president of the Union and editor of the student newspaper \"Varsity\".\n\nBorn in Nottingham, Harris spent his childhood in a small rented house on a Nottingham council estate. His ambition to become a writer arose at an early age, from visits to the local printing plant where his father worked. Harris went to Belvoir High School in Bottesford, and then King Edward VII School, Melton Mowbray, where a hall was later named after him. There he wrote plays and edited the school magazine. Harris read English literature at Selwyn College, Cambridge. While at Cambridge, Harris was elected president of the Cambridge Union and editor of the oldest student newspaper at the university, \"Varsity\".\n\nAfter leaving Cambridge, Harris joined the BBC and worked on news and current affairs programmes such as \"Panorama\" and \"Newsnight\". In 1987, at the age of thirty, he became political editor of \"The Observer\". He later wrote regular columns for the \"Sunday Times\" and the \"Daily Telegraph\".\n\nHarris's first book appeared in 1982. \"A Higher Form of Killing\", a study of chemical and biological warfare, was written with fellow BBC journalist Jeremy Paxman. Other non-fiction works followed: \"Gotcha! The Government, the Media and the Falklands Crisis\" (1983), \"The Making of Neil Kinnock\" (1984), \"Selling Hitler\" (1986), an investigation of the Hitler Diaries scandal, and \"Good and Faithful Servant\" (1990), a study of Bernard Ingham, Margaret Thatcher's press secretary.\n\nHarris's million-selling alternative-history first novel \"Fatherland\" has as its setting a world where Germany has won the Second World War. Publication enabled Harris to become a full-time novelist. HBO made a film based on the novel in 1994.\n\nHarris stated that the proceeds from the book enabled him to buy a house in the country, where he still lives.\n\nHis second novel \"Enigma\" portrayed the breaking of the German Enigma code during the Second World War at Cambridge University and Bletchley Park. It went on to become a major film film, with Dougray Scott and Kate Winslet starring and with a screenplay by Tom Stoppard.\n\n\"Archangel\" was another international best seller. It follows a British historian in contemporary Russia as he hunts for a secret notebook, believed to be Stalin's diary. In 2005 the BBC made it into a mini-series starring Daniel Craig.\n\nIn 2003 Harris turned his attention to ancient Rome with his acclaimed \"Pompeii.\" The novel is about a Roman aqueduct engineer, working near the city of Pompeii just before the eruption of Vesuvius in 79 AD. As the aqueducts begin to malfunction, he investigates and realises the volcano is shifting the ground and damaging the system and is near eruption. Meanwhile, he falls in love with the young daughter of a powerful local businessman who was illicitly dealing with his predecessor to divert municipal water for his own uses, and will do anything to keep that deal going.\n\nHe followed this in 2006 with \"Imperium\", the first novel in a trilogy centered on the life of the great Roman orator Cicero.\n\nHarris was an early and enthusiastic backer of British Prime Minister Tony Blair (a personal acquaintance) and a donor to New Labour, but the war in Iraq blunted his enthusiasm. \"We had our ups and downs, but we didn't really fall out until the invasion of Iraq, which made no sense to me,\" Harris has said.\n\nIn 2007, after Blair resigned, Harris dropped his other work to write \"The Ghost\". The title refers both to a professional ghostwriter, whose lengthy memorandum forms the novel, and to his immediate predecessor who, as the action opens, has just drowned in gruesome and mysterious circumstances.\n\nThe dead man has been ghosting the autobiography of a recently unseated British prime minister called Adam Lang, a thinly veiled version of Blair. The fictional counterpart of Cherie Blair is depicted as a sinister manipulator of her husband. Harris told \"The Guardian\" before publication: \"The day this appears a writ might come through the door. But I would doubt it, knowing him.\"\n\nHarris said in a U.S. National Public Radio interview that politicians like Lang and Blair, particularly when they have been in office for a long time, become divorced from everyday reality, read little and end up with a pretty limited overall outlook. When it comes to writing their memoirs, they therefore tend to have all the more need of a ghostwriter.\n\nHarris hinted at a third, far less obvious, allusion hidden in the novel's title, and, more significantly, at a possible motive for having written the book in the first place. Blair, he said, had himself been ghostwriter, in effect, to President Bush when giving public reasons for invading Iraq: he had argued the case better than had the President himself.\n\nThe \"New York Observer\", headlining its otherwise hostile review \"The Blair Snitch Project\", commented that the book's \"shock-horror revelation\" was \"so shocking it simply can't be true, though if it were it would certainly explain pretty much everything about the recent history of Great Britain.\"\n\nThe second novel in the Cicero trilogy, \"Lustrum\", was published in October 2009. It was released in February 2010 in the US under the alternative title of \"Conspirata\".\n\nHis novel \"The Fear Index\", focusing on the 2010 Flash Crash, was published by Hutchinson in September 2011. It follows an American expat hedge fund operator living in Geneva who activates a new system of computer algorithms that he names VIXAL-4, which is designed to operate faster than human beings, but which begins to become uncontrollable by its human operators.\n\n\"An Officer and a Spy\" is the story of French officer Georges Picquart, a historical character, who is promoted in 1895 to run France's \"Statistical Section\", its secret intelligence division. He gradually realises that Alfred Dreyfus has been unjustly imprisoned for acts of espionage committed by another man who is still free and still spying for the Germans. He risks his career and his life to expose the truth.\n\n\"Dictator\" is the long-promised conclusion to Harris's Cicero trilogy. It was published by Hutchinson on 8 October 2015.\n\n\"Conclave\", published on 22 September 2016, is a novel \"set over 72 hours in the Vatican\", preceding \"the election of a fictional Pope.\"\n\nHis latest novel, published on 21 September 2017, is a thriller set during the negotiations for the 1938 Munich Agreement between Hitler and UK Prime Minister Neville Chamberlain. The story is told through the eyes of two young civil servants - one German, Hartmann, and one English, Legat, who reunite at the fateful summit, six years after they were friends at university.\n\nIn an interview on the BBC's \"The Andrew Marr Show\" on 24 June 2018 Harris stated that his next novel, due in 2019, would be set in the future. It will be published by Hutchinson on 5 September 2019.\n\nIn 2007, Harris wrote a screenplay of his novel \"Pompeii\" for director Roman Polanski. Harris acknowledged in many interviews that the plot of his novel was inspired by Polanski's film \"Chinatown\", and Polanski said it was precisely that similarity that had attracted him to \"Pompeii\". The film, to be produced by Summit Entertainment, was announced at the Cannes Film Festival in 2007 as potentially the most expensive European film ever made, set to be shot in Spain. Media reports suggested Polanski wanted Orlando Bloom and Scarlett Johansson to play the two leads. The film was cancelled in September 2007 as a result of a looming actors' strike.\n\nPolanski and Harris then turned to Harris's bestseller, \"The Ghost\". They co-wrote a script and Polanski announced filming for early 2008, with Nicolas Cage, Pierce Brosnan, Tilda Swinton and Kim Cattrall starring. The film was then postponed by a year, with Ewan McGregor and Olivia Williams replacing Cage and Swinton.\n\nThe film, retitled \"The Ghost Writer\" in all territories except the UK, was shot in early 2009 in Berlin and on the island of Sylt in the North Sea, which stood in for London and Martha's Vineyard respectively, owing to Polanski's inability to travel legally to those places. In spite of his incarceration, he oversaw post-production from his house arrest and the film premiered at the Berlin Film Festival in February 2010, with Polanski winning the Silver Bear for Best Director award. Harris and Polanski later shared a César Award for Best Adapted Screenplay.\n\nHarris was inspired to write his novel \"An Officer and a Spy\" by Polanski's longtime interest in the Dreyfus affair. He also wrote a screenplay based on the story, which Polanski was to direct in 2012. The screenplay was first titled \"D\", after the initial written on the secret file that secured Dreyfus' conviction. After many years of production difficulties, it is set to film in the fall of 2018 as \"J'accuse\", starring Jean Dujardin and co-starring Mathieu Amalric and Olivier Gourmet. It will be produced by Alain Goldman and distributed by Gaumont.\n\nIn June 2018 Harris reiterated his support for Roman Polanski, and branded criticisms of Polanski's crimes as being a problem of culture and fashion “The culture has completely changed...And so the question is: ‘Do you then say, OK fine, I follow the culture.’ Or do I say: ‘Well, he hasn’t done anything since then. He won the Oscar, he got a standing ovation in Los Angeles.’The zeitgeist has changed. Do you change with it? I don’t know, to be honest with you. Morally, I don’t see why I should change my position because the fashion has changed.”\n\nHarris has appeared on the BBC satirical panel game \"Have I Got News for You\" in episode three of the first series in 1990, and in episode four of the second series a year later. In the first he appeared as a last-minute replacement for the politician Roy Hattersley. He made a third appearance on the programme on 12 October 2007, seventeen years, to the day, after his first appearance. Since the gap between his second and third appearance was nearly 16 years, Harris enjoyed the distinction of the longest gap between two successive appearances in the show's history until Eddie Izzard appeared on 22 April 2016, 20 years after his appearance on Episode 1 of Series 11 (19 April 1996).\n\nOn 2 December 2010, Harris appeared on the radio programme \"Desert Island Discs\", when he spoke about his childhood and his friendships with Tony Blair and Roman Polanski.\n\nHarris appeared on the American PBS show \"Charlie Rose\" on 10 February 2012. Harris discussed his novel \"The Fear Index\" which he likened to a modern-day Gothic novel along the lines of Mary Shelley's \"Frankenstein\". Harris also discussed the adaptation of his novel, \"The Ghost\" that came out as the movie, \"The Ghost Writer\" directed by Roman Polanski.\n\nHarris was a columnist for the \"Sunday Times\", but gave it up in 1997. He returned to journalism in 2001, writing for the \"Daily Telegraph\". He was named \"Columnist of the Year\" at the 2003 British Press Awards.\n\nHarris lives in a former vicarage in Kintbury, near Newbury, Berkshire, with his wife Gill Hornby, herself a writer and sister of best-selling novelist Nick Hornby. They have four children. Harris contributed a short story, \"PMQ\", to Hornby's 2000 collection \"Speaking with the Angel\".\n\nFormerly a donor to the Labour Party, he renounced his support for the party after the appointment of Guardian journalist Seumas Milne as its communications director by leader Jeremy Corbyn, tweeting: \"Council house born. Comprehensive-school educated. Voted Foot, Kinnock. But not for private-school apologists for IRA and Stalin. Sorry\". He now supports the Liberal Democrats.\n\n\n\n\n\n"}
{"id": "7342167", "url": "https://en.wikipedia.org/wiki?curid=7342167", "title": "Silvio Zavala", "text": "Silvio Zavala\n\nSilvio Arturo Zavala Vallado (February 7, 1909 – December 5, 2014) was a pioneer in law history studies and Mexico’s institutions. He died at age 105 on December 12, 2014.\n\nSilvio Zavala was born on February 7, 1909 in Mérida, Yucatán. \nHe studied at the National University of Mexico and at the University of Madrid, where he received a Ph.D. in Law. \n\nHe began his professional career in Spain in the Center for Historic Studies in Madrid.\n\nHe has been a member of El Colegio Nacional since January 6, 1947, and of the Board of the Chronicle of Mexico City. He received the 1969 National Literature Award; the Vasco de Quiroga Medal (1986); the Rafael Heliodoro Valle Award (1988); the Eligio Ancona Medal; and the Prince of Asturias Award. He served as Ambassador of Mexico in France from 1966 to 1975. \n\nHe has written over sixty books and two hundred and fifty articles. In May 2008, at the age of 99, the National Institute of Anthropology and History (INAH) bestowed upon him the Acknowledgment to a Lifetime Career to honor his work as a historian, scholar, researcher, thinker and cultural advocate. He died in Mexico City on December 5, 2014 at the age of 105.\n\n\n"}
{"id": "11607269", "url": "https://en.wikipedia.org/wiki?curid=11607269", "title": "Sophie de Schaepdrijver", "text": "Sophie de Schaepdrijver\n\nBaroness Sophie De Schaepdrijver (b. Kortrijk, 11 September 1961) is a Belgian historian.\n\nShe graduated in history at the Vrije Universiteit Brussel (Brussels, Belgium) and obtained a PhD with a dissertation on \"Elites for the Capital? Foreign Migration to mid-nineteenth-century Brussels\" at the University of Amsterdam.\n\nFrom 1986 until 1988, she was assistant professor at the Department of History of the Free University of Amsterdam. From 1988 until 1990, she worked as a dissertation fellow at the Amsterdam School for Social Science Research. She worked from 1990 until 1991 as associate professor at the Department of History of Groningen University. From 1991 until 1995, she worked as associate professor at Leiden University.\n\nShe left for the United States in 1995, where from 1995 until 1996, she was a fellow at the National Humanities Center, Research Triangle Park, N.C. . From 1996 until 2001, she was a visiting associate professor at New York University. From 1999 until 2000 she worked as a visiting fellow at Princeton University. Since 2001, she is Associate Professor of Modern European History at the Pennsylvania State University.\n\nIn 2005-2006 she was a fellow at the \"Netherlands Institute for Advances Study\" (NIAS) in Wassenaar.\n\nHer latest book is a study of the resistance heroine Gabrielle Petit and her memory.\n\n\n\n"}
{"id": "45333258", "url": "https://en.wikipedia.org/wiki?curid=45333258", "title": "Stefan Maechler", "text": "Stefan Maechler\n\nStefan Maechler is a Swiss historian and expert on anti-Semitism and Switzerland's treatment of Holocaust refugees during and after World War II. Maechler studied history and German literature at the University of Zurich. He was commissioned by the Schocken Books specializing in Judaica to conduct a full-scale investigation into the life of writer Binjamin Wilkomirski whose memoir \"Fragments\", published by Schocken in 1996, sparked international controversy. Maechler studied hundreds of personal documents, and has interviewed eyewitnesses and families of survivors in seven countries. He was given unrestricted access to government files, and subsequently, discovered facts that completely refuted Wilkomirski's bestselling book as a forgery. Maechler published his findings in a book called \"The Wilkomirski Affair: A Study in Biographical Truth\" by Schocken Books, New York, in 2001 (496 pp.); originally in the German language as \"Der Fall Wilkomirski: über die Wahrheit einer Biographie\" by Pendo Verlag AG, Zurich, 2000. Maechler lives and works in Zurich.\n\n"}
{"id": "616237", "url": "https://en.wikipedia.org/wiki?curid=616237", "title": "Swabia", "text": "Swabia\n\nSwabia (; , colloquially \"Schwabenland\" or \"Ländle\"; in English also archaic \"Suabia\" or \"Svebia\") is a cultural, historic and linguistic region in southwestern Germany. \nThe name is ultimately derived from the medieval Duchy of Swabia, one of the German stem duchies, representing the territory of Alemannia, whose inhabitants interchangeably were called \"Alemanni\" or \"Suebi\".\n\nThis territory would include all of the Alemannic German area, but the modern concept of Swabia is more restricted, due to the collapse of the duchy of Swabia in the thirteenth century. Swabia as understood in modern ethnography roughly coincides with the Swabian Circle of the Holy Roman Empire as it stood during the Early Modern period, now divided between the states of Bavaria and Baden-Württemberg.\n\nSwabians (\"Schwaben\", singular \"Schwabe\") are the natives of Swabia and speakers of Swabian German. Their number was estimated at close to 0.8 million by SIL Ethnologue as of 2006, compared to a total population of 7.5 million in the regions of Tübingen, Stuttgart and Bavarian Swabia.\n\nLike many cultural regions of Europe, Swabia's borders are not clearly defined. However, today it is normally thought of as comprising the former Swabian Circle, or equivalently the former state of Württemberg (with the Prussian Hohenzollern Province), or the modern districts of Tübingen (excluding the former Baden regions of the Bodenseekreis district), Stuttgart, and the administrative region of Bavarian Swabia.\n\nIn the Middle Ages, the term Swabia indicated a larger area, covering all the lands associated with the Frankish stem duchy of Alamannia stretching from the Vosges Mountains in the west to the broad Lech river in the east: This also included the region of Alsace and the later Margraviate of Baden on both sides of the Upper Rhine Valley, as well as modern German-speaking Switzerland, the Austrian state of Vorarlberg and the principality of Liechtenstein in the south.\n\nLike all of Southern Germany, what is now Swabia was part of the La Tène culture, and as such has a Celtic (Gaulish) substrate. In the Roman era, it was part of the Raetia province. \n\nThe name \"Suebia\" is derived from that of the \"Suebi\". \nIt is used already by Tacitus in the 1st century, albeit in a different geographical sense:\nHe calls the Baltic Sea the \"Mare Suevicum\" (\"Suebian Sea\") after the Suiones, and ends his description of the Suiones and Sitones with \"Here Suebia ends\" (\"Hic Suebiae finis\").\nBy the mid-3rd century, groups Suebi form the core element of the new tribal alliance known as the Alamanni, who expanded towards the Roman Limes east of the Rhine and south of the Main.\nThe Alamanni were sometimes referred to as Suebi even at this time, and their new area of settlement came to be known as Suebia.\nIn the migration period, the Suebi (Alamanni) crossed the Rhine in 406 and some of them established the Kingdom of the Suebi in Galicia. Another group settled in parts of Pannonia, after the Huns were defeated in 454 in the Battle of Nedao.\n\nThe Alemanni were ruled by independent kings throughout the 4th to 5th century centuries, but fell under Frankish domination in the 6th (Battle of Tolbiac 496).\nBy the late 5th century, the area settled by the Alemanni extended to the Alsace and the Swiss Plateau, bordering on the Bavarii to the east, the Franks to the north, the remnants of Roman Gaul to the west, and the Lombards and Goths, united in the Kingdom of Odoacer, to the south.\n\nThe name \"Alamannia\" was used by the 8th century, and from the 9th century, \"Suebia\" was occasionally used for \"Alamannia\", while \"Alamannia\" was increasinly used to refer to the Alsace specifically. By the 12th century, \"Suebia\" rather than \"Alamannia\" was used consistently for the territory of the Duchy of Swabia.\n\nSwabia was one of the original stem duchies of East Francia, the later Holy Roman Empire, as it developed in the 9th and 10th centuries. Due to the foundation of the important abbeys of St. Gallen and Reichenau, Swabia became an important center of Old High German literary culture during this period.\n\nIn the later Carolingian period, Swabia became once again de facto independent, by the early 10th century mostly ruled by two dynasties, the Hunfriding counts in Raetia Curiensis and the Ahalolfings ruling the Baar estates around the upper Neckar and Danube rivers. \nThe conflict between the two dynasties was decided in favour of Hunfriding Burchard II at the Battle of Winterthur (919). Burchard's rule as duke was acknowledged as such by the newly elected king Henry the Fowler, and in the 960s the duchy under Burchard III was incpororated in the Holy Roman Empire under Otto I.\n\nThe Hohenstaufen dynasty, which ruled the Holy Roman Empire in the 12th and 13th centuries, arose out of Swabia, but following the execution of Conradin, the last Hohenstaufen, on October 29, 1268, the duchy was not reappointed during the Great Interregnum. In the following years the original duchy gradually broke up into many smaller units.\n\nRudolf I of Habsburg, elected in 1273 as emperor, tried to restore the duchy, but met the opposition of the higher nobility who aimed to limit the power of the emperor. Instead, he confiscated the former estates of the Hohenstaufen as imperial property of the Holy Roman Empire, and declared most of the cities formerly belonging to Hohenstaufen to be Free Imperial Cities, and the more powerful abbeys within the former duchy to be Imperial Abbeys.\n\nThe rural regions were merged into the Imperial Shrievalty (\"Reichslandvogtei\") of Swabia, which was given as Imperial Pawn to Duke Leopold III of Austria in 1379 and again to Sigismund, Archduke of Austria in 1473/1486. He took the title of a \"Prince of Swabia\" and integrated the Shrievalty of Swabia in the realm of Further Austria.\n\nThe Swabian League of Cities was first formed on 20 November 1331, when twenty-two imperial cities of the former Duchy of Swabia banded together in support of the Emperor Louis IV, who in return promised not to mortgage any of them to any imperial vassal. Among the founding cities were Augsburg, Heilbronn, Reutlingen, and Ulm. The counts of Württemberg, Oettingen, and Hohenberg were induced to join in 1340.\n\nThe defeat of the city league by Count Eberhard II of Württemberg in 1372\nled to the formation of a new league of fourteen Swabian cities on 4 July 1376. \nThe emperor refused to recognise the newly revitalised Swabian League, seeing it as a rebellion, and this led to an \"imperial war\" against the league. The renewed league defeated an imperial army at the Battle of Reutlingen on 14 May 1377.\nBurgrave Frederick V of Hohenzollern, finally defeated the league in 1388 at Döffingen. The next year the city league disbanded according to the resolutions of the Reichstag at Eger.\n\nThe major dynasties that arose out medieval Swabia were the Habsburgs and the Hohenzollerns, who rose to prominence in Northern Germany. Also stemming from Swabia are the local dynasties of the dukes of Württemberg and the Margraves of Baden. The Welf family went on to rule in Bavaria and Hanover, and are ancestral to the British Royal Family that has ruled since 1714. Smaller feudal dynasties eventually disappeared, however; for example, branches of the Montforts and Hohenems lived until modern times, and the Fürstenberg survive still. The region proved to be one of the most divided in the empire, containing, in addition to these principalities, numerous free cities, ecclesiastical territories, and fiefdoms of lesser counts and knights.\n\nA new Swabian League (\"Schwäbischer Bund\") was formed in 1488, opposing the expansionist Bavarian dukes from the House of Wittelsbach and the revolutionary threat from the south in the form of the Swiss.\nIn 1519, the League conquered Württemberg and sold it to Charles V after its duke Ulrich seized the Free Imperial City of Reutlingen during the interregnum that followed the death of Maximilian I. It helped to suppress the Peasants' Revolt in 1524–26 and defeat an alliance of robber barons in the Franconian War. The Reformation caused the league to be disbanded in 1534. \n\nThe territory of Swabia as understood today emerges in the early modern period. It corresponds to the Swabian Circle established in 1512.\nThe Old Swiss Confederacy was \"de facto\" independent from Swabia from 1499 as a result of the Swabian War, while the Margraviate of Baden had been detached from Swabia since the twelfth century.\nFearing the power of the greater princes, the cities and smaller secular rulers of Swabia joined to form the Swabian League in the fifteenth century. The League was quite successful, notably expelling the Duke of Württemberg in 1519 and putting in his place a Habsburg governor, but the league broke up a few years later over religious differences inspired by the Reformation, and the Duke of Württemberg was soon restored.\n\nThe region was quite divided by the Reformation. While secular princes such as the Duke of Württemberg and the Margrave of Baden-Durlach, as well as most of the Free Cities, became Protestant, the ecclesiastical territories (including the bishoprics of Augsburg, Konstanz and the numerous Imperial abbeys) remained Catholic, as did the territories belonging to the Habsburgs (Further Austria), the Sigmaringen branch of the House of Hohenzollern, and the Margrave of Baden-Baden.\n\nIn the wake of the territorial reorganization of the empire of 1803 by the \"Reichsdeputationshauptschluss\", the shape of Swabia was entirely changed. All the ecclesiastical estates were secularized, and most of the smaller secular states, and almost all of the free cities, were mediatized, leaving only Württemberg, Baden, and Hohenzollern as sovereign states. Much of Eastern Swabia became part of Bavaria, forming what is now the Swabian administrative region of Bavaria.\nIn contemporary usage, \"Schwaben\" is often taken to refer to Bavarian Swabia exclusively.\nBaden, historically part of the duchy of Swabia but not of the Swabian Circle, is no longer commonly included in the term.\n\nSIL Ethnologue cites an estimate of 819,000 Swabian speakers as of 2006. This corresponds to roughly 10% of the total population of the Swabian region, or roughly 1% of the total population of Germany.\n\nAs an ethno-linguistic group, Swabians are closely related to other speakers of Alemannic German, i.e. Badeners, Alsatians, and German-speaking Swiss.\n\nSwabian German is traditionally spoken in the upper Neckar basin (upstream of Heilbronn), along the upper Danube between Tuttlingen and Donauwörth, and on the left bank of the Lech, in an area centered on the Swabian Alps roughly stretching from Stuttgart to Augsburg.\n\nMany Swabian surnames end with the suffixes \"-le\", \"-(l)er\", \"-el\", \"-ehl\", and \"-lin\", typically from the Middle High German diminutive suffix \"-elîn\" (Modern Standard German \"-lein\"). Examples would be: \"Schäuble\", \"Egeler\", \"Rommel\", and \"Gmelin\". The popular surname \"Schwab\" is derived from this area, meaning literally \"Swabian\".\n\n\n"}
{"id": "18909942", "url": "https://en.wikipedia.org/wiki?curid=18909942", "title": "Tenerian culture", "text": "Tenerian culture\n\nThe Tenerian culture is a prehistoric industry that existed between the 5th millennium BC and mid-3rd millennium BC in the Sahara Desert. This spans the Neolithic Subpluvial and later desiccation, during the middle Holocene.\n\nReygasse first used the term Tenerian in 1934 with subsequent scholars producing a clearer definition. The Missions Berliet to the Aïr Mountains (Aïr Massif) in northern Niger produced the clearest definition prior to J. Desmond Clarke's expedition to Adrar Bous in early 1970, the results of which were published in November 2008.\n\nHuman remains belonging to the Tenerian culture were first found at Adrar Bous in the Aïr Mountains. Other Tenerian specimens were also discovered at Gobero, located in Niger in the Ténéré desert. This region was lush at the time, and Tenerians were specialized cattle herders who also occasionally fished and hunted.\n\nGobero was discovered in 2000 during an archaeological expedition led by Paul Sereno, which sought dinosaur remains. Two distinct prehistoric cultures were discovered at the site: the early Holocene Kiffian culture, and the middle Holocene Tenerian culture. The Kiffians were a prehistoric people who preceded the Tenerians and vanished approximately 8000 years ago, when the desert became very dry. The desiccation lasted until around 4600 BC, the period to when the earliest artefacts associated with the Tenerians have been dated. Some 200 skeletons have been discovered at Gobero.\n\nThe Tenerians were considerably shorter in height and less robust than the earlier Kiffians. Craniometric analysis also indicates that they were osteologically distinct. The Kiffian skulls are akin to those of the Late Pleistocene Iberomaurusians, early Holocene Capsians, and mid-Holocene Mechta groups, whereas the Tenerian crania are more like those of Mediterranean groups.\n\nGraves show that the Tenerians observed spiritual traditions, as they were buried with artifacts such as jewelry made of hippo tusks and clay pots. The most interesting find is a triple burial of an adult female and two children, dated to 5300 years ago. The fossil has been estimated through their teeth as being five and eight years old, hugging each other. Pollen residue indicates they were buried on a bed of flowers. The three are assumed to have died within 24 hours of each other, but as their skeletons hold no apparent trauma (they did not die violently) and they have been buried so elaborately - unlikely if they had died of a plague - the cause of their deaths is a mystery.\n\nApproximately 4500 years ago, the region became dry again. The Tenerian culture vanished, with its makers possibly seeking new pasturage elsewhere.\n\n\n"}
{"id": "11718624", "url": "https://en.wikipedia.org/wiki?curid=11718624", "title": "Trend surface analysis", "text": "Trend surface analysis\n\nTrend surface analysis is a mathematical technique used in environmental sciences (archeology, geology, soil science, etc.). Trend surface analysis (also called trend surface mapping) is a method based on low-order polynomials of spatial coordinates for estimating a regular grid of points from scattered observations - for example, from archeological finds or from soil survey.\n"}
{"id": "17743943", "url": "https://en.wikipedia.org/wiki?curid=17743943", "title": "United Nations Security Council Resolution 363", "text": "United Nations Security Council Resolution 363\n\nUnited Nations Security Council Resolution 363, adopted on November 29, 1974, after considering a report by the Secretary-General regarding the United Nations Disengagement Observer Force, the Council noted the efforts made to establish a durable and just peace in the Middle East and expressed its concern over the prevailing state of tension in the area. The Council reaffirmed that the agreements on disengagement of forces were only a step toward peace and called upon all the parties concerned to implement immediately resolution 338, decided to renew the mandate of the Force for another six months and decided that the Secretary-General would submit a report at the end of that period regarding developments in the situation and measures taken to implement resolution 338. \n\nThe resolution passed with 13 votes to none, while China and Iraq did not participate in voting.\n\n\n"}
{"id": "1721249", "url": "https://en.wikipedia.org/wiki?curid=1721249", "title": "Uyghurlar", "text": "Uyghurlar\n\nUyghurlar (in English: \"The Uyghurs\") is a book by poet Turghun Almas on the history of the \"6,000 year history\" of the Uyghur ethnic group of the Xinjiang region of China. It was published in the People's Republic of China in 1989, at a high point of liberalization of academic freedom and ethnic minority policy in China. The book uses a stylized wolf on its cover that is a widely recognized symbol of Pan-Turkism. It was one of the books of the period that presented an \"alternative Uyghur history\", based on Soviet historiography during the Sino-Soviet split, that advanced the thesis that the Uyghurs were \"indigenous\" to Xinjiang and should have an independent state. It was also one of the first books to publicize the term East Turkestan, which suggests a kinship to a \"West Turkestan\" in the independent Central Asian states. In contrast to the official Chinese history of Xinjiang, which states that the region was an integral part of China since the Han dynasty, the book takes a nationalist view, saying that many \"Uyghur\" states throughout history were independent of, or even dominant over, China.\n\nThe book makes a number of non-orthodox theories about history, including that the Tarim mummies indicate that the Uyghurs were \"older than Chinese civilization itself\", and that the Uyghurs invented the compass, gunpowder, papermaking, and printing. It concluded, \"If the Jews could reclaim their homeland after 3,000 years, the Uyghurs should be able to reclaim their homeland after 3,000 to 6,000 years\". In response to the book's growing popularity among Uyghurs, in February 1991, the Xinjiang CPC Propaganda Department and the Xinjiang Academy of Social Sciences jointly organized an academic conference to discuss the historical claims in \"Uyghurlar\", as well as those in two of Almas's other books. More than 140 historians, ethnographers, archaeologists, and literature specialists from different ethnic groups in Xinjiang and Beijing scrutinized the research of the book, concluding that it \"distorted and falsified history\". The government soon publicized a pamphlet called \"One Hundred Mistakes of Turghun Almas's \"Uyghurlar\"\" to publicize the book's historical flaws, which had the opposite effect of increasing interest in the book. The book was banned and Almas was said to be placed under \"virtual house arrest\" in Urumqi.\n"}
{"id": "5965191", "url": "https://en.wikipedia.org/wiki?curid=5965191", "title": "V. T. Rajshekar", "text": "V. T. Rajshekar\n\nV. T. Rajshekar, in full Vontibettu Thimmappa Rajshekar, (born 1932) is an Indian journalist who is the founder and editor of the \"Dalit Voice\", which has been described in a release by Human Rights Watch as \"India’s most widely circulated Dalit journal\".\n\nBorn into the upper caste Shetty community, he is the son of late P.S. Thimmappa Shetty, who retired as the Collector of South Kanara District.Who has belong to forward caste \"Bunt\" community.\nHe was formerly a journalist on the Indian Express, where he worked for 25 years. He is the founder of the 'Dalit Voice' organisation a radical wing of the broader movement for Dalit interests.\nHe is also the author of a great number of pamphlets and books, mainly published by his own organisation.\n\nStarted in 1981, \"Dalit Voice\" is a periodical launched by Rajshekhar. Under Rajshekhar's leadership the Dalit Voice organisation formulated an Indian variant of afrocentrism similar to that of the Nation of Islam in the USA but it is different from other magazines in many aspects. It is notable for the radical antisemitism it preaches and also its link to Afrocentrist ideologies. The book declares the Indian castes as nations within the nation of India. It argues for the strengthening of each caste.\n\n\"Dalit Voice\" has published articles about 'Zionist conspiracies' regarding Hitler and the Third Reich. They have also supported the Iranian government and Mahmoud Ahmadinejad's denial of the Holocaust.\n\nIn 1986 Rajshekar’s passport was confiscated because of \"anti-Hinduism writings outside of India\". The same year, he was arrested in Bangalore under India’s Terrorism and Anti-Disruptive Activities Act. Rajshekar told Human Rights Watch that this arrest was for an editorial he had written in \"Dalit Voice\", that another writer who republished the editorial was also arrested, and that he was eventually released with an apology. Rajshekar has also been arrested under the Sedition Act and under the Indian Penal Code for creating disaffection between communities.\n\nRajshekar was married to Hema Rajshekar until her death. His son Salil Shetty works for Amnesty International in London.\n\n\n"}
{"id": "45669196", "url": "https://en.wikipedia.org/wiki?curid=45669196", "title": "Édouard Ardaillon", "text": "Édouard Ardaillon\n\nÉdouard Muller Ardaillon (born 4 May 1867 at Mazères in Ariège, died 19 September 1926 at Oran in Algeria) was a French historian, archaeologist and geographer.\n\nAfter graduating from the Boys' Catholic College of Sainte-Marie in Saint-André-de-Cubzac, he undertook a bachelor of arts. He was a scholar of the lycée Louis-le-Grand from 1884 to 1887. In 1887, he enrolled in the École Normale Supérieure where he achieved the Agrégation in 1890; he then joined the École française d'Athènes (graduated 1891).\n\nIn 1897, he defended his thesis on the mines of Laurion, the silver mines near Athens, whose rich deposits and intense exploitation played a key role in the development of Athenian power in the classical period; it still remains a reference work on this subject. He also carried out excavations in the port of Delos and visited the Cyclades, Ionia, Lydia and Rhodes. In June 1894 he married a young Greek girl while he was in Athens, with whom he went on to have two children.\n\nFrom November 1896 he was in charge of a geography programme in the faculty of Arts at Lille, where he became Chair of geography in 1899. Under his direction, the geography department of this university became important. His career received the encouragement and support of Georges Perrot, director of l'École normale supérieure; Théophile Homolle, Director of the École française d'Athènes, and Charles Bayet, rector of Lille university.\n\nIn 1905 he became rector of the University of Besançon. Finally, he was rector of the University of Algiers from 1908 until his death in 1926.\n\n"}
