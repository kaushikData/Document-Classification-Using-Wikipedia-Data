{"id": "8900170", "url": "https://en.wikipedia.org/wiki?curid=8900170", "title": "1955 in radio", "text": "1955 in radio\n\nThe year 1955 saw a number of significant events in radio broadcasting history.\n\n\n\n\n"}
{"id": "65155", "url": "https://en.wikipedia.org/wiki?curid=65155", "title": "26th century BC", "text": "26th century BC\n\nThe 26th century BC was a century which lasted from the year 2600 BC to 2501 BC.\n\n\n\n\n\n"}
{"id": "14794218", "url": "https://en.wikipedia.org/wiki?curid=14794218", "title": "Abbas Vali", "text": "Abbas Vali\n\nProfessor Abbas Vali (born 1949) is a Kurdish political and social theorist specialising in modern and contemporary political thought and modern Middle Eastern Politics.\nVali was born in 1949 in Mahabad, Iranian Kurdistan. He received his primary and secondary education in Tabriz. He obtained a BA in Political Science from the National University of Iran in 1973. He then moved to the UK to continue his graduate studies in modern political and social theory. He obtained an MA in Politics from the University of Keele in 1976. He then received his PhD in Sociology from the University of London in 1983. This was followed by a post-doctoral research fellowship funded by the Economic and Social Research Council in 1984.\n\nAbbas Vali began his academic carrier in 1986 in the Department of Political Theory and Government at the University of Wales, Swansea. He was invited by the Kurdistan Regional Government (KRG) to establish and lead a new university in Erbil in 2005. He was the Vice-Chancellor of the University of Kurdistan before he was removed for disagreements with the KRG over the management of the university in May 2008. Professor Vali has since been teaching Modern Social and Political Theory in the Department of Sociology at Bogazici University in Istanbul.\n\n\n\n"}
{"id": "12290229", "url": "https://en.wikipedia.org/wiki?curid=12290229", "title": "Ashkharbek Kalantar", "text": "Ashkharbek Kalantar\n\nAshkharbek Kalantar (; February 11, 1884, in Ardvi, Armenia – June 1942) was an Armenian archaeologist and historian. He had important role in founding of archaeology in Armenia. Born in Armenian noble families of Loris-Melikov and (from maternal side), he graduated St. Petersburg University in 1911 under Nicholas Marr. He was appointed a Fellow of the Archaeological Institute, of Russian Imperial Archaeological Society and the keeper of the Asiatic Museum in St. Petersburg. He was one of the founders of Yerevan State University. Throughout his lifetime, Ashkharbek Kalantar has authored more than 80 scholarly articles.\nAshkharbek Kalantar was born in Ardvi, Armenia on February 11, 1884. He received his early education at the Nersisian School in Tiflis, graduating from there in 1903. He continued his studies at the St. Petersburg University with Nicholas Marr as his teacher. He terminated his studies in 1911 to become a member of the archaeological society of the University.\n\nAlready a student, since 1907 Kalantar participated the archaeological excavations of Nicholas Marr in Armenian medieval capital Ani. In 1914 Kalantar was appointed the head of the XIIIth Ani Archaeological excavation campaign. In 1918 he organized the evacuation of about 6000 items from the Ani Museum, which are currently in History Museum of Armenia in Yerevan. He was the last archaeologist to describe monuments, mostly in Ani region, which did not survive since 1920s.\n\nIn 2013 an international expedition repeated Kalantar's expedition of August–September 1920 and compared the present, often desperate situation of over 10 monuments of Ani region (Alaman, Arjo-Arij, Bagaran, Khtskonk, Mren, etc.).\n\nIn 1910s Kalantar studied the ancient monuments in the Lori and Surmali regions, the basilica in Zor, headed the excavations in medieval monastery Vanstan (Imirzek) in Armenia and revealed its epigraphic materials. In 1917 with Nicholas Adontz he participated the II Van Archaeological expedition and studied the Urartian inscriptions there.\n\nFrom 1920 to the 1930s he revealed the existence of a pre-Urartian irrigation system on Mt. Aragats and Geghama range in Armenia, discovered vishaps (stone monuments) there and studied the rock carved figures, also published articles on the Urartian inscriptions found in Armenia.\n\nIn 1918 and 1919 he lectured in Transcaucasian university in Tiflis, in 1919 he becomes one of seven founding members of the Yerevan State University, the founder of the chair of archaeology and oriental studies.\n\nWith architect Alexander Tamanian and painter Martiros Saryan he was one of founders of the Commission of Ancient Monuments in Armenia. During 1920-1938 he organized over 30 expeditions in Armenia.\n\nIn 1931 Kalantar directed the excavations in Old Vagharshapat. In 1930s with Alexander Tamanian acted to save the two basilica churches, Katoghike and Poghos-Petros in Yerevan (both were finally destroyed by the ruling regime).\n\nIn 1935 Kalantar was appointed as the member of the Council of Armenian branch of Soviet Academy of Sciences.\n\nAshkharbek Kalantar authored over 80 articles. The English translations of his selected works were published in 3 volumes in 1994, 1999, 2004 by Recherches et Publications, Paris-Neuchâtel, and in Armenian in Yerevan, 2007.\n\nAmerican Journal of Archaeology (AJA 100, 638, 1996) reviewing Kalantar's volume writes:\n\"While Lehmann-Haupt and Marr are often credited with sparking investigations into the history and prehistory of eastern Anatolia and\nsouthern Transcaucasia, this compilation* of selected writings and photographs of Ashkharbek Kalantar (1884-1942) makes a persuasive case that it was he who most thoroughly shaped archaeology in the Armenian highlands.\"\n\n\"Kalantar's life and work provide a testimony to the enduring importance of the Armenian highlands to world history and prehistory...\"\nOn 19 March 1938 Kalantar was arrested in Yerevan by the regime, among other professors, as an ‘enemy of nation’ (‘the professors process’); the precise date and place of his death in Russia are unknown.\n\nThe bust of Ashkharbek Kalantar was opened in February 2015 in the entrance hall of Yerevan State University.\n\nEdik Minasyan, Dean of the Faculty of History at Yerevan State University, said of Kalantar:\n"}
{"id": "35563635", "url": "https://en.wikipedia.org/wiki?curid=35563635", "title": "Battle at Springmartin", "text": "Battle at Springmartin\n\nThe Battle at Springmartin was a series of gun battles in Belfast, Northern Ireland on 13–14 May 1972. It involved the British Army, the Provisional Irish Republican Army (IRA), and the Ulster Volunteer Force (UVF).\n\nThe violence began when a car bomb, planted by Ulster loyalists, exploded outside a crowded public house in the mainly Irish nationalist and Catholic district of Ballymurphy. UVF snipers then opened fire on the survivors from an abandoned high-rise flat. This began the worst fighting in Northern Ireland since the suspension of the Parliament of Northern Ireland and the imposition of direct rule from London. For the rest of the night and throughout the next day, local IRA units fought gun battles with both the UVF and British Army. Most of the fighting took place along the interface between the Catholic Ballymurphy and Ulster Protestant Springmartin housing estates, and the British Army base that sat between them.\n\nSeven people were killed in the violence: five civilians (four Catholics, one Protestant), a British soldier and a member of the IRA Youth Section. Four of the dead were teenagers.\n\nShortly after 5:00 on Saturday 13 May 1972, a car bomb exploded without warning outside Kelly's Bar, at the junction of the Springfield Road and Whiterock Road. The pub was in a mainly Irish Catholic and nationalist area and most of its customers were from the area. At the time of the blast, the pub was crowded with men watching an association football match between England and West Germany on colour television. Sixty-three people were injured, eight of them seriously. John Moran (19), who had been working at Kelly's as a part-time barman, died of his injuries on 23 May.\n\nAt first, the British Army claimed that the blast had been an \"accident\" caused by a Provisional IRA bomb. The Secretary of State for Northern Ireland, William Whitelaw, told the House of Commons on 18 May that the blast was caused by a Provisional IRA bomb that exploded prematurely. However, locals suspected that the loyalist Ulster Defence Association (UDA) had planted the bomb. Republican sources said that IRA volunteers would not have risked storing such a large amount of explosives in such a crowded pub. It later emerged that the bomb had indeed been planted by loyalists.\n\nA memorial plaque on the site of the former pub names three members of staff who lost their lives as a result of the bomb and the gun battles that followed. It reads: \"...here on 13th May 1972 a no warning Loyalist car bomb exploded. As a result, 66 people were injured and three innocent members of staff of Kelly's Bar lost their lives. They were: Tommy McIlroy (died 13th May 1972), John Moran (died from his injuries 23rd May 1972), Gerard Clarke (died from his injuries 6th September 1989).\"\n\nThe night before the bombing, gunmen from the UVF West Belfast Brigade had taken up position along the second floor of an abandoned row of maisonettes (or flats) at the edge of the Protestant Springmartin estate. The flats overlooked the Catholic Ballymurphy estate. Rifles, mostly Second World War stock, were ferried to the area from dumps in the Shankill.\n\nNot long after the explosion, the UVF unit opened fire on those gathered outside the wrecked pub, including those who had been caught in the blast. A British Army spokesman said that the shooting began at about 5:35 , when 30 high-velocity shots were heard. Social Democratic and Labour Party Member of Parliament Gerry Fitt said that shots had been fired from the Springmartin estate only minutes after the bombing. William Whitelaw, however, claimed that the shooting did not begin until 40 minutes after the blast. Ambulances braved the gunfire to reach the wounded, which included a number of children. Tommy McIlroy (50), a Catholic civilian who worked at Kelly's Bar, was shot in the chest and killed outright. He was the first to be killed in the violence.\nMembers of both the Provisional and Official wings of the IRA \"joined forces to return the fire\", using Thompson submachine guns, M1 carbines and a Bren light machine gun. When British troops arrived on the scene, they too were fired upon by IRA units. Corporal Alan Buckley (22) of the 1st Battalion The Kings Regiment was fatally shot by the Provisionals on Whiterock Road. A platoon of soldiers then gave covering fire while a medical officer tried to help him. Another soldier was also wounded in the gunfight. Following this, 300 members of the Parachute Regiment were sent to back up the King's Own Scottish Borderers.\n\nOver the next few hours there were 35 separate shooting incidents reported, making it the most violent night since the suspension of the Northern Ireland government and imposition of Direct Rule from London earlier that year. The IRA exchanged fire with both the British Army and with the UVF snipers on the Springmartin flats. Most of the IRA's fire was aimed at the Henry Taggart Army base—near the Springmartin flats—which was hit by over 400 rounds in the first 14 hours of the battle. Although most of the republican gunfire came from the Ballymurphy estate, British soldiers also reported shots being fired from the nearby mountain slopes. According to journalist Malachi O'Doherty, a source claimed that the British Army had also fired into Belfast City Cemetery between the Whiterock and Springfield roads.\n\nTwo more people were killed that night. The first was 15-year-old Michael Magee, a member of Fianna Éireann (the IRA youth wing), who was found shot in the chest at New Barnsley Crescent, near his home. He died shortly after he was brought to the Royal Victoria Hospital. Two men who took him there claimed they were beaten by British soldiers who had just heard of Corporal Buckley's death. A death notice said that Magee was killed by the British Army but the republican publication \"Belfast Graves\" claimed he had been accidentally shot. The other was a Catholic civilian, Robert McMullan (32), who was shot at New Barnsley Park, also near his home. Witnesses said there was heavy gunfire in the area at 8 and then \"a single shot rang out and Robert McMullan fell to the ground\". It is thought that he was shot by soldiers firing from Henry Taggart base.\n\nOn the first night of the battle, the Royal Ulster Constabulary (RUC) arrested two young UVF members, Trevor King and William Graham. They were found at a house in Blackmountain Pass trying to fix a rifle that had jammed. During a search of the house, the RUC found three Steyr rifles, ammunition and illuminating flares.\n\nThe fighting between the IRA, UVF and British Army resumed the following day. According to the book \"UVF\" (1997), British soldiers were moved into the ground floor of the abandoned flats while the UVF snipers continued firing from the flats above them. The soldiers and UVF were both firing into Ballymurphy, and according to the book both were \"initially unaware of each other\". However, according to a UVF gunman involved in the battle, there was collusion between the UVF and British soldiers. He alleged that a British foot patrol caught a UVF unit hiding guns in a bin but ignored their cache with a wink when the UVF member said the guns were \"rubbish\". According to Jim Cusack and Henry McDonald, Jim Hanna — who later became UVF Chief of Staff — was one of the snipers operating from Springmartin during the battle. Jim Hanna told journalist Kevin Myers that, during the clashes, a British Army patrol helped Hanna and two other UVF members get into Corry's Timber Yard, which overlooked the Catholic Ballymurphy estate. When a British Army Major heard of the incident he ordered his men to withdraw, but they did not arrest the UVF members, who were allowed to hold their position. The IRA's Ballymurphy unit was returning fire at an equal rate and some 400 strike marks were later counted on the flats.\n\nIn the Springmartin estate, gunfire killed Protestant teenager John Pedlow (17) and wounded his friend. According to the book \"Lost Lives\", they had been shot by soldiers. His friend said that they had been walking home from a shop when there was a burst of gunfire, which \"came from near the Taggart Memorial Army post and seemed to be directed towards Black Mountain Parade\". However, Malcolm Sutton's \"Index of Deaths from the Conflict in Ireland\" states that he was killed by the IRA. An inquest into Pedlow's death found that he had been hit by a .303 bullet, which was likely a ricochet. Pedlow was given a loyalist funeral, but police said there was nothing to link him with any \"illegal organisation or acts\".\n\nUVF snipers continued to fire from the high-rise flats on the hill at Springmartin Road. About three hours after the shooting of Pedlow, a bullet fatally struck a 13-year-old Catholic girl, Martha Campbell, as she walked along Springhill Avenue. She was among a group of young girls and a witness said the firing must have been directed at himself and the girls, as nobody else was in the area at the time. Reliable loyalist sources say that the schoolgirl was shot by the UVF.\n\nShortly afterwards, the loyalist UDA used roadblocks and barricades to seal-off the Woodvale area into a \"no-go\" zone, controlled by the UDA's B Company, which was then commanded by former British soldier Davy Fogel.\n\n"}
{"id": "40863726", "url": "https://en.wikipedia.org/wiki?curid=40863726", "title": "Battle of Gaza (2007)", "text": "Battle of Gaza (2007)\n\nThe Battle of Gaza, also referred to as Hamas' takeover of Gaza, was a military conflict between Fatah and Hamas, that took place in the Gaza Strip between the June 10 and 15, 2007. It was a prominent event in the Fatah–Hamas conflict, centered on the struggle for power, after Fatah lost the parliamentary elections of 2006. Hamas fighters took control of the Gaza Strip and removed Fatah officials. The battle resulted in the dissolution of the unity government and the \"de facto\" division of the Palestinian territories into two entities, the West Bank governed by the Palestinian National Authority, and Gaza governed by Hamas.\n\nThe ICRC estimated that at least 118 people were killed and more than 550 wounded during the fighting in the week up to 15 June.\n\nIn 2003, the Basic Law of the Palestinian Authority (PA) was amended and a semi-presidential form of government was established, whereby a constitution creates a directly elected fixed-term president, plus a prime minister and cabinet collectively responsible to the legislature. While some writers do point to the potential advantages of semi-presidentialism, the academic consensus is largely unsupportive of this form of government. \n... in January 2006, Hamas gained a majority at the legislative elections. This led to ‘cohabitation’ between a Fatah president and a Hamas prime minister and government and created ‘two competing centers of power’, which is said to be so damaging for fragile democracies with semi-presidential constitutions. Following the election, and in the context of an extremely difficult domestic and international situation, the Palestinian Authority (PA) descended into civil war. By June 2007, the PA had, in effect, split in two, with Hamas ruling the Gaza Strip and Fatah retaining authority over the West Bank. In the same month President Abbas declared a state of emergency and dismissed the Hamas prime minister, Ismail Haniya, though Hamas maintains that this decision was unconstitutional and still considers Haniya to be the legitimate head of government. Whatever the legality of the situation, by this time the governance structures of the Basic Law had all but broken down.\n\nIt is very difficult to determine to what extent semi-presidentialism is responsible for the problems of governance in the PA in the period 2006–2007. Indeed, usually, the importance of institutions in shaping the behaviour of political actors in the Palestinian context is often neglected in favour of other factors, notably the inherently conflictual nature of the relationship between Fatah and Hamas. Moreover, the internal and external problems faced by the PA are greater than those faced by almost any other jurisdiction in the world and it would be naive to suggest that semi-presidentialism per se was anything other than a contributory factor to the problems faced by the Authority since January 2006.\n\nThe Palestinian Basic Law (2003) is by definition pertinent to the evaluation of the legality of actions of Palestinian parties in the aftermath of the 2006 Palestinian Legislature elections. President Abbas had very specific authorities under the Basic Law. Article 88 - If the president of the state or the prime minister, in case of necessity, suggest the dissolution of the Representative Council to the Council of Ministers, then its dissolution may be approved by a majority of two-thirds of its members. Under article 126 the President is the supreme head of the Palestinian national security forces which had to be headed by a 'cognizant minister' (appointed by the Prime Minister - article 137). Article 137 dictates that the Prime Minister forms the government of Palestine and speaks for it, is head of the Representative Council, and that he appoints ministers and senior positions. Article 153 forbids the formation of armed groups outside the network of national security forces. The police (article 154), on the other hand, form a civilian organisation, part of the Ministry of the Interior (appointed by the Prime Minister). The victors, Hamas, ruled this governing body in 2006/2007 via the Representative Council. Hamas's Ismail Haniyeh, was the prime minister.\n\nYasser Arafat, the President of the Palestinian Authority, died on 11 November 2004. The Palestinian presidential election to fill the position took place on 9 January 2005 in both the West Bank and Gaza, but were boycotted by both Hamas and Islamic Jihad. The election resulted in PLO and Fatah chairman Mahmoud Abbas being elected President to a four-year term. On 8 February 2005 the Palestinian Authority President Mahmoud Abbas and Israeli Prime Minister Ariel Sharon announced a cease-fire. On 17 March 2005 Hamas endorsed the ceasefire. On 19 March 2005 twelve Palestinian factions, including Fatah, Hamas, Islamic Jihad, Popular Front for the Liberation of Palestine (PFLP) and Democratic Front for the Liberation of Palestine (DFLP) signed the Palestinian Cairo Declaration, which reaffirmed the status of the Palestine Liberation Organization (PLO) as the sole legitimate representative of the Palestinian people through the participation in it of all forces and factions according to democratic principles. The Declaration implied a reform of the PLO by the inclusion in the PLO of Hamas and Islamic Jihad. On 12 September 2005 Israel completed its disengagement from the Gaza strip.\n\nThe Addameer organisation recorded that, on 26 September 2005, during the lead-up to the 2006 elections, Israel launched a campaign of arrest against Palestinian Legislative Council (PLC) members. 450 members of Hamas were detained, mostly involved in the 2006 PLC elections. On 25 January 2006 the Palestinian legislative election, judged to be free and fair by international observers, took place. It resulted in a Hamas victory, which surprised Israel and the United States which had expected their favoured partner, Fatah, to retain power. On 27 January U.S.President George Bush said \"the landslide victory of the militant Islamic group Hamas was rejection of the \"status quo\" and a repudiation of the \"old guard\" that had failed to provide honest government and services\". On 30 January 2006, the Quartet (United States, Russia, United Nations, and European Union) predicated future foreign assistance to the Palestinian Authority on the future government's commitment to non-violence, recognition of the State of Israel, and acceptance of previous agreements. Hamas rejected these conditions, saying that \"the \"unfair conditions would endanger the well-being of Palestinians\". This view was echoed by Prince Saud of Saudi Arabia who believed that the international community are being \"unreasonable\". Saudi Foreign Minister Prince Saud al-Faisal told reporters in Malaysia: \"The European Union insisted on having elections in Palestine, and this is the result of what they asked for. Now to come around, and say [they] don't accept the will of the people that was expressed through democratic means, seems an unreasonable position to take.\" The BBC's diplomatic correspondent, James Robbins, said the Quartet's response was chosen with care: \"They did not demand a renunciation of violence or immediate recognition of Israel, but a commitment to these things in the future\". \n\nShortly after Hamas established the new Palestinian government on 29 March 2006, which was led by Hamas leader Ismail Haniya as Prime Minister and comprised mostly Hamas members, after Fatah and other factions refused to join in a national unity government. The Quartet suspended its foreign assistance program, and Israel imposed economic sanctions and a blockade of the Gaza Strip. \n\nFollowing the abduction by Hamas militants of Gilad Shalit on 25 June 2006 in a cross-border raid via an underground tunnel out of Gaza, Israel arrested 49 senior Hamas officials, including 33 parliamentarians, nearly a quarter of PLC members and ministers on the West Bank, and intensified the boycott of Gaza and took other punitive measures.\n\nMahmoud Abbas was under pressure by the Quartet on the Middle East, which considered Hamas' unacceptable as it was perceived to undermine decades of international efforts to secure a peaceful resolution to the conflict. Therefore, the international community decided to boycott the Hamas-led government by severing diplomatic ties and halting financial aid until Hamas would fulfill certain conditions. It was suggested that Abbas could use his constitutional powers to dismiss the government and call for new elections, which were intended to yield a different result and reinstall Fatah in power on the grounds that the Palestinian electorate would perceive Hamas as a failure. The threat of new elections was never carried out because it emerged that Hamas might in fact be returned to power despite its inability to implement its manifesto and because the movement itself strongly signalled that\ncalling new elections although a constitutional prerogative of the President, would amount to ‘a coup against Palestinian legitimacy and the will of the Palestinian people’\n\nThe US and Israel attempted to undermine Hamas and force it from power while strengthening the position of President Abbas.\n\nThe new government clashed with President Abbas, who shared power with it based on the Basic Law. Through presidential decrees, Abbas took exclusive presidential authority over several administrative powers and periodically made threats of dismissal.\n\nAfter refusing to accept the plan of the Hamas-led PA government to reform the security sector loyal to Abbas and Fatah, Abbas placed the security forces under his direct control and built up his own Presidential Guard. Hamas then created a parallel security force, which was made up of its own members of the al-Qassam Brigades. The two forces refused to cooperate. Hamas’ forces represented a tradition of armed resistance, whereas those of Fatah's were committed to the upholding of the Oslo Accords.\n\nBy 2007, Hamas was unable to pay salaries or get recognition from European donor countries and international organisations. This led to the first fighting between Hamas and Fatah. In the last month of 2006, factional fighting left 33 people dead. On 7 January 2007, Palestinian Authority President Mahmoud Abbas ordered the Hamas-led Interior Ministry's paramilitary police force, the most powerful armed unit outside his control in the factional fighting, to be incorporated into the security apparatus loyal to him. The ministry responded with defiance, announcing plans to double the size of its force. The combative announcements raised the prospect of an intensified armed standoff. Abbas' only means of enforcing the order appeared to be coercive action by police and security units under his command, but they were relatively weak in the Gaza Strip, Hamas's stronghold.\n\nDocuments published in Palestine Papers reveal the British intelligence MI6, helped draw up a security plan for Fatah-led Palestinian Authority. The plan mentioned as objective \"encourage and enable the Palestinian Authority (PA) to fully meet its security obligations under Phase 1 of the Roadmap\". It proposes a number of ways of \"degrading the capabilities of rejectionists\", naming Hamas (the victorious party in the recently elected PA executive government), PIJ (Palestinian Islamic Jihad) and the Fatah-linked al-Aqsa Brigades. The plan was described by the Guardian as a \"wide-ranging crackdown on Hamas\".\n\nIn 2004, the plan was passed to Jibril Rajoub, a senior Fatah official of the PA. The bulk of the plan has since been carried out. Issues noted in the plan were suicide bombing, weapons smuggling, Qassam rockets and \"terror finance\". Its most controversial section recommended that \"Degrading the capabilities of the rejectionists – Hamas, PIJ\" [Palestinian Islamic Jihad] \"and the\" [Fatah-linked] \"Al Aqsa Brigades – through the disruption of their leaderships' communications and command and control capabilities; the detention of key middle-ranking officers; and the confiscation of their arsenals and financial resources\". Suggesting temporary internment of leaders and activists, the closure of radio stations and the replacement of imams in mosques.\n\nAfter 2006 elections, Hamas announced the formation of its own security service, the \"Executive Force\", appointing Jamal Abu Samhadana, a prominent militant, at its head. Abbas had denounced the move as unconstitutional, saying that only the Palestinian president could command armed forces. U.S. training program began after that. According to Lt. Gen. Keith Dayton, the U.S. security coordinator \"We are involved in building up the Presidential Guard, instructing it, assisting it to build itself up and giving them ideas. We are not training the forces to confront Hamas,\" adding that \"Hamas is receiving money and arms from Iran and possibly Syria, and we must make sure that the moderate forces will not be erased,\".\n\nThe American effort was part of a broader international package proposed to the Quartet to strengthen Mahmoud Abbas's Palestinian security forces as Hamas threatens to increase its parallel Executive Force to 6,000 men. Training for Fatah \"Presidential Guard\" was provided by Egypt, Jordan and Turkey. Additional non-lethal equipment and fund for the purchase of arms were provided. Israel, too, allowed light arms to flow to members of the Presidential Guard, though Jordan and Egypt\n\nIsrael's Security Agency supported President Abbas and the Presidential Guard, but was concerned about the weakness of Fatah. There was a plan to add the PLO's Jordan-based Badr Brigade to the Presidential Guard. As of October 2006, \"while clashes between Hamas and elements of Fatah had been fierce\", Israel Security Agency Director Yuval Diskin did not believe the Palestinians were on the verge of a civil war because neither side wanted one, but \"should something happen to Haniyeh on the Hamas side, or Mohammed Dahlan or Rashid Abu Shabak on the Fatah side, anything could happen.\"\n\nAccording to the IISS, the June 2007 escalation was triggered by Hamas' conviction that the PA's Presidential Guard, loyal to Mahmoud Abbas, was being positioned to take control of Gaza. The US had helped build up the Presidential Guard to 3,500 men since August 2006.\n\nThe Hamas-led government met with Israeli opposition and Quartet sanctions, and President Abbas and the Fatah-dominated PLO developed a plan to replace the Hamas government with one acceptable to Israel and the international community. According to the plan, unveiled in Al Jazeera's Palestine Papers, a national unity government or a government of technocrats would be formed by the end of November to prepare early presidential and legislative elections by mid-2007. If the establishment of a government meeting the Quartet's conditions failed, President Abbas would announce a state of emergency, dismiss the government and form an emergency government, or call early elections. The developers of the plan were aware that an emergency government could legally govern only for 1 month without a vote of confidence by the PLC. An \"Action Plan Leading to Early Elections\" envisioned a strong enlargement of Fatah's Presidential Guard, internal reform of Fatah, empowering of presidential institutions, resumption of aid by the international community through the President’s Office, and end of withholding of taxes by Israel.\n\nBy October 2006, the US administration, Israel, many Arab governments, and much of Fatah – including most of Abbas’ key advisors still held the view that if Hamas did not unambiguously accept the Quartet’s conditions, it should and could be forced out of power through early elections, its dismissal and appointment of an emergency government or a popular referendum seeking Palestinian agreement to the Quartet’s conditions. The plan had also been put forward by US President Bush in his meeting with Abbas on 20 September 2006.\n\nThe plan was largely carried out:\n\nElections, however, did not take place. Already in December 2006, Abbas announced a plan for early elections \"not be held until mid-2007\", provoking tensions and some clashes between Hamas and Fatah supporters. In July 2007, he again called for early elections. Other calls were in October 2009 and in February 2011.\n\nOn June 10, 2007, the Fatah–Hamas conflict culminated in clashes, between the Fatah-allied forces on one side and the Hamas-allied forces on the other side. Major Fatah forces were the National Security Forces, particularly the \"Presidential Guard\". Main force of Hamas was the \"Executive Force\".\n\nHamas militants seized several Fatah members and threw one of them, Mohammed Sweirki, an officer in the elite Palestinian Presidential Guard, off the top of the tallest building in Gaza, a 15-story apartment building. In retaliation, Fatah militants attacked and killed the imam of the city's Great Mosque, Mohammed al-Rifati. They also opened fire on the home of Prime Minister Ismail Haniyeh. Just before midnight, a Hamas militant was thrown off a 12-story building.\n\nOn June 11, the residences of both Mahmoud Abbas, Fatah's leader and the Palestinian Authority president, and of then-Prime Minister Ismail Haniya, of Hamas, were targeted with gun and shell fire.\n\nOn June 12, Hamas began attacking posts held by their Fatah faction rivals. Hundreds of Hamas fighters had moved on the positions after giving their occupants two hours to leave. A major Fatah base in the northern town of Jabaliya fell to Hamas fighters, witnesses told AFP news agency. Heavy fighting also raged around the main Fatah headquarters in Gaza City, with Hamas militants attacking with rocket-propelled grenades and automatic weapons.\n\nOn June 13, Hamas seized the headquarters of the Fatah-controlled National Security Forces in northern Gaza. Gunmen fought for control of high-rise buildings serving as sniper positions and Hamas said it had bulldozed a Fatah outpost controlling Gaza's main north-south road. Also on that day, an explosion wrecked the Khan Younis headquarters of the Fatah-linked Preventive Security Service, killing five people.\n\nOn June 14, Hamas gunmen completed the takeover of the central building of the Palestinian Preventive Security Service's headquarters in the Gaza Strip. The Hamas members took over vehicles and weapons in the compound, which was considered the Palestinian Authority's main symbol in the Strip. The Preventive Security Service cooperated with Israel in the past, and has been armed by the United States. It has been identified with Fatah powerhouse Mohammed Dahlan, who has become a figure hated by the Islamists in Gaza. The gunmen who entered the compound held a prayer there and waved a flag on the building's rooftop. At least 10 people were killed. Hamas TV broadcast a display of weapons inside the building, as well as jeeps, mortar shells and bulletproof vests seized in the compound, which according to Hamas, were smuggled to Fatah by Israel and the Americans in the past few months through the border with Egypt.\n\nHamas members held a prayer in the compound, which they referred to as the \"heresy compound.\" Hamas also changed the name of the neighborhood where the building is located from \"Tel al-Hawa\" to \"Tel al-Islam.\"\n\nOn the afternoon of June 14, the Associated Press reported an explosion that rocked Gaza City. According to Fatah officials, security forces withdrew from their post and blew it up in order to not let Hamas take it over. The security forces afterwards repositioned to another location. Later on June 14, Hamas also took control of the southern Gaza Strip city Rafah which lies near an already closed border crossing with Egypt, which is monitored by Israeli, Palestinian and European Union security forces. The EU staff had, at that time, already been relocated to the Israeli city of Ashkelon for safety reasons. On June 14, Abbas dissolved the Palestinian-Hamas unity government, on 15 June, Hamas completed the control over Gaza.\n\nAs a result of the battle, Hamas got complete control of Gaza. The pro-Fatah view is, that it was a plain military coup by Hamas. The pro-Hamas view is, that the US drew up a plan to arm Fatah cadres with the aim of forcefully removing Hamas from power in Gaza. According to the pro-Hamas view, Fatah fighters, led by commander Mohammed Dahlan with logistical support from the US Central Intelligence Agency, were planning to carry out a bloody coup against Hamas. Then, Hamas pre-emptively took control over Gaza.\n\nIn an April 2008 article in \"Vanity Fair\" magazine, the journalist David Rose published confidential documents, apparently originating from the US State Department, which would prove that the United States collaborated with the Palestinian Authority and Israel to attempt the violent overthrow of Hamas in the Gaza Strip, and that Hamas pre-empted the coup. The documents suggest that a government with Hamas should meet the demands of the Middle East Quartet, otherwise President Mahmoud Abbas should declare a \"state of emergency\", which effectively would dissolve the current unity government, or the government should collapse by other means. Rose quotes former Vice President Dick Cheney’s chief Middle East adviser David Wurmser, accusing the Bush administration of “engaging in a dirty war in an effort to provide a corrupt dictatorship [led by Abbas] with victory.” He believes that Hamas had no intention of taking Gaza until Fatah forced its hand. “It looks to me that what happened wasn’t so much a coup by Hamas but an attempted coup by Fatah that was pre-empted before it could happen”\n\nAccording to Alastair Crooke, the then British Prime Minister Tony Blair decided in 2003 to tie UK and EU security policy in the West Bank and Gaza to a US-led counter-insurgency against Hamas. This led to an internal policy contradiction that pre-empted the EU from mounting any effective foreign policy on the \"peace process\" alternative to that of the US. At a political level, the EU \"talked the talk\" of reconciliation between Fatah and Hamas, Palestinian state-building and democracy. At the practical level, the EU \"walked the walk\" of disruption, detention, seizing finances, and destroying the capabilities of one [Hamas] of the two factions and prevented the parliament from exercising any function.\n\nAccording to Crooke, the Quartet conditions for engagement with Hamas, which the EU had endorsed after the 2006 elections, were conditions raised precisely in order to prevent Hamas from meeting them, rather than as guidelines intended to open the path for diplomatic solutions. Then, British and American intelligence services were preparing a \"soft\" coup to remove Hamas from power in Gaza.\n\nHuman Rights Watch accused both sides of violating international humanitarian law, in some instances amounting to war crimes. The accusations include targeting and killing civilians, public executions of political opponents and captives, throwing prisoners off high-rise apartment buildings, fighting in hospitals, and shooting from a jeep marked with \"TV\" insignia. The International Committee of the Red Cross denounced attacks in and around two hospitals in the northern part of the Gaza strip.\n\nDuring the fighting several incidents of looting took place: a crowd took furniture, wall tiles and personal belongings from the villa of the by now deceased Palestinian leader Yasser Arafat; the home of former Fatah commander Mohammed Dahlan was also looted: \"An AFP correspondent witnessed dozens of Palestinians taking everything they could carry from Dahlan's villa – furniture, pot plants and even the kitchen sink, complete with plumbing fixtures such as taps,\"; and at the Muntada, Abbas's seafront presidential compound, witnesses reported seeing Hamas fighters remove computers, documents and guns.\n\nOn June 14, 2007, Palestinian President Mahmoud Abbas, reacted to the Hamas takeover by declaring a state of emergency. He dismissed the unity government led by Ismail Haniyeh, and by presidential decree installed Salam Fayyad as Prime Minister. Haniyeh refused to accept his dismissal, accusing Abbas of participating in a US-led plot to overthrow him. Experts in Palestinian law and independent members of the PLC have questioned the legitimacy of the Fayyad government. According to the Palestinian Basic Law, the President can dismiss the prime minister but the dismissed government continues to function as a caretaker government until a new government is formed and receives a vote of confidence from an absolute majority of the Palestinian Legislative Council. The Hamas-majority PLC has never met to confirm the Fayyad government. President Abbas by presidential decree in September 2007 changed the voting system for the PLC into a full proportional representation system, bypassing the dysfunctional PLC.\n\nWith the dissolution of the Hamas-led unity government, the territory controlled by the Palestinian Authority was \"de facto\" divided into two entities: the Hamas-controlled government of the Gaza Strip, and the West Bank, governed by the Palestinian National Authority.\n\nThe international community recognized the emergency government. Within days, the US recognized the Fayyad government and ended the 15-month economic and political boycott of the Palestinian Authority in a bid to bolster President Abbas and the new Fatah-led Fayyad government. The European Union similarly announced plans to resume direct aid to the Palestinians, while Israel released to Abbas Palestinian tax revenues that Israel had withheld since Hamas took control of the Palestinian Legislative Council. The Middle East Quartet reiterated their continued support to Abbas and resumed normal relations with the Fatah-led PA. UN Secretary General Ban Ki-moon urged international support for Abbas's efforts \"to restore law and order\". Israel and Egypt began a blockade of the Gaza Strip.\n\nA Hamas spokesman in Gaza, Fouzi Barhoum, said earlier that Hamas was imposing Islamic law in Gaza but this was denied by exiled Hamas leader Khaled Mashal.\n\nSheik Abu Saqer, leader of Jihadia Salafiya, an Islamic outreach movement that recently announced the opening of a \"military wing\" to enforce Muslim law in Gaza. \"I expect our Christian neighbors to understand the new Hamas rule means real changes. They must be ready for Islamic rule if they want to live in peace in Gaza.\" The sole Christian bookstore in Gaza was attacked and the owner murdered.\n\nHamas has captured thousands of small arms and eight armored combat vehicles supplied by the United States, Egypt, and Jordan to the Palestinian Authority.\n\nAccording to Muhammad Abdel-El of the Hamas-allied Popular Resistance Committees, Hamas and its allies have captured quantities of foreign intelligence, including CIA files. Abu Abdullah of Hamas' \"military wing\", the Izz ad-Din al-Qassam Brigades, claims Hamas will make portions of the documents public, in a stated attempt to expose covert relations between the United States and \"traitor\" Arab countries.\n\nWhile Hamas collected most of the 15,000 weapons registered to the former security forces, it failed to collect more than a fraction of the 400,000 weapons that are in the hands of various clans, and said that it would not touch weapons used for fighting Israel, only those that might be used against Hamas.\n\n"}
{"id": "7801511", "url": "https://en.wikipedia.org/wiki?curid=7801511", "title": "Birth registration in Ancient Rome", "text": "Birth registration in Ancient Rome\n\nBirth certificates for Roman citizens were introduced during the reign of Augustus (27 BC–14 AD). Until the time of Alexander Severus (222–235 AD), it was required that these documents be written in Latin as a marker of \"Romanness\" \"(Romanitas)\".\n\nThere are 21 extant birth registration documents of Roman citizens. A standard birth registration included the date of birth.\n\nCompleting birth registrations in Roman society were not compulsory. Whereas penalties for failure to register in the census existed, no known penalties existed in regard to birth registrations. In terms of Roman law, individuals who did not register their birth were neither penalized nor disadvantaged: there are imperial rescripts (a written answer of a Roman emperor to a query or petition in writing) that state that the failure to register children should not deprive them or their right to legitimacy, and there are recorded statements of Roman Emperors Diocletian and Maximian that inform an individual that “It is a well-established rule of law that though a declaration of birth has been lost, your status is not adversely affected.”\n\nBirth registrations could be used as proof of age; however, from historical evidence, it is clear that they were not regarded as sufficient proof in themselves. Oral and written evidence could be used as proof of age. For instance, the emperor Hadrian stated in a rescript that when the age of an individual was at issue, all proofs of age should be furnished and a decision reached based on the most credible evidence. In another case, the Roman jurist Modestinus concluded that in order to prove one’s age for exemption of certain responsibilities, “age is proved either by notices of birth or by other customary (lawful) evidence.”\nRoman society did not stigmatize illegitimacy to the extent of later Western societies. A freeborn person who was illegitimate enjoyed higher social status than a freedman. Illegitimate children did have some disadvantages under the law. Their birth could not be officially registered during the first 150 years when birth certificates existed. A law passed in 178 AD, however, gave illegitimate children the same right as legitimate children to share in their mother's property if she died without leaving a will. After the Empire had come under Christian rule, this right was taken away from those born outside wedlock.\n\nFor illegitimate children, the date of birth was more complex and less authoritative since it was either as originally recorded or as copied from the public register.\n\nTwo separate processes of birth registrations existed in Roman Egypt: one process for Roman citizens that was conducted in Latin, and another process for Greco-Egyptians that was conducted in Greek. These two processes were in legal terms, totally unrelated.\n\nThere are 34 available birth registration documents of Greco-Egyptian citizens that span some 270 years. With the initiative of the father or another close relative, standard birth registrations included the name and current age of the individual concerned and was addressed to an official. \n\nGreco-Egyptian birth registrations were not compulsory and were more of a certification of status than proof of birth. The census eliminated the need of birth registrations because the information gathered from birth registrations merely supplemented the information from the census. Age was particularly important for determining who was liable to pay the poll tax at the age of 14 years. Birth registrations could provide the age of the individual; however, the census was held every 14 years to ensure that no one escaped the tax and also provided this information. The census was more efficient and thorough than the system of birth registrations in Greco-Egyptian society, and government officials relied on the information from the census far more than birth registrations.\n\n"}
{"id": "2465828", "url": "https://en.wikipedia.org/wiki?curid=2465828", "title": "Boris Rybakov", "text": "Boris Rybakov\n\nBoris Alexandrovich Rybakov (Russian: Бори́с Алекса́ндрович Рыбако́в, 3 June 1908, Moscow – 27 December 2001) was a Soviet and Russian historian who personified the anti-Normanist vision of Russian history. He is the father of Indologist Rostislav Rybakov.\n\nRybakov held a chair in Russian history at the Moscow University since 1939, was a deputy dean of the university in 1952-54, and administered the Russian History Institute for 40 years. His first groundbreaking monograph was the \"Handicrafts of Ancient Rus\" (1948), which sought to demonstrate the economic superiority of Kievan Rus to contemporary Western Europe.\n\nRybakov led important excavations in Moscow, Novgorod, Zvenigorod, Chernigov, Pereyaslav, Tmutarakan and Putivl and published his findings in numerous monographs, including \"Antiquities of Chernigov\" (1949), \"The Chronicles and Bylinas of Ancient Rus\" (1963), \"The First Centuries of Russian history\" (1964), \"The Tale of Igor's Campaign and Its Contemporaries\" (1971), \"Muscovite Maps of the 15th and early 16th Centuries\" (1974), and \"Herodotus' Scythia\" (1979). In the latter book he viewed the Scythians described by Herodotus as ancestors of modern Slavic nations.\n\nIn his older years, Rybakov attempted to reconstruct the pantheon and myths of Slavic religion. He outlined his ideas in \"Ancient Slavic Paganism\" (1981) and \"Ancient Paganism of Rus\" (1987). Some of these reconstructions have been heavily criticized as far-fetched.\n\n\n"}
{"id": "29269618", "url": "https://en.wikipedia.org/wiki?curid=29269618", "title": "Causes of the Great Recession", "text": "Causes of the Great Recession\n\nMany factors directly and indirectly caused the Great Recession (which started in 2007 with the US subprime mortgage crisis), with experts and economists placing different weights on particular causes.\n\nMajor causes of the initial subprime mortgage crisis and following recession include: International trade imbalances and lax lending standards contributing to high levels of developed country household debt and real-estate bubbles that have since burst; U.S. government housing policies; and limited regulation of non-depository financial institutions. Once the recession began, various responses were attempted with different degrees of success. These included fiscal policies of governments; monetary policies of central banks; measures designed to help indebted consumers refinance their mortgage debt; and inconsistent approaches used by nations to bail out troubled banking industries and private bondholders, assuming private debt burdens or socializing losses.\n\nOne narrative describing the causes of the crisis begins with the significant increase in savings available for investment during the 2000–2007 period when the global pool of fixed-income securities increased from approximately $36 trillion in 2000 to $80 trillion by 2007. This \"Giant Pool of Money\" increased as savings from high-growth developing nations entered global capital markets. Investors searching for higher yields than those offered by U.S. Treasury bonds sought alternatives globally.\n\nThe temptation offered by such readily available savings overwhelmed the policy and regulatory control mechanisms in country after country, as lenders and borrowers put these savings to use, generating bubble after bubble across the globe.\n\nWhile these bubbles have burst, causing asset prices (e.g., housing and commercial property) to decline, the liabilities owed to global investors remain at full price, generating questions regarding the solvency of consumers, governments, and banking systems. The effect of this debt overhang is to slow consumption and therefore economic growth and is referred to as a \"balance sheet recession\" or debt-deflation.\n\nThe fall in asset prices (such as subprime mortgage-backed securities) during 2007 and 2008 caused the equivalent of a bank run on the U.S., which includes investment banks and other non-depository financial entities. This system had grown to rival the depository system in scale yet was not subject to the same regulatory safeguards. Struggling banks in the U.S. and Europe cut back lending causing a credit crunch. Consumers and some governments were no longer able to borrow and spend at pre-crisis levels. Businesses also cut back their investments as demand faltered and reduced their workforces. Higher unemployment due to the recession made it more difficult for consumers and countries to honor their obligations. This caused financial institution losses to surge, deepening the credit crunch, thereby creating an adverse feedback loop.\n\nThe U.S. Financial Crisis Inquiry Commission reported its findings in January 2011. It concluded that \"the crisis was avoidable and was caused by: Widespread failures in financial regulation, including the Federal Reserve’s failure to stem the tide of toxic mortgages; Dramatic breakdowns in corporate governance including too many financial firms acting recklessly and taking on too much risk; An explosive mix of excessive borrowing and risk by households and Wall Street that put the financial system on a collision course with crisis; Key policy makers ill prepared for the crisis, lacking a full understanding of the financial system they oversaw; and systemic breaches in accountability and ethics at all levels.\"\n\nThe immediate or proximate cause of the crisis in 2008 was the failure or risk of failure at major financial institutions globally, starting with the rescue of investment bank Bear Stearns in March 2008 and the failure of Lehman Brothers in September 2008. Many of these institutions had invested in risky securities that lost much or all of their value when U.S. and European housing bubbles began to deflate during the 2007-2009 period, depending on the country. Further, many institutions had become dependent on short-term (overnight) funding markets subject to disruption.\n\nThe origin of these housing bubbles involves two major factors: 1) Low interest rates in the U.S. and Europe following the 2000-2001 U.S. recession; and 2) Significant growth in savings available from developing nations due to ongoing trade imbalances. These factors drove a large increase in demand for high-yield investments. Large investment banks connected the housing markets to this large supply of savings via innovative new securities, fueling housing bubbles in the U.S. and Europe.\n\nMany institutions lowered credit standards to continue feeding the global demand for mortgage securities, generating huge profits that their investors shared. They also shared the risk. When the bubbles developed, household debt levels rose sharply after the year 2000 globally. Households became dependent on being able to refinance their mortgages. Further, U.S. households often had adjustable rate mortgages, which had lower initial interest rates and payments that later rose. When global credit markets essentially stopped funding mortgage-related investments in the 2007-2008 period, U.S. homeowners were no longer able to refinance and defaulted in record numbers, leading to the collapse of securities backed by these mortgages that now pervaded the system.\n\nThe failure rates of subprime mortgages were the first symptom of a credit boom turned to bust and of a real estate shock. But large default rates on subprime mortgages cannot account for the severity of the crisis. Rather, low-quality mortgages acted as an accelerant to the fire that spread through the entire financial system. The latter had become fragile as a result of several factors that are unique to this crisis: the transfer of assets from the balance sheets of banks to the markets, the creation of complex and opaque assets, the failure of ratings agencies to properly assess the risk of such assets, and the application of fair value accounting. To these novel factors, one must add the now standard failure of regulators and supervisors in spotting and correcting the emerging weaknesses.\n\nFederal Reserve Chair Ben Bernanke testified in September 2010 regarding the causes of the crisis. He wrote that there were shocks or triggers (i.e., particular events that touched off the crisis) and vulnerabilities (i.e., structural weaknesses in the financial system, regulation and supervision) that amplified the shocks. Examples of triggers included: losses on subprime mortgage securities that began in 2007 and a run on the shadow banking system that began in mid-2007, which adversely affected the functioning of money markets. Examples of vulnerabilities in the \"private\" sector included: financial institution dependence on unstable sources of short-term funding such as repurchase agreements or Repos; deficiencies in corporate risk management; excessive use of leverage (borrowing to invest); and inappropriate usage of derivatives as a tool for taking excessive risks. Examples of vulnerabilities in the \"public\" sector included: statutory gaps and conflicts between regulators; ineffective use of regulatory authority; and ineffective crisis management capabilities. Bernanke also discussed \"Too big to fail\" institutions, monetary policy, and trade deficits.\n\nEconomists surveyed by the University of Chicago rated the factors that caused the crisis in order of importance. The results included: 1) Flawed financial sector regulation and supervision; 2) Underestimating risks in financial engineering (e.g., CDOs); 3) Mortgage fraud and bad incentives; 4) Short-term funding decisions and corresponding runs in those markets (e.g., repo); and 5) Credit rating agency failures.\n\nThe majority report of the U.S. Financial Crisis Inquiry Commission (supported by six Democrat appointees without Republican participation) reported its findings in January 2011. It concluded that \"the\ncrisis was avoidable and was caused by: Widespread failures in financial regulation, including the Federal Reserve’s failure to stem the tide of toxic mortgages; Dramatic breakdowns in corporate governance including too many financial firms acting\nrecklessly and taking on too much risk; An explosive mix of excessive borrowing and risk by households and Wall Street that put the financial system on a collision course with crisis; Key policy makers ill prepared for the crisis, lacking a full understanding of the financial system they oversaw; and systemic breaches in accountability and ethics at all levels.\"\n\nThere are several \"narratives\" attempting to place the causes of the crisis into context, with overlapping elements. Five such narratives include:\n\nBetween 1997 and 2006, the price of the typical American house increased by 124%. During the two decades ending in 2001, the national median home price ranged from 2.9 to 3.1 times median household income. This ratio rose to 4.0 in 2004, and 4.6 in 2006. This housing bubble resulted in quite a few homeowners refinancing their homes at lower interest rates, or financing consumer spending by taking out second mortgages secured by the price appreciation.\n\nBy September 2008, average U.S. housing prices had declined by over 20% from their mid-2006 peak. Easy credit, and a belief that house prices would continue to appreciate, had encouraged many subprime borrowers to obtain adjustable-rate mortgages. These mortgages enticed borrowers with a below market interest rate for some predetermined period, followed by market interest rates for the remainder of the mortgage's term. Borrowers who could not make the higher payments once the initial grace period ended would try to refinance their mortgages. Refinancing became more difficult, once house prices began to decline in many parts of the USA. Borrowers who found themselves unable to escape higher monthly payments by refinancing began to default. During 2007, lenders had begun foreclosure proceedings on nearly 1.3 million properties, a 79% increase over 2006. This increased to 2.3 million in 2008, an 81% increase vs. 2007. As of August 2008, 9.2% of all mortgages outstanding were either delinquent or in foreclosure.\n\nThe Economist described the issue this way: \"No part of the financial crisis has received so much attention, with so little to show for it, as the tidal wave of home foreclosures sweeping over America. Government programmes have been ineffectual, and private efforts not much better.\" Up to 9 million homes may enter foreclosure over the 2009-2011 period, versus one million in a typical year. At roughly U.S. $50,000 per foreclosure according to a 2006 study by the Chicago Federal Reserve Bank, 9 million foreclosures represents $450 billion in losses.\n\nBased on the assumption that sub-prime lending precipitated the crisis, some have argued that the Clinton Administration may be partially to blame. This GAO chart demonstrates that sub-prime and Alt-A loans peaked after 2003.\n\nIn addition to easy credit conditions, there is evidence that both competitive pressures and some government regulations contributed to an increase in the amount of subprime lending during the years preceding the crisis. Major U.S. investment banks and, to a lesser extent, government-sponsored enterprises like Fannie Mae played an important role in the expansion of higher-risk lending.\n\nThe term \"subprime\" refers to the credit quality of particular borrowers, who have weakened credit histories and a greater risk of loan default than prime borrowers. The value of U.S. subprime mortgages was estimated at $1.3 trillion as of March 2007, with over 7.5 million first-lien subprime mortgages outstanding.\n\nSubprime mortgages remained below 10% of all mortgage originations until 2004, when they spiked to nearly 20% and remained there through the 2005-2006 peak of the United States housing bubble. A proximate event to this increase was the April 2004 decision by the U.S. Securities and Exchange Commission (SEC) to relax the net capital rule, which encouraged the largest five investment banks to dramatically increase their financial leverage and aggressively expand their issuance of mortgage-backed securities. Subprime mortgage payment delinquency rates remained in the 10-15% range from 1998 to 2006, then began to increase rapidly, rising to 25% by early 2008.\n\nIn addition to considering higher-risk borrowers, lenders offered increasingly risky loan options and borrowing incentives.\nMortgage underwriting standards declined gradually during the boom period, particularly from 2004 to 2007. The use of automated loan approvals let loans be made without appropriate review and documentation. In 2007, 40% of all subprime loans resulted from automated underwriting. The chairman of the Mortgage Bankers Association claimed that mortgage brokers, while profiting from the home loan boom, did not do enough to examine whether borrowers could repay. Mortgage fraud by lenders and borrowers increased enormously.\n\nA study by analysts at the Federal Reserve Bank of Cleveland found that the average difference between subprime and prime mortgage interest rates (the \"subprime markup\") declined significantly between 2001 and 2007. The quality of loans originated also worsened gradually during that period. The combination of declining risk premia and credit standards is common to boom and bust credit cycles. The authors also concluded that the decline in underwriting standards did not directly trigger the crisis, because the gradual changes in standards did not statistically account for the large difference in default rates for subprime mortgages issued between 2001-2005 (which had a 10% default rate within one year of origination) and 2006-2007 (which had a 20% rate). In other words, standards gradually declined but defaults suddenly jumped. Furthermore, the authors argued that the trend in worsening loan quality was harder to detect with rising housing prices, as more refinancing options were available, keeping the default rate lower.\n\nIn 2004, the Federal Bureau of Investigation warned of an \"epidemic\" in mortgage fraud, an important credit risk of non-prime mortgage lending, which, they said, could lead to \"a problem that could have as much impact as the S&L crisis\".\n\nA down payment refers to the cash paid to the lender for the home and represents the initial homeowners' equity or financial interest in the home. A low down payment means that a home represents a highly leveraged investment for the homeowner, with little equity relative to debt. In such circumstances, only small declines in the value of the home result in negative equity, a situation in which the value of the home is less than the mortgage amount owed. In 2005, the median down payment for first-time home buyers was 2%, with 43% of those buyers making no down payment whatsoever. By comparison, China has down payment requirements that exceed 20%, with higher amounts for non-primary residences.\n\nEconomist Nouriel Roubini wrote in Forbes in July 2009 that: \"Home prices have already fallen from their peak by about 30%. Based on my analysis, they are going to fall by at least 40% from their peak, and more likely 45%, before they bottom out. They are still falling at an annualized rate of over 18%. That fall of at least 40%-45% percent of home prices from their peak is going to imply that about half of all households that have a mortgage—about 25 million of the 51 million that have mortgages—are going to be underwater with negative equity and will have a significant incentive to walk away from their homes.\"\n\nEconomist Stan Leibowitz argued in the \"Wall Street Journal\" that the extent of equity in the home was the key factor in foreclosure, rather than the type of loan, credit worthiness of the borrower, or ability to pay. Although only 12% of homes had negative equity (meaning the property was worth less than the mortgage obligation), they comprised 47% of foreclosures during the second half of 2008. Homeowners with negative equity have less financial incentive to stay in the home.\n\nThe \"L.A. Times\" reported the results of a study that found homeowners with high credit scores at the time of entering a mortgage are 50% more likely to \"strategically default\" - abruptly and intentionally pull the plug and abandon the mortgage — compared with lower-scoring borrowers. Such strategic defaults were heavily concentrated in markets with the highest price declines. An estimated 588,000 strategic defaults occurred nationwide during 2008, more than double the total in 2007. They represented 18% of all serious delinquencies that extended for more than 60 days in the fourth quarter of 2008.\n\nPredatory lending refers to the practice of unscrupulous lenders, to enter into \"unsafe\" or \"unsound\" secured loans for inappropriate purposes. A classic bait-and-switch method was used by Countrywide, advertising low interest rates for home refinancing. Such loans were written into mind-numbingly detailed contracts and then swapped for more expensive loan products on the day of closing. Whereas the advertisement might have stated that 1% or 1.5% interest would be charged, the consumer would be put into an adjustable rate mortgage (ARM) in which the interest charged would be greater than the amount of interest paid. This created negative amortization, which the credit consumer might not notice until long after the loan transaction had been consummated.\n\nCountrywide, sued by California Attorney General Jerry Brown for \"Unfair Business Practices\" and \"False Advertising\" was making high cost mortgages \"to homeowners with weak credit, adjustable rate mortgages (ARMs) that allowed homeowners to make interest-only payments.\". When housing prices decreased, homeowners in ARMs then had little incentive to pay their monthly payments, since their home equity had disappeared. This caused Countrywide's financial condition to deteriorate, ultimately resulting in a decision by the Office of Thrift Supervision to seize the lender.\n\nCountrywide, according to Republican Lawmakers, had involved itself in making low-cost loans to politicians, for purposes of gaining political favors.\n\nFormer employees from Ameriquest, which was United States's leading wholesale lender, described a system in which they were pushed to falsify mortgage documents and then sell the mortgages to Wall Street banks eager to make fast profits. There is growing evidence that such mortgage frauds may be a large cause of the crisis.\n\nOthers have pointed to the passage of the Gramm–Leach–Bliley Act by the 106th Congress, and over-leveraging by banks and investors eager to achieve high returns on capital.\n\nIn a June 2009 speech, U.S. President Barack Obama argued that a \"culture of irresponsibility\" was an important cause of the crisis. He criticized executive compensation that \"rewarded recklessness rather than responsibility\" and Americans who bought homes \"without accepting the responsibilities.\" He continued that there \"was far too much debt and not nearly enough capital in the system. And a growing economy bred complacency.\" Excessive consumer housing debt was in turn caused by the mortgage-backed security, credit default swap, and collateralized debt obligation sub-sectors of the finance industry, which were offering irrationally low interest rates and irrationally high levels of approval to subprime mortgage consumers. Formulas for calculating aggregate risk were based on the gaussian copula which wrongly assumed that individual components of mortgages were independent. In fact the credit-worthiness of almost every new subprime mortgage was highly correlated with that of any other, due to linkages through consumer spending levels which fell sharply when property values began to fall during the initial wave of mortgage defaults. Debt consumers were acting in their rational self-interest, because they were unable to audit the finance industry's opaque faulty risk pricing methodology.\n\nA key theme of the crisis is that many large financial institutions did not have a sufficient financial cushion to absorb the losses they sustained or to support the commitments made to others. Using technical terms, these firms were highly leveraged (i.e., they maintained a high ratio of debt to equity) or had insufficient capital to post as collateral for their borrowing. A key to a stable financial system is that firms have the financial capacity to support their commitments. Michael Lewis and David Einhorn argued: \"The most critical role for regulation is to make sure that the sellers of risk have the capital to support their bets.\"\n\nU.S. households and financial institutions became increasingly indebted or overleveraged during the years preceding the crisis. This increased their vulnerability to the collapse of the housing bubble and worsened the ensuing economic downturn.\n\nSeveral economists and think tanks have argued that income inequality is one of the reasons for this over-leveraging. The \"New York Times\" reported in October 2012 that research by the Brookings Institution, the I.M.F. and dozens of economists at top research universities indicated that starting in the 1970s, earnings were squeezed for low- and middle-income households. They borrowed to improve their standards of living, buying bigger houses than they could afford and using those houses as piggy banks. Research by Raghuram Rajan indicated that: \"Starting in the early 1970s, advanced economies found it increasingly difficult to grow...the shortsighted political response to the anxieties of those falling behind was to ease their access to credit. Faced with little regulatory restraint, banks overdosed on risky loans.\"\n\nTo counter the 2000 Stock Market Crash and subsequent economic slowdown, the Federal Reserve eased credit availability and drove interest rates down to lows not seen in many decades. These low interest rates facilitated the growth of debt at all levels of the economy, chief among them private debt to purchase more expensive housing. High levels of debt have long been recognized as a causative factor for recessions. Any debt default has the possibility of causing the lender to also default, if the lender is itself in a weak financial condition and has too much debt. This second default in turn can lead to still further defaults through a domino effect. The chances of these follow-up defaults is increased at high levels of debt. Attempts to prevent this domino effect by bailing out Wall Street lenders such as AIG, Fannie Mae, and Freddie Mac have had mixed success. The takeover is another example of attempts to stop the dominoes from falling.There was a real irony in the recent intervention by the Federal Reserve System to provide the money that enabled the firm of JPMorgan Chase to buy Bear Stearns before it went bankrupt. The point was to try to prevent a domino effect of panic in the financial markets that could lead to a downturn in the economy.\n\nExcessive consumer housing debt was in turn caused by the mortgage-backed security, credit default swap, and collateralized debt obligation sub-sectors of the finance industry, which were offering irrationally low interest rates and irrationally high levels of approval to subprime mortgage consumers because they were calculating aggregate risk using gaussian copula formulas that strictly assumed the independence of individual component mortgages, when in fact the credit-worthiness almost every new subprime mortgage was highly correlated with that of any other because of linkages through consumer spending levels which fell sharply when property values began to fall during the initial wave of mortgage defaults. Debt consumers were acting in their rational self-interest, because they were unable to audit the finance industry's opaque faulty risk pricing methodology.\n\nAccording to M.S. Eccles, who was appointed chairman of the Federal Reserve by FDR and held that position until 1948, excessive debt levels were not a source cause of the Great Depression. Increasing debt levels were caused by a concentration of wealth during the 1920s, causing the middle and poorer classes, which saw a relative and/or actual decrease in wealth, to go increasingly into debt in an attempt to maintain or improve their living standards. According to Eccles this concentration of wealth was the source cause of the Great Depression. The ever-increasing debt levels eventually became unpayable, and therefore unsustainable, leading to debt defaults and the financial panics of the 1930s. The concentration of wealth in the modern era parallels that of the 1920s and has had similar effects. Some of the causes of wealth concentration in the modern era are lower tax rates for the rich, such as Warren Buffett paying taxes at a lower rate than the people working for him, policies such as propping up the stock market, which benefit the stock owning rich more than the middle or poorer classes who own little or no stock, and bailouts which funnel tax money collected largely from the middle class to bail out large corporations largely owned by the rich.\n\nThe International Monetary Fund (IMF) reported in April 2012: \"Household debt soared in the years leading up to the Great Recession. In advanced economies, during the five years preceding 2007, the ratio of household debt to income rose by an average of 39 percentage points, to 138 percent. In Denmark, Iceland, Ireland, the Netherlands, and Norway, debt peaked at more than 200 percent of household income. A surge in household debt to historic highs also occurred in emerging economies such as Estonia, Hungary, Latvia, and Lithuania. The concurrent boom in both house prices and the stock market meant that household debt relative to assets held broadly stable, which masked households’ growing exposure to a sharp fall in asset prices. When house prices declined, ushering in the global financial crisis, many households saw their wealth shrink relative to their debt, and, with less income and more unemployment, found it harder to meet mortgage payments. By the end of 2011, real house prices had fallen from their peak by about 41% in Ireland, 29% in Iceland, 23% in Spain and the United States, and 21% in Denmark. Household defaults, underwater mortgages (where the loan balance exceeds the house value), foreclosures, and fire sales are now endemic to a number of economies. Household deleveraging by paying off debts or defaulting on them has begun in some countries. It has been most pronounced in the United States, where about two-thirds of the debt reduction reflects defaults.\n\nThis refers to homeowners borrowing and spending against the value of their homes, typically via a home equity loan or when selling the home. Free cash used by consumers from home equity extraction doubled from $627 billion in 2001 to $1,428 billion in 2005 as the housing bubble built, a total of nearly $5 trillion over the period, contributing to economic growth worldwide. U.S. home mortgage debt relative to GDP increased from an average of 46% during the 1990s to 73% during 2008, reaching $10.5 trillion.\n\nEconomist Tyler Cowen explained that the economy was highly dependent on this home equity extraction: \"In the 1993-1997 period, home owners extracted an amount of equity from their homes equivalent to 2.3% to 3.8% GDP. By 2005, this figure had increased to 11.5% GDP.\"\n\nSpeculative borrowing in residential real estate has been cited as a contributing factor to the subprime mortgage crisis. During 2006, 22% of homes purchased (1.65 million units) were for investment purposes, with an additional 14% (1.07 million units) purchased as vacation homes. During 2005, these figures were 28% and 12%, respectively. In other words, a record level of nearly 40% of homes purchases were not intended as primary residences. David Lereah, NAR's chief economist at the time, stated that the 2006 decline in investment buying was expected: \"Speculators left the market in 2006, which caused investment sales to fall much faster than the primary market.\"\n\nHousing prices nearly doubled between 2000 and 2006, a vastly different trend from the historical appreciation at roughly the rate of inflation. While homes had not traditionally been treated as investments subject to speculation, this behavior changed during the housing boom. Media widely reported condominiums being purchased while under construction, then being \"flipped\" (sold) for a profit without the seller ever having lived in them. Some mortgage companies identified risks inherent in this activity as early as 2005, after identifying investors assuming highly leveraged positions in multiple properties.\n\nOne 2017 NBER study argued that real estate investors (i.e., those owning 2+ homes) were more to blame for the crisis than subprime borrowers: \"The rise in mortgage defaults during the crisis was concentrated in the middle of the credit score distribution, and mostly attributable to real estate investors\" and that \"credit growth between 2001 and 2007 was concentrated in the prime segment, and debt to high-risk [subprime] borrowers was virtually constant for all debt categories during this period.\" The authors argued that this investor-driven narrative was more accurate than blaming the crisis on lower-income, subprime borrowers. A 2011 Fed study had a similar finding: \"In states that experienced the largest housing booms and busts, at the peak of the market almost half of purchase mortgage originations were associated with investors. In part by apparently misreporting their intentions to occupy the property, investors took on more leverage, contributing to higher rates of default.\" The Fed study reported that mortgage originations to investors rose from 25% in 2000 to 45% in 2006, for Arizona, California, Florida, and Nevada overall, where housing price increases during the bubble (and declines in the bust) were most pronounced. In these states, investor delinquency rose from around 15% in 2000 to over 35% in 2007 and 2008.\n\nNicole Gelinas of the Manhattan Institute described the negative consequences of not adjusting tax and mortgage policies to the shifting treatment of a home from conservative inflation hedge to speculative investment. Economist Robert Shiller argued that speculative bubbles are fueled by \"contagious optimism, seemingly impervious to facts, that often takes hold when prices are rising. Bubbles are primarily social phenomena; until we understand and address the psychology that fuels them, they're going to keep forming.\"\n\nMortgage risks were underestimated by every institution in the chain from originator to investor by underweighting the possibility of falling housing prices given historical trends of rising prices. Misplaced confidence in innovation and excessive optimism led to miscalculations by both public and private institutions.\n\nKeynesian economist Hyman Minsky described how speculative borrowing contributed to rising debt and an eventual collapse of asset values.\nEconomist Paul McCulley described how Minsky's hypothesis translates to the current crisis, using Minsky's words: \"...from time to time, capitalist economies exhibit inflations and debt deflations which seem to have the potential to spin out of control. In such processes, the economic system's reactions to a movement of the economy amplify the movement--inflation feeds upon inflation and debt-deflation feeds upon debt deflation.\" In other words, people are momentum investors by nature, not value investors. People naturally take actions that expand the apex and nadir of cycles. One implication for policymakers and regulators is the implementation of counter-cyclical policies, such as contingent capital requirements for banks that increase during boom periods and are reduced during busts.\n\nThe former CEO of Citigroup Charles O. Prince said in November 2007: \"As long as the music is playing, you've got to get up and dance.\" This metaphor summarized how financial institutions took advantage of easy credit conditions, by borrowing and investing large sums of money, a practice called leveraged lending. Debt taken on by financial institutions increased from 63.8% of U.S. gross domestic product in 1997 to 113.8% in 2007.\n\nA 2004 SEC decision related to the net capital rule allowed USA investment banks to issue substantially more debt, which was then used to help fund the housing bubble through purchases of mortgage-backed securities. The change in regulation left the capital adequacy requirement at the same level but added a \"risk weighting\" that lowered capital requirements on AAA rated bonds and tranches. This led to a shift from first loss tranches to highly rated less risky tranches and was seen as an improvement in risk management in the spirit of the European Basel accords.\n\nFrom 2004-07, the top five U.S. investment banks each significantly increased their financial leverage (see diagram), which increased their vulnerability to a financial shock. These five institutions reported over $4.1 trillion in debt for fiscal year 2007, about 30% of USA nominal GDP for 2007. Lehman Brothers was liquidated, Bear Stearns and Merrill Lynch were sold at fire-sale prices, and Goldman Sachs and Morgan Stanley became commercial banks, subjecting themselves to more stringent regulation. With the exception of Lehman, these companies required or received government support.\n\nFannie Mae and Freddie Mac, two U.S. government-sponsored enterprises, owned or guaranteed nearly $5 trillion in mortgage obligations at the time they were placed into conservatorship by the U.S. government in September 2008.\n\nThese seven entities were highly leveraged and had $9 trillion in debt or guarantee obligations, an enormous concentration of risk, yet were not subject to the same regulation as depository banks.\n\nIn a May 2008 speech, Ben Bernanke quoted Walter Bagehot: \"A good banker will have accumulated in ordinary times the reserve he is to make use of in extraordinary times.\" However, this advice was not heeded by these institutions, which had used the boom times to increase their leverage ratio instead.\n\nThe theory of laissez-faire capitalism suggests that financial institutions would be risk-averse because failure would result in liquidation. But the Federal Reserve's 1984 rescue of Continental Illinois and the 1998 rescue of the Long-Term Capital Management hedge fund, among others, showed that institutions which failed to exercise due diligence could reasonably expect to be protected from the consequences of their mistakes. The belief that they would not be allowed to fail created a moral hazard, which allegedly contributed to the late-2000s recession. (In \"The system\" Eduardo Galeano wrote, \"Bankruptcies are socialized, profits are privatized.\")\n\nHowever, even without the too big to fail syndrome, the short-term structure of compensation packages creates perverse incentives for executives to maximize the short-term performance of their companies at the expense of the long term. William K. Black developed the concept of control fraud to describe executives who pervert good business rules to transfer substantial wealth to themselves from shareholders and customers. Their companies may report phenomenal profits in the short term only to lose substantial amounts of money when their Ponzi schemes finally collapse. Some of the individuals Black described were prosecuted for fraud, but many are allowed to keep their wealth with little more than a public rebuke that seems to have little impact on their future. Eileen Foster was fired as a Vice President of Bank of America for trying too hard to inform her managers of systematic fraud in their home loans unit. Richard Bowen, chief underwriter of Citigroup's consumer division, was demoted with 218 of his 220 employees reassigned allegedly for attempting to inform several senior executives that over 80 percent of their mortgages violated Citigroup's own standards.\n\nIn its \"Declaration of the Summit on Financial Markets and the World Economy,\" dated 15 November 2008, leaders of the Group of 20 cited the following causes related to features of the modern financial markets:\nThe term financial innovation refers to the ongoing development of financial products designed to achieve particular client objectives, such as offsetting a particular risk exposure (such as the default of a borrower) or to assist with obtaining financing. Examples pertinent to this crisis included: the adjustable-rate mortgage; the bundling of subprime mortgages into mortgage-backed securities (MBS) or collateralized debt obligations (CDO) for sale to investors, a type of securitization; and a form of credit insurance called credit default swaps(CDS). The usage of these products expanded dramatically in the years leading up to the crisis. These products vary in complexity and the ease with which they can be valued on the books of financial institutions.\n\nThe CDO in particular enabled financial institutions to obtain investor funds to finance subprime and other lending, extending or increasing the housing bubble and generating large fees. Approximately $1.6 trillion in CDO's were originated between 2003-2007. A CDO essentially places cash payments from multiple mortgages or other debt obligations into a single pool, from which the cash is allocated to specific securities in a priority sequence. Those securities obtaining cash first received investment-grade ratings from rating agencies. Lower priority securities received cash thereafter, with lower credit ratings but theoretically a higher rate of return on the amount invested. A sample of 735 CDO deals originated between 1999 and 2007 showed that subprime and other less-than-prime mortgages represented an increasing percentage of CDO assets, rising from 5% in 2000 to 36% in 2007.\n\nFor a variety of reasons, market participants did not accurately measure the risk inherent with this innovation or understand its impact on the overall stability of the financial system. For example, the pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. The average recovery rate for \"high quality\" CDOs has been approximately 32 cents on the dollar, while the recovery rate for mezzanine CDO's has been approximately five cents for every dollar. These massive, practically unthinkable, losses have dramatically impacted the balance sheets of banks across the globe, leaving them with very little capital to continue operations.\n\nOthers have pointed out that there were not enough of these loans made to cause a crisis of this magnitude. In an article in Portfolio Magazine, Michael Lewis spoke with one trader who noted that \"There weren’t enough Americans with [bad] credit taking out [bad loans] to satisfy investors’ appetite for the end product.\" Essentially, investment banks and hedge funds used financial innovation to synthesize more loans using derivatives. \"They were creating [loans] out of whole cloth. One hundred times over! That’s why the losses are so much greater than the loans.\"\n\nPrinceton professor Harold James wrote that one of the byproducts of this innovation was that MBS and other financial assets were \"repackaged so thoroughly and resold so often that it became impossible to clearly connect the thing being traded to its underlying value.\" He called this a \"...profound flaw at the core of the U.S. financial system...\"\n\nAnother example relates to AIG, which insured obligations of various financial institutions through the usage of credit default swaps. The basic CDS transaction involved AIG receiving a premium in exchange for a promise to pay money to party A in the event party B defaulted. However, AIG did not have the financial strength to support its many CDS commitments as the crisis progressed and was taken over by the government in September 2008. U.S. taxpayers provided over $180 billion in government support to AIG during 2008 and early 2009, through which the money flowed to various counterparties to CDS transactions, including many large global financial institutions.\n\nAuthor Michael Lewis wrote that CDS enabled speculators to stack bets on the same mortgage bonds and CDO's. This is analogous to allowing many persons to buy insurance on the same house. Speculators that bought CDS insurance were betting that significant defaults would occur, while the sellers (such as AIG) bet they would not. In addition, Chicago Public Radio and the \"Huffington Post\" reported in April 2010 that market participants, including a hedge fund called Magnetar Capital, encouraged the creation of CDO's containing low quality mortgages, so they could bet against them using CDS. NPR reported that Magnetar encouraged investors to purchase CDO's while simultaneously betting against them, without disclosing the latter bet.\n\nCredit rating agencies are under scrutiny for having given investment-grade ratings to MBSs based on risky subprime mortgage loans. These high ratings enabled these MBS to be sold to investors, thereby financing the housing boom. These ratings were believed justified because of risk reducing practices, such as credit default insurance and equity investors willing to bear the first losses. However, there are also indications that some involved in rating subprime-related securities knew at the time that the rating process was faulty.\n\nAn estimated $3.2 trillion in loans were made to homeowners with bad credit and undocumented incomes (e.g., subprime or Alt-A mortgages) between 2002 and 2007. Economist Joseph Stiglitz stated: \"I view the rating agencies as one of the key culprits...They were the party that performed the alchemy that converted the securities from F-rated to A-rated. The banks could not have done what they did without the complicity of the rating agencies.\" Without the AAA ratings, demand for these securities would have been considerably less. Bank writedowns and losses on these investments totaled $523 billion as of September 2008.\n\nThe ratings of these securities was a lucrative business for the rating agencies, accounting for just under half of Moody's total ratings revenue in 2007. Through 2007, ratings companies enjoyed record revenue, profits and share prices. The rating companies earned as much as three times more for grading these complex products than corporate bonds, their traditional business. Rating agencies also competed with each other to rate particular MBS and CDO securities issued by investment banks, which critics argued contributed to lower rating standards. Interviews with rating agency senior managers indicate the competitive pressure to rate the CDO's favorably was strong within the firms. This rating business was their \"golden goose\" (which laid the proverbial golden egg or wealth) in the words of one manager. Author Upton Sinclair (1878–1968) famously stated: \"It is difficult to get a man to understand something when his job depends on not understanding it.\" From 2000-2006, structured finance (which includes CDO's) accounted for 40% of the revenues of the credit rating agencies. During that time, one major rating agency had its stock increase six-fold and its earnings grew by 900%.\n\nCritics allege that the rating agencies suffered from conflicts of interest, as they were paid by investment banks and other firms that organize and sell structured securities to investors. On 11 June 2008, the SEC proposed rules designed to mitigate perceived conflicts of interest between rating agencies and issuers of structured securities. On 3 December 2008, the SEC approved measures to strengthen oversight of credit rating agencies, following a ten-month investigation that found \"significant weaknesses in ratings practices,\" including conflicts of interest.\n\nBetween Q3 2007 and Q2 2008, rating agencies lowered the credit ratings on $1.9 trillion in mortgage-backed securities. Financial institutions felt they had to lower the value of their MBS and acquire additional capital so as to maintain capital ratios. If this involved the sale of new shares of stock, the value of the existing shares was reduced. Thus ratings downgrades lowered the stock prices of many financial firms.\n\nThe limitations of many, widely used financial models also were not properly understood (see for example ). Li's Gaussian copula formula assumed that the price of CDS was correlated with and could predict the correct price of mortgage backed securities. Because it was highly tractable, it rapidly came to be used by a huge percentage of CDO and CDS investors, issuers, and rating agencies. According to one wired.com article: \"Then the model fell apart. Cracks started appearing early on, when financial markets began behaving in ways that users of Li's formula hadn't expected. The cracks became full-fledged canyons in 2008—when ruptures in the financial system's foundation swallowed up trillions of dollars and put the survival of the global banking system in serious peril... Li's Gaussian copula formula will go down in history as instrumental in causing the unfathomable losses that brought the world financial system to its knees.\"\n\nAs financial assets became more complex, less transparent, and harder and harder to value, investors were reassured by the fact that both international bond rating agencies and bank regulators, who came to rely on them, accepted as valid some complex mathematical models that theoretically showed the risks were much smaller than they turned out to be. George Soros commented that \"The super-boom got out of hand when the new products became so complicated that the authorities could no longer calculate the risks and started relying on the risk management methods of the banks themselves. Similarly, the rating agencies relied on the information provided by the originators of synthetic products. It was a shocking abdication of responsibility.\" \n\nComplex financing structures called structured investment vehicles (SIV) or conduits enabled banks to move significant amounts of assets and liabilities, including unsold CDO's, off their books. This had the effect of helping the banks maintain regulatory minimum capital ratios. They were then able to lend anew, earning additional fees. Author Robin Blackburn explained how they worked: Off balance sheet financing also made firms look less leveraged and enabled them to borrow at cheaper rates.\n\nBanks had established automatic lines of credit to these SIV and conduits. When the cash flow into the SIV's began to decline as subprime defaults mounted, banks were contractually obligated to provide cash to these structures and their investors. This \"conduit-related balance sheet pressure\" placed strain on the banks' ability to lend, both raising interbank lending rates and reducing the availability of funds.\n\nIn the years leading up to the crisis, the top four U.S. depository banks moved an estimated $5.2 trillion in assets and liabilities off-balance sheet into these SIV's and conduits. This enabled them to essentially bypass existing regulations regarding minimum capital ratios, thereby increasing leverage and profits during the boom but increasing losses during the crisis. Accounting guidance was changed in 2009 that will require them to put some of these assets back onto their books, which significantly reduces their capital ratios. One news agency estimated this amount at between $500 billion and $1 trillion. This effect was considered as part of the stress tests performed by the government during 2009.\n\nDuring March 2010, the bankruptcy court examiner released a report on Lehman Brothers, which had failed spectacularly in September 2008. The report indicated that up to $50 billion was moved off-balance sheet in a questionable manner by management during 2008, with the effect of making its debt level (leverage ratio) appear smaller. Analysis by the Federal Reserve Bank of New York indicated big banks mask their risk levels just prior to reporting data quarterly to the public.\n\nCertain financial innovation may also have the effect of circumventing regulations, such as off-balance sheet financing that affects the leverage or capital cushion reported by major banks. For example, Martin Wolf wrote in June 2009: \"...an enormous part of what banks did in the early part of this decade – the off-balance-sheet vehicles, the derivatives and the 'shadow banking system' itself – was to find a way round regulation.\"\n\nNiall Ferguson wrote that the financial sector became increasingly concentrated in the years leading up to the crisis, which made the stability of the financial system more reliant on just a few firms, which were also highly leveraged:\nBy contrast, some scholars have argued that fragmentation in the mortgage securitization market led to increased risk taking and a deterioration in underwriting standards.\n\nThe Shadow banking system grew to exceed the size of the depository system, but was not subject to the same requirements and protections. Nobel laureate Paul Krugman described the run on the shadow banking system as the \"core of what happened\" to cause the crisis. \"As the shadow banking system expanded to rival or even surpass conventional banking in importance, politicians and government officials should have realized that they were re-creating the kind of financial vulnerability that made the Great Depression possible – and they should have responded by extending regulations and the financial safety net to cover these new institutions. Influential figures should have proclaimed a simple rule: anything that does what a bank does, anything that has to be rescued in crises the way banks are, should be regulated like a bank.\" He referred to this lack of controls as \"malign neglect.\"\n\nCritics of government policy argued that government lending programs were the main cause of the crisis. The Financial Crisis Inquiry Commission (report of the Democratic party majority) stated that Fannie Mae and Freddie Mac, government affordable housing policies, and the Community Reinvestment Act were not primary causes of the crisis. The Republican members of the commission disagreed.\n\nIn 1992, the Democratic-controlled 102nd Congress under the George H. W. Bush administration weakened regulation of Fannie Mae and Freddie Mac with the goal of making available more money for the issuance of home loans. The Washington Post wrote: \"Congress also wanted to free up money for Fannie Mae and Freddie Mac to buy mortgage loans and specified that the pair would be required to keep a much smaller share of their funds on hand than other financial institutions. Whereas banks that held $100 could spend $90 buying mortgage loans, Fannie Mae and Freddie Mac could spend $97.50 buying loans. Finally, Congress ordered that the companies be required to keep more capital as a cushion against losses if they invested in riskier securities. But the rule was never set during the Clinton administration, which came to office that winter, and was only put in place nine years later.\"\n\nSome economists have pointed to deregulation efforts as contributing to the collapse. In 1999, the Republican controlled 106th Congress U.S. Congress under the Clinton administration passed the Gramm-Leach-Bliley Act, which repealed part of the Glass–Steagall Act of 1933. This repeal has been criticized by some for having contributed to the proliferation of the complex and opaque financial instruments at the heart of the crisis. However, some economists object to singling out the repeal of Glass–Steagall for criticism. Brad DeLong, a former advisor to President Clinton and economist at the University of California, Berkeley and Tyler Cowen of George Mason University have both argued that the Gramm-Leach-Bliley Act softened the impact of the crisis by allowing for mergers and acquisitions of collapsing banks as the crisis unfolded in late 2008.\n\nTwo important factors that contributed to the United States housing bubble were low U.S. interest rates and a large U.S. trade deficit. Low interest rates made bank lending more profitable, while trade deficits resulted in large capital inflows to the U.S. Both made funds for borrowing plentiful and relatively inexpensive.\n\nFrom 2000 to 2003, the Federal Reserve lowered the federal funds rate target from 6.5% to 1.0%. This was done to soften the effects of the collapse of the dot-com bubble and of the September 2001 terrorist attacks, and to combat the perceived risk of deflation.\nThe Fed then raised the Fed funds rate significantly between July 2004 and July 2006. This contributed to an increase in 1-year and 5-year adjustable-rate mortgage (ARM) rates, making ARM interest rate resets more expensive for homeowners. This may have also contributed to the deflating of the housing bubble, as asset prices generally move inversely to interest rates and it became riskier to speculate in housing.\n\nGlobalization and trade imbalances contributed to enormous inflows of money into the U.S. from high savings countries, fueling debt-driven consumption and the housing bubble. The ratio of household debt to disposable income rose from 77% in 1990 to 127% by 2007. The steady entry into the world economy of new export-oriented economies began with Japan and the Asian tigers in the 1980s and peaked with China in the early 2000s, representing more than two billion newly employable workers. The integration of these high-savings, lower wage economies into the global economy, combined with dramatic productivity gains made possible by new information technologies and the globalization of corporate supply chains, decisively shifted the balance of global supply and demand. By 2000, the world economy was beset by excess supplies of labor, capital, and productive capacity relative to global demand. But the collapse of the consumer credit and housing price bubbles brought an end to this pattern of debt-financed economic growth and left the U.S. with the massive debt overhang.\n\nThis globalization can be measured in growing trade deficits in developed countries such as the U.S. and Europe. In 2005, Ben Bernanke addressed the implications of the USA's high and rising current account deficit, resulting from USA imports exceeding its exports, which was itself caused by a global saving glut. Between 1996 and 2004, the USA current account deficit increased by $650 billion, from 1.5% to 5.8% of GDP. Financing these deficits required the USA to borrow large sums from abroad, much of it from countries running trade surpluses, mainly the emerging economies in Asia and oil-exporting nations. The balance of payments identity requires that a country (such as the USA) running a current account deficit also have a capital account (investment) surplus of the same amount. Hence large and growing amounts of foreign funds (capital) flowed into the USA to finance its imports. This created demand for various types of financial assets, raising the prices of those assets while lowering interest rates. Foreign investors had these funds to lend, either because they had very high personal savings rates (as high as 40% in China), or because of high oil prices. Bernanke referred to this as a \"saving glut.\" A \"flood\" of funds (capital or liquidity) reached the USA financial markets. Foreign governments supplied funds by purchasing USA Treasury bonds and thus avoided much of the direct impact of the crisis. USA households, on the other hand, used funds borrowed from foreigners to finance consumption or to bid up the prices of housing and financial assets. Financial institutions invested foreign funds in mortgage-backed securities. USA housing and financial assets dramatically declined in value after the housing bubble burst.\n\nMartin Wolf has argued that \"inordinately mercantilist currency policies\" were a significant cause of the U.S. trade deficit, indirectly driving a flood of money into the U.S. as described above. In his view, China maintained an artificially weak currency to make Chinese goods relatively cheaper for foreign countries to purchase, thereby keeping its vast workforce occupied and encouraging exports to the U.S. One byproduct was a large accumulation of U.S. dollars by the Chinese government, which were then invested in U.S. government securities and those of Fannie Mae and Freddie Mac, providing additional funds for lending that contributed to the housing bubble.\n\nEconomist Paul Krugman also wrote similar comments during October 2009, further arguing that China's currency should have appreciated relative to the U.S. dollar beginning around 2001. Various U.S. officials have also indicated concerns with Chinese exchange rate policies, which have not allowed its currency to appreciate significantly relative to the dollar despite large trade surpluses. In January 2009, Timothy Geithner wrote: \"Obama -- backed by the conclusions of a broad range of economists -- believes that China is manipulating its currency...the question is how and when to broach the subject in order to do more good than harm.\"\n\nThe cause of the crisis can be seen also in principles of technological development and in long economic waves based on technological revolutions. Daniel Šmihula believes that this crisis and stagnation are a result of the end of the long economic cycle originally initiated by the Information and telecommunications technological revolution in 1985-2000.\nThe market has been already saturated by new \"technical wonders\" (e.g. everybody has his own mobile phone) and – what is more important - in the developed countries the economy reached limits of productivity in conditions of existing technologies. A new economic revival can come only with a new technological revolution (a hypothetical Post-informational technological revolution). Šmihula expects that it will happen in about 2014-15.\n\nBehavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. This is called the paradox of thrift. Economist Hyman Minsky also described a \"paradox of deleveraging\" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.\n\nDuring April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: \"Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole.\"\n\nIn a Peabody Award winning program, NPR correspondents argued that a \"Giant Pool of Money\" (represented by $70 trillion in worldwide fixed income investments) sought higher yields than those offered by U.S. Treasury bonds early in the decade, which were low due to low interest rates and trade deficits discussed above. Further, this pool of money had roughly doubled in size from 2000 to 2007, yet the supply of relatively safe, income generating investments had not grown as fast. Investment banks on Wall Street answered this demand with the mortgage-backed security (MBS) and collateralized debt obligation (CDO), which were assigned safe ratings by the credit rating agencies. In effect, Wall Street connected this pool of money to the mortgage market in the U.S., with enormous fees accruing to those throughout the mortgage supply chain, from the mortgage broker selling the loans, to small banks that funded the brokers, to the giant investment banks behind them. By approximately 2003, the supply of mortgages originated at traditional lending standards had been exhausted. However, continued strong demand for MBS and CDO began to drive down lending standards, as long as mortgages could still be sold along the supply chain. Eventually, this speculative bubble proved unsustainable.\n\nIn a June 2008 speech, U.S. Treasury Secretary Timothy Geithner, then President and CEO of the NY Federal Reserve Bank, placed significant blame for the freezing of credit markets on a \"run\" on the entities in the \"parallel\" banking system, also called the shadow banking system. These entities became critical to the credit markets underpinning the financial system, but were not subject to the same regulatory controls. Further, these entities were vulnerable because they borrowed short-term in liquid markets to purchase long-term, illiquid and risky assets. This meant that disruptions in credit markets would make them subject to rapid deleveraging, selling their long-term assets at depressed prices. He described the significance of these entities: \"In early 2007, asset-backed commercial paper conduits, in structured investment vehicles, in auction-rate preferred securities, tender option bonds and variable rate demand notes, had a combined asset size of roughly $2.2 trillion. Assets financed overnight in triparty repo grew to $2.5 trillion. Assets held in hedge funds grew to roughly $1.8 trillion. The combined balance sheets of the then five major investment banks totaled $4 trillion. In comparison, the total assets of the top five bank holding companies in the United States at that point were just over $6 trillion, and total assets of the entire banking system were about $10 trillion.\" He stated that the \"combined effect of these factors was a financial system vulnerable to self-reinforcing asset price and credit cycles.\"\n\nNobel laureate and liberal political columnist Paul Krugman described the run on the shadow banking system as the \"core of what happened\" to cause the crisis. \"As the shadow banking system expanded to rival or even surpass conventional banking in importance, politicians and government officials should have realized that they were re-creating the kind of financial vulnerability that made the Great Depression possible—and they should have responded by extending regulations and the financial safety net to cover these new institutions. Influential figures should have proclaimed a simple rule: anything that does what a bank does, anything that has to be rescued in crises the way banks are, should be regulated like a bank.\" He referred to this lack of controls as \"malign neglect.\" Some researchers have suggested that competition between GSEs and the shadow banking system led to a deterioration in underwriting standards.\n\nFor example, investment bank Bear Stearns was required to replenish much of its funding in overnight markets, making the firm vulnerable to credit market disruptions. When concerns arose regarding its financial strength, its ability to secure funds in these short-term markets was compromised, leading to the equivalent of a bank run. Over four days, its available cash declined from $18 billion to $3 billion as investors pulled funding from the firm. It collapsed and was sold at a fire-sale price to bank JP Morgan Chase March 16, 2008.\n\nAmerican homeowners, consumers, and corporations owed roughly $25 trillion during 2008. American banks retained about $8 trillion of that total directly as traditional mortgage loans. Bondholders and other traditional lenders provided another $7 trillion. The remaining $10 trillion came from the securitization markets, meaning the parallel banking system. The securitization markets started to close down in the spring of 2007 and nearly shut-down in the fall of 2008. More than a third of the private credit markets thus became unavailable as a source of funds. In February 2009, Ben Bernanke stated that securitization markets remained effectively shut, with the exception of conforming mortgages, which could be sold to Fannie Mae and Freddie Mac.\n\n\"The Economist\" reported in March 2010: \"Bear Stearns and Lehman Brothers were non-banks that were crippled by a silent run among panicky overnight \"repo\" lenders, many of them money market funds uncertain about the quality of securitized collateral they were holding. Mass redemptions from these funds after Lehman's failure froze short-term funding for big firms.\"\n\nDuring the boom period, enormous fees were paid to those throughout the mortgage supply chain, from the mortgage broker selling the loans, to small banks that funded the brokers, to the giant investment banks behind them. Those originating loans were paid fees for selling them, regardless of how the loans performed. Default or credit risk was passed from mortgage originators to investors using various types of financial innovation. This became known as the \"originate to distribute\" model, as opposed to the traditional model where the bank originating the mortgage retained the credit risk. In effect, the mortgage originators were left with nothing at risk, giving rise to a moral hazard that separated behavior and consequence.\n\nEconomist Mark Zandi described moral hazard as a root cause of the subprime mortgage crisis. He wrote: \"...the risks inherent in mortgage lending became so widely dispersed that no one was forced to worry about the quality of any single loan. As shaky mortgages were combined, diluting any problems into a larger pool, the incentive for responsibility was undermined.\" He also wrote: \"Finance companies weren't subject to the same regulatory oversight as banks. Taxpayers weren't on the hook if they went belly up [pre-crisis], only their shareholders and other creditors were. Finance companies thus had little to discourage them from growing as aggressively as possible, even if that meant lowering or winking at traditional lending standards.\"\n\nThe New York State Comptroller's Office has said that in 2006, Wall Street executives took home bonuses totaling $23.9 billion. \"Wall Street traders were thinking of the bonus at the end of the year, not the long-term health of their firm. The whole system—from mortgage brokers to Wall Street risk managers—seemed tilted toward taking short-term risks while ignoring long-term obligations. The most damning evidence is that most of the people at the top of the banks didn't really understand how those [investments] worked.\"\n\nInvestment banker incentive compensation was focused on fees generated from assembling financial products, rather than the performance of those products and profits generated over time. Their bonuses were heavily skewed towards cash rather than stock and not subject to \"claw-back\" (recovery of the bonus from the employee by the firm) in the event the MBS or CDO created did not perform. In addition, the increased risk (in the form of financial leverage) taken by the major investment banks was not adequately factored into the compensation of senior executives.\n\nBank CEO Jamie Dimon argued: \"Rewards have to track real, sustained, risk-adjusted performance. Golden parachutes, special contracts, and unreasonable perks must disappear. There must be a relentless focus on risk management that starts at the top of the organization and permeates down to the entire firm. This should be business-as-usual, but at too many places, it wasn't.\"\n\nCritics have argued that the regulatory framework did not keep pace with financial innovation, such as the increasing importance of the shadow banking system, derivatives and off-balance sheet financing. In other cases, laws were changed or enforcement weakened in parts of the financial system. Several critics have argued that the most critical role for regulation is to make sure that financial institutions have the ability or capital to deliver on their commitments. Critics have also noted de facto deregulation through a shift in market share toward the least regulated portions of the mortgage market.\n\nKey examples of regulatory failures include:\n\n\nAuthor Roger Lowenstein summarized some of the regulatory problems that caused the crisis in November 2009:\n\n\"1) Mortgage regulation was too lax and in some cases nonexistent; 2) Capital requirements for banks were too low; 3) Trading in derivatives such as credit default swaps posed giant, unseen risks; 4) Credit ratings on structured securities such as collateralized-debt obligations were deeply flawed; 5) Bankers were moved to take on risk by excessive pay packages; 6) The government’s response to the crash also created, or exacerbated, moral hazard. Markets now expect that big banks won’t be allowed to fail, weakening the incentives of investors to discipline big banks and keep them from piling up too many risky assets again.\"\n\nA 2011 documentary film, argues that deregulation led to the crisis, and is geared towards a general audience.\n\nA variety of conflicts of interest have been argued as contributing to this crisis:\n\nBanks in the U.S. lobby politicians extensively. A November 2009 report from economists of the International Monetary Fund (IMF) writing independently of that organization indicated that:\n\nThe study concluded that: \"the prevention of future crises might require weakening political influence of the financial industry or closer monitoring of lobbying activities to understand better the incentives behind it.\"\n\nThe Boston Globe reported during that during January–June 2009, the largest four U.S. banks spent these amounts ($ millions) on lobbying, despite receiving taxpayer bailouts: Citigroup $3.1; JP Morgan Chase $3.1; Bank of America $1.5; and Wells Fargo $1.4.\n\nThe \"New York Times\" reported in April 2010: \"An analysis by Public Citizen found that at least 70 former members of Congress were lobbying for Wall Street and the financial services sector last year, including two former Senate majority leaders (Trent Lott and Bob Dole), two former House majority leaders (Richard A. Gephardt and Dick Armey) and a former House speaker (J. Dennis Hastert). In addition to the lawmakers, data from the Center for Responsive Politics counted 56 former Congressional aides on the Senate or House banking committees who went on to use their expertise to lobby for the financial sector.\"\n\nThe Financial Crisis Inquiry Commission reported in January 2011 that \"...from 1998 to 2008, the financial sector expended $2.7 billion in reported federal lobbying expenses; individuals and political action committees in the sector made more than $1 billion in campaign contributions.\"\n\nA 2012 book by Hedrick Smith, \"Who Stole the American Dream?\", suggests that the Powell Memo was instrumental in setting a new political direction for US business leaders that led to \"America’s contemporary economic malaise.\"\n\nA commodity price bubble was created following the collapse in the housing bubble. The price of oil nearly tripled from $50 to $140 from early 2007 to 2008, before plunging as the financial crisis began to take hold in late 2008. Experts debate the causes, which include the flow of money from housing and other investments into commodities to speculation and monetary policy. An increase in oil prices tends to divert a larger share of consumer spending into gasoline, which creates downward pressure on economic growth in oil importing countries, as wealth flows to oil-producing states. Spiking instability in the price of oil over the decade leading up to the price high of 2008 has also been proposed as a causal factor in the financial crisis.\n\nA cover story in \"BusinessWeek\" magazine claims that economists mostly failed to predict the worst international economic crisis since the Great Depression of the 1930s. The Wharton School of the University of Pennsylvania online business journal examines why economists failed to predict a major global financial crisis. An article in the New York Times informs that economist Nouriel Roubini warned of such crisis as early as September 2006, and the article goes on to state that the profession of economics is bad at predicting recessions. According to The Guardian, Roubini was ridiculed for predicting a collapse of the housing market and worldwide recession, while The New York Times labelled him \"Dr. Doom\". However, there are examples of other experts who gave indications of a financial crisis.\n\nThe failure to forecast the \"Great Recession\" has caused a lot of soul searching in the economics profession. The Queen of England herself asked why had nobody noticed that the credit crunch was on its way, and a group of economists—experts from business, the City, its regulators, academia, and government—tried to explain in a letter.\n\nAnother probable cause of the crisis—and a factor that unquestionably amplified its magnitude—was widespread miscalculation by banks and investors of the level of risk inherent in the unregulated Collateralized debt obligation and credit default swap markets. Under this theory, banks and investors systematized the risk by taking advantage of low interest rates to borrow tremendous sums of money that they could only pay back if the housing market continued to increase in value.\n\nAccording to an article published in \"Wired\", the risk was further systematized by the use of David X. Li's Gaussian copula model function to rapidly price Collateralized debt obligations based on the price of related credit default swaps. Because it was highly tractable, it rapidly came to be used by a huge percentage of CDO and CDS investors, issuers, and rating agencies. According to one wired.com article: \"Then the model fell apart. Cracks started appearing early on, when financial markets began behaving in ways that users of Li's formula hadn't expected. The cracks became full-fledged canyons in 2008—when ruptures in the financial system's foundation swallowed up trillions of dollars and put the survival of the global banking system in serious peril...Li's Gaussian copula formula will go down in history as instrumental in causing the unfathomable losses that brought the world financial system to its knees.\"\n\nThe pricing model for CDOs clearly did not reflect the level of risk they introduced into the system. It has been estimated that the \"from late 2005 to the middle of 2007, around $450bn of CDO of ABS were issued, of which about one third were created from risky mortgage-backed bonds...[o]ut of that pile, around $305bn of the CDOs are now in a formal state of default, with the CDOs underwritten by Merrill Lynch accounting for the biggest pile of defaulted assets, followed by UBS and Citi.\" The average recovery rate for high quality CDOs has been approximately 32 cents on the dollar, while the recovery rate for mezzanine CDO's has been approximately five cents for every dollar. These massive, practically unthinkable, losses have dramatically impacted the balance sheets of banks across the globe, leaving them with very little capital to continue operations.\n\nAustrian economics argue that the crisis is consistent with the Austrian Business Cycle Theory, in which credit created through the policies of central banking gives rise to an artificial boom, which is inevitably followed by a bust. This perspective argues that the monetary policy of central banks creates excessive quantities of cheap credit by setting interest rates below where they would be set by a free market. This easy availability of credit inspires a bundle of malinvestments, particularly on long term projects such as housing and capital assets, and also spurs a consumption boom as incentives to save are diminished. Thus an unsustainable boom arises, characterized by malinvestments and overconsumption.\n\nBut the created credit is not backed by any real savings nor is in response to any change in the real economy, hence, there are physically not enough resources to finance either the malinvestments or the consumption rate indefinitely. The bust occurs when investors collectively realize their mistake. This happens usually some time after interest rates rise again. The liquidation of the malinvestments and the consequent reduction in consumption throw the economy into a recession, whose severity mirrors the scale of the boom's excesses.\n\nAustrian economists argue that the conditions previous to the crisis of the late 2000s correspond to the scenario described above. The central bank of the United States, led by Federal Reserve Chairman Alan Greenspan, kept interest rates very low for a long period of time to blunt the recession of the early 2000s. The resulting malinvestment and over-consumption of investors and consumers prompted the development of a housing bubble that ultimately burst, precipitating the financial crisis. The resulting devaluation of investors' share portfolio reduced consumption even further and aggravated the consequences of the bursting of the bubble, since in the post-Bretton Woods finance-led model of capitalism the stock market had transformed from a mechanism that used to finance the supply side of the economy into a mechanism that financed the consumption side; thus, consumers' purchasing power evaporated together with the value of their stock portfolio. This crisis, together with sudden and necessary deleveraging and cutbacks by consumers, businesses and banks, led to the recession. Austrian Economists argue further that while they probably affected the nature and severity of the crisis, factors such as a lack of regulation, the Community Reinvestment Act, and entities such as Fannie Mae and Freddie Mac are insufficient by themselves to explain it.\n\nA positively sloped yield curve allows Primary Dealers (such as large investment banks) in the Federal Reserve system to fund themselves with cheap short term money while lending out at higher long-term rates. This strategy is profitable so long as the yield curve remains positively sloped. However, it creates a liquidity risk if the yield curve were to become inverted and banks would have to refund themselves at expensive short term rates while losing money on longer term loans.\n\nThe narrowing of the yield curve from 2004 and the inversion of the yield curve during 2007 resulted (with the expected 1 to 3-year delay) in a bursting of the housing bubble and a wild gyration of commodities prices as moneys flowed out of assets like housing or stocks and sought safe haven in commodities. The price of oil rose to over $140 per barrel in 2008 before plunging as the financial crisis began to take hold in late 2008.\n\nOther observers have doubted the role that the yield curve plays in controlling the business cycle. In a May 24, 2006 story CNN Money reported: \"...in recent comments, Fed Chairman Ben Bernanke repeated the view expressed by his predecessor Alan Greenspan that an inverted yield curve is no longer a good indicator of a recession ahead.\"\n\nAn empirical study by John B. Taylor concluded that the crisis was: (1) caused by excess monetary expansion; (2) prolonged by an inability to evaluate counter-party risk due to opaque financial statements; and (3) worsened by the unpredictable nature of government's response to the crisis.\n\nEconomist James D. Hamilton has argued that the increase in oil prices in the period of 2007 through 2008 was a significant cause of the recession. He evaluated several different approaches to estimating the impact of oil price shocks on the economy, including some methods that had previously shown a decline in the relationship between oil price shocks and the overall economy. All of these methods \"support a common conclusion; had there been no increase in oil prices between 2007:Q3 and 2008:Q2, the US economy would not have been in a recession over the period 2007:Q4 through 2008:Q3.\" Hamilton's own model, a time-series econometric forecast based on data up to 2003, showed that the decline in GDP could have been successfully predicted to almost its full extent given knowledge of the price of oil. The results imply that oil prices were entirely responsible for the recession. Hamilton acknowledged that this was probably not the entire cause but maintained that it showed that oil price increases made a significant contribution to the downturn in economic growth.\n\nIt has also been debated that the root cause of the crisis is overproduction of goods caused by globalization (and especially vast investments in countries such as China and India by western multinational companies over the past 15–20 years, which greatly increased global industrial output at a reduced cost). Overproduction tends to cause deflation and signs of deflation were evident in October and November 2008, as commodity prices tumbled and the Federal Reserve was lowering its target rate to an all-time-low 0.25%. On the other hand, ecological economist Herman Daly suggests that it is not actually an economic crisis, but rather a crisis of . This reflects a claim made in the 1972 book \"Limits to Growth\", which stated that without major deviation from the policies followed in the 20th century, a permanent end of economic growth could be reached sometime in the first two decades of the 21st century, due to gradual depletion of natural resources.\n\n\n"}
{"id": "440276", "url": "https://en.wikipedia.org/wiki?curid=440276", "title": "Chiyou", "text": "Chiyou\n\nChiyou () was a tribal leader of the Nine Li tribe (九黎) in ancient China. He is best known as the tyrant who lost against the future Yellow Emperor during the Three Sovereigns and Five Emperors era in Chinese mythology. For the Hmong people, Chiyou was a sagacious mythical king. He has a particularly complex and controversial ancestry, as he may fall under Dongyi Miao or even Man, depending on the source and view. Today, Chiyou is honored and worshipped as the God of War and one of the three legendary founding fathers of China.\n\nAccording to the Song dynasty history book \"Lushi\", Chiyou's surname was Jiang (姜), and he was a descendant of Yandi.\n\nAccording to legend, Chiyou had a bronze head with a metal forehead. He had 4 eyes and 6 arms, wielding terrible sharp weapons in every hand. In some sources, Chiyou had certain features associated with : his head was that of a bull with two horns, although the body was that of a human. He is said to have been unbelievably fierce, and to have had 81 brothers. Historical sources often described him as 'cruel and greedy', as well as 'tyrannical'. Some sources have asserted that the figure 81 should rather be associated with 81 clans in his kingdom. Chiyou knows the constellations and the ancients spells for calling upon the weather. For example, he called upon a fog to surround Huangdi and his soldiers during the Battle of Zhuolu.\n\nChiyou is regarded as a leader of the Nine Li tribe (九黎, RPA ) by nearly all sources. However, his exact ethnic affiliations are quite complex, with multiple sources reporting him as belonging to various tribes, in addition to a number of diverse peoples supposed to have directly descended from him.\n\nSome sources from later dynasties, such as the \"Guoyu book\", considered Chiyou's Li tribe to be related to the ancient San miao tribe (三苗).\nIn the ancient Zhuolu Town is a statue of Chiyou commemorating him as the original ancestor of the Hmong people. The place is regarded as the birthplace of the San miao / Miao people, the Hmong being a subgroup of the Miao. In sources following the Hmong view, the \"nine Li\" tribe is called the \"Jiuli\" kingdom, Jiuli meaning \"nine Li\". Modern Han Chinese scholar Weng Dujian considers Jiuli and San Miao to be Man southerners. Chiyou has also been counted as part of the Dongyi.\n\nWhen the Yan emperor was leading his tribe and conflicts with Nine Li tribes led by Chiyou, the Yan emperor stood no chance and lost the fight. He escaped, and later ended up in Zhuolu begging for help from the Yellow Emperor. At this point the epic battle between Chiyou and the Yellow Emperor's forces began. The battle last for 10 years with Chiyou having the upper hand. \nDuring the Battle of Zhuolu, Chiyou breathed out a thick fog and obscured the sunlight. The battle dragged on for days while the emperor's side was in danger. Only after the Yellow Emperor invented the south-pointing chariot, did he find his way out of the battlefield. Chiyou then conjured up a heavy storm. The Yellow Emperor then called upon the drought demon Nüba (女魃), who blew away the storm clouds and cleared the battlefield. Chiyou and his army could not hold up, and were later killed by the Yellow Emperor. After this defeat, the Yellow Emperor is said to become the ancestor of all Huaxia Chinese. The Hmong were forced to live in the mountains and leave their Li kingdom. After Chiyou's death, it is said that it rained blood for some time.\n\nAccording to the \"Records of the Grand Historian\", Qin Shi Huang worshiped Chiyou as the God of War, and Liu Bang worshiped at Chiyou's shrine before his decisive battle against Xiang Yu. The mythical title God of War was given to Chiyou because the Yellow Emperor and Yan Emperor could not defeat Chiyou alone. Altogether, Chiyou won 9 major battles including 80 minor confrontations. On the 10th and final war, both emperors combined their forces and conquered Chiyou.\n\nIn one mythical episode, after Chiyou had claimed he could not be conquered, the goddess Nuwa dropped a stone tablet on him from Mount Tai. Chiyou failed to crush the stone, but still managed to escape. From then on, the 5-finger-shaped stone tablet, inscribed \"Mount Tai \"shigandang\"\" (泰山石敢當) became a spiritual weapon to ward off evil and disasters.\n\nAccording to notes by the Qing Dynasty painter Luo Ping: \"Yellow Emperor ordered his men to have Chiyou beheaded... seeing that Chiyou's head was separated from his body, later sages had his image engraved on sacrificial vessels as a warning to those that would covet power and wealth.\"\n\nThe Tale of Heike mentions a comet \"of the type called Chiyou's Banner or Red Breath.\"\n\nAccording to the controversial Korean history book \"Hwandan Gogi\", compiled by Uncho Gye Yeon-su in 1911, and later published in 1979, Chiyou was also an ancestor of the Koreans. He is listed there as the 14th (out of 18) head of the State of Shinshi (or 'Baedal'), with the Korean form of his name, \"Jaoji Hwanung of Baedal\". In this account, rather than being killed or defeated in the Battle of Zhuolu, Chiyou is victorious and captures the Chinese Emperor Hwang Di alive, rendering him subject to Shinshi.\n\nA recently published Korean novel, entitled \"Chiyou, the King of Heaven\" (Chinese: 蚩尤天皇; Korean: 치우천왕기), also claims that Chiyou was an ancestral leader of Koreans in the \"old country\" (Joo Shin, 주신 in Korean), and that he defeated the Yellow Emperor at the Battle of Zhuolu (탁록, \"Takrok\" in Korean).\n\n\n\n"}
{"id": "12685990", "url": "https://en.wikipedia.org/wiki?curid=12685990", "title": "Collar (order)", "text": "Collar (order)\n\nA collar is an ornate chain, often made of gold and enamel, and set with precious stones, which is worn about the neck as a symbol of membership in various chivalric orders. It is a particular form of the livery collar, the grandest form of the widespread phenomenon of livery in the Middle Ages and Early Modern Period. Orders which have several grades often reserve the collar for the highest grade (usually called the Grand Cross). The links of the chain are usually composed of symbols of the order, and the badge (also called \"decoration\", \"cross\" or \"jewel\") of the order normally hangs down in front. Sometimes the badge is referred to by what is depicted on it; for instance, the badge that hangs from the chain of the Order of the Garter is referred to as \"the George\".\n\nThe first of the Orders of Knighthood were the military orders of crusaders who used red, green or black crosses of velvet on their mantles, to distinguish their brotherhoods. Later the members of knightly orders used rings, embroidred dragons and even garters as the symbol of their order. In the late Middle Ages the knights wore their insignia ever more prominently and medaillons, crosses and jewels in the shape of animals began to be worn on chains around the neck, known as livery collars. After the 17th century the heyday of the collar was over. They were worn only on ceremonial occasions and replaced in daily life by stars pinned to the breast, and badges suspended from ribbons. Many orders retained their collars and when orders were divided into several ranks or grades the collar was usually reserved for the highest rank. The notable exception is Portugal. \n\nCollars of various devices are worn by the knights of some of the European orders of knighthood. The custom was begun by Philip III, Duke of Burgundy, who gave his Knights of the Golden Fleece, badges depicting a golden fleece hung from a collar of flints, steels and sparks. Following this new fashion, Louis XI of France, when instituting his Order of St. Michael in 1469, gave the knights collars composed of scallop shells linked on a chain. The chain was doubled by Charles VIII, and the pattern underwent other changes before the order lapsed in 1830. \n\nAt the end of the 18th century most of the European orders had only one rank; that of Knight and although they usually had collars, the cross or badge was now worn on a ribbon around the neck or over the right shoulder. When the orders became more democratic several ranks were introduced and only the highest grade, the \"Grand Commanders\" or \"Grand Crosses\", wore collars. The Netherlands never had collars but several Belgian, most of the Austrian and Prussian orders, and several Portuguese orders had collars. In Portugal all the members of these orders of knighthood wear a collar but the collars of the Grand Cross are more elaborate.\n\nSometimes the collar is used as the insignia of office of the Grand Master of the order. For instance, the President of France wears the collar of the Order of the Legion of Honour. In other countries such as Brazil the collar is a rank above that of a Grand Cross and it is reserved for the president and foreign heads of state. Napoleon I introduced the \"Grand aigle\" (Grand Eagle) to replace the Grand Cross as the highest rank in his Legion of Honour. Napoleon dispensed 15 such golden collars of the Legion among his kinsmen and the highest of his ministers. This collar did not survive his downfall and was abolished in 1815. \n\nUntil the reign of Henry VIII, the Order of the Garter, most ancient of the great knightly orders, had no collar. But the Tudor king wished to match the continental sovereigns in all things, and the present collar of the Garter knights, with its golden knots and its buckled garters enclosing white roses set on red roses, has its origin in the Tudor age. Most of the British orders of knighthood have collars and they are still worn on special occasions (see Collar days). The Distinguished Service Order, the Order of Merit, the Order of the Companions of Honour and the Imperial Service Order are the exceptions.\n\nIn heraldry, most members of orders are permitted to display the collar of their order on their coat of arms (if they are in fact entitled to wear the collar). There are often very strict rules as to how exactly the collar is to be displayed. Normally it will entirely encircle the escutcheon (shield), or the collar may be partially hidden by it. Sometimes, only a part of the collar and the badge will extend below the escutcheon.\n\nCollars of different orders are often depicted in the heraldic achievement of various monarchs, encircling the escutcheon. Though the standard achievement used most often may depict specific collars, this does not preclude the use of or substitution with other collars to which someone may be entitled to. Some achievements depict multiple collars while others depict only one; The coat of arms of the Norwegian monarch only depicts the collar of the Order of St. Olav encircling the shield while that of Denmark's depicts the collars of the nation's two chivalric orders: the Order of the Elephant and the Order of Dannebrog. In the greater arms of Sweden, the collar of the Order of Seraphim is used. The collar of the Order of Leopold is also depicted in the national arms of Belgium.\n\nWhen a member of an order dies, they are not usually buried with the collar, but it may be displayed on a pillow placed on the coffin (along with other decorations that the member may have) during the funeral. Many orders require that after the death of a member of the order, the collar and perhaps other insignia be returned to the issuing authority. Often, the requirement is that a male relative personally return the award to the order.\n\nMany orders also do have a chain as an ornament that is worn at more official ceremonial occasions (worn by knights of a single class order or members of the highest class of a multi-class order). However, in some orders Collar is a separate rank above that of Grand Cross, i.e.:\n\n\n\n\n\n\"* indicates that the insignia must be returned upon the death of the recipient\"<br>\n\"† indicates that the order is now dormant but has not been formally abolished\"<br>\n\n"}
{"id": "4283625", "url": "https://en.wikipedia.org/wiki?curid=4283625", "title": "Communization", "text": "Communization\n\nCommunization (or communisation in British English) mainly refers to a contemporary communist theory in which there is a \"mixing-up of insurrectionist anarchism, the communist ultra-left, post-autonomists, anti-political currents, groups like the Invisible Committee, as well as more explicitly ‘communizing’ currents, such as \"Théorie Communiste\". \"Obviously at the heart of the word is communism and, as the shift to communization suggests, communism as a particular activity and process...\" It is important to note the big differences in perception and usage. Some groups start out from an activist voluntarism (\"Tiqqun\", \"Invisible Committee\"), while others derive communization as an historical and social result emerging out of capital's development over the last decades (\"Endnotes\", \"Théorie Communiste\"). \"Endnotes\" totally distinguishes itself from the mixing of all sorts of meanings of the word \"communization\" and explicitly refers to the different reception in the Anglophone world as opposed to the original French milieu from which it emerged as a critique.\n\nIn communist political theory, communization is the process of abolishing ownership of the means of production, which, in societies dominated by the capitalist mode of production, are owned by individual capitalists, states, or other collective bodies. In some versions of communist theory, communization is understood as the transfer of ownership from private capitalist hands to the collective hands of producers, whether in the form of co-operative enterprises or communes, or through the mediation of a state or federation of workers' councils on a local, national, or global scale. In other programs, such as those of some left communists (e.g. Gilles Dauvé, Jacques Camatte), autonomists (e.g., Mario Tronti), and libertarian communists (e.g. Peter Kropotkin), communization means the abolition of property itself along with any state-like institutions claiming to represent a given subset of humanity. In these accounts humanity as a whole, directly or indirectly, would take over the task of the production of goods for use (and not for exchange). People would then have free access to those goods rather than exchanging labor for money, and distribution would take place according to the maxim \"from each according to his ability, to each according to his need.\"\n\nThe term communization was not used by Karl Marx and Friedrich Engels, but it was employed in the above sense by early Marxists. Communization in this sense is equivalent to the establishment of the \"higher phase\" of communist society described by Marx in \"Critique of the Gotha Program\". In \"State and Revolution\", Vladimir Lenin referred to the lower phase, organized around the principle \"To each according to his contribution\", as \"socialism\", with the higher phase as \"complete communism\", or \"full communism\", as Joseph Stalin would later put it. Thus both Lenin and Stalin gave grounds for thinking of communization not as a transition from capitalism to communism, but as a transition from socialism to communism, a transition that would take place after the working class had seized power, and which may last a long time (in the 1930s Stalin conceived of \"full communism\" as still a long way off). Thus the interval between the two transitions came to be seen as a necessary \"period of transition\" between the workers' revolution and communism.\n\nIt appears that within so-called \"communist\" regimes the demand for \"communization\" was associated with an impatience with the \"period of transition\" and a desire to break with the remaining capitalist forms (e.g., money, wage labor) still in place in those regimes. Those pushing for a move toward \"communizing\" in this sense were typically denounced as \"ultra-left\", with their suggestions dismissed as impractical and utopian, but they were able to point to the historical examples of the Paris Commune and the Spanish Revolution, where more radical measures of popular collectivization had been taken than in the Russian and Chinese revolutions, as well as to the German Revolution of 1918–19 and the Italian councils movement of 1919-1920 in which the historic \"left communist\" tendencies had been formed.\n\nThe association of the term communization with a self-identified \"ultra-left\" was cemented in France in the 1970s, where it came to describe not a transition to a higher phase of communism but a vision of communist revolution itself. Thus the 1975 Pamphlet \"A World Without Money\" states: “insurrection and communisation are intimately linked. There would not be first a period of insurrection and then later, thanks to this insurrection, the transformation of social reality. The insurrectional process derives its force from communisation itself.” This vision was opposed to the statism and vanguardism of the Leninist conception of revolution, but it also identified the perceived failure of the Russian and Chinese revolutions (carried out on the Leninist politico-military model) with the insufficiency of measures taken to abolish capitalist social relations (e.g. lack of direct collectivization, persistence of monetary relations). It also reversed the supposed \"pragmatism\" of the Leninist focus on the state, arguing that the final goal of the \"withering away of the state\" could hardly be advanced by the seizure of state power and the establishment of a \"revolutionary\" bureaucracy, but that the most practical means to achieve this goal would rather be the abolition of the capitalist relations (money, capital, wages) on which state power depends. Thus La Banquise writes:\nWithin this 1970s French tendency \"communization\" thus came to represent the absence of a period of transition and a conception of revolution as the application of communist measures throughout the economy and society. The term is still used in this sense in France today and has spread into English usage as a result of the translation of texts by Gilles Dauvé and \"Théorie Comuniste\", two key figures in this tendency.\n\nIn collaboration with other left communists such as François Martin and Karl Nesic, Dauvé has attempted to fuse, critique, and develop different left communist currents, most notably the Italian movement associated with Amadeo Bordiga (and its heretical journal \"Invariance\"), German-Dutch council communism, and the French perspectives associated with \"Socialisme ou Barbarie\" and the Situationist International. He has focused on theoretical discussions of economic issues concerning the controversial failure of Second International Marxism (including both Social Democracy and Leninist \"Communism\"), the global revolutionary upsurge of the 1960s and its subsequent dissolution, and on developments in global capitalist accumulation and class struggle.\n\nIn the late 1990s a close but not identical sense of \"communization\" was developed by the French post-situationist group Tiqqun. In keeping with their ultra-left predecessors, Tiqqun's predilection for the term seems to be its emphasis on communism as an immediate process rather than a far-off goal, but for Tiqqun it is no longer synonymous with \"the revolution\" considered as an historical event, but rather becomes identifiable with all sorts of activities – from squatting and setting up communes to simply \"sharing\" – that would typically be understood as \"pre-revolutionary\". From an ultra-left perspective such a politics of \"dropping-out\" or, as Tiqqun put it, \"desertion\" — setting up spaces and practices that are held to be partially autonomous from capitalism — is typically dismissed as either naive or reactionary. Due to the popularity of the Tiqqun-related works \"Call\" and \"The Coming Insurrection\" in US anarchist circles it tended to be this latter sense of \"communization\" that was employed in US anarchist and \"insurrectionist\" communiques, notably within the Californian student movement of 2009-2010. \"More recently its ideas have been elaborated and extended in discussions with like-minded groups including the English language \"Endnotes\" and the Swedish journal \"Riff Raff\". Together these collectives have recently collaborated to produce \"Sic – an international journal of communisation\" (issue number one was published in 2011).\"\n\n\n"}
{"id": "41273531", "url": "https://en.wikipedia.org/wiki?curid=41273531", "title": "Counter-IED equipment", "text": "Counter-IED equipment\n\nCounter-IED equipment are created primarily for military and law enforcement. They are used for standoff detection of explosives and explosive precursor components and defeating the Improvised Explosive Devices (IEDs) devices themselves as part of a broader counter-terrorism, counter-insurgency, or law enforcement effort.\n\nDetection techniques and specific systems with assessed Technological Readiness Levels (TRLs) are described by both capabilities and characteristics.\n\nA list of detection techniques and systems' capabilities include:\n\nA description of the characteristics includes:\n\nAlso of concern are any hazard identifications, the impact of each system and/or technique on DOTMLPF-P, and acquisition recommendations.\n\nTalon: The TALON transmits in color, black and white, infrared, and/or night vision to its operator, who may be up to 1,000 m away. It can run off lithium-ion batteries for a maximum of 7 days on standby independently before needing recharging. It has an 8.5 hour battery life at normal operating speeds, 2 standard lead batteries providing 2 hours each and 1 optional Lithium Ion providing an additional 4.5 hours. It weighs less than 100 lb (45 kg) or 60 lb (27 kg) for the Reconnaissance version. Its cargo bay accommodates a variety of sensor payloads. The robot is controlled through a two-way radio or a Fiber-optic link from a portable or wearable Operator Control Unit (OCU) that provides continuous data and video feedback for precise vehicle positioning. The (IED/EOD) TALON Carries sensors and a robotic manipulator, which is used by the U.S. Military for explosive ordnance disposal and disarming improvised explosive devices.\n\nSmall Unmanned Ground Vehicle (SUGV): SUGVs are lightweight, rugged, specialized systems suitable for military applications in congested urban settings to give users the ability to see around corners and into tight spaces.\n\nPackbot: The PackBot is a series of military robots by iRobot.\n\nThrowbots: Throwbots (from \"throwable robot\") are rugged, highly portable, and instantly and easily deployable reconnaissance robots.\n\nBody Armor: Soldiers, Sailors, Airmen and Marines have a large assortment of wearable protection against the effects of blasts and shrapnel. There have been many advancements made in ergonomics, blast resistant material and infection prevention over the last couple decades. Currently there are many options available for dismounted troops to protect them from all types of danger. Below are a few of the currently fielded systems and what is to come.\n\nE-SAPI/X-SAPI ballistic plates: Armored plates (of a shape and curvature to be placed against the body) that provide protection from explosively-formed projectiles.\n\nPelvic Protection System: To reduce casualties and minimize damage to vital areas of the body the U.S. Army teamed with other organizations and the industry to develop and rapidly field the Pelvic Protective System. The system is currently composed of two layers, an inner layer (underwear) and outer layer (ballistic protection)\n\nUS - Mine Resistant Ambush Protected (MRAP) vehicle: The MRAP program was prompted by U.S. deaths in Iraq. As recently as 2007, the U.S. military has ordered the production of about 10,000 MRAPs at a cost of over $500,000 each, and planned to order more MRAPs. Currently there are many different variants produced by several different manufacturers.\n\nU.S. - Assault Breacher Vehicle (ABV): The M1 ABV is a 70-ton armored vehicle nicknamed \"The Shredder,\" it is designed to clear paths for troops to advance through minefields or areas where improvised explosive devices might be buried. The ABVs can be equipped with a plow and bulldozer blade to breach obstacles or dig up mines. They can also be equipped with a line charge, packed with C4 explosives that can be launched and detonated from the vehicle. ABVs first got extensive use in Afghanistan in 2010 when the U.S. Marines brought them in to help deal with the IEDs, a popular weapon of the Taliban there. \"The Assault Breacher Vehicle is a tracked, combat-(engineered) vehicle designed to provide the capability for deliberate and in-stride breaching of mine fields and complex obstacles for the 1st Heavy Brigade Combat Team according to the 2d Infantry Division.\n\nBritish - Mastiff 3 Protected Patrol Vehicle: The Mastiff is a heavily armored, 6 x six-wheel-drive patrol vehicle which carries eight troops, plus two crew. It is currently on its third variation. It is suitable for road patrols and convoys and is the newest in a range of protected patrol vehicles being used for operations. Mastiff has a maximum speed of 90kph, is armed with the latest weapon systems, including a 7.62mm general purpose machine gun, 12.7mm heavy machine gun or 40mm automatic grenade launcher. They have Bowman radios and electronic countermeasures and are fitted with additional armor beyond the standard level to ensure they have the best possible protection.\n\nThe Caterpillar D9 is a large track-type tractor designed and manufactured by Caterpillar Inc. It is usually sold as a bulldozer equipped with a detachable large blade and a rear ripper attachment. The D9, with 354 kW (474 hp) of gross power and an operating weight of 49 tons, is in the upper end of Caterpillar's track-type tractors, which range in size from the D3 57 kW (77 hp), 8 tons, to the D11 698 kW (935 hp), 104 tons. The size, durability, reliability, and low operating costs have made the D9 one of the most popular large track-type tractors in the world. The Komatsu D275A is one of its most direct competitors.\n\nThe Israel Defense Forces Combat Engineering Corps uses an armored version of the D9, called IDF Caterpillar D9 \"Doobi\", to clear paths and operational terrain from landmines and a various IEDs. The heavy armor and durable construction of the IDF D9 enable it to withstand very heavy \"belly charges\" (IEDs weighing more than 100 kg planted underground to hit the hull of an armored fighting vehicle) which are capable of destroying main battle tanks. The IDF also have a remote-controlled version of the D9N, called \"Raam HaShachar\" (\"Dawn Thunder\" in Hebrew) to clear IEDs in very dangerous environments.\n\nThe United States Army uses an armored version of the Caterpillar D7 to clear landmines. A remote version of the D7 exists.\n\nUS - Bradley Fighting Vehicle\nThe Bradley Fighting Vehicle (BFV) is an American fighting vehicle platform manufactured by BAE Systems Land and Armaments, formerly United Defense. It was named after U.S. General Omar Bradley. The Bradley is designed to transport infantry or scouts with armor protection while providing covering fire to suppress enemy troops and armored vehicles. There are several Bradley variants, including the M2 Infantry Fighting Vehicle and the M3 Cavalry Fighting Vehicle. The M2 holds a crew of three: a commander, a gunner and a driver, as well as six fully equipped soldiers. The M3 mainly conducts scout missions and carries two scouts in addition to the regular crew of three, with space for additional TOW missiles.\n\nGermany - Puma\n\nUK - Future Rapid Effect System (FRES) Specialist vehicle\n\nUS - Ground Combat Vehicle (GCV)\n\nUS - Stryker \nThe IAV Stryker is a family of eight-wheeled, armored fighting vehicles derived from the Canadian LAV III and produced by General Dynamics Land Systems for the United States Army. It has 4-wheel drive (8x4) and can be switched to all-wheel drive (8x8). The vehicle is named for two American servicemen who posthumously received the Medal of Honor: Private First Class Stuart S. Stryker, who died in World War II and Specialist Four Robert F. Stryker, who died in the Vietnam War.\n\nU.S. - M1 Abrams Main Battle Tank\nThe M1 Abrams is an American third-generation main battle tank produced by the United States. It is named after General Creighton Abrams, former Army Chief of Staff and Commander of U.S. military forces in the Vietnam War from 1968 to 1972. Highly mobile, designed for modern armored ground warfare,[10] the M1 is well armed and heavily armored. Notable features include the use of a powerful gas turbine engine (multifuel capable, usually fueled with JP8 jet fuel), the adoption of sophisticated composite armor, and separate ammunition storage in a blow-out compartment for crew safety. Weighing nearly 68 short tons (almost 62 metric tons), it is one of the heaviest main battle tanks in service.\n\nRussia - T-90\n\nSouth Korea - K2 Black Panther\n\nTurkey - Altay\n\nU.S. - M113 APC: APCs are usually armed with only a machine gun. They are usually not designed to take part in a direct-fire battle, but to carry troops to the battlefield safe from shrapnel and ambush. They may have wheels or tracks. Examples include the American M113 (tracked), the French VAB (wheeled), the Dutch/German GTK Boxer (wheeled) and the Soviet BTR (wheeled). The infantry fighting vehicle is a further development of the armoured personnel carrier. In addition to the task of carrying infantry to battle safely they are more heavily armed and armoured and are designed for direct combat.\n\nUS - Marine Personnel Carrier (MPC)\n\nBrazil - Viatura Blindada Transporte de Pessoal, Media de Rodas (VBTP-MR) Guarani vehicle procurement program\n\nGermany and Netherlands - Boxer\n\nFrance - Véhicule Blindé Multirole (VBMR)\n\nSouth Africa - Sapula\n\nChina - Mine Resistant Ambush Protected All-Terrain Vehicle (M-ATV)\n\nU.S. - Joint Light Tactical vehicle (JLTV):The JLTV is a United States military (specifically U.S. Army, USSOCOM, and U.S. Marine Corps) program to replace the Humvee that is currently in service[2] with a family of more survivable vehicles with greater payload. In particular, the Humvee was not designed to be an armored combat and scout vehicle but has been employed as one, whereas the JLTV will be designed from the ground up for this role. Production is planned for 2015. The U.S. Army planned to buy 60,000 and the U.S. Marine Corps planned for 5,500 vehicles in 2010.[3]\n\nU.S. - High Mobility, Multi-purpose, Wheeled Vehicle (HMMWV) or \"Hummer\"; The HMMWV, commonly known as the Humvee, is a four-wheel drive military automobile produced by AM General.[6] It has largely supplanted the roles formerly served by smaller jeeps such as the M151 1⁄4-ton (230 kg), the M561 \"Gama Goat\", their M718A1 and M792 ambulance versions, the CUCV, and other light trucks. Primarily used by the United States military, it is also used by numerous other countries and organizations and even in civilian adaptations. The Humvee's widespread use in the Persian Gulf War helped inspire the civilian Hummer automotive marque.\n\nCanada - Tactical Armored Patrol Vehicle (TAPV)\n\nAustralia - Protected Mobility Vehicles–Light (PMV-L)\n\nFrance - Porteur Polyvalent Terrestre (PPT)\n\nCanada - Medium Support Vehicle System (MSVS)\n\nSouth Africa - Vistula\n\nDuke Version 3 Vehicle mounted CREW system: Duke V3, manufactured by SRCTec, Inc., is a counter radio-controlled improvised explosive device (RCIED) electronic warfare (CREW) system that was developed to provide U.S. forces critical, life-saving protection against a wide range of threats. It is a field deployable system that was designed to have minimal size, weight and power requirements while providing simple operation and optimal performance in order to provide force protection against radio-controlled IEDs. CREW Duke V3 consists of a primary unit known as the CREW Duke V2 and a secondary unit that features advanced electronic warfare subsystems to counter emerging advanced RCIED technologies. Advanced EW components and techniques are implemented to combat complex threat infrastructures in order to provide a maximum protection radius while minimizing the overall system cost and prime power consumption requirements.\n\nCVRJ (CREW Vehicle Receiver Jammer) U.S. Marines: , The primary purpose of the CVRJ system is to defeat existing Radio Frequency (RF) threats and newly identified Hard-to-Kill RF threats. The CVRJ system accomplishes its primary mission by jamming each threat's transmitted RF signals. The secondary purpose of the CVRJ system is to add the capability to combine multiple internal RF signals and external RF inputs from other systems, and serve as the conduit for transmitting those RF signals while maintaining system interoperability. It accomplishes both missions via 15 waveform programmable RF channels. The system is software controlled to meet specific threats. Indicators on the CVRJ front panel and Remote Control Unit (RCU) allow the operator to observe system health and diagnostic messages. Built-In-Test (BIT) routines run during system initialization and operation that notify the operator of system faults by illuminating indicators referred to as \"annunciators\" and by displaying text messages on the RCU display. The system is highly automated which reduces operator interaction. The system draws up to 36 amps of vehicle power, weighs approximately 69 lbs, and measures 13\"H x 14\"W x 19\"D.\n\nVehicle Jammer System STAR V: Protective modular jamming system STAR V 740 is intended for a protection against RCIED. The system either prevents the activation of RCIED or it can significantly reduce the distance for a bomb activation. The system is used to protect the special EOD teams or for a convoy protection.\nThe jamming is performed by random frequency sweeping in a few frequencies sub-bands at the same time. Each sub-band has a possibility to set up to two communication windows for mutual radio communication. The jamming system is equipped with 8 wideband transmitters, 3 low pass filters, 1 combiner, 6 Omni-directional antennas and 8 wide-band digital exciters. The higher level of jamming efficiency is accomplished by using more parallel subsystems and digital technology.\nThe jamming system is intended as a mobile system which is installed in the vehicle. It is equipped with the Omni-directional antennas that are part of the system. The output power of the jammer is up to 740 W. The jamming system is easily controlled and the failures are easily diagnosed. It is controlled on the front panel. The operator can switch on/off particular transmitting systems and subsystems, set up to three jamming sub-bands in each. Dwell time in each sub-band can also be modified. Jammer is equipped with IP, RS 485 and USB interface. System is also equipped with special SW which can be installed on a notebook or a PC. Special remote control box is also included. The system's voltage is from 22 V to 30 Volts.\n\nConvoy Jammer System HP 3260 H: Modular jamming system intended to protect surrounding vehicles and personnel against RCIED. Designed for maximum frequency coverage and protection range, the system is used for both, civilian and military motorcades. The modularity enables users to scale the system according to operational requirements and the software allows programming of all signal generators independently to ensure utmost configurability and maximising the effectiveness of the jamming signals.\n\nThor III dismounted CREW system: The Thor III system consists of three dismounted man-pack subsystems, one battery charger, and twenty-four batteries (BB-2590/U). Each subsystem contains a R/T (low band, mid band or high band), a Remote Control Unit (RCU), an integration/pack frame, an Rx/Tx Antenna (low band, mid band, or high band), a GPS antenna, cables, and software. Each subsystem is housed in a separate transit case with protective covers. The purpose of the Thor III dismounted system is to provide the user in the field with a wearable Radio-Controlled Improvised Explosive Device (RCIED) jammer that has been designed to counter an array of frequency diverse threats. The system is an expandable, active and reactive, scanning-receiver-based jammer with multiple jamming signal sources that allow it to counter multiple simultaneous threats.\n\nJoint IED Neutralizer (JIN): In 2005, Ionatron attempted to develop an anti IED device that would \"zap\" IEDs from a distance by using lasers to ionize the air and allow man-made lightning to shoot towards the devices detonating them at a safe distance. By using femtosecond lasers light pulses that last less than a ten-trillionth of a second JIN could carve conductive channels of ionized oxygen in the air. Through these channels, Ionatron's blaster sent man-made lightning bolts.\n\nThor IED Zappers: The vehicular system is mounted on a remotely controlled weapon station, carrying the laser beam director and high-energy laser and coaxial 12.7mm machine gun to neutralize improvised explosive devices from a safe, standoff distance.\n\nUltra Wide Band High Powered Electro Magnetics: An UWB-HPEM system typically consists of the following components: a battery-based direct current power supply, an actuation system, a semiconductor-based ultra-wideband pulse generator and an ultra-wideband antenna. Depending on the type of threat, it can either set off a sensor-triggered IEDs in a controlled explosion or prevent it from being remotely detonated by radio or mobile phone. A UWB-HPEM system can be loaded onto a vehicle, creating an electromagnetic protection zone for a convoy, potentially in combination with other systems.\n\nIED Countermeasure Equipment (ICE): In the fall of 2004, the Army Research Laboratory (ARL) at White Sands Missile Range in New Mexico and New Mexico State University's Physical Science Laboratory developed a jamming system that uses low-power radio frequency energy to block the radio signals that detonate enemy IEDs. The IED Countermeasure Equipment is typically mounted on a vehicle and is used to neutralize IEDs when avoiding, disarming, or destroying them is not practical. So far, several thousand ICE systems have been deployed to U.S. military personnel.\n\nGround Ordnance Land Disruptor: G.O.L.D is a user filled, explosively driven Counter-IED system that renders buried IEDs safe through a combination of disruption, component separation and expulsion from the ground allowing the IED to remain biometrically intact.\n\nRhino: Rhino is a box-shaped heating device attached to a long pole that can be mounted to the front of a vehicle to prematurely detonate any buried IEDs in front of the vehicle.\n\nA variety of technologies are used to detect landmines, improvised explosive devices (IED) and unexploded ordnance (UXO), including acoustic sensors, animals and biologically-based detection systems (bees, dogs, pigs, rats), chemical sensors, electromagnetic sensors and hyperspectral sensor analysis, generalized radar techniques, ground penetrating radar, lidar and electro-optical sensors (including hyperspectral and millimeter wave), magnetic signatures, nuclear sensors, optical sensors, seismic acoustic sensors, and thermal detection.\n\nCounter-IED Reconnaissance Planes: The U.S. Army's Task Force ODIN-E flies manned reconnaissance aircraft that use an array of full-motion video (FMV), electro-optical (EO), infrared (IR), and synthetic aperture radar (SAR) imagery sensors to find IEDs.\n\nIED Volumetric Detection:\n\nMicrowave Based Explosive Caches Detection: Raytheon UK's Soteria vehicle-mounted stand-off system provides high-definition IED detection, confirmation and threat diagnostics from a significant distance. Soteria's optical processing technology has the following capabilities: a high probability of IED detection with a low false positive rate, detection of high, medium, low and zero metal content IEDs, assisted target recognition, and day and night operability. Soteria is also equipped with ground vibration monitoring capabilities in the front of the vehicle.\n\nNon-linear Junction Detector (NLJD): A portable NLJD allows the operator to search voids and areas where they are unable to gain physical or visual access, in order to detect electronic components and determine if the area is free from IEDs.\n\nLaser IED Detection: Scientists are learning to adapt lasers to detect, or defeat, IEDs.\n\nMine detectors: A portable, hand-held or worn device to detect buried IEDs. There are many different models from several different companies currently in use worldwide by U.S. and coalition forces. These are not your run of the mill metal detectors that you can buy at your local store, they are highly sophisticated, ultra sensitive, programmable devices.\n\nAerostats are balloons equipped with stabilized electro optical, infrared, and radar sensors which are manned 24 hours a day. The Persistent Threat Detection System (PTDS) is the largest and most capable Aerostat ever used in combat. First used in 2004 (Camp Slayer, Iraq). It can sit for months thousands of feet above a base. Known as the \"unblinking eye\", Aerostats provide real-time High Definition imagery of the surrounding area, day or night, and are strategically placed for surveillance purposes. They enhance situational awareness and improve force protection. Aerostats can be used to reconnoiter routes before friendly forces travel them and to provide over watch for dismounted troops or convoys. They can also serve as a communications and Full Motion Video (FMV) relay platform to extend the range and disseminate situational awareness. They are linked with several ground-based sensors, including acoustic sensors that detect and locate weapon fire or blasts.\n\nThe Persistent Threat Detection System (PTDS) is a large helium-filled lighter than air system designed by Lockheed Martin to provide soldiers long range intelligence, surveillance, reconnaissance and communication assistance.\n\nSince the Civil War, when Union Soldiers utilized hot air balloons to serve as a surveillance platform, airship technology has been a part of the Army's inventory. As U.S. forces began a troop surge in Afghanistan while maintaining security in Iraq, the need to provide soldiers with a persistent view of the battlefield was critical.\n\nIn 2003, Lockheed Martin engineers began updating existing naval aerostats with durable materials capable of achieving lift while carrying larger payloads of sensors, cameras and audio equipment. New tethers—lined with a mix of copper wires and fiber-optic cables—transmitted data to a ground control station, which then disseminated near real-time information of hostile activity to operational forces.\n\nThe aerostats are reconnaissance tool, gathering intelligence from 100 miles in every direction, 24 hours a day, for weeks on end. In Iraq and Afghanistan, there was a special need for enhanced surveillance, especially in the attempt to counter improvised explosive devices.\n\nThe Army Research Laboratory developed and then mounted PTDS with an acoustic-sensor array, known as the Unattended Transient Acoustic MASINT Sensor (UTAMS). The technology detects, locates, and cues a collocated imager to transient sounds, such as enemy mortar, rocket launches, and IED attacks, and calculates the ground location of the threat source. Adding this airborne detection – localization – cueing capability provides accurate intelligence to PTDS. PTDS is compatible with other technology developed by the Army Research Laboratory, such as Serenity Payload and FireFly.\n\nThe first PTDS was deployed by the US Army in 2004 and 37 PTDS units were acquired by 2010. Lockheed Martin delivered the final PTDS to the US Army in May 2012, bringing the total number of systems procured by the US Army to 66.\n\nThe airship has been one of the Army's major weapons since 2004 and was recognized by the Department of Army Engineers and Scientists as the Army's greatest invention in 2005.\n\nA UGV is a vehicle kit system that advances perception, localization and motion planning to protect from IED threats and increase performance in autonomous missions. They typically are adaptable to any tactical wheeled vehicle for the purpose of supervised autonomous navigation in either a lead or follow role. UGVs are multi-sensor systems which use registration techniques to provide accurate positioning estimates without needing to rely on continuous tracking through a lead vehicle or GPS signals. When equipped with a UGV, each vehicle is capable of navigation to the objective independently.\n"}
{"id": "40620454", "url": "https://en.wikipedia.org/wiki?curid=40620454", "title": "Cultural depictions of the dog", "text": "Cultural depictions of the dog\n\nCultural depictions of dogs extend back thousands of years to when dogs were portrayed on the walls of caves. Representations of dogs in art became more elaborate as individual breeds evolved and the relationships between human and canine developed. Hunting scenes were popular in the Middle Ages and the Renaissance. Dogs were depicted to symbolize guidance, protection, loyalty, fidelity, faithfulness, watchfulness, and love.\nAs dogs became more domesticated, they were shown as companion animals, often painted sitting on a lady's lap. Throughout the art history there is an overwhelming presence of dogs as status symbols and pets in painting mainly in the Western art. The dogs were brought to houses and were allowed to live in the house and cherished as part of the family, and were regarded as mostly in the upper classes who used them for hunting and could afford to feed them. Hunting dogs were generally connected to the aristocracy. Only the nobility were allowed to keep hunting dogs, and this would signal status, and it was the signalment of a noble man. Dog portraits became increasingly popular in the 18th century, and the establishment of The Kennel Club in the UK in 1873 and the American Kennel Club in 1884 introduced breed standards or 'word pictures', which further encouraged the popularity of dog portraiture.\n\nThe walls of caves and tombs dating back to the Bronze Age have illustrations or statues of dogs. These generally portray dogs used for hunting, and even children's toys and ceramics depicting dogs.\nSome of the prehistoric paintings are found in Bhimbetka rock shelters, these paintings depict dog on a leash with a man. Rock art of Tassili n'Ajjer also include depiction of dog.\nThe Ancient Greeks and Romans, contrary to the Semitic cultures favored dogs as pets, and valued them for their faithfulness and courage and were often seen on Greek and Roman reliefs and ceramics as symbols of fidelity. Cats were not favoured over dogs, on contrary Ancient Greeks and Romans didn't keep cats as pets. Dogs were given as gifts among lovers and kept them as both pets, status symbols or kept for guardians and as hunting dogs. Dogs were appreciated by the Greek for their faith and love. Homeros Odyssey tells the story of Odysseus who had a dog called Argos he raised and who was the only one that recognized him when he returned home after his travels, disguised to conceals his appearance. Only the old dog recognized him instantly. This theme has been often depicted in on ancient Greek vases.\n\nThe Ancient Romans kept three types of dogs. Hunting dogs; especially sighthounds, Molossus type of dogs like the Neapolitan Mastiff were often depicted in reliefs and mosaics with the words Cave Canem; and small companion dogs of Maltese type as women's lap dogs. Greyhounds were often depicted too, often as sculptures. Large dogs were often used in war by the Roman army in attack formations or for wolf-hunting on horseback, which was a popular sport.\n\nGenerally, dogs symbolize faith and loyalty. A dog, when included in an allegorical painting, portrays the attribute of fidelity personified. In a portrait of a married couple, a dog placed in a woman's lap or at her feet can represent marital fidelity. If the portrait is of a widow, a dog can represent her continuing faithfulness to the memory of her late husband.\n\nAn example of a dog representing marital fidelity is present in Jan van Eyck's \"Arnolfini Portrait\". An oil painting on oak panel dated 1434 by the Early Netherlandish painter Jan van Eyck, it is a small full-length double portrait, which is believed to represent the Italian merchant Giovanni di Nicolao Arnolfini and his wife, presumably in their home in the Flemish city of Bruges. It portrays a wedding scene, where the people invited to witness the ceremony can be seen in the convex mirror at the back, the mirror symbolizing the eye of God. In those times people were not always married in the church, it was enough that two witnesses were present to make a wedding legal. A number of symbols can be found in the picture: the fruit, the symbol of fertility and wealth, the shoes removed (this is a holy place), and the dog. The oranges casually placed to the left are a sign of wealth; they were very expensive in Burgundy, and may have been one of the items dealt in by Arnolfini. The little dog symbolizes in the Middle Ages iconography faithfulness, devotion or loyalty, or can be seen as an emblem of lust, signifying the couple's desire to have a child. Unlike the couple, the dog looks out to meet the gaze of the viewer. The dog could also be simply a lap dog, a gift from husband to wife. Many wealthy women in the court had lap dogs as companions. In that case the dog could reflect wealth or social status.\nDuring the Middle Ages, images of dogs were often carved on tombstones to represent the deceased's feudal loyalty or marital fidelity.\n\nHunting scenes were common topics in medieval and Renaissance art. Hunting in the medieval period was a sport exclusive to the aristocracy and hunting was an essential part of court etiquette. Depictions of person together with a hunting dog, hawks or falcons would signal status, or was the signalment of a noble man - since the hunting dogs were connected to aristocracy, and only the nobility was hunting. Hunting was considered as a sport, but also a way of showing bravery, chivalric virtue while hunting dangerous prey such as wild boars and bears. Hunting dogs allow humans to pursue and kill prey that would otherwise be very difficult or dangerous to hunt. Different breeds of dogs were used for different types of hunting. Hunting with dogs was so popular that during the Middle Ages the wild bears had already been hunted to extinction in England.\n\nEven as animal domestication became relatively widespread and after the development of agriculture, hunting was usually a significant contributor to the human food supply. Hunting filled also a practical necessity, assuring food for the tables of the nobility. Nobles spending their money on packs of specially bred hounds were not unusual.\n\nGaston Phoebus (30 April 1331 – 1391) wrote a manual on hunting and there are several tapestries from the period which depict hunting scenes.\n\nEven if nobility was hunting, hunting has been forbidden to servants, peasants and also the Roman Catholic Church clerics. Thus the \"Corpus Juris Canonici\" (C. ii, X, De cleric. venat.) says, \"We forbid to all servants of God hunting and expeditions through the woods with hounds; and we also forbid them to keep hawks or falcons.\" The Fourth Council of the Lateran, held under Pope Innocent III, decreed (canon xv): \"We interdict hunting or hawking to all clerics.\" The decree of the Council of Trent is worded more mildly: \"Let clerics abstain from illicit hunting and hawking\" (Sess. XXIV, De reform., c. xii), which seems to imply that not all hunting is illicit, and canonists generally make a distinction declaring noisy (\"clamorosa\") hunting unlawful, but not quiet (\"quieta\") hunting. Thus, hunting dogs are seldom seen depicted in the company of clerics, but very often in the company of in the highest social class of that society.\n\nFor the same reason, depictions of dogs can be found in heraldry. In monarchies, the aristocracy are a class of people who either possess hereditary titles granted by a monarch or are related to such people, and thanks to the specific connection of the aristocrats with the hunting dogs, dogs were often shown as symbols in heraldry. In the late Middle Ages and the Renaissance, heraldry became a highly developed discipline. Dogs of various types, and occasionally of specific breeds, occur as charges and supporters in many coats of arms, and often symbolise courage, vigilance, loyalty and fidelity.\n\nThree encaustic tiles dating from the 15th century featured a white hound, the Talbot family crest and the inscription \"Sir John Talbot\" (the 1st Earl of Shrewsbury). Part of a set of four, the tiles were possibly originally used on a church floor. The term talbot is used in heraldry to refer to a good-mannered hunting dog. The Talbot dog always depicts the Talbot coat of arms and is the original hound used as an English heraldic symbol. It is portrayed in the family arms of several noble German families and at least seven other English families.\n\nDuring the 16th and 17th century, dogs were depicted in hunting scenes, or depicted representing social status, as a lap dog, or sometimes as a personal friend. They may also used as symbols in painting. The Greek philosopher Diogenes (404-323 BC) was depicted by Jean-Léon Gérôme, in the company of dogs that also served as emblems of his \"Cynic\" (Greek: \"kynikos,\" dog-like) philosophy, which emphasized an austere existence. Diogenes stated that \"Unlike human beings who either dupe others or are duped, dogs will give an honest bark at the truth. Other dogs bite their enemies, I bite my friends to save them.\" The Greek philosopher is seated in his abode, the earthenware tub, - also depicted sometimes in sitting in a barrel, - in the Metroon, Athens, lighting the lamp in daylight with which he was to go in searching for an honest man.\n\nHendrik Martenszoon Sorgh portrayed probably a scholar noted by its significance to its particular profession, depicting a book, or a Protestant preacher and theologian with the Bible opened on the table. A dog depicted as a companion to the scholar is a symbol of fidelity, vigilance and regularity in the research due to assigned to dogs natural intelligence and intuition, and the parrot is a symbol of erudition and eloquence.\n\nNetsuke are Japanese miniature sculptures of great artistic merit that were also serving a practical function as a storage place for kimomos that traditionally had no pockets. These small boxes had a carved, button-like toggle called a \"netsuke\". Most netsuke production was during around 1615–1868, in the Edo period in Japan. Among others motifs, netsukes depicted even dogs. \nThe tradition of showing dogs in hunting scenes continued through to the 18th century.\n\nThe picture entitled \"A Distinguished Member of the Humane Society\" depicts a dog that was well known in London at that time. The dog depicted by Sir Edwin Landseer 1838, was a Newfoundland called \"Bob\", who was found in a shipwreck off the coast of England. The dog found his way to the London waterfront, where he became known for saving people from drowning, a total of twenty-three times over the course of fourteen years. For this, he was made a distinguished member of the Royal Humane Society, granting him a medal and access to food. The Newfoundlanders with white patches are now recognized as a breed of their own, as a \"Landseer\".\nThe painting was described by \"The Art Journal\" 1838 as being \"one of the best and most interesting publications of the year\", and \"Mr Thomas Landseer's first great effort in this department of the art\". A 19th century copy of the painting by George Cole was sold by auctioneers Bonhams for £7,200 in March 2007.\n\nBy the Victorian era, the mainly sporting tradition remained but after the establishment of The Kennel Club in the UK in 1873 and the American Kennel Club in 1884 introduced breed standards or 'word pictures', dog portraits soared in popularity. There were differences between the British and European style of depiction; William Secord, a world expert on canine art, described it by stating: \"Belgian, Dutch, Flemish and German artists were more influenced by realism, depicting the dog the way it really looked, with dirt on it’s coat and slobber and that kind of thing. You see Alfred Stevens, who's Belgian, do street dogs and dogs that are suffering, which in England you never see. British depictions were more idealized. They want it pretty.”\n\nThe prices achieved for canine art increased the 1980s–90s and started to gain popularity in established art circles rather than antique markets. Buyers can generally be divided into three dominant categories: hunters; breeders and exhibitors of pedigree dogs; and owners of companion animals.\n\nPablo Picasso frequently included his canine companions in his paintings. Particularly well known and often featured in his work was a Dachshund, named Lump, who actually belonged to David Douglas Duncan but lived with Picasso.\n\nDepictions of dogs have been extended as well to the artform of photography, a noted example being the work of photographer Elliott Erwitt.\n\n"}
{"id": "7571231", "url": "https://en.wikipedia.org/wiki?curid=7571231", "title": "Davenport Tablets", "text": "Davenport Tablets\n\nThe Davenport Tablets are three inscribed slate tablets found in mounds near Davenport, Iowa.\n\nThe first two tablets were discovered on January 10, 1877 by a local clergyman, the Reverend Jacob Gass, while engaged in an emergency excavation (due to the imminent transfer of the access rights) at the site known as Cook's Farm. In an excavation a year later (the access rights having been restored), Charles Harrison, the president of the Davenport Academy of Natural Sciences, while excavating there with Gass, found a third tablet. They are often associated in discussions with a pipe found by Gass and another Lutheran minister, the Reverend Ad Blumer in 1880 in a separate group of mounds, referred to as the 'elephant pipe' by Gass. Blumer gave the pipe to the Academy and shortly after his donation, the Academy acquired a similar pipe from Gass which he reported had been found by a farmer in Louisa County, Iowa. Charles Putnam wrote a vindication of the artifacts in 1885.\n\nInitially, the authenticity of the Davenport artifacts was not questioned, and even received good reviews from people like Spencer Baird, of the Smithsonian Institution, and businessman Charles E. Putnam. However, as the debate escalated from the pages of minor scholarly journals to the foremost news in the journal \"Science\", eventually the tablets’ authenticity fell under the criticism of the new Smithsonian spokesman, Cyrus Thomas. Thomas lambasted them as “anomalous waifs,” that had absolutely no supporting, or contextual, evidence to aide in their authenticity.\n\nUniversity of Iowa Professor, Marshall McKusick, now refers to the find and the circumstances surrounding it as “The Davenport Conspiracy”. McKusick suggested that the tablets were modified roof tiles stolen off the Old Slate House, a house of prostitutes, even though Gass described finding them in a burial mound on the Cook family farm.\n\nMcKusick suggested that the contextual ambiguity of the tablets – along with questions of Gass' honesty as an archaeologist, and even rumors of a plot by envious colleagues to plant the pseudo-artifacts in an effort to discredit and to expel the foreign-born Gass from his recently awarded post at the Davenport Academy – discredit the credibility of the Davenport Tablets.\n\nIn his 1991 book, \"The Davenport Conspiracy Revisited\", Professor Marshall McKusick asserts that Gass may have been the victim of an ill-advised joke played on him by fellow Davenport Academy members, who were possibly motivated by their jealousy of a foreign-born outsider in their midst. In 1874 Gass had made important discoveries of beautiful and complex Native American art at the Cook farm, such as copper axes. The level of technical ability and artistic craftsmanship by ancient Native Americans was evident in these artifacts. At a time when people digging along the Mississippi River in Iowa and Illinois were turning up nothing, Gass had the luck of hitting a genuine archaeological jackpot. After that date it is questionable as to what the motives of his academic rivals and relatives were.\n\nAnother explanation for the dubious origins of the artifacts might involve the credibility of Gass himself. It is believed that Gass dealt in fake Native American effigy pipes, such as the many examples illustrated in \"The Davenport Conspiracy Revisited\". Genuine effigy pipes are a testament to the creative abilities of the ancient Native American Indians, but their counterfeits are of poor quality. Made of shale, clay, and limestone, these frauds were often traded amongst Gass and his colleagues, many ending up in the Davenport Academy museum. However, it is possible that Gass himself was not the perpetrator of these fakes, but was again under the influence of people who were jealous of his abilities and luck in selecting excavation sites. This time though, it was his own relatives, Edwin Gass and Adolph Blumer that persuaded him to take these fakes seriously and trade them.\n\n\n"}
{"id": "833011", "url": "https://en.wikipedia.org/wiki?curid=833011", "title": "Duck and cover", "text": "Duck and cover\n\n\"Duck and cover\" is a method of personal protection against the effects of a nuclear explosion. Ducking and covering is useful at conferring a degree of protection to personnel situated outside the radius of the nuclear fireball but still within sufficient range of the nuclear explosion that standing upright and uncovered is likely to cause serious injury or death. In the most literal interpretation, the focus of the maneuver is primarily on protective actions one can take during the first few crucial seconds-to-minutes after the event, while the film by the same name and a full encompassing of the advice, also caters to providing protection up to weeks after the event.\n\nThe countermeasure is intended as an alternative to the more effective target/citywide emergency evacuation when these crisis relocation programs would not be possible due to travel and time constraints. Maneuvers similar, but not identical, to \"Duck and Cover\" are also taught as the response to other sudden destructive events, the maneuvers that are advisable in an earthquake or tornado, in the comparable situation where preventive emergency evacuation is similarly not an option, again, due to time constraints. In these analogously powerful events, \"Drop, Cover and Hold on\" likewise prevents injury or death if no other safety measures are taken.\n\nAs a countermeasure to the lethal effects of nuclear explosions, \"Duck and Cover\" is effective in both the event of a surprise nuclear attack, and during a nuclear attack of which the public has received some warning, which would likely be about a few minutes prior to the nuclear weapon arriving.\n\nImmediately after one sees the first flash of intense heat and light of the developing nuclear fireball, one should stop, get under some cover and drop/duck to the ground. There, one should assume a prone-like position, lying face-down, and to afford protection against the continuing heat of the explosion further cover exposed skin and the back of one's head with one's clothes; or, if no excess cover or cloth is available, one should cover the back of one's head and neck with one's hands.\n\nSimilar instructions, as presented in the \"Duck and Cover \" film, are contained in the British 1964 public information film \"Civil Defence Information Bulletin No. 5\" and in the 1980s \"Protect and Survive\" public information series. Children in the Soviet Union likewise received almost identical classes on countermeasures, according to \"Inside the Kremlin's Cold War\" authors Zubok and Pleshakov.\n\nIn U.S. Army training, soldiers are taught to fall down immediately and cover their face and hands in much the same way as is described above.\n\nIn the classroom scene of the film, the rapid employment of school desks, as an improvised shelter following the awareness of the initial light flash, is a countermeasure primarily to offer protection from potential ballistic window glass lacerations when the slower moving blast wave arrived. However, in higher blast pressure zones, where partial-to-total building collapse may occur, it would also serve a similar role to that borne out from experience in urban search and rescue, where voids under the debris of collapsed buildings are common places for survivors to be found. More rigid examples of void-forming-tables to shelter under include the \"Morrison indoor shelter\", which was widely distributed by the millions in Britain as a protective measure against building collapse, brought about by blast pressures generated during the conventional bombing of cities in World War II.\n\nUnder the conditions where some warning is given, one is advised to find the nearest bomb shelter, or if one could not be found, any well-built building to stay and shelter in place. Sheltering is, as depicted in the film, also the final phase of the \"duck and cover\" countermeasure in the surprise attack scenario.\n\nThe \"duck and cover\" countermeasure could save thousands. This is because people, being naturally inquisitive, would instead run to windows to try to locate the source of the immensely bright flash generated at the instant of the explosion. During this time, unbeknownst to them, the slower moving blast wave, would be rapidly advancing toward their position, only to arrive and cause the window glass to implode, shredding onlookers. In the testimony of Dr. Hiroshi Sawachika, although he was sufficiently far away from the Hiroshima bomb himself and was not behind a pane of window glass when the blast wave arrived, those in his company who were had serious blast injury wounds, with broken glass and pieces of wood stuck into them.\n\nSimilar advice to \"duck and cover\" is given in many situations where structural destabilization or flying debris may be expected, such as during an earthquake or tornado. At a sufficient distance from a nuclear explosion, the blast wave produces similar results to these natural phenomena, so similar countermeasures are taken. In areas where earthquakes are common, a countermeasure known as \"Drop, Cover, and Hold On!\" is practiced. Likewise, in tornado-prone areas of the United States, especially those within Tornado Alley, tornado drills involve teaching children to move closer to the floor and to cover the backs of their heads to prevent injury from flying debris. Some US states also practice annual emergency tornado drills.\n\nThe dangers of viewing explosions behind window glass were known of before the Atomic Age began, being a common source of injury and death from large chemical explosions. The Halifax Explosion of 1917, an ammunition ship exploding with the energy of roughly 2.9 kilotons of TNT, injured the eyes and faces of hundreds of people who stayed behind and looked out of their windows after seeing a bright flash, with 200 blinded by broken glass when the slower moving blast arrived. Every window in the city of Halifax, Nova Scotia, was shattered in this catastrophe of human error.\n\nIn the Record of the \"Nagasaki A-bomb War Disaster\", those close to the hypocenter (Matsuyama township), were described as all having been killed, with the exception of \"a child who was in an air-raid shelter.\" A little further away, Professor Seiki of Nagasaki Medical School Hospital was building an air-raid dugout 400 m from the hypocenter of the detonation and survived. Chimoto-san, who was atop a distant hill that creates the valley in which Nagasaki is situated, performed the similar \"hit the deck\" maneuver upon seeing the bomb drop, which was notably \"prior\" to the detonation. However despite having these few seconds of relatively unique warning, he did not stay on the ground for long enough after the \"flash\" subsided, and received some translational injuries due to prematurely standing-up again, at which point the slower moving blast wave swept past him and carried him with it for a few meters.\n\nAccording to the 1946 book \"Hiroshima\" and other books which cover both bombings, in the days between the atomic bombings of Hiroshima and Nagasaki, some survivors of the first bombing went to Nagasaki and taught others about ducking after the atomic \"flash\" and informed them about the particularly dangerous threat of imploding window glass. As a result of this and other factors, far fewer died in the initial blast at Nagasaki as compared to those who were not taught to duck and cover. The general population however was not warned of the heat or blast danger following an atomic flash, due to the new and unknown nature of the atomic bomb. Many people in Hiroshima and Nagasaki died while searching the skies, curious to locate the source of the brilliant flash.\n\nWhen people are indoors, running to windows to investigate the source of bright flashes in the sky still remains a common and natural response to experiencing a bright flash. Thus, although the advice to duck and cover is over half a century old, ballistic glass lacerations caused the majority of the 1000 human injuries following the Chelyabinsk meteor air burst of February 15, 2013.\nThis response was also observed among people in the vicinity of Hiroshima and Nagasaki.\n\nThe United States' monopoly on nuclear weapons was broken by the Soviet Union in 1949 when it tested its first nuclear explosive, the RDS-1. With this, many in the US Government, as well as many citizens, perceived that the United States was more vulnerable than it had ever been before. In 1950, during the first big Civil Defense push of the Cold War—and coinciding with the \"Alert America!\" initiative to educate Americans on nuclear preparedness, the adult-orientated Survival Under Atomic Attack was published. It contains \"duck and cover\" or more accurately, cover and then duck advice without using those specific terms in its \"Six Survival Secrets For Atomic Attacks\" section. \"1. Try to Get Shielded 2. Drop Flat on Ground or Floor 3. Bury Your Face in Your Arms (\"crook of your elbow\")\". The child-oriented film \"Duck and Cover\" was produced a year later by the Federal Civil Defense Administration in 1951.\n\nEducation efforts on the effects of nuclear weapons proceeded with stops-and-starts in the US due to competing alternatives. In a once classified, 1950s era, US war game that looked at varying levels of war escalation, warning and pre-emptive attacks in the late 1950s early 1960s, it was estimated that approximately 27 million US citizens would have been saved with civil defense education. At the time however the cost of a full-scale civil defense program was regarded as lesser in effectiveness, in cost-benefit analysis than a ballistic missile defense (Nike Zeus) system, and as the Soviet adversary was believed to be rapidly increasing their nuclear stockpile, the efficacy of both would begin to enter a diminishing returns trend. When more became known about the cost and limitations of the Nike Zeus system, in the early 1960s the head of the department of defense under president John F. Kennedy,\nRobert McNamara, determined the ineffectiveness of the Nike-Zeus system, especially in its benefit-cost ratio compared to other options. For instance, fallout shelters would save more Americans for far less money.\n\nWithin a considerable radius from the surface of the nuclear fireball, 0–3 kilometers—largely depending on the explosion's height, yield and position of personnel—ducking and covering would offer negligible protection against the intense heat, blast and prompt ionizing radiation following a nuclear explosion. Beyond that range, however, many lives would be saved by following the simple advice, especially since at that range the main hazard is not from ionizing radiation but from blast injuries and sustaining thermal \"flash\" burns to unprotected skin. Furthermore, following the bright flash of light of the nuclear fireball, the explosion's blast wave would take from first light, 7 to 10 seconds to reach a person standing 3 km from the surface of the nuclear fireball, with the exact time of arrival being dependent on the speed of sound in air in their area. The time delay between the moment of an explosion's flash and the arrival of the slower moving blast wave is analogous to the commonly experienced time delay between the observation of a flash of lightning and the arrival of thunder during a lightning storm, thus at the distances that the advice would be most effective, there would be more than ample amounts of time to take the prompt countermeasure of 'duck and cover' against the blast's direct effects and flying debris. For very large explosions it can take 30 seconds or more, after the silent moment of flash, for a potentially dangerous blast wave over-pressure to arrive at, or hit, your position.\n\nIt is also worth noting that the graphs of lethal ranges of weapon effects as a function of yield, that are commonly encountered, are the unobstructed \"open air\", or \"free air\" ranges that assume amongst other things, a perfectly level target area, no passive shielding such as attenuating effects from urban terrain masking, e.g. skyscraper shadowing, and so on. Therefore, they are thus considered to present an overestimate of the lethal ranges that would be encountered in an urban setting in the real world, with this being most evident following a ground burst with explosive yield similar to first generation nuclear weapons.\n\nTo highlight the effect that being indoors, and especially below ground can make, despite the lethal open air radiation, blast and thermal zone extending well past her position at Hiroshima, Akiko Takakura survived the effects of the 16 kt atomic bomb at a distance of 300 meters from ground zero, sustaining only minor injuries, due in greatest part to her position in the lobby of the Bank of Japan, a reinforced concrete building, at the time of the nuclear explosion, and to highlight the protection conferred to an individual who is below ground during a nuclear air burst, Eizo Nomura survived the same blast at Hiroshima at a distance of 170 meters from ground zero. Nomura, who was in the basement of what is now known as the \"rest house\", also a reinforced concrete building, lived into his early 80s.\n\nIn contrast to these cases of survival, the unknown person sitting outside on the steps of the Sumitomo Bank next door to the Bank of Hiroshima on the morning of the bombing—and therefore fully exposed—suffered what would have eventually been lethal third- to fourth-degree burns from the near instant nuclear weapon \"flash\" if they hadn't been killed by the slower moving blast wave when it reached them approximately one second later.\n\nTo elucidate the effects on lying flat on the ground in attenuating a weapons blast, Miyoko Matsubara, one of the Hiroshima maidens, when recounting the bombing in an interview in 1999, said that she was outdoors and \"less than 1 mile\" from the hypocenter of the Little Boy bomb. Upon observing the nuclear weapons silent flash she quickly lay flat on the ground, while those who were standing directly next to her, and her other fellow students, had simply disappeared from her sight when the blast wave arrived and blew them away.\n\nPosition of the body can have a considerable influence in protection from blast effects. Lying prone on the ground will often materially lessen direct blast effects because of the protective defilade effects of irregularities in the ground surface. Ground also tends to deflect some of the blast forces upward. Standing close to a wall, even on the side from which the blast is coming, also lessens some of the effect. Orientation of the body also affects severity of the effect of blast. Anterior exposure of the body may result in lung injury, lateral position may result in more damage to one ear than the other, while minimal effects are to be anticipated with the posterior surface of the body (feet) toward the source of the blast.\n\nThe human body is more resistant to sheer overpressure than most buildings, however, the powerful winds produced by this overpressure, as in a hurricane, are capable of throwing human bodies into objects or throwing debris at high velocity, both with lethal results, rendering casualties highly dependent on surroundings. For example, Sumiteru Taniguchi recounts that, while clinging to the tremoring road surface after the Fat Man detonation, he witnessing another child being blown away, the destruction of buildings around him and stones flying through the air. Similarly, Akihiro Takahashi and his classmates were blown by the blast of Little Boy by a distance of about 10 meters, having survived due to not colliding with any walls etc. during his flight through the air. Likewise, Katsuichi Hosoya had a near identical testimony.\n\nIn the testimony of Dr. Hiroshi Sawachika, although he was sufficiently far away from the Hiroshima bomb himself and not behind a pane of window glass when the blast wave arrived, those in his company who were had serious blast injury wounds, with broken glass and pieces of wood stuck into them.\n\nAccording to the 1946 book \"Hiroshima\" and other books which cover both bombings, in the days between the atomic bombings of Hiroshima and Nagasaki, some survivors of the first bombing went to Nagasaki and told others what they had done to survive after the initial \"flash\" and informed them about the particularly dangerous threat of imploding window glass. As a result of this timely warning, a number of lives were saved in the initial blast at Nagasaki. However these informed people were the exception and in both Hiroshima and Nagasaki many died while searching the skies, curious to locate the source of the brilliant flash.\n\nWhen people are indoors, running to windows to investigate the source of bright flashes in the sky still remains a common and natural response to experiencing a bright flash. Thus, although the advice to duck and cover is over half a century old, ballistic glass lacerations caused the majority of the 1000 human injuries following the Chelyabinsk meteor air burst of February 15, 2013.\n\nThe dangers of viewing explosions behind window glass was known of before the Atomic Age began, being a common source of injury and death from large chemical explosions. In the accidental Halifax Explosion of 1917, an ammunition ship exploded with the force of roughly 2.9 kilotons of TNT, and injured the eyes and faces of hundreds of people who looked out of their windows after seeing a bright flash. Every window in the city of Halifax, Nova Scotia, was shattered.\n\nDuring the 2013 Chelyabinsk meteor explosion, a fourth-grade teacher in Chelyabinsk, Yulia Karbysheva, saved 44 children from potentially life-threatening ballistic window glass cuts by ordering them to hide under their desks when she saw the flash. Despite not knowing the origin of the intense flash of light, she ordered her students to execute a duck and cover drill. Ms. Karbysheva, who herself did not duck and cover but remained standing, was seriously lacerated when the explosion's blast wave arrived, and window glass blew in, severing a tendon in one of her arms; however, not one of her students, who she ordered to hide under their desks, suffered a cut. A follow up study of the effects of the meteor airburst determined that the windows most prone to breaking when exposed to a blast overpressure are those of school buildings, which tend to be large in area.\n\nWhile the bombings of Hiroshima and Nagasaki demonstrated that the urban area of glass breakage is nearly 16 times greater than the area of significant structural/building damage, although improved building codes since then may contribute to better building survival, there would be a higher likelihood of glass breakage and therefore potential injury/death for people near windows because many modern buildings have larger windows.\n\nThe advice to \"cover\" one's exposed skin with anything that can cast a shadow, like the picnic blanket and newspaper used by the family in the film, may seem absurd at first when one considers the capabilities of a nuclear weapon, but even the thinnest of barriers such as cloth, or plant leaves would reduce the severity of burns on the skin from the thermal radiation (the thermal radiation/\"flash\" is light, similar in average emission spectrum/color to sunlight, emitting in the ultraviolet, visible light, and infrared range but with a higher light intensity than sunlight, and this combination of light rays is capable of delivering radiant burning energy to exposed skin areas. While the duration of emittance of this burning thermal radiation, which can be experienced by people within range, increases with yield, it is usually at least a few seconds long.)\n\nGreat importance is given to closing eyelids and covering the location of the eyes as temporary and permanent flash blindness is a risk potential without this covering, especially at night.\n\nA photograph taken about 1.3 km from the hypocenter of the Hiroshima bomb explosion showed that the shadowing effect of leaves from a nearby shrub protected a wooden utilities pole from charring discoloration due to the burst of thermal radiation, however the rest of the telephone pole, not under the protection of the leaves, was charred almost completely black. The difference in required flash-energy necessary to produce essentially immediate, though transitory, non-propagating flaming, and that required to achieve a continued self-sustained propagating flaming are orders of magnitude in difference for most combustible materials. In the case of untreated timber it is largely dependent on the depth of char.\n\nWhile not designed for those faced with low-yield neutron bombs or for those who are, in general, so close to the nuclear fireball that prompt/initial radiation would be life-threatening in the short-medium term, ducking and covering would nevertheless slightly reduce exposure to the \"initial\" gamma rays, specifically the portion emitted after the first flash of visible light. The initial gamma rays are defined as those emitted from the fireball & following mushroom cloud which can reach personnel on the ground for a total of approximately 1 minute, at which point the intensity of the radiation has diminished and the atmosphere itself is thick enough to act as full shielding.\n\nAs approximately half of these gamma rays are emitted in the first second and the other half, over the following 59 alongside gamma rays being mostly emitted in a straight line, people laying on the ground will more likely have obstacles serving as radiation protection such as building walls, foundations, car engines, etc. between their bodies and the radiation emitted from both the fireball and the accompanying lower levels of radiation that continue to arrive at the ground for about 1 minute, during the mushroom cloud phase, which is termed \"cloudshine\". It would also give protection from the even smaller fraction of radiation that changes direction and is randomly reflected and scattered by the air/\"skyshine\". Approximately \"One and one half inches\"/37 mm of steel will reduce gamma dose by half. Its half-value thickness.\n\nThe effective gamma ray energy of the cloudshine is not especially high, 200 KeV.\n\nUnlike the relatively low-yield, or low explosive energy \"A-bombs\" dropped on Hiroshima and Nagasaki, which did result in a sizable proportion of injuries from prompt radiation, higher yield \"hydrogen bombs\" (thermonuclear weapons) are not expected to result in very many such injuries – as the range at which the ionizing radiation from higher yield devices is of primary concern, is already well inside the hyper-lethal blast and flash burn areas.\n\nApart from the intrinsic \"prompt effects\" of nuclear detonations, that of thermal flash, blast and initial radiation releases, if any part of the fireball of the nuclear detonation contacts the ground, in what is known as a surface burst, another, comparatively slowly increasing, radiation hazard will also begin to form in the immediate area.\n\nPutting aside the possibility of the detonation occurring during an already established heavy rain-storm, the formation of this life-threatening \"delayed nuclear radiation\" manifests only when the altitude, or \"height of burst\" of the explosion, is such that both the fireball and the buoyant updrafts it creates, sufficiently heats and lifts the soil that was below it into the core of the mushroom cloud. Once there, the very hot radioactive isotope products of the nuclear reactions that produced the explosion, begin to coalesce with the cooler and denser soil. Upon cooling, this mixture begins to locally \"fall-out\" or precipitate-out of the mushroom cloud, falling back to the surface of the earth, near to the point of detonation, over the next few minutes and hours.\n\nWhile the duck and cover countermeasure, in its most basic form, offers a small to negligible protection against fallout, the technique assumes that after the effects of the blast and initial radiation subside, with the latter of which being no longer a threat after about \"twenty seconds\" to 1 minute post detonation, a person who ducks and covers will realize when it is wise to cease ducking and covering (after the blast and initial radiation danger has passed) and to then seek out a more sheltered area, like an established or improvised fallout shelter to protect themselves from the ensuing potential local fallout danger, as depicted in the film.\n\nAfter all, \"Duck and Cover\" is a first response countermeasure only, in much the same way that \"Drop, Cover and Hold On\" is during an earthquake, with the advice having served its purpose once the earthquake has passed, and possibly other dangers—like a tsunami or fallout—may be looming, which then require movement to high ground and radiation protection, respectively.\n\nHowever, if such a shelter is unavailable, the person should then be advised to follow the Shelter in Place protocol, or if given, emergency evacuation advice. Evacuation orders would entail exiting the area completely by following a path perpendicular to the wind direction, and therefore perpendicular to the path of the fallout plume. Taking upper atmospheric winds into account, surface winds alone are not to be depended upon as indicative of the direction of fallout movement. \"Sheltering in place\" is staying indoors, in a preferably sealed tight basement, or internal room, for a number of hours, with the oxygen supply available in such a scenario being more than sufficient for 3+ hours in even the smallest average room, under the assumption that the improvised seal is perfect, until carbon dioxide levels begin to reach unsafe values and necessitate room unsealing for a number of minutes to create a room air change.\n\nIn the era the advice was originally given, the most common nuclear weapons were weapons comparable to the US Fat Man and Soviet Joe-1 in yield. The most far-reaching dangers that initially come from the nuclear explosion of this, and higher, yield weapons as airbursts, are the initial flash/heat and blast effects and not from fallout. This is due to the fact that when nuclear weapons are detonated to maximize the range of building destruction, that is, maximize the range of surface blast damage, an airburst is the preferred nuclear fuzing height, as it exploits the mach stem phenomenon. This phenomenon of a blast wave occurs when the blast reaches the ground and is reflected. Below a certain reflection angle the reflected wave and the incident wave merge and form a reinforced horizontal wave; this is known as the 'Mach stem' (named after Ernst Mach) and is a form of constructive interference and consequently extends the range of high pressure. Air-burst fuzing also increases the range that people's skin will have a line-of-sight with the nuclear fireball. However, as a result of the high altitude of the explosion, most of the radioactive bomb debris is dispersed into the stratosphere, with a great column of air therefore placed between the vast majority of the bomb debris/fission reaction products and people on the ground for a number of crucial days before it falls out of the atmosphere in a comparatively dilute fashion. This \"delayed fallout\" is henceforth not an immediate concern to those near the blast. On the other hand, the only time that fallout is rapidly concentrated in a potentially lethal fashion in the local/regional area around the explosion is when the nuclear fireball makes contact with the ground surface, with an explosion that does so, being aptly termed a surface burst. For example, in the Operation Crossroads tests of 1946 on Bikini Atoll, using two explosive devices of the same design and yield, the first, Test Able (an air burst) had little local fallout, but the infamous Test Baker (a near surface shallow underwater burst) left the local test targets badly contaminated with radioactive fallout.\n\nWidespread radioactive fallout itself was not recognized as a threat among the public at large before 1954, until the widely publicized story of the 15-megaton surface burst of the experimental test shot Castle Bravo on the Marshall Islands. The explosive yield of the Castle Bravo device \"the Shrimp\" was unexpectedly high, and therefore correspondingly higher amounts of local fallout were produced. When this arrived at their location carried by the wind, this caused the 23 crew members on a Japanese fishing boat known as the Lucky Dragon to come down with acute radiation sickness with varying degrees of seriousness and due to complications in the treatment of the ship's radio operator months after the exposure, resulted in his death.\n\nIt is, however, unlikely that a well-funded belligerent with nuclear weapons would waste their weapons with fuzing to explode below or on the surface, as both test shot Baker, and Castle Bravo were respectively. Instead, to maximize the range of city blast destruction and immediate death, an air burst is preferred, as the ≈500 meter explosion heights of the only nuclear weapons used on cities, Little Boy and Fat Man also attest to. Moreover, with air bursts the total amount of radiation contained in the fallout, in units of activity/becquerel, is somewhat less than the total that would be released from a surface or subsurface burst, as in comparison, depending on the height of burst, little to no neutron activation or neutron induced gamma activity of soil occurs from air bursts.\nTherefore, the initial danger from concentrated local/'early' fallout (which takes on the color of the soil around the fireball, commonly with a dusty pumice or ash-like appearance, as experienced by the crew of the Lucky Dragon) remains low in a global nuclear war scenario. Instead the fallout most likely to be encountered by most survivors in this scenario is expected to be the less dangerous but widely spread global/'late' fallout. An air burst at optimum height will produce a negligible amount of early fallout.\n\nA notable comparison to underline this is found when one compares the 50 megaton air-burst Tsar Bomba, which produced no concentrated local/early fallout, and thus no known deaths from radiation, with the surface burst of the 15 megaton Castle Bravo, which in comparison, due to the local fallout produced, was implicated in the death of 1 of 23 crew on the Lucky Dragon and made the entire Bikini Atoll unfit for further nuclear testing until enough time elapsed and the intensity of the radiation field had decayed to acceptable levels.\nFurthermore, regardless of if a nuclear attack on a city is of the surface or air-burst variety or a mixture of both, the advice to shelter in place, in the interior of well-built homes, or if available, fallout shelters, as suggested in the film \"Duck and Cover\", will drastically reduce one's chance of absorbing a hazardous dose of radiation. A real-world example of this occurred after the Castle Bravo test where, in contrast to the crew of the Lucky Dragon, the firing crew that triggered the explosion safely sheltered in their firing station until after a number of hours had passed and the radiation levels outside fell to dose rate levels safe enough for an evacuation to be considered. The comparative safety experienced by the Castle Bravo firing crew served as a proof of concept to civil defense personnel that shelter in place (or \"buttoning up\" as it was known then) is an effective strategy in mitigating the potentially serious health effects of local fallout.\n\nThe minimum typical protection factor of the fallout shelters in US cities is 40 or more. In many cases these shelters are nothing more than the interior of pre-existing well-built buildings that have been inspected, and following their protection factors being calculated, re-purposed as fallout shelters.\n\nA protection factor of at least 40 means that the radiation shielding provided by the shelter reduces the radiation dose experienced by at least 40 times that which would be experienced outside the shelter with no shielding. \"Protection factor\" is equivalent to the modern term \"dose reduction factor\".\n\nDuring the first hour after a nuclear explosion, radioactivity levels drop precipitously. Radioactivity levels are further reduced by about 90% after another 7 hours and by about 99% after 2 days. An accurate rule of thumb, applicable in the time-period of days to a few weeks post-detonation which approximates the radioactive dose rate generated by the decay of the myriad of isotopes present in nuclear fallout, is the \"7/10 rule\". The rule states that for each 7-fold increase in time the dose rate drops by a factor of 10. For example, assuming the fallout process has ended 24 hours post detonation and the dose rate would be lethal if a few hours of exposure occurred, 50 roentgens per hour, then 7 days after detonation the dose rate will be 5 R/hr and 49 days after detonation (7×7 days) the dose rate will be 0.5 R/hr at which point no special precautions would need to be taken and venturing outside into that dose rate for an hour or two would pose a close to negligible health hazard, thus permitting an evacuation to be done with acceptable safety to a known contamination free zone. Following a surface-burst nuclear detonation, approximately 80 percent of the fallout would be deposited on the ground during the first 24 hours.\n\nSome agencies that promoted \"evacuate immediately\" guidance as a response to potentially lethal fallout arriving, advice which may have been influenced by these agencies assuming simplistic single wind-driven cigar/Gaussian shaped fallout contours would be representative of reality, have since retracted this advice. This can actually result in higher radiation exposures as it would put people outdoors and in harm's way when the radiation levels would be highest. The Modeling and Analysis Coordination Working Group (MACWG) – which was set up to resolve conflicting advice given by various agencies, has reaffirmed that the best blanket advice that would reduce the number of casualties by the greatest amount is: \"Early, adequate sheltering followed by informed, delayed evacuation.\"\n\nExpert advice published in the 2010 document \"Planning Guidance for Response to a Nuclear Detonation\" is to shelter in place, in an area away from building fires, for at least 1 to 2 hours following a nuclear detonation and fallout arriving, and the greatest benefit, assuming personnel are in a building with a high protection factor, is sheltering for no less than 12 to 24 hours before evacuation. Therefore, sheltering for the first few hours can save lives. Indeed, death and injury from local fallout is regarded by experts as the most preventable of all the effects of a nuclear detonation, being simply dependent on if personnel know how to identify an adequate shelter when they see one and enter one quickly, with the number of potential people saved being cited as in the hundreds of thousands. Or even higher if the remaining occupants of the city are made aware of the contaminated areas, by emergency systems, within hours of the event's aftermath. In 2009 to 2013 a further iteration on sheltering-in-place was made to determine the optimal improvised fallout-shelter-residence-times following a nuclear detonation, with computer analysis, and including a summary of prior studies and guidance. It was found that individuals should quickly get into the best intact building at least under 5 minutes distant in travel time following the detonation, and they should stay there for at least 30 minutes before venturing out to find a shelter with a higher protection factor but that is a greater travel time away than 10 minutes. However, although this would be effective in cases where the initial building protection factor is less than about 10, it requires a high degree of individual situational awareness that may be optimistic to assume following the shock of a nuclear detonation. If a building with a PF of 20 or more is nearby, such as the fallout shelters depicted in the film, in the vast majority of fallout circumstances, it would not be advisable to leave it until 3+ hours have elapsed following the initial arrival of the local fallout.\n\nFollowing a single IND (improvised nuclear device) detonation in the US, the National Atmospheric Release Advisory Center (NARAC) would, within minutes to at most hours, after the detonation have a reliable prediction of the fallout plume size and direction. When armed with this prediction they would then begin attempting to corroborate this with readings from radiation survey meter equipment that would fly over close to the ground in the affected area by means of helicopter or drone (UAV) aircraft on material intelligence gathering missions, which would also follow within tens of minutes to at most hours after the detonation.\n\nOnce a general outline and direction of the fallout is determined, disseminating this information to citizens sheltering-in-place would soon follow, by means of loudspeaker, radio, cell phone etc., with a \"Fallout App\" containing maps for smart phones being regarded as an area of interest so that survivors don't inadvertently evacuate downwind further into harm's way. A number of questions the affected public are likely to have after a nuclear detonation have been compiled and pre-answered to help communications in the immediate aftermath.\n\nIn respect to the other non-lethal weapon effects from an IND detonated on or near the surface, the detonation's blast wave would likely produce a momentary electric grid blackout due to the loss of a large portion of a city's electrical equipment drawing power/electrical load, while the electromagnetic pulse (EMP) from a surface/ground-burst explosion would cause little damage outside the blast area, so cell phone towers that survive the blast should be capable of carrying communications. But if communications during the 9/11 attacks or after a major hurricane are anything to go by, and the cell phone network towers survive, the service would be overloaded (a mass call event) and thereby made useless soon after; however, if prior arrangements between the cell network and emergency responders are made to give them priority and bar access to all other individuals, then it may be an effective service.\n\nThe Civil Defense (CD) shelters, as depicted in the film, were stocked for such an eventuality. They contained amongst other things, at least one ruggedized CDV-715 radiation survey meter and one CD emergency radio receiver which would respectively be used to facilitate a safe delayed evacuation, regardless of outside help though if communications continued, the radio receiver was to inform them of the outside situation as it developed.\n\nThe dubious assumption that \"only the cockroaches\" would survive the post-war fallout environment was frequently used in an attempt to criticize \"Duck and Cover\" during the height of the cold war, contextually at a time when discussion of a total war involved the much greater US-Soviet arsenal of nuclear weapons that were then in existence. However even at that time, this assumption was shown to be misled, as scientifically detailed in areas including the 1988 book \"Would the Insects Inherit the Earth and Other Subjects of Concern to Those Who Worry About Nuclear War\".\n\nIn material terms, the primary life-threatening risk survivors and downwinders could face in the long-term after a nuclear explosion or war, is the \"nuclear famine\" issue, the potential continuation of hostilities by conventional warfare and radioactive contamination of the food and water supplies, disrupting the normal distribution and consumption, of these vital goods.\n\nCold-War continuity of government planners and civil defense organizations in general have always had this disruption, or \"nuclear famine\" issue in mind, as widespread infrastructure destruction producing starvation conditions was also seen during and after WWII. Papers such as \"On Reorganizing After Nuclear Attack\", and \"Survival of the relocated population of the U.S. after a nuclear attack\" by Nobel Prize winner, Eugene Wigner, detail the thought and attention that went into long-term survival, relocation and reconstruction.\n\nNumerous human and agricultural decontamination countermeasures exist for the two most persistent and biologically significant isotopes, cesium-137, strontium-90 and long-lived fallout contamination in general, with the most visible and immediate act that will prevent a potentially large dose to the public, taking the form of using shielded bulldozers to skim off the layer of topsoil that the fallout had settled on, a restorative practice that was fielded upon the creation of Lake Chagan. The creation of human decontamination tents at the entrances of buildings and when lower levels of risk exist, the use of clean room air showers as a form of contamination control to prevent the spread of radionuclides that adhere to dust, into building interiors, would also be advisable to reduce the elevated risk of radiation induced cancer that would otherwise occur. Air showers may be paired with electrostatic precipitators to attract the dust to collection plates, forestalling a re-suspension that may otherwise be inhaled. Moreover use of the open access radioecology research on decontamination and conventional agriculture in the Chernobyl-Polesie State Radioecological Reserve and around the Fukushima accident, would both be implemented in the event of any widespread fallout contamination, with particular emphasis on bioremediation of radionuclides from soil and aquifers. Although less of a hazard than external exposure, for internal decontamination, that may be required after assessment in a whole-body counting session. The chelation therapies of AFCF/\"Giese salt\", Radiogardase and DPTA are effective.\n\nThe technology developed and deployed due to the Fukushima reactor-water decontamination mandate, includes the mobile reverse osmosis \"Landysh\" water treatment ship, the zeolite based \"Actiflo\", the \"SARRY\" ion exchange cesium removal system, based on silicotitanate \"IONSIV\" crystalline rock, and most recently the 62 multi-nuclide removal system (NURES), frequently referred to as the Advanced Liquid Processing System (ALPS). In 2016 tritiated water also began to be filtered.\n\nResearchers at the American Chemical Society have further suggested that aquaponics would be an ideal socially-acceptable solution in the post-contamination environment, as it does not use soil to grow fish and vegetables, thus completely alleviating the radiophobia surrounding food that always follows long-lived contamination incidents. Others who have approached the problem from a far more extreme view, assuming far worse events such as comet impacts, as discussed in the book \"Feeding Everyone No Matter What\", have suggested; natural-gas-digesting bacteria the most well known being Methylococcus capsulatus, that is presently used as a feed in Fish farming, Bark bread a long-standing famine food utilizing the edible inner bark of trees once a part of Scandinavian history during the Little Ice Age and the expansion of leaf protein concentrate and larger scale wood digesting fungiculture for fungal protein, with the most common of which being shittake mushrooms and honey fungi, as they do not need sunlight or soil to grow. More advanced techniques mentioned, that are not presently economical also include variations of wood or cellulosic biofuel production, which typically already creates edible sugars/xylitol from inedible cellulose, as an intermediate product before the final step of alcohol generation.\n\nSome historians and filmmakers, exemplified by the 1982 \"The Atomic Cafe\", have thus far sought to dismiss civil defense advice as mere propaganda, despite, as other historians have found, detailed scientific research programs laying behind the much-mocked government civil defense pamphlets of the 1950s and 1960s, including the prompt advice of ducking and covering.\n\nIn U.S. Army training, soldiers are taught to immediately fall down, covering face and hands in much the same way as is described by the advice to duck and cover.\n\nThe exercises of Cold War civil defense are seen by historian Guy Oakes in 1994, as having less practical use than psychological use: to keep the danger of nuclear war high on the public mind, while also attempting to assure the American people that something could be done to defend against nuclear attack. However, according to contemporary Cold War civil defense pamphlets, like \"Civil Defence: Why we need it\" released in 1981, civil defense countermeasures were presented as analogous to seat belts, and that the suggestion that knowing what steps to take in the \"slight\" possibility that a nuclear explosion occurs in your region, keeps such calamities high on the public mind, is \"like saying people who wear seat belts are expecting to have more crashes than those who do not\", and as with a seat belt, there is never a suggestion that if the countermeasure were implemented, it would save everyone.\n\nMoreover, civil defense was not solely a US-UK or nuclear club phenomenon; countries with long histories of neutrality, such as Switzerland, are \"foremost in their civil defence precautions.\" The Swiss civil defense network has an overcapacity of nuclear fallout shelters for the country's population size, and by law, new homes must still be built with a fallout shelter as of 2011.\n\nDucking and covering does have certain applications in other, more natural disasters. In states prone to tornadoes, school children are urged to \"duck and cover\" against a solid inner wall of a school, if time does not permit seeking better shelter—such as a storm cellar—during a tornado warning. The tactic is also widely practiced in schools in states along the West Coast of the United States, where earthquakes are commonplace. Ducking and covering in either scenario would theoretically afford significant protection from falling or flying debris.\n\nIn an earthquake, which are generally of a natural tectonic plate origin (although they can be artificially generated by the detonation of a nuclear explosive device in which sufficient energy is transmitted into the ground, with an extreme case to serve as an example of this phenomenon being the Operation Grommet \"Cannikin\" test of the 5 megaton W71 warhead exploded deep underground on Amchitka Island in 1971, which produced a seismic shock quake of 7.0 on the Richter magnitude scale) people are encouraged, regardless of the cause of the quake, to \"drop, cover, and hold on\": to get underneath a piece of furniture, cover their heads and hold on to the furniture. This advice also encourages people not to run out of a shaking building, because a large majority of earthquake injuries are due to broken bones from people falling and tripping during shaking. While it is unlikely that \"drop, cover and hold on\" will protect against a building collapse, in earthquake-prone areas in the United States building codes require that buildings withstand quakes up to an expected magnitude enough to allow evacuation after shaking stops. and thus a building collapse of these structures (even during an earthquake) is rare. \"Drop, cover and hold on\" may not be appropriate for all locations or building types, but the Red Cross advises it is the appropriate emergency response to an earthquake in the United States.\n\n\n"}
{"id": "7848092", "url": "https://en.wikipedia.org/wiki?curid=7848092", "title": "Eastern Arabia", "text": "Eastern Arabia\n\nEastern Arabia was historically known as Bahrain () until the 18th century. This region stretched from the south of Basra along the Persian Gulf coast and included the regions of Bahrain, Kuwait, Al-Hasa, Qatif, United Arab Emirates, Qatar, Southern Iraq, and Northern Oman. The entire coastal strip of Eastern Arabia was known as “Bahrain” for ten centuries.\n\nUntil very recently, the whole of Eastern Arabia, from southern Iraq to the mountains of Oman, was a place where people moved around, settled and married unconcerned by national borders. The people of Eastern Arabia shared a culture based on the sea; they are seafaring peoples.\n\nThe Arab states of the Persian Gulf are solely Eastern Arabia, the borders of the Arabic-speaking Gulf do not extend beyond Eastern Arabia. The modern-day states of Bahrain, Kuwait, Oman, Qatar and UAE are the archetypal Gulf Arab states. Saudi Arabia is often considered a Gulf Arab state although most Saudis do not live in Eastern Arabia.\n\nIn Arabic, \"Bahrayn\" is the dual form of \"bahr\" (“sea”), so \"al-Bahrayn\" means \"the Two Seas\". However, which two seas were originally intended remains in dispute. The term appears five times in the Qur'an, but does not refer to the modern islandoriginally known to the Arabs as “Awal”but rather to the oases of al-Katif and Hadjar (modern al-Hasa). It is unclear when the term began to refer exclusively to the Awal islands, but it was probably after the 15th century. Today, Bahrain's “two seas” are instead generally taken to be the bay east and west of the coast, the seas north and south of the island, or the salt and fresh water present above and below the ground. In addition to wells, there are places in the sea north of Bahrain where fresh water bubbles up in the middle of the salt water, noted by visitors since antiquity.\n\nAn alternate theory offered by Al-Hasa was that the two seas were the Great Green Ocean and a peaceful lake on the mainland; still another provided by al-Jawahari is that the more formal name \"Bahri\" (lit. “belonging to the sea”) would have been misunderstood and so was opted against. The term \"Gulf Arab\" solely refers, geographically, to inhabitants of eastern Arabia. The term \"Khaleejis\" is often misused to identify all the inhabitants of the Arabian Peninsula.\n\nThe inhabitants of Eastern Arabia's Gulf coast share similar cultures and music styles such as fijiri, sawt and liwa. The most noticeable cultural trait of Eastern Arabia's Gulf Arabs is their orientation and focus towards the sea. Maritime-focused life in the small Gulf Arab states has resulted in a sea-oriented society where livelihoods have traditionally been earned in marine industries.\n\nThe Arabs of Eastern Arabia speak a dialect known as Gulf Arabic. Most Saudis do not speak Gulf Arabic because most Saudis do not live in Eastern Arabia. There are approximately 2 million Gulf Arabic speakers in Saudi Arabia, mostly in the coastal eastern region. Before the GCC was formed in 1981, the term \"Khaleeji\" was solely used to refer to the inhabitants of Eastern Arabia.\n\nIn pre-Islamic times, the population of Eastern Arabia consisted of partially Christianized Arabs, Arab Zoroastrians, Jews and Aramaic-speaking agriculturalists. Some sedentary dialects of Eastern Arabia exhibit Akkadian, Aramaic and Syriac features. The sedentary people of pre-Islamic Bahrain were Aramaic speakers and to some degree Persian speakers, while Syriac functioned as a liturgical language.\n\nDilmun appears first in Sumerian cuneiform clay tablets dated to the end of fourth millennium BC, found in the temple of goddess Inanna, in the city of Uruk. The adjective Dilmun is used to describe a type of axe and one specific official; in addition there are lists of rations of wool issued to people connected with Dilmun.\n\nDilmun was mentioned in two letters dated to the reign of Burna-Buriash II (c. 1370 BC) recovered from Nippur, during the Kassite dynasty of Babylon. These letters were from a provincial official, Ilī-ippašra, in Dilmun to his friend Enlil-kidinni in Mesopotamia. The names referred to are Akkadian. These letters and other documents, hint at an administrative relationship between Dilmun and Babylon at that time. Following the collapse of the Kassite dynasty, Mesopotamian documents make no mention of Dilmun with the exception of Assyrian inscriptions dated to 1250 BC which proclaimed the Assyrian king to be king of Dilmun and Meluhha. Assyrian inscriptions recorded tribute from Dilmun. There are other Assyrian inscriptions during the first millennium BC indicating Assyrian sovereignty over Dilmun. Dilmun was also later on controlled by the Kassite dynasty in Mesopotamia.\n\nOne of the early sites discovered in Bahrain indicate that Sennacherib, king of Assyria (707–681 BC), attacked northeast Persian Gulf and captured Bahrain. The most recent reference to Dilmun came during the Neo-Babylonian dynasty. Neo-Babylonian administrative records, dated 567 BC, stated that Dilmun was controlled by the king of Babylon. The name of Dilmun fell from use after the collapse of Neo-Babylon in 538 BC.\n\nThere is both literary and archaeological evidence of extensive trade between Ancient Mesopotamia and the Indus Valley civilization (probably correctly identified with the land called \"Meluhha\" in Akkadian). Impressions of clay seals from the Indus Valley city of Harappa were evidently used to seal bundles of merchandise, as clay seal impressions with cord or sack marks on the reverse side testify. A number of these Indus Valley seals have turned up at Ur and other Mesopotamian sites.\n\nThe “Persian Gulf” types of circular, stamped (rather than rolled) seals known from Dilmun, that appear at Lothal in Gujarat, India, and Failaka, as well as in Mesopotamia, are convincing corroboration of the long-distance sea trade. What the commerce consisted of is less known: timber and precious woods, ivory, lapis lazuli, gold, and luxury goods such as carnelian and glazed stone beads, pearls from the Persian Gulf, shell and bone inlays, were among the goods sent to Mesopotamia in exchange for silver, tin, woolen textiles, olive oil and grains. Copper ingots from Oman and bitumen which occurred naturally in Mesopotamia may have been exchanged for cotton textiles and domestic fowl, major products of the Indus region that are not native to Mesopotamia. Instances of all of these trade goods have been found. The importance of this trade is shown by the fact that the weights and measures used at Dilmun were in fact identical to those used by the Indus, and were not those used in Southern Mesopotamia.\n\nMesopotamian trade documents, lists of goods, and official inscriptions mentioning Meluhha supplement Harappan seals and archaeological finds. Literary references to Meluhhan trade date from the Akkadian, the Third Dynasty of Ur, and Isin-Larsa Periods (c. 2350 – 1800 BC), but the trade probably started in the Early Dynastic Period (c. 2600 BC). Some Meluhhan vessels may have sailed directly to Mesopotamian ports, but by the Isin-Larsa Period, Dilmun monopolized the trade. The Bahrain National Museum assesses that its \"Golden Age\" lasted from c. 2200 BC to 1600 BC. Discoveries of ruins under the Persian Gulf maybe of Dilmun.\n\nIn the Mesopotamian epic poem Epic of Gilgamesh, Gilgamesh had to pass through Mount Mashu to reach Dilmun, Mount Mashu is usually identified with the whole of the parallel Lebanon and Anti-Lebanon ranges, with the narrow gap between these mountains constituting the tunnel.\n\nDilmun, sometimes described as “the place where the sun rises” and “the Land of the Living”, is the scene of some versions of the Sumerian creation myth, and the place where the deified Sumerian hero of the flood, Utnapishtim (Ziusudra), was taken by the gods to live forever. Thorkild Jacobsen's translation of the Eridu Genesis calls it \"Mount Dilmun\" which he locates as a \"“faraway, half-mythical place”\".\n\nDilmun is also described in the epic story of Enki and Ninhursag as the site at which the Creation occurred. The promise of Enki to Ninhursag, the Earth Mother:\nFor Dilmun, the land of my lady's heart, I will create long waterways, rivers and canals, whereby water will flow to quench the thirst of all beings and bring abundance to all that lives.\nNinlil, the Sumerian goddess of air and south wind had her home in Dilmun. It is also featured in the Epic of Gilgamesh.\n\nHowever, in the early epic \"“Enmerkar and the Lord of Aratta”\", the main events, which center on Enmerkar's construction of the ziggurats in Uruk and Eridu, are described as taking place in a world \"before Dilmun had yet been settled\".\n\nGerrha (), was an ancient city of Eastern Arabia, on the west side of the Persian Gulf. More accurately, the ancient city of Gerrha has been determined to have existed near or under the present fort of Uqair. This fort is 50 miles northeast of Al-Hasa in the Eastern Province of Saudi Arabia. This site was first proposed by R E Cheesman in 1924.\n\nGerrha and Uqair are archaeological sites on the eastern coast of the Arabian Peninsula only 60 miles from the ancient burial grounds of Dilmun on the island of Bahrain.\n\nPrior to Gerrha, the area belonged to the Dilmun civilization, which was conquered by the Assyrian Empire in 709 BC. Gerrha was the center of an Arab kingdom from approximately 650 BC to circa 300 AD. The kingdom was attacked by Antiochus III the Great in 205-204 BC, though it seems to have survived. It is currently unknown exactly when Gerrha fell, but the area was under Sassanid Persian control after 300 AD.\n\nGerrha was described by Strabo as inhabited by Chaldean exiles from Babylon, who built their houses of salt and repaired them by the application of salt water. Pliny the Elder (lust. Nat. vi. 32) says it was 5 miles in circumference with towers built of square blocks of salt.\n\nGerrha was destroyed by the Qarmatians in the end of the 9th century where all inhabitants were massacred (300,000). It was 2 miles from the Persian Gulf near current day Hofuf. The researcher Abdulkhaliq Al Janbi argued in his book that Gerrha was most likely the ancient city of Hajar, located in modern-day Al Ahsa, Saudi Arabia. Al Janbi's theory is the most widely accepted one by modern scholars, although there are some difficulties with this argument given that Al Ahsa is 60 km inland and thus less likely to be the starting point for a trader's route, making the location within the archipelago of islands comprising the modern Kingdom of Bahrain, particularly the main island of Bahrain itself, another possibility.\n\nVarious other identifications of the site have been attempted, Jean Baptiste Bourguignon d'Anville choosing Qatif, Carsten Niebuhr preferring Kuwait and C Forster suggesting the ruins at the head of the bay behind the islands of Bahrain.\n\nBahrain was referred to by the Greeks as Tylos, the centre of pearl trading, when Nearchus came to discover it serving under Alexander the Great. From the 6th to 3rd century BC Bahrain was included in Persian Empire by Achaemenians, an Iranian dynasty. The Greek admiral Nearchus is believed to have been the first of Alexander's commanders to visit this islands, and he found a verdant land that was part of a wide trading network; he recorded: “That in the island of Tylos, situated in the Persian Gulf, are large plantations of cotton tree, from which are manufactured clothes called \"sindones\", a very different degrees of value, some being costly, others less expensive. The use of these is mostly confined to India, but extends also to Arabia.” The Greek historian, Theophrastus, states that much of the islands were covered in these cotton trees as their major economic activity and that Tylos was famous for exporting walking canes engraved with emblems that were customarily carried in Babylon. Ares was also worshipped by the Greek discoverers.\n\nIt is not known whether Bahrain was part of the Seleucid Empire, although the archaeological site at Qalat Al Bahrain has been proposed as a Seleucid base in the Persian Gulf. Alexander had planned to settle the eastern shores of the Persian Gulf with Greek colonists, and although it is not clear that this happened on the scale he envisaged, Tylos was very much part of the Hellenised world: the language of the upper classes was Greek (although Aramaic was in everyday use), while Zeus was worshipped in the form of the Arabian sun-god Shams. Tylos even became the site of Greek athletic contests.\n\nThe name Tylos is thought to be a Hellenisation of the Semitic, Tilmun (from Dilmun). The term Tylos was commonly used for the islands until Ptolemy’s \"Geographia\" when the inhabitants are referred to as ‘Thilouanoi’. Some place names in Bahrain go back to the Tylos era, for instance, the residential suburb of Arad in Muharraq, is believed to originate from “Arados”, the ancient Greek name for Muharraq island.\nHerodotus's account (written c. 440 BC) refers to the Io and Europa myths. (\"History,\" I:1). \n\nThe Greek historian Strabo believed the Phoenicians originated from Bahrain. Herodotus also believed that the homeland of the Phoenicians was Bahrain. This theory was accepted by the 19th-century German classicist Arnold Heeren who said that: \"In the Greek geographers, for instance, we read of two islands, named Tyrus or Tylos, and Arad, Bahrain, which boasted that they were the mother country of the Phoenicians, and exhibited relics of Phoenician temples.\" The people of Tyre in particular have long maintained Persian Gulf origins, and the similarity in the words \"Tylos\" and \"Tyre\" has been commented upon. \n\nWith the waning of Seleucid Greek power, Tylos was incorporated into Characene or Mesenian, the state founded in what today is Kuwait by Hyspaosines in 127 BC. A building inscriptions found in Bahrain indicate that Hyspoasines occupied the islands, (and it also mention his wife, Thalassia).\n\nFrom the 3rd century BC to arrival of Islam in the 7th century AD, Eastern Arabia was controlled by two other Iranian dynasties of Parthians and Sassanids.\n\nBy about 250 BC, the Seleucids lost their territories to the Parthians, an Iranian tribe from Central Asia. The Parthian Arsacid dynasty brought the Persian Gulf under their control and extended their influence as far as Oman. Because they needed to control the Persian Gulf trade route, the Parthians established garrisons on the southern coast of the Persian Gulf.\n\nIn the 3rd century AD, the Sasanians succeeded the Parthians and held the area until the rise of Islam four centuries later. Ardashir, the first ruler of the Sasanian dynasty, marched forward Oman and Bahrain and defeat Sanatruq (or Satiran), probably the Parthian governor of Eastern Arabia. He appointed his son Shapur I as governor of Eastern Arabia. Shapur constructed a new city there and named it Batan Ardashir after his father. At this time, Eastern Arabia incorporated in the southern Sassanid province covering over the Persian Gulfs southern shore plus the archipelago of Bahrain. The southern province of the Sassanids was subdivided into three districts of Haggar (Hofuf, Saudi Arabia), Batan Ardashir (al-Qatif province, Saudi Arabia), and Mishmahig (Muharraq, Bahrain; also referred to as Samahij) (In Middle-Persian/Pahlavi means \"ewe-fish\".) included the Bahrain archipelago which earlier called Aval.\n\nThe Christian name used for the region encompassing north-eastern Arabia was Beth Qatraye, or \"the Isles\". The name translates to 'region of the Qataris' in Syriac. Though it must be pointed out that Qatar is today expanded to include Bahrain just as India was supposed to include the land from Afghanistan till Philippines during those times It included Bahrain, Tarout Island, Al-Khatt, Al-Hasa, and Qatar.\n\nBy the 5th century, Beth Qatraye was a major centre for Nestorian Christianity, which had come to dominate the southern shores of the Persian Gulf. As a sect, the Nestorians were often persecuted as heretics by the Byzantine Empire, but eastern Arabia was outside the Empire's control offering some safety. Several notable Nestorian writers originated from Beth Qatraye, including Isaac of Nineveh, , and Ahob of Qatar. Christianity was blunted by the arrival of Islam in Eastern Arabia by 628. In 676, the bishops of Beth Qatraye stopped attending synods; although the practice of Christianity persisted in the region until the late 9th century.\n\nThe dioceses of Beth Qatraye did not form an ecclesiastical province, except for a short period during the mid-to-late 7th century. They were instead subject to the Metropolitan of Fars.\n\nIn 410, according to the Oriental Syriac Church synodal records, a bishop named Batai was excommunicated from the church in Bahrain. It was also the site eastern Arabia of worship of a Bull deity called Awal. Worshippers reputedly built a large statue to Awal in Muharraq, although it has now been lost, and for many centuries after Tylos, the islands of Bahrain were known as ‘Awal’.\n\nFrom the time when Islam emerged in the 7th century until the early 16th century, the term Bahrain referred to the wider historical region of eastern Arabia stretching from Basrah to the Strait of Hormuz along the Persian Gulf coast.\n\nEastern Arabians were amongst the first to embrace Islam. The Prophet Mohammed ruled eastern Arabia through one of his representatives, Al-Ala'a Al-Hadhrami. Eastern Arabia embraced Islam in 629 (the seventh year of hijra). During the time of Umar I the famous companion of Mohammad, Abu Hurayrah, was the governor of eastern Arabia. Umar I also appointed Uthman bin Abi Al Aas as governor of the area. Al Khamis Mosque, founded in 692, was one of the earliest mosques built in eastern Arabia, in the era of Umayyad caliph Umar II.\n\nThe expansion of Islam did not affect eastern Arabia's reliance on trade, and its prosperity continued to be dependent on markets in India and Mesopotamia. After Baghdad emerged as the seat of the caliph in 750 and the main centre of Islamic civilization, eastern Arabia greatly benefited from the city's increased demand for foreign goods especially from China and South Asia.\n\nEastern Arabia, specifically Bahrain, became a principal centre of knowledge for hundreds of years stretching from the early days of Islam in the 6th century to the 18th century. Philosophers of eastern Arabia were highly esteemed, such as the 13th-century mystic, Sheikh Maitham Al Bahrani (died in 1299). (The mosque of Sheikh Maitham and his tomb can be visited in the outskirts of the capital, Manama, near the district of Mahooz.).\n\nIn the end of the 3rd Hijri century, Abu Sa'id al-Hasan al-Janaby led the Revolution of al-Qaramita, a rebellion by a messianic Ismaili sect originating in Kufa in present-day Iraq. Al-Janaby took over the city of Hajr, Bahrain's capital at that time, and al-Hasa, which he made the capital of his republic. Once in control of the state he sought to create a utopian society.\n\nThe Qarmatians' goal was to build a society based on reason and equality. The state was governed by a council of six with a chief who was a first among equals. All property within the community was distributed evenly among all initiates. The Qarmatians were organized as an esoteric society but not as a secret one; their activities were public and openly propagated, but new members had to undergo an initiation ceremony involving seven stages. The Qarmatian world view was one where every phenomenon repeated itself in cycles, where every incident was replayed over and over again, a concept similar to the cycle of life and Karma.\n\nFor much of the 10th century the Qarmatians were the most powerful force in the Persian Gulf and Middle East, controlling the coast of Oman, and collecting tribute from the Abbasid caliph in Baghdad and from the rival Ismaili Fatimid caliph in Cairo, whom they did not recognize. The land they ruled over was extremely wealthy with a huge slave-based economy according to academic Yitzhak Nakash:\n\nThe Qarmatians were defeated in battle in 976 by the Abbasids, which encouraged them to look inward to build their utilitarian society. Around 1058, a revolt on the island of Bahrain led by two Shi'a members of the Abd al-Qays tribe, Abul-Bahlul al-'Awwam and Abu'l-Walid Muslim, precipitated the waning of Qarmatian power and eventually the ascendancy to power of the Uyunids, an Arab dynasty belonging to the Abdul Qays tribe.\n\nThe Al Uyuni, Uyunids (), were an Arab dynasty that ruled eastern Arabia for 163 years, from the 11th to the 13th centuries. They were the remnants of Bani Abdul Qays tribe and seized the country from the Qarmatians with the military assistance of Great Seljuq Empire in the year 1077-1078 AD. It then fell to the Usfurids of Banu Uqayl in 651 AH (1253 AD). The famous poet Ali bin al Mugrab Al Uyuni is a descendant of the Uyunids.\n\nThe Usfurids were an Arab dynasty that in 1253 gained control of eastern Arabia, They were a branch of the Banu Uqayl tribe of the Banu Amir group, and are named after the dynasty's founder, Usfur ibn Rashid. They were initially allies of the Qarmatians and their successors, the Uyunids, but eventually overthrew the latter and seized power themselves. The Usfurids' takeover came after Uyunid power had been weakened by invasion in 1235 by the Salgharid Atabeg of Fars.\n\nThe Usfurids had an uneasy relationship with the main regional power at the time, the Persian princes in Hormuz, who took control of Bahrain and Qatif in 1320. However, the Hormuzi rulers did not seem to have firm control of the islands, and during the 14th century Bahrain was disputed as numerous neighbours sought tribute from the wealth accumulated from its pearl fisheries. In the 15th century another branch of the Banu Amir emerged, the Jabrids, who built a more stable polity in eastern Arabia.\n\nThe Jarwanid Dynasty was a Shia dynasty that ruled eastern Arabia in the 14th century. It was founded by Jerwan I bin Nasser and was based in Qatif. The dynasty was a vassal of the Kingdom of Ormus.\n\nThe Jarwanids belonged to the clan of Bani Malik. It is disputed whether they belonged to the Banu Uqayl—the tribe of their predecessors the Usfurids and their successors the Jabrids—or to the Banu Abdul Qays, to whom the Uyunid dynasty (1076–1235) belonged. The Jarwanids came to power some time in the 14th century, after expelling the forces of Sa'eed ibn Mughamis, the chief of the Muntafiq tribe based in the Iraqi city of Basrah.\n\nContemporary sources such Ibn Battuta and Ibn Hajar describe the Jarwanids as being “extreme Rawafidh,” a term for Shi'ites who rejected the first three Caliphs, while a 15th-century Sunni scholar from Egypt describes them as being “remnants of the Qarmatians.” Historian Juan Cole concludes from this that they were Isma'ilis. However, the Twelver Shi'ite sect was promoted under their rule, and Twelver scholars held the judgeships and other important positions, including the chief of the \"hisba\". Also, unlike under the Qarmatians, Islamic prayers were held in the mosques under Jarwanid rule, and prayer was called under the Shi'ite formula. A Twelver scholar of the 14th century, Jamaluddeen Al-Mutawwa', belonged to the house of Jarwan. According to Al-Humaydan, who specialized in the history of eastern Arabia, the Jarwanids were Twelvers, and the term \"Qaramita\" was used simply as an epithet for \"Shi'ite.\"\n\nThe Jabrids () were a dynasty that dominated eastern Arabia in the 15th and 16th centuries. They were descendants of the tribe of Uqayl, a branch of Bani 'Amir.\n\nTheir most prominent ruler was Ajwad ibn Zamil, who died in 1507. He was described by his contemporaries as having been “of Najdi origin.” Ajwad's elder brother had earlier established the dynasty in the early 15th century by deposing and killing the last Jarwanid ruler in Qatif. At their height, the Jabrids controlled the entire Arabian coast on the Persian Gulf, including the islands of Bahrain, and regularly led expeditions into central Arabia and Oman. One contemporary scholar described Ajwad ibn Zamil as “the king of al-Ahsa and Qatif and the leader of the people of Najd.” Following his death, his kingdom was divided among some of his descendants, with Migrin ibn Zamil (possibly his grandson) inheriting al-Hasa, Qatif, and Bahrain. Migrin fell in battle in Bahrain in a failed attempt to repel an invasion of Bahrain by the Portuguese in 1521.\n\nThe Jabrid kingdom collapsed soon afterwards on the mainland, after an invasion of al-Hasa by Muntafiq tribe of Basrah, and later by the Ottoman Turks. One branch of the Jabrids remained active in Oman, however, for nearly another three centuries. It is unknown for sure what became of the non-Omani Jabrids. Some believe they left to Iraq, while others believe they are identical with the Jubur section of the Bani Khalid confederation, who eventually took control of the region after the Jabrids.\n\nThe main branches of the tribe are the Al Humaid, the Juboor, the Du'um, the Al Janah, the Grusha, the Al Musallam, the 'Amayer, the Al Subaih and the Mahashir. The chieftainship of the Bani Khalid has traditionally been held by the clan of Al Humaid. The Bani Khalid dominated the deserts surrounding the Al-Hasa and Al-Qatif oases during the 16th and 17th centuries. Under Barrak ibn Ghurayr of the Al Humaid, the Bani Khalid were able to expel Ottoman forces from the cities and towns in 1670 and proclaim their rule over the region. Ibn Ghurayr made his capital in Al-Mubarraz, where remnants of his castle stand today. According to Arabian folklore, one chief of the Bani Khalid attempted to protect the prized desert bustard (\"habari\") from extinction by prohibiting the bedouin in his realm from poaching the bird's eggs, earning the tribe the appellation of \"protectors of the eggs of the habari\", an allusion to the chief's absolute supremacy over his realm. The first chieftain of the “Khawalid” was Haddori.\n\nLike a vast majority of their subject people, in time the Khalidis adopted Shia Islam if they were not already so at the time of their ascendency. This led to a lasting animosity between them and the staunchly anti-Shia Wahhabis and the House of Saud, from the mid-18th century to the present.\n\nThe Bani Khalid of eastern Arabia maintained ties with members of their tribe who had settled in Nejd during their earlier migration eastwards, and also cultivated clients among the rulers of the Nejdi towns, such as the Al Mu'ammar of al-Uyayna. When the emir of Uyayna adopted the ideas of Muhammad ibn Abd al-Wahhab, the Khalidi chief ordered him to cease support for Ibn Abd al-Wahhab and expel him from his town. The emir agreed, and Ibn Abd al-Wahhab moved to neighboring Dir'iyyah, where he joined forces with the Al Saud. The Bani Khalid remained staunch enemies of the Saudis and their allies and attempted to invade Nejd and Diriyyah in an effort to stop Saudi expansion. Their efforts failed, however, and after conquering Nejd, the Saudis invaded the Bani Khalid's domain in al-Hasa and deposed the Al 'Ura'yir in 1793.\n\nWhen the Egyptians under Muhammad Ali dynasty invaded Arabia and deposed the Al Saud in 1818, they reoccupied al-Hasa and al-Qatif and reinstated members of the Al 'Uray'ir as governors of the region on their behalf. The Bani Khalid were no longer the potent military force they once were at this time, and tribes such as the Ajman, the Dawasir, Subay', and Mutayr began encroaching on the Bani Khalid's desert territories. They were also beset by internal quarrels over leadership. Though the Bani Khalid were able to forge an alliance with the 'Anizzah tribe in this period, they were eventually defeated by an alliance of several tribes along with the Al Saud, who had reestablished their rule in Riyadh in 1823. A battle with an alliance led by the Mutayr and 'Ajman tribes in 1823, and another battle with the Subay' and the Al Saud in 1830, brought the rule of the Bani Khalid to a close. The Ottomans appointed a governor from Bani Khalid over al-Hasa once more in 1874, but his rule also was short-lived.\n\n"}
{"id": "10544951", "url": "https://en.wikipedia.org/wiki?curid=10544951", "title": "Enoch calendar", "text": "Enoch calendar\n\nThe Enoch calendar is an ancient calendar described in the pseudepigraphal Book of Enoch. It divided the year into four seasons of exactly 13 weeks each. Each such season consisted of two 30-day months followed by one 31-day month, with the 31st day ending the season, so that Enoch's Year consisted of exactly 364 days. \n\nThere is some evidence that the group whose writings were found at Qumran used a variation of the Enoch calendar (see Qumran calendar).\n\nThe Enoch calendar was purportedly given to Enoch by the angel Uriel. Four named days, inserted as the 31st day of every third month, were named instead of numbered, which \"placed them outside the numbering\". The Book of Enoch gives the count of 2,912 days for 8 years, which divides out to exactly 364 days per year. This specifically excludes any periodic intercalations.\n\nCalendar expert John Pratt wrote that \"The Enoch calendar has been criticized as hopelessly primitive because, with only 364 days, it would get out of sync with the seasons so quickly: in only 25 years the seasons would arrive an entire month early. Such a gross discrepancy, however, merely indicates that the method of intercalation has been omitted.\" Pratt pointed out that by adding an extra week at the end of every seventh year (or Sabbatical year), and then adding two extra weeks to every fourth Sabbatical year (or every 28 years), the calendar could be as accurate as the Julian calendar.\n\n\nSee the various writings of Julian Morgenstern, James C. VanderKam and others. \n\n\"Sabbatical Years and the Year of Jubilee\". Sidney B. Hoenig; \"The Jewish Quarterly Review\", New Series, Vol. 59, No. 3 (Jan., 1969), pp. 222-236.\n\n\"A Possible Method of Intercalation for the Calendar of the Book of Jubilees\". E. R. Leach; \"Vetus Testamentum\", Vol. 7, Fasc. 4 (Oct., 1957), pp. 392-397.\n\n\"Jubilee Calendar Rescued from the Flood Narrative\". S. Najm & Ph. Guillaume. Retrieved 6/22/2008 from http://www.arts.ualberta.ca/JHS/Articles/Article_31.htm\n\n\"Sabbatical, Jubilee, and the Temple of Solomon.\" L. W. Casperson. \"Vetus Testamentum\", Vol. 53, No. 3, 2003, pp. 283-296(14).\n\n\"Calendars of the Dead_sea-Scroll Sect\". Edward L. Cohen; \"CUBO Mathematica Educacional\"; Vol. 52 No. 2, (1-16). Junio 2003.\n\n\"Biblical Calendars\". J. van Goudoever. Leiden, E. J. Brill, 1959. \n\n\"Tracing the Origin of the Sabbatical Calendar in the Priestly Narrative (Genesis 1 to Joshua 5)\". Philippe Guillaume. Retrieved 6/22/2008 from http://www.arts.ualberta.ca/JHS/Articles/article_43.htm.\n\n\"Studies in the Hebrew Calendar: (Interpretation of a Difficult Passage in the Palestinian Talmud)\". Solomon Gandz. \"Proceedings of the American Academy for Jewish Research\", Vol. 17, (1947-1948), pp. 9-17.\n\n\"Chronology of the Account of the Flood in P.--A Contribution to the History of the Jewish Calendar. Benjamin Wisner Bacon. \"Hebraica\", Vol. 8, No. 1/2 (Oct., 1891 - Jan., 1892), pp. 79-88.\n\n\"The Calendar of the Book of Jubilees, Its Origin and its Character.\" Julian Morgenstern. \"Vetus Testamentum\", Vol. 5, Fasc. 1 (Jan., 1955) pp. 34-76. \n\n\"The Judean Calendar during the Second Commonwealth and the Scrolls.\" Solomon Zeitlin. \"The Jewish Quarterly Review\". New Series, Vol. 57, No. 1 (Jul., 1966), pp. 28-45.\n"}
{"id": "24534207", "url": "https://en.wikipedia.org/wiki?curid=24534207", "title": "Erbil", "text": "Erbil\n\nErbil, also spelt Arbil or Irbil, locally called Hawler/Hewler by the Kurds is the capital of legal government of Kurdistan region and the most populous city in the Kurdish inhabited areas. It is located approximately north of neighboring Baghdad Iraq. It has about 850,000 inhabitants, and Erbil governorate has a permanent population of 2,009,367 .\n\nHuman settlement at Erbil can be dated back to possibly 5th millennium BC, and it is one of the oldest continuously inhabited areas in the world. At the heart of the city is the ancient Citadel of Erbil. The earliest historical reference to the region dates to the Third Dynasty of Ur of Sumer, when King Shulgi mentioned the city of Urbilum. The city was later settled by the Assyrians.\n\nErbil became an integral part of the kingdom of Assyria by at least the 21st century BC through to the end of the seventh century BC, after it was captured by the Gutians, and it was known in Assyrian annals variously as \"Urbilim\", \"Arbela\" and \"Arba-ilu\". After this it was part of the geopolitical province of Assyria under several empires in turn, including the Median Empire, the Achaemenid Empire (Achaemenid Assyria), Macedonian Empire, Seleucid Empire, Parthian Empire, Roman Assyria and Sasanian Empire (Asōristān), as well as being the capital of the tributary state of Adiabene between the mid-second century BC and early second century AD.\n\nFollowing the Muslim conquest of Persia, it no longer remained a unitary region, and during the Middle Ages, the city came to be ruled by the Seljuk and Ottoman empires.\n\nErbil's archaeological museum houses a large collection of pre-Islamic artefacts, particularly the art of Mesopotamia, and is a centre for archaeological projects in the area. The city was designated as Arab Tourism Capital 2014 by the Arab Council of Tourism. In July 2014, the Citadel of Arbil was inscribed as a World Heritage site.\n\nThe city has an ethnically diverse population of Kurds (the majority ethnic group), Armenians, Assyrians, Arabs, Iraqi Turkmens, Yezidis, Shabakis and Mandaeans. It is equally religiously diverse, with believers of Sunni Islam, Shia Islam, Christianity (mainly followed by Assyrians and Armenians), Yezidism, Yarsanism, Shabakism and Mandaeism extant in and around Erbil.\n\nThe name \"Erbil\" was mentioned in Sumerian holy writings of third millennium BC as \"Urbilum\", \"Urbelum\" or \"Urbillum\", which appears to originate from \"Arbilum\" Later, the Akkadians and Assyrians by a folk etymology rendered the name as \"arba'ū ilū\" to mean (\"four gods\"). The city became a centre for the worship of the Mesopotamian goddess Ishtar. In classical times the city became known as \"Arbela\" (), from the Syriac form of the name. In Old Persian, the city was called Arbairā.\n\nToday, the modern Kurdish name of the city, \"Hewlêr\", appears to be a corruption of the name \"Arbel\" by a series of metatheses of consonants. The nicknames of Arbil include: Cradle of Iraqi civilization, old as history, cradle of humankind, defeater of Hulaku and mirror of Kurdistan´s beauty.\n\nThe region in which Erbil lies was largely under Sumerian domination from c. 3000 BC, until the rise of the Akkadian Empire (2335–2154 BC) which united all of the Akkadian Semites and Sumerians of Mesopotamia under one rule. Today the Assyrian people, a Syriac-speaking community who claim descent from Akkadian speakers, endure as a minority in northern Iraq, north east Syria, south east Turkey and north west Iran, their population is estimated to be 3.3 million.\n\nThe first mention of Erbil in literary sources' comes from the archives of the east Semitic speaking kingdom of Ebla. They record two journeys to Erbil (Irbilum) by a messenger from Ebla around 2300 BC. Later, Erridupizir, king of the language isolate speaking kingdom of Gutium, captured the city in 2150 BC.\n\nThe Neo-Sumerian ruler of Ur, Amar-Sin, sacked \"Urbilum\" in his second year, c. 1975 BC\n\nErbil was an integral part of Assyria from around 2050 BC, becoming a relatively important city during the Old Assyrian Empire (1975–1750 BC), Middle Assyrian Empire (1365–1050 BC) and the Neo Assyrian Empire (935–605 BC), until the last of these empires fell between 612–599 BC, however it remained part of Assyria under Persian, Greek, Parthian, Roman and Sassanid rule until the first half of the 7th century AD.\n\nUnder the Median Empire, Cyaxares \"might\" have settled a number of people from the Ancient Iranian tribe of Sagartians in the Assyrian cities of Arbela and Arrapha (modern Kirkuk), probably as a reward for their help in the capture of Nineveh. The Persian emperor Cyrus the Great occupied Assyria in 547 BC and established it as an Achaemenid satrapy called in Old Persian \"Aθurā (Athura)\", with Arbela as the capital.\n\nThe Battle of Gaugamela, in which Alexander the Great defeated Darius III of Persia in 331 BC, took place approximately west of Erbil. After the battle, Darius managed to flee to the city, and, somewhat inaccurately, the confrontation is sometimes known as the \"Battle of Arbela\". Subsequently, Arbela was part of Alexander's Empire. After the death of Alexander the Great in 323 BC, Arbela became part of the Hellenistic Seleucid Empire.\n\nErbil became part of the region disputed between Rome and Persia under the Sasanids. The ancient Assyrian kingdom of Adiabene (the Greek form of the Assyrian Ḥadyab) had its centre at Erbil, and the town and kingdom are known in Jewish Middle Eastern history for the conversion of the royal family to Judaism. During the Parthian era to early Sassanid era, Erbil became the capital of the Assyrian state of Adiabene.\n\nIts populace then gradually converted from the Mesopotamian religion between the 1st and 4th centuries to the Assyrian Church of the East Christianity (and to a lesser degree to the Syriac Orthodox Church), with Pkidha traditionally becoming its first bishop around 104 AD, although the ancient Assyrian religion did not die out entirely in the region until the 10th century AD. The metropolitanate of Ḥadyab in Arbela (Syriac: \"Arbel\") became a centre of eastern Syriac Christianity until late in the Middle Ages.\n\nAs many of the Eastern Aramaic-speaking Assyrians who had converted to Christianity adopted Biblical (including Jewish) names, most of the early bishops had Eastern Aramaic or Jewish/Biblical names, which does not suggest that many of the early Christians in this city were converts from Judaism. It served as the seat of a Metropolitan of the Assyrian Church of the East. From the city's Christian period come many church fathers and well-known authors in Syriac.\n\nFollowing the Muslim conquest of Persia, the Sasanid province of Assuristan, of which Erbil made part of, was dissolved, and from the mid 7th century AD the region saw a gradual influx of Muslim peoples, predominantly Arabs, Kurds and Turkic peoples.\n\nThe most notable Kurdish tribe in the region were the Hadhbani, of which several individuals also acted as governors for the city from the late 10th century until the 12th century when it was conquered by the Zengids and its governorship given to the Turkic Begtegenids, who retained the city during the Ayyubid era Yaqut al-Hamawi further describes Erbil as being mostly Kurdish-populated in the 13th century.\n\nWhen the Mongols invaded the Near East in the 13th century, they attacked Arbil for the first time in 1237. They plundered the lower town but had to retreat before an approaching Caliphate army and had to put off the capture of the citadel. After the fall of Baghdad to Hülegü and the Mongols in 1258, the last Begtegenid ruler surrendered to the Mongols, claiming the Kurdish garrison of the city would follow suit; they refused this however, therefore the Mongols returned to Arbil and were able to capture the citadel after a siege lasting six months. Hülegü then appointed an Assyrian Christian governor to the town, and the Syriac Orthodox Church was allowed to build a church.\n\nAs time passed, sustained persecutions of Christians, Jews and Buddhists throughout the Ilkhanate began in earnest in 1295 under the rule of Oïrat amir Nauruz, which affected the indigenous Assyrian Christians greatly. This manifested early on in the reign of the Ilkhan Ghazan. In 1297, after Ghazan had felt strong enough to overcome Nauruz's influence, he put a stop to the persecutions.\n\nDuring the reign of the Ilkhan Öljeitü the Assyrian Christian inhabitants retreated to the citadel to escape persecution. In the Spring of 1310, the Malek (governor) of the region attempted to seize it from them with the help of the Kurds. Despite the Turkic bishop Mar Yahballaha's best efforts to avert the impending doom, the citadel was at last taken after a siege by Ilkhanate troops and Kurdish tribesmen on 1 July 1310, and all the defenders were massacred, including many of the Assyrian inhabitants of the lower town.\n\nHowever, the city's Eastern Aramaic-speaking Assyrian population remained numerically significant until the destruction of the city by the forces of Timur in 1397.\n\nIn the Middle Ages, Erbil was ruled successively by the Umayyads, the Abbasids, the Buwayhids, the Seljuks and then the Atabegs of Erbil (1131–1232), under whom it was a Turkmen state; they were in turn followed by the Ilkhanids, the Jalayirids, the Kara Koyunlu, the Timurids and the Ak Koyunlu. Erbil was the birthplace of the famous 12th and 13th century Kurdish historians and writers Ibn Khallikan and Ibn al-Mustawfi. Erbil and all of Iraq passed into the hands of the Ottoman Turks in the 16th century. Erbil was part of the Musul Vilayet in Ottoman Empire for 400 years until World War I, when the Ottomans and their Kurdish and Turcoman allies were defeated by the British Empire, with the aid of the Assyrians and Armenians, and the Ottoman Turks ejected.\n\nThe modern town of Erbil stands on a tell topped by an Ottoman fort. During the Middle Ages, Erbil became a major trading centre on the route between Baghdad and Mosul, a role which it still plays today with important road links to the outside world.\nToday, Erbil is both multi-ethnic and multi-religious, with the Kurds forming the largest ethnic group in the city, with smaller numbers of Arabs, Assyrians, Turcoman, Armenians, Yazidis, Shabaks, Circassians, Kawliya, Iranians and Mandeans also extant.\n\nThe parliament of the Kurdistan Autonomous Region was established in Erbil in 1970 after negotiations between the Iraqi government and the Kurdistan Democratic Party (KDP) led by Mustafa Barzani, but was effectively controlled by Saddam Hussein until the Kurdish uprising at the end of the 1991 Gulf War. The legislature ceased to function effectively in the mid-1990s when fighting broke out between the two main Kurdish factions, the Kurdistan Democratic Party and the Patriotic Union of Kurdistan (PUK). The city was captured by the KDP in 1996 with the assistance of the Iraqi government of Saddam Hussein. The PUK then established an alternative Kurdish government in Sulaimaniyah. KDP claimed that on March 1996 PUK asked for Iran's help to fight KDP. Considering this as a foreign attack on Iraq's soil, the KDP asked the Iraqi government for help.\n\nThe Kurdish Parliament in Erbil reconvened after a peace agreement was signed between the Kurdish parties in 1997, but had no real power. The Kurdish government in Erbil had control only in the western and northern parts of the autonomous region. During the 2003 Invasion of Iraq, a United States special forces task force was headquartered just outside Erbil. The city was the scene of celebrations on 10 April 2003 after the fall of the Ba'ath regime.\n\nDuring the coalition forces occupation of Iraq, sporadic attacks hit Erbil. Parallel bomb attacks against Eid celebrations killed 109 people on 1 February 2004. Responsibility was claimed by the Ansar al-Sunnah, and stated to be in solidarity with Ansar al-Islam. A suicide bombing on 4 May 2005 killed 60 civilians and injured 150 more outside a police recruiting centre.\n\nThe Erbil International Airport opened in the city in 2005. In 2015, the Christian Nestorian Church has moved its seat from Chicago to Erbil.\n\n\"Main article\": Downtown Erbil\n\nDowntown Erbil is a large shopping complex in Gulan street, Erbil. The project was coordinated by Emaar Properties, the GCC's largest real estate developer. Emaar is well known for international big projects like Downtown Dubai and Burj Khalifa. The 2 billion dollar project was launched by Nasri group of Iraq. \n\nThe Erbil based American village is a large villa complex that features western style (mostly American designed) houses for people to rent and buy from. Unlike other neighborhoods in Erbil the American village has 24 hours of electricity with added security and other services.\"American, Hawlerian or Palace homes; each style offering luxurious comfort, refined design inspired by the architectural style of the American East Coast and featuring the most advanced amenities. Everything and all the services you need are here in this fully independent and self sufficient residential community. Make yourself at home in your exclusive American style community where you can walk across landscaped gardens, watch your children safely having fun in the playground, and enjoy the spectacular mountain view.\"\n\nSet at the forefront of one of Erbil’s most sought after and prestigious communities, Empire Avenue offers the best possible location to live, work and play.\n\nWith a host of stylish cafes, top restaurants and an array of fashionable stores, Empire Avenue enables residents and visitors alike to not only experience the best possible lifestyle, but the very best of Erbil.\n\nEmpire Avenue isn’t simply an urban walkway– it’s a sanctuary. Here, lush green landscapes and wide boardwalks and fascinating fountains provide the luxurious backdrop for pedestrian-friendly living set within a warm and energetic urban setting.\n\nErbil International Airport is one of Iraq's busiest airports and is near the city. Services includes direct flights to many domestic destinations such as Baghdad international airport. There are international flights from Erbil such as to countries; Netherlands, Germany, Saudi Arabia, Austria, Turkey, Jordan and many more flights elsewhere around the world. There are occasionally seasonal flights from Erbil international airport. Erbil International Airport was briefly closed to international commercial flights in September 2017 by the Iraqi government in retaliation for the Kurdish independence vote but reopened in March 2018.\n\nMost of Kurdistan's transportation is with local Taxis, that are operated by private companies and also managed by the government of Kurdistan. Hawler Taxi, also known as Kurd Taxi, will be the first and only taxi service in Erbil to offer app-based taxi bookings in the Erbil area and eventually across the Kurdistan region. Currently Kurd Taxi/Hawler Taxi is only available in Erbil, but once the application is fully launched, there is hope to also launch Duhok Taxi and Slemani Taxi in the future.\n\nErbil's climate is hot-summer Mediterranean (\"Csa\") according to Köppen climate classification, with extremely hot summers and cool wet winters. January is the wettest month.\n\nThe Citadel of Arbil is a tell or occupied mound in the historical heart of Erbil, rising between from the surrounding plain. The buildings on top of the tell stretch over a roughly oval area of occupying . It has been claimed that the site is the oldest continuously inhabited town in the world. The earliest evidence for occupation of the citadel mound dates to the 5th millennium BC and possibly earlier. It appears for the first time in historical sources during the Ur III period and gained particular importance during the Neo-Assyrian Empire (10th to 7th centuries BC) period. West of the citadel at Ary Kon quarter, a chamber tomb dating to the Neo-Assyrian Empire period has been excavated. During the Sassanian period and the Abbasid Caliphate, Erbil was an important centre for Assyrian Christianity and the Assyrians. After the Mongols captured the citadel in 1258, Erbil's importance began to decline.\n\nDuring the 20th century, the urban structure was significantly modified, as a result of which a number of houses and public buildings were destroyed. In 2007, the High Commission for Erbil Citadel Revitalization (HCECR) was established to oversee the restoration of the citadel. In the same year, all inhabitants, except one family, were evicted from the citadel as part of a large restoration project. Since then, archaeological research and restoration works have been carried out at and around the tell by various international teams and in co-operation with local specialists, and many areas remain off-limits to visitors due to the danger of unstable walls and infrastructure. The government plans to have 50 families live in the citadel once it is renovated.\n\nThe only religious structure that currently survives in the citadel is the Mulla Afandi Mosque. When it was fully occupied, the citadel was divided in three districts or \"mahallas\": from east to west the Serai, the Takya and the Topkhana. The Serai was occupied by notable families; the Takya district was named after the homes of dervishes, which are called \"takyas\"; and the Topkhana district housed craftsmen and farmers. Other sights to visit in the citadel include the bathing rooms (\"hammam\") built in 1775 located near the mosque and the Textile Museum. Erbil citadel has been inscribed on the World Heritage List on 21 June 2014 .\n\n\nThe local major football team is Erbil SC which plays its football matches at Franso Hariri Stadium (named after the assassinated Assyrian politician, former governor of Erbil city Franso Hariri) which is based in the south part of central Erbil. Erbil SC were the first Kurdish team to make it to the AFC Champions league.\n\n\n\n\n"}
{"id": "53705141", "url": "https://en.wikipedia.org/wiki?curid=53705141", "title": "Erhan Afyoncu", "text": "Erhan Afyoncu\n\nErhan Afyoncu (1967, Tokat) is a Turkish historian, writer, academician, television programmer and columnist. Rector of the National Defense University.\n\nHe saw his primary and secondary education in Tokat, the place of birth. After graduating from Gazi Osman Paşa High School in 1984, Marmara University Atatürk Education Faculty Department of Social Studies Education Department began.\n\nHe graduated in 1988 and started working as a research assistant in the same department a year later. He completed his master's degree with \"Necati Efendi History of Crime\" (Russian Sefaratname) thesis and completed his doctorate in 1997 with \"Defterhâne-i Âmire\" (XVI-XVIII. Centuries) in Ottoman Empire State Organization. He became assistant professor in 2000, associate professor in 2008, professor in 2014. In 2001, he moved to the Department of History of Science and Literature. In 2010, he became deputy head of the Department of History of Marmara University Faculty of Arts and Sciences. By 2016, Marmara University was the dean of the Faculty of Science and Literature. He made a history program called \"Back Room of History\" with Murat Bardakçı on Habertürk TV. He also wrote in the \"Haberturk History\" magazine, whose first issue appeared on May 30, 2010, but whose publication life ended on May 22, He also undertook the academic coordination of the magazine. On April 11, 2012, President Abdullah Gül was appointed as a member of the Board of Directors of Atatürk Culture, Language and History Higher Institution. In 2016, he was appointed to the rector of the National Defense University.\n\n"}
{"id": "285918", "url": "https://en.wikipedia.org/wiki?curid=285918", "title": "Golden Horns of Gallehus", "text": "Golden Horns of Gallehus\n\nThe Golden Horns of Gallehus were two horns made of sheet gold, discovered in Gallehus, north of Møgeltønder in Southern Jutland, Denmark.\nThe horns dated to the early 5th century, i.e. the beginning of the Germanic Iron Age.\n\nThe horns were found in 1639 and in 1734, respectively, at locations only some 15–20 metres apart. They were composed of segments of double sheet gold. The two horns were found incomplete; the longer one found in 1639 had seven segments with ornaments, to which six plain segments and a plain rim were added, possibly by the 17th-century restorer. The shorter horn found in 1734 had six segments, a narrow one bearing a Proto-Norse Elder Futhark inscription at the rim and five ornamented with images. It is uncertain whether the horns were intended as drinking horns, or as blowing horns, although drinking horns have more pronounced history as luxury items made from precious metal.\n\nThe original horns were stolen and melted down in 1802. Casts made of the horns in the late 18th century were also lost. Replicas of the horns must thus rely on 17th and 18th-century drawings exclusively and are accordingly fraught with uncertainty. Nevertheless, replicas of the original horns were produced and are exhibited at the National Museum of Denmark, Copenhagen, and the Moesgaard Museum, near Aarhus, Denmark. These replicas also have a history of having been stolen and retrieved twice, in 1993 and in 2007.\n\nThe horns are the subject of one of the best-known poems in Danish literature, \"The Golden Horns\" (\"Guldhornene\"), by Adam Oehlenschläger.\n\nBoth horns consisted of two layers of gold sheet, the inner sheet of lesser quality, amalgamated with silver, the outer sheet of pure gold. The outer sheet was constructed from a number of rings, each covered with cast figures soldered onto the rings, with yet more figures chased into the rings between the larger figures. The second horn bore an Elder Futhark inscription in Proto-Norse which is of great value for Germanic linguistics.\n\nBoth horns were once the same length, but a segment of the narrow end of the second (shorter) horn, which was missing when it was found (1734), had already been plowed up and recovered prior to 1639. It also was subsequently melted down and lost.\nThe longer horn in its restored state was 75.8 cm in length, as measured along the outer perimeter; the opening diameter was 10.4 cm., and the horn weighed 3.2 kg.\n\nBecause the casts made of the horns were lost, it is uncertain whether the horns were simply curved or whether they had a winding, helix-like curvature like a natural ox-horn.\n\nThe second horn bore the following Elder Futhark inscription (DR 12 †U):\n\nThis is read as a sentence in Proto-Norse, \"\", translating to \"I Hlewagastiz Holtijaz made the horn\". This inscription is among the earliest inscriptions in the Elder Futhark that record a full sentence, and the earliest preserving a line of alliterative verse.\n\nThe meaning of the given name \"Hlewagastiz\" is debated: it may mean either \"lee guest\" or \"fame guest\". \"Holtijaz\" may either be a patronymic, \"son (or descendant) of Holt\", or express a characteristic such as \"of the wood\".\n\nThe two rows of images in the top segment of the longer horn have been taken as a cipher encoding a runic text of 22 letters, although there is no universally accepted decipherment. Hartner (1969) read \"luba horns ens helpa hjoho\", an \"apotropaic sentence\" translated by Hartner as \"may I, the potion of this horn, bring help to the clan\".\n\nThe figures embossed on the horns combine depictions of numerous anthropomorphic, zoomorphic and hybrid motifs. In addition to the main figures which are soldered to the horn, protruding from the horn surface, there are a number of additional figures and ornaments realized by chasing.\n\nThere are numerous attempts at their interpretation, all of them speculative. Interpretation is especially difficult since it has to rely on the imprecise drawings made before the loss of the original horns. Interpretations usually try to align the iconography with Germanic mythology, although Mediterranean (Byzantine) elements are also cited.\n\nObvious parallels with Germanic Migration Period art are rather limited. There are large considerable number of serpents, some of them intertwining in the fashion of the wider animal style of Dark Age Europe. A figure holding a horn may be compared to the \"valkyrie\" shown on the Tjängvide image stone. Two masked figures armed with sword and shield on the smaller horn are reminiscent of other Germanic depictions of sword-dances, e.g. on the Sutton Hoo helmet. Two other armed figures with large horns or antlers may be compared to horned helmet iconography, or to the \"Cernunnos\" figure on the Gundestrup cauldron. The peculiar image of a tricephalous figure has been taken as a pagan predecessor of depictions of a tricephalous Christ symbolizing the Trinity in Christian iconography but is difficult to place in a contemporary context. Olrik (1918) nevertheless recognized a number of Norse gods among the figures, including Tyr, Odin, Thor and Freyr.\n\nFrankfurt historian of science Willy Hartner in 1969 published an interpretation involving gematria and archaeoastronomy, taking many of the figures as representing constellations, claiming that the iconography refers to a lunar eclipse of 4 November 412 and a solar eclipse of 16 April 413.\n\nThe first horn (A) had figures arranged in seven segments, The second horn (B) had six segments including the narrow one containing the runic inscription around the rim.\n\nThe longer horn was discovered on July 20, 1639 by a peasant girl named Kirsten Svendsdatter in the village of Gallehus, near Møgeltønder, Denmark when she saw it protrude above the ground. She wrote a letter to King Christian IV of Denmark who retrieved it and in turn gave it to the Danish prince Christian, who refurbished it into a drinking-horn, adding a golden pommel to be screwed on at the narrow end to close it up.\n\nThe Danish antiquarian Olaus Wormius wrote a treatise named \"De aureo cornu\" on the first Golden Horn in 1641.\nThe first preserved sketch of the horn comes from this treatise. Wormius notes that he had not seen the horn in the state in which it was found, and it cannot now be determined whether the rim and the narrow segments devoid of ornamentation were modern additions like the pommel.\n\nIn 1678, the horn was described in the \"Journal des Savants\".\n\nAbout 100 years later on April 21, 1734 the other (shorter, damaged) horn was found by Erich Lassen not far from the first horn. He gave it to the count of Schackenborg who in turn delivered it to King Christian VI of Denmark and received 200 rigsdaler in return. From this moment both horns were stored at \"Det kongelige Kunstkammer\" (The Royal Chamber of Art) at Christiansborg, currently the Danish Rigsarkivet (national archive). The shorter horn was described in a treatise by archivist Richard Joachim Paulli in the same year.\n\nOn May 4, 1802, the horns were stolen by a goldsmith and watchmaker named Niels Heidenreich from Foulum, who entered a storage area containing the horns using forged keys. Heidenreich took the horns home and melted them down to recycle the gold. The theft was discovered the next day and a bounty of 1,000 rigsdaler was advertised in the papers.\n\nThe grandmaster of the goldsmiths guild, Andreas Holm, suspected that Heidenreich had been involved, since he had tried to sell Holm forged “pagodas” (Indian coins with god motifs), made from bad gold mixed with brass. Holm and his colleagues had kept watch on Heidenreich and saw him dump coin stamps in the town moat. He was arrested on April 27, 1803, and confessed on April 30. On June 10, Heidenreich was sentenced to prison, and not released until 1840. He died four years later. His buyers returned the recycled gold, which ended up in coins, not copies of the horns.\n\nHowever, a set of plaster casts of the horns had been made for a cardinal in Rome, but they had already been lost in a shipwreck off the Corsican coast. Approximate copies were instead created from sketches. The horns pictured above are newer copies, made in 1980.\n\nIn 1993, copies of the horns were stolen from Moesgaard Museum, which were shortly after recovered ditched in a forest near Hasselager. These copies are made of gilded brass.\n\nOn September 17, 2007, a set of modern gilded silver copies were stolen from Kongernes Jelling museum at 4:30 in the morning, but were recovered shortly after on September 19, 2007.\n\n\n"}
{"id": "755401", "url": "https://en.wikipedia.org/wiki?curid=755401", "title": "History of Croatia since 1995", "text": "History of Croatia since 1995\n\nThis is the history of Croatia since the end of the Croatian War of Independence.\n\nIn November 1995 the war in Croatia ended. Around 20,000 people were killed in the war, while official figures on wartime damage published in Croatia in 1996 specify 180,000 destroyed housing units, 25% of the Croatian economy destroyed, and US$27 billion of material damage. \"Europe Review 2003/04\" estimated the war damage at US$37 billion in damaged infrastructure, lost economic output, and refugee-related costs, while GDP dropped 21% in the period. 15 percent of housing units and 2,423 cultural heritage structures, including 495 sacral structures, were destroyed or damaged. The war imposed an additional economic burden of very high military expenditures. By 1994, as Croatia rapidly developed into a de facto war economy, the military consumed as much as 60 percent of total government spending.\n\nFollowing the end of the war, parliamentary elections were held in 1995, which resulted in a victory of the ruling Croatian Democratic Union. Zlatko Mateša became the new Prime Minister of Croatia, replacing Nikica Valentić, and formed the first peacetime government of independent Croatia. Elections for the Zagreb Assembly were held at the same time, which were won by the opposition. This led to the Zagreb crisis since the president refused to provide formal confirmation to the opposition parties proposed Mayor of Zagreb. In 1996 mass demonstrations were held in Zagreb in response to revoking broadcasting license to Radio 101, a radio station that was critical towards the ruling party.\n\nAs a result of the macro-stabilization programs, the negative growth of GDP during the early 1990s stopped and turned into a positive trend. Post-war reconstruction activity provided another impetus to growth. Consumer spending and private sector investments, both of which were postponed during the war, contributed to the growth in 1995-97 and improved economic conditions. Real GDP growth in 1995 was 6,8%, in 1996 5,9% and in 1997 6,6%.\n\nIn 1995 a Ministry of Privatization was established with Ivan Penić as its first minister. Privatization in Croatia had barely begun when war broke out in 1991 and its transformation from a planned economy to a market economy was thus slow and unsteady. The ruling party was criticised for transferring enterprises to a group of privileged owners connected to the party.\n\nCroatia became a member of the Council of Europe on 6 November 1996. President of Croatia Franjo Tuđman won the 1997 presidential elections with 61.4% of the votes and was re-elected to a second five-year term. Marina Matulović-Dropulić became the Mayor of Zagreb having won the 1997 local elections, which formally ended the Zagreb crisis.\n\nThe remaining part of former Republic of Serbian Krajina, areas adjacent to FR Yugoslavia, negotiated a peaceful reintegration process with the Croatian Government. The so-called Erdut Agreement made the area a temporary protectorate of the United Nations Transitional Administration for Eastern Slavonia, Baranja and Western Sirmium. The area was formally re-integrated into Croatia on 15 January 1998. On 3 October 1998 Pope John Paul II beatified Cardinal Alojzije Stepinac.\n\nValue-added tax was introduced in 1998 and the central government budget was in surplus that year. At the end of 1998 Croatia went into a recession and GDP growth slowed down to 1,9%. The recession continued throughout 1999 when GDP fell by 0,9%. Unemployment increased from around 10% in 1996 and 1997 to 11,4% in 1998. By the end of 1999 it reached 13,6%. The country emerged from the recession in the 4th quarter of 1999.\n\nTuđman died in 1999 and in the early 2000 parliamentary elections, the nationalist Croatian Democratic Union (HDZ) government was replaced by a center-left coalition, with Ivica Račan as prime minister. At the same time, presidential elections were held which were won by a moderate, Stjepan Mesić.\n\nThe new Račan government amended the Constitution, changing the political system from a semi-presidential system to a parliamentary system, transferring most executive presidential powers from the president onto the institutions of the Parliament and the Prime Minister. Nevertheless, the President remained the Commander-in-Chief, and notably used this power in response to the Twelve Generals' Letter.\n\nEconomic growth picked up in 2000 following the recession. The new government started several large building projects, including state-sponsored housing and the building of the vital Zagreb-Split Highway, today's A1. Economic growth in the 2000s was stimulated by a credit boom led by newly privatized banks, capital investment, especially in road construction, a rebound in tourism and credit-driven consumer spending. Inflation remained tame and the currency, the kuna, stable.\nThe country rebounded from a mild recession in 1998/1999 and achieved notable economic growth during the following years. The unemployment rate would continue to rise until 2001 when it finally started falling. Return of refugees accelerated as many homes were rebuilt by the government; most Croats had already returned (except for some in Vukovar), whereas only a third of the Serbs had done so, impeded by unfavorable property laws as well as ethnic and economic issues.\n\nThe Račan government is often credited with bringing Croatia out of semi-isolation of the Tuđman era. Croatia became a World Trade Organization (WTO) member on 30 November 2000. The country signed a Stabilization and Association Agreement (SAA) with the European Union in October 2001, and applied for membership in February/March 2003.\n\nIn late 2003, new parliamentary elections were held and a reformed HDZ party won under leadership of Ivo Sanader, who became prime minister. After some delay caused by controversy over extradition of army generals to the ICTY, in 2004 the European Commission finally issued a recommendation that the accession negotiations with Croatia should begin. Its report on Croatia described it as a modern democratic society with a competent economy and the ability to take on further obligations, provided it continued the reform process.\n\nThe country was given EU applicant status on 18 June 2004 and a negotiations framework was set up in March 2005. Actual negotiations began after the capture of general Ante Gotovina in December 2005, which resolved outstanding issues with the ICTY in the Hague. However, numerous complications stalled the negotiating process, most notably during Slovenia's blockade of Croatia's EU accession from December 2008 until September 2009.\n\nIn August 2007, Croatia experienced a tragedy when during the fires that ravaged its coast, 12 firemen died as a result of a fire on Kornat island.\n\nSanader was reelected in the closely contested 2007 parliamentary election.\n\nThe October 2008 assassination of Ivo Pukanić was one of several prominent murders in Croatia at the time which were attributed to organized crime and associated by the media with an increased occurrence of crime in Zagreb in 2008. In March 2008, an 18-year-old Bad Blue Boys football fan was killed in the Ribnjak Park, followed by a retaliation by other BBB members. A mob beating at a bus stop in the high-traffic Većeslav Holjevac Avenue ensued, with deadly consequences for 18 year old Luka Ritz. The October 2008 murder of Ivana Hodak, the daughter of Zvonimir Hodak, the lawyer who defended alleged war profiteer general Vladimir Zagorec, caused the Prime Minister Ivo Sanader to fire Marijan Benko, chief of Zagreb Police Department, Berislav Rončević, Minister of Internal Affairs and Ana Lovrin, Minister of Justice. Rončević was replaced by Tomislav Karamarko and Lovrin by Ivan Šimonović. In February 2009, the Hodak murder was attributed by the police to a homeless unemployed man blaming Zvonimir Hodak for his problems in life.\n\nIn June 2009, Sanader abruptly resigned his post, leaving scarce explanation for his actions, and rumours of involvement in various criminal cases became increasingly rampant.\n\nJadranka Kosor assumed the head of the government following Sanader's resignation. Kosor introduced austerity measures to counter the economic crisis and launched an anti-corruption campaign aimed at public officials.\n\nJadranka Kosor signed an agreement with Borut Pahor, the premier of Slovenia, in November 2009, that ended Slovenia's blockade of Croatia's EU accession and allowed Croatian EU entry negotiations to proceed.\n\nIn the first round of the 2010 presidential election the HDZ candidate Andrija Hebrang achieved an embarrassing 12% claiming third place, the lowest result for an HDZ presidential candidate ever. Ivo Josipović, the candidate of the largest opposition party, the Social Democratic Party of Croatia, won a landslide victory in the resulting runoff on 10 January.\n\nIvo Sanader tried to come back in HDZ in early 2010, but was then ejected, charged for corruption by authorities, and later arrested in Austria.\n\nIn June 2010, Kosor proposed loosening the labor law and making it more business friendly, in order to foster economic growth. The proposed new labour law would have set a six-month deadline for hammering out a new collective agreement after the existing one expires. After that, the workers' rights would be subject to separate agreements with individual employers instead. The changing of the labour law was greatly opposed by five trade unions: a petition demanding a referendum gathered 813,016 signatures, far more than the required 449,506 signatures (10 percent of all voters in Croatia), in the first successful popular referendum attempt.\n\nOpinion polling was done for the prospective referendum: an Ipsos Puls for Nova TV poll of 8 July 2010 at a sample of 646 indicated 64% would support the referendum, 15% would be against, and 21% were undecided.\nCRO Demoskop also polled on the matter on 1 and 2 July at a sample of 1300, and found 88.6% of the polled would support it.\n\nAfter the Ministry of Administration completed its examination of the signatures, the Croatian Government first hinted that of all submitted signatures, no more than 330 thousand are valid, which would be insufficient for starting a referendum. After a public backlash, they nevertheless passed the signatures on to the Parliament. The government then decided to withdraw the reform proposals on 3 September 2010. The Croatian Parliament could not decide conclusively whether this rendered the referendum proposal moot or not, and instead passed the judgement on to the Constitutional Court of Croatia. The court decided on 20 October 2010 that there was no longer any need to hold the referendum. It ordered the government not to subject any changes to the labor law in the following year.\n\nThe government and labour unions later agreed that there would be a different referendum instead, asking the question \"Do you agree that a referendum must be called if so requested by 200,000 registered voters, and that the time for the collection of the required number of voters' signatures should be 30 days?\". It was planned to be held at the same time as the EU accession referendum, but did not happen.\n\nThe 2011 Croatian protests were anti-government street protests in Croatia started on 22 February 2011, after a call to protest over the Internet, and continued almost daily. The protests brought together diverse political persuasions in response to recent government corruption scandals and worries regarding upcoming EU accession, and called for the resignation of Kosor and early elections. They were met by a violent police reaction and a ban on assembly in front of the Croatian Parliament in Zagreb. On 26 February, tens of thousands of protesters met in the Croatian capital Zagreb's Ban Jelačić Square to express their support for indicted Croatian War of Independence veterans and demand for Kosor's government to resign. Several dozen people were injured and arrested as anti-government protests degenerated into clashes with police.\nIn the following few weeks the number of protesters rose to some 10,000 people, but later the protests gradually stopped.\n\nOn 30 June 2011 the EU accession agreement was concluded, giving Croatia the all-clear to join, with a projected accession date of 1 July 2013.\n\nThe Croatian parliamentary election, 2011 was held on 4 December 2011, and the Kukuriku coalition won. Zoran Milanović became the prime minister of the coalition government.\n\nIn January 2012, the new government organized a referendum for EU membership that passed with 66.27%. After the referendum, the Sabor ratified the accession treaty, the Treaty of Accession 2011.\n\nThe Milanović cabinet endured a major change when the first deputy prime minister Radimir Čačić resigned in November 2012, having been convicted of vehicular manslaughter in Hungary.\n\nIn the Trial of Gotovina et al, following an initial guilty verdict in April 2011, Ante Gotovina and Mladen Markač were ultimately acquitted in November 2012. Mirjan Damaška, a law professor at the University of Yale stated for the occasion that the theory about the \"joint criminal enterprise\" would have caused historical, political and law complications for Croatia, but that, as a result of the appeal, Croatia's founding has been internationally recognized as legal via its Operation Storm offensive.\n\nFollowing the successful ratification of its accession treaty in all existing EU member states, Croatia joined the European Union on 1 July 2013.\n\nShortly after joining the European union a question of extradition of Josip Perković arose. A member of Yugoslavian secret service UDBA, Germany accused Perković of murder of a Croat citizen Stjepan Đureković ( who is suspected to have stolen 200 million dollars from INA petrol company and has subsequently fled to Germany ).\nAt first Croatia was unwilling to extradite Perković under the justification that its constitution prohibited it ( all political crimes fall barred after 2002 ). However, when threatened with sanctions in case of non compliance, the parliament quickly voted to change the constitution to allow the extradition. Finally, the law is to come into effect on January the first 2014.\n\nOn December the first 2013, Croatia held its third referendum since becoming independent. The referendum question was \"Do you define marriage as a union between a man and a woman\", 65% of Croats voted yes, however, with a significantly low turnout of only 38% of eligible voters. The referendum was organized by the ultra-conservative group \"In the name of family\"'. The Catholic Church urged people to vote yes, while the government, 88 civil society organizations and local celebrities advised citizens to vote no. The referendum has raised much controversy and increase of violence against LGBT people in Croatian for this and following years. Same-sex registered partnership was introduced the following year, granting same-sex couples equal rights to marriage, except the LGBT adoption.\n\nThe sixth presidential election since independence took place in 2014 and 2015, with a first round held on 28 December 2014 between four candidates: the incumbent Ivo Josipović in office since 2010 and supported by the ruling coalition, former foreign minister and NATO official Kolinda Grabar-Kitarović nominated by the opposition HDZ, Živi zid activist Ivan Sinčić and right-wing populist Milan Kujundžić. Josipović and Grabar-Kitarović won the most votes, but fell short of the required 50% + 1 vote needed to win outright and proceeded to the second round held on 11 January 2015. In the second round Kolinda Grabar-Kitarović was elected the first female president of croatia, winning with 50,74% of the vote and receiving 32.509 votes more than Josipović, thus making the latter the first Croatian president not to succeed in being reelected to office and also resulting in the narrowest margin of victory in any presidential election to date in Croatia.\n\nIn 2015, after Hungary built a fence along its border with Serbia, over 17000 refugees crossed the border into Croatia. They are seeking an alternative route into the Schengen area, as part of the refugee crisis, with most refugees fleeing the Syrian civil war and Iraq War (2014–present). As the Croatian Prime minister Zoran Milanović stated that Croatia could not become a \"refugee camp\" and that it had become overwhelmed by the number of refugees entering the country, it was decided that Croatia would stop registering them and that a certain number of refugees be let into Hungary and Slovenia, attempts were made, however, by the authorities of those countries to obstruct the passage of refugees. The refugee crisis and immigration may also, for the first time in Croatia's history, become a subject of political debate during the next Croatian parliamentary election to be held in 2015.\n"}
{"id": "199450", "url": "https://en.wikipedia.org/wiki?curid=199450", "title": "Information industry", "text": "Information industry\n\nThe information industry or information industries are industries that are information intensive in one way or the other. It is considered one of the most important economic sectors for a variety of reasons. \n\nThere are many different kinds of information industries, and many different ways to classify them. Although there is no standard or distinctively better way of organizing those different views, the following section offers a review of what the term \"information industry\" might entail, and why. Alternative conceptualizations are that of knowledge industry and information-related occupation. The term \"information industry\" is mostly identified with computer programming, system design, telecommunications, and others. \n\nFirst, there are companies which produce and sell information in the form of goods or services. Media products such as television programs and movies, published books and periodicals would constitute probably among the most accepted part of what information goods can be. Some information is provided not as a tangible commodity but as a service. Consulting is among the least controversial of this kind. However, even for this category, disagreements can occur due to the vagueness of the term \"information.\" For some, information is knowledge about a subject, something one can use to improve the performance of other activities—it does not include arts and entertainments. For others, information is something that is mentally processed and consumed, either to improve other activities (such as production) or for personal enjoyment; it would include artists and architects. For yet others, information may include anything that has to do with sensation, and therefore information industries may include even such things as restaurant, amusement parks, and prostitution to the extent that food, park ride, and sexual intercourse have to do with senses. In spite of the definitional problems, industries producing information goods and services are called information industries.\n\nSecond, there are information processing services. Some services, such as legal services, banking, insurance, computer programming, data processing, testing, and market research, require intensive and intellectual processing of information. Although those services do not necessarily provide information, they often offer expertise in making decisions on behalf of clients. These kinds of service industries can be regarded as an information-intensive part of various industries that is externalized and specialized. \n\nThird, there are industries that are vital to the dissemination of the information goods mentioned above. For example, telephone, broadcasting and book retail industries do not produce much information, but their core business is to disseminate information others produced. These industries handle predominantly information and can be distinguished from wholesale or retail industries in general. It is just a coincidence, one can argue, that some of those industries are separately existing from the more obvious information-producing industries. For example, in the United States, as well as some other countries, broadcasting stations produce very limited amount of programs they broadcast. But this is not the only possible form of division of labor. If legal, economic, cultural, and historical circumstances were different, the broadcasters would have been the producers of their own programs. Therefore, in order to capture the information related activities of the economy, it might be a good idea to include this type of industry. These industries show how much of an economy is about information, as opposed to materials. It is useful to differentiate production of valuable information from processing that information in a sophisticated way, from the movement of information. \n\nFourth, there are manufacturers of information-processing devices that require research and sophisticated decision-making. These products are vital to information-processing activities of above mentioned industries. The products include computers of various levels and many other microelectronic devices, as well as software programs. Printing and copying machines, measurement and recording devices of various kinds, electronic or otherwise, are also in this category. The role of these tools are to automate certain information-processing activities. The use of some of these tools may be very simple (as in the case of some printing), and the processing done by the tools may be very simple (as in copying and some calculations) rather than intellectual and sophisticated. In other words, the specialization of these industries in an economy is neither production of information nor sophisticated decision-making. Instead, this segment serves as an infrastructure for those activities, making production of information and decision-making services will be a lot less efficient. In addition, these industries tend to be \"high-tech\" or research intensive - trying to find more efficient ways to boost efficiency of information production and sophisticated decision-making. For example, the function of a standard calculator is quite simple and it is easy to how to use it. However, manufacturing a well-functioning standard calculator takes a lot of processes, far more than the task of calculation performed by the users. \n\nFifth, there are very research-intensive industries that do not serve as infrastructure to information-production or sophisticated decision-making. Pharmaceutical, food-processing, some apparel design, and some other \"high-tech\" industries belong to this type. These products are not exclusively for information production or sophisticated decision-making, although many are helpful. Some services, such as medical examination are in this category as well. One can say these industries involve a great deal of sophisticated decision-making, although that part is combined with manufacturing or \"non-informational\" activities. \n\nFinally, there are industries that are not research intensive, but serve as infrastructure for information production and sophisticated decision-making. Manufacturing of office furniture would be a good example, although it sometimes involves research in ergonomics and development of new materials. \n\nAs stated above, this list of candidates for information industries is not a definitive way of organizing differences that researchers may pay attention to when they define the term. Among the difficulties is, for example, the position of advertising industry.\n\nInformation industries are considered important for several reasons. Even among the experts who think industries are important, disagreements may exist regarding which reason to accept and which to reject.\n\nFirst, information industries is a rapidly growing part of economy. The demand for information goods and services from consumers is increasing. In case of consumers, media including music and motion picture, personal computers, video game-related industries, are among the information industries. In case of businesses, information industries include computer programming, system design, so-called FIRE (finance, insurance, and real estate) industries, telecommunications, and others. When demand for these industries are growing nationally or internationally, that creates an opportunity for an urban, regional, or national economy to grow rapidly by specializing on these sectors. \n\nSecond, information industries are considered to boost innovation and productivity of other industries. An economy with a strong information industry might be a more competitive one than others, other factors being equal. \n\nThird, some believe that the effect of the changing economic structure (or composition of industries within an economy) is related to the broader social change. As information becomes the central part of our economic activities we evolve into an \"information society\", with an increased role of mass media, digital technologies, and other mediated information in our daily life, leisure activities, social life, work, politics, education, art, and many other aspects of society.\n\n"}
{"id": "2934522", "url": "https://en.wikipedia.org/wiki?curid=2934522", "title": "Iosif Amusin", "text": "Iosif Amusin\n\nIosif Davidovich Amusin (; French: \"Joseph Amoussine\", 29 November 1910, Vitebsk – 12 June 1984, Leningrad) was a Soviet historian, orientalist, hebraist and papyrologyst, was specialist in the history of the Ancient Near East and Qumran studies. \n\nAmusin was twice (in 1928 and 1938) arrested and sentenced for Zionist connections and \"anti-Soviet\" activity (acquitted posthumously in 1989). Graduated from the Historical Faculty of Leningrad University (1935–1941). Served as a medical officer during the Second World War.\n\nAfter 1945, Amusin taught ancient history at the Leningrad Pedagogical Institute and the Leningrad University until the anti-Semitic campaign against the so-called \"cosmopolitanism,\" when he lost his job and, after a long period of unemployment, began lecturing at the Ulianovsk Pedagogical Institute (1950–1954).\n\nUpon returning in Leningrad in 1954, Amusin became a research fellow at the Institute of Archaeology and the Institute of Oriental Studies of the Soviet Academy of Sciences in Leningrad. From the late 1950s, he published about 100 works on the Qumran and Dead Sea Scrolls.\n"}
{"id": "5921009", "url": "https://en.wikipedia.org/wiki?curid=5921009", "title": "Janet Coleman", "text": "Janet Coleman\n\nJanet Coleman FRHistS (born 1945, New York City) is a British academic and historian of political theory.\n\nShe is currently the Professor of Ancient and Medieval Political Thought at the London School of Economics. She was the first woman to receive a chair in the LSE government department. Her research interests include ancient Greek and Roman political thought, medieval philosophy, and theories of citizenship and the state.\n\nColeman studied at L'Ecole Pratique des Hautes Etudes in Paris and received her Ph.D. degree from Yale University. She has held teaching appointments in politics at Exeter University and on the History Faculty of the University of Cambridge. \n\nIn 1980 she co-founded (with Iain Hampsher-Monk) the academic journal \"History of Political Thought\", which she continues to co-edit. She is a Fellow of the Royal Historical Society.\n\nColeman has taught at LSE since 1989, where from 2001 to 2004 she held a Leverhulme Major Research Fellowship. Her lectures in the introductory government course at the LSE are known for her attempts to \"'be' political philosophers from the ancient Greeks to Machiavelli.\" Coleman plans to retire in 2010. She has been offered a Global Distinguished Professorship at New York University.\n\nColeman resides in Cambridge.\n\n\n"}
{"id": "15565663", "url": "https://en.wikipedia.org/wiki?curid=15565663", "title": "John A. Murphy", "text": "John A. Murphy\n\nJohn A. Murphy (born 17 January 1927) is an Irish historian and a former senator. He is currently Emeritus Professor of history at University College Cork (UCC).\n\nMurphy was born in Macroom, County Cork, and has said he was very bookish as a boy. He won a County Council scholarship in 1945 to study history at UCC, and graduated in 1948 with a first-class honours degree and first place in both History and Latin. He took an MA in Cork before taking up a teaching post at the diocesan seminary at Farranferris in Cork city.\n\nIn 1960 he became an assistant lecturer at UCC, and was appointed Professor of Irish History in 1971, holding that chair until his retirement in 1990. His 1975 book \"Ireland in the Twentieth Century\" was one of the first surveys of contemporary Irish history.\n\nFrom 1977 to 1982, and between 1987 and 1992, Murphy represented the National University of Ireland constituency as an independent member of Seanad Éireann. As a senator, he was noted for his advocacy of political and cultural pluralism. Earlier he had been a supporter of Noël Browne's Mother and Child Scheme.\n\nOn 13 May 2015, in the run up to the Irish marriage equality referendum, he wrote to \"The Irish Times\", describing the proposed constitutional amendment to permit same-sex marriage as \"grotesque nonsense.\" \n\n\n"}
{"id": "59024285", "url": "https://en.wikipedia.org/wiki?curid=59024285", "title": "Jordan Antiquities Database and Information System", "text": "Jordan Antiquities Database and Information System\n\nThe Jordan Antiquities Database and Information System (JADIS) was a computer database of antiquities in Jordan, the first of its kind in the Arab world. It was established by the Department of Antiquities in 1990, in cooperation with the American Center for Oriental Research in Amman and sponsored by the United States Agency for International Development. JADIS was in use until 2002, when it was superseded by a new system, MEGA-J. Over 10,841 antiquities were registered in the database.\n\n"}
{"id": "2423733", "url": "https://en.wikipedia.org/wiki?curid=2423733", "title": "Joseph Kervyn de Lettenhove", "text": "Joseph Kervyn de Lettenhove\n\nJoseph-Marie-Bruno-Constantin, Baron Kervyn de Lettenhove (17 August 1817 – 3 April 1891) was a Belgian historian and politician.\n\nHe was a member of the Catholic Constitutional party and sat in the Belgian Chamber of Deputies as member for Eeklo. In 1870 he was appointed a member of the cabinet of Jules Joseph d'Anethan as minister of the interior. However his official career was short. The cabinet appointed as governor of Limburg one Decker, who had been entangled in the financial speculations of Langand-Dumonceau by which the whole clerical party had been discredited, and which provoked riots. The cabinet was forced to resign, and thereafter Kervyn de Lettenhove devoted himself entirely to literature and history.\n\nHe had already become known as the author of a book on Jean Froissart (Brussels, 1855), which was crowned by the French Academy. He was also a correspondent of foreign scientific societies, and preëminent in his own country as an investigator of the national antiquities. He made translations of some of Milton's shorter poems (1839) and edited the \"Lettres et négociations de Philippe de Commines\" (1867).\n\nHe edited a series of chronicles: \"Chroniques relatives a l'histoire de la Belgique sous la domination des ducs de Bourgogne\" (Brussels, 1870–1873), and \"Relations politiques des Pays Bas et de l'Angleterre sous le regne de Philippe II\" (Brussels, 1882–1892). He wrote a history of \"Les Hugenots et les Gueux\" (Bruges, 1883–1885) in the spirit of a violent Roman Catholic partisan, but with much industry and learning. Other works include:\n\n\nHe died at Sint-Michiels near Bruges in 1891, the community in which he was born in 1817. One of his children was art historian Henri Kervyn de Lettenhove.\n\n\n"}
{"id": "54674740", "url": "https://en.wikipedia.org/wiki?curid=54674740", "title": "Karmic Debts", "text": "Karmic Debts\n\nThree lives. Three destinies. Three eras.\n\n\"Karmic Debts\" is a 2013 novel, the eight book by the Estonian author Inga Raitar.\n\nIt consists of three entwined stories that takes the readers to a journey from a secluded Tibetan Monastery in the 16th century, through the Great Famine in the 1800s, all the way to present day. The karmic threads start to twist and intertwine between the stories leaving you questioning - why does the fateful karma interfere with our lives and how far does it reach? How do we end up in the relationships that we have? What is the thing bringing them forward?\n\nThe book was translated to English in 2017, and can be found as a Kindle e-book.\n\nIt would appear as if there are three distinct tales unraveling in this novel – one taking place long ago in a Tibetan monastery, another during the Great Famine in Ireland and the third somewhere in the present day. The book is presented by the main characters point of view and the stories are linked to one another in a way that makes it seem, as if time does not flow in a linear way.\n\nIn each story, the characters are looking for answers to questions about real love, and how or why does it come to our life? Why do you feel at times that you have known certain people for a lifetime before you even really meet? .\n\nThe book was originally published as a series of short-stories in the Estonian biggest women's magazine \"Naised\", but it was never finished. The author wanted to add an extra dimension to the story and stopped writing to the magazine each month.\n\nKarmic Debts\" is a romantic love story and, at the same time, an exciting fantasy tale about the mysterious strings of karma that are hidden behind all of our relationships.\n\nIris, a young and successful woman, meets a stranger on a business trip, who is far from her “type”. Nonetheless, a spark ignites that turns her life as she knows it, upside down.\n\nThe beautiful bastard daughter of an Irish lord, Irene, falls in love with a poor farmers boy. This will determine the life of the whole village and set her fate.\n\nA devotee named Ikaro bears witness to his best friend’s foul secret. The boy must choose, whether to betray everything he believes holy or acknowledge what he truly wants.\n"}
{"id": "40703996", "url": "https://en.wikipedia.org/wiki?curid=40703996", "title": "Kinelarty", "text": "Kinelarty\n\nKinelarty () is a former Irish district and barony in County Down, Northern Ireland. It lies east of the centre of the county, and is bordered by five other baronies: Iveagh Upper, Lower Half to the west; Lecale Upper to the south and south-east; Lecale Lower to the west; Castlereagh Upper to the north; and Iveagh Lower, Upper Half to the north-west.\n\nKinelarty derives its name from the Irish \"Cineál Fhaghartaigh\", which means Faghartach's (Fogarty's) kindred. This was the name of an Irish district, the chiefs of which were the \"Mac Artáin\" (MacCartan).\n\nThe Mac Artáin descend from Artán, grandson of Fagartaigh of Uí Echach Cobo (anglicised as Iveagh). The Mac Artáin, as is professed throughout the \"Annals of the Four Masters\" and in parts the \"Annals of Innisfallen\" reigned supreme as High Kings for considerable periods of history further positioning themselves as lords of Iveagh.\n\nBy 1177, the Norman John de Courcy had arrived in Ulster and set about conquering most of eastern Ulster, forming the Earldom of Ulster. The only clans who were able to exist independently in eastern Ulster during this time were in the interior away from the sea-coast, where the Uí Tuirtre, north of Lough Neagh, and the Uí Echach Cobo.\n\nDuring the 14th century the Normans in Ulster faded as a result of the Bruce Invasion, with this period seeing the Mac Aonghusa and Mac Artáin clans emerge and expand from Uibh Echach, with their respective territories becoming the basis of the future baronies of Iveagh and Kinelarty. With the fall of Norman power in Ulster, the \"Clann Aodha Bhuidhe\" (Clandeboy) branch of the O'Neills took control of north Down, with the chiefs of Kinelarty eventually becoming their tributaries.\n\nEarly mentions in regards to Kinelarty and the MacCartans include: \n\n\nBy the Elizabethan era, Kinelarty was simultaneously known under variations of \"MacCarton's country\", and during the reign of James I simultaneously under variations of \"Killenarten\", before once again becoming simply known as variations of Kinelarty.\n\nBelow is a list of settlements in Kinelarty:\n\nBelow is a list of civil parishes in Kinelarty:\n"}
{"id": "25515276", "url": "https://en.wikipedia.org/wiki?curid=25515276", "title": "Krzysztof Kawalec", "text": "Krzysztof Kawalec\n\nProf. dr hab. Krzysztof Maria Kawalec (born 1954) is a Polish historian, lecturer and professor at the University of Wrocław. He specializes in the history of Polish political thought of 19th and 20th centuries, Second Polish Republic and right-wing National Democracy political camp.\n\nKawalec graduated from Juliusz Słowacki Grammar School in Wrocław in 1973 and later entered studies at the University of Wrocław, where he studied e.g. under prof. Henryk Zieliński. Since 1978 he works as academician at the Institute of History of University of Wrocław. Since the creation of Solidarity trade union at his university in 1980 he was a member of this organization.\n\nHe received his doctoral degree in September 1996 and a professor degree in 2001. His book \"Spadkobiercy niepokornych: Dzieje polskiej myśli politycznej 1918-1939\" (Heirs of Unyielding: History of Polish Political Thought 1918-1939), published in 2000, won the KLIO Prize (Nagroda KLIO) in 2000 for the best scientific monography.\n\n"}
{"id": "1956592", "url": "https://en.wikipedia.org/wiki?curid=1956592", "title": "Leopold Labedz", "text": "Leopold Labedz\n\nLeopold Łabędź (22 January 1920 – 22 March 1993) was an anti-communist Anglo-Polish commentator on the Soviet Union.\n\nŁabędź was born to a Polish Jewish doctor in Russia. The family soon returned to Warsaw and the young Łabędź decided to follow his father into the medical profession. He studied medicine in Paris. In 1939, he fled to the Soviet zone of occupation and was imprisoned by the Soviets in the Gulag.\n\nHe left the Soviet Union in 1942 as part of the Polish Army led by General Władysław Anders. After the war he studied at Bologna University before settling in London, where he studied at the London School of Economics. Strongly anti-communist, Łabędź edited \"Survey\" journal and headed the London office of Committee for the Defense of Workers known by its Polish abbreviation as KOR.\n\nŁabędź often campaigned for the \"Solidarity\" union in Poland, and for political prisoners in the Soviet Union. Łabędź was one of Aleksandr Solzhenitsyn's principal champions in the West and often defended the Russian writer against the charge of anti-semitism.\n\n\n"}
{"id": "7460352", "url": "https://en.wikipedia.org/wiki?curid=7460352", "title": "List of martyrs' monuments and memorials", "text": "List of martyrs' monuments and memorials\n\nThis is a list of martyrs' monuments and memorials sorted by country:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are other memorials at Lewes in Sussex,\nand in other places.\n\n"}
{"id": "21367006", "url": "https://en.wikipedia.org/wiki?curid=21367006", "title": "Magnus Ilmjärv", "text": "Magnus Ilmjärv\n\nMagnus Ilmjärv (born 31 August 1961 in Viljandi) is an Estonian historian. He graduated from the University of Tartu \"cum laude\" in 1988 and defended his MA thesis in 1997, in June 2004 defended his PhD at the University of Helsinki. Since 2006, Ilmjärv has been the director of the Estonian Institute of History (Ajaloo Instituut). He specializes in Baltic history and international relations between two World Wars. His most notable work is arguably the \"Silent Submission\", which covers the loss of independence of the three Baltic states. In Estonia, he is probably best known for his harsh criticism on the Estonian leadership in 1940, incl. Konstantin Päts.\n\n"}
{"id": "9189804", "url": "https://en.wikipedia.org/wiki?curid=9189804", "title": "Martin J. S. Rudwick", "text": "Martin J. S. Rudwick\n\nMartin John Spencer Rudwick (born 1932) is a British geologist, historian, and academic. He is an emeritus professor of History at the University of California, San Diego and an affiliated research scholar at Cambridge University's Department of History and Philosophy of Science. His principal field of study is the history of the earth sciences; his work has been described as the \"definitive histories of the pre-Darwinian earth sciences\". Rudwick was an early scholar to critique the conflict thesis regarding religion and science.\n\nRudwick was awarded the Sue Tyler Friedman Medal in 1988. In 2008, he was elected a Fellow of the British Academy (FBA). He was the recipient of the 2007 George Sarton Medal from the History of Science Society.\n\n\n"}
{"id": "7092541", "url": "https://en.wikipedia.org/wiki?curid=7092541", "title": "Materialism and Empirio-criticism", "text": "Materialism and Empirio-criticism\n\nMaterialism and Empirio-criticism (Russian: \"Материализм и эмпириокритицизм, Materializm i empiriokrititsizm\") is a philosophical work by Vladimir Lenin, published in 1909. It was an obligatory subject of study in all institutions of higher education in the Soviet Union, as a seminal work of dialectical materialism, a part of the curriculum called \"Marxist–Leninist Philosophy\". Lenin argued that human perceptions correctly and accurately reflect the objective external world.\n\nLenin formulates the fundamental philosophical contradiction between idealism and materialism as follows:\n\"Materialism is the recognition of 'objects in themselves' or objects outside the mind; the ideas and sensations are copies or images of these objects. The opposite doctrine (idealism) says: the objects do not exist, outside the mind '; they are 'connections of sensations'.\"\n\nThe book, whose full title is \"Materialism and Empirio-criticism. Critical Comments on a Reactionary Philosophy\", was written by Lenin from February through October 1908 while he was exiled in Geneva and London and was published in Moscow in May 1909 by Zveno Publishers. The original manuscript and preparatory materials have been lost.\n\nMost of the book was written when Lenin was in Geneva, apart from the one month spent in London, where he visited the library of the British Museum to access modern philosophical and natural science material. The index lists in excess of 200 sources for the book.\n\nIn December 1908, Lenin moved from Geneva to Paris, where he worked until April 1909 on correcting the proofs. Some passages were edited to avoid tsarist censorship. It was published in Imperial Russia with great difficulty. Lenin insisted on the rapid distribution of the book and stressed that \"not only literary but also serious political obligations\" were involved in its publication.\n\nThe book was written as a reaction and criticism to the three-volume work \"Empiriomonism\" (1904–1906) by Alexander Bogdanov, his political opponent within the Party. In June 1909, Bogdanov was defeated at a Bolshevik mini-conference in Paris and expelled from the Central Committee, but he still retained a relevant role in the Party's left wing. He participated in the Russian Revolution and after 1917, he was appointed director of the Socialist Academy of Social Sciences.\n\n\"Materialism and Empirio-criticism\" was republished in Russian in 1920 with an introduction attacking Bogdanov by Vladimir Nevsky. It subsequently appeared in over 20 languages and acquired canonical status in Marxist–Leninist philosophy.\n\nIn Chapter I: The Epistemology of Empiriocriticism and Dialectical Materialism I, Lenin then discusses the \"solipsism\" of Mach and Avenarius.\n\nIn Chapter II: The Epistemology of Empiriocriticism and Dialectical Materialism II, Lenin, Tschernow and Basarov confront the views of Ludwig Feuerbach, Joseph Dietzgen and Friedrich Engels and comment on the criterion of practice in epistemology.\n\nIn Chapter III: The Epistemology of Empiriocriticism and Dialectical Materialism III, Lenin seeks to define \"matter\" and \"experience\" and addresses the questions of causality and necessity in nature as well as \"freedom and necessity\" and the \"principle of the economy of thought\".\n\nIn Chapter IV: The philosophical idealists as collaborators and successors of empirio-criticism, Lenin deals with left and right Kant criticism, with the philosophy of immanence, Bogdanov's empiri-monism, and the critique of Hermann von Helmholtz on the \"theory of symbols.\"\n\nIn chapter V: The latest revolution in science and philosophical idealism, Lenin deals with the thesis that \"the crisis of physics\" \"has disappeared matter\". In this context he speaks of a \"physical idealism\" and notes (on p. 260): \"For the only\" property \"of matter to whose acknowledgment philosophical materialism is bound is the property of being objective reality, outside of our consciousness.\" \n\nIn Chapter VI: Empiriocriticism and Historical Materialism, Lenin discusses authors such as Bogdanov, Suvorov, Ernst Haeckel and Ernst Mach.\n\nIn an addition to Chapter IV, Lenin addresses the question: \"From what side did N. G. Chernyshevsky criticize Kantianism?\"\n\nLenin cites a broad range of philosophers:\n\n\n\n\n\n"}
{"id": "58469218", "url": "https://en.wikipedia.org/wiki?curid=58469218", "title": "Maxime Steinberg", "text": "Maxime Steinberg\n\nMaxime Steinberg was a Belgian historian, described as \"Belgium’s principal Holocaust historian\".\n\nMaxime Steinberg was born into a Jewish family in Brussels, Belgium on 13 December 1936. His father had immigrated from Poland in 1930. During World War II, his parents were arrested and deported to Nazi concentation camps. His mother was killed. Maxime and his brother were hidden in rural Walloon Brabant.\n\nSteinberg studied at the Free University of Brussels (ULB) under Jean Stengers, initially interested in the history of the Belgian far-left. He worked as a teacher. A communism, Steinberg was also active in Belgium's public sector trade union. In 1982, he returned to ULB to work as an associate professor at the Institute for the Study of Judaism (\"Institut d'Etudes du Judaïsme\"). He completed his doctoral thesis on the Holocaust in Belgium in 1987 under Stenger's supervision.\n\nSteinberg is best-known for his magnum opus on the Holocaust in Belgium, published as \"L'Étoile et le Fusil\" (\"The Star and the Rifle\") between 1983 and 1987, which \"revolutionised historiography on the persecution of the Jews in Belgium\". It was published in three tomes (four volumes), representing the first scholarly narrative on the subject. According to Lieven Saerens, \"L'Étoile et le Fusil\"\n\nSteinberg was also known as a public historian. He was a historical expert called during the much-publicised trial of Kurt Asche (1980-81) and in designing the exhibition of Museum of the Deportation and Resistance in Mechelen, Belgium. \n\n"}
{"id": "12303979", "url": "https://en.wikipedia.org/wiki?curid=12303979", "title": "Moshe Lewin", "text": "Moshe Lewin\n\nMoshe \"Misha\" Lewin, pronounced \"Luh-VENE\" (7 November 1921 – 14 August 2010), was a scholar of Russian and Soviet history. He was a major figure in the school of Soviet studies which emerged in the 1960s.\n\nMoshe Lewin was born in 1921 in Wilno, Poland (now Vilnius, Lithuania), the son of ethnic Jewish parents who later died in the Holocaust. Lewin lived in Poland for the first 20 years of his life, fleeing to the Soviet Union in June 1941 just ahead of the invading Nazi army. \n\nFor the next two years, Lewin worked as a collective farm worker and as a blast furnace operator in a metallurgical factory. In summer 1943, he enlisted in the Soviet army and was sent to officers' training school. He was promoted on the last day of the war.\n\nIn 1946, Lewin returned to Poland before emigrating to France. A believer in Labor Zionism from his youth, in 1951 Lewin emigrated again, this time to Israel, where he worked for a time on a kibbutz and as a journalist. In his thirties, he took up academic studies, receiving his Bachelor of Arts from Tel Aviv University, in 1961.\n\nThat same year, Lewin was awarded a research scholarship to the Sorbonne in Paris, where he studied the collectivization of Soviet agriculture. In 1964, he gained his Ph.D there.\n\nNewly qualified with his doctorate, Lewin was named Director of Study at l'École des hautes études, Paris, where he served from 1965 to 1966. During this time he converted his Sorbonne dissertation into a book, published in French in 1966 and in English two years later as \"Russian Peasants and Soviet Power\".\n\nThis monograph dealt with the Soviet grain procurement crisis of 1928 and the associated political battle, a bitter fight which resulted in a decision to forcibly collectivize Soviet agriculture. \n\nIn this work, Lewin emphasized collectivization as a practical (albeit extreme) solution to a real world problem facing the Soviet regime, one out of several potential solutions to a crisis situation. Rather than an inevitable and predestined action, collectivization was cast as a brutal manifestation of \"realpolitik\" — a view in marked contrast to the traditionalist historiography of the day. \"Russian Peasants and Soviet Power\" was initially projected as the first part of a long study of the social history of Soviet Russia down to 1934, although the project seems to have been abandoned, perhaps as duplicative of the work of British historians E.H. Carr and R.W. Davies.\n\nLewin's other 1968 publication, \"Lenin's Last Struggle\", was an extended essay that charted the evolution of Lenin's thinking about the growing bureaucracy of Soviet Russia. In it, Lewin additionally chronicled the politics of the post-Lenin succession struggle during the time of Lenin's final illness, emphasizing \"lost\" alternatives to the actual path of historical development. \n\nIn this book Lewin again offered a perspective in marked contrast to the voluminous writings of the totalitarianist school that then dominated academic writing about the Soviet Union, casting the USSR as a monolithic and fundamentally unchanging structure.\n\nFrom 1967 to 1968, Lewin was a senior fellow at Columbia University in New York City. Upon completion of his Columbia fellowship, he took a post as a research professor at Birmingham University, England from 1968 until 1978.\n\nDuring this period Lewin published \"Political Undercurrents in Soviet Economic Debates: From Bukharin to the Modern Reformers,\" which, along with the work of Princeton University professor Stephen F. Cohen, helped to restore the name and ideas of Nikolai Bukharin to the academic debate concerning the Soviet 1920s. Lewin noted that many of the same criticisms which Bukharin leveled against Stalin during the political battles of 1928 and 1929 in the USSR were later \"adopted by current reformers as their own,\" thereby adding a contemporary importance to the study of the historical past.\n\nAfter leaving Birmingham, Lewin returned to the United States. He took up a professorship at the University of Pennsylvania, where he remained until his retirement in 1995.\n\nAlthough regarded as a doyen of social history and a godfather of the so-called \"revisionist\" movement of young social historians who came to the fore in Soviet studies during the 1970s and 1980s, Lewin's own work largely centered on the relationship between high politics and economic policy. \n\nOne notable exception came with the publication in 1985 of a collection of Lewin's essays and lectures entitled \"The Making of the Soviet System.\" In this work, Lewin visited a number of key topics of social history such as rural social mores, popular religion, customary law in rural society, the social structure of the Russian peasantry, and social relations within Soviet industry. He emerged as a critic of the politicized \"What are they up to?\" orientation of Soviet studies, favoring a more apolitical perspective that attempted to answer the question, \"What makes the Russians tick?\"\n\nLewin's final works attempted to analyze the rise of Mikhail Gorbachev, and his brief efforts at top-down reform of the communist system, and to set the rise and fall of Soviet communism in historical perspective. \n\nIn his last book, \"The Soviet Century\" (2005), Lewin argued that the political and economic system of the former Soviet Union constituted a sort of \"bureaucratic absolutism\" akin to the Prussian bureaucratic monarchy of the 18th Century which had \"ceased to accomplish the task it had once been capable of performing\" and therefore given way.\n\nIn 1992, Lewin was honored with a \"Festschrift\" edited by historians Nick Lampert and Gábor Rittersporn entitled \"Stalinism: Its Nature and Aftermath: Essays in Honour of Moshe Lewin.\" Contributors to the volume included economic historians Alec Nove and R.W. Davies as well as key social historians such as Lewis Siegelbaum and Ronald Grigor Suny, among others.\n\nIn the Lewin \"Festschrift,\" co-editor Lampert summarized Lewin's work in the following manner:\n\"The scope of Lewin's explorations has been very wide, dealing with a panorama of social classes and groups, with the lower depths of society as well as the bosses, with informal social norms as well as formal law, with popular religion as well as established ideology. The range of his intellectual debts is also broad, owing as much to Weber as to Marx, emphasising as much the power of ideologies and myths in human behaviour as the weight of economic structure. The key thing is the perception of society as a socio-cultural whole, though Lewin always remained open to new pathways that might appear in the course of research, always eclectic in the best sense, always eschewing the pursuit of a grand theory for all history — a pursuit which only leads you away from the rich canvas of concrete human experience.\"\n\nMoshe Lewin died on 14 August 2010 in Paris. His papers are housed at the University of Pennsylvania in Philadelphia.\n\n\n"}
{"id": "2183239", "url": "https://en.wikipedia.org/wiki?curid=2183239", "title": "Ndogboyosoi War", "text": "Ndogboyosoi War\n\nThe Ndogboyosoi War, also known as the Bush Devil War, was an episode of political violence that occurred in 1982 between supporters of the All People's Congress (APC) and the Sierra Leone People's Party (SLPP) in Sierra Leone. The violence was centered in Pujehun District, especially in the Soro-Gbema chiefdom. It was triggered by the ruling APC party's alleged electoral manipulation and the intervention of a special squad of customs police against supporters of the SLPP candidate.\n\nThere was no process of reconciliation following the violence. Children of those killed in the fighting or of those who died in detention were among the first to join the Revolutionary United Front (RUF), a rebel group which began a civil war in eastern and southern Sierra Leone nine years later.\n"}
{"id": "3735153", "url": "https://en.wikipedia.org/wiki?curid=3735153", "title": "Non-circulating legal tender", "text": "Non-circulating legal tender\n\nNon-circulating legal tender (NCLT) refers to coins that are theoretically legal tender and could circulate but do not because their issue price, and/or their melt value at the time of issue is significantly above the arbitrary legal tender value placed thereon. They are sold to collectors and investors with no intention that they be used as money. Notable examples would include commemoratives, proofs, bullion coins, presentation sets, patterns and the like.\n\nSome coins intended as NCLT have historically circulated such as the 1893 World's Columbian Exposition Half Dollars which was a commemorative and the 1856 Flying Eagle cent which was a pattern.\n\nPrivate issues are not NCLT because they are not legal tender and are properly viewed as medals.\n"}
{"id": "44058323", "url": "https://en.wikipedia.org/wiki?curid=44058323", "title": "Petras Stankeras", "text": "Petras Stankeras\n\nPetras Stankeras (born 29 June 1948 in Trakai, Lithuania) is a Lithuanian historian specializing in World War II.\n\nIn 1972, Stankeras graduated from the Faculty of History of Vilnius Pedagogical Institute. He worked as a senior research fellow of the Lithuanian Central State Archive in 1973–1977 and an engineer of road construction in 1977–1982. In March 1982, Stankeras joined the Lithuanian Ministry of the Interior. He achieved the rank of lieutenant colonel. From 2000 to 25 November 2010, he was a career public employee of the Ministry of the Interior. Member of Lithuanian police veterans association.\n\nFrom the age of fifteen, Stankeras took a great interest in the history of World War II, German National Socialism, Italian Fascism, and the Lithuanian police. He put together a catalog of 20,000 personnel files of various war criminals and an archive of 100,000 photos.\n\nIn 2000, Stankeras defended his Ph.D. dissertation \"Lithuanian Police during the Nazi Occupation in 1941–1944 (Organizational Structure and Personnel)\" at Vytautas Magnus University. The work was published by the Genocide and Resistance Research Centre of Lithuania in 1998. Another book on the same topic was published in 2008 and partially translated into Russian in 2009. In 2014, Stankeras published \"Pralaimėta Adolfo Hitlerio kova\" (\"The Lost Struggle of Adolf Hitler\") in which he translated and annotated large excerpts from Hitler's \"Mein Kampf\". In addition, Stankeras coauthored 12 scientific books, published 250 articles in scientific and popular magazines in Lithuania, Poland and the United States, presented both at national and international academic conferences. Stankeras is a board member of the Lithuanian Military History Society.\n\nOn 8 November 2010, magazine \"Veidas\" published his article about the Nuremberg trials, in which the following sentence was printed: \"It is important as well that the Nuremberg process provided a legal basis for the legend of about 6 million supposedly murdered Jews even though in fact the court did not have a single document signed by Hitler ordering to exterminate Jews (this document, if it exists, has not been found, even though a million dollar reward has been promised)\". The sentence caught attention on 23 November and was interpreted to express denial of the Holocaust. Leonidas Donskis published a blog post while seven ambassadors (Great Britain, Estonia, Netherlands, Norway, France, Finland and Sweden) sent a letter to the Minister of Interior Raimundas Palaitis. Stankeras was forced to resign on 25 November 2010. In a subsequent interview Stankeras explained that he intended only to question the six-million figure, while editor-in-chief of \"Veidas\" admitted that the location of the word \"supposedly\" was a copy-editing error. Other commentators pointed out that the overall article was sympathetic to the Nazis on trial. The Prosecutor's Office of Vilnius County started a criminal pre-trial investigation, but dropped it in February 2011. Simon Wiesenthal Center condemned the prosecutor's decision, while \"Veidas\" lamented Stankeras' ruined reputation.\n\n"}
{"id": "4548177", "url": "https://en.wikipedia.org/wiki?curid=4548177", "title": "Rule of the Dukes", "text": "Rule of the Dukes\n\nThe Rule of the Dukes was an interregnum in the Lombard Kingdom of Italy (574/5–584/5) during which Italy was ruled by the Lombard dukes of the old Roman provinces and urban centres. The interregnum is said to have lasted a decade according to Paul the Deacon, but all other sources—the \"Fredegarii Chronicon\", the \"Origo Gentis Langobardorum\", the \"Chronicon Gothanum\", and the Copenhagen continuator of Prosper Tiro—accord it twelve. Here is how Paul describes the dukes' rule:\n\nAfter his death the Langobards had no king for ten years but were under dukes, and each one of the dukes held possession of his own city, Zaban of Ticinum, Wallari of Bergamus, Alichis of Brexia, Euin of Tridentum, Gisulf of Forum Julii. But there were thirty other dukes besides these in their own cities. In these days many of the noble Romans were killed from love of gain, and the remainder were divided among their \"guests\" and made tributaries, that they should pay the third part of their products to the Langobards. By these dukes of the Langobards in the seventh year from the coming of Alboin and of his whole people, the churches were despoiled, the priests killed, the cities overthrown, the people who had grown up like crops annihilated, and besides those regions which Alboin had taken, the greater part of Italy was seized and subjugated by the Langobards. \n\nThe \"Origo\" gives a shorter version of the same events:\n\nThe rest of the Langobards set over themselves a king named, Cleph, of the stock of Beleos, and Cleph reigned two years and died. And the dukes of the Langobards administered justice for twelve years and after these things they set up over themselves a king named Autari the son of Cleph. And Autari took as his wife Theudelenda, a daughter of Garipald and of Walderada from Bavaria.\nThe Lombards had entered the Italian peninsula in 568 under Alboin. Under Alboin's successor, Cleph, they continued to expand at the expense of the Byzantines. Cleph's reign was short and his rule hard. Upon his death, the Lombards did not elect another leader-king, leaving the territorial dukes the highest authorities in Lombard territories. According to Fredegar, they were forced to pay tribute to the Franks, and this lasted until the accession of Adaloald.\n\nThe dukes were unable to organise themselves under a single leader capable of continuing their successes against the Byzantines. When they invaded Frankish Provence (584/5), the Frankish kings Guntram and Childebert II counter-invaded northern Italy, took Trent, and opened negotiations with the emperor Tiberius II, sovereign of the hard-pressed exarchate of Ravenna. Finally, tired of disunion, fearing a pincer action from a Byzantine–Frankish alliance, and lacking the leadership necessary to withstand combined military forces, the dukes elected as king Authari. They ceded to him the old capital of Pavia and half of their ducal demesnes, though the fidelity to their oath with which this last promise was carried out is suspect. With the election of a king and the payment of tribute, the last Frankish troops still in Italy left.\n\nAmong the known reigning dukes of the times were:\n\n"}
{"id": "49228686", "url": "https://en.wikipedia.org/wiki?curid=49228686", "title": "Souk Al Asr", "text": "Souk Al Asr\n\nSouk Al Asr (; English: Era market) is one of the souks of Tunis, specialized in selling antique products. The souk is mainly intended for the middle class and poor.\n\nIt is located behind Bab Al Gorjani, one of the medina's doors and near three neighbourhoods inhabited by poor people of Tunis, i.e. Mellassine, Saida Manoubia and Helal City.\n\nThe souk didn't exist before the Husainid era (1705-1957), while other souks emerged under the Hafsid dynasty (1228-1537). It was a tiny souk that stood between prayers of the afternoon and the sunset (third and fourth prayers of the day).\n\nSouk Al Asr is totally the opposite of its own name as it solds only old products like household utensils, antique furniture and other rare items. This market is full of items that can be particularly useful and rare, that are not likely to be found elsewhere.\n"}
{"id": "35323955", "url": "https://en.wikipedia.org/wiki?curid=35323955", "title": "Timeline of deportations of French Jews to death camps", "text": "Timeline of deportations of French Jews to death camps\n\nThis is a timeline of deportations of French Jews to Nazi extermination camps in German-occupied Europe during World War II. The overall total of Jews deported from France is a minimum of 75,721.\n\n\n\nThe overall total of Jews deported from France is a minimum of 75,721.\n"}
{"id": "615926", "url": "https://en.wikipedia.org/wiki?curid=615926", "title": "Timeline of scientific experiments", "text": "Timeline of scientific experiments\n\nThe timeline below shows the date of publication of major scientific experiments:\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56114", "url": "https://en.wikipedia.org/wiki?curid=56114", "title": "Urbanization", "text": "Urbanization\n\nUrbanization refers to the population shift from rural to urban residency, the gradual increase in the proportion of people living in urban areas, and the ways in which each society adapts to this change. It is predominantly the process by which towns and cities are formed and become larger as more people begin living and working in central areas. Although the two concepts are sometimes used interchangeably, urbanization should be distinguished from urban growth: urbanization is \"the proportion of the total national population living in areas classed as urban\", while urban growth refers to \"the absolute number of people living in areas classed as urban\". The United Nations projected that half of the world's population would live in urban areas at the end of 2008. It is predicted that by 2050 about 64% of the developing world and 86% of the developed world will be urbanized. That is equivalent to approximately 3 billion urbanites by 2050, much of which will occur in Africa and Asia. Notably, the United Nations has also recently projected that nearly all global population growth from 2017 to 2030 will be absorbed by cities, about 1.1 billion new urbanites over the next 13 years.\n\nUrbanization is relevant to a range of disciplines, including urban planning, geography, sociology, economics, and public health. The phenomenon has been closely linked to modernization, industrialization, and the sociological process of rationalization. Urbanization can be seen as a specific condition at a set time (e.g. the proportion of total population or area in cities or towns) or as an increase in that condition over time. So urbanization can be quantified either in terms of, say, the level of urban development relative to the overall population, or as the rate at which the urban proportion of the population is increasing. Urbanization creates enormous social, economic and environmental changes, which provide an opportunity for sustainability with the “potential to use resources more efficiently, to create more sustainable land use and to protect the biodiversity of natural ecosystems.”\n\nUrbanization is not merely a modern phenomenon, but a rapid and historic transformation of human social roots on a global scale, whereby predominantly rural culture is being rapidly replaced by predominantly urban culture. The first major change in settlement patterns was the accumulation of hunter-gatherers into villages many thousand years ago. Village culture is characterized by common bloodlines, intimate relationships, and communal behavior, whereas urban culture is characterized by distant bloodlines, unfamiliar relations, and competitive behavior. This unprecedented movement of people is forecast to continue and intensify during the next few decades, mushrooming cities to sizes unthinkable only a century ago. As a result, the world urban population growth curve has up till recently followed a quadratic-hyperbolic pattern.<ref>\"Introduction to Social Macrodynamics: Secular Cycles and Millennial Trends.\" Moscow: URSS, 2006; Korotayev A.\nThe World System urbanization dynamics. \"History & Mathematics: Historical Dynamics and Development of Complex Societies\". Edited by Peter Turchin, Leonid Grinin, Andrey Korotayev, and Victor C. de Munck. Moscow: KomKniga, 2006. \n"}
{"id": "55891165", "url": "https://en.wikipedia.org/wiki?curid=55891165", "title": "Verene Shepherd", "text": "Verene Shepherd\n\nVerene Albertha Shepherd (née Lazarus; born 1951) is a Jamaican academic who is a professor of social history at the University of the West Indies in Mona. She is the director of the university's Institute for Gender and Development Studies, and specialises in Jamaican social history and diaspora studies.\n\nShepherd was born in Hopewell, Saint Mary Parish, one of the ten children of Ruthlyn and Alfred Lazarus. She attended Huffstead Basic School, Rosebank Primary School, and St. Mary High School, and then completed a teaching certificate at Shortwood Teachers' College. Shepherd went on to the University of the West Indies, where she completed a BA in history in 1976 and a M.Phil. in history in 1982. She was later awarded a PhD from the University of Cambridge in 1988 for her thesis on the economic history of colonial Jamaica.\n\nIn 1988, Shepherd joined the Department of History at the University of the West Indies. She was elevated to a full professorship in 2001, and in 2010 was appointed director of the Institute for Gender and Development Studies. She has served as president of the Association of Caribbean Historians, chair of the Jamaica National Heritage Trust, chair of the Jamaica National Bicentenary Committee. Shepherd specialises in Jamaican social history and diaspora studies. She is an advocate of reparations for slavery, and in 2016 was appointed co-chair of Jamaica's National Council on Reparations. Shepherd has also held several positions within the Office of the United Nations High Commissioner for Human Rights, serving as a member of the Working Group of Experts on People of African Descent (WGEPAD) and the Committee on the Elimination of Racial Discrimination. She was chair of WGEPAD from 2011 to 2014, and lobbied for the creation of the International Decade for People of African Descent.\n\nIn 2013, in her role as chair of WGEPAD, Shepherd was asked to inquire into Zwarte Piet (\"Black Pete\"). She authored a letter, on \"headed, official UN high commission for human rights paper\" to the Dutch government proposing that it move towards ending the tradition, and in a later interview with \"EenVandaag\" described the character as \"a throwback to slavery\". Her remark complicated and further polarized an ongoing national debate, accompanied by protests and social media campaigns, in particular as she was supposedly speaking on behalf of the UN when this was not actually the case. Her remarks that Sinterklaas, a national feast in the Netherlands central to Dutch culture, could just be done away with in its entirety because \"one Santa Claus is enough\", (apparently referring to the American version who partially descends from Sinterklaas but is decidedly not the same), caused much anger and unleashed a public reaction that continues to have ramifications years later. A number of public figures, including Mark Rutte and Geert Wilders, spoke out in defence of Zwarte Piet. A Belgian UNESCO official later claimed that Shepherd had no authority to speak on behalf of the UN and was \"abusing the name of the UN to bring her own agenda to the media\".\n\n"}
{"id": "31384924", "url": "https://en.wikipedia.org/wiki?curid=31384924", "title": "Victor Tcherikover", "text": "Victor Tcherikover\n\nVictor A. Tcherikover (‎; 1894–1958) was a Russian-born Israeli scholar.\n\nBorn in Russia, he settled in Palestine in 1925. He was one of the first teachers at the Hebrew University of Jerusalem, and headed the departments of general history and classical studies. He specialized in Jewish history in Palestine and Egypt during the Graeco-Roman period.\n\n"}
{"id": "29766167", "url": "https://en.wikipedia.org/wiki?curid=29766167", "title": "Walter Ofonagoro", "text": "Walter Ofonagoro\n\nWalter Ofonagoro (born 24 June 1940) is a Nigerian scholar, politician and businessman who is a former Minister of Information and Culture, Federal Republic of Nigeria. He is also the Chairman of Stanwal Securities Limited (member of the Nigerian Stock Exchange), as well as Chairman of Merit Microfinance Bank Ltd.\n\nBorn and raised in Port Harcourt Rivers State, Nigeria on June 24, 1940, Dr. Ofonagoro is the third child and second son in a family of fifteen. His father, Chief Gabriel Obioha Ofonagoro (Duruishimbu IV of Umuanu Amaigbo and Ugochinyere Igbo 1 of Amaigbo) was at the time of his birth, Assistant Transport Manager of UAC Bulk Oil Plant, Port Harcourt. His mother, Lolo Gladys Ogonnaya Ofonagoro was a dealer in UAC Textiles. He was educated at Baptist Day School Port Harcourt from 1947 to 1954, and subsequently at Baptist High School Port Harcourt from 1955 to 1959. He then went on to study at Holy Family College Abak in Akwa Ibom State, Nigeria where he studied for his A-levels and passed the Cambridge Higher School Certificate examination with Distinction in 1961. Afterwards he went to study at Trinity College, University of Toronto Canada where he graduated with a BA. First Class Hons. Modern History, in 1966. He also studied at Columbia University, New York, where he earned an MA in African Economic History with Distinction in June 1967, and finally got his PhD in African Economic History with Distinction on February 2, 1972.He was a distinguished lecturer in the Department of History at the University of Lagos, Nigeria\n\nMany years prior to his work in the Ministry of Information and Culture, Dr. Ofonagoro first began his career in 1962 as a tutor in Baptist High School Port Harcourt where he taught English, Latin and History. From 1968 to 1975, he held several esteemed positions in Columbia University, New York including; Columbia University Preceptor, Instructor in History and Associate Professor of History. During his tenure at Columbia, he helped to organize the Columbia-Morningside Lecture Series on African Heritage from 1968 to 1976. He was also appointed Adviser to Undergraduate History Majors at Columbia University School of General Studies; and was a member of the University Senate from 1970 to 1972. He also served as a visiting professor at various universities in the New York/New Jersey area such as; Queens College, New York University, Brooklyn College, Long Island University, Rutgers College, and City College. He then returned to Nigeria in 1976 where he taught as a Senior Lecturer in the Department of History, University of Lagos until June 1982. During that time, he served in the Postgraduate Assembly which established the Post Graduate School of the University of Lagos. He was also a visiting Senior Lecturer in Economic History at the University of Ibadan from 1978 to 1979. He pioneered the teaching of African Economic History at the University of Lagos (1977 to 1982) and the University of Ibadan (1977 to 1978). He withdrew his services from the University of Lagos in 1982 to pursue a career in politics and business.\n\nDr. Ofonagoro was a contributor on occasionally featured articles and editorials published in the Eastern Nigerian Guardian newspaper, Port Harcourt, from 1959 to 1960. He was also a frequent contributor to radio, television and newspaper discussions in the Canadian and American media. He also lectured on Africa on the Sunrise Semester program of CBS TV in New York. On his return to Nigeria in 1976, he continued his active role in media such as radio, television and newspapers. He also delivered public lectures at the National Institute for Policy and Strategic Studies, Kuru Plateau State and the Command and Staff College in Jaji, Kaduna State.\n\nIn 1979, he was appointed as Consultant Political Analyst to the Nigeria Television Authority by the government of the Federal Republic of Nigeria. His programme Verdict 79 was an epic on the Election of 1979. The programme ended in March 1980.\n\nHe has led Federal Government Delegations to foreign countries, including a delegation to the International Symposium on the Renaissance in Sofia, Bulgaria (1981). In the same year, he led a delegation to Shipyard Split, in former Yugoslavia, to take delivery of \"MV River Maje\" on behalf of the federal government which was acquired for the Nigerian National Shipping Line. Again in October 1981, he was appointed adviser to the Nigerian delegation and member of the special political committee of the United Nations General Assembly by the federal government of Nigeria.\n\nIn 1982, he was appointed Chairman of the News Agency of Nigeria, and shortly after he became Director-General of the NTA in 1983.\nIn 1993, he was Director of Communications in the Presidential Campaign of Bashir Tofa (under the banner of the National Republican Convention) during the elections of 1993.\n\nFrom 1994 to 1995, Dr. Ofonagoro was appointed Member of the National Constitutional Conference Commission, where he served as Chairman of the Organization, the Conference Convention and the Publicity Committee of the Constitutional Conference Commission which served as the secretariat of the Constitutional Conference. He was also appointed a member of some other sub-committees at the conference such as The Committee on National Defence, The Committee on Fundamental Human Rights, Press Freedom and Citizenship, and The Constitution Drafting Committee.\n\nIn 1995, he was elected Chairman of the Intergovernmental Council for the Development of Communications Among Non-Aligned Countries (IGC).\nOn March 20, 1995, Dr. Ofonagoro was appointed Federal Minister of Information and Culture as the Constitutional Conference began to wind down. He thus became a member of the Federal Executive Council and during his administration, helped to restructure the Federal Ministry of Information. While with the Ministry, he embarked on a mission to promote a positive image of the country which had been damaged overseas.\n\nDuring his time with the Ministry of Information, Dr. Ofonagoro led a delegation to Germany and WIPO Geneva (1995). Nigeria won the WIPO Gold Medal for Advancements in Copyright Law and Administration for that year. Among his responsibilities on that mission was to negotiate and sign cultural and bilateral agreements on behalf of Nigeria with friendly foreign countries. He has also led delegations to places such as North Korea, Iran, Ethiopia and Iraq, where he met Saddam Hussein in 1996.\n\nDuring his tenure as Honorable Minister of Information and Culture, Dr. Ofonagoro signed, on behalf of Nigeria, cultural agreements with about 20 countries and organizations. In robust facilitation of meaningful relations with the countries concerned.\n\nHe had ensured full development of the NTA Television College in Jos (1983) and commissioned its engineering and journalism facilities in that year. National Film Institute, National Film Archives, and the National Film Processing Laboratories were also established in Jos (1995) during his administration. These measures were necessary to establish Nollywood as an industry on a firm footing by providing the necessary facilities for training of the operators right here in Nigeria. Also, 1996 saw the granting of licenses, on a large scale, to private sector operators to own and operate radio and TV stations in Nigeria. This created a market for independent film producers to operate profitably and thus, facilitated the development of Nollywood.\n\nHe has also held many other positions which are listed below as follows:\n\nFrom 1981 to 1983, Dr. Ofonagoro has served as Director/Chairman of the Finance and Establishment Committee of the Board of Amalgamated Tin Mines of Nigeria Bukuru, Plateau State, the largest tin mining Company in Nigeria. From 1981 to 1984, he has served as Chairman of Finance and Establishment Committee of the ATMN Board of Directors.\n\nHe has also operated as Director, Chairman and CEO of the following Stanwal Group of Companies which include Stanwal Builders Nigeria Ltd, Stanwal Consultants Nigeria Ltd, and SUNOD Transport and Haulage Company Ltd.\nFrom 1989 to 1995, he was Director of Thomas Kingsley Securities Ltd (Member of Nigeria Stock Exchange). Then he established his own stock brokerage firm (Stanwal Securities Ltd.) in 1991 which was initially known as Atlantic Securities Ltd. Atlantic Securities was renamed to Stanwal Securities in order to avoid any confusion of identity with other similarly named brokerage houses in the Nigerian Capital Market. On June 20, 1997, the firm was granted a license by the Securities and Exchange Commission to operate as a fully independent securities brokerage house with an initial paid-up capital of N5,639,000. However, as at 30 June 2015 the company had achieved a paid-up capital of N251,704,028 and had authorized capital of N1bn, but the net shareholders' funds stood at N72,990,619. Under the circumstances, with the current nose-dive of oil prices and with the capital market in a freefall, the board of Stanwal is reviewing strategies for future business activities. On the brighter side, Stanwal Consultants Ltd. won a Gold medal for Excellence in Business Practice from the Foundation for Excellence in Business Practice, Geneva, Switzerland.\n\nDr. Ofonagoro is also a principal shareholder, and the present Chairman of Merit Microfinance Bank Ltd. (formerly known as Amaigbo Community Bank Ltd). He has gained considerable experience investing in equities in the New York Stock Exchange since 1970, and the Nigerian Stock Exchange since 1977. In 1983, the Nigerian Stock Exchange hosted Dr. Ofonagoro to a private luncheon at the Metropolitan Club Victoria Island, Lagos, in appreciation of his role in popularizing the Capital Market through television broadcasting.\n\nDr. Ofonagoro is married to Lolo Stephanie Nirmala Ofonagoro (née Lobo) of Mumbai, India, who holds the following traditional titles in Nigeria:\n\nAn educationist, Mrs. Ofonagoro earned her Bachelors and master's degrees in Early Childhood Education from the City University of New York, 1973 and 1975 respectively, and was in 1978, appointed Head of University of Lagos Women's Society Nursery School. A position which she held until 2003 when she retired. Earlier, she had taught at the Muslim Teacher Training College, Surulere, Lagos. She has since established a creche in Ikoyi known as Cuddles Daycare. Dr. and Mrs. Ofonagoro have four children: one daughter and three sons.\n\nDr. Ofonagoro is a life member of the Imo State Chapter of the Nigeria Red Cross, and patron of various student unions, youth clubs and chapters of the Nigerian Union of Journalists across the country. He is also a registered trustee of Amaigbo Town Union. He is a member of the Board of Trustees of St. Mary's Joint Hospital Amaigbo, and Patron of Nze na Ozo Titleholders Society of Amaigbo. He currently resides at his country home, Duruishimbu Villa, Umuobi, Amaigbo, Imo State.\n\n\nDr. Ofonagoro has published extensively in Canada, USA, UK, Europe and Africa. Among his long list of publications are the following:\n\n\n"}
