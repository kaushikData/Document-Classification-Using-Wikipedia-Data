{"id": "805753", "url": "https://en.wikipedia.org/wiki?curid=805753", "title": "Architect (The Matrix)", "text": "Architect (The Matrix)\n\nThe Architect is a fictional character in the films \"The Matrix Reloaded\" and \"The Matrix Revolutions\". He is portrayed by Helmut Bakaitis. He also makes an appearance in the MMORPG \"The Matrix Online\".\n\nThe Architect created the first Matrix as a utopia for the humans whose minds inhabited it. However, the human minds rejected this first attempt as a perfect world and beta 1 of the Matrix crashed. A second attempt added the varying grotesqueries of human nature and a basic cause and effect, but this beta was also a failure. \nThe Architect turned to a more intuitive program designed to understand human nature and psychology to augment the framework of the next Matrix. This time, the power of choice was added to the programming, where humans would be allowed the power to choose, even if the person was only aware of the choice on a vague, subconscious level.\n\nThis version of the Matrix worked, except for approximately 1 percent of human minds. These humans were apparently bodily ejected from the power plant. Some of these humans survived to join Zion.\n\nThe Architect noted that the Matrix was not as perfect as he initially envisioned; the addition of 'choice' to the Matrix's programming added an unpredictable element to the Architect's equations and would eventually cause the Matrix to suffer a destructive system crash. This 'systemic anomaly' was personified within the Matrix by a semi-mythological figure that could 'break free' of the Matrix's control, and change it in whatever manner he desired. The 'One', as this figure came to be known, was subconsciously compelled to travel to the Matrix's mainframe with critical source code for its eventual reboot.\n\nTogether with the human intuitive program the Oracle (which could be considered the \"mother\" of the Matrix as the Architect could be considered the \"father\") for human minds to understand, the concept of the Prophecy was formed. The intuitive program (known to the humans as the Oracle) would tell of this story to the small members of a human resistance that periodically infiltrated the Matrix, who would find the anomaly and help him to find the Architect's office, hidden deep within a fortified building. There, the Architect would use his measures of control to keep the Anomaly, and in turn both Zion and the Matrix, in check. In each of the first five cycles of the lire Matrix, the Anomaly, known to the humans as The One, manifested itself within the Matrix and eventually found the Architect's office.\n\nThe room has two exits, one leading to the Source and the other to the Matrix proper (constructeur as hell and heaven.) The Architect tells the One that Zion is about to be destroyed and that humanity's only chance of survival rests with the One. If the One fails to go to the Source, the system will eventually suffer a catastrophic failure that leads to the death of every human still connected to the Matrix; combined with the destruction of Zion, the entire human race will become extinct. In order to prevent this result, the One must travel to the Source, reloading the master program, and then select a small number of individuals to rebuild Zion. In each of these five cycles, the One enters the door to the Source, the Matrix is reloaded, and Zion is destroyed and subsequently reborn.\n\nOn the sixth iteration, Neo, the sixth Anomaly, appears on schedule before the Architect. The Architect is surprised that this One, unlike his predecessors, is quicker of thought. This sixth Anomaly possesses the same dispensation for protecting humanity as the others, but unlike the other Ones has a deep attachment to one human: a Zion resistance member named Trinity.\n\nThe Architect delivers the usual speech and threat, but he can already see that this One will not comply as his predecessors did. Neo leaves the Architect to save his love, and leaves the future of the Matrix in doubt.\n\nThe Oracle tells Neo more about the Architect at their final meeting. She says that the Architect's purpose is to balance the equation of the Matrix, while her purpose is to \"unbalance\" that equation. She also tells Neo that, as a program designed to be mathematically precise, the Architect doesn't understand the inherently unpredictable nature of choice. She tells Neo to head to the true location of the Source, the Machine City, to save not only humanity, but the Machine world as well.\nAfter Neo sacrifices himself to stabilize the Matrix, the Machines gather Neo's body and successfully 'reboot' the Matrix. The Architect then meets the Oracle and speaks of the \"dangerous game\" that she played, and agrees to honor the truce that Neo brokered for his part in rebooting the Matrix.\n\nNear the climax of \"The Matrix Reloaded\", Neo meets the Architect face to face in a large oval-shaped room with two doors, whose walls are covered with television monitors. (A close-up of these monitors is briefly seen early in \"The Matrix\" when showing Neo sitting in the interrogation room, but is not identified as such at the time.) Taking the form of a cold, humorless, elderly white-haired man in a light gray suit, he is a computer program that created the Matrix and now oversees its functioning. His artificial nature is more readily apparent than that of other programs personified as humans. The Architect is extremely mechanical in his actions, in that he speaks in long logical chains of reasoning, utilizing several connectors (discourse markers) such as \"ergo\", \"concordantly\", and \"thus\", and has little variance in his tone of voice. He also has little facial expression beyond smirks and glares, but does exhibit emotion on limited occasions, such as regret, annoyance and arrogance.\n\nThe Architect's first attempt at a Matrix was a utopia, but it failed miserably and many human lives were lost when the inhabitants refused to accept it. The Architect then redesigned the Matrix to reflect the darker side of human nature and history, but the dystopian version failed too. The solution to this problem was discovered by the Oracle: a version of the Matrix that gave humans the unconscious choice of accepting it. This version was accepted by ninety-nine percent of the Matrix' test subjects, and the Matrix was rewritten to allow for freedom of choice. The remaining one percent, that did not accept the Matrix, would eventually destabilize the system so badly that the Matrix might catastrophically crash, killing every human that was still connected.\n\nIn \"The Matrix Revolutions\", the Oracle explains to Neo that the true purpose of the Architect is to balance the mathematical equations that make up the programming of the Matrix, and he is unable to see the world as anything beyond a series of equations. It is also because of this that he is unable to comprehend choice and free will and cannot see the results of such choices as they are no more than variable factors in an equation to him.\n\nWith the new Matrix in place, a system was enacted to control the inhabitants who refused to accept it. While the Oracle was able to guide the actions of the humans who left the Matrix through prophecy, it was the Architect who programmed \"The One\" that would fulfill these prophecies. \"The One\" was made carrying not only the source code of the Matrix \"\"Prime Program\", which gave him his outstanding powers over the Matrix, but also with a profound attachment to humanity that would later motivate him to fulfill the prophecies being spread by the Oracle. Every time the free humans had grown strong enough to start threatening machine hegemony, \"The One\" would be born into the Matrix.\n\nAs the prophecies were fulfilled by \"The One\", the machines would begin building an army to destroy Zion. Under the guidance of the Oracle, \"The One\" would find his way to the machine mainframe, also called \"The Source\", convinced that his actions there would end the war on behalf of the humans. Because the Architect resides in a room that lies on the path to the Source, the One would invariably encounter him along the way. During this encounter, the Architect would reveal his influence over the preceding events and the reason the Matrix had been designed to allow a small percentage of its inhabitants to escape. He would then present \"The One\" with a choice, symbolized by the two doors in his office:\n\n\nThe machinations of the Architect and the Oracle were successful in maintaining the status quo to the point that, until Neo, all incarnations of \"The One\" had chosen to cooperate with the Machines in order to preserve humanity.\n\nThe Architect offered Neo the same choice he offered his five predecessors. Unlike previous Ones, Neo was experiencing his programmed attachment to humanity in a specific way: in his love for Trinity. At the same time Neo had met with the Architect, Trinity was in the Matrix being chased by an Agent in a reenactment of a nightmare Neo had that ended with her apparent death.\n\nDuring their conversation, Neo claims that the machines cannot allow humanity to be destroyed as they are using them for power and thus could not survive if they were killed. In response, the Architect, although his face remains unmoved, states in a grave voice, \"There are levels of survival we are prepared to accept\".\"\n\nPresented with a choice between the destruction of humanity or losing Trinity, Neo sees no choice. Motivated by his love for Trinity and not wanting to play into The Architect's ultimatum like his predecessors, he defies The Architect and chooses to attempt to save Trinity. Even though the Architect had asserted that her death was certain and his attempt to save her would mean doom for all humanity, Neo returns to the Matrix in an attempt to save her and end the machines' control of humanity.\n\nBefore Neo departs he warns The Architect, \"\"If I were you, I would hope that we don't meet again\".\" The Architect simply replies, \"\"We won't\".\"\n\nIn the final scene of the film, the Architect joins the Oracle, commenting that she \"played a very dangerous game\", referring to the Oracle's role in guiding Neo as he defied the Architect's system of control. He then promises her that the humans who desire release from the Matrix will gain it. When she asks if he will keep his word he replies, \"What do you think I am? Human?\"\n\n\n"}
{"id": "11731170", "url": "https://en.wikipedia.org/wiki?curid=11731170", "title": "Area chart", "text": "Area chart\n\nAn area chart or area graph displays graphically quantitative data. It is based on the line chart. The area between axis and line are commonly emphasized with colors, textures and hatchings. Commonly one compares two or more quantities with an area chart.\n\nWilliam Playfair is usually credited with inventing the area charts as well as the line, bar, and pie charts. His book \"The Commercial and Political Atlas\", published in 1786, contained a number of time-series graphs, including \"Interest of the National Debt from the Revolution\" and \"Chart of all the Imports and Exports to and from England from the Year 1700 to 1782\" that are often described as the first area charts in history.\n\nArea charts are used to represent cumulated totals using numbers or percentages (stacked area charts in this case) over time.\nUse the area chart for showing trends over time among related attributes. The area chart is like the plot chart except that the area below the plotted line is filled in with color to indicate volume.\n\nWhen multiple attributes are included, the first attribute is plotted as a line with color fill followed by the second attribute, and so on.\n\nArea charts which use vertical and horizontal lines to connect the data points in a series forming a step-like progression are called \"step-area charts\".\n\nArea charts in which data points are connected by smooth curves instead of straight lines are called \"spline-area charts\".\n\n"}
{"id": "14469299", "url": "https://en.wikipedia.org/wiki?curid=14469299", "title": "Autonomous convergence theorem", "text": "Autonomous convergence theorem\n\nIn mathematics, an autonomous convergence theorem is one of a family of related theorems which specify conditions guaranteeing global asymptotic stability of a continuous autonomous dynamical system.\n\nThe Markus–Yamabe conjecture was formulated as an attempt to give conditions for global stability of continuous dynamical systems in two dimensions. However, the Markus–Yamabe conjecture does not hold for dimensions higher than two, a problem which autonomous convergence theorems attempt to address. The first autonomous convergence theorem was constructed by Russell Smith. This theorem was later refined by Michael Li and James Muldowney.\n\nA comparatively simple autonomous convergence theorem is as follows:\n\nThis autonomous convergence theorem is very closely related to the Banach fixed-point theorem.\n\nNote: this is an intuitive description of how autonomous convergence theorems guarantee stability, not a strictly mathematical description.\n\nThe key point in the example theorem given above is the existence of a negative logarithmic norm, which is derived from a vector norm. The vector norm effectively measures the distance between points in the vector space on which the differential equation is defined, and the negative logarithmic norm means that distances between points, as measured by the corresponding vector norm, are decreasing with time under the action of formula_5. So long as the trajectories of all points in the phase space are bounded, all trajectories must therefore eventually converge to the same point.\n\nThe autonomous convergence theorems by Russell Smith, Michael Li and James Muldowney work in a similar manner, but they rely on showing that the area of two-dimensional shapes in phase space decrease with time. This means that no periodic orbits can exist, as all closed loops must shrink to a point. If the system is bounded, then according to Pugh's closing lemma there can be no chaotic behaviour either, so all trajectories must eventually reach an equilibrium.\n\nMichael Li has also developed an extended autonomous convergence theorem which is applicable to dynamical systems containing an invariant manifold.\n"}
{"id": "20966805", "url": "https://en.wikipedia.org/wiki?curid=20966805", "title": "Bochner–Riesz mean", "text": "Bochner–Riesz mean\n\nThe Bochner–Riesz mean is a summability method often used in harmonic analysis when considering convergence of Fourier series and Fourier integrals. It was introduced by Salomon Bochner as a modification of the Riesz mean.\n\nDefine\n\nLet formula_2 be a periodic function, thought of as being on the n-torus, formula_3, and having Fourier coefficients formula_4 for formula_5. Then the Bochner–Riesz means of complex order formula_6, formula_7 of (where formula_8 and formula_9) are defined as\n\nAnalogously, for a function formula_2 on formula_12 with Fourier transform formula_13, the Bochner–Riesz means of complex order formula_6, formula_15 (where formula_8 and formula_9) are defined as\n\nFor formula_19 and formula_20, formula_21 and formula_22 may be written as convolution operators, where the convolution kernel is an approximate identity. As such, in these cases, considering the almost everywhere convergence of Bochner–Riesz means for functions in formula_23 spaces is much simpler than the problem of \"regular\" almost everywhere convergence of Fourier series/integrals (corresponding to formula_24). In higher dimensions, the convolution kernels become more \"badly behaved\" (specifically, for formula_25, the kernel is no longer integrable) and establishing almost everywhere convergence becomes correspondingly more difficult.\n\nAnother question is that of for which formula_6 and which formula_27 the Bochner–Riesz means of an formula_23 function converge in norm. This is of fundamental importance for formula_29, since regular spherical norm convergence (again corresponding to formula_24) fails in formula_23 when formula_32. This was shown in a paper of 1971 by Charles Fefferman. By a transference result, the formula_12 and formula_3 problems are equivalent to one another, and as such, by an argument using the uniform boundedness principle, for any particular formula_35, formula_23 norm convergence follows in both cases for exactly those formula_6 where formula_38 is the symbol of an formula_23 bounded Fourier multiplier operator. For formula_40, this question has been completely resolved, but for formula_41, it has only been partially answered. The case of formula_20 is not interesting here as convergence follows for formula_35 in the most difficult formula_24 case as a consequence of the formula_23 boundedness of the Hilbert transform and an argument of Marcel Riesz.\n\n"}
{"id": "13925681", "url": "https://en.wikipedia.org/wiki?curid=13925681", "title": "Bramble–Hilbert lemma", "text": "Bramble–Hilbert lemma\n\nIn mathematics, particularly numerical analysis, the Bramble–Hilbert lemma, named after James H. Bramble and Stephen Hilbert, bounds the error of an approximation of a function formula_1 by a polynomial of order at most formula_2 in terms of derivatives of formula_1 of order formula_4. Both the error of the approximation and the derivatives of formula_1 are measured by formula_6 norms on a bounded domain in formula_7. This is similar to classical numerical analysis, where, for example, the error of linear interpolation formula_1 can be bounded using the second derivative of formula_1. However, the Bramble–Hilbert lemma applies in any number of dimensions, not just one dimension, and the approximation error and the derivatives of formula_1 are measured by more general norms involving averages, not just the maximum norm.\n\nAdditional assumptions on the domain are needed for the Bramble–Hilbert lemma to hold. Essentially, the boundary of the domain must be \"reasonable\". For example, domains that have a spike or a slit with zero angle at the tip are excluded. Lipschitz domains are reasonable enough, which includes convex domains and domains with continuously differentiable boundary. \n\nThe main use of the Bramble–Hilbert lemma is to prove bounds on the error of interpolation of function formula_1 by an operator that preserves polynomials of order up to formula_2, in terms of the derivatives of formula_1 of order formula_4. This is an essential step in error estimates for the finite element method. The Bramble–Hilbert lemma is applied there on the domain consisting of one element (or, in some superconvergence results, a small number of elements).\n\nBefore stating the lemma in full generality, it is useful to look at some simple special cases. In one dimension and for a function formula_1 that has formula_4 derivatives on interval formula_17, the lemma reduces to\n\nwhere formula_19 is the space of all polynomials of order at most formula_2.\n\nIn the case when formula_21, formula_22, formula_23, and formula_1 is twice differentiable, this means that there exists a polynomial formula_25 of degree one such that for all formula_26,\n\nThis inequality also follows from the well-known error estimate for linear interpolation by choosing formula_25 as the linear interpolant of formula_1.\n\nSuppose formula_30 is a bounded domain in formula_31, formula_32, with boundary formula_33 and diameter formula_34. formula_35 is the Sobolev space of all function formula_1 on formula_30 with weak derivatives formula_38 of order formula_39 up to formula_40 in formula_41. Here, formula_42 is a multiindex, formula_43 formula_44 and formula_45 denotes the derivative formula_46 times with respect to formula_47, formula_48 times with respect to formula_49, and so on. The Sobolev seminorm on formula_50 consists of the formula_51 norms of the highest order derivatives,\n\nand\n\nformula_54 is the space of all polynomials of order up to formula_40 on formula_31. Note that formula_57 for all formula_58 and formula_59, so formula_60 has the same value for any formula_58.\n\nLemma (Bramble and Hilbert) Under additional assumptions on the domain formula_30, specified below, there exists a constant formula_63 independent of formula_64 and formula_1 such that for any formula_66 there exists a polynomial formula_58 such that for all formula_68\n\nThe lemma was proved by Bramble and Hilbert under the assumption that formula_30 satisfies the strong cone property; that is, there exists a finite open covering formula_71 of formula_33 and corresponding cones formula_73 with vertices at the origin such that formula_74 is contained in formula_30 for any formula_76 formula_77.\n\nThe statement of the lemma here is a simple rewriting of the right-hand inequality stated in Theorem 1 in. The actual statement in is that the norm of the factorspace formula_78 is equivalent to the formula_79 seminorm. The formula_79 norm is not the usual one but the terms are scaled with formula_34 so that the right-hand inequality in the equivalence of the seminorms comes out exactly as in the statement here.\n\nIn the original result, the choice of the polynomial is not specified, and the value of constant and its dependence on the domain formula_30 cannot be determined from the proof.\n\nAn alternative result was given by Dupont and Scott under the assumption that the domain formula_30 is star-shaped; that is, there exists a ball formula_84 such that for any formula_85, the closed convex hull of formula_86 is a subset of formula_30. Suppose that formula_88 is the supremum of the diameters of such balls. The ratio formula_89 is called the chunkiness of formula_30.\n\nThen the lemma holds with the constant formula_91, that is, the constant depends on the domain formula_30 only through its chunkiness formula_93 and the dimension of the space formula_94. In addition, formula_95 can be chosen as formula_96, where formula_97 is the averaged Taylor polynomial, defined as\n\nwhere\n\nis the Taylor polynomial of degree at most formula_2 of formula_1 centered at formula_102 evaluated at formula_76, and formula_104 is a function that has derivatives of all orders, equals to zero outside of formula_84, and such that\n\nSuch function formula_107 always exists.\n\nFor more details and a tutorial treatment, see the monograph by Brenner and Scott. The result can be extended to the case when the domain formula_30 is the union of a finite number of star-shaped domains, which is slightly more general than the strong cone property, and other polynomial spaces than the space of all polynomials up to a given degree.\n\nThis result follows immediately from the above lemma, and it is also called sometimes the Bramble–Hilbert lemma, for example by Ciarlet. It is essentially Theorem 2 from.\n\nLemma Suppose that formula_109 is a continuous linear functional on formula_79 and formula_111 its dual norm. Suppose that formula_112 for all formula_58. Then there exists a constant formula_114 such that\n\n"}
{"id": "42451632", "url": "https://en.wikipedia.org/wiki?curid=42451632", "title": "Brian Alspach", "text": "Brian Alspach\n\nBrian Roger Alspach is a mathematician whose main research interest is in graph theory. Alspach has also studied the mathematics behind poker, and writes for \"Poker Digest \"and \"Canadian Poker Player\" magazines.\n\nBrian Alspach was born on May 29, 1938 in North Dakota. He attended the University of Washington from 1957 to 1961, receiving his B.A. in 1961. He taught at a junior high school for one year before beginning his graduate studies. In 1964 he received his master's degree and in 1966 he obtained his Ph.D. from the University of California, Santa Barbara under the supervision of Paul Kelly. He taught at Simon Fraser University for 33 years. He retired from there in 1998. He currently works as an adjunct professor at the University of Regina and has been there since 1999. He is responsible for creating an industrial mathematics degree at Simon Fraser University.\n\nBrian Alspach believes that the growth and future of mathematics will depend on the business people in the industrial businesses. His interests are in graph theory and its applications. One of his theories of coverings and decomposition has been applied to scheduling issues that can arise in the business world. Alspach states that his biggest issue with this is trying to explain such complex math to people in the business world with only a basic understanding of math. He has mentored a total of 13 Ph.D. students. His wife is the vice president of academics at the University of Regina where he is currently an adjunct professor.\n\nOne of his first publications was an article titled \"Cycles of each length in regular tournaments\", which was published in the Canadian Mathematical Bulletin (November, 1967).\n\nAnother influential piece of Brian Alspach is \"Point-symmetric graphs and digraphs of prime order and transitive permutation groups of prime degree\", which was published in the Journal of Combinatorial Theory (August, 1973).\n\nIn his article titled \"Isomorphism of circulant graphs and digraphs\" which was published in Discrete Mathematics (February, 1979).\nHe discusses the isomorphism problem for a special class of graphs.\n\nBrian Alspach coauthored an article with T.D. Parsons titled \"A construction for vertex –transitive graph\" published in the Canadian Journal of Mathematics (April, 1982).\n\nAlspach's conjecture, posed by Alspach in 1981, concerns the characterization of disjoint cycle covers of complete graphs with prescribed cycle lengths.\nWith Heather Gavlas Jordon, in 2001, Alspach proved a special case, on the decomposition of complete graphs into cycles that all have the same length.\nThis is possible if and only if the complete graph has an odd number of vertices (so its degree is even), the given cycle length is at most the number of vertices (so that cycles of that length exist), and the given length divides the number of edges of the graph. A proof of the full conjecture was published in 2014.\n\n"}
{"id": "12022054", "url": "https://en.wikipedia.org/wiki?curid=12022054", "title": "Cannon's algorithm", "text": "Cannon's algorithm\n\nIn computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.\n\nIt is especially suitable for computers laid out in an \"N\" × \"N\" mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.\n\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.\n\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA)\nis a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.\n\nWhen multiplying two n×n matrices A and B, we need \"n\"×n processing nodes p arranged in a 2d grid. Initially p is responsible for a and b.\nWe need to select k in every iteration for every Processor Element (PE) so that processors don't access the same data for computing formula_1.\n\nTherefore processors in the same row / column must begin summation with different indexes. If for example \"PE(0,0)\" calculates formula_2 in the first step, \"PE(0,1) chooses\" formula_3 first. The selection of \"k := (i + j) mod n\" for \"PE(i,j)\" satisfies this constraint for the first step.\n\nIn the first step we distribute the input matrices between the processors based on the previous rule.\n\nIn the next iterations we choose a new \"k' := (k + 1) mod n\" for every processor. This way every processor will continue accessing different values of the matrices. The needed data is then always at the neighbour processors. \"A PE(i,j)\" needs then the formula_4 from \"PE(i,(j + 1) mod n)\" and the formula_5 from \"PE((i + 1) mod n,j)\" for the next step. This means that formula_4 has to be passed cyclically to the left and also formula_5 cyclically upwards. The results of the multiplications are summed up as usual. After n steps, each processor has calculated all formula_1 once and its sum is thus the searched formula_9.\n\nAfter the initial distribution of each processor, only the data for the next step has to be stored. These are the intermediate result of the previous sum, a formula_10 and a formula_11. This means that all three matrices only need to be stored in memory once evenly distributed across the processors.\n\nIn practise we have much fewer processors than the matrix elements. We can replace the matrix elements with submatrices, so that every processor processes more values. The scalar multiplication and addition become sequential matrix multiplication and addition. The width and height of the submatrices will be formula_12.\n\nThe runtime of the algorithm is formula_13 , where formula_14 is the time of the initial distribution of the matrices in the first step, formula_15 is the calculation of the intermediate results and formula_16 and formula_17 stands for the time it takes to establish a connection and transmission of byte respectively.\n\nA disadvantage of the algorithm is that there are many connection setups, with small message sizes. It would be better to be able to transmit more data in each message.\n\n\n"}
{"id": "1866872", "url": "https://en.wikipedia.org/wiki?curid=1866872", "title": "Canonical ring", "text": "Canonical ring\n\nIn mathematics, the pluricanonical ring of an algebraic variety \"V\" (which is non-singular), or of a complex manifold, is the graded ring \n\nof sections of powers of the canonical bundle \"K\". Its \"n\"th graded component (for formula_2) is:\n\nthat is, the space of sections of the \"n\"-th tensor product \"K\" of the canonical bundle \"K\".\n\nThe 0th graded component formula_4 is sections of the trivial bundle, and is one-dimensional as \"V\" is projective. The projective variety defined by this graded ring is called the canonical model of \"V\", and the dimension of the canonical model is called the Kodaira dimension of \"V\". \n\nOne can define an analogous ring for any line bundle \"L\" over \"V\"; the analogous dimension is called the Iitaka dimension. A line bundle is called big if the Iitaka dimension equals the dimension of the variety.\n\nThe canonical ring and therefore likewise the Kodaira dimension is a birational invariant: Any birational map between smooth compact complex manifolds induces an isomorphism between the respective canonical rings. As a consequence one can define the Kodaira dimension of a singular space as the Kodaira dimension of a desingularization. Due to the birational invariance this is well defined, i.e., independent of the choice of the desingularization.\n\nA basic conjecture is that the pluricanonical ring is finitely generated. This is considered a major step in the Mori program.\n\nThe dimension\n\nis the classically defined \"n\"-th plurigenus of \"V\". The pluricanonical divisor formula_6, via the corresponding linear system of divisors, gives a map to projective space formula_7, called the \"n\"-canonical map.\n\nThe size of \"R\" is a basic invariant of \"V\", and is called the Kodaira dimension.\n\n"}
{"id": "37191231", "url": "https://en.wikipedia.org/wiki?curid=37191231", "title": "Capped octahedral molecular geometry", "text": "Capped octahedral molecular geometry\n\nIn chemistry, the capped octahedral molecular geometry describes the shape of compounds where seven atoms or groups of atoms or ligands are arranged around a central atom defining the vertices of a gyroelongated triangular pyramid. This shape has C symmetry and is one of the three common shapes for heptacoordinate transition metal complexes, along with the pentagonal bipyramid and the capped trigonal prism.\n\nExamples of the capped octahedral molecular geometry are the heptafluoromolybdate (MoF) and the heptafluorotungstate (WF) ions.\n"}
{"id": "10059981", "url": "https://en.wikipedia.org/wiki?curid=10059981", "title": "Category of manifolds", "text": "Category of manifolds\n\nIn mathematics, the category of manifolds, often denoted Man, is the category whose objects are manifolds of smoothness class \"C\" and whose morphisms are \"p\"-times continuously differentiable maps. This is a category because the composition of two \"C\" maps is again continuous and of class \"C\".\n\nOne is often interested only in \"C\"-manifolds modelled on spaces in a fixed category \"A\", and the category of such manifolds is denoted Man(\"A\"). Similarly, the category of \"C\"-manifolds modelled on a fixed space \"E\" is denoted Man(\"E\").\n\nOne may also speak of the category of smooth manifolds, Man, or the category of analytic manifolds, Man.\n\nLike many categories, the category Man is a concrete category, meaning its objects are sets with additional structure (i.e. a topology and an equivalence class of atlases of charts defining a \"C\"-differentiable structure) and its morphisms are functions preserving this structure. There is a natural forgetful functor\nto the category of topological spaces which assigns to each manifold the underlying topological space and to each \"p\"-times continuously differentiable function the underlying continuous function of topological spaces. Similarly, there is a natural forgetful functor\nto the category of sets which assigns to each manifold the underlying set and to each \"p\"-times continuously differentiable function the underlying function.\n"}
{"id": "25403351", "url": "https://en.wikipedia.org/wiki?curid=25403351", "title": "Chang's conjecture", "text": "Chang's conjecture\n\nIn model theory, a branch of mathematical logic, Chang's conjecture, attributed to Chen Chung Chang by , states that every model of type (ω,ω) for a countable language has an elementary submodel of type (ω, ω). A model is of type (α,β) if it is of cardinality α and a unary relation is represented by a subset of cardinality β. The usual notation is formula_1.\n\nThe axiom of constructibility implies that Chang's conjecture fails. Silver proved the consistency of Chang's conjecture from the consistency of an ω-Erdős cardinal. Hans-Dieter Donder showed the reverse implication: if CC holds, then ω is ω-Erdős in K.\n\nMore generally, Chang's conjecture for two pairs (α,β), (γ,δ) of cardinals is the claim\nthat every model of type (α,β) for a countable language has an elementary submodel of type (γ,δ). \nThe consistency of formula_2 was shown by Laver from the consistency of a huge cardinal.\n\n"}
{"id": "19585960", "url": "https://en.wikipedia.org/wiki?curid=19585960", "title": "Charlier polynomials", "text": "Charlier polynomials\n\nIn mathematics, Charlier polynomials (also called Poisson–Charlier polynomials) are a family of orthogonal polynomials introduced by Carl Charlier.\nThey are given in terms of the generalized hypergeometric function by\nwhere formula_2 are Laguerre polynomials. They satisfy the orthogonality relation\n\n\n"}
{"id": "169176", "url": "https://en.wikipedia.org/wiki?curid=169176", "title": "Clearance rate", "text": "Clearance rate\n\nIn criminal justice, clearance rate is calculated by dividing the number of crimes that are \"cleared\" (a charge being laid) by the total number of crimes recorded. Clearance rates are used by various groups as a measure of crimes solved by the police. \n\nClearance rates can be problematic for measuring the performance of police services and for comparing various police services. This is because a police force may employ a different way of measuring clearance rates. For example, each police force may have a different method of recording when a \"crime\" has occurred and different criteria for determining when a crime has been \"cleared.\" One police force may appear to have a much better clearance rate because of its calculation methodology.\n\nIn System Conflict Theory, it is argued that clearance rates cause the police to focus on \"appearing \"to solve crimes (generating high clearance rate scores) rather than actually solving crimes. Further focus on clearance rates may result in effort being expended to attribute crimes (correctly or incorrectly) to a criminal, which may not result in retribution, compensation, rehabilitation or deterrence.\n\nDefinition: A measure of investigative effectiveness that compares the number of crimes reported or discovered to the number of crimes solved through arrest or other means (such as the death of the suspect).\nCriminal Justice Today, An Introductory Text For The 21st Century. Fourteenth edition by Frank Schmalleger\n"}
{"id": "29847590", "url": "https://en.wikipedia.org/wiki?curid=29847590", "title": "Commitment device", "text": "Commitment device\n\nA commitment device is, according to journalist Stephen J. Dubner and economist Steven Levitt, a way to lock yourself into following a plan of action that you might not want to do but you know is good for you. In other words, a commitment device is a way to give yourself a reward or punishment to make an empty promise stronger and believable.\n\nA commitment device is a technique where someone makes it easier for themselves to avoid akrasia (acting against one's better judgment), particularly procrastination.\n\nCommitment devices have two major features. They are voluntarily adopted for use and they tie consequences to follow-through failures. Consequences can be immutable (irreversible, such as a monetary consequence) or mutable (allows for the possibility of future reversal of the consequence).\n\nThe term \"commitment device\" is used in both economics and game theory. In particular, the concept is relevant to the fields of economics and especially the study of decision making (Brocas, \"et al.\").\n\nA common example comes from mythology: Odysseus' plan to survive hearing the sirens' song without jumping overboard. Economist Jodi Beggs writes \"Commitment devices are a way to overcome the discrepancy between an individual's short-term and long-term preferences; in other words, they are a way for self-aware people to modify their incentives or set of possible choices in order to overcome impatience or other irrational behavior. You know the story of Ulysses tying himself to the mast so that he couldn't be lured in by the song of the Sirens? You can think of that as the quintessential commitment device\" (Beggs 2009).\n\nBehavioral economist Daniel Goldstein describes how commitment devices established in \"cold states\" help an protect themselves against impulsive decisions in later, emotional, stimulated, \"hot states\". Goldstein says that, despite their usefulness, commitment devices nevertheless have drawbacks. Namely, they still rely on some self-control. \nGoldstein says that, for one, a commitment device can promote learned helplessness in the agent. If the agent enters a situation where the device does not incentivize commitment, the agent may not have enough will power or ability to control themselves. (Goldstein uses the example of a cake falling into the grey area of a diet, so it is eaten excessively.) Second, commitment devices can usually be reversed. (An unplugged distracting electronic can be plugged back in.)\nGoldstein says \"In effect you are like Odysseus and the first mate in one person. You're binding yourself, and then you're weaseling your way out of it, and then you're beating yourself up for it afterwards.\"\n\n\nIt can be challenging to promote uptake of commitment devices. In the field of health, for example, commitment devices have the potential to increase patient adherence to their health goals, but utilization of these techniques is low. Health professionals can potentially increase patient uptake of commitment devices by increasing their accessibility, making policies opt-out, and leveraging patients’ social networks.\n\nExamples of commitment devices abound. Dubner and Levitt give the example of Han Xin, a general in Ancient China, who positioned his soldiers with their backs to a river, making it impossible for them to flee, thereby leaving them no choice but to attack the enemy head-on. They also present various commitment devices related to weight loss (2007). In addition, some game theorists have argued that human emotions and sense of honor are forms of commitment device (Arslan 2011 & Ross and Dumouchel 2004). Other examples include announcing commitments publicly and mutually assured destruction (Straker 2011), as well as software programs that block internet access for a predetermined period of time.\n\n\n"}
{"id": "16820442", "url": "https://en.wikipedia.org/wiki?curid=16820442", "title": "Director string", "text": "Director string\n\nIn mathematics, in the area of lambda calculus and computation, directors or director strings are a mechanism for keeping track of the free variables in a term. Loosely speaking, they can be understood as a kind of memoization for free variables; that is, as an optimization technique for rapidly locating the free variables in a term algebra or in a lambda expression. Director strings were introduced by Kennaway and Sleep in 1982 and further developed by Sinot, Fernández and Mackie as a mechanism for understanding and controlling the computational complexity cost of beta reduction.\n\nIn beta reduction, one defines the value of the expression on the left to be that on the right:\n\nWhile this is a conceptually simple operation, the computational complexity of the step can be non-trivial: a naive algorithm would scan the expression \"E\" for all occurrences of the free variable \"x\". Such an algorithm is clearly \"O\"(\"n\") in the length of the expression \"E\". Thus, one is motivated to somehow track the occurrences of the free variables in the expression. One may attempt to track the position of \"every\" free variable, wherever it may occur in the expression, but this can clearly become very costly in terms of storage; furthermore, it provides a level of detail that is not really needed. Director strings suggest that the correct model is to track free variables in a hierarchical fashion, by tracking their use in component terms.\n\nConsider, for simplicity, a term algebra, that is, a collection of free variables, constants, and operators which may be freely combined. Assume that a term \"t\" takes the form \n\nwhere \"f\" is a function, of arity \"n\", with no free variables, and the formula_4 are terms that may or may not contain free variables. Let \"V\" denote the set of all free variables that may occur in the set of all terms. The director is then the map\n\nfrom the free variables to the power set formula_6 of the set formula_7. The values taken by formula_8 are simply a list of the indices of the formula_4 in which a given free variable occurs. Thus, for example, if a free variable formula_10 occurs in formula_11 and formula_12 but in no other terms, then one has formula_13.\n\nThus, for every term formula_14 in the set of all terms \"T\", one maintains a function formula_8, and instead of working only with terms \"t\", one works with pairs formula_16. Thus, the time complexity of finding the free variables in \"t\" is traded for the space complexity of maintaining a list of the terms in which a variable occurs.\n\nAlthough the above definition is formulated in terms of a term algebra, the general concept applies more generally, and can be defined both for combinatory algebras and for lambda calculus proper, specifically, within the framework of explicit substitution.\n\n\n"}
{"id": "679218", "url": "https://en.wikipedia.org/wiki?curid=679218", "title": "ESC/Java", "text": "ESC/Java\n\nESC/Java (and more recently ESC/Java2), the \"Extended Static Checker for Java,\" is a programming tool that attempts to find common run-time errors in Java programs at compile time. The underlying approach used in ESC/Java is referred to as extended static checking, which is a collective name referring to a range of techniques for statically checking the correctness of various program constraints. For example, that an integer variable is greater-than-zero, or lies between the bounds of an array. This technique was pioneered in ESC/Java (and its predecessor, ESC/Modula-3) and can be thought of as an extended form of type checking. Extended static checking usually involves the use of an automated theorem prover and, in ESC/Java, the Simplify theorem prover was used.\n\nESC/Java is neither sound nor complete. This was intentional and aims to reduce the number of errors and/or warnings reported to the programmer, in order to make the tool more useful in practice. However, it does mean that: firstly, there are programs that ESC/Java will erroneously consider to be incorrect (known as \"false-positives\"); secondly, there are incorrect programs it will consider to be correct (known as \"false-negatives\"). Examples in the latter category include errors arising from modular arithmetic and/or multithreading.\n\nESC/Java was originally developed at the Compaq Systems Research Center (SRC). SRC launched the project in 1997, after work on their original extended static checker, ESC/Modula-3, ended in 1996. In 2002, SRC released the source code for ESC/Java and related tools. Recent versions of ESC/Java are based around the Java Modeling Language (JML). Users can control the amount and kinds of checking by annotating their programs with specially formatted comments or \"pragmas\".\n\nThe University of Nijmegen's \"Security of Systems\" group released alpha versions of ESC/Java2, an extended version of ESC/Java that processes the JML specification language through 2004. From 2004 to 2009, ESC/Java2 development was managed by the KindSoftware Research Group at University College Dublin, which in 2009 moved to the IT University of Copenhagen, and in 2012 to the Technical University of Denmark. Over the years, ESC/Java2 has gained many new features including the ability to reason with multiple theorem provers and integration with Eclipse.\n\nOpenJML, the successor of ESC/Java2, is available for Java 1.8. The source is available at https://github.com/OpenJML\n\n\n\n"}
{"id": "18104093", "url": "https://en.wikipedia.org/wiki?curid=18104093", "title": "Frostman lemma", "text": "Frostman lemma\n\nIn mathematics, and more specifically, in the theory of fractal dimensions, Frostman's lemma provides a convenient tool for estimating the Hausdorff dimension of sets.\n\nLemma: Let \"A\" be a Borel subset of R, and let \"s\" > 0. Then the following are equivalent:\n\nOtto Frostman proved this lemma for closed sets \"A\" as part of his PhD dissertation at Lund University in 1935. The generalization to Borel sets is more involved, and requires the theory of Suslin sets.\n\nA useful corollary of Frostman's lemma requires the notions of the \"s\"-capacity of a Borel set \"A\" ⊂ R, which is defined by\n\n(Here, we take inf ∅ = ∞ and  = 0. As before, the measure formula_3 is unsigned.) It follows from Frostman's lemma that for Borel \"A\" ⊂ R\n"}
{"id": "21051195", "url": "https://en.wikipedia.org/wiki?curid=21051195", "title": "Greedy coloring", "text": "Greedy coloring\n\nIn the study of graph coloring problems in mathematics and computer science, a greedy coloring is a coloring of the vertices of a graph formed by a greedy algorithm that considers the vertices of the graph in sequence and assigns each vertex its first available color. Greedy colorings do not in general use the minimum number of colors possible. However, they have been used in mathematics as a technique for proving other results about colorings and in computer science as a heuristic to find colorings with few colors.\n\nA crown graph (a complete bipartite graph , with the edges of a perfect matching removed) is a particularly bad case for greedy coloring: if the vertex ordering places two vertices consecutively whenever they belong to one of the pairs of the removed matching, then a greedy coloring will use colors, while the optimal number of colors for this graph is two. There also exist graphs such that with high probability a randomly chosen vertex ordering leads to a number of colors much larger than the minimum. Therefore, it is of some importance in greedy coloring to choose the vertex ordering carefully. The number of colors produced by the greedy coloring for the worst ordering of a given graph is called its Grundy number.\n\nIt is NP-complete to determine, for a given graph and number , whether there exists an ordering of the vertices of that forces the greedy algorithm to use or more colors. In particular, this means that it is difficult to find the worst ordering for .\n\nThe vertices of any graph may always be ordered in such a way that the greedy algorithm produces an optimal coloring. For, given any optimal coloring in which the smallest color set is maximal, the second color set is maximal with respect to the first color set, etc., one may order the vertices by their colors. Then when one uses a greedy algorithm with this order, the resulting coloring is automatically optimal. More strongly, perfectly orderable graphs (which include chordal graphs, comparability graphs, and distance-hereditary graphs) have an ordering that is optimal both for the graph itself and for all of its induced subgraphs. However, finding an optimal ordering for an arbitrary graph is NP-hard (because it could be used to solve the NP-complete graph coloring problem), and recognizing perfectly orderable graphs is also NP-complete. For this reason, heuristics have been used that attempt to reduce the number of colors while not guaranteeing an optimal number of colors.\n\nA commonly used ordering for greedy coloring is to choose a vertex of minimum degree, order the remaining vertices recursively, and then place last in the ordering. If every subgraph of a graph contains a vertex of degree at most , then the greedy coloring for this ordering will use at most colors. The smallest such is commonly known as the degeneracy of the graph.\n\nFor a graph of maximum degree , any greedy coloring will use at most colors. Brooks' theorem states that with two exceptions (cliques and odd cycles) at most colors are needed. One proof of Brooks' theorem involves finding a vertex ordering in which the first two vertices are adjacent to the final vertex but not adjacent to each other, and each subsequent vertex has at least one earlier neighbor. For an ordering with this property, the greedy coloring algorithm uses at most colors.\n\nIt is possible to define a greedy coloring algorithm in which the vertices of the given graph are colored in a given sequence but in which the color chosen for each vertex is not necessarily the first available color; alternative color selection strategies have been studied within the framework of online algorithms. In the online graph-coloring problem, vertices of a graph are presented one at a time in an arbitrary order to a coloring algorithm; the algorithm must choose a color for each vertex, based only on the colors of and adjacencies among already-processed vertices. In this context, one measures the quality of a color selection strategy by its competitive ratio, the ratio between the number of colors it uses and the optimal number of colors for the given graph.\n\nIf no additional restrictions on the graph are given, the optimal competitive ratio is only slightly sublinear. However, for interval graphs, a constant competitive ratio is possible, while for bipartite graphs and sparse graphs a logarithmic ratio can be achieved. Indeed, for sparse graphs, the standard greedy coloring strategy of choosing the first available color achieves this competitive ratio, and it is possible to prove a matching lower bound on the competitive ratio of any online coloring algorithm.\n\n"}
{"id": "372470", "url": "https://en.wikipedia.org/wiki?curid=372470", "title": "Grothendieck's Galois theory", "text": "Grothendieck's Galois theory\n\nIn mathematics, Grothendieck's Galois theory is an abstract approach to the Galois theory of fields, developed around 1960 to provide a way to study the fundamental group of algebraic topology in the setting of algebraic geometry. It provides, in the classical setting of field theory, an alternative perspective to that of Emil Artin based on linear algebra, which became standard from about the 1930s.\n\nThe approach of Alexander Grothendieck is concerned with the category-theoretic properties that characterise the categories of finite \"G\"-sets for a fixed profinite group \"G\". For example, \"G\" might be the group denoted formula_1, which is the inverse limit of the cyclic additive groups Z/nZ — or equivalently the completion of the infinite cyclic group Z for the topology of subgroups of finite index. A finite \"G\"-set is then a finite set \"X\" on which \"G\" acts through a quotient finite cyclic group, so that it is specified by giving some permutation of \"X\".\n\nIn the above example, a connection with classical Galois theory can be seen by regarding formula_1 as the profinite Galois group Gal(F/F) of the algebraic closure F of any finite field \"F\", over \"F\". That is, the automorphisms of F fixing \"F\" are described by the inverse limit, as we take larger and larger finite splitting fields over \"F\". The connection with geometry can be seen when we look at covering spaces of the unit disk in the complex plane with the origin removed: the finite covering realised by the \"z\" map of the disk, thought of by means of a complex number variable \"z\", corresponds to the subgroup \"n\".Z of the fundamental group of the punctured disk.\n\nThe theory of Grothendieck, published in SGA1, shows how to reconstruct the category of \"G\"-sets from a \"fibre functor\" Φ, which in the geometric setting takes the fibre of a covering above a fixed base point (as a set). In fact there is an isomorphism proved of the type\n\nthe latter being the group of automorphisms (self-natural equivalences) of Φ. An abstract classification of categories with a functor to the category of sets is given, by means of which one can recognise categories of \"G\"-sets for \"G\" profinite.\n\nTo see how this applies to the case of fields, one has to study the tensor product of fields. Later developments in topos theory make this all part of a theory of \"atomic toposes\".\n\n\n\n"}
{"id": "31575765", "url": "https://en.wikipedia.org/wiki?curid=31575765", "title": "Haynsworth inertia additivity formula", "text": "Haynsworth inertia additivity formula\n\nIn mathematics, the Haynsworth inertia additivity formula, discovered by Emilie Virginia Haynsworth (1916–1985), concerns the number of positive, negative, and zero eigenvalues of a Hermitian matrix and of block matrices into which it is partitioned.\n\nThe \"inertia\" of a Hermitian matrix \"H\" is defined as the ordered triple\n\nwhose components are respectively the numbers of positive, negative, and zero eigenvalues of \"H\". Haynsworth considered a partitioned Hermitian matrix\n\nwhere \"H\" is nonsingular and \"H\" is the conjugate transpose of \"H\". The formula states:\n\nwhere \"H\"/\"H\" is the Schur complement of \"H\" in \"H\":\n\n"}
{"id": "2099648", "url": "https://en.wikipedia.org/wiki?curid=2099648", "title": "How to Lie with Statistics", "text": "How to Lie with Statistics\n\nHow to Lie with Statistics is a book written by Darrell Huff in 1954 presenting an introduction to statistics for the general reader. Not a statistician, Huff was a journalist who wrote many \"how to\" articles as a freelancer.\n\nThe book is a brief illustrated volume outlining errors when it comes to the interpretation of statistics, and how these errors may create incorrect conclusions. \n\nIn the 1960s and 1970s, it became a standard textbook introduction to the subject of statistics for many college students. It has become one of the best-selling statistics books in history, with over one and a half million copies sold in the English-language edition. It has also been widely translated.\n\nThemes of the book include \"Correlation does not imply causation\" and \"Using random sampling\". It also shows how statistical graphs can be used to distort reality, for example by truncating the bottom of a line or bar chart, so that differences seem larger than they are, or by representing one-dimensional quantities on a pictogram by two- or three-dimensional objects to compare their sizes, so that the reader forgets that the images do not scale the same way the quantities do.\n\nThe original edition contained illustrations by artist Irving Geis. In a UK edition, these were replaced with cartoons by Mel Calman.\n\n\n\n\n"}
{"id": "39305952", "url": "https://en.wikipedia.org/wiki?curid=39305952", "title": "Iterated limit", "text": "Iterated limit\n\nIn multivariable calculus, an iterated limit is an expression of the form\n\nOne has an expression whose value depends on at least two variables, one takes the limit as one of the two variables approaches some number, getting an expression whose value depends only on the other variable, and then one takes the limit as the other variable approaches some number. This is not defined in the same way as the limit\n\nwhich is not an iterated limit. To say that this latter limit of a function of more than one variable is equal to a particular number \"L\" means that \"ƒ\"(\"x\", \"y\") can be made as close to \"L\" as desired by making the point (\"x\", \"y\") close enough to the point (\"p\", \"q\"). It does not involve first taking one limit and then another.\n\nIt is not in all cases true that\n\nAmong the standard counterexamples are those in which\n\nand\n\nand (\"p\", \"q\") = (0, 0).\n\nIn the first example, the values of the two iterated limits differ from each other:\n\nand\n\nIn the second example, the two iterated limits are equal to each other despite the fact that the limit as (\"x\", \"y\") → (0, 0) does not exist:\n\nand\n\nbut the limit as (\"x\", \"y\") → (0, 0) along the line \"y\" = \"x\" is different:\n\nIt follows that\n\ndoes not exist.\n\nA sufficient condition for () to hold is \"Moore-Osgood theorem\": If formula_11 exists pointwise for each \"y\" different from \"q\" and if formula_12 converges uniformly for \"x\"≠\"p\" then the double limit and the iterated limits exist and are equal.\n\n"}
{"id": "17053866", "url": "https://en.wikipedia.org/wiki?curid=17053866", "title": "Jeu de taquin", "text": "Jeu de taquin\n\nIn the mathematical field of combinatorics, jeu de taquin is a construction due to which defines an equivalence relation on the set of skew standard Young tableaux. A jeu de taquin slide is a transformation where the numbers in a tableau are moved around in a way similar to how the pieces in the fifteen puzzle move. Two tableaux are jeu de taquin equivalent if one can be transformed into the other via a sequence of such slides.\n\n\"Jeu de taquin\" (literally \"teasing game\") is the .\n\nGiven a skew standard Young tableau \"T\" of skew shape formula_1, pick an adjacent empty cell \"c\" that can be added to the skew diagram formula_2; what this means is that \"c\" must share at least one edge with some cell in \"T\", and formula_3 must also be a skew diagram. There are two kinds of slide, depending on whether \"c\" lies to the upper left of \"T\" or to the lower right. Suppose to begin with that \"c\" lies to the upper left. Slide the number from its neighbouring cell into \"c\"; if \"c\" has neighbours both to its right and below, then pick the smallest of these two numbers. (This rule is designed so that the tableau property of having increasing rows and columns will be preserved.) If the cell that just has been emptied has no neighbour to its right or below, then the slide is completed. Otherwise, slide a number into that cell according to the same rule as before, and continue in this way until the slide is completed. After this transformation, the resulting tableau (with the now-empty cell removed) is still a skew (or possibly straight) standard Young tableau.\n\nThe other kind of slide, when \"c\" lies to the lower right of \"T\", just goes in the opposite direction. In this case, one slides numbers into an empty cell from the neighbour to its left or above, picking the larger number whenever there is a choice. The two types of slides are mutual inverses – a slide of one kind can be undone using a slide of the other kind.\n\nThe two slides described above are referred to as slides into the cell \"c\". The first kind of slide (when \"c\" lies to the upper left of \"T\") is said to be an inward slide; the second kind is referred to as an outward slide.\n\nThe word \"slide\" is synonymous to the French word \"glissement\", which is occasionally also used in English literature.\n\nJeu-de-taquin slides change not only the relative order of the entries of a tableau, but also its shape. In the definition given above, the result of a jeu-de-taquin slide is given as a skew diagram along with a skew standard tableau having it as shape. Often, it is better to work with skew shapes rather than skew diagrams. (Recall that every skew shape formula_1 gives rise to a skew diagram formula_5, but this is not an injective correspondence because, e. g., the distinct skew shapes formula_6 and formula_7 yield the same skew diagram.) For this reason, it is useful to modify the above definition of a jeu-de-taquin slide in such a way that, when given a skew shape along with a skew standard tableau and an addable cell as an input, it yields a well-defined skew \"shape\" along with a skew standard tableau at its output. This is done as follows: An inward slide of a skew tableau \"T\" of skew shape formula_1 into a cell \"c\" is defined as above when \"c\" is a corner of formula_9 (that is, when formula_10 is a Young diagram), and the resulting skew shape is set to be formula_11 where \"d\" is the empty cell at the end of the sliding procedure. An outward slide of a skew tableau \"T\" of skew shape formula_1 into a cell \"c\" is defined as above when \"c\" is a cocorner of formula_13 (that is, when formula_14 is a Young diagram), and the resulting skew shape is set to be formula_15 where \"d\" is the empty cell at the end of the sliding procedure.\n\nJeu de taquin slides generalize to skew semistandard (as opposed to skew standard) tableaux and retain most of their properties in that generality. The only change that has to be made to the definition of an inward slide, in order for it to generalize, is a rule on how to proceed when the (temporarily) empty cell has neighbours below and to its right, and these neighbours are filled with equal numbers. In this situation, the neighbour \"below\" (not the one to the right) has to be slid into the empty cell. A similar change is needed in the definition of an outward slide (where one has to choose the neighbor above). These changes may seem arbitrary, but they actually make the \"only reasonable choices\"—meaning the only choices that keep the columns of the tableau (disregarding the empty cell) strictly increasing (as opposed to just weakly increasing).\n\nGiven a skew standard or skew semistandard tableau \"T\", one can iteratively apply inward slides to \"T\" until the tableau becomes straight-shape (which means no more inward slides are possible). This can generally be done in many different ways (one can freely choose into which cell to slide first), but the resulting straight-shape tableau is known to be the same for all possible choices. This tableau is called the rectification of \"T\".\n\nTwo skew semistandard tableaux \"T\" and \"S\" are said to be jeu-de-taquin equivalent if one can transform one of them into the other using a sequence (possibly empty) of slides (both inward and outward slides being allowed). Equivalently, two skew semistandard tableaux \"T\" and \"S\" are jeu-de-taquin equivalent if and only if they have the same rectification.\n\nThere are various ways to associate a word (in the sense of combinatorics, i. e., a finite sequence of elements of an alphabet—here the set of positive integers) to every Young tableau. We choose the one apparently most popular: We associate to every Young tableau \"T\" the word obtained by concatenating the rows of \"T\" from the bottom row to the top row. (Each row of \"T\" is seen as a word simply by reading its entries from left to right, and we draw Young tableaux in English notation so that the longest row of a straight-shape tableau appears at the top.) This word will be referred to as the reading word, or briefly as the word, of \"T\".\n\nIt can then be shown that two skew semistandard tableaux \"T\" and \"S\" are jeu-de-taquin equivalent if and only if the reading words of \"T\" and \"S\" are Knuth equivalent. As a consequence, the rectification of a skew semistandard tableau \"T\" can also be obtained as the insertion tableau of the reading word of \"T\" under the Robinson-Schensted correspondence.\n\nJeu de taquin can be used to define an operation on standard Young tableaux of any given shape, which turns out to be an involution, although this is not obvious from the definition. One starts by emptying the square in the top-left corner, turning the tableau into a skew tableau with one less square. Now apply a jeu de taquin slide to turn that skew tableau into a straight one, which will free one square on the outside border. Then fill this square with the negative of the value that was originally removed at the top-left corner; this negated value is considered part of a new tableau rather than of the original tableau, and its position will not change in the sequel. Now as long as the original tableau has some entries left, repeat the operation of removing the entry \"x\" of the top-left corner, performing a jeu de taquin slide on what is left of the original tableau, and placing the value −\"x\" into the square so freed. When all entries of the original tableau have been handled, their negated values are arranged in such a way that rows and columns are increasing. Finally one can add an appropriate constant to all entries to obtain a Young tableau with positive entries.\n\nJeu de taquin is closely connected to such topics as the Robinson–Schensted–Knuth correspondence, the Littlewood–Richardson rule, and Knuth equivalence.\n\n"}
{"id": "54457435", "url": "https://en.wikipedia.org/wiki?curid=54457435", "title": "Jónsson term", "text": "Jónsson term\n\nIn universal algebra, within mathematics, a Jónsson term or majority term is a term \"t\" with exactly three free variables that satisfies the equations \"t\"(\"x\", \"x\", \"y\") = \"t\"(\"x\", \"y\", \"x\") = \"t\"(\"y\", \"x\", \"x\") = \"x\".\n\nFor example for lattices, the term (\"x\" ∧ \"y\") ∨ (\"y\" ∧ \"z\") ∨ (\"z\" ∧ \"x\") is a Jónsson term.\n\nJónsson terms are named after the Icelandic logician Bjarni Jónsson.\n"}
{"id": "33498023", "url": "https://en.wikipedia.org/wiki?curid=33498023", "title": "Kappa calculus", "text": "Kappa calculus\n\nIn mathematical logic, category theory, and\ncomputer science, kappa calculus is a\nformal system for defining first-order\nfunctions.\n\nUnlike lambda calculus, kappa calculus has no\nhigher-order functions; its functions are\nnot first class objects. Kappa-calculus can be\nregarded as \"a reformulation of the first-order fragment of typed\nlambda calculus\".\n\nBecause its functions are not first-class objects, evaluation of kappa\ncalculus expressions does not require\nclosures.\n\n\"The definition below has been adapted from the diagrams on pages 205\nand 207 of Hasegawa.\"\n\nKappa calculus consists of \"types\" and \"expressions,\" given by the\ngrammar below:\n\nIn other words,\n\n\nThe formula_13 and the subscripts of , , and formula_14 are\nsometimes omitted when they can be unambiguously determined from the\ncontext.\n\nJuxtaposition is often used as an abbreviation for a combination of\nformula_14 and composition:\n\n\"The presentation here uses sequents (formula_17) rather than hypothetical judgments in order to ease comparison with the simply typed lambda calculus. This requires the additional Var rule, which does not appear in Hasegawa\"\n\nIn kappa calculus an expression has two types: the type of its \"source\" and the type of its \"target\". The notation formula_18 is used to indicate that expression e has source type formula_19 and target type formula_20.\n\nExpressions in kappa calculus are assigned types according to the following rules:\n\nIn other words,\n\n\nKappa calculus obeys the following equalities:\n\n\nThe last two equalities are reduction rules for the calculus,\nrewriting from left to right.\n\nThe type can be regarded as the unit type. Because of this, any two functions whose argument type is the same and whose result type is should be equal – since there is only a single value of type both functions must return that value for every argument (Terminality).\n\nExpressions with type formula_47 can be regarded as \"constants\" or values of \"ground type\"; this is because is the unit type, and so a function from this type is necessarily a constant function. Note that the kappa rule allows abstractions only when the variable being abstracted has the type formula_47 for some . This is the basic mechanism which ensures that all functions are first-order.\n\nKappa calculus is intended to be the internal language of\n\"contextually complete\" categories.\n\nExpressions with multiple arguments have source types which are\n\"right-imbalanced\" binary trees. For example, a function f with three\narguments of types A, B, and C and result type D will have type\n\nIf we define left-associative juxtaposition formula_50 as an abbreviation\nfor formula_51, then – assuming that\nformula_52, formula_53, and\nformula_54 – we can apply this function:\n\nSince the expression formula_56 has source type , it is a \"ground value\" and may be passed as an argument to another function. If formula_57, then\n\nMuch like a curried function of type\nformula_59 in lambda calculus, partial\napplication is possible:\n\nHowever no higher types (i.e. formula_61) are involved. Note that because the source type of is not , the following expression cannot be well-typed under the assumptions mentioned so far:\n\nBecause successive application is used for multiple\narguments it is not necessary to know the arity of a function in\norder to determine its typing; for example, if we know that\nformula_54 then the expression\n\nis well-typed as long as has type\nand . This property is important when calculating\nthe principal type of an expression, something\nwhich can be difficult when attempting to exclude higher-order\nfunctions from typed lambda calculi by restricting the grammar of types.\n\nBarendregt originally introduced the term\n\"functional completeness\" in the context of combinatory algebra.\nKappa calculus arose out of efforts by Lambek to formulate an appropriate analogue of functional\ncompleteness for arbitrary categories (see Hermida and Jacobs, section 1). Hasegawa later developed kappa\ncalculus into a usable (though simple) programming language including\narithmetic over natural numbers and primitive recursion. Connections to arrows\nwere later investigated by Power, Thielecke, and others.\n\nIt is possible to explore versions of kappa calculus with\nsubstructural types such as linear, affine,\nand ordered types. These extensions require eliminating or\nrestricting the formula_7 expression. In such circumstances\nthe type operator is not a true cartesian product,\nand is generally written to make this clear.\n"}
{"id": "31302509", "url": "https://en.wikipedia.org/wiki?curid=31302509", "title": "Klee–Minty cube", "text": "Klee–Minty cube\n\nThe Klee–Minty cube or Klee–Minty polytope (named after Victor Klee and ) is a unit hypercube of variable dimension whose corners have been perturbed. Klee and Minty demonstrated that George Dantzig's simplex algorithm has poor worst-case performance when initialized at one corner of their \"squashed cube\".\n\nIn particular, many optimization algorithms for linear optimization exhibit poor performance when applied to the Klee–Minty cube. In 1973 Klee and Minty showed that Dantzig's simplex algorithm was not a polynomial-time algorithm when applied to their cube. Later, modifications of the Klee–Minty cube have shown poor behavior both for other basis-exchange pivoting algorithms and also for interior-point algorithms.\n\nThe Klee–Minty cube was originally specified with a parameterized system of linear inequalities, with the dimension as the parameter. When the dimension is two, the \"cube\" is a squashed square. When the dimension is three, the \"cube\" is a squashed cube. Illustrations of the \"cube\" have appeared besides algebraic descriptions.\n\nThe Klee–Minty polytope is given by\n\nThis has \"D\" variables, \"D\" constraints other than the \"D\" non-negativity constraints, and 2 vertices, just as a \"D\"-dimensional hypercube does. If the objective function to be maximized is\n\nand if the initial vertex for the simplex algorithm is the origin, then the algorithm as formulated by Dantzig visits all 2 vertices, finally reaching the optimal vertex formula_3\n\nThe Klee–Minty cube has been used to analyze the performance of many algorithms, both in the worst case and on average. The time complexity of an algorithm counts the number of arithmetic operations sufficient for the algorithm to solve the problem. For example, Gaussian elimination requires on the order of\" D\" operations, and so it is said to have polynomial time-complexity, because its complexity is bounded by a cubic polynomial. There are examples of algorithms that do not have polynomial-time complexity. For example, a generalization of Gaussian elimination called Buchberger's algorithm has for its complexity an exponential function of the problem data (the degree of the polynomials and the number of variables of the multivariate polynomials). Because exponential functions eventually grow much faster than polynomial functions, an exponential complexity implies that an algorithm has slow performance on large problems.\n\nIn mathematical optimization, the Klee–Minty cube is an example that shows the worst-case computational complexity of many algorithms of linear optimization. It is a deformed cube with exactly  2 corners in dimension \"D\". Klee and Minty showed that Dantzig's simplex algorithm visits all corners of a (perturbed) cube in dimension \"D\" in the worst case.\n\nModifications of the Klee–Minty construction showed similar exponential time complexity for other pivoting rules of simplex type, which maintain primal feasibility, such as Bland's rule. Another modification showed that the criss-cross algorithm, which does not maintain primal feasibility, also visits all the corners of a modified Klee–Minty cube. Like the simplex algorithm, the criss-cross algorithm visits all 8 corners of the three-dimensional cube in the worst case.\n\nFurther modifications of the Klee–Minty cube have shown poor performance of central-path–following algorithms for linear optimization, in that the central path comes arbitrarily close to each of the corners of a cube. This \"vertex-stalking\" performance is surprising, because such path-following algorithms have polynomial-time complexity for linear optimization.\n\nThe Klee–Minty cube has also inspired research on average-case complexity. When eligible pivots are made randomly (and not by the rule of steepest descent), Dantzig's simplex algorithm needs on average quadratically many steps (on the order of O(\"D\").\nStandard variants of the simplex algorithm takes on average \"D\" steps for a cube. When it is initialized at a random corner of the cube, the criss-cross algorithm visits only \"D\" additional corners, however, according to a 1994 paper by Fukuda and Namiki. Both the simplex algorithm and the criss-cross algorithm visit exactly 3 additional corners of the three-dimensional cube on average.\n\n\n\nThe first two links have both an algebraic construction and a picture of a three-dimensional Klee–Minty cube:\n\n"}
{"id": "2908224", "url": "https://en.wikipedia.org/wiki?curid=2908224", "title": "Klein geometry", "text": "Klein geometry\n\nIn mathematics, a Klein geometry is a type of geometry motivated by Felix Klein in his influential Erlangen program. More specifically, it is a homogeneous space \"X\" together with a transitive action on \"X\" by a Lie group \"G\", which acts as the symmetry group of the geometry.\n\nFor background and motivation see the article on the Erlangen program.\n\nA Klein geometry is a pair where \"G\" is a Lie group and \"H\" is a closed Lie subgroup of \"G\" such that the (left) coset space \"G\"/\"H\" is connected. The group \"G\" is called the principal group of the geometry and \"G\"/\"H\" is called the space of the geometry (or, by an abuse of terminology, simply the \"Klein geometry\"). The space of a Klein geometry is a smooth manifold of dimension\n\nThere is a natural smooth left action of \"G\" on \"X\" given by\nClearly, this action is transitive (take ), so that one may then regard \"X\" as a homogeneous space for the action of \"G\". The stabilizer of the identity coset is precisely the group \"H\".\n\nGiven any connected smooth manifold \"X\" and a smooth transitive action by a Lie group \"G\" on \"X\", we can construct an associated Klein geometry by fixing a basepoint \"x\" in \"X\" and letting \"H\" be the stabilizer subgroup of \"x\" in \"G\". The group \"H\" is necessarily a closed subgroup of \"G\" and \"X\" is naturally diffeomorphic to \"G\"/\"H\".\n\nTwo Klein geometries and are geometrically isomorphic if there is a Lie group isomorphism so that . In particular, if \"φ\" is conjugation by an element , we see that and are isomorphic. The Klein geometry associated to a homogeneous space \"X\" is then unique up to isomorphism (i.e. it is independent of the chosen basepoint \"x\").\n\nGiven a Lie group \"G\" and closed subgroup \"H\", there is natural right action of \"H\" on \"G\" given by right multiplication. This action is both free and proper. The orbits are simply the left cosets of \"H\" in \"G\". One concludes that \"G\" has the structure of a smooth principal \"H\"-bundle over the left coset space \"G\"/\"H\":\n\nThe action of \"G\" on need not be effective. The kernel of a Klein geometry is defined to be the kernel of the action of \"G\" on \"X\". It is given by\nThe kernel \"K\" may also be described as the core of \"H\" in \"G\" (i.e. the largest subgroup of \"H\" that is normal in \"G\"). It is the group generated by all the normal subgroups of \"G\" that lie in \"H\".\n\nA Klein geometry is said to be effective if and locally effective if \"K\" is discrete. If is a Klein geometry with kernel \"K\", then is an effective Klein geometry canonically associated to .\n\nA Klein geometry is geometrically oriented if \"G\" is connected. (This does \"not\" imply that \"G\"/\"H\" is an oriented manifold). If \"H\" is connected it follows that \"G\" is also connected (this is because \"G\"/\"H\" is assumed to be connected, and is a fibration).\n\nGiven any Klein geometry , there is a geometrically oriented geometry canonically associated to with the same base space \"G\"/\"H\". This is the geometry where \"G\" is the identity component of \"G\". Note that .\n\nA Klein geometry is said to be reductive and \"G\"/\"H\" a reductive homogeneous space if the Lie algebra formula_4 of \"H\" has an \"H\"-invariant complement in formula_5.\n\nIn the following table, there is a description of the classical geometries, modeled as Klein geometries.\n"}
{"id": "48124047", "url": "https://en.wikipedia.org/wiki?curid=48124047", "title": "Kuranishi structure", "text": "Kuranishi structure\n\nIn mathematics, especially in topology, a Kuranishi structure is a smooth analogue of scheme structure. If a topological space is endowed with a Kuranishi structure, then locally it can be identified with the zero set of a smooth map formula_1, or the quotient of such a zero set by a finite group. Kuranishi structure was introduced by Japanese mathematicians Kenji Fukaya and Kaoru Ono in the study of Gromov–Witten invariants in symplectic geometry and named after Masatake Kuranishi.\n\nLet formula_2 be a compact metrizable topological space. Let formula_3 be a point. A Kuranishi neighborhood of formula_4 (of dimension formula_5) is a 5-tuple\n\nwhere\n\n\nThey should satisfy that formula_12.\n\nIf formula_13 and formula_6, formula_15 are their Kuranishi neighborhoods respectively, then a coordinate change from formula_16 to formula_17 is a triple\n\nwhere\n\n\nIn addition, they must satisfy the compatibility condition:\n\n\nA Kuranishi structure on formula_2 of dimension formula_5 is a collection\n\nwhere\n\n\nIn addition, the coordinate changes must satisfy the cocycle condition, namely, whenever formula_34, we require that\n\nover the regions where both sides are defined.\n\nIn Gromov–Witten theory, one needs to define integration over the moduli space of stable maps formula_36. They are maps formula_37 from a nodal Riemann surface with genus formula_38 and formula_39 marked points into a symplectic manifold formula_2, such that each component satisfies the Cauchy–Riemann equation\n\nIf the moduli space is a smooth, compact, oriented manifold or orbifold, then the integration (or a fundamental class) can be defined. When the symplectic manifold formula_2 is semi-positive, this is indeed the case (except for codimension 2 boundaries of the moduli space) if the almost complex structure formula_43 is perturbed generically. However, when formula_2 is not semi-positive, the moduli space may contain configurations for which one component is a multiple cover of a holomorphic sphere formula_45 whose intersection with the first Chern class of formula_2 is negative. Such configurations make the moduli space very singular so a fundamental class cannot be defined in the usual way.\n\nThe notion of Kuranishi structure was a way of defining a virtual fundamental cycle, which plays the same role as a fundamental cycle when the moduli space is cut out transversely. It was first used by Fukaya and Ono in defining the Gromov–Witten invariants and Floer homology, and was further developed when Fukaya, Oh, Ohta, Ono studied the Lagrangian intersection Floer theory.\n"}
{"id": "31567389", "url": "https://en.wikipedia.org/wiki?curid=31567389", "title": "M-Labs", "text": "M-Labs\n\nM-Labs (formerly known as the Milkymist project) is a company and community who develop, manufacture and sell advanced open hardware devices and solutions. It is best known for the Milkymist system-on-chip (SoC) which is among the first commercialized system-on-chip designs with free HDL source code.\n\nM-Labs technologies have been reused in diverse applications. For example, NASA's Communication Navigation and Networking Reconfigurable Testbed (CoNNeCT) experiment uses the memory controller that was originally developed for the Milkymist One and published under the terms of the GNU General Public License (GPL).\n\nThe project was presented at several open source and hacker conferences such as the Chaos Communication Congress, FOSDEM, Libre Software Meeting, and Libre Graphics Meeting 2011. It was also featured on the \"Make\" magazine blog and the Milkymist One board was included in their \"ultimate open source hardware gift guide 2010\".\n\nThe Milkymist system-on-chip uses the LatticeMico32 (LM32) core as a general purpose processor. It is a RISC 32-bit big endian CPU with a memory management unit (MMU) developed later by M-Labs contributors. It is supported by the GCC compiler and can run RTEMS and μClinux. There is also an experimental back-end for LLVM targeting this microprocessor.\n\nThe LM32 microprocessor is assisted by a texture mapping unit and a programmable floating point VLIW coprocessor which are used by the Flickernoise video synthesis software. It is also surrounded by various peripheral cores to support every I/O device of the Milkymist One. The system-on-chip interconnect uses three bridged buses and mixes the Wishbone protocol with two custom protocols used for configuration registers and high performance DMA with the SDRAM.\n\nThe architecture of the Milkymist system-on-chip is largely documented in the project founder's Master thesis report. Most components of the system-on-chip, except the LatticeMico32 core, were custom developed and placed under the GNU GPL license.\n\nThe QEMU emulator can be used to run and debug Milkymist SoC binaries on another computer.\n\nThe Milkymist One video synthesizer and reconfigurable computer is the main product released by the project. It was manufactured by Qi Hardware, a start-up founded by former Openmoko employees. It was first sold at the Chaos Communication Congress in 2010, as an \"early developer kit\" for interested hackers, open source activists and pioneers who could tolerate the remaining software and FPGA design shortcomings. A more refined version, including case and accessories, was later offered for sale.\n\nThe technical specifications of the Milkymist One are as follows:\n\nThe design files of the printed circuit board and the CAD files of the case were released under the Creative Commons Attribution-Share Alike license.\n\nFlickernoise is the video synthesis software that runs on the Milkymist One. It is heavily inspired by MilkDrop and uses a similar, and largely compatible, scripting language to define and program the visual effects. However, while MilkDrop is designed to run automatically in a music player, Flickernoise is focused on the interactivity of the visuals for use in live performances. The software supports the programming of visual effects that transform a live video stream coming from a camera connected to the Milkymist One, as well as input from OpenSoundControl, DMX512 and MIDI controllers.\n\nFlickernoise runs on the RTEMS real-time operating system, and uses many POSIX software libraries that were ported to this operating system such as libpng, libjpeg, jbig2dec, OpenJPEG, FreeType, MuPDF and liblo for OpenSoundControl support. The streamlined hardware platform along with the use of a real-time operating system allows the system to have a lower response time than an equivalent PC-based setup. The user interface is based on a variant of the Genode FX toolkit\n\nFlickernoise is also free software, released under the terms of the GNU General Public License.\n\nIn May 2014, M-Labs entered a partnership with NIST to develop a next-generation open source control system for quantum information experiments. The system, called ARTIQ (Advanced Real-Time Infrastructure for Quantum physics), is a combination of software and gateware that enables synchronized control of many devices with nanosecond-level timing resolution and sub-microsecond latency, while retaining features of high level programming languages.\n\nSome of the ideas and code from Milkymist SoC have been reused in ARTIQ.\n\nIn 2016 M-Labs partnered with ARL and ISE to develop ARTIQ Sinara, an open source hardware and software-defined radio platform.\n"}
{"id": "644671", "url": "https://en.wikipedia.org/wiki?curid=644671", "title": "Mirror symmetry (string theory)", "text": "Mirror symmetry (string theory)\n\nIn algebraic geometry and theoretical physics, mirror symmetry is a relationship between geometric objects called Calabi–Yau manifolds. The term refers to a situation where two Calabi–Yau manifolds look very different geometrically but are nevertheless equivalent when employed as extra dimensions of string theory.\n\nMirror symmetry was originally discovered by physicists. Mathematicians became interested in this relationship around 1990 when Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that it could be used as a tool in enumerative geometry, a branch of mathematics concerned with counting the number of solutions to geometric questions. Candelas and his collaborators showed that mirror symmetry could be used to count rational curves on a Calabi–Yau manifold, thus solving a longstanding problem. Although the original approach to mirror symmetry was based on physical ideas that were not understood in a mathematically precise way, some of its mathematical predictions have since been proven rigorously.\n\nToday, mirror symmetry is a major research topic in pure mathematics, and mathematicians are working to develop a mathematical understanding of the relationship based on physicists' intuition. Mirror symmetry is also a fundamental tool for doing calculations in string theory, and it has been used to understand aspects of quantum field theory, the formalism that physicists use to describe elementary particles. Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.\n\nIn physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. These strings look like small segments or loops of ordinary string. String theory describes how strings propagate through space and interact with each other. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. Splitting and recombination of strings correspond to particle emission and absorption, giving rise to the interactions between particles.\n\nThere are notable differences between the world described by string theory and the everyday world. In everyday life, there are three familiar dimensions of space (up/down, left/right, and forward/backward), and there is one dimension of time (later/earlier). Thus, in the language of modern physics, one says that spacetime is four-dimensional. One of the peculiar features of string theory is that it requires extra dimensions of spacetime for its mathematical consistency. In superstring theory, the version of the theory that incorporates a theoretical idea called supersymmetry, there are six extra dimensions of spacetime in addition to the four that are familiar from everyday experience.\n\nOne of the goals of current research in string theory is to develop models in which the strings represent particles observed in high energy physics experiments. For such a model to be consistent with observations, its spacetime must be four-dimensional at the relevant distance scales, so one must look for ways to restrict the extra dimensions to smaller scales. In most realistic models of physics based on string theory, this is accomplished by a process called compactification, in which the extra dimensions are assumed to \"close up\" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.\n\nCompactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold. A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.\n\nAfter Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions, many physicists began studying these manifolds. In the late 1980s, Lance Dixon, Wolfgang Lerche, Cumrun Vafa, and Nick Warner noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold. Instead, two different versions of string theory called type IIA string theory and type IIB can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.\n\nThe mirror symmetry relationship is a particular example of what physicists call a duality. In general, the term \"duality\" refers to a situation where two seemingly different physical theories turn out to be equivalent in a nontrivial way. If one theory can be transformed so it looks just like another theory, the two are said to be dual under that transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena. Such dualities play an important role in modern physics, especially in string theory.\n\nRegardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative algebraic geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions. A classical problem of enumerative geometry is to enumerate the rational curves on a Calabi–Yau manifold such as the one illustrated above. By applying mirror symmetry, mathematicians have translated this problem into an equivalent problem for the mirror Calabi–Yau, which turns out to be easier to solve.\n\nIn physics, mirror symmetry is justified on physical grounds. However, mathematicians generally require rigorous proofs that do not require an appeal to physical intuition. From a mathematical point of view, the version of mirror symmetry described above is still only a conjecture, but there is another version of mirror symmetry in the context of topological string theory, a simplified version of string theory introduced by Edward Witten, which has been rigorously proven by mathematicians. In the context of topological string theory, mirror symmetry states that two theories called the A-model and B-model are equivalent in the sense that there is a duality relating them. Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition.\n\nThe idea of mirror symmetry can be traced back to the mid-1980s when it was noticed that a string propagating on a circle of radius formula_1 is physically equivalent to a string propagating on a circle of radius formula_2 in appropriate units. This phenomenon is now known as T-duality and is understood to be closely related to mirror symmetry. In a paper from 1985, Philip Candelas, Gary Horowitz, Andrew Strominger, and Edward Witten showed that by compactifying string theory on a Calabi–Yau manifold, one obtains a theory roughly similar to the standard model of particle physics that also consistently incorporates an idea called supersymmetry. Following this development, many physicists began studying Calabi–Yau compactifications, hoping to construct realistic models of particle physics based on string theory. Cumrun Vafa and others noticed that given such a physical model, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold. Instead, there are two Calabi–Yau manifolds that give rise to the same physics.\n\nBy studying the relationship between Calabi–Yau manifolds and certain conformal field theories called Gepner models, Brian Greene and Ronen Plesser found nontrivial examples of the mirror relationship. Further evidence for this relationship came from the work of Philip Candelas, Monika Lynker, and Rolf Schimmrigk, who surveyed a large number of Calabi–Yau manifolds by computer and found that they came in mirror pairs.\n\nMathematicians became interested in mirror symmetry around 1990 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that mirror symmetry could be used to solve problems in enumerative geometry that had resisted solution for decades or more. These results were presented to mathematicians at a conference at the Mathematical Sciences Research Institute (MSRI) in Berkeley, California in May 1991. During this conference, it was noticed that one of the numbers Candelas had computed for the counting of rational curves disagreed with the number obtained by Norwegian mathematicians Geir Ellingsrud and Stein Arild Strømme using ostensibly more rigorous techniques. Many mathematicians at the conference assumed that Candelas's work contained a mistake since it was not based on rigorous mathematical arguments. However, after examining their solution, Ellingsrud and Strømme discovered an error in their computer code and, upon fixing the code, they got an answer that agreed with the one obtained by Candelas and his collaborators.\n\nIn 1990, Edward Witten introduced topological string theory, a simplified version of string theory, and physicists showed that there is a version of mirror symmetry for topological string theory. This statement about topological string theory is usually taken as the definition of mirror symmetry in the mathematical literature. In an address at the International Congress of Mathematicians in 1994, mathematician Maxim Kontsevich presented a new mathematical conjecture based on the physical idea of mirror symmetry in topological string theory. Known as homological mirror symmetry, this conjecture formalizes mirror symmetry as an equivalence of two mathematical structures: the derived category of coherent sheaves on a Calabi–Yau manifold and the Fukaya category of its mirror.\n\nAlso around 1995, Kontsevich analyzed the results of Candelas, which gave a general formula for the problem of counting rational curves on a quintic threefold, and he reformulated these results as a precise mathematical conjecture. In 1996, Alexander Givental posted a paper that claimed to prove this conjecture of Kontsevich. Initially, many mathematicians found this paper hard to understand, so there were doubts about its correctness. Subsequently, Bong Lian, Kefeng Liu, and Shing-Tung Yau published an independent proof in a series of papers. Despite controversy over who had published the first proof, these papers are now collectively seen as providing a mathematical proof of the results originally obtained by physicists using mirror symmetry. In 2000, Kentaro Hori and Cumrun Vafa gave another physical proof of mirror symmetry based on T-duality.\n\nWork on mirror symmetry continues today with major developments in the context of strings on surfaces with boundaries. In addition, mirror symmetry has been related to many active areas of mathematics research, such as the McKay correspondence, topological quantum field theory, and the theory of stability conditions. At the same time, basic questions continue to vex. For example, mathematicians still lack an understanding of how to construct examples of mirror Calabi–Yau pairs though there has been progress in understanding this issue.\n\nMany of the important mathematical applications of mirror symmetry belong to the branch of mathematics called enumerative geometry. In enumerative geometry, one is interested in counting the number of solutions to geometric questions, typically using the techniques of algebraic geometry. One of the earliest problems of enumerative geometry was posed around the year 200 BCE by the ancient Greek mathematician Apollonius, who asked how many circles in the plane are tangent to three given circles. In general, the solution to the problem of Apollonius is that there are eight such circles.\nEnumerative problems in mathematics often concern a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic (see the illustration) is defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.\n\nGeneralizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.\n\nBy the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish. According to mathematician Mark Gross, \"As the old problems had been solved, people went back to check Schubert's numbers with modern techniques, but that was getting pretty stale.\" The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parkes showed that mirror symmetry could be used to count the number of degree three curves on a quintic Calabi–Yau. Candelas and his collaborators found that these six-dimensional Calabi–Yau manifolds can contain exactly 317,206,375 curves of degree three.\n\nIn addition to counting degree-three curves on a quintic three-fold, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians. Although the methods used in this work were based on physical intuition, mathematicians have gone on to prove rigorously some of the predictions of mirror symmetry. In particular, the enumerative predictions of mirror symmetry have now been rigorously proven.\n\nIn addition to its applications in enumerative geometry, mirror symmetry is a fundamental tool for doing calculations in string theory. In the A-model of topological string theory, physically interesting quantities are expressed in terms of infinitely many numbers called Gromov–Witten invariants, which are extremely difficult to compute. In the B-model, the calculations can be reduced to classical integrals and are much easier. By applying mirror symmetry, theorists can translate difficult calculations in the A-model into equivalent but technically easier calculations in the B-model. These calculations are then used to determine the probabilities of various physical processes in string theory. Mirror symmetry can be combined with other dualities to translate calculations in one theory into equivalent calculations in a different theory. By outsourcing calculations to different theories in this way, theorists can calculate quantities that are impossible to calculate without the use of dualities.\n\nOutside of string theory, mirror symmetry is used to understand aspects of quantum field theory, the formalism that physicists use to describe elementary particles. For example, gauge theories are a class of highly symmetric physical theories appearing in the standard model of particle physics and other parts of theoretical physics. Some gauge theories which are not part of the standard model, but which are nevertheless important for theoretical reasons, arise from strings propagating on a nearly singular background. For such theories, mirror symmetry is a useful computational tool. Indeed, mirror symmetry can be used to perform calculations in an important gauge theory in four spacetime dimensions that was studied by Nathan Seiberg and Edward Witten and is also familiar in mathematics in the context of Donaldson invariants. There is also a generalization of mirror symmetry called 3D mirror symmetry which relates pairs of quantum field theories in three spacetime dimensions.\n\nIn string theory and related theories in physics, a \"brane\" is a physical object that generalizes the notion of a point particle to higher dimensions. For example, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. The word brane comes from the word \"membrane\" which refers to a two-dimensional brane.\n\nIn string theory, a string may be open (forming a segment with two endpoints) or closed (forming a closed loop). D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter \"D\" in D-brane refers to a condition that it satisfies, the Dirichlet boundary condition.\n\nMathematically, branes can be described using the notion of a category. This is a mathematical structure consisting of \"objects\", and for any pair of objects, a set of \"morphisms\" between them. In most examples, the objects are mathematical structures (such as sets, vector spaces, or topological spaces) and the morphisms are functions between these structures. One can also consider categories where the objects are D-branes and the morphisms between two branes formula_3 and formula_4 are states of open strings stretched between formula_3 and formula_4.\n\nIn the B-model of topological string theory, the D-branes are complex submanifolds of a Calabi–Yau together with additional data that arise physically from having charges at the endpoints of strings. Intuitively, one can think of a submanifold as a surface embedded inside the Calabi–Yau, although submanifolds can also exist in dimensions different from two. In mathematical language, the category having these branes as its objects is known as the derived category of coherent sheaves on the Calabi–Yau. In the A-model, the D-branes can again be viewed as submanifolds of a Calabi–Yau manifold. Roughly speaking, they are what mathematicians call special Lagrangian submanifolds. This means among other things that they have half the dimension of the space in which they sit, and they are length-, area-, or volume-minimizing. The category having these branes as its objects is called the Fukaya category.\n\nThe derived category of coherent sheaves is constructed using tools from complex geometry, a branch of mathematics that describes geometric curves in algebraic terms and solves geometric problems using algebraic equations. On the other hand, the Fukaya category is constructed using symplectic geometry, a branch of mathematics that arose from studies of classical physics. Symplectic geometry studies spaces equipped with a symplectic form, a mathematical tool that can be used to compute area in two-dimensional examples.\n\nThe homological mirror symmetry conjecture of Maxim Kontsevich states that the derived category of coherent sheaves on one Calabi–Yau manifold is equivalent in a certain sense to the Fukaya category of its mirror. This equivalence provides a precise mathematical formulation of mirror symmetry in topological string theory. In addition, it provides an unexpected bridge between two branches of geometry, namely complex and symplectic geometry.\n\nAnother approach to understanding mirror symmetry was suggested by Andrew Strominger, Shing-Tung Yau, and Eric Zaslow in 1996. According to their conjecture, now known as the SYZ conjecture, mirror symmetry can be understood by dividing a Calabi–Yau manifold into simpler pieces and then transforming them to get the mirror Calabi–Yau.\n\nThe simplest example of a Calabi–Yau manifold is a two-dimensional torus or donut shape. Consider a circle on this surface that goes once through the hole of the donut. An example is the red circle in the figure. There are infinitely many circles like it on a torus; in fact, the entire surface is a union of such circles.\n\nOne can choose an auxiliary circle formula_7 (the pink circle in the figure) such that each of the infinitely many circles decomposing the torus passes through a point of formula_7. This auxiliary circle is said to \"parametrize\" the circles of the decomposition, meaning there is a correspondence between them and points of formula_7. The circle formula_7 is more than just a list, however, because it also determines how these circles are arranged on the torus. This auxiliary space plays an important role in the SYZ conjecture.\n\nThe idea of dividing a torus into pieces parametrized by an auxiliary space can be generalized. Increasing the dimension from two to four real dimensions, the Calabi–Yau becomes a K3 surface. Just as the torus was decomposed into circles, a four-dimensional K3 surface can be decomposed into two-dimensional tori. In this case the space formula_7 is an ordinary sphere. Each point on the sphere corresponds to one of the two-dimensional tori, except for twenty-four \"bad\" points corresponding to \"pinched\" or singular tori.\n\nThe Calabi–Yau manifolds of primary interest in string theory have six dimensions. One can divide such a manifold into 3-tori (three-dimensional objects that generalize the notion of a torus) parametrized by a 3-sphere formula_7 (a three-dimensional generalization of a sphere). Each point of formula_7 corresponds to a 3-torus, except for infinitely many \"bad\" points which form a grid-like pattern of segments on the Calabi–Yau and correspond to singular tori.\n\nOnce the Calabi–Yau manifold has been decomposed into simpler parts, mirror symmetry can be understood in an intuitive geometric way. As an example, consider the torus described above. Imagine that this torus represents the \"spacetime\" for a physical theory. The fundamental objects of this theory will be strings propagating through the spacetime according to the rules of quantum mechanics. One of the basic dualities of string theory is T-duality, which states that a string propagating around a circle of radius formula_1 is equivalent to a string propagating around a circle of radius formula_2 in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum formula_16 and winding number formula_17 in one description, it will have momentum formula_17 and winding number formula_16 in the dual description. By applying T-duality simultaneously to all of the circles that decompose the torus, the radii of these circles become inverted, and one is left with a new torus which is \"fatter\" or \"skinnier\" than the original. This torus is the mirror of the original Calabi–Yau.\n\nT-duality can be extended from circles to the two-dimensional tori appearing in the decomposition of a K3 surface or to the three-dimensional tori appearing in the decomposition of a six-dimensional Calabi–Yau manifold. In general, the SYZ conjecture states that mirror symmetry is equivalent to the simultaneous application of T-duality to these tori. In each case, the space formula_7 provides a kind of blueprint that describes how these tori are assembled into a Calabi–Yau manifold.\n\n\n\n\n"}
{"id": "3204608", "url": "https://en.wikipedia.org/wiki?curid=3204608", "title": "Müntz–Szász theorem", "text": "Müntz–Szász theorem\n\nThe Müntz–Szász theorem is a basic result of approximation theory, proved by Herman Müntz in 1914 and Otto Szász (1884–1952) in 1916. Roughly speaking, the theorem shows to what extent the Weierstrass theorem on polynomial approximation can have holes dug into it, by restricting certain coefficients in the polynomials to be zero. The form of the result had been conjectured by Sergei Bernstein before it was proved.\n\nThe theorem, in a special case, states that a necessary and sufficient condition for the monomials\n\nto span a dense subset of the Banach space \"C\"[\"a\",\"b\"] of all continuous functions with complex number values on the closed interval [\"a\",\"b\"] with \"a\" > 0, with the uniform norm, is that the sum\n\nof the reciprocals, taken over \"S\", should diverge, i.e. \"S\" is a large set. For an interval [0, \"b\"], the constant functions are necessary: assuming therefore that 0 is in \"S\", the condition on the other exponents is as before.\n\nMore generally, one can take exponents from any strictly increasing sequence of positive real numbers, and the same result holds. Szász showed that for complex number exponents, the same condition applied to the sequence of real parts.\n\nThere are also versions for the \"L\" spaces.\n\n"}
{"id": "6370069", "url": "https://en.wikipedia.org/wiki?curid=6370069", "title": "Operator grammar", "text": "Operator grammar\n\nOperator grammar is a mathematical theory of human language that explains how language carries information. This theory is the culmination of the life work of Zellig Harris, with major publications toward the end of the last century. Operator Grammar proposes that each human language is a self-organizing system in which both the syntactic and semantic properties of a word are established purely in relation to other words. Thus, no external system (metalanguage) is required to define the rules of a language. Instead, these rules are learned through exposure to usage and through participation, as is the case with most social behavior. The theory is consistent with the idea that language evolved gradually, with each successive generation introducing new complexity and variation.\n\nOperator Grammar posits three universal constraints: dependency (certain words depend on the presence of other words to form an utterance), likelihood (some combinations of words and their dependents are more likely than others) and reduction (words in high likelihood combinations can be reduced to shorter forms, and sometimes omitted completely). Together these provide a theory of language information: dependency builds a predicate–argument structure; likelihood creates distinct meanings; reduction allows compact forms for communication.\n\nThe fundamental mechanism of operator grammar is the dependency constraint: certain words (operators) require that one or more words (arguments) be present in an utterance. In the sentence \"John wears boots\", the operator \"wears\" requires the presence of two arguments, such as \"John\" and \"boots\". (This definition of dependency differs from other dependency grammars in which the arguments are said to depend on the operators.)\n\nIn each language the dependency relation among words gives rise to syntactic categories in which the allowable arguments of an operator are defined in terms of their dependency requirements. Class N contains words (e.g. \"John\", \"boots\") that do not require the presence of other words. Class O contains the words (e.g. \"sleeps\") that require exactly one word of type N. Class O contains the words (e.g. \"wears\") that require two words of type N. Class O contains the words (e.g. \"because\") that require two words of type O, as in \"John stumbles because John wears boots\". Other classes include O (\"is possible\"), O (\"put\"), O (\"with\", \"surprise\"), O (\"know\"), O (\"ask\") and O (\"attribute\").\n\nThe categories in operator grammar are universal and are defined purely in terms of how words relate to other words, and do not rely on an external set of categories such as noun, verb, adjective, adverb, preposition, conjunction, etc. The dependency properties of each word are observable through usage and therefore learnable.\n\nThe dependency constraint creates a structure (syntax) in which any word of the appropriate class can be an argument for a given operator. The likelihood constraint places additional restrictions on this structure by making some operator/argument combinations more likely than others. Thus, \"John wears hats\" is more likely than \"John wears snow\" which in turn is more likely than \"John wears vacation\". The likelihood constraint creates meaning (semantics) by defining each word in terms of the words it can take as arguments, or of which it can be an argument.\n\nEach word has a unique set of words with which it has been observed to occur called its selection. The coherent selection of a word is the set of words for which the dependency relation has above average likelihood. Words that are similar in meaning have similar coherent selection. This approach to meaning is self-organizing in that no external system is necessary to define what words mean. Instead, the meaning of the word is determined by its usage within a population of speakers. Patterns of frequent use are observable and therefore learnable. New words can be introduced at any time and defined through usage.\n\nIn this sense, link grammar could be viewed as a kind of operator grammar, in that the linkage of words is determined entirely by their context, and that each selection is assigned a log-likelihood.\n\nThe reduction constraint acts on high likelihood combinations of operators and arguments and makes more compact forms. Certain reductions allow words to be omitted completely from an utterance. For example, \"I expect John to come\" is reducible to \"I expect John\", because \"to come\" is highly likely under \"expect\". The sentence \"John wears boots and John wears hats\" can be reduced to \"John wears boots and hats\" because repetition of the first argument \"John\" under the operator \"and\" is highly likely. \"John reads things\" can be reduced to \"John reads\", because the argument \"things\" has high likelihood of occurring under any operator.\n\nCertain reductions reduce words to shorter forms, creating pronouns, suffixes and prefixes (morphology). \"John wears boots and John wears hats\" can be reduced to \"John wears boots and he wears hats\", where the pronoun \"he\" is a reduced form of \"John\". Suffixes and prefixes can be obtained by appending other freely occurring words, or variants of these. \"John is able to be liked\" can be reduced to \"John is likeable\". \"John is thoughtful\" is reduced from \"John is full of thought\", and \"John is anti-war\" from \"John is against war\".\n\nModifiers are the result of several of these kinds of reductions, which give rise to adjectives, adverbs, prepositional phrases, subordinate clauses, etc.\n\n\nEach language has a unique set of reductions. For example, some languages have morphology and some don’t; some transpose short modifiers and some do not. Each word in a language participates only in certain kinds of reductions. However, in each case, the reduced material can be reconstructed from knowledge of what is likely in the given operator/argument combination. The reductions in which each word participates are observable and therefore learnable, just as one learns a word’s dependency and likelihood properties.\n\nThe importance of reductions in operator grammar is that they separate sentences that contain reduced forms from those that don’t (base sentences). All reductions are paraphrases, since they do not remove any information, just make sentences more compact. Thus, the base sentences contain all the information of the language and the reduced sentences are variants of these. Base sentences are made up of simple words without modifiers and largely without affixes, e.g. \"snow falls\", \"sheep eat grass\", \"John knows sheep eat grass\", \"that sheep eat snow surprises John\".\n\nEach operator in a sentence makes a contribution in information according to its likelihood of occurrence with its arguments. Highly expected combinations have low information; rare combinations have high information. The precise contribution of an operator is determined by its selection, the set of words with which it occurs with high frequency. The arguments \"boots\", \"hats\", \"sheep\", \"grass\" and \"snow\" differ in meaning according to the operators for which they can appear with high likelihood in first or second argument position. For example, \"snow\" is expected as first argument of \"fall\" but not of \"eat\", while the reverse is true of \"sheep\". Similarly, the operators \"eat\", \"devour\", \"chew\" and \"swallow\" differ in meaning to the extent that the arguments they select and the operators that select them differ.\n\nOperator grammar predicts that the information carried by a sentence is the accumulation of contributions of each argument and operator. The increment of information that a given word adds to a new sentence is determined by how it was used before. In turn, new usages stretch or even alter the information content associated with a word. Because this process is based on high frequency usage, the meanings of words are relatively stable over time, but can change in accordance with the needs of a linguistic community.\n\n"}
{"id": "421085", "url": "https://en.wikipedia.org/wiki?curid=421085", "title": "Paraconsistent logic", "text": "Paraconsistent logic\n\nA paraconsistent logic is a logical system that attempts to deal with contradictions in a discriminating way. Alternatively, paraconsistent logic is the subfield of logic that is concerned with studying and developing paraconsistent (or \"inconsistency-tolerant\") systems of logic.\n\nInconsistency-tolerant logics have been discussed since at least 1910 (and arguably much earlier, for example in the writings of Aristotle); however, the term \"paraconsistent\" (\"beside the consistent\") was not coined until 1976, by the Peruvian philosopher Francisco Miró Quesada.\n\nIn classical logic (as well as intuitionistic logic and most other logics), contradictions entail everything. This curious feature, known as the principle of explosion or \"ex contradictione sequitur quodlibet\" (Latin, \"from a contradiction, anything follows\") can be expressed formally as\n\nWhich means: if \"P\" and its negation ¬\"P\" are both assumed to be true, then \"P\" is assumed to be true, from which it follows that at least one of the claims \"P\" and some other (arbitrary) claim \"A\" is true. However, if we know that either \"P\" or \"A\" is true, and also that \"P\" is not true (that ¬\"P\" is true) we can conclude that \"A\", which could be anything, is true. Thus if a theory contains a single inconsistency, it is trivial—that is, it has every sentence as a theorem.\n\nThe characteristic or defining feature of a paraconsistent logic is that it rejects the principle of explosion. As a result, paraconsistent logics, unlike classical and other logics, can be used to formalize inconsistent but non-trivial theories.\n\nParaconsistent logics are propositionally \"weaker\" than classical logic; that is, they deem \"fewer\" propositional inferences valid. The point is that a paraconsistent logic can never be a propositional extension of classical logic, that is, propositionally validate everything that classical logic does. In some sense, then, paraconsistent logic is more conservative or cautious than classical logic. It is due to such conservativeness that paraconsistent languages can be more \"expressive\" than their classical counterparts including the hierarchy of metalanguages due to Alfred Tarski et al. According to Solomon Feferman [1984]: \"…natural language abounds with directly or indirectly self-referential yet apparently harmless expressions—all of which are excluded from the Tarskian framework.\" This expressive limitation can be overcome in paraconsistent logic.\n\nA primary motivation for paraconsistent logic is the conviction that it ought to be possible to reason with inconsistent information in a controlled and discriminating way. The principle of explosion precludes this, and so must be abandoned. In non-paraconsistent logics, there is only one inconsistent theory: the trivial theory that has every sentence as a theorem. Paraconsistent logic makes it possible to distinguish between inconsistent theories and to reason with them.\n\nResearch into paraconsistent logic has also led to the establishment of the philosophical school of dialetheism (most notably advocated by Graham Priest), which asserts that true contradictions exist in reality, for example groups of people holding opposing views on various moral issues. Being a dialetheist rationally commits one to some form of paraconsistent logic, on pain of otherwise embracing trivialism, i.e. accepting that all contradictions (and equivalently all statements) are true. However, the study of paraconsistent logics does not necessarily entail a dialetheist viewpoint. For example, one need not commit to either the existence of true theories or true contradictions, but would rather prefer a weaker standard like empirical adequacy, as proposed by Bas van Fraassen.\n\nIn classical logic Aristotle's three laws, namely, the excluded middle (\"p\" or ¬\"p\"), non-contradiction ¬ (\"p\" ∧ ¬\"p\") and identity (\"p\" iff \"p\"), are regarded as the same, due to the inter-definition of the connectives. Moreover, traditionally contradictoriness (the presence of contradictions in a theory or in a body of knowledge) and triviality (the fact that such a theory entails all possible consequences) are assumed inseparable, granted that negation is available. These views may be philosophically challenged, precisely on the grounds that they fail to distinguish between contradictoriness and other forms of inconsistency.\n\nOn the other hand, it is possible to derive triviality from the 'conflict' between consistency and contradictions, once these notions have been properly distinguished. The very notions of consistency and inconsistency may be furthermore internalized at the object language level.\n\nParaconsistency involves tradeoffs. In particular, abandoning the principle of explosion requires one to abandon at least one of the following two principles:\nBoth of these principles have been challenged.\n\nOne approach is to reject disjunction introduction but keep disjunctive syllogism and transitivity. In this approach, rules of natural deduction hold, except for disjunction introduction and excluded middle; moreover, inference A⊢B does not necessarily mean entailment A⇒B. Also, the following usual Boolean properties hold: double negation as well as associativity, commutativity, distributivity, De Morgan, and idempotence inferences (for conjunction and disjunction). Furthermore, inconsistency-robust proof by contradiction holds for entailment (A⇒(B∧¬B))⊢¬A.\n\nAnother approach is to reject disjunctive syllogism. From the perspective of dialetheism, it makes perfect sense that disjunctive syllogism should fail. The idea behind this syllogism is that, if \"¬ A\", then \"A\" is excluded and \"B\" can be inferred from \"A ∨ B\". However, if \"A\" may hold as well as \"¬A\", then the argument for the inference is weakened.\n\nYet another approach is to do both simultaneously. In many systems of relevant logic, as well as linear logic, there are two separate disjunctive connectives. One allows disjunction introduction, and one allows disjunctive syllogism. Of course, this has the disadvantages entailed by separate disjunctive connectives including confusion between them and complexity in relating them.\n\nFurthermore, the rule of proof by contradiction (below) just by itself is inconsistency non-robust in the sense that the negation of every proposition can be proved from a contradiction.\nStrictly speaking, having just the rule above is paraconsistent because it is not the case that \"every\" proposition can be proved from a contradiction. However, if the rule double negation elimination (formula_1) is added as well, then every proposition can be proved from a contradiction. Double negation elimination does not hold for intuitionistic logic.\n\nOne well-known system of paraconsistent logic is the simple system known as LP (\"Logic of Paradox\"), first proposed by the Argentinian logician F. G. Asenjo in 1966 and later popularized by Priest and others.\n\nOne way of presenting the semantics for LP is to replace the usual functional valuation with a relational one. The binary relation formula_2 relates a formula to a truth value: formula_3 means that formula_4 is true, and formula_5 means that formula_4 is false. A formula must be assigned \"at least\" one truth value, but there is no requirement that it be assigned \"at most\" one truth value. The semantic clauses for negation and disjunction are given as follows:\n(The other logical connectives are defined in terms of negation and disjunction as usual.)\nOr to put the same point less symbolically:\n(Semantic) logical consequence is then defined as truth-preservation:\nNow consider a valuation formula_2 such that formula_3 and formula_5 but it is not the case that formula_17. It is easy to check that this valuation constitutes a counterexample to both explosion and disjunctive syllogism. However, it is also a counterexample to modus ponens for the material conditional of LP. For this reason, proponents of LP usually advocate expanding the system to include a stronger conditional connective that is not definable in terms of negation and disjunction.\n\nAs one can verify, LP preserves most other inference patterns that one would expect to be valid, such as De Morgan's laws and the usual introduction and elimination rules for negation, conjunction, and disjunction. Surprisingly, the logical truths (or tautologies) of LP are precisely those of classical propositional logic. (LP and classical logic differ only in the \"inferences\" they deem valid.) Relaxing the requirement that every formula be either true or false yields the weaker paraconsistent logic commonly known as first-degree entailment (FDE). Unlike LP, FDE contains no logical truths.\n\nIt must be emphasized that LP is but one of \"many\" paraconsistent logics that have been proposed. It is presented here merely as an illustration of how a paraconsistent logic can work.\n\nOne important type of paraconsistent logic is relevance logic. A logic is \"relevant\" iff it satisfies the following condition:\n\nIt follows that a relevance logic cannot have (\"p\" ∧ ¬\"p\") → \"q\" as a theorem, and thus (on reasonable assumptions) cannot validate the inference from {\"p\", ¬\"p\"} to \"q\".\n\nParaconsistent logic has significant overlap with many-valued logic; however, not all paraconsistent logics are many-valued (and, of course, not all many-valued logics are paraconsistent). Dialetheic logics, which are also many-valued, are paraconsistent, but the converse does not hold.\n\nIntuitionistic logic allows \"A\" ∨ ¬\"A\" not to be equivalent to true, while paraconsistent logic allows \"A\" ∧ ¬\"A\" not to be equivalent to false. Thus it seems natural to regard paraconsistent logic as the \"dual\" of intuitionistic logic. However, intuitionistic logic is a specific logical system whereas paraconsistent logic encompasses a large class of systems. Accordingly, the dual notion to paraconsistency is called paracompleteness, and the \"dual\" of intuitionistic logic (a specific paracomplete logic) is a specific paraconsistent system called \"anti-intuitionistic\" or \"dual-intuitionistic logic\" (sometimes referred to as \"Brazilian logic\", for historical reasons). The duality between the two systems is best seen within a sequent calculus framework. While in intuitionistic logic the sequent\n\nis not derivable, in dual-intuitionistic logic\n\nis not derivable. Similarly, in intuitionistic logic the sequent\n\nis not derivable, while in dual-intuitionistic logic\n\nis not derivable. Dual-intuitionistic logic contains a connective # known as \"pseudo-difference\" which is the dual of intuitionistic implication. Very loosely, can be read as \"\"A\" but not \"B\". However, # is not truth-functional as one might expect a 'but not' operator to be; similarly, the intuitionistic implication operator cannot be treated like \". Dual-intuitionistic logic also features a basic connective ⊤ which is the dual of intuitionistic ⊥: negation may be defined as \n\nA full account of the duality between paraconsistent and intuitionistic logic, including an explanation on why dual-intuitionistic and paraconsistent logics do not coincide, can be found in Brunner and Carnielli (2005).\n\nThese other logics avoid explosion: implicational propositional calculus, minimal logic, positive propositional calculus, and equivalential calculus. Minimal logic is both paraconsistent and paracomplete (a subsystem of intuitionistic logic). The other three simply do not allow one to express a contradiction to begin with since they lack the ability to form negations.\n\nHere is an example of a three-valued logic which is paraconsistent and \"ideal\" as defined in \"Ideal Paraconsistent Logics\" by O. Arieli, A. Avron, and A. Zamansky, especially pages 22-23. The three truth-values are: \"t\" (true only), \"b\" (both true and false), and \"f\" (false only).\n\nA formula is true if its truth-value is either \"t\" or \"b\" for the valuation being used. A formula is a tautology of paraconsistent logic if it is true in every valuation which maps atomic propositions to {\"t\", \"b\", \"f\"}. Every tautology of paraconsistent logic is also a tautology of classical logic. For a valuation, the set of true formulas is closed under modus ponens and the deduction theorem. Any tautology of classical logic which contains no negations is also a tautology of paraconsistent logic (by merging \"b\" into \"t\"). This logic is sometimes referred to as \"Pac\" or \"LFI1\".\n\nSome tautologies of paraconsistent logic are:\n\nSome tautologies of classical logic which are \"not\" tautologies of paraconsistent logic are:\n\nSuppose we are faced with a contradictory set of premises Γ and wish to avoid being reduced to triviality. In classical logic, the only method one can use is to reject one or more of the premises in Γ. In paraconsistent logic, we may try to compartmentalize the contradiction. That is, weaken the logic so that Γ→\"X\" is no longer a tautology provided the propositional variable \"X\" does not appear in Γ. However, we do not want to weaken the logic any more than is necessary for that purpose. So we wish to retain modus ponens and the deduction theorem as well as the axioms which are the introduction and elimination rules for the logical connectives (where possible).\n\nTo this end, we add a third truth-value \"b\" which will be employed within the compartment containing the contradiction. We make \"b\" a fixed point of all the logical connectives.\n\nWe must make \"b\" a kind of truth (in addition to \"t\") because otherwise there would be no tautologies at all.\n\nTo ensure that modus ponens works, we must have\nthat is, to ensure that a true hypothesis and a true implication lead to a true conclusion, we must have that a not-true (\"f\") conclusion and a true (\"t\" or \"b\") hypothesis yield a not-true implication.\n\nIf all the propositional variables in Γ are assigned the value \"b\", then Γ itself will have the value \"b\". If we give \"X\" the value \"f\", then\nSo Γ→\"X\" will not be a tautology.\n\nLimitations:\n(1) There must not be constants for the truth values because that would defeat the purpose of paraconsistent logic. Having \"b\" would change the language from that of classical logic. Having \"t\" or \"f\" would allow the explosion again because\nwould be tautologies. Note that \"b\" is not a fixed point of those constants since \"b\" ≠ \"t\" and \"b\" ≠ \"f\".\n\n(2) This logic's ability to contain contradictions applies only to contradictions among particularized premises, not to contradictions among axiom schemas.\n\n(3) The loss of disjunctive syllogism may result in insufficient commitment to developing the 'correct' alternative, possibly crippling mathematics.\n\n(4) To establish that a formula Γ is equivalent to Δ in the sense that either can be substituted for the other wherever they appear as a subformula, one must show\nThis is more difficult than in classical logic because the contrapositives do not necessarily follow.\n\nParaconsistent logic has been applied as a means of managing inconsistency in numerous domains, including:\n\nSome philosophers have argued against dialetheism on the grounds that the counterintuitiveness of giving up any of the three principles above outweighs any counterintuitiveness that the principle of explosion might have.\n\nOthers, such as David Lewis, have objected to paraconsistent logic on the ground that it is simply impossible for a statement and its negation to be jointly true. A related objection is that \"negation\" in paraconsistent logic is not really \"negation\"; it is merely a subcontrary-forming operator.\n\nApproaches exist that allow for resolution of inconsistent beliefs without violating any of the intuitive logical principles. Most such systems use multi-valued logic with Bayesian inference and the Dempster-Shafer theory, allowing that no non-tautological belief is completely (100%) irrefutable because it must be based upon incomplete, abstracted, interpreted, likely unconfirmed, potentially uninformed, and possibly incorrect knowledge (of course, this very assumption, if non-tautological, entails its own refutability, if by \"refutable\" we mean \"not completely [100%] irrefutable\"). These systems effectively give up several logical principles in practice without rejecting them in theory.\n\nNotable figures in the history and/or modern development of paraconsistent logic include:\n\n\n\n\n"}
{"id": "23864280", "url": "https://en.wikipedia.org/wiki?curid=23864280", "title": "Parity learning", "text": "Parity learning\n\nParity learning is a problem in machine learning. An algorithm that solves this problem must find a function \"ƒ\", given some samples (\"x\", \"ƒ\"(\"x\")) and the assurance that \"ƒ\" computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.\n\nIn Learning Parity with Noise (LPN), the samples may contain some error. Instead of samples (\"x\", \"ƒ\"(\"x\")), the algorithm is provided with (\"x\", \"y\"), where \"y\" = 1 − \"ƒ\"(\"x\") with some small probability. The noisy version of the parity learning problem is conjectured to be hard. \n\n\n"}
{"id": "12928899", "url": "https://en.wikipedia.org/wiki?curid=12928899", "title": "Pollard's kangaroo algorithm", "text": "Pollard's kangaroo algorithm\n\nIn computational number theory and computational algebra, Pollard's kangaroo algorithm (also Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper as his better-known ρ algorithm for solving the same problem. Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime \"p\", it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group.\n\nSuppose formula_1 is a finite cyclic group of order formula_2 which is generated by the element formula_3, and we seek to find the discrete logarithm formula_4 of the element formula_5 to the base formula_3. In other words, we seek formula_7 such that formula_8. The lambda algorithm allows us to search for formula_4 in some subset formula_10. We may search the entire range of possible logarithms by setting formula_11 and formula_12, although in this case Pollard's rho algorithm for logarithms is more efficient. We proceed as follows:\n\n1. Choose a set formula_13 of integers and define a pseudorandom map formula_14.\n\n2. Choose an integer formula_15 and compute a sequence of group elements formula_16 according to:\n3. Compute\nObserve that:\n4. Begin computing a second sequence of group elements formula_21 according to:\nand a corresponding sequence of integers formula_24 according to:\nObserve that:\n5. Stop computing terms of formula_27 and formula_28 when either of the following conditions are met:\n\nPollard gives the time complexity of the algorithm as formula_38, based on a probabilistic argument which follows from the assumption that \"f\" acts pseudorandomly. Note that when the size of the set {\"a\", …, \"b\"} to be searched is measured in bits, as is normal in complexity theory, the set has size log(\"b\" − \"a\"), and so the algorithm's complexity is formula_39, which is exponential in the problem size. For this reason, Pollard's lambda algorithm is considered an exponential time algorithm. For an example of a subexponential time discrete logarithm algorithm, see the index calculus algorithm.\n\nThe algorithm is well known by two names.\n\nThe first is \"Pollard's kangaroo algorithm\". This name is a reference to an analogy used in the paper presenting the algorithm, where the algorithm is explained in terms of using a \"tame\" kangaroo to trap a \"wild\" kangaroo. Pollard has explained that this analogy was inspired by a \"fascinating\" article published in the same issue of \"Scientific American\" as an exposition of the RSA public key cryptosystem. The article described an experiment in which a kangaroo's \"energetic cost of locomotion, measured in terms of oxygen consumption at various speeds, was determined by placing kangaroos on a treadmill\".\n\nThe second is \"Pollard's lambda algorithm\". Much like the name of another of Pollard's discrete logarithm algorithms, Pollard's rho algorithm, this name refers to the similarity between a visualisation of the algorithm and the Greek letter lambda (formula_40). The shorter stroke of the letter lambda corresponds to the sequence formula_31, since it starts from the position b to the right of x. Accordingly, the longer stroke corresponds to the sequence formula_27, which \"collides with\" the first sequence (just like the strokes of a lambda intersect) and then follows it subsequently.\n\nPollard has expressed a preference for the name \"kangaroo algorithm\", as this avoids confusion with some parallel versions of his rho algorithm, which have also been called \"lambda algorithms\".\n\n"}
{"id": "1156819", "url": "https://en.wikipedia.org/wiki?curid=1156819", "title": "Population ecology", "text": "Population ecology\n\nThe development of population ecology owes much to demography and actuarial life tables. Population ecology is important in conservation biology, especially in the development of population viability analysis (PVA) which makes it possible to predict the long-term probability of a species persisting in a given habitat patch. Although population ecology is a subfield of biology, it provides interesting problems for mathematicians and statisticians who work in population dynamics.\n\nThe most fundamental law of population ecology is Thomas Malthus' exponential law of population growth.\n\n\"A population will grow (or decline) exponentially as long as the environment experienced by all individuals in the population remains constant.\"\n\nThis principle in population ecology provides the basis for formulating predictive theories and tests that follow:\n\nSimplified population models usually start with four key variables (four demographic processes) including death, birth, immigration, and emigration. Mathematical models used to calculate changes in population demographics and evolution hold the assumption (or null hypothesis) of no external influence. Models can be more mathematically complex where \"...several competing hypotheses are simultaneously confronted with the data.\" For example, in a closed system where immigration and emigration does not take place, the rate of change in the number of individuals in a population can be described as:\n\nwhere \"N\" is the total number of individuals in the population, \"B\" is the raw number of births, \"D\" is the raw number of deaths, \"b\" and \"d\" are the per capita rates of birth and death respectively, and \"r\" is the per capita average number of surviving offspring each individual has. This formula can be read as the rate of change in the population (\"dN/dT\") is equal to births minus deaths (B - D).\n\nUsing these techniques, Malthus' population principle of growth was later transformed into a mathematical model known as the :\n\nwhere \"N\" is the biomass density, \"a\" is the maximum per-capita rate of change, and \"K\" is the carrying capacity of the population. The formula can be read as follows: the rate of change in the population (\"dN/dT\") is equal to growth (\"aN\") that is limited by carrying capacity \"(1-N/K)\". From these basic mathematical principles the discipline of population ecology expands into a field of investigation that queries the demographics of real populations and tests these results against the statistical models. The field of population ecology often uses data on life history and matrix algebra to develop projection matrices on fecundity and survivorship. This information is used for managing wildlife stocks and setting harvest quotas \n\nThe population model below can be manipulated to mathematically infer certain properties of geometric populations. A population with a size that increases geometrically is a population where generations of reproduction do not overlap. In each generation there is an effective population size denoted as N which constitutes the number of individuals in the population that are able to reproduce \"and\" will reproduce in any reproductive generation in concern. In the population model below it is assumed that N is the effective population size.\n\nAssumption 01: N = N\n\nN = N + B + I - D - E\n\nAssumption 02: There is no migration to or from the population (N)\n\nI = E = 0\n\nN = N + B - D\n\nThe raw birth and death rates are related to the per capita birth and death rates:\n\nB = b × N\n\nD = d × N\n\nb = B / N\n\nd = D / N\n\nTherefore:\n\nN = N + (b × N) - (d × N)\n\nAssumption 03: b and d are constant (i.e. they don't change each generation).\n\nN = N + (bN) - (dN)\n\nTake the term N out of the brackets.\n\nN = N + (b - d)N\n\nb - d = R\n\nN = N + RN\n\nN = (N + RN)\n\nTake the term N out of the brackets again.\n\nN = (1 + R)N\n\n1 + R = λ\n\nN = λN\n\nTherefore:\n\nN = λN\n\nThe doubling time of a population is the time required for the population to grow to twice its size. We can calculate the doubling time of a geometric population using the equation: N = λN by exploiting our knowledge of the fact that the population (N) is twice its size (2N) after the doubling time.\n\n2N = λ × N\n\nλ = 2N / N\n\nλ = 2\n\nThe doubling time can be found by taking logarithms. For instance:\n\nt × log(λ) = log(2)\n\nlog(2) = 1\n\nt × log(λ) = 1\n\nt = 1 / log(λ)\n\nOr:\n\nt × ln(λ) = ln(2)\n\nt = ln(2) / ln(λ)\n\nt = 0.693... / ln(λ)\n\nTherefore:\n\nt = 1 / log(λ) = 0.693... / ln(λ) \n\nThe half-life of a population is the time taken for the population to decline to half its size. We can calculate the half-life of a geometric population using the equation: N = λN by exploiting our knowledge of the fact that the population (N) is half its size (0.5N) after a half-life.\n\n0.5N = λ × N\n\nλ = 0.5N / N\n\nλ = 0.5\n\nThe half-life can be calculated by taking logarithms (see above).\n\nt = 1 / log(λ) = ln(0.5) / ln(λ) \n\nR = b - d\n\nN = N + RN\n\nN - N = RN\n\nN - N = ΔN\n\nΔN = RN\n\nΔN/N = R\n\n1 + R = λ\n\nN = λN\n\nλ = N / N\n\nIn geometric populations, R and λ represent growth constants (see 2 and 2.3). In exponential populations however, the intrinsic growth rate, also known as intrinsic rate of increase (r) is the relevant growth constant. Since generations of reproduction in a geometric population do not overlap (e.g. reproduce once a year) but do in an exponential population, geometric and exponential populations are usually considered to be mutually exclusive. However, geometric constants and exponential constants share the mathematical relationship below.\n\nThe growth equation for exponential populations is\n\nN = Ne\n\nAssumption: N \"(of a geometric population)\" = N \"(of an exponential population)\".\n\nTherefore:\n\nNe = Nλ\n\nN cancels on both sides.\n\nNe / N = λ\n\ne = λ\n\nTake the natural logarithms of the equation. Using natural logarithms instead of base 10 or base 2 logarithms simplifies the final equation as ln(e) = 1.\nrt × ln(e) = t × ln(λ)\n\nrt × 1 = t × ln(λ)\n\nrt = t × ln(λ)\n\nt cancels on both sides.\n\nrt / t = ln(λ)\n\nThe results:\n\nr = ln(λ)\n\nand\n\ne = λ\n\nAn important concept in population ecology is the r/K selection theory. The first variable is \"r\" (the intrinsic rate of natural increase in population size, density independent) and the second variable is \"K\" (the carrying capacity of a population, density dependent).\nAn \"r\"-selected species (e.g., many kinds of insects, such as aphids) is one that has high rates of fecundity, low levels of parental investment in the young, and high rates of mortality before individuals reach maturity. Evolution favors productivity in r-selected species. In contrast, a \"K\"-selected species (such as humans) has low rates of fecundity, high levels of parental investment in the young, and low rates of mortality as individuals mature. Evolution in \"K\"-selected species favors efficiency in the conversion of more resources into fewer offspring.\n\nPopulations are also studied and conceptualized through the \"metapopulation\" concept. The metapopulation concept was introduced in 1969: \"as a population of populations which go extinct locally and recolonize.\" Metapopulation ecology is a simplified model of the landscape into patches of varying levels of quality. Patches are either occupied or they are not. Migrants moving among the patches are structured into metapopulations either as sources or sinks. Source patches are productive sites that generate a seasonal supply of migrants to other patch locations. Sink patches are unproductive sites that only receive migrants. In metapopulation terminology there are emigrants (individuals that leave a patch) and immigrants (individuals that move into a patch). Metapopulation models examine patch dynamics over time to answer questions about spatial and demographic ecology. An important concept in metapopulation ecology is the rescue effect, where small patches of lower quality (i.e., sinks) are maintained by a seasonal influx of new immigrants. Metapopulation structure evolves from year to year, where some patches are sinks, such as dry years, and become sources when conditions are more favorable. Ecologists utilize a mixture of computer models and field studies to explain metapopulation structure.\n\nThe older term, autecology (from Greek: αὐτο, \"auto\", \"self\"; οίκος, oikos, \"household\"; and λόγος, logos, \"knowledge\"), refers to roughly the same field of study as population ecology. It derives from the division of ecology into autecology—the study of individual species in relation to the environment—and synecology—the study of groups of organisms in relation to the environment—or community ecology. Odum (1959, p. 8) considered that synecology should be divided into population ecology, community ecology, and ecosystem ecology, defining autecology as essentially \"species ecology.\" However, for some time biologists have recognized that the more significant level of organization of a species is a population, because at this level the species gene pool is most coherent. In fact, Odum regarded \"autecology\" as no longer a \"present tendency\" in ecology (i.e., an archaic term), although included \"species ecology\"—studies emphasizing life history and behavior as adaptations to the environment of individual organisms or species—as one of four subdivisions of ecology.\n\nThe first journal publication of the Society of Population Ecology, titled \"Population Ecology\" (originally called \"Researches on Population Ecology\") was released in 1952.\n\nScientific articles on population ecology can also be found in the \"Journal of Animal Ecology\", \"Oikos\" and other journals.\n\n\n"}
{"id": "44946754", "url": "https://en.wikipedia.org/wiki?curid=44946754", "title": "Pratibandhaka", "text": "Pratibandhaka\n\nPratibandhaka (Sanskrit:प्रतिबन्धक) variously means – 'opposition', 'resistance', 'investment', 'blockade', 'siege', 'invariable and inseparable connection', 'cessation', 'disappointment'; it also means – 'impediment', 'obstacle', 'cognitive blocker', 'antidote' or 'preventive measure'. \"Pratibandhaka\" is a causal dependency and refers to something that must perform the specific function of obstructing. \n\nPrabhachandra, the disciple of Akalarika, endorses the Jaina argument that the causal complex has the causal power of producing effect owing to the existence of some extra-sensory power (\" shakti \"), that the hindrance in the origination of an effect is caused by counter-agents (\"pratibandhaka\") is due to the special power. The followers of Jainism believe that the universe is full of karmic molecules whose mundane inflow towards the soul is caused by its own vibratory activities through mind, speech and body. Karma is the invisible power that explains causality, and the matter that binds the soul as a result of actions. But the notion of some power connected with causation is rejected by the Nyaya school which school concludes that a cognitive state is perceptible only when all causal conditions are present, \"abhava\" is perceived only in constructive cognitive state. The Mimamsakas hold the view the power connected with causation can be destroyed by the presence of an antidote (\"pratibandhaka\") and can be resuscitated by an antidote to the antidote. \n\nPervasion (\"vyapti\") is the logical ground for inference which is a valid means of knowledge, and guarantees the truth of conclusion. It is the unconditional and constant concomitant relationship between the pervaded and the pervade. Any person desiring emancipation (a \"mumuksu\") cannot gain liberation (\"moksha\") without surmounting the obstacles (\"pratibandhakas\") related to the connection with the body in the form of powerful and wicked actions or sinful deeds (\"pāpa\"). The physical body (\"prakṛti\"), by itself, is an obstacle to the union with the Supreme Being for it has within it imprisoned the self (\"ātman\"). \n\nGangesa, the author of \"Tattvacintāmaṇi\" who had examined the possibility of dialectical reasoning as a way to grasp pervasion, in the \"anumāna-khanda\" of the same text states that pervasion is pursued so long as there is doubt because there is contradiction but does not require deviation; doubt is an invalid cognitive act and fallacious reasoning is the ground for contradiction, the nature of doubt and fallacious reasoning both being conceptual is not of determinate character . Thus, dialectical reasoning is blocking of the opposing view and continues so long as doubt persists. Gangesa agrees that since pervasion is a universal invariant concomitance, therefore, the possibility of a counter example cannot be ruled out, and concludes that contradiction as natural opposition cannot block an infinite regress, it is the doubter’s own behaviour proving the lie to the doubt that blocks it acting as the \"pratibandhaka\". Gangesa uses the term, \"pratibandhaka\", to refer to a natural opposition in cognitive logic, as a preventer. Pervasion by its absence is the cause of hindrance (\"pratibandhaka\") to inferential knowledge which in turn is the cause of pervasion. Even though both grasp their objects directly, in \"savikalpa\" its contents become objects of reflective awareness which is not the case in \"nirvikalpa\". Contradiction occurs only when one epistemic state is blocked by a pratibandhaka.\n\nGangesa has defined inferential knowledge as the cognition generated by cognition of a property belonging to a locus and qualified by a pervasion. A seed remaining intact does not sprout, and the destruction of the seed is a condition for the sprout to arise, the former is the obstacle (\"pratibandhaka\") to sprout, and the latter indicates the absence of such obstacle.\n\nAccording to Advaita Vedanta, obstruction (\"pratibandhaka\") of superimposition (\"adhyasa\") is true knowledge, the absence of which obstruction is lack of true knowledge or ignorance (\"ajñāna\" or \"avidyā\"). \"Pratibandhaka\" is that obstacle which prevents the production of an effect in causal conditions. According to the Nyaya school an effect is the counter-entity of its own prior non-existence and a fresh beginning. Swami Vidyaranya lists four such obstacles or impediments, which are:-\n\na) binding attachment to the objects of the senses, b) dullness of the intellect, c) indulgence in improper and illogical arguments and d) the deep conviction that the Self is an agent and an enjoyer (Panchadasi IX.43). He explains:-\n\nthat through the practice of inner control and other qualifications and through hearing the truth and so forth, suitable for counter-acting the impediments, the latter slowly perish, and one realizes his Self as Brahman (Panchadasi IX.44). The Yogi enters into the heaven of meritorious because of his practice of enquiry provided his enquiry is not impeded on account of the results of past evil deeds and his strong desire for Brahmaloka is not suppressed by him. \nIn Ayurveda, the word \"pratibandhaka\" as \"vyādhyutpāda pratibandhaka\" refers to the power of the body to resist disease which power (\"vyādhi virodhaka\") when weakened can be restored by means of administering \"triphala\" fruits and the likes as an herbal rasayana.\n"}
{"id": "591394", "url": "https://en.wikipedia.org/wiki?curid=591394", "title": "Principle of explosion", "text": "Principle of explosion\n\nThe principle of explosion (Latin: \"ex falso (sequitur) quodlibet\" (EFQ), \"from falsehood, anything (follows)\", or \"ex contradictione (sequitur) quodlibet\" (ECQ), \"from contradiction, anything (follows)\"), or the principle of Pseudo-Scotus, is the law of classical logic, intuitionistic logic and similar logical systems, according to which any statement can be proven from a contradiction. That is, once a contradiction has been asserted, any proposition (including their negations) can be inferred from it. This is known as deductive explosion. The proof of this principle was first given by 12th century French philosopher William of Soissons.\n\nAs a demonstration of the principle, consider two contradictory statements – \"All lemons are yellow\" and \"Not all lemons are yellow\", and suppose (for the sake of argument) that both are simultaneously true. If that is the case, anything can be proven, e.g. \"unicorns exist\", by using the following argument:\n\nDue to the principle of explosion, the existence of a contradiction (inconsistency) in a formal axiomatic system is disastrous; since any statement can be proved true it trivializes the concepts of truth and falsity. Around the turn of the 20th century, the discovery of contradictions such as Russell's paradox at the foundations of mathematics thus threatened the entire structure of mathematics. Mathematicians such as Gottlob Frege, Ernst Zermelo, Abraham Fraenkel, and Thoralf Skolem put much effort into revising set theory to eliminate these contradictions, resulting in the modern Zermelo–Fraenkel set theory. \n\nIn a different solution to these problems, a few mathematicians have devised alternate theories of logic called paraconsistent logics, which eliminate the principle of explosion. These allow some contradictory statements to be proved without affecting other proofs.\n\nIn symbolic logic, the principle of explosion can be expressed in the following way\n\nBelow is a formal proof of the principle using symbolic logic\n\nThis is just the symbolic version of the informal argument given in the introduction, with formula_3 standing for \"all lemons are yellow\" and formula_6 standing for \"Unicorns exist\". From \"all lemons are yellow and not all lemons are yellow\" (1), we infer \"all lemons are yellow\" (2) and \"not all lemons are yellow\" (3); from \"all lemons are yellow\" (2), we infer \"all lemons are yellow or unicorns exist\" (4); and from \"not all lemons are yellow\" (3) and \"all lemons are yellow or unicorns exist\" (4), we infer \"unicorns exist\" (5). Hence, if all lemons are yellow and not all lemons are yellow, then unicorns exist.\n\nAn alternate argument for the principle stems from model theory. A sentence formula_3 is a \"semantic consequence\" of a set of sentences formula_11 only if every model of formula_11 is a model of formula_3. But there is no model of the contradictory set formula_14. A fortiori, there is no model of formula_14 that is not a model of formula_6. Thus, vacuously, every model of formula_14 is a model of formula_6. Thus formula_6 is a semantic consequence of formula_14.\n\nParaconsistent logics have been developed that allow for sub-contrary forming operators. Model-theoretic paraconsistent logicians often deny the assumption that there can be no model of formula_21 and devise semantical systems in which there are such models. Alternatively, they reject the idea that propositions can be classified as true or false. Proof-theoretic paraconsistent logics usually deny the validity of one of the steps necessary for deriving an explosion, typically including disjunctive syllogism, disjunction introduction, and reductio ad absurdum.\n\nThe metamathematical value of the principle of explosion is that for any logical system where this principle holds, any derived theory which proves ⊥ (or an equivalent form, formula_22) is worthless because \"all\" its statements would become theorems, making it impossible to distinguish truth from falsehood. That is to say, the principle of explosion is an argument for the law of non-contradiction in classical logic, because without it all truth statements become meaningless.\n\n"}
{"id": "2724082", "url": "https://en.wikipedia.org/wiki?curid=2724082", "title": "Resolution (logic)", "text": "Resolution (logic)\n\nIn mathematical logic and automated theorem proving, resolution is a rule of inference leading to a refutation theorem-proving technique for sentences in propositional logic and first-order logic. In other words, iteratively applying the resolution rule in a suitable way allows for telling whether a propositional formula is satisfiable and for proving that a first-order formula is unsatisfiable. Attempting to prove a satisfiable first-order formula as unsatisfiable may result in a nonterminating computation; this problem doesn't occur in propositional logic.\n\nThe resolution rule can be traced back to Davis and Putnam (1960); however, their algorithm required trying all ground instances of the given formula. This source of combinatorial explosion was eliminated in 1965 by John Alan Robinson's syntactical unification algorithm, which allowed one to instantiate the formula during the proof \"on demand\" just as far as needed to keep refutation completeness.\n\nThe clause produced by a resolution rule is sometimes called a resolvent.\n\nThe resolution rule in propositional logic is a single valid inference rule that produces a new clause implied by two clauses containing complementary literals. A literal is a propositional variable or the negation of a propositional variable. Two literals are said to be complements if one is the negation of the other (in the following,\nformula_1 is taken to be the complement to formula_2). The resulting clause contains all the literals that do not have complements.\nFormally:\nwhere\n\nThe clause produced by the resolution rule is called the \"resolvent\" of the two input clauses. It is the principle of \"consensus\" applied to clauses rather than terms.\n\nWhen the two clauses contain more than one pair of complementary literals, the resolution rule can be applied (independently) for each such pair; however, the result is always a tautology.\n\nModus ponens can be seen as a special case of resolution (of a one-literal clause and a two-literal clause).\nis equivalent to\n\nWhen coupled with a complete search algorithm, the resolution rule yields a sound and complete algorithm for deciding the \"satisfiability\" of a propositional formula, and, by extension, the validity of a sentence under a set of axioms.\n\nThis resolution technique uses proof by contradiction and is based on the fact that any sentence in propositional logic can be transformed into an equivalent sentence in conjunctive normal form. The steps are as follows.\n\n\nOne instance of this algorithm is the original Davis–Putnam algorithm that was later refined into the DPLL algorithm that removed the need for explicit representation of the resolvents.\n\nThis description of the resolution technique uses a set \"S\" as the underlying data-structure to represent resolution derivations. Lists, Trees and Directed Acyclic Graphs are other possible and common alternatives. Tree representations are more faithful to the fact that the resolution rule is binary. Together with a sequent notation for clauses, a tree representation also makes it clear to see how the resolution rule is related to a special case of the cut-rule, restricted to atomic cut-formulas. However, tree representations are not as compact as set or list representations, because they explicitly show redundant subderivations of clauses that are used more than once in the derivation of the empty clause. Graph representations can be as compact in the number of clauses as list representations and they also store structural information regarding which clauses were resolved to derive each resolvent.\n\nformula_11\n\nIn plain language: Suppose formula_4 is false. In order for the premise formula_13 to be true, formula_5 must be true.\nAlternatively, suppose formula_4 is true. In order for the premise formula_16 to be true, formula_2 must be true. Therefore regardless of falsehood or veracity of formula_4, if both premises hold, then the conclusion formula_19 is true.\n\nResolution rule can be generalized to first-order logic to:\n\nwhere formula_21 is a most general unifier of formula_22 and formula_23 and formula_24 and formula_25 have no common variables.\n\nThe clauses formula_26 and formula_27 can apply this rule with formula_28 as unifier.\n\nHere x is a variable and b is a constant.\n\nHere we see that\n\n\nIn first order logic, resolution condenses the traditional syllogisms of logical inference down to a single rule.\n\nTo understand how resolution works, consider the following example syllogism of term logic:\n\nOr, more generally:\n\nTo recast the reasoning using the resolution technique, first the clauses must be converted to conjunctive normal form (CNF). In this form, all quantification becomes implicit: universal quantifiers on variables (\"X\", \"Y\", ...) are simply omitted as understood, while existentially-quantified variables are replaced by Skolem functions.\n\nSo the question is, how does the resolution technique derive the last clause from the first two? The rule is simple:\n\n\nTo apply this rule to the above example, we find the predicate \"P\" occurs in negated form\n\nin the first clause, and in non-negated form\n\nin the second clause. \"X\" is an unbound variable, while \"a\" is a bound value (term). Unifying the two produces the substitution\n\nDiscarding the unified predicates, and applying this substitution to the remaining predicates (just \"Q\"(\"X\"), in this case), produces the conclusion:\n\nFor another example, consider the syllogistic form\n\nOr more generally,\n\nIn CNF, the antecedents become:\n\nNow, unifying \"Q\"(\"X\") in the first clause with ¬\"Q\"(\"Y\") in the second clause means that \"X\" and \"Y\" become the same variable anyway. Substituting this into the remaining clauses and combining them gives the conclusion:\n\nThe resolution rule, as defined by Robinson, also incorporated factoring, which unifies two literals in the same clause, before or during the application of resolution as defined above. The resulting inference rule is refutation-complete, in that a set of clauses is unsatisfiable if and only if there exists a derivation of the empty clause using resolution alone.\n\nParamodulation is a related technique for reasoning on sets of clauses where the predicate symbol is equality. It generates all \"equal\" versions of clauses, except reflexive identities. The paramodulation operation takes a positive \"from\" clause, which must contain an equality literal. It then searches an \"into\" clause with a subterm that unifies with one side of the equality. The subterm is then replaced by the other side of the equality. The general aim of paramodulation is to reduce the system to atoms, reducing the size of the terms when substituting.\n\n\n\nApproaches to non-clausal resolution, i.e. resolution of first-order formulas that need not be in clausal normal form, are presented in:\n\n"}
{"id": "2003831", "url": "https://en.wikipedia.org/wiki?curid=2003831", "title": "Ruth Moufang", "text": "Ruth Moufang\n\nRuth Moufang (January 10, 1905 – November 26, 1977) was a German mathematician. \n\nBorn to a German chemist Dr. Eduard Moufang and Else Fecht Moufang, she studied mathematics at the University of Frankfurt. In 1931 she received her Ph.D. on projective geometry under the direction of Max Dehn, and in 1932 spent a fellowship year in Rome. After her year in Rome, she returned to Germany to lecture at the University of Königsberg and the University of Frankfurt. Her research in projective geometry built upon the work of David Hilbert. She was responsible for ground-breaking work on non-associative algebraic structures, including the Moufang loops named after her.\n\nIn 1933 Moufang showed Desargues's theorem does not hold in the Cayley plane. The Cayley plane uses octonion coordinates which do not satisfy the associative law. Such connections between geometry and algebra had been previously noted by Karl von Staudt and David Hilbert. Ruth Moufang thus initiated a new branch of geometry called Moufang planes.\n\nDenied permission to teach by the minister of education of Nazi Germany, she worked in private industry until 1946, when she became the first woman professor at the University of Frankfurt.\n\n"}
{"id": "1058218", "url": "https://en.wikipedia.org/wiki?curid=1058218", "title": "Semigroup action", "text": "Semigroup action\n\nIn algebra and theoretical computer science, an action or act of a semigroup on a set is a rule which associates to each element of the semigroup a transformation of the set in such a way that the product of two elements of the semigroup (using the semigroup operation) is associated with the composite of the two corresponding transformations. The terminology conveys the idea that the elements of the semigroup are \"acting\" as transformations of the set. From an algebraic perspective, a semigroup action is a generalization of the notion of a group action in group theory. From the computer science point of view, semigroup actions are closely related to automata: the set models the state of the automaton and the action models transformations of that state in response to inputs.\n\nAn important special case is a monoid action or act, in which the semigroup is a monoid and the identity element of the monoid acts as the identity transformation of a set. From a category theoretic point of view, a monoid is a category with one object, and an act is a functor from that category to the category of sets. This immediately provides a generalization to monoid acts on objects in categories other than the category of sets.\n\nAnother important special case is a transformation semigroup. This is a semigroup of transformations of a set, and hence it has a tautological action on that set. This concept is linked to the more general notion of a semigroup by an analogue of Cayley's theorem.\n\n\"(A note on terminology: the terminology used in this area varies, sometimes significantly, from one author to another. See the article for details.)\"\n\nLet \"S\" be a semigroup. Then a (left) semigroup action (or act) of \"S\" is a set \"X\" together with an operation which is compatible with the semigroup operation * as follows:\nThis is the analogue in semigroup theory of a (left) group action, and is equivalent to a semigroup homomorphism into the set of functions on \"X\". Right semigroup actions are defined in a similar way using an operation satisfying .\n\nIf \"M\" is a monoid, then a (left) monoid action (or act) of \"M\" is a (left) semigroup action of \"M\" with the additional property that\nwhere \"e\" is the identity element of \"M\". This correspondingly gives a monoid homomorphism. Right monoid actions are defined in a similar way. A monoid \"M\" with an action on a set is also called an operator monoid.\n\nA semigroup action of \"S\" on \"X\" can be made into monoid act by adjoining an identity to the semigroup and requiring that it acts as the identity transformation on \"X\". \n\nIf \"S\" is a semigroup or monoid, then a set \"X\" on which \"S\" acts as above (on the left, say) is also known as a (left) S\"-act, S\"-set, S\"-action, S\"-operand, or left act over \"S\". Some authors do not distinguish between semigroup and monoid actions, by regarding the identity axiom () as empty when there is no identity element, or by using the term unitary \"S\"-act for an \"S\"-act with an identity. Furthermore, since a monoid is a semigroup, one can consider semigroup actions of monoids.\n\nThe defining property of an act is analogous to the associativity of the semigroup operation, and means that all parentheses can be omitted. It is common practice, especially in computer science, to omit the operations as well so that both the semigroup operation and the action are indicated by juxtaposition. In this way strings of letters from \"S\" act on \"X\", as in the expression \"stx\" for \"s\", \"t\" in \"S\" and \"x\" in \"X\".\n\nIt is also quite common to work with right acts rather than left acts. However, every right S-act can be interpreted as a left act over the opposite semigroup, which has the same elements as S, but where multiplication is defined by reversing the factors, , so the two notions are essentially equivalent. Here we primarily adopt the point of view of left acts.\n\nIt is often convenient (for instance if there is more than one act under consideration) to use a letter, such as formula_1, to denote the function\ndefining the formula_3-action and hence write formula_4 in place of formula_5. Then for any formula_6 in formula_3, we denote by\nthe transformation of formula_9 defined by\nBy the defining property of an formula_3-act, formula_1 satisfies\nFurther, consider a function formula_14. It is the same as formula_15 (see currying). Because formula_16 is a bijection, semigroup actions can be defined as functions formula_17 which satisfies\nI.e. formula_1 is a semigroup action of formula_3 on formula_9 iff formula_22 is a semigroup homomorphism from formula_3 to the full transformation monoid of formula_9.\n\nLet \"X\" and \"X\"′ be \"S\"-acts. Then an \"S\"-homomorphism from \"X\" to \"X\"′ is a map\nsuch that \nThe set of all such \"S\"-homomorphisms is commonly written as formula_29.\n\n\"M\"-homomorphisms of \"M\"-acts, for \"M\" a monoid, are defined in exactly the same way.\n\nFor a fixed semigroup \"S\", the left \"S\"-acts are the objects of a category, denoted \"S\"-Act, whose morphisms are the \"S\"-homomorphisms. The corresponding category of right \"S\"-acts is sometimes denoted by Act-\"S\". (This is analogous to the categories \"R\"-Mod and Mod-\"R\" of left and right modules over a ring.)\n\nFor a monoid \"M\", the categories \"M\"-Act and Act-\"M\" are defined in the same way.\n\n\nA correspondence between transformation semigroups and semigroup actions is described below. If we restrict it to faithful semigroup actions, it has nice properties.\n\nAny transformation semigroup can be turned into a semigroup action by the following construction. For any transformation semigroup formula_3 of formula_9, define a semigroup action formula_1 of formula_3 on formula_9 as formula_52 for formula_53. This action is faithful, which is equivalent to formula_22 being injective.\n\nConversely, for any semigroup action formula_1 of formula_3 on formula_9, define a transformation semigroup formula_58. In this construction we \"forget\" the set formula_3. formula_60 is equal to the image of formula_22. Lets denote formula_22 as formula_63 for brevity. If formula_22 is injective, then formula_63 is a semigroup isomorphism from formula_3 to formula_60. In other words, if formula_1 is faithful, then we forget nothing important. This claim is made precise by the following observation: if we turn formula_60 back into a semigroup action formula_70 of formula_60 on formula_9, then formula_73 for all formula_53. formula_1 and formula_70 are \"isomorphic\" via formula_63, i.e., we essentially recovered formula_1. Thus some authors see no distinction between faithful semigroup actions and transformation semigroups.\n\nTransformation semigroups are of essential importance for the structure theory of finite state machines in automata theory. In particular, a \"semiautomaton\" is a triple (\"Σ\",\"X\",\"T\"), where \"Σ\" is a non-empty set called the \"input alphabet\", \"X\" is a non-empty set called the \"set of states\" and \"T\" is a function\ncalled the \"transition function\". Semiautomata arise from deterministic automata by ignoring the initial state and the set of accept states.\n\nGiven a semiautomaton, let \"T\": \"X\" → \"X\", for \"a\" ∈ \"Σ\", denote the transformation of \"X\" defined by \"T\"(\"x\") = \"T\"(\"a\",\"x\"). Then the semigroup of transformations of \"X\" generated by {\"T\" : \"a\" ∈ \"Σ\"} is called the \"characteristic semigroup\" or \"transition system\" of (\"Σ\",\"X\",\"T\"). This semigroup is a monoid, so this monoid is called the \"characteristic\" or \"transition monoid\". It is also sometimes viewed as an \"Σ\"-act on \"X\", where \"Σ\" is the free monoid of strings generated by the alphabet \"Σ\" and the action of strings extends the action of \"Σ\" via the property\n\nKrohn–Rhodes theory, sometimes also called \"algebraic automata theory\", gives powerful decomposition results for finite transformation semigroups by cascading simpler components.\n\n"}
{"id": "2017636", "url": "https://en.wikipedia.org/wiki?curid=2017636", "title": "Set packing", "text": "Set packing\n\nSet packing is a classical NP-complete problem in computational complexity theory and combinatorics, and was one of Karp's 21 NP-complete problems.\n\nSuppose one has a finite set \"S\" and a list of subsets of \"S\". Then, the set packing problem asks if some \"k\" subsets in the list are pairwise disjoint (in other words, no two of them share an element).\n\nMore formally, given a universe formula_1 and a family formula_2 of subsets of formula_1,\na \"packing\" is a subfamily formula_4 of sets such that all sets in formula_5 are pairwise disjoint. The size of the packing is formula_6. In the set packing decision problem, the input is a pair formula_7 and an integer formula_8; the question is whether\nthere is a set packing of size formula_8 or more. In the set packing optimization problem, the input is a pair formula_7, and the task is to find a set packing that uses the most sets. \n\nThe problem is clearly in NP since, given \"k\" subsets, we can easily verify that they are pairwise disjoint in polynomial time.\n\nThe optimization version of the problem, maximum set packing, asks for the maximum number of pairwise disjoint sets in the list. It is a maximization problem that can be formulated naturally as an integer linear program, belonging to the class of packing problems.\n\nThe maximum set packing problem can be formulated as the following integer linear program.\nAs a simple example, suppose your kitchen contains a collection of different food ingredients (formula_1), and you have a cook-book with a collection of recipes ( formula_2). Each recipe requires a subset of the food ingredients. You want to prepare the largest possible collection of recipes from the cook-book. You are actually looking for a set-packing (formula_5) on (formula_14) - a collection of recipes whose sets of ingredients are pairwise disjoint.\n\nAs another example, suppose you're at a convention of foreign ambassadors, each of which speaks English and also various other languages. You want to make an announcement to a group of them, but because you don't trust them, you don't want them to be able to speak among themselves without you being able to understand them. To ensure this, you will choose a group such that no two ambassadors speak the same language, other than English. On the other hand you also want to give your announcement to as many ambassadors as possible. In this case, the elements of the set are languages other than English, and the subsets are the sets of languages spoken by a particular ambassador. If two sets are disjoint, those two ambassadors share no languages other than English. A maximum set packing will choose the largest possible number of ambassadors under the desired constraint. Although this problem is hard to solve in general, in this example a good heuristic is to choose ambassadors who only speak unusual languages first, so that not too many others are disqualified.\n\nThere is a weighted version of the set packing problem in which each subset is assigned a real weight and it is this weight we wish to maximize: formula_15\n\nIn our simple example above, we might weight the recipes according to the number of friends that love the resulting dishes, so that our dinner will please the largest number of friends. \n\nThis seems to make the problem harder, but most known results for the unweighted problem apply to the weighted problem as well.\n\nThe set packing problem may be hard for some \"k\", but it's not hard to find a \"k\" for which it is easy on a particular input. For example, we can use a greedy algorithm where we look for the set which intersects the smallest number of other sets, add it to our solution, and remove the sets it intersects. We continually do this until no sets are left, and we have a set packing of some size, although it may not be the maximum set packing. Although no algorithm can always produce results close to the maximum (see next section), on many practical inputs these heuristics do so.\n\nThe set packing problem is not only NP-complete, but its optimization version (general maximum set packing problem) has been proven as difficult to approximate as the maximum clique problem; in particular, it cannot be approximated within any constant factor. The best known algorithm approximates it within a factor of formula_16.\nThe weighted variant can also be approximated as well.\n\nHowever, the problem does have a variant which is more tractable: if we assume no subset exceeds \"k\"≥3 elements, the answer can be approximated within a factor of \"k\"/2 + ε for any ε > 0; in particular, the problem with 3-element sets can be approximated within about 50%. In another more tractable variant, if no element occurs in more than \"k\" of the subsets, the answer can be approximated within a factor of \"k\". This is also true for the weighted version.\n\nThere is a one-to-one polynomial-time reduction between the independent set problem and the set packing problem:\n\nThis is also a bidirectional PTAS reduction, and it shows that the two problems are equally difficult to approximate.\n\nMatching and 3-dimensional matching are special cases of set packing. A maximum-size matching can be found in polynomial time, but finding a largest 3-dimensional matching or a largest independent set is NP-hard.\n\nSet packing is one among a family of problems related to covering or partitioning the elements of a set. One closely related problem is the set cover problem. Here, we are also given a set \"S\" and a list of sets, but the goal is to determine whether we can choose \"k\" sets that together contain every element of \"S\". These sets may overlap. The optimization version finds the minimum number of such sets. The maximum set packing need not cover every possible element.\n\nThe NP-complete exact cover problem, on the other hand, requires every element to be contained in exactly one of the subsets. Finding such an exact cover at all, regardless of size, is an NP-complete problem. However, if we create a singleton set for each element of \"S\" and add these to the list, the resulting problem is about as easy as set packing.\n\nKarp originally showed set packing NP-complete via a reduction from the clique problem.\n\nSee also: Packing in a hypergraph.\n\n\n"}
{"id": "10614436", "url": "https://en.wikipedia.org/wiki?curid=10614436", "title": "Shephard's problem", "text": "Shephard's problem\n\nIn mathematics, Shephard's problem, is the following geometrical question asked by : if \"K\" and \"L\" are centrally symmetric convex bodies in \"n\"-dimensional Euclidean space such that whenever \"K\" and \"L\" are projected onto a hyperplane, the volume of the projection of \"K\" is smaller than the volume of the projection of \"L\", then does it follow that the volume of \"K\" is smaller than that of \"L\"?\n\nIn this case, \"centrally symmetric\" means that the reflection of \"K\" in the origin, \"−K\", is a translate of \"K\", and similarly for \"L\". If \"π\" : R → Π is a projection of R onto some \"k\"-dimensional hyperplane Π (not necessarily a coordinate hyperplane) and \"V\" denotes \"k\"-dimensional volume, Shephard's problem is to determine the truth or falsity of the implication\n\n\"V\"(\"π\"(\"K\")) is sometimes known as the brightness of \"K\" and the function \"V\"  \"π\" as a (\"k\"-dimensional) brightness function.\n\nIn dimensions \"n\" = 1 and 2, the answer to Shephard's problem is \"yes\". In 1967, however, Petty and Schneider showed that the answer is \"no\" for every \"n\" ≥ 3. The solution of Shephard's problem requires Minkowski's first inequality for convex bodies and the notion of projection bodies of convex bodies.\n\n\n"}
{"id": "3445204", "url": "https://en.wikipedia.org/wiki?curid=3445204", "title": "Størmer's theorem", "text": "Størmer's theorem\n\nIn number theory, Størmer's theorem, named after Carl Størmer, gives a finite bound on the number of consecutive pairs of smooth numbers that exist, for a given degree of smoothness, and provides a method for finding all such pairs using Pell equations. It follows from the Thue–Siegel–Roth theorem that there are only a finite number of pairs of this type, but Størmer gave a procedure for finding them all.\n\nIf one chooses a finite set formula_1 of prime numbers then the -smooth numbers are defined as the set of integers\n\nthat can be generated by products of numbers in . Then Størmer's theorem states that, for every choice of , there are only finitely many pairs of consecutive -smooth numbers. Further, it gives a method of finding them all using Pell equations.\n\nStørmer's original procedure involves solving a set of roughly 3 Pell equations, in each one finding only the smallest solution. A simplified version of the procedure, due to D. H. Lehmer, is described below; it solves fewer equations but finds more solutions in each equation.\n\nLet \"P\" be the given set of primes, and define a number to be \"P\"-smooth if all its prime factors belong to \"P\". Assume \"p\" = 2; otherwise there could be no consecutive \"P\"-smooth numbers, because all \"P\"-smooth numbers would be odd. Lehmer's method involves solving the Pell equation\nfor each \"P\"-smooth square-free number \"q\" other than 2. Each such number \"q\" is generated as a product of a subset of \"P\", so there are 2 − 1 Pell equations to solve. For each such equation, let \"x, y\" be the generated solutions, for \"i\" in the range from 1 to max(3, (\"p\" + 1)/2) (inclusive), where \"p\" is the largest of the primes in \"P\".\n\nThen, as Lehmer shows, all consecutive pairs of \"P\"-smooth numbers are of the form (\"x\" − 1)/2, (\"x\" + 1)/2. Thus one can find all such pairs by testing the numbers of this form for \"P\"-smoothness.\n\nTo find the ten consecutive pairs of {2,3,5}-smooth numbers (in music theory, giving the superparticular ratios for just tuning) let \"P\" = {2,3,5}. There are seven \"P\"-smooth squarefree numbers \"q\" (omitting the eighth \"P\"-smooth squarefree number, 2): 1, 3, 5, 6, 10, 15, and 30, each of which leads to a Pell equation. The number of solutions per Pell equation required by Lehmer's method is max(3, (5 + 1)/2) = 3, so this method generates three solutions to each Pell equation, as follows.\n\n\nStørmer's original result can be used to show that the number of consecutive pairs of integers that are smooth with respect to a set of \"k\" primes is at most 3 − 2. Lehmer's result produces a tighter bound for sets of small primes: (2 − 1) × max(3,(\"p\"+1)/2).\n\nThe number of consecutive pairs of integers that are smooth with respect to the first \"k\" primes are\nThe largest integer from all these pairs, for each \"k\", is\nOEIS also lists the number of pairs of this type where the larger of the two integers in the pair is square or triangular , as both types of pair arise frequently.\n\nLouis Mordell wrote about this result, saying that it \"is very pretty, and there are many applications of it.\"\n\n used Størmer's method to prove Catalan's conjecture on the nonexistence of consecutive perfect powers (other than 8,9) in the case where one of the two powers is a square.\n\nSeveral authors have extended Størmer's work by providing methods for listing the solutions to more general diophantine equations, or by providing more general divisibility criteria for the solutions to Pell equations.\n\nthan using Pell's equation to find all solutions.\n\nIn the musical practice of just intonation, musical intervals can be described as ratios between positive integers. More specifically, they can be described as ratios between members of the harmonic series. Any musical tone can be broken into its fundamental frequency and harmonic frequencies, which are integer multiples of the fundamental. This series is conjectured to be the basis of natural harmony and melody. The tonal complexity of ratios between these harmonics is said to get more complex with higher prime factors. To limit this tonal complexity, an interval is said to be \"n\"-limit when both its numerator and denominator are \"n\"-smooth. Furthermore, superparticular ratios are very important in just tuning theory as they represent ratios between adjacent members of the harmonic series.\n\nStørmer's theorem allows all possible superparticular ratios in a given limit to be found. For example, in the 3-limit (Pythagorean tuning), the only possible superparticular ratios are 2/1 (the octave), 3/2 (the perfect fifth), 4/3 (the perfect fourth), and 9/8 (the whole step). That is, the only pairs of consecutive integers that have only powers of two and three in their prime factorizations are (1,2), (2,3), (3,4), and (8,9). If this is extended to the 5-limit, six additional superparticular ratios are available: 5/4 (the major third), 6/5 (the minor third), 10/9 (the minor tone), 16/15 (the minor second), 25/24 (the minor semitone), and 81/80 (the syntonic comma). All are musically meaningful.\n\n"}
{"id": "430588", "url": "https://en.wikipedia.org/wiki?curid=430588", "title": "Truncated mean", "text": "Truncated mean\n\nA truncated mean or trimmed mean is a statistical measure of central tendency, much like the mean and median. It involves the calculation of the mean after discarding given parts of a probability distribution or sample at the high and low end, and typically discarding an equal amount of both. This number of points to be discarded is usually given as a percentage of the total number of points, but may also be given as a fixed number of points.\n\nFor most statistical applications, 5 to 25 percent of the ends are discarded; the 25% trimmed mean (when the lowest 25% and the highest 25% are discarded) is known as the interquartile mean. For example, given a set of 8 points, trimming by 12.5% would discard the minimum and maximum value in the sample: the smallest and largest values, and would compute the mean of the remaining 6 points. \n\nThe median can be regarded as a fully truncated mean and is most robust. As with other trimmed estimators, the main advantage of the trimmed mean is robustness and higher efficiency for mixed distributions and heavy-tailed distribution (like the Cauchy distribution), at the cost of lower efficiency for some other less heavily-tailed distributions (such as the normal distribution). For intermediate distributions the differences between the efficiency of the mean and the median are not very big, e.g. for the student-t distribution with 2 degrees of freedom the variances for mean and median are nearly equal.\n\nIn some regions of Central Europe it is also known as a Windsor mean, but this name should not be confused with the Winsorized mean: in the latter, the observations that the trimmed mean would discard are instead replaced by the largest/smallest of the remaining values.\n\nDiscarding only the maximum and minimum is known as the ', particularly in management statistics. This is also known as the ' (for example in US agriculture, like the Average Crop Revenue Election), due to its use in Olympic events, such as the ISU Judging System in figure skating, to make the score robust to a single outlier judge.\n\nWhen the percentage of points to discard does not yield a whole number, the trimmed mean may be defined by interpolation, generally linear interpolation, between the nearest whole numbers. For example, if you need to calculate the 15% trimmed mean of a sample containing 10 entries, strictly this would mean discarding 1 point from each end (equivalent to the 10% trimmed mean). If interpolating, one would instead compute the 10% trimmed mean (discarding 1 point from each end) and the 20% trimmed mean (discarding 2 points from each end), and then interpolating, in this case averaging these two values. Similarly, if interpolating the 12% trimmed mean, one would take the weighted average: weight the 10% trimmed mean by 0.8 and the 20% trimmed mean by 0.2.\n\nThe truncated mean is a useful estimator because it is less sensitive to outliers than the mean but will still give a reasonable estimate of central tendency or mean for many statistical models. In this regard it is referred to as a robust estimator. For example, in its use in Olympic judging, truncating the maximum and minimum prevents a single judge from increasing or lowering the overall score by giving an exceptionally high or low score.\n\nOne situation in which it can be advantageous to use a truncated mean is when estimating the location parameter of a Cauchy distribution, a bell shaped probability distribution with (much) fatter tails than a normal distribution. It can be shown that the truncated mean of the middle 24% sample order statistics (i.e., truncate the sample by 38%) produces an estimate for the population location parameter that is more efficient than using either the sample median or the full sample mean. However, due to the fat tails of the Cauchy distribution, the efficiency of the estimator decreases as more of the sample gets used in the estimate. Note that for the Cauchy distribution, neither the truncated mean, full sample mean or sample median represents a maximum likelihood estimator, nor are any as asymptotically efficient as the maximum likelihood estimator; however, the maximum likelihood estimate is more difficult to compute, leaving the truncated mean as a useful alternative.\n\nThe truncated mean uses more information from the distribution or sample than the median, but unless the underlying distribution is symmetric, the truncated mean of a sample is unlikely to produce an unbiased estimator for either the mean or the median.\n\nIt is possible to perform a Student's t-test based on the truncated mean, this is called Yuen's t-test , which also has several implementations in R \n\nThe scoring method used in many sports that are evaluated by a panel of judges is a truncated mean: \"discard the lowest and the highest scores; calculate the mean value of the remaining scores\".\n\nThe Libor benchmark interest rate is calculated as a trimmed mean: given 18 response, the top 4 and bottom 4 are discarded, and the remaining 10 are averaged (yielding trim factor of 4/18 ≈ 22%).\n\nConsider the data set consisting of:\n\nThe 5th percentile (−6.75) lies between −40 and −5, while the 95th percentile (148.6) lies between 101 and 1053 (values shown in bold). Then, a 5% trimmed mean would result in the following:\n\nThis example can be compared with the one using the Winsorising procedure.\n\n"}
{"id": "2695407", "url": "https://en.wikipedia.org/wiki?curid=2695407", "title": "Truth-table reduction", "text": "Truth-table reduction\n\nIn computability theory, a truth-table reduction is a reduction from one set of natural numbers to another. \nAs a \"tool\", it is weaker than Turing reduction, since not every Turing reduction between sets can be performed by a truth-table reduction, but every truth-table reduction can be performed by a Turing reduction. For the same reason it is said to be a stronger reducibility than Turing reducibility, because it implies Turing reducibility. A weak truth-table reduction is a related type of reduction which is so named because it weakens the constraints placed on a truth-table reduction, and provides a weaker equivalence classification; as such, a \"weak truth-table reduction\" can actually be more powerful than a truth-table reduction as a \"tool\", and perform a reduction which is not performable by truth table.\n\nA Turing reduction from a set \"B\" to a set \"A\" computes the membership of a single element in \"B\" by asking questions about the membership of various elements in \"A\" during the computation; it may adaptively determine which questions it asks based upon answers to previous questions. In contrast, a truth-table reduction or a weak truth-table reduction must present all of its (finitely many) oracle queries at the same time. In a truth-table reduction, the reduction also gives a boolean function (a truth table) which, when given the answers to the queries, will produce the final answer of the reduction. In a weak truth-table reduction, the reduction uses the oracle answers as a basis for further computation which may depend on the given answers but may not ask further questions of the oracle.\n\nEquivalently, a weak truth-table reduction is a Turing reduction for which the use of the reduction is bounded by a computable function. For this reason, they are sometimes referred to as bounded Turing (bT) reductions rather than as weak truth-table (wtt) reductions.\n\nAs every truth-table reduction is a Turing reduction, if \"A\" is truth-table reducible to \"B\" (\"A\" ≤ \"B\"), then \"A\" is also Turing reducible to \"B\" (\"A\" ≤ \"B\"). Considering also one-one reducibility, many-one reducibility and weak truth-table reducibility,\nor in other words, one-one reducibility implies many-one reducibility, which implies truth-table reducibility, which in turn implies weak truth-table reducibility, which in turn implies Turing reducibility.\n\n"}
{"id": "39811070", "url": "https://en.wikipedia.org/wiki?curid=39811070", "title": "Verification condition generator", "text": "Verification condition generator\n\nA verification condition generator is a common sub-component of an automated program verifier that synthesizes formal verification conditions by analyzing a program's source code using a method based upon Hoare logic. VC generators may require that the source code contains logical annotations provided by the programmer or the compiler such as pre/post-conditions and loop invariants (a form of proof-carrying code). VC generators are often coupled with SMT solvers in the backend of a program verifier. After a verification condition generator has created the verification conditions they are passed to an automated theorem prover, which can then formally prove the correctness of the code.\n\nMethods have been proposed to use the operational semantics of machine languages to automatically generate verification condition generators.\n"}
{"id": "18863206", "url": "https://en.wikipedia.org/wiki?curid=18863206", "title": "Vincenty's formulae", "text": "Vincenty's formulae\n\nVincenty's formulae are two related iterative methods used in geodesy to calculate the distance between two points on the surface of a spheroid, developed by Thaddeus Vincenty (1975a). They are based on the assumption that the figure of the Earth is an oblate spheroid, and hence are more accurate than methods that assume a spherical Earth, such as great-circle distance.\n\nThe first (direct) method computes the location of a point that is a given distance and azimuth (direction) from another point. The second (inverse) method computes the geographical distance and azimuth between two given points. They have been widely used in geodesy because they are accurate to within 0.5 mm (0.020″) on the Earth ellipsoid.\n\nVincenty's goal was to express existing algorithms for geodesics on an ellipsoid in a form that minimized the program length (see the first sentence of his paper). His unpublished report (1975b) mentions the use of a Wang 720 desk calculator, which had only a few kilobytes of memory. To obtain good accuracy for long lines, the solution uses the classical solution of Legendre (1806), Bessel (1825), and Helmert (1880) based on the auxiliary sphere. (Vincenty relied on formulation of this method given by Rainsford, 1955.) Legendre showed that an ellipsoidal geodesic can be exactly mapped to a great circle on the auxiliary sphere by mapping the geographic latitude to reduced latitude and setting the azimuth of the great circle equal to that of the geodesic. The longitude on the ellipsoid and the distance along the geodesic are then given in terms of the longitude on the sphere and the arc length along the great circle by simple integrals. Bessel and Helmert gave rapidly converging series for these integrals, which allow the geodesic to be computed with arbitrary accuracy.\n\nIn order to minimize the program size, Vincenty took these series, re-expanded them using the first term of each series as the small parameter, and truncated them to formula_1. This resulted in compact expressions for the longitude and distance integrals. The expressions were put in Horner (or \"nested\") form, since this allows polynomials to be evaluated using only a single temporary register. Finally, simple iterative techniques were used to solve the implicit equations in the direct and inverse methods; even though these are slow (and in the case of the inverse method it sometimes does\nnot converge), they result in the least increase in code size.\n\nDefine the following notation:\nGiven the coordinates of the two points (\"Φ\", \"L\") and (\"Φ\", \"L\"), the inverse problem finds the azimuths \"α\", \"α\" and the ellipsoidal distance \"s\".\n\nCalculate \"U\", \"U\" and \"L\", and set initial value of \"λ\" = \"L\". Then iteratively evaluate the following equations until \"λ\" converges:\n\nWhen \"λ\" has converged to the desired degree of accuracy (10 corresponds to approximately 0.06mm), evaluate the following:\n\nBetween two nearly antipodal points, the iterative formula may fail to converge; this will occur when the first guess at \"λ\" as computed by the equation above is greater than \"π\" in absolute value.\n\nGiven an initial point (\"Φ\", \"L\") and initial azimuth, \"α\", and a distance, \"s\", along the geodesic the problem is to find the end point (\"Φ\", \"L\") and azimuth, \"α\".\n\nStart by calculating the following:\n\nThen, using an initial value formula_24, iterate the following equations until there is no significant change in \"σ\":\n\nOnce \"σ\" is obtained to sufficient accuracy evaluate:\n\nIf the initial point is at the North or South pole, then the first equation is indeterminate.\nIf the initial azimuth is due East or West, then the second equation is indeterminate. \nIf a double valued \"atan2\" type function is used, then these values are usually handled correctly.\n\nIn his letter to Survey Review in 1976, Vincenty suggested replacing his series expressions for \"A\" and \"B\" with simpler formulas using Helmert's expansion parameter \"k\":\n\nformula_34\n\nformula_35\n\nwhereformula_36\n\nAs noted above, the iterative solution to the inverse problem fails to converge or converges slowly for nearly antipodal points. An example of slow convergence is (\"Φ\", \"L\") = (0°, 0°) and (\"Φ\", \"L\") = (0.5°, 179.5°) for the WGS84 ellipsoid. This requires about 130 iterations to give a result accurate to 1 mm. Depending on how the inverse method is implemented, the algorithm might return the correct result (19936288.579 m), an incorrect result, or an error indicator. An example of an incorrect result is provided by the NGS online utility, which returns a distance that is about 5 km too long. Vincenty suggested a method of accelerating the convergence in such cases (Rapp, 1973).\n\nAn example of a failure of the inverse method to converge is (\"Φ\", \"L\") = (0°, 0°) and (\"Φ\", \"L\") = (0.5°, 179.7°) for the WGS84 ellipsoid. In an unpublished report, Vincenty (1975b) gave an alternative iterative scheme to handle such cases. This converges to the correct result 19944127.421 m after about 60 iterations; however, in other cases many thousands of iterations are required.\n\nNewton's method has been successfully used to give rapid convergence for all pairs of input points (Karney, 2013).\n\n\n\n"}
{"id": "22797782", "url": "https://en.wikipedia.org/wiki?curid=22797782", "title": "Weak inverse", "text": "Weak inverse\n\nIn mathematics, the term weak inverse is used with several meanings.\n\nIn the theory of semigroups, a weak inverse of an element \"x\" in a semigroup is an element \"y\" such that . If every element has a weak inverse, the semigroup is called an \"E\"-inversive or \"E\"-dense semigroup. An \"E\"-inversive semigroup may equivalently be defined by requiring that for every element , there exists such that and are idempotents.\n\nAn element \"x\" of \"S\" for which there is an element \"y\" of \"S\" such that is called regular. A regular semigroup is a semigroup in which every element is regular. This is a stronger notion than weak inverse. Every \"E\"-inversive semigroup is regular, but not vice versa.\n\nIf every element \"x\" in \"S\" has a unique inverse \"y\" in \"S\" in the sense that and then \"S\" is called an inverse semigroup.\n\nIn category theory, a weak inverse of an object \"A\" in a monoidal category \"C\" with monoidal product ⊗ and unit object \"I\" is an object \"B\" such that both and are isomorphic to the unit object \"I\" of \"C\". A monoidal category in which every morphism is invertible and every object has a weak inverse is called a 2-group.\n\n"}
