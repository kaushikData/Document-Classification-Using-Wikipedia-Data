{"id": "8469271", "url": "https://en.wikipedia.org/wiki?curid=8469271", "title": "Abacus Harmonicus", "text": "Abacus Harmonicus\n\nAbacus Harmonicus, or Abacum Arithmetico-Harmonicum is a table and tabular method described in Athanasius Kircher's comprehensive 1650 work on music, the \"Musurgia Vniversalis\". The purpose is to generate counterpoint combinations. Also mentioned in early editions of the Encyclopædia Britannica, it is best described by the author's caption: \"wonderful table that reveals all the secret art of counterpoint\".\n"}
{"id": "13218486", "url": "https://en.wikipedia.org/wiki?curid=13218486", "title": "Annales Scientifiques de l'École Normale Supérieure", "text": "Annales Scientifiques de l'École Normale Supérieure\n\nAnnales Scientifiques de l'École Normale Supérieure is a French scientific journal of mathematics published by the Société Mathématique de France. It was established in 1864 by the French chemist Louis Pasteur and published articles in mathematics, physics, chemistry, biology, and geology. In 1900, it became a purely mathematical journal. It is published with help of the Centre national de la recherche scientifique. Its web site is hosted by the mathematics department of the École Normale Supérieure.\n\n"}
{"id": "14585023", "url": "https://en.wikipedia.org/wiki?curid=14585023", "title": "Avrim Blum", "text": "Avrim Blum\n\nAvrim Blum (born 27 May 1966) is a computer scientist. In 2007, he was made a Fellow of the Association for Computing Machinery \"for contributions to learning theory and algorithms.\" Blum attended MIT, where he received his Ph.D. in 1991 under professor Ron Rivest. He was a professor of computer science at Carnegie Mellon University from 1991 to 2017.\n\nIn 2017, he joined Toyota Technological Institute at Chicago as professor and Chief Academic Officer.\n\nHis main work has been in the area of theoretical computer science, with particular activity in the fields of machine learning, computational learning theory, algorithmic game theory, and algorithms.\n\nAvrim is the son of two other well-known computer scientists, Manuel Blum, 1995 Turing Award winner, and Lenore Blum.\n\n\n"}
{"id": "53055349", "url": "https://en.wikipedia.org/wiki?curid=53055349", "title": "Axiom of finite choice", "text": "Axiom of finite choice\n\nIn mathematics, the axiom of finite choice is a weak version of the axiom of choice which asserts that if formula_1 is a family of non-empty \"finite sets\", then\nIf every set can be linearly ordered, the axiom of finite choice follows.\n\nAn important application is that when formula_3 is a measure space where formula_4 is the counting measure and formula_5 is a function s.t.\nthen formula_7 for at most countably many formula_8.\n"}
{"id": "457812", "url": "https://en.wikipedia.org/wiki?curid=457812", "title": "Bi-quinary coded decimal", "text": "Bi-quinary coded decimal\n\nBi-quinary coded decimal is a numeral encoding scheme used in many abacuses and in some early computers, including the Colossus. The term bi-quinary indicates that the code comprises both a two-state (\"bi\") and a five-state (\"quin\"ary) component. The encoding resembles that used by many abaci, with four beads indicating either 0 through 4 or 5 through 9 and another bead indicating which of those ranges.\n\nSeveral human languages, most notably Khmer and Wolof, also use biquinary systems. For example, the Khmer word for 6, \"pram muoy\", literally means \"five [plus] one\". The numerals from 0 to 9 in Japanese Sign Language is based on bi-quinary, with the thumb acting as 5 units, and the rest of the fingers each standing for 1 unit.\n\nSeveral different representations of bi-quinary coded decimal have been used by different machines. The two-state component is encoded as one or two bits, and the five-state component is encoded using three to five bits. Some examples are:\n\n\n\n\n\n"}
{"id": "3264277", "url": "https://en.wikipedia.org/wiki?curid=3264277", "title": "British Mathematical Olympiad", "text": "British Mathematical Olympiad\n\nThe British Mathematical Olympiad (BMO) forms part of the selection process for the UK International Mathematical Olympiad team. It is organised by the British Mathematical Olympiad Subtrust, which is part of the United Kingdom Mathematics Trust. There are two rounds, the BMO1 and the BMO2.\n\nThe first round of the BMO is held in December, and from 2006 is an open entry competition, costing £17 to enter. However, this fee is waived for those who\n(1) achieve the qualifying mark in the Senior Mathematical Challenge and \n(2) are British citizens, or will have studied for 3 full years of full-time secondary education in the UK by the time they leave school.\nThe paper lasts 3½ hours, and consists of six questions (from 2005), each worth 10 marks.\n\nCandidates are encouraged to write full proofs to the questions they attempt, as a full answer to a question is worth many more marks than incomplete answers to several questions. This is because of the marking scheme: an answer is marked on either a \"0+\" or a \"10-\" mark scheme, depending on whether the answer looks generally complete or not. So if an answer is judged incomplete or unfinished, it is awarded a few marks for progress and relevant observations, whereas if it is presented as complete and correct, marks are deducted for faults, poor reasoning, or unproven assumptions. As a result, it is quite uncommon for an answer to score a middling mark (i.e. 4–6).\n\nRoughly 1300 students sit the BMO1 paper each year, and many of these attain a very low score as a percentage of the total number of marks available—the median score in 2004 was approximately 5 or 6 (out of 50). However, the paper is meant to be a way of selecting the best young mathematicians in the country, and is therefore very difficult. In addition to the British students, there is a history of about 20 students from New Zealand being invited to take part. Generally only about 30–40 students will score more than 50% in BMO1, with perhaps only 1 or 2 scoring above 90%.\n\nIn 2005, UKMT changed the system and added an extra easier question meaning the median is now raised. In 2008, 23 got over 40 out of 60 and around 50 got over 30. 31 was the mark needed to get into the BMO2.\n\nFor the December 2010 paper, the mark needed for a year 13 pupil to automatically qualify for BMO2 was 28; it was 22 for a year 12 pupil and 17 for a pupil in year 11 and below.\n\nFrom the results of BMO1, 100 students are chosen to be invited to sit BMO2—generally from amongst the top scorers, although younger students scoring slightly less than their older counterparts are often chosen also. It is possible to pay £22 to sit BMO2, but the candidate must have sat BMO1.\n\nBMO2 (known as the \"Further International Selection Test, FIST\" from 1972 to 1991) is normally held in early February, and is significantly more difficult than BMO1. BMO2 also lasts 3½ hours, but consists of only four questions, each worth 10 marks. Like the BMO1 paper, it is not designed to test knowledge of advanced mathematics, but rather the candidate's ability to apply basic ideas to unusual problems.\n\nHalf of the candidates score less than 10, and it is rare for someone to score more than 35.\n\nTwenty of the top scorers from BMO2 are subsequently invited to the training camp at Trinity College, Cambridge for the first stage of the IMO UK team selection.\n\n\"For more information about IMO selection in other countries, see International Mathematical Olympiad selection process\"\n\nSince 1985, further selection tests have been used after BMO2 to select the IMO team. (The team was selected following the single BMO paper from 1967 to 1971, then following the FIST paper for some years from 1972.) Initially these third-stage tests resulted in selection of both team and reserve; from 1993 a squad (team plus reserve) was selected following these tests with the team being separated from the reserve after further correspondence training, and after further selection tests from 2001 onwards. The third-stage tests have had names including FIST 2 (1985), Second International Selection Test (SIST), Reading Selection Test (1987), Final Selection Test (FST, 1992 to 2001) and First Selection Test (FST, from 2002); the fourth-stage tests have been Team Selection Test (TST, 2001) and Next Selection Test (NST, 2002 onwards). These tests have been held at training and selection camps in several locations, recently Trinity College, Cambridge and Oundle School.\n\n\n"}
{"id": "7776927", "url": "https://en.wikipedia.org/wiki?curid=7776927", "title": "Bulk synchronous parallel", "text": "Bulk synchronous parallel\n\nThe bulk synchronous parallel (BSP) abstract computer is a bridging model for designing parallel algorithms. It serves a purpose similar to the parallel random access machine (PRAM) model. BSP differs from PRAM by not taking communication and synchronization for granted. An important part of analyzing a BSP algorithm rests on quantifying the synchronization and communication needed.\n\nThe BSP model was developed by Leslie Valiant of Harvard University during the 1980s. The definitive article was published in 1990.\n\nBetween 1990 and 1992, Leslie Valiant and Bill McColl of Oxford University worked on ideas for a distributed memory BSP programming model, in Princeton and at Harvard. Between 1992 and 1997, McColl led a large research team at Oxford that developed various BSP programming libraries, languages and tools, and also numerous massively parallel BSP algorithms. With interest and momentum growing, McColl then led a group from Oxford, Harvard, Florida, Princeton, Bell Labs, Columbia and Utrecht that developed and published the BSPlib Standard for BSP programming in 1996.\n\nValiant developed an extension to the BSP model in the 2000s, leading to the publication of the Multi-BSP model in 2011.\n\nIn 2017, McColl developed a major new extension of the BSP model that provides fault tolerance and tail tolerance for large-scale parallel computations in AI, Analytics and HPC. \n\nA BSP computer consists of\nThis is commonly interpreted as a set of processors which may follow different threads of computation, with each processor equipped with fast local memory and interconnected by a communication network.\nA BSP algorithm relies heavily on the third feature; a computation proceeds in a series of global \"supersteps\", which consists of three components:\n\nThe computation and communication actions do not have to be ordered in time. Communication typically takes the form of the one-sided \"put\" and \"get\" Direct Remote Memory Access (DRMA) calls, rather than paired two-sided \"send\" and \"receive\" message passing calls.\nThe barrier synchronization concludes the superstep: it ensures that all one-sided communications are properly concluded. Systems based on two-sided communication include this synchronisation cost implicitly for every message sent. The method for barrier synchronisation relies on the hardware facility of the BSP computer. In Valiant's original paper, this facility periodically checks if the end of the current superstep is reached globally. The period of this check is denoted by formula_1.\n\nThe figure below shows this in a diagrammatic form. The processes are not regarded as having a particular linear order (from left to right or otherwise), and may be mapped to processors in any way.\n\nThe BSP model is also well-suited to enable automatic memory management for distributed-memory computing through overdecomposition of the problem and oversubscription of the processors. The computation is divided into more logical processes than there are physical processors, and processes are randomly assigned to processors. This strategy can be shown statistically to lead to almost perfect load balancing, both of work and communication.\n\nIn many parallel programming systems, communications are considered at the level of individual actions: sending and receiving a message, memory to memory transfer, etc. This is difficult to work with, since there are many simultaneous communication actions in a parallel program, and their interactions are typically complex. In particular, it is difficult to say much about the time any single communication action will take to complete.\n\nThe BSP model considers communication actions \"en masse\". This has the effect that an upper bound on the time taken to communicate a set of data can be given. BSP considers all communication actions of a superstep as one unit, and assumes all individual messages sent as part of this unit have a fixed size.\n\nThe maximum number of incoming or outgoing messages for a superstep is denoted by formula_2. The ability of a communication network to deliver data is captured by a parameter formula_3, defined such that it takes time formula_4 for a processor to deliver formula_2 messages of size 1.\n\nA message of length formula_6 obviously takes longer to send than a message of size 1. However, the BSP model does not make a distinction between a message length of formula_6 or formula_6 messages of length 1. In either case the cost is said to be formula_9.\n\nThe parameter formula_3 is dependent on the following factors:\n\nIn practice, formula_3 is determined empirically for each parallel computer. Note that formula_3 is not the normalised single-word delivery time, but the single-word delivery time under continuous traffic conditions.\n\nThe one-sided communication of the BSP model requires barrier synchronization. \nBarriers are potentially costly, but avoid the possibility of deadlock or livelock, \nsince barriers cannot create circular data dependencies. Tools to detect them and deal with them are unnecessary. Barriers also permit novel forms of fault tolerance.\n\nThe cost of barrier synchronization is influenced by a couple of issues:\n\nThe cost of a barrier synchronization is denoted by formula_13. Note that formula_14 if the synchronisation mechanism of the BSP computer is as suggested by Valiant. \n\nIn practice, a value of formula_13 is determined empirically.\n\nOn large computers barriers are expensive, and this is increasingly so on large scales. There is a large body of literature on removing synchronization points from existing algorithms, both in the context of BSP computing and beyond. For example, many algorithms allow for the local detection of the global end of a superstep simply by comparing local information to the number of messages already received. This drives the cost of a global synchronisation, compared to the minimally required latency of communication, to zero. Yet also this minimal latency is expected to increase further for future supercomputer architectures and network interconnects; the BSP model, along with other models for parallel computation, require adaptation to cope with this trend. Multi-BSP is one BSP-based solution.\n\nThe cost of a superstep is determined as the sum of three terms; the cost of the longest running local computation, the cost of global communication between the processors, and the cost of the barrier synchronisation at the end of the superstep. The cost of one superstep\nfor formula_16 processors:\n\nformula_17\nwhere formula_18 is the cost for the local computation in process formula_19, and formula_20 is the number of messages sent or received by process formula_19. Note that homogeneous processors are assumed here. It is more common for the expression to be written as formula_22 where formula_23 and formula_2 are maxima. The cost of the algorithm then, is the sum of the costs of each superstep.\n\nformula_25\nwhere formula_26 is the number of supersteps.\n\nformula_27, formula_28, and formula_26 are usually modelled as functions, that vary with problem size. These three characteristics of a BSP algorithm are usually described in terms of asymptotic notation, e.g. formula_30.\n\nInterest in BSP has soared in recent years, with Google adopting it as a major technology for graph analytics at massive scale via technologies like Pregel and MapReduce. Also, with the next generation of Hadoop decoupling the MapReduce model from the rest of the Hadoop infrastructure, there are now active open source projects to add explicit BSP programming, as well as other high performance parallel programming models, on top of Hadoop. Examples are Apache Hama and Apache Giraph.\n\nBSP has been extended by many authors to address concerns about BSP's unsuitability for modelling specific architectures or computational paradigms. One example of this is the decomposable BSP model. The model has also been used in the creation of a number of new programming languages and interfaces, such as Bulk Synchronous Parallel ML (BSML), BSPLib, Apache Hama, and Pregel.\n\nNotable implementations of the BSPLib standard are the Paderborn University BSP library and the Oxford BSP Toolset by Jonathan Hill. Modern implementations include BSPonMPI (which simulates BSP on top of the Message Passing Interface), and MulticoreBSP (a novel implementation targeting modern shared-memory architectures). MulticoreBSP for C is especially notable for its capability of starting nested BSP runs, thus allowing for explicit Multi-BSP programming.\n\n\n"}
{"id": "31866635", "url": "https://en.wikipedia.org/wiki?curid=31866635", "title": "CC system", "text": "CC system\n\nIn computational geometry, a CC system or counterclockwise system is a ternary relation introduced by Donald Knuth to model the clockwise ordering of triples of points in general position in the Euclidean plane.\n\nA CC system is required to satisfy the following axioms, for all distinct points \"p\", \"q\", \"r\", \"s\", and \"t\":\n\n\nTriples of points that are not distinct are not considered as part of the relation.\n\nA CC system may be defined from any set of points in the Euclidean plane, with no three of the points collinear, by including in the relation a triple of distinct points whenever the triple lists these three points in counterclockwise order around the triangle that they form. Using the Cartesian coordinates of the points, the triple \"pqr\" is included in the relation exactly when\nThe condition that the points are in general position is equivalent to the requirement that this matrix determinant is never zero for distinct points \"p\", \"q\", and \"r\".\n\nHowever, not every CC system comes from a Euclidean point set in this way.\n\nCC systems can also be defined from pseudoline arrangements, or from sorting networks in which the compare-exchange operations only compare adjacent pairs of elements (as in for instance bubble sort), and every CC system can be defined in this way. This relation is not one-to-one, but the numbers of nonisomorphic CC systems on \"n\" points, of pseudoline arrangements with \"n\" lines, and of sorting networks on \"n\" values, are within polynomial factors of each other.\n\nThere exists a two-to-one correspondence between CC systems and uniform acyclic oriented matroids of rank 3. These matroids in turn have a 1-1 correspondence to topological equivalence classes of pseudoline arrangements with one marked cell.\n\nThe information given by a CC system is sufficient to define a notion of a convex hull within a CC system. The convex hull is the set of ordered pairs \"pq\" of distinct points with the property that, for every third distinct point \"r\", \"pqr\" belongs to the system. It forms a cycle, with the property that every three points of the cycle, in the same cyclic order, belong to the system. By adding points one at a time to a CC system, and maintaining the convex hull of the points added so far in its cyclic order using a binary search tree, it is possible to construct the convex hull in time \"O\"(\"n\" log \"n\"), matching the known time bounds for convex hull algorithms for Euclidean points.\n\nIt is also possible to find a single convex hull vertex, as well as the combinatorial equivalent of a bisecting line through a system of points, from a CC system in linear time. The construction of an extreme vertex allows the Graham scan algorithm for convex hulls to be generalized from point sets to CC systems, with a number of queries to the CC system that matches (to within lower-order terms) the number of comparisons needed in comparison sorting.\n\nThe number of non-isomorphic CC systems on \"n\" points is\n\nThese numbers grow exponentially in \"n\"; in contrast, the number of realizable CC systems grows exponentially only in Θ(\"n\" log \"n\").\n\nMore precisely, the number \"C\" of non-isomorphic CC systems on \"n\" points is at most\nKnuth conjectures more strongly that these numbers obey the recursive inequality\n\n"}
{"id": "408195", "url": "https://en.wikipedia.org/wiki?curid=408195", "title": "Cauchy principal value", "text": "Cauchy principal value\n\nIn mathematics, the Cauchy principal value, named after Augustin Louis Cauchy, is a method for assigning values to certain improper integrals which would otherwise be undefined.\n\nDepending on the type of singularity in the integrand \"f\", the Cauchy principal value is defined according to the following rules:\n\nIn some cases it is necessary to deal simultaneously with singularities both at a finite number \"b\" and at infinity. This is usually done by a limit of the form\n\nThe Cauchy principal value can also be defined in terms of contour integrals of a complex-valued function \"f\"(\"z\"); \"z\" = \"x\" + \"iy\", with a pole on a contour \"C\". Define \"C\"(\"ε\") to be the same contour where the portion inside the disk of radius \"ε\" around the pole has been removed. Provided the function \"f\"(\"z\") is integrable over \"C(ε)\" no matter how small ε becomes, then the Cauchy principal value is the limit:\n\nIn the case of Lebesgue-integrable functions, that is, functions which are integrable in absolute value, these definitions coincide with the standard definition of the integral.\n\nIf the function \"f\"(\"z\") is meromorphic, the Sokhotski–Plemelj theorem relates the principal value of the integral over \"C\" with the mean-value of the integrals with the contour displaced slightly above and below, so that the residue theorem can be applied to those integrals.\n\nPrincipal value integrals play a central role in the discussion of Hilbert transforms.\n\nLet formula_9 be the set of bump functions, i.e., the space of smooth functions with compact support on the real line formula_10. Then the map\n\ndefined via the Cauchy principal value as\n\nis a distribution. The map itself may sometimes be called the principal value (hence the notation p.v.). This distribution appears, for example, in the Fourier transform of the Sign function and the Heaviside step function.\n\nTo prove the existence of the limit \nfor a Schwartz function formula_14, first observe that formula_15 is continuous on formula_16, as\n\nsince formula_19 is continuous and L'Hospital's rule applies.\n\nTherefore, formula_20 exists and by applying the mean value theorem to formula_21, we get that\n\nAs furthermore\n\nwe note that the map formula_11 is bounded by the usual seminorms for Schwartz functions formula_25. Therefore, this map defines, as it is obviously linear, a continuous functional on the Schwartz space and therefore a tempered distribution.\n\nNote that the proof needs formula_25 merely to be continuously differentiable in a neighbourhood of formula_27 and formula_28 to be bounded towards infinity. The principal value therefore is defined on even weaker assumptions such as formula_29 integrable with compact support and differentiable at 0.\n\nThe principal value is the inverse distribution of the function formula_30 and is almost the only distribution with this property:\nwhere formula_32 is a constant and formula_33 the Dirac distribution.\n\nIn a broader sense, the principal value can be defined for a wide class of singular integral kernels on the Euclidean space formula_34. If formula_32 has an isolated singularity at the origin, but is an otherwise \"nice\" function, then the principal-value distribution is defined on compactly supported smooth functions by\nSuch a limit may not be well defined, or, being well-defined, it may not necessarily define a distribution. It is, however, well-defined if formula_32 is a continuous homogeneous function of degree formula_38 whose integral over any sphere centered at the origin vanishes. This is the case, for instance, with the Riesz transforms.\n\nConsider the difference in values of two limits:\n\nThe former is the Cauchy principal value of the otherwise ill-defined expression\n\nSimilarly, we have\n\nbut\n\nThe former is the principal value of the otherwise ill-defined expression\n\nDifferent authors use different notations for the Cauchy principal value of a function formula_45, among others:\n\n"}
{"id": "31013835", "url": "https://en.wikipedia.org/wiki?curid=31013835", "title": "Chain linking", "text": "Chain linking\n\nChain linking is a statistical method, defined by the Organisation for Economic Co-operation and Development as:\n\nChain linking is popularly used with GDP/GNP data, to measure changes over time, giving a chained volume series.\n"}
{"id": "1679210", "url": "https://en.wikipedia.org/wiki?curid=1679210", "title": "Club filter", "text": "Club filter\n\nIn mathematics, particularly in set theory, if formula_1 is a regular uncountable cardinal then formula_2, the filter of all sets containing a club subset of formula_1, is a formula_1-complete filter closed under diagonal intersection called the club filter.\n\nTo see that this is a filter, note that formula_5 since it is thus both closed and unbounded (see club set). If formula_6 then any subset of formula_1 containing formula_8 is also in formula_2, since formula_8, and therefore anything containing it, contains a club set.\n\nIt is a formula_1-complete filter because the intersection of fewer than formula_1 club sets is a club set. To see this, suppose formula_13 is a sequence of club sets where formula_14. Obviously formula_15 is closed, since any sequence which appears in formula_16 appears in every formula_17, and therefore its limit is also in every formula_17. To show that it is unbounded, take some formula_19. Let formula_20 be an increasing sequence with formula_21 and formula_22 for every formula_23. Such a sequence can be constructed, since every formula_17 is unbounded. Since formula_14 and formula_1 is regular, the limit of this sequence is less than formula_1. We call it formula_28, and define a new sequence formula_29 similar to the previous sequence. We can repeat this process, getting a sequence of sequences formula_30 where each element of a sequence is greater than every member of the previous sequences. Then for each formula_23, formula_30 is an increasing sequence contained in formula_17, and all these sequences have the same limit (the limit of formula_30). This limit is then contained in every formula_17, and therefore formula_16, and is greater than formula_37.\n\nTo see that formula_2 is closed under diagonal intersection, let formula_39, formula_40 be a sequence of club sets, and let formula_41. To show formula_16 is closed, suppose formula_43 and formula_44. Then for each formula_45, formula_46 for all formula_47. Since each formula_48 is closed, formula_49 for all formula_50, so formula_51. To show formula_16 is unbounded, let formula_14, and define a sequence formula_54, formula_55 as follows: formula_56, and formula_57 is the minimal element of formula_58 such that formula_59. Such an element exists since by the above, the intersection of formula_54 club sets is club. Then formula_61 and formula_62, since it is in each formula_17 with formula_64.\n\n"}
{"id": "1198956", "url": "https://en.wikipedia.org/wiki?curid=1198956", "title": "Conditional convergence", "text": "Conditional convergence\n\nIn mathematics, a series or integral is said to be conditionally convergent if it converges, but it does not converge absolutely.\n\nMore precisely, a series formula_1 is said to converge conditionally if \nformula_2 exists and is a finite number (not ∞ or −∞), but formula_3\n\nA classic example is the alternating series given by formula_4 which converges to formula_5, but is not absolutely convergent (see Harmonic series).\n\nBernhard Riemann proved that a conditionally convergent series of real numbers may be rearranged to converge to any value at all, including ∞ or −∞; see \"Riemann series theorem\". The Lévy–Steinitz theorem identifies the set of values to which a series of terms in R can converge.\n\nA typical conditionally convergent integral is that on the non-negative real axis of formula_6 (see Fresnel integral).\n\n\n"}
{"id": "15098681", "url": "https://en.wikipedia.org/wiki?curid=15098681", "title": "Cone of curves", "text": "Cone of curves\n\nIn mathematics, the cone of curves (sometimes the Kleiman-Mori cone) of an algebraic variety formula_1 is a combinatorial invariant of much importance to the birational geometry of formula_1. \n\nLet formula_1 be a proper variety. By definition, a (real) \"1-cycle\" on formula_1 is a formal linear combination formula_5 of irreducible, reduced and proper curves formula_6, with coefficients formula_7. \"Numerical equivalence\" of 1-cycles is defined by intersections: two 1-cycles formula_8 and formula_9 are numerically equivalent if formula_10 for every Cartier divisor formula_11 on formula_1. Denote the real vector space of 1-cycles modulo numerical equivalence by formula_13.\n\nWe define the \"cone of curves\" of formula_1 to be\n\nwhere the formula_6 are irreducible, reduced, proper curves on formula_1, and formula_18 their classes in formula_13. It is not difficult to see that formula_20 is indeed a convex cone in the sense of convex geometry.\n\nOne useful application of the notion of the cone of curves is the Kleiman condition, which says that a (Cartier) divisor formula_11 on a complete variety formula_1 is ample if and only if formula_23 for any nonzero element formula_24 in formula_25, the closure of the cone of curves in the usual real topology. (In general, formula_20 need not be closed, so taking the closure here is important.)\n\nA more involved example is the role played by the cone of curves in the theory of minimal models of algebraic varieties. Briefly, the goal of that theory is as follows: given a (mildly singular) projective variety formula_1, find a (mildly singular) variety formula_28 which is birational to formula_1, and whose canonical divisor formula_30 is nef. The great breakthrough of the early 1980s (due to Mori and others) was to construct (at least morally) the necessary birational map from formula_1 to formula_28 as a sequence of steps, each of which can be thought of as contraction of a formula_33-negative extremal ray of formula_20. This process encounters difficulties, however, whose resolution necessitates the introduction of the flip.\n\nThe above process of contractions could not proceed without the fundamental result on the structure of the cone of curves known as the Cone Theorem. The first version of this theorem, for smooth varieties, is due to Mori; it was later generalised to a larger class of varieties by Kollár, Reid, Shokurov, and others. Mori's version of the theorem is as follows:\n\nCone Theorem. Let formula_1 be a smooth projective variety. Then\n\n1. There are countably many rational curves formula_6 on formula_1, satisfying formula_38, and \n\n2. For any positive real number formula_40 and any ample divisor formula_41,\n\nwhere the sum in the last term is finite.\n\nThe first assertion says that, in the closed half-space of formula_13 where intersection with formula_33 is nonnegative, we know nothing, but in the complementary half-space, the cone is spanned by some countable collection of curves which are quite special: they are rational, and their 'degree' is bounded very tightly by the dimension of formula_1. The second assertion then tells us more: it says that, away from the hyperplane formula_46, extremal rays of the cone cannot accumulate. \n\nIf in addition the variety formula_1 is defined over a field of characteristic 0, we have the following assertion, sometimes referred to as the Contraction Theorem:\n\n3. Let formula_48 be an extremal face of the cone of curves on which formula_33 is negative. Then there is a unique morphism formula_50 to a projective variety \"Z\", such that formula_51 and an irreducible curve formula_8 in formula_1 is mapped to a point by formula_54 if and only if formula_55.\n(See also: contraction morphism).\n\n"}
{"id": "20286769", "url": "https://en.wikipedia.org/wiki?curid=20286769", "title": "Correlation inequality", "text": "Correlation inequality\n\nA correlation inequality is any of a number of inequalities satisfied by the correlation functions of a model. Such inequalities are of particular use in statistical mechanics and in percolation theory.\n\nExamples include:\n"}
{"id": "8751011", "url": "https://en.wikipedia.org/wiki?curid=8751011", "title": "Cross-figure", "text": "Cross-figure\n\nA cross-figure (also variously called cross number puzzle or figure logic) is a puzzle similar to a crossword in structure, but with entries which consist of numbers rather than words, with individual digits being entered in the blank cells. The numbers can be clued in various ways:\n\n\nCross-figures which use mostly the first type of clue may be used for educational purposes, but most enthusiasts would agree that this clue type should be used rarely, if at all. Without this type a cross-figure may superficially seem to be impossible to solve, since no answer can apparently be filled in until another has first been found, which without the first type of clue appears impossible. However, if a different approach is adopted where, instead of trying to find complete answers (as would be done for a crossword) one gradually narrows down the possibilities for individual cells (or, in some cases, whole answers) then the problem becomes tractable. For example, if 12 across and 7 down both have three digits and the clue for 12 across is \"7 down times 2\", one can work out that (i) the last digit of 12 across must be even, (ii) the first digit of 7 down must be 1, 2, 3 or 4, and (iii) the first digit of 12 across must be between 2 and 9 inclusive. (It is an implicit rule of cross-figures that numbers cannot start with 0; however, some puzzles explicitly allow this) By continuing to apply this sort of argument, a solution can eventually be found. Another implicit rule of cross-figures is that no two answers should be the same (in cross-figures allowing numbers to start with 0, 0123 and 123 may be considered different.)\n\nA curious feature of cross-figures is that it makes perfect sense for the setter of a puzzle to try to solve it him or herself. Indeed, the setter should ideally do this (without direct reference to the answer) as it is essentially the only way to find out if the puzzle has a single unique solution. Alternatively, there are computer programs available that can be used for this purpose; however, they may not make it clear how difficult the puzzle is. \n\nGiven that some basic mathematical knowledge is needed to solve cross-figures, they are much less popular than crosswords. As a result, very few books of them have ever been published. Dell Magazines publishes a magazine called \"Math Puzzles and Logic Problems\" six times a year which generally contains as many as a dozen of these puzzles, which they name \"Figure Logics\". A magazine called \"Figure it Out\", which was dedicated to number puzzles, included some, but it was very short-lived. This also explains why cross-figures have fewer established conventions than crosswords (especially cryptic crosswords). One exception is the use of the semicolon (;) to attach two strings of numbers together, for example 1234;5678 becomes 12345678. Some cross-figures voluntarily ignore this option and other \"non-mathematical\" approaches (e.g. palindromic numbers and repunits) where same result can be achieved through algebraic means.\n\n"}
{"id": "3417630", "url": "https://en.wikipedia.org/wiki?curid=3417630", "title": "Damerau–Levenshtein distance", "text": "Damerau–Levenshtein distance\n\nIn information theory and computer science, the Damerau–Levenshtein distance (named after Frederick J. Damerau and Vladimir I. Levenshtein) is a string metric for measuring the edit distance between two sequences. Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations (consisting of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters) required to change one word into the other.\n\nThe Damerau–Levenshtein distance differs from the classical Levenshtein distance by including transpositions among its allowable operations in addition to the three classical single-character edit operations (insertions, deletions and substitutions).\n\nIn his seminal paper, Damerau stated that these four operations correspond to more than 80% of all human misspellings. Damerau's paper considered only misspellings that could be corrected with at most one edit operation. While the original motivation was to measure distance between human misspellings to improve applications such as spell checkers, Damerau–Levenshtein distance has also seen uses in biology to measure the variation between protein sequences.\n\nTo express the Damerau–Levenshtein distance between two strings formula_1 and formula_2 a function formula_3 is defined, whose value is a distance between an formula_4–symbol prefix (initial substring) of string formula_1 and a formula_6–symbol prefix of formula_2.\n\nThe \"restricted distance\" function is defined recursively as:,\n\nformula_8\n\nwhere formula_9 is the indicator function equal to 0 when formula_10 and equal to 1 otherwise.\n\nEach recursive call matches one of the cases covered by the Damerau–Levenshtein distance:\n\nThe Damerau–Levenshtein distance between and is then given by the function value for full strings: formula_15 where formula_16 denotes the length of string and formula_17 is the length of .\n\nPresented here are two algorithms: the first, simpler one, computes what is known as the \"optimal string alignment distance\" or \"restricted edit distance\", while the second one computes the Damerau–Levenshtein distance with adjacent transpositions. Adding transpositions adds significant complexity. The difference between the two algorithms consists in that the \"optimal string alignment algorithm\" computes the number of edit operations needed to make the strings equal under the condition that no substring is edited more than once, whereas the second one presents no such restriction.\n\nTake for example the edit distance between CA and ABC. The Damerau–Levenshtein distance LD(CA,ABC) = 2 because CA → AC → ABC, but the optimal string alignment distance OSA(CA,ABC) = 3 because if the operation CA → AC is used, it is not possible to use AC → ABC because that would require the substring to be edited more than once, which is not allowed in OSA, and therefore the shortest sequence of operations is CA → A → AB → ABC. Note that for the optimal string alignment distance, the triangle inequality does not hold: OSA(CA,AC) + OSA(AC,ABC) < OSA(CA,ABC), and so it is not a true metric.\n\nOptimal string alignment distance can be computed using a straightforward extension of the Wagner–Fischer dynamic programming algorithm that computes Levenshtein distance. In pseudocode:\n\nThe difference from the algorithm for Levenshtein distance is the addition of one recurrence:\n\nThe following algorithm computes the true Damerau–Levenshtein distance with adjacent transpositions; this algorithm requires as an additional parameter the size of the alphabet , so that all entries of the arrays are in :\n\nTo devise a proper algorithm to calculate unrestricted Damerau–Levenshtein distance note that there always exists an optimal sequence of edit operations, where once-transposed letters are never modified afterwards. (This holds as long as the cost of a transposition, formula_18, is at least the average of the cost of an insertion and deletion, i.e., formula_19.) Thus, we need to consider only two symmetric ways of modifying a substring more than once: (1) transpose letters and insert an arbitrary number of characters between them, or (2) delete a sequence of characters and transpose letters that become adjacent after deletion. The straightforward implementation of this idea gives an algorithm of cubic complexity: formula_20, where \"M\" and \"N\" are string lengths. Using the ideas of Lowrance and Wagner, this naive algorithm can be improved to be formula_21 in the worst case.\n\nIt is interesting that the bitap algorithm can be modified to process transposition. See the information retrieval section of for an example of such an adaptation.\n\nDamerau–Levenshtein distance plays an important role in natural language processing. In natural languages, strings are short and the number of errors (misspellings) rarely exceeds 2. In such circumstances, restricted and real edit distance differ very rarely. Oommen and Loke even mitigated the limitation of the restricted edit distance by introducing \"generalized transpositions\". Nevertheless, one must remember that the restricted edit distance usually does not satisfy the triangle inequality and, thus, cannot be used with metric trees.\n\nSince DNA frequently undergoes insertions, deletions, substitutions, and transpositions, and each of these operations occurs on approximately the same timescale, the Damerau–Levenshtein distance is an appropriate metric of the variation between two strands of DNA. More common in DNA, protein, and other bioinformatics related alignment tasks is the use of closely related algorithms such as Needleman–Wunsch algorithm or Smith–Waterman algorithm.\n\nThe algorithm can be used with any set of words, like vendor names. Since entry is manual by nature there is a risk of entering a false vendor. A fraudster employee may enter one real vendor such as \"Rich Heir Estate Services\" versus a false vendor \"Rich Hier State Services\". The fraudster would then create a false bank account and have the company route checks to the real vendor and false vendor. The Damerau–Levenshtein algorithm will detect the transposed and dropped letter and bring attention of the items to a fraud examiner.\n\nThe U.S. Government uses the Damerau–Levenshtein distance with its Consolidated Screening List API.\n\n"}
{"id": "15640469", "url": "https://en.wikipedia.org/wiki?curid=15640469", "title": "Dickson polynomial", "text": "Dickson polynomial\n\nIn mathematics, the Dickson polynomials, denoted , form a polynomial sequence introduced by . They were rediscovered by in his study of Brewer sums and have at times, although rarely, been referred to as Brewer polynomials.\n\nOver the complex numbers, Dickson polynomials are essentially equivalent to Chebyshev polynomials with a change of variable, and, in fact, Dickson polynomials are sometimes called Chebyshev polynomials.\n\nDickson polynomials are generally studied over finite fields, where they sometimes may not be equivalent to Chebyshev polynomials. One of the main reasons for interest in them is that for fixed , they give many examples of \"permutation polynomials\"; polynomials acting as permutations of finite fields.\n\nFor integer and in a commutative ring with identity (often chosen to be the finite field ) the Dickson polynomials (of the first kind) over are given by\n\nThe first few Dickson polynomials are\n\nThey may also be generated by the recurrence relation for ,\n\nwith the initial conditions and .\n\nThe Dickson polynomials of the second kind, , are defined by\nThey have not been studied much, and have properties similar to those of Dickson polynomials of the first kind.\nThe first few Dickson polynomials of the second kind are\n\nThey may also be generated by the recurrence relation for ,\n\nwith the initial conditions and .\n\nThe are the unique monic polynomials satisfying the functional equation\n\nwhere and .\n\nThey also satisfy a composition rule,\n\nThe also satisfy a functional equation\nfor , , with and .\n\nThe Dickson polynomial is a solution of the ordinary differential equation\nand the Dickson polynomial is a solution of the differential equation\nTheir ordinary generating functions are\n\nBy the recurrence relation above, Dickson polynomials are Lucas sequences. Specifically, for , the Dickson polynomials of the first kind are Fibonacci polynomials, and Dickson polynomials of the second kind are Lucas polynomials.\n\nBy the composition rule above, when α is idempotent, composition of Dickson polynomials of the first kind is commutative.\n\nformula_13\n\n\nformula_14\n\nA permutation polynomial (for a given finite field) is one that acts as a permutation of the elements of the finite field.\n\nThe Dickson polynomial (considered as a function of with α fixed) is a permutation polynomial for the field with elements if and only if is coprime to .\n\nFurther, proved that any permutation polynomial over the finite field whose degree is simultaneously coprime to and less than must be a composition of Dickson polynomials and linear polynomials.\n\nDickson polynomials of both kinds over finite fields can be thought of as initial members of a sequence of generalized Dickson polynomials referred to as Dickson polynomials of the th kind. Specifically, for with for some prime and any integers and , the th Dickson polynomial of the th kind over , denoted by , is defined by\nand\n\nThe significant properties of the Dickson polynomials also generalize:\n\n"}
{"id": "3698082", "url": "https://en.wikipedia.org/wiki?curid=3698082", "title": "Entitative graph", "text": "Entitative graph\n\nAn entitative graph is an element of the diagrammatic syntax for logic that Charles Sanders Peirce developed under the name of qualitative logic beginning in the 1880s, taking the coverage of the formalism only as far as the propositional or sentential aspects of logic are concerned. See 3.468, 4.434, and 4.564 in Peirce's \"Collected Papers\".\n\nThe syntax is:\n\nThe semantics are:\n\nA \"proof\" manipulates a graph, using a short list of rules, until the graph is reduced to an empty cut or the blank page. A graph that can be so reduced is what is now called a tautology (or the complement thereof). Graphs that cannot be simplified beyond a certain point are analogues of the satisfiable formulas of first-order logic.\n\nPeirce soon abandoned the entitative graphs for the existential graphs, whose sentential (\"alpha\") part is dual to the entitative graphs. He developed the existential graphs until they became another formalism for what are now termed first-order logic and normal modal logic.\n\nThe primary algebra of G. Spencer-Brown is isomorphic to the entitative graphs.\n\n\n"}
{"id": "965348", "url": "https://en.wikipedia.org/wiki?curid=965348", "title": "Functional calculus", "text": "Functional calculus\n\nIn mathematics, a functional calculus is a theory allowing one to apply mathematical functions to mathematical operators. It is now a branch (more accurately, several related areas) of the field of functional analysis, connected with spectral theory. (Historically, the term was also used synonymously with calculus of variations; this usage is obsolete, except for functional derivative. Sometimes it is used in relation to types of functional equations, or in logic for systems of predicate calculus.)\n\nIf \"f\" is a function, say a numerical function of a real number, and \"M\" is an operator, there is no particular reason why the expression\n\nshould make sense. If it does, then we are no longer using \"f\" on its original function domain. In the tradition of operational calculus, algebraic expressions in operators are handled irrespective of their meaning. This passes nearly unnoticed if we talk about 'squaring a matrix', though, which is the case of \"f\"(\"x\") = \"x\" and \"M\" an \"n\"×\"n\" matrix. The idea of a functional calculus is to create a \"principled\" approach to this kind of overloading of the notation. \n\nThe most immediate case is to apply polynomial functions to a square matrix, extending what has just been discussed. In the finite-dimensional case, the polynomial functional calculus yields quite a bit of information about the operator. For example, consider the family of polynomials which annihilates an operator \"T\". This family is an ideal in the ring of polynomials. Furthermore, it is a nontrivial ideal: let \"n\" be the finite dimension of the algebra of matrices, then {\"I\", \"T\", \"T\"...\"T\"} is linearly dependent. So ∑ \"α T\" = 0 for some scalars \"α\", not all equal to 0. This implies that the polynomial ∑ \"α x\" lies in the ideal. Since the ring of polynomials is a principal ideal domain, this ideal is generated by some polynomial \"m\". Multiplying by a unit if necessary, we can choose \"m\" to be monic. When this is done, the polynomial \"m\" is precisely the minimal polynomial of \"T\". This polynomial gives deep information about \"T\". For instance, a scalar \"α\" is an eigenvalue of \"T\" if and only if \"α\" is a root of \"m\". Also, sometimes \"m\" can be used to calculate the exponential of \"T\" efficiently. \n\nThe polynomial calculus is not as informative in the infinite-dimensional case. Consider the unilateral shift with the polynomials calculus; the ideal defined above is now trivial. Thus one is interested in functional calculi more general than polynomials. The subject is closely linked to spectral theory, since for a diagonal matrix or multiplication operator, it is rather clear what the definitions should be.\n\n"}
{"id": "22697854", "url": "https://en.wikipedia.org/wiki?curid=22697854", "title": "Hadamard's method of descent", "text": "Hadamard's method of descent\n\nIn mathematics, the method of descent is the term coined by the French mathematician Jacques Hadamard as a method for solving a partial differential equation in several real or complex variables, by regarding it as the specialisation of an equation in more variables, constant in the extra parameters. This method has been used to solve the wave equation, the heat equation and other versions of the Cauchy initial value problem.\n\nAs wrote:\n\n"}
{"id": "5850977", "url": "https://en.wikipedia.org/wiki?curid=5850977", "title": "Hidden semi-Markov model", "text": "Hidden semi-Markov model\n\nA hidden semi-Markov model (HSMM) is a statistical model with the same structure as a hidden Markov model except that the unobservable process is semi-Markov rather than Markov. This means that the probability of there being a change in the hidden state depends on the amount of time that has elapsed since entry into the current state. This is in contrast to hidden Markov models where there is a constant probability of changing state given survival in the state up to that time.\n\nFor instance modelled daily rainfall using a hidden semi-Markov model. If the underlying process (e.g. weather system) does not have a geometrically distributed duration, an HSMM may be more appropriate.\n\nThe model was first published by Leonard E. Baum and Ted Petrie in 1966.\n\nStatistical inference for hidden semi-Markov models is more difficult than in hidden Markov models, since algorithms like the Baum-Welch algorithm are not directly applicable, and must be adapted requiring more resources.\n\n\n\n\n"}
{"id": "8136831", "url": "https://en.wikipedia.org/wiki?curid=8136831", "title": "Hilbert basis (linear programming)", "text": "Hilbert basis (linear programming)\n\nThe Hilbert basis of a convex cone \"C\" is a minimal set of integer vectors such that every integer vector in \"C\" is a conical combination of the vectors in the Hilbert basis with integer coefficients.\n\nGiven a lattice formula_1 and a convex polyhedral cone with generators formula_2\n\nwe consider the monoid formula_4. By Gordan's lemma this monoid is finitely generated, i.e., there exists a finite set of lattice points formula_5 such that every lattice point formula_6 is an integer conical combination of these points:\n\nThe cone \"C\" is called pointed, if formula_8 implies formula_9. In this case there exists a unique minimal generating set of the monoid formula_4 - the Hilbert basis of \"C\". It is given by set of irreducible lattice points: An element formula_6 is called irreducible if it can not be written as the sum of two non-zero elements, i.e., formula_12 implies formula_13 or formula_14.\n\n"}
{"id": "21425767", "url": "https://en.wikipedia.org/wiki?curid=21425767", "title": "Hobbes–Wallis controversy", "text": "Hobbes–Wallis controversy\n\nThe Hobbes–Wallis controversy was a polemic debate that continued from the mid-1650s well into the 1670s, between the philosopher Thomas Hobbes and the mathematician John Wallis. It was sparked by \"De corpore\", a philosophical work by Hobbes in the general area of physics. The book contained not only a theory of mathematics subordinating it to geometry and geometry to kinematics, but a claimed proof of the squaring of the circle by Hobbes. While Hobbes retracted this particular proof, he returned to the topic with other attempted proofs. A pamphleteering exchange continued for decades. It drew in the newly formed Royal Society, and its experimental philosophy, to which Hobbes was (on principle) opposed.\n\nThe sustained nature of the exchanges can be attributed to several strands of the intellectual situation of the time. In mathematics there were open issues, namely the priority (pedagogic, or theoretical) to be assigned to geometry and algebra; and the status of algebra itself, which (from an English standpoint) had been pulled together by the text of William Oughtred, as more than a collection of symbolic abbreviations. Socially, the formation of the group of Royal Society members, and the status of the publication \"Philosophical Transactions\", was brought to a point as the quarrel proceeded, with Hobbes playing the outsider versus the self-selecting guild. \n\nHobbes was an easy target, on the ground chosen by Wallis. The failure of his attempts to solve the impossible problems he set himself were inevitable, but he neither backed down completely, nor applied adequate self-criticism. And on the level of character, Wallis was as intransigent as Hobbes was dogmatic, and this inflicted damage on both of their reputations. Quentin Skinner writes: \"There is no doubt that at the personal level Wallis behaved badly (as was widely conceded at the time).\"\n\nPart of the significance of the controversy is that Hobbes felt that, in the later stages, the Royal Society was in some way complicit in the attacks from Wallis, despite the fact that he had many friends as Fellows in it. This attitude presented one of the obstacles to Hobbes himself becoming a member, though not the only one.\n\nHobbes in \"Leviathan\" (1651) joined others in attacks on the existing Oxbridge academic system, essentially a monopoly in England of university teaching. These attacks, especially that of John Webster in \"Examen academiarum\", stung replies from Oxford professors. Wallis joined in, but the first wave of rebuttals came from other major names.\n\nThe issue of the universities was heavily loaded at the time, and the orthodox Presbyterian minister Thomas Hall lined up with \"Vindiciae literarum\" (1654). He had been arguing since \"The Pulpit Guarded\" (1651) that university learning was the bastion of defence against proliferating unorthodoxy and heresy. Webster had put the other side of the argument, in \"The Saints Guide\" (1653), casting doubt on the need for a university-educated clergy.\n\nIn 1654 Seth Ward (1617–1689), the Savilian Professor of Astronomy, replied in \"Vindiciae academiarum\" to the assaults. It was an anonymous publication of Ward and John Wilkins, but not intended to conceal its authorship (JohN WilkinS signed N.S. and SetH WarD signed H.D.). The agenda and tone for the controversy was first set by Ward when he launched a general attack on Hobbes. Wilkins wrote a preface to \"Vindiciae academiarum\"; the main text by Ward mentioned Hobbes, who was the particular target of an appendix. Ward claimed in both places that Hobbes had plagiarised Walter Warner. Before \"Leviathan\", Wilkins certainly was not hostile to Hobbes, and in fact wrote a Latin poem for the 1650 \"Humane Nature; or the Fundamental Elements of Policy\", an edition of part of the \"Elements of Law\" of Hobbes; and the preface to that book has been attributed to Ward. But the emergence of the full scope of the philosophy of Hobbes in \"Leviathan\" lost him allies who may have shared somewhat in his starting assumptions, but who felt a need to distance themselves from his conclusions, as Ward did in his \"Philosophicall Essay\" of 1652. Ward went on to make a full-dress attack on Hobbes the philosopher, the \"In Thomae Hobbii philosophiam exercitatio epistolica\" of 1656, dedicated to Wilkins.\n\nErrors in \"De Corpore\", in the mathematical sections, opened Hobbes to criticism also from John Wallis, Savilian Professor of Geometry. \n\nWallis's \"Elenchus geometriae Hobbianae\", published in 1655, contained an elaborate criticism of Hobbes's attempt to put the foundations of mathematical science in its place within knowledge. Hobbes had limited his interest to geometry, restricting the scope of mathematics. \n\nThe book was dedicated to John Owen, and in prefatory remarks Wallis (a Presbyterian) avows that his differences with Hobbes are largely rooted in theology. Hobbes himself wrote to Samuel de Sorbière in the same year, saying the controversy was not merely scientific. He regarded the use of infinite quantities as the thin end of the wedge for a return of scholasticism, and behind Wallis he saw \"all the Ecclesiastics of England\". Sorbière visited Wallis in Oxford; but his analysis of Wallis as stereotypical pedant helped not at all in the quarrel.\n\nHobbes took care to remove some mistakes exposed by Wallis, before allowing an English translation of the \"De Corpore\" to appear in 1656. But he still attacked Wallis in a series of \"Six Lessons to the Professors of Mathematics\", included with the \"De Corpore\" translation. Wallis defended himself, and re-confronted Hobbes with his mathematical inconsistencies. Hobbes responded with \"Marks of the Absurd Geometry, Rural Language, Scottish Church Politics, and Barbarisms of John Wallis, Professor of Geometry and Doctor of Divinity\". It has been suggested that Hobbes was still trying to cultivate John Owen at this point: Owen was both the leading Independent theologian and Cromwell's choice as Vice-Chancellor of Oxford, and Hobbes softened his critical line on the universities while stoking up the quarrel with Wallis. Further, the religious dimension (\"Scottish Church Politics\" refers to the Presbyterianism of Wallis, not shared by Owen) has been seen as a presage of later analysis of \"Behemoth\", the book Hobbes wrote in 1668 as a post-mortem on the English Revolution. The various thrusts were parried by Wallis in a reply (\"Hobbiani puncti dispunctio\", 1657).\n\nWallis published a comprehensive treatise on the general principles of calculus (\"Mathesis universalis\", 1657). Here he strongly advocated giving priority to the approach through arithmetic and algebra. This was quite contrary to the arguments of both Hobbes and Isaac Barrow. Hobbes set store on the \"demonstrable\" status of geometry, in the \"Six Lessons\". Jon Parkin writes:\n\nMathematicians sympathetic to Hobbes included François du Verdus and François Pelau, and some of his works were later translated into English for pedagogic use by Venterus Mandey; but he was not backed up by a \"school\". On the other side as critics were Claude Mylon, Laurence Rooke, Viscount Brouncker, John Pell, Christiaan Huyghens; much of the criticism Hobbes received was by private correspondence, or in the case of Pell direct contact. Henry Stubbe, later a vehement critic of the Royal Society, assured Hobbes in 1657 he had some (unnamed) supporters in Oxford.\n\nHobbes decided again to attack the new methods of mathematical analysis and by the spring of 1660, he had put his criticism and assertions into five dialogues under the title \"Examinatio et emendatio mathematicae hodiernae qualis explicatur in libris Johannis Wallisii\", with a sixth dialogue so called, consisting almost entirely of seventy or more propositions on the circle and cycloid. Wallis, however, would not take the bait.\n\nHobbes then tried another tack, having solved, as he thought, another ancient problem, the duplication of the cube. He had his solution brought out anonymously in French, so as to put his critics off the scent. He slipped in algebraic terms in early efforts, by cubing to the answer 2. While Hobbes would withdraw some arguments as erroneous, he distinguished between \"errors of negligence\" and \"errors of principle\", and found the latter much harder to admit. He was led to argue that the doctrine of \"n\"th roots in algebra (one contribution of Wallis) did not adequately model the geometric notions based on area and volume. René François Walter de Sluse walked through Hobbes's proof in one version, clearing the radicals to come down to a numerical assertion it implied (97,336 = 97,556), which could only be accepted as an approximation. Hobbes replied with an idiosyncratic appeal to a form of dimensional analysis, where algebraic quantities are non-dimensional. In general, his positions hardened after 1660.\n\nWallis publicly refuted the solution, but Hobbes claimed the credit of it. He republished it (in modified form), with his remarks, at the end of the 1661 \"Dialogus Physicus\".\n\nThe \"Dialogus physicus, sive De natura aeris\" attacked Robert Boyle and other friends of Wallis who were forming themselves into a society (incorporated as the Royal Society in 1662) for experimental research. The full Latin title of the book mentioned Gresham College as the experimental base of Boyle's group (see Gresham College and the formation of the Royal Society), followed immediately by a reference to the duplication of the cube, which in Hobbes's latest version was included as an appendix. Hobbes chose to take as the manifesto of the new academy Boyle's \"New Experiments touching the Spring of the Air\" (1660). Hobbes saw the whole approach as a direct contravention of the method of physical inquiry enjoined in the \"De Corpore\". He had reasoned out his own conclusions years before from speculative principles, and he warned them that if they were not content to begin where he had left off, their work would come to naught. This attack from Hobbes was one of several at the time: other opponents of Boyle were Franciscus Linus and Henry More. The issues at stake now had broadened out, and this was a choice Hobbes made, with their implications reaching beyond those of the first phase.\n\nTo Hobbes, Boyle replied himself, in the \"Examen of Mr T. Hobbes\", which appeared as an appendix to a second edition (1662) of the \"New Experiments\", along with an answer to Linus. But first Wallis was drawn in again, with the satire \"Hobbius heauton-timorumenos\" (1662). It included the accusation that Hobbes used purely verbal tactics, preferring his own semantics of a term such as \"air\", to cast doubt on the existence of a vacuum. \n\nHobbes reacted to personal attack by keeping aloof from scientific controversy for some years. He did write a letter about himself in the third person, \"Considerations upon the Reputation, Loyalty, Manners and Religion of Thomas Hobbes's\". In this biographical piece, he told his own and Wallis's \"little stories during the time of the late rebellion\". Wallis did not attempt a reply.\n\nHobbes never became a Fellow of the Royal Society, which was formally founded right at the time when the controversy drew in Boyle, and it has been debated why. Possible explanations are that he was difficult (cantakerous, even), and in other ways incompatible with the Society as club; or that the attacks by Wallis had successfully diminished his reputation, by showing that he was a lightweight in mathematics, part of a bigger polemic plan to show his thought generally as unoriginal, coming secondhand from others. Another simple explanation is that Hobbes was too \"controversial\" in the modern sense: he was excluded for reasons of image management.\n\nIt is possible that Hobbes's objections to academia extended to the Society. John Aubrey reports that Hobbes thought he had a small group of enemies there. Wallis, Ward and Wilkins were indeed key members of the early Royal Society, having been in the precursor group (\"Oxford Philosophical Club\") in Oxford.\n\nQuentin Skinner therefore proposed, in a 1969 paper \"Hobbes and the politics of the early Royal Society\", that small-group politics explained enough: those three kept Hobbes out of the Royal Society at the start; and that his continuing absence is sufficiently explained by Hobbes's resentment at such treatment. Certainly Hobbes took it badly that Wallis could use the \"Philosophical Transactions\" to publish his critical views, for example in a review of Hobbes's \"Rosetum geometricum\", and complained about this in 1672 to Henry Oldenburg. \n\nRecent scholarly explanations are more complex. It is argued by Noel Malcolm that the general position of Hobbes, in 'mechanistic philosophy', was close enough to that current in the Royal Society to be compatible (even given the debate with Boyle), but that his reputation from the political and religious side made him untouchable, and the Society kept him at arm's length for that reason.\n\nAfter a time Hobbes began a further period of controversial activity, which he dragged out until his ninetieth year. The first piece, published in 1666, \"De principiis et ratiocinatione geometrarum\", was an attack on geometry professors. Three years later he brought his three mathematical achievements together in \"Quadratura circuli, Cubatio sphaerae, Duplicitio cubii\", and as soon as they were once more refuted by Wallis, reprinted them with an answer to the objections. Wallis, who had promised to leave him alone, refuted him again before the year was out. The exchange dragged on through numerous other papers until 1678.\n\n\n\n\n\n"}
{"id": "44132789", "url": "https://en.wikipedia.org/wiki?curid=44132789", "title": "Hyperbolic geometric graph", "text": "Hyperbolic geometric graph\n\nA hyperbolic geometric graph (HGG) or hyperbolic geometric network (HGN) is a special type of spatial network where (1) latent coordinates of nodes are sprinkled according to a probability density function into a\nhyperbolic space of constant negative curvature and (2) an edge between two nodes is present if they are close according to a function of the metric (typically either a Heaviside step function resulting in deterministic connections between vertices closer than a certain threshold distance, or a decaying function of hyperbolic distance yielding the connection probability). A HGG generalizes a random geometric graph (RGG) whose embedding space is Euclidean.\n\nMathematically, a HGG is a graph formula_1 with a vertex set \"V\" (cardinality formula_2) and a edge set \"E\" constructed by considering the nodes as points placed onto a 2-dimensional hyperbolic space formula_3 of constant negative Gaussian curvature, formula_4 and cut-off radius formula_5, i.e. the radius of the Poincaré disk which can be visualized using a hyperboloid model.\nEach point formula_6 has hyperbolic polar coordinates formula_7 with formula_8 and formula_9.\n\nThe hyperbolic law of cosines allows to measure the distance formula_10 between two points formula_6 and formula_12,\nThe angle formula_15 is the (smallest) angle between the two\nposition vectors.\n\nIn the simplest case, an edge formula_16 is established iff (if and only if) two nodes are within a certain \"neighborhood radius\" formula_17, formula_18, this corresponds to an influence threshold.\n\nIn general, a link will be established with a probability depending on the distance formula_10. \nA \"connectivity decay function\" formula_20 represents the probability of assigning an edge to a pair of nodes at distance formula_21. \nIn this framework, the simple case of \"hard-code\" neighborhood like in random geometric graphs is referred to as \"truncation decay function\".\n\nFor formula_22 (Gaussian curvature formula_23), HGGs form an ensemble of networks for which is possible to express the degree distribution analytically as closed form for the limiting case of large number of nodes. This is worth mentioning since this is not true for many ensembles of graphs.\n\nHGGs have been suggested as promising model for social networks where the hyperbolicity appears through a competition between \"similarity\" and \"popularity\" of an individual.\n"}
{"id": "20110824", "url": "https://en.wikipedia.org/wiki?curid=20110824", "title": "Infinity", "text": "Infinity\n\nInfinity (symbol: ) is a concept describing something without any bound or larger than any natural number. Philosophers have speculated about the nature of the infinite, for example Zeno of Elea, who proposed many paradoxes involving infinity, and Eudoxus of Cnidus, who used the idea of infinitely small quantities in his method of exhaustion. Modern mathematics uses the general concept of infinity in the solution of many practical and theoretical problems, such as in calculus and set theory, and the idea is also used in physics and the other sciences.\n\nIn mathematics, \"infinity\" is often treated as a number (i.e., it counts or measures things: \"an infinite number of terms\") but it is not the same sort of number as either a natural or a real number.\n\nGeorg Cantor formalized many ideas related to infinity and infinite sets during the late 19th and early 20th centuries. In the theory he developed, there are infinite sets of different sizes (called cardinalities). For example, the set of integers is countably infinite, while the infinite set of real numbers is uncountable.\n\nAncient cultures had various ideas about the nature of infinity. The ancient Indians and Greeks did not define infinity in precise formalism as does modern mathematics, and instead approached infinity as a philosophical concept.\n\nThe earliest recorded idea of infinity comes from Anaximander, a pre-Socratic Greek philosopher who lived in Miletus. He used the word apeiron which means infinite or limitless. However, the earliest attestable accounts of mathematical infinity come from Zeno of Elea (born ), a pre-Socratic Greek philosopher of southern Italy and member of the Eleatic School founded by Parmenides. Aristotle called him the inventor of the dialectic. He is best known for his paradoxes, described by Bertrand Russell as \"immeasurably subtle and profound\".\n\nIn accordance with the traditional view of Aristotle, the Hellenistic Greeks generally preferred to distinguish the potential infinity from the actual infinity; for example, instead of saying that there are an infinity of primes, Euclid prefers instead to say that there are more prime numbers than contained in any given collection of prime numbers.\n\nHowever, recent readings of the Archimedes Palimpsest have found that Archimedes had an understanding about actual infinite quantities. According to \"Nonlinear Dynamic Systems and Controls\", Archimedes has been found to be \"the first to rigorously address the science of infinity with infinitely large sets using precise mathematical proofs.\"\n\nThe Jain mathematical text Surya Prajnapti (c. 4th–3rd century BCE) classifies all numbers into three sets: enumerable, innumerable, and infinite. Each of these was further subdivided into three orders:\n\nIn this work, two basic types of infinite numbers are distinguished. On both physical and ontological grounds, a distinction was made between (\"countless, innumerable\") and \"ananta\" (\"endless, unlimited\"), between rigidly bounded and loosely bounded infinities.\n\nEuropean mathematicians started using infinite numbers and expressions in a systematic fashion in the 17th century. In 1655 John Wallis first used the notation formula_1 for such a number in his \"De sectionibus conicis\" and exploited it in area calculations by dividing the region into infinitesimal strips of width on the order of formula_2 But in \"Arithmetica infinitorum\" (1655 also) he indicates infinite series, infinite products and infinite continued fractions by writing down a few terms or factors and then appending \"&c.\" For example, \"1, 6, 12, 18, 24, &c.\"\n\nIn 1699 Isaac Newton wrote about equations with an infinite number of terms in his work \"De analysi per aequationes numero terminorum infinitas\".\n\nHermann Weyl opened a mathematico-philosophic address given in 1930 with:\n\nThe infinity symbol formula_1 (sometimes called the lemniscate) is a mathematical symbol representing the concept of infinity. The symbol is encoded in Unicode at and in LaTeX as codice_1.\n\nIt was introduced in 1655 by John Wallis, and, since its introduction, has also been used outside mathematics in modern mysticism and literary symbology.\n\nLeibniz, one of the co-inventors of infinitesimal calculus, speculated widely about infinite numbers and their use in mathematics. To Leibniz, both infinitesimals and infinite quantities were ideal entities, not of the same nature as appreciable quantities, but enjoying the same properties in accordance with the Law of Continuity.\n\nIn real analysis, the symbol formula_1, called \"infinity\", is used to denote an unbounded limit. The notation formula_5 means that \"x\" grows without bound, and formula_6 means that  \"x\" decreases without bound. If \"f\"(\"t\") ≥ 0 for every \"t\", then\n\nInfinity is also used to describe infinite series:\n\nInfinity can be used not only to define a limit but as a value in the extended real number system. Points labeled formula_16 and formula_17 can be added to the topological space of the real numbers, producing the two-point compactification of the real numbers. Adding algebraic properties to this gives us the extended real numbers. We can also treat formula_16 and formula_17 as the same, leading to the one-point compactification of the real numbers, which is the real projective line. Projective geometry also refers to a line at infinity in plane geometry, a plane at infinity in three-dimensional space, and a hyperplane at infinity for general dimensions, each consisting of points at infinity.\n\nIn complex analysis the symbol formula_1, called \"infinity\", denotes an unsigned infinite limit. formula_5 means that the magnitude formula_22 of \"x\" grows beyond any assigned value. A point labeled formula_1 can be added to the complex plane as a topological space giving the one-point compactification of the complex plane. When this is done, the resulting space is a one-dimensional complex manifold, or Riemann surface, called the extended complex plane or the Riemann sphere. Arithmetic operations similar to those given above for the extended real numbers can also be defined, though there is no distinction in the signs (therefore one exception is that infinity cannot be added to itself). On the other hand, this kind of infinity enables division by zero, namely formula_24 for any nonzero complex number \"z\". In this context it is often useful to consider meromorphic functions as maps into the Riemann sphere taking the value of formula_1 at the poles. The domain of a complex-valued function may be extended to include the point at infinity as well. One important example of such functions is the group of Möbius transformations.\n\nThe original formulation of infinitesimal calculus by Isaac Newton and Gottfried Leibniz used infinitesimal quantities. In the twentieth century, it was shown that this treatment could be put on a rigorous footing through various logical systems, including smooth infinitesimal analysis and nonstandard analysis. In the latter, infinitesimals are invertible, and their inverses are infinite numbers. The infinities in this sense are part of a hyperreal field; there is no equivalence between them as with the Cantorian transfinites. For example, if H is an infinite number, then H + H = 2H and H + 1 are distinct infinite numbers. This approach to non-standard calculus is fully developed in .\n\nA different form of \"infinity\" are the ordinal and cardinal infinities of set theory. Georg Cantor developed a system of transfinite numbers, in which the first transfinite cardinal is aleph-null (ℵ), the cardinality of the set of natural numbers. This modern mathematical conception of the quantitative infinite developed in the late nineteenth century from work by Cantor, Gottlob Frege, Richard Dedekind and others, using the idea of collections, or sets.\n\nDedekind's approach was essentially to adopt the idea of one-to-one correspondence as a standard for comparing the size of sets, and to reject the view of Galileo (which derived from Euclid) that the whole cannot be the same size as the part (however, see Galileo's paradox where he concludes that positive integers which are squares and all positive integers are the same size). An infinite set can simply be defined as one having the same size as at least one of its proper parts; this notion of infinity is called Dedekind infinite. The diagram gives an example: viewing lines as infinite sets of points, the left half of the lower blue line can be mapped in a one-to-one manner (green correspondences) to the higher blue line, and, in turn, to the whole lower blue line (red correspondences); therefore the whole lower blue line and its left half have the same cardinality, i.e. \"size\".\n\nCantor defined two kinds of infinite numbers: ordinal numbers and cardinal numbers. Ordinal numbers may be identified with well-ordered sets, or counting carried on to any stopping point, including points after an infinite number have already been counted. Generalizing finite and the ordinary infinite sequences which are maps from the positive integers leads to mappings from ordinal numbers, and transfinite sequences. Cardinal numbers define the size of sets, meaning how many members they contain, and can be standardized by choosing the first ordinal number of a certain size to represent the cardinal number of that size. The smallest ordinal infinity is that of the positive integers, and any set which has the cardinality of the integers is countably infinite. If a set is too large to be put in one to one correspondence with the positive integers, it is called \"uncountable\". Cantor's views prevailed and modern mathematics accepts actual infinity. Certain extended number systems, such as the hyperreal numbers, incorporate the ordinary (finite) numbers and infinite numbers of different sizes.\n\nOne of Cantor's most important results was that the cardinality of the continuum formula_26 is greater than that of the natural numbers formula_27; that is, there are more real numbers R than natural numbers N. Namely, Cantor showed that formula_28 (see Cantor's diagonal argument or Cantor's first uncountability proof).\n\nThe continuum hypothesis states that there is no cardinal number between the cardinality of the reals and the cardinality of the natural numbers, that is, formula_29 (see Beth one). This hypothesis can neither be proved nor disproved within the widely accepted Zermelo–Fraenkel set theory, even assuming the Axiom of Choice.\n\nCardinal arithmetic can be used to show not only that the number of points in a real number line is equal to the number of points in any segment of that line, but that this is equal to the number of points on a plane and, indeed, in any finite-dimensional space.\nThe first of these results is apparent by considering, for instance, the tangent function, which provides a one-to-one correspondence between the interval (−π/2, π/2) and R (see also Hilbert's paradox of the Grand Hotel). The second result was proved by Cantor in 1878, but only became intuitively apparent in 1890, when Giuseppe Peano introduced the space-filling curves, curved lines that twist and turn enough to fill the whole of any square, or cube, or hypercube, or finite-dimensional space. These curves can be used to define a one-to-one correspondence between the points on one side of a square and the points in the square.\n\nInfinite-dimensional spaces are widely used in geometry and topology, particularly as classifying spaces, such as Eilenberg−MacLane spaces. Common examples are the infinite-dimensional complex projective space K(Z,2) and the infinite-dimensional real projective space K(Z/2Z,1).\n\nThe structure of a fractal object is reiterated in its magnifications. Fractals can be magnified indefinitely without losing their structure and becoming \"smooth\"; they have infinite perimeters—some with infinite, and others with finite surface areas. One such fractal curve with an infinite perimeter and finite surface area is the Koch snowflake.\n\nLeopold Kronecker was skeptical of the notion of infinity and how his fellow mathematicians were using it in the 1870s and 1880s. This skepticism was developed in the philosophy of mathematics called finitism, an extreme form of mathematical philosophy in the general philosophical and mathematical schools of constructivism and intuitionism.\n\nIn physics, approximations of real numbers are used for continuous measurements and natural numbers are used for discrete measurements (i.e. counting). It is therefore assumed by physicists that no measurable quantity could have an infinite value, for instance by taking an infinite value in an extended real number system, or by requiring the counting of an infinite number of events. It is, for example, presumed impossible for any type of body to have infinite mass or infinite energy. Concepts of infinite things such as an infinite plane wave exist, but there are no experimental means to generate them.\n\nThe practice of refusing infinite values for measurable quantities does not come from \"a priori\" or ideological motivations, but rather from more methodological and pragmatic motivations . One of the needs of any physical and scientific theory is to give usable formulas that correspond to or at least approximate reality. As an example, if any object of infinite gravitational mass were to exist, any usage of the formula to calculate the gravitational force would lead to an infinite result, which would be of no benefit since the result would be always the same regardless of the position and the mass of the other object. The formula would be useful neither to compute the force between two objects of finite mass nor to compute their motions. If an infinite mass object were to exist, any object of finite mass would be attracted with infinite force (and hence acceleration) by the infinite mass object, which is not what we can observe in reality. Sometimes infinite result of a physical quantity may mean that the theory being used to compute the result may be approaching the point where it fails. This may help to indicate the limitations of a theory.\n\nThis point of view does not mean that infinity cannot be used in physics. For convenience's sake, calculations, equations, theories and approximations often use infinite series, unbounded functions, etc., and may involve infinite quantities. Physicists however require that the end result be physically meaningful. In quantum field theory infinities arise which need to be interpreted in such a way as to lead to a physically meaningful result, a process called renormalization.\n\nHowever, there are some theoretical circumstances where the end result is infinity. One example is the singularity in the description of black holes. Some solutions of the equations of the general theory of relativity allow for finite mass distributions of zero size, and thus infinite density. This is an example of what is called a mathematical singularity, or a point where a physical theory breaks down. This does not necessarily mean that physical infinities exist; it may mean simply that the theory is incapable of describing the situation properly. Two other examples occur in inverse-square force laws of the gravitational force equation of Newtonian gravity and Coulomb's law of electrostatics. At r=0 these equations evaluate to infinities.\n\nThe first published proposal that the universe is infinite came from Thomas Digges in 1576. Eight years later, in 1584, the Italian philosopher and astronomer Giordano Bruno proposed an unbounded universe in \"On the Infinite Universe and Worlds\": \"Innumerable suns exist; innumerable earths revolve around these suns in a manner similar to the way the seven planets revolve around our sun. Living beings inhabit these worlds.\"\n\nCosmologists have long sought to discover whether infinity exists in our physical universe: Are there an infinite number of stars? Does the universe have infinite volume? Does space \"go on forever\"? This is an open question of cosmology. The question of being infinite is logically separate from the question of having boundaries. The two-dimensional surface of the Earth, for example, is finite, yet has no edge. By travelling in a straight line with respect to the Earth's curvature one will eventually return to the exact spot one started from. The universe, at least in principle, might have a similar topology. If so, one might eventually return to one's starting point after travelling in a straight line through the universe for long enough.\n\nThe curvature of the universe can be measured through multipole moments in the spectrum of the cosmic background radiation. As to date, analysis of the radiation patterns recorded by the WMAP spacecraft hints that the universe has a flat topology. This would be consistent with an infinite physical universe.\n\nHowever, the universe could be finite, even if its curvature is flat. An easy way to understand this is to consider two-dimensional examples, such as video games where items that leave one edge of the screen reappear on the other. The topology of such games is toroidal and the geometry is flat. Many possible bounded, flat possibilities also exist for three-dimensional space.\n\nThe concept of infinity also extends to the multiverse hypothesis, which, when explained by astrophysicists such as Michio Kaku, posits that there are an infinite number and variety of universes.\n\nIn logic an infinite regress argument is \"a distinctively philosophical kind of argument purporting to show that a thesis is defective because it generates an infinite series when either (form A) no such series exists or (form B) were it to exist, the thesis would lack the role (e.g., of justification) that it is supposed to play.\"\n\nThe IEEE floating-point standard (IEEE 754) specifies the positive and negative infinity values (and also indefinite values). These are defined as the result of arithmetic overflow, division by zero, and other exceptional operations.\n\nSome programming languages, such as Java and J, allow the programmer an explicit access to the positive and negative infinity values as language constants. These can be used as greatest and least elements, as they compare (respectively) greater than or less than all other values. They have uses as sentinel values in algorithms involving sorting, searching, or windowing.\n\nIn languages that do not have greatest and least elements, but do allow overloading of relational operators, it is possible for a programmer to \"create\" the greatest and least elements. In languages that do not provide explicit access to such values from the initial state of the program, but do implement the floating-point data type, the infinity values may still be accessible and usable as the result of certain operations.\n\nPerspective artwork utilizes the concept of vanishing points, roughly corresponding to mathematical points at infinity, located at an infinite distance from the observer. This allows artists to create paintings that realistically render space, distances, and forms. Artist M. C. Escher is specifically known for employing the concept of infinity in his work in this and other ways.\n\nVariations of chess played on an unbounded board are called infinite chess.\n\nCognitive scientist George Lakoff considers the concept of infinity in mathematics and the sciences as a metaphor. This perspective is based on the basic metaphor of infinity (BMI), defined as the ever-increasing sequence <1,2,3...>.\n\nThe symbol is often used romantically to represent eternal love. Several types of jewelry are fashioned into the infinity shape for this purpose.\n\n\n\n\n\n"}
{"id": "6736044", "url": "https://en.wikipedia.org/wiki?curid=6736044", "title": "Julius Richard Büchi", "text": "Julius Richard Büchi\n\nJulius Richard Büchi (1924–1984) was a Swiss logician and mathematician.\n\nHe received his Dr. sc. nat. in 1950 at the ETH Zürich under supervision of Paul Bernays and Ferdinand Gonseth. Shortly afterwards he went to Purdue University, Lafayette, Indiana. He and his first student Lawrence Landweber had a major influence on the development of theoretical computer science.\n\nTogether with his friend Saunders Mac Lane, a student of Paul Bernays as well, Büchi published numerous celebrated works. He invented what is now known as the Büchi automaton, a finite state automaton accepting certain collections of infinite words known as omega-regular languages. The \"\"n\" squares' problem\", known also as Büchi's problem, is an open problem from number theory, closely related to Hilbert's tenth problem.\n\n"}
{"id": "10375543", "url": "https://en.wikipedia.org/wiki?curid=10375543", "title": "LLT polynomial", "text": "LLT polynomial\n\nIn mathematics, an LLT polynomial is one of a family of symmetric functions introduced by Alain Lascoux, Bernard Leclerc, and Jean-Yves Thibon (1997) as \"q\"-analogues of products of Schur functions.\n\nJ. Haglund, M. Haiman, N. Loehr (2005) showed how to expand Macdonald polynomials in terms of LLT polynomials. Ian Grojnowski and Mark Haiman (preprint) proved a positivity conjecture for LLT polynomials that combined with the previous result implies the Macdonald positivity conjecture for Macdonald polynomials, and extended the definition of LLT polynomials to arbitrary finite root systems.\n\n"}
{"id": "34676009", "url": "https://en.wikipedia.org/wiki?curid=34676009", "title": "LP-type problem", "text": "LP-type problem\n\nIn the study of algorithms, an LP-type problem (also called a generalized linear program) is an optimization problem that shares certain properties with low-dimensional linear programs and that may be solved by similar algorithms. LP-type problems include many important optimization problems that are not themselves linear programs, such as the problem of finding the smallest circle containing a given set of planar points. They may be solved by a combination of randomized algorithms in an amount of time that is linear in the number of elements defining the problem, and subexponential in the dimension of the problem.\n\nLP-type problems were defined by as problems in which one is given as input a finite set of elements, and a function that maps subsets of to values from a totally ordered set. The function is required to satisfy two key properties:\n\nA \"basis\" of an LP-type problem is a set with the property that every proper subset of has a smaller value of than itself, and the \"dimension\" (or \"combinatorial dimension\") of an LP-type problem is defined to be the maximum cardinality of a basis.\n\nIt is assumed that an optimization algorithm may evaluate the function only on sets that are themselves bases or that are formed by adding a single element to a basis. Alternatively, the algorithm may be restricted to two primitive operations: a \"violation test\" that determines, for a basis and an element whether , and a \"basis computation\" that (with the same inputs) finds a basis of }. The task for the algorithm to perform is to evaluate by only using these restricted evaluations or primitives.\n\nA linear program may be defined by a system of non-negative real variables, subject to linear inequality constraints, together with a non-negative linear objective function to be minimized. This may be placed into the framework of LP-type problems by letting be the set of constraints, and defining (for a subset of the constraints) to be the minimum objective function value of the smaller linear program defined by . With suitable general position assumptions (in order to prevent multiple solution points having the same optimal objective function value), this satisfies the monotonicity and locality requirements of an LP-type problem, and has combinatorial dimension equal to the number of variables. Similarly, an integer program (consisting of a collection of linear constraints and a linear objective function, as in a linear program, but with the additional restriction that the variables must take on only integer values) satisfies both the monotonicity and locality properties of an LP-type problem, with the same general position assumptions as for linear programs. Theorems of and show that, for an integer program with variables, the combinatorial dimension is at most .\n\nMany natural optimization problems in computational geometry are LP-type:\n\n\nLP-type problems have also been used to determine the optimal outcomes of certain games in algorithmic game theory, improve vertex placement in finite element method meshes, solve facility location problems, analyze the time complexity of certain exponential-time search algorithms, and reconstruct the three-dimensional positions of objects from their two-dimensional images.\n\n gave an algorithm for low-dimensional linear programming that may be adapted to the LP-type problem framework. Seidel's algorithm takes as input the set and a separate set (initially empty) of elements known to belong to the optimal basis. It then considers the remaining elements one-by-one in a random order, performing violation tests for each one and, depending on the result, performing a recursive call to the same algorithm with a larger set of known basis elements. It may be expressed with the following pseudocode:\n\nIn a problem with combinatorial dimension , the violation test in the th iteration of the algorithm fails only when is one of the remaining basis elements, which happens with probability at most . Based on this calculation, it can be shown that overall the expected number of violation tests performed by the algorithm is , linear in but worse than exponential in .\n\n defines two algorithms, a recursive algorithm and an iterative algorithm, for linear programming based on random sampling techniques, and suggests a combination of the two that calls the iterative algorithm from the recursive algorithm. The recursive algorithm repeatedly chooses random samples whose size is approximately the square root of the input size, solves the sampled problem recursively, and then uses violation tests to find a subset of the remaining elements that must include at least one basis element:\n\nIn each iteration, the expected size of is , and whenever is nonempty it includes at least one new element of the eventual basis of . Therefore, the algorithm performs at most iterations, each of which performs violation tests and makes a single recursive call to a subproblem of size .\n\nClarkson's iterative algorithm assigns weights to each element of , initially all of them equal. It then chooses a set of elements from at random, and computes the sets and as in the previous algorithm. If the total weight of is at most times the total weight of (as happens with constant probability) then the algorithm doubles the weights of every element of , and as before it repeats this process until becomes empty. In each iteration, the weight of the optimal basis can be shown to increase at a greater rate than the total weight of , from which it follows that the algorithm must terminate within iterations.\n\nBy using the recursive algorithm to solve a given problem, switching to the iterative algorithm for its recursive calls, and then switching again to Seidel's algorithm for the calls made by the iterative algorithm, it is possible solve a given LP-type problem using violation tests.\n\nWhen applied to a linear program, this algorithm can be interpreted as being a dual simplex method. With certain additional computational primitives beyond the violation test and basis computation primitives, this method can be made deterministic.\n\n describe an algorithm that uses an additional property of linear programs that is not always held by other LP-type problems, that all bases have the same cardinality of each other. If an LP-type problem does not have this property, it can be made to have it by adding new dummy elements and by modifying the function to return the ordered pair of its old value and of the number , ordered lexicographically.\n\nRather than adding elements of \"S\" one at a time, or finding samples of the elements, describe an algorithm that removes elements one at a time. At each step it maintains a basis that may initially be the set of dummy elements. It may be described with the following pseudocode:\n\nIn most of the recursive calls of the algorithm, the violation test succeeds and the if statement is skipped. However, with a small probability the violation test fails and the algorithm makes an additional basis computation and then an additional recursive call. As the authors show, the expected time for the algorithm is linear in \"n\" and exponential in the square root of . By combining this method with Clarkson's recursive and iterative procedures, these two forms of time dependence can be separated out from each other, resulting in an algorithm that performs O(\"dn\") violation tests in the outer recursive algorithm and a number that is exponential in the square root of in the lower levels of the algorithm.\n\n considers a variation of LP-type optimization problems in which one is given, together with the set and the objective function , a number ; the task is to remove elements from in order to make the objective function on the remaining set as small as possible. For instance, when applied to the smallest circle problem, this would give the smallest circle that contains all but of a given set of planar points. He shows that, for all non-degenerate LP-type problems (that is, problems in which all bases have distinct values) this problem may be solved in time , by solving a set of LP-type problems defined by subsets of .\n\nSome geometric optimization problems may be expressed as LP-type problems in which the number of elements in the LP-type formulation is significantly greater than the number of input data values for the optimization problem. As an example, consider a collection of points in the plane, each moving with constant velocity. At any point in time, the diameter of this system is the maximum distance between two of its points. The problem of finding a time at which the diameter is minimized can be formulated as minimizing the pointwise maximum of quasiconvex functions, one for each pair of points, measuring the Euclidean distance between the pair as a function of time. Thus, it can be solved as an LP-type problem of combinatorial dimension two on a set of elements, but this set is significantly larger than the number of input points.\n\nChan's algorithm performs the following steps:\n\nWith the assumption that the decision algorithm takes an amount of time that grows at least polynomially as a function of the input size , Chan shows that the threshold for switching to an explicit LP formulation and the number of subsets in the partition can be chosen in such a way that the implicit LP-type optimization algorithm also runs in time .\n\nFor instance, for the minimum diameter of moving points, the decision algorithm needs only to calculate the diameter of a set of points at a fixed time, a problem that can be solved in time using the rotating calipers technique. Therefore, Chan's algorithm for finding the time at which the diameter is minimized also takes time .\nChan uses this method to find a point of maximal Tukey depth among a given collection of points in -dimensional Euclidean space, in time . A similar technique was used by to find a point of maximal Tukey depth for the uniform distribution on a convex polygon.\n\nThe discovery of linear time algorithms for linear programming and the observation that the same algorithms could in many cases be used to solve geometric optimization problems that were not linear programs goes back at least to , who gave a linear expected time algorithm for both three-variable linear programs and the smallest circle problem. However, Megiddo formulated the generalization of linear programming geometrically rather than combinatorially, as a convex optimization problem rather than as an abstract problem on systems of sets. Similarly, and Clarkson (in the 1988 conference version of ) observed that their methods could be applied to convex programs as well as linear programs. showed that the minimum enclosing ellipsoid problem could also be formulated as a convex optimization problem by adding a small number of non-linear constraints. The use of randomization to improve the time bounds for low dimensional linear programming and related problems was pioneered by Clarkson and by .\n\nThe definition of LP-type problems in terms of functions satisfying the axioms of locality and monotonicity is from , but other authors in the same timeframe formulated alternative combinatorial generalizations of linear programs. For instance, in a framework developed by , the function is replaced by a total ordering on the subsets of . It is possible to break the ties in an LP-type problem to create a total order, but only at the expense of an increase in the combinatorial dimension.\nAdditionally, as in LP-type problems, Gärtner defines certain primitives for performing computations on subsets of elements; however, his formalization does not have an analogue of the combinatorial dimension.\n\nAnother abstract generalization of both linear programs and linear complementarity problems, formulated by and later studied by several other authors, concerns orientations of the edges of a hypercube with the property that every face of the hypercube (including the whole hypercube as a face) has a unique sink, a vertex with no outgoing edges. An orientation of this type may be formed from an LP-type problem by corresponding the subsets of \"S\" with the vertices of a hypercube in such a way that two subsets differ by a single element if and only if the corresponding vertices are adjacent, and by orienting the edge between neighboring sets towards if and towards otherwise. The resulting orientation has the additional property that it forms a directed acyclic graph, from which it can be shown that a randomized algorithm can find the unique sink of the whole hypercube (the optimal basis of the LP-type problem) in a number of steps exponential in the square root of .\n\nThe more recently developed framework of violator spaces generalizes LP-type problems, in the sense that every LP-type problem can be modeled by a violator space but not necessarily vice versa. Violator spaces are defined similarly to LP-type problems, by a function that maps sets to objective function values, but the values of are not ordered. Despite the lack of ordering, every set has a well-defined set of bases (the minimal sets with the same value as the whole set) that can be found by variations of Clarkson's algorithms for LP-type problems. Indeed, violator spaces have been shown to exactly characterize the systems that can be solved by Clarkson's algorithms.\n\n"}
{"id": "17216567", "url": "https://en.wikipedia.org/wiki?curid=17216567", "title": "Limiting density of discrete points", "text": "Limiting density of discrete points\n\nIn information theory, the limiting density of discrete points is an adjustment to the formula of Claude Shannon for differential entropy.\n\nIt was formulated by Edwin Thompson Jaynes to address defects in the initial definition of differential entropy.\n\nShannon originally wrote down the following formula for the entropy of a continuous distribution, known as differential entropy:\nUnlike Shannon's formula for the discrete entropy, however, this is not the result of any derivation (Shannon simply replaced the summation symbol in the discrete version with an integral) and it turns out to lack many of the properties that make the discrete entropy a useful measure of uncertainty. In particular, it is not invariant under a change of variables and can even become negative. In addition, it is not even dimensionally correct. Since formula_2 would be dimensionless, formula_3 must have units of formula_4, which means that the argument to the logarithm is not dimensionless as required.\n\nJaynes (1963, 1968) argued that the formula for the continuous entropy should be derived by taking the limit of increasingly dense discrete distributions. Suppose that we have a set of formula_5 discrete points formula_6, such that in the limit formula_7 their density approaches a function formula_8 called the \"invariant measure\". \n\nTypically, when this is written, the term formula_10 is omitted, as that would typically not be finite. So the actual common definition is\n\nWhere it is unclear whether or not the formula_10 term should be omitted, one could write\n\nNotice that in Jaynes' formula, formula_14 is a probability density. It is clear that for any finite formula_15 that formula_14 is simply a uniform density over the quantization of the continuous space that is used in the Riemann sum. In the limit, formula_14 is the continuous limiting density of points in the quantization used to represent the continuous variable formula_18.\n\nSuppose one had a number format that took on formula_15 possible values, distributed as per formula_14. Then formula_21 (if formula_5 is large enough that the continuous approximation is valid) is the discrete entropy of the variable formula_18 in this encoding. This is equal to the average number of bits required to transmit this information, and is no more than formula_24. Therefore, formula_25 may be thought of as the amount of information gained by knowing that the variable formula_18 follows the distribution formula_3, and is not uniformly distributed over the possible quantized values, as would be the case if it followed formula_14. It should come as no surprise that formula_29 is actually the (negative) Kullback–Leibler divergence from formula_8 to formula_3, which is thought of as the information gained by learning that a variable previously thought to be distributed as formula_14 is actually distributed as formula_3.\n\nJaynes' continuous entropy formula has the property of being invariant under a change of variables, provided that formula_8 and formula_35 are transformed in the same way. (This motivates the name \"invariant measure\" for \"m\".) This solves many of the difficulties that come from applying Shannon's continuous entropy formula. Jaynes himself dropped the formula_10 term as it was not relevant to his work (maximum entropy distributions), and it is somewhat awkward to have an infinite term in the calculation. Unfortunately, this cannot be helped if the quantization is made arbitrarily fine, as would be the case in the continuous limit. Note that formula_25 as defined here (without the formula_24 term) would always be non-positive, because a KL divergence would always be non-negative.\n\nIf it is the case that formula_14 is constant over some interval of size formula_40, and formula_3 is essentially zero outside that interval, then the limiting density of discrete points (LDDP) is closely related to the differential entropy formula_42\n"}
{"id": "58986227", "url": "https://en.wikipedia.org/wiki?curid=58986227", "title": "Lis Brack-Bernsen", "text": "Lis Brack-Bernsen\n\nLis Brack-Bernsen (born 1946) is a Danish and Swiss mathematician, historian of science, and historian of mathematics, known for her work on Babylonian astronomy. She is an extraordinary professor of the history of science at the University of Regensburg.\n\nBrack-Bernsen was born in Copenhagen on March 2, 1946. She earned a diploma in mathematics with a minor in physics from the University of Copenhagen in 1970, with Olaf Schmidt as a mentor, \nand completed her Ph.D. in the history of mathematics in 1974 at the University of Basel, also with studies at Stony Brook University. Her dissertation was \"Die Basler Mayatafeln; astronomische Deutung der Inschriften auf den Türstürzen 2 und 3 aus Tempel IV in Tikal\", and was promoted by J. O. Fleckenstein.\n\nShe worked as a lecturer at the University of Copenhagen for 1974–1975, as a researcher at Stony Brook University from 1975 to 1977, and as a researcher in Grenoble and Regensburg from 1977 to 1979. However, at this time she left research to raise a family.\n\nIn 1997 she completed her habilitation at Goethe University Frankfurt. She worked as a \"privatdozentin\" at Goethe University until 1999, when she moved to the University of Regensburg.\n\nBrack-Bernsen was elected to the Academy of Sciences Leopoldina in 2009.\n\nA festschrift, \"Studies on the Ancient Exact Sciences in Honour of Lis Brack-Bernsen\"\n(John Steele and Mathieu Ossendrijver, eds.) was published by Edition Topoi in 2017.\n\n"}
{"id": "51318078", "url": "https://en.wikipedia.org/wiki?curid=51318078", "title": "Loss development factor", "text": "Loss development factor\n\nLoss development factors or LDFs are used in insurance pricing and reserving to adjust claims to their projected ultimate level. Insurance claims, especially in long-tailed lines such as liability insurance, are often not paid out immediately. Claims adjusters set initial case reserves for claims; however, it is often impossible to predict immediately what the final amount of an insurance claim will be, due to uncertainty around defense costs, settlement amounts, and trial outcomes (in addition to several other factors). Loss development factors are used by actuaries, underwriters, and other insurance professionals to \"develop\" claim amounts to their estimated final value. Ultimate loss amounts are necessary for determining an insurance company's carried reserves. They are also useful for determining adequate insurance premiums, when loss experience is used as a rating factor\n\nLoss development factors are used in all triangular methods of loss reserving, such as the chain-ladder method.\n\nIncurred but not reported\n\n"}
{"id": "33589680", "url": "https://en.wikipedia.org/wiki?curid=33589680", "title": "Mathematical finance", "text": "Mathematical finance\n\nMathematical finance, also known as quantitative finance, is a field of applied mathematics, concerned with mathematical modeling of financial markets. Generally, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock (\"see: Valuation of options; Financial modeling; Asset pricing\"). The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.\n\nMathematical finance also overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models (\"see: Quantitative analyst\"), while the former focuses, in addition to analysis, on building tools of implementation for the models. In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk- and portfolio management on the other.\n\nFrench mathematician Louis Bachelier is considered the author of the first scholarly work on mathematical finance, published in 1900. But mathematical finance emerged as a discipline in the 1970s, following the work of Fischer Black, Myron Scholes and Robert Merton on option pricing theory.\n\nToday many universities offer degree and research programs in mathematical finance.\n\nThere are two separate branches of finance that require advanced quantitative techniques: derivatives pricing, and risk and portfolio management. One of the main differences is that they use different probabilities, namely the risk-neutral probability (or arbitrage-pricing probability), denoted by \"Q\", and the actual (or actuarial) probability, denoted by \"P\".\n\nThe goal of derivatives pricing is to determine the fair price of a given security in terms of more liquid securities whose price is determined by the law of supply and demand. The meaning of \"fair\" depends, of course, on whether one considers buying or selling the security. Examples of securities being priced are plain vanilla and exotic options, convertible bonds, etc.\n\nOnce a fair price has been determined, the sell-side trader can make a market on the security. Therefore, derivatives pricing is a complex \"extrapolation\" exercise to define the current market value of a security, which is then used by the sell-side community. \nQuantitative derivatives pricing was initiated by Louis Bachelier in \"The Theory of Speculation\" (\"Théorie de la spéculation\", published 1900), with the introduction of the most basic and most influential of processes, the Brownian motion, and its applications to the pricing of options. The Brownian motion is derived using the Langevin equation and the discrete random walk. Bachelier modeled the time series of changes in the logarithm of stock prices as a random walk in which the short-term changes had a finite variance. This causes longer-term changes to follow a Gaussian distribution.\n\nThe theory remained dormant until Fischer Black and Myron Scholes, along with fundamental contributions by Robert C. Merton, applied the second most influential process, the geometric Brownian motion, to option pricing. For this M. Scholes and R. Merton were awarded the 1997 Nobel Memorial Prize in Economic Sciences. Black was ineligible for the prize because of his death in 1995.\n\nThe next important step was the fundamental theorem of asset pricing by Harrison and Pliska (1981), according to which the suitably normalized current price \"P\" of a security is arbitrage-free, and thus truly fair, only if there exists a stochastic process \"P\" with constant expected value which describes its future evolution:\n\nA process satisfying () is called a \"martingale\". A martingale does not reward risk. Thus the probability of the normalized security price process is called \"risk-neutral\" and is typically denoted by the blackboard font letter \"formula_1\".\n\nThe relationship () must hold for all times t: therefore the processes used for derivatives pricing are naturally set in continuous time.\n\nThe quants who operate in the Q world of derivatives pricing are specialists with deep knowledge of the specific products they model.\n\nSecurities are priced individually, and thus the problems in the Q world are low-dimensional in nature.\nCalibration is one of the main challenges of the Q world: once a continuous-time parametric process has been calibrated to a set of traded securities through a relationship such as (1), a similar relationship is used to define the price of new derivatives.\n\nThe main quantitative tools necessary to handle continuous-time Q-processes are Itō’s stochastic calculus, simulation and partial differential equations (PDE’s).\n\nRisk and portfolio management aims at modeling the statistically derived probability distribution of the market prices of all the securities at a given future investment horizon. \nThis \"real\" probability distribution of the market prices is typically denoted by the blackboard font letter \"formula_2\", as opposed to the \"risk-neutral\" probability \"formula_1\" used in derivatives pricing. Based on the P distribution, the buy-side community takes decisions on which securities to purchase in order to improve the prospective profit-and-loss profile of their positions considered as a portfolio.\n\nFor their pioneering work, Markowitz and Sharpe, along with Merton Miller, shared the 1990 Nobel Memorial Prize in Economic Sciences, for the first time ever awarded for a work in finance.\n\nThe portfolio-selection work of Markowitz and Sharpe introduced mathematics to investment management. With time, the mathematics has become more sophisticated. Thanks to Robert Merton and Paul Samuelson, one-period models were replaced by continuous time, Brownian-motion models, and the quadratic utility function implicit in mean–variance optimization was replaced by more general increasing, concave utility functions. Furthermore, in more recent years the focus shifted toward estimation risk, i.e., the dangers of incorrectly assuming that advanced time series analysis alone can provide completely accurate estimates of the market parameters.\n\nMuch effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow Theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of \"technical analysis\" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics.\n\nOver the years, increasingly sophisticated mathematical models and derivative pricing strategies have been developed, but their credibility was damaged by the financial crisis of 2007–2010.\nContemporary practice of mathematical finance has been subjected to criticism from figures within the field notably by Paul Wilmott, and by Nassim Nicholas Taleb, in his book \"The Black Swan\". Taleb claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading. Wilmott and Emanuel Derman published the \"Financial Modelers' Manifesto\" in January 2009 which addresses some of the most serious concerns.\nBodies such as the Institute for New Economic Thinking are now attempting to develop new theories and methods.\n\nIn general, modeling the changes by distributions with finite variance is, increasingly, said to be inappropriate. In the 1960s it was discovered by Benoit Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy alpha-stable distributions. The scale of change, or volatility, depends on the length of the time interval to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation. But the problem is that it does not solve the problem as it makes parametrization much harder and risk control less reliable. See also Variance gamma process #Option pricing.\n\n"}
{"id": "32487382", "url": "https://en.wikipedia.org/wiki?curid=32487382", "title": "Michael Scott Jacobson", "text": "Michael Scott Jacobson\n\nMichael S. Jacobson is a mathematician, and Professor of Mathematical & Statistical Sciences in the Department of Mathematical & Statistical Science at the University of Colorado Denver. He served as Chair from 2003 to 2012 and was on loan serving as a program director in EHR/DUE at the National Science Foundation.\n\nJacobson did his undergraduate studies at SUNY Stony Brook in mathematics, graduating in 1975 and received his MS and Ph.D. in 1977 and 1980, respectively, from Emory University under the supervision of Henry S. Sharp Jr. He spent 23 years at the University of Louisville, where in addition to being promoted to Professor in 1988, also served as Department Chair and Associate Dean for Research and Graduate Studies.\n\nJacobson has been at the University of Colorado Denver since 2003.\n\nJacobson specializes in combinatorics and in particular graph theory. He has published more than 140 mathematical papers on topics including Ramsey theory, Hamiltonian graphs, domination in graphs and extremal graph theory. He has had over 70 collaborators including Kenneth Bogart, Stefan Burr, Gary Chartrand, Guantao Chen, Paul Erdős, Ralph Faudree, Ron Gould, András Gyárfás, Frank Harary, Stephen Hedetniemi, Linda Lesniak, Fred McMorris, K. Brooks Reid, Richard Schelp, Edward Scheinerman and Douglas B. West.\n\n"}
{"id": "15022034", "url": "https://en.wikipedia.org/wiki?curid=15022034", "title": "Mulgara (software)", "text": "Mulgara (software)\n\nMulgara is a triplestore and fork of the original Kowari project. It is Open Source, scalable, and transaction-safe. Mulgara instances can be queried via the iTQL query language and the SPARQL query language.\n\nKowari was first made available for download in beta form on October 26, 2003. In April 2004, Tucana Technologies Inc demonstrated the Tucana Knowledge Server (TKS), a proprietary RDF database relying on Kowari as the basis. A steady number of releases occurred throughout 2004, including version 1.0.5 and 1.1 pre-release. The development of TKS stalled due to difficulties with funding at the end of 2004, while the development of Kowari continued on.\n\nIn September 2005, Tucana was bought by Northrop Grumman. In January 2006, Northrop Grumman threatened a Kowari developer with legal action if he released any new version of Kowari. As a consequence, Kowari was forked in July 2006. It was renamed to Mulgara as Northrop Grumman owned the Kowari trademark. All development on Kowari has stopped and the community moved to Mulgara. The legal cloud surrounding Kowari was eventually resolved, one of the outcomes was the adoption of the Open Software License 3.0. Since 2008 all new code is being licensed with the Apache 2.0 License.\n\nSince 2006 Mulgara 1.0.0 has been released, significant changes to the transaction architecture was made to support JTA, SPARQL support, a Jena API, and integration with Sesame has been added. As of January 10, 2012 the latest version is 2.1.13.\n\nMulgara is not based on a relational database due to the large numbers of table joins encountered by relational systems when dealing with metadata. Instead, Mulgara is a completely new database optimized for metadata management. Mulgara models hold metadata in the form of short subject-predicate-object statements, much like the W3C's Resource Description Framework (RDF) standard. Metadata may be imported into or exported from Mulgara in RDF or Notation 3 form.\n\n\n"}
{"id": "161243", "url": "https://en.wikipedia.org/wiki?curid=161243", "title": "Nine-point circle", "text": "Nine-point circle\n\nIn geometry, the nine-point circle is a circle that can be constructed for any given triangle. It is so named because it passes through nine significant concyclic points defined from the triangle. These nine points are:\n\n\nThe nine-point circle is also known as Feuerbach's circle, Euler's circle, Terquem's circle, the six-points circle, the twelve-points circle, the \"n\"-point circle, the medioscribed circle, the mid circle or the circum-midcircle. Its center is the nine-point center of the triangle.\n\nThe diagram above shows the nine significant points of the nine-point circle. Points \"D\", \"E\", and \"F\" are the midpoints of the three sides of the triangle. Points \"G\", \"H\", and \"I\" are the feet of the altitudes of the triangle. Points \"J\", \"K\", and \"L\" are the midpoints of the line segments between each altitude's vertex intersection (points \"A\", \"B\", and \"C\") and the triangle's orthocenter (point \"S\").\n\nFor an acute triangle, six of the points (the midpoints and altitude feet) lie on the triangle itself; for an obtuse triangle two of the altitudes have feet outside the triangle, but these feet still belong to the nine-point circle.\n\nAlthough he is credited for its discovery, Karl Wilhelm Feuerbach did not entirely discover the nine-point circle, but rather the six point circle, recognizing the significance of the midpoints of the three sides of the triangle and the feet of the altitudes of that triangle. (\"See Fig. 1, points\" D, E, F, G, H, \"and\" I.) (At a slightly earlier date, Charles Brianchon and Jean-Victor Poncelet had stated and proven the same theorem.) But soon after Feuerbach, mathematician Olry Terquem himself proved the existence of the circle. He was the first to recognize the added significance of the three midpoints between the triangle's vertices and the orthocenter. (\"See Fig. 1, points\" J, K, \"and\" L.) Thus, Terquem was the first to use the name nine-point circle.\n\nIn 1822 Karl Feuerbach discovered that any triangle's nine-point circle is externally tangent to that triangle's three excircles and internally tangent to its incircle; this result is known as Feuerbach's theorem. He proved that:\n\nThe triangle center at which the incircle and the nine-point circle touch is called the Feuerbach point.\n\n\n\"Figure 3\"\n\n\n\"Figure 4\"\n\n\n\n\n\n\n\n\n\nThe circle is an instance of a conic section and the nine-point circle is an instance of the general nine-point conic that has been constructed with relation to a triangle \"ABC\" and a fourth point \"P\", where the particular nine-point circle instance arises when \"P\" is the orthocenter of \"ABC\". The vertices of the triangle and \"P\" determine a complete quadrilateral and three \"diagonal points\" where opposite sides of the quadrilateral intersect. There are six \"sidelines\" in the quadrilateral; the nine-point conic intersects the midpoints of these and also includes the diagonal points. The conic is an ellipse when \"P\" is interior to \"ABC\" or in a region sharing vertical angles with the triangle, but a nine-point hyperbola occurs when \"P\" is in one of the three adjacent regions, and the hyperbola is rectangular when P lies on the circumcircle of \"ABC\".\n\n\n\n"}
{"id": "9010084", "url": "https://en.wikipedia.org/wiki?curid=9010084", "title": "PTAS reduction", "text": "PTAS reduction\n\nIn computational complexity theory, a PTAS reduction is an approximation-preserving reduction that is often used to perform reductions between solutions to optimization problems. It preserves the property that a problem has a polynomial time approximation scheme (PTAS) and is used to define completeness for certain classes of optimization problems such as APX. Notationally, if there is a PTAS reduction from a problem A to a problem B, we write formula_1.\n\nWith ordinary polynomial-time many-one reductions, if we can describe a reduction from a problem A to a problem B, then any polynomial-time solution for B can be composed with that reduction to obtain a polynomial-time solution for the problem A. Similarly, our goal in defining PTAS reductions is so that given a PTAS reduction from an optimization problem A to a problem B, a PTAS for B can be composed with the reduction to obtain a PTAS for the problem A.\n\nFormally, we define a PTAS reduction from A to B using three polynomial-time computable functions, \"f\", \"g\", and \"α\", with the following properties:\n\n\nFrom the definition it is straightforward to show that:\n\nL-reductions imply PTAS reductions. As a result, one may show the existence of a PTAS reduction via a L-reduction instead.\n\nPTAS reductions are used to define completeness in APX, the class of optimization problems with constant-factor approximation algorithms.\n\n\n"}
{"id": "2685999", "url": "https://en.wikipedia.org/wiki?curid=2685999", "title": "Physical symbol system", "text": "Physical symbol system\n\nA physical symbol system (also called a formal system) takes physical patterns (symbols), combining them into structures (expressions) and manipulating them (using processes) to produce new expressions.\n\nThe physical symbol system hypothesis (PSSH) is a position in the philosophy of artificial intelligence formulated by Allen Newell and Herbert A. Simon. They wrote:\n\nThis claim implies both that human thinking is a kind of symbol manipulation (because a symbol system is necessary for intelligence) and that machines can be intelligent (because a symbol system is sufficient for intelligence).\n\nThe idea has philosophical roots in Hobbes (who claimed reasoning was \"nothing more than reckoning\"), Leibniz (who attempted to create a logical calculus of all human ideas), Hume (who thought perception could be reduced to \"atomic impressions\") and even Kant (who analyzed all experience as controlled by formal rules). The latest version is called the computational theory of mind, associated with philosophers Hilary Putnam and Jerry Fodor.\n\nThe hypothesis has been criticized strongly by various parties, but is a core part of AI research. A common critical view is that the hypothesis seems appropriate for higher-level intelligence such as playing chess, but less appropriate for commonplace intelligence such as vision. A distinction is usually made between the kind of high level symbols that directly correspond with objects in the world, such as <nowiki><dog></nowiki> and <nowiki><tail></nowiki> and the more complex \"symbols\" that are present in a machine like a neural network.\n\nExamples of physical symbol systems include:\n\nThe physical symbol system hypothesis claims that both of these are also examples of physical symbol systems:\n\nTwo lines of evidence suggested to Allen Newell and Herbert A. Simon that \"symbol manipulation\" was the essence of both human and machine intelligence: the development of artificial intelligence programs and psychological experiments on human beings.\n\nFirst, in the early decades of AI research there were a number of very successful programs that used high level symbol processing, such as Newell and Herbert A. Simon's General Problem Solver or Terry Winograd's SHRDLU. John Haugeland named this kind of AI research \"Good Old Fashioned AI\" or GOFAI. Expert systems and logic programming are descendants of this tradition. The success of these programs suggested that symbol processing systems could simulate any intelligent action.\n\nAnd second, psychological experiments carried out at the same time found that, for difficult problems in logic, planning or any kind of \"puzzle solving\", people used this kind of symbol processing as well. AI researchers were able to simulate the step by step problem solving skills of people with computer programs. This collaboration and the issues it raised eventually would lead to the creation of the field of cognitive science. (This type of research was called \"cognitive simulation\".) This line of research suggested that human problem solving consisted primarily of the manipulation of high level symbols.\n\nIn Newell and Simon's arguments, the \"symbols\" that the hypothesis is referring to are physical objects that represent things in the world, symbols such as <nowiki><dog></nowiki> that have a recognizable meaning or denotation and can be composed with other symbols to create more complex symbols.\n\nHowever, it is also possible to interpret the hypothesis as referring to the simple abstract 0s and 1s in the memory of a digital computer or the stream of 0s and 1s passing through the perceptual apparatus of a robot. These are, in some sense, symbols as well, although it is not always possible to determine exactly what the symbols are standing for. In this version of the hypothesis, no distinction is being made between \"symbols\" and \"signals\", as David Touretzky and Dean Pomerleau explain.\n\nUnder this interpretation, the physical symbol system hypothesis asserts merely that intelligence can be \"digitized\". This is a weaker claim. Indeed, Touretzky and Pomerleau write that if symbols and signals are the same thing, then \"[s]ufficiency is a given, unless one is a dualist or some other sort of mystic, because physical symbol systems are Turing-universal.\" The widely accepted Church–Turing thesis holds that any Turing-universal system can simulate any conceivable process that can be digitized, given enough time and memory. Since any digital computer is Turing-universal, any digital computer can, in theory, simulate anything that can be digitized to a sufficient level of precision, including the behavior of intelligent organisms. The necessary condition of the physical symbol systems hypothesis can likewise be finessed, since we are willing to accept almost any signal as a form of \"symbol\" and all intelligent biological systems have signal pathways.\n\nNils Nilsson has identified four main \"themes\" or grounds in which the physical symbol system hypothesis has been attacked.\n\nHubert Dreyfus attacked the necessary condition of the physical symbol system hypothesis, calling it \"the psychological assumption\" and defining it thus:\nDreyfus refuted this by showing that human intelligence and expertise depended primarily on unconscious instincts rather than conscious symbolic manipulation. Experts solve problems quickly by using their intuitions, rather than step-by-step trial and error searches. Dreyfus argued that these unconscious skills would never be captured in formal rules.\n\nJohn Searle's Chinese room argument, presented in 1980, attempted to show that a program (or any physical symbol system) could not be said to \"understand\" the symbols that it uses; that the symbols themselves have no meaning or semantic content, and so the machine can never be truly intelligent from symbol manipulation alone. \n\nIn the sixties and seventies, several laboratories attempted to build robots that used symbols to represent the world and plan actions (such as the Stanford Cart). These projects had limited success. In the middle eighties, Rodney Brooks of MIT was able to build robots that had superior ability to move and survive without the use of symbolic reasoning at all. Brooks (and others, such as Hans Moravec) discovered that our most basic skills of motion, survival, perception, balance and so on did not seem to require high level symbols at all, that in fact, the use of high level symbols was more complicated and less successful.\n\nIn a 1990 paper Elephants Don't Play Chess, robotics researcher Rodney Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"\n\nGeorge Lakoff, Mark Turner and others have argued that our abstract skills in areas such as mathematics, ethics and philosophy depend on unconscious skills that derive from the body, and that conscious symbol manipulation is only a small part of our intelligence.\n\n\n"}
{"id": "10129174", "url": "https://en.wikipedia.org/wiki?curid=10129174", "title": "Plan (drawing)", "text": "Plan (drawing)\n\nPlans are a set of drawings or two-dimensional diagrams used to describe a place or object, or to communicate building or fabrication instructions. Usually plans are drawn or printed on paper, but they can take the form of a digital file.\n\nThese plans are used in a range of fields from architecture, urban planning, landscape architecture, mechanical engineering, civil engineering, industrial engineering to systems engineering.\n\nPlans are often for technical purposes such as architecture, engineering, or planning. Their purpose in these disciplines is to accurately and unambiguously capture all the geometric features of a site, building, product or component. Plans can also be for presentation or orientation purposes, and as such are often less detailed versions of the former. The end goal of plans is either to portray an existing place or object, or to convey enough information to allow a builder or manufacturer to realize a design.\n\nThe term \"plan\" may casually be used to refer to a single view, sheet, or drawing in a set of plans. More specifically a plan view is an orthographic projection looking down on the object, such as in a floor plan.\n\nThe process of producing plans, and the skill of producing them, is often referred to as technical drawing. A working drawing is a type of technical drawing, which is part of the documentation needed to build an engineering product or architecture. Typically in architecture these could include civil drawings, architectural drawings, structural drawings, mechanical drawings, electrical drawings, and plumbing drawings. In engineering, these drawings show all necessary data to manufacture a given object, such as dimensions and angles.\n\nPlans are often prepared in a \"set\". The set includes all the information required for the purpose of the set, and may exclude views or projections which are unnecessary. A set of plans can be on standard office-sized paper or on large sheets. It can be stapled, folded or rolled as required. A set of plans can also take the form of a digital file in a proprietary format such as DWG or an exchange file format such as DXF or PDF.\n\nPlans are often referred to as \"blueprints\" or \"bluelines\". However, the terms are rapidly becoming an anachronism, since these copying methods have mostly been superseded by reproduction processes that yield black or multicolour lines on white paper, or by electronic representations of information.\n\nPlans are usually \"scale drawings\", meaning that the plans are drawn at a specific ratio relative to the actual size of the place or object. Various scales may be used for different drawings in a set. For example, a floor plan may be drawn at 1:48 (or 1/4\"=1'-0\") whereas a detailed view may be drawn at 1:24 (or 1/2\"=1'-0\"). Site plans are often drawn at 1\" = 20' (1:240) or 1\" = 30' (1:360).\n\nIn the metric system the ratios commonly are 1:5, 1:10, 1:20, 1:50, 1:100, 1:200, 1:500, 1:1000, 1:2000 and 1:5000\n\nBecause plans represent three-dimensional objects on a two-dimensional plane, the use of views or projections is crucial to the legibility of plans. Each projection is achieved by assuming a vantage point from which to see the place or object, and a type of projection. These projection types are:\n\n\nThere is no universal standard for sheet order, however the following describes a common approach:\n\n\nWhere additional systems are complex and require many details for installation, specialized additional plan drawings may be used, such as:\n\n"}
{"id": "17487236", "url": "https://en.wikipedia.org/wiki?curid=17487236", "title": "Promise theory", "text": "Promise theory\n\nPromise Theory, in the context of information science, is a model of voluntary cooperation between individual, autonomous actors or agents who publish their intentions to one another in the form of promises. It is a form of labelled graph theory, describing discrete networks of agents joined by the unilateral promises they make.\n\nA promise is a declaration of intent whose purpose is to increase the recipient's certainty about a claim of past, present or future behaviour. For a promise to increase certainty, the recipient needs to trust the promiser, but trust can also be built on the verification (or assessment) that previous promises have been kept, thus trust plays a symbiotic relationship with promises. Each agent assesses its belief in the promise's outcome or intent. Thus Promise Theory is about the relativity of autonomous agents.\n\nOne of the goals of Promise Theory is to offer a model that unifies the physical (or dynamical) description of an information system with its intended meaning, i.e. its semantics. This has been used to describe configuration management of resources in information systems, amongst other things.\n\nPromise Theory was proposed by Mark Burgess in 2004, in the context of computer science, in order to solve problems present in obligation-based computer management schemes for policy-based management. However its usefulness was quickly seen to go far beyond computing. The simple model of a promise used in Promise Theory (now called 'micro-promises') can easily address matters of Economics and Organization. Promise Theory has since been developed by Burgess in collaboration with Dutch computer scientist Jan Bergstra, resulting in a book: Promise Theory: Principles and Applications. published in 2013.\n\nInterest in promise theory has grown in the IT industry, with several products citing it.\n\nObligations, rather than promises have been the traditional way of guiding behaviour.\nPromise Theory's point of departure from obligation logics is the idea that all agents in a system should have autonomy of control—i.e. that they cannot be coerced or forced into a specific behaviour. Obligation theories in computer science often view an obligation as a deterministic command that causes its proposed outcome. In Promise Theory an agent may only make promises about its own behaviour. For autonomous agents it is meaningless to make promises about another's behaviour.\n\nAlthough this assumption could be interpreted morally or ethically, in Promise Theory this is simply a pragmatic `engineering' principle, which leads to a more complete documentation of the intended roles of the actors or agents within the whole. The reason for this is that, when one is not allowed to make assumptions about others' behaviour, one is forced to document every promise more completely in order to make predictions; thus it leads to a more complete documentation which in turn points out the possible failure modes by which cooperative behaviour could fail.\n\nCommand and control systems like those that motivate obligation theories can easily be reproduced by having agents voluntarily promise to follow the instructions of another agent (this is also viewed as a more realistic model of behaviour). Since a promise can always be withdrawn, there is no contradiction between voluntary cooperation and command and control.\n\nIn Philosophy and Law a promise is often viewed as something that leads to an obligation. Promise Theory rejects that point of view. Bergstra and Burgess have shown that the concept of a promise is quite independent of that of obligation and indeed is simpler.\n\nThe role of obligations in increasing certainty is unclear, since obligations can come from anywhere and an aggregation of non-local constraints cannot be resolved by a local agent: this means that obligations can actually increase uncertainty. In a world of promises, all constraints on an agent are self-imposed and local (even if they are suggested by outside agents), thus all contradictions can be resolved locally.\n\nThe theory of commitments in multi-agent systems has some similarities with aspects of promise theory, but there are key differences. In Promise Theory a commitment is a subset of intentions. Since a promise is a published intention, a commitment may or may not be a promise. A detailed comparison of Promises and Commitments in the senses intended in their respective fields is forthcoming, and not a trivial matter.\n\nPromises can be valuable to the promisee or even to the promiser. They might also lead to costs. There is thus an economic story to tell about promises. The economics of promises naturally motivate `selfish agent' behaviour and Promise Theory can be seen as a motivation for game theoretical decision making, in which multiple promises play the role of strategies in a game.\n\nThe theory of promises as applied to organization bears some resemblance to the theory of Institutional Diversity by Elinor Ostrom.\nSeveral of the same themes and consideration appear; the main difference is that Ostrom focuses, like many authors, on the role of external rules and obligations. Promise Theory takes the opposite viewpoint that obeyance of rules is a voluntary act and hence it makes sense to focus on those voluntary promises. An attempt to force obeyance without a promise is considered to constitute an attack.\nOne benefit of a Promise Theory approach is that it does not require special structural elements (e.g. Ostrom's institutional \"Positions\") to describe different roles in a collaborate network—these may also be viewed as promises in Promise Theory; thus there is a parsimony that helps to avoid an explosion of concepts, and perhaps more importantly admits mathematical formalization. The algebra and calculus of promises allows simple reasoning in a mathematical framework.\n\nIn spite of the generality of Promise Theory, it was originally proposed by Burgess as a way of modelling the computer management software CFEngine and its autonomous behaviour. Existing theories based on obligation were unsuitable. CFEngine uses a model of autonomy both as a way of avoiding distributed inconsistency in policy and as a security principle against external attack: no agent can be forced to receive information or instructions from another agent, thus all cooperation is voluntary. For many users of the software, this property has been instrumental in both keeping their systems safe and adapting to the local requirements.\n\nIn computer science, the Promise theory describes policy governed services, in a framework of completely autonomous agents, which assist one another by voluntary cooperation alone. It is a framework for analyzing realistic models of modern networking, and as a formal model for swarm intelligence.\n\nPromise theory may be viewed as a logical and graph theoretical framework for understanding complex relationships in networks, where many constraints have to be met, which was developed at Oslo University College, by drawing on ideas from several different lines of research conducted there, including policy based management, graph theory, logic and configuration management. It uses a constructivist approach that builds conventional management structures from graphs of interacting, autonomous agents. Promises can be asserted either from an agent to itself or from one agent to another and each promise implies a constraint on the behavior of the promising agent. The atomicity of the promises makes them a tool for finding contradictions and inconsistencies.\n\nThe promises made by autonomous agents lead to a mutually approved graph structure, which in turn leads to spatial structures in which the agents represent point-like locations. This allows models of smart spaces, i.e. semantically labeled or even functional spaces, such as databases, knowledge maps, warehouses, hotels, etc., to be unified with other more conventional descriptions of space and time. The model of semantic spacetime uses promise theory to discuss these spacetime concepts.\n\nPromises are more mathematically primitive than graph adjacencies, since a link requires the mutual consent of two autonomous agents, thus the concept of a connected space requires more work to build structure. This makes them mathematically interesting as a notion of space, and offers a useful way of modeling physical and virtual information systems.\n\n"}
{"id": "1207057", "url": "https://en.wikipedia.org/wiki?curid=1207057", "title": "Promptuary", "text": "Promptuary\n\nThe promptuary was a calculating machine invented by the 16th-century Scottish mathematician John Napier and described in the second edition of his book \"Rabdologiae\" in which he also described Napier's bones.\n\nIt is an extension of Napier's Bones, using two sets of rods to achieve multi-digit multiplication. The rods for the multiplicand are similar to Napier's Bones, with repetitions of the values. The set of rods for the multiplier are shutters or masks for each digit placed over the multiplicand rods. The results are then tallied from the digits showing as with other lattice multiplication methods.\nThe final form described by Napier took advantage of symmetries to compact the rods, and used the materials of the day to hold system of metal plates, placed inside a wooden frame.\n\n"}
{"id": "19196523", "url": "https://en.wikipedia.org/wiki?curid=19196523", "title": "Randomness", "text": "Randomness\n\nRandomness is the lack of pattern or predictability in events. A random sequence of events, symbols or steps has no order and does not follow an intelligible pattern or combination. Individual random events are by definition unpredictable, but in many cases the frequency of different outcomes over a large number of events (or \"trials\") is predictable. For example, when throwing two dice, the outcome of any particular roll is unpredictable, but a sum of 7 will occur twice as often as 4. In this view, randomness is a measure of uncertainty of an outcome, rather than haphazardness, and applies to concepts of chance, probability, and information entropy.\n\nThe fields of mathematics, probability, and statistics use formal definitions of randomness. In statistics, a random variable is an assignment of a numerical value to each possible outcome of an event space. This association facilitates the identification and the calculation of probabilities of the events. Random variables can appear in random sequences. A random process is a sequence of random variables whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. These and other constructs are extremely useful in probability theory and the various applications of randomness.\n\nRandomness is most often used in statistics to signify well-defined statistical properties. Monte Carlo methods, which rely on random input (such as from random number generators or pseudorandom number generators), are important techniques in science, as, for instance, in computational science. By analogy, quasi-Monte Carlo methods use quasirandom number generators.\n\nRandom selection, when narrowly associated with a simple random sample, is a method of selecting items (often called units) from a population where the probability of choosing a specific item is the proportion of those items in the population. For example, with a bowl containing just 10 red marbles and 90 blue marbles, a random selection mechanism would choose a red marble with probability 1/10. Note that a random selection mechanism that selected 10 marbles from this bowl would not necessarily result in 1 red and 9 blue. In situations where a population consists of items that are distinguishable, a random selection mechanism requires equal probabilities for any item to be chosen. That is, if the selection process is such that each member of a population, of say research subjects, has the same probability of being chosen then we can say the selection process is random.\n\nIn ancient history, the concepts of chance and randomness were intertwined with that of fate. Many ancient peoples threw dice to determine fate, and this later evolved into games of chance. Most ancient cultures used various methods of divination to attempt to circumvent randomness and fate.\n\nThe Chinese of 3000 years ago were perhaps the earliest people to formalize odds and chance. The Greek philosophers discussed randomness at length, but only in non-quantitative forms. It was only in the 16th century that Italian mathematicians began to formalize the odds associated with various games of chance. The invention of the calculus had a positive impact on the formal study of randomness. In the 1888 edition of his book \"The Logic of Chance\" John Venn wrote a chapter on \"The conception of randomness\" that included his view of the randomness of the digits of the number pi by using them to construct a random walk in two dimensions.\n\nThe early part of the 20th century saw a rapid growth in the formal analysis of randomness, as various approaches to the mathematical foundations of probability were introduced. In the mid- to late-20th century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.\n\nAlthough randomness had often been viewed as an obstacle and a nuisance for many centuries, in the 20th century computer scientists began to realize that the \"deliberate\" introduction of randomness into computations can be an effective tool for designing better algorithms. In some cases such randomized algorithms outperform the best deterministic methods.\n\nMany scientific fields are concerned with randomness:\n\nIn the 19th century, scientists used the idea of random motions of molecules in the development of statistical mechanics to explain phenomena in thermodynamics and the properties of gases.\n\nAccording to several standard interpretations of quantum mechanics, microscopic phenomena are objectively random. That is, in an experiment that controls all causally relevant parameters, some aspects of the outcome still vary randomly. For example, if a single unstable atom is placed in a controlled environment, it cannot be predicted how long it will take for the atom to decay—only the probability of decay in a given time. Thus, quantum mechanics does not specify the outcome of individual experiments but only the probabilities. Hidden variable theories reject the view that nature contains irreducible randomness: such theories posit that in the processes that appear random, properties with a certain statistical distribution are at work behind the scenes, determining the outcome in each case.\n\nThe modern evolutionary synthesis ascribes the observed diversity of life to random genetic mutations followed by natural selection. The latter retains some random mutations in the gene pool due to the systematically improved chance for survival and reproduction that those mutated genes confer on individuals who possess them.\n\nSeveral authors also claim that evolution and sometimes development require a specific form of randomness, namely the introduction of qualitatively new behaviors. Instead of the choice of one possibility among several pre-given ones, this randomness corresponds to the formation of new possibilities.\n\nThe characteristics of an organism arise to some extent deterministically (e.g., under the influence of genes and the environment) and to some extent randomly. For example, the \"density\" of freckles that appear on a person's skin is controlled by genes and exposure to light; whereas the exact location of \"individual\" freckles seems random.\n\nAs far as behavior is concerned, randomness is important if an animal is to behave in a way that is unpredictable to others. For instance, insects in flight tend to move about with random changes in direction, making it difficult for pursuing predators to predict their trajectories.\n\nThe mathematical theory of probability arose from attempts to formulate mathematical descriptions of chance events, originally in the context of gambling, but later in connection with physics. Statistics is used to infer the underlying probability distribution of a collection of empirical observations. For the purposes of simulation, it is necessary to have a large supply of random numbers or means to generate them on demand.\n\nAlgorithmic information theory studies, among other topics, what constitutes a random sequence. The central idea is that a string of bits is random if and only if it is shorter than any computer program that can produce that string (Kolmogorov randomness)—this means that random strings are those that cannot be compressed. Pioneers of this field include Andrey Kolmogorov and his student Per Martin-Löf, Ray Solomonoff, and Gregory Chaitin. For the notion of infinite sequence, one normally uses Per Martin-Löf's definition.\nThat is, an infinite sequence is random if and only if it withstands all recursively enumerable null sets. The other notions of random sequences include (but not limited to): recursive randomness and Schnorr randomness which are based on recursively computable martingales. It was shown by Yongge Wang that these randomness notions are generally different.\n\nRandomness occurs in numbers such as log (2) and pi. The decimal digits of pi constitute an infinite sequence and \"never repeat in a cyclical fashion.\" Numbers like pi are also considered likely to be normal, which means their digits are random in a certain statistical sense.\n\nPi certainly seems to behave this way. In the first six billion decimal places of pi, each of the digits from 0 through 9 shows up about six hundred million times. Yet such results, conceivably accidental, do not prove normality even in base 10, much less normality in other number bases.\n\nIn statistics, randomness is commonly used to create simple random samples. This lets surveys of completely random groups of people provide realistic data. Common methods of doing this include drawing names out of a hat or using a random digit chart. A random digit chart is simply a large table of random digits.\n\nIn information science, irrelevant or meaningless data is considered noise. Noise consists of a large number of transient disturbances with a statistically randomized time distribution.\n\nIn communication theory, randomness in a signal is called \"noise\" and is opposed to that component of its variation that is causally attributable to the source, the signal.\n\nIn terms of the development of random networks, for communication randomness rests on the two simple assumptions of Paul Erdős and Alfréd Rényi who said that there were a fixed number of nodes and this number remained fixed for the life of the network, and that all nodes were equal and linked randomly to each other.\n\nThe random walk hypothesis considers that asset prices in an organized market evolve at random, in the sense that the expected value of their change is zero but the actual value may turn out to be positive or negative. More generally, asset prices are influenced by a variety of unpredictable events in the general economic environment.\n\nRandom selection can be an official method to resolve tied elections in some jurisdictions. Its use in politics is very old, as office holders in Ancient Athens were chosen by lot, there being no voting.\n\nRandomness can be seen as conflicting with the deterministic ideas of some religions, such as those where the universe is created by an omniscient deity who is aware of all past and future events. If the universe is regarded to have a purpose, then randomness can be seen as impossible. This is one of the rationales for religious opposition to evolution, which states that non-random selection is applied to the results of random genetic variation.\n\nHindu and Buddhist philosophies state that any event is the result of previous events, as reflected in the concept of karma, and as such there is no such thing as a random event or a first event.\n\nIn some religious contexts, procedures that are commonly perceived as randomizers are used for divination. Cleromancy uses the casting of bones or dice to reveal what is seen as the will of the gods.\n\nIn most of its mathematical, political, social and religious uses, randomness is used for its innate \"fairness\" and lack of bias.\n\nPolitics: Athenian democracy was based on the concept of isonomia (equality of political rights) and used complex allotment machines to ensure that the positions on the ruling committees that ran Athens were fairly allocated. Allotment is now restricted to selecting jurors in Anglo-Saxon legal systems and in situations where \"fairness\" is approximated by randomization, such as selecting jurors and military draft lotteries.\n\nGames: Random numbers were first investigated in the context of gambling, and many randomizing devices, such as dice, shuffling playing cards, and roulette wheels, were first developed for use in gambling. The ability to produce random numbers fairly is vital to electronic gambling, and, as such, the methods used to create them are usually regulated by government Gaming Control Boards. Random drawings are also used to determine lottery winners. Throughout history, randomness has been used for games of chance and to select out individuals for an unwanted task in a fair way (see drawing straws).\n\nSports: Some sports, including American football, use coin tosses to randomly select starting conditions for games or seed tied teams for postseason play. The National Basketball Association uses a weighted lottery to order teams in its draft.\n\nMathematics: Random numbers are also employed where their use is mathematically important, such as sampling for opinion polls and for statistical sampling in quality control systems. Computational solutions for some types of problems use random numbers extensively, such as in the Monte Carlo method and in genetic algorithms.\n\nMedicine: Random allocation of a clinical intervention is used to reduce bias in controlled trials (e.g., randomized controlled trials).\n\nReligion: Although not intended to be random, various forms of divination such as cleromancy see what appears to be a random event as a means for a divine being to communicate their will. (See also Free will and Determinism).\n\nIt is generally accepted that there exist three mechanisms responsible for (apparently) random behavior in systems:\n\n\nThe many applications of randomness have led to many different methods for generating random data. These methods may vary as to how unpredictable or statistically random they are, and how quickly they can generate random numbers.\n\nBefore the advent of computational random number generators, generating large amounts of sufficiently random numbers (important in statistics) required a lot of work. Results would sometimes be collected and distributed as random number tables.\n\nThere are many practical measures of randomness for a binary sequence. These include measures based on frequency, discrete transforms, and complexity, or a mixture of these. These include tests by Kak, Phillips, Yuen, Hopkins, Beth and Dai, Mund, and Marsaglia and Zaman.\n\nQuantum Non-Locality has been used to certify the presence of genuine randomness in a given string of numbers.\n\nPopular perceptions of randomness are frequently mistaken, based on fallacious reasoning or intuitions.\n\nThis argument is, \"In a random selection of numbers, since all numbers eventually appear, those that have not come up yet are 'due', and thus more likely to come up soon.\" This logic is only correct if applied to a system where numbers that come up are removed from the system, such as when playing cards are drawn and not returned to the deck. In this case, once a jack is removed from the deck, the next draw is less likely to be a jack and more likely to be some other card. However, if the jack is returned to the deck, and the deck is thoroughly reshuffled, a jack is as likely to be drawn as any other card. The same applies in any other process where objects are selected independently, and none are removed after each event, such as the roll of a die, a coin toss, or most lottery number selection schemes. Truly random processes such as these do not have memory, making it impossible for past outcomes to affect future outcomes.\n\nIn a random sequence of numbers, a number may be said to be cursed because it has come up less often in the past, and so it is thought that it will occur less often in the future. A number may be assumed to be blessed because it has occurred more often than others in the past, and so it is thought likely to come up more often in the future. This logic is valid only if the randomisation is biased, for example with a loaded die. If the die is fair, then previous rolls give no indication of future events.\n\nIn nature, events rarely occur with perfectly equal frequency, so observing outcomes to determine which events are more probable makes sense. It is fallacious to apply this logic to systems designed to make all outcomes equally likely, such as shuffled cards, dice, and roulette wheels.\n\nIn the beginning of a scenario, one might calculate the probability of a certain event. The fact is, as soon as one gains more information about that situation, they may need to re-calculate the probability.\nSay we are told that a woman has two children. If we ask whether either of them is a girl, and are told yes, what is the probability that the other child is also a girl? Considering this new child independently, one might expect the probability that the other child is female is ½ (50%). But by building a probability space (illustrating all possible outcomes), we see that the probability is actually only ⅓ (33%). This is because the possibility space illustrates 4 ways of having these two children: boy-boy, girl-boy, boy-girl, and girl-girl. But we were given more information. Once we are told that one of the children is a female, we use this new information to eliminate the boy-boy scenario. Thus the probability space reveals that there are still 3 ways to have two children where one is a female: boy-girl, girl-boy, girl-girl. Only ⅓ of these scenarios would have the other child also be a girl. Using a probability space, we are less likely to miss one of the possible scenarios, or to neglect the importance of new information. For further information, see Boy or girl paradox.\n\nThis technique provides insights in other situations such as the Monty Hall problem, a game show scenario in which a car is hidden behind one of three doors, and two goats are hidden as booby prizes behind the others. Once the contestant has chosen a door, the host opens one of the remaining doors to reveal a goat, eliminating that door as an option. With only two doors left (one with the car, the other with another goat), the player must decide to either keep their decision, or switch and select the other door. Intuitively, one might think the player is choosing between two doors with equal probability, and that the opportunity to choose another door makes no difference. But probability spaces reveal that the contestant has received new information, and can increase their chances of winning by changing to the other door.\n\n\n\n"}
{"id": "8517402", "url": "https://en.wikipedia.org/wiki?curid=8517402", "title": "Symmetric successive over-relaxation", "text": "Symmetric successive over-relaxation\n\nIn applied mathematics, symmetric successive over-relaxation (SSOR), is a preconditioner.\n\nIf the original matrix can be split into diagonal, lower and upper triangular as formula_1 then the SSOR preconditioner matrix is defined as\n\nIt can also be parametrised by formula_3 as follows.\n\n"}
{"id": "55536887", "url": "https://en.wikipedia.org/wiki?curid=55536887", "title": "Tamara G. Kolda", "text": "Tamara G. Kolda\n\nTamara \"Tammy\" G. Kolda is an American applied mathematician and Distinguished Member of Technical Staff at Sandia National Laboratories. She is noted for her contributions in computational science, multilinear algebra, data mining, graph algorithms, mathematical optimization, parallel computing, and software engineering. She is currently a member of the SIAM Board of Trustees and serves as associate editor for both the \"SIAM Journal on Scientific Computing\" and the \"SIAM Journal on Matrix Analysis and Applications\". She received her bachelors degree in mathematics in 1992 from the University of Maryland Baltimore County and her PhD in applied mathematics from the University of Maryland College Park in 1997. She was a Householder Postdoctoral Fellow at Oak Ridge National Laboratory from 1997 to 1999 before joining Sandia National Laboratories. Kolda received a Presidential Early Career Award for Scientists and Engineers in 2003, best paper prizes at the 2008 IEEE International Conference on Data Mining and the 2013 SIAM International Conference on Data Mining, and has been a distinguished member of the Association for Computing Machinery since 2011. She was elected a Fellow of the Society for Industrial and Applied Mathematics in 2015.\n"}
{"id": "28096792", "url": "https://en.wikipedia.org/wiki?curid=28096792", "title": "Trace distance", "text": "Trace distance\n\nIn quantum mechanics, and especially quantum information and the study of open quantum systems, the trace distance \"T\" is a metric on the space of density matrices and gives a measure of the distinguishability between two states. It is the quantum generalization of the Kolmogorov distance for classical probability distributions.\n\nThe trace distance is just half of the trace norm of the difference of the matrices:\n\n(The trace norm is the Schatten norm for \"p\"=1.) The purpose of the factor of two is to restrict the trace distance between two normalized density matrices to the range [0, 1] and to simplify formulas in which the trace distance appears.\n\nSince density matrices are Hermitian, \n\nwhere the formula_3 are eigenvalues of the Hermitian, but not necessarily positive, matrix formula_4.\n\nIt can be shown that the trace distance satisfies the equation\nwhere the maximization can be carried out over all POVMs formula_6.\nformula_7 is the difference in probability that the outcome of the measurement be formula_8, depending on whether the system was in the state formula_9 or formula_10. Thus the trace distance is the probability difference maximized over all possible measurements: it gives a measure of the maximum probability of distinguishing between two states with an optimal measurement.\n\nFor example, suppose Alice prepares a system in either the state formula_9 or formula_10, each with probability formula_13 and sends it to Bob who has to discriminate between the two states. It is easy to show that with the optimal measurement, Bob has the probability\nof correctly identifying in which state Alice prepared the system.\n\nThe trace distance has the following properties\n\nFor qubits, the trace distance is equal to half the Euclidean distance in the Bloch representation.\n\nThe fidelity of two quantum states formula_24 is related to the trace distance formula_25 by the inequalities\n\nThe upper bound inequality becomes an equality when formula_9 and formula_10 are pure states.\n\nThe trace distance is a generalization of the total variation distance, and for two commuting density matrices, has the same value as the total variation distance of the two corresponding probability distributions.\n"}
{"id": "491097", "url": "https://en.wikipedia.org/wiki?curid=491097", "title": "Variational principle", "text": "Variational principle\n\nA variational principle is a scientific principle used within the calculus of variations, which develops general methods for finding functions which extremize the value of quantities that depend upon those functions. For example, to answer this question: \"What is the shape of a chain suspended at both ends?\" we can use the variational principle that the shape must minimize the gravitational potential energy.\n\nAny physical law which can be expressed as a variational principle describes a self-adjoint operator. These expressions are also called Hermitian. Such an expression describes an invariant under a Hermitian transformation.\n\nFelix Klein's Erlangen program attempted to identify such invariants under a group of transformations. In what is referred to in physics as Noether's theorem, the Poincaré group of transformations (what is now called a gauge group) for general relativity defines symmetries under a group of transformations which depend on a variational principle, or action principle.\n\n\n"}
{"id": "42596301", "url": "https://en.wikipedia.org/wiki?curid=42596301", "title": "WRF-SFIRE", "text": "WRF-SFIRE\n\nWRF-SFIRE is a coupled atmosphere-wildfire model, which combines the Weather Research and Forecasting Model (WRF) with a fire-spread model, implemented by the level-set method. A version from 2010 was released based on the WRF 3.2 as WRF-Fire.\n\n\n"}
{"id": "48058843", "url": "https://en.wikipedia.org/wiki?curid=48058843", "title": "Water pouring puzzle", "text": "Water pouring puzzle\n\nWater pouring puzzles (also called water jug problems or measuring puzzles) are a class of puzzle involving a finite collection of water jugs of known integer capacities (in terms of a liquid measure such as liters or gallons).\nInitially each jug contains a known integer volume of liquid, not necessarily equal to its capacity.\nPuzzles of this type ask how many steps of pouring water from one jug to another (until either one jug becomes empty or the other becomes full) are needed to reach a goal state, specified in terms of the volume of liquid that must be present in some jug or jugs.\n\nIt is a common assumption, stated as part of these puzzles, that the jugs in the puzzle are irregularly shaped and unmarked, so that it is impossible to accurately measure\nany quantity of water that does not completely fill a jug. Other assumptions of these problems may include that no water can be spilled, and that each step pouring water from a source\njug to a destination jug stops when either the source jug\nis empty or the destination jug is full, whichever happens first.\n\nThe standard puzzle of this kind works with three jugs of capacity 8, 5 and 3 liters. These are initially filled with 8, 0 and 0 liters. In the goal state they should filled with 4, 4 and 0 liters.\nThe puzzle may be solved in seven steps, passing through the following sequence of states (denoted as a bracketed triple of the three volumes of water in the three jugs):\n\nThe rules are sometimes formulated by adding a \nsource (tap) and a drain (sink) which provide an infinite amount\nof additional water and an opportunity to pour all liquid from any jug into the\nsink. Filling a jug to the rim from the tap or pouring the entire \ncontents of jug into the drain each count as one step while\nsolving the problem. This version of the puzzle was featured in a scene of the 1995 movie \"Die Hard with a Vengeance\".\n\nThis variant is identical to the original, as a third container capable of holding the contents of the first two is mathematically equivalent to a tap or drain capable of filling or emptying both containers.\n\nIf the number of jugs is three, the filling status after\neach step can be described in a diagram of barycentric coordinates,\nbecause the sum of all three integers stays the same throughout\nall steps. In consequence the steps can be visualized as some\nkind of billard moves in the (clipped) coordinate system on a\ntriangular lattice.\n\n"}
{"id": "33462303", "url": "https://en.wikipedia.org/wiki?curid=33462303", "title": "Zahlbericht", "text": "Zahlbericht\n\nIn mathematics, the Zahlbericht (number report) was a report on algebraic number theory by .\n\nIn 1893 the German mathematical society invited Hilbert and Minkowski to write reports on the theory of numbers. They agreed that Minkowski would cover the more elementary parts of number theory while Hilbert would cover algebraic number theory. Minkowski eventually abandoned his report, while Hilbert's report was published in 1897. It was reprinted in volume 1 of his collected works, and republished in an English translation in 1998.\n\nSome earlier reports on number theory include the report by H. J. S. Smith in 6 parts between 1859 and 1865, reprinted in , and the report by . wrote an update of Hilbert's Zahlbericht that covered class field theory (republished in 1 volume as ).\n\nPart 1 covers the theory of general number fields, including ideals, discriminants, differents, units, and ideal classes. \n\nPart 2 covers Galois number fields, including in particular Hilbert's theorem 90.\n\nPart 3 covers quadratic number fields, including the theory of genera, and class numbers of quadratic fields.\n\nPart 4 covers cyclotomic fields, including the Kronecker–Weber theorem (theorem 131), the Hilbert–Speiser theorem (theorem 132), and the Eisenstein reciprocity law for \"l\"th power residues (theorem 140) .\n\nPart 5 covers Kummer number fields, and ends with Kummer's proof of Fermat's last theorem for regular primes.\n\n\n"}
