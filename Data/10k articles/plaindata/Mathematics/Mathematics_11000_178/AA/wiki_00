{"id": "45590593", "url": "https://en.wikipedia.org/wiki?curid=45590593", "title": "1881–82 census of the Ottoman Empire", "text": "1881–82 census of the Ottoman Empire\n\n1881–82 census of the Ottoman Empire was a multi-year census effort that the preparations for the forms and registration committees finished in 1884-85 (also refereed as 1881-83 census) which from this date a continuous flow of information collected with yearly reports until final record issued in 1893 (also refereed as 1881-93 census). The first official census (1881–1893) took 10 years to finish. Grand Vizier Cevat Pasha submitted the census records in a bound manuscript to the sultan.\n"}
{"id": "209248", "url": "https://en.wikipedia.org/wiki?curid=209248", "title": "21 (number)", "text": "21 (number)\n\n21 (twenty-one) is the natural number following 20 and preceding 22.\n\n21 is:\n\n\nNote that a necessary condition for \"n\" is that for any \"a\" coprime to \"n\", \"a\" and \"n\" - \"a\" must satisfy the condition above, therefore at least one of \"a\" and \"n\" - \"a\" must only have factor 2 and 5.\n\nLet formula_3 donate the quantity of the numbers smaller than \"n\" that only have factor 2 and 5 and that are coprime to \"n\", we instantly have formula_4.\n\nWe can easily see that for sufficiently large \"n\", formula_5, but formula_6, formula_7 as \"n\" goes to infinity, thus formula_4 fails to hold for sufficiently large \"n\".\n\nIn fact, For every \"n\" > 2, we have\nand\nso formula_4 fails to hold when \"n\" > 273 (actually, when \"n\" > 33).\n\nJust check a few numbers to see that \"n\" = 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 21.\n21 appears in the Padovan sequence, preceded by the terms 9, 12, 16 (it is the sum of the first two of these).\n\n\n\n\n21 is:\n"}
{"id": "399075", "url": "https://en.wikipedia.org/wiki?curid=399075", "title": "44 (number)", "text": "44 (number)\n\n44 (forty-four) is the natural number following 43 and preceding 45.\n\nForty-four is a tribonacci number, an octahedral number and the number of derangements of 5 items. \n\nSince the greatest prime factor of 44 + 1 = 1937 is 149 and thus more than 44 twice, 44 is a Størmer number.\n\nGiven Euler's totient function, φ(44) = 20 and φ(69) = 44.\n\n44 is a repdigit\n\nIn decimal notation (base 10), 44 is a palindromic number and a happy number.\n\n\n\n\nForty-four is:\nAD 44, 44 BC, 1944, 2044, etc.\n\n"}
{"id": "403321", "url": "https://en.wikipedia.org/wiki?curid=403321", "title": "65 (number)", "text": "65 (number)\n\n65 (sixty-five) is the natural number following 64 and preceding 66.\n\nSixty-five is the 23rd semiprime and the 3rd of the form (5.q). It is an octagonal number. It is also a Cullen number. Given 65, the Mertens function returns 0.\n\nThis number is the magic constant of 5 by 5 normal magic square:\n\nformula_1\n\nThis number is also the magic constant of n-Queens Problem for n = 5.\n\n65 is the smallest integer that can be expressed as a sum of two distinct positive squares in two ways, 65 = 8 + 1 = 7 + 4.\n\nIt appears in the Padovan sequence, preceded by the terms 28, 37, 49 (it is the sum of the first two of these).\n\nThere are only 65 known Euler's idoneal numbers.\n\n65 = 1 + 2 + 3 + 4 + 5.\n\n65 is the length of the hypotenuse of 4 different Pythagorean triangles, the lowest number to have more than 2: 65 = 16 + 63 = 33 + 56 = 39 + 52 = 25 + 60. The first two are \"primitive\", and 65 is the lowest number to be the largest side of more than one such triple.\n\n\n\n\n"}
{"id": "391878", "url": "https://en.wikipedia.org/wiki?curid=391878", "title": "85 (number)", "text": "85 (number)\n\n85 (eighty-five) is the natural number following 84 and preceding 86.\n85 is:\n\n\n\n\nIn U.S. college athletics, schools that are members of NCAA Division I are limited to providing athletic scholarships to a maximum of 85 football players in a given season. The specifics vary by the two Division I football subdivisions:\n\n\n\n\n"}
{"id": "12137424", "url": "https://en.wikipedia.org/wiki?curid=12137424", "title": "Akamai Foundation", "text": "Akamai Foundation\n\nThe Akamai Foundation is a private corporate foundation dedicated to fostering excellence in mathematics, with the aim of promoting math’s importance and encouraging America’s next generation of technology innovators.\n\nThe Akamai Foundation is a core tenet of Akamai’s overarching commitment to corporate responsibility. The company has a long history of supporting programs designed to attract more diversity to the technology industry through initiatives such as Akamai Technical Academy and Girls Who Code; providing disaster relief and humanitarian aid globally; enabling volunteerism by connecting employees to the communities in which Akamai operates; and promoting environmental sustainability through investments in alternative energy.\n"}
{"id": "32257774", "url": "https://en.wikipedia.org/wiki?curid=32257774", "title": "Algebraic cobordism", "text": "Algebraic cobordism\n\nIn mathematics, algebraic cobordism is an analogue of complex cobordism for smooth quasi-projective schemes over a field. It was introduced by .\n\nAn oriented cohomology theory on the category of smooth quasi-projective schemes Sm over a field \"k\" consists of a contravariant functor \"A\"* from Sm to commutative graded rings, together with push-forward maps \"f\" whenever \"f\":\"Y\"→\"X\" has relative dimension \"d\" for some \"d\". These maps have to satisfy various conditions similar to those satisfied by complex cobordism. In particular they are \"oriented\", which means roughly that they behave well on vector bundles; this is closely related to the condition that a generalized cohomology theory has a complex orientation.\n\nOver a field of characteristic 0, algebraic cobordism is the universal oriented cohomology theory for smooth varieties. In other words there is a unique morphism of oriented cohomology theories from algebraic cobordism to any other oriented cohomology theory.\n\nThe algebraic cobordism ring of generalized flag varieties has been computed by .\n\n"}
{"id": "44268432", "url": "https://en.wikipedia.org/wiki?curid=44268432", "title": "Automorphic Forms on GL(2)", "text": "Automorphic Forms on GL(2)\n\nAutomorphic Forms on GL(2) is a book by where they rewrite Hecke's theory of modular forms in terms of the representation theory of GL(2) over local fields and adele rings of global fields. A second volume by gives an interpretation of some results by Rankin and Selberg in terms of the representation theory of GL(2) × GL(2).\n\n"}
{"id": "51884528", "url": "https://en.wikipedia.org/wiki?curid=51884528", "title": "Autonomous agency theory", "text": "Autonomous agency theory\n\nAutonomous agency theory (AAT) is a viable system theory (VST) which models autonomous social complex adaptive systems. It can be used to model the relationship between an agency and its environment(s), and these may include other interactive agencies. The nature of that interaction is determined by both the agency's external and internal attributes and constraints. Internal attributes may include immanent dynamic \"self\" processes that drive agency change.\n\nStafford Beer coined the term \"viable systems\" in the 1950s, and developed it within his management cybernetics theories. He designed his viable system model as a diagnostic tool for organisational pathologies (conditions of social ill-health). This model involves a system concerned with operations and their direct management, and a meta-system that \"observes\" the system and controls it. Beer's work refers to Maturana's concept of autopoiesis, which explains why living systems actually live. However, Beer did not make general use the concept in his modelling process.\n\nIn the 1980s Eric Schwarz developed an alternative model from the principles of complexity science. This not only embraces the ideas of autopoiesis (self-production), but also autogenesis (self-creation) which responds to a proposition that living systems also need to learn to maintain their viability. Self-production and self-creation are both networks of processes that connect an operational system of agency structure from which behaviour arises, an observing relational meta-system, this itself observed by an \"existential\" meta-meta-system. As such Schwarz' VST constitutes a different paradigm from that of Beer.\n\nAAT is a development of Schwarz' paradigm through the addition of propositions setting it in a knowledge context.\n\nAAT is a generic modelling approach that has the capacity to anticipate future potentials for behaviour. Such anticipation occurs because behaviour in the agency as a living system is \"structure determined\", where the structure itself of the agency is responsible for that anticipation. This is like anticipating the behaviour of both a tiger or a giraffe when faced with food options. The tiger has a structure that allows it to have speed, strength and sharp inbuilt weapons to kill moving prey, but the giraffe has a structure that allows it to acquire its food in high places in a way the tiger could not duplicate. Even if a giraffe has the speed to chase prey, it does not have the resources to kill and eat it.\nAgency generic structure is a substructure defined by three systems that are, in general terms, referred to as:\nThese generic systems are ontologically distinct; their natures being determined by the context in which the autonomous agency exists. The substructure also maintains a superstructure that is constructed through context related propositional theory. Superstructural theory may include attributes of collective identity, cognition, emotion, personality; purpose and intention; self-reference, self-awareness, self-reflection, self-regulation and self-organisation. The substructural systems are connected by autopoietic and autogenetic networks of processes as shown in Figure 1 below.\n\nThe terminology becomes simplified when the existential system is taken to be culture, and it is recognised that Piaget's concept of operative intelligence is equivalent to autopoiesis, and his figurative intelligence to autogenesis. The noumenal system now becomes a personality system, and autonomous agency theory now becomes cultural agency theory (CAT). This is normally used to model plural situations like organisations or a nation states, when its personality system is taken to have normative characteristics (see also Normative personality), that is, driven by cultural norms as represented in Figure 2 below. This has developed further through mindset agency theory enabling agency behaviour to be anticipated.\n\nA feature of this modelling approach is that the properties of the cultural system act as an attractor for the agency as a whole, providing constraint for the properties of its personality and operative systems. This attraction ceases with cultural instability, when CAT reduces to instrumentality with no capacity to learn. Another feature is driven by possibilities of recursion permitted using Beer's proposition of viability law: every viable system contains and is contained in a viable system.\n\nCultural agency theory (CAT) as a development of AAT. It is principally used to model organisational contexts that have at least potentially stable cultures. The existential system of AAT becomes the cultural system, the figurative system become a normative personality, and the operative system now represents the organisational structure that facilitates and constrains behaviour.\n\nThe cultural system may be regarded as a (second order) \"observer\" of the instrumental couple that occurs between the normative personality and the operative system. The function of this couple is to manifest figurative attributes of the personality, like goals or ideology, operatively consequently influencing behaviour. This instrumental nature occurs through feedforward processes such that personality attributes can be processed for operative action. Where there are issues in doing this, feedback processes create imperatives for adjustment. This is like having a goal, and finding that it cannot be implemented, thereby having to reconsider the goal. This instrumental couple can also be seen in terms of the operative system and its first order \"observing\" system, the normative personality. So, while personality is a first order \"observer\" of CAT's operative system, it is ultimately directed by its second order cultural \"observer\" system.\n\nA development of this has occurred using trait theory from psychology. Unlike other trait theories of personality, this adopts epistemic traits that centres on values, an approach that tends to be more stable (since basic values tend to be stable) in terms of personality testing and retesting, than other approaches that use (for instance) agency preferences (like Myers-Briggs Type Indicator) that may change between test and retest. This trait theory for the normative personality is called mindset agency theory, and is a development of Maruyama's Mindscape Theory.\n\nThe cognitive process by which personality is represented through epistemic trait functions (called types), can be explained through both instrumental and epistemic rationality, where instrumental rationality (also referred to as utilitarian, and related to the expectations about the behaviour of other human beings or objects in the environment given some cognitive basis for those expectation) is independent of, if constrained by, epistemic rationality (related to the formation of beliefs in an unbiased manner, normally set in terms of believable propositions: due to their being strongly supported by evidence, as opposed to being agnostic towards propositions that are unsupported by \"sufficient\" evidence, whatever this means). Applications of CAT could be found in social, political and economical sciences, for instance recend studies analyzed Donald Trump and Theresa May personalities.\n\nStafford Beer's (1979) viable system model is a well-known diagnostic model that comes out of his management cybernetics paradigm. Related to this is the idea of first order and second order cybernetics. Cybernetics is concerned with feedforward and feedback processes, and first order cybernetics is concerned with this relationship between the system and its environment. Second order cybernetics is concerned with the relationship between the system and its internal meta-system (that some refer to as \"the observer\" of the system). Von Foerster has referred to second order cybernetics as the \"cybernetics of cybernetics\". While attempts to explore higher orders of cybernetics have been made, no development into a general theory of higher cybernetic orders has emerged from this paradigm. \nIn contrast, extending the principles of autonomous agency theory, a generic model has been formulated for the generation of higher cybernetic orders, developed using the concepts of recursion and incursion as proposed by Dubois. The model is reflective, for instance, of processes of knowledge creation for community learning and symbolic convergence theory. This nth order theory of cybernetics links with \"the cybernetics of cybernetics\" by assigning to its second order cybernetic concept inferences that may arise from any higher order cybernetics that may exist, if unperceived. The network of processes in this general representation of higher cybernetic orders is expressed in terms of orders of autopoiesis, so that for instance autogenesis may be seen as a second order of autopoiesis.\n\n"}
{"id": "40880715", "url": "https://en.wikipedia.org/wiki?curid=40880715", "title": "Beneš method", "text": "Beneš method\n\nIn queueing theory, a discipline within the mathematical theory of probability, Beneš approach or Beneš method is a result for an exact or good approximation to the probability distribution of queue length. It was introduced by Václav E. Beneš in 1963.\n\nThe method introduces a quantity referred to as the \"virtual waiting time\" to define the remaining workload in the queue at any time. This process is a step function which jumps upward with new arrivals to the system and otherwise is linear with negative gradient. By giving a relation for the distribution of unfinished work in terms of the excess work, the difference between arrivals and potential service capacity, it turns a time-dependent virtual waiting time problem into \"an integral that, in principle, can be solved.\"\n"}
{"id": "21391406", "url": "https://en.wikipedia.org/wiki?curid=21391406", "title": "Benson Mates", "text": "Benson Mates\n\nBenson Mates (May 19, 1919 in Portland, Oregon – May 14, 2009 in Berkeley, California) was an American philosopher, noted for his work in logic, the history of philosophy, and skepticism. Mates studied philosophy and mathematics at the University of Oregon, Cornell University, and the University of California at Berkeley. Some of his teachers included J. Barkley Rosser, Harold Cherniss, and Alfred Tarski. From 1948 until his retirement in 1989, he was a professor of philosophy at Berkeley. He remained Professor Emeritus of philosophy at University of California at Berkeley until his death.\n\nMates's 1948 dissertation, \"On the Logic of the Old Stoa\", formed the basis for his 1953 book \"Stoic Logic\", of which Peter Geach wrote, \"Stoic logic is a difficult subject [...] Dr. Mates's monograph is a strenuous and successful effort to overcome these difficulties.\" Mates's 1965 book, \"Elementary Logic\", remains a widely used introductory textbook in symbolic logic. His 1986 study of Leibniz is also highly regarded.\n\nIn his own philosophical work, Mates defends a stance akin to Pyrrhonian skepticism. He argues that the major problems of philosophy (such as the liar paradox, the existence of an external world, and free will) are intelligible and non-trivial yet utterly defy solution. Unlike the classical Pyrrhonists, however, Mates finds that skeptical arguments lead to unsatisfactory perplexity rather than ataraxia.\n\n\n"}
{"id": "1011270", "url": "https://en.wikipedia.org/wiki?curid=1011270", "title": "Bourbaki–Witt theorem", "text": "Bourbaki–Witt theorem\n\nIn mathematics, the Bourbaki–Witt theorem in order theory, named after Nicolas Bourbaki and Ernst Witt, is a basic fixed point theorem for partially ordered sets. It states that if \"X\" is a non-empty chain complete poset, and \n\nsuch that \n\nthen \"f\" has a fixed point. Such a function \"f\" is called \"inflationary\" or \"progressive\".\n\nIf the poset \"X\" is finite then the statement of the theorem has a clear interpretation that leads to the proof. The sequence of successive iterates, \n\nwhere \"x\" is any element of \"X\", is monotone increasing. By the finiteness of \"X\", it stabilizes:\n\nIt follows that \"x\" is a fixed point of \"f\".\n\nPick some formula_6. Define a function \"K\" recursively on the ordinals as follows:\n\nIf formula_9 is a limit ordinal, then by construction \n\nis a chain in \"X\". Define \n\nThis is now an increasing function from the ordinals into \"X\". It cannot be strictly increasing, as if it were we would have an injective function from the ordinals into a set, violating Hartogs' lemma. Therefore the function must be eventually constant, so for some \n\nthat is,\n\nSo letting \n\nwe have our desired fixed point. Q.E.D.\n\nThe Bourbaki–Witt theorem has various important applications. One of the most common is in the proof that the axiom of choice implies Zorn's lemma. We first prove it for the case where \"X\" is chain complete and has no maximal element. Let \"g\" be a choice function on \n\nDefine a function \n\nby \n\nThis is allowed as, by assumption, the set is non-empty. Then \"f\"(\"x\") > \"x\", so \"f\" is an inflationary function with no fixed point, contradicting the theorem.\n\nThis special case of Zorn's lemma is then used to prove the Hausdorff maximality principle, that every poset has a maximal chain, which is easily seen to be equivalent to Zorn's Lemma.\n\nBourbaki–Witt has other applications. In particular in computer science, it is used in the theory of computable functions.\nIt is also used to define recursive data types, e.g. linked lists, in domain theory.\n\n"}
{"id": "56835844", "url": "https://en.wikipedia.org/wiki?curid=56835844", "title": "Cryptographic multilinear map", "text": "Cryptographic multilinear map\n\nA cryptographic formula_1-multilinear map is a kind of multilinear map, that is, a function formula_2 such that for any integers formula_3 and elements formula_4, formula_5, and which in addition is efficiently computable and satisfy some security properties. It has several applications on cryptography, as key exchange protocols, identity-based encryption, and broadcast encryption. There exist constructions of cryptographic 2-multilinear maps, known as bilinear maps, however, the problem of constructing such multilinear maps for formula_6 seems much more difficult and the security of the proposed candidates is still unclear.\n\nIn this case, multilinear maps are mostly known as bilinear maps or parings, and they are usually defined as follows: Let formula_7 be two additive cyclic groups of prime order formula_8, and formula_9 another cyclic group of order formula_8 written multiplicatively. A pairing is a map: formula_11, which satisfies the following properties:\n\nIn addition, for security purposes, the discrete logarithm problem is required to be hard in both formula_15 and formula_16.\n\nWe say that a map formula_2 is a formula_1-multilinear map if it satisfies the following properties:\n\nIn addition, for security purposes, the discrete logarithm problem is required to be hard in formula_31.\n"}
{"id": "1554404", "url": "https://en.wikipedia.org/wiki?curid=1554404", "title": "De Finetti diagram", "text": "De Finetti diagram\n\nA de Finetti diagram is a ternary plot used in population genetics. It is named after the Italian statistician Bruno de Finetti (1906–1985) and is used to graph the genotype frequencies of populations, where there are two alleles and the population is diploid. It is based on an equilateral triangle, and Viviani's theorem concerning any point within the triangle, and the three lines from that point that are perpendicular to the sides of the triangle. The sum of the lengths of the lines is a fixed value, regardless of the position of the point. This value (the sum of the lengths) is set to be 1.\n\nThe de Finetti diagram has been put to extensive use in population genetics by A.W.F. Edwards in his book \"Foundations of Mathematical Genetics\". In its simplest form the diagram can be used to show the range of genotype frequencies for which Hardy-Weinberg equilibrium is satisfied (the curve within the diagram). A.W.F. Edwards and Chris Cannings extended its use to demonstrate the changes that occur in allele frequencies under natural selection.\n\n\n\n"}
{"id": "53734", "url": "https://en.wikipedia.org/wiki?curid=53734", "title": "Double-precision floating-point format", "text": "Double-precision floating-point format\n\nDouble-precision floating-point format is a computer number format, usually occupying 64 bits in computer memory; it represents a wide dynamic range of numeric values by using a floating radix point.\n\nFloating point is used to represent fractional values, or when a wider range is needed than is provided by fixed point (of the same bit width), even if at the cost of precision. Double precision may be chosen when the range or precision of single precision would be insufficient.\n\nIn the IEEE 754-2008 standard, the 64-bit base-2 format is officially referred to as binary64; it was called double in IEEE 754-1985. IEEE 754 specifies additional floating-point formats, including 32-bit base-2 \"single precision\" and, more recently, base-10 representations.\n\nOne of the first programming languages to provide single- and double-precision floating-point data types was Fortran. Before the widespread adoption of IEEE 754-1985, the representation and properties of floating-point data types depended on the computer manufacturer and computer model, and upon decisions made by programming-language implementers. E.g., GW-BASIC's double-precision data type was the 64-bit MBF floating-point format.\nDouble-precision binary floating-point is a commonly used format on PCs, due to its wider range over single-precision floating point, in spite of its performance and bandwidth cost. As with single-precision floating-point format, it lacks precision on integer numbers when compared with an integer format of the same size. It is commonly known simply as \"double\". The IEEE 754 standard specifies a binary64 as having:\nThe sign bit determines the sign of the number (including when this number is zero, which is signed).\n\nThe exponent field can be interpreted as either an 11-bit signed integer from −1024 to 1023 (2's complement) or an 11-bit unsigned integer from 0 to 2047, which is the accepted biased form in the IEEE 754 binary64 definition. If the unsigned integer format is used, the exponent value used in the arithmetic is the exponent shifted by a bias – for the IEEE 754 binary64 case, an exponent value of 1023 represents the actual zero (i.e. for 2 to be one, e must be 1023). Exponents range from −1022 to +1023 because exponents of −1023 (all 0s) and +1024 (all 1s) are reserved for special numbers.\n\nThe 53-bit significand precision gives from 15 to 17 significant decimal digits precision (2 ≈ 1.11 × 10). If a decimal string with at most 15 significant digits is converted to IEEE 754 double-precision representation, and then converted back to a decimal string with the same number of digits, the final result should match the original string. If an IEEE 754 double-precision number is converted to a decimal string with at least 17 significant digits, and then converted back to double-precision representation, the final result must match the original number.\n\nThe format is written with the significand having an implicit integer bit of value 1 (except for special data, see the exponent encoding below). With the 52 bits of the fraction significand appearing in the memory format, the total precision is therefore 53 bits (approximately 16 decimal digits, 53 log(2) ≈ 15.955). The bits are laid out as follows:\n\nThe real value assumed by a given 64-bit double-precision datum with a given biased exponent formula_1 \nand a 52-bit fraction is\nor\n\nBetween 2=4,503,599,627,370,496 and 2=9,007,199,254,740,992 the representable numbers are exactly the integers. For the next range, from 2 to 2, everything is multiplied by 2, so the representable numbers are the even ones, etc. Conversely, for the previous range from 2 to 2, the spacing is 0.5, etc.\n\nThe spacing as a fraction of the numbers in the range from 2 to 2 is 2.\nThe maximum relative rounding error when rounding a number to the nearest representable one (the machine epsilon) is therefore 2.\n\nThe 11 bit width of the exponent allows the representation of numbers between 10 and 10, with full 15–17 decimal digits precision. By compromising precision, the subnormal representation allows even smaller values up to about 5 × 10.\n\nThe double-precision binary floating-point exponent is encoded using an offset-binary representation, with the zero offset being 1023; also known as exponent bias in the IEEE 754 standard. Examples of such representations would be:\nThe exponents codice_1 and codice_2 have a special meaning:\nwhere F is the fractional part of the significand. All bit patterns are valid encoding.\n\nExcept for the above exceptions, the entire double-precision number is described by:\n\nformula_4\n\nIn the case of subnormals (\"e\"=0) the double-precision number is described by:\n\nformula_5\n\nEncodings of qNaN and sNaN are not completely specified in IEEE 754 and depend on the processor. Most processors, such as the x86 family and the ARM family processors, use the most significant bit of the significand field to indicate a quiet NaN; this is what is recommended by IEEE 754. The PA-RISC processors use the bit to indicate a signaling NaN.\n\nBy default, / rounds down, instead of up like single precision, because of the odd number of bits in the significand.\n\nIn more detail:\n\nUsing double-precision floating-point variables and mathematical functions (e.g., sin, cos, atan2, log, exp and sqrt) are slower than working with their single precision counterparts. One area of computing where this is a particular issue is for parallel code running on GPUs. For example, when using NVIDIA's CUDA platform, calculations with double precision take approximately 2 times longer to complete compared to those done using single precision.\n\nDoubles are implemented in many programming languages in different ways such as the following. On processors with only dynamic precision, such as x86 without SSE2 (or when SSE2 is not used, for compatibility purpose) and with extended precision used by default, software may have difficulties to fulfill some requirements.\n\nC and C++ offer a wide variety of arithmetic types. Double precision is not required by the standards (except by the optional annex F of C99, covering IEEE 754 arithmetic), but on most systems, the codice_7 type corresponds to double precision. However, on 32-bit x86 with extended precision by default, some compilers may not conform to the C standard and/or the arithmetic may suffer from double rounding.\n\nCommon Lisp provides the types SHORT-FLOAT, SINGLE-FLOAT, DOUBLE-FLOAT and LONG-FLOAT. Most implementations provide SINGLE-FLOATs and DOUBLE-FLOATs with the other types appropriate synonyms. Common Lisp provides exceptions for catching floating-point underflows and overflows, and the inexact floating-point exception, as per IEEE 754. No infinities and NaNs are described in the ANSI standard, however, several implementations do provide these as extensions.\n\nAs specified by the ECMAScript standard, all arithmetic in JavaScript shall be done using double-precision floating-point arithmetic.\n"}
{"id": "29321898", "url": "https://en.wikipedia.org/wiki?curid=29321898", "title": "Eguchi–Hanson space", "text": "Eguchi–Hanson space\n\nIn mathematics and theoretical physics, the Eguchi–Hanson space is a non-compact, self-dual, asymptotically locally Euclidean (ALE) metric on the cotangent bundle of the 2-sphere \"T\"\"S\". The holonomy group of this 4-real-dimensional manifold is SU(2), as it is for a Calabi-Yau K3 surface. While the metric is generally attributed to the physicists Eguchi and Hanson, it was actually discovered independently by the mathematician Eugenio Calabi around the same time.\n\nThe Eguchi-Hanson metric has Ricci tensor equal to zero, making it a solution to the vacuum Einstein equations of general relativity, albeit with Riemannian rather than Lorentzian metric signature. It may be regarded as a resolution of the \"A\" singularity according to the ADE classification which is the singularity at the fixed point of the \"C\"/\"Z\" orbifold where the \"Z\" group inverts the signs of both complex coordinates in \"C\".\n\nAside from its inherent importance in pure geometry, the space is important in string theory. Certain types of K3 surfaces can be approximated as a combination of several Eguchi–Hanson metrics.\n\nThe Eguchi–Hanson metric is the prototypical example of a gravitational instanton.\n"}
{"id": "45336140", "url": "https://en.wikipedia.org/wiki?curid=45336140", "title": "Enumerator (computer science)", "text": "Enumerator (computer science)\n\nAn enumerator is a Turing machine that lists, possibly with repetitions, elements of some set \"S\", which it is said to enumerate. A set enumerated by some enumerator is said to be recursively enumerable.\n\nAn enumerator formula_1 is usually represented as a 2-tape Turing machine. One working tape, and one print tape. It can be defined by a 7-tuple, following the notation of a Turing machine:\nformula_2\nwhere\n"}
{"id": "33877779", "url": "https://en.wikipedia.org/wiki?curid=33877779", "title": "Financial correlation", "text": "Financial correlation\n\nFinancial correlations measure the relationship between the changes of two or more financial variables over time. For example, the prices of equity stocks and fixed interest bonds often move in opposite directions: when investors sell stocks, they often use the proceeds to buy bonds and vice versa. In this case, stock and bond prices are negatively correlated.\n\nFinancial correlations play a key role in modern finance. Under the capital asset pricing model (CAPM; a model recognised by a Nobel prize), an increase in diversification increases the return/risk ratio. Measures of risk include value at risk, expected shortfall, and portfolio return variance.\n\nThere are several statistical measures of the degree of financial correlations. The Pearson product-moment correlation coefficient is sometimes applied to finance correlations. However, the limitations of Pearson correlation approach in finance are evident. First, linear dependencies as assessed by the Pearson correlation coefficient do not appear often in finance. Second, linear correlation measures are only natural dependence measures if the joint distribution of the variables is elliptical. However, only few financial distributions such as the multivariate normal distribution and the multivariate student-t distribution are special cases of elliptical distributions, for which the linear correlation measure can be meaningfully interpreted. Third, a zero Pearson product-moment correlation coefficient does not necessarily mean independence, because only the two first moments are considered. For example, formula_1 (\"y\" ≠ 0) will lead to Pearson correlation coefficient of zero, which is arguably misleading. Since the Pearson approach is unsatisfactory to model financial correlations, quantitative analysts have developed specific financial correlation measures. Accurately estimating correlations requires the modeling process of marginals to incorporate characteristics such as skewness and kurtosis. Not accounting for these attributes can lead to severe estimation error in the correlations and covariances that have negative biases (as much as 70% of the true values). In a practical application in portfolio optimization, accurate estimation of the variance-covariance matrix is paramount. Thus, forecasting with Monte-Carlo simulation with the Gaussian copula and well-specified marginal distributions are effective.\n\nSteven Heston applied a correlation approach to negatively correlate stochastic stock returns formula_2 and stochastic volatility formula_3. The core equations of the original Heston model are the two stochastic differential equations, SDEs\n\nand\n\nwhere S is the underlying stock, formula_6 is the expected growth rate of formula_7, and formula_3 is the stochastic volatility of formula_7 at time t. In equation (2), g is the mean reversion rate (gravity), which pulls the variance formula_10 to its long term mean formula_11, and formula_12 is the volatility of the volatility σ(t). dz(t) is the standard Brownian motion, i.e. formula_13, formula_14 is i.i.d., in particular formula_15 is a random drawing from a standardized normal distribution n~(0,1). In equation (1), the underlying formula_7 follows the standard geometric Brownian motion, which is also applied in Black–Scholes–Merton model, which however assumes constant volatility. \nThe correlation between the stochastic processes (1) and (2) is introduced by correlating the two Brownian motions formula_17 and formula_18. The instantaneous correlation formula_19 between the Brownian motions is\n\nThe definition (3) can be conveniently modeled with the identity\n\nwhere formula_22 and formula_23 are independent, and formula_24 and formula_25 are independent, t ≠ t’.\n\nThe Cointelation SDE connects the SDE's above to the concept of mean reversion and drift which are usually concepts that are misunderstood by practitioners.\n\nA further financial correlation measure, is the binomial correlation approach of Lucas (1995). We define the binomial events formula_26 and formula_27 where formula_28 is the default time of entity formula_29 and formula_30 is the default time of entity formula_31. Hence if entity formula_29 defaults before or at time formula_33, the random indicator variable formula_34 will take the value in 1, and 0 otherwise. The same applies to formula_31. Furthermore, formula_36 and formula_37 is the default probability of formula_29 and formula_31 respectively, and formula_40 is the joint probability of default. The standard deviation of a one-trial binomial event is formula_41, where P is the probability of outcome X. Hence, we derive the joint default dependence coefficient of the binomial events formula_42and formula_43 as\n\nBy construction, equation (5) can only model binomial events, for example default and no default. The binomial correlation approach of equation (5) is a limiting case of the Pearson correlation approach discussed in section 1. As a consequence, the significant shortcomings of the Pearson correlation approach for financial modeling apply also to the binomial correlation model.\n\nA fairly recent, famous as well as infamous correlation approach applied in finance is the copula approach. Copulas go back to Sklar (1959). Copulas were introduced to finance by Vasicek (1987) and Li (2000).\n\nCopulas simplify statistical problems. They allow the joining of multiple univariate distributions to a single multivariate distribution. Formally, a copula function C transforms an n-dimensional function on the interval [0,1] into a unit-dimensional one:\n\nMore explicitly, let formula_46 be a uniform random vector with formula_47 and formula_48. Then there exists a copula function formula_49 such that\n\nwhere F is the joint cumulative distribution function and formula_51, \"i\" = 1, …, \"n\" are the univariate marginal distributions. formula_52 is the inverse of formula_51. If the marginal distributions formula_54 are continuous, it follows that C is unique. For properties and proofs of equation (11), see Sklar (1959) and Nelsen (2006). \nNumerous types of copula functions exist. They can be broadly categorized in one-parameter copulas as the Gaussian copula, and the Archimedean copula, which comprise Gumbel, Clayton and Frank copulas. Often cited two-parameter copulas are student-t, Fréchet, and Marshall-Olkin. For an overview of these copulas, see Nelsen (2006). \n\nHence, from equations (7) and (8) we derive the Gaussian default time copula CGD, \nIn equation (9) the terms formula_58 map the cumulative default probabilities Q of asset i for time t, formula_55, percentile to percentile to standard normal. The mapped standard normal marginal distributions formula_60 are then joined to a single n-variate distribution formula_61 by applying the correlation structure of the multivariate normal distribution with correlation matrix R. The probability of n correlated defaults at time t is given by formula_61.\n\nNumerous non-academic articles have been written demonizing the copula approach and blaming it for the 2007/2008 global financial crisis, see for example Salmon 2009, Jones 2009, and Lohr 2009. .\n\n(a) Tail dependence\n\nIn a crisis, financial correlations typically increase, see studies by Das, Duffie, Kapadia, and Saita (2007) and Duffie, Eckner, Horel and Saita (2009) and references therein. Hence it would be desirable to apply a correlation model with high co-movements in the lower tail of the joint distribution. It can be mathematically shown that the Gaussian copula has relative low tail dependence, as seen in the following scatter plots.\nFigure 1: Scatter plots of different copula models\n\nAs seen in Figure 1b, the student-t copula exhibits higher tail dependence and might be better suited to model financial correlations. Also, as seen in Figure 1(c), the Gumbel copula exhibits high tail dependence especially for negative co-movements. Assuming that correlations increase when asset prices decrease, the Gumbel copula might also be a good correlation approach for financial modeling.\n\n(b) Calibration\n\nA further criticism of the Gaussian copula is the difficulty to calibrate it to market prices. In practice, typically a single correlation parameter (not a correlation matrix) is used to model the default correlation between any two entities in a collateralized debt obligation, CDO. Conceptually this correlation parameter should be the same for the entire CDO portfolio. However, traders randomly alter the correlation parameter for different tranches, in order to derive desired tranche spreads. Traders increase the correlation for ‘extreme’ tranches as the equity tranche or senior tranches, referred to as the correlation smile. This is similar to the often cited implied volatility smile in the Black–Scholes–Merton model. Here traders increase the implied volatility especially for out-of-the money puts, but also for out-of-the money calls to increase the option price..\n\nIn a mean-variance optimization framework, accurate estimation of the variance-covariance matrix is paramount. Thus, forecasting with Monte-Carlo simulation with the Gaussian copula and well-specified marginal distributions are effective. Allowing the modeling process to allow for empirical characteristics in stock returns such as auto-regression, asymmetric volatility, skewness, and kurtosis is important. Not accounting for these attributes lead to severe estimation error in the correlations and variances that have negative biases (as much as 70% of the true values).\n\n(c) Risk management\n\nA further criticism of the Copula approach is that the copula model is static and consequently allows only limited risk management, see Finger (2009) or Donnelly and Embrechts (2010). The original copulas models of Vasicek (1987) and Li (2000) and several extensions of the model as Hull and White (2004) or Gregory and Laurent (2004) do have a one period time horizon, i.e. are static. In particular, there is no stochastic process for the critical underlying variables default intensity and default correlation. However, even in these early copula formulations, back testing and stress testing the variables for different time horizons can give valuable sensitivities, see Whetten and Adelson (2004) and Meissner, Hector, and. Rasmussen (2008). \nIn addition, the copula variables can be made a function of time as in Hull, Predescu, and White (2005). This still does not create a fully dynamic stochastic process with drift and noise, which allows flexible hedging and risk management. The best solutions are truly dynamic copula frameworks, see section ‘Dynamic Copulas’ below.\n\nBefore the global 2007–08 financial crisis, numerous market participants trusted the copula model uncritically and naively. However, the 2007–08 crisis was less a matter of a particular correlation model, but rather an issue of \"irrational complacency\". In the extremely benign time period from 2003 to 2006, proper hedging, proper risk management and stress test results were largely ignored. The prime example is AIG’s London subsidiary, which had sold credit default swaps and collateralized debt obligations in an amount of close to $500 billion without conducting any major hedging. For an insightful paper on inadequate risk management leading up to the crisis, see “A personal view of the crisis – Confessions of a Risk Manager” (The Economist 2008). In particular, if any credit correlation model is fed with benign input data as low default intensities and low default correlation, the risk output figures will be benign, ‘garbage in garbage out’ in modeling terminology.\n\nA core enhancement of copula models are dynamic copulas, introduced by Albanese et al. (2005) and (2007). The \"dynamic conditioning\" approach models the evolution of multi-factor super-lattices, which correlate the return processes of each entity at each time step. Binomial dynamic copulas apply combinatorial methods to avoid Monte Carlo simulations. Richer dynamic Gaussian copulas apply Monte Carlo simulation and come at the cost of requiring powerful computer technology.\n\nIn order to avoid specifying the default correlation between each entity pair in a portfolio a factorization is often applied. This leads to conditionally independent default (CID) modeling. The most widely applied CID model is the one-factor Gaussian copula (OFGC) model. It was the de facto market model for pricing CDOs before the 2007/2008 global financial crisis. The core equation of the OFGC model\nwhere formula_64 and formula_65 are random drawings from formula_66 and formula_67. As a result, the latent variable formula_68, sometimes interpreted as the asset value of i, see Turc, Very, Benhamou and Alvarez et al. (2005), is also n~(0,1). The common factor formula_64 can be interpreted as the economic environment, possibly represented by the return of the S&P 500. formula_65 is the idiosyncratic component, the ‘strength’ of entity i, possibly measured by entity i’s stock price return. From equation (10) we see, that the correlation between entities i is modeled indirectly by conditioning the latent variable formula_68 on the common factor formula_64. For example, for p =1, the latent variables of all entities formula_73, so the formula_68 are identical in each simulation. For p = 0, all latent variable for all entities formula_75, hence the formula_68 are independent. Importantly, once we fix the value of M, the defaults of the n entities are (conditionally on M) mutually independent.\n\nAs of 2010, the OFGC is the basis for credit risk management in Basel II. The benefits of the model are simplicity and intuition. One of the main shortcomings of the model is that traders when pricing CDOs randomly alter the correlation parameter for different CDO tranches to achieve desired tranche spreads. However conceptually, the correlation parameter should be identical for the whole portfolio.\n\nContagion default modeling can be viewed as a variation of CID modeling. As discussed in section 2.3, in the CID framework, correlation is modeled by conditioning on a common market factor M, which impacts all entities to the same degree. The lower the random drawing for M, the higher is the default intensity of all entities (unless ρ = 0). Hence CID modeling can elucidate default clustering. In contrast, contagion approaches model the default intensity of an entity as a function of the default of another entity. Hence contagion default modeling incorporates counterparty risk, i.e. the direct impact of a defaulting entity on the default intensity of another entity. In particular, after a default of a particular entity, the default intensity of all assets in the portfolio increases. This default contagion then typically fades exponentially to non-contagious default intensity levels. See the papers of Davis and Lo (2001) and Jarrow and Yu (2001), who pioneered contagion default modeling.\n\nWithin the credit correlation modeling framework, a fairly new correlation approach is top–down modeling. Here the evolution of the portfolio intensity distribution is derived directly, i.e. abstracting from the default intensities of individual entities. Top-down models are typically applied in practice if:\n\n\nTop–down models are typically more parsimonious, computationally efficient and can often be calibrated better to market prices than bottom–up models. Although seemingly important information such as the default intensities of individual entities is disregarded, a top-down model can typically better capture properties of the portfolio such as volatility or correlation smiles. In addition, the default information of individual entities can often be inferred by random thinning techniques, see Giesecke, Goldberg and Ding (2007) for details.\nWithin the top-down framework, Schönbucher (2006) creates a time-inhomogeneous Markov-chain of transition rates. Default correlation is introduced by changes in the volatility of transition rates. For certain parameter constellations, higher volatility means faster transition to lower states as default, and as a consequence implies higher default correlation, and vice versa. Similarly, Hurd and Kuznetsov (2006a) and (2006b) induce correlation by a random change in the speed of time. A faster speed of time means faster transition to a lower state, possibly default, and as a result increases default correlation, and vice versa. For a comparative analysis of correlation approaches in finance, see Albanese, Li, Lobachevskiy, and Meissner (2010).\n"}
{"id": "1773596", "url": "https://en.wikipedia.org/wiki?curid=1773596", "title": "Finite model theory", "text": "Finite model theory\n\nFinite model theory (FMT) is a subarea of model theory (MT). MT is the branch of mathematical logic which deals with the relation between a formal language (syntax) and its interpretations (semantics). FMT is a restriction of MT to interpretations on finite structures, which have a finite universe.\n\n\nA single finite structure can always be axiomatized in first-order logic, where axiomatized in a language L means described uniquely up to isomorphism by a single L-sentence. Similarly, any finite collection of finite structures can always be axiomatized in first-order logic. Some, but not all, infinite collections of finite structures can also be axiomatized by a single first-order sentence.\n\nIs a language L expressive enough to axiomatize a single finite structure S?\nA structure like (1) in the figure can be described by FO sentences in the logic of graphs like\n\n\nHowever, these properties do not axiomatize the structure, since for structure (1') the above properties hold as well, yet structures (1) and (1') are not isomorphic.\n\nInformally the question is whether by adding enough properties, these properties together describe exactly (1) and are valid (all together) for no other structure (up to isomorphism).\n\nFor a single finite structure it is always possible to precisely describe the structure by a single FO sentence. The principle is illustrated here for a structure with one binary relation formula_4 and without constants:\n\nall for the same tuple formula_13, yielding the FO sentence formula_14.\n\nThe method of describing a single structure by means of a first-order sentence can easily be extended for any fixed number of structures. A unique description can be obtained by the disjunction of the descriptions for each structure. For instance, for two structures formula_15 and formula_16 with defining sentences formula_17 and formula_18 this would be\n\nBy definition, a set containing an infinite structure falls outside the area that FMT deals with. Note that infinite structures can never be discriminated in FO, because of the Löwenheim–Skolem theorem, which implies that no first-order theory with an infinite model can have a unique model up to isomorphism.\n\nThe most famous example is probably Skolem's theorem, that there is a countable non-standard model of arithmetic.\n\nIs a language L expressive enough to describe exactly (up to isomorphism) those finite structures that have certain property P?\nThe descriptions given so far all specify the number of elements of the universe. Unfortunately most interesting sets of structures are not restricted to a certain size, like all graphs that are trees, are connected or are acyclic. Thus to discriminate a finite number of structures is of special importance.\n\nInstead of a general statement, the following is a sketch of a methodology to differentiate between structures that can and cannot be discriminated.\n\n1. The core idea is that whenever one wants to see if a Property P can be expressed in FO, one chooses structures A and B, where A does have P and B doesn't. If for A and B the same FO sentences hold, then P cannot be expressed in FO. In short:\n\nformula_20 and formula_21\n\nwhere formula_21 is shorthand for formula_23 for all FO-sentences α, and P represents the class of structures with property P.\n\n2. The methodology considers countably many subsets of the language, the union of which forms the language itself. For instance, for FO consider classes FO[m] for each m. For each m the above core idea then has to be shown. That is:\n\nformula_20 and formula_25\n\nwith a pair formula_26 for each formula_27 and α (in ≡) from FO[m]. It may be appropriate to choose the classes FO[m] to form a partition of the language.\n\n3. One common way to define FO[m] is by means of the quantifier rank qr(α) of a FO formula α, which expresses the depth of quantifier nesting. For example, for a formula in prenex normal form, qr is simply the total number of its quantifiers. Then FO[m] can be defined as all FO formulas α with qr(α) ≤ m (or, if a partition is desired, as those FO formulas with quantifier rank equal to m).\n\n4. Thus it all comes down to showing formula_23 on the subsets FO[m]. The main approach here is to use the algebraic characterization provided by Ehrenfeucht–Fraïssé games. Informally, these take a single partial isomorphism on A and B and extend it m times, in order to either prove or disprove formula_25, dependent on who wins the game.\n\nWe want to show that the property that the size of an orderered structure A=(A, ≤) is even, can not be expressed in FO.\n\n1. The idea is to pick A ∈ EVEN and B ∉ EVEN, where EVEN is the class of all structures of even size.\n\n2. We start with two ordered structures A and B with universes A = {1, 2, 3, 4} and B = {1, 2, 3}. Obviously A ∈ EVEN and B ∉ EVEN.\n\n3. For m = 2, we can now show* that in a 2-move Ehrenfeucht–Fraïssé game on A and B the duplicator always wins, and thus A and B cannot be discriminated in FO[2], i.e. A formula_30 α ⇔ B formula_30 α for every α ∈ FO[2].\n4. Next we have to scale the structures up by increasing m. For example, for m = 3 we must find an A and B such that the duplicator always wins the 3-move game. This can be achieved by A = {1, ..., 8} and B = {1, ..., 7}. More generally, we can choose A = {1, ..., 2} and B = {1, ..., 2-1}; for any m the duplicator always wins the m-move game for this pair of structures*.\n\n5. Thus EVEN on finite ordered structures cannot be expressed in FO.\n\n(*) Note that the proof of the result of the Ehrenfeucht–Fraïssé game has been omitted, since it is not the main focus here.\n\nA substantial fragment of SQL (namely that which is effectively relational algebra) is based on first-order logic (more precisely can be translated in domain relational calculus by means of Codd's theorem), as the following example illustrates: Think of a database table \"GIRLS\" with the columns \"FIRST_NAME\" and \"LAST_NAME\". This corresponds to a binary relation, say G(f, l) on FIRST_NAME X LAST_NAME. The FO query {l : G('Judy', l)}, which returns all the last names where the first name is 'Judy', would look in SQL like this:\n\nNotice, we assume here, that all last names appear only once (or we should use SELECT DISTINCT since we assume that relations and answers are sets, not bags).\n\nNext we want to make a more complex statement. Therefore, in addition to the \"GIRLS\" table we have a table \"BOYS\" also with the columns \"FIRST_NAME\" and \"LAST_NAME\". Now we want to query the last names of all the girls that have the same last name as at least one of the boys. The FO query is {(f,l) : ∃h ( G(f, l) ∧ B(h, l) )}, and the corresponding SQL statement is:\n\nNotice that in order to express the \"∧\" we introduced the new language element \"IN\" with a subsequent select statement. This makes the language more expressive for the price of higher difficulty to learn and implement. This is a common trade-off in formal language design. The way shown above (\"IN\") is by far not the only one to extend the language. An alternative way is e.g. to introduce a \"JOIN\" operator, that is:\n\nFirst-order logic is too restrictive for some database applications, for instance because of its inability to express transitive closure. This has led to more powerful constructs being added to database query languages, such as recursive WITH in . More expressive logics, like fixpoint logics, have therefore been studied in finite model theory because of their relevance to database theory and applications.\n\nNarrative data contains no defined relations. Thus the logical structure of text search queries can be expressed in Propositional Logic, like in:\n\nNote that the challenges in full text search are different from database querying, like ranking of results.\n\n\n"}
{"id": "17799610", "url": "https://en.wikipedia.org/wiki?curid=17799610", "title": "Formalized Music", "text": "Formalized Music\n\nFormalized Music: Thought and Mathematics in Composition is a book by Greek composer, architect, and engineer Iannis Xenakis in which he explains his motivation, philosophy, and technique for composing music with stochastic mathematical functions. It was published in Paris in 1963 as \"Musiques formelles: nouveaux principes formels de composition musicale\" as a special double issue of \"La Revue musicale\" and republished in an expanded edition in 1981 in Paris by Stock Musique. It was later translated into English with three added chapters and published in 1971 by Indiana University Press, republished in 1992 by Pendragon Press with a second edition published in 2001, also by Pendragon. The book contains the complete FORTRAN program code for one of Xenakis's early computer music composition programs GENDY. It has been described as a groundbreaking work.\n\n"}
{"id": "54352245", "url": "https://en.wikipedia.org/wiki?curid=54352245", "title": "Fractional Laplacian", "text": "Fractional Laplacian\n\nIn mathematics, the fractional Laplacian is an operator which generalizes the notion of derivatives to fractional powers.\n\nFor formula_1 can be defined on functions formula_2 as a Fourier multiplier given by the formula\n\nwhere the Fourier transform formula_4 of a function formula_2 is given by\n\nMore concretely, the fractional Laplacian can be written as a singular integral operator defined by\n\nSome authors prefer to adopt the convention of defining the fractional Laplacian of order s as formula_8 (as defined above), where now <math>0, so that the notion of order matches that of a (pseudo-)differential operator.\n\n\n"}
{"id": "20420210", "url": "https://en.wikipedia.org/wiki?curid=20420210", "title": "Gerrit Mannoury", "text": "Gerrit Mannoury\n\nGerrit Mannoury (17 May 1867 – 30 January 1956) was a Dutch philosopher and mathematician, professor at the University of Amsterdam and communist, known as the central figure in the signific circle, a Dutch counterpart of the Vienna circle.\n\nGerrit Mannoury was born on 17 May 1867 in Wormerveer, and died on 30 January 1956 in Amsterdam. On 8 August 1907 he married Elizabeth Maria Berkelbach van der Sprenkel, with whom he had three daughters and a son, Jan Mannoury. His father Gerrit Mannoury, a sea-captain, had died in China when he was three years old. He attended the Hogere Burgerschool (HBS) in Amsterdam, where he graduated in 1885. The same year he received a Teacher's Degree in Accounting and in Mechanics. In 1902 he also received a Teacher's Degree in Mathematics. Mannoury was a self-educated mathematician. Because he was a teacher he couldn't attend lessons at the University of Amsterdam. He did receive privat lessons from Diederik Korteweg. He was awarded a PhD in Mathematics late in life, in 1946, with L.E.J. Brouwer as his promotores. \n\nMannoury started working in primary education in Amsterdam, Bloemendaal and Helmond. In 1910 he started teaching at the Hoogere Burger School (HBS) at Vlissingen. In 1902 he had been appointed Privatdozent at the University of Amsterdam and in 1917 he was made professor there. He retired in 1937). He lectured on the philosophy of mathematics, and on mechanics, analytics and descriptive and projective geometry.\n\nMannoury was, with Diederik Korteweg, one of the most important teachers of Luitzen Egbertus Jan Brouwer at Amsterdam University, Mannoury especially philosophically. The first appearance of the names \"formalism\" and \"intuitionism\" in Brouwer's writings, were in a review of Gerrit Mannoury's book \"Methodologisches und Philosophisches zur Elementar-Mathematik\" (Methodological and philosophical remarks on elementary mathematics) from 1909. Two other Dutch scientists he inspired were philosopher and logician Evert W. Beth and psychologist Adriaan de Groot.\n\nHe died in Amsterdam.\n\nMannoury's main inspirations were G. W. F. Hegel, G.J.P.J. Bolland and F. H. Bradley. He was also inspired by the work of Friedrich Nietzsche, Baruch Spinoza, the French mathematician philosopher of science Henri Poincaré and the English positivism of Bertrand Russell. Mannoury combined a logical-mathematical way of thinking with a deep insight into the human soul.\n\nMannoury was a prolific and polymathic writer who published books, articles, reviews, and pamphlets. \n\n"}
{"id": "2618270", "url": "https://en.wikipedia.org/wiki?curid=2618270", "title": "Gravitational instanton", "text": "Gravitational instanton\n\nIn mathematical physics and differential geometry, a gravitational instanton is a four-dimensional complete Riemannian manifold satisfying the vacuum Einstein equations. They are so named because they are analogues in quantum theories of gravity of instantons in Yang–Mills theory. In accordance with this analogy with self-dual Yang–Mills instantons, gravitational instantons are usually assumed to look like four dimensional Euclidean space at large distances, and to have a self-dual Riemann tensor. Mathematically, this means that they are asymptotically locally Euclidean (or perhaps asymptotically locally flat) hyperkähler 4-manifolds, and in this sense, they are special examples of Einstein manifolds. From a physical point of view, a gravitational instanton is a non-singular solution of the vacuum Einstein equations with \"positive-definite\", as opposed to Lorentzian, metric.\n\nThere are many possible generalizations of the original conception of a gravitational instanton: for example one can allow gravitational instantons to have a nonzero cosmological constant or a Riemann tensor which is not self-dual. One can also relax the boundary condition that the metric is asymptotically Euclidean.\n\nThere are many methods for constructing gravitational instantons, including the Gibbons–Hawking Ansatz, twistor theory, and the hyperkähler quotient construction.\n\n\nBy specifying the 'boundary conditions', i.e. the asymptotics of the metric 'at infinity' on a noncompact Riemannian manifold, gravitational instantons are divided into a few classes, such as asymptotically locally Euclidean spaces (ALE spaces), asymptotically locally flat spaces (ALF spaces). There also exist ALG spaces whose name is chosen by induction.\n\nIt will be convenient to write the gravitational instanton solutions below using left-invariant 1-forms on the three-sphere S\n(viewed as the group Sp(1) or SU(2)). These can be defined in terms of Euler angles by\n\nThe Eguchi–Hanson space is important in many other contexts of geometry and theoretical physics. Its metric is given by\n\nwhere formula_6.\nThis metric is smooth everywhere if it has no conical singularity at formula_7, formula_8. For formula_9 this happens if formula_10 has a period of formula_11, which gives a flat metric on R; However, for formula_12 this happens if formula_10 has a period of formula_14.\n\nAsymptotically (i.e., in the limit formula_15) the metric looks like\nwhich naively seems as the flat metric on R. However, for formula_12, formula_10 has only half the usual periodicity, as we have seen. Thus the metric is asymptotically R with the identification formula_19, which is a Z subgroup of SO(4), the rotation group of R. Therefore, the metric is said to be asymptotically \nR/Z.\n\nThere is a transformation to another coordinate system, in which the metric looks like\nwhere\nformula_21\nFor some \"n\" points formula_22, \"i\" = 1, 2..., \"n\".\nThis gives a multi-center Eguchi–Hanson gravitational instanton, which is again smooth everywhere if the angular coordinates have the usual periodicities (to avoid conical singularities). The asymptotic limit (formula_23) is equivalent to taking all formula_22 to zero, and by changing coordinates back to r, formula_25 and formula_26, and redefining formula_27, we get the asymptotic metric\n\nThis is R/Z = C/Z, because it is R with the angular coordinate formula_10 replaced by formula_30, which has the wrong periodicity (formula_31 instead of formula_11). In other words, it is R identified under formula_33, or, equivalently, C identified under \"z\" ~ formula_34 \"z\" for \"i\" = 1, 2.\n\nTo conclude, the multi-center Eguchi–Hanson geometry is a Kähler Ricci flat geometry which is asymptotically C/Z. According to Yau's theorem this is the only geometry satisfying these properties. Therefore, this is also the geometry of a C/Z orbifold in string theory after its conical singularity has been smoothed away by its \"blow up\" (i.e., deformation).\n\nformula_35\n\nwhere\n\nformula_36\n\nformula_37 corresponds to multi-Taub–NUT, formula_38 and formula_39 is flat space, and formula_38 and formula_41 is the Eguchi–Hanson solution (in different coordinates).\n\n"}
{"id": "1728613", "url": "https://en.wikipedia.org/wiki?curid=1728613", "title": "Guillaume de l'Hôpital", "text": "Guillaume de l'Hôpital\n\nGuillaume François Antoine, Marquis de l'Hôpital (; 1661 – 2 February 1704) was a French mathematician. His name is firmly associated with l'Hôpital's rule for calculating limits involving indeterminate forms 0/0 and ∞/∞. Although the rule did not originate with l'Hôpital, it appeared in print for the first time in his treatise on the infinitesimal calculus, entitled \"Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes\". This book was a first systematic exposition of differential calculus. Several editions and translations to other languages were published and it became a model for subsequent treatments of calculus.\n\nL'Hôpital was born into a military family. His father was Anne-Alexandre de l'Hôpital, a Lieutenant-General of the King's army, Comte de Saint-Mesme and the of Gaston, Duke of Orléans. His mother was Elisabeth Gobelin, a daughter of Claude Gobelin, Intendant in the King's Army and Councilor of the State.\n\nL'Hôpital abandoned a military career due to poor eyesight and pursued his interest in mathematics, which was apparent since his childhood. For a while, he was a member of Nicolas Malebranche's circle in Paris and it was there that in 1691 he met young Johann Bernoulli, who was visiting France and agreed to supplement his Paris talks on infinitesimal calculus with private lectures to l'Hôpital at his estate at Oucques. In 1693, l'Hôpital was elected to the French academy of sciences and even served twice as its vice-president. Among his accomplishments were the determination of the arc length of the logarithmic graph, one of the solutions to the brachistochrone problem, and the discovery of a turning point singularity on the involute of a plane curve near an inflection point.\n\nL'Hôpital exchanged ideas with Pierre Varignon and corresponded with Gottfried Leibniz, Christiaan Huygens, and Jacob and Johann Bernoulli. His \"Traité analytique des sections coniques et de leur usage pour la résolution des équations dans les problêmes tant déterminés qu'indéterminés\" (\"Analytic treatise on conic sections\") was published posthumously in Paris in 1707.\n\nIn 1696 l'Hôpital published his book \"Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes\" (\"Infinitesimal calculus with applications to curved lines\"). This was the first textbook on infinitesimal calculus and it presented the ideas of differential calculus and their applications to differential geometry of curves in a lucid form and with numerous figures; however, it did not consider integration. The history leading to the book's publication became a subject of a protracted controversy. In a letter from 17 March 1694, l'Hôpital made the following proposal to Johann Bernoulli: in exchange for an annual payment of 300 Francs, Bernoulli would inform l'Hôpital of his latest mathematical discoveries, withholding them from correspondence with others, including Varignon. Bernoulli's immediate response has not been preserved, but he must have agreed soon, as the subsequent letters show. L'Hôpital may have felt fully justified in describing these results in his book, after acknowledging his debt to Leibniz and the Bernoulli brothers, \"especially the younger one\" (Johann). Johann Bernoulli grew increasingly unhappy with the accolades bestowed on l'Hôpital's work and complained in private correspondence about being sidelined. After l'Hôpital's death, he publicly revealed their agreement and claimed credit for the statements and portions of the text of \"Analyse\", which were supplied to l'Hôpital in letters. Over a period of many years, Bernoulli made progressively stronger allegations about his role in the writing of \"Analyse\", culminating in the publication of his old work on integral calculus in 1742: he remarked that this is a continuation of his old lectures on differential calculus, which he discarded since l'Hôpital had already included them in his famous book. For a long time, these claims were not regarded as credible by many historians of mathematics, because l'Hôpital's mathematical talent was not in doubt, while Bernoulli was involved in several other priority disputes. For example, both H. G. Zeuthen and Moritz Cantor, writing at the cusp of the 20th century, dismissed Bernoulli's claims on these grounds. However, in 1921 Paul Schafheitlin discovered a manuscript of Bernoulli's lectures on differential calculus from 1691–1692 in the Basel University library. The text showed remarkable similarities to l'Hôpital's writing, substantiating Bernoulli's account of the book's origin.\n\nL'Hôpital's pedagogical brilliance in arranging and presenting the material remains universally recognized. Regardless of the exact authorship (one should also note that the book was first published anonymously), \"Analyse\" was remarkably successful in popularizing the ideas of differential calculus stemming from Leibniz.\n\nL'Hôpital married , also a member of the nobility, and the inheritor of large estates in Brittany. She shared his interest in mathematics. The marriage produced one son and three daughters.\n\n"}
{"id": "3044121", "url": "https://en.wikipedia.org/wiki?curid=3044121", "title": "Half range Fourier series", "text": "Half range Fourier series\n\nA half range Fourier series is a Fourier series defined on an interval formula_1 instead of the more common formula_2, with the implication that the analyzed function formula_3 should be extended to formula_4 as either an even (f(-x)=f(x)) or odd function (f(-x)=-f(x)). This allows the expansion of the function in a series solely of sines (odd) or cosines (even). The choice between odd and even is typically motivated by boundary conditions associated with a differential equation satisfied by formula_5.\n\nExample\n\nCalculate the half range Fourier sine series for the function formula_6 where formula_7.\n\nSince we are calculating a sine series,\nformula_8 \nNow,\nformula_9\n\nWhen n is odd,\nformula_10\nWhen n is even,\nformula_11\nthus\nformula_12\n\nWith the special case formula_13, hence the required Fourier sine series is\n\n<math>\\cos(x) = "}
{"id": "14897496", "url": "https://en.wikipedia.org/wiki?curid=14897496", "title": "Henri Brocard", "text": "Henri Brocard\n\nPierre René Jean Baptiste Henri Brocard (12 May 1845 – 16 January 1922) was a French meteorologist and mathematician, in particular a geometer. His best-known achievement is the invention and discovery of the properties of the Brocard points, the Brocard circle, and the Brocard triangle, all bearing his name.\n\nContemporary mathematician Nathan Court wrote that he, along with Émile Lemoine and Joseph Neuberg, was one of the three co-founders of modern triangle geometry. He is listed as an Emeritus at the International Academy of Science, was awarded the Ordre des Palmes Académiques, and was an officer of the Légion d'honneur.\n\nHe spent most of his life studying meteorology as an officer in the French Navy, but seems to have made no notable original contributions to the subject.\n\nPierre René Jean Baptiste Henri Brocard was born on 12 May 1845, in Vignot (a part of Commercy), Meuse to Elizabeth Auguste Liouville and Jean Sebastien Brocard. He attended the Lycée in Marseilles as a young child, and then the Lycée in Strasbourg. After graduating from the Lycée he entered the Academy in Strasbourg where he was prepared for the examination for entrance to the prestigious École Polytechnique in Paris, to which he was accepted in 1865.\n\nBrocard attended the École Polytechnique from 1865 to 1867.\n\nAs was the norm at the time, he, after graduation, became a technical officer in the French military, which had been reorganized in 1866. He acted as a meteorologist in the French navy, and general technician as well.\n\nBrocard soon saw active service, as Napoleon III declared war upon Prussia. Brocard was one of the 120,000 men under Marshal MacMahon led to Metz to free the French army of the Rhine. The French army, however, was defeated on 31 August at the Battle of Sedan, and was taken prisoner along with approximately 83,000 other combatants.\n\nAfter Brocard was freed, he returned to his military position and continued teaching, publishing his mathematical articles in the most popular mathematical journal of that time, (also called \"Nouvelles annales mathématiques\"). He joined the Société Mathématique de France in 1873, just a year after its founding. In 1875, he was inducted into the French Association for the Advancement of Science as well as the French Meteorological Society. He was shortly after sent to northern Africa, where he served as a military technician for the French forces stationed in Algiers, the seat of French Africa. While in Algiers, Brocard founded the Meteorological Institute of Algiers. Brocard also visited Oran while in northern Africa, which was occupied by the French in 1831.\n\nDuring a meeting of the French Association for the Advancement of Science, Brocard presented a self-written article entitled \"Etudes d'un nouveau cercle du plan du triangle\", his first paper on the Brocard points, the Brocard triangle, and the Brocard circle, all of which today bear his name.\n\nIn 1884 Brocard returned to France. He served with the Meteorological Commission in Montpellier before moving to Grenoble and lastly Bar-le-duc. He honorably retired from the French military in 1910 as a lieutenant colonel. His remaining two major publications were \"Notes de bibliographie des courbes géométriques\" (1897, 1899, published in two volumes) and the \"Courbes géométriques remarquables\" (1920, posthumous 1967, also published in two volumes) which was written in collaboration with T. Lemoyne.\n\nBrocard attended the International Congress of Mathematicians at Zurich in 1897, Paris in 1900, Heidelberg in 1904, Rome in 1908, Cambridge, England in 1912, and Strasbourg in 1920.\n\nBrocard spent the last years of his life in Bar-le-Duc. He was offered the presidency of Bar-le-Duc's \"Letters, Sciences, and Arts Society\", of which he had been a longtime member and correspondent for several foreign academies of, but declined. He died on 16 January 1922 while on a trip to Kensington, London, England.\n\nBrocard's most well-known contributions to mathematics are the Brocard points, the Brocard circle, and the Brocard triangle. The positive Brocard point (sometimes known as the first Brocard point) of a Euclidean plane triangle is the interior point of the triangle for which the three angles formed by two of the vertices and the point are equal. Their common value is the Brocard angle of the triangle. The Brocard circle of the triangle is a circle having a diameter of the line segment between the circumcenter and symmedian. It contains the Brocard points. The Brocard triangle of a triangle is a triangle formed by the intersection of line from a vertex to its corresponding Brocard point and a line from another vertex to its corresponding Brocard point and the other two points constructed using different combinations of vertices and Brocard points. The Brocard triangle is inscribed in the Brocard circle.\n\nBrocard published various other papers on mathematics during his time at Bar-le-duc, none of which became as well known as \"Etudes d'un nouveau cercle du plan du triangle\". He also conjured up a famous unsolved problem thought to have no answers other than the three he provided. This problem is called Brocard's Problem. One other achievement of his is guessing at the meaning of the cryptic title of one of Girard Desargues' papers, \"DALG\". In his paper \"Analyse d'autographes et autres écrits de Girard Desargues\", he surmised that it stood for \"Des Argues, Lyonnais, Géometre\", which is the generally accepted title.\n\nThough Brocard made no major notable original discoveries in meteorology, he founded the Meteorological Institute in Algiers and served as a meteorological technician during his time in the French military. He also published several notable papers on meteorology.\n\n\n"}
{"id": "48740", "url": "https://en.wikipedia.org/wiki?curid=48740", "title": "Henri Poincaré", "text": "Henri Poincaré\n\nJules Henri Poincaré (; ; 29 April 1854 – 17 July 1912) was a French mathematician, theoretical physicist, engineer, and philosopher of science. He is often described as a polymath, and in mathematics as \"The Last Universalist,\" since he excelled in all fields of the discipline as it existed during his lifetime.\n\nAs a mathematician and physicist, he made many original fundamental contributions to pure and applied mathematics, mathematical physics, and celestial mechanics. He was responsible for formulating the Poincaré conjecture, which was one of the most famous unsolved problems in mathematics until it was solved in 2002–2003 by Grigori Perelman. In his research on the three-body problem, Poincaré became the first person to discover a chaotic deterministic system which laid the foundations of modern chaos theory. He is also considered to be one of the founders of the field of topology.\n\nPoincaré made clear the importance of paying attention to the invariance of laws of physics under different transformations, and was the first to present the Lorentz transformations in their modern symmetrical form. Poincaré discovered the remaining relativistic velocity transformations and recorded them in a letter to Hendrik Lorentz in 1905. Thus he obtained perfect invariance of all of Maxwell's equations, an important step in the formulation of the theory of special relativity. In 1905, Poincaré first proposed gravitational waves (\"ondes gravifiques\") emanating from a body and propagating at the speed of light as being required by the Lorentz transformations.\n\nThe Poincaré group used in physics and mathematics was named after him.\n\nPoincaré was born on 29 April 1854 in Cité Ducale neighborhood, Nancy, Meurthe-et-Moselle into an influential family. His father Leon Poincaré (1828–1892) was a professor of medicine at the University of Nancy. His younger sister Aline married the spiritual philosopher Emile Boutroux. Another notable member of Henri's family was his cousin, Raymond Poincaré, a fellow member of the Académie française, who would serve as President of France from 1913 to 1920. Poincaré was raised in the Roman Catholic faith, but later left the religion. He became a freethinker, believing the universe to be sufficient truth and was said to be an atheist.\n\nDuring his childhood he was seriously ill for a time with diphtheria and received special instruction from his mother, Eugénie Launois (1830–1897).\n\nIn 1862, Henri entered the Lycée in Nancy (now renamed the in his honour, along with Henri Poincaré University, also in Nancy). He spent eleven years at the Lycée and during this time he proved to be one of the top students in every topic he studied. He excelled in written composition. His mathematics teacher described him as a \"monster of mathematics\" and he won first prizes in the concours général, a competition between the top pupils from all the Lycées across France. His poorest subjects were music and physical education, where he was described as \"average at best\". However, poor eyesight and a tendency towards absentmindedness may explain these difficulties. He graduated from the Lycée in 1871 with a bachelor's degree in letters and sciences.\n\nDuring the Franco-Prussian War of 1870, he served alongside his father in the Ambulance Corps.\n\nPoincaré entered the École Polytechnique in 1873 and graduated in 1875. There he studied mathematics as a student of Charles Hermite, continuing to excel and publishing his first paper (\"Démonstration nouvelle des propriétés de l'indicatrice d'une surface\") in 1874. From November 1875 to June 1878 he studied at the École des Mines, while continuing the study of mathematics in addition to the mining engineering syllabus, and received the degree of ordinary mining engineer in March 1879.\n\nAs a graduate of the École des Mines, he joined the Corps des Mines as an inspector for the Vesoul region in northeast France. He was on the scene of a mining disaster at Magny in August 1879 in which 18 miners died. He carried out the official investigation into the accident in a characteristically thorough and humane way.\n\nAt the same time, Poincaré was preparing for his Doctorate in Science in mathematics under the supervision of Charles Hermite. His doctoral thesis was in the field of differential equations. It was named \"Sur les propriétés des fonctions définies par les équations aux différences partielles\". Poincaré devised a new way of studying the properties of these equations. He not only faced the question of determining the integral of such equations, but also was the first person to study their general geometric properties. He realised that they could be used to model the behaviour of multiple bodies in free motion within the solar system. Poincaré graduated from the University of Paris in 1879.\n\nAfter receiving his degree, Poincaré began teaching as junior lecturer in mathematics at the University of Caen in Normandy (in December 1879). At the same time he published his first major article concerning the treatment of a class of automorphic functions.\n\nThere, in Caen, he met his future wife, Louise Poulin d'Andesi (Louise Poulain d'Andecy) and on 20 April 1881, they married. Together they had four children: Jeanne (born 1887), Yvonne (born 1889), Henriette (born 1891), and Léon (born 1893).\n\nPoincaré immediately established himself among the greatest mathematicians of Europe, attracting the attention of many prominent mathematicians. In 1881 Poincaré was invited to take a teaching position at the Faculty of Sciences of the University of Paris; he accepted the invitation. During the years of 1883 to 1897, he taught mathematical analysis in École Polytechnique.\n\nIn 1881–1882, Poincaré created a new branch of mathematics: qualitative theory of differential equations. He showed how it is possible to derive the most important information about the behavior of a family of solutions without having to solve the equation (since this may not always be possible). He successfully used this approach to problems in celestial mechanics and mathematical physics.\n\nHe never fully abandoned his mining career to mathematics. He worked at the Ministry of Public Services as an engineer in charge of northern railway development from 1881 to 1885. He eventually became chief engineer of the Corps de Mines in 1893 and inspector general in 1910.\n\nBeginning in 1881 and for the rest of his career, he taught at the University of Paris (the Sorbonne). He was initially appointed as the \"maître de conférences d'analyse\" (associate professor of analysis). Eventually, he held the chairs of Physical and Experimental Mechanics, Mathematical Physics and Theory of Probability, and Celestial Mechanics and Astronomy.\n\nIn 1887, at the young age of 32, Poincaré was elected to the French Academy of Sciences. He became its president in 1906, and was elected to the Académie française on 5 March 1908.\n\nIn 1887, he won Oscar II, King of Sweden's mathematical competition for a resolution of the three-body problem concerning the free motion of multiple orbiting bodies. (See three-body problem section below.)\nIn 1893, Poincaré joined the French Bureau des Longitudes, which engaged him in the synchronisation of time around the world. In 1897 Poincaré backed an unsuccessful proposal for the decimalisation of circular measure, and hence time and longitude. It was this post which led him to consider the question of establishing international time zones and the synchronisation of time between bodies in relative motion. (See work on relativity section below.)\n\nIn 1899, and again more successfully in 1904, he intervened in the trials of Alfred Dreyfus. He attacked the spurious scientific claims of some of the evidence brought against Dreyfus, who was a Jewish officer in the French army charged with treason by colleagues.\n\nPoincaré was the President of the Société Astronomique de France (SAF), the French astronomical society, from 1901 to 1903.\n\nPoincaré had two notable doctoral students at the University of Paris, Louis Bachelier (1900) and Dimitrie Pompeiu (1905).\n\nIn 1912, Poincaré underwent surgery for a prostate problem and subsequently died from an embolism on 17 July 1912, in Paris. He was 58 years of age. He is buried in the Poincaré family vault in the Cemetery of Montparnasse, Paris.\n\nA former French Minister of Education, Claude Allègre, proposed in 2004 that Poincaré be reburied in the Panthéon in Paris, which is reserved for French citizens only of the highest honour.\n\nPoincaré made many contributions to different fields of pure and applied mathematics such as: celestial mechanics, fluid mechanics, optics, electricity, telegraphy, capillarity, elasticity, thermodynamics, potential theory, quantum theory, theory of relativity and physical cosmology.\n\nHe was also a populariser of mathematics and physics and wrote several books for the lay public.\n\nAmong the specific topics he contributed to are the following:\n\nThe problem of finding the general solution to the motion of more than two orbiting bodies in the solar system had eluded mathematicians since Newton's time. This was known originally as the three-body problem and later the \"n\"-body problem, where \"n\" is any number of more than two orbiting bodies. The \"n\"-body solution was considered very important and challenging at the close of the 19th century. Indeed, in 1887, in honour of his 60th birthday, Oscar II, King of Sweden, advised by Gösta Mittag-Leffler, established a prize for anyone who could find the solution to the problem. The announcement was quite specific:\n\nGiven a system of arbitrarily many mass points that attract each according to Newton's law, under the assumption that no two points ever collide, try to find a representation of the coordinates of each point as a series in a variable that is some known function of time and for all of whose values the series converges uniformly.\n\nIn case the problem could not be solved, any other important contribution to classical mechanics would then be considered to be prizeworthy. The prize was finally awarded to Poincaré, even though he did not solve the original problem. One of the judges, the distinguished Karl Weierstrass, said, \"This work cannot indeed be considered as furnishing the complete solution of the question proposed, but that it is nevertheless of such importance that its publication will inaugurate a new era in the history of celestial mechanics.\" (The first version of his contribution even contained a serious error; for details see the article by Diacu and the book by Barrow-Green). The version finally printed contained many important ideas which led to the theory of chaos. The problem as stated originally was finally solved by Karl F. Sundman for \"n\" = 3 in 1912 and was generalised to the case of \"n\" > 3 bodies by Qiudong Wang in the 1990s.\n\nPoincaré's work at the Bureau des Longitudes on establishing international time zones led him to consider how clocks at rest on the Earth, which would be moving at different speeds relative to absolute space (or the \"luminiferous aether\"), could be synchronised. At the same time Dutch theorist Hendrik Lorentz was developing Maxwell's theory into a theory of the motion of charged particles (\"electrons\" or \"ions\"), and their interaction with radiation. In 1895 Lorentz had introduced an auxiliary quantity (without physical interpretation) called \"local time\" formula_1\nand introduced the hypothesis of length contraction to explain the failure of optical and electrical experiments to detect motion relative to the aether (see Michelson–Morley experiment).\nPoincaré was a constant interpreter (and sometimes friendly critic) of Lorentz's theory. Poincaré as a philosopher was interested in the \"deeper meaning\". Thus he interpreted Lorentz's theory and in so doing he came up with many insights that are now associated with special relativity. In (1898), Poincaré said, \"\nA little reflection is sufficient to understand that all these affirmations have by themselves no meaning. They can have one only as the result of a convention.\" He also argued that scientists have to set the constancy of the speed of light as a postulate to give physical theories the simplest form.\nBased on these assumptions he discussed in 1900 Lorentz's \"wonderful invention\" of local time and remarked that it arose when moving clocks are synchronised by exchanging light signals assumed to travel with the same speed in both directions in a moving frame.\n\nIn 1881 Poincaré described hyperbolic geometry in terms of Weierstrass coordinates of the hyperboloid model. There, he formulated transformations leaving invariant the Lorentz interval formula_2, which makes them mathematically equivalent to the Lorentz transformations in 2+1 dimensions.\n\nHe discussed the \"principle of relative motion\" in two papers in 1900\nand named it the principle of relativity in 1904, according to which no physical experiment can discriminate between a state of uniform motion and a state of rest.\nIn 1905 Poincaré wrote to Lorentz about Lorentz's paper of 1904, which Poincaré described as a \"paper of supreme importance.\" In this letter he pointed out an error Lorentz had made when he had applied his transformation to one of Maxwell's equations, that for charge-occupied space, and also questioned the time dilation factor given by Lorentz.\nIn a second letter to Lorentz, Poincaré gave his own reason why Lorentz's time dilation factor was indeed correct after all—it was necessary to make the Lorentz transformation form a group—and he gave what is now known as the relativistic velocity-addition law.\nPoincaré later delivered a paper at the meeting of the Academy of Sciences in Paris on 5 June 1905 in which these issues were addressed. In the published version of that he wrote:\n\nThe essential point, established by Lorentz, is that the equations of the electromagnetic field are not altered by a certain transformation (which I will call by the name of Lorentz) of the form:\n\nand showed that the arbitrary function formula_4 must be unity for all formula_5 (Lorentz had set formula_6 by a different argument) to make the transformations form a group. In an enlarged version of the paper that appeared in 1906 Poincaré pointed out that the combination formula_7 is invariant. He noted that a Lorentz transformation is merely a rotation in four-dimensional space about the origin by introducing formula_8 as a fourth imaginary coordinate, and he used an early form of four-vectors. Poincaré expressed a lack of interest in a four-dimensional reformulation of his new mechanics in 1907, because in his opinion the translation of physics into the language of four-dimensional geometry would entail too much effort for limited profit. So it was Hermann Minkowski who worked out the consequences of this notion in 1907.\n\nLike others before, Poincaré (1900) discovered a relation between mass and electromagnetic energy. While studying the conflict between the action/reaction principle and Lorentz ether theory, he tried to determine whether the center of gravity still moves with a uniform velocity when electromagnetic fields are included. He noticed that the action/reaction principle does not hold for matter alone, but that the electromagnetic field has its own momentum. Poincaré concluded that the electromagnetic field energy of an electromagnetic wave behaves like a fictitious fluid (\"fluide fictif\") with a mass density of \"E\"/\"c\". If the center of mass frame is defined by both the mass of matter \"and\" the mass of the fictitious fluid, and if the fictitious fluid is indestructible—it's neither created or destroyed—then the motion of the center of mass frame remains uniform. But electromagnetic energy can be converted into other forms of energy. So Poincaré assumed that there exists a non-electric energy fluid at each point of space, into which electromagnetic energy can be transformed and which also carries a mass proportional to the energy. In this way, the motion of the center of mass remains uniform. Poincaré said that one should not be too surprised by these assumptions, since they are only mathematical fictions.\n\nHowever, Poincaré's resolution led to a paradox when changing frames: if a Hertzian oscillator radiates in a certain direction, it will suffer a recoil from the inertia of the fictitious fluid. Poincaré performed a Lorentz boost (to order \"v\"/\"c\") to the frame of the moving source. He noted that energy conservation holds in both frames, but that the law of conservation of momentum is violated. This would allow perpetual motion, a notion which he abhorred. The laws of nature would have to be different in the frames of reference, and the relativity principle would not hold. Therefore, he argued that also in this case there has to be another compensating mechanism in the ether.\n\nPoincaré himself came back to this topic in his St. Louis lecture (1904). This time (and later also in 1908) he rejected the possibility that energy carries mass and criticized the ether solution to compensate the above-mentioned problems:\n\nHe also discussed two other unexplained effects: (1) non-conservation of mass implied by Lorentz's variable mass formula_9, Abraham's theory of variable mass and Kaufmann's experiments on the mass of fast moving electrons and (2) the non-conservation of energy in the radium experiments of Madame Curie.\n\nIt was Albert Einstein's concept of mass–energy equivalence (1905) that a body losing energy as radiation or heat was losing mass of amount \"m\" = \"E\"/\"c\" that resolved Poincaré's paradox, without using any compensating mechanism within the ether. The Hertzian oscillator loses mass in the emission process, and momentum is conserved in any frame. However, concerning Poincaré's solution of the Center of Gravity problem, Einstein noted that Poincaré's formulation and his own from 1906 were mathematically equivalent.\n\nIn 1905 Henri Poincaré first proposed gravitational waves (\"ondes gravifiques\") emanating from a body and propagating at the speed of light. \"Il importait d'examiner cette hypothèse de plus près et en particulier de rechercher quelles modifications elle nous obligerait à apporter aux lois de la gravitation. C'est ce que j'ai cherché à déterminer ; j'ai été d'abord conduit à supposer que la propagation de la gravitation n'est pas instantanée, mais se fait avec la vitesse de la lumière.\"\n\nEinstein's first paper on relativity was published three months after Poincaré's short paper, but before Poincaré's longer version. Einstein relied on the principle of relativity to derive the Lorentz transformations and used a similar clock synchronisation procedure (Einstein synchronisation) to the one that Poincaré (1900) had described, but Einstein's paper was remarkable in that it contained no references at all. Poincaré never acknowledged Einstein's work on special relativity. However, Einstein expressed sympathy with Poincaré's outlook obliquely in a letter to Hans Vaihinger on 3 May 1919, when Einstein considered Vaihinger's general outlook to be close to his own and Poincaré's to be close to Vaihinger's. In public, Einstein acknowledged Poincaré posthumously in the text of a lecture in 1921 called \"Geometrie und Erfahrung\" in connection with non-Euclidean geometry, but not in connection with special relativity. A few years before his death, Einstein commented on Poincaré as being one of the pioneers of relativity, saying \"Lorentz had already recognised that the transformation named after him is essential for the analysis of Maxwell's equations, and Poincaré deepened this insight still further ...\"\n\nPoincaré's work in the development of special relativity is well recognised, though most historians stress that despite many similarities with Einstein's work, the two had very different research agendas and interpretations of the work. Poincaré developed a similar physical interpretation of local time and noticed the connection to signal velocity, but contrary to Einstein he continued to use the ether-concept in his papers and argued that clocks at rest in the ether show the \"true\" time, and moving clocks show the local time. So Poincaré tried to keep the relativity principle in accordance with classical concepts, while Einstein developed a mathematically equivalent kinematics based on the new physical concepts of the relativity of space and time.\n\nWhile this is the view of most historians, a minority go much further, such as E. T. Whittaker, who held that Poincaré and Lorentz were the true discoverers of relativity.\n\nPoincaré introduced group theory to physics, and was the first to study the group of Lorentz transformations. He also made major contributions to the theory of discrete groups and their representations.\nThe subject is clearly defined by Felix Klein in his \"Erlangen Program\" (1872): the geometry invariants of arbitrary continuous transformation, a kind of geometry. The term \"topology\" was introduced, as suggested by Johann Benedict Listing, instead of previously used \"Analysis situs\". Some important concepts were introduced by Enrico Betti and Bernhard Riemann. But the foundation of this science, for a space of any dimension, was created by Poincaré. His first article on this topic appeared in 1894.\n\nHis research in geometry led to the abstract topological definition of homotopy and homology. He also first introduced the basic concepts and invariants of combinatorial topology, such as Betti numbers and the fundamental group. Poincaré proved a formula relating the number of edges, vertices and faces of \"n\"-dimensional polyhedron (the Euler–Poincaré theorem) and gave the first precise formulation of the intuitive notion of dimension.\n\nPoincaré published two now classical monographs, \"New Methods of Celestial Mechanics\" (1892–1899) and \"Lectures on Celestial Mechanics\" (1905–1910). In them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). They introduced the small parameter method, fixed points, integral invariants, variational equations, the convergence of the asymptotic expansions. Generalizing a theory of Bruns (1887), Poincaré showed that the three-body problem is not integrable. In other words, the general solution of the three-body problem can not be expressed in terms of algebraic and transcendental functions through unambiguous coordinates and velocities of the bodies. His work in this area was the first major achievement in celestial mechanics since Isaac Newton.\n\nThese monographs include an idea of Poincaré, which later became the base for mathematical \"chaos theory\" (see, in particular, the Poincaré recurrence theorem) and the general theory of dynamical systems.\nPoincaré authored important works on astronomy for the equilibrium figures of a gravitating rotating fluid. He introduced the important concept of bifurcation points and proved the existence of equilibrium figures such as the non-ellipsoids, including ring-shaped and pear-shaped figures, and their stability. For this discovery, Poincaré received the Gold Medal of the Royal Astronomical Society (1900).\n\nAfter defending his doctoral thesis on the study of singular points of the system of differential equations, Poincaré wrote a series of memoirs under the title \"On curves defined by differential equations\" (1881–1882). In these articles, he built a new branch of mathematics, called \"qualitative theory of differential equations\". Poincaré showed that even if the differential equation can not be solved in terms of known functions, yet from the very form of the equation, a wealth of information about the properties and behavior of the solutions can be found. In particular, Poincaré investigated the nature of the trajectories of the integral curves in the plane, gave a classification of singular points (saddle, focus, center, node), introduced the concept of a limit cycle and the loop index, and showed that the number of limit cycles is always finite, except for some special cases. Poincaré also developed a general theory of integral invariants and solutions of the variational equations. For the finite-difference equations, he created a new direction – the asymptotic analysis of the solutions. He applied all these achievements to study practical problems of mathematical physics and celestial mechanics, and the methods used were the basis of its topological works.\n\nPoincaré's work habits have been compared to a bee flying from flower to flower. Poincaré was interested in the way his mind worked; he studied his habits and gave a talk about his observations in 1908 at the Institute of General Psychology in Paris. He linked his way of thinking to how he made several discoveries.\n\nThe mathematician Darboux claimed he was \"un intuitif\" (intuitive), arguing that this is demonstrated by the fact that he worked so often by visual representation. He did not care about being rigorous and disliked logic. (Despite this opinion, Jacques Hadamard wrote that Poincaré's research demonstrated marvelous clarity and Poincaré himself wrote that he believed that logic was not a way to invent but a way to structure ideas and that logic limits ideas.)\n\nPoincaré's mental organisation was not only interesting to Poincaré himself but also to Édouard Toulouse, a psychologist of the Psychology Laboratory of the School of Higher Studies in Paris. Toulouse wrote a book entitled \"Henri Poincaré\" (1910). In it, he discussed Poincaré's regular schedule:\n\nThese abilities were offset to some extent by his shortcomings:\n\nIn addition, Toulouse stated that most mathematicians worked from principles already established while Poincaré started from basic principles each time (O'Connor et al., 2002).\n\nHis method of thinking is well summarised as:\n\nPoincaré was dismayed by Georg Cantor's theory of transfinite numbers, and referred to it as a \"disease\" from which mathematics would eventually be cured.\nPoincaré said, \"There is no actual infinite; the Cantorians have forgotten this, and that is why they have fallen into contradiction.\"\n\nAwards\n\nNamed after him\n\nHenri Poincaré did not receive the Nobel Prize in Physics, but he had influential advocates like Henri Becquerel or committee member Gösta Mittag-Leffler. The nomination archive reveals that Poincaré received a total of 51 nominations between 1904 and 1912, the year of his death. Of the 58 nominations for the 1910 Nobel Prize, 34 named Poincaré. Nominators included Nobel laureates Hendrik Lorentz and Pieter Zeeman (both of 1902), Marie Curie (of 1903), Albert Michelson (of 1907), Gabriel Lippmann (of 1908) and Guglielmo Marconi (of 1909).\n\nThe fact that renowned theoretical physicists like Poincaré, Boltzmann or Gibbs were not awarded the Nobel Prize is seen as evidence that the Nobel committee had more regard for experimentation than theory. In Poincaré's case, several of those who nominated him pointed out that the greatest problem was to name a specific discovery, invention, or technique.\n\nPoincaré had philosophical views opposite to those of Bertrand Russell and Gottlob Frege, who believed that mathematics was a branch of logic. Poincaré strongly disagreed, claiming that intuition was the life of mathematics. Poincaré gives an interesting point of view in his book \"Science and Hypothesis\":\n\nPoincaré believed that arithmetic is a synthetic science. He argued that Peano's axioms cannot be proven non-circularly with the principle of induction (Murzi, 1998), therefore concluding that arithmetic is \"a priori\" synthetic and not analytic. Poincaré then went on to say that mathematics cannot be deduced from logic since it is not analytic. His views were similar to those of Immanuel Kant (Kolak, 2001, Folina 1992). He strongly opposed Cantorian set theory, objecting to its use of impredicative definitions.\n\nHowever, Poincaré did not share Kantian views in all branches of philosophy and mathematics. For example, in geometry, Poincaré believed that the structure of non-Euclidean space can be known analytically. Poincaré held that convention plays an important role in physics. His view (and some later, more extreme versions of it) came to be known as \"conventionalism\". Poincaré believed that Newton's first law was not empirical but is a conventional framework assumption for mechanics (Gargani, 2012). He also believed that the geometry of physical space is conventional. He considered examples in which either the geometry of the physical fields or gradients of temperature can be changed, either describing a space as non-Euclidean measured by rigid rulers, or as a Euclidean space where the rulers are expanded or shrunk by a variable heat distribution. However, Poincaré thought that we were so accustomed to Euclidean geometry that we would prefer to change the physical laws to save Euclidean geometry rather than shift to a non-Euclidean physical geometry.\n\nPoincaré's famous lectures before the Société de Psychologie in Paris (published as \"Science and Hypothesis\", \"The Value of Science\", and \"Science and Method\") were cited by Jacques Hadamard as the source for the idea that creativity and invention consist of two mental stages, first random combinations of possible solutions to a problem, followed by a critical evaluation.\n\nAlthough he most often spoke of a deterministic universe, Poincaré said that the subconscious generation of new possibilities involves chance.\n\nIt is certain that the combinations which present themselves to the mind in a kind of sudden illumination after a somewhat prolonged period of unconscious work are generally useful and fruitful combinations... all the combinations are formed as a result of the automatic action of the subliminal ego, but those only which are interesting find their way into the field of consciousness... A few only are harmonious, and consequently at once useful and beautiful, and they will be capable of affecting the geometrician's special sensibility I have been speaking of; which, once aroused, will direct our attention upon them, and will thus give them the opportunity of becoming conscious... In the subliminal ego, on the contrary, there reigns what I would call liberty, if one could give this name to the mere absence of discipline and to disorder born of chance.\nPoincaré's two stages—random combinations followed by selection—became the basis for Daniel Dennett's two-stage model of free will.\n\n\n\nPopular writings on the philosophy of science:\n\nOn algebraic topology:\n\nOn celestial mechanics:\n\nOn the philosophy of mathematics:\n\nOther:\n\n\n\n\n"}
{"id": "3612259", "url": "https://en.wikipedia.org/wiki?curid=3612259", "title": "Hierarchy (mathematics)", "text": "Hierarchy (mathematics)\n\nIn mathematics, a hierarchy is a set-theoretical object, consisting of a preorder defined on a set. This is often referred to as an ordered set, though that is an ambiguous term that many authors reserve for partially ordered sets or totally ordered sets. The term \"pre-ordered set\" is unambiguous, and is always synonymous with a mathematical hierarchy. The term \"hierarchy\" is used to stress a \"hierarchical\" relation among the elements.\n\nSometimes, a set comes equipped with a natural hierarchical structure. For example, the set of natural numbers \"N\" is equipped with a natural pre-order structure, where formula_1 whenever we can find some other number formula_2 so that formula_3. That is, formula_4 is bigger than formula_5 only because we can get to formula_4 from formula_5 \"using\" formula_2. This is true for any commutative monoid. On the other hand, the set of integers \"Z\" requires a more sophisticated argument for its hierarchical structure, since we can always solve the equation formula_3 by writing formula_10.\n\nA mathematical hierarchy (a pre-ordered set) should not be confused with the more general concept of a hierarchy in the social realm, particularly when one is constructing computational models which are used to describe real-world social, economic or political systems. These hierarchies, or complex networks, are much too rich to be described in the category Set of sets. This is not just a pedantic claim; there are also mathematical hierarchies which are not describable using set theory.\n\nAnother natural hierarchy arises in computer science, where the word refers to partially ordered sets whose elements are classes of objects of increasing complexity. In that case, the preorder defining the hierarchy is the class-containment relation. Containment hierarchies are thus special cases of hierarchies.\n\nIndividual elements of a hierarchy are often called levels and a hierarchy is said to be infinite if it has infinitely many distinct levels but said to collapse if it has only finitely many distinct levels.\n\nIn theoretical computer science, the time hierarchy is a classification of decision problems according to the amount of time required to solve them.\n"}
{"id": "855150", "url": "https://en.wikipedia.org/wiki?curid=855150", "title": "Infinite divisibility", "text": "Infinite divisibility\n\nInfinite divisibility arises in different ways in philosophy, physics, economics, order theory (a branch of mathematics), and probability theory (also a branch of mathematics). One may speak of infinite divisibility, or the lack thereof, of matter, space, time, money, or abstract mathematical objects such as the continuum.\n\nThe idea of divisibility of matter was first proposed by Indian philosopher Kanada (philosopher) around 500 BCE\n\nThis theory is explored in Plato's dialogue Timaeus and was also supported by Aristotle. Andrew Pyle gives a lucid account of infinite divisibility in the first few pages of his \"Atomism and its Critics\". There he shows how infinite divisibility involves the idea that there is some extended item, such as an apple, which can be divided infinitely many times, where one never divides down to point, or to atoms of any sort. Many professional philosophers claim that infinite divisibility involves either a collection of \"an infinite number of items\" (since there are infinite divisions, there must be an infinite collection of objects), or (more rarely), \"point-sized items\", or both. Pyle states that the mathematics of infinitely divisible extensions involve neither of these — that there are infinite divisions, but only finite collections of objects and they never are divided down to point extension-less items.\n\nZeno questioned how an arrow can move if at one moment it is here and motionless and at a later moment be somewhere else and motionless, like a motion picture.\n\nIn reference to Zeno's paradox of the arrow in flight, Alfred North Whitehead writes that \"an infinite number of acts of becoming may take place in a finite time if each subsequent act is smaller in a convergent series\":\n\nUntil the discovery of quantum mechanics, no distinction was made between the question of whether matter is infinitely divisible and the question of whether matter can be \"cut\" into smaller parts ad infinitum.\n\nAs a result, the Greek word \"átomos\" (\"ἄτομος\"), which literally means \"uncuttable\", is usually translated as \"indivisible\". Whereas the modern atom is indeed divisible, it actually is uncuttable: there is no partition of space such that its parts correspond to material parts of the atom. In other words, the quantum-mechanical description of matter no longer conforms to the cookie cutter paradigm. This casts fresh light on the ancient conundrum of the divisibility of matter. The multiplicity of a material object—the number of its parts—depends on the existence, not of delimiting surfaces, but of internal spatial relations (relative positions between parts), and these lack determinate values. According to the Standard Model of particle physics, the particles that make up an atom—quarks and electrons—are point particles: they do not take up space. What makes an atom nevertheless take up space is \"not\" any spatially extended \"stuff\" that \"occupies space\", and that might be cut into smaller and smaller pieces, \"but\" the indeterminacy of its internal spatial relations.\n\nPhysical space is often regarded as infinitely divisible: it is thought that any region in space, no matter how small, could be further split. Time is similarly considered as infinitely divisible.\n\nHowever, the pioneering work of Max Planck (1858–1947) in the field of quantum physics suggests that there is, in fact, a minimum distance (now called the Planck length, 1.616229(38)×10 metres) and therefore a minimum time interval (the amount of time which light takes to traverse that distance in a vacuum, 5.39116(13) × 10 seconds, known as the Planck time) smaller than which meaningful \"measurement\" is impossible.\n\nOne dollar, or one euro, is divided into 100 cents; one can only pay in increments of a cent. It is quite commonplace for prices of some commodities such as gasoline to be in increments of a tenth of a cent per gallon or per litre. If gasoline costs $3.979 per gallon and one buys 10 gallons, then the \"extra\" 9/10 of a cent comes to ten times that: an \"extra\" 9 cents, so the cent in that case gets paid. Money is infinitely divisible in the sense that it is based upon the real number system. However, modern day coins are not divisible (in the past some coins were weighed with each transaction, and were considered divisible with no particular limit in mind). There is a point of precision in each transaction that is useless because such small amounts of money are insignificant to humans. The more the price is multiplied the more the precision could matter. For example, when buying a million shares of stock, the buyer and seller might be interested in a tenth of a cent price difference, but it's only a choice. Everything else in business measurement and choice is similarly divisible to the degree that the parties are interested. For example, financial reports may be reported annually, quarterly, or monthly. Some business managers run cash-flow reports more than once per day.\n\nAlthough time may be infinitely divisible, data on securities prices are reported at discrete times. For example, if one looks at records of stock prices in the 1920s, one may find the prices at the end of each day, but perhaps not at three-hundredths of a second after 12:47 PM. A new method, however, theoretically, could report at double the rate, which would not prevent further increases of velocity of reporting. Perhaps paradoxically, technical mathematics applied to financial markets is often simpler if infinitely divisible time is used as an approximation. Even in those cases, a precision is chosen with which to work, and measurements are rounded to that approximation. In terms of human interaction, money and time are divisible, but only to the point where further division is not of value, which point cannot be determined exactly.\n\nTo say that the field of rational numbers is infinitely divisible (i.e. order theoretically dense) means that between any two rational numbers there is another rational number. By contrast, the ring of integers is not infinitely divisible.\n\nInfinite divisibility does not imply gap-less-ness: the rationals do not enjoy the least upper bound property. That means that if one were to partition the rationals into two non-empty sets \"A\" and \"B\" where \"A\" contains all rationals less than some irrational number (\"π\", say) and \"B\" all rationals greater than it, then \"A\" has no largest member and \"B\" has no smallest member. The field of real numbers, by contrast, is both infinitely divisible and gapless. Any linearly ordered set that is infinitely divisible and gapless, and has more than one member, is uncountably infinite. For a proof, see Cantor's first uncountability proof. Infinite divisibility alone implies infiniteness but not uncountability, as the rational numbers exemplify.\n\nTo say that a probability distribution \"F\" on the real line is infinitely divisible means that if \"X\" is any random variable whose distribution is \"F\", then for every positive integer \"n\" there exist \"n\" independent identically distributed random variables \"X\", ..., \"X\" whose sum is equal in distribution to \"X\" (those \"n\" other random variables do not usually have the same probability distribution as \"X\").\n\nThe Poisson distribution, the stuttering Poisson distribution, the negative binomial distribution, and the Gamma distribution are examples of infinitely divisible distributions — as are the normal distribution, Cauchy distribution and all other members of the stable distribution family. The skew-normal distribution is an example of a non-infinitely divisible distribution. (See Domínguez-Molina and Rocha Arteaga (2007).)\n\nEvery infinitely divisible probability distribution corresponds in a natural way to a Lévy process, i.e., a stochastic process { \"X\" : \"t\" ≥ 0 } with stationary independent increments (\"stationary\" means that for \"s\" < \"t\", the probability distribution of \"X\" − \"X\" depends only on \"t\" − \"s\"; \"independent increments\" means that that difference is independent of the corresponding difference on any interval not overlapping with [\"s\", \"t\"], and similarly for any finite number of intervals).\n\nThis concept of infinite divisibility of probability distributions was introduced in 1929 by Bruno de Finetti.\n\n\n"}
{"id": "17297405", "url": "https://en.wikipedia.org/wiki?curid=17297405", "title": "John Casey (mathematician)", "text": "John Casey (mathematician)\n\nJohn Casey (12 May 1820, Kilbehenny, Co. Limerick, Ireland – 3 January 1891, Dublin) was a respected Irish geometer. He is most famous for Casey's theorem on a circle that is tangent to four other circles, an extension of the problem of Apollonius. However, he contributed several novel proofs and perspectives on Euclidean geometry. He and Émile Lemoine are considered to be the co-founders of the modern geometry of the circle and the triangle.\nHe was born in Kilbehenny, Limerick, Ireland and educated locally at Mitchelstown, before becoming a teacher under the Board of National Education. He later became headmaster of the Central Model Schools in Kilkenny City. He subsequently entered Trinity College in 1858, where he was elected a Scholar in 1861 and was awarded the degree of BA in 1862. He was then Mathematics Master at Kingston School (1862–1873), Professor of Higher Mathematics and Mathematical Physics at the newly founded Catholic University of Ireland (1873–1881) and Lecturer in Mathematics at University College, Dublin (1881–1891). In parallel, he also taught at St Patrick's Drumcondra (1876-1885), which was not uncommon for UCD staff up till the 1930s.\n\nIn 1869, Casey was awarded the Honorary Degree of Doctor of Laws by Dublin University. He was elected a Fellow of the Royal Society in June, 1875. He was elected to the Royal Irish Academy and in 1880 became a member of its council. In 1878 the Academy conferred upon him the much coveted Cunningham Gold Medal. His work was also acknowledged by the Norwegian Government, among others. He was elected a member of the Societe Mathematique de France in 1884 and received the honorary degree of Doctor of Laws from the Royal University of Ireland in 1885.\n\n\n\n\n"}
{"id": "52598261", "url": "https://en.wikipedia.org/wiki?curid=52598261", "title": "Join (algebraic geometry)", "text": "Join (algebraic geometry)\n\nIn algebraic geometry, given irreducible subvarieties \"V\", \"W\" of a projective space P, the ruled join of \"V\" and \"W\" is the union of all lines from \"V\" to \"W\" in P, where \"V\", \"W\" are embedded into P so that the last (resp. first) \"n\" + 1 coordinates on \"V\" (resp. \"W\") vanish. It is denoted by \"J\"(\"V\", \"W\"). For example, if \"V\" and \"W\" are linear subspaces, then their join is the linear span of them, the smallest linear subcontaining them.\n\nThe join of several subvarieties is defined in a similar way.\n\n\n"}
{"id": "28653915", "url": "https://en.wikipedia.org/wiki?curid=28653915", "title": "Kanamori–McAloon theorem", "text": "Kanamori–McAloon theorem\n\nIn mathematical logic, the Kanamori–McAloon theorem, due to , gives an example of an incompleteness in Peano arithmetic, similar to that of the Paris–Harrington theorem.\nThey showed that a certain finitistic special case of a theorem in Ramsey theory due to Erdős and Rado is not provable in Peano arithmetic.\n\n"}
{"id": "30973898", "url": "https://en.wikipedia.org/wiki?curid=30973898", "title": "Lambda-connectedness", "text": "Lambda-connectedness\n\nIn applied mathematics, lambda-connectedness (or λ-connectedness) deals with partial connectivity for a discrete space.\n\nAssume that a function on a discrete space (usually a graph) is given. A degree of connectivity (connectedness) will be defined to measure the connectedness of the space with respect to the function. It was invented to create a new method for image segmentation. The method has expanded to handle other problems related to uncertainty for incomplete information analysis. \nFor a digital image and a certain value of formula_1, two pixels are called formula_1-connected if there is a path linking those two pixels and the connectedness of this path is at least formula_1. formula_1-connectedness is an equivalence relation.\n\nConnectedness is a basic measure in many areas of mathematical science and social sciences. In graph theory, two vertices are said to be connected if there is a path between them. In topology, two points are connected if there is a continuous\nfunction that could move from one point to another continuously. In management science, for example, in an institution,\ntwo individuals are connected if one person is under the supervision of the other. Such connected relations only describe either full connection or no connection. lambda-connectedness is introduced to measure incomplete or fuzzy relations between two vertices, points, human beings, etc.\n\nIn fact, partial relations have been studied in other aspects. Random graph theory allows one to assign a probability to each edge of a graph. This method assumes, in most cases, each edge has the same probability. On the other hand, Bayesian networks are often used for inference and analysis when relationships between each pair of states/events, denoted by vertices, are known. These relationships are usually represented by conditional probabilities\namong these vertices and are usually obtained from outside of the system.\n\nformula_1-connectedness is based on graph theory; however, graph theory only deals with vertices and edges with or without weights. In order to define a partial, incomplete, or fuzzy connectedness, one needs to assign a function on the vertex in the graph. Such a function is called a potential function. It can be used to represent the intensity of an image, the surface of a \"XY\"-domain, or the utility function of a management or economic network.\n\nA generalized definition of formula_1-connectedness can be described as follows: a simple system formula_7, where formula_8 is called a potential function of formula_9. If formula_7 is an image, then formula_9 is a 2D or 2D grid space and formula_8 is an intensity function. For a color image, one can use formula_13 to represent formula_8.\nA neighbor connectivity will be first defined on a pair of adjacent points. Then one can define the general connectedness for any two points.\n\nAssume formula_15 is used to measure the neighbor-connectivity of x,y where x and y are adjacent.\nIn graph \"G\" = (\"V\", \"E\"), a finite sequence formula_16 is called a path, if formula_17.\n\nThe path-connectivity formula_18 of a path formula_19\nis defined as\nFinally, the degree of connectedness (connectivity) of two vertices x,y with respect to formula_8 is defined as\nFor a given formula_23, point formula_24 and formula_25 are said to be formula_1-connected if formula_27.\n\nformula_1-connectedness is a equivalence relation. It can be used in image segmentation.\n"}
{"id": "8153084", "url": "https://en.wikipedia.org/wiki?curid=8153084", "title": "Letters to a Young Mathematician", "text": "Letters to a Young Mathematician\n\nLetters to a Young Mathematician () is a 2006 book by Ian Stewart, and is part of Basic Books' \"Art of Mentoring\" series. Stewart mentions in the preface that he considers this book an update to G.H. Hardy's \"A Mathematician's Apology\".\n\nThe book is made up of letters to a fictional correspondent of Stewart's, an aspiring mathematician named Meg. The roughly chronological letters follow Meg from her high school years up to her receiving tenure from an American university.\n\nReviews of the book were generally positive. Fernando Q. Gouvêa's review for the MAA calls it \"full of good advice, much of it direct and to the point\" and later, that \"while it won't change the world, it may well help some young people decide to be (or not to be) mathematicians.\" In Emma Carberry's review for the AMS, reacted differently, saying that \"one does not so much feel the benefit of a ream of practical advice, but rather of exposure to the inner realm of mathematics\". A review in \"Nature\" was harsher, however, saying that \"there is a general lack of information ... [and] too much jargon\" and that it \"suffers from being written entirely for a US audience\", but even this review finds a bright note, \"The letter in which Stewart tells Meg how to teach undergraduates should be compulsory reading for all lecturers and tutors.\" \n"}
{"id": "2374187", "url": "https://en.wikipedia.org/wiki?curid=2374187", "title": "List of things named after Leonhard Euler", "text": "List of things named after Leonhard Euler\n\nIn mathematics and physics, there are a large number of topics named in honor of Swiss mathematician Leonhard Euler (1707–1783), who made many important discoveries and innovations. Many of these items named after Euler include their own unique function, equation, formula, identity, number (single or sequence), or other mathematical entity. Many of these entities have been given simple and ambiguous names such as Euler's function, Euler's equation, and Euler's formula.\n\nEuler's work touched upon so many fields that he is often the earliest written reference on a given matter. In an effort to avoid naming everything after Euler, some discoveries and theorems are attributed to the first person to have proved them \"after\" Euler.\n\n\nUsually, \"Euler's equation\" refers to one of (or a set of) differential equations (DEs). It is customary to classify them into ODEs and PDEs.\n\nOtherwise, \"Euler's equation\" might refer to a non-differential equation, as in these three cases:\n\n\n\n\n\n\n\n\n\n\nSelected topics from above, grouped by subject.\n\n\n\n\n\n\n\n\n"}
{"id": "2976395", "url": "https://en.wikipedia.org/wiki?curid=2976395", "title": "Maria Klawe", "text": "Maria Klawe\n\nMaria Margaret Klawe ( ; born 1951) is a computer scientist and the fifth president of Harvey Mudd College (since July 1, 2006). Born in Toronto in 1951, she became a naturalized U.S. citizen in 2009. She was previously Dean of the School of Engineering and Applied Science at Princeton University.\n\nKlawe was born in Toronto, Ontario. She lived in Scotland from ages 4 to 12, and then returned to Canada, living with her family in Edmonton, Alberta.\nKlawe studied at the University of Alberta, dropped out to travel the world, and returned to earn her B.Sc. in 1973. She stayed at Alberta for her graduate studies, and in 1977 she earned her Ph.D. there in mathematics. She joined the mathematics faculty at Oakland University as an assistant professor in 1977 but only stayed for a year. She started a second Ph.D., in computer science, at the University of Toronto, but was offered a faculty position there before completing the degree. When she made the decision to get a Ph.D in computer science she had never studied the subject before. There weren't many undergraduate classes at the time so she enrolled in upper-level courses and studied about 16 hours a day to do well. She spent eight years in the industry, serving at IBM's Almaden Research Center in San Jose, California, first as a research scientist, then as manager of the Discrete Mathematics Group and manager of the Mathematics and Related Computer Science Department. She and her husband Nick Pippenger then moved to the University of British Columbia, where she stayed for 15 years and served as head of the Department of Computer Science from 1988 to 1995, vice president of student and academic services from 1995 to 1998, and dean of science from 1998 to 2002.\nFrom UBC she moved to Princeton and then Harvey Mudd College, where she is the first woman president. When she arrived at Mudd only about 30% of students and faculty were female. Today about 45% of the students and over 40% of the faculty are female. She became a citizen of the United States on January 29, 2009. Later in 2009, she joined the board of directors of the Microsoft Corporation.\n\nKlawe was inducted as a fellow of the Association for Computing Machinery in 1996, a founding fellow of the Canadian Information Processing Society in 2006, a fellow of the American Academy of Arts and Sciences in 2009, a fellow of the American Mathematical Society in 2012, and a fellow of the Association for Women in Mathematics in 2019.\n\nShe has been awarded honorary doctorates from Ryerson Polytechnic University in 2001, the University of Waterloo in 2003, Queen's University in 2004, Dalhousie University in 2005, Acadia University in 2006, the University of Alberta in 2007, the University of Ottawa in 2008, the University of British Columbia in 2010, the University of Toronto in 2015, Concordia University in 2016, and McGill University in 2018 \n\nShe was the winner of the 2014 Woman of Vision ABIE Award for Leadership from the Anita Borg Institute.\n\nIn 2018 she was featured among \"America's Top 50 Women In Tech\" by Forbes.\n\nShe also served as the president of the Association for Computing Machinery from 2002-2004, and in 2004 won their A. Nico Habermann award.\n\nSome of Klawe's best-cited research works concern algorithms for solving geometric optimization problems, distributed leader election, and the art gallery problem, and studies of the effects of gender on electronic game-playing. She founded the Aphasia Project, a collaboration between UBC and Princeton to study aphasia and develop cognitive aids for people suffering from it, after her friend Anita Borg developed brain cancer.\n\nKlawe has been heavily involved with increasing the representation of women in STEM fields. While Klawe was the dean at UBC, she became the NSERC-IBM chair for Women in Science and Engineering. She was in charge of increasing female participation in science and engineering. During her five years as the chair appointment she increased female computer science majors from 16% to 27% and increased the number of female computer science faculty from 2 to 7. In 1991, together with Nancy Leveson, she founded CRA-W (The Computing Research Association's Committee on the Status of Women in Computing Research) and served as its first co-chair. She was also a personal friend of Anita Borg and served as the chair of the Board of Trustees of the Anita Borg Institute for Women and Technology from 1996 to 2011. Klawe was a huge advocate for salary negotiation by women, disagreeing with Microsoft's CEO Satya Nadella, when he said \"It's not really about asking for a raise, but knowing and having faith that the system will give you the right raise. That might be one of the initial 'super powers,' that quite frankly, women (who) don't ask for a raise have. It's good karma. It will come back.\"\n\nKlawe's believes that women should take an entry level computer science course during their first year at college that focuses on portraying the field as fun and engaging rather than trying to convince women to stay. She believes that if programming courses are taken at the middle school level then they have another four years of high school for peer pressure to get them disinterested again.\" This is what she does at Harvey Mudd. She attributes the lack of women in technical fields due to how the media portrays women. In an interview with PBS she explains how TV shows in the 1970s showed men along with women who had successful careers such as doctors or lawyers and that caused the number of women going into medicine skyrocket. Klawe emphasizes that the introductory courses offered need to be presented in a problem-solving environment, not a competitive one where a few males dominate the conversation. Klawe believes the \"testosterone culture\" prevents women from continuing on with CS because the males that know everything scare away anyone who is trying to learn. Currently, Klawe is working on helping biology majors learn computer science by working with UCSD to create a biology themed introductory computer science course. Another project she's working on is an online course called MOOC aimed at 10th grade students.\n\nKlawe has also exhibited her watercolors.\n\n"}
{"id": "59204579", "url": "https://en.wikipedia.org/wiki?curid=59204579", "title": "Marta Bunge", "text": "Marta Bunge\n\nMarta Cavallo Bunge is a Argentine-Canadian mathematician specializing in category theory, and known for her work on synthetic calculus of variations and synthetic differential topology. She is a professor emeritus at McGill University.\n\nBunge was a student at a teacher's college in Buenos Aires, the daughter of Ricardo and María-Teresa Cavallo.\nShe met Argentine philosopher Mario Bunge while auditing one of his courses, and they eloped in late 1958 (as his second marriage).\n\nBunge earned her Ph.D. from the University of Pennsylvania in 1966. Her dissertation, \"Categories of Set Valued Functors\", was jointly supervised by Peter J. Freyd and William Lawvere.\nWhen she was offered a postdoctoral research position at McGill in 1966, her husband followed her there, and they remained in Canada afterwards.\nShe became an assistant professor at McGill in 1969, was promoted to full professor in 1985, and retired as a professor emeritus in 2003.\n\nWith her doctoral student Jonathan Funk, Bunge is the co-author of \"Singular Coverings of Toposes\" (Lecture Notes in Mathematics 1890, Springer, 2006).\nWith Felipe Gago and Ana María San Luis, Bunge is the co-author of \"Synthetic Differential Topology\" (London Mathematical Society Lecture Note Series 448, Cambridge University Press, 2018).\n\n"}
{"id": "54904665", "url": "https://en.wikipedia.org/wiki?curid=54904665", "title": "Natural bundle", "text": "Natural bundle\n\nIn mathematics, a natural bundle is any fiber bundle associated to the \"s\"-frame bundle formula_1 for some formula_2. It turns out that its transition functions depend functionally on local changes of coordinates in the base manifold formula_3 together with their partial derivatives up to order at most formula_4. \n\nThe concept of a natural bundle was introduced by Albert Nijenhuis as a modern reformulation of the classical concept of an arbitrary bundle of geometric objects.\n\nAn example of natural bundle (of first order) is the tangent bundle formula_5 of a manifold formula_3.\n\n"}
{"id": "53880799", "url": "https://en.wikipedia.org/wiki?curid=53880799", "title": "Nikolai Chernov", "text": "Nikolai Chernov\n\nNikolai Chernov (November 10, 1956 – August 7, 2014)\nwas a Ukrainian-American mathematician.\nSince 1994, he worked at University of Alabama at Birmingham. \nIn 2012, he became a Fellow of the American Mathematical Society.\n"}
{"id": "37658021", "url": "https://en.wikipedia.org/wiki?curid=37658021", "title": "Null model", "text": "Null model\n\nIn mathematics, for example in the study of statistical properties of graphs, a null model is type of random object that matches one specific object in some of its features, or more generally satisfies a collection of constraints, but which is otherwise taken to be an unbiasedly random structure. The null model is used as a term of comparison, to verify whether the object in question displays some non-trivial features (properties that wouldn't be expected on the basis of chance alone or as a consequence of the constraints), such as community structure in graphs. An appropriate null model behaves in accordance with a reasonable null hypothesis for the behavior of the system under investigation.\n\nOne null model of utility in the study of complex networks is that proposed by Newman and Girvan, consisting of a randomized version of an original graph formula_1, produced through edges being rewired at random, under the constraint that the expected degree of each vertex matches the degree of the vertex in the original graph.\n\nThe null model is the basic concept behind the definition of modularity, a function which evaluates the goodness of partitions of a graph into clusters. In particular, given a graph formula_1 and a specific community partition formula_3 (an assignment of a community-index formula_4 (here taken as an integer from formula_5 to formula_6) to each vertex formula_7 in the graph), the modularity measures the difference between the number of links from/to each pair of communities, from that expected in a graph that is completely random in all respects other than the set of degrees of each of the vertices (the degree sequence). In other words, the modularity contrasts the exhibited community structure in formula_1 with that of a null model, which in this case is the configuration model (the maximally random graph subject to a constraint on the degree of each vertex).\n\n"}
{"id": "43476", "url": "https://en.wikipedia.org/wiki?curid=43476", "title": "Operations research", "text": "Operations research\n\nOperations research, or operational research in British usage, is a discipline that deals with the application of advanced analytical methods to help make better decisions. Further, the term 'operational analysis' is used in the British (and some British Commonwealth) military as an intrinsic part of capability development, management and assurance. In particular, operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals (COEIA), which support British defense capability acquisition decision-making.\n\nIt is often considered to be a sub-field of applied mathematics. The terms management science and decision science are sometimes used as synonyms.\n\nEmploying techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost) of some real-world objective. Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.\nOperational research (OR) encompasses a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system. Because of the computational and statistical nature of most of these fields, OR also has strong ties to computer science and analytics. Operational researchers faced with a new problem must determine which of these techniques are most appropriate given the nature of the system, the goals for improvement, and constraints on time and computing power.\n\nThe major sub-disciplines in modern operational research, as identified by the journal \"Operations Research\", are:\n\nIn the decades after the two world wars, the tools of operations research were more widely applied to problems in business, industry and society. Since that time, operational research has expanded into a field widely used in industries ranging from petrochemicals to airlines, finance, logistics, and government, moving to a focus on the development of mathematical models that can be used to analyse and optimize complex systems, and has become an area of active academic and industrial research.\n\nIn the 17th century, mathematicians like Christiaan Huygens and Blaise Pascal (problem of points) tried to solve problems involving complex decisions with probability. Others in the 18th and 19th centuries solved these types of problems with combinatorics. Charles Babbage's research into the cost of transportation and sorting of mail led to England's universal \"Penny Post\" in 1840, and studies into the dynamical behaviour of railway vehicles in defence of the GWR's broad gauge. Beginning in the 20th century, study of inventory management could be considered the origin of modern operations research with economic order quantity developed by Ford W. Harris in 1913. Operational research may have originated in the efforts of military planners during World War I (convoy theory and Lanchester's laws). Percy Bridgman brought operational research to bear on problems in physics in the 1920s and would later attempt to extend these to the social sciences. \n\nModern operational research originated at the Bawdsey Research Station in the UK in 1937 and was the result of an initiative of the station's superintendent, A. P. Rowe. Rowe conceived the idea as a means to analyse and improve the working of the UK's early warning radar system, Chain Home (CH). Initially, he analysed the operating of the radar equipment and its communication networks, expanding later to include the operating personnel's behaviour. This revealed unappreciated limitations of the CH network and allowed remedial action to be taken.\n\nScientists in the United Kingdom including Patrick Blackett (later Lord Blackett OM PRS), Cecil Gordon, Solly Zuckerman, (later Baron Zuckerman OM, KCB, FRS), C. H. Waddington, Owen Wansbrough-Jones, Frank Yates, Jacob Bronowski and Freeman Dyson, and in the United States with George Dantzig looked for ways to make better decisions in such areas as logistics and training schedules\n\nThe modern field of operational research arose during World War II. In the World War II era, operational research was defined as \"a scientific method of providing executive departments with a quantitative basis for decisions regarding the operations under their control\". Other names for it included operational analysis (UK Ministry of Defence from 1962) and quantitative management.\n\nDuring the Second World War close to 1,000 men and women in Britain were engaged in operational research. About 200 operational research scientists worked for the British Army.\n\nPatrick Blackett worked for several different organizations during the war. Early in the war while working for the Royal Aircraft Establishment (RAE) he set up a team known as the \"Circus\" which helped to reduce the number of anti-aircraft artillery rounds needed to shoot down an enemy aircraft from an average of over 20,000 at the start of the Battle of Britain to 4,000 in 1941.\nIn 1941, Blackett moved from the RAE to the Navy, after first working with RAF Coastal Command, in 1941 and then early in 1942 to the Admiralty. Blackett's team at Coastal Command's Operational Research Section (CC-ORS) included two future Nobel prize winners and many other people who went on to be pre-eminent in their fields. They undertook a number of crucial analyses that aided the war effort. Britain introduced the convoy system to reduce shipping losses, but while the principle of using warships to accompany merchant ships was generally accepted, it was unclear whether it was better for convoys to be small or large. Convoys travel at the speed of the slowest member, so small convoys can travel faster. It was also argued that small convoys would be harder for German U-boats to detect. On the other hand, large convoys could deploy more warships against an attacker. Blackett's staff showed that the losses suffered by convoys depended largely on the number of escort vessels present, rather than the size of the convoy. Their conclusion was that a few large convoys are more defensible than many small ones.\n\nWhile performing an analysis of the methods used by RAF Coastal Command to hunt and destroy submarines, one of the analysts asked what colour the aircraft were. As most of them were from Bomber Command they were painted black for night-time operations. At the suggestion of CC-ORS a test was run to see if that was the best colour to camouflage the aircraft for daytime operations in the grey North Atlantic skies. Tests showed that aircraft painted white were on average not spotted until they were 20% closer than those painted black. This change indicated that 30% more submarines would be attacked and sunk for the same number of sightings. As a result of these findings Coastal Command changed their aircraft to using white undersurfaces.\n\nOther work by the CC-ORS indicated that on average if the trigger depth of aerial-delivered depth charges (DCs) were changed from 100 feet to 25 feet, the kill ratios would go up. The reason was that if a U-boat saw an aircraft only shortly before it arrived over the target then at 100 feet the charges would do no damage (because the U-boat wouldn't have had time to descend as far as 100 feet), and if it saw the aircraft a long way from the target it had time to alter course under water so the chances of it being within the 20-foot kill zone of the charges was small. It was more efficient to attack those submarines close to the surface when the targets' locations were better known than to attempt their destruction at greater depths when their positions could only be guessed. Before the change of settings from 100 feet to 25 feet, 1% of submerged U-boats were sunk and 14% damaged. After the change, 7% were sunk and 11% damaged. (If submarines were caught on the surface, even if attacked shortly after submerging, the numbers rose to 11% sunk and 15% damaged). Blackett observed \"there can be few cases where such a great operational gain had been obtained by such a small and simple change of tactics\".\n\nBomber Command's Operational Research Section (BC-ORS), analyzed a report of a survey carried out by RAF Bomber Command. For the survey, Bomber Command inspected all bombers returning from bombing raids over Germany over a particular period. All damage inflicted by German air defences was noted and the recommendation was given that armour be added in the most heavily damaged areas. This recommendation was not adopted because the fact that the aircraft returned with these areas damaged indicated these areas were not vital, and adding armour to non-vital areas where damage is acceptable negatively affects aircraft performance. Their suggestion to remove some of the crew so that an aircraft loss would result in fewer personnel losses, was also rejected by RAF command. Blackett's team made the logical recommendation that the armour be placed in the areas which were completely untouched by damage in the bombers which returned. They reasoned that the survey was biased, since it only included aircraft that returned to Britain. The untouched areas of returning aircraft were probably vital areas, which, if hit, would result in the loss of the aircraft. This story has been disputed, with a similar damage assessment study completed in the US by the Statistical Research Group at Columbia University and was the result of work done by Abraham Wald.\n\nWhen Germany organized its air defences into the Kammhuber Line, it was realized by the British that if the RAF bombers were to fly in a bomber stream they could overwhelm the night fighters who flew in individual cells directed to their targets by ground controllers. It was then a matter of calculating the statistical loss from collisions against the statistical loss from night fighters to calculate how close the bombers should fly to minimize RAF losses.\n\nThe \"exchange rate\" ratio of output to input was a characteristic feature of operational research. By comparing the number of flying hours put in by Allied aircraft to the number of U-boat sightings in a given area, it was possible to redistribute aircraft to more productive patrol areas. Comparison of exchange rates established \"effectiveness ratios\" useful in planning. The ratio of 60 mines laid per ship sunk was common to several campaigns: German mines in British ports, British mines on German routes, and United States mines in Japanese routes.\n\nOperational research doubled the on-target bomb rate of B-29s bombing Japan from the Marianas Islands by increasing the training ratio from 4 to 10 percent of flying hours; revealed that wolf-packs of three United States submarines were the most effective number to enable all members of the pack to engage targets discovered on their individual patrol stations; revealed that glossy enamel paint was more effective camouflage for night fighters than traditional dull camouflage paint finish, and the smooth paint finish increased airspeed by reducing skin friction.\n\nOn land, the operational research sections of the Army Operational Research Group (AORG) of the Ministry of Supply (MoS) were landed in Normandy in 1944, and they followed British forces in the advance across Europe. They analyzed, among other topics, the effectiveness of artillery, aerial bombing and anti-tank shooting.\n\nWith expanded techniques and growing awareness of the field at the close of the war, operational research was no longer limited to only operational, but was extended to encompass equipment procurement, training, logistics and infrastructure. Operations Research also grew in many areas other than the military once scientists learned to apply its principles to the civilian sector. With the development of the simplex algorithm for linear programming in 1947 and the development of computers over the next three decades, Operations Research can now \"solve problems with hundreds of thousands of variables and constraints. Moreover, the large volumes of data required for such problems can be stored and manipulated very efficiently.\"\n\n\nOperational research is also used extensively in government where evidence-based policy is used.\n\nIn 1967 Stafford Beer characterized the field of management science as \"the business use of operations research\". However, in modern times the term management science may also be used to refer to the separate fields of organizational studies or corporate strategy. Like operational research itself, management science (MS) is an interdisciplinary branch of applied mathematics devoted to optimal decision planning, with strong links with economics, business, engineering, and other sciences. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and meaningful management decisions by arriving at optimal or near optimal solutions to complex decision problems. Management scientists help businesses to achieve their goals using the scientific methods of operational research.\n\nThe management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. Of course, the techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.\n\nManagement science is concerned with developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems, as well as designing and developing new and better models of organizational excellence.\n\nThe application of these models within the corporate sector became known as management science.\n\nSome of the fields that have considerable overlap with Operations Research and Management Science include:\n\nApplications are abundant such as in airlines, manufacturing companies, service organizations, military branches, and government. The range of problems and issues to which it has contributed insights and solutions is vast. It includes:\nManagement is also concerned with so-called 'soft-operational analysis' which concerns methods for strategic planning, strategic decision support, problem structuring methods. \nIn dealing with these sorts of challenges, mathematical modeling and simulation may not be appropriate or may not suffice. Therefore, during the past 30 years, a number of non-quantified modeling methods have been developed. These include:\n\nThe International Federation of Operational Research Societies (IFORS) is an umbrella organization for operational research societies worldwide, representing approximately 50 national societies including those in the US, UK, France, Germany, Italy, Canada, Australia, New Zealand, Philippines, India, Japan and South Africa. The constituent members of IFORS form regional groups, such as that in Europe. Other important operational research organizations are Simulation Interoperability Standards Organization (SISO) and Interservice/Industry Training, Simulation and Education Conference (I/ITSEC)\n\nIn 2004 the US-based organization INFORMS began an initiative to market the OR profession better, including a website entitled \"The Science of Better\" which provides an introduction to OR and examples of successful applications of OR to industrial problems. This initiative has been adopted by the Operational Research Society in the UK, including a website entitled \"Learn about OR\".\n\nThe Institute for Operations Research and the Management Sciences (INFORMS) publishes thirteen scholarly journals about operations research, including the top two journals in their class, according to 2005 Journal Citation Reports. They are:\n\n\n\n\n\n\n\n\n\n"}
{"id": "35737249", "url": "https://en.wikipedia.org/wiki?curid=35737249", "title": "Poincaré–Birkhoff theorem", "text": "Poincaré–Birkhoff theorem\n\nIn symplectic topology and dynamical systems, Poincaré–Birkhoff theorem (also known as Poincaré–Birkhoff fixed point theorem and Poincaré's last geometric theorem) states that every area-preserving, orientation-preserving homeomorphism of an annulus that rotates the two boundaries in opposite directions has at least two fixed points.\n\nThe Poincaré–Birkhoff theorem was discovered by Henri Poincaré, who published it in a 1912 paper titled \"Sur un théorème de géométrie\", and proved it for some special cases. The general case was proved by George D. Birkhoff in his 1913 paper titled \"Proof of Poincaré's geometric theorem\".\n\n"}
{"id": "43844668", "url": "https://en.wikipedia.org/wiki?curid=43844668", "title": "Princeton Lectures in Analysis", "text": "Princeton Lectures in Analysis\n\nThe Princeton Lectures in Analysis is a series of four mathematics textbooks, each covering a different area of mathematical analysis. They were written by Elias M. Stein and Rami Shakarchi and published by Princeton University Press between 2003 and 2011. They are, in order, \"Fourier Analysis: An Introduction\"; \"Complex Analysis\"; \"Real Analysis: Measure Theory, Integration, and Hilbert Spaces\"; and \"Functional Analysis: Introduction to Further Topics in Analysis\".\n\nStein and Shakarchi wrote the books based on a sequence of intensive undergraduate courses Stein began teaching in the spring of 2000 at Princeton University. At the time Stein was a mathematics professor at Princeton and Shakarchi was a graduate student in mathematics. Though Shakarchi graduated in 2002, the collaboration continued until the final volume was published in 2011. The series emphasizes the unity among the branches of analysis and the applicability of analysis to other areas of mathematics.\n\nThe \"Princeton Lectures in Analysis\" has been identified as a well written and influential series of textbooks, suitable for advanced undergraduates and beginning graduate students in mathematics.\n\nThe first author, Elias M. Stein, is a mathematician who has made significant research contributions to the field of mathematical analysis. Before 2000 he had authored or co-authored several influential advanced textbooks on analysis.\n\nBeginning in the spring of 2000, Stein taught a sequence of four intensive undergraduate courses in analysis at Princeton University, where he was a mathematics professor. At the same time he collaborated with Rami Shakarchi, then a graduate student in Princeton's math department studying under Charles Fefferman, to turn each of the courses into a textbook. Stein taught Fourier analysis in that first semester, and by the fall of 2000 the first manuscript was nearly finished. That fall Stein taught the course in complex analysis while he and Shakarchi worked on the corresponding manuscript. Paul Hagelstein, then a postdoctoral scholar in the Princeton math department, was a teaching assistant for this course. In spring 2001, when Stein moved on to the real analysis course, Hagelstein started the sequence anew, beginning with the Fourier analysis course. Hagelstein and his students used Stein and Shakarchi's drafts as texts, and they made suggestions to the authors as they prepared the manuscripts for publication. The project received financial support from Princeton University and from the National Science Foundation.\n\nShakarchi earned his Ph.D. from Princeton in 2002 and moved to London to work in finance. Nonetheless he continued working on the books, even as his employer, Lehman Brothers, collapsed in 2008. The first two volumes were published in 2003. The third followed in 2005, and the fourth in 2011. Princeton University Press published all four.\n\nThe volumes are split into seven to ten chapters each. Each chapter begins with an epigraph providing context for the material and ends with a list of challenges for the reader, split into Exercises, which range in difficulty, and more difficult Problems. Throughout the authors emphasize the unity among the branches of analysis, often referencing one branch within another branch's book. They also provide applications of the theory to other fields of mathematics, particularly partial differential equations and number theory.\n\n\"Fourier Analysis\" covers the discrete, continuous, and finite Fourier transforms and their properties, including inversion. It also presents applications to partial differential equations, Dirichlet's theorem on arithmetic progressions, and other topics. Because Lebesgue integration is not introduced until the third book, the authors use Riemann integration in this volume. They begin with Fourier analysis because of its central role within the historical development and contemporary practice of analysis.\n\n\"Complex Analysis\" treats the standard topics of a course in complex variables as well as several applications to other areas of mathematics. The chapters cover the complex plane, Cauchy's integral theorem, meromorphic functions, connections to Fourier analysis, entire functions, the gamma function, the Riemann zeta function, conformal maps, elliptic functions, and theta functions.\n\n\"Real Analysis\" begins with measure theory, Lebesgue integration, and differentiation in Euclidean space. It then covers Hilbert spaces before returning to measure and integration in the context of abstract measure spaces. It concludes with a chapter on Hausdorff measure and fractals.\n\n\"Functional Analysis\" has chapters on several advanced topics in analysis: L spaces, distributions, the Baire category theorem, probability theory including Brownian motion, several complex variables, and oscillatory integrals.\n\nThe books \"received rave reviews indicating they are all outstanding works written with remarkable clarity and care.\" Reviews praised the exposition, identified the books as accessible and informative for advanced undergraduates or graduate math students, and predicted they would grow in influence as they became standard references for graduate courses. William Ziemer wrote that the third book omitted material he expected to see in an introductory graduate text but nonetheless recommended it as a reference.\n\nPeter Duren compared Stein and Shakarchi's attempt at a unified treatment favorably with Walter Rudin's textbook \"Real and Complex Analysis\", which Duren calls too terse. On the other hand, Duren noted that this sometimes comes at the expense of topics that reside naturally within only one branch. He mentioned in particular geometric aspects of complex analysis covered in Lars Ahlfors's textbook but noted that Stein and Shakarchi also treat some topics Ahlfors skips.\n\n\n"}
{"id": "16722187", "url": "https://en.wikipedia.org/wiki?curid=16722187", "title": "Regression discontinuity design", "text": "Regression discontinuity design\n\nIn statistics, econometrics, political science, epidemiology, and related disciplines, a regression discontinuity design (RDD) is a quasi-experimental pretest-posttest design that elicits the causal effects of interventions by assigning a cutoff or threshold above or below which an intervention is assigned. By comparing observations lying closely on either side of the threshold, it is possible to estimate the average treatment effect in environments in which randomization is unfeasible. First applied by Donald Thistlethwaite and Donald Campbell to the evaluation of scholarship programs, the RDD has become increasingly popular in recent years.\n\nThe intuition behind the RDD is well illustrated using the evaluation of merit-based scholarships. The main problem with estimating the causal effect of such an intervention is the endogeneity of performance to the assignment of treatment (e.g. scholarship award): Since high-performing students are more likely to be awarded the merit scholarship \"and\" continue performing well at the same time, comparing the outcomes of awardees and non-recipients would lead to an upward bias of the estimates. Even \"if\" the scholarship did not improve grades at all, awardees would have performed better than non-recipients, simply because scholarships were given to students who were performing well ex ante.\n\nDespite the absence of an experimental design, a RDD can exploit exogenous characteristics of the intervention to elicit causal effects. If all students above a given grade—for example 80%—are given the scholarship, it is possible to elicit the local treatment effect by comparing students around the 80% cut-off: The intuition here is that a student scoring 79% is likely to be very similar to a student scoring 81%—given the pre-defined threshold of 80%, however, one student will receive the scholarship while the other will not. Comparing the outcome of the awardee (treatment group) to the counterfactual outcome of the non-recipient (control group) will hence deliver the local treatment effect.\n\nThe two most common approaches to estimation using a RDD are nonparametric and parametric (normally polynomial regression).\n\nThe most common non-parametric method used in the RDD context is a local linear regression. This is of the form:\n\nwhere formula_2 is the treatment cut-off and formula_3 is a binary variable equal to one if formula_4. Letting formula_5 be the bandwidth of data used, we have formula_6. Different slopes and intercepts fit data on either side of the cutoff. Typically either a rectangular kernel (no weighting) or a triangular kernel are used. Research favors the triangular kernel but the rectangular kernel has a more straightforward interpretation.\n\nThe major benefit of using non-parametric methods in a RDD is that they provide estimates based on data closer to the cut-off, which is intuitively appealing. This reduces some bias that can result from using data farther away from the cutoff to estimate the discontinuity at the cutoff. More formally, local linear regressions are preferred because they have better bias properties and have better convergence. However, the use of both types estimation, if feasible, is a useful way to argue that the estimated results do not rely too heavily on the particular approach taken.\n\nAn example of a parametric estimation is:\nwhere\nand formula_9 is the treatment cut-off.\nNote that the polynomial part can be shortened or extended according to the needs.\n\n\nRegression discontinuity design requires that treatment assignment is \"as good as random\" at the threshold for treatment. If this holds, then it guarantees that those who just barely received treatment are comparable to those who just barely did not receive treatment, as treatment status is effectively random.\n\nTreatment assignment at the threshold can be \"as good as random\" if there is randomness in the assignment variable and the agents considered (individuals, firms, etc.) cannot perfectly manipulate their treatment status. For example, if the treatment is passing an exam, where a grade of 50% is required, then this example is a valid regression discontinuity design so long as grades are somewhat random, due either to randomness of grading or randomness of student performance.\n\nStudents must not also be able to perfectly manipulate their grade so as to perfectly determine their treatment status. Two examples include students being able to convince teachers to \"mercy pass\" them, or students being allowed to re-take the exam until they pass. In the former case, those students who barely fail but are able to secure a \"mercy pass\" may differ from those who just barely fail but cannot secure a \"mercy pass\". This leads to selection bias, as the treatment and control groups now differ. In the latter case, some students may decide to retake the exam, stopping once they pass. This also leads to selection bias since only some students will decide to retake the exam.\n\nIt is impossible to definitively test for validity if agents are able to perfectly determine their treatment status. However, there are some tests that can provide evidence that either supports or discounts the validity of the regression discontinuity design.\n\nMcCrary (2008) suggested examining the density of observations of the assignment variable. If there is a discontinuity in the density of the assignment variable at the threshold for treatment, then this may suggest that some agents were able to perfectly manipulate their treatment status.\n\nFor example, if several students are able to get a \"mercy pass\", then there will be more students who just barely passed the exam than who just barely failed. Similarly, if students are allowed to retake the exam until they pass, then there will be a similar result. In both cases, this will likely show up when the density of exam grades is examined. \"Gaming the system\" in this manner could bias the treatment effect estimate.\n\nSince the validity of the regression discontinuity design relies on those who were just barely treated being the same as those who were just barely not treated, it makes sense to examine if these groups are similar based on observable variables. For the earlier example, one could test if those who just barely passed have different characteristics (demographics, family income, etc.) than those who just barely failed. Although some variables may differ for the two groups based on random chance, most of these variables should be the same.\n\nSimilar to the continuity of observable variables, one would expect there to be continuity in predetermined variables at the treatment cut-off. Since these variables were determined before the treatment decision, treatment status should have no effect on them. Consider the earlier merit-based scholarship example. If the outcome of interest is future grades, then we would not expect the scholarship to affect earlier grades. If a discontinuity in predetermined variables is present at the treatment cut-off, then this puts the validity of the regression discontinuity design into question.\n\nIf discontinuities are present at other points of the assignment variable, where these are not expected, then this may make the regression discontinuity design suspect. Consider the example of Carpenter and Dobkin (2011) who studied the effect of legal access to alcohol in the United States. As access to alcohol increases at age 21, this leads to changes in various outcomes, such as mortality rates and morbidity rates. If mortality and morbidity rates also increase discontinuously at other ages, then it throws the interpretation of the discontinuity at age 21 into question.\n\nIf parameter estimates are sensitive to removing or adding covariates to the model, then this may cast doubt on the validity of the regression discontinuity design. A significant change may suggest that those who just barely got treatment differ in these covariates from those who just barely did not get treatment. Including covariates would remove some of this bias. If a large amount of bias is present, and the covariates explain a significant amount of this, then their inclusion or exclusion would significantly change the parameter estimate.\n\nRecent work has shown how to add covariates, under what conditions doing so is valid, and the potential for increased precision.\n\n\n\nThe identification of causal effects hinges on the crucial assumption that there is indeed a sharp cut-off, around which there is a discontinuity in the probability of assignment from 0 to 1. In reality, however, cut-offs are often not strictly implemented (e.g. exercised discretion for students who just fell short of passing the threshold) and the estimates will hence be biased.\n\nIn contrast to the sharp regression discontinuity design, a fuzzy regression discontinuity design (FRDD) does not require a sharp discontinuity in the probability of assignment but is applicable as long as the probability of assignment is different. The intuition behind it is related to the instrumental variable strategy and intention to treat.\n\nWhen the assignment variable is continuous (e.g. student aid) and depends predictably on another observed variable (e.g. family income), one can identify treatment effects using sharp changes in the slope of the treatment function. This technique was coined \"regression kink design\" by Nielsen, Sørensen, and Tabe (2010), though they cite similar earlier analyses. They write, \"This approach resembles the regression discontinuity idea. Instead of a discontinuity of in the level of the stipend-income function, we have a discontinuity in the slope of the function.\" Rigorous theoretical foundations were provided by Card et al. (2012) and an empirical application by Bockerman et al. (2018). \nNote that \"regression kinks\" (or \"kinked regression\") can also mean a type of segmented regression, which is a different type of analysis.\n\n\n\n"}
{"id": "16996923", "url": "https://en.wikipedia.org/wiki?curid=16996923", "title": "Richard Pendlebury", "text": "Richard Pendlebury\n\nRichard Pendlebury (1847, Liverpool – 1902) was a British mathematician, musician, bibliophile and mountaineer.\n\nHe went up to St John's College, Cambridge in 1866 and graduated senior wrangler in 1870: he was then elected to a college fellowship. He was appointed University Lecturer in Mathematics in 1888. He collected early mathematical books and printed music, donating his collections to his college and university. His presentation of a collection of music books and manuscripts to the Fitzwilliam Museum stimulated the formation of the Music Faculty at Cambridge University.\n\nIn 1872, along with the guide Ferdinand Imseng and other climbers, he made the first ascent of the Monte Rosa east face from Macugnaga.\n\n\n"}
{"id": "285512", "url": "https://en.wikipedia.org/wiki?curid=285512", "title": "State (computer science)", "text": "State (computer science)\n\nIn information technology and computer science, a program is described as stateful if it is designed to remember preceding events or user interactions; the remembered information is called the state of the system.\n\nThe set of states a system can occupy is known as its state space. In a discrete system, the state space is countable and often finite, and the system's internal behaviour or interaction with its environment consists of separately occurring individual actions or events, such as accepting input or producing output, that may or may not cause the system to change its state. Examples of such systems are digital logic circuits and components, automata and formal language, computer programs, and computers. The output of a digital circuit or computer program at any time is completely determined by its current inputs and its state.\n\nDigital logic circuits can be divided into two types: combinational logic, whose output signals are dependent only on its present input signals, and sequential logic, whose outputs are a function of both the current inputs and the past history of inputs. In sequential logic, information from past inputs is stored in electronic memory elements, such as flip-flops. The stored contents of these memory elements, at a given point in time, is collectively referred to as the circuit's \"state\" and contains all the information about the past to which the circuit has access.\n\nSince each binary memory element, such as a flip-flop, has only two possible states, \"one\" or \"zero\", and there is a finite number of memory elements, a digital circuit has only a certain finite number of possible states. If N is the number of binary memory elements in the circuit, the maximum number of states a circuit can have is 2.\n\nSimilarly, a computer program stores data in variables, which represent storage locations in the computer's memory. The contents of these memory locations, at any given point in the program's execution, is called the program's \"state\".\n\nA more specialized definition of state is used for computer programs that operate serially or sequentially on streams of data, such as parsers, firewalls, communication protocols and encryption. Serial programs operate on the incoming data characters or packets sequentially, one at a time. In some of these programs, information about previous data characters or packets received is stored in variables and used to affect the processing of the current character or packet. This is called a \"stateful protocol\" and the data carried over from the previous processing cycle is called the \"state\". In others, the program has no information about the previous data stream and starts \"fresh\" with each data input; this is called a \"stateless protocol\".\n\nImperative programming is a programming paradigm (way of designing a programming language) that describes computation in terms of the program state, and of the statements which change the program state. In declarative programming languages, the program describes the desired results and doesn't specify changes to the state directly.\n\nThe output of a sequential circuit or computer program at any time is completely determined by its current inputs and current state. Since each binary memory element has only two possible states, 0 or 1, the total number of different states a circuit can assume is finite, and fixed by the number of memory elements. If there are \"N\" binary memory elements, a digital circuit can have at most 2 distinct states. The concept of state is formalized in an abstract mathematical model of computation called a finite state machine, used to design both sequential digital circuits and computer programs.\n\nAn example of an everyday device that has a state is a television set. To change the channel of a TV, the user usually presses a \"channel up\" or \"channel down\" button on the remote control, which sends a coded message to the set. In order to calculate the new channel that the user desires, the digital tuner in the television must have stored in it the number of the \"current channel\" it is on. It then adds one or subtracts one from this number to get the number for the new channel, and adjusts the TV to receive that channel. This new number is then stored as the \"current channel\". Similarly, the television also stores a number that controls the level of volume produced by the speaker. Pressing the \"volume up\" or \"volume down\" buttons increments or decrements this number, setting a new level of volume. Both the \"current channel\" and \"current volume\" numbers are part of the TV's state. They are stored in non-volatile memory, which preserves the information when the TV is turned off, so when it is turned on again the TV will return to its previous station and volume level.\n\nAs another example, the state of a microprocessor is the contents of all the memory elements in it: the accumulators, storage registers, data caches, and flags. When computers such as laptops go into a hibernation mode to save energy by shutting down the processor, the state of the processor is stored on the computer's hard disk, so it can be restored when the computer comes out of hibernation, and the processor can take up operations where it left off.\n\n"}
{"id": "7301396", "url": "https://en.wikipedia.org/wiki?curid=7301396", "title": "Tetracategory", "text": "Tetracategory\n\nIn category theory, a tetracategory is a weakened definition of a 4-category.\n\n\n"}
{"id": "1783069", "url": "https://en.wikipedia.org/wiki?curid=1783069", "title": "Time derivative", "text": "Time derivative\n\nA time derivative is a derivative of a function with respect to time, usually interpreted as the rate of change of the value of the function. The variable denoting time is usually written as formula_1.\n\nA variety of notations are used to denote the time derivative. In addition to the normal (Leibniz's) notation,\n\nA very common short-hand notation used, especially in physics, is the 'over-dot'. I.E.\n\nHigher time derivatives are also used: the second derivative with respect to time is written as\n\nwith the corresponding shorthand of formula_5.\n\nAs a generalization, the time derivative of a vector, say:\n\nis defined as the vector whose components are the derivatives of the components of the original vector. That is,\n\nTime derivatives are a key concept in physics. For example, for a changing position formula_8, its time derivative formula_3 is its velocity, and its second derivative with respect to time, formula_5, is its acceleration. Even higher derivatives are sometimes also used: the third derivative of position with respect to time is known as the jerk. See motion graphs and derivatives.\n\nA large number of fundamental equations in physics involve first or second time derivatives of quantities. Many other fundamental quantities in science are time derivatives of one another:\nand so on.\n\nA common occurrence in physics is the time derivative of a vector, such as velocity or displacement. In dealing with such a derivative, both magnitude and orientation may depend upon time.\n\nFor example, consider a particle moving in a circular path. Its position is given by the displacement vector formula_11, related to the angle, \"θ\", and radial distance, \"r\", as defined in the figure:\n\nFor purposes of this example, set . The displacement (position) at any time \"t\" is then\n\nThis form shows the motion described by r(\"t\") is in a circle of radius \"r\" because the \"magnitude\" of r(\"t\") is given by\n\nusing the trigonometric identity and where formula_15 is the usual euclidean dot product.\n\nWith this form for the displacement, the velocity now is found. The time derivative of the displacement vector is the velocity vector. In general, the derivative of a vector is a vector made up of components each of which is the derivative of the corresponding component of the original vector. Thus, in this case, the velocity vector is:\n\nThus the velocity of the particle is nonzero even though the magnitude of the position (that is, the radius of the path) is constant. The velocity is directed perpendicular to the displacement, as can be established using the dot product:\n\nAcceleration is then the time-derivative of velocity:\n\nThe acceleration is directed inward, toward the axis of rotation. It points opposite to the position vector and perpendicular to the velocity vector. This inward-directed acceleration is called centripetal acceleration.\n\nIn differential geometry, quantities are often expressed with respect to the local covariant basis, formula_19, where \"i\" ranges over the number of dimensions. The components of a vector formula_20 expressed this way transform as a contravariant tensor, as shown in the expression formula_21, invoking Einstein summation convention. If we want to calculate the time derivates of these components along a trajectory, so that we have formula_22, we can define a new operator, the invariant derivative formula_23, which will continue to return contravariant tensors:\n\nwhere formula_25 (with formula_26 being the \"j\"th coordinate) captures the components of the velocity in the local covariant basis, and formula_27 are the Christoffel symbols for the coordinate system. Note that explicit dependence on \"t\" has been repressed in the notation. We can then write:\n\nas well as:\n\nIn terms of the covariant derivative, formula_30, we have:\n\nIn economics, many theoretical models of the evolution of various economic variables are constructed in continuous time and therefore employ time derivatives. See for example exogenous growth model and . One situation involves a stock variable and its time derivative, a flow variable. Examples include: \n\n\nSometimes the time derivative of a flow variable can appear in a model:\n\n\nAnd sometimes there appears a time derivative of a variable which, unlike the examples above, is not measured in units of currency:\n\n\n"}
{"id": "193053", "url": "https://en.wikipedia.org/wiki?curid=193053", "title": "Triviality (mathematics)", "text": "Triviality (mathematics)\n\nIn mathematics, the adjective trivial is frequently used for objects (for example, groups or topological spaces) that have a very simple structure. The noun triviality usually refers to a simple technical aspect of some proof or definition. The origin of the term in mathematical language comes from the medieval trivium curriculum. The antonym nontrivial is commonly used by engineers and mathematicians to indicate a statement or theorem that is not obvious or easy to prove.\n\nIn mathematics, the term trivial is frequently used for objects (for examples, groups or topological spaces) that have a very simple structure.\n\nExamples are as follows:\n\"Trivial\" can also be used to describe solutions to an equation that have a very simple structure, but for the sake of completeness cannot be omitted. These solutions are called the trivial solutions. For example, consider the differential equation\nwhere \"y\" = \"f\"(\"x\") is a function whose derivative is \"y\"′. The trivial solution is\nwhile a nontrivial solution is\n\nThe differential equation formula_2 with boundary conditions formula_3 is important in math and physics, for example describing a particle in a box in quantum mechanics, or standing waves on a string. It always has the solution formula_4. This solution is considered obvious and is called the \"trivial\" solution. In some cases, there may be other solutions (sinusoids), which are called \"nontrivial\".\n\nSimilarly, mathematicians often describe Fermat's Last Theorem as asserting that there are no nontrivial integer solutions to the equation formula_5 when \"n\" is greater than 2. Clearly, there \"are\" some solutions to the equation. For example, formula_6 is a solution for any \"n\", but such solutions are all obvious and uninteresting, and hence \"trivial\".\n\n\"Trivial\" may also refer to any easy case of a proof, which for the sake of completeness cannot be ignored. For instance, proofs by mathematical induction have two parts: the \"base case\" that shows that the theorem is true for a particular initial value such as \"n\" = 0 or \"n\" = 1 and then an inductive step that shows that if the theorem is true for a certain value of \"n\", it is also true for the value \"n\" + 1. The base case is often trivial and is identified as such, although there are situations where the base case is difficult but the inductive step is trivial. Similarly, one might want to prove that some property is possessed by all the members of a certain set. The main part of the proof will consider the case of a nonempty set, and examine the members in detail; in the case where the set is empty, the property is trivially possessed by all the members, since there are none. (See also Vacuous truth.)\n\nA common joke in the mathematical community is to say that \"trivial\" is synonymous with \"proved\"—that is, any theorem can be considered \"trivial\" once it is known to be true. Another joke concerns two mathematicians who are discussing a theorem; the first mathematician says that the theorem is \"trivial\". In response to the other's request for an explanation, he then proceeds with twenty minutes of exposition. At the end of the explanation, the second mathematician agrees that the theorem is trivial. These jokes point out the subjectivity of judgments about triviality. The joke also applies when the first mathematician says the theorem is trivial, but is unable to prove it himself. Often, as a joke, the theorem is then referred to as \"intuitively obvious\". Someone experienced in calculus, for example, would consider the statement that\n\nTriviality also depends on context. A proof in functional analysis would probably, given a number, trivially assume the existence of a larger number. When proving basic results about the natural numbers in elementary number theory though, the proof may very well hinge on the remark that any natural number has a successor (which should then in itself be proved or taken as an axiom, see Peano's axioms).\n\nIn some texts, a \"trivial proof\" refers to a statement involving a material implication where the consequent, or \"Q\", in \"P\"→\"Q\", is always true. Here, the proof follows simply from noting that \"Q\" is always true, as the implication is then true regardless of the truth value of the antecedent, \"P\".\n\nA related concept is a vacuous truth, where the antecedent, \"P\", in the material implication \"P\"→\"Q\" is always false. Here, the implication is always true regardless of the truth value of the consequent, \"Q\".\n\n\n\n"}
{"id": "3614395", "url": "https://en.wikipedia.org/wiki?curid=3614395", "title": "Tube lemma", "text": "Tube lemma\n\nIn mathematics, particularly topology, the tube lemma is a useful tool in order to prove that the finite product of compact spaces is compact. It is in general, a concept of point-set topology.\n\nBefore giving the lemma, one notes the following terminology:\n\n\nTube Lemma: Let \"X\" and \"Y\" be topological spaces with \"Y\" compact, and consider the product space \"X\" × \"Y\". If \"N\" is an open set containing a slice in \"X\" × \"Y\", then there exists a tube in \"X\" × \"Y\" containing this slice and contained in \"N\".\n\nUsing the concept of closed maps, this can be rephrased concisely as follows: if \"X\" is any topological space and \"Y\" a compact space, then the projection map \"X\" × \"Y\" → \"X\" is closed.\n\nGeneralized Tube Lemma: Let \"X\" and \"Y\" be topological spaces and consider the product space \"X\" × \"Y\". Let \"A\" be a compact subset of \"X\" and \"B\" be a compact subset of \"Y\". If \"N\" is an open set containing \"A\" × \"B\", then there exists \"U\" open in \"X\" and \"V\" open in \"Y\" such that formula_1.\n\n1. Consider \"R\" × \"R\" in the product topology, that is the Euclidean plane, and the open set \"N\" = { (\"x\", \"y\") : |\"x\"·\"y\"| < 1 }. The open set \"N\" contains {0} × \"R\", but contains no tube, so in this case the tube lemma fails. Indeed, if \"W\" × \"R\" is a tube containing {0} × \"R\" and contained in \"N\", \"W\" must be a subset of (−1/\"x\", +1/\"x\") for all positive integers \"x\" which means \"W\" = {0} contradicting the fact that \"W\" is open in \"R\" (because \"W\" × \"R\" is a tube). This shows that the compactness assumption is essential.\n\n2. The tube lemma can be used to prove that if \"X\" and \"Y\" are compact topological spaces, then \"X\" × \"Y\" is compact as follows:\n\nLet {\"G\"} be an open cover of \"X\" × \"Y\"; for each \"x\" belonging to \"X\", cover the slice {\"x\"} × \"Y\" by finitely many elements of {\"G\"} (this is possible since {\"x\"} × \"Y\" is compact being homeomorphic to \"Y\"). Call the union of these finitely many elements \"N\". By the tube lemma, there is an open set of the form \"W\" × \"Y\" containing {\"x\"} × \"Y\" and contained in \"N\". The collection of all \"W\" for \"x\" belonging to \"X\" is an open cover of \"X\" and hence has a finite subcover \"W\"  ∪ ... ∪ \"W\". Then for each \"x\", \"W\" × \"Y\" is contained in \"N\". Using the fact that each \"N\" is the finite union of elements of \"G\" and that the finite collection (\"W\" × \"Y\") ∪ ... ∪ (\"W\" × \"Y\") covers \"X\" × \"Y\", the collection \"N\" ∪ ... ∪ \"N\" is a finite subcover of \"X\" × \"Y\".\n\n3. By example 2 and induction, one can show that the finite product of compact spaces is compact.\n\n4. The tube lemma cannot be used to prove the Tychonoff theorem, which generalizes the above to infinite products.\n\nThe tube lemma follows from the generalized tube lemma by taking formula_2 and formula_3. It therefore suffices to prove the generalized tube lemma. By the definition of the product topology, for each formula_4 there are open sets formula_5 and formula_6 such that formula_7. Fix some formula_8. Then formula_9 is an open cover of formula_10. Since formula_10 is compact, this cover has a finite subcover; namely, there is a finite formula_12 such that formula_13. Set formula_14. Since formula_15 is finite, formula_16 is open. Also formula_17 is open. Moreover, the construction of formula_16 and formula_19 implies that formula_20. We now essentially repeat the argument to drop the dependence on formula_21. Let formula_22 be a finite subset such that formula_23 and set formula_24. It then follows by the above reasoning that formula_25 and formula_26 and formula_27 are open, which completes the proof.\n\n\n"}
