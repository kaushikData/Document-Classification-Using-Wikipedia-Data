{"id": "47489599", "url": "https://en.wikipedia.org/wiki?curid=47489599", "title": "2-functor", "text": "2-functor\n\nIn mathematics, a 2-functor is a morphism between 2-categories. They may be defined formally using enrichment by saying that a 2-category is exactly a \"Cat\"-enriched category and a 2-functor is a \"Cat\"-functor .\n\nExplicitly, if \"C\" and \"D\" are 2-categories then a 2-functor formula_1 consists of\nsuch that each formula_5 strictly preserves identity objects and they commute with horizontal composition in \"C\" and \"D\".\n\nSee for more details and for lax versions.\n"}
{"id": "600892", "url": "https://en.wikipedia.org/wiki?curid=600892", "title": "Arbitrary-precision arithmetic", "text": "Arbitrary-precision arithmetic\n\nIn computer science, arbitrary-precision arithmetic, also called bignum arithmetic, multiple-precision arithmetic, or sometimes infinite-precision arithmetic, indicates that calculations are performed on numbers whose digits of precision are limited only by the available memory of the host system. This contrasts with the faster fixed-precision arithmetic found in most arithmetic logic unit (ALU) hardware, which typically offers between 8 and 64 bits of precision.\n\nSeveral modern programming languages have built-in support for bignums, and others have libraries available for arbitrary-precision integer and floating-point math. Rather than store values as a fixed number of binary bits related to the size of the processor register, these implementations typically use variable-length arrays of digits.\n\nArbitrary precision is used in applications where the speed of arithmetic is not a limiting factor, or where precise results with very large numbers are required. It should not be confused with the symbolic computation provided by many computer algebra systems, which represent numbers by expressions such as , and can thus \"represent\" any computable number with infinite precision.\n\nA common application is public-key cryptography, whose algorithms commonly employ arithmetic with integers having hundreds of digits. Another is in situations where artificial limits and overflows would be inappropriate. It is also useful for checking the results of fixed-precision calculations, and for determining the optimum value for coefficients needed in formulae, for example the that appears in Gaussian integration.\n\nArbitrary precision arithmetic is also used to compute fundamental mathematical constants such as π to millions or more digits and to analyze the properties of the digit strings or more generally to investigate the precise behaviour of functions such as the Riemann zeta function where certain questions are difficult to explore via analytical methods. Another example is in rendering fractal images with an extremely high magnification, such as those found in the Mandelbrot set.\n\nArbitrary-precision arithmetic can also be used to avoid overflow, which is an inherent limitation of fixed-precision arithmetic. Similar to a 5-digit odometer's display which changes from 99999 to 00000, a fixed-precision integer may exhibit \"wraparound\" if numbers grow too large to represent at the fixed level of precision. Some processors can instead deal with overflow by \"saturation,\" which means that if a result would be unrepresentable, it is replaced with the nearest representable value. (With 16-bit unsigned saturation, adding any positive amount to 65535 would yield 65535.) Some processors can generate an exception if an arithmetic result exceeds the available precision. Where necessary, the exception can be caught and recovered from—for instance, the operation could be restarted in software using arbitrary-precision arithmetic.\n\nIn many cases, the task or the programmer can guarantee that the integer values in a specific application will not grow large enough to cause an overflow. Such guarantees may be based on pragmatic limits: a school attendance program may have a task limit of 4,000 students. A programmer may design the computation so that intermediate results stay within specified precision boundaries.\n\nSome programming languages such as Lisp, Python, Perl, Haskell and Ruby use, or have an option to use, arbitrary-precision numbers for \"all\" integer arithmetic. Although this reduces performance, it eliminates the possibility of incorrect results (or exceptions) due to simple overflow. It also makes it possible to guarantee that arithmetic results will be the same on all machines, regardless of any particular machine's word size. The exclusive use of arbitrary-precision numbers in a programming language also simplifies the language, because \"a number is a number\" and there is no need for multiple types to represent different levels of precision.\n\nArbitrary-precision arithmetic is considerably slower than arithmetic using numbers that fit entirely within processor registers, since the latter are usually implemented in hardware arithmetic whereas the former must be implemented in software. Even if the computer lacks hardware for certain operations (such as integer division, or all floating-point operations) and software is provided instead, it will use number sizes closely related to the available hardware registers: one or two words only and definitely not N words. There are exceptions, as certain \"variable word length\" machines of the 1950s and 1960s, notably the IBM 1620, IBM 1401 and the Honeywell \"Liberator\" series, could manipulate numbers bound only by available storage, with an extra bit that delimited the value.\n\nNumbers can be stored in a fixed-point format, or in a floating-point format as a significand multiplied by an arbitrary exponent. However, since division almost immediately introduces infinitely repeating sequences of digits (such as 4/7 in decimal, or 1/10 in binary), should this possibility arise then either the representation would be truncated at some satisfactory size or else rational numbers would be used: a large integer for the numerator and for the denominator. But even with the greatest common divisor divided out, arithmetic with rational numbers can become unwieldy very quickly: 1/99 − 1/100 = 1/9900, and if 1/101 is then added, the result is 10001/999900.\n\nThe size of arbitrary-precision numbers is limited in practice by the total storage available, the variables used to index the digit strings, and computation time. A 32-bit operating system may limit available storage to less than 4 GB. A programming language using 32-bit integers can only index 4 GB. If multiplication is done with an algorithm, it would take on the order of steps to multiply two one-million-word numbers.\n\nNumerous algorithms have been developed to efficiently perform arithmetic operations on numbers stored with arbitrary precision. In particular, supposing that digits are employed, algorithms have been designed to minimize the asymptotic complexity for large .\n\nThe simplest algorithms are for addition and subtraction, where one simply adds or subtracts the digits in sequence, carrying as necessary, which yields an algorithm (see big O notation).\n\nComparison is also very simple. Compare the high-order digits (or machine words) until a difference is found. Comparing the rest of the digits/words is not necessary. The worst case is , but usually it will go much faster.\n\nFor multiplication, the most straightforward algorithms used for multiplying numbers by hand (as taught in primary school) require operations, but multiplication algorithms that achieve complexity have been devised, such as the Schönhage–Strassen algorithm, based on fast Fourier transforms, and there are also algorithms with slightly worse complexity but with sometimes superior real-world performance for smaller . The Karatsuba multiplication is such an algorithm.\n\nFor division, see division algorithm.\n\nFor a list of algorithms along with complexity estimates, see computational complexity of mathematical operations.\n\nFor examples in x86 assembly, see external links.\n\nIn some languages such as REXX, the precision of all calculations must be set before doing a calculation. Other languages, such as Python and Ruby extend the precision automatically to prevent overflow.\n\nThe calculation of factorials can easily produce very large numbers. This is not a problem for their usage in many formulae (such as Taylor series) because they appear along with other terms, so that—given careful attention to the order of evaluation—intermediate calculation values are not troublesome. If approximate values of factorial numbers are desired, Stirling's approximation gives good results using floating-point arithmetic. The largest representable value for a fixed-size integer variable may be exceeded even for relatively small arguments as shown in the table below. Even floating-point numbers are soon outranged, so it may help to recast the calculations in terms of the logarithm of the number.\n\nBut if exact values for large factorials are desired, then special software is required, as in the pseudocode that follows, which implements the classic algorithm to calculate 1, 1×2, 1×2×3, 1×2×3×4, etc. the successive factorial numbers.\n\nWith the example in view, a number of details can be discussed. The most important is the choice of the representation of the big number. In this case, only integer values are required for digits, so an array of fixed-width integers is adequate. It is convenient to have successive elements of the array represent higher powers of the base.\n\nThe second most important decision is in the choice of the base of arithmetic, here ten. There are many considerations. The scratchpad variable must be able to hold the result of a single-digit multiply \"plus the carry\" from the prior digit's multiply. In base ten, a sixteen-bit integer is certainly adequate as it allows up to 32767. However, this example cheats, in that the value of is not itself limited to a single digit. This has the consequence that the method will fail for or so. In a more general implementation, would also use a multi-digit representation. A second consequence of the shortcut is that after the multi-digit multiply has been completed, the last value of \"carry\" may need to be carried into multiple higher-order digits, not just one.\n\nThere is also the issue of printing the result in base ten, for human consideration. Because the base is already ten, the result could be shown simply by printing the successive digits of array \"digit\", but they would appear with the highest-order digit last (so that 123 would appear as \"321\"). The whole array could be printed in reverse order, but that would present the number with leading zeroes (\"00000...000123\") which may not be appreciated, so we decided to build the representation in a space-padded text variable and then print that. The first few results (with spacing every fifth digit and annotation added here) are:\n\nWe could try to use the available arithmetic of the computer more efficiently. A simple escalation would be to use base 100 (with corresponding changes to the translation process for output), or, with sufficiently wide computer variables (such as 32-bit integers) we could use larger bases, such as 10,000. Working in a power-of-2 base closer to the computer's built-in integer operations offers advantages, although conversion to a decimal base for output becomes more difficult. On typical modern computers, additions and multiplications take constant time independent of the values of the operands (so long as the operands fit in single machine words), so there are large gains in packing as much of a bignumber as possible into each element of the digit array. The computer may also offer facilities for splitting a product into a digit and carry without requiring the two operations of \"mod\" and \"div\" as in the example, and nearly all arithmetic units provide a \"carry flag\" which can be exploited in multiple-precision addition and subtraction. This sort of detail is the grist of machine-code programmers, and a suitable assembly-language bignumber routine can run much faster than the result of the compilation of a high-level language, which does not provide access to such facilities.\n\nFor a single-digit multiply the working variables must be able to hold the value (base-1) + carry, where the maximum value of the carry is (base-1). Similarly, the variables used to index the digit array are themselves limited in width. A simple way to extend the indices would be to deal with the bignumber's digits in blocks of some convenient size so that the addressing would be via (block \"i\", digit \"j\") where \"i\" and \"j\" would be small integers, or, one could escalate to employing bignumber techniques for the indexing variables. Ultimately, machine storage capacity and execution time impose limits on the problem size.\n\nIBM's first business computer, the IBM 702 (a vacuum-tube machine) of the mid-1950s, implemented integer arithmetic \"entirely in hardware\" on digit strings of any length from 1 to 511 digits. The earliest widespread software implementation of arbitrary-precision arithmetic was probably that in Maclisp. Later, around 1980, the operating systems VAX/VMS and VM/CMS offered bignum facilities as a collection of string functions in the one case and in the languages EXEC 2 and REXX in the other.\n\nAn early widespread implementation was available via the IBM 1620 of 1959–1970. The 1620 was a decimal-digit machine which used discrete transistors, yet it had hardware (that used lookup tables) to perform integer arithmetic on digit strings of a length that could be from two to whatever memory was available. For floating-point arithmetic, the mantissa was restricted to a hundred digits or fewer, and the exponent was restricted to two digits only. The largest memory supplied offered 60 000 digits, however Fortran compilers for the 1620 settled on fixed sizes such as 10, though it could be specified on a control card if the default was not satisfactory.\n\nArbitrary-precision arithmetic in most computer software is implemented by calling an external library that provides data types and subroutines to store numbers with the requested precision and to perform computations.\n\nDifferent libraries have different ways of representing arbitrary-precision numbers, some libraries work only with integer numbers, others store floating point numbers in a variety of bases (decimal or binary powers). Rather than representing a number as single value, some store numbers as a numerator/denominator pair (rationals) and some can fully represent computable numbers, though only up to some storage limit. Fundamentally, Turing machines cannot represent all real numbers, as the cardinality of exceeds the cardinality of .\n\n\n\n"}
{"id": "21482414", "url": "https://en.wikipedia.org/wiki?curid=21482414", "title": "Architectural drawing", "text": "Architectural drawing\n\nAn architectural drawing or architect's drawing is a technical drawing of a building (or building project) that falls within the definition of architecture. Architectural drawings are used by architects and others for a number of purposes: to develop a design idea into a coherent proposal, to communicate ideas and concepts, to convince clients of the merits of a design, to enable a building contractor to construct it, as a record of the completed work, and to make a record of a building that already exists.\n\nArchitectural drawings are made according to a set of conventions, which include particular views (floor plan, section etc.), sheet sizes, units of measurement and scales, annotation and cross referencing. Conventionally, drawings were made in ink on paper or a similar material, and any copies required had to be laboriously made by hand. The twentieth century saw a shift to drawing on tracing paper, so that mechanical copies could be run off efficiently.\n\nThe development of the computer had a major impact on the methods used to design and create technical drawings, making manual drawing almost obsolete, and opening up new possibilities of form using organic shapes and complex geometry. Today the vast majority of drawings are created using CAD software.\n\nThe size of drawings reflects the materials available and the size that is convenient to transport – rolled up or folded, laid out on a table, or pinned up on a wall. The draughting process may impose limitations on the size that is realistically workable. Sizes are determined by a consistent paper size system, according to local usage. Normally the largest paper size used in modern architectural practice is ISO A0 () or in the USA Arch E () or Large E size ().\n\nArchitectural drawings are drawn to scale, so that relative sizes are correctly represented. The scale is chosen both to ensure the whole building will fit on the chosen sheet size, and to show the required amount of detail. At the scale of one eighth of an inch to one foot (1:96) or the metric equivalent 1 to 100, walls are typically shown as simple outlines corresponding to the overall thickness. At a larger scale, half an inch to one foot (1:24) or the nearest common metric equivalent 1 to 20, the layers of different materials that make up the wall construction are shown. Construction details are drawn to a larger scale, in some cases full size (1 to 1 scale).\n\nScale drawings enable dimensions to be \"read\" off the drawing, i.e. measured directly. Imperial scales (feet and inches) are equally readable using an ordinary ruler. On a one-eighth inch to one foot scale drawing, the one-eighth divisions on the ruler can be read off as feet. Architects normally use a scale ruler with different scales marked on each edge. A third method, used by builders in estimating, is to measure directly off the drawing and multiply by the scale factor.\n\nDimensions can be measured off drawings made on a stable medium such as vellum. All processes of reproduction introduce small errors, especially now that different copying methods mean that the same drawing may be re-copied, or copies made in several different ways. Consequently, dimensions need to be written (\"figured\") on the drawing. The disclaimer \"Do not scale off dimensions\" is commonly inscribed on architects drawings, to guard against errors arising in the copying process.\n\n\"This section deals with the conventional views used to represent a building or structure. See the Types of architectural drawing section below for drawings classified according to their purpose.\"\n\nA floor plan is the most fundamental architectural diagram, a view from above showing the arrangement of spaces in building in the same way as a map, but showing the arrangement at a particular level of a building. Technically it is a horizontal section cut through a building (conventionally at four feet / one metre and twenty centimetres above floor level), showing walls, windows and door openings and other features at that level. The plan view includes anything that could be seen below that level: the floor, stairs (but only up to the plan level), fittings and sometimes furniture. Objects above the plan level (e.g. beams overhead) can be indicated as dashed lines.\n\nGeometrically, plan view is defined as a vertical orthographic projection of an object on to a horizontal plane, with the horizontal plane cutting through the building.\n\nA site plan is a specific type of plan, showing the whole context of a building or group of buildings. A site plan shows property boundaries and means of access to the site, and nearby structures if they are relevant to the design. For a development on an urban site, the site plan may need to show adjoining streets to demonstrate how the design fits into the urban fabric. Within the site boundary, the site plan gives an overview of the entire scope of work. It shows the buildings (if any) already existing and those that are proposed, usually as a building footprint; roads, parking lots, footpaths, hard landscaping, trees and planting. For a construction project, the site plan also needs to show all the services connections: drainage and sewer lines, water supply, electrical and communications cables, exterior lighting etc.\n\nSite plans are commonly used to represent a building proposal prior to detailed design: drawing up a site plan is a tool for deciding both the site layout and the size and orientation of proposed new buildings. A site plan is used to verify that a proposal complies with local development codes, including restrictions on historical sites. In this context the site plan forms part of a legal agreement, and there may be a requirement for it to be drawn up by a licensed professional: architect, engineer, landscape architect or land surveyor.\n\nAn elevation is a view of a building seen from one side, a flat representation of one façade. This is the most common view used to describe the external appearance of a building. Each elevation is labelled in relation to the compass direction it faces, e.g. looking toward the north you would be seeing the southern elevation of the building. Buildings are rarely a simple rectangular shape in plan, so a typical elevation may show all the parts of the building that are seen from a particular direction.\n\nGeometrically, an elevation is a horizontal orthographic projection a building on to a vertical plane, the vertical plane normally being parallel to one side of the building.\n\nArchitects also use the word elevation as a synonym for façade, so the north elevation is literally the north-facing wall of the building.\n\nA cross section, also simply called a section, represents a vertical plane cut through the object, in the same way as a floor plan is a horizontal section viewed from the top. In the section view, everything cut by the section plane is shown as a bold line, often with a solid fill to show objects that are cut through, and anything seen beyond generally shown in a thinner line. Sections are used to describe the relationship between different levels of a building. In the Observatorium drawing illustrated here, the section shows the dome which can be seen from the outside, a second dome that can only be seen inside the building, and the way the space between the two accommodates a large astronomical telescope: relationships that would be difficult to understand from plans alone.\n\nA sectional elevation is a combination of a cross section, with elevations of other parts of the building seen beyond the section plane.\n\nGeometrically, a cross section is a horizontal orthographic projection of a building on to a vertical plane, with the vertical plane cutting through the building.\n\nIsometric and axonometric projections are a simple way of representing a three dimensional object, keeping the elements to scale and showing the relationship between several sides of the same object, so that the complexities of a shape can be clearly understood.\n\nThere is some confusion about the terms isometric and axonometric. “Axonometric is a word that has been used by architects for hundreds of years. Engineers use the word axonometric as a generic term to include isometric, diametric and trimetric drawings.” This article uses the terms in the architecture-specific sense.\n\nDespite fairly complex geometrical explanations, for the purposes of practical draughting the difference between isometric and axonometric is simple (see diagram above). In both, the plan is drawn on a skewed or rotated grid, and the verticals are projected vertically on the page. All lines are drawn to scale so that relationships between elements are accurate. In many cases a different scale is required for different axes, and again this can be calculated but in practice was often simply estimated by eye.\n\nTraditional draughting techniques used 30–60 and 45 degree set squares, and that determined the angles used in these views. Once the adjustable square became common those limitations were lifted.\n\nThe axonometric gained in popularity in the twentieth century, not just as a convenient diagram but as a formal presentation technique, adopted in particular by the Modern Movement. Axonometric drawings feature prominently in the influential 1970's drawings of Michael Graves, James Stirling and others, using not only straightforward views but worms-eye view, unusually and exaggerated rotations of the plan, and exploded elements.\n\nThe axonometric view is not readily generated by CAD programmes which create views from a three dimensional model. Consequently, it is now rarely used.\n\nDetail drawings show a small part of the construction at a larger scale, to show how the component parts fit together. They are also used to show small surface details, for example decorative elements. Section drawings at large scale are a standard way of showing building construction details, typically showing complex junctions (such as floor to wall junction, window openings, eaves and roof apex) that cannot be clearly shown on a drawing that includes the full height of the building. A full set of construction details needs to show plan details as well as vertical section details. One detail is seldom produced in isolation: a set of details shows the information needed to understand the construction in three dimensions. Typical scales for details are 1/10, 1/5 and full size.\n\nIn traditional construction, many details were so fully standardised, that few detail drawings were required to construct a building. For example, the construction of a sash window would be left to the carpenter, who would fully understand what was required, but unique decorative details of the facade would be drawn up in detail. In contrast, modern buildings need to be fully detailed because of the proliferation of different products, methods and possible solutions.\n\nPerspective in drawing is an approximate representation on a flat surface of an image as it is perceived by the eye. The key concepts here are:\n\nThe basic categorization of artificial perspective is by the number of vanishing points:\n\nThe normal convention in architectural perspective is to use two-point perspective, with all the verticals drawn as verticals on the page.\n\nThree-point perspective gives a casual, photographic snapshot effect. In professional architectural photography, conversely, a view camera or a perspective control lens is used to eliminate the third vanishing point, so that all the verticals are vertical on the photograph, as with the perspective convention. This can also be done by digital manipulation of a photograph taken with a standard lens.\n\nAerial perspective is a technique in painting, for indicating distance by approximating the effect of the atmosphere on distant objects. In daylight, as an ordinary object gets further from the eye, its contrast with the background is reduced, its colour saturation is reduced, and its colour becomes more blue. Not to be confused with aerial view or bird's eye view, which is the view as seen (or imagined) from a high vantage point. In J M Gandy's perspective of the Bank of England (see illustration at the beginning of this article), Gandy portrayed the building as a picturesque ruin in order to show the internal plan arrangement, a precursor of the cutaway view.\n\nA montage image is produced by superimposing a perspective image of a building on to a photographic background. Care is needed to record the position from which the photograph was taken, and to generate the perspective using the same viewpoint. This technique is popular in computer visualisation, where the building can be photorealistically rendered, and the final image is intended to be almost indistinguishable from a photograph.\n\nA sketch is a rapidly executed freehand drawing, a quick way to record and develop an idea, not intended as a finished work. A diagram could also be drawn freehand but deals with symbols, to develop the logic of a design. Both can be worked up into a more presentable form and used to communicate the principles of a design.\n\nIn architecture, the finished work is expensive and time consuming, so it is important to resolve the design as fully as possible before construction work begins. Complex modern buildings involve a large team of different specialist disciplines, and communication at the early design stages is essential to keep the design moving towards a coordinated outcome. Architects (and other designers) start investigating a new design with sketches and diagrams, to develop a rough design that provides an adequate response to the particular design problems.\n\nThere are two basic elements to a building design, the aesthetic and the practical. The aesthetic element includes the layout and visual appearance, the anticipated feel of the materials, and cultural references that will influence the way people perceive the building. Practical concerns include space allocated for different activities, how people enter and move around the building, daylight and artificial lighting, acoustics, traffic noise, legal matters and building codes, and many other issues. While both aspects are partly a matter of customary practice, every site is different. Many architects actively seek innovation, thereby increasing the number of problems to be resolved.\n\nArchitectural legend often refers to designs made on the back of an envelope/napkin/cigarette packet/bubblegum wrapper. Initial thoughts are important, even if they have to be discarded along the way, because they provide the central idea around which the design can develop. Although a sketch is inaccurate, it is disposable and allows for freedom of thought, for trying different ideas quickly. Choice becomes sharply reduced once the design is committed to a scale drawing, and the sketch stage is almost always essential.\n\nDiagrams are mainly used to resolve practical matters. In the early phases of the design architects use diagrams to develop, explore, and communicate ideas and solutions. They are essential tools for thinking, problem solving, and communication in the design disciplines. Diagrams can be used to resolve spatial relationships, but they can also represent forces and flows, e.g. the forces of sun and wind, or the flows of people and materials through a building.\n\nAn exploded view diagram shows component parts dis-assembled in some way, so that each can be seen on its own. These views are common in technical manuals, but are also used in architecture, either in conceptual diagrams or to illustrate technical details. In a cutaway view parts of the exterior are omitted to show the interior, or details of internal construction. Although common in technical illustration, including many building products and systems, the cutaway is in fact little-used in architectural drawing.\n\nArchitectural drawings are produced for a specific purpose, and can be classified accordingly. Several elements are often included on the same sheet, for example a sheet showing a plan together with the principal façade.\n\nDrawings intended to explain a scheme and to promote its merits. Working drawings may include tones or hatches to emphasise different materials, but they are diagrams, not intended to appear realistic. Basic presentation drawings typically include people, vehicles and trees, taken from a library of such images, and are otherwise very similar in style to working drawings. Rendering is the art of adding surface textures and shadows to show the visual qualities of a building more realistically. An architectural illustrator or graphic designer may be employed to prepare specialist presentation images, usually perspectives or highly finished site plans, floor plans and elevations etc.\n\nMeasured drawings of existing land, structures and buildings. Architects need an accurate set of survey drawings as a basis for their working drawings, to establish exact dimensions for the construction work. Surveys are usually measured and drawn up by specialist land surveyors.\n\nHistorically, architects have made record drawings in order to understand and emulate the great architecture known to them. In the Renaissance, architects from all over Europe studied and recorded the remains of the Roman and Greek civilizations, and used these influences to develop the architecture of the period. Records are made both individually, for local purposes, and on a large scale for publication. Historic surveys worth referring to include:\n\nRecord drawings are also used in construction projects, where \"as-built\" drawings of the completed building take account of all the variations made during the course of construction.\n\nA comprehensive set of drawings used in a building construction project: these will include not only architect's drawings but structural and services engineer's drawings etc. Working drawings logically subdivide into location, assembly and component drawings.\nTraditionally, working drawings would typically combine plans, sections, elevations and some details to provide a complete explanation of a building on one sheet. That was possible because little detail was included, the building techniques involved being common knowledge amongst building professionals. Modern working drawings are much more detailed and it is standard practice to isolate each view on a separate sheet. Notes included on drawings are brief, referring to standardised specification documents for more information. Understanding the layout and construction of a modern building involves studying an often-sizeable set of drawings and documents.\n\nUntil the latter part of the 20th century, all architectural drawings were manually produced, if not by the architects, then by trained (but less skilled) draughtsmen (or drafters), who did not generate the design, but did make many of the less important decisions. This system has continued with CAD draughting: many design architects have little or no knowledge of CAD software programmes, relying upon others to take their designs beyond the sketch stage. Draughtsmen often specialize in a type of structure, such as residential or commercial, or in a type of construction: timber frame, reinforced concrete, prefabrication, etc.\n\nThe traditional tools of the architect were the drawing board or draughting table, T-square and set squares, protractor, compasses, pencil, and drawing pens of different types. Drawings were made on vellum, coated linen, and tracing paper. Lettering would either be done by hand, mechanically using a stencil, or a combination of the two. Ink lines were drawn with a ruling pen, a relatively sophisticated device similar to a dip-in pen, but with adjustable line width, capable of producing a very fine controlled line width. Ink pens had to be dipped into ink frequently. Draughtsmen worked standing up, keeping the ink on a separate table to avoid spilling ink on the drawing.\n\nDevelopments in the 20th century included the parallel motion drawing board, as well as more complex improvements on the basic T-square. The development of reliable technical drawing pens allowed for faster draughting and stencilled lettering. Letraset dry transfer lettering and half-tone sheets were popular from the 1970s until computers made those processes obsolete.\n\nComputer-aided design is the use of computer software to create drawings. Today the vast majority of technical drawings of all kinds are made using CAD. Instead of drawing lines on paper, the computer records equivalent information electronically. There are many advantages to this system: repetition is reduced because complex elements can be copied, duplicated and stored for re-use. Errors can be deleted, and the speed of draughting allows many permutations to be tried before the design is finalised. On the other hand, CAD drawing encourages a proliferation of detail and increased expectations of accuracy, aspects which reduce the efficiency originally expected from the move to computerisation.\n\nProfessional CAD software such as AutoCAD is complex and requires both training and experience before the operator becomes fully productive. Consequently, skilled CAD operators are often divorced from the design process. Simpler software such as SketchUp and Vectorworks allows for more intuitive drawing and is intended as a design tool.\n\nCAD is used to create all kinds of drawings, from working drawings to photorealistic perspective views. Architectural renderings (also called visualisations) are made by creating a three-dimensional model using CAD. The model can be viewed from any direction to find the most useful viewpoints. Different software (for example Autodesk 3ds Max) is then used to apply colour and texture to surfaces, and to represent shadows and reflections. The result can be accurately combined with photographic elements: people, cars, background landscape.\n\nBuilding information modeling (BIM) is the logical development of CAD drawing, a relatively new technology but fast becoming mainstream. The design team collaborates to create a three-dimensional computer model, and all plans and other two-dimensional views are generated directly from the model, ensuring spatial consistency. The key innovation here is to share the model via the internet, so that all the design functions (site survey, architecture, structure and services) can be integrated into a single model, or as a series of models associated with each specialism that are shared throughout the design development process. Some form of management, not necessarily by the architect, needs to be in place to resolve conflicting priorities. The starting point of BIM is spatial design, but it also enables components to be quantified and scheduled directly from the information embedded in the model.\n\nAn architectural animation is a short film showing how a proposed building will look: the moving image makes three-dimensional forms much easier to understand. An animation is generated from a series of hundreds or even thousands of still images, each made in the same way as an architectural visualisation. A computer-generated building is created using a CAD programme, and that is used to create more or less realistic views from a sequence of viewpoints. The simplest animations use a moving viewpoint, while more complex animations can include moving objects: people, vehicles, and so on.\n\nReprographics or reprography covers a variety of technologies, media, and support services used to make multiple copies of original drawings. Prints of architectural drawings are still sometimes called blueprints, after one of the early processes which produced a white line on blue paper. The process was superseded by the dye-line print system which prints black on white coated paper (Whiteprint). The standard modern processes are the ink-jet printer, laser printer and photocopier, of which the ink-jet and laser printers are commonly used for large-format printing. Although colour printing is now commonplace, it remains expensive above A3 size, and architect's working drawings still tend to adhere to the black and white / greyscale aesthetic.\n"}
{"id": "2021", "url": "https://en.wikipedia.org/wiki?curid=2021", "title": "Atle Selberg", "text": "Atle Selberg\n\nAtle Selberg (14 June 1917 – 6 August 2007) was a Norwegian mathematician known for his work in analytic number theory, and in the theory of automorphic forms, in particular bringing them into relation with spectral theory. He was awarded the Fields Medal in 1950.\n\nSelberg was born in Langesund, Norway, the son of teacher Anna Kristina Selberg and mathematician Ole Michael Ludvigsen Selberg. Two of his brothers also went on to become mathematicians as well, and the remaining one became a professor of engineering. \n\nWhile he was still at school he was influenced by the work of Srinivasa Ramanujan and he found an exact analytical formula for the partition function as suggested by the works of Ramanujan; however, this result was first published by Hans Rademacher. During the war he fought against the German invasion of Norway, and was imprisoned several times. \nHe studied at the University of Oslo and completed his Ph.D. in 1943.\n\nDuring World War II, Selberg worked in isolation due to the German occupation of Norway. After the war his accomplishments became known, including a proof that a positive proportion of the zeros of the Riemann zeta function lie on the line formula_1. \nAfter the war, he turned to sieve theory, a previously neglected topic which Selberg's work brought into prominence. In a 1947 paper he introduced the Selberg sieve, a method well adapted in particular to providing auxiliary upper bounds, and which contributed to Chen's theorem, among other important results.\n\nIn 1948 Selberg submitted two papers in \"Annals of Mathematics\" in which he proved by elementary means the theorems for primes in arithmetic progression and the density of primes. This challenged the widely held view of his time that certain theorems are only obtainable with the advanced methods of complex analysis. Both results were based on his work on the asymptotic formula\nwhere\nfor primes formula_4. He established this result by elementary means in March 1948, and by July of that year, Selberg and Paul Erdős each obtained elementary proofs of the prime number theorem, both using the asymptotic formula above as a starting point. Circumstances leading up to the proofs, as well as publication disagreements, led to a bitter dispute between the two mathematicians.\n\nFor his fundamental accomplishments during the 1940s, Selberg received the 1950 Fields Medal.\n\nSelberg moved to the United States and settled at the Institute for Advanced Study in Princeton, New Jersey in the 1950s where he remained until his death. During the 1950s he worked on introducing spectral theory into number theory, culminating in his development of the Selberg trace formula, the most famous and influential of his results. In its simplest form, this establishes a duality between the lengths of closed geodesics on a compact Riemann surface and the eigenvalues of the Laplacian, which is analogous to the duality between the prime numbers and the zeros of the zeta function.\n\nHe was awarded the 1986 Wolf Prize in Mathematics. He was also awarded an honorary Abel Prize in 2002, its founding year, before the awarding of the regular prizes began.\n\nSelberg received many distinctions for his work in addition to the Fields Medal, the Wolf Prize and the Gunnerus Medal. He was elected to the Norwegian Academy of Science and Letters, the Royal Danish Academy of Sciences and Letters and the American Academy of Arts and Sciences.\n\nIn 1972 he was awarded an honorary degree, doctor philos. honoris causa, at the Norwegian Institute of Technology, later part of Norwegian University of Science and Technology.\n\nSelberg had two children, Ingrid Selberg and Lars Selberg. Ingrid Selberg is married to playwright Mustapha Matura.\n\nHe died at home in Princeton on 6 August 2007 of heart failure.\n\n\n\n"}
{"id": "15483838", "url": "https://en.wikipedia.org/wiki?curid=15483838", "title": "Borel determinacy theorem", "text": "Borel determinacy theorem\n\nIn descriptive set theory, the Borel determinacy theorem states that any Gale–Stewart game whose payoff set is a Borel set is determined, meaning that one of the two players will have a winning strategy for the game.\n\nThe theory was proved by Donald A. Martin in 1975, and is being applied in descriptive set theory to show that Borel sets in Polish spaces have regularity properties such as the perfect set property and the property of Baire.\n\nThe theorem is also known for its metamathematical properties. In 1971, before the theorem was proved, Harvey Friedman showed that any proof of the theorem in Zermelo–Fraenkel set theory must make repeated use of the axiom of replacement. Later results showed that stronger determinacy theorems cannot be proven in Zermelo–Fraenkel set theory, although they are relatively consistent with it, if certain large cardinals are consistent.\n\nA Gale–Stewart game is a two-player game of perfect information. The game is defined using a set \"A\", and is denoted \"G\". The two players alternate turns, and each player is aware of all moves before making the next one. On each turn, each player chooses a single element of \"A\" to play. The same element may be chosen more than once without restriction. The game can be visualized through the following diagram, in which the moves are made from left to right, with the moves of player I above and the moves of player II below.\nformula_1\nThe play continues without end, so that a single play of the game determines an infinite sequence formula_2 of elements of \"A\". The set of all such sequences is denoted \"A\". The players are aware, from the beginning of the game, of a fixed payoff set (a.k.a. \"winning set\") that will determine who wins. The payoff set is a subset of \"A\". If the infinite sequence created by a play of the game is in the payoff set, then player I wins. Otherwise, player II wins; there are no ties.\n\nThis definition initially does not seem to include traditional perfect information games such as chess, since the set of moves available in such games changes every turn. However, this sort of case can be handled by declaring that a player who makes an illegal move loses immediately, so that the Gale-Stewart notion of a game does in fact generalize the concept of a game defined by a game tree.\n\nA winning strategy for a player is a function that tells the player what move to make from any position in the game, such that if the player follows the function he or she will surely win. More specifically, a winning strategy for player I is a function \"f\" that takes as input sequences of elements of A of even length and returns an element of \"A\", such that player I will win every play of the form\n\nformula_3\n\nA winning strategy for player II is a function \"g\" that takes odd-length sequences of elements of \"A\" and returns elements of \"A\", such that player II will win every play of the form\n\nformula_4\nAt most one player can have a winning strategy; if both players had winning strategies, and played the strategies against each other, only one of the two strategies could win that play of the game. If one of the players has a winning strategy for a particular payoff set, that payoff set is said to be determined.\n\nFor a given set \"A\", whether a subset of \"A\" will be determined depends to some extent on its topological structure. For the purposes of Gale–Stewart games, the set \"A\" is endowed with the discrete topology, and \"A\" endowed with the resulting product topology, where \"A\" is viewed as a countably infinite topological product of \"A\" with itself. In particular, when \"A\" is the set {0,1}, the topology defined on \"A\" is exactly the ordinary topology on Cantor space, and when \"A\" is the set of natural numbers, it is the ordinary topology on Baire space.\n\nThe set \"A\" can be viewed as the set of paths through a certain tree, which leads to a second characterization of its topology. The tree consists of all finite sequences of elements of \"A\", and the children of a particular node σ of the tree are exactly the sequences that extend σ by one element. Thus if \"A\" = { 0, 1 }, the first level of the tree consists of the sequences 〈 0 〉 and 〈 1 〉; the second level consists of the four sequences 〈 0, 0 〉, 〈 0, 1 〉, 〈 1, 0 〉, 〈 1, 1 〉; and so on. For each of the finite sequences σ in the tree, the set of all elements of \"A\" that begin with σ is a basic open set in the topology on \"A\". The open sets of \"A\" are precisely the sets expressible as unions of these basic open sets. The closed sets, as usual, are those whose complement is open.\n\nThe Borel sets of \"A\" are the smallest class of subsets of \"A\" that includes the open sets and is closed under complement and countable union. That is, the Borel sets are the smallest σ-algebra of subsets of \"A\" containing all the open sets. The Borel sets are classified in the Borel hierarchy based on how many times the operations of complement and countable union are required to produce them from open sets.\n\nGale and Stewart (1953) proved that if the payoff set is an open or closed subset of \"A\" then the Gale–Stewart game with that payoff set is always determined. Over the next twenty years, this was extended to slightly higher levels of the Borel hierarchy through ever more complicated proofs. This led to the question of whether the game must be determined whenever the payoff set is a Borel subset of \"A\". It was known that, using the axiom of choice, it is possible to construct a subset of {0,1} that is not determined (Kechris 1995, p. 139).\n\nHarvey Friedman (1971) proved that any proof that all Borel subsets of Cantor space ({0,1} ) were determined would require repeated use of the axiom of replacement, an axiom not typically required to prove theorems about \"small\" objects such as Cantor space.\n\nDonald A. Martin (1975) proved that for any set \"A\", all Borel subsets of \"A\" are determined. Because the original proof was quite complicated, Martin published a shorter proof in 1982 that did not require as much technical machinery. In his review of Martin's paper, Drake describes the second proof as \"surprisingly straightforward.\"\n\nThe field of descriptive set theory studies properties of Polish spaces (essentially, complete separable metric spaces). The Borel determinacy theorem has been used to establish many properties of Borel subsets of these spaces. For example, all Borel subsets of Polish spaces have the perfect set property and the property of Baire.\n\nThe Borel determinacy theorem is of interest for its metamethematical properties as well as its consequences in descriptive set theory.\n\nDeterminacy of closed sets of \"A\" for arbitrary \"A\" is equivalent to the axiom of choice over ZF (Kechris 1995, p. 139). When working in set-theoretical systems where the axiom of choice is not assumed, this can be circumvented by considering generalized strategies known as quasistrategies (Kechris 1995, p. 139) or by only considering games where \"A\" is the set of natural numbers, as in the axiom of determinacy.\n\nZermelo set theory (Z) is Zermelo–Fraenkel set theory without the axiom of replacement. It differs from ZF in that Z does not prove that the power set operation can be iterated uncountably many times beginning with an arbitrary set. In particular, \"V\", a particular countable level of the cumulative hierarchy, is a model of Zermelo set theory. The axiom of replacement, on the other hand, is only satisfied by \"V\" for significantly larger values of κ, such as when κ is a strongly inaccessible cardinal. Friedman's theorem of 1971 showed that there is a model of Zermelo set theory (with the axiom of choice) in which Borel determinacy fails, and thus Zermelo set theory cannot prove the Borel determinacy theorem.\n\nSeveral set-theoretic principles about determinacy stronger than Borel determinacy are studied in descriptive set theory. They are closely related to large cardinal axioms.\n\nThe axiom of projective determinacy states that all projective subsets of a Polish space are determined. It is known to be unprovable in ZFC but relatively consistent with it and implied by certain large cardinal axioms. The existence of a measurable cardinal is enough to imply over ZFC that all analytic subsets of Polish spaces are determined.\n\nThe axiom of determinacy states that all subsets of all Polish spaces are determined. It is inconsistent with ZFC but in ZF + DC (Zermelo-Fraenkel set theory plus the axiom of dependent choice) it is equiconsistent with certain large cardinal axioms.\n\n\n"}
{"id": "224300", "url": "https://en.wikipedia.org/wiki?curid=224300", "title": "Bowen ratio", "text": "Bowen ratio\n\nIn meteorology and hydrology, the Bowen ratio is used to describe the type of heat transfer for a surface that has moisture. Heat transfer can either occur as sensible heat (differences in temperature without evapotranspiration) or latent heat (the energy required during a change of state, without a change in temperature). The Bowen ratio is the mathematical method generally used to calculate heat lost (or gained) in a substance; it is the ratio of energy fluxes from one state to another by sensible heat and latent heating respectively. It is calculated by the equation: \n\nwhere formula_2 is sensible heating and formula_3 is latent heating. The quantity was named by Harald Sverdrup after Ira Sprague Bowen (1898–1973), an astrophysicist whose theoretical work on evaporation to air from water bodies made first use of it, and it is used most commonly in meteorology and hydrology. In this context, when the magnitude of formula_4 is less than one, a greater proportion of the available energy at the surface is passed to the atmosphere as latent heat than as sensible heat, and the converse is true for values of formula_4 greater than one. As formula_6, however, formula_4 becomes unbounded making the Bowen ratio a poor choice of variable for use in formulae, especially for arid surfaces. For this reason the evaporative fraction is sometimes a more appropriate choice of variable representing the relative contributions of the turbulent energy fluxes to the surface energy budget.\n\nThe Bowen ratio is related to the evaporative fraction, formula_8, through the equation,\n\nThe Bowen ratio is an indicator of the type of surface. The Bowen ratio, formula_10, is less than one over surfaces with abundant water supplies.\n\n\n\n"}
{"id": "12784536", "url": "https://en.wikipedia.org/wiki?curid=12784536", "title": "Buffon's noodle", "text": "Buffon's noodle\n\nIn geometric probability, the problem of Buffon's noodle is a variation on the well-known problem of Buffon's needle, named after Georges-Louis Leclerc, Comte de Buffon who lived in the 18th century. That problem solved by Buffon was the earliest geometric probability problem to be solved.\n\nSuppose there exist an infinite number of equally spaced parallel lines, and we were to randomly toss a needle whose length is less than or equal to the distance between adjacent lines. What is the probability that the needle will cross a line? The formula is formula_1, where \"D\" is the distance between two adjacent lines, and \"L\" is the length of the needle.\n\nThe interesting thing about the formula is that it stays the same even when you bend the needle in any way you want (subject to the constraint that it must lie in a plane), making it a \"noodle\"—a rigid plane curve. We drop the assumption that the length of the noodle is no more than the distance between the parallel lines.\n\nThe probability distribution of the number of crossings depends on the shape of the noodle, but the expected number of crossings does not; it depends only on the length \"L\" of the noodle and the distance \"D\" between the parallel lines (observe that a curved noodle may cross a single line multiple times).\n\nThis fact may be proved as follows (see Klain and Rota). First suppose the noodle is piecewise linear, i.e. consists of \"n\" straight pieces. Let \"X\" be the number of times the \"i\"th piece crosses one of the parallel lines. These random variables are not independent, but the expectations are still additive due to the linearity of expectation:\n\nRegarding a curved noodle as the limit of a sequence of piecewise linear noodles, we conclude that the expected number of crossings per toss is proportional to the length; it is some constant times the length \"L\". Then the problem is to find the constant. In case the noodle is a circle of diameter equal to the distance \"D\" between the parallel lines, then \"L\" = π\"D\" and the number of crossings is exactly 2, with probability 1. So when \"L\" = π\"D\" then the expected number of crossings is 2. Therefore, the expected number of crossings must be 2\"L\"/(π\"D\").\n\nThere is one more surprising consequence. In case the noodle is any closed curve of constant width D the number of crossings is also exactly 2. This implies Barbier's theorem asserting that the perimeter is the same as that of a circle.\n\n\n"}
{"id": "10605858", "url": "https://en.wikipedia.org/wiki?curid=10605858", "title": "CDMF", "text": "CDMF\n\nIn cryptography, CDMF (Commercial Data Masking Facility) is an algorithm developed at IBM in 1992 to reduce the security strength of the 56-bit DES cipher to that of 40-bit encryption, at the time a requirement of U.S. restrictions on export of cryptography. Rather than a separate cipher from DES, CDMF constitutes a key generation algorithm, called \"key shortening\". It is one of the cryptographic algorithms supported by S-HTTP.\n\nLike DES, CDMF accepts a 64-bit input key, but not all bits are used.\nThe algorithm consists of the following steps:\n\n\nThe resulting 64-bit data is to be used as a DES key. Due to step 3, a brute force attack needs to test only 2 possible keys.\n\n"}
{"id": "24758132", "url": "https://en.wikipedia.org/wiki?curid=24758132", "title": "Constant (mathematics)", "text": "Constant (mathematics)\n\nIn mathematics, the adjective constant means non-varying. The noun constant may have two different meanings. It may refer to a fixed and well-defined number or other mathematical object. The term mathematical constant (and also physical constant) is sometimes used to distinguish this meaning from the other one. A constant may also refer to a constant function or its value (it is a common usage to identify them). Such a constant is commonly represented by a variable which does not depend on the main variable(s) of the studied problem. This is the case, for example, for a constant of integration which is an arbitrary constant function (not depending on the variable of integration) added to a particular antiderivative to get all the antiderivatives of the given function.\n\nFor example, a general quadratic function is commonly written as:\n\nwhere \"a\", \"b\" and \"c\" are constants (or parameters), while \"x\" is the variable, a placeholder for the argument of the function being studied. A more explicit way to denote this function is\n\nwhich makes the function-argument status of \"x\" clear, and thereby implicitly the constant status of \"a\", \"b\" and \"c\". In this example \"a\", \"b\" and \"c\" are coefficients of the polynomial. Since \"c\" occurs in a term that does not involve \"x\", it is called the constant term of the polynomial and can be thought of as the coefficient of \"x\"; any polynomial term or expression of degree zero is a constant.\n\nA constant may be used to define a constant function that ignores its arguments and always gives the same value. A constant function of a single variable, such as formula_3, has a graph that is a horizontal straight line, parallel to the \"x\"-axis. Such a function always takes the same value (in this case, 5) because its argument does not appear in the expression defining the function.\n\nThe context-dependent nature of the concept of \"constant\" can be seen in this example from elementary calculus:\n\n\"Constant\" means not depending on some variable; not changing as that variable changes. In the first case above, it means not depending on \"h\"; in the second, it means not depending on \"x\".\n\nSome values occur frequently in mathematics and are conventionally denoted by a specific symbol. These standard symbols and their values are called mathematical constants. Examples include:\n\nIn calculus, constants are treated in several different ways depending on the operation. For example, the derivative of a constant function is zero. This is because the derivative measures the rate of change of a function with respect to a variable, and since constants, by definition, do not change, their derivative is therefore zero. Conversely, when integrating a constant function, the constant is multiplied by the variable of integration. During the evaluation of a limit, the constant remains the same as it was before and after evaluation.\n\nIntegration of a function of one variable often involves a constant of integration. This arises because of the integral operator's nature as the inverse of the differential operator, meaning the aim of integration is to recover the original function before differentiation. The differential of a constant function is zero, as noted above, and the differential operator is a linear operator, so functions that only differ by a constant term have the same derivative. To acknowledge this, a constant of integration is added to an indefinite integral; this ensures that all possible solutions are included. The constant of integration is generally written as 'c' and represents a constant with a fixed but undefined value.\n\nIf is the constant function such that formula_7 for every then\n\n"}
{"id": "9999200", "url": "https://en.wikipedia.org/wiki?curid=9999200", "title": "Content (measure theory)", "text": "Content (measure theory)\n\nIn mathematics, a content is a set function like a measure but a content need not be countably additive, but must only be finitely additive. A content is a real function formula_1 defined on a field of sets formula_2 such that\n\nAn example of a content is a measure, which is a \"σ\"-additive \"content\" defined on a \"σ\"-field. Every (real-valued) measure is a content, but not vice versa. Contents give a good notion of integrating bounded functions on a space but can behave badly when integrating unbounded functions, while measures give a good notion of integrating unbounded functions.\n\nAn example of a content that is not a measure on a σ-algebra is the content on all subsets of the positive integers that has value 1/2 on any integer \"n\" and is infinite on any infinite subset.\n\nAn example of a content on the positive integers that is always finite but is not a measure can be given as follows. Take a positive linear functional on the bounded sequences that is 0 if the sequence has only a finite number of nonzero elements and takes value 1 on the sequence 1, 1, 1, ..., so the functional in some sense gives an \"average value\" of any bounded sequence. (Such a functional cannot be constructed explicitly, but exists by the Hahn–Banach theorem.) Then the content of a set of positive integers is the average value of the sequence that is 1 on this set and 0 elsewhere. Informally, one can think of the content of a subset of integers as the \"chance\" that a randomly chosen integer lies in this subset (though this is not compatible with the usual definitions of chance in probability theory, which assume countable additivity).\n\nIn general integration of functions with respect to a content does not behave well. However there is a well-behaved notion of integration provided that the function is bounded and the total content of the space is finite, given as follows.\n\nSuppose that the total content of a space is finite. \nIf \"f\" is a bounded function on the space such that the inverse image of any open subset of the reals has a content, then we can define the integral of \"f\" with respect to the content as\nwhere the \"A\" form a finite collections of disjoint half-open sets whose union covers the range of \"f\", and α is any element of \"A\", and where the limit is taken as the diameters of the sets \"A\" tend to 0.\n\nSuppose that μ is a measure on some space \"X\". The bounded measurable functions on \"X\" form a Banach space with respect to the supremum norm. The positive elements of the dual of this space correspond to bounded contents λ \"Χ\", with the value of λ on \"f\" given by the integral formula_7. Similarly one can form the space of essentially bounded functions, with the norm given by the essential supremum, and the positive elements of the dual of this space are given by bounded contents that vanish on sets of measure 0.\n\nThere are several ways to construct a measure μ from a content λ on a topological space. This section gives one such method for locally compact Hausdorff spaces such that the content is defined on all compact subsets. In general the measure is not an extension of the content, as the content may fail to be countably additive, and the measure may even be identically zero even if the content is not.\n\nFirst restrict the content to compact sets. This gives a function λ of compact sets \"C\" with the following properties:\n\nThere are also examples of functions λ as above not constructed from contents. \nAn example is given by the construction of Haar measure on a locally compact group. One method of constructing such a Haar measure is to produce a left-invariant function λ as above on the compact subsets of the group, which can then be extended to a left-invariant measure.\n\nGiven λ as above, we define a function μ on all open sets by \nThis has the following properties: \n\nGiven μ as above, we extend the function μ to all subsets of the topological space by \nThis is an outer measure, in other words it has the following properties: \n\nThe function μ above is an outer measure on the family of all subsets. Therefore it becomes a measure when restricted to the measurable subsets for the outer measure, which are the subsets \"E\" such that μ(\"X\") = μ(\"X\"∩\"E\") + μ(\"X\"\\\"E\") for all subsets \"X\". If the space is locally compact then every open set is measurable for this measure.\n\nThe measure μ does not necessarily coincide with the content λ on compact sets, However it does if λ is regular in the sense that \nfor any compact \"C\", λ(\"C\") is the inf of λ(\"D\") for compact sets \"D\" containing \"C\" in their interiors.\n"}
{"id": "10921962", "url": "https://en.wikipedia.org/wiki?curid=10921962", "title": "Cytoscape", "text": "Cytoscape\n\nCytoscape is an open source bioinformatics software platform for visualizing molecular interaction networks and integrating with gene expression profiles and other state data. Additional features are available as plugins. Plugins are available for network and molecular profiling analyses, new layouts, additional file format support and connection with databases and searching in large networks. Plugins may be developed using the Cytoscape open Java software architecture by anyone and plugin community development is encouraged. Cytoscape also has a JavaScript-centric sister project named Cytoscape.js that can be used to analyse and visualise graphs in JavaScript environments, like a browser.\n\nCytoscape was originally created at the Institute of Systems Biology in Seattle in 2002. Now, it is developed by an international consortium of open source developers. Cytoscape was initially made public in July, 2002 (v0.8); the second release (v0.9) was in November, 2002, and v1.0 was released in March 2003. Version 1.1.1 is the last stable release for the 1.0 series. Version 2.0 was initially released in 2004; Cytoscape 2.83, the final 2.xx version, was released in May 2012. Version 3.0 was released Feb 1, 2013, and the latest version, 3.4.0, was released in May 2016.\n\nThe Cytoscape core developer team continues to work on this project and released Cytoscape 3.0 in 2013. This represented a major change in the Cytoscape architecture; it is a more modularized, expandable and maintainable version of the software.\n\nWhile Cytoscape is most commonly used for biological research applications, it is agnostic in terms of usage. Cytoscape can be used to visualize and analyze network graphs of any kind involving nodes and edges (e.g., social networks). A key aspect of the software architecture of Cytoscape is the use of plugins for specialized features. Plugins are developed by core developers and the greater user community.\n\nInput\n\nVisualization\n\nAnalysis\n\n\n"}
{"id": "293511", "url": "https://en.wikipedia.org/wiki?curid=293511", "title": "D'Alembert operator", "text": "D'Alembert operator\n\nIn special relativity, electromagnetism and wave theory, the d'Alembert operator (denoted by a box: formula_1), also called the d'Alembertian, wave operator, or box operator is the Laplace operator of Minkowski space. The operator is named after French mathematician and physicist Jean le Rond d'Alembert.\n\nIn Minkowski space, in standard coordinates , it has the form\n\nHere ∇ is the 3-dimensional Laplacian and is the inverse Minkowski metric with \nNote that the and summation indices range from 0 to 3: see Einstein notation. We have assumed units such that the speed of light = 1.\n\nSome authors also use the negative metric signature of , with formula_7.\n\nLorentz transformations leave the Minkowski metric invariant, so the d'Alembertian yields a Lorentz scalar. The above coordinate expressions remain valid for the standard coordinates in every inertial frame.\n\nThere are a variety of notations for the d'Alembertian. The most common is the symbol formula_1 (Unicode: ): the four sides of the box representing the four dimensions of space-time and the formula_9 which emphasizes the scalar property through the squared term (much like the Laplacian). This symbol is sometimes called the quabla (\"cf\". nabla symbol). In keeping with the triangular notation for the Laplacian, sometimes formula_10 is used.\n\nAnother way to write the d'Alembertian in flat standard coordinates is formula_11. This notation is used extensively in quantum field theory, where partial derivatives are usually indexed, so the lack of an index with the squared partial derivative signals the presence of the d'Alembertian.\n\nSometimes formula_1 is used to represent the four-dimensional Levi-Civita covariant derivative. The symbol formula_13 is then used to represent the space derivatives, but this is coordinate chart dependent.\n\nThe wave equation for small vibrations is of the form\nwhere is the displacement.\n\nThe wave equation for the electromagnetic field in vacuum is\nwhere is the electromagnetic four-potential.\n\nThe Klein–Gordon equation has the form\n\nThe Green's function, formula_17, for the d'Alembertian is defined by the equation\n\nwhere formula_19 is the multidimensional Dirac delta function and formula_20 and formula_21 are two points in Minkowski space.\n\nA special solution is given by the \"retarded Green's function\" which corresponds to signal propagation only forward in time\n\nwhere formula_23 is the Heaviside step function.\n\n\n"}
{"id": "36837348", "url": "https://en.wikipedia.org/wiki?curid=36837348", "title": "David Conlon", "text": "David Conlon\n\nDavid Conlon (born 1982) is an Irish mathematician. He represented Ireland in the International Mathematical Olympiad in 1998 and 1999. He was an undergraduate in Trinity College Dublin, where he was elected a Scholar in 2001 and graduated in 2003. He earned a Ph.D. from Cambridge University in 2009. He is a fellow of Wadham College, Oxford and is a Professor of Discrete Mathematics in the Mathematics Institute at the University of Oxford. His research interests are in Hungarian-style combinatorics, particularly Ramsey theory, extremal graph theory, combinatorial number theory, and probabilistic methods in combinatorics. \n\nConlon has worked in Ramsey theory. In particular, he proved the first superpolynomial improvement on the Erdős–Szekeres bound on diagonal Ramsey numbers. \n\nHe won the European Prize in Combinatorics in 2011, for his work in Ramsey theory and for his progress on Sidorenko's conjecture that, for any bipartite graph \"H\", random bipartite graphs have the fewest subgraphs isomorphic to \"H\".\n\n"}
{"id": "27545816", "url": "https://en.wikipedia.org/wiki?curid=27545816", "title": "Degeneracy (graph theory)", "text": "Degeneracy (graph theory)\n\nIn graph theory, a \"k\"-degenerate graph is an undirected graph in which every subgraph has a vertex of degree at most \"k\": that is, some vertex in the subgraph touches \"k\" or fewer of the subgraph's edges. The degeneracy of a graph is the smallest value of \"k\" for which it is \"k\"-degenerate. The degeneracy of a graph is a measure of how sparse it is, and is within a constant factor of other sparsity measures such as the arboricity of a graph.\n\nDegeneracy is also known as the k\"-core number, width, and linkage, and is essentially the same as the coloring number or Szekeres-Wilf number (named after ). \"k\"-degenerate graphs have also been called k\"-inductive graphs. The degeneracy of a graph may be computed in linear time by an algorithm that repeatedly removes minimum-degree vertices. The connected components that are left after all vertices of degree less than \"k\" have been removed are called the \"k\"-cores of the graph and the degeneracy of a graph is the largest value \"k\" such that it has a \"k\"-core.\n\nEvery finite forest has either an isolated vertex (incident to no edges) or a leaf vertex (incident to exactly one edge); therefore, trees and forests are 1-degenerate graphs. Every 1-degenerate graph is a forest.\n\nEvery finite planar graph has a vertex of degree five or less; therefore, every planar graph is 5-degenerate, and the degeneracy of any planar graph is at most five. Similarly, every outerplanar graph has degeneracy at most two, and the Apollonian networks have degeneracy three.\n\nThe Barabási–Albert model for generating random scale-free networks is parameterized by a number \"m\" such that each vertex that is added to the graph has \"m\" previously-added vertices. It follows that any subgraph of a network formed in this way has a vertex of degree at most \"m\" (the last vertex in the subgraph to have been added to the graph) and Barabási–Albert networks are automatically \"m\"-degenerate.\n\nEvery \"k\"-regular graph has degeneracy exactly \"k\". More strongly, the degeneracy of a graph equals its maximum vertex degree if and only if at least one of the connected components of the graph is regular of maximum degree. For all other graphs, the degeneracy is strictly less than the maximum degree.\n\nThe coloring number of a graph \"G\" was defined by to be the least κ for which there exists an ordering of the vertices of \"G\" in which each vertex has fewer than κ neighbors that are earlier in the ordering. It should be distinguished from the chromatic number of \"G\", the minimum number of colors needed to color the vertices so that no two adjacent vertices have the same color; the ordering which determines the coloring number provides an order to color the vertices of G with the coloring number, but in general the chromatic number may be smaller.\n\nThe degeneracy of a graph \"G\" was defined by as the least \"k\" such that every induced subgraph of \"G\" contains a vertex with \"k\" or fewer neighbors. The definition would be the same if arbitrary subgraphs are allowed in place of induced subgraphs, as a non-induced subgraph can only have vertex degrees that are smaller than or equal to the vertex degrees in the subgraph induced by the same vertex set.\n\nThe two concepts of coloring number and degeneracy are equivalent: in any finite graph the degeneracy is just one less than the coloring number. For, if a graph has an ordering with coloring number κ then in each subgraph \"H\" the vertex that belongs to \"H\" and is last in the ordering has at most κ − 1 neighbors in \"H\". In the other direction, if \"G\" is \"k\"-degenerate, then an ordering with coloring number \"k\" + 1 can be obtained by repeatedly finding a vertex \"v\" with at most \"k\" neighbors, removing \"v\" from the graph, ordering the remaining vertices, and adding \"v\" to the end of the order.\n\nA third, equivalent formulation is that \"G\" is \"k\"-degenerate (or has coloring number at most \"k\" + 1) if and only if the edges of \"G\" can be oriented to form a directed acyclic graph with outdegree at most \"k\". Such an orientation can be formed by orienting each edge towards the earlier of its two endpoints in a coloring number ordering. In the other direction, if an orientation with outdegree \"k\" is given, an ordering with coloring number \"k\" + 1 can be obtained as a topological ordering of the resulting directed acyclic graph.\n\nA \"k\"-core of a graph \"G\" is a maximal subgraph of \"G\" in which all vertices have degree at least \"k\". Equivalently, it is one of the connected components of the subgraph of \"G\" formed by repeatedly deleting all vertices of degree less than \"k\". If a non-empty \"k\"-core exists, then, clearly, \"G\" has degeneracy at least \"k\", and the degeneracy of \"G\" is the largest \"k\" for which \"G\" has a \"k\"-core.\n\nA vertex formula_1 has \"coreness\" formula_2 if it belongs to a\nformula_2-core but not to any formula_4-core.\n\nThe concept of a \"k\"-core was introduced to study the clustering structure of social networks and to describe the evolution of random graphs; it has also been applied in bioinformatics, network visualization, Internet structure, spreading of economic crises, identifying influential spreaders, brain cortex structure or relisience of networks in Ecology. \nA method to identify k-core structure in a weighted network was also developed.\n\n\"K\"-core percolation (called also bootstrap percolation) theory was developed by Dorogovtsev et al^ on random networks. They find a hybrid phase transition with a jump emergence of the \"k\"-core as in a first order phase transition as well as a singularity as in a continuous transition. The model shows cascading failure processes.\n\nA model combining cascading failures from \"k\"-core and from interdependent networks was recently developed and analyzed. It is found that the phase diagram of the combined processes is very rich and includes novel features that do not appear in each of the processes separately.\n\nAs describe, it is possible to find a vertex ordering of a finite graph \"G\" that optimizes the coloring number of the ordering, in linear time, by using a bucket queue to repeatedly find and remove the vertex of smallest degree. The degeneracy is then the highest degree of any vertex at the moment it is removed.\n\nIn more detail, the algorithm proceeds as follows:\n\nAt the end of the algorithm, \"k\" contains the degeneracy of \"G\" and \"L\" contains a list of vertices in an optimal ordering for the coloring number. The \"i\"-cores of \"G\" are the prefixes of \"L\" consisting of the vertices added to \"L\" after \"k\" first takes a value greater than or equal to \"i\".\n\nInitializing the variables \"L\", \"d\", \"D\", and \"k\" can easily be done in linear time. Finding each successively removed vertex \"v\" and adjusting the cells of \"D\" containing the neighbors of \"v\" take time proportional to the value of \"d\" at that step; but the sum of these values is the number of edges of the graph (each edge contributes to the term in the sum for the later of its two endpoints) so the total time is linear.\n\nIf a graph \"G\" is oriented acyclically with outdegree \"k\", then its edges may be partitioned into \"k\" forests by choosing one forest for each outgoing edge of each node. Thus, the arboricity of \"G\" is at most equal to its degeneracy. In the other direction, an \"n\"-vertex graph that can be partitioned into \"k\" forests has at most \"k\"(\"n\" − 1) edges and therefore has a vertex of degree at most 2\"k\"− 1 – thus, the degeneracy is less than twice the arboricity. One may also compute in polynomial time an orientation of a graph that minimizes the outdegree but is not required to be acyclic. The edges of a graph with such an orientation may be partitioned in the same way into \"k\" pseudoforests, and conversely any partition of a graph's edges into \"k\" pseudoforests leads to an outdegree-\"k\" orientation (by choosing an outdegree-1 orientation for each pseudoforest), so the minimum outdegree of such an orientation is the pseudoarboricity, which again is at most equal to the degeneracy. The thickness is also within a constant factor of the arboricity, and therefore also of the degeneracy.\n\nA \"k\"-degenerate graph has chromatic number at most \"k\" + 1; this is proved by a simple induction on the number of vertices\nwhich is exactly like the proof of the six-color theorem for planar graphs. Since chromatic number is an upper bound on the order of\nthe maximum clique, the latter invariant is also at most degeneracy plus one. By using a greedy coloring algorithm on an ordering with optimal coloring number, one can graph color a \"k\"-degenerate graph using at most \"k\" + 1 colors.\n\nA \"k\"-vertex-connected graph is a graph that cannot be partitioned into more than one component by the removal of fewer than \"k\" vertices, or equivalently a graph in which each pair of vertices can be connected by \"k\" vertex-disjoint paths. Since these paths must leave the two vertices of the pair via disjoint edges, a \"k\"-vertex-connected graph must have degeneracy at least \"k\". Concepts related to \"k\"-cores but based on vertex connectivity have been studied in social network theory under the name of structural cohesion.\n\nIf a graph has treewidth or pathwidth at most \"k\", then it is a subgraph of a chordal graph which has a perfect elimination ordering in which each vertex has at most \"k\" earlier neighbors. Therefore, the degeneracy is at most equal to the treewidth and at most equal to the pathwidth. However, there exist graphs with bounded degeneracy and unbounded treewidth, such as the grid graphs.\n\nThe Erdős–Burr conjecture relates the degeneracy of a graph \"G\" to the Ramsey number of \"G\", the largest \"n\" such that any two-edge-coloring of an \"n\"-vertex complete graph must contain a monochromatic copy of \"G\". Specifically, the conjecture is that for any fixed value of \"k\", the Ramsey number of \"k\"-degenerate graphs grows linearly in the number of vertices of the graphs. The conjecture was proven by .\n\nAlthough concepts of degeneracy and coloring number are frequently considered in the context of finite graphs, the original motivation for was the theory of infinite graphs. For an infinite graph \"G\", one may define the coloring number analogously to the definition for finite graphs, as the smallest cardinal number α such that there exists a well-ordering of the vertices of \"G\" in which each vertex has fewer than α neighbors that are earlier in the ordering. The inequality between coloring and chromatic numbers holds also in this infinite setting; state that, at the time of publication of their paper, it was already well known.\n\nThe degeneracy of random subsets of infinite lattices has been studied under the name of bootstrap percolation.\n\n\n"}
{"id": "29875125", "url": "https://en.wikipedia.org/wiki?curid=29875125", "title": "Divisibility sequence", "text": "Divisibility sequence\n\nIn mathematics, a divisibility sequence is an integer sequence formula_1 indexed by positive integers \"n\" such that\n\nfor all \"m\", \"n\". That is, whenever one index is a multiple of another one, then the corresponding term also is a multiple of the other term. The concept can be generalized to sequences with values in any ring where the concept of divisibility is defined.\n\nA strong divisibility sequence is an integer sequence formula_1 such that for all positive integers \"m\", \"n\",\n\nEvery strong divisibility sequence is a divisibility sequence: if formula_5 then formula_6. Then by the strong divisibility property, formula_7 and therefore formula_8.\n\n\n"}
{"id": "24093107", "url": "https://en.wikipedia.org/wiki?curid=24093107", "title": "Double-star snark", "text": "Double-star snark\n\nIn the mathematical field of graph theory, the double-star snark is a snark with 30 vertices and 45 edges.\n\nIn 1975, Rufus Isaacs introduced two infinite families of snarks—the flower snark and the BDS snark, a family that includes the two Blanuša snarks, the Descartes snark and the Szekeres snark (BDS stands for Blanuša Descartes Szekeres). Isaacs also discovered one 30-vertex snark that does not belongs to the BDS family and that is not a flower snark — the double-star snark.\n\nAs a snark, the double-star graph is a connected, bridgeless cubic graph with chromatic index equal to 4. The double-star snark is non-planar and non-hamiltonian but is hypohamiltonian. It has book thickness 3 and queue number 2.\n"}
{"id": "1182975", "url": "https://en.wikipedia.org/wiki?curid=1182975", "title": "Dual basis in a field extension", "text": "Dual basis in a field extension\n\nIn mathematics, the linear algebra concept of dual basis can be applied in the context of a finite extension \"L\"/\"K\", by using the field trace. This requires the property that the field trace \"Tr\" provides a non-degenerate quadratic form over \"K\". This can be guaranteed if the extension is separable; it is automatically true if \"K\" is a perfect field, and hence in the cases where \"K\" is finite, or of characteristic zero.\n\nA dual basis is not a concrete basis like the polynomial basis or the normal basis; rather it provides a way of using a second basis for computations. \n\nConsider two bases for elements in a finite field, GF(\"p\"):\n\nand\n\nthen \"B\" can be considered a dual basis of \"B\" provided\n\nHere the trace of a value in GF(\"p\") can be calculated as follows:\n\nUsing a dual basis can provide a way to easily communicate between devices that use different bases, rather than having to explicitly convert between bases using the change of bases formula. Furthermore, if a dual basis is implemented then conversion from an element in the original basis to the dual basis can be accomplished with multiplication by the multiplicative identity (usually 1).\n"}
{"id": "19236494", "url": "https://en.wikipedia.org/wiki?curid=19236494", "title": "Edwin E. Moise", "text": "Edwin E. Moise\n\nEdwin Evariste Moise (; December 22, 1918 – December 18, 1998)\nwas an American mathematician and mathematics education reformer. After his retirement from mathematics he became a literary critic of 19th-century English poetry and had several notes published in that field.\n\nEdwin E. Moise was born December 22, 1918 in New Orleans, Louisiana.\nHe graduated from Tulane University in 1940. He worked as a cryptanalyst and Japanese translator for the Office of the Chief of Naval Operations during World War II.\n\nHe received his Ph.D. degree in mathematics from the University of Texas in 1947. His dissertation was titled \"An indecomposable continuum which is homeomorphic to each of its nondegenerate subcontinua,\" a topic in continuum theory, and was written under the direction of Robert Lee Moore. In his dissertation Moise coined the term pseudo-arc.\n\nMoise taught at the University of Michigan from 1947 to 1960. He was James B. Conant Professor of education and mathematics at Harvard University from 1960 to 1971. He held a Distinguished Professorship at Queens College, City University of New York from 1971 to 1987.\n\nMoise started working on the topology of 3-manifolds while at the University of Michigan. During 1949–1951 he held an appointment at the Institute for Advanced Study during which he proved Moise's theorem that every 3-manifold can be triangulated in an essentially unique way.\n\nMoise joined the School Mathematics Study Group when it started in 1958, as a member of the geometry writing team. The team produced several course outlines and sample pages for a 10th grade geometry course, and then Moise and Floyd L. Downs wrote a geometry textbook, based on the team's approach, that was published in 1964. The textbook used metric postulates instead of Euclid's postulates, a controversial approach supported by some mathematicians such as Saunders Mac Lane but opposed by others such as Alexander Wittenberg and Morris Kline.\n\nMoise was a president of the Mathematical Association of America, a vice-president of the American Mathematical Society, a Fellow of the American Academy of Arts and Sciences, and was on the Executive Committee of the International Commission on Mathematical Instruction.\n\nMoise retired from Queens College in 1987 and started a second career studying 19th century English poetry. He had six short notes of literary criticism published.\n\nIn the middle and late 1960s, Moise was among the few members of the senior faculty at Harvard University who strongly and publicly opposed the Vietnam War.\n\nMoise died in New York City on December 18, 1998, aged 79.\n\n\n\n"}
{"id": "13933682", "url": "https://en.wikipedia.org/wiki?curid=13933682", "title": "External (mathematics)", "text": "External (mathematics)\n\nThe term external is useful for describing certain algebraic structures. The term comes from the concept of an external binary operation which is a binary operation that draws from some \"external set\". To be more specific, a left external binary operation on \"S\" over \"R\" is a function formula_1 and a right external binary operation on \"S\" over \"R\" is a function formula_2 where \"S\" is the set the operation is defined on, and \"R\" is the external set (the set the operation is defined \"over\").\n\nThe \"external\" concept is a generalization rather than a specialization, and as such, it is different from many terms in mathematics. A similar but opposite concept is that of an \"internal binary function\" from \"R\" to \"S\", defined as a function formula_3. Internal binary functions are like binary functions, but are a form of specialization, so they only accept a subset of the domains of binary functions. Here we list these terms with the function signatures they imply, along with some examples:\n\n\nSince monoids are defined in terms of binary operations, we can define an \"external monoid\" in terms of \"external binary operations\". For the sake of simplicity, unless otherwise specified, a \"left\" external binary operation is implied. Using the term \"external\", we can make the generalizations:\n\n\nMuch of the machinery of modules and vector spaces are fairly straightforward, or discussed above. The only thing not covered yet is their distribution axioms. The external ring multiplication formula_25 is externally distributive in formula_26 over the ring formula_27 iff:\n\nUsing these terminology we can make the following local generalizations:\n\nNow that we have all the terminology we need, we can make simple connections between various structures:\n\nIt could be argued that we already have terms for the concepts described here, like dynamical systems, group actions, modules, and vector spaces. However, there is still no other terminology available for an external monoid for which this terminology gives us a concise expression. Above all else, this is a reason this term should be of use in the mathematical community.\n"}
{"id": "350164", "url": "https://en.wikipedia.org/wiki?curid=350164", "title": "Farey sequence", "text": "Farey sequence\n\nIn mathematics, the Farey sequence of order \"n\" is the sequence of completely reduced fractions, either between 0 and 1, or without this restriction, which when in lowest terms have denominators less than or equal to \"n\", arranged in order of increasing size.\n\nWith the restricted definition, each Farey sequence starts with the value 0, denoted by the fraction ⁄, and ends with the value 1, denoted by the fraction ⁄ (although some authors omit these terms).\n\nA Farey sequence is sometimes called a Farey series, which is not strictly correct, because the terms are not summed. \n\nThe Farey sequences of orders 1 to 8 are :\n\nFarey sequences are named after the British geologist John Farey, Sr., whose letter about these sequences was published in the \"Philosophical Magazine\" in 1816. Farey conjectured, without offering proof, that each new term in a Farey sequence expansion is the mediant of its neighbours. Farey's letter was read by Cauchy, who provided a proof in his \"Exercices de mathématique\", and attributed this result to Farey. In fact, another mathematician, Charles Haros, had published similar results in 1802 which were not known either to Farey or to Cauchy. Thus it was a historical accident that linked Farey's name with these sequences. This is an example of Stigler's law of eponymy.\n\nThe Farey sequence of order \"n\" contains all of the members of the Farey sequences of lower orders. In particular \"F\" contains all of the members of \"F\" and also contains an additional fraction for each number that is less than \"n\" and coprime to \"n\". Thus \"F\" consists of \"F\" together with the fractions and . \n\nThe middle term of a Farey sequence \"F\" is always , \nfor \"n\" > 1. From this, we can relate the lengths of \"F\" and \"F\" using Euler's totient function formula_1 :\n\nUsing the fact that |\"F\"| = 2, we can derive an expression for the length of \"F\":\n\nWe also have :\n\nand by a Möbius inversion formula : \nwhere µ(\"d\") is the number-theoretic Möbius function, and formula_6 is the floor function. \n\nThe asymptotic behaviour of |\"F\"| is :\n\nThe index formula_8 of a fraction formula_9 in the Farey sequence formula_10 is simply the position that formula_9 occupies in the sequence. This is of special relevance as it is used in an alternative formulation of the Riemann hypothesis, see below. Various useful properties follow:\n\nThe index of formula_17 where formula_18 and formula_19 is the least common multiple of the first formula_20 numbers, formula_21, is given by:\n\nFractions which are neighbouring terms in any Farey sequence are known as a \"Farey pair\" and have the following properties.\n\nIf and are neighbours in a Farey sequence, with  < , then their difference  −  is equal to . Since \n\nthis is equivalent to saying that \n\nThus and are neighbours in \"F\", and their difference is .\n\nThe converse is also true. If \n\nfor positive integers \"a\",\"b\",\"c\" and \"d\" with \"a\" < \"b\" and \"c\" < \"d\" then and will be neighbours in the Farey sequence of order max(\"b,d\").\n\nIf has neighbours and in some Farey sequence, with \n\nthen is the mediant of and – in other words, \n\nThis follows easily from the previous property, since if , then , , .\n\nIt follows that if and are neighbours in a Farey sequence then the first term that appears between them as the order of the Farey sequence is incremented is \n\nwhich first appears in the Farey sequence of order .\n\nThus the first term to appear between and is , which appears in \"F\".\n\nThe \"Stern–Brocot tree\" is a data structure showing how the sequence is built up from 0 (= ) and 1 (= ), by taking successive mediants.\n\nFractions that appear as neighbours in a Farey sequence have closely related continued fraction expansions. Every fraction has two continued fraction expansions — in one the final term is 1; in the other the final term is greater than 1. If , which first appears in Farey sequence \"F\", has continued fraction expansions\n\nthen the nearest neighbour of in \"F\" (which will be its neighbour with the larger denominator) has a continued fraction expansion\n\nand its other neighbour has a continued fraction expansion \n\nFor example, has the two continued fraction expansions and , and its neighbours in \"F\" are , which can be expanded as ; and , which can be expanded as .\n\nFarey sequences are very useful to find rational approximations of irrational numbers.\n\nIn physics systems featuring resonance phenomena Farey sequences provide a very elegant and efficient method to compute resonance locations in 1D and 2D.\n\nThere is a connection between Farey sequence and Ford circles.\n\nFor every fraction \"p\"/\"q\" (in its lowest terms) there is a Ford circle C[\"p\"/\"q\"], which is the circle with radius 1/(2\"q\") and centre at (\"p\"/\"q\", 1/(2\"q\")). Two Ford circles for different fractions are either disjoint or they are tangent to one another—two Ford circles never intersect. If 0 < \"p\"/\"q\" < 1 then the Ford circles that are tangent to C[\"p\"/\"q\"] are precisely the Ford circles for fractions that are neighbours of \"p\"/\"q\" in some Farey sequence.\n\nThus \"C\"[2/5] is tangent to \"C\"[1/2], \"C\"[1/3], \"C\"[3/7], \"C\"[3/8] etc.\n\nFarey sequences are used in two equivalent formulations of the Riemann hypothesis. Suppose the terms of formula_29 are formula_30. Define formula_31, in other words formula_32 is the difference between the \"k\"th term of the \"n\"th Farey sequence, and the \"k\"th member of a set of the same number of points, distributed evenly on the unit interval. In 1924 Jérôme Franel proved that the statement\n\nis equivalent to the Riemann hypothesis, and then Edmund Landau remarked (just after Franel's paper) that the statement\n\nis also equivalent to the Riemann hypothesis.\n\nA surprisingly simple algorithm exists to generate the terms of \"F\" in either traditional order (ascending) or non-traditional order (descending). The algorithm computes each successive entry in terms of the previous two entries using the mediant property given above. If and are the two given entries, and is the unknown next entry, then  = . Since is in lowest terms, there must be an integer \"k\" such that \"kc\" = \"a\" + \"p\" and \"kd\" = \"b\" + \"q\", giving \"p\" = \"kc\" − \"a\" and \"q\" = \"kd\" − \"b\". If we consider \"p\" and \"q\" to be functions of \"k\", then\nso the larger \"k\" gets, the closer gets to .\n\nTo give the next term in the sequence \"k\" must be as large as possible, subject to \"kd\" − \"b\" ≤ \"n\" (as we are only considering numbers with denominators not greater than \"n\"), so \"k\" is the greatest integer ≤ . Putting this value of \"k\" back into the equations for \"p\" and \"q\" gives\n\nThis is implemented in Python as:\nBrute-force searches for solutions to Diophantine equations in rationals can often take advantage of the Farey series (to search only reduced forms). The lines marked (*) can also be modified to include any two adjacent terms so as to generate terms only larger (or smaller) than a given term.\n\n\n\n"}
{"id": "53143101", "url": "https://en.wikipedia.org/wiki?curid=53143101", "title": "Graeme Milton", "text": "Graeme Milton\n\nGraeme Milton is an American mathematician, currently Distinguished Professor at University of Utah and also previously the Eisenbud Professor at Mathematical Sciences Research Institute in 2010 and also a Full Professor at Courant Institute of Mathematical Sciences.\n\nGraeme W. Milton received B.Sc. and M.Sc degrees in Physics from the University of Sydney in 1980 and 1982 respectively. He received a Ph.D degree in Physics from Cornell University in 1985, after which he joined the Caltech Physics Department as a Weingart Fellow from 1984 to 1986. He then joined the Courant Institute of Mathematical Sciences where he stayed until 1994 when he joined the faculty at the University of Utah as a full professor. He has received numerous honors and awards, including a Alfred P. Sloan Fellowship and a Packard Fellowship, both in 1988. He was an Invited Speaker for the 1998 International Congress of Mathematicians. He was awarded the Ralph E. Kleinman Prize in 2003 by the Society for Industrial and Applied Mathematics for “his many deep contributions to the modeling and analysis of composite materials.”\n"}
{"id": "45093024", "url": "https://en.wikipedia.org/wiki?curid=45093024", "title": "Henda Swart", "text": "Henda Swart\n\nHendrika Cornelia Scott (Henda) Swart (born 1939, died February 2016 [age 77-78]) was a South African mathematician, a professor emeritus of mathematics at the University of KwaZulu-Natal and a professor at the University of Cape Town\n\nSwart began teaching at the University of Natal in 1962. She was the first person to earn a doctorate in mathematics from Stellenbosch University, in 1971, with a dissertation on the geometry of projective planes supervised by Kurt-Rüdiger Kannenberg. In 1977, her research interests shifted from geometry to graph theory, which she continued to publish in for the rest of her career.\n\nShe was the editor-in-chief of the journal \"Utilitas Mathematica\", and was vice president of the Institute of Combinatorics and its Applications. In 1996 she became a fellow of the Royal Society of South Africa.\n"}
{"id": "381750", "url": "https://en.wikipedia.org/wiki?curid=381750", "title": "Hilbert's sixteenth problem", "text": "Hilbert's sixteenth problem\n\nHilbert's 16th problem was posed by David Hilbert at the Paris conference of the International Congress of Mathematicians in 1900, as part of his list of 23 problems in mathematics.\n\nThe original problem was posed as the \"Problem of the topology of algebraic curves and surfaces\" (\"Problem der Topologie algebraischer Kurven und Flächen\").\n\nActually the problem consists of two similar problems in different branches of mathematics:\n\nThe first problem is yet unsolved for \"n\" = 8. Therefore, this problem is what usually is meant when talking about Hilbert's sixteenth problem in real algebraic geometry. The second problem also remains unsolved: no upper bound for the number of limit cycles is known for any \"n\" > 1, and this is what usually is meant by Hilbert's sixteenth problem in the field of dynamical systems.\n\nIn 1876 Harnack investigated algebraic curves in the real projective plane and found that curves of degree \"n\" could have no more than\n\nseparate connected components. Furthermore, he showed how to construct curves that attained that upper bound, and thus that it was the best possible bound. Curves with that number of components are called M-curves.\n\nHilbert had investigated the M-curves of degree 6, and found that the 11 components always were grouped in a certain way. His challenge to the mathematical community now was to completely investigate the possible configurations of the components of the M-curves.\n\nFurthermore, he requested a generalization of Harnack's Theorem to algebraic surfaces and a similar investigation of surfaces with the maximum number of components.\n\nHere we are going to consider polynomial vector fields in the real plane, that is a system of differential equations of the form:\n\nwhere both \"P\" and \"Q\" are real polynomials of degree \"n\".\n\nThese polynomial vector fields were studied by Poincaré, who had the idea of abandoning the search for finding exact solutions to the system, and instead attempted to study the qualitative features of the collection of all possible solutions.\n\nAmong many important discoveries, he found that the limit sets of such solutions need not be a stationary point, but could rather be a periodic solution. Such solutions are called limit cycles.\n\nThe second part of Hilbert's 16th problem is to decide an upper bound for the number of limit cycles in polynomial vector fields of degree \"n\" and, similar to the first part, investigate their relative positions.\n\nIt was shown in 1991/1992 by Yulii Ilyashenko and Jean Écalle that every polynomial vector field in the plane has only finitely many limit cycles (a 1923 article by Henri Dulac claiming a proof of this statement had been shown to contain a gap in 1981). This statement is not obvious, since it is easy to construct smooth (C) vector fields in the plane with infinitely many concentric limit cycles.\n\nThe question whether there exists a finite upper bound \"H\"(\"n\") for the number of limit cycles of planar polynomial vector fields of degree \"n\" remains unsolved for any \"n\" > 1. (\"H\"(1) = 0 since linear vector fields do not have limit cycles.) Evgenii Landis and Ivan Petrovsky claimed a solution in the 1950s, but it was shown wrong in the early 1960s. Quadratic plane vector fields with four limit cycles are known.\n\nIn his speech, Hilbert presented the problems as:\n\nHilbert continues:\n\n"}
{"id": "2067069", "url": "https://en.wikipedia.org/wiki?curid=2067069", "title": "IDEA NXT", "text": "IDEA NXT\n\nIn cryptography, the IDEA NXT algorithm (previously known as FOX) is a block cipher designed by Pascal Junod and Serge Vaudenay of EPFL (Lausanne, Switzerland). It was conceived between 2001 and 2003. The project was originally named FOX and was published in 2003. In May 2005 it was announced by MediaCrypt under the name IDEA NXT. IDEA NXT is the successor to the International Data Encryption Algorithm (IDEA) and also uses the Lai-Massey scheme. MediaCrypt AG holds patents on elements of IDEA and IDEA NXT. The cipher is specified in two configurations: NXT64 (with block of 64 bits, key of 128 bits, 16 rounds) and NXT128 (with block of 128 bits, key of 256 bits, 16 rounds).\n\n"}
{"id": "20908760", "url": "https://en.wikipedia.org/wiki?curid=20908760", "title": "Institut de Mathématiques de Toulouse", "text": "Institut de Mathématiques de Toulouse\n\nThe Institut de Mathématiques de Toulouse (IMT) is the Toulouse Mathematics Institute, a CNRS Research Laboratory which federates the mathematics community of the Toulouse area in France. The IMT is constituted by three main teams (in 2008):\n\n\nThe IMT is one of the largest French research centers in mathematics, and its scientific activities cover almost all domains of mathematics. It comprises approximately 180 permanent researchers and 100 PhD students (in 2008), belonging to various institutions of the University of Toulouse.\n\nThe main buildings of the IMT are located on the Paul Sabatier University campus.\n\nThe IMT is responsible for the Fermat Prize and the Annales de la Faculté des Sciences de Toulouse.\n\n"}
{"id": "15094960", "url": "https://en.wikipedia.org/wiki?curid=15094960", "title": "Journal of Combinatorial Theory", "text": "Journal of Combinatorial Theory\n\nThe Journal of Combinatorial Theory, Series A and Series B, are mathematical journals specializing in combinatorics and related areas. They are published by Elsevier. \"Series A\" is concerned primarily with structures, designs, and applications of combinatorics. \"Series B\" is concerned primarily with graph and matroid theory. The two series are two of the leading journals in the field and are widely known as \"JCTA\" and \"JCTB\".\n\nThe journal was founded in 1966 by Frank Harary and Gian-Carlo Rota. Originally there was only one journal, which was split into two parts in 1971 as the field grew rapidly.\n\nInfluential articles that appeared in the journal include Katona's elegant proof of the Erdős–Ko–Rado theorem and a series of papers spanning over 500 pages, appearing from 1983 to 2004, by Neil Robertson and Paul D. Seymour on the topic of graph minors, which together constitute the proof of the graph minor theorem.\n\nA selection of a few influential articles is listed below.\n"}
{"id": "58977389", "url": "https://en.wikipedia.org/wiki?curid=58977389", "title": "Journal of Computational and Applied Mathematics", "text": "Journal of Computational and Applied Mathematics\n\nThe Journal of Computational and Applied Mathematics is a peer-reviewed scientific journal covering computational and applied mathematics. It was established in 1975 and is published biweekly by Elsevier. The editors-in-chief are Yalchin Efendiev (Texas A&M University), Taketomo Mitsui (Nagoya University), Michael Kwok-Po Ng (Hong Kong Baptist University), Fatih Tank (Ankara University), and Luc Wuytack (University of Antwerp). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.632. \n"}
{"id": "48977562", "url": "https://en.wikipedia.org/wiki?curid=48977562", "title": "Katugampola fractional operators", "text": "Katugampola fractional operators\n\nIn mathematics, Katugampola fractional operators are integral operators that generalize the \"Riemann–Liouville\" and the \"Hadamard\" fractional operators into a unique form. The Katugampola fractional integral generalizes both the Riemann–Liouville fractional integral and the Hadamard fractional integral into a single form and It is also closely related to the Erdelyi–Kober operator that generalizes the Riemann–Liouville fractional integral. Katugampola fractional derivative has been defined using the Katugampola fractional integral and as with any other fractional differential operator, it also extends the possibility of taking real number powers or complex number powers of the integral and differential operators.\n\nThese operators have been defined on the following extended-Lebesgue space.\n\nLet formula_1 be the space of those Lebesgue measurable functions formula_2 on formula_3 for which formula_4, where the norm is defined by \nfor formula_6 and for the case formula_7\n\nIt is defined via the following integrals \nfor formula_9 and formula_10 This integral is called the \"left-sided\" fractional integral. Similarly, the \"right-sided\" fractional integral is defined by,\nfor formula_11 and formula_12.\n\nThese are the fractional generalizations of the formula_13-fold left- and right-integrals of the form\n\nand\n\nrespectively. Even though the integral operators in question are close resemblance of the famous Erdélyi–Kober operator, it is not possible to obtain the Hadamard fractional integrals as a direct consequence of the Erdélyi–Kober operators. Also, there is a corresponding fractional derivative, which generalizes the \"Riemann–Liouville\" and the \"Hadamard fractional derivatives\". As with the case of fractional integrals, the same is not true for the Erdélyi–Kober operator.\n\nAs with the case of other fractional derivatives, it is defined via the Katugampola fractional integral.\n\nLet formula_17 and formula_18 The generalized fractional derivatives, corresponding to the generalized fractional integrals () and () are defined, respectively, for formula_19, by\nand\nrespectively, if the integrals exist.\n\nThese operators generalize the Riemann–Liouville and Hadamard fractional derivatives into a single form, while the Erdelyi–Kober fractional is a generalization of the Riemann–Liouville fractional derivative. When, formula_22, the fractional derivatives are referred to as Weyl-type derivatives.\n\nThere is a Caputo-type modification of the Katugampola derivative that is now known as the Caputo–Katugampola fractional derivative.\nLet formula_23 and formula_24. The C-K fractional derivative of order formula_25 of the function formula_26 with respect to parameter formula_24 can be expressed as\n\nIt satisfies the following result. Assume that formula_29, then the C-K derivative has the following equivalent form \n\nAnother recent generalization is the \"Hilfer-Katugampola\" fractional derivative. Let order formula_31 and type formula_32. The fractional derivative (left-sided/right-sided),\nwith respect to formula_33, with formula_34, is defined by\n\nwhere formula_36, for functions formula_37 in which the expression on the right hand side \nexists, where formula_38 is the generalized fractional integral \ngiven in ().\n\nAs in the case of Laplace transforms, Mellin transforms will be used specially when solving differential equations. The Mellin transforms of the \"left-sided\" and \"right-sided\" versions of Katugampola Integral operators are given by \n\nLet formula_39 and formula_18 Then,\n\nfor formula_42, if formula_43 exists for formula_44.\n\nKatugampola operators satisfy the following Hermite-Hadamard type inequalities:\n\nLet formula_45 and formula_18. If formula_2 is a convex function on formula_3, then \nwhere formula_50.\n\nWhen formula_51, in the above result, the following Hadamard type inequality holds:\n\nLet formula_45. If formula_2 is a convex function on formula_3, then \nwhere formula_56 and formula_57 are left- and right-sided Hadamard fractional integrals.\n\nThese operators have been mentioned in the following works:\n\n\nThe CRONE (R) Toolbox, a Matlab and Simulink Toolbox dedicated to fractional calculus, can be downloaded at http://cronetoolbox.ims-bordeaux.fr\n"}
{"id": "43094970", "url": "https://en.wikipedia.org/wiki?curid=43094970", "title": "Key checksum value", "text": "Key checksum value\n\nIn cryptography, a Key Checksum Value (KCV) is checksum of the key value used to compare keys without knowing their actual values.\n\nA KCV normally consists of a zero-block encrypted with the key, or a cryptographically secure hash over the key (also called a fingerprint)\n"}
{"id": "209627", "url": "https://en.wikipedia.org/wiki?curid=209627", "title": "Klein–Gordon equation", "text": "Klein–Gordon equation\n\nThe Klein–Gordon equation (Klein–Fock–Gordon equation or sometimes Klein–Gordon–Fock equation) is a relativistic wave equation, related to the Schrödinger equation. It is second order in space and time and manifestly Lorentz covariant. It is a quantized version of the relativistic energy–momentum relation. Its solutions include a quantum scalar or pseudoscalar field, a field whose quanta are spinless particles. Its theoretical relevance is similar to that of the Dirac equation. Electromagnetic interactions can be incorporated, forming the topic of scalar electrodynamics, but because common spinless particles like the pi mesons are unstable and also experience the strong interaction (with unknown interaction term in the Hamiltonian), the practical utility is limited.\n\nThe equation can be put into the form of a Schrödinger equation. In this form it is expressed as two coupled differential equations, each of first order in time. The solutions have two components, reflecting the charge degree of freedom in relativity. It admits a conserved quantity, but this is not positive definite. The wave function cannot therefore be interpreted as a probability amplitude. The conserved quantity is instead interpreted as electric charge and the norm squared of the wave function is interpreted as a charge density. The equation describes all spinless particles with positive, negative as well as zero charge. \n\nAny solution of the free Dirac equation is, component-wise, a solution of the free Klein–Gordon equation.\n\nThe equation does not form the basis of a consistent quantum relativistic \"one-particle\" theory. There is no known such theory for particles of any spin. For full reconciliation of quantum mechanics with special relativity quantum field theory is needed, in which the Klein–Gordon equation reemerges as the equation obeyed by the components of all free quantum fields. In quantum field theory, the solutions of the free (noninteracting) versions of the original equations still play a role. They are needed to build the Hilbert space (Fock space) and to express quantum field by using complete sets (spanning sets of Hilbert space) of wave functions.\n\nThe Klein–Gordon equation with mass parameter formula_1 is\nSolutions of the equation are complex-valued functions formula_3 of the time variable formula_4 and space variables formula_5; the Laplacian formula_6 acts on the space variables only.\n\nThe equation is often abbreviated as\n\nwhere and is the d'Alembert operator, defined by\n\nThe Klein–Gordon equation is often written in natural units:\n\nThe form of the Klein–Gordon equation is derived by requiring that plane wave solutions of the equation:\n\nobey the energy momentum relation of special relativity:\n\nUnlike the Schrödinger equation, the Klein–Gordon equation admits two values of for each , one positive and one negative. Only by separating out the positive and negative frequency parts does one obtain an equation describing a relativistic wavefunction. For the time-independent case, the Klein–Gordon equation becomes\n\nwhich is formally the same as the homogeneous screened Poisson equation.\n\nThe equation was named after the physicists Oskar Klein and Walter Gordon, who in 1926 proposed that it describes relativistic electrons. Other authors making similar claims in that same year were Vladimir Fock, Johann Kudar, Théophile de Donder and Frans-H. van den Dungen, and Louis de Broglie. Although it turned out that modeling the electron's spin required the Dirac equation, the Klein–Gordon equation correctly describes the spinless relativisitic composite particles, like the pion. On July 4, 2012, European Organization for Nuclear Research CERN announced the discovery of the Higgs boson. Since the Higgs boson is a spin-zero particle, it is the first observed ostensibly elementary particle to be described by the Klein–Gordon equation. Further experimentation and analysis is required to discern whether the Higgs boson observed is that of the Standard Model, or a more exotic, possibly composite, form. \n\nThe Klein–Gordon equation was first considered as a quantum wave equation by Schrödinger in his search for an equation describing de Broglie waves. The equation is found in his notebooks from late 1925, and he appears to have prepared a manuscript applying it to the hydrogen atom. Yet, because it fails to take into account the electron's spin, the equation predicts the hydrogen atom's fine structure incorrectly, including overestimating the overall magnitude of the splitting pattern by a factor of for the -th energy level. The Dirac equation relativistic spectrum is, however, easily recovered if the orbital momentum quantum number is replaced by total angular momentum quantum number . In January 1926, Schrödinger submitted for publication instead \"his\" equation, a non-relativistic approximation that predicts the Bohr energy levels of hydrogen without fine structure.\n\nIn 1926, soon after the Schrödinger equation was introduced, Vladimir Fock wrote an article about its generalization for the case of magnetic fields, where forces were dependent on velocity, and independently derived this equation. Both Klein and Fock used Kaluza and Klein's method. Fock also determined the gauge theory for the wave equation. The Klein–Gordon equation for a free particle has a simple plane wave solution.\n\nThe non-relativistic equation for the energy of a free particle is\n\nBy quantizing this, we get the non-relativistic Schrödinger equation for a free particle,\nwhere \n\nis the momentum operator ( being the del operator), and \n\nis the energy operator.\n\nThe Schrödinger equation suffers from not being relativistically invariant, meaning that it is inconsistent with special relativity.\n\nIt is natural to try to use the identity from special relativity describing the energy:\n\nThen, just inserting the quantum mechanical operators for momentum and energy yields the equation\n\nThe square root of a differential operator can be defined with the help of Fourier transformations, but due to the asymmetry of space and time derivatives, Dirac found it impossible to include external electromagnetic fields in a relativistically invariant way. So he looked for another equation that can be modified in order to describe the action of electromagnetic forces. In addition, this equation, as it stands, is nonlocal (see also Introduction to nonlocal equations).\n\nKlein and Gordon instead began with the square of the above identity, i.e.\n\nwhich, when quantized, gives\n\nwhich simplifies to\n\nRearranging terms yields\n\nSince all reference to imaginary numbers has been eliminated from this equation, it can be applied to fields that are real valued as well as those that have complex values.\n\nRewriting the first two terms using the inverse of the Minkowski metric , and writing the Einstein summation convention explicitly we get\n\nThus the Klein–Gordon equation can be written in a covariant notation. This often means an abbreviation in the form of\n\nwhere\n\nand\n\nThis operator is called the d'Alembert operator.\n\nToday this form is interpreted as the relativistic field equation for spin-0 particles. Furthermore, any \"component\" of any solution to the free Dirac equation (for a spin-one-half particle) is automatically a solution to the free Klein–Gordon equation. This generalizes to particles of any spin due extension to the Bargmann–Wigner equations. Furthermore, in quantum field theory, every component of every quantum field must satisfy the free Klein–Gordon equation, making the equation a generic expression of quantum fields.\n\nThe Klein–Gordon equation can be generalized to describe a field in some potential as:\n\nThe conserved current associated to the U(1) symmetry of a complex field formula_28 satisfying the Klein–Gordon equation reads\n\nThe form of the conserved current can be derived systematically by applying Noether's theorem to the U(1) symmetry. We will not do so here, but simply give a proof that this conserved current is correct.\n\nThe Klein–Gordon equation for a free particle can be written as\n\nWe look for plane wave solutions of the form\n\nfor some constant angular frequency and wave number . Substitution gives the \"dispersion relation\":\n\nEnergy and momentum are seen to be proportional to and :\n\nSo the dispersion relation is just the classic relativistic equation:\n\nFor massless particles, we may set , recovering the relationship between energy and momentum for massless particles:\n\nThe Klein–Gordon equation can also be derived via a variational method by considering the action:\n\nwhere is the Klein–Gordon field and is its mass. The complex conjugate of is written . If the scalar field is taken to be real-valued, then and it is customary to introduce a factor of for both terms.\n\nApplying the formula for the Hilbert stress–energy tensor to the Lagrangian density (the quantity inside the integral), we can derive the stress–energy tensor of the scalar field. It is\n\nBy integration of the time–time component over all space, one may show that both the positive and negative frequency plane wave solutions can be physically associated with particles with \"positive\" energy. This is not the case for the Dirac equation and its energy–momentum tensor.\n\nThere is a simple way to make any field interact with electromagnetism in a gauge invariant way: replace the derivative operators with the gauge covariant derivative operators. This is because to maintain symmetry of the physical equations for the wavefunction formula_39 under a local \"U\"(1) gauge transformation formula_40 where formula_41 is a locally variable phase angle, which transformation redirects the wavefunction in the complex phase space defined by formula_42, it is required that ordinary derivatives formula_43 be replaced by gauge-covariant derivatives formula_44 while the gauge fields transform as formula_45. The Klein–Gordon equation therefore becomes:\n\nin natural units, where is the vector potential. While it is possible to add many higher order terms, for example,\n\nthese terms are not renormalizable in 3 + 1 dimensions.\n\nThe field equation for a charged scalar field multiplies by , which means the field must be complex. In order for a field to be charged, it must have two components that can rotate into each other, the real and imaginary parts.\n\nThe action for a charged scalar is the covariant version of the uncharged action:\n\nIn general relativity, we include the effect of gravity by replacing partial with covariant derivatives and the Klein–Gordon equation becomes (in the mostly pluses signature)\n\nor equivalently\n\nwhere \"g\" is the inverse of the metric tensor that is the gravitational potential field, \"g\" is the determinant of the metric tensor, is the covariant derivative and is the Christoffel symbol that is the gravitational force field.\n\n\n\n"}
{"id": "21291593", "url": "https://en.wikipedia.org/wiki?curid=21291593", "title": "Krylov–Bogoliubov averaging method", "text": "Krylov–Bogoliubov averaging method\n\nThe Krylov–Bogolyubov averaging method (Krylov–Bogolyubov method of averaging) is a mathematical method for approximate analysis of oscillating processes in non-linear mechanics. The method is based on the averaging principle when the exact differential equation of the motion is replaced by its averaged version. The method is named after Nikolay Krylov and Nikolay Bogoliubov.\n\nVarious averaging schemes for studying problems of celestial mechanics were used since works of Gauss, Fatou, Delone, Hill. The importance of the contribution of Krylov and Bogoliubov is that they developed a general averaging approach and proved that the solution of the averaged system approximates the exact dynamics.\n\nKrylov–Bogoliubov averaging can be used to approximate oscillatory problems when a classical perturbation expansion fails. That is singular perturbation problems of oscillatory type, for example Einstein's correction to the perihelion precession of Mercury.\n\nThe method deals with differential equations in the form\n\nfor a smooth function \"f\" along with appropriate initial conditions. The parameter \"ε\" is assumed to satisfy\n\nIf \"ε\" = 0 then the equation becomes that of the simple harmonic oscillator with constant forcing, and the general solution is\n\nwhere \"A\" and \"B\" are chosen to match the initial conditions. The solution to the perturbed equation (when \"ε\" ≠ 0) is assumed to take the same\nform, but now \"A\" and \"B\" are allowed to vary with \"t\" (and \"ε\"). If it is also assumed that \nthen it can be shown that \"A\" and \"B\" satisfy the differential equation:\nwhere formula_6. Note that this equation is still exact — no approximation has been made as yet. The method of Krylov and Bogolyubov is to note that the functions A and B vary slowly\nwith time (in proportion to ε), so their dependence on φ can be (approximately) removed by averaging on the right hand side of the previous equation:\nwhere formula_8 and formula_9 are held fixed during the integration. After solving this (possibly) simpler set of differential equations, the Krylov–Bogolyubov averaged approximation for the original function is then given by\nThis approximation has been shown to satisfy \nwhere t satisfies\nfor some constants formula_13 and formula_14, independent of ε.\n"}
{"id": "37440876", "url": "https://en.wikipedia.org/wiki?curid=37440876", "title": "Limited principle of omniscience", "text": "Limited principle of omniscience\n\nIn constructive mathematics, the limited principle of omniscience (LPO) and the lesser limited principle of omniscience (LLPO) are axioms that are nonconstructive but are weaker than the full law of the excluded middle . The LPO and LLPO axioms are used to gauge the amount of nonconstructivity required for an argument, as in constructive reverse mathematics. They are also related to weak counterexamples in the sense of Brouwer. \n\nThe limited principle of omniscience states :\n\nThe lesser limited principle of omniscience states:\n\nIt can be proved constructively that the law of the excluded middle implies LPO, and LPO implies LLPO. However, none of these implications can be reversed in typical systems of constructive mathematics. \n\nThe term \"omniscience\" comes from a thought experiment regarding how a mathematician might tell which of the two cases in the conclusion of LPO holds for a given sequence (\"a\"). Answering the question \"is there a \"k\" with \"a\" = 1?\" negatively, assuming the answer is negative, seems to require surveying the entire sequence. Because this would require the examination of infinitely many terms, the axiom stating it is possible to make this determination was dubbed an \"omniscience principle\" by .\n\n"}
{"id": "372467", "url": "https://en.wikipedia.org/wiki?curid=372467", "title": "Magic hypercube", "text": "Magic hypercube\n\nIn mathematics, a magic hypercube is the \"k\"-dimensional generalization of magic squares, magic cubes and magic tesseracts; that is, a number of integers arranged in an \"n\" × \"n\" × \"n\" × ... × \"n\" pattern such that the sum of the numbers on each pillar (along any axis) as well as the main space diagonals is equal to a single number, the so-called magic constant of the hypercube, denoted \"M\"(\"n\"). It can be shown that if a magic hypercube consists of the numbers 1, 2, ..., \"n\", then it has magic number\n\nFor \"n\" = 4, this sequence is .\n\nFour-, five-, six-, seven- and eight-dimensional magic hypercubes of order three have been constructed by J. R. Hendricks.\n\nMarian Trenkler proved the following theorem:\nA \"p\"-dimensional magic hypercube of order \"n\" exists if and only if\n\"p\" > 1 and \"n\" is different from 2 or \"p\" = 1. A construction of a magic hypercube follows from the proof.\n\nThe R programming language includes a module, library(magic), that will create magic hypercubes of any dimension (with \"n\" a multiple of 4).\n\nChange to more modern conventions here-after (basically k ==> n and n ==> m)\n\nIt is customary to denote the dimension with the letter 'n' and the order of a hypercube with the letter 'm'.\n\n\nFurther: In this article the analytical number range [0..m-1] is being used. For the regular number range [1..m] you can add 1 to each number. This has absolutely no effect on the properties of the hypercube.\n\nIf, in addition, the numbers on every cross section diagonal also sum up to the hypercube's magic number, the hypercube is called a perfect magic hypercube; otherwise, it is called a semiperfect magic hypercube. The number \"n\" is called the order of the magic hypercube.\n\nThe above definition of \"perfect\" assumes that one of the older definitions for perfect magic cubes is used. See Magic Cube Classes.\nThe Universal Classification System for Hypercubes (John R. Hendricks) requires that for any dimension hypercube, \"all\" possible lines sum correctly for the hypercube to be considered \"perfect\" magic. Because of the confusion with the term \"perfect\", nasik is now the preferred term for \"any\" magic hypercube where all possible lines sum to \"S\". Nasik was defined in this manner by C. Planck in 1905. A nasik magic hypercube has (3 − 1) lines of \"m\" numbers passing through each of the \"m\" cells.\n\nin order to keep things in hand a special notation was developed:\n\nNote: The notation for position can also be used for the value on that position. Then, where it is appropriate, dimension and order can be added to it, thus forming: [i]\n\nAs is indicated 'k' runs through the dimensions, while the coordinate 'i' runs through all possible values, when values 'i' are outside the range it is simply moved back into the range by adding or subtracting appropriate multiples of m, as the magic hypercube resides in n-dimensional modular space.\n\nThere can be multiple 'k' between bracket, these can't have the same value, though in undetermined order, which explains the equality of:\n\nformula_4\n\nOf course given 'k' also one value 'i' is referred to.\nWhen a specific coordinate value is mentioned the other values can be taken as 0, which is especially the case when the amount of 'k's are limited using pe. #k=1 as in:\n\nformula_5\n\n(#j=n-1 can be left unspecified) j now runs through all the values in [0..k-1,k+1..n-1].\n\nFurther: without restrictions specified 'k' as well as 'i' run through all possible values, in combinations same letters assume same values. Thus makes it possible to specify a particular line within the hypercube (see r-agonal in pathfinder section)\n\nNote: as far as I know this notation is not in general use yet(?), Hypercubes are not generally analyzed in this particular manner.\n\nFurther: \"perm(0..n-1)\" specifies a permutation of the n numbers 0..n-1.\n\nBesides more specific constructions two more general construction method are noticeable:\n\nThis construction generalizes the movement of the chessboard horses (vectors formula_7) to more general movements (vectors formula_8). The method starts at the position P and further numbers are sequentially placed at positions formula_9 further until (after m steps) a position is reached that is already occupied, a further vector is needed to find the next free position. Thus the method is specified by the n by n+1 matrix:\nThis positions the number 'k' at position:\nC. Planck gives in his 1905 article \"The theory of Path Nasiks\" conditions to create with this method \"Path Nasik\" (or modern {perfect}) hypercubes.\n\n(modular equations).\nThis method is also specified by an n by n+1 matrix. However this time it multiplies the n+1 vector [x..,x,1], After this multiplication the result is taken modulus m to achieve the n (Latin) hypercubes:\nof radix m numbers (also called \"digits\"). On these LP's \"digit changing\" (?i.e. Basic manipulation) are generally applied before these LP's are combined into the hypercube:\n\nJ.R.Hendricks often uses modular equation, conditions to make hypercubes of various quality can be found on http://www.magichypercubes.com/Encyclopedia at several places (especially p-section)\n\nBoth methods fill the hypercube with numbers, the knight-jump guarantees (given appropriate vectors) that every number is present. The Latin prescription only if the components are orthogonal (no two digits occupying the same position)\n\nAmongst the various ways of compounding, the multiplication can be considered as the most basic of these methods. The basic multiplication is given by:\n\nMost compounding methods can be viewed as variations of the above, As most qualifiers are invariant under multiplication one can for example place any aspectial variant of H in the above equation, besides that on the result one can apply a manipulation to improve quality. Thus one can specify pe the J. R. Hendricks / M. Trenklar doubling. These things go beyond the scope of this article.\n\nA hypercube knows n! 2 Aspectial variants, which are obtained by coordinate reflection ([i] --> [(-i)]) and coordinate permutations ([i] --> [i]) effectively giving the Aspectial variant:\nWhere reflect(k) true iff coordinate k is being reflected, only then 2 is added to R.\nAs is easy to see, only n coordinates can be reflected explaining 2, the n! permutation of n coordinates explains the other factor to the total amount of \"Aspectial variants\"!\n\nAspectial variants are generally seen as being equal. Thus any hypercube can be represented shown in \"normal position\" by:\n\nBesides more specific manipulations, the following are of more general nature\n\n\nNote: <nowiki>'#'</nowiki>, '^', '_' and '=' are essential part of the notation and used as manipulation selectors.\n\nDefined as the exchange of components, thus varying the factor m in m, because there are n component hypercubes the permutation is over these n components\n\nThe exchange of coordinate [i] into [i], because of n coordinates a permutation over these n directions is required.\nThe term transpose (usually denoted by ) is used with two dimensional matrices, in general though perhaps \"coordinate permutation\" might be preferable.\n\nDefined as the change of [i] into [perm(i)] alongside the given \"axial\"-direction. Equal permutation along various axes can be combined by adding the factors 2. Thus defining all kinds of r-agonal permutations for any r. Easy to see that all possibilities are given by the corresponding permutation of m numbers.\n\nNoted be that reflection is the special case:\nFurther when all the axes undergo the same ;permutation (R = 2-1) an n-agonal permutation is achieved, In this special case the 'R' is usually omitted so:\n\nUsually being applied at component level and can be seen as given by [i] in perm([i]) since a component is filled with radix m digits, a permutation over m numbers is an appropriate manner to denote these.\n\nJ. R. Hendricks called the directions within a hypercubes \"pathfinders\", these directions are simplest denoted in a ternary number system as:\n\nThis gives 3 directions. since every direction is traversed both ways one can limit to the upper half [(3-1)/2..,3-1)] of the full range.\n\nWith these pathfinders any line to be summed over (or r-agonal) can be specified:\n\nwhich specifies all (broken) r-agonals, p and q ranges could be omitted from this description. The main (unbroken) r-agonals are thus given by the slight modification of the above:\n\nA hypercube H with numbers in the analytical numberrange [0..m-1] has the magic sum:\n\nBesides more specific qualifications the following are the most important, \"summing\" of course stands for \"summing correctly to the magic sum\"\n\n\nNote: This series doesn't start with 0 since a nill-agonal doesn't exist, the numbers correspond with the usual name-calling: 1-agonal = monagonal, 2-agonal = diagonal, 3-agonal = triagonal etc.. Aside from this the number correspond to the amount of \"-1\" and \"1\" in the corresponding pathfinder.\n\nIn case the hypercube also sum when all the numbers are raised to the power p one gets p-multimagic hypercubes. The above qualifiers are simply prepended onto the p-multimagic qualifier. This defines qualifications as {r-agonal 2-magic}. Here also \"2-\" is usually replaced by \"bi\", \"3-\" by \"tri\" etc. (\"1-magic\" would be \"monomagic\" but \"mono\" is usually omitted). The sum for p-Multimagic hypercubes can be found by using Faulhaber's formula and divide it by m.\n\nAlso \"magic\" (i.e. {1-agonal n-agonal}) is usually assumed, the Trump/Boyer {diagonal} cube is technically seen {1-agonal 2-agonal 3-agonal}.\n\nNasik magic hypercube gives arguments for using {nasik} as synonymous to {perfect}. The strange generalization of square 'perfect' to using it synonymous to {diagonal} in cubes is however also resolve by putting curly brackets around qualifiers, so {perfect} means {pan r-agonal; r = 1..n} (as mentioned above).\n\nsome minor qualifications are:\n\nWhere:\n∑ is symbolic for summing all possible k's, there are 2 possibilities for 1.\n[i + 1] expresses [i] and all its r-agonal neighbors.\nfor {complete} the complement of [i] is at position [i + (m/2) ; #k=n ].\n\nfor squares: {compact complete} is the \"modern/alternative qualification\" of what Dame Kathleen Ollerenshaw called most-perfect magic square, {compact complete} is the qualifier for the feature in more than 2 dimensions\nCaution: some people seems to equate {compact} with {compact} instead of {compact}. Since this introductory article is not the place to discuss these kind of issues I put in the dimensional pre-superscript to both these qualifiers (which are defined as shown) \nconsequences of {compact} is that several figures also sum since they can be formed by adding/subtracting order 2 sub-hyper cubes. Issues like these go beyond this articles scope.\n\nThe following hypercubes serve special purposes;\n\n N : [i] = ∑ i m\nThis hypercube can be seen as the source of all numbers. A procedure called \"Dynamic numbering\" makes use of the isomorphism of every hypercube with this normal, changing the source, changes the hypercube. Usually these sources are limited to direct products of normal hypercubes or normal hyperbeams (defined as having possibly other orders along the various directions).\n\n 1 : [i] = 1\nThe hypercube that is usually added to change the here used \"analytic\" number range into the \"regular\" number range. Other constant hypercubes are of course multiples of this one.\n\nBased on XML, the file format Xml-Hypercubes is developed to describe various hypercubes to ensure human readability as well as programmatical usability. Besides full listings the format offers the ability to invoke mentioned constructions (amongst others)\n\n\n\n"}
{"id": "44752364", "url": "https://en.wikipedia.org/wiki?curid=44752364", "title": "Mara Neusel", "text": "Mara Neusel\n\nMara Dicle Neusel (May 14, 1964 – September 5, 2014) was a mathematician, author, teacher and an advocate for women in mathematics. The focus of her mathematical work was on invariant theory, which can be briefly described at the study of group actions and their fixed points.\n\nMara Neusel was born in Stuttgart, Germany, one of two children of Günter and Aylâ (Helvacioglu) Neusel. In 2001 she became the fourth woman to earn the advanced degree \"venia legendi\" (Habilitation) in mathematics from the University of Göttingen, following in the footsteps of the first woman mathematician to be awarded the venia legend from Göttingen in 1919, Emmy Noether.\n\nProfessor Neusel was the author of a research monograph, an advanced undergraduate text, and a memoir for the American Mathematical Society: \"Invariant Theory and Finite Groups\",\"Invariant Theory\", and \"Inverse Invariant Theory and Steenrod Operations\". The exposition in the text \"Invariant Theory\" \"stands out by its masterly clarity, comprehensiveness, profundity, and didactical disposition.\" Neusel served on the editorial boards of \"Advances in Pure Mathematics\" and the \"International Journal of Mathematics and Applied Statistics\". A tireless advocate for girls and women in mathematics, Dr. Neusel established \"Emmy Noether High School Mathematics Days\" in May 2003 which continues to be celebrated with workshops and mathematical competitions. She co-founded the Young Women in Mathematics group at Texas Tech and received a diversity grant to support the group.\n\nDr. Neusel began her career at Texas Tech University in 2002 as an associate professor and was promoted to full professor in 2009. She also held visiting appointments at Yale University, the University of Minnesota, and the University of Notre Dame.\n\nProfessor Neusel was had long term memberships in the American Mathematical Society and the Association for Women in Mathematics. Neusel co-organized a special session on \"Homological Algebra and Its Applications\" at the 2005 Meeting of the American Mathematical Society in Lubbock, Texas as well as a special session on \"Commutative Algebra and Algebraic Geometry\" at the 40th Anniversary Celebration of the Association for Women in Mathematics at Brown University in 2011.\n"}
{"id": "15743680", "url": "https://en.wikipedia.org/wiki?curid=15743680", "title": "Negacyclic convolution", "text": "Negacyclic convolution\n\nIn mathematics, negacyclic convolution is a convolution between two vectors \"a\" and \"b\".\n\nIt is also called skew circular convolution or wrapped convolution. It results from multiplication of a skew circulant matrix, generated by vector \"a\", with vector \"b\".\n\n"}
{"id": "624708", "url": "https://en.wikipedia.org/wiki?curid=624708", "title": "Persistence of a number", "text": "Persistence of a number\n\nIn mathematics, the persistence of a number is the number of times one must apply a given operation to an integer before reaching a fixed point at which the operation no longer alters the number.\n\nUsually, this involves additive or multiplicative persistence of an integer, which is how often one has to replace the number by the sum or product of its digits until one reaches a single digit. Because the numbers are broken down into their digits, the additive or multiplicative persistence depends on the radix. In the remainder of this article, base ten is assumed.\n\nThe single-digit final state reached in the process of calculating an integer's additive persistence is its digital root. Put another way, a number's additive persistence counts how many times we must sum its digits to arrive at its digital root.\n\nThe additive persistence of 2718 is 2: first we find that 2 + 7 + 1 + 8 = 18, and then that 1 + 8 = 9. The multiplicative persistence of 39 is 3, because it takes three steps to reduce 39 to a single digit: 39 → 27 → 14 → 4. Also, 39 is the smallest number of multiplicative persistence 3.\n\nFor a radix of 10, there is thought to be no number with a multiplicative persistence > 11: this is known to be true for numbers up to 10. The smallest numbers with persistence 0, 1, ... are:\nThe search for these numbers can be sped up by using additional properties of the decimal digits of these record-breaking numbers. These digits must be sorted, and except for the first two digits, all digits must be 7, 8, or 9. There are also additional restrictions on the first two digits.\nBased on these restrictions, the number of candidates for \"n\"-digit numbers with record-breaking persistence is only proportional to the square of \"n\", a tiny fraction of all \"n\"-digit numbers. However, any number that is missing from the sequence above would have multiplicative persistence > 11; such numbers are believed not to exist, and would need to have over 200 digits if they do exist.\n\nThe additive persistence of a number, however, can become arbitrarily large (proof: For a given number formula_1, the persistence of the number consisting of formula_1 repetitions of the digit 1 is 1 higher than that of formula_1). The smallest numbers of additive persistence 0, 1, ... are:\nThe next number in the sequence (the smallest number of additive persistence 5) is 2 × 10 − 1 (that is, 1 followed by 2222222222222222222222 9's). For any fixed base, the sum of the digits of a number is proportional to its logarithm; therefore, the additive persistence is proportional to the iterated logarithm. More about the additive persistence of a number can be found here.\n\n"}
{"id": "4786797", "url": "https://en.wikipedia.org/wiki?curid=4786797", "title": "Pointwise", "text": "Pointwise\n\nIn mathematics, the qualifier pointwise is used to indicate that a certain property is defined by considering each value formula_1 of some function formula_2 An important class of pointwise concepts are the \"pointwise operations\" – operations defined on functions by applying the operations to function values separately for each point in the domain of definition. Important relations can also be defined pointwise.\n\nExamples include\n\nwhere formula_4.\n\nSee pointwise product, scalar.\n\nPointwise operations inherit such properties as associativity, commutativity and distributivity from corresponding operations on the codomain. An example of an operation on functions which is \"not\" pointwise is convolution.\n\nBy taking some algebraic structure formula_5 in the place of formula_6, we can turn the set of all functions formula_7 to the carrier set of formula_5 into an algebraic structure of the same type in an analogous way.\n\nComponentwise operations are usually defined on vectors, where vectors are elements of the set formula_9 for some natural number formula_10 and some field formula_11. If we denote the formula_12-th component of any vector formula_13 as formula_14, then componentwise addition is formula_15.\n\nComponentwise operations can be defined on matrices. Matrix addition, where formula_16 is a componentwise operation while matrix multiplication is not.\n\nA tuple can be regarded as a function, and a vector is a tuple. Therefore, any vector formula_13 corresponds to the function formula_18 such that formula_19, and any componentwise operation on vectors is the pointwise operation on functions corresponding to those vectors.\n\nIn order theory it is common to define a pointwise partial order on functions. With \"A\", \"B\" posets, the set of functions \"A\" → \"B\" can be ordered by \"f\" ≤ \"g\" if and only if (∀\"x\" ∈ A) \"f\"(\"x\") ≤ \"g\"(\"x\"). Pointwise orders also inherit some properties of the underlying posets. For instance if A and B are continuous lattices, then so is the set of functions \"A\" → \"B\" with pointwise order. Using the pointwise order on functions one can concisely define other important notions, for instance:\n\n\nAn example of infinitary pointwise relation is pointwise convergence of functions — a sequence of functions \nwith\nconverges pointwise to a function formula_22 if for each formula_23 in formula_7\n\n\"For order theory examples:\"\n"}
{"id": "29295573", "url": "https://en.wikipedia.org/wiki?curid=29295573", "title": "Quantitative analysis (finance)", "text": "Quantitative analysis (finance)\n\nQuantitative analysis is the use of models, or algorithms, to model risks in general, or to evaluate assets for investment. The process usually consists of searching vast databases for patterns, such as correlations among liquid assets or price-movement patterns (trend following or mean reversion). The resulting strategies may involve high-frequency trading.\n\nSome of the larger investment managers using quantitative analysis include Renaissance Technologies, Winton Group, D. E. Shaw & Co., AQR Capital Management, and Two Sigma Investments.\n\n"}
{"id": "10299080", "url": "https://en.wikipedia.org/wiki?curid=10299080", "title": "Reduced residue system", "text": "Reduced residue system\n\nAny subset \"R\" of the integers is called a reduced residue system modulo \"n\" if:\n\n\nHere formula_1 denotes Euler's totient function.\n\nA reduced residue system modulo \"n\" can be formed from a complete residue system modulo \"n\" by removing all integers not relatively prime to \"n\". For example, a complete residue system modulo 12 is {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}. The so-called totatives 1, 5, 7 and 11 are the only integers in this set which are relatively prime to 12, and so the corresponding reduced residue system modulo 12 is {1,5,7,11}. The cardinality of this set can be calculated with the totient function: formula_2. Some other reduced residue systems modulo 12 are:\n\n\n\n\n\n"}
{"id": "185076", "url": "https://en.wikipedia.org/wiki?curid=185076", "title": "Relevance logic", "text": "Relevance logic\n\nRelevance logic, also called relevant logic, is a kind of non-classical logic requiring the antecedent and consequent of implications to be relevantly related. They may be viewed as a family of substructural or modal logics. (It is generally, but not universally, called \"relevant logic\" by Australian logicians, and \"relevance logic\" by other English-speaking logicians.)\n\nRelevance logic aims to capture aspects of implication that are ignored by the \"material implication\" operator in classical truth-functional logic, namely the notion of relevance between antecedent and conditional of a true implication. This idea is not new: C. I. Lewis was led to invent modal logic, and specifically strict implication, on the grounds that classical logic grants paradoxes of material implication such as the principle that a falsehood implies any proposition. Hence \"if I'm a donkey, then two and two is four\" is true when translated as a material implication, yet it seems intuitively false since a true implication must tie the antecedent and consequent together by some notion of relevance. And whether or not I'm a donkey seems in no way relevant to whether two and two is four.\n\nHow does relevance logic formally capture a notion of relevance? In terms of a syntactical constraint for a propositional calculus, it is necessary, but not sufficient, that premises and conclusion share atomic formulae (formulae that do not contain any logical connectives). In a predicate calculus, relevance requires sharing of variables and constants between premises and conclusion. This can be ensured (along with stronger conditions) by, e.g., placing certain restrictions on the rules of a natural deduction system. In particular, a Fitch-style natural deduction can be adapted to accommodate relevance by introducing tags at the end of each line of an application of an inference indicating the premises relevant to the conclusion of the inference. Gentzen-style sequent calculi can be modified by removing the weakening rules that allow for the introduction of arbitrary formulae on the right or left side of the sequents.\n\nA notable feature of relevance logics is that they are paraconsistent logics: the existence of a contradiction will not cause \"explosion\". This follows from the fact that a conditional with a contradictory antecedent that does not share any propositional or predicate letters with the consequent cannot be true (or derivable).\n\nRelevance logic was proposed in 1928 by Russian Soviet philosopher Ivan E. Orlov (1886–circa 1936) in his strictly mathematical paper \"The Logic of Compatibility of Propositions\" published in Matematicheskii Sbornik.\nThe basic idea of relevant implication appears in medieval logic, and some pioneering work was done by Ackermann,\nMoh,\nand Church\nin the 1950s. Drawing on them, Nuel Belnap and Alan Ross Anderson (with others) wrote the \"magnum opus\" of the subject, \"Entailment: The Logic of Relevance and Necessity\" in the 1970s (the second volume being published in the nineties). They focused on both systems of entailment and systems of relevance, where implications of the former kinds are supposed to be both relevant and necessary.\n\nThe early developments in relevance logic focused on the stronger systems. The development of the Routley-Meyer semantics brought out a range of weaker logics. The weakest of these logics is the relevance logic B. It is axiomatized with the following axioms and rules.\nThe rules are the following.\nStronger logics can be obtained by adding any of the following axioms.\nThere are some notable logics stronger than B that can be obtained by adding axioms to B as follows.\n\nThe standard model theory for relevance logics is the Routley-Meyer ternary-relational semantics developed by Richard Routley and Robert Meyer. A Routley-Meyer frame F for a propositional language is a quadruple (W,R,*,0), where W is a non-empty set, R is a ternary relation on W, and * is a function from W to W, and formula_31. A Routley-Meyer model M is a Routley-Meyer frame F together with a valuation, formula_32, that assigns a truth value to each atomic proposition relative to each point formula_33. There are some conditions placed on Routley-Meyer frames. Define formula_34 as formula_35.\nWrite formula_46 and formula_47 to indicate that the formula formula_48 is true, or not true, respectively, at point formula_49 in formula_50. \nOne final condition on Routley-Meyer models is the hereditariness condition.\nBy an inductive argument, hereditariness can be shown to extend to complex formulas, using the truth conditions below.\n\nThe truth conditions for complex formulas are as follows.\n\nA formula formula_48 holds in a model formula_50 just in case formula_67. A formula formula_48 holds on a frame formula_69 iff A holds in every model formula_70. A formula formula_48 is valid in a class of frames iff A holds on every frame in that class. \nThe class of all Routley-Meyer frames satisfying the above conditions validates that relevance logic B. One can obtain Routley-Meyer frames for other relevance logics by placing appropriate restrictions on R and on *. These conditions are easier to state using some standard definitions. Let formula_72 be defined as formula_73, and let formula_74 be defined as formula_75. Some of the frame conditions and the axioms they validate are the following.\n\nThe last two conditions validate forms of weakening that relevance logics were originally developed to avoid. They are included to show the flexibility of the Routley-Meyer models.\n\nOperational models for negation-free fragments of relevance logics were developed by Alasdair Urquhart in his PhD thesis and in subsequent work. The intuitive idea behind the operational models is that points in a model are pieces of information, and combining information supporting a conditional with the information supporting its antecedent yields some information that supports the consequent. Since the operational models do not generally interpret negation, this section will consider only languages with a conditional, conjunction, and disjunction.\n\nAn operational frame formula_69 is a triple formula_77, where formula_78 is a non-empty set, formula_79, and formula_80 is a binary operation on formula_78. Frames have conditions, some of which may be dropped to model different logics. The conditions Urquhart proposed to model the conditional of the relevance logic R are the following.\nUnder these conditions, the operational frame is a join semilattice.\n\nAn operational model formula_50 is a frame formula_69 with a valuation formula_88 that maps pairs of points and atomic propositions to truth values, T or F. formula_88 can be extended to a valuation formula_32 on complex formulas as follows.\n\nA formula formula_48 holds in a model formula_50 iff formula_67. A formula formula_48 is valid in a class of models formula_101 iff it holds in each model formula_102. \n\nThe conditional fragment of R is sound and complete with respect to the class of semilattice models. The logic with conjunction and disjunction is properly stronger than the conditional, conjunction, disjunction fragment of R. In particular, the formula formula_103 is valid for the operational models but it is invalid in R. The logic generated by the operational models for R has a complete axiomatic proof system, due Kit Fine and to Gerald Charlwood. Charlwood also provided a natural deduction system for the logic, which he proved equivalent to the axiomatic system. Charlwood showed that his natural deduction system is equivalent to a system provided by Dag Prawitz.\n\nThe operational semantics can be adapted to model the conditional of E by adding a non-empty set of worlds formula_104 and an accessibility relation formula_105 on formula_106 to the frames. The accessibility relation is required to be reflexive and transitive, to capture the idea that E's conditional has an S4 necessity. The valuations then map triples of atomic propositions, points, and worlds to truth values. The truth condition for the conditional is changed to the following. \n\nThe operational semantics can be adapted to model the conditional of T by adding a relation formula_105 on formula_109. The relation is required to obey the following conditions.\nThe truth condition for the conditional is changed to the following.\n\nThere are two ways to model the contraction-less relevance logics TW and RW with the operational models. The first way is to drop the condition that formula_82. The second way is to keep the semilattice conditions on frames and add a binary relation, formula_118, of disjointness to the frame. For these models, the truth conditions for the conditional is changed to the following, with the addition of the ordering in the case of TW.\n\nUrquhart showed that the semilattice logic for R is properly stronger than the positive fragment of R. Lloyd Humberstone provided an enrichment of the operational models that permitted a different truth condition for disjunction. The resulting class of models generates exactly the positive fragment of R.\n\nSome relevance logics can be given algebraic models, such as the logic R. The algebraic structures for R are de Morgan monoids, which are sextuples formula_120 where\nThe operation formula_137 interpreting the conditional of R is defined as formula_138. \nA de Morgan monoid is a residuated lattice, obeying the following residuation condition. \n\nAn interpretation formula_140 is a homomorphism from the propositional language to a de Morgan monoid formula_50 such that \n\nGiven a de Morgan monoid formula_50 and an interpretation formula_140, one can say that formula formula_48 holds on formula_140 just in case formula_151. A formula formula_48 is valid just in case it holds on all interpretations on all de Morgan monoids. The logic R is sound and complete for de Morgan monoids.\n\n\n\n"}
{"id": "36758654", "url": "https://en.wikipedia.org/wiki?curid=36758654", "title": "Rigidity matroid", "text": "Rigidity matroid\n\nIn the mathematics of structural rigidity, a rigidity matroid is a matroid that describes the number of degrees of freedom of an undirected graph with rigid edges of fixed lengths, embedded into Euclidean space. In a rigidity matroid for a graph with \"n\" vertices in \"d\"-dimensional space, a set of edges that defines a subgraph with \"k\" degrees of freedom has matroid rank \"dn\" − \"k\". A set of edges is independent if and only if, for every edge in the set, removing the edge would increase the number of degrees of freedom of the remaining subgraph.\n\nA \"framework\" is an undirected graph, embedded into \"d\"-dimensional Euclidean space by providing a \"d\"-tuple of Cartesian coordinates for each vertex of the graph. From a framework with \"n\" vertices and \"m\" edges, one can define a matrix with \"m\" rows and \"nd\" columns, an expanded version of the incidence matrix of the graph called the \"rigidity matrix\". In this matrix, the entry in row \"e\" and column (\"v\",\"i\") is zero if \"v\" is not an endpoint of edge \"e\". If, on the other hand, edge \"e\" has vertices \"u\" and \"v\" as endpoints, then the value of the entry is the difference between the \"i\"th coordinates of \"v\" and \"u\".\n\nThe rigidity matroid of the given framework is a linear matroid that has as its elements the edges of the graph. A set of edges is independent, in the matroid, if it corresponds to a set of rows of the rigidity matrix that is linearly independent. A framework is called \"generic\" if the coordinates of its vertices are algebraically independent real numbers. Any two generic frameworks on the same graph \"G\" determine the same rigidity matroid, regardless of their specific coordinates. This is the (\"d\"-dimensional) rigidity matroid of \"G\".\n\nA \"load\" on a framework is a system of forces on the vertices (represented as vectors). A \"stress\" is a special case of a load, in which equal and opposite forces are applied to the two endpoints of each edge (which may be imagined as a spring) and the forces formed in this way are added at each vertex. Every stress is an \"equilibrium load\", a load that does not impose any translational force on the whole system (the sum of its force vectors is zero) nor any rotational force. A linear dependence among the rows of the rigidity matrix may be represented as a \"self-stress\", an assignment of equal and opposite forces to the endpoints of each edge that is not identically zero but that adds to zero at every vertex. Thus, a set of edges forms an independent set in the rigidity matroid if and only if it has no self-stress.\n\nThe vector space of all possible loads, on a system of \"n\" vertices, has dimension \"dn\", among which the equilibrium loads form a subspace of dimension\nformula_1. An independent set in the rigidity matroid has a system of equilibrium loads whose dimension equals the cardinality of the set, so the maximum rank that any set in the matroid can have is formula_1. If a set has this rank, it follows that its set of stresses is the same as the space of equilibrium loads. Alternatively and equivalently, in this case every equilibrium load on the framework may be \"resolved\" by a stress that generates an equal and opposite set of forces, and the framework is said to be statically rigid.\n\nIf the vertices of a framework are in a motion, then that motion may be described over small scales of distance by its gradient, a vector for each vertex specifying its speed and direction. The gradient describes a linearized approximation to the actual motion of the points, in which each point moves at constant velocity in a straight line. The gradient may be described as a row vector that has one real number coordinate for each pair formula_3 where formula_4 is a vertex of the framework and formula_5 is the index of one of the Cartesian coordinates of formula_6-dimensional space; that is, the dimension of the gradient is the same as the width of the rigidity matrix.\n\nIf the edges of the framework are assumed to be rigid bars that can neither expand nor contract (but can freely rotate) then any motion respecting this rigidity must preserve the lengths of the edges: the derivative of length, as a function of the time over which the motion occurs, must remain zero. This condition may be expressed in linear algebra as a constraint that the gradient vector of the motion of the vertices must have zero inner product with the row of the rigidity matrix that represents the given edge. Thus, the family of gradients of (infinitesimally) rigid motions is given by the nullspace of the rigidity matrix. For frameworks that are not in generic position, it is possible that some infinitesimally rigid motions (vectors in the nullspace of the rigidity matrix) are not the gradients of any continuous motion, but this cannot happen for generic frameworks.\n\nA rigid motion of the framework is a motion such that, at each point in time, the framework is congruent to its original configuration. Rigid motions include translations and rotations of Euclidean space; the gradients of rigid motions form a linear space having the translations and rotations as bases, of dimension formula_7, which must always be a subspace of the nullspace of the rigidity matrix.\nBecause the nullspace always has at least this dimension, the rigidity matroid can have rank at most formula_1, and when it does have this rank the only motions that preserve the lengths of the edges of the framework are the rigid motions. In this case the framework is said to be first-order (or infinitesimally) rigid. More generally, an edge formula_9 belongs to the matroid closure operation of a set formula_10 if and only if there does not exist a continuous motion of the framework that changes the length of formula_9 but leaves the lengths of the edges in formula_10 unchanged.\n\nAlthough defined in different terms (column vectors versus row vectors, or forces versus motions) static rigidity and first-order rigidity reduce to the same properties of the underlying matrix and therefore coincide with each other. In two dimensions, the generic rigidity matroid also describes the number of degrees of freedom of a different kind of motion, in which each edge is constrained to stay parallel to its original position rather than being constrained to maintain the same length; however, the equivalence between rigidity and parallel motion breaks down in higher dimensions.\n\nA framework has a \"unique realization\" in \"d\"-dimensional space if every placement of the same graph with the same edge lengths is congruent to it. Such a framework must necessarily be rigid, because otherwise there exists a continuous motion bringing it to a non-congruent placement with the same edge lengths, but unique realizability is stronger than rigidity. For instance, the diamond graph (two triangles sharing an edge) is rigid in two dimensions, but it is not uniquely realizable because it has two different realizations, one in which the triangles are on opposite sides of the shared edge and one in which they are both on the same side. Uniquely realizable graphs are important in applications that involve reconstruction of shapes from distances, such as triangulation in land surveying, the determination of the positions of the nodes in a wireless sensor network, and the reconstruction of conformations of molecules via nuclear magnetic resonance spectroscopy.\n\nBruce Hendrickson defined a graph to be \"redundantly rigid\" if it remains rigid after removing any one of its edges. In matroidal terms, this means that the rigidity matroid has the full rank formula_1 and that the matroid does not have any coloops. Hendrickson\nproved that every uniquely realizable framework (with generic edge lengths) is either a complete graph or a formula_14-vertex-connected, redundantly rigid graph, and he conjectured that this is an exact characterization of the uniquely realizable frameworks. The conjecture is true for one and two dimensions; in the one-dimensional case, for instance, a graph is uniquely realizable if and only if it is connected and bridgeless. However, Henrickson's conjecture is false for three or more dimensions. For frameworks that are not generic, it is NP-hard to determine whether a given framework is uniquely realizable.\n\n define a graph as being formula_15-sparse if every nonempty subgraph with formula_16 vertices has at most formula_17 edges, and formula_15-tight if it is formula_15-sparse and has exactly formula_17 edges. From the consideration of loads and stresses it can be seen that a set of edges that is independent in the rigidity matroid forms a formula_21-sparse graph, for if not there would exist a subgraph whose number of edges would exceed the dimension of its space of equilibrium loads, from which it follows that it would have a self-stress.\nBy similar reasoning, a set of edges that is both independent and rigid forms a formula_21-tight graph. For instance, in one dimension, the independent sets form the edge sets of forests, (1,1)-sparse graphs, and the independent rigid sets form the edge sets of trees, (1,1)-tight graphs. In this case the rigidity matroid of a framework is the same as the graphic matroid of the corresponding graph.\n\nIn two dimensions, showed that the same characterization is true: the independent sets form the edge sets of (2,3)-sparse graphs and the independent rigid sets form the edge sets of (2,3)-tight graphs. Based on this work the (2,3)-tight graphs (the graphs of minimally rigid generic frameworks in two dimensions) have come to be known as Laman graphs. The family of Laman graphs on a fixed set of formula_16 vertices forms the set of bases of the rigidity matroid of a complete graph, and more generally for every graph formula_24 that forms a rigid framework in two dimensions, the spanning Laman subgraphs of formula_24 are the bases of the rigidity matroid of formula_24.\n\nHowever, in higher dimensions not every formula_21-tight graph is minimally rigid, and characterizing the minimally rigid graphs (the bases of the rigidity matroid of the complete graph) is an important open problem.\n"}
{"id": "2547402", "url": "https://en.wikipedia.org/wiki?curid=2547402", "title": "Saturation (graph theory)", "text": "Saturation (graph theory)\n\nLet formula_1 be a graph and formula_2 a matching in formula_3. A vertex formula_4 is said to be saturated by formula_2 if there is an edge in formula_2 incident to formula_7. A vertex formula_4 with no such edge is said to be unsaturated by formula_2. We also say that formula_2 saturates formula_7.\n\n"}
{"id": "24824751", "url": "https://en.wikipedia.org/wiki?curid=24824751", "title": "Self-dissimilarity", "text": "Self-dissimilarity\n\nSelf-dissimilarity is a measure of complexity defined in a series of papers by David Wolpert and William G. Macready.\nThe degrees of self-dissimilarity between the patterns of a system observed at various scales (e.g. the average matter density of a physical body for volumes at different orders of magnitude) constitute a complexity \"signature\" of that system.\n\n"}
{"id": "58607583", "url": "https://en.wikipedia.org/wiki?curid=58607583", "title": "Space-filling polyhedron", "text": "Space-filling polyhedron\n\nA space-filling polyhedron is a polyhedron that can be used to fill all of three-dimensional space via translations, rotations and/or reflections. Any periodic tiling or honeycomb of three-space can in fact be generated by translating a primitive cell polyhedron.\n\n"}
{"id": "49481270", "url": "https://en.wikipedia.org/wiki?curid=49481270", "title": "System U", "text": "System U\n\nIn mathematical logic, System U and System U are pure type systems, i.e. special forms of a typed lambda calculus with an arbitrary number of sorts, axioms and rules (or dependencies between the sorts). They were both proved inconsistent by Jean-Yves Girard in 1972. This result led to the realization that Martin-Löf's original 1971 type theory was inconsistent as it allowed the same \"Type in Type\" behaviour that Girard's paradox exploits.\n\nSystem U is defined as a pure type system with\n\nSystem U is defined the same with the exception of the formula_4 rule.\n\nThe sorts formula_5 and formula_6 are conventionally called “Type” and “Kind”, respectively; the sort formula_7 doesn't have a specific name. The two axioms describe the containment of types in kinds (formula_8) and kinds in formula_7 (formula_10). Intuitively, the sorts describe a hierarchy in the \"nature\" of the terms.\n\nThe rules govern the dependencies between the sorts: formula_21 says that values may depend on values (functions), formula_22 allows values to depend on types (polymorphism), formula_23 allows types to depend on types (type operators), and so on.\n\nThe definitions of System U and U allow the assignment of polymorphic kinds to \"generic constructors\" in analogy to polymorphic types of terms in classical polymorphic lambda calculi, such as System F. An example of such a generic constructor might be (where \"k\" denotes a kind variable)\n\nThis mechanism is sufficient to construct a term with the type formula_25, which implies that every type is inhabited. By the Curry–Howard correspondence, this is equivalent to all logical propositions being provable, which makes the system inconsistent.\n\nGirard's paradox is the type-theoretic analogue of Russell's paradox in set theory.\n\n"}
{"id": "32122937", "url": "https://en.wikipedia.org/wiki?curid=32122937", "title": "Systems of Logic Based on Ordinals", "text": "Systems of Logic Based on Ordinals\n\nSystems of Logic Based on Ordinals was the PhD dissertation of the mathematician Alan Turing.\n\nTuring’s thesis is not about a new type of formal logic, nor was he interested in so-called ‘ranked logic’ systems derived from ordinal or relative numbering, in which comparisons can be made between truth-states on the basis of relative veracity. Instead, Turing investigated the possibility of resolving the Godelian incompleteness condition using Cantor’s method of infinites. This condition can be stated thus- in all systems with finite sets of axioms, an exclusive-or condition applies to expressive power and provability; ie one can have power and no proof, or proof and no power, but not both.\n\nThe thesis is an exploration of formal mathematical systems after Gödel's theorem. Gödel showed for that any formal system S powerful enough to represent arithmetic, there is a theorem G which is true but the system is unable to prove. G could be added as an additional axiom to the system in place of a proof. However this would create a new system S' with its own unprovable true theorem G', and so on. Turing's thesis considers iterating the process to infinity, creating a system with an infinite set of axioms.\n\nThe thesis was completed at Princeton under Alonzo Church and was a classic work in mathematics which introduced the concept of ordinal logic. \n\nMartin Davis states that although Turing's use of a computing oracle is not a major focus of the dissertation, it has proven to be highly influential in theoretical computer science, e.g. in the polynomial time hierarchy.\n\n"}
{"id": "20240388", "url": "https://en.wikipedia.org/wiki?curid=20240388", "title": "The Princeton Companion to Mathematics", "text": "The Princeton Companion to Mathematics\n\nThe Princeton Companion to Mathematics is a book, edited by Timothy Gowers with associate editors June Barrow-Green and Imre Leader, and published in 2008 by Princeton University Press (). It provides an extensive overview of mathematics, and is noted for the high caliber of the contributors. The book was a 2011 winner of the Euler Book Prize of the Mathematical Association of America, given annually to \"an outstanding book about mathematics\".\n\nThe book concentrates primarily on modern pure mathematics rather than applied mathematics, although it does also cover both applications of mathematics and the mathematics that relates to those applications;\nit provides a broad overview of the significant ideas and developments in research mathematics. It is organized into eight parts:\nDespite its length, the range of topics included is selective rather than comprehensive: some important established topics such as diophantine approximation are omitted, transcendental number theory, differential geometry, and cohomology get short shrift, and the most recent frontiers of research are also generally not included.\n\nThe book's authors have attempted to keep their work accessible by forgoing abstraction and technical nomenclature as much as possible and by making heavy use of concrete examples and illustrations. Compared to the concise and factual coverage of mathematics in sources such as Wikipedia and MathWorld, the articles in the \"Princeton Companion\" are intended to be more reflective and discursive, and to convey the beauty and depth of modern mathematics. Quoting a passage from Bertrand Russell that \"Pure Mathematics is the class of all propositions of the form \"p\" implies \"q\"\", the editor of the \"Companion\" states that it \"is about everything that Russell’s definition leaves out.\"\n\nThe core sections of the \"Companion\" are aimed primarily at readers who are already familiar with mathematics at the undergraduate level. Much of the rest of the book, such as its collection of biographies, would be accessible to a mathematically inclined high school student, and there is enough depth of coverage in the book to interest even professional research mathematicians. Reviewer Jonathan Borwein summarizes the audience for this book broadly:\nThe contributors to \"The Princeton Companion to Mathematics\" consist of 133 of the world's best mathematicians. Timothy Gowers, its editor, is the recipient of the Fields Medal, considered to be the top honor in mathematics. Other contributors include Fields medalists Michael Atiyah, Alain Connes, Charles Fefferman, and Terence Tao, and well-known mathematicians Noga Alon, George Andrews, Béla Bollobás, John P. Burgess, Clifford Cocks, Ingrid Daubechies, Persi Diaconis, Jordan Ellenberg, Oded Goldreich, Andrew Granville, Jeremy Gray, Frank Kelly, Sergiu Klainerman, Jon Kleinberg, János Kollár, Peter Lax, Dusa McDuff, Barry Mazur, Carl Pomerance, Eleanor Robson, Peter Sarnak, Madhu Sudan, Clifford Taubes, and Avi Wigderson. Among the historians who contributed to it are Charles C. Gillispie, Ivor Grattan-Guinness, Jeremy Gray, Niccolò Guicciardini, , Eberhard Knobloch, Karen Hunger Parshall, Eleanor Robson, and .\n\nGowers and the \"Princeton Companion\" were the 2011 winners of the Euler Book Prize of the Mathematical Association of America, given annually to \"an outstanding book about mathematics\".\n\nThe \"Princeton Companion\" was also listed as an outstanding title by \"\", a publication of the American Library Association, in 2009.\n\n\n"}
{"id": "349755", "url": "https://en.wikipedia.org/wiki?curid=349755", "title": "Uniform norm", "text": "Uniform norm\n\nIn mathematical analysis, the uniform norm (or sup norm) assigns to real- or complex-valued bounded functions \"f\" defined on a set \"S\" the non-negative number\n\nThis norm is also called the supremum norm, the Chebyshev norm, or the infinity norm. The name \"uniform norm\" derives from the fact that a sequence of functions formula_2 converges to f under the metric derived from the uniform norm if and only if formula_3 converges to formula_4 uniformly.\n\nIf we allow unbounded functions, this formula does not yield a norm or metric in a strict sense, although the obtained so-called extended metric still allows one to define a topology on the function space in question.\n\nIf \"f\" is a continuous function on a closed interval, or more generally a compact set, then it is bounded and the supremum in the above definition is attained by the Weierstrass extreme value theorem, so we can replace the supremum by the maximum. In this case, the norm is also called the maximum norm.\nIn particular, for the case of a vector formula_5 in finite dimensional coordinate space, it takes the form\n\nThe reason for the subscript \"∞\" is that whenever \"f\" is continuous\n\nwhere\n\nwhere \"D\" is the domain of \"f\" (and the integral amounts to a sum if \"D\" is a discrete set).\n\nThe binary function\n\nis then a metric on the space of all bounded functions (and, obviously, any of its subsets) on a particular domain. A sequence { \"f\" : \"n\" = 1, 2, 3, ... } converges uniformly to a function \"f\" if and only if\n\nWe can define closed sets and closures of sets with respect to this metric topology; closed sets in the uniform norm are sometimes called \"uniformly closed\" and closures \"uniform closures\". The uniform closure of a set of functions A is the space of all functions that can be approximated by a sequence of uniformly-converging functions on A. For instance, one restatement of the Stone–Weierstrass theorem is that the set of all continuous functions on formula_11 is the uniform closure of the set of polynomials on formula_11.\n\nFor complex continuous functions over a compact space, this turns it into a C* algebra.\n\n"}
{"id": "46578848", "url": "https://en.wikipedia.org/wiki?curid=46578848", "title": "Weyl distance function", "text": "Weyl distance function\n\nIn combinatorial geometry, the Weyl distance function is a function that behaves in some ways like the distance function of a metric space, but instead of taking values in the positive real numbers, it takes values in a group of reflections, called the Weyl group (named for Hermann Weyl). This distance function is defined on the collection of chambers in a mathematical structure known as a building, and its value on a pair of chambers a minimal sequence of reflections (in the Weyl group) to go from one chamber to the other. An adjacent sequence of chambers in a building is known as a gallery, so the Weyl distance function is a way of encoding the information of a minimal gallery between two chambers. In particular, the number of reflections to go from one chamber to another coincides with the length of the minimal gallery between the two chambers, and so gives a natural metric (the gallery metric) on the building. According to , the Weyl distance function is something like a geometric vector: it encodes both the magnitude (distance) between two chambers of a building, as well as the direction between them.\n\nWe record here definitions from . Let be the Coxeter complex associated to a group \"W\" generated by a set of reflections \"S\". The vertices of are the elements of \"W\", and the chambers of the complex are the cosets of \"S\" in \"W\". The vertices of each chamber can be \"colored\" in a one-to-one manner by the elements of \"S\" so that no adjacent vertices of the complex receive the same color. This coloring, although essentially canonical, is not quite unique. The coloring of a given chamber is not uniquely determined by its realization as a coset of \"S\". But once the coloring of a single chamber has been fixed, the rest of the Coxeter complex is uniquely colorable. Fix such a coloring of the complex.\n\nA gallery is a sequence of adjacent chambers\nBecause these chambers are adjacent, any consecutive pair formula_2 of chambers share all but one vertex. Denote the color of this vertex by formula_3. The Weyl distance function between formula_4 and formula_5 is defined by\nIt can be shown that this does not depend on the choice of gallery connecting formula_4 and formula_5.\n\nNow, a building is a simplicial complex that is organized into apartments, each of which is a Coxeter complex (satisfying some coherence axioms). Buildings are colorable, since the Coxeter complexes that make them up are colorable. A coloring of a building is associated with a uniform choice of Weyl group for the Coxeter complexes that make it up, allowing it to be regarded as a collection of words on the set of colors with relations. Now, if formula_9 is a gallery in a building, then define the Weyl distance between formula_4 and formula_5 by\nwhere the formula_3 are as above. As in the case of Coxeter complexes, this does not depend on the choice of gallery connecting the chambers formula_4 and formula_5.\n\nThe gallery distance formula_16 is defined as the minimal word length needed to express formula_17 in the Weyl group. Symbolically, formula_18.\n\nThe Weyl distance function satisfies several properties that parallel those of distance functions in metric spaces:\n\nIn addition to the properties listed above, the Weyl distance function satisfies the following property:\n\nIn fact, this property together with the two listed in the \"Properties\" section furnishes an abstract \"metrical\" characterization of buildings, as follows. Suppose that (\"W\",\"S\") is a Coxeter system consisting of a Weyl group \"W\" generated by reflections belonging to the subset \"S\". A building of type (\"W\",\"S\") is a pair consisting of a set \"C\" of \"chambers\" and a function:\nsuch that the three properties listed above are satisfied. Then \"C\" carries the canonical structure of a building, in which is the Weyl distance function.\n\n"}
{"id": "4066001", "url": "https://en.wikipedia.org/wiki?curid=4066001", "title": "Ω-consistent theory", "text": "Ω-consistent theory\n\nIn mathematical logic, an ω-consistent (or omega-consistent, also called numerically segregative) theory is a theory (collection of sentences) that is not only (syntactically) consistent (that is, does not prove a contradiction), but also avoids proving certain infinite combinations of sentences that are intuitively contradictory. The name is due to Kurt Gödel, who introduced the concept in the course of proving the incompleteness theorem.\n\nA theory \"T\" is said to interpret the language of arithmetic if there is a translation of formulas of arithmetic into the language of \"T\" so that \"T\" is able to prove the basic axioms of the natural numbers under this translation.\n\nA \"T\" that interprets arithmetic is ω-inconsistent if, for some property \"P\" of natural numbers (defined by a formula in the language of \"T\"), \"T\" proves \"P\"(0), \"P\"(1), \"P\"(2), and so on (that is, for every standard natural number \"n\", \"T\" proves that \"P\"(\"n\") holds), but \"T\" also proves that there is some natural number \"n\" (necessarily nonstandard) such that \"P\"(\"n\") \"fails\". This may not generate a contradiction within \"T\" because \"T\" may not be able to prove for any \"specific\" value of \"n\" that \"P\"(\"n\") fails, only that there \"is\" such an \"n\".\n\n\"T\" is ω-consistent if it is \"not\" ω-inconsistent.\n\nThere is a weaker but closely related property of Σ-soundness. A theory \"T\" is Σ-sound (or 1-consistent, in another terminology) if every Σ-sentence provable in \"T\" is true in the standard model of arithmetic N (i.e., the structure of the usual natural numbers with addition and multiplication).\nIf \"T\" is strong enough to formalize a reasonable model of computation, Σ-soundness is equivalent to demanding that whenever \"T\" proves that a computer program \"C\" halts, then \"C\" actually halts. Every ω-consistent theory is Σ-sound, but not vice versa.\n\nMore generally, we can define an analogous concept for higher levels of the arithmetical hierarchy. If Γ is a set of arithmetical sentences (typically Σ for some \"n\"), a theory \"T\" is Γ-sound if every Γ-sentence provable in \"T\" is true in the standard model. When Γ is the set of all arithmetical formulas, Γ-soundness is called just (arithmetical) soundness.\nIf the language of \"T\" consists \"only\" of the language of arithmetic (as opposed to, for example, set theory), then a sound system is one whose model can be thought of as the set ω, the usual set of mathematical natural numbers. The case of general \"T\" is different, see ω-logic below.\n\nΣ-soundness has the following computational interpretation: if the theory proves that a program \"C\" using a Σ-oracle halts, then \"C\" actually halts.\n\nWrite PA for the theory Peano arithmetic, and Con(PA) for the statement of arithmetic that formalizes the claim \"PA is consistent\". Con(PA) could be of the form \"For every natural number \"n\", \"n\" is not the Gödel number of a proof from PA that 0=1\". (This formulation uses 0=1 instead of a direct contradiction; that gives the same result, because PA certainly proves ¬0=1, so if it proved 0=1 as well we would have a contradiction, and on the other hand, if PA proves a contradiction, then it proves anything, including 0=1.)\n\nNow, assuming PA is really consistent, it follows that PA + ¬Con(PA) is also consistent, for if it were not, then PA would prove Con(PA) (since an inconsistent theory proves every sentence), contradicting Gödel's second incompleteness theorem. However, PA + ¬Con(PA) is \"not\" ω-consistent. This is because, for any particular natural number \"n\", PA + ¬Con(PA) proves that \"n\" is not the Gödel number of a proof that 0=1 (PA itself proves that fact; the extra assumption ¬Con(PA) is not needed). However, PA + ¬Con(PA) proves that, for \"some\" natural number \"n\", \"n\" \"is\" the Gödel number of such a proof (this is just a direct restatement of the claim ¬Con(PA) ).\n\nIn this example, the axiom ¬Con(PA) is Σ, hence the system PA + ¬Con(PA) is in fact Σ-unsound, not just ω-inconsistent.\n\nLet \"T\" be PA together with the axioms \"c\" ≠ \"n\" for each natural number \"n\", where \"c\" is a new constant added to the language. Then \"T\" is arithmetically sound (as any nonstandard model of PA can be expanded to a model of \"T\"), but ω-inconsistent (as it proves formula_1, and \"c\" ≠ \"n\" for every number \"n\").\n\nΣ-sound ω-inconsistent theories using only the language of arithmetic can be constructed as follows. Let \"I\"Σ be the subtheory of PA with the induction schema restricted to Σ-formulas, for any \"n\" > 0. The theory \"I\"Σ is finitely axiomatizable, let thus \"A\" be its single axiom, and consider the theory \"T\" = \"I\"Σ + ¬\"A\". We can assume that \"A\" is an instance of the induction schema, which has the form\nIf we denote the formula\nby \"P\"(\"n\"), then for every natural number \"n\", the theory \"T\" (actually, even the pure predicate calculus) proves \"P\"(\"n\"). On the other hand, \"T\" proves the formula formula_4, because it is logically equivalent to the axiom ¬\"A\". Therefore, \"T\" is ω-inconsistent.\n\nIt is possible to show that \"T\" is Π-sound. In fact, it is Π-conservative over the (obviously sound) theory \"I\"Σ. The argument is more complicated (it relies on the provability of the Σ-reflection principle for \"I\"Σ in \"I\"Σ).\n\nLet ω-Con(PA) be the arithmetical sentence formalizing the statement \"PA is ω-consistent\". Then the theory PA + ¬ω-Con(PA) is unsound (Σ-unsound, to be precise), but ω-consistent. The argument is similar to the first example: a suitable version of the Hilbert-Bernays-Löb derivability conditions holds for the \"provability predicate\" ω-Prov(\"A\") = ¬ω-Con(PA + ¬\"A\"), hence it satisfies an analogue of Gödel's second incompleteness theorem.\n\nThe concept of theories of arithmetic whose integers are the true mathematical integers is captured by ω-logic. Let \"T\" be a theory in a countable language which includes a unary predicate symbol \"N\" intended to hold just of the natural numbers, as well as specified names 0, 1, 2, …, one for each (standard) natural number (which may be separate constants, or constant terms such as 0, 1, 1+1, 1+1+1, …, etc.). Note that \"T\" itself could be referring to more general objects, such as real numbers or sets; thus in a model of \"T\" the objects satisfying \"N\"(\"x\") are those that \"T\" interprets as natural numbers, not all of which need be named by one of the specified names.\n\nThe system of ω-logic includes all axioms and rules of the usual first-order predicate logic, together with, for each \"T\"-formula \"P\"(\"x\") with a specified free variable \"x\", an infinitary ω-rule of the form:\n\nThat is, if the theory asserts (i.e. proves) \"P\"(\"n\") separately for each natural number \"n\" given by its specified name, then it also asserts \"P\" collectively for all natural numbers at once via the evident finite universally quantified counterpart of the infinitely many antecedents of the rule. For a theory of arithmetic, meaning one with intended domain the natural numbers such as Peano arithmetic, the predicate \"N\" is redundant and may be omitted from the language, with the consequent of the rule for each \"P\" simplifying to formula_7.\n\nAn ω-model of \"T\" is a model of \"T\" whose domain includes the natural numbers and whose specified names and symbol \"N\" are standardly interpreted, respectively as those numbers and the predicate having just those numbers as its domain (whence there are no nonstandard numbers). If \"N\" is absent from the language then what would have been the domain of \"N\" is required to be that of the model, i.e. the model contains only the natural numbers. (Other models of \"T\" may interpret these symbols nonstandardly; the domain of \"N\" need not even be countable, for example.) These requirements make the ω-rule sound in every ω-model. As a corollary to the omitting types theorem, the converse also holds: the theory \"T\" has an ω-model if and only if it is consistent in ω-logic.\n\nThere is a close connection of ω-logic to ω-consistency. A theory consistent in ω-logic is also ω-consistent (and arithmetically sound). The converse is false, as consistency in ω-logic is a much stronger notion than ω-consistency. However, the following characterization holds: a theory is ω-consistent if and only if its closure under \"unnested\" applications of the ω-rule is consistent.\n\nIf the theory \"T\" is recursively axiomatizable, ω-consistency has the following characterization, due to C. Smoryński:\nHere, formula_9 is the set of all Π-sentences valid in the standard model of arithmetic, and formula_10 is the uniform reflection principle for \"T\", which consists of the axioms\nfor every formula formula_12 with one free variable. In particular, a finitely axiomatizable theory \"T\" in the language of arithmetic is ω-consistent if and only if \"T\" + PA is formula_13-sound.\n\n"}
