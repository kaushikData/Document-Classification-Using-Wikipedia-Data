{"id": "208254", "url": "https://en.wikipedia.org/wiki?curid=208254", "title": "16 (number)", "text": "16 (number)\n\n16 (sixteen) is the natural number following 15 and preceding 17. 16 is a composite number, and a square number, being 4 = 4 × 4. It is the smallest number with exactly five divisors, its proper divisors being , , and .\n\nIn English speech, the numbers 16 and 60 are sometimes confused, as they sound very similar.\n\nSixteen is the fourth power of two. For this reason, 16 was used in weighing light objects in several cultures. The British have 16 ounces in one pound; the Chinese used to have 16 \"liangs\" in one \"jin\". In old days, weighing was done with a beam balance to make equal splits. It would be easier to split a heap of grains into sixteen equal parts through successive divisions than to split into ten parts. Chinese Taoists did finger computation on the trigrams and hexagrams by counting the finger tips and joints of the fingers with the tip of the thumb. Each hand can count up to 16 in such manner. The Chinese abacus uses two upper beads to represent the 5s and 5 lower beads to represent the 1s, the 7 beads can represent a hexadecimal digit from 0 to 15 in each column.\n\nSixteen is an even number and a square number.\n\nSixteen is the fourth power of two.\n\nSixteen is the only integer that equals \"m\" and \"n\", for some unequal integers \"m\" and \"n\" (\"m\" = 4, \"n\" = 2, or vice versa). It has this property because 2 = 2 × 2. It is also equal to 2 (see tetration).\n\n16 is the base of the hexadecimal number system, which is used extensively in computer science.\n\n\n\nIn Spanish and Portuguese, 16 is the first compound number (Spanish: \"dieciséis\", European Portuguese: \"dezasseis\", Brazilian Portuguese: \"dezesseis\"); the numbers 11 (Spanish: \"once\", Portuguese: \"onze\") through 15 (Spanish: \"quince\", Portuguese: \"quinze\") have their own names.\n\n\nMany leagues and tournaments have 16 teams, for example:\n\nIn both the NBA and NHL, 16 teams qualify for the respective league playoffs; it is also the number of wins needed to win the title (both leagues have four playoff rounds, with four wins in seven games needed to win each round).\n\nThe regular season of the National Football League currently consists of 16 games.\n\nIn AFL Women's, the top-level league of women's Australian rules football, each team has 16 players on the field at any given time (as opposed to the 18 of almost all other competitions in the sport, most notably the parent Australian Football League for men).\n\n\n"}
{"id": "51411922", "url": "https://en.wikipedia.org/wiki?curid=51411922", "title": "Algorithmic paradigm", "text": "Algorithmic paradigm\n\nAn algorithmic paradigm, algorithm design paradigm, algorithmic technique, or algorithmic strategy is a generic method or approach which underlies the design of a class of algorithms. It is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program. Examples of algorithmic paradigms include the greedy algorithm in optimization problems, dynamic programming, prune and search, and divide and conquer algorithms. More specialized algorithmic paradigms used in parameterized complexity include kernelization and iterative compression. In computational geometry, additional algorithmic paradigms include sweep line algorithms, rotating calipers, and randomized incremental construction.\n"}
{"id": "55964224", "url": "https://en.wikipedia.org/wiki?curid=55964224", "title": "Alspach's conjecture", "text": "Alspach's conjecture\n\nAlspach's conjecture is a mathematical theorem that characterizes the disjoint cycle covers of complete graphs with prescribed cycle lengths. It is named after Brian Alspach, who posed it as a research problem in 1981. A proof was published by .\n\nIn this context, a disjoint cycle cover is a set of simple cycles, no two of which use the same edge, that include all of the edges of a graph. For a disjoint cycle cover to exist, it is necessary for every vertex to have even degree, because the degree of each vertex is two times the number of cycles that include that vertex, an even number. And for the cycles in a disjoint cycle cover to have a given collection of lengths,\nit is also necessary for the sum of the given cycle lengths to equal the total number of edges in the given graph. Alspach conjectured that, for complete graphs, these two necessary conditions are also sufficient: if formula_1 is odd (so that the degrees are even) and a given list of cycle lengths (all at most formula_1) adds to formula_3 (the number of edges in the complete graph) then the complete graph formula_4 can always be decomposed into cycles of the given length. It is this statement that Bryant, Horsley, and Pettersson proved.\n\nFor complete graphs formula_4 whose number formula_1 of vertices is even, Alspach conjectured that it is always possible to decompose the graph into a perfect matching and a collection of cycles of prescribed lengths summing to formula_7. In this case the matching eliminates the odd degree at each vertex, leaving a subgraph of even degree, and the remaining condition is again that the sum of the cycle lengths equals the number of edges to be covered. This variant of the conjecture was also proven by Bryant, Horsley, and Pettersson.\n\nThe Oberwolfach problem on decompositions of complete graphs into copies of a given 2-regular graph is related, but neither is a special case of the other.\nIf formula_8 is a 2-regular graph, with formula_1 vertices, formed from a disjoint union of cycles of certain lengths, then a solution to the Oberwolfach problem for formula_8 would also provide a decomposition of the complete graph into formula_11 copies of each of the cycles of formula_8. However, not every decomposition of formula_4 into this many cycles of each size can be grouped into disjoint cycles that form copies of formula_8, and on the other hand not every instance of Alspach's conjecture involves sets of cycles that have formula_11 copies of each cycle.\n\n"}
{"id": "10324074", "url": "https://en.wikipedia.org/wiki?curid=10324074", "title": "Arithmetic rope", "text": "Arithmetic rope\n\nThe arithmetic rope, or knotted rope, was a widely used arithmetic tool in the Middle Ages that could be used to solve many mathematical and geometrical problems.\n\nAn arithmetic rope generally has at least 13 knots—therefore, it is often called thirteen-knot-rope—placed at equal intervals. More knots were beneficial, especially for multiplication and division.\n\nIn medieval architecture, the knotted rope was indispensable for architects, because it allowed the construction of equilateral and right-angled triangles, as well as circles.\n\nIn the depiction of the liberal arts in Hortus deliciarum, the allegory of arithmetics is a female figure with a knotted rope.\n\n"}
{"id": "577366", "url": "https://en.wikipedia.org/wiki?curid=577366", "title": "Banach–Alaoglu theorem", "text": "Banach–Alaoglu theorem\n\nIn functional analysis and related branches of mathematics, the Banach–Alaoglu theorem (also known as Alaoglu's theorem) states that the closed unit ball of the dual space of a normed vector space is compact in the weak* topology. A common proof identifies the unit ball with the weak* topology as a closed subset of a product of compact sets with the product topology. As a consequence of Tychonoff's theorem, this product, and hence the unit ball within, is compact.\n\nA proof of this theorem for separable normed vector spaces was published in 1932 by Stefan Banach, and the first proof for the general case was published in 1940 by the mathematician Leonidas Alaoglu.\n\nSince the Banach–Alaoglu theorem is proven via Tychonoff's theorem, it relies on the ZFC axiomatic framework, in particular the axiom of choice. Most mainstream functional analysis also relies on ZFC. However, the theorem does \"not\" rely upon the axiom of choice in the separable case (see below): in this case one actually has a constructive proof.\n\nThis theorem has applications in physics when one describes the set of states of an algebra of observables, namely that any states can be written as a convex linear combination of so-called pure states.\n\nLet X be a normed space, the dual X* is hence also a normed space (with the operator norm).\n\nThe closed unit ball of X* is compact with respect to the weak* topology. (cf. also section \"dual\" in the article \"topological vector space\")\n\nThis is a motivation for having different topologies on a same space since in contrast the unit ball in the norm topology is compact if and only if the space is finite-dimensional, cf. Riesz lemma.\n\nA special case of the Banach–Alaoglu theorem is the sequential version of the theorem, which asserts that the closed unit ball of the dual space of a separable normed vector space is sequentially compact in the weak* topology. In fact, the weak* topology on the closed unit ball of the dual of a separable space is metrizable, and thus compactness and sequential compactness are equivalent.\n\nSpecifically, let \"X\" be a separable normed space and \"B\" the closed unit ball in \"X\". Since \"X\" is separable, let {\"x\"} be a countable dense subset. Then the following defines a metric for \"x\", \"y\" ∈ \"B\"\n\nin which formula_2 denotes the duality pairing of \"X\" with \"X\". Sequential compactness of \"B\" in this metric can be shown by a diagonalization argument similar to the one employed in the proof of the Arzelà–Ascoli theorem.\n\nDue to the constructive nature of its proof (as opposed to the general case, which is based on the axiom of choice), the sequential Banach–Alaoglu theorem is often used in the field of partial differential equations to construct solutions to PDE or variational problems. For instance, if one wants to minimize a functional  formula_3  on the dual of a separable normed vector space \"X\", one common strategy is to first construct a minimizing sequence  formula_4  which approaches the infimum of \"F\", use the sequential Banach–Alaoglu theorem to extract a subsequence that converges in the weak* topology to a limit \"x\", and then establish that \"x\" is a minimizer of \"F\". The last step often requires \"F\" to obey a (sequential) lower semi-continuity property in the weak* topology.\n\nWhen \"X\" is the space of finite Radon measures on the real line (so that  formula_5  is the space of continuous functions vanishing at infinity, by the Riesz representation theorem), the sequential Banach–Alaoglu theorem is equivalent to the Helly selection theorem.\n\nThe Bourbaki–Alaoglu theorem is a generalization by Bourbaki to dual topologies on locally convex spaces.\n\nGiven a separated locally convex space \"X\" with continuous dual \"X\" ' then the polar \"U\" of any neighbourhood \"U\" in \"X\" is compact in the weak topology σ(\"X\" ',\"X\") on \"X\" '.\n\nIn the case of a normed vector space, the polar of a neighbourhood is closed and norm-bounded in the dual space. For example, the polar of the unit ball is the closed unit ball in the dual. Consequently, for normed vector space (and hence Banach spaces) the Bourbaki–Alaoglu theorem is equivalent to the Banach–Alaoglu theorem.\n\nFor any \"x\" in \"X\", let\n\nand\n\nSince each \"D\" is a compact subset of the complex plane, \"D\" is also compact in the product topology by Tychonoff theorem.\n\nWe can identify the closed unit ball in \"X*\", \"B\"(\"X*\"), as a subset of \"D\" in a natural way:\n\nThis map is injective and continuous, with \"B\"(\"X*\") having the weak-* topology and \"D\" the product topology. Its inverse, defined on its range, is also continuous.\n\nThe theorem will be proved if the range of the above map is closed. But this is also clear. If one has a net\n\nin \"D\", then the functional defined by\n\nlies in \"B\"(\"X*\").\n\nAs a consequence, \"B(H)\" has the Heine–Borel property, if equipped with either the weak operator or the ultraweak topology.\n\nIt should be cautioned that despite appearances, the Banach–Alaoglu theorem does \"not\" imply that the weak-* topology is locally compact. This is because the closed unit ball is only a neighborhood of the origin in the strong topology, but is usually not a neighbourhood of the origin in the weak-* topology, as it has empty interior in the weak* topology, unless the space is finite-dimensional. In fact, it is a result of Weil that all locally compact Hausdorff topological vector spaces must be finite-dimensional.\n\n\n\n"}
{"id": "201022", "url": "https://en.wikipedia.org/wiki?curid=201022", "title": "Bell number", "text": "Bell number\n\nIn combinatorial mathematics, the Bell numbers count the possible partitions of a set. These numbers have been studied by mathematicians since the 19th century, and their roots go back to medieval Japan, but they are named after Eric Temple Bell, who wrote about them in the 1930s.\n\nStarting with \"B\" = \"B\" = 1, the first few Bell numbers are:\n\nThe \"n\"th of these numbers, \"B\", counts the number of different ways to partition a set that has exactly \"n\" elements, or equivalently, the number of equivalence relations on it. \nOutside of mathematics, the same number also counts the number of different rhyme schemes for \"n\"-line poems.\n\nAs well as appearing in counting problems, these numbers have a different interpretation, as moments of probability distributions. In particular, \"B\" is the \"n\"th moment of a Poisson distribution with mean 1.\n\nIn general, \"B\" is the number of partitions of a set of size \"n\". A partition of a set \"S\" is defined as a set of nonempty, pairwise disjoint subsets of \"S\" whose union is \"S\". For example, \"B\" = 5 because the 3-element set {\"a\", \"b\", \"c\"} can be partitioned in 5 distinct ways:\n\n\"B\" is 1 because there is exactly one partition of the empty set. Every member of the empty set is a nonempty set (that is vacuously true), and their union is the empty set. Therefore, the empty set is the only partition of itself. As suggested by the set notation above, we consider neither the order of the partitions nor the order of elements within each partition. This means that the following partitionings are all considered identical:\nIf, instead, different orderings of the sets are considered to be different partitions, then the number of these ordered partitions is given by the ordered Bell numbers.\n\nIf a number \"N\" is a squarefree positive integer (meaning that it is the product of some number \"n\" of distinct prime numbers), then \"B\" gives the number of different multiplicative partitions of \"N\". These are factorizations of \"N\" into numbers greater than one, treating two factorizations as the same if they have the same factors in a different order. For instance, 30 is the product of the three primes 2, 3, and 5, and has \"B\" = 5 factorizations:\n\nThe Bell numbers also count the rhyme schemes of an \"n\"-line poem or stanza. A rhyme scheme describes which lines rhyme with each other, and so may be interpreted as a partition of the set of lines into rhyming subsets. Rhyme schemes are usually written as a sequence of Roman letters, one per line, with rhyming lines given the same letter as each other, and with the first lines in each rhyming set labeled in alphabetical order. Thus, the 15 possible four-line rhyme schemes are AAAA, AAAB, AABA, AABB, AABC, ABAA, ABAB, ABAC, ABBA, ABBB, ABBC, ABCA, ABCB, ABCC, and ABCD.\n\nThe Bell numbers come up in a card shuffling problem mentioned in the addendum to . If a deck of \"n\" cards is shuffled by repeatedly removing the top card and reinserting it anywhere in the deck (including its original position at the top of the deck), with exactly \"n\" repetitions of this operation, then there are \"n\" different shuffles that can be performed. Of these, the number that return the deck to its original sorted order is exactly \"B\". Thus, the probability that the deck is in its original order after shuffling it in this way is \"B\"/\"n\", which is significantly larger than the 1/\"n\"! probability that would describe a uniformly random permutation of the deck.\n\nRelated to card shuffling are several other problems of counting special kinds of permutations that are also answered by the Bell numbers. For instance, the \"n\"th Bell number equals number of permutations on \"n\" items in which no three values that are in sorted order have the last two of these three consecutive. In a notation for generalized permutation patterns where values that must be consecutive are written adjacent to each other, and values that can appear non-consecutively are separated by a dash, these permutations can be described as the permutations that avoid the pattern 1-23. The permutations that avoid the generalized patterns 12-3, 32-1, 3-21, 1-32, 3-12, 21-3, and 23-1 are also counted by the Bell numbers. The permutations in which every 321 pattern (without restriction on consecutive values) can be extended to a 3241 pattern are also counted by the Bell numbers. However, the Bell numbers grow too quickly to count the permutations that avoid a pattern that has not been generalized in this way: by the (now proven) Stanley–Wilf conjecture, the number of such permutations is singly exponential, and the Bell numbers have a higher asymptotic growth rate than that.\n\nThe Bell numbers can easily be calculated by creating the so-called Bell triangle, also called Aitken's array or the Peirce triangle after Alexander Aitken and Charles Sanders Peirce.\n\n\nHere are the first five rows of the triangle constructed by these rules:\n\nThe Bell numbers appear on both the left and right sides of the triangle.\n\nThe Bell numbers satisfy a recurrence relation involving binomial coefficients:\nIt can be explained by observing that, from an arbitrary partition of \"n\" + 1 items, removing the set containing the first item leaves a partition of a smaller set of \"k\" items for some number \"k\" that may range from 0 to \"n\". There are formula_8 choices for the \"k\" items that remain after one set is removed, and \"B\" choices of how to partition them.\n\nA different summation formula represents each Bell number as a sum of Stirling numbers of the second kind\nThe Bell numbers can also be approximated using the Lambert W function, a function with the same growth rate as the logarithm, as \n\nuniformly for formula_12 as formula_13, where formula_14 and each formula_15 and formula_16 are known expressions in formula_17.\n\nThe asymptotic expression\n\nwas established by .\n\n raised the question of whether infinitely many Bell numbers are also prime numbers. The first few Bell numbers that are prime are:\ncorresponding to the indices 2, 3, 7, 13, 42 and 55 .\n\nThe next Bell prime is \"B\", which is approximately 9.30740105 × 10. , it is the largest known prime Bell number. Phil Carmody showed it was a probable prime in 2002. After 17 months of computation with Marcel Martin's ECPP program Primo, Ignacio Larrosa Cañestro proved it to be prime in 2004. He ruled out any other possible primes below \"B\", later extended to \"B\" by Eric Weisstein.\n\nThe Bell numbers are named after Eric Temple Bell, who wrote about them in 1938, following up a 1934 paper in which he studied the Bell polynomials. Bell did not claim to have discovered these numbers; in his 1938 paper, he wrote that the Bell numbers \"have been frequently investigated\" and \"have been rediscovered many times\". Bell cites several earlier publications on these numbers, beginning with which gives Dobinski's formula for the Bell numbers. Bell called these numbers \"exponential numbers\"; the name \"Bell numbers\" and the notation \"B\" for these numbers was given to them by .\n\nThe first exhaustive enumeration of set partitions appears to have occurred in medieval Japan, where (inspired by the popularity of the book \"The Tale of Genji\") a parlor game called \"genji-ko\" sprang up,\nin which guests were given five packets of incense to smell and were asked to guess which ones were the same as each other and which were different. The 52 possible solutions, counted by the Bell number \"B\", were recorded by 52 different diagrams, which were printed above the chapter headings in some editions of The Tale of Genji.\n\nIn Srinivasa Ramanujan's second notebook, he investigated both Bell polynomials and Bell numbers.\nEarly references for the Bell triangle, which has the Bell numbers on both of its sides, include and .\n\n\n\n"}
{"id": "228161", "url": "https://en.wikipedia.org/wiki?curid=228161", "title": "Bernoulli polynomials", "text": "Bernoulli polynomials\n\nIn mathematics, the Bernoulli polynomials, named after Jacob Bernoulli, occur in the study of many special functions and, in particular the Riemann zeta function and the Hurwitz zeta function. This is in large part because they are an Appell sequence (i.e. a Sheffer sequence for the ordinary derivative operator). Unlike orthogonal polynomials, the Bernoulli polynomials are remarkable in that the number of crossings of the \"x\"-axis in the unit interval does not go up as the degree of the polynomials goes up. In the limit of large degree, the Bernoulli polynomials, appropriately scaled, approach the sine and cosine functions.\nA similar set of polynomials, based on a similar generating function, is the family of Euler polynomials.\n\nThe Bernoulli polynomials \"B\" admit a variety of different representations. Which among them should be taken to be the definition may depend on one's purposes.\n\nfor \"n\" ≥ 0, where \"b\" are the Bernoulli numbers.\n\nThe generating function for the Bernoulli polynomials is\n\nThe generating function for the Euler polynomials is\n\nThe Bernoulli polynomials are also given by\n\nwhere \"D\" = \"d\"/\"dx\" is differentiation with respect to \"x\" and the fraction is expanded as a formal power series. It follows that \ncf. integrals below.\n\nThe Bernoulli polynomials are the unique polynomials determined by\n\nThe integral transform\n\non polynomials \"f\", simply amounts to \nThis can be used to produce the inversion formulae below.\n\nAn explicit formula for the Bernoulli polynomials is given by\n\nNote the remarkable similarity to the globally convergent series expression for the Hurwitz zeta function. Indeed, one has\n\nwhere \"ζ\"(\"s\", \"q\") is the Hurwitz zeta; thus, in a certain sense, the Hurwitz zeta generalizes the Bernoulli polynomials to non-integer values of \"n\".\n\nThe inner sum may be understood to be the \"n\"th forward difference of \"x\"; that is,\n\nwhere Δ is the forward difference operator. Thus, one may write\n\nThis formula may be derived from an identity appearing above as follows. Since the forward difference operator Δ equals\nwhere \"D\" is differentiation with respect to \"x\", we have, from the Mercator series\n\nAs long as this operates on an \"m\"th-degree polynomial such as \"x\", one may let \"n\" go from 0 only up to \"m\".\n\nAn integral representation for the Bernoulli polynomials is given by the Nörlund–Rice integral, which follows from the expression as a finite difference.\n\nAn explicit formula for the Euler polynomials is given by\n\nThis may also be written in terms of the Euler numbers \"E\" as\n\nWe have\n\n(assuming 0 = 1). See Faulhaber's formula for more on this.\n\nThe Bernoulli numbers are given by formula_18\n\nThis definition gives formula_19 for formula_20.\n\nAn alternate convention defines the Bernoulli numbers as formula_21\n\nThe two conventions differ only for formula_22 since formula_23.\n\nThe Euler numbers are given by formula_24\n\nThe first few Bernoulli polynomials are:\n\nThe first few Euler polynomials are:\n\nAt higher \"n\", the amount of variation in \"B\"(\"x\") between \"x\" = 0 and \"x\" = 1 gets large. For instance,\n\nwhich shows that the value at \"x\" = 0 (and at \"x\" = 1) is −3617/510 ≈ −7.09, while at \"x\" = 1/2, the value is 118518239/3342336 ≈ +7.09. D.H. Lehmer showed that the maximum value of \"B\"(\"x\") between 0 and 1 obeys\n\nunless \"n\" is 2 modulo 4, in which case\n\n(where formula_30 is the Riemann zeta function), while the minimum obeys\n\nunless \"n\" is 0 modulo 4, in which case\n\nThese limits are quite close to the actual maximum and minimum, and Lehmer gives more accurate limits as well.\n\nThe Bernoulli and Euler polynomials obey many relations from umbral calculus:\n\n(Δ is the forward difference operator).\n\nThese polynomial sequences are Appell sequences:\n\nThese identities are also equivalent to saying that these polynomial sequences are Appell sequences. (Hermite polynomials are another example.)\n\nZhi-Wei Sun and Hao Pan established the following surprising symmetry relation: If and , then\n\nwhere\n\nThe Fourier series of the Bernoulli polynomials is also a Dirichlet series, given by the expansion\n\nNote the simple large \"n\" limit to suitably scaled trigonometric functions.\n\nThis is a special case of the analogous form for the Hurwitz zeta function\n\nThis expansion is valid only for 0 ≤ \"x\" ≤ 1 when \"n\" ≥ 2 and is valid for 0 < \"x\" < 1 when \"n\" = 1.\n\nThe Fourier series of the Euler polynomials may also be calculated. Defining the functions\n\nand\n\nfor formula_50, the Euler polynomial has the Fourier series\n\nand\n\nNote that the formula_53 and formula_54 are odd and even, respectively:\n\nand\n\nThey are related to the Legendre chi function formula_57 as\n\nand\n\nThe Bernoulli and Euler polynomials may be inverted to express the monomial in terms of the polynomials.\n\nSpecifically, evidently from the above section on #Representation by an integral operator, it follows that \n\nand\n\nThe Bernoulli polynomials may be expanded in terms of the falling factorial formula_62 as\n\nwhere formula_64 and\n\ndenotes the Stirling number of the second kind. The above may be inverted to express the falling factorial in terms of the Bernoulli polynomials:\n\nwhere\n\ndenotes the Stirling number of the first kind.\n\nThe multiplication theorems were given by Joseph Ludwig Raabe in 1851:\n\nFor a natural number ,\n\nIndefinite integrals\n\nDefinite integrals\n\nA periodic Bernoulli polynomial is a Bernoulli polynomial evaluated at the fractional part of the argument . These functions are used to provide the remainder term in the Euler–Maclaurin formula relating sums to integrals. The first polynomial is a sawtooth function.\n\nStrictly these functions are not polynomials at all and more properly should be termed the periodic Bernoulli functions.\n\nThe following properties are of interest, valid for all formula_75:\n\n\n"}
{"id": "2732301", "url": "https://en.wikipedia.org/wiki?curid=2732301", "title": "Boolean-valued model", "text": "Boolean-valued model\n\nIn mathematical logic, a Boolean-valued model is a generalization of the ordinary Tarskian notion of structure from model theory. In a Boolean-valued model, the truth values of propositions are not limited to \"true\" and \"false\", but instead take values in some fixed complete Boolean algebra.\n\nBoolean-valued models were introduced by Dana Scott, Robert M. Solovay, and Petr Vopěnka in the 1960s in order to help understand Paul Cohen's method of forcing. They are also related to Heyting algebra semantics in intuitionistic logic.\n\nFix a complete Boolean algebra \"B\" and a first-order language \"L\"; the signature of \"L\" will consist of a collection of constant symbols, function symbols, and relation symbols.\n\nA Boolean-valued model for the language \"L\" consists of a universe \"M\", which is a set of elements (or names), together with interpretations for the symbols. Specifically, the model must assign to each constant symbol of \"L\" an element of \"M\", and to each \"n\"-ary function symbol \"f\" of \"L\" and each \"n\"-tuple <a...,a> of elements of \"M\", the model must assign an element of \"M\" to the term \"f\"(a...,a).\n\nInterpretation of the atomic formulas of \"L\" is more complicated. To each pair \"a\" and \"b\" of elements of \"M\", the model must assign a truth value ||\"a\"=\"b\"|| to the expression \"a\"=\"b\"; this truth value is taken from the Boolean algebra \"B\". Similarly, for each \"n\"-ary relation symbol \"R\" of \"L\" and each \"n\"-tuple <a...,a> of elements of \"M\", the model must assign an element of \"B\" to be the truth value ||\"R\"(a...,a)||.\n\nThe truth values of the atomic formulas can be used to reconstruct the truth values of more complicated formulas, using the structure of the Boolean algebra. For propositional connectives, this is easy; one simply applies the corresponding Boolean operators to the truth values of the subformulae. For example, if φ(\"x\") and ψ(\"y\",\"z\") are formulas with one and two free variables, respectively, and if \"a\", \"b\", \"c\" are elements of the model's universe to be substituted for \"x\", \"y\", and \"z\", then the truth value of\nis simply\n\nThe completeness of the Boolean algebra is required to define truth values for quantified formulas. If φ(\"x\") is a formula with free variable \"x\" (and possibly other free variables that are suppressed), then\nwhere the right-hand side is to be understood as the supremum in \"B\" of the set of all truth values ||φ(\"a\")|| as \"a\" ranges over \"M\".\n\nThe truth value of a formula is sometimes referred to as its probability. However, these are not probabilities in the ordinary sense, because they are not real numbers, but rather elements of the complete Boolean algebra \"B\".\n\nGiven a complete Boolean algebra \"B\" there is a Boolean-valued model denoted by \"V\", which is the Boolean-valued analogue of the von Neumann universe \"V\". (Strictly speaking, \"V\" is a proper class, so we need to reinterpret what it means to be a model appropriately.) Informally, the elements of \"V\" are \"Boolean-valued sets\". Given an ordinary set \"A\", every set either is or is not a member; but given a Boolean-valued set, every set has a certain, fixed \"probability\" of being a member of \"A\". Again, the \"probability\" is an element of \"B\", not a real number. The concept of Boolean-valued sets resembles, but is not the same as, the notion of a fuzzy set.\n\nThe (\"probabilistic\") elements of the Boolean-valued set, in turn, are also Boolean-valued sets, whose elements are also Boolean-valued sets, and so on. In order to obtain a non-circular definition of Boolean-valued set, they are defined inductively in a hierarchy similar to the cumulative hierarchy. For each ordinal α of \"V\", the set \"V\" is defined as follows. \nThe class \"V\" is defined to be the union of all sets \"V\".\n\nIt is also possible to relativize this entire construction to some transitive model \"M\" of ZF (or sometimes a fragment thereof). The Boolean-valued model \"M\" is obtained by applying the above construction \"inside\" \"M\". The restriction to transitive models is not serious, as the Mostowski collapsing theorem implies that every \"reasonable\" (well-founded, extensional) model is isomorphic to a transitive one. (If the model \"M\" is not transitive things get messier, as \"M\"'s interpretation of what it means to be a \"function\" or an \"ordinal\" may differ from the \"external\" interpretation.)\n\nOnce the elements of \"V\" have been defined as above, it is necessary to define \"B\"-valued relations of equality and membership on \"V\". Here a \"B\"-valued relation on \"V\" is a function from \"V\"×\"V\" to \"B\". To avoid confusion with the usual equality and membership, these are denoted by ||\"x\"=\"y\"|| and ||\"x\"∈\"y\"|| for \"x\" and \"y\" in \"V\". They are defined as follows:\n\nThe symbols ∑ and ∏ denote the least upper bound and greatest lower bound operations, respectively, in the complete Boolean algebra \"B\". At first sight the definitions above appear to be circular: ||  ∈ || depends on || = ||, which depends on || ⊆ ||, which depends on || ∈ ||. However, a close examination shows that the definition of || ∈ || only depends on || ∈ || for elements of smaller rank, so || ∈ || and ||  = || are well defined functions from \"V\"×\"V\" to \"B\".\n\nIt can be shown that the \"B\"-valued relations || ∈ || and || = || on \"V\" make \"V\" into a Boolean-valued model of set theory. Each sentence of first order set theory with no free variables has a truth value in \"B\"; it must be shown that the axioms for equality and all the axioms of ZF set theory (written without free variables) have truth value 1 (the largest element of \"B\"). This proof is straightforward, but it is long because there are many different axioms that need to be checked.\n\nSet theorists use a technique called forcing\nto obtain independence results and to construct models of set theory for other purposes. The method was originally developed by Paul Cohen but has been greatly extended since then. In one form, forcing \"adds to the universe\" a generic subset of a poset, the poset being designed to impose interesting properties on the newly added object. The wrinkle is that (for interesting posets) it can be proved that there simply \"is\" no such generic subset of the poset. There are three usual ways of dealing with this:\n\nBoolean-valued models can be used to give semantics to syntactic forcing; the price paid is that the semantics is not 2-valued (\"true or false\"), but assigns truth values from some complete Boolean algebra. Given a forcing poset \"P\", there is a corresponding complete Boolean algebra \"B\", often obtained as the collection of regular open subsets of \"P\", where the topology on \"P\" is defined by declaring all lower sets open (and all upper sets closed). (Other approaches to constructing \"B\" are discussed below.)\n\nNow the order on \"B\" (after removing the zero element) can replace \"P\" for forcing purposes, and the forcing relation can be interpreted semantically by saying that, for \"p\" an element of \"B\" and φ a formula of the forcing language,\nwhere ||φ|| is the truth value of φ in \"V\".\n\nThis approach succeeds in assigning a semantics to forcing over \"V\" without resorting to fictional generic objects. The disadvantages are that the semantics is not 2-valued, and that the combinatorics of \"B\" are often more complicated than those of the underlying poset \"P\".\n\nOne interpretation of forcing starts with a countable transitive model \"M\" of ZF set theory, a partially ordered set \"P\", and a \"generic\" subset \"G\" of \"P\", and constructs a new model of ZF set theory from these objects. (The conditions that the model be countable and transitive simplify some technical problems, but are not essential.) Cohen's construction can be carried out using Boolean-valued models as follows. \n\nWe now explain these steps in more detail.\n\nFor any poset \"P\" there is a complete Boolean algebra \"B\" and a map \"e\" from \"P\" to \"B\" (the non-zero elements of \"B\") such that the image is dense, \"e\"(\"p\")≤\"e\"(\"q\") whenever \"p\"≤\"q\", and \"e\"(\"p\")\"e\"(\"q\")=0 whenever \"p\" and \"q\" are incompatible. This Boolean algebra is unique up to isomorphism. It can be constructed as the algebra of regular open sets in the topological space of \"P\" (with underlying set \"P\", and a base given by the sets \"U\" of elements \"q\" with \"q\"≤\"p\").\n\nThe map from the poset \"P\" to the complete Boolean algebra \"B\" is not injective in general. The map is injective if and only if \"P\" has the following property: if every \"r\"≤\"p\" is compatible with \"q\", then \"p\"≤\"q\".\n\nby mapping \"U\" to true and its complement to false. Conversely, given such a homomorphism, the inverse image of true is an ultrafilter, so ultrafilters are essentially the same as homomorphisms to {true, false}. (Algebraists might prefer to use maximal ideals instead of ultrafilters: the complement of an ultrafilter is a maximal ideal, and conversely the complement of a maximal ideal is an ultrafilter.)\n\nIf \"g\" is a homomorphism from a Boolean algebra \"B\" to a Boolean algebra \"C\" and \"M\" is any \n\"B\"-valued model of ZF (or of any other theory for that matter) we can turn \"M\" into a \"C\" -valued model by applying the homomorphism \"g\" to the value of all formulas. In particular if \"C\" is {true, false} we get a {true, false}-valued model. This is almost the same as an ordinary model: in fact we get an ordinary model on the set of equivalence classes under || = || of a {true, false}-valued model. So we get an ordinary model of ZF set theory by starting from \"M\", a Boolean algebra \"B\", and an ultrafilter \"U\" on \"B\".\n\nWe have seen that forcing can be done using Boolean-valued models, by constructing a Boolean algebra with ultrafilter from a poset with a generic subset. It is also possible to go back the other way: given a Boolean algebra \"B\", we can form a poset \"P\" of all the nonzero elements of \"B\", and a generic ultrafilter on \"B\" restricts to a generic set on \"P\". So the techniques of forcing and Boolean-valued models are essentially equivalent.\n\n"}
{"id": "38734569", "url": "https://en.wikipedia.org/wiki?curid=38734569", "title": "Communicating finite-state machine", "text": "Communicating finite-state machine\n\nIn computer science, a communicating finite-state machine is a finite state machine labeled with \"receive\" and \"send\" operations over some alphabet of channels. They were introduced by Brand and Zafiropulo, and can be used as a model of concurrent processes like Petri nets. Communicating finite state machines are used frequently for modeling a communication protocol since they make it possible to detect major protocol design errors, including boundedness, deadlocks, and unspecified receptions.\n\nThe advantage of communicating finite state machines is that they make it possible to decide many properties in communication protocols, beyond the level of just detecting such properties. This advantage rules out the need for human assistance or restriction in generality.\n\nIt has been proved with the introduction of the concept itself that when two finite state machines communicate with only one type of messages, boundedness, deadlocks, and unspecified reception state can be decided and identified while such is not the case when the machines communicate with two or more types of messages. Later, it has been further proved that when only one finite state machine communicates with single type of message while the communication of its partner is unconstrained, we can still decide and identify boundedness, deadlocks, and unspecified reception state.\n\nIt has been further proved that when the message priority relation is empty, boundedness, deadlocks and unspecified reception state can be decided even under the condition in which there are two or more types of messages in the communication between finite state machines.\n\nBoundedness, deadlocks, and unspecified reception state are all decidable in polynomial time (which means that a particular problem can be solved in tractable, not infinite, amount of time) since the decision problems regarding them are nondeterministic logspace complete.\n\nCommunicating finite state machines can be the most powerful in situations where the propagation delay is not negligible (so that several messages can be in transit at one time) and in situations where it is natural to describe the protocol parties and the communication medium as separate entities.\n\nHierarchical state machines are finite state machines whose states themselves can be other machines. Since a communicating finite state machine is characterized by concurrency, the most notable trait in a communicating hierarchical state machine is the coexistence of hierarchy and concurrency. This had been considered highly suitable as it signifies stronger interaction inside the machine. \n\nHowever, it was proved that the coexistence of hierarchy and concurrency intrinsically costs language inclusion, language equivalence, and all of universality.\n"}
{"id": "2471934", "url": "https://en.wikipedia.org/wiki?curid=2471934", "title": "Constraint counting", "text": "Constraint counting\n\nIn mathematics, constraint counting is counting the number of constraints in order to compare it with the number of variables, parameters, etc. that are free to be determined, the idea being that in most cases the number of independent choices that can be made is the excess of the latter over the former.\n\nFor example, in linear algebra if the number of constraints (independent equations) in a system of linear equations equals the number of unknowns then precisely one solution exists; if there are fewer independent equations than unknowns, an infinite number of solutions exist; and if the number of independent equations exceeds the number of unknowns, then no solutions exist.\n\nIn the context of partial differential equations, constraint counting is a crude but often useful way of counting the number of \"free functions\" needed to specify a solution to a partial differential equation.\n\nConsider a second order partial differential equation in three variables, such as the two-dimensional wave equation\nIt is often profitable to think of such an equation as a \"rewrite rule\" allowing us to rewrite arbitrary partial derivatives of the function formula_2 using fewer partials than would be needed for an arbitrary function. For example, if formula_3 satisfies the wave equation, we can rewrite \nwhere in the first equality, we appealed to the fact that \"partial derivatives commute\".\n\nTo answer this in the important special case of a linear partial differential equation, Einstein asked: how many of the partial derivatives of a solution can be linearly independent? It is convenient to record his answer using an ordinary generating function\nwhere formula_6 is a natural number counting the number of linearly independent partial derivatives (of order k) of an arbitrary function in the solution space of the equation in question.\n\nWhenever a function satisfies some partial differential equation, we can use the corresponding rewrite rule to eliminate some of them, because \"further mixed partials have necessarily become linearly dependent\". Specifically, the power series counting the variety of \"arbitrary\" functions of three variables (no constraints) is\nbut the power series counting those in the solution space of some second order p.d.e. is\nwhich records that we can eliminate \"one\" second order partial formula_9, \"three\" third order partials formula_10, and so forth.\n\nMore generally, the o.g.f. for an arbitrary function of n variables is\nwhere the coefficients of the infinite power series of the generating function are constructed using an appropriate infinite sequence of binomial coefficients, and the power series for a function required to satisfy a linear m-th order equation is\n\nNext,\nwhich can be interpreted to predict that a solution to a second order linear p.d.e. in \"three\" variables is expressible by two \"freely chosen\" functions of \"two\" variables, one of which is used immediately, and the second, only after taking a \"first derivative\", in order to express the solution.\n\nTo verify this prediction, recall the solution of the initial value problem\nApplying the Laplace transform formula_15 gives\nApplying the Fourier transform formula_17 to the two spatial variables gives\nor\nApplying the inverse Laplace transform gives\nApplying the inverse Fourier transform gives\nwhere\nHere, p,q are arbitrary (sufficiently smooth) functions of two variables, so (due their modest time dependence) the integrals P,Q also count as \"freely chosen\" functions of two variables; as promised, one of them is differentiated once before adding to the other to express the general solution of the initial value problem for the two dimensional wave equation.\n\nIn the case of a nonlinear equation, it will only rarely be possible to obtain the general solution in closed form. However, if the equation is \"quasilinear\" (linear in the highest order derivatives), then we can still obtain approximate information similar to the above: specifying a member of the solution space will be \"modulo nonlinear quibbles\" equivalent to specifying a certain number of functions in a smaller number of variables. The number of these functions is the \"Einstein strength\" of the p.d.e. In the simple example above, the strength is two, although in this case we were able to obtain more precise information.\n\n"}
{"id": "32106812", "url": "https://en.wikipedia.org/wiki?curid=32106812", "title": "Convergence (logic)", "text": "Convergence (logic)\n\nIn mathematics, computer science and logic, convergence refers to the idea that different sequences of transformations come to a conclusion in a finite amount of time (the transformations are terminating), and that the conclusion reached is independent of the path taken to get to it (they are confluent).\n\nMore formally, a preordered set of term rewriting transformations are said to be convergent if they are confluent and terminating.\n\n"}
{"id": "51017812", "url": "https://en.wikipedia.org/wiki?curid=51017812", "title": "DONE", "text": "DONE\n\nThe Data-based Online Nonlinear Extremumseeker (DONE) algorithm is a black-box optimization algorithm.\nDONE models the unknown cost function and attempts to find an optimum of the underlying function.\nThe DONE algorithm is suitable for optimizing costly and noisy functions and does not require derivatives.\nAn advantage of DONE over similar algorithms, such as Bayesian Optimization, is that the computational cost per iteration is independent of the number of function evaluations.\n\nThe DONE algorithm was first proposed by Hans Verstraete and Sander Wahls. The algorithm fits a surrogate model based on random Fourier features and then uses a well-known L-BFGS algorithm to find an optimum of the surrogate model.\n\nDONE was first demonstrated for maximizing the signal in optical coherence tomography measurements, but has since then been applied to various other applications. For example, it was used to help extending the field of view in light sheet fluorescence microscopy.\n"}
{"id": "13374894", "url": "https://en.wikipedia.org/wiki?curid=13374894", "title": "Delta invariant", "text": "Delta invariant\n\nIn mathematics, in the theory of algebraic curves, a delta invariant measures the number of double points concentrated at a point. It is a non-negative integer.\nDelta invariants are discussed in the \"Classification of singularities\" section of the algebraic curve article.\n"}
{"id": "51410046", "url": "https://en.wikipedia.org/wiki?curid=51410046", "title": "Folded-t and half-t distributions", "text": "Folded-t and half-t distributions\n\nIn statistics, the folded-\"t\" and half-\"t\" distributions are derived from Student's \"t\"-distribution by taking the absolute values of variates. This is analogous to the folded-normal and the half-normal statistical distributions being derived from the normal distribution.\n\nThe folded non-standardized \"t\" distribution is the distribution of the absolute value of the non-standardized \"t\" distribution with formula_1 degrees of freedom; its probability density function is given by:\nThe half-\"t\" distribution results as the special case of formula_3, and the standardized version as the special case of formula_4.\n\nFolded-\"t\" and half-\"t\" generalize the folded normal and half-normal distributions by allowing for finite degrees-of-freedom (the normal analogues constitute the limiting cases of infinite degrees-of-freedom). Since the Cauchy distribution constitutes the special case of a Student-\"t\" distribution with one degree of freedom, the families of folded and half-\"t\" distributions include the folded Cauchy and half-Cauchy distributions for formula_5.\n\nIf formula_3, the folded-\"t\" distribution reduces to the special case of the half-\"t\" distribution. Its probability density function then simplifies to\nThe half-\"t\" distribution's first two moments (expectation and variance) are given by:\nand\n\n\n\n"}
{"id": "1635110", "url": "https://en.wikipedia.org/wiki?curid=1635110", "title": "Free logic", "text": "Free logic\n\nA free logic is a logic with fewer existential presuppositions than classical logic. Free logics may allow for terms that do not denote any object. Free logics may also allow models that have an empty domain. A free logic with the latter property is an inclusive logic.\n\nIn classical logic there are theorems that clearly presuppose that there is something in the domain of discourse. Consider the following classically valid theorems.\n\nA valid scheme in the theory of equality which exhibits the same feature is\n\nInformally, if F is '=y', G is 'is Pegasus', and we substitute 'Pegasus' for y, then (4) appears to allow us to infer from 'everything identical with Pegasus is Pegasus' that something is identical with Pegasus. The problem comes from substituting nondesignating constants for variables: in fact, we cannot do this in standard formulations of first-order logic, since there are no nondesignating constants. Classically, ∃x(x=y) is deducible from the open equality axiom y=y by particularization (i.e. (3) above).\n\nIn free logic, (1) is replaced with\n\nSimilar modifications are made to other theorems with existential import (e.g. the Rule of Particularization becomes (Ar → (E!r → ∃xAx)).\n\nAxiomatizations of free-logic are given by Theodore Hailperin (1957), Jaakko Hintikka (1959), Karel Lambert (1967), and Richard L. Mendelsohn (1989).\n\nKarel Lambert wrote in 1967: \"In fact, one may regard free logic... literally as a theory about singular existence, in the sense that it lays down certain minimum conditions for that concept.\" The question that concerned the rest of his paper was then a description of the theory, and to inquire whether it gives a necessary and sufficient condition for existence statements.\n\nLambert notes the irony in that Willard Van Orman Quine so vigorously defended a form of logic that only accommodates his famous dictum, \"To be is to be the value of a variable,\" when the logic is supplemented with Russellian assumptions of description theory. He criticizes this approach because it puts too much ideology into a logic, which is supposed to be philosophically neutral. Rather, he points out, not only does free logic provide for Quine's criterion—it even proves it! This is done by brute force, though, since he takes as axioms formula_6 and formula_7, which neatly formalizes Quine's dictum. So, Lambert argues, to reject his construction of free logic requires you to reject Quine's philosophy, which requires some argument and also means that whatever logic you develop is always accompanied by the stipulation that you must reject Quine to accept the logic. Likewise, if you reject Quine then you must reject free logic. This amounts to the contribution that free logic makes to ontology.\n\nThe point of free logic, though, is to have a formalism that implies no particular ontology, but that merely makes an interpretation of Quine both formally possible and simple. An advantage of this is that formalizing theories of singular existence in free logic brings out their implications for easy analysis. Lambert takes the example of the theory proposed by Wesley C. Salmon and George Nahknikian, which is that to exist is to be self-identical.\n\n\n"}
{"id": "4774780", "url": "https://en.wikipedia.org/wiki?curid=4774780", "title": "GReAT", "text": "GReAT\n\nGraph Rewriting and Transformation (GReAT) is a Model Transformation Language (MTL) for Model Integrated Computing available in the GME environment. GReAT has a rich pattern specification sublanguage, a graph transformation sublanguage and a high level control-flow sublanguage. It has been designed to address the specific needs of the model transformation area. The GME environment is an example of a Model Driven Engineering (MDE) framework.\n\n\n"}
{"id": "26012311", "url": "https://en.wikipedia.org/wiki?curid=26012311", "title": "Harold Edwards (mathematician)", "text": "Harold Edwards (mathematician)\n\nHarold Mortimer Edwards, Jr. (born August 6, 1936) is an American mathematician working in number theory, algebra, and the history and philosophy of mathematics.\n\nHe was one of the co-founding editors, with Bruce Chandler, of \"The Mathematical Intelligencer\".\nHe is the author of expository books on the Riemann zeta function, on Galois theory, and on Fermat's Last Theorem. He wrote a book on Leopold Kronecker's work on divisor theory providing a systematic exposition of that work—a task that Kronecker never completed. He has written textbooks on linear algebra, calculus, and number theory. He also wrote a book of essays on constructive mathematics.\n\nEdwards received his Ph.D. in 1961 from Harvard University, under the supervision of Raoul Bott.\nHe has taught at Harvard and Columbia University; he joined the faculty at New York University in 1966, and has been an emeritus professor since 2002.\n\nIn 1980, Edwards won the Leroy P. Steele Prize for Mathematical Exposition of the American Mathematical Society, for his books on the Riemann zeta function and Fermat's Last Theorem. For his contribution in the field of the history of mathematics he was awarded the Albert Leon Whiteman Memorial Prize by the AMS in 2005. In 2012 he became a fellow of the American Mathematical Society.\n\nEdwards is married to Betty Rollin, a former NBC News correspondent, author, and breast cancer survivor.\n\n\n\n"}
{"id": "3553654", "url": "https://en.wikipedia.org/wiki?curid=3553654", "title": "Identity theorem", "text": "Identity theorem\n\nIn complex analysis, a branch of mathematics, the identity theorem for holomorphic functions states: given functions \"f\" and \"g\" holomorphic on a domain \"D\" (open and connected subset), if \"f\" = \"g\" on some formula_1, formula_2 having an accumulation point, then \"f\" = \"g\" on \"D\". \n\nThus a holomorphic function is completely determined by its values on a (possibly quite small) neighborhood in \"D\". This is not true for real-differentiable functions. In comparison, holomorphy, or complex-differentiability, is a much more rigid notion. Informally, one sometimes summarizes the theorem by saying holomorphic functions are \"hard\" (as opposed to, say, continuous functions which are \"soft\").\n\nThe underpinning fact from which the theorem is established is the developability of a holomorphic function into its Taylor series.\n\nThe connectedness assumption on the domain \"D\" is necessary. For example, if \"D\" consists of two disjoint open set, formula_3 can be formula_4 on one open set, and formula_5 on another, while formula_6 is formula_4 on one, and formula_8 on another.\n\nIf two holomorphic functions \"f\" and \"g\" on a domain \"D\" agree on a set S which has an accumulation point \"c\" in \"D\", then \"f = g\" on a disk in formula_9 centered at formula_10.\n\nTo prove this, it is enough to show that formula_11 for all formula_12. \n\nIf this is not the case, let \"m\" be the smallest nonnegative integer with formula_13. By holomorphy, we have the following Taylor series representation in some open neighborhood U of \"c\":\n\nBy continuity, \"h\" is non-zero in some small open disk \"B\" around \"c\". But then \"f\" − \"g\" ≠ 0 on the punctured set \"B\" − {\"c\"}. This contradicts the assumption that \"c\" is an accumulation point of {\"f = g\"}.\n\nThis lemma shows that for a complex number \"a\", the fiber \"f\"(\"a\") is a discrete (and therefore countable) set, unless \"f\" = \"a\".\n\nDefine the set on which formula_3 and formula_6 have the same Taylor expansion:\n\nWe'll show formula_18 is nonempty, open, and closed. Then by connectedness of formula_9, formula_18 must be all of formula_9, which implies formula_22 on formula_23.\n\nBy the lemma, formula_24 in a disk centered on a disk in formula_9 centered at formula_10, they have the same Taylor series at formula_10, so formula_28, formula_18 is nonempty.\n\nAs formula_3 and formula_6 are holomorphic on formula_9, formula_33, the Taylor series of formula_3 and formula_6 at formula_36 have non-zero radius of convergence. Therefore, the open disk formula_37 also lies in \"S\" for some \"r\". So \"S\" is open.\n\nBy holomorphy of formula_3 and formula_6, they have holomorphic derivatives, so all formula_40 are continuous. This means that formula_41 is closed for all formula_42. formula_18 is an intersection of closed sets, so it's closed\n"}
{"id": "27453461", "url": "https://en.wikipedia.org/wiki?curid=27453461", "title": "Integrated information theory", "text": "Integrated information theory\n\nIntegrated information theory (IIT) attempts to explain what consciousness is and why it might be associated with certain physical systems. Given any such system, the theory predicts whether that system is conscious, to what degree it is conscious, and what particular experience it is having (see Central identity). According to IIT, a system's consciousness is determined by its causal properties and is therefore an intrinsic, fundamental property of any physical system.\n\nIIT was proposed by neuroscientist Giulio Tononi in 2004, and has been continuously developed over the past decade. The latest version of the theory, labeled \"IIT 3.0\", was published in 2014.\n\nDavid Chalmers has argued that any attempt to explain consciousness in purely physical terms (i.e. to start with the laws of physics as they are currently formulated and derive the necessary and inevitable existence of consciousness) eventually runs into the so-called \"hard problem\". Rather than try to start from physical principles and arrive at consciousness, IIT \"starts with consciousness\" (accepts the existence of consciousness as certain) and reasons about the properties that a postulated physical substrate would have to have in order to account for it. The ability to perform this jump from phenomenology to mechanism rests on IIT's assumption that if a conscious experience can be fully accounted for by an underlying physical system, then the properties of the physical system must be constrained by the properties of the experience.\n\nSpecifically, IIT moves from phenomenology to mechanism by attempting to identify the essential properties of conscious experience (dubbed \"axioms\") and, from there, the essential properties of conscious physical systems (dubbed \"postulates\").\n\nThe axioms are intended to capture the essential aspects of every conscious experience. Every axiom should apply to every possible experience.\n\nThe wording of the axioms has changed slightly as the theory has developed, and the most recent and complete statement of the axioms is as follows: \nThe axioms describe regularities in conscious experience, and IIT seeks to explain these regularities. What could account for the fact that every experience exists, is structured, is differentiated, is unified, and is definite? IIT argues that the existence of an underlying causal system with these same properties offers the most parsimonious explanation. Thus a physical system, if conscious, is so by virtue of its causal properties.\n\nThe properties required of a conscious physical substrate are called the \"postulates,\" since the existence of the physical substrate is itself only postulated (remember, IIT maintains that the only thing one can be sure of is the existence of one's own consciousness). In what follows, a \"physical system\" is taken to be a set of elements, each with two or more internal states, inputs that influence that state, and outputs that are influenced by that state (neurons or logic gates are the natural examples). Given this definition of \"physical system\", the postulates are:\n</math>]]), thus laying maximal claim to intrinsic existence. ... With respect to causation, this has the consequence that the \"winning\" cause-effect structure excludes alternative cause-effect structures specified over overlapping elements, otherwise there would be causal overdetermination. ... The exclusion postulate can be said to enforce Occam's razor (entities should not be multiplied beyond necessity): it is more parsimonious to postulate the existence of a single cause-effect structure over a system of elements—the one that is maximally irreducible from the system's intrinsic perspective—than a multitude of overlapping cause-effect structures whose existence would make no further difference. The exclusion postulate also applies to individual mechanisms: a subset of elements in a state specifies the cause-effect repertoire that is maximally irreducible (MICE) within the system (formula_1), called a core concept, or concept for short. Again, it cannot additionally specify a cause-effect repertoire overlapping over the same elements, because otherwise the difference a mechanism makes would be counted multiple times. ... Finally, the exclusion postulate also applies to spatio-temporal grains, implying that a conceptual structure is specified over a definite grain size in space (either quarks, atoms, neurons, neuronal groups, brain areas, and so on) and time (either microseconds, milliseconds, seconds, minutes, and so on), the one at which formula_2 reaches a maximum. ... Once more, this implies that a mechanism cannot specify a cause-effect repertoire at a particular temporal grain, and additional effects at a finer or coarser grain, otherwise the differences a mechanism makes would be counted multiple times.\n\nFor a complete and thorough account of the mathematical formalization of IIT, see reference. What follows is intended as a brief summary, adapted from, of the most important quantities involved. Pseudocode for the algorithms used to calculate these quantities can be found at reference.\n\nA system refers to a set of elements, each with two or more internal states, inputs that influence that state, and outputs that are influenced by that state. A mechanism refers to a subset of system elements. The mechanism-level quantities below are used to assess the integration of any given mechanism, and the system-level quantities are used to assess the integration of sets of mechanisms (\"sets of sets\").\n\nIn order to apply the IIT formalism to a system, its full transition probability matrix (TPM) must be known. The TPM specifies the probability with which any state of a system transitions to any other system state. Each of the following quantities is calculated in a bottom-up manner from the system's TPM.\n\nFor a system of formula_3 simple binary elements, cause-effect space is formed by formula_4 axes, one for each possible past and future state of the system. Any cause-effect repertoire formula_5, which specifies the probability of each possible past and future state of the system, can be easily plotted as a point in this high-dimensional space: The position of this point along each axis is given by the probability of that state as specified by formula_5. If a point is also taken to have a scalar magnitude (which can be informally thought of as the point's \"size\", for example), then it can easily represent a concept: The concept's cause-effect repertoire specifies the location of the point in cause-effect space, and the concept's formula_7 value specifies that point's magnitude.\n\nIn this way, a conceptual structure formula_8 can be plotted as a constellation of points in cause-effect space. Each point is called a star, and each star's magnitude (formula_7) is its size.\n\nIIT addresses the mind-body problem by proposing an identity between phenomenological properties of experience and causal properties of physical systems: \"The conceptual structure specified by a complex of elements in a state is identical to its experience.\"\n\nSpecifically, the form of the conceptual structure in cause-effect space completely specifies the quality of the experience, while the irreducibility formula_10 of the conceptual structure specifies the level to which it exists (i.e., the complex's level of consciousness). The maximally irreducible cause-effect repertoire of each concept within a conceptual structure specifies what the concept contributes to the quality of the experience, while its irreducibility formula_11 specifies how much the concept is present in the experience.\n\nAccording to IIT, an experience is thus an intrinsic property of a complex of mechanisms in a state.\n\nThe calculation of even a modestly-sized system's formula_12 is often computationally intractable, so efforts have been made to develop heuristic or proxy measures of integrated information. For example, Masafumi Oizumi has developed formula_13, a practical approximation for integrated information that solves the theoretical shortcomings of previously proposed proxy measures, such as the one proposed by Adam Barrett.\n\nA significant computational challenge in calculating integrated information is finding the Minimum Information Partition of a neural system, which requires iterating through all possible network partitions. To solve this problem, Daniel Toker has suggested using the most modular decomposition of a network as an extremely quick proxy for the Minimum Information Partition.\n\nWhile the algorithm for assessing a system's formula_12 and conceptual structure is relatively straightforward, its high time complexity makes it computationally intractable for many systems of interest. Heuristics and approximations can sometimes be used to provide ballpark estimates of a complex system's integrated information, but precise calculations are often impossible. These computational challenges, combined with the already difficult task of reliably and accurately assessing consciousness under experimental conditions, make testing many of the theory's predictions difficult.\n\nDespite these challenges, researchers have attempted to use measures of information integration and differentiation to assess levels of consciousness in a variety of subjects. For instance, a recent study using a less computationally-intensive proxy for formula_12 was able to reliably discriminate between varying levels of consciousness in wakeful, sleeping (dreaming vs. non-dreaming), anesthetized, and comatose (vegetative vs. minimally-conscious vs. locked-in) individuals.\n\nIIT also makes several predictions which fit well with existing experimental evidence, and can be used to explain some counterintuitive findings in consciousness research. For example, IIT can be used to explain why some brain regions, such as the cerebellum do not appear to contribute to consciousness, despite their size and/or functional importance.\n\nIntegrated Information Theory has received both broad criticism and support.\n\nNeuroscientist Christof Koch, who has helped to develop the theory, has called IIT \"the only really promising fundamental theory of consciousness\". Technologist Virgil Griffith says \"IIT is currently the leading theory of consciousness.\"\n\nChallenges to IIT:\n\n\n\n\n\n\n"}
{"id": "4703917", "url": "https://en.wikipedia.org/wiki?curid=4703917", "title": "International Society for Mathematical Sciences", "text": "International Society for Mathematical Sciences\n\nThe International Society for Mathematical Sciences is a mathematics society, primarily based in Japan. It was formerly known as the Japanese Association of Mathematical Sciences, and was founded in 1948 by Tatsujiro Shimizu.\n\nThe ISMS publishes a bimonthly scientific journal, \"Scientiae Mathematicae Japonicae\" (), which was formed in 2001 from the merger of two journals previously published by the same society, \"Mathematica Japonica\", founded in 1948, and \"Scientiae Mathematicae\", which published nine issues over three volumes in 1998, 1999, and 2000. In addition the ISMS holds an annual meeting and publishes a Japanese language mathematics magazine, \"Kaiho\", and a monthly newsletter, \"Notices from the ISMS\".\n\n"}
{"id": "6935363", "url": "https://en.wikipedia.org/wiki?curid=6935363", "title": "Invariant differential operator", "text": "Invariant differential operator\n\nIn mathematics and theoretical physics, an invariant differential operator is a kind of mathematical map from some objects to an object of similar type. These objects are typically functions on formula_1, functions on a manifold, vector valued functions, vector fields, or, more generally, sections of a vector bundle.\n\nIn an invariant differential operator formula_2, the term \"differential operator\" indicates that the value formula_3 of the map depends only on formula_4 and the derivatives of formula_5 in formula_6. The word \"invariant\" indicates that the operator contains some symmetry. This means that there is a group formula_7 with a group action on the functions (or other objects in question) and this action is preserved by the operator:\n\nUsually, the action of the group has the meaning of a change of coordinates (change of observer) and the invariance means that the operator has the same expression in all admissible coordinates.\n\nLet \"M\" = \"G\"/\"H\" be a homogeneous space for a Lie group G and a Lie subgroup H. Every representation formula_9 gives rise to a vector bundle\n\nSections formula_11 can be identified with\n\nIn this form the group \"G\" acts on sections via\n\nNow let \"V\" and \"W\" be two vector bundles over \"M\". Then a differential operator\n\nthat maps sections of \"V\" to sections of \"W\" is called invariant if\n\nfor all sections formula_16 in formula_17 and elements \"g\" in \"G\". All linear invariant differential operators on homogeneous parabolic geometries, i.e. when \"G\" is semi-simple and \"H\" is a parabolic subgroup, are given dually by homomorphisms of generalized Verma modules.\n\nGiven two connections formula_18 and formula_19 and a one form formula_20, we have\nfor some tensor formula_22. Given an equivalence class of connections formula_23, we say that an operator is invariant if the form of the operator does not change when we change from one connection in the equivalence class to another. For example, if we consider the equivalence class of all torsion free connections, then the tensor Q is symmetric in its lower indices, i.e. formula_24. Therefore we can compute\nwhere brackets denote skew symmetrization. This shows the invariance of the exterior derivative when acting on one forms.\nEquivalence classes of connections arise naturally in differential geometry, for example:\n\n\n\nGiven a metric \n\non formula_31, we can write the sphere formula_32 as the space of generators of the nil cone\n\nIn this way, the flat model of conformal geometry is the sphere formula_34 with formula_35 and P the stabilizer of a point in formula_31. A classification of all linear conformally invariant differential operators on the sphere is known (Eastwood and Rice, 1987).\n\n\n"}
{"id": "34422078", "url": "https://en.wikipedia.org/wiki?curid=34422078", "title": "Jacob Fox", "text": "Jacob Fox\n\nJacob Fox (born Jacob Licht in 1984) is an American mathematician. He is a professor at Stanford University. His research interests are in Hungarian-style combinatorics, particularly Ramsey theory, extremal graph theory, combinatorial number theory, and probabilistic methods in combinatorics.\n\nFox grew up in West Hartford, Connecticut and attended Hall High School. As a senior he won second place overall and first place in his category in the annual Intel Science Talent Search,\nalso winning the Karl Menger Memorial Prize of the American Mathematical Society for his project. The project was titled \"Rainbow Ramsey Theory: Rainbow Arithmetic Progressions and Anti-Ramsey Results\"\nand was based on a research project he did at a six-week summer camp in mathematics at the Massachusetts Institute of Technology (MIT);\nhe also participated in an earlier high school mathematics program at Ohio State University.\n\nFox became an undergraduate at MIT, and was awarded the 2006 Morgan Prize for several research publications in combinatorics.\nFox completed his Ph.D. in 2010 from Princeton University; his dissertation, supervised by Benny Sudakov, was titled \"Ramsey Numbers\".\nAfter working in the mathematics department at MIT from 2010 to 2014, he joined the faculty of Stanford University in 2015.\n\nIn 2010, Fox was awarded the Dénes Kőnig Prize, an early-career award of the Society for Industrial and Applied Mathematics Activity Group on Discrete Mathematics.\nHe was an invited speaker at the International Congress of Mathematicians in 2014. He was awarded the Oberwolfach Prize in 2016.\n\n"}
{"id": "457816", "url": "https://en.wikipedia.org/wiki?curid=457816", "title": "James Challis", "text": "James Challis\n\nJames Challis FRS (12 December 1803 – 3 December 1882) was an English clergyman, physicist and astronomer. Plumian Professor of Astronomy and Experimental Philosophy and the director of the Cambridge Observatory, he investigated a wide range of physical phenomena though made few lasting contributions outside astronomy. He is best remembered for his missed opportunity to discover the planet Neptune in 1846.\n\nChallis was born in Braintree, Essex where his father, John Challis, was a stonemason. After attending various local schools, he graduated from Trinity College, Cambridge in 1825 as Senior Wrangler and first Smith's prizeman. He was elected a fellow of Trinity in 1826 and was ordained in 1830. He held the benefice of Papworth Everard, Cambridgeshire from the college until 1852. In 1831 Challis married Sarah Copsey, \"née\" Chandler, a widow, and consequently resigned his Trinity fellowship. The couple had a son and a daughter.\n\nIn 1836, he became director of the Cambridge Observatory and Plumian Professor, holding the latter post until his death. He lectured in all areas of physics. As examiner for the Smith's prize, he appraised the early work of G. G. Stokes, Arthur Cayley, John Couch Adams, William Thomson (later Lord Kelvin), Peter Guthrie Tait and James Clerk Maxwell. For over a decade, in correspondence and publications, Challis repeatedly disagreed with Stokes's conclusions from his research.\n\nChallis was referee for Thomson and for Stokes in their respective applications for chairs at the University of Glasgow, and for Maxwell at Aberdeen. He and Thomson together set and examined the Adams prize topic on Saturn's rings, won by Maxwell in 1857.\n\nChallis succeeded George Biddell Airy at the observatory and gradually improved the instrumentation and accuracy of observations. He made some early observations of the fracture of comet 3D/Biela into two pieces on 15 January 1846 and re-observed both fragments in 1852. He published over 60 scientific papers recording other observations of comets and asteroids. He invented the meteoroscope (1848) and the transit-reducer (1849). Challis published twelve volumes of \"Astronomical Observations Made at the Observatory of Cambridge\".\n\nHe and his wife lived at the observatory as genial hosts for 25 years, though Challis once left his wife to guard an intruder while he summoned assistance. Challis eventually resigned the observatory post because of the chronic stress that his inability to keep up with processing new astronomical observations was causing him. His predecessor Airy had taken a more relaxed attitude. He was succeeded by Adams though he maintained his professorship until his death.\n\nIn 1846, Airy finally persuaded a skeptical Challis to join in the search for an eighth planet in the solar system. Adams had predicted the location of such a planet as early as 1844, based on irregularities in the orbit of Uranus. Adams failed to promote his prediction successfully and there was little enthusiasm for a systematic search of the heavens until Airy's intervention. Challis finally began his, somewhat reluctant, search in July 1846, unaware that Frenchman Urbain Le Verrier had independently made an identical prediction. German astronomer Johann Gottfried Galle, assisted by Heinrich Louis d'Arrest, finally confirmed Le Verrier's prediction on 23 September. The planet was named \"Neptune\". It soon became apparent from Challis's notebooks that he had observed Neptune twice, a month earlier, failing to make the identification through lack of diligence and a current star chart.\n\nChallis was full of remorse but blamed his neglect on the pressing business of catching up on the backlog of astronomical observations from the observatory. As he reflected in a letter to Airy of 12 October 1846:\n\nChallis also worked in hydrodynamics and in optics where he supported the wave theory of light and advanced the theory of a luminiferous ether as a medium for its propagation. However, he rejected the idea that the ether was an elastic solid, insisting that it was a fluid, bringing him into conflict with Airy and Stokes. Driven by Sir Isaac Newton's somewhat obscure assertion of \"a certain most subtle spirit which pervades and lies hid in all gross bodies\", Challis was driven to attempt to derive all physical phenomena from a model of inert spherical atoms embedded in an elastic fluid ether, an enterprise described as an attempt at a \"Victorian unified field theory\". His work included a mechanical explanation of gravitation. His ideas won few supporters.\n\nChallis took issue with Charles Wycliffe Goodwin's views on Genesis expressed in \"Essays and Reviews\" (1860). Challis saw Genesis as an \"antecedent plan\" for creation, rather than a literal chronology, and argued that the biblical account could be reconciled with the geological record. He went on to interpret the word \"law\", as used in a spiritual sense by Saint Paul, in the sense of scientific law.\n\nChallis published 225 papers in mathematics, physics and astronomy. He was re-elected fellow of Trinity in 1870. He died in Cambridge and was buried beside his wife in Mill Road Cemetery, Cambridge. His wealth when he died was £781 ().\n\nDespite the embarrassment over Neptune, Challis did make genuine contributions to astronomy. His blend of theology and science was in the spirit of Stokes, and his search for a unified theory akin to the endeavours of Thomson and Maxwell. However, despite his tenacity in advocating his physical and theological theories, they had little impact, and in fact Richard Carrington credited him as his professor with inspiring his decision to pursue astronomy rather than become a clergyman. Olin J. Eggen claimed that \"At a later time, or under less amiable circumstances, he would have been branded a charlatan. He would now be as forgotten as his peculiar ideas had not the events surrounding the discovery of Neptune in 1845 given him a genuine opportunity for scientific immortality. But he fumbled it.\"\n\n\n\n\n\n"}
{"id": "18739069", "url": "https://en.wikipedia.org/wiki?curid=18739069", "title": "Jeffery–Williams Prize", "text": "Jeffery–Williams Prize\n\nThe Jeffery–Williams Prize is a mathematics award presented annually by the Canadian Mathematical Society. The award is presented to individuals in recognition of outstanding contributions to mathematical research. The first award was presented in 1968. The prize was named in honor of the mathematicians Ralph Lent Jeffery and Lloyd Williams.\n\nSource: Canadian Mathematical Society\n"}
{"id": "42123338", "url": "https://en.wikipedia.org/wiki?curid=42123338", "title": "Lattice path", "text": "Lattice path\n\nIn combinatorics, a lattice path formula_1 in formula_2 of length formula_3 with steps in formula_4 is a sequence formula_5 such that each consecutive difference formula_6 lies in formula_7. \nA lattice path may lie in any lattice in formula_8, but the integer lattice formula_2 is most commonly used.\n\nAn example of a lattice path in formula_10 of length 5 with steps in formula_11\nis formula_12.\n\nA North-East (NE) lattice path is a lattice path in formula_10 with steps in formula_14. The formula_15 steps are called North steps and denoted by formula_16's;\nthe formula_17 steps are called East steps and denoted by formula_18's.\n\nNE lattice paths most commonly begin at the origin. This convention allows us to encode all the information about a NE lattice path formula_19\nin a single permutation word. The length of the word gives us the number of steps of the lattice path, formula_3. The order of the formula_16's and formula_18's communicates the sequence of formula_19. Furthermore, the number of formula_16's and the number of formula_18's in the word determines the end point of formula_19.\n\nIf the permutation word for a NE lattice path contains formula_27 formula_16steps and formula_29 \nformula_18 steps, and if the path begins at the origin, then the path necessarily ends at formula_31. This follows because you have \"walked\" exactly formula_27 steps North and formula_29 steps East from formula_34.\n\nLattice paths are often used to count other combinatorial objects. Similarly, there are many combinatorial objects that count the number of lattice paths of a certain kind. This occurs when the lattice paths are in bijection with the object in question. For example,\n\n\nNE lattice paths have close connections to the number of combinations, which are counted by the binomial coefficient, and arranged in Pascal's triangle. The diagram below demonstrates some of these connections.\n\nThe number of lattice paths from formula_34 to formula_55 is equal to the binomial coefficient formula_56. The diagram shows this for formula_57. If one rotates the diagram 135° clockwise about the origin and extend it to include all formula_58, one obtains Pascal's triangle. This result is no surprise, because the formula_59 entry of the formula_35 row of Pascal's Triangle is the binomial coefficient \nformula_61.\n\nThe graphical representation of NE lattice paths lends itself to many bijective proofs involving combinations. Here are a few examples.\n\n\n\"Proof\": The right-hand side is equal to the number of NE lattice paths from formula_34 to formula_43. Each of these NE lattice paths intersects exactly one of the lattice points in the rectangular array with coordinates formula_65 for formula_66. This is shown in the figure below for formula_67: Every NE lattice path from formula_34 to formula_69 intersects exactly one of the colored nodes.\n\nOn the left-hand side, the binomial coefficient squared, formula_70, represents two copies of the set of NE lattice paths from\nformula_34 to formula_72 attached endpoint to start point. Rotate the second copy 90° clockwise. This does not change the combinatorics of the object: formula_73. So the total number of lattice paths remains the same.\n\nSuperimpose the NE lattice paths squared onto the same rectangular array, as seen in the figure below. We see that all NE lattice paths from formula_34 to formula_43 are accounted for. In particular, notice that any lattice path passing through the red lattice point (for example) is counted by the squared set of lattice paths (also shown in red). formula_76\n"}
{"id": "8240558", "url": "https://en.wikipedia.org/wiki?curid=8240558", "title": "Line–line intersection", "text": "Line–line intersection\n\nIn Euclidean geometry, the intersection of a line and a line can be the empty set, a point, or a line. Distinguishing these cases and finding the intersection point have use, for example, in computer graphics, motion planning, and collision detection.\n\nIn three-dimensional Euclidean geometry, if two lines are not in the same plane they are called skew lines and have no point of intersection. If they are in the same plane there are three possibilities: if they coincide (are not distinct lines) they have an infinitude of points in common (namely all of the points on either of them); if they are distinct but have the same slope they are said to be parallel and have no points in common; otherwise they have a single point of intersection.\n\nThe distinguishing features of non-Euclidean geometry are the number and locations of possible intersections between two lines and the number of possible lines with no intersections (parallel lines) with a given line.\n\nA necessary condition for two lines to intersect is that they are in the same plane—that is, are not skew lines. Satisfaction of this condition is equivalent to the tetrahedron with vertices at two of the points on one line and two of the points on the other line being degenerate in the sense of having zero volume. For the algebraic form of this condition, see .\n\nFirst we consider the intersection of two lines formula_1 and formula_2 in 2-dimensional space, with line formula_1 being defined by two distinct points formula_4 and formula_5, and line formula_2 being defined by two distinct points formula_7 and formula_8.\n\nThe intersection formula_9 of line formula_1 and formula_2 can be defined using determinants.\n\nThe determinants can be written out as:\n\nNote that the intersection point is for the infinitely long lines defined by the points, rather than the line segments between the points, and can produce an intersection point beyond the lengths of the line segments. In order to find the position of the intersection in respect to the line segments, we can define lines formula_1 and formula_2 in terms of first degree Bézier parameters:\n\n(where \"t\" and \"u\" are real numbers). Then the intersection point is found with one of the following values of \"t\" or \"u\":\n\nwith:\n\nSubsequently, one can check if the intersection points falls within the first line segment by testing if 0.0 ≤ \"t\" ≤ 1.0, and check if it falls within the second line segment by testing if 0.0 ≤ \"u\" ≤ 1.0.\nWhen the two lines are parallel or coincident the denominator is zero:\n\nIf the lines are almost parallel, then a computer solution might encounter numeric problems implementing the solution described above: the recognition of this condition might require an approximate test in a practical application. An alternate approach might be to rotate the line segments so that one of them is horizontal, whence the solution of the rotated parametric form of the second line is easily obtained. Careful discussion of the special cases is required (parallel lines/coincident lines, overlapping/non-overlapping intervals).\n\nThe formula_20 and formula_21 coordinates of the point of intersection of two non-vertical lines can easily be found using the following substitutions and rearrangements.\n\nSuppose that two lines have the equations formula_22 and formula_23 where formula_24 and formula_25 are the slopes (gradients) of the lines and where formula_26 and formula_27 are the \"y\"-intercepts of the lines. At the point where the two lines intersect (if they do), both formula_21 coordinates will be the same, hence the following equality:\n\nWe can rearrange this expression in order to extract the value of formula_20,\n\nand so, \n\nTo find the \"y\" coordinate, all we need to do is substitute the value of \"x\" into either one of the two line equations, for example, into the first:\n\nHence, the point of intersection is \nNote if \"a\" = \"b\" then the two lines are parallel. If \"c\" ≠ \"d\" as well, the lines are different and there is no intersection, otherwise the two lines are identical.\n\nBy using homogeneous coordinates, the intersection point of two implicitly defined lines can be determined quite easily. In 2D, every point can be defined as a projection of a 3D point, given as the ordered triple formula_35. The mapping from 3D to 2D coordinates is formula_36. We can convert 2D points to homogeneous coordinates by defining them as formula_37.\n\nAssume that we want to find intersection of two infinite lines in 2-dimensional space, defined as formula_38 and formula_39. We can represent these two lines in line coordinates as formula_40 and formula_41,\n\nThe intersection formula_42 of two lines is then simply given by,\n\nformula_43\n\nIf formula_44 the lines do not intersect.\n\nIn two dimensions, more than two lines almost certainly do not intersect at a single point. To determine if they do and, if so, to find the intersection point, write the \"i\"-th equation (\"i\" = 1, ...,\"n\") as formula_45 and stack these equations into matrix form as\n\nwhere the \"i\"-th row of the \"n\" × 2 matrix \"A\" is formula_47, \"w\" is the 2 × 1 vector (\"x, y\"), and the \"i\"-th element of the column vector \"b\" is \"b\". If \"A\" has independent columns, its rank is 2. Then if and only if the rank of the augmented matrix [\"A\" | \"b\" ] is also 2, there exists a solution of the matrix equation and thus an intersection point of the \"n\" lines. The intersection point, if it exists, is given by\n\nwhere formula_49 is the Moore-Penrose generalized inverse of formula_50 (which has the form shown because \"A\" has full column rank). Alternatively, the solution can be found by jointly solving any two independent equations. But if the rank of \"A\" is only 1, then if the rank of the augmented matrix is 2 there is no solution but if its rank is 1 then all of the lines coincide with each other.\n\nThe above approach can be readily extended to three dimensions. In three or more dimensions, even two lines almost certainly do not intersect; pairs of non-parallel lines that do not intersect are called skew lines. But if an intersection does exist it can be found, as follows.\n\nIn three dimensions a line is represented by the intersection of two planes, each of which has an equation of the form formula_51 Thus a set of \"n\" lines can be represented by 2\"n\" equations in the 3-dimensional coordinate vector \"w\" = (\"x\", \"y\", \"z\"):\n\nwhere now \"A\" is 2\"n\" × 3 and \"b\" is 2\"n\" × 1. As before there is a unique intersection point if and only if \"A\" has full column rank and the augmented matrix [\"A\" | \"b\" ] does not, and the unique intersection if it exists is given by\n\nIn two or more dimensions, we can usually find a point that is mutually closest to two or more lines in a least-squares sense.\n\nIn the two-dimensional case, first, represent line \"i\" as a point, formula_54, on the line and a unit normal vector, formula_55, perpendicular to that line. That is, if formula_56 and formula_57 are points on line 1, then let formula_58 and let\n\nwhich is the unit vector along the line, rotated by 90 degrees.\n\nNote that the distance from a point, \"x\" to the line formula_60 is given by\n\nAnd so the squared distance from a point, \"x\", to a line is\n\nThe sum of squared distances to many lines is the cost function:\n\nThis can be rearranged:\n\nTo find the minimum, we differentiate with respect to \"x\" and set the result equal to the zero vector:\n\nso\n\nand so\n\nWhile formula_55 is not well-defined in more than two dimensions, this can be generalized to any number of dimensions by noting that formula_69 is simply the (symmetric) matrix with all eigenvalues unity except for a zero eigenvalue in the direction along the line providing a seminorm on the distance between formula_54 and another point giving the distance to the line. In any number of dimensions, if formula_71 is a unit vector \"along\" the \"i\"-th line, then\n\nwhere \"I\" is the identity matrix, and so\n\n"}
{"id": "20745603", "url": "https://en.wikipedia.org/wiki?curid=20745603", "title": "List of mathematics education journals", "text": "List of mathematics education journals\n\nThis is a list of notable academic journals in the field of mathematics education.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22354099", "url": "https://en.wikipedia.org/wiki?curid=22354099", "title": "Long code (mathematics)", "text": "Long code (mathematics)\n\nIn theoretical computer science and coding theory, the long code is an error-correcting code that is locally decodable. Long codes have an extremely poor rate, but play a fundamental role in the theory of hardness of approximation.\n\nLet formula_1 for formula_2 be the list of \"all\" functions from formula_3.\nThen the long code encoding of a message formula_4 is the string formula_5 where formula_6 denotes concatenation of strings.\nThis string has length formula_7.\n\nThe Walsh-Hadamard code is a subcode of the long code, and can be obtained by only using functions formula_8 that are linear functions when interpreted as functions formula_9 on the finite field with two elements. Since there are only formula_10 such functions, the block length of the Walsh-Hadamard code is formula_10.\n\nAn equivalent definition of the long code is as follows:\nThe Long code encoding of formula_12 is defined to be the truth table of the Boolean dictatorship function on the formula_13th coordinate, i.e., the truth table of formula_14 with formula_15.\nThus, the Long code encodes a formula_16-bit string as a formula_17-bit string.\n\nThe long code does not contain repetitions, in the sense that the function formula_8 computing the formula_19th bit of the output is different from any function formula_20 computing the formula_13th bit of the output for formula_22.\nAmong all codes that do not contain repetitions, the long code has the longest possible output.\nMoreover, it contains all non-repeating codes as a subcode.\n"}
{"id": "4288963", "url": "https://en.wikipedia.org/wiki?curid=4288963", "title": "Maximum common induced subgraph", "text": "Maximum common induced subgraph\n\nIn graph theory and theoretical computer science, a maximum common induced subgraph of two graphs \"G\" and \"H\" is a graph that is an induced subgraph of both \"G\" and \"H\",\nand that has as many vertices as possible.\n\nFinding this graph is NP-hard.\nIn the associated decision problem, the input is two graphs \"G\" and \"H\" and a number \"k\". The problem is to decide whether \"G\" and \"H\" have a common induced subgraph with at least \"k\" vertices. This problem is NP-complete. It is a generalization of the induced subgraph isomorphism problem, which arises when \"k\" equals the number of vertices in the smaller of \"G\" and \"H\", so that this entire graph must appear as an induced subgraph of the other graph.\n\nBased on hardness of approximation results for the maximum independent set problem, the maximum common induced subgraph problem is also hard to approximate. This implies that, unless P = NP, there is no approximation algorithm that, in polynomial time on formula_1-vertex graphs, always finds a solution within a factor of formula_2 of optimal, for any formula_3.\n\nOne possible solution for this problem is to build a modular product graph of \"G\" and \"H\".\nIn this graph, the largest clique corresponds to a maximum common induced subgraph of \"G\" and \"H\". Therefore, algorithms for finding maximum cliques can be used to find the maximum common induced subgraph.\n\nMaximum common induced subgraph algorithms have a long tradition in cheminformatics and pharmacophore mapping.\n\n"}
{"id": "24464863", "url": "https://en.wikipedia.org/wiki?curid=24464863", "title": "Michael Somos", "text": "Michael Somos\n\nMichael Somos is an American mathematician, who was a visiting scholar in the Georgetown University Mathematics and Statistics department for four years and is currently an affiliate researcher in that department. In the late eighties he proposed a conjecture about certain polynomial recurrences, now called Somos sequences, that surprisingly in some cases contain only integers. Somos' quadratic recurrence constant is also named after him.\n\n\n"}
{"id": "365627", "url": "https://en.wikipedia.org/wiki?curid=365627", "title": "Missing square puzzle", "text": "Missing square puzzle\n\nThe missing square puzzle is an optical illusion used in mathematics classes to help students reason about geometrical figures; or rather to teach them not to reason using figures, but to use only textual descriptions and the axioms of geometry. It depicts two arrangements made of similar shapes in slightly different configurations. Each apparently forms a 13×5 right-angled triangle, but one has a 1×1 hole in it.\n\nThe key to the puzzle is the fact that neither of the 13×5 \"triangles\" is truly a triangle, because what appears to be the hypotenuse is bent. In other words, the \"hypotenuse\" does not maintain a consistent slope, even though it may appear that way to the human eye. \nA true 13×5 triangle cannot be created from the given component parts. The four figures (the yellow, red, blue and green shapes) total 32 units of area. The apparent triangles formed from the figures are 13 units wide and 5 units tall, so it appears that the area should be formula_1 units. However, the blue triangle has a ratio of 5:2 (=2.5), while the red triangle has the ratio 8:3 (≈2.667), so the apparent combined hypotenuse in each figure is actually bent. With the bent hypotenuse, the first figure actually occupies a combined 32 units, while the second figure occupies 33, including the \"missing\" square.\n\nThe amount of bending is approximately 1/28th of a unit (1.245364267°), which is difficult to see on the diagram of the puzzle, and was illustrated as a graphic. Note the grid point where the red and blue triangles in the lower image meet (5 squares to the right and two units up from the lower left corner of the combined figure), and compare it to the same point on the other figure; the edge is slightly under the mark in the upper image, but goes through it in the lower. Overlaying the hypotenuses from both figures results in a very \"thin parallelogram\" (represented with the four red dots) with an area of exactly one grid square, so the \"missing\" area.\n\nAccording to Martin Gardner, this particular puzzle was invented by a New York City amateur magician, Paul Curry, in 1953. However, the principle of a dissection paradox has been known since the start of the 16th century. \n\nThe integer dimensions of the parts of the puzzle (2, 3, 5, 8, 13) are successive Fibonacci numbers, which leads to the exact unit area in the \"thin parallelogram\".\nMany other geometric dissection puzzles are based on a few simple properties of the Fibonacci sequence.\n\nSam Loyd's paradoxical dissection demonstrates two rearrangements of an 8×8 square. In the \"larger\" rearrangement (the 5×13 rectangle in the image to the right), the gaps between the figures have a combined unit square more area than their square gaps counterparts, creating an illusion that the figures there take up more space than those in the original square figure. In the \"smaller\" rearrangement (the shape below the 5×13 rectangle), each quadrilateral needs to overlap the triangle by an area of half a unit for its top/bottom edge to align with a grid line, resulting overall loss in one unit square area.\n\nMitsunobu Matsuyama's \"Paradox\" uses four congruent quadrilaterals and a small square, which form a larger square. When the quadrilaterals are rotated about their centers they fill the space of the small square, although the total area of the figure seems unchanged. The apparent paradox is explained by the fact that the side of the new large square is a little smaller than the original one. If \"θ\" is the angle between two opposing sides in each quadrilateral, then the quotient between the two areas is given by sec\"θ\". For \"θ\" = 5°, this is approximately 1.00765, which corresponds to a difference of about 0.8%.\n\n\n"}
{"id": "52564", "url": "https://en.wikipedia.org/wiki?curid=52564", "title": "Partial differential equation", "text": "Partial differential equation\n\nIn mathematics, a partial differential equation (PDE) is a differential equation that contains beforehand unknown multivariable functions and their partial derivatives. PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a computer model. A special case is ordinary differential equations (ODEs), which deal with functions of a single variable and their derivatives.\n\nPDEs can be used to describe a wide variety of phenomena such as sound, heat, diffusion, electrostatics, electrodynamics, fluid dynamics, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.\n\nPartial differential equations (PDEs) are equations that involve rates of change with respect to continuous variables. The position of a rigid body is specified by six parameters, but the configuration of a fluid is given by the continuous distribution of several parameters, such as the temperature, pressure, and so forth. The dynamics for the rigid body take place in a finite-dimensional configuration space; the dynamics for the fluid occur in an infinite-dimensional configuration space. This distinction usually makes PDEs much harder to solve than ordinary differential equations (ODEs), but here again, there will be simple solutions for linear problems. Classic domains where PDEs are used include acoustics, fluid dynamics, electrodynamics, and heat transfer.\n\nA partial differential equation (PDE) for the function is an equation of the form\n\nIf is a linear function of and its derivatives, then the PDE is called linear. Common examples of linear PDEs include the heat equation, the wave equation, Laplace's equation, Helmholtz equation, Klein–Gordon equation, and Poisson's equation.\n\nA relatively simple PDE is\n\nThis relation implies that the function is independent of . However, the equation gives no information on the function's dependence on the variable . Hence the general solution of this equation is\n\nwhere is an arbitrary function of . The analogous ordinary differential equation is\n\nwhich has the solution\n\nwhere is any constant value. These two examples illustrate that general solutions of ordinary differential equations (ODEs) involve arbitrary constants, but solutions of PDEs involve arbitrary functions. A solution of a PDE is generally not unique; additional conditions must generally be specified on the boundary of the region where the solution is defined. For instance, in the simple example above, the function can be determined if is specified on the line .\n\nAlthough the issue of existence and uniqueness of solutions of ordinary differential equations has a very satisfactory answer with the Picard–Lindelöf theorem, that is far from the case for partial differential equations. The Cauchy–Kowalevski theorem states that the Cauchy problem for any partial differential equation whose coefficients are analytic in the unknown function and its derivatives, has a locally unique analytic solution. Although this result might appear to settle the existence and uniqueness of solutions, there are examples of linear partial differential equations whose coefficients have derivatives of all orders (which are nevertheless not analytic) but which have no solutions at all: see Lewy (1957). Even if the solution of a partial differential equation exists and is unique, it may nevertheless have undesirable properties. The mathematical study of these questions is usually in the more powerful context of weak solutions.\n\nAn example of pathological behavior is the sequence (depending upon ) of Cauchy problems for the Laplace equation\n\nwith boundary conditions\n\nwhere is an integer. The derivative of with respect to approaches zero uniformly in as increases, but the solution is\n\nThis solution approaches infinity if is not an integer multiple of for any non-zero value of . The Cauchy problem for the Laplace equation is called \"ill-posed\" or \"not well-posed\", since the solution does not continuously depend on the data of the problem. Such ill-posed problems are not usually satisfactory for physical applications.\n\nThe existence of solutions for the Navier–Stokes equations, a partial differential equation, is part of one of the Millennium Prize Problems.\n\nIn PDEs, it is common to denote partial derivatives using subscripts. That is:\n\nEspecially in physics, del or nabla () is often used to denote spatial derivatives, and for time derivatives. For example, the wave equation (described below) can be written as\n\nor\n\nwhere is the Laplace operator.\n\nSome linear, second-order partial differential equations can be classified as parabolic, hyperbolic and elliptic. Others, such as the Euler–Tricomi equation, have different types in different regions. The classification provides a guide to appropriate initial and boundary conditions and to the smoothness of the solutions.\n\nAssuming , the general linear second-order PDE in two independent variables has the form\n\nwhere the coefficients , , ... may depend upon and . If over a region of the -plane, the PDE is second-order in that region. This form is analogous to the equation for a conic section:\n\nMore precisely, replacing by , and likewise for other variables (formally this is done by a Fourier transform), converts a constant-coefficient PDE into a polynomial of the same degree, with the top degree (a homogeneous polynomial, here a quadratic form) being most significant for the classification.\n\nJust as one classifies conic sections and quadratic forms into parabolic, hyperbolic, and elliptic based on the discriminant , the same can be done for a second-order PDE at a given point. However, the discriminant in a PDE is given by due to the convention of the term being rather than ; formally, the discriminant (of the associated quadratic form) is , with the factor of 4 dropped for simplicity.\n\n\nIf there are independent variables , a general linear partial differential equation of second order has the form\n\nThe classification depends upon the signature of the eigenvalues of the coefficient matrix .\n\n\nThe classification of partial differential equations can be extended to systems of first-order equations, where the unknown is now a vector with components, and the coefficient matrices are by matrices for . The partial differential equation takes the form\n\nwhere the coefficient matrices and the vector may depend upon and . If a hypersurface is given in the implicit form\n\nwhere has a non-zero gradient, then is a characteristic surface for the operator at a given point if the characteristic form vanishes:\n\nThe geometric interpretation of this condition is as follows: if data for are prescribed on the surface , then it may be possible to determine the normal derivative of on from the differential equation. If the data on and the differential equation determine the normal derivative of on , then is non-characteristic. If the data on and the differential equation \"do not\" determine the normal derivative of on , then the surface is characteristic, and the differential equation restricts the data on : the differential equation is \"internal\" to .\n\n\nIf a PDE has coefficients that are not constant, it is possible that it will not belong to any of these categories but rather be of mixed type. A simple but important example is the Euler–Tricomi equation\n\nwhich is called elliptic-hyperbolic because it is elliptic in the region , hyperbolic in the region , and degenerate parabolic on the line .\nIn the phase space formulation of quantum mechanics, one may consider the quantum Hamilton's equations for trajectories of quantum particles. These equations are infinite-order PDEs. However, in the semiclassical expansion, one has a finite system of ODEs at any fixed order of . The evolution equation of the Wigner function is also an infinite-order PDE. The quantum trajectories are quantum characteristics, with the use of which one could calculate the evolution of the Wigner function.\n\nLinear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables. This technique rests on a characteristic of solutions to differential equations: if one can find any solution that solves the equation and satisfies the boundary conditions, then it is \"the\" solution (this also applies to ODEs). We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter, and then see if this can be made to solve the problem.\n\nIn the method of separation of variables, one reduces a PDE to a PDE in fewer variables, which is an ordinary differential equation if in one variable – these are in turn easier to solve.\n\nThis is possible for simple PDEs, which are called separable partial differential equations, and the domain is generally a rectangle (a product of intervals). Separable PDEs correspond to diagonal matrices – thinking of \"the value for fixed \" as a coordinate, each coordinate can be understood separately.\n\nThis generalizes to the method of characteristics, and is also used in integral transforms.\n\nIn special cases, one can find characteristic curves on which the equation reduces to an ODE – changing coordinates in the domain to straighten these curves allows separation of variables, and is called the method of characteristics.\n\nMore generally, one may find characteristic surfaces.\n\nAn integral transform may transform the PDE to a simpler one, in particular, a separable PDE. This corresponds to diagonalizing an operator.\n\nAn important example of this is Fourier analysis, which diagonalizes the heat equation using the eigenbasis of sinusoidal waves.\n\nIf the domain is finite or periodic, an infinite sum of solutions such as a Fourier series is appropriate, but an integral of solutions such as a Fourier integral is generally required for infinite domains. The solution for a point source for the heat equation given above is an example of the use of a Fourier integral.\n\nOften a PDE can be reduced to a simpler form with a known solution by a suitable change of variables. For example, the Black–Scholes PDE\n\nis reducible to the heat equation\n\nby the change of variables (for complete details see )\n\nInhomogeneous equations can often be solved (for constant coefficient PDEs, always be solved) by finding the fundamental solution (the solution for a point source), then taking the convolution with the boundary conditions to get the solution.\n\nThis is analogous in signal processing to understanding a filter by its impulse response.\n\nThe superposition principle applies to any linear system, including linear systems of PDEs. A common visualization of this concept is the interaction of two waves in phase being combined to result in a greater amplitude, for example . The same principle can be observed in PDEs where the solutions may be real or complex and additive. superposition\nIf and are solutions of linear PDE in some function space , then with any constants and are also a solution of that PDE in the same function space.\n\nThere are no generally applicable methods to solve nonlinear PDEs. Still, existence and uniqueness results (such as the Cauchy–Kowalevski theorem) are often possible, as are proofs of important qualitative and quantitative properties of solutions (getting these results is a major part of analysis). Computational solution to the nonlinear PDEs, the split-step method, exist for specific equations like nonlinear Schrödinger equation.\n\nNevertheless, some techniques can be used for several types of equations. The -principle is the most powerful method to solve underdetermined equations. The Riquier–Janet theory is an effective method for obtaining information about many analytic overdetermined systems.\n\nThe method of characteristics (similarity transformation method) can be used in some very special cases to solve partial differential equations.\n\nIn some cases, a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution. Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods. Many interesting problems in science and engineering are solved in this way using computers, sometimes high performance supercomputers.\n\nFrom 1870 Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source; and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.\n\nA general approach to solving PDEs uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform and finally finding exact analytic solutions to the PDE.\n\nSymmetry methods have been recognized to study differential equations arising in mathematics, physics, engineering, and many other disciplines.\n\nThe adomian decomposition method, the Lyapunov artificial small parameter method, and He's homotopy perturbation method are all special cases of the more general homotopy analysis method. These are series expansion methods, and except for the Lyapunov method, are independent of small physical parameters as compared to the well known perturbation theory, thus giving these methods greater flexibility and solution generality.\n\nThe three most widely used numerical methods to solve PDEs are the finite element method (FEM), finite volume methods (FVM) and finite difference methods (FDM), as well other kind of methods called Meshfree methods, which were made to solve problems where the before mentioned methods are limited. The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM. Other hybrid versions of FEM and Meshfree methods include the generalized finite element method (GFEM), extended finite element method (XFEM), spectral finite element method (SFEM), meshfree finite element method, discontinuous Galerkin finite element method (DGFEM), Element-Free Galerkin Method (EFGM), Interpolating Element-Free Galerkin Method (IEFGM), etc.\n\nThe finite element method (FEM) (its practical application often known as finite element analysis (FEA)) is a numerical technique for finding approximate solutions of partial differential equations (PDE) as well as of integral equations. The solution approach is based either on eliminating the differential equation completely (steady state problems), or rendering the PDE into an approximating system of ordinary differential equations, which are then numerically integrated using standard techniques such as Euler's method, Runge–Kutta, etc.\n\nFinite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives.\n\nSimilar to the finite difference method or finite element method, values are calculated at discrete places on a meshed geometry. \"Finite volume\" refers to the small volume surrounding each node point on a mesh. In the finite volume method, surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals, using the divergence theorem. These terms are then evaluated as fluxes at the surfaces of each finite volume. Because the flux entering a given volume is identical to that leaving the adjacent volume, these methods conserve mass by design.\n\n\n"}
{"id": "376948", "url": "https://en.wikipedia.org/wiki?curid=376948", "title": "Power of two", "text": "Power of two\n\nIn mathematics, a power of two is a number of the form where is an integer, i.e. the result of exponentiation with number two as the base and integer  as the exponent.\n\nIn a context where only integers are considered, is restricted to non-negative values, so we have 1, 2, and 2 multiplied by itself a certain number of times.\n\nBecause two is the base of the binary numeral system, powers of two are common in computer science. Written in binary, a power of two always has the form 100…000 or 0.00…001, just like a power of ten in the decimal system.\n\nTwo to the power of , written as , is the number of ways the bits in a binary word of length can be arranged. A word, interpreted as an unsigned integer, can represent values from 0 () to  () inclusively. Corresponding signed integer values can be positive, negative and zero; see signed number representations. Either way, one less than a power of two is often the upper bound of an integer in binary computers. As a consequence, numbers of this form show up frequently in computer software. As an example, a video game running on an 8-bit system might limit the score or the number of items the player can hold to 255—the result of using a byte, which is 8 bits long, to store the number, giving a maximum value of . For example, in the original \"Legend of Zelda\" the main character was limited to carrying 255 rupees (the currency of the game) at any given time, and the video game Pac-Man famously shuts down at level 255.\n\nPowers of two are often used to measure computer memory. A byte is now considered eight bits (an octet, resulting in the possibility of 256 values (2). (The term \"byte\" once meant (and in some cases, still means) a collection of bits, typically of 5 to 32 bits, rather than only an 8-bit unit.) The prefix \"kilo\", in conjunction with \"byte\", may be, and has traditionally been, used, to mean 1,024 (2). However, in general, the term \"kilo\" has been used in the International System of Units to mean 1,000 (10). Binary prefixes have been standardized, such as \"kibi\" (Ki) meaning 1,024. Nearly all processor registers have sizes that are powers of two, 32 or 64 being most common.\n\nPowers of two occur in a range of other places as well. For many disk drives, at least one of the sector size, number of sectors per track, and number of tracks per surface is a power of two. The logical block size is almost always a power of two.\n\nNumbers that are not powers of two occur in a number of situations, such as video resolutions, but they are often the sum or product of only two or three powers of two, or powers of two minus one. For example, , and . Put another way, they have fairly regular bit patterns.\n\nA prime number that is one less than a power of two is called a Mersenne prime. For example, the prime number 31 is a Mersenne prime because it is 1 less than 32 (2). Similarly, a prime number (like 257) that is one more than a positive power of two is called a Fermat prime—the exponent itself is a power of two. A fraction that has a power of two as its denominator is called a dyadic rational. The numbers that can be represented as sums of consecutive positive integers are called polite numbers; they are exactly the numbers that are not powers of two.\n\nThe geometric progression 1, 2, 4, 8, 16, 32, … (or, in the binary numeral system, 1, 10, 100, 1000, 10000, 100000, … ) is important in number theory. Book IX, Proposition 36 of \"Elements\" proves that if the sum of the first terms of this progression is a prime number (and thus is a Mersenne prime as mentioned above), then this sum times the th term is a perfect number. For example, the sum of the first 5 terms of the series 1 + 2 + 4 + 8 + 16 = 31, which is a prime number. The sum 31 multiplied by 16 (the 5th term in the series) equals 496, which is a perfect number.\n\nBook IX, Proposition 35, proves that in a geometric series if the first term is subtracted from the second and last term in the sequence, then as the excess of the second is to the first—so is the excess of the last to all those before it. (This is a restatement of our formula for geometric series from above.) Applying this to the geometric progression 31, 62, 124, 248, 496 (which results from 1, 2, 4, 8, 16 by multiplying all terms by 31), we see that 62 minus 31 is to 31 as 496 minus 31 is to the sum of 31, 62, 124, 248. Therefore, the numbers 1, 2, 4, 8, 16, 31, 62, 124 and 248 add up to 496 and further these are all the numbers that divide 496. For suppose that divides 496 and it is not amongst these numbers. Assume is equal to , or 31 is to as is to 16. Now cannot divide 16 or it would be amongst the numbers 1, 2, 4, 8 or 16.\nTherefore, 31 cannot divide . And since 31 does not divide and measures 496, the fundamental theorem of arithmetic implies that must divide 16 and be amongst the numbers 1, 2, 4, 8 or 16. Let be 4, then must be 124, which is impossible since by hypothesis is not amongst the numbers 1, 2, 4, 8, 16, 31, 62, 124 or 248.\n\nStarting with 2 the last digit is periodic with period 4, with the cycle 2–4–8–6–, and starting with 4 the last two digits are periodic with period 20. These patterns are generally true of any power, with respect to any base. The pattern continues, of course, where each pattern has starting point , and the period is the multiplicative order of 2 modulo , which is  = 4 ×  (see Multiplicative group of integers modulo n).\n\nThe first few powers of 2 are slightly larger than those same powers of 1000:\n\nBecause data (specifically integers) and the addresses of data are stored using the same hardware, and the data is stored in one or more octets (), double exponentials of two are common. For example,\n\nSeveral of these numbers represent the number of values representable using common computer data types. For example, a 32-bit word consisting of 4 bytes can represent distinct values, which can either be regarded as mere bit-patterns, or are more commonly interpreted as the unsigned numbers from 0 to , or as the range of signed numbers between and . Also see tetration and lower hyperoperations. For more about representing signed numbers see two's complement.\n\nIn a connection with nimbers these numbers are often called \"Fermat 2-powers\".\n\nThe numbers formula_1 form an irrationality sequence: for every sequence formula_2 of positive integers, the series\nconverges to an irrational number. Despite the rapid growth of this sequence, it is the slowest-growing irrationality sequence known.\n\n\nThe sum of all -choose binomial coefficients is equal to . Consider the set of all -digit binary integers. Its cardinality is . It is also the sums of the cardinalities of certain subsets: the subset of integers with no 1s (consisting of a single number, written as 0s), the subset with a single 1, the subset with two 1s, and so on up to the subset with 1s (consisting of the number written as 1s). Each of these is in turn equal to the binomial coefficient indexed by and the number of 1s being considered (e.g., there are 10-choose-3 binary numbers with ten digits that include exactly three 1s).\n\nThe number of vertices of an -dimensional hypercube is . Similarly, the number of -faces of an -dimensional cross-polytope is also and the formula for the number of -faces an -dimensional cross-polytope has is formula_4.\n\nThe sum of the reciprocals of the powers of two is 1. The sum of the reciprocals of the squared powers of two is 1/3.\n\nThe smallest natural power of two whose decimal representation begins with 7 is\n\n\n"}
{"id": "44740826", "url": "https://en.wikipedia.org/wiki?curid=44740826", "title": "Process performance qualification protocol", "text": "Process performance qualification protocol\n\nH\n\nProcess performance qualification protocol is a component of process validation: process qualification. This step is vital in maintaining ongoing production quality by recording and having available for review essential conditions, controls, testing, and expected manufacturing outcome of a production process. The Food and Drug Administration recommends the following criteria be included in a PPQ protocol:\n\nDeviations from the standard operation procedures should be made within the framework of the protocol and at the approval of relevant quality control departments. The protocol should cover all aspects of production including: equipment, personnel, raw materials, and operating environment. The FDA further recommends a documentation of the protocol be published internally. The report should include:\n\n"}
{"id": "183478", "url": "https://en.wikipedia.org/wiki?curid=183478", "title": "Proof theory", "text": "Proof theory\n\nProof theory is a major branch of mathematical logic that represents proofs as formal mathematical objects, facilitating their analysis by mathematical techniques. Proofs are typically presented as inductively-defined data structures such as plain lists, boxed lists, or trees, which are constructed according to the axioms and rules of inference of the logical system. As such, proof theory is syntactic in nature, in contrast to model theory, which is semantic in nature.\n\nSome of the major areas of proof theory include structural proof theory, ordinal analysis, provability logic, reverse mathematics, proof mining, automated theorem proving, and proof complexity. Much research also focuses on applications in computer science, linguistics, and philosophy.\n\nAlthough the formalisation of logic was much advanced by the work of such figures as Gottlob Frege, Giuseppe Peano, Bertrand Russell, and Richard Dedekind, the story of modern proof theory is often seen as being established by David Hilbert, who initiated what is called Hilbert's program in the foundations of mathematics. The central idea of this program was that if we could give finitary proofs of consistency for all the sophisticated formal theories needed by mathematicians, then we could ground these theories by means of a metamathematical argument, which shows that all of their purely universal assertions (more technically their provable formula_1 sentences) are finitarily true; once so grounded we do not care about the non-finitary meaning of their existential theorems, regarding these as pseudo-meaningful stipulations of the existence of ideal entities.\n\nThe failure of the program was induced by Kurt Gödel's incompleteness theorems, which showed that any ω-consistent theory that is sufficiently strong to express certain simple arithmetic truths, cannot prove its own consistency, which on Gödel's formulation is a formula_1 sentence. However, modified versions of Hilbert's program emerged and research has been carried out on related topics. This has led, in particular, to:\n\nIn parallel to the rise and fall of Hilbert's program, the foundations of structural proof theory were being founded. Jan Łukasiewicz suggested in 1926 that one could improve on Hilbert systems as a basis for the axiomatic presentation of logic if one allowed the drawing of conclusions from assumptions in the inference rules of the logic. In response to this Stanisław Jaśkowski (1929) and Gerhard Gentzen (1934) independently provided such systems, called calculi of natural deduction, with Gentzen's approach introducing the idea of symmetry between the grounds for asserting propositions, expressed in introduction rules, and the consequences of accepting propositions in the elimination rules, an idea that has proved very important in proof theory. Gentzen (1934) further introduced the idea of the sequent calculus, a calculus advanced in a similar spirit that better expressed the duality of the logical connectives, and went on to make fundamental advances in the formalisation of intuitionistic logic, and provide the first combinatorial proof of the consistency of Peano arithmetic. Together, the presentation of natural deduction and the sequent calculus introduced the fundamental idea of analytic proof to proof theory.\n\nStructural proof theory is the subdiscipline of proof theory that studies the specifics of proof calculi. The three most well-known styles of proof calculi are:\n\nEach of these can give a complete and axiomatic formalization of propositional or predicate logic of either the classical or intuitionistic flavour, almost any modal logic, and many substructural logics, such as relevance logic or linear logic. Indeed, it is unusual to find a logic that resists being represented in one of these calculi.\n\nProof theorists are typically interested in proof calculi that support a notion of analytic proof. The notion of analytic proof was introduced by Gentzen for the sequent calculus; there the analytic proofs are those that are cut-free. Much of the interest in cut-free proofs comes from the subformula property: every formula in the end sequent of a cut-free proof is a subformula of one of the premises. This allows one to show consistency of the sequent calculus easily; if the empty sequent were derivable it would have to be a subformula of some premise, which it is not. Gentzen's midsequent theorem, the Craig interpolation theorem, and Herbrand's theorem also follow as corollaries of the cut-elimination theorem.\n\nGentzen's natural deduction calculus also supports a notion of analytic proof, as shown by Dag Prawitz. The definition is slightly more complex: we say the analytic proofs are the normal forms, which are related to the notion of normal form in term rewriting. More exotic proof calculi such as Jean-Yves Girard's proof nets also support a notion of analytic proof.\n\nStructural proof theory is connected to type theory by means of the Curry-Howard correspondence, which observes a structural analogy between the process of normalisation in the natural deduction calculus and beta reduction in the typed lambda calculus. This provides the foundation for the intuitionistic type theory developed by Per Martin-Löf, and is often extended to a three way correspondence, the third leg of which are the cartesian closed categories.\n\nOther research topics in structural theory include analytic tableau, which apply the central idea of analytic proof from structural proof theory to provide decision procedures and semi-decision procedures for a wide range of logics, and the proof theory of substructural logics.\n\nOrdinal analysis is a powerful technique for providing combinatorial consistency proofs for subsystems of arithmetic, analysis, and set theory. Gödel's second incompleteness theorem is often interpreted as demonstrating that finitistic consistency proofs are impossible for theories of sufficient strength. Ordinal analysis allows one to measure precisely the infinitary content of the consistency of theories. For a consistent recursively axiomatized theory T, one can prove in finitistic arithmetic that the well-foundedness of a certain transfinite ordinal implies the consistency of T. Gödel's second incompleteness theorem implies that the well-foundedness of such an ordinal cannot be proved in the theory T.\n\nConsequences of ordinal analysis include (1) consistency of subsystems of classical second order arithmetic and set theory relative to constructive theories, (2) combinatorial independence results, and (3) classifications of provably total recursive functions and provably well-founded ordinals.\n\nOrdinal analysis was originated by Gentzen, who proved the consistency of Peano Arithmetic using transfinite induction up to ordinal ε. Ordinal analysis has been extended to many fragments of first and second order arithmetic and set theory. One major challenge has been the ordinal analysis of impredicative theories. The first breakthrough in this direction was Takeuti's proof of the consistency of Π-CA using the method of ordinal diagrams.\n\n\"Provability logic\" is a modal logic, in which the box operator is interpreted as 'it is provable that'. The point is to capture the notion of a proof predicate of a reasonably rich formal theory. As basic axioms of the provability logic GL (Gödel-Löb), which captures provable in Peano Arithmetic, one takes modal analogues of the Hilbert-Bernays derivability conditions and Löb's theorem (if it is provable that the provability of A implies A, then A is provable).\n\nSome of the basic results concerning the incompleteness of Peano Arithmetic and related theories have analogues in provability logic. For example, it is a theorem in GL that if a contradiction is not provable then it is not provable that a contradiction is not provable (Gödel's second incompleteness theorem). There are also modal analogues of the fixed-point theorem. Robert Solovay proved that the modal logic GL is complete with respect to Peano Arithmetic. That is, the propositional theory of provability in Peano Arithmetic is completely represented by the modal logic GL. This straightforwardly implies that propositional reasoning about provability in Peano Arithmetic is complete and decidable.\n\nOther research in provability logic has focused on first-order provability logic, polymodal provability logic (with one modality representing provability in the object theory and another representing provability in the meta-theory), and interpretability logics intended to capture the interaction between provability and interpretability. Some very recent research has involved applications of graded provability algebras to the ordinal analysis of arithmetical theories.\n\nReverse mathematics is a program in mathematical logic that seeks to determine which axioms are required to prove theorems of mathematics. The field was founded by Harvey Friedman. Its defining method can be described as \"going backwards from the theorems to the axioms\", in contrast to the ordinary mathematical practice of deriving theorems from axioms. The reverse mathematics program was foreshadowed by results in set theory such as the classical theorem that the axiom of choice and Zorn's lemma are equivalent over ZF set theory. The goal of reverse mathematics, however, is to study possible axioms of ordinary theorems of mathematics rather than possible axioms for set theory.\n\nIn reverse mathematics, one starts with a framework language and a base theory—a core axiom system—that is too weak to prove most of the theorems one might be interested in, but still powerful enough to develop the definitions necessary to state these theorems. For example, to study the theorem \"Every bounded sequence of real numbers has a supremum\" it is necessary to use a base system that can speak of real numbers and sequences of real numbers.\n\nFor each theorem that can be stated in the base system but is not provable in the base system, the goal is to determine the particular axiom system (stronger than the base system) that is necessary to prove that theorem. To show that a system \"S\" is required to prove a theorem \"T\", two proofs are required. The first proof shows \"T\" is provable from \"S\"; this is an ordinary mathematical proof along with a justification that it can be carried out in the system \"S\". The second proof, known as a reversal, shows that \"T\" itself implies \"S\"; this proof is carried out in the base system. The reversal establishes that no axiom system \"S′\" that extends the base system can be weaker than \"S\" while still proving \"T\".\n\nOne striking phenomenon in reverse mathematics is the robustness of the \"Big Five\" axiom systems. In order of increasing strength, these systems are named by the initialisms RCA, WKL, ACA, ATR, and Π-CA. Nearly every theorem of ordinary mathematics that has been reverse mathematically analyzed has been proven equivalent to one of these five systems. Much recent research has focused on combinatorial principles that do not fit neatly into this framework, like RT (Ramsey's theorem for pairs).\n\nResearch in reverse mathematics often incorporates methods and techniques from recursion theory as well as proof theory.\n\nFunctional interpretations are interpretations of non-constructive theories in functional ones. Functional interpretations usually proceed in two stages. First, one \"reduces\" a classical theory C to an intuitionistic one I. That is, one provides a constructive mapping that translates the theorems of C to the theorems of I. Second, one reduces the intuitionistic theory I to a quantifier free theory of functionals F. These interpretations contribute to a form of Hilbert's program, since they prove the consistency of classical theories relative to constructive ones. Successful functional interpretations have yielded reductions of infinitary theories to finitary theories and impredicative theories to predicative ones.\n\nFunctional interpretations also provide a way to extract constructive information from proofs in the reduced theory. As a direct consequence of the interpretation one usually obtains the result that any recursive function whose totality can be proven either in I or in C is represented by a term of F. If one can provide an additional interpretation of F in I, which is sometimes possible, this characterization is in fact usually shown to be exact. It often turns out that the terms of F coincide with a natural class of functions, such as the primitive recursive or polynomial-time computable functions. Functional interpretations have also been used to provide ordinal analyses of theories and classify their provably recursive functions.\n\nThe study of functional interpretations began with Kurt Gödel's interpretation of intuitionistic arithmetic in a quantifier-free theory of functionals finite type. This interpretation is commonly known as the Dialectica interpretation. Together with the double-negation interpretation of classical logic in intuitionistic logic, it provides a reduction of classical arithmetic to intuitionistic arithmetic.\n\nThe \"informal\" proofs of everyday mathematical practice are unlike the \"formal\" proofs of proof theory. They are rather like high-level sketches that would allow an expert to reconstruct a formal proof at least in principle, given enough time and patience. For most mathematicians, writing a fully formal proof is too pedantic and long-winded to be in common use.\n\nFormal proofs are constructed with the help of computers in interactive theorem proving. \nSignificantly, these proofs can be checked automatically, also by computer. Checking formal proofs is usually simple, whereas \"finding\" proofs (automated theorem proving) is generally hard. An informal proof in the mathematics literature, by contrast, requires weeks of peer review to be checked, and may still contain errors.\n\nIn linguistics, type-logical grammar, categorial grammar and Montague grammar apply formalisms based on structural proof theory to give a formal natural language semantics.\n\n\n\n"}
{"id": "30408239", "url": "https://en.wikipedia.org/wiki?curid=30408239", "title": "Quantum energy teleportation", "text": "Quantum energy teleportation\n\nQuantum energy teleportation, a hypothesis first put forward by Japanese physicist Masahiro Hotta of Tohoku University, proposes that it may be possible to teleport energy by exploiting quantum energy fluctuations of an entangled vacuum state of a quantum field. The hypothesis proposes that energy may be injected into a zero-point fluctuation of the field at one place, and extracted from a fluctuation at another place. Even for interstellar distance energy transfer, the amount of teleported energy is nonzero, but negligibly small. In contrast, the teleportation protocol will be effective in small quantum worlds of nanoscale devices like quantum computers.\n\nThe idea is a continuation of work by computer scientist Charles H. Bennett on quantum teleportation C.H. Bennett, \"et al.\" in 1993 and experimentally confirmed by various experiments in the following years. Protocols of the quantum teleportation transfer quantum information, but cannot teleport energy itself.\n\nQuantum energy teleportation is a quantum protocol which transfers locally available energy, in an operational sense, from one subsystem of a many-body system to another in an entangled ground state by using local operations and classical communication (LOCC). The locally available energy indicates the energy which can be extracted from a subsystem by local operations and harnessed for any purpose. The transfer speed can be much faster than the velocity of energy diffusion of the system. It does not allow energy transportation at superluminal (faster than light) speed, nor does it increase total energy itself contained in a distant place. Though zero-point energy of the ground state exists everywhere in the system and contributes to the amount of the total energy, it is not available by use of ordinary local operations. However, if information about a local zero-point fluctuation, which carries a portion of the zero-point energy, is obtained by a measurement of a distant subsystem via ground-state entanglement, the energy becomes available, and can be extracted by a local operation dependent on the information. The extraction of the energy is accompanied by generation of negative energy density, which is allowed in quantum physics of many-body systems including quantum fields, and retains the local energy conservation law. The remote measurement, which provides the information for energy extraction, injects energy into the measured subsystem. A portion of the injected energy, which amount is larger than that of the energy extracted from the zero-point fluctuation, becomes unavailable because of entanglement breaking by the measurement, and cannot be retrieved by local operations in the measurement region. Thus, during the protocol, the amount of locally available energy decreases in the measurement region, and it increases in the energy extraction region. The injected energy is the input of this teleportation protocol, and the extracted energy is the output.\nThe details can be found in a review article written by Hotta.\n\nExperimental verification of the teleportation has not been achieved yet. A realistic experimental proposal is provided using a semiconductor exhibiting the quantum Hall effect.\n"}
{"id": "2604269", "url": "https://en.wikipedia.org/wiki?curid=2604269", "title": "Reeb foliation", "text": "Reeb foliation\n\nIn mathematics, the Reeb foliation is a particular foliation of the 3-sphere, introduced by the French mathematician Georges Reeb (1920–1993).\n\nIt is based on dividing the sphere into two solid tori, along a 2-torus: see Clifford torus. Each of the solid tori is then foliated internally, in codimension 1, and the dividing torus surface forms one more leaf.\n\nBy Novikov's compact leaf theorem, every smooth foliation of the 3-sphere includes a compact torus leaf, bounding a solid torus foliated in the same way.\n\n"}
{"id": "37409519", "url": "https://en.wikipedia.org/wiki?curid=37409519", "title": "Rosalind Tanner", "text": "Rosalind Tanner\n\nRosalind Cecilia Hildegard Tanner (née Young) (5 February 1900 – 24 November 1992) was a mathematician and historian of mathematics. She was the eldest daughter of the mathematicians Grace and William Young. She was born and lived in Göttingen in Germany (where her parents worked at the university) until 1908. During her life she used the name Cecily.\n\nRosalind joined the University of Lausanne in 1917. She also helped her father's research between 1919 and 1921 at the University College Wales in Aberystwyth, and worked with Edward Collingwood, also of Aberystwyth, on a translation of Georges Valiron's course on Integral Functions. She received a L-És-sc (a bachelor's degree) from Lausanne in 1925.\n\nShe then studied at Girton College, Cambridge, gaining a PhD in 1929 under the supervision of Professor E. W. Hobson for research on Stieltjes integration. She accepted a teaching post at Imperial College, London where she worked until 1967.\n\nAfter 1936, most of her research was in the history of mathematics, and she had a particular interest in Thomas Harriot, an Elizabethan mathematician. She set up the Harriot Seminars in Oxford and Durham. Rosalind married William Tanner in 1953; however, he died a few months after their marriage. Her interest in sixteenth and seventeenth century mathematics continued after her retirement, and she died on 24 November 1992.\n"}
{"id": "617831", "url": "https://en.wikipedia.org/wiki?curid=617831", "title": "Semicircle", "text": "Semicircle\n\nIn mathematics (and more specifically geometry), a semicircle is a one-dimensional locus of points that forms half of a circle. The full arc of a semicircle always measures 180° (equivalently, radians, or a half-turn). It has only one line of symmetry (reflection symmetry). In non-technical usage, the term \"semicircle\" is sometimes used to refer to a half-disk, which is a two-dimensional geometric shape that also includes the diameter segment from one end of the arc to the other as well as all the interior points.\n\nBy Thales' theorem, any triangle inscribed in a semicircle with a vertex at each of the endpoints of the semicircle and the third vertex elsewhere on the semicircle is a right triangle, with right angle at the third vertex. \n\nAll lines intersecting the semicircle perpendicularly are concurrent at the center of the circle containing the given semicircle.\n\nA semicircle can be used to construct the arithmetic and geometric means of two lengths using straight-edge and compass. If we make a semicircle with a diameter of \"a\"+\"b\", then the length of its radius is the arithmetic mean of \"a\" and \"b\" (since the radius is half of the diameter). The geometric mean can be found by dividing the diameter into two segments of lengths \"a\" and \"b\", and then connecting their common endpoint to the semicircle with a segment perpendicular to the diameter. The length of the resulting segment is the geometric mean, which can be proved using the Pythagorean theorem. This can\nbe used to accomplish quadrature of a rectangle (since a square whose sides are equal to the geometric mean of the sides of a rectangle has the same area as the rectangle), and thus of any figure for which we can construct a rectangle of equal area, such as any polygon (but not a circle).\n\nThe equation of a semicircle with midpoint formula_1 on the diameter between its endpoints and which is entirely concave from below is\n\nIf it is entirely concave from above, the equation is\n\nAn arbelos is a region in the plane bounded by three semicircles connected at the corners, all on the same side of a straight line (the \"baseline\") that contains their diameters.\n\n\n"}
{"id": "44812552", "url": "https://en.wikipedia.org/wiki?curid=44812552", "title": "Stanko Bilinski", "text": "Stanko Bilinski\n\nStanko Bilinski (22 April 1909 in Našice – 6 April 1998 in Zagreb) was a Croatian mathematician and academician. He was a professor at the University of Zagreb and a fellow of the Croatian Academy of Sciences and Arts\n\nIn 1960 he discovered a rhombic dodecahedron of the second kind, the Bilinski dodecahedron. Like the standard rhombic dodecahedron, this convex polyhedron has 12 congruent rhombus sides, but they are differently shaped and arranged. Bilinski's discovery corrected a 75-year-old omission in Evgraf Fedorov's classification of convex polyhedra with congruent rhombic faces.\n\n"}
{"id": "17689596", "url": "https://en.wikipedia.org/wiki?curid=17689596", "title": "Stephen Yablo", "text": "Stephen Yablo\n\nStephen Yablo is an American philosopher. He is David W. Skinner Professor of Philosophy at the Massachusetts Institute of Technology (MIT), and taught previously at the University of Michigan, Ann Arbor. He specializes in the philosophy of logic, philosophy of mind, metaphysics, and philosophy of language.\n\nHe is married to fellow MIT philosopher Sally Haslanger.\n\nHis Ph.D. is from University of California, Berkeley, where he worked with Donald Davidson and George Myro. In 2012, he was elected a Fellow of the American Academy of Arts and Sciences.\n\nHe was born in Toronto, ON to a Polish father and Romanian-Canadian mother.\n\nIn 1993, he published a short paper showing that a liar-like paradox can be generated without self-reference. He has published a number of influential papers in philosophy of mind, philosophy of language, and metaphysics, and gave the John Locke Lectures at Oxford in 2012, which formed the basis for his book \"Aboutness\", which one reviewer described as \"an important and far-reaching book that philosophers will be discussing for a long time.\"\n\n\n"}
{"id": "30876757", "url": "https://en.wikipedia.org/wiki?curid=30876757", "title": "Survival function", "text": "Survival function\n\nThe survival function is a function that gives the probability that a patient, device, or other object of interest will survive beyond any given specified time.\n\nThe survival function is also known as the survivor function or reliability function.\n\nThe term \"reliability function\" is common in engineering while the term \"survival function\" is used in a broader range of applications, including human mortality. Another name for the survival function is the complementary cumulative distribution function.\n\nLet \"T\" be a continuous random variable with cumulative distribution function \"F\"(\"t\") on the interval <nowiki>[0,∞)</nowiki>. Its \"survival function\" or \"reliability function\" is:\n\nThe graphs below show examples of hypothetical survival functions. The x-axis is time. The y-axis is the proportion of subjects surviving. The graphs show the probability that a subject will survive beyond time t.\n\nFor example, for survival function 1, the probability of surviving longer than t = 2 months is 0.37. That is, 37% of subjects survive more than 2 months.\n\nFor survival function 2, the probability of surviving longer than t = 2 months is 0.97. That is, 97% of subjects survive more than 2 months.\n\nMedian survival may be determined from the survival function. For example, for survival function 2, 50% of the subjects survive 3.72 months. Median survival is thus 3.72 months.\n\nIn some cases, median survival cannot be determined from the graph. For example, for survival function 4, more than 50% of the subjects survive longer than the observation period of 10 months.\n\nThe survival function is one of several ways to describe and display survival data. Another useful way to display data is a graph showing the distribution of survival times of subjects. Olkin, page 426, gives the following example of survival data. The number of hours between successive failures of an air-conditioning system were recorded. The time between successive failures are 1, 3, 5, 7, 11, 11, 11, 12, 14, 14, 14, 16, 16, 20, 21, 23, 42, 47, 52, 62, 71, 71, 87, 90, 95, 120, 120, 225, 246, and 261 hours. The mean time between failures is 59.6. This mean value will be used shortly to fit a theoretical curve to the data. The figure below shows the distribution of the time between failures. The blue tick marks beneath the graph are the actual hours between successive failures.\n\nThe distribution of failure times is over-laid with a curve representing an exponential distribution. For this example, the exponential distribution approximates the distribution of failure times. The exponential curve is a theoretical distribution fitted to the actual failure times. This particular exponential curve is specified by the parameter lambda, λ= 1/(mean time between failures) = 1/59.6 = 0.0168. The distribution of failure times is called the probability density function (pdf), if time can take any positive value. In equations, the pdf is specified as f(t). If time can only take discrete values (such as 1 day, 2 days, and so on), the distribution of failure times is called the probability mass function (pmf). Most survival analysis methods assume that time can take any positive value, and f(t) is the pdf. If the time between observed air conditioner failures is approximated using the exponential function, then the exponential curve gives the probability density function, f(t), for air conditioner failure times.\n\nAnother useful way to display the survival data is a graph showing the cumulative failures up to each time point. These data may be displayed as either the cumulative number or the cumulative proportion of failures up to each time. The graph below shows the cumulative probability (or proportion) of failures at each time for the air conditioning system. The stairstep line in black shows the cumulative proportion of failures. For each step there is a blue tick at the bottom of the graph indicating an observed failure time. The smooth red line represents the exponential curve fitted to the observed data.\n\nA graph of the cumulative probability of failures up to each time point is called the cumulative distribution function, or CDF. In survival analysis, the cumulative distribution function gives the probability that the survival time is less than or equal to a specific time, t.\n\nLet T be survival time, which is any positive number. A particular time is designated by the lower case letter t. The cumulative distribution function of \"T\" is the function\n\nwhere the right-hand side represents the probability that the random variable \"T\" is less than or equal to \"t\". If time can take on any positive value, then the cumulative distribution function F(t) is the integral of the probability density function f(t).\n\nFor the air conditioning example, the graph of the CDF below illustrates that the probability that the time to failure is less than or equal to 100 hours is 0.81, as estimated using the exponential curve fit to the data.\n\nAn alternative to graphing the probability that the failure time is \"less\" than or equal to 100 hours is to graph the probability that the failure time is \"greater\" than 100 hours. The probability that the failure time is greater than 100 hours must be 1 minus the probability that the failure time is less than or equal to 100 hours, because total probability must sum to 1.\n\nThis gives\n\nP(failure time > 100 hours) = 1 - P(failure time < 100 hours) = 1 – 0.81 = 0.19.\n\nThis relationship generalizes to all failure times:\n\nP(T > t) = 1 - P(T < t) = 1 – cumulative distribution function.\n\nThis relationship is shown on the graphs below. The graph on the left is the cumulative distribution function, which is P(T < t). The graph on the right is P(T > t) = 1 - P(T < t). The graph on the right is the survival function, S(t). The fact that the S(t) = 1 – CDF is the reason that another name for the survival function is the complementary cumulative distribution function.\n\nIn some cases, such as the air conditioner example, the distribution of survival times may be approximated well by a function such as the exponential distribution. Several distributions are commonly used in survival analysis, including the exponential, Weibull, gamma, normal, log-normal, and log-logistic. These distributions are defined by parameters. The normal (Gaussian) distribution, for example, is defined by the two parameters mean and standard deviation. Survival functions that are defined by parameters are said to be parametric.\n\nIn the four survival function graphs shown above, the shape of the survival function is defined by a particular probability distribution: survival function 1 is defined by an exponential distribution, 2 is defined by a Weibull distribution, 3 is defined by a log-logistic distribution, and 4 is defined by another Weibull distribution.\n\nFor an exponential survival distribution, the probability of failure is the same in every time interval, no matter the age of the individual or device. This fact leads to the \"memoryless\" property of the exponential survival distribution: the age of a subject has no effect on the probability of failure in the next time interval. The exponential may be a good model for the lifetime of a system where parts are replaced as they fail. It may also be useful for modeling survival of living organisms over short intervals. It is not likely to be a good model of the complete lifespan of a living organism. As Efron and Hastie \n(p. 134) note, \"If human lifetimes were exponential there wouldn't be old or young people, just lucky or unlucky ones\".\n\nA key assumption of the exponential survival function is that the hazard rate is constant. In an example given above, the proportion of men dying each year was constant at 10%, meaning that the hazard rate was constant. The assumption of constant hazard may not be appropriate. For example, among most living organisms, the risk of death is greater in old age than in middle age – that is, the hazard rate increases with time. For some diseases, such as breast cancer, the risk of recurrence is lower after 5 years – that is, the hazard rate decreases with time. The Weibull distribution extends the exponential distribution to allow constant, increasing, or decreasing hazard rates.\n\nThere are several other parametric survival functions that may provide a better fit to a particular data set, including normal, lognormal, log-logistic, and gamma. The choice of parametric distribution for a particular application can be made using graphical methods or using formal tests of fit.\nThese distributions and tests are described in textbooks on survival analysis. Lawless \nhas extensive coverage of parametric models.\n\nParametric survival functions are commonly used in manufacturing applications, in part because they enable estimation of the survival function beyond the observation period. However, appropriate use of parametric functions requires that data are well modeled by the chosen distribution. If an appropriate distribution is not available, or cannot be specified before a clinical trial or experiment, then non-parametric survival functions offer a useful alternative.\n\nA parametric model of survival may not be possible or desirable. In these situations, the most common method to model the survival function is the non-parametric Kaplan–Meier estimator.\n\nEvery survival function \"S\"(\"t\") is monotonically decreasing, i.e. formula_3 for all formula_4.\n\nIt is a property of a random variable that maps a set of events, usually associated with mortality or failure of some system, onto time.\n\nThe time, \"t\" = 0, represents some origin, typically the beginning of a study or the start of operation of some system. \"S\"(0) is commonly unity but can be less to represent the probability that the system fails immediately upon operation.\n\nSince the CDF is a right-continuous function, the survival function formula_5 is also right-continuous.\n\n"}
{"id": "4535485", "url": "https://en.wikipedia.org/wiki?curid=4535485", "title": "Sylvester's sequence", "text": "Sylvester's sequence\n\nIn number theory, Sylvester's sequence is an integer sequence in which each member of the sequence is the product of the previous members, plus one. The first few terms of the sequence are:\n\nSylvester's sequence is named after James Joseph Sylvester, who first investigated it in 1880. Its values grow doubly exponentially, and the sum of its reciprocals forms a series of unit fractions that converges to 1 more rapidly than any other series of unit fractions with the same number of terms. The recurrence by which it is defined allows the numbers in the sequence to be factored more easily than other numbers of the same magnitude, but, due to the rapid growth of the sequence, complete prime factorizations are known only for a few of its members. Values derived from this sequence have also been used to construct finite Egyptian fraction representations of 1, Sasakian Einstein manifolds, and hard instances for online algorithms.\n\nFormally, Sylvester's sequence can be defined by the formula\nThe product of an empty set is 1, so \"s\" = 2.\n\nAlternatively, one may define the sequence by the recurrence\nIt is straightforward to show by induction that this is equivalent to the other definition.\n\nThe Sylvester numbers grow doubly exponentially as a function of \"n\". Specifically, it can be shown that\n\nfor a number \"E\" that is approximately 1.26408473530530... This formula has the effect of the following algorithm:\nThis would only be a practical algorithm if we had a better way of calculating E to the requisite number of places than calculating s and taking its repeated square root.\n\nThe double-exponential growth of the Sylvester sequence is unsurprising if one compares it to the sequence of Fermat numbers \"F\"; the Fermat numbers are usually defined by a doubly exponential formula, formula_4, but they can also be defined by a product formula very similar to that defining Sylvester's sequence:\n\nThe unit fractions formed by the reciprocals of the values in Sylvester's sequence generate an infinite series:\nThe partial sums of this series have a simple form,\nThis may be proved by induction, or more directly by noting that the recursion implies that\nso the sum telescopes\nSince this sequence of partial sums (\"s\"-2)/(\"s\"-1) converges to one, the overall series forms an infinite Egyptian fraction representation of the number one:\nOne can find finite Egyptian fraction representations of one, of any length, by truncating this series and subtracting one from the last denominator:\nThe sum of the first \"k\" terms of the infinite series provides the closest possible underestimate of 1 by any \"k\"-term Egyptian fraction. For example, the first four terms add to 1805/1806, and therefore any Egyptian fraction for a number in the open interval (1805/1806,1) requires at least five terms.\n\nIt is possible to interpret the Sylvester sequence as the result of a greedy algorithm for Egyptian fractions, that at each step chooses the smallest possible denominator that makes the partial sum of the series be less than one. Alternatively, the terms of the sequence after the first can be viewed as the denominators of the odd greedy expansion of 1/2.\n\nAs Sylvester himself observed, Sylvester's sequence seems to be unique in having such quickly growing values, while simultaneously having a series of reciprocals that converges to a rational number. This sequence provides an example showing that double-exponential growth is not enough to cause an integer sequence to be an irrationality sequence.\n\nTo make this more precise, it follows from results of that, if a sequence of integers formula_12 grows quickly enough that\nand if the series\nconverges to a rational number \"A\", then, for all \"n\" after some point, this sequence must be defined by the same recurrence\nthat can be used to define Sylvester's sequence.\n\nIf \"i\" < \"j\", it follows from the definition that \"s\" ≡ 1 (mod \"s\"). Therefore, every two numbers in Sylvester's sequence are relatively prime. The sequence can be used to prove that there are infinitely many prime numbers, as any prime can divide at most one number in the sequence. More strongly, no prime factor of a number in the sequence can be congruent to 5 (mod 6), and the sequence can be used to prove that there are infinitely many primes congruent to 7 (mod 12).\nMuch remains unknown about the factorization of the numbers in the Sylvester's sequence. For instance, it is not known if all numbers in the sequence are squarefree, although all the known terms are.\n\nAs describes, it is easy to determine which Sylvester number (if any) a given prime \"p\" divides: simply compute the recurrence defining the numbers modulo \"p\" until finding either a number that is congruent to zero (mod \"p\") or finding a repeated modulus. Via this technique he found that 1166 out of the first three million primes are divisors of Sylvester numbers, and that none of these primes has a square that divides a Sylvester number. \nThe set of primes which can occur as factors of Sylvester numbers is of density zero in the set of all primes: indeed, the number of such primes less than \"x\" is formula_17.\n\nThe following table shows known factorizations of these numbers, (except the first four, which are all prime):\n\nAs is customary, P\"n\" and C\"n\" denote prime and composite numbers \"n\" digits long.\n\n use the properties of Sylvester's sequence to define large numbers of Sasakian Einstein manifolds having the differential topology of odd-dimensional spheres or exotic spheres. They show that the number of distinct Sasakian Einstein metrics on a topological sphere of dimension 2\"n\" − 1 is at least proportional to \"s\" and hence has double exponential growth with \"n\".\n\nAs describe, and used values derived from Sylvester's sequence to construct lower bound examples for online bin packing algorithms. similarly use the sequence to lower bound the performance of a two-dimensional cutting stock algorithm.\n\nZnám's problem concerns sets of numbers such that each number in the set divides but is not equal to the product of all the other numbers, plus one. Without the inequality requirement, the values in Sylvester's sequence would solve the problem; with that requirement, it has other solutions derived from recurrences similar to the one defining Sylvester's sequence. Solutions to Znám's problem have applications to the classification of surface singularities (Brenton and Hill 1988) and to the theory of nondeterministic finite automata.\n\n\n\n"}
{"id": "8722775", "url": "https://en.wikipedia.org/wiki?curid=8722775", "title": "Systematic code", "text": "Systematic code\n\nIn coding theory, a systematic code is any error-correcting code in which the input data is embedded in the encoded output. Conversely, in a non-systematic code the output does not contain the input symbols.\n\nSystematic codes have the advantage that the parity data can simply be appended to the source block, and receivers do not need to recover the original source symbols if received correctly – this is useful for example if error-correction coding is combined with a hash function for quickly determining the correctness of the received source symbols, or in cases where errors occur in erasures and a received symbol is thus always correct. Furthermore, for engineering purposes such as synchronization and monitoring, it is desirable to get reasonable good estimates of the received source symbols without going through the lengthy decoding process which may be carried out at a remote site at a later time.\n\nEvery non-systematic linear code can be transformed into a systematic code with essentially the same properties (i.e., minimum distance).\nBecause of the advantages cited above, linear error-correcting codes are therefore generally implemented as systematic codes. However, for certain decoding algorithms such as sequential decoding or maximum-likelihood decoding, a non-systematic structure can increase performance in terms of undetected decoding error probability when the minimum \"free\" distance of the code is larger.\n\nFor a systematic linear code, the generator matrix, formula_1, can always be written as formula_2, where formula_3 is the identity matrix of size formula_4.\n\n"}
{"id": "3739933", "url": "https://en.wikipedia.org/wiki?curid=3739933", "title": "Turing's proof", "text": "Turing's proof\n\nTuring's proof is a proof by Alan Turing, first published in January 1937 with the title On Computable Numbers, with an Application to the Entscheidungsproblem. It was the second proof of the assertion (Alonzo Church's proof was first) that some decision problems are \"undecidable\": there is no single algorithm that infallibly gives a correct \"yes\" or \"no\" answer to each instance of the problem. In his own words:\n\"...what I shall prove is quite different from the well-known results of Gödel ... I shall now show that there is no general method which tells whether a given formula U is provable in K [\"Principia Mathematica\"]...\" (\"Undecidable\", p. 145).\n\nTuring preceded this proof with two others. The second and third both rely on the first. All rely on his development of type-writer-like \"computing machines\" that obey a simple set of rules and his subsequent development of a \"universal computing machine\".\n\nIn 1905, Jules Richard presented this profound paradox. Alan Turing's first proof constructs this paradox with his so-called computing machine and proves that this machine cannot answer a simple question: will this machine be able to determine if \"any\" computing machine (including itself) will become trapped in an unproductive \"infinite loop\" (i.e. it fails to continue its computation of the diagonal number).\n\nA succinct definition of Richard's paradox is found in Whitehead and Russell's \"Principia Mathematica\":\n\nTuring's proof is complicated by a large number of definitions, and confounded with what Martin Davis called \"petty technical details\" and \"...technical details [that] are incorrect as given\" (Davis's commentary in \"Undecidable\", p. 115). Turing himself published \"A correction\" in 1937: \"The author is indebted to P. Bernays for pointing out these errors\" (\"Undecidable\", p. 152).\n\nSpecifically, in its original form the third proof is badly marred by technical errors. And even after Bernays' suggestions and Turing's corrections, errors remained in the description of the universal machine. And confusingly, since Turing was unable to correct his original paper, some text within the body harks to Turing's flawed first effort.\n\nBernays' corrections may be found in \"Undecidable\", pp. 152–154; the original is to be found as:\n\nThe on-line version of Turing's paper has these corrections in an addendum; however, corrections to the Universal Machine must be found in an analysis provided by Emil Post.\n\nAt first, the only mathematician to pay close attention to the details of the proof was Post (cf. Hodges p. 125) — mainly because he had arrived simultaneously at a similar reduction of \"algorithm\" to primitive machine-like actions, so he took a personal interest in the proof. Strangely (perhaps World War II intervened) it took Post some ten years to dissect it in the \"Appendix\" to his paper \"Recursive Unsolvability of a Problem of Thue\", 1947 (reprinted in \"Undecidable\", p. 293).\n\n\"Before readers tackle \"Proof #3\" they are advised to place those corrections on their copy of the proof.\"\n\nOther problems present themselves: In his \"Appendix\" Post commented indirectly on the paper's difficulty and directly on its \"outline nature\" (Post in \"Undecidable\", p. 299) and \"intuitive form\" of the proofs (\"ibid\".). Post had to infer various points:\n\nAnyone who has ever tried to read the paper will understand Hodges' complaint:\n\nIn his proof that the Entscheidungsproblem can have no solution, Turing proceeded from two proofs that were to lead to his final proof. His first theorem is most relevant to the halting problem, the second is more relevant to Rice's theorem.\n\nFirst proof: that no \"computing machine\" exists that can decide whether or not an arbitrary \"computing machine\" (as represented by an integer 1, 2, 3, . . .) is \"circle-free\" (i.e. goes on printing its number in binary ad infinitum): \"...we have no general process for doing this in a finite number of steps\" (p. 132, \"ibid\".). Turing's proof, although it seems to use the \"diagonal process\", in fact shows that his machine (called H) cannot calculate its own number, let alone the entire diagonal number (Cantor's diagonal argument): \"The fallacy in the argument lies in the assumption that B [the diagonal number] is computable\" (\"Undecidable\", p. 132). The proof does not require much mathematics.\n\nSecond proof: This one is perhaps more familiar to readers as Rice's theorem: \"We can show further that \"there can be no machine E which, when supplied with the S.D [\"program\"] of an arbitrary machine M, will determine whether M ever prints a given symbol (0 say)\"\" (his italics, \"Undecidable\", p. 134).\n\nThird proof: \"Corresponding to each computing machine M we construct a formula Un(M) and we show that, if there is a general method for determining whether Un(M) is provable, then there is a general method for determining whether M ever prints 0\" (\"Undecidable\", p. 145)\n\n[Readers who brave Proof #3 should come equipped with a solid background in (i) logic (ii) the paper of Kurt Gödel \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems\" (reprinted in \"Undecidable\", p. 5). For assistance with Gödel's paper they should consult e.g. Ernest Nagel and James R. Newman, \"Godel's Proof\", New York University Press, 1958.]\n\nThis proof requires the use of formal logic to prove a first lemma, followed by a brief word-proof of the second:\n\nFinally, in only 64 words and symbols Turing proves by \"reductio ad absurdum\" that \"the Hilbert Entscheidungsproblem can have no solution\" (\"Undecidable\", p. 145).\n\nTuring created a thicket of abbreviations; see the glossary at the end of this for help.\n\nSome key clarifications:\n\nTuring begins the proof with the assertion of the existence of a “decision/determination” machine D. When fed any S.D (string of symbols A, C, D, L, R, N, semicolon “;”) it will determine if this S.D (symbol string) represents a \"computing machine\" that is either \"circular\" — and therefore \"un-satisfactory u\" — or \"circle-free\" — and therefore \"satisfactory s\".\n\nTuring makes no comment about how machine D goes about its work. For sake of argument, we suppose that D would first look to see if the string of symbols is \"well-formed\" (i.e. in the form of an algorithm and not just a scramble of symbols), and if not then discard it. Then it would go “circle-hunting”. To do this perhaps it would use “heuristics” (tricks: taught or learned). For purposes of the proof, these details are not important.\n\nTuring then describes (rather loosely) the algorithm (method) to be followed by a machine he calls H. Machine H contains within it the decision-machine D (thus D is a “subroutine” of H). Machine H’s algorithm is expressed in H’s table of instructions, or perhaps in H’s Standard Description on tape and united with the universal machine U; Turing does not specify this.\n\nMachine H is responsible for converting \"any\" number N into an equivalent S.D symbol string for sub-machine D to test. (In programming parlance: H passes an arbitrary \"S.D” to D, and D returns “satisfactory” or “unsatisfactory”.) Machine H is also responsible for keeping a tally R (“Record”?) of successful numbers (we suppose that the number of “successful” S.D's, i.e. R, is much less than the number of S.D's tested, i.e. N). Finally, H prints on a section of its tape a diagonal number “beta-primed” B’. H creates this B’ by “simulating” (in the computer-sense) the “motions” of each “satisfactory” machine/number; eventually this machine/number under test will arrive at its Rth “figure” (1 or 0), and H will print it. H then is responsible for “cleaning up the mess” left by the simulation, incrementing N and proceeding onward with its tests, \"ad infinitum\".\n\nAn example: Suppose machine H has tested 13472 numbers and produced 5 satisfactory numbers, i.e. H has converted the numbers 1 through 13472 into S.D’s (symbol strings) and passed them to D for test. As a consequence H has tallied 5 satisfactory numbers and run the first one to its 1st “figure”, the second to its 2nd figure, the third to its 3rd figure, the fourth to its 4th figure, and the fifth to its 5th figure. The count now stands at N = 13472, R = 5, and B’ = “.10011” (for example). H cleans up the mess on its tape, and proceeds:\n\n\"H\" increments \"N\" = 13473 and converts \"13473\" to symbol string ADRLD. If sub-machine D deems ADLRD unsatisfactory, then H leaves the tally-record R at 5. H will increment the number N to 13474 and proceed onward. On the other hand, if D deems ADRLD satisfactory then H will increment R to 6. H will convert N (again) into ADLRD [this is just an example, ADLRD is probably useless] and “run” it using the universal machine U until this machine-under-test (U \"running\" ADRLD) prints its 6th “figure” i.e. 1 or 0. H will print this 6th number (e.g. “0”) in the “output” region of its tape (e.g. B’ = “.100110”).\n\nH cleans up the mess, and then increments the number \"N\" to 13474.\n\nThe whole process unravels when H arrives at its own number K. We will proceed with our example. Suppose the successful-tally/record R stands at 12. H finally arrives at its own number minus 1, i.e. N = K-1 = 4335...3214, and this number is unsuccessful. Then H increments N to produce K = 4355...3215, i.e. its own number. H converts this to “LDDR...DCAR” and passes it to decision-machine D. Decision-machine D must return “satisfactory” (that is: H must \"by definition\" go on and on testing, \"ad infinitum\", because it is \"circle-free\"). So H now increments tally R from 12 to 13 and then re-converts the number-under-test K into its S.D and uses U to simulate it. But this means that H will be simulating its own motions. What is the first thing the simulation will do? This simulation K-aka-H either creates a new N or “resets” the “old” N to 1. This \"K-aka-H\" either creates a new R or “resets” the “old” R to 0. Old-H “runs” new \"K-aka-H\" until it arrives at its 12th figure.\n\nBut it never makes it to the 13th figure; K-aka-H eventually arrives at 4355...3215, again, and \"K-aka-H\" must repeat the test. \"K-aka-H\" will never reach the 13th figure. The H-machine probably just prints copies of itself \"ad infinitum\" across blank tape. But this contradicts the premise that H is a satisfactory, non-circular computing machine that goes on printing the diagonal numbers's 1's and 0's forever. (We will see the same thing if N is reset to 1 and R is reset to 0.)\n\nIf the reader does not believe this, they can write a \"stub\" for decision-machine D (stub \"D\" will return \"satisfactory\") and then see for themselves what happens at the instant machine H encounters its own number.\n\nLess than one page long, the passage from premises to conclusion is obscure.\n\nTuring proceeds by \"reductio ad absurdum\". He asserts the existence of a machine E, which when given the S.D (Standard Description, i.e. \"program\") of an arbitrary machine M, will determine whether M ever prints a given symbol (0 say). He does not assert that this M is a \"computing machine\".\n\nGiven the existence of machine E, Turing proceeds as follows:\n\nThe difficulty in the proof is step 1. The reader will be helped by realizing that Turing is not explaining his subtle handiwork. (In a nutshell: he is using certain equivalencies between the “existential-“ and “universal-operators” together with their equivalent expressions written with logical operators.)\n\nHere's an example: Suppose we see before us a parking lot full of hundreds of cars. We decide to go around the entire lot looking for: “Cars with flat (bad) tires”. After an hour or so we have found two “cars with bad tires.” We can now say with certainty that “Some cars have bad tires”. Or we could say: “It’s not true that ‘All the cars have good tires’”. Or: “It is true that: ‘not all the cars have good tires”. Let us go to another lot. Here we discover that “All the cars have good tires.” We might say, “There’s not a single instance of a car having a bad tire.” Thus we see that, if we can say something about each car separately then we can say something about ALL of them collectively.\n\nThis is what Turing does:\nFrom \"M\" he creates a collection of machines {\"M\"1, \"M\"2, \"M\"3, \"M\"4, ..., \"Mn\"} and about each he writes a sentence: “\"X\" prints at least one 0” and allows only two “truth values”, True = blank or False = :0:. One by one he determines the truth value of the sentence for each machine and makes a string of blanks or :0:, or some combination of these. We might get something like this: “\"M\"1 prints a 0” = True AND “\"M\"2 prints a 0” = True AND “\"M\"3 prints a 0” = True AND “\"M\"4 prints a 0” = False, ... AND “\"Mn\" prints a 0” = False. He gets the string\nif there are an infinite number of machines \"Mn\". If on the other hand if every machine had produced a \"True\" then the expression on the tape would be\n\nThus Turing has converted statements about each machine considered separately into a single \"statement\" (string) about all of them. Given the machine (he calls it G) that created this expression, he can test it with his machine E and determine if it ever produces a 0. In our first example above we see that indeed it does, so we know that not all the M's in our sequence print 0s. But the second example shows that, since the string is blanks then every Mn in our sequence has produced a 0.\n\nAll that remains for Turing to do is create a process to create the sequence of Mn's from a single M.\n\nSuppose \"M\" prints this pattern:\n\nTuring creates another machine F that takes M and crunches out a sequence of Mn's that successively convert the first n 0's to “0-bar” (0):\nHe claims, without showing details, that this machine F is truly build-able. We can see that one of a couple things could happen. F may run out of machines that have 0's, or it may have to go on \"ad infinitum\" creating machines to “cancel the zeros”.\n\nTuring now combines machines E and F into a composite machine G. G starts with the original M, then uses F to create all the successor-machines M1, M2. . ., Mn. Then G uses E to test each machine starting with M. If E detects that a machine never prints a zero, G prints :0: for that machine. If E detects that a machine does print a 0 (we assume, Turing doesn’t say) then G prints :: or just skips this entry, leaving the squares blank. We can see that a couple things can happen.\n\nNow, what happens when we apply E to G itself?\n\nAs we can apply the same process for determining if M prints 1 infinitely often. When we combine these processes, we can determine that M does, or does not, go on printing 1's and 0's \"ad infinitum\". Thus we have a method for determining if M is circle-free. By Proof 1 this is impossible. So the first assertion that E exists, is wrong: E does not exist.\n\nHere Turing proves \"that the Hilbert Entscheidungsproblem can have no solution\" (\"Undecidable\", p. 145). Here he\n\n\n\nTuring demonstrates the existence of a formula Un(M) which says, in effect, that \"in some complete configuration of M, 0 appears on the tape\" (p. 146). This formula is TRUE, that is, it is \"constructible\", and he shows how to go about this.\n\nThen Turing proves two Lemmas, the first requiring all the hard work. (The second is the converse of the first.) Then he uses \"reductio ad absurdum\" to prove his final result:\n\n\n\n\"If readers intend to study the proof in detail they should correct their copies of the pages of the third proof with the corrections that Turing supplied\".\n\nTo (even attempt to) follow the technical details, the reader will need to understand the definition of \"provable\" and be aware of important \"clues\".\n\n\"Provable\" means, in the sense of Gödel, that (i) the axiom system itself is powerful enough to produce (express) the sentence \"This sentence is provable\", and (ii) that in any arbitrary \"well-formed\" proof the symbols lead by axioms, definitions, and substitution to the symbols of the conclusion.\n\nFirst clue: \"Let us put the description of M into the first standard form of §6\". Section 6 describes the very specific \"encoding\" of machine M on the tape of a \"universal machine\" U. This requires the reader to know some idiosyncrasies of Turing's universal machine U and the encoding scheme.\n\n(i) The universal machine is a set of \"universal\" instructions that reside in an \"instruction table\". Separate from this, on U's tape, a \"computing machine\" M will reside as \"M-code\". The universal table of instructions can print on the tape the symbols A, C, D, 0, 1, u, v, w, x, y, z, : . The various machines M can print these symbols only indirectly by commanding U to print them.\n\n(ii) The \"machine code\" of M consists of only a few letters and the semicolon, i.e. D, C, A, R, L, N, ; . Nowhere within the \"code\" of M will the numerical \"figures\" (symbols) 1 and 0 ever appear. If M wants U to print a symbol from the collection blank, 0, 1 then it uses one of the following codes to tell U to print them. To make things more confusing, Turing calls these symbols S0, S1, and S2, i.e.\n\n(iii) A \"computing machine\", whether it is built directly into a table (as his first examples show), or as machine-code M on universal-machine U's tape, prints its number on blank tape (to the right of M-code, if there is one) as 1s and 0s forever proceeding to the right.\n\n(iv) If a \"computing machine\" is U+\"M-code\", then \"M-code\" appears first on the tape; the tape has a left end and the \"M-code\" starts there and proceeds to the right on alternate squares. When the M-code comes to an end (and it must, because of the assumption that these M-codes are finite algorithms), the \"figures\" will begin as 1s and 0s on alternate squares, proceeding to the right forever. Turing uses the (blank) alternate squares (called \"E\"- \"eraseable\"- squares) to help U+\"M-code\" keep track of where the calculations are, both in the M-code and in the \"figures\" that the machine is printing.\n\n(v) A \"complete configuration\" is a printing of all symbols on the tape, including M-code [?] and \"figures\" up to that point, together with the figure currently being scanned (with a pointer-character printed to the left of the scanned symbol ?). If we have interpreted Turing's meaning correctly, this will be a hugely long set of symbols. But whether the entire M-code must be repeated is unclear; only a printing of the current M-code instruction is necessary plus the printing of all figures with a figure-marker).\n\n(vi) Turing reduced the vast possible number of instructions in \"M-code\" (again: the code of M to appear on the tape) to a small canonical set, one of three similar to this: {qi Sj Sk R ql} e.g. \"If machine is executing instruction #qi and symbol Sj is on the square being scanned, then Print symbol Sk and go Right and then go to instruction ql\": The other instructions are similar, encoding for \"Left\" L and \"No motion\" N. It is this set that is encoded by the string of symbols qi = DA...A, Sj = DC...C, Sk = DC...C, R, ql = DA...A. Each instruction is separated from another one by the semicolon. For example, {q5, S1 S0 L q3} means: Instruction #5: If scanned symbol is 0 then print blank, go Left, then go to instruction #3. It is encoded as follows\n\nSecond clue: Turing is using ideas introduced in Gödel's paper, that is, the \"Gödelization\" of (at least part of) the formula for Un(M). This clue appears only as a footnote on page 138 (\"Undecidable\"): \"A sequence of r primes is denoted by ^(r)\" (\"ibid\".) [Here, r inside parentheses is \"raised\".] This \"sequence of primes\" appears in a formula called F^(n).\n\nThird clue: This reinforces the second clue. Turing's original attempt at the proof uses the expression\nEarlier in the paper Turing had previously used this expression (p. 138) and defined N(u) to mean \"u is a non-negative integer\" (\"ibid\".) (i.e. a Gödel number). But, with the Bernays corrections, Turing abandoned this approach (i.e. the use of N(u)) and the only place where \"the Gödel number\" appears explicitly is where he uses F^(n).\n\nWhat does this mean for the proof? The first clue means that a simple examination of the M-code on the tape will not reveal if a symbol 0 is ever printed by U+\"M-code\". A testing-machine might look for the appearance of DC in one of the strings of symbols that represent an instruction. But will this instruction ever be \"executed?\" Something has to \"run the code\" to find out. This something can be a machine, or it can be lines in a formal proof, i.e. Lemma #1.\n\nThe second and third clues mean that, as its foundation is Gödel's paper, the proof is difficult.\n\n\n\nFrankel has defined \"provable\" earlier in his book:\n\nThus a \"sentence\" is a string of symbols, and a theorem is a string of strings of symbols.\n\nTuring is confronted with the following tasks:\n\nThus the \"string of sentences\" will be strings of strings of symbols. The only allowed individual symbols will come from Godel's symbols defined in his paper.(In the following example we use the \"<\" and \">\" around a \"figure\" to indicate that the \"figure\" is the symbol being scanned by the machine).\n\nIn the following, we have to remind ourselves that every one of Turing's “computing machines” is a binary-number generator/creator that begins work on “blank tape”. Properly constructed, it always cranks away ad infinitum, but its instructions are always finite. In Turing's proofs, Turing's tape had a “left end” but extended right ad infinitum. For sake of example below we will assume that the “machine” is not a Universal machine, but rather the simpler “dedicated machine” with the instructions in the Table.\n\nOur example is based on a \"modified\" Post–Turing machine model of a Turing Machine. This model prints only the symbols 0 and 1. The blank tape is considered to be all b's. Our modified model requires us to add two more instructions to the 7 Post–Turing instructions. The abbreviations that we will use are:\nIn the cases of R, L, E, P0, and P1 after doing its task the machine continues on to the next instruction in numerical sequence; ditto for the jumps if their tests fail.\n\nBut, for brevity, our examples will only use three squares. And these will always start as there blanks with the scanned square on the left: i.e. bbb. With two symbols 1, 0 and blank we can have 27 distinct configurations:\n\nWe must be careful here, because it is quite possible that an algorithm will (temporarily) leave blanks in between figures, then come back and fill something in. More likely, an algorithm may do this intentionally. In fact, Turing's machine does this—it prints on alternate squares, leaving blanks between figures so it can print locator symbols.\n\nTuring always left alternate squares blank so his machine could place a symbol to the left of a figure (or a letter if the machine is the universal machine and the scanned square is actually in the “program”). In our little example we will forego this and just put symbols ( ) around the scanned symbol, as follows:\n\nLet us write a simple program:\n\nRemember that we always start with blank tape. The complete configuration prints the symbols on the tape followed by the next instruction:\n\nLet us add “jump” into the formula. When we do this we discover why the complete configuration must include the tape symbols. (Actually, we see this better, below.) This little program prints three “1”s to the right, reverses direction and moves left printing 0’s until it hits a blank. We will print all the symbols that our machine uses:\nHere at the end we find that a blank on the left has “come into play” so we leave it as part of the total configuration.\n\nGiven that we have done our job correctly, we add the starting conditions and see “where the theorem goes”. The resulting configuration—the number 110—is the PROOF.\n\n\n1 computable number — a number whose decimal is computable by a machine, i.e. by finite means (e.g. an algorithm)\n\n2 M — a machine with a finite instruction table and a scanning/printing head. M moves an infinite tape divided into squares each “capable of bearing a symbol”. The machine-instructions are only the following: move one square left, move one square right, on the scanned square print symbol p, erase the scanned square, if the symbol is p then do instruction aaa, if the scanned symbol is not p then do instruction aaa, if the scanned symbol is none then do instruction aaa, if the scanned symbol is any do instruction aaa [where “aaa” is an instruction-identifier].\n\n3 computing machine — an M that prints two kinds of symbols, symbols of the first type are called “figures” and are only binary symbols 1 and 0; symbols of the second type are any other symbols.\n\n4 figures — symbols 1 and 0, a.k.a. “symbols of the first kind”\n\n5 m-configuration — the instruction-identifier, either a symbol in the instruction table, or a string of symbols representing the instruction- number on the tape of the universal machine (e.g. \"DAAAAA = #5\")\n\n6 symbols of the second kind — any symbols other than 1 and 0\n\n7 circular — an unsuccessful computating machine. It fails to print, ad infinitum, the figures 0 or 1 that represent in binary the number it computes\n\n8 circle-free — a successful computating machine. It prints, ad infinitum, the figures 0 or 1 that represent in binary the number it computes\n\n9 sequence — as in “sequence computed by the machine”: symbols of the first kind a.k.a. figures a.k.a. symbols 0 and 1.\n\n10 computable sequence — can be computed by a circle-free machine\n\n11 S.D – Standard Description: a sequence of symbols A, C, D, L, R, N, “;” on a Turing machine tape\n\n12 D.N — Description Number: an S.D converted to a number: 1=A, 2=C, 3 =D, 4=L, 5=R, 6=N, 7=;\n\n13 M(n) — a machine whose D.N is number “n”\n\n14 satisfactory — a S.D or D.N that represents a circle-free machine\n\n15 U — a machine equipped with a “universal” table of instructions. If U is “supplied with a tape on the beginning of which is written the S.D of some computing machine M, U will compute the same sequence as M.”\n\n16 β’—“beta-primed”: A so-called “diagonal number” made up of the n-th figure (i.e. 0 or 1) of the n-th computable sequence [also: the computable number of H, see below]\n\n17 u — an unsatisfactory, i.e. circular, S.D\n\n18 s — satisfactory, i.e. circle-free S.D\n\n19 D — a machine contained in H (see below). When supplied with the S.D of any computing machine M, D will test M's S.D and if circular mark it with “u” and if circle-free mark it with “s”\n\n20 H — a computing machine. H computes B’, maintains R and N. H contains D and U and an unspecified machine (or process) that maintains N and R and provides D with the equivalent S.D of N. E also computes the figures of B’ and assembles the figures of B’.\n\n21 R — a record, or tally, of the quantity of successful (circle-free) S.D tested by D\n\n22 N — a number, starting with 1, to be converted into an S.D by machine E. E maintains N.\n\n23 K — a number. The D.N of H.\n\n\n5 m-configuration — the instruction-identifier, either a symbol in the instruction table, or a string of symbols representing the instruction's number on the tape of the universal machine (e.g. \"DAAAAA = instruction #5\"). In Turing's S.D the m-configuration appears twice in each instruction, the left-most string is the \"current instruction\"; the right-most string is the next instruction.\n\n24 complete configuration — the number (figure 1 or 0) of the scanned square, the complete sequence of all symbols on the tape, and the m-configuration (the instruction-identifier, either a symbol or a string of symbols representing a number, e.g. \"instruction DAAAA = #5\")\n\n25 RSi(x, y) — \"in the complete configuration x of M the symbol on square y is Si. \"complete configuration\" is definition #5,\n\n26 I(x, y) — \"in the complete configuration x of M the square y is scanned\"\n\n27 Kqm(x) — \"in the complete configuration x of M the machine-configuration (instruction number) is qm\"\n\n28 F(x,y) — \"y is the \"immediate\" successor of x\" (follows Gödel's use of \"f\" as the successor-function).\n\n29 G(x,y) — \"x precedes y\", not necessarily immediately\n\n30 Inst{qi, Sj Sk L ql} is an abbreviation, as are Inst{qi, Sj Sk R ql}, and Inst{qi, Sj Sk N ql}. See below.\n\nTuring reduces his instruction set to three “canonical forms” – one for Left, Right, and No-movement. Si and Sk are symbols on the tape.\nFor example, the operations in the first line are PSk = PRINT symbol Sk from the collection A, C, D, 0, 1, u, v, w, x, y, z, :, then move tape LEFT.\n\nThese he further abbreviated as:\n(N1) qi Sj Sk L qm\n(N2) qi Sj Sk R qm\n(N3) qi Sj Sk N qm\n\nIn Proof #3 he calls the first of these “Inst{qi Sj Sk L ql}”, and he shows how to write the entire machine S.D as the logical conjunction (logical OR): this string is called “Des(M)”, as in “Description-of-M”.\ni.e. if the machine prints 0 then 1's and 0's on alternate squares to the right ad infinitum it might have the table (a similar example appears on page 119):\nIf put them into the “ Inst( ) form” the instructions will be the following (remembering: S0 is blank, S1 = 0, S2 = 1):\nThe reduction to the Standard Description (S.D) will be:\nThis agrees with his example in the book (there will be a blank between each letter and number). Universal machine U uses the alternate blank squares as places to put \"pointers\".\n\n"}
{"id": "48898634", "url": "https://en.wikipedia.org/wiki?curid=48898634", "title": "Two ears theorem", "text": "Two ears theorem\n\nIn geometry, the two ears theorem states that every simple polygon with more than three vertices has at least two ears, vertices that can be removed from the polygon without introducing any crossings. The two ears theorem is equivalent to the existence of polygon triangulations. It is frequently attributed to Gary H. Meisters, but was proved earlier by Max Dehn.\n\nAn ear of a polygon is defined as a vertex such that the line segment between the two neighbors of lies entirely in the interior of the polygon. The two ears theorem states that every simple polygon has at least two ears.\n\nAn ear and its two neighbors form a triangle within the polygon that is not crossed by any other part of the polygon. Removing a triangle of this type produces a polygon with fewer sides, and repeatedly removing ears allows any simple polygon to be triangulated.\n\nConversely, if a polygon is triangulated, the weak dual of the triangulation (a graph with one vertex per triangle and one edge per pair of adjacent triangles) will be a tree and each leaf of the tree will form an ear. Since every tree with more than one vertex has at least two leaves, every triangulated polygon with more than one triangle has at least two ears. Thus, the two ears theorem is equivalent to the fact that every simple polygon has a triangulation.\n\nAn ear is called \"exposed\" when it forms a vertex of the convex hull of the polygon. However, it is possible for a polygon to have no exposed ears.\n\nEars are a special case of a \"principal vertex\", a vertex such that the line segment connecting the vertex's neighbors does not cross the polygon or touch any other vertex of it. A principal vertex for which this line segment lies outside the polygon is called a \"mouth\". Analogously to the two ears theorem, every non-convex simple polygon has at least one mouth. Polygons with the minimum number of principal vertices of both types, two ears and a mouth, are called anthropomorphic polygons.\n\nThe two ears theorem is often attributed to a 1975 paper by Gary H. Meisters, from which the \"ear\" terminology originated. However, the theorem was proved earlier by Max Dehn (circa 1899) as part of a proof of the Jordan curve theorem. To prove the theorem, Dehn observes that every polygon has at least three convex vertices. If one of these vertices, , is not an ear, then it can be connected by a diagonal to another vertex inside the triangle formed by and its two neighbors; can be chosen to be the vertex within this triangle that is farthest from line . This diagonal decomposes the polygon into two smaller polygons, and repeated decomposition by ears and diagonals eventually produces a triangulation of the whole polygon, from which an ear can be found as a leaf of the dual tree.\n\n"}
{"id": "34608521", "url": "https://en.wikipedia.org/wiki?curid=34608521", "title": "Vivid designator", "text": "Vivid designator\n\nIn modal logic and the philosophy of language, a vivid designator is a term which is \"believed\" to designate the same thing in all possible worlds and nothing else where such an object does not exist in a possible world. It is the analogue, in the sense of believing, of a rigid designator, which \"is\" (refers to) the same in all possible worlds, rather than is just \"believed\" to be so.\n\nWillard Van Orman Quine credits David Kaplan (who in turn Montgomery Furth) for the term \"vivid designator\" in his 1953 paper \"Reference and Modality\". He examines the separation between \"de re\" and \"de dicto\" and does away with \"de re\" statements, because \"de re\" statements can only work for names that are used referentially. In fact, both rigid designators and vivid designators are similarly dependent on context and empty otherwise. The same is true of the whole quantified modal logic of necessity because it collapses if essence is withdrawn.\n\n"}
{"id": "308290", "url": "https://en.wikipedia.org/wiki?curid=308290", "title": "W. W. Rouse Ball", "text": "W. W. Rouse Ball\n\nWalter William Rouse Ball, known as W. W. Rouse Ball (; 14 August 1850 – 4 April 1925), was a British mathematician, lawyer, and fellow at Trinity College, Cambridge from 1878 to 1905. He was also a keen amateur magician, and the founding president of the Cambridge Pentacle Club in 1919, one of the world's oldest magic societies.\n\nBall was the son and heir of Walter Frederick Ball, of 3, St John's Park Villas, South Hampstead, London. Educated at University College School, he entered Trinity College, Cambridge in 1870, became a scholar and first Smith's Prizeman, and gained his BA in 1874 as second Wrangler. He became a Fellow of Trinity in 1875, and remained one for the rest of his life.\n\nHe is buried at the Parish of the Ascension Burial Ground in Cambridge.\n\nHe is commemorated in the naming of the small pavilion, now used as changing rooms and toilets, on Jesus Green in Cambridge.\n\n (1st ed. 1888 and later editions). Dover 1960 republication of fourth edition: .\n (1st ed. 1892; later editions with H.S.M. Coxeter)\n (1st ed. 1918). Macmillan and Co., Limited 1918: .\n\n\n"}
{"id": "40882433", "url": "https://en.wikipedia.org/wiki?curid=40882433", "title": "Wilson's model of information behavior", "text": "Wilson's model of information behavior\n\nWilson's model of information seeking behaviour was born out of a need to focus the field of information and library science on human use of information, rather than the use of information systems and sources. Previous studies undertaken in the field were primarily concerned with systems, specifically, how an individual uses a system. Very little had been written that examined an individual's information needs, or how information seeking behaviour related to other task-oriented behaviours. Thomas D. Wilson's first model had its origins in a presentation at the University of Maryland in 1971 when \"an attempt was made to map the processes involved in what was known at the time as \"user need research\".\n\nPublished in 1981, Wilson's first model outlined the factors leading to information seeking, and the barriers inhibiting action. It stated that information-seeking was prompted by an individual's physiological, cognitive, or affective needs, which have their roots in personal factors, role demands, or environmental context. In order to satisfy these needs, an individual makes demands upon a system by acting as an intermediary, or through the use of technology. The information provided by the system is then evaluated to determine if it satisfies the individual's needs. This first model was based on an understanding of human information seeking behaviors that are best understood as three interwoven frameworks: The user, the information system, and the information resource.\n\nWilson later built upon his original model in order to understand the personal circumstance, social role, and environmental context in which an information need is created. This new model, altered in 1994 incorporated Ellis' stages of information-seeking: starting, browsing, differentiating, monitoring, extracting, verifying and ending. It also displayed the physiological, affective, and cognitive needs that give rise to information seeking behaviour. The model recognized that an information need is not a need in and of itself, but rather one that stems from a previous psychological need. These needs are generated by the interplay of personal habits and political, economic, and technological factors in an individual's environmental. The factors that drive needs can also obstruct an individual's search for information.\n\nIn 1996 Wilson proposed a third model that built upon the previous two. This model incorporated several new elements that helped to demonstrate the stages experienced by the 'person in context', or searcher, when looking for information. These included an intermediate stage between the acknowledgement of a need and the initiation of action, a redefining of the barriers he proposed in his second model as \"intervening variables\" to show that factors can be supportive or preventative a feedback loop, and an \"activating mechanism\" stage. 'Activating mechanisms' identify relevant impetus that prompt a decision to seek information, and integrate behavioural theories such as 'stress/coping theory', 'risk/reward theory' and 'social learning theory'.\n\nIn 1999, Wilson developed a general model that brought together different areas of research in the study of information behavior. The model represented research topics as a series of nested fields, with information behavior as the general area of investigation, information-seeking behavior as its sub-set, and information searching behavior as a further sub-set.\n\nWilson's model has changed over time, and will continue to evolve as technology and the nature of information changes. The model has been cited and discussed by leaders in the information science field, and can be integrated with other significant theories of information behaviour. Wilson describes the model diagrams as elaborating on one another, saying \"no one model stands alone and in using the model to guide the development of research ideas, it is necessary to examine and reflect upon all of the diagrams\".\n\nRecently, there has been a shift from theorizing on research already conducted on information behaviour, to pursuing \"research within specific theoretical contexts\". Wilson's Model is \"aimed at linking theories to action\"; however, it is this move from theory to action that is proving slow. Through numerous qualitative studies, \"we now have many in depth investigations into the information seeking behavior of small samples of people\". Despite these studies, there have not been many links made between this research and changes in policy or practice.\n\n"}
