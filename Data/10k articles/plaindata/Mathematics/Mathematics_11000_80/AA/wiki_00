{"id": "40695", "url": "https://en.wikipedia.org/wiki?curid=40695", "title": "Adder–subtractor", "text": "Adder–subtractor\n\nIn digital circuits, an adder–subtractor is a circuit that is capable of adding or subtracting numbers (in particular, binary). Below is a circuit that does adding \"or\" subtracting\" depending on a control signal. It is also possible to construct a circuit that performs both addition and subtraction at the same time.\n\nHaving an \"n\"-bit adder for \"A\" and \"B\", then . Then, assume the numbers are in two's complement. Then to perform , two's complement theory says to invert each bit with a NOT gate then add one. This yields , which is easy to do with a slightly modified adder.\n\nBy preceding each \"A\" input bit on the adder with a 2-to-1 multiplexer where:\nthat has control input \"D\" that is also connected to the initial carry, then the modified adder performs\nThis works because when the \"A\" input to the adder is really and the carry in is 1. Adding \"B\" to and 1 yields the desired subtraction of .\n\nA way you can mark number \"A\" as positive or negative without using a multiplexer on each bit is to use an XOR gate to precede each bit instead. \nThis produces the same truth table for the bit arriving at the adder as the multiplexer solution does since the XOR gate output will be what the input bit is when and the inverted input bit when .\n\nAdders are a part of the core of an arithmetic logic unit (ALU). The control unit decides which operations an ALU should perform (based on the op code being executed) and sets the ALU operation. The \"D\" input to the adder–subtractor above would be one such control line from the control unit.\n\nThe adder–subtractor above could easily be extended to include more functions. For example, a 2-to-1 multiplexer could be introduced on each \"B\" that would switch between zero and \"B\"; this could be used (in conjunction with ) to yield the two's complement of \"A\" since .\n\nA further step would be to change the 2-to-1 multiplex on \"A\" to a 4-to-1 with the third input being zero, then replicating this on \"B\" thus yielding the following output functions:\n\nBy adding more logic in front of the adder, a single adder can be converted into much more than just an adder—an ALU.\n\n"}
{"id": "508012", "url": "https://en.wikipedia.org/wiki?curid=508012", "title": "Berry–Esseen theorem", "text": "Berry–Esseen theorem\n\nIn probability theory, the central limit theorem states that, under certain circumstances, the probability distribution of the scaled mean of a random sample converges to a normal distribution as the sample size increases to infinity. Under stronger assumptions, the Berry–Esseen theorem, or Berry–Esseen inequality, gives a more quantitative result, because it also specifies the rate at which this convergence takes place by giving a bound on the maximal error of approximation between the normal distribution and the true distribution of the scaled sample mean. The approximation is measured by the Kolmogorov–Smirnov distance. In the case of independent samples, the convergence rate is , where is the sample size, and the constant is estimated in terms of the third absolute normalized moments.\n\nStatements of the theorem vary, as it was independently discovered by two mathematicians, Andrew C. Berry (in 1941) and Carl-Gustav Esseen (1942), who then, along with other authors, refined it repeatedly over subsequent decades.\n\nOne version, sacrificing generality somewhat for the sake of clarity, is the following:\n\nThat is: given a sequence of independent and identically distributed random variables, each having mean zero and positive variance, if additionally the third absolute moment is finite, then the cumulative distribution functions of the standardized sample mean and the standard normal distribution differ (vertically, on a graph) by no more than the specified amount. Note that the approximation error for all \"n\" (and hence the limiting rate of convergence for indefinite \"n\" sufficiently large) is bounded by the order of \"n\".\n\nCalculated values of the constant \"C\" have decreased markedly over the years, from the original value of 7.59 by , to 0.7882 by , then 0.7655 by , then 0.7056 by , then 0.7005 by , then 0.5894 by , then 0.5129 by , then 0.4785 by . The detailed review can be found in the papers , . The best estimate , \"C\" < 0.4748, follows from the inequality\ndue to , since σ ≤ ρ and 0.33554 · 1.415 < 0.4748. However, if ρ ≥ 1.286σ, then the estimate \nwhich is also proved in , gives an even tighter upper estimate.\n\nIt is easy to make sure that ψ≤ψ. Due to this circumstance inequality (3) is conventionally called the Berry–Esseen inequality, and the quantity ψ is called the Lyapunov fraction of the third order. Moreover, in the case where the summands \"X\", ..., \"X\" have identical distributions \nand thus the bounds stated by inequalities (1), (2) and (3) coincide apart from the constant.\n\nRegarding \"C\", obviously, the lower bound established by remains valid:\n\nThe upper bounds for \"C\" were subsequently lowered from the original estimate 7.59 due to to (considering recent results only) 0.9051 due to , 0.7975 due to , 0.7915 due to , 0.6379 and 0.5606 due to and . the best estimate is 0.5600 obtained by .\n\n\n\n"}
{"id": "56546646", "url": "https://en.wikipedia.org/wiki?curid=56546646", "title": "Bettye Anne Case", "text": "Bettye Anne Case\n\nBettye Anne Busbee Case is Olga Larson Professor Emerita of Mathematics at Florida State University. Her mathematical research concerns complex variables; she has also published on mathematics education and the history of mathematics.\nShe is the editor of the books \"A Century of Mathematical Meetings\" (American Mathematical Society, 1996) and \"Complexities: Women in Mathematics\" (with Anne M. Leggett, Princeton University Press, 2005).\n\nCase graduated from the University of Alabama in 1962. She earned her Ph.D. in 1970 from the same university; her dissertation, \"On Non-Analytic Functions Related to a System of Partial Differential Equations\", was supervised by Mario O. González. She taught at the Florida Institute of Technology and then at Tallahassee Community College for nine years before joining the Florida State University faculty as an associate professor in 1982.\n\nCase was the founding director of both the undergraduate program in actuarial science and the graduate area financial mathematics at Florida State. She was active member of the Association for Women in Mathematics, and coordinated their meetings at mathematics conferences from 1984 to 2015.\n\nFlorida State named Case the Olga Larson Professor in 2004. In 2016 the Association for Women in Mathematics presented Case a Lifetime Service Award in recognition for her many decades of service to the AWM, particularly as Meetings Coordinator and long time member of the Executive Committee. \nIn 2018 she was honored as one of the inaugural Fellows of the Association for Women in Mathematics.\n"}
{"id": "828320", "url": "https://en.wikipedia.org/wiki?curid=828320", "title": "Birkhoff's axioms", "text": "Birkhoff's axioms\n\nIn 1932, G. D. Birkhoff created a set of four postulates of Euclidean geometry in the plane, sometimes referred to as Birkhoff's axioms. These postulates are all based on basic geometry that can be confirmed experimentally with a scale and protractor. Since the postulates build upon the real numbers, the approach is similar to a model-based introduction to Euclidean geometry.\n\nBirkhoff's axiom system was utilized in the secondary-school textbook by Birkhoff and Beatley.\nThese axioms were also modified by the School Mathematics Study Group to provide a new standard for teaching high school geometry, known as SMSG axioms.\nA few other textbooks in the foundations of geometry use variants of Birkhoff's axioms.\nThe distance between two points and  is denoted by ; the angle formed by three points is denoted by .\n\nPostulate I: Postulate of line measure. \nThe set of points on any line can be put into a 1:1 correspondence with the real numbers so that for all points and .\n\nPostulate II: Point-line postulate. \nThere is one and only one line, , that contains any two given distinct points and .\n\nPostulate III: Postulate of angle measure. \nThe set of rays through any point can be put into 1:1 correspondence with the real numbers so that if and are points (not equal to ) of and , respectively, the difference of the numbers associated with the lines and is . Furthermore, if the point on varies continuously in a line not containing the vertex , the number varies continuously also.\n\nPostulate IV: Postulate of similarity. \nGiven two triangles and and some constant such that and , then , and .\n\n"}
{"id": "28045698", "url": "https://en.wikipedia.org/wiki?curid=28045698", "title": "Cannonball problem", "text": "Cannonball problem\n\nIn the mathematics of figurate numbers, the cannonball problem asks which numbers are both square and square pyramidal. The problem can be stated as: given a square arrangement of cannonballs, for what size squares can these cannonballs also be arranged into a square pyramid.\nEquivalently, which squares can be represented as the sum of consecutive squares, starting from 1?\n\nWhen cannonballs are stacked within a square frame, the number of balls is a square pyramidal number; Thomas Harriot gave a formula for this number around 1587, answering a question posed to him by Sir Walter Raleigh on their expedition to America. \nÉdouard Lucas formulated the cannonball problem as a Diophantine equation\nor\nand conjectured that the only solutions are \"N\" = 1, \"M\" = 1, and \"N\" = 24, \"M\" = 70. It was not until 1918 that G. N. Watson found a proof for this fact, using elliptic functions. More recently, elementary proofs have been published.\nThe solution \"N\" = 24, \"M\" = 70. can be used for constructing the Leech Lattice. The result has relevance to the bosonic string theory in 26 dimensions.\n\nAlthough it is possible to tile a geometric square with unequal squares, it is not possible to do so with a solution to the cannonball problem. The squares with side lengths from 1 to 24 have areas equal to the square with side length 70, but they cannot be arranged to tile it.\n\nThe only numbers that are simultaneously triangular and square pyramidal, are 1, 55, 91, and 208335..\n\nThere are no numbers (other than the trivial solution 1) that are both tetrahedral and square pyramidal.\n\n"}
{"id": "515096", "url": "https://en.wikipedia.org/wiki?curid=515096", "title": "Canonical form", "text": "Canonical form\n\nIn mathematics and computer science, a canonical, normal, or standard form of a mathematical object is a standard way of presenting that object as a mathematical expression. The distinction between \"canonical\" and \"normal\" forms varies by subfield. In most fields, a canonical form specifies a \"unique\" representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.\n\nThe canonical form of a positive integer in decimal representation is a finite sequence of digits that does not begin with zero.\n\nMore generally, for a class of objects on which an equivalence relation is defined, a canonical form consists in the choice of a specific object in each class. For example, Jordan normal form is a canonical form for matrix similarity, and the row echelon form is a canonical form, when one considers as equivalent a matrix and its left product by an invertible matrix.\n\nIn computer science, and more specifically in computer algebra, when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a canonical form is a representation such that every object has a unique representation. Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. However canonical forms frequently depend on arbitrary choices (like ordering the variables), and this introduces difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, \"normal form\" is a weaker notion: A normal form is a representation such that zero is uniquely represented. This allows testing for equality by putting the difference of two objects in normal form.\n\nCanonical form can also mean a differential form that is defined in a natural (canonical) way.\n\nIn computer science, data that has more than one possible representation can often be canonicalized into a completely unique representation called its canonical form. Putting something into canonical form is canonicalization.\n\nSuppose we have some set \"S\" of objects, with an equivalence relation \"R\". A canonical form is given by designating some objects of \"S\" to be \"in canonical form\", such that every object under consideration is equivalent to exactly one object in canonical form. In other words, the canonical forms in \"S\" represent the equivalence classes, once and only once. To test whether two objects are equivalent, it then suffices to test their canonical forms for equality.\nA canonical form thus provides a classification theorem and more, in that it not just classifies every class, but gives a distinguished (canonical) representative.\n\nFormally, a canonicalization with respect to an equivalence relation \"R\" on a set \"S\" is a mapping \"c\":\"S\"→\"S\" such that for all \"s\", \"s\", \"s\" ∈ \"S\":\nProperty 3 is redundant, it follows by applying 2 to 1.\n\nIn practical terms, one wants to be able to recognize the canonical forms. There is also a practical, algorithmic question to consider: how to pass from a given object \"s\" in \"S\" to its canonical form \"s\"*? Canonical forms are generally used to make operating with equivalence classes more effective. For example, in modular arithmetic, the canonical form for a residue class is usually taken as the least non-negative integer in it. Operations on classes are carried out by combining these representatives and then reducing the result to its least non-negative residue.\nThe uniqueness requirement is sometimes relaxed, allowing the forms to be unique up to some finer equivalence relation, like allowing reordering of terms (if there is no natural ordering on terms).\n\nA canonical form may simply be a convention, or a deep theorem.\n\nFor example, polynomials are conventionally written with the terms in descending powers: it is more usual to write \"x\" + \"x\" + 30 than \"x\" + 30 + \"x\", although the two forms define the same polynomial. By contrast, the existence of Jordan canonical form for a matrix is a deep theorem.\n\nNote: in this section, \"up to\" some equivalence relation E means that the canonical form is not unique in general, but that if one object has two different canonical forms, they are E-equivalent.\n\n\n\nIn analytic geometry:\n\nBy contrast, there are alternative forms for writing equations. For example, the equation of a line may be written as a linear equation in point-slope and slope-intercept form.\n\nConvex polyhedra can be put into canonical form such that:\n\nStandard form is used by many mathematicians and scientists to write extremely large numbers in a more concise and understandable way.\n\n\n\n\n\n\n\nIn graph theory, a branch of mathematics, graph canonization is the problem finding a canonical form of a given graph \"G\". A canonical form is a labeled graph Canon(\"G\") that is isomorphic to \"G\", such that every graph that is isomorphic to \"G\" has the same canonical form as \"G\". Thus, from a solution to the graph canonization problem, one could also solve the problem of graph isomorphism: to test whether two graphs \"G\" and \"H\" are isomorphic, compute their canonical forms Canon(\"G\") and Canon(\"H\"), and test whether these two canonical forms are identical.\n\nCanonical differential forms include the canonical one-form and canonical symplectic form, important in the study of Hamiltonian mechanics and symplectic manifolds.\n\nIn computing, the reduction of data to any kind of canonical form is commonly called \"data normalization\".\n\nFor instance, database normalization is the process of organizing the fields and tables of a relational database to minimize redundancy and dependency. \n\nIn the field of software security, a common vulnerability is unchecked malicious input. The mitigation for this problem is proper input validation. Before input validation may be performed, the input must be normalized, i.e., eliminating encoding (for instance HTML encoding) and reducing the input data to a single common character set.\n\nOther forms of data, typically associated with signal processing (including audio and imaging) or machine learning, can be normalized in order to provide a limited range of values.\n\n\n"}
{"id": "49767151", "url": "https://en.wikipedia.org/wiki?curid=49767151", "title": "Chain-ladder method", "text": "Chain-ladder method\n\nThe chain-ladder or development method is a prominent actuarial loss reserving technique.\n\nThe chain-ladder method is used in both the property and casualty and health insurance fields. Its intent is to estimate incurred but not reported claims and project ultimate loss amounts.\n\nThe primary underlying assumption of the chain-ladder method is that historical loss development patterns are indicative of future loss development patterns.\n\nAccording to Jacqueline Friedland's \"Estimating Unpaid Claims Using Basic Techniques,\" there are seven steps to apply the chain-ladder technique:\n\n\nAge-to-age factors, also called loss development factors (LDFs) or link ratios, represent the ratio of loss amounts from one valuation date to another, and they are intended to capture growth patterns of losses over time. These factors are used to project where the ultimate amount losses will settle.\n\nFirst, losses (either reported or paid) are compiled into a triangle, where the rows represent accident years and the columns represent valuation dates. For example, 43,169,009 represents loss amounts related to claims occurring in 1998, valued as of 24 months.\n\nNext, age-to-age factors are determined by calculating the ratio of losses at subsequent valuation dates. From 24 months to 36 months, accident year 1998 losses increased from 43,169,009 to 45,568,919, so the corresponding age-to-age factor is 45,568,919 / 43,169,009 = 1.056. A \"tail factor\" is selected (in this case, 1.000) to project from the latest valuation age to ultimate.\n\nFinally, averages of the age-to-age factors are calculated. Judgmental selections are made after observing several averages. The age-to-age factors are then multiplied together to obtain cumulative development factors.\n\nThe cumulative development factors multiplied by the reported (or paid) losses to project ultimate losses.\n\nIncurred but not reported can be obtained by subtracting reported losses from ultimate losses, in this case, 569,172,456 - 543,481,587 = 25,690,869.\n\nThe chain-ladder technique is only accurate when patterns of loss development in the past can be assumed to continue in the future. In contrast to other loss reserving methods such as the Bornhuetter-Ferguson method, it relies only on past experience to arrive at an incurred but not reported claims estimate.\n\nWhen there are changes to an insurer's operations, such as a change in claims settlement times, changes in claims staffing, or changes to case reserve practices, the chain-ladder method will not produce an accurate estimate without adjustments.\n\nThe chain-ladder method is also very responsive to changes in experience, and as a result, it may be unsuitable for very volatile lines of business.\n\n"}
{"id": "29746775", "url": "https://en.wikipedia.org/wiki?curid=29746775", "title": "Chordal problem", "text": "Chordal problem\n\nIn the book there is a generalization\nof the equichordal point problem attributed to R. Gardner.\n\nwhere formula_8 is a constant not depending on the chord. In this article\nwe will call a point formula_1 satisfying equation\na chordal point, or formula_6-chordal point.\n\nThe template for all chordal problems is this:\n\nThe center of the circle is a solution of the chordal equation\nfor an arbitrary formula_6. One can show a continuum of solutions\nfor many formula_6, for example, formula_13. The method of construction such solutions\nis by writing the equation of the curve in the form formula_14 in polar coordinates.\nFor formula_13, the solution may be found in this article.\n\nThis is an example of a curve with one equichordal point. based on an example in.\nThe core idea is that we may start with any Jordan arc given in polar coordinates by\nan equation formula_16, formula_17, and complement\nit to a closed Jordan curve given by the equation formula_18 for all formula_19. Along the way, we must satisfy some number of conditions to ensure continuity of the resulting curve.\n\nLet us define a function formula_20 by the formula:\nwhere formula_22 is a real parameter and formula_23.\nThis function is clearly defined for all real formula_24, but we only use its\nvalues for formula_17. Clearly, formula_26.\n\nWe define the second function formula_27\nby the formula:\nThis function has the following properties:\nThese properties imply that the curve given in polar coordinates by the equation formula_18\nis a closed Jordan curve and that the origin is an equichordal point.\n\nThe construction presented here and based on results in a curve which is\nformula_40 but not formula_41, with the exception of formula_42, when the curve becomes a circle. Rychlik formulated conditions on the Fourier series of formula_27 which easily allow constructing of curves with one equichordal point, including analytic curves. Rychlik gives a specific example of an analytic curve:\nFourier series analysis in Rychlik's paper reveals the pattern of Fourier coefficients of all suitable functions formula_27.\n\nFor formula_13 we obtain the equichordal point problem, and for\nformula_47 we obtain the equireciprocal point problem considered\nby Klee.\n\nWe may also consider a more general relationship between formula_48\nand formula_49. For example, the equiproduct point problem\nis obtained by considering the equation:\n\nEquivalently,\n\nThis naturally leads to a more general class of problems. For a given function formula_52 we may study the equations:\n\nEven more generally, we could consider a function formula_54 of two real variables. We need to assume that formula_55 is symmetric, i.e. formula_56. Then we may consider the equation:\nClearly, formula_54 needs only be defined for positive formula_22 and formula_60. Thus, the family of chordal problems of this type is parameterized by symmetric functions of two variables.\n\nThis has been the most famous of the chordal problems.\nIn this case, the equation states that every chord passing through formula_1\nhas the same length. It has become known as the equichordal point problem, and was fully solved in 1996 by Marek Rychlik.\n\nKlee proved that the ellipse solves the equireciprocal point problem, with the ellipse foci serving\nas the two equireciprocal points. However, in addition to the ellipses, many\nsolutions of low smoothness also exist, as it was shown in. From the point of view of the equichordal point problem, this is due to the lack of hyperbolicity of the fixed points of a certain map of the plane.\n\nThe method used in Rychlik's proof for the equichordal point problem may only generalize to some rational values of formula_6. A reasonable\nconjecture could be:\n\n"}
{"id": "14084019", "url": "https://en.wikipedia.org/wiki?curid=14084019", "title": "Cornelius Neale", "text": "Cornelius Neale\n\nCornelius Neale (12 August 1789 – February 1823, Chiswick) was an English clergyman.\n\nCornelius Neale came from a London family with an Evangelical background: his father James Neale was one of the founders of the London Missionary Society. He entered St John's College, Cambridge and graduated Senior Wrangler in 1812, with first Smith's Prize and the second Chancellor's medal. He was elected a fellow of his college.\n\nHe was ordained and took a curacy in Leicester. He died of consumption in February 1823.\n\nIn 1816 he married Susannah, daughter of John Mason Good: they had one son, John Mason Neale.\n\n\n"}
{"id": "16550517", "url": "https://en.wikipedia.org/wiki?curid=16550517", "title": "Crispin Nash-Williams", "text": "Crispin Nash-Williams\n\nCrispin St. John Alvah Nash-Williams (19 December 1932 – 20 January 2001) was a British mathematician. His research interest was in the field of discrete mathematics, especially graph theory.\n\nNash-Williams was born on 19 December 1932 in Cardiff, Wales; his father, Victor Erle Nash-Williams, was an archaeologist at University College Cardiff, and his mother had studied classics at Oxford University. After studying mathematics at Cambridge University, earning the title of Senior Wrangler in 1953, he remained for his graduate studies at Cambridge, studying under the supervision of Shaun Wylie and David Rees. He continued his studies for a year at Princeton University, with Norman Steenrod; all three of Wylie, Rees, and Steenrod are listed as the supervisors of his Ph.D. dissertation. He finished his dissertation in 1958, but before doing so he returned to the UK as an assistant lecturer at the University of Aberdeen. He kept his position at Aberdeen through ten years and two promotions until 1967, when he moved to the University of Waterloo and became one of the three faculty members in the newly formed Department of Combinatorics there. In 1972 he returned to Aberdeen, as Professor of Pure Mathematics; in 1975 he moved to the University of Reading, where he took the chair previously held by Richard Rado, who had been one of his dissertation examiners. In 1996 he retired; he died on 20 January 2001 in Ascot, Berkshire, where his brother was rector.\n\nHe was elected to the Royal Society of Edinburgh in 1969. In 1994, the University of Waterloo gave him an honorary doctorate for his contributions to combinatorics. A conference in his honor was held on his retirement in 1996, the proceedings of which were published as a festschrift. The 18th British Combinatorial Conference, held in Sussex in July 2001, was dedicated to his memory.\n\nHilton writes that \"Themes running through his papers are Hamiltonian cycles, Eulerian graphs, spanning trees, the marriage problem, detachments, reconstruction, and infinite graphs.\"\nIn his first papers Nash-Williams considered the knight's tour and random walk problems on infinite graphs; the latter paper included an important recurrence criterion for general Markov chains, and was also the first to apply electrical network techniques of Rayleigh to random walks. His dissertation, which he finished in 1958, concerned generalizations of Euler tours to infinite graphs. Welsh writes that his subsequent work defining and characterizing the arboricity of graphs (discovered in parallel and independently by W. T. Tutte) has \"had a huge impact,\" in part because of its implications in matroid theory. Nash-Williams also studied k-edge-connected graphs, Hamiltonian cycles in dense graphs, versions of the reconstruction conjecture for infinite graphs, and the theory of quasi-orders. He also gave a short elegant proof of Kruskal's tree theorem.\n"}
{"id": "12237984", "url": "https://en.wikipedia.org/wiki?curid=12237984", "title": "Cutaway drawing", "text": "Cutaway drawing\n\nA cutaway drawing, also called a cutaway diagram is a 3D graphics, drawing, diagram and or illustration, in which surface elements of a three-dimensional model are selectively removed, to make internal features visible, but without sacrificing the outer context entirely.\n\nAccording to Diepstraten et al. (2003) \"the purpose of a cutaway drawing is to allow the viewer to have a look into an otherwise solid opaque object. Instead of letting the inner object shine through the surrounding surface, parts of outside object are simply removed. This produces a visual appearance as if someone had cutout a piece of the object or sliced it into parts. Cutaway illustrations avoid ambiguities with respect to spatial ordering, provide a sharp contrast between foreground and background objects, and facilitate a good understanding of spatial ordering\".\n\nThough cutaway drawing are not dimensioned manufacturing blueprints, they are meticulously drawn by a handful of devoted artists who either had access to manufacturing details or deduced them by observing the visible evidence of the hidden skeleton (e.g. rivet lines, etc.). The goal of this drawings in studies can be to identify common design patterns for particular vehicle classes. Thus, the accuracy of most of these drawings, while not 100 percent, is certainly high enough for this purpose.\n\nThe technique is used extensively in computer-aided design, see first image. It has also been incorporated into the user interface of some video games. In The Sims, for instance, users can select through a control panel whether to view the house they are building with no walls, cutaway walls, or full walls.\n\nThe cutaway view and the exploded view were minor graphic inventions of the Renaissance that also clarified pictorial representation. This cutaway view originates in the early fifteenth century notebooks of Marino Taccola (1382 – 1453). In the 16th century cutaway views in definite form were used in Georgius Agricola's (1494-1555) mining book \"De Re Metallica\" to illustrate underground operations. The 1556 book is a complete and systematic treatise on mining and extractive metallurgy, illustrated with many fine and interesting woodcuts which illustrate every conceivable process to extract ores from the ground and metal from the ore, and more besides. It shows the many watermills used in mining, such as the machine for lifting men and material into and out of a mine shaft, see image.\n\nThe term \"Cutaway drawing\" was already in use in the 19th century but, became popular in the 1930s.\n\nThe location and shape to cut the outside object depends on many different factors, for example:\n\nThese factors, according to Diepstraten et al. (2003), \"can seldom be formalized in a simple algorithm, But the properties of cutaway can be distinguish in two classes of cutaways of a drawing\":\n\nSome more examples of cutaway drawings, from products and systems to architectural building. \n\nSimilar types of technical drawings\n\n"}
{"id": "22684368", "url": "https://en.wikipedia.org/wiki?curid=22684368", "title": "Decision tree model", "text": "Decision tree model\n\nIn computational complexity and communication complexity theories the decision tree model is the model of computation or communication in which an algorithm or communication process is considered to be basically a decision tree, i.e., a sequence of branching operations based on comparisons of some quantities, the comparisons being assigned the unit computational cost.\n\nThe branching operations are called \"tests\" or \"queries\". In this setting the algorithm in question may be viewed as a computation of a Boolean function formula_1 where the input is a series of queries and the output is the final decision. Every query is dependent on previous queries.\n\nSeveral variants of decision tree models have been introduced, depending on the complexity of the operations allowed in the computation of a single comparison and the way of branching.\n\nDecision trees models are instrumental in establishing lower bounds for computational complexity for certain classes of computational problems and algorithms: the lower bound for worst-case computational complexity is proportional to the largest depth among the decision trees for all possible inputs for a given computational problem. The computation complexity of a problem or an algorithm expressed in terms of the decision tree model is called decision tree complexity or query complexity.\n\nThe model in which every decision is based on the comparison of two numbers within constant time is called a \"decision tree model\". It was introduced to establish computational complexity of sorting and searching.\n\nThe simplest illustration of this lower bound technique is for the problem of finding the smallest number among \"n\" numbers using only comparisons. In this case the decision tree model is a binary tree. Algorithms for this searching problem may result in \"n\" different outcomes (since any of the \"n\" given numbers may turn out to be the smallest one). It is known that the depth of a binary tree with \"n\" leaves is at least formula_2, which gives a lower bound of formula_3 for the searching problem. \n\nHowever this lower bound is known to be slack, since the following simple argument shows that at least \"n\" - 1 comparisons are needed: Before the smallest number can be determined, every number except the smallest must \"lose\" (compare greater) in at least one comparison.\n\nAlong the same lines the lower bound of formula_4 for sorting may be proved. In this case, the existence of numerous comparison-sorting algorithms having this time complexity, such as mergesort and heapsort, demonstrates that the bound is tight.\n\nLinear decision trees, just like the simple decision trees, make a branching decision based on a set of values as input. As opposed to binary decision trees, linear decision trees have three output branches. A linear function formula_5 is being tested and branching decisions are made based on the sign of the function (negative, positive, or 0).\n\nGeometrically, formula_6 defines a hyperplane in R. A set of input values reaching any particular nodes represents the intersection of the half-planes defined by the nodes.\n\nAlgebraic decision trees are a generalization of linear decision trees to allow test functions to be polynomials of degree \"d\". Geometrically, the space is divided into semi-algebraic sets (a generalization of hyperplane). The evaluation of the complexity is more difficult.\n\nIf the output of a decision tree is formula_6, for all formula_8, the decision tree is said to \"compute\" formula_9. The depth of a tree is the maximum number of queries that can happen before a leaf is reached and a result obtained. formula_10, the deterministic decision tree complexity of formula_9 is the smallest depth among all deterministic decision trees that compute formula_9.\n\nOne way to define a randomized decision tree is to add additional nodes to the tree, each controlled by a probability formula_13. Another equivalent definition is to select a whole decision tree at the beginning from a set of decision trees based on a probability distribution. Based on this second definition, the complexity of the randomized tree is defined as the greatest depth among all the trees associated with probabilities greater than 0. formula_14 is defined as the complexity of the lowest-depth randomized decision tree whose result is formula_6 with probability at least formula_16 for all formula_8 (i.e., with bounded 2-sided error).\n\nformula_14 is known as the Monte Carlo randomized decision-tree complexity, because the result is allowed to be incorrect with bounded two-sided error. The Las Vegas decision-tree complexity formula_19 measures the \"expected\" depth of a decision tree that must be correct (i.e., has zero-error). There is also a one-sided bounded-error version known as formula_20.\n\nThe nondeterministic decision tree complexity of a function is known more commonly as the certificate complexity of that function. It measures the number of input bits that a nondeterministic algorithm would need to look at in order to evaluate the function with certainty.\n\nThe quantum decision tree complexity formula_21 is the depth of the lowest-depth quantum decision tree that gives the result formula_6 with probability at least formula_16 for all formula_8. Another quantity, formula_25, is defined as the depth of the lowest-depth quantum decision tree that gives the result formula_6 with probability 1 in all cases (i.e. computes formula_9 exactly). formula_21 and formula_25 are more commonly known as quantum query complexities, because the direct definition of a quantum decision tree is more complicated than in the classical case. Similar to the randomized case, we define formula_30 and formula_31.\n\nIt follows immediately from the definitions that for all formula_32-bit Boolean functions formula_9,\nformula_34, and formula_35.\n\nBlum and Impagliazzo, Hartmanis and Hemachandra, and Tardos independently discovered that formula_36. Noam Nisan found that the Monte Carlo randomized decision tree complexity is also polynomially related to deterministic decision tree complexity: formula_37. (Nisan also showed that formula_38.) A tighter relationship is known between the Monte Carlo and Las Vegas models: formula_39. This relationship is optimal up to polylogarithmic factors.\n\nThe quantum decision tree complexity formula_21 is also polynomially related to formula_10. Midrijanis showed that formula_42, improving a quartic bound due to Beals et al. Beals et al. also showed that formula_43, and this is still the best known bound. However, the largest known gap between deterministic and quantum query complexities is only quadratic. A quadratic gap is achieved for the OR function; formula_44 while formula_45.\n\nIt is important to note that these polynomial relationships are valid only for \"total\" Boolean functions. For partial Boolean functions, that have a domain a subset of formula_46, an exponential separation between formula_25 and formula_10 is possible; the first example of such a problem was discovered by Deutsch and Jozsa.\n\nThese relationships can be summarized by the following inequalities, which are true up to constant factors:\n\n"}
{"id": "13688204", "url": "https://en.wikipedia.org/wiki?curid=13688204", "title": "EHP spectral sequence", "text": "EHP spectral sequence\n\nIn mathematics, the EHP spectral sequence is a spectral sequence used for inductively calculating the homotopy groups of spheres localized at some prime \"p\". It is described in more detail in and . It is related to the EHP long exact sequence of ; the name \"EHP\" comes from the fact that George W. Whitehead named 3 of the maps of his sequence \"E\" (the first letter of the German word \"Einhängung\" meaning \"suspension\"), \"H\" (for Heinz Hopf, as this map is the second Hopf–James invariant), and \"P\" (related to Whitehead products).\n\nFor formula_1 the spectral sequence uses some exact sequences associated to the fibration \nwhere formula_3 stands for a loop space and the (2) is localization of a topological space at the prime 2. This gives a spectral sequence with formula_4 term equal to \nand converging to formula_6 (stable homotopy groups of spheres localized at 2). The spectral sequence has the advantage that the input is previously calculated homotopy groups. It was used by to calculate the first 31 stable homotopy groups of spheres.\n\nFor arbitrary primes one uses some fibrations found by : \nwhere formula_9 is the formula_10-skeleton of the loop space formula_11. (For formula_1, the space formula_9 is the same as formula_14, so Toda's fibrations at formula_1 are the same as the James fibrations.)\n\n"}
{"id": "46885960", "url": "https://en.wikipedia.org/wiki?curid=46885960", "title": "Economics of networks", "text": "Economics of networks\n\nEconomics of networks is an increasing new field on the border of economics and network sciences. It is concerned with understanding of economic phenomena by using network concepts and the tools of network science. Some main authors in this field are Sanjeev Goyal, Matthew O. Jackson and Rachel Kranton.\n\nThis term should not be confused with network economics or network externality, a theory explaining that a product or service has an increasing demand, that is, the more people use it, the more utility it brings.\n\nUsing the concept of networks during the analysis of markets can enable us to better understand its functioning. On the border of network science and market theory, several models have emerged explaining different aspects of markets.\n\nExchange theory explains how economic transactions, tradon of favors, communication of information or other goods’ exchanges are affected by the structure of relationship among the involved participants. The main concept is that the act of exchange depends on the agents’ other opportunities and their environment, and thus getting a deeper understanding is possible only by examining these factors. The position of a given agent in the network, for example, can endow her with power over the auctions and deals she make with her partners.\n\nAs part of exchange theory, bilateral trading models consider sellers and buyers and use game theory models of the bargaining in networks in order to predict the behaviour of agents depending on the type of network. The outcome of transactions can be determined by, for example, the number of sellers a buyer is connected to, or vice versa for which Corominas-Bosch built a very simple model. Another case is when the agents agree on the transaction through an auction and their decision making during the auction depends on the link structure. Kranton and Minehart came to the conclusion that if we consider markets as networks it can enable sellers to pool uncertainty in demand. As building links is costly, due to the trade-off not everybody need to be linked to everybody in the network. Sparsity in the network will prove to be efficient.\n\nThe first networks in economics were discovered prior to the development of network science. Károly Polány, Claude Levi-Strauss or Bronislaw Malinowski studied such tribes where complicated gift exchange mechanisms constructed networks between groups, families or islands. Although modern trade systems differ fundamentally, such systems based on reciprocity can still survive and reciprocity-based or personalised exchange deals persists even when a market would be more efficient. According to Kranton, informal exchange can exist in networks if transactions are more reciprocal than market-based. In this case, market exchange is hard to find and associated with high search costs, therefore yields low utility. Personalised exchange agreements ensure the possibility of long term agreements.\n\nRecent studies have tried to examine the deeper connection between socioeconomic factors and phenomena and the scale free property. They found that business networks have scale-free property, and that the merger among companies decreases the average separation between firms and increase cliquishness. In another research, scientists found that payment flows in an online payment system exhibits free-scale property, high clustering coefficient and small world phenomenon, and that after 9/11 attacks the connectivity of the network reduced and average path length increased. These results were found to be useful in order to understand how to overcome a possible contagion of similar disturbances in payment networks.\n\nWorld trade is generally highlighted as a typical example for huge networks. The interconnectedness of the countries can both have positive and negative externalities. It has been shown that the world trade web exhibits scale free properties, where the main hub is the United States. 18 out of 21 analyzed developed countries showed large synchronization in economic performance and cycles with the US during 1975-2000. The remaining three countries are special cases. Austria’s performance correlates highly with that of Germany, though Germany and Japan took completely different economic paths. It seems, despite their embeddedness into the global economy, that the unusual economic measures following Germany’s unification in 1992 and the Plaza Accord in 1985 (which appreciated the Japanese Yen), drove these two countries off the normal economic track. The importance of regional economic (and political) cooperation also appears in this analysis.\n\n\n\n"}
{"id": "1104849", "url": "https://en.wikipedia.org/wiki?curid=1104849", "title": "Efim Zelmanov", "text": "Efim Zelmanov\n\nEfim Isaakovich Zelmanov (; born 7 September 1955 in Khabarovsk) is a Russian-American mathematician, known for his work on combinatorial problems in nonassociative algebra and group theory, including his solution of the restricted Burnside problem. He was awarded a Fields Medal at the International Congress of Mathematicians in Zürich in 1994.\n\nZelmanov was born into a Jewish family in Khabarovsk, Soviet Union (now in Russia). He entered Novosibirsk State University in 1972, when he was 17 years old. He obtained doctoral degree at Novosibirsk State University in 1980, and a higher degree at Leningrad State University in 1985. He had a position in Novosibirsk until 1987, when he left the Soviet Union.In 1990 he moved to the United States, becoming a professor at the University of Wisconsin–Madison. He was at the University of Chicago in 1994/5, then at Yale University. As of 2011, he is a professor at the University of California, San Diego and a Distinguished Professor at the Korea Institute for Advanced Study.\nZelmanov was elected a member of the U.S. National Academy of Sciences in 2001, becoming, at the age of 47, the youngest member of the mathematics section of the academy.\nHe is also an elected member of the American Academy of Arts and Sciences (1996) and a foreign member of the Korean Academy of Science and Engineering and of the Spanish Royal Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society.\n\nZelmanov gave invited talks at the International Congress of Mathematicians in Warsaw (1983), Kyoto (1990) and Zurich (1994). He was awarded Honorary Doctor degrees from the University of Alberta, Canada (2011), Shevchenko National University of Kyiv, Ukraine (2012), the Universidad Internacional Menéndez Pelayo in Santander, Spain (2015) and the University of Lincoln, UK (2016).\n\nZelmanov's early work was on Jordan algebras in the case of infinite dimensions. He was able to show that Glennie's identity in a certain sense generates all identities that hold. He then showed that the Engel identity for Lie algebras implies nilpotence, in the case of infinite dimensions.\n\n"}
{"id": "11840868", "url": "https://en.wikipedia.org/wiki?curid=11840868", "title": "Entropy power inequality", "text": "Entropy power inequality\n\nIn information theory, the entropy power inequality is a result that relates to so-called \"entropy power\" of random variables. It shows that the entropy power of suitably well-behaved random variables is a superadditive function. The entropy power inequality was proved in 1948 by Claude Shannon in his seminal paper \"A Mathematical Theory of Communication\". Shannon also provided a sufficient condition for equality to hold; Stam (1959) showed that the condition is in fact necessary.\n\nFor a random variable \"X\" : Ω → R with probability density function \"f\" : R → R, the differential entropy of \"X\", denoted \"h\"(\"X\"), is defined to be\n\nand the entropy power of \"X\", denoted \"N\"(\"X\"), is defined to be\n\nIn particular, \"N\"(\"X\") = |\"K\"| when \"X\" is normal distributed with covariance matrix \"K\".\n\nLet \"X\" and \"Y\" be independent random variables with probability density functions in the \"L\" space \"L\"(R) for some \"p\" > 1. Then\n\nMoreover, equality holds if and only if \"X\" and \"Y\" are multivariate normal random variables with proportional covariance matrices.\n\n\n"}
{"id": "2452847", "url": "https://en.wikipedia.org/wiki?curid=2452847", "title": "Giovanni Vacca (mathematician)", "text": "Giovanni Vacca (mathematician)\n\nGiovanni Enrico Eugenio Vacca (18 November 1872 – 6 January 1953) was an Italian mathematician, Sinologist and historian of science. \n\nVacca studied mathematics and graduated from the University of Genoa in 1897 under the guidance of G. B. Negri. He was a politically active student and was banished for that from Genoa in 1897. He moved to Turin and became an assistant to Giuseppe Peano. In 1899 he studied, at Hanover, unpublished manuscripts of Gottfried Wilhelm Leibniz, which he published in 1903. Around 1898 Vacca became interested in Chinese language and culture after attending a Chinese exhibition in Turin. He took private lessons of Chinese and continued to study it at the University of Florence. Vacca then traveled to China in 1907–8 and defended a PhD in Chinese studies in 1910. In 1911, he became a lecturer in Chinese literature at the University of Rome. In 1922, he moved to Florence and taught Chinese literature and language at university until 1947. \n\nThe interests of Vacca were almost equally split between mathematics, Sinology and history of science, with a corresponding number of papers being 38, 47 and 45. In 1910, Vacca developed a complex number iteration for pi:\n\nThe calculation efficiency of these formulas is significantly worse than of the modern Borwein's algorithm – they converge by only about half a decimal point with each iteration.\n\nVacca published his two major contributions to mathematics in 1910 and 1926, on series expansion (later named Vacca series) of the Euler constant. They are, respectively\nVacca noted in 1910 that:\n"}
{"id": "7611764", "url": "https://en.wikipedia.org/wiki?curid=7611764", "title": "Hall algebra", "text": "Hall algebra\n\nIn mathematics, the Hall algebra is an associative algebra with a basis corresponding to isomorphism classes of finite abelian \"p\"-groups. It was first discussed by but forgotten until it was rediscovered by , both of whom published no more than brief summaries of their work. The Hall polynomials are the structure constants of the Hall algebra. The Hall algebra plays an important role in the theory of Masaki Kashiwara and George Lusztig regarding canonical bases in quantum groups. generalized Hall algebras to more general categories, such as the category of representations of a quiver.\n\nA finite abelian \"p\"-group \"M\" is a direct sum of cyclic \"p\"-power components formula_1 where\nformula_2 is a partition of formula_3 called the \"type\" of \"M\". Let formula_4 be the number of subgroups \"N\" of \"M\" such that \"N\" has type formula_5 and the quotient \"M/N\" has type formula_6. Hall proved that the functions \"g\" are polynomial functions of \"p\" with integer coefficients. Thus we may replace \"p\" with an indeterminate \"q\", which results in the Hall polynomials \n\nHall next constructs an associative ring formula_8 over formula_9, now called the Hall algebra. This ring has a basis consisting of the symbols formula_10 and the structure constants of the multiplication in this basis are given by the Hall polynomials: \n\nIt turns out that \"H\" is a commutative ring, freely generated by the elements formula_12 corresponding to the elementary \"p\"-groups. The linear map from \"H\" to the algebra of symmetric functions defined on the generators by the formula\n\n(where \"e\" is the \"n\"th elementary symmetric function) uniquely extends to a ring homomorphism and the images of the basis elements formula_10 may be interpreted via the Hall–Littlewood symmetric functions. Specializing \"q\" to 0, these symmetric functions become Schur functions, which are thus closely connected with the theory of Hall polynomials.\n\n"}
{"id": "2576185", "url": "https://en.wikipedia.org/wiki?curid=2576185", "title": "Happy ending problem", "text": "Happy ending problem\n\nThe \"happy ending problem\" (so named by Paul Erdős because it led to the marriage of George Szekeres and Esther Klein) is the following statement:\n\nThis was one of the original results that led to the development of Ramsey theory.\n\nThe happy ending theorem can be proven by a simple case analysis: if four or more points are vertices of the convex hull, any four such points can be chosen. If on the other hand, the point set has the form of a triangle with two points inside it, the two inner points and one of the triangle sides can be chosen. See for an illustrated explanation of this proof, and for a more detailed survey of the problem.\n\nThe Erdős–Szekeres conjecture states precisely a more general relationship between the number of points in a general-position point set and its largest convex polygon, namely that the smallest number of points for which any general position arrangement contains a convex subset of formula_1 points is formula_2 . It remains unproven, but less precise bounds are known.\n\n proved the following generalisation:\n\nThe proof appeared in the same paper that proves the Erdős–Szekeres theorem on monotonic subsequences in sequences of numbers.\n\nLet \"f\"(\"N\") denote the minimum \"M\" for which any set of \"M\" points in general position must contain a convex \"N\"-gon. It is known that\n\nOn the basis of the known values of \"f\"(\"N\") for \"N\" = 3, 4 and 5, Erdős and Szekeres conjectured in their original paper that\nThey proved later, by constructing explicit examples, that\nbut the best known upper bound when \"N\" ≥ 7 is\n\nThere is also the question of whether any sufficiently large set of points in general position has an \"empty\" convex quadrilateral, pentagon, etc.,\nthat is, one that contains no other input point. The original solution to the happy ending problem can be adapted to show that any five points in general position have an empty convex quadrilateral, as shown in the illustration, and any ten points in general position have an empty convex pentagon. However, there exist arbitrarily large sets of points in general position that contain no empty convex heptagon.\n\nFor a long time the question of the existence of empty hexagons remained open, but and proved that every sufficiently large point set in general position contains a convex empty hexagon. More specifically, Gerken showed that the number of points needed is no more than \"f\"(9) for the same function \"f\" defined above, while Nicolás showed that the number of points needed is no more than \"f\"(25). supplies a simplification of Gerken's proof that however requires more points, \"f\"(15) instead of \"f\"(9). At least 30 points are needed: there exists a set of 29 points in general position with no empty convex hexagon.\n\nThe problem of finding sets of \"n\" points minimizing the number of convex quadrilaterals is equivalent to minimizing the crossing number in a straight-line drawing of a complete graph. The number of quadrilaterals must be proportional to the fourth power of \"n\", but the precise constant is not known.\n\nIt is straightforward to show that, in higher-dimensional Euclidean spaces, sufficiently large sets of points will have a subset of \"k\" points that forms the vertices of a convex polytope, for any \"k\" greater than the dimension: this follows immediately from existence of convex \"k\"-gons in sufficiently large planar point sets, by projecting the higher-dimensional point set into an arbitrary two-dimensional subspace. However, the number of points necessary to find \"k\" points in convex position may be smaller in higher dimensions than it is in the plane, and it is possible to find subsets that are more highly constrained. In particular, in \"d\" dimensions, every \"d\" + 3 points in general position have a subset of \"d\" + 2 points that form the vertices of a cyclic polytope. More generally, for every \"d\" and \"k\" > \"d\" there exists a number \"m\"(\"d\",\"k\") such that every set of \"m\"(\"d\",\"k\") points in general position has a subset of \"k\" points that form the vertices of a neighborly polytope.\n\n"}
{"id": "36779096", "url": "https://en.wikipedia.org/wiki?curid=36779096", "title": "Hemiperfect number", "text": "Hemiperfect number\n\nIn number theory, a hemiperfect number is a positive integer with a half-integral abundancy index.\n\nFor a given odd number \"k\", a number \"n\" is called \"k\"-hemiperfect if and only if the sum of all positive divisors of \"n\" (the divisor function, \"σ\"(\"n\")) is equal to  × n.\n\nThe following table gives an overview of the smallest \"k\"-hemiperfect numbers for \"k\" ≤ 17 :\n\nFor example, 24 is 5-hemiperfect because the sum of the divisors of 24 is\n\n"}
{"id": "1633368", "url": "https://en.wikipedia.org/wiki?curid=1633368", "title": "Hensel's lemma", "text": "Hensel's lemma\n\nIn mathematics, Hensel's lemma, also known as Hensel's lifting lemma, named after Kurt Hensel, is a result in modular arithmetic, stating that if a polynomial equation has a simple root modulo a prime number , then this root corresponds to a unique root of the same equation modulo any higher power of , which can be found by iteratively \"lifting\" the solution modulo successive powers of . More generally it is used as a generic name for analogues for complete commutative rings (including \"p\"-adic fields in particular) of the Newton method for solving equations. Since \"p\"-adic analysis is in some ways simpler than real analysis, there are relatively neat criteria guaranteeing a root of a polynomial.\n\nMany equivalent statements of Hensel's lemma exist. Arguably the most common statement is the following.\n\nAssume formula_1 is a field complete with respect to a normalised discrete valuation formula_2. Suppose, furthermore, that formula_3 is the ring of integers of formula_1 (i.e. all elements of formula_1 with non-negative valuation), let formula_6 be such that formula_7 and let formula_8 denote the residue field. Let formula_9 be a polynomial with coefficients in formula_3. If the reduction formula_11 has a simple root (i.e. there exists formula_12 such that formula_13 and formula_14), then there exists a unique formula_15 such that formula_16 and the reduction formula_17 in formula_18.\n\nAnother way of stating this (in less generality) is: let formula_19 be a polynomial with integer (or \"p\"-adic integer) coefficients, and let \"m\",\"k\" be positive integers such that \"m\" ≤ \"k\". If \"r\" is an integer such that\n\nthen there exists an integer \"s\" such that\n\nFurthermore, this \"s\" is unique modulo \"p\", and can be computed explicitly as the integer such that\n\nNote that formula_20 so that the condition formula_28 is met. As an aside, if formula_29, then 0, 1, or several \"s\" may exist (see Hensel Lifting below).\n\nThe lemma derives from considering the Taylor expansion of \"f\" around \"r\". From formula_30, we see that \"s\" has to be of the form \"s = r + tp\" for some integer \"t\". \n\nLet formula_31, where formula_32, hence\n\nReducing both sides modulo formula_35, we see that for formula_22 to hold, we need\n\nThen we note that formula_38 for some integer formula_39 since formula_40 is a root of formula_41\".\" Thus,\nwhich is to say\n\nSolving for formula_44 in formula_45 gives the explicit formula for \"s\" mentioned above. The assumption that formula_46 is not divisible by \"p\" ensures that formula_46 has an inverse mod formula_48 which is necessarily unique. Hence a solution for \"t\" exists uniquely modulo formula_48, and \"s\" exists uniquely modulo formula_50.\n\nUsing the lemma, one can \"lift\" (i.e. add multiples of the next power of p) a root \"r\" of the polynomial \"f\" mod \"p\" to a new root \"s\" mod \"p\" such that \"r\" ≡ \"s\" mod \"p\" (by taking \"m\"=1; taking larger \"m\" follows by induction). In fact, a root mod \"p\" is also a root mod \"p\", so the roots mod \"p\" are precisely the liftings of roots mod \"p\". The new root \"s\" is congruent to \"r\" mod \"p\", so the new root also satisfies formula_51. So the lifting can be repeated, and starting from a solution \"r\" of formula_41 we can derive a sequence of solutions \"r\", \"r\", ... of the same congruence for successively higher powers of \"p\", provided formula_53 for the initial root \"r\". This also shows that \"f\" has the same number of roots mod \"p\" as mod \"p\", mod \"p\" , or any other higher power of \"p\", provided the roots of \"f\" mod \"p\" are all simple.\n\nWhat happens to this process if \"r\" is not a simple root mod \"p\"? If we have a root mod \"p\" at which the derivative mod \"p\" is 0, then there is \"not\" a unique lifting of a root mod \"p\" to a root mod \"p\": either there is no lifting to a root mod \"p\" or there are multiple choices:\n\nThat is, formula_57 for all integers \"t\".\nTherefore if formula_58 then there is no lifting of \"r\" to a root of \"f\"(\"x\") mod \"p\", while if formula_59 then every lifting of \"r\" to modulus \"p\" is a root of \"f\"(\"x\") mod \"p\".\n\nTo see the difficulty that can arise in concrete examples, take \"p\" = 2, \"f\"(\"x\") = \"x\" + 1, and \"r\" = 1. Then \"f\"(1) ≡ 0 mod 2 and f'(1) ≡ 0 mod 2. We have \"f\"(1) = 2 ≠ 0 mod 4 which means that no lifting of 1 to modulus 4 is a root of \"f\"(\"x\") mod 4.\n\nOn the other hand, if we take \"f\"(\"x\") = \"x\" − 17 then, as before, 1 is a root of \"f\"(\"x\") mod 2 and the derivative is 0 mod 2. But since \"f\"(1) is 0 mod 4, then we can lift our solution to modulo 4 and both 1 and 3 are solutions. The derivative is still 0 mod 2, so \"a priori\" we don't know whether we can lift them to modulo 8, but in fact we can, since \"f\"(1) is 0 mod 8 and \"f\"(3) is 0 mod 8, giving solutions at 1, 3, 5, and 7 mod 8. Since of these only \"f\"(1) and \"f\"(7) are 0 mod 16 we can lift only 1 and 7 to modulo 16, giving 1, 7, 9, and 15 mod 16. Of these, only 7 and 9 give \"f\"(\"x\")=0 mod 32, so these can be raised giving 7, 9, 23, and 25 mod 32. It turns out that (for this example f(x)) for every integer \"k\" ≥ 3, there are four liftings of 1 mod 2 to a root of \"f\"(\"x\") mod 2.\n\nIn the \"p\"-adic numbers, where we can make sense of rational numbers modulo powers of \"p\" as long as the denominator is not a multiple of \"p\", the recursion from \"r\" (roots mod \"p\") to \"r\" (roots mod \"p\") can be expressed in a much more intuitive way. Instead of choosing \"t\" to be an(y) integer which solves the congruence\nformula_60, let \"t\" be the rational number formula_61 (the \"p\" here is not really a denominator since \"f\"(\"r\") is divisible by \"p\"). Then set\n\nThis fraction may not be an integer, but it is a \"p\"-adic integer, and the sequence of numbers \"r\" converges in the \"p\"-adic integers to a root of \"f\"(\"x\") = 0. Moreover, the displayed recursive formula for the (new) number \"r\" in terms of \"r\" is precisely Newton's method for finding roots to equations in the real numbers.\n\nBy working directly in the \"p\"-adics and using the \"p\"-adic absolute value, there is a version of Hensel's lemma which can be applied even if we start with a solution of \"f\"(\"a\") ≡ 0 mod \"p\" such that f'(\"a\") ≡ 0 mod \"p\". We just need to make sure the number f'(\"a\") is not exactly 0. This more general version is as follows:\nif there is an integer \"a\" which satisfies |\"f\"(\"a\")| < |f′(\"a\")|, then there is a unique \"p\"-adic integer \"b\" such \"f\"(\"b\") = 0 and |\"b\"-\"a\"| < |f'(\"a\")|. The construction of \"b\" amounts to showing that the recursion from Newton's method with initial value \"a\" converges in the \"p\"-adics and we let \"b\" be the limit. The uniqueness of \"b\" as a root fitting the condition |\"b\"-\"a\"| < |f'(\"a\")| needs additional work.\n\nThe statement of Hensel's lemma given above (taking formula_63) is a special case of this more general version, since the conditions that \"f\"(\"a\") ≡ 0 mod \"p\" and f'(\"a\") ≠ 0 mod \"p\" say that |\"f\"(\"a\")| < 1 and |f'(\"a\")| = 1.\n\nSuppose that \"p\" is an odd prime number and \"a\" is a quadratic residue modulo \"p\" that is nonzero mod \"p\". Then Hensel's lemma implies that \"a\" has a square root in the ring of \"p\"-adic integers Z. Indeed, let \"f\"(\"x\")=\"x\"-\"a\". Its derivative is 2\"x\", so if \"r\" is a square root of \"a\" mod \"p\" we have\n\nwhere the second condition depends on \"p\" not being 2. The basic version of Hensel's lemma tells us that starting from \"r\"= \"r\" we can recursively construct a sequence of integers { \"r\" } such that\n\nThis sequence converges to some \"p\"-adic integer \"b\" and \"b\"=\"a\". In fact, \"b\" is the unique square root of \"a\" in Z congruent to \"r\" modulo \"p\". Conversely, if \"a\" is a perfect square in Z and it is not divisible by \"p\" then it is a nonzero quadratic residue mod \"p\". Note that the quadratic reciprocity law allows one to easily test whether \"a\" is a nonzero quadratic residue mod \"p\", thus we get a practical way to determine which \"p\"-adic numbers (for \"p\" odd) have a \"p\"-adic square root, and it can be extended to cover the case \"p\"=2 using the more general version of Hensel's lemma (an example with 2-adic square roots of 17 is given later).\n\nTo make the discussion above more explicit, let us find a \"square root of 2\" (the solution to formula_67) in the 7-adic integers. Modulo 7 one solution is 3 (we could also take 4), so we set formula_68. Hensel's lemma then allows us to find formula_69 as follows:\n\nAnd sure enough, formula_77. (If we had used the Newton method recursion directly in the 7-adics, then \"r\" = \"r\" - f(\"r\")/f'(\"r\") = 3 - 7/6 = 11/6, and 11/6 ≡ 10 mod 7.)\n\nWe can continue and find formula_78. Each time we carry out the calculation (that is, for each successive value of \"k\"), one more base 7 digit is added for the next higher power of 7. In the 7-adic integers this sequence converges, and the limit is a square root of 2 in Z which has initial 7-adic expansion\n\nIf we started with the initial choice formula_80 then Hensel's lemma would produce a square root of 2 in Z which is congruent to 4 (mod 7) instead of 3 (mod 7) and in fact this second square root would be the negative of the first square root (which is consistent with 4 = -3 mod 7).\n\nAs an example where the original version of Hensel's lemma is not valid but the more general one is, let \"f\"(\"x\") = \"x\" - 17 and \"a\" = 1. Then \"f\"(\"a\") = -16 and f'(\"a\") = 2, so |\"f\"(\"a\")| < |f′(\"a\")|, which implies there is a unique 2-adic integer \"b\" satisfying \"b\" = 17 and |\"b\"- \"a\"| < |f'(\"a\")| = 1/2, i.e., \"b\" ≡ 1 mod 4. There are two square roots of 17 in the 2-adic integers, differing by a sign, and although they are congruent mod 2 they are not congruent mod 4. This is consistent with the general version of Hensel's lemma only giving us a unique 2-adic square root of 17 that is congruent to 1 mod 4 rather than mod 2. If we had started with the initial approximate root \"a\" = 3 then we could apply the more general Hensel's lemma again to find a unique 2-adic square root of 17 which is congruent to 3 mod 4. This is the other 2-adic square root of 17.\n\nIn terms of lifting roots of \"x\" - 17 from one modulus 2 to the next 2, the lifts starting with the root 1 mod 2 are as follows:\n\nFor every \"k\" at least 3, there are \"four\" roots of \"x\" - 17 mod 2, but if we look at their 2-adic expansions we can see that in pairs they are converging to just \"two\" 2-adic limits. For instance, the four roots mod 32 break up into two pairs of roots which each look the same mod 16:\n\nThe 2-adic square roots of 17 have expansions\n\nAnother example where we can use the more general version of Hensel's lemma but not the basic version is a proof that any 3-adic integer \"c\" ≡ 1 mod 9 is a cube in Z. Let \"f\"(\"x\") = \"x\" - c and take initial approximation \"a\" = 1. The basic Hensel's lemma cannot be used to find roots of \"f\"(\"x\") since f'(\"r\") ≡ 0 mod 3 for every \"r\". To apply the general version of Hensel's lemma we want |f(1)| < |f'(1)|, which means \"c\" ≡ 1 mod 27. That is, if \"c\" ≡ 1 mod 27 then the general Hensel's lemma tells us \"f\"(\"x\") has a 3-adic root, so \"c\" is a 3-adic cube. However, we wanted to have this result under the weaker condition that \"c\" ≡ 1 mod 9. If \"c\" ≡ 1 mod 9 then \"c\" ≡ 1, 10, or 19 mod 27. We can apply the general Hensel's lemma three times depending on the value of \"c\" mod 27: if \"c\" ≡ 1 mod 27 then use \"a\" = 1, if \"c\" ≡ 10 mod 27 then use \"a\" = 4 (since 4 is a root of \"f\"(\"x\") mod 27), and if \"c\" ≡ 19 mod 27 then use \"a\" = 7. (It is not true that every \"c\" ≡ 1 mod 3 is a 3-adic cube, e.g., 4 is not a 3-adic cube since it is not a cube mod 9.)\n\nIn a similar way, after some preliminary work Hensel's lemma can be used to show that for any \"odd\" prime number \"p\", any \"p\"-adic integer \"c\" which is 1 mod \"p\" is a \"p\"-th power in Z.\n\nSuppose \"A\" is a commutative ring, complete with respect to an ideal formula_81, and let formula_82 be a polynomial with coefficients in \"A\". Then if \"a\" ∈ \"A\" is an \"approximate root\" of \"f\" in the sense that it satisfies\n\nthen there is an exact root \"b\" ∈ \"A\" of \"f\" \"close to\" \"a\"; that is,\n\nand\n\nFurther, if \"f\" ′(\"a\") is not a zero-divisor then \"b\" is unique.\n\nAs a special case, if formula_86 and \"f\" ′(\"a\") is a unit in \"A\" then there is a unique solution to \"f\"(\"b\") = 0 in \"A\" such that formula_85\n\nThis result can be generalized to several variables as follows:\n\nTheorem: Let \"A\" be a commutative ring that is complete with respect to an ideal m ⊂ \"A\" and\n\"f\"(x) ∈ \"A\"[\"x\", …, \"x\"] for \"i\" = 1...,\"n\" be a system of \"n\" polynomials in \"n\" variables over \"A\". Let f = (\"f\"...,\"f\"), viewed as a mapping from \"A\" to \"A\", and let \"J\"(x) be the Jacobian matrix of f. Suppose some a = (\"a\", …, \"a\") ∈ \"A\" is an approximate solution to f = 0 in the sense that\n\nfor 1 ≤ \"i\" ≤ \"n\". Then there is some b = (\"b\", …, \"b\") in \"A\" satisfying f(b) = 0, i.e.,\n\nand furthermore this solution is \"close\" to a in the sense that\n\nfor 1 ≤ \"i\" ≤ \"n\".\n\nAs a special case, if \"f\"(a) ≡ 0 mod m for all \"i\" and det J(a) is a unit in \"A\" then there is a solution to f(b) = 0 with \"b\" ≡ \"a\" mod m for all \"i\".\n\nWhen \"n\" = 1, a = \"a\" is an element of \"A\" and \"J\"(a) = \"J\"(\"a\") is \"f\" ′(\"a\"). The hypotheses of this multivariable Hensel's lemma reduce to the ones which were stated in the one-variable Hensel's lemma.\n\nCompleteness of a ring is not a necessary condition for the ring to have the Henselian property: Goro Azumaya in 1950 defined a commutative local ring satisfying the Henselian property for the maximal ideal m to be a Henselian ring.\n\nMasayoshi Nagata proved in the 1950s that for any commutative local ring \"A\" with maximal ideal m there always exists a smallest ring \"A\" containing \"A\" such that \"A\" is Henselian with respect to m\"A\". This \"A\" is called the Henselization of \"A\". If \"A\" is noetherian, \"A\" will also be noetherian, and \"A\" is manifestly algebraic as it is constructed as a limit of étale neighbourhoods. This means that \"A\" is usually much smaller than the completion \"Â\" while still retaining the Henselian property and remaining in the same category.\n\n\n"}
{"id": "4529537", "url": "https://en.wikipedia.org/wiki?curid=4529537", "title": "Indeterminate equation", "text": "Indeterminate equation\n\nAn indeterminate equation, in mathematics, is an equation for which there is more than one solution; for example, 2\"x\" = \"y\" is a simple indeterminate equation, as are a\"x\" + b\"y\" = c and \"x\" = 1. Indeterminate equations cannot be solved uniquely. Prominent examples include the following:\n\nUnivariate polynomial equation:\n\nwhich has multiple solutions for the variable \"x\" in the complex plane unless it can be rewritten in the form formula_2.\n\nNon-degenerate conic equation:\n\nwhere at least one of the given parameters \"A\", \"B\", and \"C\" is non-zero, and \"x\" and \"y\" are real variables.\n\nPell's equation:\n\nwhere \"P\" is a given integer that is not a square number, and in which the variables \"x\" and \"y\" are required to be integers.\n\nThe equation of Pythagorean triples:\n\nin which the variables \"x\", \"y\", and \"z\" are required to be positive integers.\n\nThe equation of the Fermat–Catalan conjecture:\n\nin which the variables \"a\", \"b\", \"c\" are required to be coprime positive integers and the variables \"m\", \"n\", and \"k\" are required to be positive integers the sum of whose reciprocals is less than 1.\n\n"}
{"id": "769022", "url": "https://en.wikipedia.org/wiki?curid=769022", "title": "Intermediate logic", "text": "Intermediate logic\n\nIn mathematical logic, a superintuitionistic logic is a propositional logic extending intuitionistic logic. Classical logic is the strongest consistent superintuitionistic logic; thus, consistent superintuitionistic logics are called intermediate logics (the logics are intermediate between intuitionistic logic and classical logic).\n\nA superintuitionistic logic is a set \"L\" of propositional formulas in a countable set of\nvariables \"p\" satisfying the following properties:\nSuch a logic is intermediate if furthermore\n\nThere exists a continuum of different intermediate logics. Specific intermediate logics are often constructed by adding one or more axioms to intuitionistic logic, or by a semantical description. Examples of intermediate logics include:\n\nSuperintuitionistic or intermediate logics form a complete lattice with intuitionistic logic as the bottom and the inconsistent logic (in the case of superintuitionistic logics) or classical logic (in the case of intermediate logics) as the top. Classical logic is the only coatom in the lattice of superintuitionistic logics; the lattice of intermediate logics also has a unique coatom, namely SmL.\n\nThe tools for studying intermediate logics are similar to those used for intuitionistic logic, such as Kripke semantics. For example, Gödel–Dummett logic has a simple semantic characterization in terms of total orders.\n\nGiven a Heyting algebra \"H\", the set of propositional formulas that are valid in \"H\" is an intermediate logic. Conversely, given an intermediate logic it is possible to construct its Lindenbaum algebra which is a Heyting algebra.\n\nAn intuitionistic Kripke frame \"F\" is a partially ordered set, and a Kripke model \"M\" is a Kripke frame with valuation such that formula_4 is an upper subset of \"F\". The set of propositional formulas that are valid in \"F\" is an intermediate logic. Given an intermediate logic \"L\" it is possible to construct a Kripke model \"M\" such that the logic of \"M\" is \"L\" (this construction is called \"canonical model\"). A Kripke frame with this property may not exist, but a general frame always does.\n\nLet \"A\" be a propositional formula. The \"Gödel–Tarski translation\" of \"A\" is defined recursively as follows:\n\n\nIf \"M\" is a modal logic extending S4 then ρ\"M\" = {\"A\" | \"T\"(\"A\") ∈ \"M\"} is a superintuitionistic logic, and \"M\" is called a \"modal companion\" of ρ\"M\". In particular:\n\n\nFor every intermediate logic \"L\" there are many modal logics \"M\" such that \"L\" = ρ\"M\".\n\n\n"}
{"id": "19657952", "url": "https://en.wikipedia.org/wiki?curid=19657952", "title": "Ladder graph", "text": "Ladder graph\n\nIn the mathematical field of graph theory, the ladder graph \"L\" is a planar undirected graph with \"2n\" vertices and \"3n-2\" edges.\n\nThe ladder graph can be obtained as the Cartesian product of two path graphs, one of which has only one edge: \"L\" = \"P\" × \"P\". \n\nBy construction, the ladder graph L is isomorphic to the grid graph \"G\" and looks like a ladder with \"n\" rungs. It is Hamiltonian with girth 4 (if \"n>1\") and chromatic index 3 (if \"n>2\").\n\nThe chromatic number of the ladder graph is 2 and its chromatic polynomial is formula_1.\n\nSometimes the term \"ladder graph\" is used for the \"n\"</sub> × \"P\" ladder rung graph, which is the graph union of \"n\" copies of the path graph P.\n\nThe circular ladder graph \"CL\" is constructible by connecting the four 2-degree vertices in a \"straight\" way, or by the Cartesian product of a cycle of length \"n≥3\" and an edge.\nIn symbols, \"CL\" = \"C\" × \"P\". It has \"2n\" nodes and \"3n\" edges.\nLike the ladder graph, it is connected, planar and Hamiltonian, but it is bipartite if and only if \"n\" is even.\n\nCircular ladder graph are the polyhedral graphs of prisms, so they are more commonly called prism graphs.\n\nCircular ladder graphs:\n\nConnecting the four 2-degree vertices \"crosswise\" creates a cubic graph called a Möbius ladder.\n"}
{"id": "43515506", "url": "https://en.wikipedia.org/wiki?curid=43515506", "title": "Laguerre formula", "text": "Laguerre formula\n\nThe Laguerre formula (named after Edmond Laguerre) provides the acute angle formula_1 between two proper real lines, as follows:\n\nwhere:\n\nThe expression between vertical bars is a real number.\n\nLaguerre formula can be useful in computer vision, since the absolute conic has an image on the retinal plane which is invariant under camera displacements, and the cross ratio of four collinear points is the same for their images on the retinal plane.\n\nIt may be assumed that the lines go through the origin. Any isometry leaves the absolute conic invariant, this allows to take as the first line the \"x\" axis and the second line lying in the plane \"z\"=0. The homogeneous coordinates of the above four points are\n\nrespectively. Their nonhomogeneous coordinates on the infinity line of the plane \"z\"=0 are formula_13, formula_14, 0, formula_15. (Exchanging formula_7 and formula_8 changes the cross ratio into its inverse, so the formula for formula_1 gives the same result.) Now from the formula of the cross ratio we have\nformula_19\n\n"}
{"id": "5168545", "url": "https://en.wikipedia.org/wiki?curid=5168545", "title": "Langevin dynamics", "text": "Langevin dynamics\n\nIn physics, Langevin dynamics is an approach to the mathematical modeling of the dynamics of molecular systems, originally developed by the French physicist Paul Langevin. The approach is characterized by the use of simplified models while accounting for omitted degrees of freedom by the use of stochastic differential equations.\n\nA molecular system in the real world is unlikely to be present in vacuum. Jostling of solvent or air molecules causes friction, and the occasional high velocity collision will perturb the system. Langevin dynamics attempts to extend molecular dynamics to allow for these effects. Also, Langevin dynamics allows temperature to be controlled like with a thermostat, thus approximating the canonical ensemble.\n\nLangevin dynamics mimics the viscous aspect of a solvent. It does not fully model an implicit solvent; specifically, the model does not account for the electrostatic screening and also not for the hydrophobic effect. It should also be noted that for denser solvents, hydrodynamic interactions are not captured via Langevin dynamics.\n\nFor a system of formula_1 particles with masses formula_2, with coordinates formula_3 that constitute a time-dependent random variable, the resulting Langevin equation is \n\nwhere formula_5 is the particle interaction potential; formula_6 is the gradient operator such that formula_7 is the force calculated from the particle interaction potentials; the dot is a time derivative such that formula_8 is the velocity and formula_9 is the acceleration; formula_10 is the viscosity; \"T\" is the temperature, \"k\" is Boltzmann's constant; and formula_11 is a delta-correlated stationary Gaussian process with zero-mean, satisfying\n\nHere, formula_14 is the Dirac delta.\n\nIf the main objective is to control temperature, care should be exercised to use a small damping constant formula_10. As formula_10 grows, it spans from the inertial all the way to the diffusive (Brownian) regime. The Langevin dynamics limit of non-inertia is commonly described as Brownian dynamics. Brownian dynamics can be considered as overdamped Langevin dynamics, i.e. Langevin dynamics where no average acceleration takes place.\n\nThe Langevin equation can be\nreformulated as a Fokker–Planck equation that governs the probability distribution of the random variable \"X\".\n\n\n"}
{"id": "1269900", "url": "https://en.wikipedia.org/wiki?curid=1269900", "title": "Legendre chi function", "text": "Legendre chi function\n\nIn mathematics, the Legendre chi function is a special function whose Taylor series is also a Dirichlet series, given by\n\nAs such, it resembles the Dirichlet series for the polylogarithm, and, indeed, is trivially expressible in terms of the polylogarithm as\n\nThe Legendre chi function appears as the discrete Fourier transform, with respect to the order ν, of the Hurwitz zeta function, and also of the Euler polynomials, with the explicit relationships given in those articles.\n\nThe Legendre chi function is a special case of the Lerch transcendent, and is given by \n"}
{"id": "57193995", "url": "https://en.wikipedia.org/wiki?curid=57193995", "title": "Legendre moment", "text": "Legendre moment\n\nIn mathematics, Legendre moments are a type of image moment and are achieved by using the Legendre polynomial. Legendre moments are used in areas of image processing including: pattern and object recognition, image indexing, line fitting, feature extraction, edge detection, and texture analysis. Legendre moments have been studied as a means to reduce image moment calculation complexity by limiting the amount of information redundancy through approximation.\n\nWith order of \"m\" + \"n\", and object intensity function \"f\"(\"x\",\"y\"):\n\nwhere \"m\",\"n\" = 1, 2, 3, ... with the \"n\"th-order Legendre polynomials being:\n\nwhich can also be written:\n\nwhere \"D\"(\"n\") = floor(\"n\"/2). The set of Legendre polynomials {\"P\"(\"x\")} form an orthogonal set on the interval [−1,1]: \n\nA recurrence relation can be used to compute the Legendre polynomial:\n\n\"f\"(\"x\",\"y\") can be written as an infinite series expansion in terms of Legendre polynomials [−1 ≤ \"x\",\"y\" ≤ 1.]:\n"}
{"id": "537450", "url": "https://en.wikipedia.org/wiki?curid=537450", "title": "List of mathematical topics in classical mechanics", "text": "List of mathematical topics in classical mechanics\n\nThis is a list of mathematical topics in classical mechanics, by Wikipedia page. See also list of variational topics, correspondence principle.\n\n\n\n\n\n\n"}
{"id": "2645647", "url": "https://en.wikipedia.org/wiki?curid=2645647", "title": "List of numerical analysis software", "text": "List of numerical analysis software\n\nListed here are end-user computer applications intended for use with numerical or data analysis:\n\n\n\n\n\n\n"}
{"id": "31241497", "url": "https://en.wikipedia.org/wiki?curid=31241497", "title": "Longevity risk", "text": "Longevity risk\n\nA longevity risk is any potential risk attached to the increasing life expectancy of pensioners and policy holders, which can eventually result in higher pay-out ratios than expected for many pension funds and insurance companies.\n\nOne important risk to individuals who are spending down savings is that they will live longer than expected and thus exhaust their savings, dying in poverty or burdening relatives. This is also referred to as \"outliving one's savings\" or \"outliving one's assets\".\n\nIndividuals often underestimate longevity risk. In the United States, most retirees do not expect to live past 85, but this is in fact the median conditional life expectancy for men at 65 (half of 65-year-old men will live to 85 or older, and more women will).\n\nThe collapse in returns on government bonds is taking place against the backdrop of a protracted fall in returns for other core assets such as blue chip stocks, and, more importantly, a silent demographic shock. Factoring in the corresponding longevity risk, pension premiums could be raised significantly while disposable incomes stagnate and employees work longer years before retiring.\n\n\n"}
{"id": "6200269", "url": "https://en.wikipedia.org/wiki?curid=6200269", "title": "Mix network", "text": "Mix network\n\nMix networks are routing protocols that create hard-to-trace communications by using a chain of proxy servers known as \"mixes\" which take in messages from multiple senders, shuffle them, and send them back out in random order to the next destination (possibly another mix node). This breaks the link between the source of the request and the destination, making it harder for eavesdroppers to trace end-to-end communications. Furthermore, mixes only know the node that it immediately received the message from, and the immediate destination to send the shuffled messages to, making the network resistant to malicious mix nodes.\n\nEach message is encrypted to each proxy using public key cryptography; the resulting encryption is layered like a Russian doll (except that each \"doll\" is of the same size) with the message as the innermost layer. Each proxy server strips off its own layer of encryption to reveal where to send the message next. If all but one of the proxy servers are compromised by the tracer, untraceability can still be achieved against some weaker adversaries.\n\nThe concept of mix networks was first described by David Chaum in 1981. Applications that are based on this concept include anonymous remailers (such as Mixmaster) and onion routing (including Tor).\n\nParticipant \"A\" prepares a message for delivery to participant \"B\" by appending a random value R to the message, sealing it with the addressee's public key formula_1, appending B’s address, and then sealing the result with the mix's public key formula_2.\nM opens it with his private key, now he knows B’s address, and he sends formula_3 to B.\n\nformula_4\n\nTo accomplish this, the sender takes the mix’s public key (formula_2), and uses it to encrypt an envelope containing a random string (formula_6), a nested envelope addressed to the recipient, and the email address of the recipient (\"B\"). This nested envelope is encrypted with the recipient’s public key (formula_1), and contains another random string (\"R0\"), along with the body of the message being sent. Upon receipt of the encrypted top-level envelope, the mix uses its secret key to open it. Inside, it finds the address of the recipient (\"B\") and an encrypted message bound for \"B\". The random string (formula_6) is discarded.\n\nformula_9 is needed in the message in order to prevent an attacker from guessing messages. It is assumed that the attacker can observe all incoming and outgoing messages. If the random string is not used (i.e. only formula_10 is sent to formula_11) and an attacker has a good guess that the message formula_12 was sent, he can test whether formula_13 holds, whereby he can learn the content of the message. By appending the random string formula_9 the attacker is prevented from performing this kind of attack; even if he should guess the correct message (i.e. formula_15 is true) he won't learn if he is right since he doesn't know the secret value formula_9. Practically, formula_9 functions as a salt.\n\nWhat is needed now is a way for \"B\" to respond to \"A\" while still keeping the identity of \"A\" secret from \"B\".\n\nA solution is for \"A\" to form an untraceable return address formula_18 where formula_19 is its own real address, formula_20 is a public one-time key chosen for the current occasion only, and formula_21 is a key that will also act as a random string for purposes of sealing. Then, \"A\" can send this return address to \"B\" as part of a message sent by the techniques already described.\n\nB sends formula_22 to M, and M transforms it to formula_23.\nThis mix uses the string of bits formula_21 that it finds after decrypting the address part formula_25 as a key to re-encrypt the message part formula_26. Only the addressee, \"A\", can decrypt the resulting output because \"A\" created both formula_21 and formula_20. \nThe additional key formula_20 assures that the mix cannot see the content of the reply-message.\nThe following indicates how \"B\" uses this untraceable return address to form a response to \"A\", via a new kind of mix:\n\nThe message from \"A\" formula_30 \"B\":\n\nformula_31\n\nReply message from \"B\"formula_30\"A\":\n\nformula_33\n\nWhere: formula_1 = \"B\"’s public key, formula_2 = the mix’s public key.\n\nA destination can reply to a source without sacrificing source anonymity. The reply message shares all of the performance and security benefits with the anonymous messages from source to destination.\n\nAlthough mix networks provide security even if an adversary is able to view the entire path, mixing is not absolutely perfect. Adversaries can provide long term correlation attacks and track the sender and receiver of the packets.\n\nAn adversary can perform a passive attack by monitoring the traffic to and from the mix network. Analyzing the arrival times between multiple packets can reveal information. Since no changes are actively made to the packets, an attack like this is hard to detect. In a worst case of an attack, we assume that all the links of the network are observable by the adversary and the strategies and infrastructure of the mix network are known.\n\nA packet on an input link cannot be correlated to a packet on the output link based on information about the time the packet was received, the size of the packet, or the content of the packet. Packet correlation based on packet timing is prevented by batching and correlation based on content and packet size is prevented by encryption and packet padding, respectively.\n\nInter-packet intervals, that is, the time difference between observation of two consecutive packets on two network links, is used to infer if the links carry the same connection. The encryption and padding does not affect the inter-packet interval related to the same IP flow. Sequences of inter-packet interval vary greatly between connections, for example in web browsing, the traffic occurs in bursts. This fact can be used to identify a connection.\n\nActive attacks can be performed by injecting bursts of packets that contain unique timing signatures into the targeted flow. The attacker can perform attacks to attempt to identify these packets on other network links. The attacker might not be able to create new packets due to the required knowledge of symmetric keys on all the subsequent mixes. Replay packets cannot be used either as they are easily preventable through hashing and caching.\n\nLarge gaps can be created in the target flow, if the attacker drops large volumes of consecutive packets in the flow. For example, a simulation is run sending 3000 packets to the target flow, where the attacker drops the packets 1 second after the start of the flow. As the number of consecutive packets dropped increases, the effectiveness of defensive dropping decreases significantly. Introducing a large gap will almost always create a recognizable feature.\n\nThe attacker can create artificial bursts. This is done by creating a signature from artificial packets by holding them on a link for a certain period of time and then releasing them all at once. Defense dropping provides no defense in this scenario and the attacker can identify the target flow. There are other defense measures that can be taken to prevent this attack. One such solution can be adaptive padding algorithms. The more the packets are delayed, the easier it is to identify the behavior and thus better defense can be observed.\n\nAn attacker may also look into other timing attacks other than inter-packet intervals. The attacker can actively modify packet streams to observe the changes caused in the network's behavior. Packets can be corrupted to force re-transmission of TCP packets, which the behavior is easily observable to reveal information.\n\nAssuming an adversary can see messages being sent and received into threshold mixes but they can’t see the internal working of these mixes or what is sent by the same. If the adversary has left their own messages in respective mixes and they receive one of the two, they are able to determine the message sent and the corresponding sender. The adversary has to place their messages (active component) in the mix at any given time and the messages must remain there prior to a message being sent. This is not typically an active attack. Weaker adversaries can use this attack in combination with other attacks to cause more issues.\n\nMix networks derive security by changing order of messages they receive to avoid creating significant relation between the incoming and outgoing messages. Mixes create interference between messages. The interference puts bounds on the rate of information leak to an observer of the mix. In a mix of size n, an adversary observing input to and output from the mix has an uncertainty of order n in determining a match. A sleeper attack can take advantage of this. In a layered network of threshold mixes with a sleeper in each mix, there is a layer receiving inputs from senders and a second layer of mixes that forward messages to the final destination. From this, the attacker can learn that the received message could not have come from the sender into any layer 1 mix that did not fire. There is a higher probability of matching the sent and received messages with these sleepers thus communication is not completely anonymous. Mixes may also be purely timed: they randomize the order of messages received in a particular interval and attach some of them with the mixes, forwarding them at the end of the interval despite what has been received in that interval. Messages that are available for mixing will interfere, but if no messages are available, there is no interference with received messages.\n\nDavid Chaum published the concept of Mix Networks in 1979 in his paper: \"Untraceable electronic mail, return addresses, and digital pseudonyms\". The paper was for his master's degree thesis work, shortly after he was first introduced to the field of cryptography through the work of public key cryptography, Martin Hellman, Whitfield Diffie and Ralph Merkle. While public key cryptography encrypted the security of information, Chaum believed there to be personal privacy vulnerabilities in the meta data found in communications. Some vulnerabilities that enabled the compromise of personal privacy included time of messages sent and received, size of messages and the address of the original sender. He cites Martin Hellman and Whitfield's paper \"New Directions in Cryptography\" (1976) in his work.\n"}
{"id": "604268", "url": "https://en.wikipedia.org/wiki?curid=604268", "title": "Multi-index notation", "text": "Multi-index notation\n\nMulti-index notation is a mathematical notation that simplifies formulas used in multivariable calculus, partial differential equations and the theory of distributions, by generalising the concept of an integer index to an ordered tuple of indices.\n\nAn \"n\"-dimensional multi-index is an \"n\"-tuple\n\nof non-negative integers (i.e. an element of the \"n\"-dimensional set of natural numbers, denoted formula_2).\n\nFor multi-indices formula_3 and formula_4 one defines:\n\n\n\n\n\n\n\nwhere formula_11.\n\n\n\nwhere formula_14 (see also 4-gradient).\n\nThe multi-index notation allows the extension of many formulae from elementary calculus to the corresponding multi-variable case. Below are some examples. In all the following, formula_15 (or formula_16), formula_17, and formula_18 (or formula_19).\n\n\nThis formula is used for the definition of distributions and weak derivatives.\n\nIf formula_21 are multi-indices and formula_22, then\n\nThe proof follows from the power rule for the ordinary derivative; if \"α\" and \"β\" are in {0, 1, 2, . . .}, then\n\nSuppose formula_25, formula_26, and formula_22. Then we have that\n\nFor each \"i\" in {1, . . ., \"n\"}, the function formula_29 only depends on formula_30. In the above, each partial differentiation formula_31 therefore reduces to the corresponding ordinary differentiation formula_32. Hence, from equation (1), it follows that formula_33 vanishes if \"α\" > \"β\" for at least one \"i\" in {1, . . ., \"n\"}. If this is not the case, i.e., if \"α\" ≤ \"β\" as multi-indices, then\n\nfor each formula_35 and the theorem follows. formula_36\n\n\n"}
{"id": "662624", "url": "https://en.wikipedia.org/wiki?curid=662624", "title": "Noncommutative topology", "text": "Noncommutative topology\n\nIn mathematics, noncommutative topology is a term used for the relationship between topological and C*-algebraic concepts. The term has its origins in the Gelfand-Naimark theorem, which implies the duality of the category of locally compact Hausdorff spaces and the category of commutative C*-algebras. Noncommutative topology is related to analytic noncommutative geometry.\n\nThe premise behind noncommutative topology is that a noncommutative C*-algebra can be treated like the algebra of complex-valued continuous functions on a 'noncommutative space' which does not exist classically. Several topological properties can be formulated as properties for the C*-algebras without making reference to commutativity or the underlying space, and so have an immediate generalization.\nAmong these are:\n\nIndividual elements of a commutative C*-algebra correspond with continuous functions. And so certain types of functions can correspond to certain properties of a C*-algebra. For example, self-adjoint elements of a commutative C*-algebra correspond to real-valued continuous functions. Also, projections (i.e. self-adjoint idempotents) correspond to indicator functions of clopen sets.\n\nCategorical constructions lead to some examples. For example, the coproduct is the disjoint union and thus corresponds to the direct sum of algebras, which is the product of C*-algebras. Similarly, Product topology corresponds to the coproduct of C*-algebras, the tensor product of algebras. In a more specialized setting,\ncompactifications of topologies correspond to unitizations of algebras. So the one-point compactification corresponds to the minimal unitization of C*-algebras, the Stone-Čech compactification corresponds to the multiplier algebra, and corona sets corresponds with corona algebras.\n\nThere are certain examples of properties where multiple generalizations are possible and it is not clear which is preferable. For example, probability measures can correspond either to states or tracial states. Since all states are vacuously \ntracial states in the commutative case, so it is not clear whether the tracial condition is necessary to be a useful generalization.\n\nOne of the major examples of this idea is the generalization of topological K-theory to noncommutative C*-algebras in the form of operator K-theory.\n\nA further development in this is a bivariant version of K-theory called KK-theory, which has a composition product \n\nformula_1\n\nof which the ring structure in ordinary K-theory is a special case. The product gives the structure of a category to KK. It has been related to correspondences of algebraic varieties.\n"}
{"id": "1450110", "url": "https://en.wikipedia.org/wiki?curid=1450110", "title": "On Formally Undecidable Propositions of Principia Mathematica and Related Systems", "text": "On Formally Undecidable Propositions of Principia Mathematica and Related Systems\n\n\"Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I\" (\"On Formally Undecidable Propositions of Principia Mathematica and Related Systems I\") is a paper in mathematical logic by Kurt Gödel. Dated November 17, 1930, it was originally published in German in the 1931 volume of \"Monatshefte für Mathematik.\" Several English translations have appeared in print, and the paper has been included in two collections of classic mathematical logic papers. The paper contains Gödel's incompleteness theorems, now fundamental results in logic that have many implications for consistency proofs in mathematics. The paper is also known for introducing new techniques that Gödel invented to prove the incompleteness theorems. \n\nThe main results established are Gödel's first and second incompleteness theorems, which have had an enormous impact on the field of mathematical logic. These appear as theorems VI and XI, respectively, in the paper.\n\nIn order to prove these results, Gödel introduced a method now known as Gödel numbering. In this method, each sentence and formal proof in first-order arithmetic is assigned a particular natural number. Gödel shows that many properties of these proofs can be defined within any theory of arithmetic that is strong enough to define the primitive recursive functions. (The contemporary terminology for recursive functions and primitive recursive functions had not yet been established when the paper was published; Gödel used the word \"rekursiv\" (\"recursive\") for what are now known as primitive recursive functions.) The method of Gödel numbering has since become common in mathematical logic.\n\nBecause the method of Gödel numbering was novel, and to avoid any ambiguity, Gödel presented a list of 45 explicit formal definitions of primitive recursive functions and relations used to manipulate and test Gödel numbers. He used these to give an explicit definition of a formula Bew(\"x\") that is true if and only if \"x\" is the Gödel number of a sentence φ and there exists a natural number that is the Gödel number of a proof of φ. The name of this formula derives from \"Beweis\", the German word for proof. \n\nA second new technique invented by Gödel in this paper was the use of self-referential sentences. Gödel showed that the classical paradoxes of self-reference, such as \"This statement is false,\" can be recast as self-referential formal sentences of arithmetic. Informally,\nthe sentence employed to prove Gödel's first incompleteness theorem says \"This statement is not provable.\" The fact that such self-reference can be expressed within arithmetic was not known until Gödel's paper appeared; independent work of Alfred Tarski on his indefinability theorem was conducted around the same time but not published until 1936.\n\nIn footnote 48a, Gödel stated that a planned second part of the paper would establish a link between consistency proofs and type theory, but Gödel did not publish a second part of the paper before his death. His 1958 paper in \"Dialectica\" did, however, show how type theory can be used to give a consistency proof for arithmetic.\n\nDuring his lifetime three English translations of Gödel's paper were printed, but the process was not without difficulty. The first English translation was by Bernard Meltzer; it was published in 1963 as a standalone work by Basic Books and has since been reprinted by Dover and reprinted by Hawking (\"God Created the Integers\", Running Press, 2005:1097ff). The Meltzer version—described by Raymond Smullyan as a 'nice translation'—was adversely reviewed by Stefan Bauer-Mengelberg (1966). According to Dawson's biography of Gödel (Dawson 1997:216),\n\nFortunately, the Meltzer translation was soon supplanted by a better one prepared by Elliott Mendelson for Martin Davis's anthology \"The Undecidable\"; but it too was not brought to Gödel's attention until almost the last minute, and the new translation was still not wholly to his liking ... when informed that there was not time enough to consider substituting another text, he declared that Mendelson's translation was 'on the whole very good' and agreed to its publication. [ Afterward he would regret his compliance, for the published volume was marred throughout by sloppy typography and numerous misprints.]\n\nThe translation by Elliott Mendelson appears in the collection \"The Undecidable\" (Davis 1965:5ff). This translation also received a harsh review by Bauer-Mengelberg (1966), who in addition to giving a detailed list of the typographical errors also described what he believed to be serious errors in the translation.\n\nA translation by Jean van Heijenoort appears in the collection \"From Frege to Gödel: A Source Book in Mathematical Logic\" (van Heijenoort 1967). A review by Alonzo Church (1972) described this as \"the most careful translation that has been made\" but also gave some specific criticisms of it. Dawson (1997:216) notes:\n\nThe translation Gödel favored was that by Jean van Heijenoort ... In the preface to the volume van Heijenoort noted that Gödel was one of four authors who had personally read and approved the translations of his works.\n\nThis approval process was laborious. Gödel introduced changes to his text of 1931, and negotiations between the men were \"protracted\": \"Privately van Heijenoort declared that Gödel was the most doggedly fastidious individual he had ever known.\" Between them they \"exchanged a total of seventy letters and met twice in Gödel's office in order to resolve questions concerning subtleties in the meanings and usage of German and English words.\" (Dawson 1997:216-217).\n\nAlthough not a translation of the original paper, a very useful 4th version exists that \"cover[s] ground quite similar to that covered by Godel's original 1931 paper on undecidability\" (Davis 1952:39), as well as Gödel's own extensions of and commentary on the topic. This appears as \"On Undecidable Propositions of Formal Mathematical Systems\" (Davis 1965:39ff) and represents the lectures as transcribed by Stephen Kleene and J. Barkley Rosser while Gödel delivered them at the Institute for Advanced Study in Princeton N.J. in 1934. Two pages of errata and additional corrections by Gödel were added by Davis to this version. This version is also notable because in it Gödel first describes the Herbrand suggestion that gave rise to the (general, i.e. Herbrand-Gödel) form of recursion.\n\n\n"}
{"id": "56103252", "url": "https://en.wikipedia.org/wiki?curid=56103252", "title": "Padmakar–Ivan index", "text": "Padmakar–Ivan index\n\nIn chemical graph theory, the Padmakar–Ivan (PI) index is a topological index of a molecule, used in biochemistry. The Padmakar–Ivan index is a generalization introduced by Padmakar V. Khadikar and Ivan Gutman of the concept of the Wiener index, introduced by Harry Wiener. The Padmakar–Ivan index of a graph \"G\" is the sum over all edges \"uv\" of \"G\" of number of edges which are not equidistant from \"u\" and \"v\".\nLet \"G\" be a graph and \"e\" = \"uv\" an edge of \"G\". Here formula_1 denotes the number of edges lying closer to the vertex \"u\" than the vertex \"v\", and formula_2 is the number of edges lying closer to the vertex \"v\" than the vertex \"u\". The Padmakar–Ivan index of a graph \"G\" is defined as\n\nThe PI index is very important in the study of quantitative structure–activity relationship for the classification models used in the chemical, biological sciences, engineering, and nanotechnology.\n\nThe PI index of Dendrimer Nanostar of the following figure can be calculated by\n"}
{"id": "5937299", "url": "https://en.wikipedia.org/wiki?curid=5937299", "title": "Pendulum (mathematics)", "text": "Pendulum (mathematics)\n\nThe mathematics of pendulums are in general quite complicated. Simplifying assumptions can be made, which in the case of a simple pendulum allow the equations of motion to be solved analytically for small-angle oscillations.\n\nA so-called \"simple pendulum\" is an idealization of a \"real pendulum\" but in an isolated system using the following assumptions:\n\nThe differential equation which represents the motion of a simple pendulum is\n\nwhere is acceleration due to gravity, is the length of the pendulum, and is the angular displacement.\n\nThe differential equation given above is not easily solved, and there is no solution that can be written in terms of elementary functions. However adding a restriction to the size of the oscillation's amplitude gives a form whose solution can be easily obtained. If it is assumed that the angle is much less than 1 radian (often cited as less than 0.1 radians, about 6°), or\n\nthen substituting for into using the small-angle approximation,\n\nyields the equation for a harmonic oscillator,\n\nThe error due to the approximation is of order (from the Maclaurin series for ).\n\nGiven the initial conditions and , the solution becomes\n\nThe motion is simple harmonic motion where is the amplitude of the oscillation (that is, the maximum angle between the rod of the pendulum and the vertical). The period of the motion, the time for a complete oscillation (outward and return) is\n\nwhich is known as Christiaan Huygens's law for the period. Note that under the small-angle approximation, the period is independent of the amplitude ; this is the property of isochronism that Galileo discovered.\n\nIf SI units are used (i.e. measure in metres and seconds), and assuming the measurement is taking place on the Earth's surface, then , and (0.994 is the approximation to 3 decimal places).\n\nTherefore, a relatively reasonable approximation for the length and period are,\n\nwhere is the number of seconds between \"two\" beats (one beat for each side of the swing), and is measured in metres.\n\nFor amplitudes beyond the small angle approximation, one can compute the exact period by first inverting the equation for the angular velocity obtained from the energy method (),\n\nand then integrating over one complete cycle,\n\nor twice the half-cycle\n\nor four times the quarter-cycle\n\nwhich leads to\n\nNote that this integral diverges as approaches the vertical\n\nso that a pendulum with just the right energy to go vertical will never actually get there. (Conversely, a pendulum close to its maximum can take an arbitrarily long time to fall down.)\n\nThis integral can be rewritten in terms of elliptic integrals as\n\nwhere is the incomplete elliptic integral of the first kind defined by\n\nOr more concisely by the substitution\nexpressing in terms of ,\nHere is the complete elliptic integral of the first kind defined by\n\nFor comparison of the approximation to the full solution, consider the period of a pendulum of length 1 m on Earth ( = ) at initial angle 10 degrees is\nThe linear approximation gives\n\nThe difference between the two values, less than 0.2%, is much less than that caused by the variation of with geographical location.\n\nFrom here there are many ways to proceed to calculate the elliptic integral.\n\nGiven and the Legendre polynomial solution for the elliptic integral:\n\nwhere denotes the double factorial, an exact solution to the period of a pendulum is:\n\nFigure 4 shows the relative errors using the power series. is the linear approximation, and to include respectively the terms up to the 2nd to the 10th powers.\n\nAnother formulation of the above solution can be found if the following Maclaurin series:\nis used in the Legendre polynomial solution above.\nThe resulting power series is:\n\nmore fractions available in .\n\nGiven and the arithmetic–geometric mean solution of the elliptic integral:\n\nwhere is the arithmetic-geometric mean of and .\n\nThis yields an alternative and faster-converging formula for the period:\n\nThe first iteration of this algorithm gives\n\nThis approximation has the relative error of less than 1% for angles up to 96.11 degrees. Since formula_27 the expression can be written more concisely as\n\nThe second order expansion of formula_29 reduces to formula_30\n\nA second iteration of this algorithm gives\n\nThis second approximation has a relative error of less than 1% for angles up to 163.10 degrees.\n\nThe Fourier series expansion of formula_32 is given by\nwhere formula_34 is the elliptic nome, formula_35, and formula_36 the angular frequency.\nIf one defines\nformula_34 can be approximated using the expansion\n(see ). Note that for formula_40 we have formula_41, thus the approximation is applicable even for large amplitudes.\n\nThe animations below depict the motion of a simple (frictionless) pendulum with increasing amounts of initial displacement of the bob, or equivalently increasing initial velocity. The small graph above each pendulum is the corresponding phase plane diagram; the horizontal axis is displacement and the vertical axis is velocity. With a large enough initial velocity the pendulum does not oscillate back and forth but rotates completely around the pivot.\n\nA compound pendulum (or physical pendulum) is one where the rod is not massless, and may have extended size; that is, an arbitrarily shaped rigid body swinging by a pivot. In this case the pendulum's period depends on its moment of inertia around the pivot point.\n\nThe equation of torque gives:\n\nwhere:\n\nThe torque is generated by gravity so:\n\nwhere:\n\nHence, under the small-angle approximation ,\n\nwhere is the moment of inertia of the body about its center of mass.\n\nThe expression for is of the same form as the conventional simple pendulum and gives a period of\n\nAnd a frequency of\n\nIf the initial angle is taken into consideration (for large amplitudes), then the expression for formula_47 becomes: \n\nand gives a period of: \n\nwhere is the maximum angle of oscillation (with respect to the vertical) and is the complete elliptic integral of the first kind.\n\nThe Jacobian elliptic function that expresses the position of a pendulum as a function of time is a doubly periodic function with a real period and an imaginary period. The real period is of course the time it takes the pendulum to go through one full cycle. Paul Appell pointed out a physical interpretation of the imaginary period: if is the maximum angle of one pendulum and is the maximum angle of another, then the real period of each is the magnitude of the imaginary period of the other. This interpretation, involving dual forces in opposite directions, might be further clarified and generalized to other classical problems in mechanics with dual solutions.\n\nCoupled pendulums can affect each other's motion, either through a direction connection (such as a spring connecting the bobs) or through motions in a supporting structure (such as a tabletop). The equations of motion for two identical simple pendulums coupled by a spring connecting the bobs can be obtained using Lagrangian Mechanics.\n\nThe kinetic energy of the system is:\n\nwhere formula_51 is the mass of the bobs, formula_52 is the length of the strings, and formula_53, formula_54 are the angular displacements of the two bobs from equilibrium.\n\nThe potential energy of the system is:\n\nwhere formula_56 is the gravitational acceleration, and formula_57 is the spring constant. The displacement formula_58 of the spring from its equilibrium position assumes the small angle approximation.\n\nThe Lagrangian is then\n\nwhich leads to the following set of coupled differential equations:\n\nAdding and subtracting these two equations in turn, and applying the small angle approximation, gives two harmonic oscillator equations in the variables formula_61 and formula_62:\n\nwith the corresponding solutions\n\nwhere\n\nand formula_66, formula_67, formula_47, formula_69 are constants of integration.\n\nExpressing the solutions in terms of formula_53 and formula_54 alone:\n\nIf the bobs are not given an initial push, then the condition formula_73 requires formula_74, which gives (after some rearranging):\n\n\n\n"}
{"id": "48416288", "url": "https://en.wikipedia.org/wiki?curid=48416288", "title": "Per-user unitary rate control", "text": "Per-user unitary rate control\n\nPer-user unitary rate control (PURC) is a multi-user MIMO scheme. PURC uses both transmission pre-coding and multi-user scheduling. By doing that, the network capacity is further enhanced than the capacity of the single-user MIMO scheme.\n\n\nRecently, PURC has been adopted in the IEEE 802.16m system description documentation (SDD) and the concept of this scheme was included in 3GPP LTE standard.\n\nPer-user unitary rate control (PURC) is a practical multi-user MIMO solution. PURC allows a base station to transmit different data streams to multiple users simultaneously. The base station selects target users from candidate users based on the information fed by users. Transmission data are multiplied by a pre-coding matrix selected from the set of predefined matrices before transmission. The selection of a pre-coding matrix is determined based on the information provided by users. The selection of both target users and a pre-coding matrix according to the information provided by mobiles enables the utilization of multi-user diversity and data multiplexing at the same time. Moreover. Using predefined precoding matrices reduces feedback overhead from users to the base station. Pre-coding matrices used in this scheme is unitary. The use of unitary pre-coding matrices facilitates the estimation of interference from other users' data to the unintended user.\n\nThe operation of PURC is mathematically described for the transmitter and receiver sides, respectively.\n\nIt is assumed that the base station employs formula_1 transmission antennas. The formula_2 transmission signal vector is given by\nwhere formula_4 is the formula_2 linear precoding vector. PURC generates formula_4 based on the received finite channel status information, which is delivered to the base station from the user equipment (UE) through uplink feedback signaling. The feedback signal consists of index in a look-up table of a precoding codebook.\n\nEvery receiver has a receive antenna array with formula_7 elements. The receive signal vector at user formula_8 is modeled as follows:\nwhere formula_10 and formula_11 are the formula_12 received symbol and noise, respectively, and formula_13 is the formula_14 matrix with the channel coefficients.\n\nThe figure illustrates the throughput advantage of PURC over the conventional single-user and no scheduling scheme, assuming that the codebook size is one, i.e., formula_15. For larger codebook sizes the performance can be better than the performance of the unit-size codebook. Because of codebook-based multi-user scheduling, PURC outperforms the conventional single-user and no scheduling scheme when the number of users is larger than one. Note that the performance plotted in the figure for the two systems were obtained assuming linear receiver.\n\n\n\n\n[1] Borko Furht, Syed A. Ahson, Long Term Evolution: 3GPP LTE Radio and Cellular Technology\n\n[2] George V. Tsoulos, MIMO system technology for wireless communications\n\n[3] Radio Network Planning and Optimisation for UMTS by Jaana Laiho, Achim Wacker, and Tomá Novosad (Hardcover – Feb 10, 2006)\n\n[4] MIMO System Technology for Wireless Communications (Electrical Engineering and Applied Signal Processing) by George Tsoulos (Hardcover – Mar 28, 2006)\n\n[5] Marcos Katz,Frank H. P. Fitzek, WiMAX Evolution: Emerging Technologies and Applications\n\n[6] Broadband wireless access and local networks: mobile WiMax and WiFi (공)저: Byeong Gi Lee,Sunghyun Choi\n\n[1] Jianhong Zheng, Zongzhi Huang, Jianrong Wei, \"\"A Novel Multi-User MIMO Scheme in OFDM Systems,\" Circuits, Communications and Systems, Pacific-Asia Conference on, pp. 43–46, 2009 Pacific-Asia Conference on Circuits, Communications and Systems, 2009.\n\n[2] D. J. Love, R. W. Heath, Jr., V. K. N. Lau, D. Gesbert, B. D. Rao, and M. Andrews, \"An overview of limited feedback in wireless communication systems ,\" IEEE J. Select. Areas Commun., vol. 26, no. 8, pp. 1341–1365, Oct. 2008\n\n[3] M. Cho, Y. Kim, D. Hong, \"Feedback Reduction of Channel Quality Information for Multiuser MIMO Systems\", pp. 798~803 (6 pages), UCI : G300-j12264717.v31n8Ap798\n\n[4] Tae Hyun Kim, Robert W. Heath Jr., Sunghyun Choi, \"Multiuser MIMO Downlink with Limited Feedback using Transmit-Beam Matching,\", IEEE ICC2008\n\n[5] M. Trivellato, F. Boccardi, and H. Huang, \"Zero-Forcing vs Unitary Beamforming in Multiuser MIMO Systems with Limited Feedback,\" in Proc. PIMRC 2008, Cannes, France, Sept. 2008.\n\n[6] K. Huang, J. G. Andrews, and Jr R. W. Heath, \"Performance of Orthogonal Beamforming for SDMA with Limited Feedabck,\" accepted for publication in IEEE Trans. Vehic. Tech., May 2008.\n\n[7] N. Ravindran, N. Jindal, and H. Huang, Beamforming with Finite Rate Feedback for LOS MIMO Downlink Channels, IEEE Globecom, Nov. 2007.\n\n[8] Kaibin Huang, MIMO Networking with Imperfect Channel State Information, Ph.D. Thesis, UTexas, Supervisor: Jafferey G. Andrews, May 2008\n\n[9] Love, D.J., Heath, R.W.' Lau, V.K.N. Gesbert, D., Rao, B.D., Andrews, M., An overview of limited feedback in wireless communication systems, JSAC 2008\n\n[10] W Choi, A Forenza, JG Andrews, RW Heath Jr, \"Opportunistic space-division multiple access with beam selection\", IEEE Transactions on Communications, 2007\n\n[11] Seong-Bo Shim, Sung Kyo Kang, and Yun Hee Kim°, Capacity Comparison of Multiuser MIMO-OFDM Transmission with Limited Feedback\n\n[12] N. Ravindran, and N. Jindal, Multi-User Diversity vs. Accurate Channel State Information in MIMO Downlink Channels, Submitted to IEEE Trans. Communications, July 2009.], (Matlab Code)\n\n[13] Bartosz Mielczarek and Witold A. Krzymien, \"Vector Quantization of Channel Information in Linear Multi-User MIMO Systems\", 2006 IEEE 9th International Symposium on Spread Spectrum Techniques and Applications\n\n[14] Mohammad Ali Khojastepour, Xiaodong Wang, and Mohammad Madihian, \"Quantized Linear MIMO Precoding for Multiuser Downlink Systems\", CISS 2008\n\n[15] Peilu Ding, Member, IEEE, David J. Love, Member, IEEE, and Michael D. Zoltowski, Fellow, IEEE, \"Multiple Antenna Broadcast Channels With Shape Feedback and Limited Feedback\", IEEE Transactions on Signal Processing, 2007, http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04244670\n\n[16] Peilu Ding, Love, D.J., Zoltowski, M.D.,WLC26-2: Limited Feedback in Multiple Antenna Broadcast Channels, Global Telecommunications Conference, 2006. GLOBECOM '06. IEEE\n\n[17] H Chae, Y Kim, JH Yang, BC Ihm, DK Kim, \"Multimode Random Beamforming for Multiuser Downlink MIMO System with Limited Feedback\", Proc. IEEE Veh. Technol. Conf, 2008 – mnet.skku.ac.kr \n\n[18] Caihua Zhang, Wei Xu, Ming Chen, Multi-mode transmission for MIMO broadcast channels with PU2RC, Proceedings of the 15th Asia-Pacific conference on Communications, 277–280, 2009\n\n\nThe following documents are standard contributions about both PU2RC related and general MU-MIMO issues.\n\n\n\n\n\n\n"}
{"id": "1057409", "url": "https://en.wikipedia.org/wiki?curid=1057409", "title": "Pierre Alphonse Laurent", "text": "Pierre Alphonse Laurent\n\nPierre Alphonse Laurent (18 July 1813 – 2 September 1854) was a French mathematician and Military Officer best known as the discoverer of the Laurent series, an expansion of a function into an infinite power series, generalizing the Taylor series expansion.\n\nHe was born in Paris, France. Pierre Laurent entered the École Polytechnique in Paris in 1830, Laurent graduated from the École Polytechnique in 1832, being one of the best students in his year, and entered the engineering corps as second lieutenant. He then attended the École d'Application at Metz until he was sent to Algeria.\n\nLaurent returned to France from Algeria around 1840 and spent six years directing operations for the enlargement of the port of Le Havre on the English Channel coast. Rouen had been the main French port up to the nineteenth century but the hydraulic construction projects on which Laurent worked in Le Havre turned it into France's main seaport. It is clear that Laurent was a good engineer, putting his deep theoretical knowledge to good practical use.\n\nIt was while Laurent was working on the construction project at Le Havre that he began to write his first mathematical papers. He submitted a memoir for the Grand Prize of the Académie des Sciences of 1842. His result was contained in a memoir submitted for the Grand Prize of the Académie des Sciences in 1843, but his submission was after the due date, and the paper was not published and never considered for the prize. Laurent died at age 41 in Paris. His work was not published until after his death.\n\n"}
{"id": "12117968", "url": "https://en.wikipedia.org/wiki?curid=12117968", "title": "Principia Cybernetica", "text": "Principia Cybernetica\n\nPrincipia Cybernetica is an international cooperation of scientists in the field of cybernetics and systems science, especially known for their Principia Cybernetica Website. They have dedicated their organization to what they call \"a computer-supported evolutionary-systemic philosophy, in the context of the transdisciplinary academic fields of Systems Science and Cybernetics\".\n\nPrincipia Cybernetica was initiated in 1989 in the USA by Cliff Joslyn and Valentin Turchin, and a year later broadened to Europe with Francis Heylighen from Belgium joining their cooperation.\n\nMajor activities of the Principia Cybernetica Project are: \n\nThe organization is associated with: \n\nThe Principia Cybernetica Web, which went online in 1993, is one of the first complex webs in the world.\nIt is still viewed as one of the most important sites on cybernetics, systems theory, complexity, and related approaches.\n\nEspecially in the 1990s the Principia Cybernetica has organized a series of workshops and international symposia on cybernetic themes. On the 1st Principia Cybernetica Workshop in June 1991 in Brussels many cyberneticists attended like Harry Bronitz, Gordon Pask, J.L. Elohim, Robert Glueck, Ranulph Glanville, Annemie Van Kerkhoven, Don McNeil, Elan Moritz, Cliff Joslyn, A. Comhaire and Valentin Turchin.\n\n\n\n"}
{"id": "32475185", "url": "https://en.wikipedia.org/wiki?curid=32475185", "title": "SIGNAL (programming language)", "text": "SIGNAL (programming language)\n\nSIGNAL is a programming language based on synchronized data-flow (flows + synchronization): a process is a set of equations on elementary flows describing both data and control.\n\nThe SIGNAL formal model provides the capability to describe systems with several clocks (polychronous systems) as relational specifications. Relations are useful as partial specifications and as specifications of non-deterministic devices (for instance a non-deterministic bus) or external processes (for instance an unsafe car driver).\n\nUsing SIGNAL allows one to specify an application, to design an architecture, to refine detailed components down to RTOS or hardware description. The SIGNAL model supports a design methodology which goes from specification to implementation, from abstraction to concretization, from synchrony to asynchrony.\n\nSIGNAL has been mainly developed in INRIAEspresso team since the 1980s, at the same time as similar programming languages, Esterel and Lustre.\n\nThe SIGNAL language was first designed for signal processing applications in the beginning of the 1980s. It has been proposed to answer the demand of new domain-specific language for the design of signal processing applications, adopting a dataflow and block-diagram style with array and sliding window operators. P. Le Guernic, A. Benveniste, and T. Gautier have been in charge of the language definition. The first paper on SIGNAL was published in 1982, while the first complete description of SIGNAL appeared in the PhD thesis of T. Gautier. The symbolic representation of SIGNAL via z/3z (over [-1,0,1]) has been introduced in 1986. A full compiler of SIGNAL based on the clock calculus on hierarchy of Boolean clocks, was described by L. Besnard in his PhD thesis in 1992. The clock calculus has been improved later by T. Amagbegnon with the proposition of arborescent canonical forms.\n\nDuring the 1990s, the application domain of the SIGNAL language has been extended into general embedded and real-time systems. The relation-oriented specification style enabled the increasing construction of the systems, and also led to the design considering multi-clocked systems, compared to the original single-clock-based implementation of Esterel and Lustre. Moreover, the design and implementation of distributed embedded systems were also taken into account in SIGNAL. The corresponding research includes the optimization methods proposed by B. Chéron, the clustering models defined by B. Le Goff, the abstraction and separate compilation formalized by O. Maffeïs, and the implementation of distributed programs developed by P. Aubry.\n\nThe Polychrony toolset is an Open Source development environment for critical/embedded systems based on SIGNAL, a real-time polychronous data-flow language. It provides a unified model-driven environment to perform design exploration by using top-down and bottom-up design methodologies formally supported by design model transformations from specification to implementation and from synchrony to asynchrony. It can be included in heterogeneous design systems with various input formalisms and output languages.\n\nPolychrony is a set of tools composed of:\n\nThe SME (SIGNAL Meta under Eclipse) environment is a front-end of Polychrony in the Eclipse environment based on Model-Driven Engineering (MDE) technologies. It consists of a set of Eclipse plug-ins which rely on the Eclipse Modeling Framework (EMF). The environment is built around SME, a metamodel of the SIGNAL language extended with mode automata concepts.\n\nThe SME environment is composed of several plug-ins which correspond to:\n\n\n"}
{"id": "1567410", "url": "https://en.wikipedia.org/wiki?curid=1567410", "title": "Sigma-ideal", "text": "Sigma-ideal\n\nIn mathematics, particularly measure theory, a \"σ\"-ideal of a sigma-algebra (\"σ\", read \"sigma,\" means countable in this context) is a subset with certain desirable closure properties. It is a special type of ideal. Its most frequent application is perhaps in probability theory.\n\nLet (\"X\",Σ) be a measurable space (meaning Σ is a \"σ\"-algebra of subsets of \"X\"). A subset \"N\" of Σ is a \"σ\"-ideal if the following properties are satisfied:\n\n(i) Ø ∈ \"N\";\n\n(ii) When \"A\" ∈ \"N\" and \"B\" ∈ Σ , \"B\" ⊆ \"A\" ⇒ \"B\" ∈ \"N\";\n\n(iii) formula_1\n\nBriefly, a sigma-ideal must contain the empty set and contain subsets and countable unions of its elements. The concept of \"σ\"-ideal is dual to that of a countably complete (\"σ\"-) filter.\n\nIf a measure \"μ\" is given on (\"X\",Σ), the set of \"μ\"-negligible sets (\"S\" ∈ Σ such that \"μ\"(\"S\") = 0) is a \"σ\"-ideal.\n\nThe notion can be generalized to preorders (\"P\",≤,0) with a bottom element 0 as follows: \"I\" is a \"σ\"-ideal of \"P\" just when\n\n(i') 0 ∈ \"I\",\n\n(ii') \"x\" ≤ \"y\" & \"y\" ∈ \"I\" ⇒ \"x\" ∈ \"I\", and\n\n(iii') given a family \"x\" ∈ \"I\" (\"n\" ∈ N), there is \"y\" ∈ \"I\" such that \"x\" ≤ \"y\" for each \"n\"\n\nThus \"I\" contains the bottom element, is downward closed, and is closed under countable suprema (which must exist). It is natural in this context to ask that \"P\" itself have countable suprema.\n\nA \"σ\"-ideal of a set X is a σ-ideal of the power set of X. That is, when no σ-algebra is specified, then one simply takes the full power set of the underlying set. For example, the meager subsets of a topological space are those in the σ-ideal generated by the collection of closed subsets with empty interior.\n\n"}
{"id": "37852490", "url": "https://en.wikipedia.org/wiki?curid=37852490", "title": "Stoic logic", "text": "Stoic logic\n\nStoic logic is the system of propositional logic developed by the Stoic philosophers in ancient Greece. It was one of the two great systems of logic in the classical world. It was largely built and shaped by Chrysippus, the third head of the Stoic school in the 3rd-century BCE. Chrysippus's logic differed from Aristotle's term logic because it was based on the analysis of propositions rather than terms. The smallest unit in Stoic logic is an \"assertible\" (the Stoic equivalent of a proposition) which is the content of a statement such as \"it is day\". Assertibles have a truth-value such that at any moment of time they are either true or false. Compound assertibles can be built up from simple ones through the use of logical connectives. The resulting syllogistic was grounded on five basic indemonstrable arguments to which all other syllogisms were claimed to be reducible.\n\nTowards the end of antiquity Stoic logic was neglected in favour of Aristotle's logic, and as a result the Stoic writings on logic did not survive, and the only accounts of it were incomplete reports by other writers. Knowledge about Stoic logic as a system was lost until the 20th-century when logicians familiar with the modern propositional calculus reappraised the ancient accounts of it.\n\nStoicism is a school of philosophy which developed in the Hellenistic period around a generation after the time of Aristotle. The Stoics believed that the universe operated according to reason, \"i.e.\" by a God which is immersed in nature itself. Logic (\"logike\") was the part of philosophy which examined reason (\"logos\"). To achieve a happy life—a life worth living—requires logical thought. The Stoics held that an understanding of ethics was impossible without logic. In the words of Brad Inwood, the Stoics believed that:\n\nAristotle's term logic can be viewed as a logic of classification. It makes use of four logical terms \"all\", \"some\", \"is/are\", and \"is/are not\" and to that extent is fairly static. The Stoics needed a logic that examines choice and consequence. The Stoics therefore developed a logic of propositions which uses connectives such as \"if ... then\", \"either ... or\", and \"not both\". Such connectives are part of everyday reasoning. Socrates in the Dialogues of Plato often asks a fellow citizen \"if\" they believe a certain thing; when they agree, Socrates then proceeds to show how the consequences are logically false or absurd, inferring that the original belief must be wrong. Similar attempts at forensic reasoning must have been used in the law-courts, and they are a fundamental part of Greek mathematics. Aristotle himself was familiar with propositions, and his pupils Theophrastus and Eudemus had examined hypothetical syllogisms, but there was no attempt by the Peripatetic school to develop these ideas into a system of logic.\n\nThe Stoic tradition of logic originated in the 4th-century BCE in a different school of philosophy known as the Megarian school. It was two dialeticians of this school, Diodorus Cronus and his pupil Philo, who developed their own theories of modalities and of conditional propositions. The founder of Stoicism, Zeno of Citium, studied under the Megarians and he was said to have been a fellow pupil with Philo. However, the outstanding figure in the development of Stoic logic was Chrysippus of Soli (c. 279 – c. 206 BCE), the third head of the Stoic school. Chrysippus shaped much of Stoic logic as we know it creating a system of propositional logic. As a logician Chrysippus is sometimes said to rival Aristotle in stature. The logical writings by Chrysippus are, however, almost entirely lost, instead his system has to be reconstructed from the partial and incomplete accounts preserved in the works of later authors such as Sextus Empiricus, Diogenes Laërtius, and Galen.\n\nTo the Stoics, logic was a wide field of knowledge which included the study of language, grammar, rhetoric and epistemology. However, all of these fields were interrelated, and the Stoics developed their logic (or \"dialectic\") within the context of their theory of language and epistemology.\n\nThe Stoics held that any meaningful utterance will involve three items: the sounds uttered; the thing which is referred to or described by the utterance; and an incorporeal item—the \"sayable\" (\"lekton\")—that which is conveyed in the language. The \"lekton\" is not a statement but the content of a statement, and it corresponds to a complete utterance. A \"lekton\" can be something such as a question or a command, but Stoic logic operates on those \"lekta\" which are called \"assertibles\" (\"axiomata\"), described as a proposition which is either true or false and which affirms or denies. Examples of assertibles include \"it is night\", \"it is raining this afternoon\", and \"noone is walking.\" The assertibles are truth-bearers. They can never be true and false at the same time (law of noncontradiction) and they must be \"at least\" true or false (law of excluded middle). The Stoics catalogued these simple assertibles according to whether they are affirmative or negative, and whether they are definite or indefinite (or both). The assertibles are much like modern propositions, however their truth value can change depending on \"when\" they are asserted. Thus an assertible such as \"it is night\" will only be true when it is night and not when it is day.\n\nSimple assertibles can be connected to each other to form compound or non-simple assertibles. This is achieved through the use of logical connectives. Chrysippus seems to have been responsible for introducing the three main types of connectives: the conditional (if), conjunctive (and), and disjunctive (or). A typical conditional takes the form of \"if p then q\"; whereas a conjunction takes the form of \"both p and q\"; and a disjunction takes the form of \"either p or q\". The or they used is exclusive, unlike the inclusive or generally used in modern formal logic. These connectives are combined with the use of not for negation. Thus the conditional can take the following four forms:\n\nLater Stoics added more connectives: the pseudo-conditional took the form of \"since p then q\"; and the causal assertible took the form of \"because p then q\". There was also a comparative (or dissertive): \"more/less (likely) p than q\".\n\nAssertibles can also be distinguished by their modal properties—whether they are possible, impossible, necessary, or non-necessary. In this the Stoics were building on an earlier Megarian debate initiated by Diodorus Cronus. Diodorus had defined \"possibility\" in a way which seemed to adopt a form of fatalism. Diodorus defined \"possible\" as \"that which either is or will be true\". Thus there are no possibilities that are forever unrealised, whatever is possible is or one day will be true. His pupil Philo, rejecting this, defined \"possible\" as \"that which is capable of being true by the proposition's own nature\", thus a statement like \"this piece of wood can burn\" is \"possible\", even if it spent its entire existence on the bottom of the ocean. Chrysippus, on the other hand, was a causal determinist: he thought that true causes inevitably give rise to their effects and that all things arise in this way. But he was not a logical determinist or fatalist: he wanted to distinguish between possible and necessary truths. Thus he took a middle position between Diodorus and Philo, combining elements of both their modal systems. Chrysippus's set of Stoic modal definitions was as follows:\n\nIn Stoic logic, an argument form contains two (or more) premisses related to one another as cause and effect. A typical Stoic syllogism is:\n\nIt has a non-simple assertible for the first premiss (\"If it is day, it is light\") and a simple assertible for second premiss (\"It is day\"). The second premiss doesn't \"always\" have to be simple but it will have fewer components than the first.\n\nIn more formal terms this type of syllogism is:\n\nThus, like Aristotle's term logic, Stoic logic uses variables, but the values of the variables are propositions not terms. Chrysippus listed five basic argument forms, which he regarded as true beyond dispute. These five indemonstrable arguments are made up of conditional, disjunction, and negation conjunction connectives, and all other arguments are reducible to these five indemonstrable arguments.\n\nThere can be many variations of these five indemonstrable arguments. For example the assertibles in the premisses can be more complex, and the following syllogism is a valid example of the second indemonstrable (\"modus tollens\"):\n\nSimilarly one can incorporate negation into these arguments. A valid example of the fourth indemonstrable (\"modus tollendo ponens\" or disjunctive syllogism) is:\n\nwhich, incorporating the principle of double negation, is equivalent to:\n\nMany arguments are not in the form of the five indemonstrables, and the task is to show how they can be reduced to one of the five types. A simple example of Stoic reduction is reported by Sextus Empiricus:\n\nThis can be reduced to two separate indemonstrable arguments of the second and third type:\n\nThe Stoics stated that complex syllogisms could be reduced to the indemonstrables through the use of four ground rules or \"themata\". Of these four \"themata\", only two have survived. One, the so-called first \"thema\", was a rule of antilogism:\n\nThe other, the third \"thema\", was a cut rule by which chain syllogisms could be reduced to simple syllogisms. The importance of these rules is not altogether clear. In the 2nd-century BCE Antipater of Tarsus is said to have introduced a simpler method involving the use of fewer \"themata\", although few details survive concerning this. In any case, the \"themata\" cannot have been a necessary part of every analysis.\n\nNext to describing inferences which are valid, another subject which engaged the Stoics was the enumeration and refutation of false arguments, and in particular of paradoxes. Part of a Stoic's logical training was to prepare the philosopher for paradoxes and help find solutions. A false argument could be one with a false premiss or which is formally incorrect, however paradoxes represented a challenge to the basic logical notions of the Stoics such as truth or falsehood. One famous paradox, known as \"The Liar\", asked \"A man says he is lying; is what he says true or false?\"—if the man says something true then it seems he is lying, but if he is lying then he is not saying something true, and so on. Chrysippus is known to have written several books on this paradox, although it is not known what solution he offered for it. Another paradox known as the \"Sorites\" or \"Heap\" asked \"How many grains of wheat do you need before you get a heap?\" It was said to challenge the idea of true or false by offering up the possibility of vagueness. The response of Chrysippus however was: \"That doesn't harm me, for like a skilled driver I shall restrain my horses before I reach the edge ... In like manner I restrain myself in advance and stop replying to sophistical questions.\"\n\nTraining in logic included a mastery of logical puzzles, the study of paradoxes, and the dissection of arguments. However, it was not an end in itself, but rather its purpose was for the Stoics to cultivate their rational powers.\n\nStoic logic was thus a method of self-discovery. Its aim was to enable ethical reflection, permit secure and confident arguing, and lead the pupil to truth. The end result would be thought that is consistent, clear and precise, and which exposes confusion, murkiness and inconsistency. Diogenes Laërtius gives a list of dialectical virtues, which were probably invented by Chrysippus:\n\nFor around five hundred years Stoic logic was one of the two great systems of logic. The logic of Chrysippus was discussed alongside that of Aristotle, and it may well have been more prominent since Stoicism was the dominant philosophical school. From a modern perspective Aristotle's term logic and the Stoic logic of propositions appear complementary, but they were sometimes regarded as rival systems. In late antiquity the Stoic school fell into decline, and the last pagan philosophical school, the Neoplatonists, adopted Aristotle's logic for their own. Only elements of Stoic logic made their way into the logical writings of later commentators such as Boethius, transmitting confused parts of Stoic logic to the Middle Ages. Propositional logic was redeveloped by Peter Abelard in the 12th-century, but by the mid-15th-century the only logic which was being studied was a simplified version of Aristotle's. In the 18th-century Immanuel Kant could pronounce that \"since Aristotle ... logic has not been able to advance a single step, and is thus to all appearance a closed and complete body of doctrine.\" To 19th-century historians, who believed that Hellenistic philosophy represented a decline from that of Plato and Aristotle, Stoic logic could only be seen with contempt. Carl Prantl thought that Stoic logic was \"dullness, triviality, and scholastic quibbling\" and he welcomed the fact that the works of Chrysippus were no longer extant. Eduard Zeller remarked that \"the whole contribution of the Stoics to the field of logic consists in their having clothed the logic of the Peripatetics with a new terminology.\"\n\nModern logic begins in the middle of the 19th-century with the work of George Boole and Augustus de Morgan, but Stoic logic was only rediscovered in the 20th-century. The first person to reappraise their ideas was the Polish logician Jan Łukasiewicz from the 1920s onwards. He was followed by Benson Mates. Although strikingly modern, Stoic logic was not a formal system in the modern sense, since their motivation was to study their ethical psychology and their general theory of nature (Stoic physics). Stoic concepts often differ from modern ones, but nevertheless there are many close parallels between Stoic and 20th-century theories.\n\na. The minimum requirement for a conditional is that the consequent follows from the antecedent. The pseudo-conditional adds that the antecedent must be also be true. The causal assertible adds an asymmetry rule such that if p is the cause/reason for q, then q cannot be the cause/reason for p. \nb. \"Stoic modal logic is not a logic of modal propositions (e.g., propositions of the type 'It is possible that it is day' ...) ... instead, their modal theory was about non-modalized propositions like 'It is day', insofar as they are possible, necessary, and so forth.\" \nc. Most of these argument forms had already been discussed by Theophrastus, but: \"It is plain that even if Theophrastus discussed (1)–(5), he did not anticipate Chrysippus' achievement. ... his Aristotelian approach to the study and organization of argument-forms would have given his discussion of mixed hypothetical syllogisms an utterly unStoical aspect.\" \nd. These Latin names date from the Middle Ages. \ne. For a brief summary of these \"themata\" see Susanne Bobzien's \"Ancient Logic\" article for the Stanford Encyclopedia of Philosophy. For a detailed (and technical) analysis of the \"themata\", including a tentative reconstruction of the two lost ones, see \n"}
{"id": "30807", "url": "https://en.wikipedia.org/wiki?curid=30807", "title": "Tangent space", "text": "Tangent space\n\nIn mathematics, the tangent space of a manifold facilitates the generalization of vectors from affine spaces to general manifolds, since in the latter case one cannot simply subtract two points to obtain a vector that gives the displacement of the one point from the other.\n\nIn differential geometry, one can attach to every point formula_1 of a differentiable manifold a tangent space—a real vector space that intuitively contains the possible directions in which one can tangentially pass through formula_1. The elements of the tangent space at formula_1 are called the tangent vectors at formula_1. This is a generalization of the notion of a bound vector in a Euclidean space. The dimension of the tangent space at every point of a connected manifold is the same as that of the manifold itself.\n\nFor example, if the given manifold is a formula_5-sphere, then one can picture the tangent space at a point as the plane that touches the sphere at that point and is perpendicular to the sphere's radius through the point. More generally, if a given manifold is thought of as an embedded submanifold of Euclidean space, then one can picture a tangent space in this literal fashion. This was the traditional approach toward defining parallel transport and was used by Dirac. More strictly, this defines an affine tangent space, which is distinct from the space of tangent vectors described by modern terminology.\n\nIn algebraic geometry, in contrast, there is an intrinsic definition of the tangent space at a point of an algebraic variety formula_6 that gives a vector space with dimension at least that of formula_6 itself. The points formula_8 at which the dimension of the tangent space is exactly that of formula_6 are called non-singular points; the others are called singular points. For example, a curve that crosses itself does not have a unique tangent line at that point. The singular points of formula_6 are those where the ‘test to be a manifold’ fails. See Zariski tangent space.\n\nOnce the tangent spaces of a manifold have been introduced, one can define vector fields, which are abstractions of the velocity field of particles moving in space. A vector field attaches to every point of the manifold a vector from the tangent space at that point, in a smooth manner. Such a vector field serves to define a generalized ordinary differential equation on a manifold: A solution to such a differential equation is a differentiable curve on the manifold whose derivative at any point is equal to the tangent vector attached to that point by the vector field.\n\nAll the tangent spaces of a manifold may be ‘glued together’ to form a new differentiable manifold with twice the dimension of the original manifold, called the tangent bundle of the manifold.\n\nThe informal description above relies on a manifold's ability to be embedded into an ambient vector space formula_11 so that the tangent vectors can ‘stick out’ of the manifold into the ambient space. However, it is more convenient to define the notion of a tangent space based solely on the manifold itself.\n\nThere are various equivalent ways of defining the tangent spaces of a manifold. While the definition via the velocity of curves is intuitively the simplest, it is also the most cumbersome to work with. More elegant and abstract approaches are described below.\n\nIn the embedded-manifold picture, a tangent vector at a point formula_1 is thought of as the velocity of a curve passing through the point formula_1. We can therefore define a tangent vector as an equivalence class of curves passing through formula_1 while being tangent to each other at formula_1.\n\nSuppose that formula_16 is a formula_17 manifold (formula_18) and that formula_19. Pick a coordinate chart formula_20, where formula_21 is an open subset of formula_16 containing formula_1. Suppose further that two curves formula_24 with formula_25 are given such that both formula_26 are differentiable in the ordinary sense (we call these differentiable curves initialized at formula_1). Then formula_28 and formula_29 are said to be equivalent at formula_30 if and only if the derivatives of formula_31 and formula_32 at formula_30 coincide. This defines an equivalence relation on the set of all differentiable curves initialized at formula_1, and equivalence classes of such curves are known as \"tangent vectors\" of formula_16 at formula_1. The equivalence class of any such curve formula_37 is denoted by formula_38. The tangent space of formula_16 at formula_1, denoted by formula_41, is then defined as the set of all tangent vectors at formula_1; it does not depend on the choice of coordinate chart formula_20.\n\nTo define vector-space operations on formula_41, we use a chart formula_20 and define a map formula_46 by formula_47. This map turns out to be bijective and may be used to transfer the vector-space operations on formula_48 over to formula_41, thus turning the latter set into an formula_50-dimensional real vector space. Again, one needs to check that this construction does not depend on the particular chart formula_20 being used, and in fact it does not.\n\nSuppose now that formula_16 is a formula_53 manifold. A real-valued function formula_54 is said to belong to formula_55 if and only if for every coordinate chart formula_20, the map formula_57 is infinitely differentiable. Note that formula_55 is a real associative algebra with respect to the pointwise product and sum of functions and scalar multiplication.\n\nPick a point formula_19. A derivation at formula_1 is defined as a linear map formula_61 that satisfies the Leibniz identity\nwhich is modeled on the product rule of calculus.\n\nIf we define addition and scalar multiplication on the set of derivations at formula_1 by\nthen we obtain a real vector space, which we define as the tangent space formula_41 of formula_16 at formula_1.\n\nThe relation between derivations at a point formula_1 and tangent vectors at formula_1 is as follows: If formula_71 is a differentiable curve initialized at formula_1, then the corresponding derivation formula_73 at formula_1 is defined by formula_75 (where the derivative is taken in the ordinary sense because formula_76 is a function from formula_77 to formula_78).\n\nGeneralizations of this definition are possible, for instance, to complex manifolds and algebraic varieties. However, instead of examining derivations formula_79 from the full algebra of functions, one must instead work at the level of germs of functions. The reason for this is that the structure sheaf may not be fine for such structures. For example, let formula_80 be an algebraic variety with structure sheaf formula_81. Then the Zariski tangent space at a point formula_82 is the collection of all formula_83-derivations formula_84, where formula_83 is the ground field and formula_86 is the stalk of formula_81 at formula_8.\n\nAgain, we start with a formula_89 manifold formula_16 and a point formula_19. Consider the ideal formula_92 of formula_93 that consists of all smooth functions formula_94 vanishing at formula_1, i.e., formula_96. Then formula_92 and formula_98 are real vector spaces, and formula_99 may be defined as the dual space of the quotient space formula_100. This latter quotient space is also known as the cotangent space of formula_16 at formula_1.\n\nWhile this definition is the most abstract, it is also the one that is most easily transferable to other settings, for instance, to the varieties considered in algebraic geometry.\n\nIf formula_79 is a derivation at formula_1, then formula_105 for every formula_106, which means that formula_79 gives rise to a linear map formula_108. Conversely, if formula_109 is a linear map, then formula_110 defines a derivation at formula_1. This yields an equivalence between tangent spaces defined via derivations and tangent spaces defined via cotangent spaces.\n\nIf formula_16 is an open subset of formula_48, then formula_16 is a formula_53 manifold in a natural manner (take coordinate charts to be identity maps on open subsets of formula_48), and the tangent spaces are all naturally identified with formula_48.\n\nAnother way to think about tangent vectors is as directional derivatives. Given a vector formula_118 in formula_48, one defines the corresponding directional derivative at a point formula_120 by\nThis map is naturally a derivation at formula_1. Furthermore, every derivation at a point in formula_48 is of this form. Hence, there is a one-to-one correspondence between vectors (thought of as tangent vectors at a point) and derivations at a point.\n\nAs tangent vectors to a general manifold at a point can be defined as derivations at that point, it is natural to think of them as directional derivatives. Specifically, if formula_118 is a tangent vector to formula_16 at a point formula_1 (thought of as a derivation), then define the directional derivative formula_127 in the direction formula_118 by\nIf we think of formula_118 as the initial velocity of a differentiable curve formula_37 initialized at formula_1, i.e., formula_133, then instead, define formula_127 by\n\nFor a formula_53 manifold formula_16, if a chart formula_138 is given with formula_139, then one can define an ordered basis formula_140 of formula_141 by\nThen for every tangent vector formula_143, one has\nThis formula therefore expresses formula_118 as a linear combination of the basis tangent vectors formula_146 defined by the coordinate chart formula_20.\n\nEvery smooth (or differentiable) map formula_148 between smooth (or differentiable) manifolds induces natural linear maps between their corresponding tangent spaces:\nIf the tangent space is defined via differentiable curves, then this map is defined by\nIf, instead, the tangent space is defined via derivations, then this map is defined by\n\nThe linear map formula_152 is called variously the derivative, total derivative, differential, or pushforward of formula_153 at formula_1. It is frequently expressed using a variety of other notations:\nIn a sense, the derivative is the best linear approximation to formula_153 near formula_1. Note that when formula_158, then the map formula_159 coincides with the usual notion of the differential of the function formula_153. In local coordinates the derivative of formula_153 is given by the Jacobian.\n\nAn important result regarding the derivative map is the following:\nThis is a generalization of the inverse function theorem to maps between manifolds.\n\n\n\n"}
{"id": "33900144", "url": "https://en.wikipedia.org/wiki?curid=33900144", "title": "Tate duality", "text": "Tate duality\n\nIn mathematics, Tate duality or Poitou–Tate duality is a duality theorem for Galois cohomology groups of modules over the Galois group of an algebraic number field or local field, introduced by and .\n\nFor a \"p\"-adic local field \"k\", local Tate duality says there is a perfect pairing of finite groups\nwhere \"M\" is a finite group scheme and \"M\"′ its dual Hom(\"M\",\"G\").\nFor a local field of characteristic \"p\">0, the statement is similar, except that the pairing takes values in formula_2\n\nFor a global field \"k\", a similar statement holds true if μ is replaced by the \"S\"-idele class group, where \"S\" is a set of primes of \"k\". See .\n\nAmong other statements, Poitou–Tate duality establishes a perfect pairing between certain Shafarevich groups. Given a global field \"k\" and a set \"S\" of primes, and the maximal extension formula_3 which is unramified outside \"S\", the Shafarevich groups capture, broadly speaking, those elements in the cohomology of formula_4 which vanish in the Galois cohomology of the local fields pertaining to the primes in \"S\".\n\nAn extension to the case where the ring of \"S\"-integers formula_5 is replaced by a regular scheme of finite type over formula_6 was shown by .\n\n\n"}
{"id": "2654296", "url": "https://en.wikipedia.org/wiki?curid=2654296", "title": "Uniform boundedness", "text": "Uniform boundedness\n\nIn mathematics, a bounded function is a function for which there exists a lower bound and an upper bound, in other words, a constant that is larger than the absolute value of any value of this function. If we consider a family of bounded functions, this constant can vary across functions in the family. If it is possible to find one constant that bounds all functions, this family of functions is uniformly bounded.\n\nThe uniform boundedness principle in functional analysis provides sufficient conditions for uniform boundedness of a family of operators.\n\nLet \nbe a family of functions indexed by formula_2, where formula_3 is an arbitrary set and formula_4 is the set of real or complex numbers. We call formula_5 uniformly bounded if there exists a real number formula_6 such that\n\nIn general let formula_8 be a metric space with metric formula_9, then the set\nis called uniformly bounded if there exists an element formula_11 from formula_8 and a real number formula_6 such that\n\n"}
{"id": "44988566", "url": "https://en.wikipedia.org/wiki?curid=44988566", "title": "Valerie King", "text": "Valerie King\n\nValerie King is an American and Canadian computer scientist who works as a professor at the University of Victoria. Her research concerns the design and analysis of algorithms; her work has included results on maximum flow and dynamic graph algorithms, and played a role in the expected linear time MST algorithm of Karger et al.\n\nKing graduated from Princeton University in 1977. She earned a law degree (Juris Doctor) from the University of California, Berkeley in 1983, and became a member of the State Bar of California, but returned to Berkeley and earned a Ph.D. in computer science in 1988 under the supervision of Richard Karp with a dissertation concerning the Aanderaa–Karp–Rosenberg conjecture.\n\nShe became a Fellow of the Association for Computing Machinery in 2014.\n\n"}
{"id": "23926237", "url": "https://en.wikipedia.org/wiki?curid=23926237", "title": "Zsolt Baranyai", "text": "Zsolt Baranyai\n\nZsolt Baranyai (June 23, 1948 in Budapest – April 6, 1978) was a Hungarian mathematician, working in combinatorics. \n\nHe graduated from the Fazekas Highschool where he was a classmate of László Lovász, Miklós Laczkovich, and Lajos Pósa. Following this he learned mathematics at the Eötvös Loránd University between 1967 and 1972. Afterwards he was a lecturer at the Analysis Department of the same university. He got his Ph.D. in 1975 and obtained the Candidate degree of the Hungarian Academy of Sciences in 1978, posthumously. \n\nHe did research in combinatorics; Baranyai's theorem on the decompositions of complete hypergraphs solved a long-standing open problem. As well as being a mathematician, Baranyai was a professional musician, who played the recorder. He died in a car accident after a concert, while touring Hungary with the Bakfark Consort.\n\n"}
