{"id": "8286430", "url": "https://en.wikipedia.org/wiki?curid=8286430", "title": "Adaptive algorithm", "text": "Adaptive algorithm\n\nAn adaptive algorithm is an algorithm that changes its behavior at the time it is run, based on information available and on \"a priori\" defined reward mechanism (or criterion). Such information could be the story of recently received data, information on the available computational resources, or other run-time acquired (or \"a priori\" known) information related to the environment in which it operates.\n\nAmong the most used adaptive algorithms is the Widrow-Hoff’s least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. In adaptive filtering the LMS is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired and the actual signal).\n\nFor example, stable partition, using no additional memory is \"O\"(\"n\" lg \"n\") but given \"O\"(\"n\") memory, it can be \"O\"(\"n\") in time. As implemented by the C++ Standard Library, codice_1 is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory. Another example is adaptive sort, whose behavior changes upon the presortedness of its input.\n\nAn example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.\n\nIn machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.\n\nIn data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.\n\nIn signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called \"adaptive\" because the window length (the size of an audio \"chunk\") can change according to the nature of the sound being compressed, to try to achieve the best-sounding compression strategy.\n\n"}
{"id": "57181252", "url": "https://en.wikipedia.org/wiki?curid=57181252", "title": "Alice T. Schafer Prize", "text": "Alice T. Schafer Prize\n\nThe Alice T. Schafer Mathematics Prize is given annually to an undergraduate woman for excellence in mathematics by the Association for Women in Mathematics (AWM). The prize, which carries a monetary award, is named for former AWM president and founding member Alice T. Schafer; it was first awarded in 1990.\n\nThe recipients of the Alice T. Schafer Mathematics Prize are:\n\n"}
{"id": "32843220", "url": "https://en.wikipedia.org/wiki?curid=32843220", "title": "Applications of UML", "text": "Applications of UML\n\nUML (Unified Modeling Language) is a modeling language used by software developers. UML can be used to develop diagrams and provide users with ready-to-use, expressive modeling examples. Some UML tools generate program language code from UML. UML can be used for modeling a system independent of a platform language. UML is a graphical language for visualizing, specifying, constructing, and documenting information about software-intensive systems. UML gives a standard way to write a system model, covering conceptual ideas. With an understanding of modeling, the use and application of UML can make the software development process more efficient.\n\nUML has applied to various activities since the second half of the 1990s and been used with object-oriented development methods.\n\nUML has been used in following areas \nUML can also be used to model nonsoftware systems, such as workflow in the legal systems, medical electronics and patient healthcare systems, and the design of hardware.\n\nThe following lists of UML diagrams and functionality summaries enable understanding of UML applications in real-world examples.\n\nStructuring diagrams show a view of a system that shows the structure of the objects, including their classifiers, relationships, attributes and operations:\n\nBehaviour diagrams are used to illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. Some Behaviour diagrams are:\n\n\nInteraction diagrams are subset of behaviour diagrams and emphasize the flow of control and data among the things in the system being modelled:\n\nWeb applications of UML can be used to model user interfaces of web applications and make the purpose of the website clear.\nWeb applications are software-intensive systems and UML is among the efficient choice of languages for modeling them. Web software complexity of an application can be minimized using various UML tools. \n\nUML-based web engineering aims at offering a UML profile that matches the needs of web development better. The following are examples: \n\nSoftware in embedded systems design needs to be looked carefully for software specification and analysis. Unified Modeling Language and extension proposals in the realtime domain can be used for the development of new design flows. UML can be used for specification, design and implementation of modern embedded systems. UML can also be used for modelling the system from functional requirements through executable specifications and for that purpose it is important to be able to model the context for an embedded system – both environmental and user-driven. \n\nSome key concepts of UML related to embedded systems:\n\nA specific UML profile, called MARTE for Modeling and Analysis of Real-Time and Embedded systems, provides some extensions dedicated to the domain.\n\n\n\"Notes\"\n\n\"Citations\"\n"}
{"id": "16487670", "url": "https://en.wikipedia.org/wiki?curid=16487670", "title": "Aurelio Baldor", "text": "Aurelio Baldor\n\nAurelio Ángel Baldor de la Vega (October 22, 1906, Havana, Cuba – April 2, 1978, Miami) was a Cuban mathematician, educator and lawyer. Baldor is the author of a secondary school algebra textbook, titled \"Algebra\", used throughout the Spanish-speaking world and published for the first time in 1941.\n\nHe was the youngest child of Daniel and Gertrudis Baldor. He was the founder and director of the Baldor School in the exclusive Vedado section of Havana. In its heyday, the school had 3,500 students and used 23 buses to provide transportation to its students. In 1959, with the arrival of Fidel Castro's communist regime, Aurelio Baldor and his family began experiencing some problems. Raúl Castro had intended to arrest Baldor, but Camilo Cienfuegos—one of Fidel Castro's own top commanders—prevented the arrest, as he highly admired and respected Baldor for his accomplishments as an educator.\n\nAfter the death of Camilo Cienfuegos approximately one month later in an airplane which disappeared over the sea, Baldor and his family left Cuba and were exiled in Mexico for a short time, and then they migrated to New Orleans, Louisiana. Afterward, they moved on to New York (Brooklyn) and New Jersey, where Baldor continued teaching at Saint Peter's College in Jersey City. He also taught daily classes in mathematics at the now defunct Stevens Academy, in Hoboken, New Jersey.\n\nHe spent much time writing mathematical theorems and exercises. Once a tall and imposing man weighing 100 kg (220 lbs), Baldor slowly began losing weight as his health declined. He died from pulmonary emphysema in Miami, FL, on April 2, 1978. His seven children, grandchildren and great-grandchildren still reside in Miami. Other family include Francisco Baldor, Maria Cristina Baldor and Aurelio Baldor's second cousin Teresita Baldor.\n\nBaldor's algebra textbook \"Algebra\" (With Graphics and 6,523 exercises and answers) published by Compañía Cultural Editora y Distribuidora de Textos Americanos, S. A. continues being used to this day in secondary schools throughout Latin America.\n"}
{"id": "542527", "url": "https://en.wikipedia.org/wiki?curid=542527", "title": "Cryptologia", "text": "Cryptologia\n\nCryptologia is a journal in cryptography published quarterly since January 1977. Its remit is all aspects of cryptography, but there is a special emphasis on historical aspects of the subject. The founding editors were Brian J. Winkel, David Kahn, Louis Kruh, Cipher A. Deavours and Greg Mellen. The current Editor-in-Chief is Craig Bauer.\n\nThe journal was initially published at the Rose-Hulman Institute of Technology. In July 1995, it moved to the United States Military Academy.\nBeginning with the January 2006 issue (Volume 30, Number 1), \"Cryptologia\" has been published by Taylor & Francis.\n\n\n"}
{"id": "4878864", "url": "https://en.wikipedia.org/wiki?curid=4878864", "title": "DISCUS", "text": "DISCUS\n\nDISCUS, or distributed source coding using syndromes, is a method for distributed source coding. It is a compression algorithm used to compress correlated data sources. The method is designed to achieve the Slepian–Wolf bound by using channel codes.\n\nDISCUS was invented by researchers S. S. Pradhan and K. Ramachandran, and first published in their\npaper \"Distributed source coding using syndromes (DISCUS): design and construction\",\npublished in the \"IEEE Transactions on Information Theory\" in 2003.\n\nMany variations of DISCUS are presented in related literature. One such popular scheme is the \"Channel Code Partitioning\" scheme, which is an a-priori scheme, to reach the Slepian–Wolf bound. Many papers illustrate simulations and experiments on channel code partitioning using the turbo codes, Hamming codes and irregular repeat-accumulate codes.\n\n\n"}
{"id": "1122242", "url": "https://en.wikipedia.org/wiki?curid=1122242", "title": "Dedekind sum", "text": "Dedekind sum\n\nIn mathematics, Dedekind sums are certain sums of products of a sawtooth function, and are given by a function \"D\" of three integer variables. Dedekind introduced them to express the functional equation of the Dedekind eta function. They have subsequently been much studied in number theory, and have occurred in some problems of topology. Dedekind sums have a large number functional equations; this article lists only a small fraction of these.\n\nDedekind sums were introduced by Richard Dedekind in a commentary on fragment XXVIII of Bernhard Riemann's collected papers.\n\nDefine the sawtooth function formula_1 as \n\nWe then let\n\nbe defined by\n\nthe terms on the right being the Dedekind sums. For the case \"a\"=1, one often writes\n\nNote that \"D\" is symmetric in \"a\" and \"b\", and hence\n\nand that, by the oddness of (()),\n\nBy the periodicity of \"D\" in its first two arguments, the third argument being the length of the period for both,\n\nIf \"d\" is a positive integer, then\n\nThere is a proof for the last equality making use of\n\nFurthermore, \"az\" = 1 (mod \"c\") implies \"D\"(\"a\",\"b\";\"c\") = \"D\"(1,\"bz\";\"c\").\n\nIf \"b\" and \"c\" are coprime, we may write \"s\"(\"b\",\"c\") as\n\nwhere the sum extends over the \"c\"-th roots of unity other than 1, i.e. over all formula_7 such that formula_8 and formula_9.\n\nIf \"b\", \"c\" > 0 are coprime, then\n\nIf \"b\" and \"c\" are coprime positive integers then\n\nRewriting this as\n\nit follows that the number 6\"c\" \"s\"(\"b\",\"c\") is an integer.\n\nIf \"k\" = (3, \"c\") then\n\nand\n\nA relation that is prominent in the theory of the Dedekind eta function is the following. Let \"q\" = 3, 5, 7 or 13 and let \"n\" = 24/(\"q\" − 1). Then given integers \"a\", \"b\", \"c\", \"d\" with \"ad\" − \"bc\" = 1 (thus belonging to the modular group), with \"c\" chosen so that \"c\" = \"kq\" for some integer \"k\" > 0, define\n\nThen one has \"n\"δ is an even integer.\n\nHans Rademacher found the following generalization of the reciprocity law for Dedekind sums: If \"a\",\"b\", and \"c\" are pairwise coprime positive integers, then\n\n"}
{"id": "7750539", "url": "https://en.wikipedia.org/wiki?curid=7750539", "title": "Deductive closure", "text": "Deductive closure\n\nDeductive closure is a property of a set of objects (usually the objects in question are statements). A set of objects, O, is said to exhibit \"closure\" or to be \"closed\" under a given operation, R, provided that for every object, x, if x is a member of O and x is R-related to any object, y, then y is a member of O. In the context of statements, a deductive closure is the set of all the statements that can be deduced from a given set of statements.\n\nIn propositional logic, the set of all true propositions exhibits deductive closure: if set O is the set of true propositions, and operation R is logical consequence (“formula_1”), then provided that proposition p is a member of O and p is R-related to q (i.e., p formula_1 q), q is also a member of O.\n\nIn epistemology, many philosophers have and continue to debate whether particular subsets of propositions—especially ones ascribing knowledge or justification of a belief to a subject—are closed under deduction.\n"}
{"id": "1598868", "url": "https://en.wikipedia.org/wiki?curid=1598868", "title": "Digit sum", "text": "Digit sum\n\nIn mathematics, the digit sum of a given integer is the sum of all its digits (e.g. the digit sum of 84001 is calculated as 8+4+0+0+1 = 13). Digit sums are most often computed using the decimal representation of the given number, but they may be calculated in any other base. Different bases give different digit sums, with the digit sums for binary being on average smaller than those for any other base.\n\nThe digit sum of a number formula_1 in base formula_2 is given by\n\nLet formula_4 be the digit sum for radix formula_5 of all non-negative integers less than formula_6. For any formula_7 and for sufficiently large formula_6, formula_9.\n\nThe sum of the decimal digits of the integers 0, 1, 2, ... is given by in the On-Line Encyclopedia of Integer Sequences. use the generating function of this integer sequence (and of the analogous sequence for binary digit sums) to derive several rapidly converging series with rational and transcendental sums.\n\nThe concept of a decimal digit sum is closely related to, but not the same as, the digital root, which is the result of repeatedly applying the digit sum operation until the remaining value is only a single digit. The digital root of any non-zero integer will be a number in the range 1 to 9, whereas the digit sum can take any value. Digit sums and digital roots can be used for quick divisibility tests: a natural number is divisible by 3 or 9 if and only if its digit sum (or digital root) is divisible by 3 or 9, respectively. For divisibility by 9, this test is called the rule of nines and is the basis of the casting out nines technique for checking calculations.\n\nDigit sums are also a common ingredient in checksum algorithms to check the arithmetic operations of early computers. Earlier, in an era of hand calculation, suggested using sums of 50 digits taken from mathematical tables of logarithms as a form of random number generation; if one assumes that each digit is random, then by the central limit theorem, these digit sums will have a random distribution closely approximating a Gaussian distribution.\n\nThe digit sum of the binary representation of a number is known as its Hamming weight or population count; algorithms for performing this operation have been studied, and it has been included as a built-in operation in some computer architectures and some programming languages. These operations are used in computing applications including cryptography, coding theory, and computer chess.\n\nHarshad numbers are defined in terms of divisibility by their digit sums, and Smith numbers are defined by the equality of their digit sums with the digit sums of their prime factorizations.\n\n"}
{"id": "1461290", "url": "https://en.wikipedia.org/wiki?curid=1461290", "title": "Dirichlet conditions", "text": "Dirichlet conditions\n\nIn mathematics, the Dirichlet conditions are sufficient conditions for a real-valued, periodic function \"f\" to be equal to the sum of its Fourier series at each point where \"f\" is continuous. Moreover, the behavior of the Fourier series at points of discontinuity is determined as well (it is the midpoint of the values of the discontinuity). These conditions are named after Peter Gustav Lejeune Dirichlet.\n\nThe conditions are:\n\nWe state Dirichlet's theorem assuming \"f\" is a periodic function of period 2π with Fourier series expansion where\n\nThe analogous statement holds irrespective of what the period of \"f\" is, or which version of the Fourier expansion is chosen (see Fourier series).\n<br>\n\nA function satisfying Dirichlet's conditions must have right and left limits at each point of discontinuity, or else the function would need to oscillate at that point, violating the condition on maxima/minima. Note that at any point where \"f\" is continuous,\n\nThus Dirichlet's theorem says in particular that under the Dirichlet conditions the Fourier series for \"f\" converges and is equal to \"f\" wherever \"f\" is continuous.\n"}
{"id": "655893", "url": "https://en.wikipedia.org/wiki?curid=655893", "title": "Elwyn Berlekamp", "text": "Elwyn Berlekamp\n\nElwyn Ralph Berlekamp (born September 6, 1940) is an American mathematician. He is a professor emeritus of mathematics and EECS at the University of California, Berkeley. Berlekamp is known for his work in coding theory and combinatorial game theory.\n\nBerlekamp was born in Dover, Ohio. His family moved to Northern Kentucky, where Berlekamp graduated from Ft. Thomas Highlands high school in Ft. Thomas, Campbell county, Kentucky. While an undergraduate at the Massachusetts Institute of Technology (MIT), he was a Putnam Fellow in 1961. He completed his bachelor's and master's degrees in electrical engineering in 1962. Continuing his studies at MIT, he finished his Ph.D. in electrical engineering in 1964; his advisors were Robert G. Gallager, Peter Elias, Claude Shannon, and John Wozencraft. Berlekamp taught electrical engineering at the University of California, Berkeley from 1964 until 1966, when he became a mathematics researcher at Bell Labs. In 1971, Berlekamp returned to Berkeley as professor of mathematics and EECS, where he served as the advisor for over twenty doctoral students. He is now professor emeritus.\n\nHe is a member of the National Academy of Engineering (1977) and the National Academy of Sciences (1999). He was elected a Fellow of the American Academy of Arts and Sciences in 1996, and became a fellow of the American Mathematical Society in 2012. In 1991, he received the IEEE Richard W. Hamming Medal, and in 1993, the Claude E. Shannon Award. In 1998, he received a Golden Jubilee Award for Technological Innovation from the IEEE Information Theory Society. He is on the board of directors of Gathering 4 Gardner.\n\nBerlekamp is the inventor of an algorithm to factor polynomials, and is one of the inventors of the Welch–Berlekamp algorithm and the Berlekamp–Massey algorithms, which are used to implement Reed–Solomon error correction. In the mid-1980s, he was president of Cyclotomics, Inc., a corporation that developed error-correcting code technology. With John Horton Conway and Richard K. Guy, he co-authored \"Winning Ways for your Mathematical Plays\", leading to his recognition as one of the founders of combinatorial game theory. He has studied various games, including dots and boxes, Fox and Geese, and, especially, Go. With David Wolfe, Berlekamp co-authored the book \"Mathematical Go\", which describes methods for analyzing certain classes of Go endgames.\n\nOutside of mathematics and computer science, Berlekamp has also been active in money management. In 1986, he began information-theoretic studies of commodity and financial futures. In 1989, Berlekamp purchased the largest interest in a trading company named Axcom Trading Advisors. After the firm's futures trading algorithms were rewritten, Axcom's Medallion Fund had a return (in 1990) of 55%, net of all management fees and transaction costs. The fund has subsequently continued to realize annualized returns exceeding 30% under management by James Harris Simons and his Renaissance Technologies Corporation.\n\nBerlekamp and his wife Jennifer have two daughters and a son and live in Piedmont, California.\n\n\n"}
{"id": "1752401", "url": "https://en.wikipedia.org/wiki?curid=1752401", "title": "Ennio de Giorgi", "text": "Ennio de Giorgi\n\nEnnio De Giorgi (8 February 1928 – 25 October 1996) was an Italian mathematician, member of the House of Giorgi, who worked on partial differential equations and the foundations of mathematics.\n\nHe solved Bernstein's problem about minimal surfaces.\n\nHe solved the 19th Hilbert problem on the regularity of solutions of elliptic partial differential equations.\n\n\n\n\n\n\n\n"}
{"id": "150287", "url": "https://en.wikipedia.org/wiki?curid=150287", "title": "Fixed-point combinator", "text": "Fixed-point combinator\n\nIn computer science's combinatory logic, a fixed-point combinator (or fixpoint combinator) is a higher-order function \"fix\" that, for any function \"f\" that has an attractive fixed point, returns a fixed point \"x\" of that function. A fixed point of a function is a value that, when applied as the input of the function, returns the same value as its output.\n\nIn other words: \"fix\", when applied to an arbitrary function \"f\", yields the same result as \"f\" applied to the result of applying \"fix\" to \"f\". It is so named because, by setting formula_1, it represents a solution to the fixed point equation,\n\nThe fixed-point combinator \"fix\" therefore satisfies the equation:\n\nA \"fixed point\" of a function \"f\" is a value that does not change under the application of the function \"f\".\n\nFunctions that satisfy the equation for fix expand as,\n\nA particular implementation of fix is Curry's paradoxical combinator Y, represented in lambda calculus by\n\nIn functional programming, the Y combinator can be used to formally define recursive functions in a programming language that does not support recursion.\n\nThis combinator may be used in implementing Curry's paradox. The heart of Curry's paradox is that untyped lambda calculus is unsound as a deductive system, and the Y combinator demonstrates that by allowing an anonymous expression to represent zero, or even many values. This is inconsistent in mathematical logic.\n\nApplied to a function with one variable the Y combinator usually does not terminate. More interesting results are obtained by applying the Y combinator to functions of two or more variables. The second variable may be used as a counter, or index. The resulting function behaves like a \"while\" or a \"for\" loop in an imperative language.\n\nUsed in this way the Y combinator implements simple recursion. In the lambda calculus it is not possible to refer to the definition of a function in a function body. Recursion may only be achieved by passing in a function as a parameter. The Y combinator demonstrates this style of programming.\n\nThe Y combinator is an implementation of a fixed-point combinator in lambda calculus. Fixed-point combinators may also be easily defined in other functional and imperative languages. The implementation in lambda calculus is more difficult due to limitations in lambda calculus.\n\nThe fixed combinator may be used in a number of different areas,\n\n\nFixed point combinators may be applied to a range of different functions, but normally will not terminate unless there is an extra parameter. When the function to be fixed refers to its parameter, another call to the function is invoked, so the calculation never gets started. Instead, the extra parameter is used to trigger the start of the calculation. \n\nThe type of the fixed point is the return type of the function being fixed. This may be a real or a function or any other type.\n\nIn the untyped lambda calculus, the function to apply the fix point combinator to may be expressed using an encoding, like Church encoding. In this case particular lambda terms (which define functions) are considered as values. \"Running\" (beta reducing) the fixed point combinator on the encoding gives a lambda term for the result which may then be interpreted as fixed point value.\n\nAlternately a function may be considered as a lambda term defined purely in lambda calculus.\n\nThese different approaches affect how a mathematician and a programmer may regard a fixed point combinator. A lambda calculus mathematician may see the \"Y\" combinator applied to a function as being an expression satisfying the fixed point equation, and therefore a solution.\n\nIn contrast a person only wanting to apply a fixed point combinator to some general programming task may see it only as a means of implementing recursion.\n\nEvery expression has one value. This is true in general mathematics and it must be true in lambda calculus. This means that in lambda calculus, applying a fixed point combinator to a function gives you an expression whose value is the fixed point of the function.\n\nHowever this is a value in the lambda calculus domain, it may not correspond to any value in the domain of the function, so in a practical sense it is not necessarily a fixed point of the function, and only in the lambda calculus domain is it a fixed point of the equation.\n\nFor example, consider,\n\nDivision of Signed numbers may be implemented in the Church encoding, so \"f\" may be represented by a lambda term. This equation has no solution in the real numbers. But in the domain of the complex numbers \"i\" and \"-i\" are solutions. This demonstrates that there may be solutions to an equation in another domain. However the lambda term for the solution for the above equation is weirder than that. The lambda term formula_7 represents the state where x could be either \"i\" or \"-i\", as one value. The information distinguishing these two values has been lost, in the change of domain.\n\nFor the lambda calculus mathematician, this is a consequence of the definition of lambda calculus. For the programmer, it means that the beta reduction of the lambda term will loop forever, never reaching a normal form.\n\nThe fixed-point combinator may be defined in mathematics and then implemented in other languages. General mathematics defines a function based on its extensional properties. That is, two functions are equal if they perform the same mapping. Lambda calculus and programming languages regard function identity as an intensional property. A function's identity is based on its implementation.\n\nA lambda calculus function (or term) is an implementation of a mathematical function. In the lambda calculus there are a number of combinator (implementations) that satisfy the mathematical definition of a fixed-point combinator.\n\nCombinatory logic is a higher-order functions theory. A combinator is a \"closed\" lambda expression, meaning that it has no free variables. The combinators may be combined to direct values to their correct places in the expression without ever naming them as variables.\n\nUsually when applied to functions of one parameter, implementations of the fixed point combinator fail to terminate. Functions with extra parameters are more interesting.\n\nThe Y combinator is an example of what makes the Lambda calculus inconsistent. So it should be regarded with suspicion. However it is safe to consider the Y combinator when defined in mathematic logic only. The definition is,\n\nIt is easy to see how \"Y f\" may be applied to one variable. Applying it to two or more variables requires adding them to the equation,\n\nThis version of the equation must be shown consistent with the previous by the definition for equality of functions,\n\nThis definition allows the two equations for Y to be regarded as equivalent, provided that the domain of \"x\" is well defined. So if \"f\" has multiple parameters the \"Y f\" may still be regarded as a fixed point, with some restrictions.\n\nThe factorial function provides a good example of how the fixed point combinator may be applied to functions of two variables. The result demonstrates simple recursion, as would be implemented in a single loop, in an imperative language. The definition of numbers used is explained in Church encoding. The fixed point function is,\n\nThis gives \"Y F n\" as,\n\nSetting formula_15 gives,\n\nThis definition puts \"F\" in the role of the body of a loop to be iterated, and is equivalent to the mathematical definition of factorial:\n\nThe \"Y\" combinator, discovered by Haskell B. Curry, is defined as:\n\nBeta reduction of this gives,\n\nBy repeatedly applying this equality we get,\n\nNote that the equality above should be thought of as a sequence of multi-step β-reductions from left to right. The lambda term formula_20 may not, in general, β-reduce to the term formula_21. One can interpret the equality signs as β-equivalences instead of multi step β-reductions to allow for going in both directions.\n\nThis fixed-point combinator may be defined as \"y\" in,\n\nAn expression for y may be derived using rules from the definition of a let expression. Firstly using the rule,\n\ngives,\n\nAlso using,\n\ngives\n\nThen using the eta reduction rule,\n\ngives,\n\nCurry's Y combinator may be readily obtained from the definition of \"y\".\nStarting with,\n\nA lambda abstraction does not support reference to the variable name, in the applied expression, so \"x\" must be passed in as a parameter to \"x\". We can think of this as replacing \"x\" by \"x x\", but formally this is not correct. Instead defining \"y\" by formula_30 gives,\n\nThe let expression may be regarded as the definition of the function \"y\", where \"z\" is the parameter. Instantiation \"z\" as \"y\" in the call gives,\n\nAnd because the parameter \"z\" always passes the function \"y\".\n\nUsing the eta reduction rule,\n\ngives,\n\nA let expression may be expressed as a lambda abstraction using,\n\ngives,\n\nThis is possibly the simplest implementation of a fixed point combinator in lambda calculus. However one beta reduction gives the more symmetrical form of Curry's Y combinator.\n\nSee also translating between let and lambda expressions.\n\nIn untyped lambda calculus fixed-point combinators are not especially rare. In fact there are infinitely many of them. In 2005 Mayer Goldberg showed that the set of fixed-point combinators of untyped lambda calculus is recursively enumerable.\n\nThe \"Y\" combinator can be expressed in the SKI-calculus as\n\nThe simplest fixed point combinator in the SK-calculus, found by John Tromp, is\n\nalthough note that it is not in normal form, which is longer. This combinator corresponds to the lambda expression\n\nThe following fixed-point combinator is simpler than the Y combinator, and β-reduces into the Y combinator; it is sometimes cited as the Y combinator itself:\n\nAnother common fixed point combinator is the Turing fixed-point combinator (named after its discoverer, Alan Turing):\n\nIt also has a simple call-by-value form:\n\nThe analog for mutual recursion is a \"polyvariadic fix-point combinator\",\n\nThe \"Z\" combinator will work in strict languages (also called eager languages, where applicative evaluation order is applied). The \"Z\" combinator has the next argument defined explicitly, preventing the expansion of \"Z\" g in the right hand side of the definition:\n\nand in lambda calculus it is an eta-expansion of the \"Y\" combinator:\n\nIn untyped lambda calculus there are terms that have the same Böhm tree as a fixed-point combinator, that is they have the same infinite extension λx.x (x (x ... )). These are called \"non-standard fixed-point combinators\". Any fixed-point combinator is also a non-standard one, but not all non-standard fixed-point combinators are fixed-point combinators because some of them fail to satisfy the equation that defines the \"standard\" ones. These strange combinators are called \"strictly non-standard fixed-point combinators\"; an example is the following combinator;\nwhere,\nThe set of non-standard fixed-point combinators is not recursively enumerable.\n\nNote that the Y combinator is a particular implementation of a fixed point combinator in lambda calculus. Its structure is determined by the limitations of lambda calculus. It is not necessary or helpful to use this structure in implementing the fixed point combinator in other languages.\n\nSimple examples of fixed point combinators implemented in some programming paradigms are given below.\n\nFor examples of implementations of the fixed point combinators in various languages see,\n\nIn a language that supports lazy evaluation, like in Haskell, it is possible to define a fixed-point combinator using the defining equation of the fixed-point combinator which is conventionally named codice_1. Since Haskell has lazy datatypes, this combinator can also be used to define fixed points of data constructors (and not only to implement recursive functions). The definition is given here, followed by some usage examples.\n\nIn a strict functional language the argument to \"f\" is expanded beforehand, yielding an infinite call sequence,\n\nThis may be resolved by defining fix with an extra parameter.\n\nThis example is a slightly interpretive implementation of a fixed point combinator. A class is used to contain the \"fix\" function, called \"fixer\". The function to be fixed is contained in a class that inherits from fixer. The \"fix\" function accesses the function to be fixed as a virtual function. As for the strict functional definition, \"fix\" is explicitly given an extra parameter \"x\", which means that lazy evaluation is not needed.\ntemplate <typename R, typename D>\nclass fixer\npublic:\nprivate:\n\nclass fact : public fixer<long, long>\n};\n\nlong result = fact().fix(5);\n\nIn polymorphic lambda calculus (System F) a polymorphic fixed-point combinator has type;\nwhere \"a\" is a type variable. That is, \"fix\" takes a function, which maps a → a and uses it to return a value of type a.\n\nIn the simply typed lambda calculus extended with recursive types, fixed-point operators can be written, but the type of a \"useful\" fixed-point operator (one whose application always returns) may be restricted.\n\nIn the simply typed lambda calculus, the fixed-point combinator Y cannot be assigned a type because at some point it would deal with the self-application sub-term formula_51 by the application rule:\n\nwhere formula_53 has the infinite type formula_54. No fixed-point combinator can in fact be typed, in those systems any support for recursion must be explicitly added to the language.\n\nIn programming languages that support recursive types, it is possible to type the Y combinator by appropriately accounting for the recursion at the type level. The need to self-apply the variable x can be managed using a type (Rec a), which is defined so as to be isomorphic to (Rec a -> a).\n\nFor example, in the following Haskell code, we have codice_2 and codice_3 being the names of the two directions of the isomorphism, with types:\n\nwhich lets us write:\n\nOr equivalently in OCaml:\n\nThe function for which any input is a fixed point is called the identity function. Formally:\n\nOther functions have the special property that after being applied once, further applications don't have any effect. More formally: \n\nSuch functions are called idempotent (see also projection). An example of such a function is the function that returns \"0\" for all even integers, and \"1\" for all odd integers.\n\nFixed-point combinators do not necessarily exist in more restrictive models of computation. For instance, they do not exist in simply typed lambda calculus.\n\nThe Y combinator allows recursion to be defined as a set of rewrite rules, without requiring native recursion support in the language.\n\nThe recursive join in relational databases implements a fixed point, by recursively adding records to a set until no more may be added.\n\nIn programming languages that support anonymous functions, fixed-point combinators allow the definition and use of anonymous recursive functions, i.e. without having to bind such functions to identifiers. In this setting, the use of fixed-point combinators is sometimes called \"anonymous recursion\".\n\n\n\n"}
{"id": "19539938", "url": "https://en.wikipedia.org/wiki?curid=19539938", "title": "Fréchet distance", "text": "Fréchet distance\n\nIn mathematics, the Fréchet distance is a measure of similarity between curves that takes into account the location and ordering of the points along the curves. It is named after Maurice Fréchet.\n\nImagine a man traversing a finite curved path while walking his dog on a leash, with the dog traversing a separate one. Assume that the dog varies its speed to keep as much slack in her leash as possible: the Fréchet distance between the two curves is the length of the shortest leash sufficient for both to traverse their separate paths. Note that the definition is symmetric with respect to the two curves—the Frechet distance would be the same if the dog was walking its owner.\n\nLet formula_1 be a metric space. A curve formula_2 in formula_1 is a continuous map from the unit interval into formula_1, i.e., formula_5. A reparameterization formula_6 of formula_7 is a continuous, non-decreasing, surjection formula_8.\n\nLet formula_2 and formula_10 be two given curves in formula_1. Then, the Fréchet distance between formula_2 and formula_10 is defined as the infimum over all reparameterizations formula_6 and formula_15 of formula_7 of the maximum over all formula_17 of the distance in formula_1 between formula_19 and formula_20. In mathematical notation, the Fréchet distance formula_21 is\n\nformula_22\n\nwhere formula_23 is the distance function of formula_1.\n\nInformally, we can think of the parameter formula_25 as \"time\". Then, formula_19 is the position of the dog and formula_20 is the position of the dog's owner at time formula_25 (or vice versa). The length of the leash between them at time formula_25 is the distance between formula_19 and formula_20. Taking the infimum over all possible reparametrizations of formula_7 corresponds to choosing the walk along the given paths where the maximum leash length is minimized. The restriction that formula_6 and formula_15 be non-decreasing means that neither the dog nor its owner can backtrack.\n\nThe Fréchet metric takes into account the flow of the two curves because the pairs of points whose distance contributes to the Fréchet distance sweep continuously along their respective curves. This makes the Fréchet distance a better measure of similarity for curves than alternatives, such as the Hausdorff distance, for arbitrary point sets. It is possible for two curves to have small Hausdorff distance but large Fréchet distance.\n\nThe Fréchet distance and its variants find application in several problems, from morphing and handwriting recognition to protein structure alignment. Alt and Godau were the first to describe a polynomial-time algorithm to compute the Fréchet distance between two polygonal curves in Euclidean space, based on the principle of parametric search. The running time of their algorithm is formula_35 for two polygonal curves with \"m\" and \"n\" segments.\n\nAn important tool for calculating the Fréchet distance of two curves is the free-space diagram, which was introduced by Alt and Godau.\nThe free-space diagram between two curves for a given distance threshold ε is a two-dimensional region in the parameter space that consist of all point pairs on the two curves at distance at most ε:\n\nformula_36\n\nThe Fréchet distance formula_21 is at most ε if and only if the free-space diagram formula_38 contains a path from the lower left corner to the upper right corner, which is monotone both in the horizontal and in the vertical direction.\n\nThe weak Fréchet distance is a variant of the classical Fréchet distance without the requirement that the endpoints move monotonically along their respective curves — the dog and its owner are allowed to backtrack to keep the leash between them short. Alt and Godau describe a simpler algorithm to compute the weak Fréchet distance between polygonal curves, based on computing minimax paths in an associated grid graph.\n\nThe discrete Fréchet distance, also called the coupling distance, is an approximation of the Fréchet metric for polygonal curves, defined by Eiter and Mannila. The discrete Fréchet distance considers only positions of the leash where its endpoints are located at vertices of the two polygonal curves and never in the interior of an edge. This special structure allows the discrete Fréchet distance to be computed in polynomial time by an easy dynamic programming algorithm.\n\nWhen the two curves are embedded in a metric space other than Euclidean space, such as a polyhedral terrain or some Euclidean space with obstacles, the distance between two points on the curves is most naturally defined as the length of the shortest path between them. The leash is required to be a geodesic joining its endpoints. The resulting metric between curves is called the geodesic Fréchet distance. Cook and Wenk describe a polynomial-time algorithm to compute the geodesic Fréchet distance between two polygonal curves in a simple polygon.\n\nIf we further require that the leash must move continuously in the ambient metric space, then we obtain the notion of the homotopic Fréchet distance between two curves. The leash cannot switch discontinuously from one position to another — in particular, the leash cannot jump over obstacles, and can sweep over a mountain on a terrain only if it is long enough. The motion of the leash describes a homotopy between the two curves. Chambers \"et al.\" describe a polynomial-time algorithm to compute the homotopic Fréchet distance between polygonal curves in the Euclidean plane with obstacles.\n\nThe Fréchet distance between two concentric circles of radius formula_39 and formula_40 respectively is formula_41\nThe longest leash is required when the owner stands still and the dog travels to the opposite side of the circle (formula_42), and the shortest leash when both owner and dog walk at a constant angular velocity around the circle (formula_43).\n\n"}
{"id": "54795621", "url": "https://en.wikipedia.org/wiki?curid=54795621", "title": "Generalized foreground-background", "text": "Generalized foreground-background\n\nGeneralized Foreground-Background (FB), also known as Least Attained Service (LAS) is a scheduling policy. It consists in scheduling the process that has received the least service so far. Similarly to SRPT, the aim of FB is to improve the performance of a system, specifically mean response time. While SRPT is optimal it is more difficult to apply in practice as it requires accurate estimations of the service time of each request. In contrast, FB does not require estimations of service times, making it more practical but also less performing than SRPT.\n"}
{"id": "82361", "url": "https://en.wikipedia.org/wiki?curid=82361", "title": "Gram–Schmidt process", "text": "Gram–Schmidt process\n\nIn mathematics, particularly linear algebra and numerical analysis, the Gram–Schmidt process is a method for orthonormalising a set of vectors in an inner product space, most commonly the Euclidean space R equipped with the standard inner product. The Gram–Schmidt process takes a finite, linearly independent set \"S\" = {\"v\", ..., \"v\"} for and generates an orthogonal set that spans the same \"k\"-dimensional subspace of R as \"S\".\n\nThe method is named after Jørgen Pedersen Gram and Erhard Schmidt, but Pierre-Simon Laplace had been familiar with it before Gram and Schmidt. In the theory of Lie group decompositions it is generalized by the Iwasawa decomposition.\n\nThe application of the Gram–Schmidt process to the column vectors of a full column rank matrix yields the QR decomposition (it is decomposed into an orthogonal and a triangular matrix).\n\nWe define the projection operator by\n\nwhere formula_2 denotes the inner product of the vectors u and v: formula_3 for vectors on R or formula_4 for vectors on C. This operator projects the vector v orthogonally onto the line spanned by vector u. If u = 0, we define formula_5. i.e., the projection map formula_6 is the zero map, sending every vector to the zero vector.\n\nThe Gram–Schmidt process then works as follows:\n\nThe sequence u, ..., u is the required system of orthogonal vectors, and the normalized vectors e, ..., e form an ortho\"normal\" set. The calculation of the sequence u, ..., u is known as \"Gram–Schmidt orthogonalization\", while the calculation of the sequence e, ..., e is known as \"Gram–Schmidt orthonormalization\" as the vectors are normalized.\n\nTo check that these formulas yield an orthogonal sequence, first compute formula_8 by substituting the above formula for u: we get zero. Then use this to compute formula_9 again by substituting the formula for u: we get zero. The general proof proceeds by mathematical induction.\n\nGeometrically, this method proceeds as follows: to compute u, it projects v orthogonally onto the subspace \"U\" generated by u, ..., u, which is the same as the subspace generated by v, ..., v. The vector u is then defined to be the difference between v and this projection, guaranteed to be orthogonal to all of the vectors in the subspace \"U\".\n\nThe Gram–Schmidt process also applies to a linearly independent countably infinite sequence {v}. The result is an orthogonal (or orthonormal) sequence {u} such that for natural number \"n\":\nthe algebraic span of v, ..., v is the same as that of u, ..., u.\n\nIf the Gram–Schmidt process is applied to a linearly dependent sequence, it outputs the 0 vector on the \"i\"th step, assuming that v is a linear combination of . If an orthonormal basis is to be produced, then the algorithm should test for zero vectors in the output and discard them because no multiple of a zero vector can have a length of 1. The number of vectors output by the algorithm will then be the dimension of the space spanned by the original inputs.\n\nA variant of the Gram–Schmidt process using transfinite recursion applied to a (possibly uncountably) infinite sequence of vectors formula_10 yields a set of orthonormal vectors formula_11 with formula_12 such that for any formula_13, the completion of the span of formula_14 is the same as that of formula_15. In particular, when applied to a (algebraic) basis of a Hilbert space (or, more generally, a basis of any dense subspace), it yields a (functional-analytic) orthonormal basis. Note that in the general case often the strict inequality formula_16 holds, even if the starting set was linearly independent, and the span of formula_11 need not be a subspace of the span of formula_10 (rather, it's a subspace of its completion).\n\nConsider the following set of vectors in R (with the conventional inner product)\n\nNow, perform Gram–Schmidt, to obtain an orthogonal set of vectors:\n\nWe check that the vectors u and u are indeed orthogonal:\nnoting that if the dot product of two vectors is \"0\" then they are orthogonal.\n\nFor non-zero vectors, we can then normalize the vectors by dividing out their sizes as shown above:\n\nWhen this process is implemented on a computer, the vectors formula_25 are often not quite orthogonal, due to rounding errors. For the Gram–Schmidt process as described above (sometimes referred to as \"classical Gram–Schmidt\") this loss of orthogonality is particularly bad; therefore, it is said that the (classical) Gram–Schmidt process is numerically unstable.\n\nThe Gram–Schmidt process can be stabilized by a small modification; this version is sometimes referred to as modified Gram-Schmidt or MGS.\nThis approach gives the same result as the original formula in exact arithmetic and introduces smaller errors in finite-precision arithmetic.\nInstead of computing the vector u as\n\nit is computed as\n\nEach step finds a vector formula_28 orthogonal to formula_29. Thus formula_28 is also orthogonalized against any errors introduced in computation of formula_29.\n\nThis method is used in the previous animation, when the intermediate v' vector is used when orthogonalizing the blue vector v.\n\nThe following MATLAB algorithm implements the stabilized Gram–Schmidt orthonormalization for Euclidean Vectors. The vectors v, ..., v (columns of matrix V, so that V(:,j) is the jth vector) are replaced by orthonormal vectors (columns of U) which span the same subspace.\nThe cost of this algorithm is asymptotically O(\"nk\") floating point operations, where \"n\" is the dimensionality of the vectors .\n\nIf the rows {\"v\", ..., \"v\"} are written as a matrix formula_32, then applying Gaussian elimination to the augmented matrix formula_33 will produce the orthogonalized vectors in place of formula_32. For example, taking formula_35 as above, we have\n\nAnd reducing this to row echelon form produces\n\nThe normalized vectors are then\n\nas in the example above.\n\nThe result of the Gram–Schmidt process may be expressed in a non-recursive formula using determinants.\n\nwhere \"D\" =1 and, for \"j\" ≥ 1, \"D \" is the Gram determinant\n\nNote that the expression for u is a \"formal\" determinant, i.e. the matrix contains both scalars\nand vectors; the meaning of this expression is defined to be the result of a cofactor expansion along\nthe row of vectors.\n\nThe determinant formula for the Gram-Schmidt is computationally slower (exponentially slower) than the recursive algorithms described above;\nit is mainly of theoretical interest.\n\nOther orthogonalization algorithms use Householder transformations or Givens rotations. The algorithms using Householder transformations are more stable than the stabilized Gram–Schmidt process. On the other hand, the Gram–Schmidt process produces the formula_43th orthogonalized vector after the formula_43th iteration, while orthogonalization using Householder reflections produces all the vectors only at the end. This makes only the Gram–Schmidt process applicable for iterative methods like the Arnoldi iteration.\n\nYet another alternative is motivated by the use of Cholesky decomposition for inverting the matrix of the normal equations in linear least squares. Let formula_45 be a full column rank matrix, whose columns need to be orthogonalized. The matrix formula_46 is Hermitian and positive definite, so it can be written as formula_47 using the Cholesky decomposition. The lower triangular matrix formula_48 with strictly positive diagonal entries is invertible. Then columns of the matrix formula_49 are orthonormal and span the same subspace as the columns of the original matrix formula_45. The explicit use of the product formula_46 makes the algorithm unstable, especially if the product's condition number is large. Nevertheless, this algorithm is used in practice and implemented in some software packages because of its high efficiency and simplicity.\n\nIn quantum mechanics there are several orthogonalization schemes with characteristics better suited for certain applications than original Gram–Schmidt. Nevertheless, it remains a popular and effective algorithm for even the largest electronic structure calculations.\n\n\n"}
{"id": "31314683", "url": "https://en.wikipedia.org/wiki?curid=31314683", "title": "Granville number", "text": "Granville number\n\nIn mathematics, specifically number theory, Granville numbers are an extension of the perfect numbers.\n\nIn 1996, Andrew Granville proposed the following construction of the set formula_1:\n\nA Granville number is an element of formula_1 for which equality holds i.e. it is equal to the sum of its proper divisors that are also in formula_1. Granville numbers are also called formula_1-perfect numbers.\n\nThe elements of formula_1 can be -deficient, -perfect, or -abundant. In particular, 2-perfect numbers are a proper subset of formula_1.\n\nNumbers that fulfill the strict form of the inequality in the above definition are known as formula_1-deficient numbers. That is, the formula_1-deficient numbers are the natural numbers for that the sum of their divisors in formula_1 is strictly less than themselves:\n\nNumbers that fulfill equality in the above definition are known as formula_1-perfect numbers. That is, the formula_1-perfect numbers are the natural numbers that are equal the sum of their divisors in formula_1. The first few formula_1-perfect numbers are:\n\nEvery perfect number is also formula_1-perfect. However, there are numbers such as 24 which are formula_1-perfect but not perfect. The only known formula_1-perfect number with three distinct prime factors is 126 = 2 · 3 · 7 .\n\nNumbers that violate the inequality in the above definition are known as formula_1-abundant numbers. That is, the formula_1-abundant numbers are the natural numbers for which the sum of their divisors in formula_1 is strictly greater than themselves:\n\nThey belong to the complement of formula_1. The first few formula_1-abundant numbers are:\n\nEvery deficient number and every perfect number is in formula_1 because the restriction of the divisors sum to members of formula_1 either decreases the divisors sum or leaves it unchanged. The first natural number that is not in formula_1 is the smallest abundant number, which is 12. The next two abundant numbers, 18 and 20, are also not in formula_1. However, the fourth abundant number, 24, is in formula_1 because the sum of its proper divisors in formula_1 is:\n\nIn other words, 24 is abundant but not formula_1-abundant because 12 is not in formula_1. In fact, 24 is formula_1-perfect - it is the smallest number that is formula_1-perfect but not perfect.\n\nThe smallest odd abundant number that is in formula_1 is 2835, and the smallest pair of consecutive numbers that are not in formula_1 are 5984 and 5985.\n"}
{"id": "54414446", "url": "https://en.wikipedia.org/wiki?curid=54414446", "title": "Graph matching", "text": "Graph matching\n\nGraph matching is the problem of finding a similarity between graphs.\n\nGraphs are commonly used to encode structural information in many fields, including computer vision and pattern recognition, and graph matching is an important tool in these areas. In these areas it is commonly assumed that the comparison is between the \"data graph\" and the \"model graph\".\n\nThe case of exact graph matching is known as the graph isomorphism problem. The problem of exact matching of a graph to a part of another graph is called subgraph isomorphism problem. \n\nThe inexact graph matching refers to matching problems when exact matching is impossible, e.g., when the number of vertices in the two graphs are different. In this case it is required to find the best possible match. For example, in image recognition applications, the results of image segmentation in image processing typically produces data graphs with the numbers of vertices much larger than in the model graphs data expected to match against. In the case of attributed graphs, even if the numbers of vertices and edges are the same, the matching still may be only inexact. \n\nTwo categories of search methods are the ones based on identification of possible and impossible pairings of vertices between the two graphs and methods which formulate graph matching as an optimization problem. Graph edit distance is one of similarity measures suggested for graph matching. A class of algorithms if called error-tolerant graph matching.\n"}
{"id": "214729", "url": "https://en.wikipedia.org/wiki?curid=214729", "title": "Harmonic number", "text": "Harmonic number\n\nIn mathematics, the -th harmonic number is the sum of the reciprocals of the first natural numbers:\n\nHarmonic numbers are related to the harmonic mean in that the -th harmonic number is also times the reciprocal of the harmonic mean of the first positive integers.\n\nHarmonic numbers have been studied since antiquity and are important in various branches of number theory. They are sometimes loosely termed harmonic series, are closely related to the Riemann zeta function, and appear in the expressions of various special functions.\n\nThe harmonic numbers roughly approximate the natural logarithm function and thus the associated harmonic series grows without limit, albeit slowly. In 1737, Leonhard Euler used the divergence of the harmonic series to provide a new proof of the infinity of prime numbers. His work was extended into the complex plane by Bernhard Riemann in 1859, leading directly to the celebrated Riemann hypothesis about the distribution of prime numbers.\n\nWhen the value of a large quantity of items has a Zipf's law distribution, the total value of the most-valuable items is proportional to the -th harmonic number. This leads to a variety of surprising conclusions regarding the long tail and the theory of network value.\n\nBertrand's postulate entails that, except for the case , the harmonic numbers are never integers.\n\nBy definition, the harmonic numbers satisfy the recurrence relation\n\nThe harmonic numbers are connected to the Stirling numbers of the first kind:\n\nThe functions\nsatisfy the property\nIn particular\nis an integral of the logarithmic function.\n\nThe harmonic numbers satisfy the series identity\n\nThere are several infinite summations involving harmonic numbers and powers of π:\n\nAn integral representation given by Euler is\n\nThe equality above is straightforward by the simple algebraic identity\n\nUsing the substitution \"x\" = 1−\"u\", another expression for \"H\" is\nA closed form expression for \"H\" is\n\nwhere \n\nThe \"n\"th harmonic number is about as large as the natural logarithm of \"n\". The reason is that the sum is approximated by the integral\n\nwhose value is ln(\"n\").\n\nThe values of the sequence \"H\" - ln(\"n\") decrease monotonically towards the limit\n\nwhere γ ≈ 0.5772156649 is the Euler–Mascheroni constant. The corresponding asymptotic expansion as is\n\nwhere formula_20 are the Bernoulli numbers.\n\nA generating function for the harmonic numbers is\n\nwhere ln(\"z\") is the natural logarithm. An exponential generating function is\n\nwhere Ein(\"z\") is the entire exponential integral. Note that\n\nwhere Γ(0, \"z\") is the incomplete gamma function.\n\nThe harmonic numbers have several interesting arithmetic properties. It is well-known that formula_24 is an integer if and only if formula_25, a result often attributed to Taeisinger. Indeed, using 2-adic valuation, it is not difficult to prove that for formula_26 the numerator of formula_24 is an odd number while the denominator of formula_24 is an even number. More precisely,\n\nwith some odd integers formula_30 and formula_31.\n\nAs a consequence of Wolstenholme's theorem, for any prime number formula_32 the numerator of formula_33is divisible by formula_34. Furthermore, Eisenstein proved that for all odd prime number formula_35 it holds formula_36where formula_37 is a Fermat quotient, with the consequence that formula_35 divides the numerator of formula_39 if and only if formula_35 is a Wieferich prime.\n\nIn 1991, Eswarathasan and Levine defined formula_41 as the set of all positive integers formula_42 such that the numerator of formula_24 is divisible by a prime number formula_35. They proved that formula_45for all prime numbers formula_32, and they called \"harmonic\" \"primes\" the primes formula_35 such that formula_41 has exactly 3 elements.\n\nEswarathasan and Levine also conjectured that formula_41 is a finite set all primes number formula_35, and that there are infinitely many harmonic primes. Boyd verified that formula_41 is finite for all prime numbers up to formula_52, but 83, 127, and 397; and he gave an heuristic suggesting that the relatively density of the harmonic primes in the set of all primes should be formula_53. Sanna showed that formula_54 has zero asymptotic density, while Bing-Ling Wu and Yong-Gao Chen proved that the number of elements of formula_54 not exceeding formula_56 is at most formula_57, for all formula_58.\n\nThe harmonic numbers appear in several calculation formulas, such as the digamma function\n\nThis relation is also frequently used to define the extension of the harmonic numbers to non-integer \"n\". The harmonic numbers are also frequently used to define γ using the limit introduced earlier:\n\nalthough\n\nconverges more quickly.\n\nIn 2002, Jeffrey Lagarias proved that the Riemann hypothesis is equivalent to the statement that\n\nis true for every integer with strict inequality if ; here σ(\"n\") denotes the sum of the divisors of \"n\".\n\nThe eigenvalues of the nonlocal problem\n\nare given by formula_64, where by convention, formula_65\n\nThe generalized harmonic number of order \"m\" of \"n\" is given by\n\nThe limit as \"n\" tends to infinity is finite if .\n\nOther notations occasionally used include\n\nThe special case of \"m\" = 0 gives formula_68\n\nThe special case of \"m\" = 1 is simply called a harmonic number and is frequently written without the \"m\", as\n\nThe smallest natural number \"k\" such that \"k\" does not divide the denominator of generalized harmonic number \"H\"(\"k\", \"n\") nor the denominator of alternating generalized harmonic number \"H′\"(\"k\", \"n\") is, for \"n\"=1, 2, ... :\n\nIn the limit as for , the generalized harmonic number converges to the Riemann zeta function\n\nThe related sum formula_71 occurs in the study of Bernoulli numbers; the harmonic numbers also appear in the study of Stirling numbers.\n\nSome integrals of generalized harmonic numbers are\n\nand\n\nand\n\nEvery generalized harmonic number of order m can be written as a function of harmonic of order m-1 using:\n\nA generating function for the generalized harmonic numbers is\n\nwhere formula_79 is the polylogarithm, and . The generating function given above for is a special case of this formula.\n\nA fractional argument for generalized harmonic numbers can be introduced as follows:\n\nFor every formula_80 integer, and formula_81 integer or not, we have from polygamma functions:\n\nwhere formula_83 is the Riemann zeta function. The relevant recurrence relation is:\nSome special values are:\n\nThe multiplication theorem applies to harmonic numbers. Using polygamma functions, we obtain\n\nor, more generally,\n\nFor generalized harmonic numbers, we have\n\nwhere formula_96 is the Riemann zeta function.\n\nThe next generalization was discussed by J. H. Conway and R. K. Guy in their 1995 book \"The Book of Numbers\". Let\n\nThen the nth hyperharmonic number of order \"r\" (\"r>0\") is defined recursively as\n\nIn particular, formula_99 is the ordinary harmonic number formula_100.\n\nThe formulae given above,\nare an integral and a series representation for a function that interpolates the harmonic numbers and, via analytic continuation, extends the definition to the complex plane other than the negative integers \"x\". The interpolating function is in fact closely related to the digamma function\n\nwhere is the digamma, and is the Euler-Mascheroni constant. The integration process may be repeated to obtain\n\nThe Taylor series for the harmonic numbers is\nwhich comes from the Taylor series for the digamma function.\n\nWhen seeking to approximate for a complex number it turns out that it is effective to first compute for some large integer , then use that to approximate a value for , and then use the recursion relation backwards times, to unwind it to an approximation for . Furthermore, this approximation is exact in the limit as goes to infinity.\n\nSpecifically, for every integer , we have that\nand we can ask that the formula be obeyed if the arbitrary integer is replaced by an arbitrary complex number \nAdding to both sides gives\nThis last expression for is well defined for any complex number except the negative integers, which fail because trying to use the recursion relation backwards through the value involves a division by zero. By construction, the function is the unique function of for which (1) , (2) for all complex values except the non-positive integers, and (3) for all complex values .\n\nBased on this last formula, it can be shown that:\nwhere γ is the Euler–Mascheroni constant or, more generally, for every \"n\" we have:\n\nThere are the following special analytic values for fractional arguments between 0 and 1, given by the integral\n\nMore values may be generated from the recurrence relation\n\nor from the reflection relation\n\nFor example:\n\nFor positive integers \"p\" and \"q\" with \"p\" < \"q\", we have:\n\nSome derivatives of fractional harmonic numbers are given by:\n\nAnd using Maclaurin series, we have for \"x\" < 1:\n\nFor fractional arguments between 0 and 1, and for \"a\" > 1:\n\n\n"}
{"id": "42599642", "url": "https://en.wikipedia.org/wiki?curid=42599642", "title": "Hemihelix", "text": "Hemihelix\n\nA hemihelix is a quasi-helical curved geometric shape characterized by repeated tendril perversions.\n"}
{"id": "6134187", "url": "https://en.wikipedia.org/wiki?curid=6134187", "title": "History of mathematical notation", "text": "History of mathematical notation\n\nThe history of mathematical notation includes the commencement, progress, and cultural diffusion of mathematical symbols and the conflict of the methods of notation confronted in a notation's move to popularity or inconspicuousness. Mathematical notation comprises the symbols used to write mathematical equations and formulas. Notation generally implies a set of well-defined representations of quantities and symbols operators. The history includes Hindu–Arabic numerals, letters from the Roman, Greek, Hebrew, and German alphabets, and a host of symbols invented by mathematicians over the past several centuries.\n\nThe development of mathematical notation can be divided in stages. The \"\"rhetorical\" stage is where calculations are performed by words and no symbols are used. The \"syncopated\"\" stage is where frequently used operations and quantities are represented by symbolic syntactical abbreviations. From ancient times through the post-classical age, bursts of mathematical creativity were often followed by centuries of stagnation. As the early modern age opened and the worldwide spread of knowledge began, written examples of mathematical developments came to light. The \"symbolic\" stage is where comprehensive systems of notation supersede rhetoric. Beginning in Italy in the 16th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This symbolic system was in use by medieval Indian mathematicians and in Europe since the middle of the 17th century, and has continued to develop in the contemporary era.\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, the focus here, the investigation into the mathematical methods and notation of the past.\n\nAlthough the history commences with that of the Ionian schools, there is no doubt that those Ancient Greeks who paid attention to it were largely indebted to the previous investigations of the Ancient Egyptians and Ancient Phoenicians. Numerical notation's distinctive feature, i.e. symbols having local as well as intrinsic values (arithmetic), implies a state of civilization at the period of its invention. Our knowledge of the mathematical attainments of these early peoples, to which this section is devoted, is imperfect and the following brief notes be regarded as a summary of the conclusions which seem most probable, and the history of mathematics begins with the symbolic sections.\n\nMany areas of mathematics began with the study of real world problems, before the underlying rules and concepts were identified and defined as abstract structures. For example, geometry has its origins in the calculation of distances and areas in the real world; algebra started with methods of solving problems in arithmetic.\n\nThere can be no doubt that most early peoples which have left records knew something of numeration and mechanics, and that a few were also acquainted with the elements of land-surveying. In particular, the Egyptians paid attention to geometry and numbers, and the Phoenicians to practical arithmetic, book-keeping, navigation, and land-surveying. The results attained by these people seem to have been accessible, under certain conditions, to travelers. It is probable that the knowledge of the Egyptians and Phoenicians was largely the result of observation and measurement, and represented the accumulated experience of many ages.\n\nWritten mathematics began with numbers expressed as tally marks, with each tally representing a single unit. The numerical symbols consisted probably of strokes or notches cut in wood or stone, and intelligible alike to all nations. For example, one notch in a bone represented one animal, or person, or anything else. The peoples with whom the Greeks of Asia Minor (amongst whom notation in western history begins) were likely to have come into frequent contact were those inhabiting the eastern littoral of the Mediterranean: and Greek tradition uniformly assigned the special development of geometry to the Egyptians, and that of the science of numbers either to the Egyptians or to the Phoenicians.\n\nThe Ancient Egyptians had a symbolic notation which was the numeration by Hieroglyphics. The Egyptian mathematics had a symbol for one, ten, one-hundred, one-thousand, ten-thousand, one-hundred-thousand, and one-million. Smaller digits were placed on the left of the number, as they are in Hindu–Arabic numerals. Later, the Egyptians used hieratic instead of hieroglyphic script to show numbers. Hieratic was more like cursive and replaced several groups of symbols with individual ones. For example, the four vertical lines used to represent four were replaced by a single horizontal line. This is found in the Rhind Mathematical Papyrus (c. 2000–1800 BC) and the Moscow Mathematical Papyrus (c. 1890 BC). The system the Egyptians used was discovered and modified by many other civilizations in the Mediterranean. The Egyptians also had symbols for basic operations: legs going forward represented addition, and legs walking backward to represent subtraction.\n\nThe Mesopotamians had symbols for each power of ten. Later, they wrote their numbers in almost exactly the same way done in modern times. Instead of having symbols for each power of ten, they would just put the coefficient of that number. Each digit was at separated by only a space, but by the time of Alexander the Great, they had created a symbol that represented zero and was a placeholder. The Mesopotamians also used a sexagesimal system, that is base sixty. It is this system that is used in modern times when measuring time and angles. Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework. The earliest evidence of written mathematics dates back to the ancient Sumerians and the system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\n\nThe majority of Mesopotamian clay tablets date from 1800 to 1600 BC, and cover topics which include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear and quadratic equations. The Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of minutes and seconds of arc to denote fractions of a degree. Babylonian advances in mathematics were facilitated by the fact that 60 has many divisors: the reciprocal of any integer which is a multiple of divisors of 60 has a finite expansion in base 60. (In decimal arithmetic, only reciprocals of multiples of 2 and 5 have finite decimal expansions.) Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. They lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context.\n\nThe history of mathematics cannot with certainty be traced back to any school or period before that of the Ionian Greeks, but the subsequent history may be divided into periods, the distinctions between which are tolerably well marked. Greek mathematics, which originated with the study of geometry, tended from its commencement to be deductive and scientific. Since the fourth century AD, Pythagoras has commonly been given credit for discovering the Pythagorean theorem, a theorem in geometry that states that in a right-angled triangle the area of the square on the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares of the other two sides. The ancient mathematical texts are available with the prior mentioned Ancient Egyptians notation and with Plimpton 322 (Babylonian mathematics c. 1900 BC). The study of mathematics as a subject in its own right begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\".\n\nPlato's influence has been especially strong in mathematics and the sciences. He helped to distinguish between pure and applied mathematics by widening the gap between \"arithmetic\", now called number theory and \"logistic\", now called arithmetic. Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Aristotle is credited with what later would be called the law of excluded middle.\n\n\"Abstract Mathematics\" is what treats of magnitude or quantity, absolutely and generally conferred, without regard to any species of particular magnitude, such as Arithmetic and Geometry, In this sense, abstract mathematics is opposed to mixed mathematics; wherein simple and abstract properties, and the relations of quantities primitively considered in mathematics, are applied to sensible objects, and by that means become intermixed with physical considerations; Such are Hydrostatics, Optics, Navigation, &c.\n\nArchimedes is generally considered to be the greatest mathematician of antiquity and one of the greatest of all time. He used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave a remarkably accurate approximation of pi. He also defined the spiral bearing his name, formulae for the volumes of surfaces of revolution and an ingenious system for expressing very large numbers.\nIn the historical development of geometry, the steps in the abstraction of geometry were made by the ancient Greeks. Euclid's Elements being the earliest extant documentation of the axioms of plane geometry— though Proclus tells of an earlier axiomatisation by Hippocrates of Chios. Euclid's \"Elements\" (c. 300 BC) is one of the oldest extant Greek mathematical treatises and consisted of 13 books written in Alexandria; collecting theorems proven by other mathematicians, supplemented by some original work. The document is a successful collection of definitions, postulates (axioms), propositions (theorems and constructions), and mathematical proofs of the propositions. Euclid's first theorem is a lemma that possesses properties of prime numbers. The influential thirteen books cover Euclidean geometry, geometric algebra, and the ancient Greek version of algebraic systems and elementary number theory. It was ubiquitous in the Quadrivium and is instrumental in the development of logic, mathematics, and science.\n\nDiophantus of Alexandria was author of a series of books called \"Arithmetica\", many of which are now lost. These texts deal with solving algebraic equations. Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nThe Greeks employed Attic numeration, which was based on the system of the Egyptians and was later adapted and used by the Romans. Greek numerals one through four were vertical lines, as in the hieroglyphics. The symbol for five was the Greek letter Π (pi), which is the letter of the Greek word for five, \"pente\". Numbers six through nine were \"pente\" with vertical lines next to it. Ten was represented by the letter (Δ) of the word for ten, \"deka\", one hundred by the letter from the word for hundred, etc.\n\nThe Ionian numeration used their entire alphabet including three archaic letters. The numeral notation of the Greeks, though far less convenient than that now in use, was formed on a perfectly regular and scientific plan, and could be used with tolerable effect as an instrument of calculation, to which purpose the Roman system was totally inapplicable. The Greeks divided the twenty-four letters of their alphabet into three classes, and, by adding another symbol to each class, they had characters to represent the units, tens, and hundreds. (Jean Baptiste Joseph Delambre's Astronomie Ancienne, t. ii.)\nThis system appeared in the third century BC, before the letters digamma (Ϝ), koppa (Ϟ), and sampi (Ϡ) became obsolete. When lowercase letters became differentiated from upper case letters, the lower case letters were used as the symbols for notation. Multiples of one thousand were written as the nine numbers with a stroke in front of them: thus one thousand was \",α\", two-thousand was \",β\", etc. M (for μὐριοι, as in \"myriad\") was used to multiply numbers by ten thousand. For example, the number 88,888,888 would be written as M,ηωπη*ηωπη\n\nGreek mathematical reasoning was almost entirely geometric (albeit often used to reason about non-geometric subjects such as number theory), and hence the Greeks had no interest in algebraic symbols. The great exception was Diophantus of Alexandria, the great algebraist. His \"Arithmetica\" was one of the texts to use symbols in equations. It was not completely symbolic, but was much more so than previous books. An unknown number was called s. The square of s was formula_1; the cube was formula_2; the fourth power was formula_3; and the fifth power was formula_4.\n\nThe Chinese used numerals that look much like the tally system. Numbers one through four were horizontal lines. Five was an X between two horizontal lines; it looked almost exactly the same as the Roman numeral for ten. Nowadays, the huāmǎ system is only used for displaying prices in Chinese markets or on traditional handwritten invoices.\n\nIn the history of the Chinese, there were those who were familiar with the sciences of arithmetic, geometry, mechanics, optics, navigation, and astronomy. Mathematics in China emerged independently by the 11th century BC. It is almost certain that the Chinese were acquainted with several geometrical or rather architectural implements; with mechanical machines; that they knew of the characteristic property of the magnetic needle; and were aware that astronomical events occurred in cycles. Chinese of that time had made attempts to classify or extend the rules of arithmetic or geometry which they knew, and to explain the causes of the phenomena with which they were acquainted beforehand. The Chinese independently developed very large and negative numbers, decimals, a place value decimal system, a binary system, algebra, geometry, and trigonometry.\nChinese mathematics made early contributions, including a place value system. The geometrical theorem known to the ancient Chinese were acquainted was applicable in certain cases (namely the ratio of sides). It is that geometrical theorems which can be demonstrated in the quasi-experimental way of superposition were also known to them. In arithmetic their knowledge seems to have been confined to the art of calculation by means of the swan-pan, and the power of expressing the results in writing. Our knowledge of the early attainments of the Chinese, slight though it is, is more complete than in the case of most of their contemporaries. It is thus instructive, and serves to illustrate the fact, that it can be known a nation may possess considerable skill in the applied arts with but our knowledge of the later mathematics on which those arts are founded can be scarce. Knowledge of Chinese mathematics before 254 BC is somewhat fragmentary, and even after this date the manuscript traditions are obscure. Dates centuries before the classical period are generally considered conjectural by Chinese scholars unless accompanied by verified archaeological evidence.\n\nAs in other early societies the focus was on astronomy in order to perfect the agricultural calendar, and other practical tasks, and not on establishing formal systems.The Chinese Board of Mathematics duties were confined to the annual preparation of an almanac, the dates and predictions in which it regulated. Ancient Chinese mathematicians did not develop an axiomatic approach, but made advances in algorithm development and algebra. The achievement of Chinese algebra reached its zenith in the 13th century, when Zhu Shijie invented method of four unknowns.\n\nAs a result of obvious linguistic and geographic barriers, as well as content, Chinese mathematics and that of the mathematics of the ancient Mediterranean world are presumed to have developed more or less independently up to the time when \"The Nine Chapters on the Mathematical Art\" reached its final form, while the \"Writings on Reckoning\" and \"Huainanzi\" are roughly contemporary with classical Greek mathematics. Some exchange of ideas across Asia through known cultural exchanges from at least Roman times is likely. Frequently, elements of the mathematics of early societies correspond to rudimentary results found later in branches of modern mathematics such as geometry or number theory. The Pythagorean theorem for example, has been attested to the time of the Duke of Zhou. Knowledge of Pascal's triangle has also been shown to have existed in China centuries before Pascal, such as by Shen Kuo.\nThe state of trigonometry in China slowly began to change and advance during the Song Dynasty (960–1279), where Chinese mathematicians began to express greater emphasis for the need of spherical trigonometry in calendarical science and astronomical calculations. The polymath Chinese scientist, mathematician and official Shen Kuo (1031–1095) used trigonometric functions to solve mathematical problems of chords and arcs. Sal Restivo writes that Shen's work in the lengths of arcs of circles provided the basis for spherical trigonometry developed in the 13th century by the mathematician and astronomer Guo Shoujing (1231–1316). As the historians L. Gauchet and Joseph Needham state, Guo Shoujing used spherical trigonometry in his calculations to improve the calendar system and Chinese astronomy. The mathematical science of the Chinese would incorporate the work and teaching of Arab missionaries with knowledge of spherical trigonometry who had come to China in the course of the thirteenth century.\n\nAlthough the origin of our present system of numerical notation is ancient, there is no doubt that it was in use among the Hindus over two thousand years ago. The algebraic notation of the Indian mathematician, Brahmagupta, was syncopated. Addition was indicated by placing the numbers side by side, subtraction by placing a dot over the subtrahend (the number to be subtracted), and division by placing the divisor below the dividend, similar to our notation but without the bar. Multiplication, evolution, and unknown quantities were represented by abbreviations of appropriate terms. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today, likely evolved over the course of the first millennium AD in India and was transmitted to the west via Islamic mathematics.\n\nDespite their name, Arabic numerals actually started in India. The reason for this misnomer is Europeans saw the numerals used in an Arabic book, \"Concerning the Hindu Art of Reckoning\", by Mohommed ibn-Musa al-Khwarizmi. Al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. Al-Khwarizmi did not claim the numerals as Arabic, but over several Latin translations, the fact that the numerals were Indian in origin was lost. The word \"algorithm\" is derived from the Latinization of Al-Khwārizmī's name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\").\n\nIslamic mathematics developed and expanded the mathematics known to Central Asian civilizations. Al-Khwārizmī gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and Al-Khwārizmī was to teach algebra in an elementary form and for its own sake. Al-Khwārizmī also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" Al-Khwārizmī also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nAl-Karaji, in his treatise \"al-Fakhri\", extends the methodology to incorporate integer powers and integer roots of unknown quantities. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham would develop analytic geometry. Al-Haytham derived the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. Al-Haytham performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. In the late 11th century, Omar Khayyam would develop algebraic geometry, wrote \"Discussions of the Difficulties in Euclid\", and wrote on the general geometric solution to cubic equations. Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals.\n\nMany Greek and Arabic texts on mathematics were then translated into Latin, which led to further development of mathematics in medieval Europe. In the 12th century, scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's and the complete text of Euclid's \"Elements\". One of the European books that advocated using the numerals was \"Liber Abaci\", by Leonardo of Pisa, better known as Fibonacci. \"Liber Abaci\" is better known for the mathematical problem Fibonacci wrote in it about a population of rabbits. The growth of the population ended up being a Fibonacci sequence, where a term is the sum of the two preceding terms.\n\nAbū al-Hasan ibn Alī al-Qalasādī (1412–1482) was the last major medieval Arab algebraist, who improved on the algebraic notation earlier used by Ibn al-Yāsamīn in the 12th century and, in the Maghreb, by Ibn al-Banna in the 13th century. In contrast to the syncopated notations of their predecessors, Diophantus and Brahmagupta, which lacked symbols for mathematical operations, al-Qalasadi's algebraic notation was the first to have symbols for these functions and was thus \"the first steps toward the introduction of algebraic symbolism.\" He represented mathematical symbols using characters from the Arabic alphabet.\n\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. The two widely used arithmetic symbols are addition and subtraction, + and −. The plus sign was used by 1360 by Nicole Oresme in his work \"Algorismus proportionum\". It is thought an abbreviation for \"et\", meaning \"and\" in Latin, in much the same way the ampersand sign also began as \"et\". Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of the distance covered by a body undergoing uniformly accelerated motion, asserting that the area under the line depicting the constant acceleration and represented the total distance traveled. The minus sign was used in 1489 by Johannes Widmann in \"Mercantile Arithmetic\" or \"Behende und hüpsche Rechenung auff allen Kauffmanschafft,\". Widmann used the minus symbol with the plus symbol, to indicate deficit and surplus, respectively. In \"Summa de arithmetica, geometria, proportioni e proportionalità\", Luca Pacioli used symbols for plus and minus symbols and contained algebra.\n\nIn the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots. In 1533, Regiomontanus's table of sines and cosines were published. Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. The radical symbol for square root was introduced by Christoph Rudolff. Michael Stifel's important work \"Arithmetica integra\" contained important innovations in mathematical notation. In 1556, Niccolò Tartaglia used parentheses for precedence grouping. In 1557 Robert Recorde published The Whetstone of Witte which used the equal sign (=) as well as plus and minus signs for the English reader. In 1564, Gerolamo Cardano analyzed games of chance beginning the early stages of probability theory. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations. Simon Stevin's book \"De Thiende\" ('the art of tenths'), published in Dutch in 1585, contained a systematic treatment of decimal notation, which influenced all later work on the real number system. The New algebra (1591) of François Viète introduced the modern notational manipulation of algebraic expressions. For navigation and accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus coin the word \"trigonometry\", publishing his \"Trigonometria\" in 1595.\n\nJohn Napier is best known as the inventor of logarithms and made common the use of the decimal point in arithmetic and mathematics. After Napier, Edmund Gunter created the logarithmic scales (lines, or rules) upon which slide rules are based, it was William Oughtred who used two such scales sliding by one another to perform direct multiplication and division; and he is credited as the inventor of the slide rule in 1622. In 1631 Oughtred introduced the multiplication sign (×) his proportionality sign, and abbreviations \"sin\" and \"cos\" for the sine and cosine functions. Albert Girard also used the abbreviations 'sin', 'cos' and 'tan' for the trigonometric functions in his treatise.\n\nJohannes Kepler was one of the pioneers of the mathematical applications of infinitesimals. René Descartes is credited as the father of analytical geometry, the bridge between algebra and geometry, crucial to the discovery of infinitesimal calculus and analysis. In the 17th century, Descartes introduced Cartesian co-ordinates which allowed the development of analytic geometry. Blaise Pascal influenced mathematics throughout his life. His \"Traité du triangle arithmétique\" (\"Treatise on the Arithmetical Triangle\") of 1653 described a convenient tabular presentation for binomial coefficients. Pierre de Fermat and Blaise Pascal would investigate probability. John Wallis introduced the infinity symbol. He similarly used this notation for infinitesimals. In 1657, Christiaan Huygens published the treatise on probability, \"On Reasoning in Games of Chance\". \n\nJohann Rahn introduced the division symbol (obelus) and the therefore sign in 1659. William Jones used π in \"Synopsis palmariorum mathesios\" in 1706 because it is the letter of the Greek word perimetron (περιμετρον), which means perimeter in Greek. This usage was popularized in 1737 by Euler. In 1734, Pierre Bouguer used double horizontal bar below the inequality sign.\n\nThe study of linear algebra emerged from the study of determinants, which were used to solve systems of linear equations. Calculus had two main systems of notation, each created by one of the creators: that developed by Isaac Newton and the notation developed by Gottfried Leibniz. Leibniz's is the notation used most often today. Newton's was simply a dot or dash placed above the function. In modern usage, this notation generally denotes derivatives of physical quantities with respect to time, and is used frequently in the science of mechanics. Leibniz, on the other hand, used the letter \"d\" as a prefix to indicate differentiation, and introduced the notation representing derivatives as if they were a special type of fraction. This notation makes explicit the variable with respect to which the derivative of the function is taken. Leibniz also created the integral symbol. The symbol is an elongated S, representing the Latin word \"Summa\", meaning \"sum\". When finding areas under curves, integration is often illustrated by dividing the area into infinitely many tall, thin rectangles, whose areas are added. Thus, the integral symbol is an elongated s, for sum.\n\nLetters of the alphabet in this time were to be used as symbols of quantity; and although much diversity existed with respect to the choice of letters, there were to be several universally recognized rules in the following history. Here thus in the history of equations the first letters of the alphabet were indicatively known as coefficients, the last letters the s (an \"incerti ordinis\"). In algebraic geometry, again, a similar rule was to be observed, the last letters of the alphabet there denoting the variable or current coordinates. Certain letters, such as formula_5, formula_6, etc., were by universal consent appropriated as symbols of the frequently occurring numbers 3.14159 ..., and 2.7182818 ..., etc., and their use in any other acceptation was to be avoided as much as possible. Letters, too, were to be employed as symbols of operation, and with them other previously mentioned arbitrary operation characters. The letters formula_7, elongated formula_8 were to be appropriated as operative symbols in the differential calculus and integral calculus, formula_9 and ∑ in the calculus of differences. In functional notation, a letter, as a symbol of operation, is combined with another which is regarded as a symbol of quantity.\n\nBeginning in 1718, Thomas Twinin used the division slash (solidus), deriving it from the earlier Arabic horizontal fraction bar. Pierre-Simon, marquis de Laplace developed the widely used Laplacian differential operator. In 1750, Gabriel Cramer developed \"Cramer's Rule\" for solving linear systems.\n\nLeonhard Euler was one of the most prolific mathematicians in history, and also a prolific inventor of canonical notation. His contributions include his use of \"e\" to represent the base of natural logarithms. It is not known exactly why formula_6 was chosen, but it was probably because the four letters of the alphabet were already commonly used to represent variables and other constants. Euler used formula_5 to represent pi consistently. The use of formula_5 was suggested by William Jones, who used it as shorthand for perimeter. Euler used formula_13 to represent the square root of negative one, although he earlier used it as an \"infinite number.\" For summation, Euler used sigma, Σ. For functions, Euler used the notation formula_14 to represent a function of formula_15. In 1730, Euler wrote the gamma function. In 1736, Euler produces his paper on the Seven Bridges of Königsberg initiating the study of graph theory.\n\nThe mathematician, William Emerson would develop the proportionality sign. Much later in the abstract expressions of the value of various proportional phenomena, the parts-per notation would become useful as a set of pseudo units to describe small values of miscellaneous dimensionless quantities. Marquis de Condorcet, in 1768, advanced the partial differential sign. In 1771, Alexandre-Théophile Vandermonde deduced the importance of topological features when discussing the properties of knots related to the geometry of position. Between 1772 and 1788, Joseph-Louis Lagrange re-formulated the formulas and calculations of Classical \"Newtonian\" mechanics, called Lagrangian mechanics. The prime symbol for derivatives was also made by Lagrange.\n\nAt the turn of the 19th century, Carl Friedrich Gauss developed the identity sign for congruence relation and, in Quadratic reciprocity, the integral part. Gauss contributed functions of complex variables, in geometry, and on the convergence of series. He gave the satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law. Gauss developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy. He would also develop the product sign. Also in this time, Niels Henrik Abel and Évariste Galois conducted their work on the solvability of equations, linking group theory and field theory.\n\nAfter the 1800s, Christian Kramp would promote factorial notation during his research in generalized factorial function which applied to non-integers. Joseph Diaz Gergonne introduced the set inclusion signs. Peter Gustav Lejeune Dirichlet developed Dirichlet \"L\"-functions to give the proof of Dirichlet's theorem on arithmetic progressions and began analytic number theory. In 1828, Gauss proved his Theorema Egregium (\"remarkable theorem\" in Latin), establishing property of surfaces. In the 1830s, George Green developed Green's function. In 1829. Carl Gustav Jacob Jacobi publishes Fundamenta nova theoriae functionum ellipticarum with his elliptic theta functions. By 1841, Karl Weierstrass, the \"father of modern analysis\", elaborated on the concept of absolute value and the determinant of a matrix.\n\nMatrix notation would be more fully developed by Arthur Cayley in his three papers, on subjects which had been suggested by reading the Mécanique analytique of Lagrange and some of the works of Laplace. Cayley defined matrix multiplication and matrix inverses. Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote \"There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants\".\nWilliam Rowan Hamilton would introduce the nabla symbol for vector differentials. This was previously used by Hamilton as a general-purpose operator sign. Hamilton reformulated Newtonian mechanics, now called Hamiltonian mechanics. This work has proven central to the modern study of classical field theories such as electromagnetism. This was also important to the development of quantum mechanics. In mathematics, he is perhaps best known as the inventor of quaternion notation and biquaternions. Hamilton also introduced the word \"tensor\" in 1846. James Cockle would develop the tessarines and, in 1849, coquaternions. In 1848, James Joseph Sylvester introduced into matrix algebra the term matrix.\n\nIn 1864 James Clerk Maxwell reduced all of the then current knowledge of electromagnetism into a linked set of differential equations with 20 equations in 20 variables, contained in \"A Dynamical Theory of the Electromagnetic Field\". (See Maxwell's equations.) The method of calculation which it is necessary to employ was given by Lagrange, and afterwards developed, with some modifications, by Hamilton's equations. It is usually referred to as Hamilton's principle; when the equations in the original form are used they are known as Lagrange's equations. In 1871 Richard Dedekind called a set of real or complex numbers which is closed under the four arithmetic operations a field. In 1873 Maxwell presented \"A Treatise on Electricity and Magnetism\".\n\nIn 1878, William Kingdon Clifford published his Elements of Dynamic. Clifford developed split-biquaternions, which he called \"algebraic motors\". Clifford obviated quaternion study by separating the dot product and cross product of two vectors from the complete quaternion notation. This approach made vector calculus available to engineers and others working in three dimensions and skeptical of the lead–lag effect in the fourth dimension. The common vector notations are used when working with vectors which are spatial or more abstract members of vector spaces, while angle notation (or phasor notation) is a notation used in electronics.\n\nIn 1881, Leopold Kronecker defined what he called a \"domain of rationality\", which is a field extension of the field of rational numbers in modern terms. In 1882, wrote the book titled \"Linear Algebra\". Lord Kelvin's aetheric atom theory (1860s) led Peter Guthrie Tait, in 1885, to publish a topological table of knots with up to ten crossings known as the Tait conjectures. In 1893, Heinrich M. Weber gave the clear definition of an abstract field. Tensor calculus was developed by Gregorio Ricci-Curbastro between 1887–96, presented in 1892 under the title \"absolute differential calculus\", and the contemporary usage of \"tensor\" was stated by Woldemar Voigt in 1898. In 1895, Henri Poincaré published \"Analysis Situs\". In 1897, Charles Proteus Steinmetz would publish , with the assistance of Ernst J. Berg.\n\nIn 1895 Giuseppe Peano issued his \"Formulario mathematico\", an effort to digest mathematics into terse text based on special symbols. He would provide a definition of a vector space and linear map. He would also introduce the intersection sign, the union sign, the membership sign (is an element of), and existential quantifier (there exists). Peano would pass to Bertrand Russell his work in 1900 at a Paris conference; it so impressed Russell that Russell too was taken with the drive to render mathematics more concisely. The result was Principia Mathematica written with Alfred North Whitehead. This treatise marks a watershed in modern literature where symbol became dominant. Ricci-Curbastro and Tullio Levi-Civita popularized the tensor index notation around 1900.\n\nAt the beginning of this period, Felix Klein's \"Erlangen program\" identified the underlying theme of various geometries, defining each of them as the study of properties invariant under a given group of symmetries. This level of abstraction revealed connections between geometry and abstract algebra. Georg Cantor would introduce the aleph symbol for cardinal numbers of transfinite sets. His notation for the cardinal numbers was the Hebrew letter formula_16 (aleph) with a natural number subscript; for the ordinals he employed the Greek letter ω (omega). This notation is still in use today in ordinal notation of a finite sequence of symbols from a finite alphabet which names an ordinal number according to some scheme which gives meaning to the language. His theory created a great deal of controversy. Cantor would, in his study of Fourier series, consider point sets in Euclidean space.\n\nAfter the turn of the 20th century, Josiah Willard Gibbs would in physical chemistry introduce middle dot for dot product and the multiplication sign for cross products. He would also supply notation for the scalar and vector products, which was introduced in \"Vector Analysis\". In 1904, Ernst Zermelo promotes axiom of choice and his proof of the well-ordering theorem. Bertrand Russell would shortly afterward introduce logical disjunction (OR) in 1906. Also in 1906, Poincaré would publish \"On the Dynamics of the Electron\" and Maurice Fréchet introduced metric space. Later, Gerhard Kowalewski and Cuthbert Edmund Cullis would successively introduce matrices notation, parenthetical matrix and box matrix notation respectively. After 1907, mathematicians studied knots from the point of view of the knot group and invariants from homology theory. In 1908, Joseph Wedderburn's structure theorems were formulated for finite-dimensional algebras over a field. Also in 1908, Ernst Zermelo proposed \"definite\" property and the first axiomatic set theory, Zermelo set theory. In 1910 Ernst Steinitz published the influential paper \"Algebraic Theory of Fields\". In 1911, Steinmetz would publish \"Theory and Calculation of Transient Electric Phenomena and Oscillations\".\nAlbert Einstein, in 1916, introduced the Einstein notation which summed over a set of indexed terms in a formula, thus exerting notational brevity. Arnold Sommerfeld would create the contour integral sign in 1917. Also in 1917, Dimitry Mirimanoff proposes axiom of regularity. In 1919, Theodor Kaluza would solve general relativity equations using five dimensions, the results would have electromagnetic equations emerge. This would be published in 1921 in \"Zum Unitätsproblem der Physik\". In 1922, Abraham Fraenkel and Thoralf Skolem independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Also in 1922, Zermelo–Fraenkel set theory was developed. In 1923, Steinmetz would publish \"Four Lectures on Relativity and Space\". Around 1924, Jan Arnoldus Schouten would develop the modern notation and formalism for the Ricci calculus framework during the absolute differential calculus applications to general relativity and differential geometry in the early twentieth century. In 1925, Enrico Fermi would describe a system comprising many identical particles that obey the Pauli exclusion principle, afterwards developing a diffusion equation (Fermi age equation). In 1926, Oskar Klein would develop the Kaluza–Klein theory. In 1928, Emil Artin abstracted ring theory with Artinian rings. In 1933, Andrey Kolmogorov introduces the \"Kolmogorov axioms\". In 1937, Bruno de Finetti deduced the \"operational subjective\" concept.\n\nMathematical abstraction began as a process of extracting the underlying essence of a mathematical concept, removing any dependence on real world objects with which it might originally have been connected, and generalizing it so that it has wider applications or matching among other abstract descriptions of equivalent phenomena. Two abstract areas of modern mathematics are category theory and model theory. Bertrand Russell, said, \"Ordinary language is totally unsuited for expressing what physics really asserts, since the words of everyday life are not sufficiently abstract. Only mathematics and mathematical logic can say as little as the physicist means to say\". Though, one can substituted mathematics for real world objects, and wander off through equation after equation, and can build a concept structure which has no relation to reality.\n\nSymbolic logic studies the purely formal properties of strings of symbols. The interest in this area springs from two sources. First, the notation used in symbolic logic can be seen as representing the words used in philosophical logic. Second, the rules for manipulating symbols found in symbolic logic can be implemented on a computing machine. Symbolic logic is usually divided into two subfields, propositional logic and predicate logic. Other logics of interest include temporal logic, modal logic and fuzzy logic. The area of symbolic logic called propositional logic, also called \"propositional calculus\", studies the properties of sentences formed from constants and logical operators. The corresponding logical operations are known, respectively, as conjunction, disjunction, material conditional, biconditional, and negation. These operators are denoted as keywords and by symbolic notation.\n\nSome of the introduced mathematical logic notation during this time included the set of symbols used in Boolean algebra. This was created by George Boole in 1854. Boole himself did not see logic as a branch of mathematics, but it has come to be encompassed anyway. Symbols found in Boolean algebra include formula_17 (AND), formula_18 (OR), and formula_19 (\"not\"). With these symbols, and letters to represent different truth values, one can make logical statements such as formula_20, that is \"(\"a\" is true OR \"a\" is \"not\" true) is true\", meaning it is true that \"a\" is either true or not true (i.e. false). Boolean algebra has many practical uses as it is, but it also was the start of what would be a large set of symbols to be used in logic. Predicate logic, originally called \"predicate calculus\", expands on propositional logic by the introduction of variables and by sentences containing variables, called predicates. In addition, predicate logic allows quantifiers. With these logic symbols and additional quantifiers from predicate logic, valid proofs can be made that are irrationally artificial, but syntactical.\n\nWhile proving his incompleteness theorems, Kurt Gödel created an alternative to the symbols normally used in logic. He used Gödel numbers, which were numbers that represented operations with set numbers, and variables with the prime numbers greater than 10. With Gödel numbers, logic statements can be broken down into a number sequence. Gödel then took this one step farther, taking the \"n\" prime numbers and putting them to the power of the numbers in the sequence. These numbers were then multiplied together to get the final product, giving every logic statement its own number.\n\nAbstraction of notation is an ongoing process and the historical development of many mathematical topics exhibits a progression from the concrete to the abstract. Various set notations would be developed for fundamental object sets. Around 1924, David Hilbert and Richard Courant published \"Methods of mathematical physics. Partial differential equations\". In 1926, Oskar Klein and Walter Gordon proposed the Klein–Gordon equation to describe relativistic particles. The first formulation of a quantum theory describing radiation and matter interaction is due to Paul Adrien Maurice Dirac, who, during 1920, was first able to compute the coefficient of spontaneous emission of an atom. In 1928, the relativistic Dirac equation was formulated by Dirac to explain the behavior of the relativistically moving electron. Dirac described the quantification of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, and Werner Heisenberg, and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles.\n\nIn 1931, Alexandru Proca developed the Proca equation (Euler–Lagrange equation) for the vector meson theory of nuclear forces and the relativistic quantum field equations. John Archibald Wheeler in 1937 develops S-matrix. Studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.\n\nIn the 1930s, the double-struck capital Z for integer number sets was created by Edmund Landau. Nicolas Bourbaki created the double-struck capital Q for rational number sets. In 1935, Gerhard Gentzen made universal quantifiers. In 1936, Tarski's undefinability theorem is stated by Alfred Tarski and proved. In 1938, Gödel proposes the constructible universe in the paper \"The Consistency of the Axiom of Choice and of the Generalized Continuum-Hypothesis\". André Weil and Nicolas Bourbaki would develop the empty set sign in 1939. That same year, Nathan Jacobson would coin the double-struck capital C for complex number sets.\n\nAround the 1930s, Voigt notation would be developed for multilinear algebra as a way to represent a symmetric tensor by reducing its order. Schönflies notation became one of two conventions used to describe point groups (the other being Hermann–Mauguin notation). Also in this time, van der Waerden notation became popular for the usage of two-component spinors (Weyl spinors) in four spacetime dimensions. Arend Heyting would introduce Heyting algebra and Heyting arithmetic.\n\nThe arrow, e.g., →, was developed for function notation in 1936 by Øystein Ore to denote images of specific elements. Later, in 1940, it took its present form, e.g., \"f: X → Y\", through the work of Witold Hurewicz. Werner Heisenberg, in 1941, proposed the S-matrix theory of particle interactions.\nBra–ket notation (Dirac notation) is a standard notation for describing quantum states, composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals. It is so called because the inner product (or dot product on a complex vector space) of two states is denoted by a bra|ket consisting of a left part, ⟨\"φ\"|, and a right part, |\"ψ\"⟩. The notation was introduced in 1939 by Paul Dirac, though the notation has precursors in Grassmann's use of the notation [\"φ\"|\"ψ\"] for his inner products nearly 100 years previously.\n\nBra–ket notation is widespread in quantum mechanics: almost every phenomenon that is explained using quantum mechanics—including a large portion of modern physics—is usually explained with the help of bra–ket notation. The notation establishes an encoded abstract representation-independence, producing a versatile specific representation (e.g., \"x\", or \"p\", or eigenfunction base) without much , or excessive reliance on, the nature of the linear spaces involved. The overlap expression ⟨\"φ\"|\"ψ\"⟩ is typically interpreted as the probability amplitude for the state \"ψ\" to collapse into the state \"ϕ\". The Feynman slash notation (Dirac slash notation) was developed by Richard Feynman for the study of Dirac fields in quantum field theory.\n\nIn 1948, Valentine Bargmann and Eugene Wigner proposed the relativistic Bargmann–Wigner equations to describe free particles and the equations are in the form of multi-component spinor field wavefunctions. In 1950, William Vallance Douglas Hodge presented \"The topological invariants of algebraic varieties\" at the Proceedings of the International Congress of Mathematicians. Between 1954 and 1957, Eugenio Calabi worked on the Calabi conjecture for Kähler metrics and the development of Calabi–Yau manifolds. In 1957, Tullio Regge formulated the mathematical property of potential scattering in the Schrödinger equation. Stanley Mandelstam, along with Regge, did the initial development of the Regge theory of strong interaction phenomenology. In 1958, Murray Gell-Mann and Richard Feynman, along with George Sudarshan and Robert Marshak, deduced the chiral structures of the weak interaction in physics. Geoffrey Chew, along with others, would promote matrix notation for the strong interaction, and the associated bootstrap principle, in 1960. In the 1960s, set-builder notation was developed for describing a set by stating the properties that its members must satisfy. Also in the 1960s, tensors are abstracted within category theory by means of the concept of monoidal category. Later, multi-index notation eliminates conventional notions used in multivariable calculus, partial differential equations, and the theory of distributions, by abstracting the concept of an integer index to an ordered tuple of indices.\n\nIn the modern mathematics of special relativity, electromagnetism and wave theory, the d'Alembert operator is the Laplace operator of Minkowski space. The Levi-Civita symbol is used in tensor calculus.\n\nAfter the full Lorentz covariance formulations that were finite at any order in a perturbation series of quantum electrodynamics, Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Quantum electrodynamics has served as the model and template for subsequent quantum field theories. Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force. In the late 1960s, the particle zoo was composed of the then known elementary particles before the discovery of quarks.\n\nA step towards the Standard Model was Sheldon Glashow's discovery, in 1960, of a way to combine the electromagnetic and weak interactions. In 1967, Steven Weinberg and Abdus Salam incorporated the Higgs mechanism into Glashow's electroweak theory, giving it its modern form. The Higgs mechanism is believed to give rise to the masses of all the elementary particles in the Standard Model. This includes the masses of the W and Z bosons, and the masses of the fermions - i.e. the quarks and leptons. Also in 1967, Bryce DeWitt published his equation under the name \"\"Einstein–Schrödinger equation\" (later renamed the \"Wheeler–DeWitt equation\"\"). In 1969, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind described space and time in terms of strings. In 1970, Pierre Ramond develop two-dimensional supersymmetries. Michio Kaku and Keiji Kikkawa would afterwards formulate string variations. In 1972, Michael Artin, Alexandre Grothendieck, Jean-Louis Verdier propose the Grothendieck universe.\n\nAfter the neutral weak currents caused by boson exchange were discovered at CERN in 1973, the electroweak theory became widely accepted and Glashow, Salam, and Weinberg shared the 1979 Nobel Prize in Physics for discovering it. The theory of the strong interaction, to which many contributed, acquired its modern form around 1973–74. With the establishment of quantum chromodynamics, a finalized a set of fundamental and exchange particles, which allowed for the establishment of a \"standard model\" based on the mathematics of gauge invariance, which successfully described all forces except for gravity, and which remains generally accepted within the domain to which it is designed to be applied. In the late 1970s, William Thurston introduced hyperbolic geometry into the study of knots with the hyperbolization theorem. The orbifold notation system, invented by Thurston, has been developed for representing types of symmetry groups in two-dimensional spaces of constant curvature. In 1978, Shing-Tung Yau deduced that the Calabi conjecture have Ricci flat metrics. In 1979, Daniel Friedan showed that the equations of motions of string theory are abstractions of Einstein equations of General Relativity.\n\nThe first superstring revolution is composed of mathematical equations developed between 1984 and 1986. In 1984, Vaughan Jones deduced the Jones polynomial and subsequent contributions from Edward Witten, Maxim Kontsevich, and others, revealed deep connections between knot theory and mathematical methods in statistical mechanics and quantum field theory. According to string theory, all particles in the \"particle zoo\" have a common ancestor, namely a vibrating string. In 1985, Philip Candelas, Gary Horowitz, Andrew Strominger, and Edward Witten would publish \"Vacuum configurations for superstrings\" Later, the tetrad formalism (tetrad index notation) would be introduced as an approach to general relativity that replaces the choice of a coordinate basis by the less restrictive choice of a local basis for the tangent bundle.\n\nIn the 1990s, Roger Penrose would propose Penrose graphical notation (tensor diagram notation) as a, usually handwritten, visual depiction of multilinear functions or tensors. Penrose would also introduce abstract index notation. In 1995, Edward Witten suggested M-theory and subsequently used it to explain some observed dualities, initiating the second superstring revolution.\nJohn Conway would further various notations, including the Conway chained arrow notation, the Conway notation of knot theory, and the Conway polyhedron notation. The Coxeter notation system classifies symmetry groups, describing the angles between with fundamental reflections of a Coxeter group. It uses a bracketed notation, with modifiers to indicate certain subgroups. The notation is named after H. S. M. Coxeter and Norman Johnson more comprehensively defined it.\n\nCombinatorial LCF notation has been developed for the representation of cubic graphs that are Hamiltonian. The cycle notation is the convention for writing down a permutation in terms of its constituent cycles. This is also called circular notation and the permutation called a \"cyclic\" or \"circular\" permutation.\n\nIn 1931, IBM produces the IBM 601 Multiplying Punch; it is an electromechanical machine that could read two numbers, up to 8 digits long, from a card and punch their product onto the same card. In 1934, Wallace Eckert used a rigged IBM 601 Multiplying Punch to automate the integration of differential equations. In 1936, Alan Turing publishes \"On Computable Numbers, With an Application to the Entscheidungsproblem\". John von Neumann, pioneer of the digital computer and of computer science, in 1945, writes the incomplete \"First Draft of a Report on the EDVAC\". In 1962, Kenneth E. Iverson developed an integral part notation that became known as Iverson Notation for manipulating arrays that he taught to his students, and described in his book \"A Programming Language\". In 1970, E.F. Codd proposed relational algebra as a relational model of data for database query languages. In 1971, Stephen Cook publishes \"The complexity of theorem proving procedures\" In the 1970s within computer architecture, Quote notation was developed for a representing number system of rational numbers. Also in this decade, the Z notation (just like the APL language, long before it) uses many non-ASCII symbols, the specification includes suggestions for rendering the Z notation symbols in ASCII and in LaTeX. There are presently various C mathematical functions (Math.h) and numerical libraries. They are libraries used in software development for performing numerical calculations. These calculations can be handled by symbolic executions; analyzing a program to determine what inputs cause each part of a program to execute. Mathematica and SymPy are examples of computational software programs based on symbolic mathematics.\n\nIn the history of mathematical notation, ideographic symbol notation has come full circle with the rise of computer visualization systems. The notations can be applied to abstract visualizations, such as for rendering some projections of a Calabi-Yau manifold. Examples of abstract visualization which properly belong to the mathematical imagination can be found in computer graphics. The need for such models abounds, for example, when the measures for the subject of study are actually random variables and not really ordinary mathematical functions.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "26117670", "url": "https://en.wikipedia.org/wiki?curid=26117670", "title": "Illuminationism", "text": "Illuminationism\n\nIlluminationist or ishraqi philosophy is a type of Islamic philosophy introduced by Shahab al-Din Suhrawardi in the twelfth century CE.\n\nInfluenced by Avicennism and Neoplatonism, the Persian or Kurdish, philosopher Shahab al-Din Suhrawardi (1155–1191), who left over 50 writings in Persian and Arabic, founded the school of Illumination. He developed a version of illuminationism (Persian حكمت اشراق \"hikmat-i ishrāq\", Arabic: حكمة الإشراق \"ḥikmat al-ishrāq\"). The Persian and Islamic school draws on ancient Iranian philosophical disciplines, Avicennism (Ibn Sina’s early Islamic philosophy), Neoplatonic thought (modified by Ibn Sina), and the original ideas of Suhrawardi.\n\nIn his \"Philosophy of Illumination\", Suhrawardi argued that light operates at all levels and hierarchies of reality (PI, 97.7–98.11). Light produces immaterial and substantial lights, including immaterial intellects (angels), human and animal souls, and even 'dusky substances', such as bodies.\n\nSuhrawardi's metaphysics is based on two principles. The first is a form of the principle of sufficient reason. The second principle is Aristotle's principle that an actual infinity is impossible.\n\nNone of Suhrawardi's works were translated into Latin, and so he remained unknown in the Latin West, although his work continued to be studied in the Islamic East.\nAccording to Hosein Nasr, Suhrawardi was unknown to the west until he was translated to western languages by contemporary thinkers such as Henry Corbin, and he remains largely unknown even in countries within the Islamic world. Suhrawardi tried to present a new perspective on questions like those of existence. He not only caused peripatetic philosophers to confront such new questions but also gave new life to the body of philosophy after Avicenna. According to John Walbridge, Suhrawardi's critiques of Peripatetic philosophy could be counted as an important turning point for his successors. Although Suhravardi was first a pioneer of Peripatetic philosophy, he later became a Platonist following a mystical experience. He is also counted as one who revived the ancient wisdom in Persia by his philosophy of illumination. His followers, such as Shahrzouri and Qutb al-Din al-Shirazi tried to continue the way of their teacher. Suhrewardi makes a distinction between two approaches in the philosophy of illumination: one approach is discursive and another is intuitive.\n\n\n\n"}
{"id": "30551989", "url": "https://en.wikipedia.org/wiki?curid=30551989", "title": "J-line", "text": "J-line\n\nIn the study of the arithmetic of elliptic curves, the \"j\"-line over any ring \"R\" is the coarse moduli scheme attached to the moduli problem Γ(1)]:\n\nwith the \"j\"-invariant normalized a la Tate: \"j\" = 0 has complex multiplication by \"Z\"[\"ζ\"], and \"j\" = 1728 by \"Z\"[\"i\"].\n\nThe \"j\"-line can be seen as giving a coordinatization of the classical modular curve of level 1, \"X\"(1), which is isomorphic to the complex projective line.\n"}
{"id": "268886", "url": "https://en.wikipedia.org/wiki?curid=268886", "title": "Johann Bernoulli", "text": "Johann Bernoulli\n\nJohann Bernoulli (also known as Jean or John; – 1 January 1748) was a Swiss mathematician and was one of the many prominent mathematicians in the Bernoulli family. He is known for his contributions to infinitesimal calculus and educating Leonhard Euler in the pupil's youth.\n\nJohann was born in Basel, the son of Nicolaus Bernoulli, an apothecary, and his wife, Margaretha Schonauer, and began studying medicine at Basel University. His father desired that he study business so that he might take over the family spice trade, but Johann Bernoulli did not like business and convinced his father to allow him to study medicine instead. However, Johann Bernoulli did not enjoy medicine either and began studying mathematics on the side with his older brother Jacob. Throughout Johann Bernoulli's education at Basel University the Bernoulli brothers worked together spending much of their time studying the newly discovered infinitesimal calculus. They were among the first mathematicians to not only study and understand calculus but to apply it to various problems.\n\nAfter graduating from Basel University Johann Bernoulli moved to teach differential equations. Later, in 1694, he married Dorothea Falkner and soon after accepted a position as the professor of mathematics at the University of Groningen. At the request of his father-in-law, Johann Bernoulli began the voyage back to his home town of Basel in 1705. Just after setting out on the journey he learned of his brother's death to tuberculosis. Johann Bernoulli had planned on becoming the professor of Greek at Basel University upon returning but instead was able to take over as professor of mathematics, his older brother's former position. As a student of Leibniz's calculus, Johann Bernoulli sided with him in 1713 in the Newton–Leibniz debate over who deserved credit for the discovery of calculus. Johann Bernoulli defended Leibniz by showing that he had solved certain problems with his methods that Newton had failed to solve. Johann Bernoulli also promoted Descartes' vortex theory over Newton's theory of gravitation. This ultimately delayed acceptance of Newton's theory in continental Europe.\nIn 1724, Johann Bernoulli entered a competition sponsored by the French Académie Royale des Sciences, which posed the question:\n\nIn defending a view previously espoused by Leibniz, he found himself postulating an infinite external force required to make the body elastic by overcoming the infinite internal force making the body hard. In consequence, he was disqualified for the prize, which was won by Maclaurin. However, Bernoulli's paper was subsequently accepted in 1726 when the Académie considered papers regarding elastic bodies, for which the prize was awarded to Pierre Mazière. Bernoulli received an honourable mention in both competitions.\n\nAlthough Jacob and Johann Bernoulli worked together before Johann graduated from Basel University, shortly after this, the two developed a jealous and competitive relationship. Johann was jealous of Jacob's position and the two often attempted to outdo each other. After Jacob's death Johann's jealousy shifted toward his own talented son, Daniel. In 1738 the father–son duo nearly simultaneously published separate works on hydrodynamics. Johann Bernoulli attempted to take precedence over his son by purposely and falsely predating his work two years prior to his son's.\n\nJohann married Dorothea Falkner, daughter of an Alderman of Basel. He was the father of Nicolaus II Bernoulli, Daniel Bernoulli and Johann II Bernoulli and uncle of Nicolaus I Bernoulli.\n\nThe Bernoulli brothers often worked on the same problems, but not without friction. Their most bitter dispute concerned finding the equation for the path followed by a particle from one point to another in the shortest time, if the particle is acted upon by gravity alone, a problem originally discussed by Galileo. In 1697 Jacob offered a reward for its solution. Accepting the challenge, Johann proposed the cycloid, the path of a point on a moving wheel, pointing out at the same time the relation this curve bears to the path described by a ray of light passing through strata of variable density. A protracted, bitter dispute then arose when Jacob challenged the solution and proposed his own. The dispute marked the origin of a new discipline, the calculus of variations.\n\nBernoulli was hired by Guillaume de l'Hôpital for tutoring in mathematics. Bernoulli and l'Hôpital signed a contract which gave l'Hôpital the right to use Bernoulli's discoveries as he pleased. L'Hôpital authored the first textbook on infinitesimal calculus, \"Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes\" in 1696, which mainly consisted of the work of Bernoulli, including what is now known as l'Hôpital's rule.\n\nSubsequently, in letters to Leibniz, Varignon and others, Bernoulli complained that he had not received enough credit for his contributions, in spite of the fact that l'Hôpital acknowledged fully his debt in the preface of his book:\n\n\n"}
{"id": "15942", "url": "https://en.wikipedia.org/wiki?curid=15942", "title": "John von Neumann", "text": "John von Neumann\n\nJohn von Neumann (; , ; December 28, 1903 – February 8, 1957) was a Hungarian-American mathematician, physicist, computer scientist, and polymath. He made major contributions to a number of fields, including mathematics (foundations of mathematics, functional analysis, ergodic theory, representation theory, operator algebras, geometry, topology, and numerical analysis), physics (quantum mechanics, hydrodynamics, and quantum statistical mechanics), economics (game theory), computing (Von Neumann architecture, linear programming, self-replicating machines, stochastic computing), and statistics.\n\nVon Neumann was generally regarded as the foremost mathematician of his time and said to be \"the last representative of the great mathematicians\". He was a pioneer of the application of operator theory to quantum mechanics in the development of functional analysis, and a key figure in the development of game theory and the concepts of cellular automata, the universal constructor and the digital computer. He published over 150 papers in his life: about 60 in pure mathematics, 20 in physics, and 60 in applied mathematics, the remainder being on special mathematical subjects or non-mathematical ones. His last work, an unfinished manuscript written while in hospital, was later published in book form as \"The Computer and the Brain\".\n\nHis analysis of the structure of self-replication preceded the discovery of the structure of DNA. In a short list of facts about his life he submitted to the National Academy of Sciences, he stated, \"The part of my work I consider most essential is that on quantum mechanics, which developed in Göttingen in 1926, and subsequently in Berlin in 1927–1929. Also, my work on various forms of operator theory, Berlin 1930 and Princeton 1935–1939; on the ergodic theorem, Princeton, 1931–1932.\"\n\nDuring World War II, von Neumann worked on the Manhattan Project; he developed the mathematical models that were behind the explosive lenses used in the implosion-type nuclear weapon. After the war, he served on the General Advisory Committee of the United States Atomic Energy Commission, and later as one of its commissioners. He was a consultant to a number of organizations, including the United States Air Force, the Army's Ballistic Research Laboratory, the Armed Forces Special Weapons Project, and the Lawrence Livermore National Laboratory. Von Neumann, theoretical physicist Edward Teller, mathematician Stanislaw Ulam and others worked out key steps in the nuclear physics involved in thermonuclear reactions and the hydrogen bomb.\n\nVon Neumann was born Neumann János Lajos to a wealthy, acculturated and non-observant Jewish family (in Hungarian the family name comes first. His given names equate to John Louis in English). His Hebrew name was Yonah. Von Neumann was born in Budapest, Kingdom of Hungary, which was then part of the Austro-Hungarian Empire. He was the eldest of three brothers; his two younger siblings were Mihály (English: Michael von Neumann; 1907–1989) and Miklós (Nicholas Vonneumann, 1911–2011). His father, Neumann Miksa (Max von Neumann, 1873–1928) was a banker, who held a doctorate in law. He had moved to Budapest from Pécs at the end of the 1880s. Miksa's father and grandfather were both born in Ond (now part of the town of Szerencs), Zemplén County, northern Hungary. John's mother was Kann Margit (English: Margaret Kann); her parents were Jakab Kann and Katalin Meisels. Three generations of the Kann family lived in spacious apartments above the Kann-Heller offices in Budapest; von Neumann's family occupied an 18-room apartment on the top floor.\n\nIn 1913, Emperor Franz Joseph elevated his father to the nobility for his service to the Austro-Hungarian Empire. The Neumann family thus acquired the hereditary appellation \"Margittai\", meaning of Margitta (today Marghita, Romania). The family had no connection with the town; the appellation was chosen in reference to Margaret, as was that chosen coat of arms depicting three marguerites. Neumann János became margittai Neumann János (John Neumann de Margitta), which he later changed to the German Johann von Neumann.\n\nVon Neumann was a child prodigy. When he was 6 years old, he could divide two 8-digit numbers in his head and could converse in Ancient Greek. When the 6-year-old von Neumann caught his mother staring aimlessly, he asked her, \"What are you calculating?\"\n\nChildren did not begin formal schooling in Hungary until they were ten years of age; governesses taught von Neumann, his brothers and his cousins. Max believed that knowledge of languages in addition to Hungarian was essential, so the children were tutored in English, French, German and Italian. By the age of 8, von Neumann was familiar with differential and integral calculus, but he was particularly interested in history. He read his way through Wilhelm Oncken's 46-volume \"Allgemeine Geschichte in Einzeldarstellungen\". A copy was contained in a private library Max purchased. One of the rooms in the apartment was converted into a library and reading room, with bookshelves from ceiling to floor.\n\nVon Neumann entered the Lutheran Fasori Evangélikus Gimnázium in 1911. This was one of the best schools in Budapest and was part of a brilliant education system designed for the elite. Under the Hungarian system, children received all their education at the one gymnasium. Despite being run by the Lutheran Church, the school was predominately Jewish in its student body. The school system produced a generation noted for intellectual achievement, which included Theodore von Kármán (b. 1881), George de Hevesy (b. 1885), Leó Szilárd (b. 1898), Dennis Gabor (b. 1900), Eugene Wigner (b. 1902), Edward Teller (b. 1908), and Paul Erdős (b. 1913). Collectively, they were sometimes known as \"The Martians\". Wigner was a year ahead of von Neumann at the Lutheran School. When asked why the Hungary of his generation had produced so many geniuses, Wigner, who won the Nobel Prize in Physics in 1963, replied that von Neumann was the only genius.\n\nAlthough Max insisted von Neumann attend school at the grade level appropriate to his age, he agreed to hire private tutors to give him advanced instruction in those areas in which he had displayed an aptitude. At the age of 15, he began to study advanced calculus under the renowned analyst Gábor Szegő. On their first meeting, Szegő was so astounded with the boy's mathematical talent that he was brought to tears. Some of von Neumann's instant solutions to the problems that Szegő posed in calculus are sketched out on his father's stationery and are still on display at the von Neumann archive in Budapest. By the age of 19, von Neumann had published two major mathematical papers, the second of which gave the modern definition of ordinal numbers, which superseded Georg Cantor's definition. At the conclusion of his education at the gymnasium, von Neumann sat for and won the Eötvös Prize, a national prize for mathematics.\n\nThere were few academic positions in Hungary for mathematicians, and those jobs that did exist were not well-paid. Von Neumann's father wanted John to follow him into industry and thereby invest his time in a more financially useful endeavor than mathematics. In fact, his father requested Theodore von Kármán to persuade his son not to take mathematics as his major. Von Neumann and his father decided that the best career path was to become a chemical engineer. This was not something that von Neumann had much knowledge of, so it was arranged for him to take a two-year, non-degree course in chemistry at the University of Berlin, after which he sat for the entrance exam to the prestigious ETH Zurich, which he passed in September 1923. At the same time, von Neumann also entered Pázmány Péter University in Budapest, as a Ph.D. candidate in mathematics. For his thesis, he chose to produce an axiomatization of Cantor's set theory. He graduated as a chemical engineer from ETH Zurich in 1926 (although Wigner says that von Neumann was never very attached to that subject), and passed his final examinations for his Ph.D. in mathematics simultaneously, of which Eugene Wigner wrote, \"Evidently a Ph.D. thesis and examination did not constitute an appreciable effort.\" He then went to the University of Göttingen on a grant from the Rockefeller Foundation to study mathematics under David Hilbert.\n\nVon Neumann's habilitation was completed on December 13, 1927, and he started his lectures as a \"privatdozent\" at the University of Berlin in 1928, being the youngest person ever elected \"privatdozent\" in its history in any subject. By the end of 1927, von Neumann had published twelve major papers in mathematics, and by the end of 1929, thirty-two papers, at a rate of nearly one major paper per month. His reputed powers of memorization and recall allowed him to quickly memorize the pages of telephone directories, and recite the names, addresses and numbers therein. In 1929, he briefly became a \"privatdozent\" at the University of Hamburg, where the prospects of becoming a tenured professor were better, but in October of that year a better offer presented itself when he was invited to Princeton University in Princeton, New Jersey.\n\nOn New Year's Day in 1930, von Neumann married Marietta Kövesi, who had studied economics at Budapest University. Von Neumann and Marietta had one child, a daughter, Marina, born in 1935. As of 2017, she is a distinguished professor of business administration and public policy at the University of Michigan. The couple divorced in 1937. In October 1938, von Neumann married Klara Dan, whom he had met during his last trips back to Budapest prior to the outbreak of World War II.\n\nPrior to his marriage to Marietta, von Neumann was baptized a Catholic in 1930. \nVon Neumann's father, Max, had died in 1929. None of the family had converted to Christianity while Max was alive, but all did afterward.\n\nIn 1933, he was offered a lifetime professorship on the faculty of the Institute for Advanced Study in New Jersey when that institution's plan to appoint Hermann Weyl fell through. He remained a mathematics professor there until his death, although he had announced his intention to resign and become a professor at large at the University of California. His mother, brothers and in-laws followed von Neumann to the United States in 1939. Von Neumann anglicized his first name to John, keeping the German-aristocratic surname of von Neumann. His brothers changed theirs to \"Neumann\" and \"Vonneumann\". Von Neumann became a naturalized citizen of the United States in 1937, and immediately tried to become a lieutenant in the United States Army's Officers Reserve Corps. He passed the exams easily, but was ultimately rejected because of his age. His prewar analysis of how France would stand up to Germany is often quoted: \"Oh, France won't matter.\"\n\nKlara and John von Neumann were socially active within the local academic community. His white clapboard house at 26 Westcott Road was one of the largest private residences in Princeton. He took great care of his clothing and would always wear formal suits. He once wore a three-piece pinstripe when he rode down the Grand Canyon astride a mule. Hilbert is reported to have asked \"Pray, who is the candidate's tailor?\" at von Neumann's 1926 doctoral exam, as he had never seen such beautiful evening clothes.\n\nVon Neumann held a lifelong passion for ancient history, being renowned for his prodigious historical knowledge. A professor of Byzantine history at Princeton once said that von Neumann had greater expertise in the field than he did.\n\nVon Neumann liked to eat and drink; his wife, Klara, said that he could count everything except calories. He enjoyed Yiddish and \"off-color\" humor (especially limericks). He was a non-smoker. In Princeton, he received complaints for regularly playing extremely loud German march music on his gramophone, which distracted those in neighboring offices, including Albert Einstein, from their work. Von Neumann did some of his best work in noisy, chaotic environments, and once admonished his wife for preparing a quiet study for him to work in. He never used it, preferring the couple's living room with its television playing loudly. Despite being a notoriously bad driver, he nonetheless enjoyed driving—frequently while reading a book—occasioning numerous arrests as well as accidents. When Cuthbert Hurd hired him as a consultant to IBM, Hurd often quietly paid the fines for his traffic tickets.\n\nVon Neumann's closest friend in the United States was mathematician Stanislaw Ulam. A later friend of Ulam's, Gian-Carlo Rota, wrote, \"They would spend hours on end gossiping and giggling, swapping Jewish jokes, and drifting in and out of mathematical talk.\" When von Neumann was dying in the hospital, every time Ulam visited, he came prepared with a new collection of jokes to cheer him up. He believed that much of his mathematical thought occurred intuitively, and he would often go to sleep with a problem unsolved and know the answer immediately upon waking up. Ulam noted that von Neumann's way of thinking might not be visual, but more aural.\n\nThe axiomatization of mathematics, on the model of Euclid's \"Elements\", had reached new levels of rigour and breadth at the end of the 19th century, particularly in arithmetic, thanks to the axiom schema of Richard Dedekind and Charles Sanders Peirce, and in geometry, thanks to Hilbert's axioms. But at the beginning of the 20th century, efforts to base mathematics on naive set theory suffered a setback due to Russell's paradox (on the set of all sets that do not belong to themselves). The problem of an adequate axiomatization of set theory was resolved implicitly about twenty years later by Ernst Zermelo and Abraham Fraenkel. Zermelo–Fraenkel set theory provided a series of principles that allowed for the construction of the sets used in the everyday practice of mathematics, but they did not explicitly exclude the possibility of the existence of a set that belongs to itself. In his doctoral thesis of 1925, von Neumann demonstrated two techniques to exclude such sets—the \"axiom of foundation\" and the notion of \"class.\"\n\nThe axiom of foundation proposed that every set can be constructed from the bottom up in an ordered succession of steps by way of the principles of Zermelo and Fraenkel. If one set belongs to another then the first must necessarily come before the second in the succession. This excludes the possibility of a set belonging to itself. To demonstrate that the addition of this new axiom to the others did not produce contradictions, von Neumann introduced a method of demonstration, called the \"method of inner models\", which later became an essential instrument in set theory.\n\nThe second approach to the problem of sets belonging to themselves took as its base the notion of class, and defines a set as a class which belongs to other classes, while a \"proper class\" is defined as a class which does not belong to other classes. Under the Zermelo–Fraenkel approach, the axioms impede the construction of a set of all sets which do not belong to themselves. In contrast, under the von Neumann approach, the class of all sets which do not belong to themselves can be constructed, but it is a \"proper class\" and not a set.\n\nWith this contribution of von Neumann, the axiomatic system of the theory of sets avoided the contradictions of earlier systems, and became usable as a foundation for mathematics, despite the lack of a proof of its consistency. The next question was whether it provided definitive answers to all mathematical questions that could be posed in it, or whether it might be improved by adding stronger axioms that could be used to prove a broader class of theorems. A strongly negative answer to whether it was definitive arrived in September 1930 at the historic mathematical Congress of Königsberg, in which Kurt Gödel announced his first theorem of incompleteness: the usual axiomatic systems are incomplete, in the sense that they cannot prove every truth which is expressible in their language. Moreover, every consistent extension of these systems would necessarily remain incomplete.\n\nLess than a month later, von Neumann, who had participated at the Congress, communicated to Gödel an interesting consequence of his theorem: that the usual axiomatic systems are unable to demonstrate their own consistency. However, Gödel had already discovered this consequence, now known as his second incompleteness theorem, and he sent von Neumann a preprint of his article containing both incompleteness theorems. Von Neumann acknowledged Gödel's priority in his next letter. He never thought much of \"the American system of claiming personal priority for everything.\"\n\nIn a series of articles that were published in 1932, von Neumann made foundational contributions to ergodic theory, a branch of mathematics that involves the states of dynamical systems with an invariant measure. Of the 1932 papers on ergodic theory, Paul Halmos writes that even \"if von Neumann had never done anything else, they would have been sufficient to guarantee him mathematical immortality\". By then von Neumann had already written his famous articles on operator theory, and the application of this work was instrumental in the von Neumann mean ergodic theorem.\n\nVon Neumann introduced the study of rings of operators, through the von Neumann algebras. A von Neumann algebra is a *-algebra of bounded operators on a Hilbert space that is closed in the weak operator topology and contains the identity operator. The von Neumann bicommutant theorem shows that the analytic definition is equivalent to a purely algebraic definition as being equal to the bicommutant. Von Neumann embarked in 1936, with the partial collaboration of F.J. Murray, on the general study of factors classification of von Neumann algebras. The six major papers in which he developed that theory between 1936 and 1940 \"rank among the masterpieces of analysis in the twentieth century\". The direct integral was later introduced in 1949 by John von Neumann.\n\nIn measure theory, the \"problem of measure\" for an -dimensional Euclidean space may be stated as: \"does there exist a positive, normalized, invariant, and additive set function on the class of all subsets of ?\" The work of Felix Hausdorff and Stefan Banach had implied that the problem of measure has a positive solution if or and a negative solution (because of the Banach–Tarski paradox) in all other cases. Von Neumann's work argued that the \"problem is essentially group-theoretic in character\": the existence of a measure could be determined by looking at the properties of the transformation group of the given space. The positive solution for spaces of dimension at most two, and the negative solution for higher dimensions, comes from the fact that the Euclidean group is a solvable group for dimension at most two, and is not solvable for higher dimensions. \"Thus, according to von Neumann, it is the change of group that makes a difference, not the change of space.\"\n\nIn a number of von Neumann's papers, the methods of argument he employed are considered even more significant than the results. In anticipation of his later study of dimension theory in algebras of operators, von Neumann used results on equivalence by finite decomposition, and reformulated the problem of measure in terms of functions. In his 1936 paper on analytic measure theory, he used the Haar theorem in the solution of Hilbert's fifth problem in the case of compact groups. In 1938, he was awarded the Bôcher Memorial Prize for his work in analysis.\n\nVon Neumann founded the field of continuous geometry. It followed his path-breaking work on rings of operators. In mathematics, continuous geometry is a substitute of complex projective geometry, where instead of the dimension of a subspace being in a discrete set 0, 1, ..., \"n\", it can be an element of the unit interval [0,1]. Earlier, Menger and Birkhoff had axiomatized complex projective geometry in terms of the properties of its lattice of linear subspaces. Von Neumann, following his work on rings of operators, weakened those axioms to describe a broader class of lattices, the continuous geometries.\nWhile the dimensions of the subspaces of projective geometries are a discrete set (the non-negative integers), the dimensions of the elements of a continuous geometry can range continuously across the unit interval [0,1]. Von Neumann was motivated by his discovery of von Neumann algebras with a dimension function taking a continuous range of dimensions, and the first example of a continuous geometry other than projective space was the projections of the hyperfinite type II factor.\n\nBetween 1937 and 1939, von Neumann worked on lattice theory, the theory of partially ordered sets in which every two elements have a greatest lower bound and a least upper bound. Garrett Birkhoff writes: \"John von Neumann's brilliant mind blazed over lattice theory like a meteor\".\n\nVon Neumann provided an abstract exploration of dimension in completed complemented modular topological lattices (properties that arise in the lattices of subspaces of inner product spaces): \"Dimension is determined, up to a positive linear transformation, by the following two properties. It is conserved by perspective mappings (\"perspectivities\") and ordered by inclusion. The deepest part of the proof concerns the equivalence of perspectivity with \"projectivity by decomposition\"—of which a corollary is the transitivity of perspectivity.\"\n\nAdditionally, \"[I]n the general case, von Neumann proved the following basic representation theorem. Any complemented modular lattice having a \"basis\" of pairwise perspective elements, is isomorphic with the lattice of all principal right-ideals of a suitable regular ring . This conclusion is the culmination of 140 pages of brilliant and incisive algebra involving entirely novel axioms. Anyone wishing to get an unforgettable impression of the razor edge of von Neumann's mind, need merely try to pursue this chain of exact reasoning for himself—realizing that often five pages of it were written down before breakfast, seated at a living room writing-table in a bathrobe.\"\n\nVon Neumann was the first to establish a rigorous mathematical framework for quantum mechanics, known as the Dirac–von Neumann axioms, with his 1932 work \"Mathematical Foundations of Quantum Mechanics\". After having completed the axiomatization of set theory, he began to confront the axiomatization of quantum mechanics. He realized, in 1926, that a state of a quantum system could be represented by a point in a (complex) Hilbert space that, in general, could be infinite-dimensional even for a single particle. In this formalism of quantum mechanics, observable quantities such as position or momentum are represented as linear operators acting on the Hilbert space associated with the quantum system.\n\nThe \"physics\" of quantum mechanics was thereby reduced to the \"mathematics\" of Hilbert spaces and linear operators acting on them. For example, the uncertainty principle, according to which the determination of the position of a particle prevents the determination of its momentum and vice versa, is translated into the \"non-commutativity\" of the two corresponding operators. This new mathematical formulation included as special cases the formulations of both Heisenberg and Schrödinger. When Heisenberg was informed von Neumann had clarified the difference between an unbounded operator that was a self-adjoint operator and one that was merely symmetric, Heisenberg replied \"Eh? What is the difference?\"\n\nVon Neumann's abstract treatment permitted him also to confront the foundational issue of determinism versus non-determinism, and in the book he presented a proof that the statistical results of quantum mechanics could not possibly be averages of an underlying set of determined \"hidden variables,\" as in classical statistical mechanics. In 1935, Grete Hermann published a paper arguing that the proof contained a conceptual error and was therefore invalid. Hermann's work was largely ignored until after John S. Bell made essentially the same argument in 1966. However, in 2010, Jeffrey Bub argued that Bell had misconstrued von Neumann's proof, and pointed out that the proof, though not valid for all hidden variable theories, does rule out a well-defined and important subset. Bub also suggests that von Neumann was aware of this limitation, and that von Neumann did not claim that his proof completely ruled out hidden variable theories. The validity of Bub's argument is, in turn, disputed. In any case, Gleason's Theorem of 1957 fills the gaps in von Neumann's approach.\n\nVon Neumann's proof inaugurated a line of research that ultimately led, through the work of Bell in 1964 on Bell's theorem, and the experiments of Alain Aspect in 1982, to the demonstration that quantum physics either requires a \"notion of reality\" substantially different from that of classical physics, or must include nonlocality in apparent violation of special relativity.\n\nIn a chapter of \"The Mathematical Foundations of Quantum Mechanics\", von Neumann deeply analyzed the so-called measurement problem. He concluded that the entire physical universe could be made subject to the universal wave function. Since something \"outside the calculation\" was needed to collapse the wave function, von Neumann concluded that the collapse was caused by the consciousness of the experimenter (although this view was accepted by Eugene Wigner, the Von Neumann–Wigner interpretation never gained acceptance amongst the majority of physicists).\n\nThough theories of quantum mechanics continue to evolve to this day, there is a basic framework for the mathematical formalism of problems in quantum mechanics which underlies the majority of approaches and can be traced back to the mathematical formalisms and techniques first used by von Neumann. In other words, discussions about interpretation of the theory, and extensions to it, are now mostly conducted on the basis of shared assumptions about the mathematical foundations.\n\nVon Neumann entropy is extensively used in different forms (conditional entropies, relative entropies, etc.) in the framework of quantum information theory. Entanglement measures are based upon some quantity directly related to the von Neumann entropy. Given a statistical ensemble of quantum mechanical systems with the density matrix formula_1, it is given by formula_2 Many of the same entropy measures in classical information theory can also be generalized to the quantum case, such as Holevo entropy and the conditional quantum entropy.\n\nQuantum information theory is largely concerned with the interpretation and uses of von Neumann entropy. The von Neumann entropy is the cornerstone in the development of quantum information theory, while the Shannon entropy applies to classical information theory. This is considered a historical anomaly, as it might have been expected that Shannon entropy was discovered prior to Von Neuman entropy, given the latter's more widespread application to quantum information theory. However, the historical reverse occurred. Von Neumann first discovered von Neumann entropy, and applied it to questions of statistical physics. Decades later, Shannon developed an information-theoretic formula for use in classical information theory, and asked von Neumann what to call it, with von Neumman telling him to call it Shannon entropy, as it was a special case of von Neumann entropy.\n\nThe formalism of density operators and matrices was introduced by von Neumann in 1927 and independently, but less systematically by Lev Landau and Felix Bloch in 1927 and 1946 respectively. The density matrix is an alternative way in which to represent the state of a quantum system, which could otherwise be represented using the wavefunction. The density matrix allows the solution of certain time-dependent problems in quantum mechanics.\n\nThe von Neumann measurement scheme, the ancestor of quantum decoherence theory, represents measurements projectively by taking into account the measuring apparatus which is also treated as a quantum object. The 'projective measurement' scheme introduced by von Neumann, led to the development of quantum decoherence theories.\n\nVon Neumann first proposed a quantum logic in his 1932 treatise \"Mathematical Foundations of Quantum Mechanics\", where he noted that projections on a Hilbert space can be viewed as propositions about physical observables. The field of quantum logic was subsequently inaugurated, in a famous paper of 1936 by von Neumann and Garrett Birkhoff, the first work ever to introduce quantum logics, wherein von Neumann and Birkhoff first proved that quantum mechanics requires a propositional calculus substantially different from all classical logics and rigorously isolated a new algebraic structure for quantum logics. The concept of creating a propositional calculus for quantum logic was first outlined in a short section in von Neumann's 1932 work, but in 1936, the need for the new propositional calculus was demonstrated through several proofs. For example, photons cannot pass through two successive filters that are polarized perpendicularly (\"e.g.\", one horizontally and the other vertically), and therefore, \"a fortiori\", it cannot pass if a third filter polarized diagonally is added to the other two, either before or after them in the succession, but if the third filter is added \"in between\" the other two, the photons will, indeed, pass through. This experimental fact is translatable into logic as the \"non-commutativity\" of conjunction formula_3. It was also demonstrated that the laws of distribution of classical logic, formula_4 and formula_5, are not valid for quantum theory.\n\nThe reason for this is that a quantum disjunction, unlike the case for classical disjunction, can be true even when both of the disjuncts are false and this is, in turn, attributable to the fact that it is frequently the case, in quantum mechanics, that a pair of alternatives are semantically determinate, while each of its members are necessarily indeterminate. This latter property can be illustrated by a simple example. Suppose we are dealing with particles (such as electrons) of semi-integral spin (spin angular momentum) for which there are only two possible values: positive or negative. Then, a principle of indetermination establishes that the spin, relative to two different directions (e.g., \"x\" and \"y\") results in a pair of incompatible quantities. Suppose that the state ɸ of a certain electron verifies the proposition \"the spin of the electron in the \"x\" direction is positive.\" By the principle of indeterminacy, the value of the spin in the direction \"y\" will be completely indeterminate for ɸ. Hence, ɸ can verify neither the proposition \"the spin in the direction of \"y\" is positive\" nor\nthe proposition \"the spin in the direction of \"y\" is negative.\" Nevertheless, the disjunction of the propositions \"the spin in the direction of \"y\" is positive or the spin in the direction of \"y\" is negative\" must be true for ɸ.\nIn the case of distribution, it is therefore possible to have a situation in which \"formula_6\", while formula_7.\n\nAs Hilary Putnam writes, von Neumann replaced classical logic with a logic constructed in orthomodular lattices (isomorphic to the lattice of subspaces of the Hilbert space of a given physical system).\n\nVon Neumann founded the field of game theory as a mathematical discipline. Von Neumann proved his minimax theorem in 1928. This theorem establishes that in zero-sum games with perfect information (i.e. in which players know at each time all moves that have taken place so far), there exists a pair of strategies for both players that allows each to minimize his maximum losses, hence the name minimax. When examining every possible strategy, a player must consider all the possible responses of his adversary. The player then plays out the strategy that will result in the minimization of his maximum loss.\n\nSuch strategies, which minimize the maximum loss for each player, are called optimal. Von Neumann showed that their minimaxes are equal (in absolute value) and contrary (in sign). Von Neumann improved and extended the minimax theorem to include games involving imperfect information and games with more than two players, publishing this result in his 1944 \"Theory of Games and Economic Behavior\" (written with Oskar Morgenstern). Morgenstern wrote a paper on game theory and thought he would show it to von Neumann because of his interest in the subject. He read it and said to Morgenstern that he should put more in it. This was repeated a couple of times, and then von Neumann became a coauthor and the paper became 100 pages long. Then it became a book. The public interest in this work was such that \"The New York Times\" ran a front-page story. In this book, von Neumann declared that economic theory needed to use functional analytic methods, especially convex sets and topological fixed-point theorem, rather than the traditional differential calculus, because the maximum-operator did not preserve differentiable functions.\n\nIndependently, Leonid Kantorovich's functional analytic work on mathematical economics also focused attention on optimization theory, non-differentiability, and vector lattices. Von Neumann's functional-analytic techniques—the use of duality pairings of real vector spaces to represent prices and quantities, the use of supporting and separating hyperplanes and convex sets, and fixed-point theory—have been the primary tools of mathematical economics ever since.\n\nVon Neumann raised the intellectual and mathematical level of economics in several influential publications. For his model of an expanding economy, von Neumann proved the existence and uniqueness of an equilibrium using his generalization of the Brouwer fixed-point theorem. Von Neumann's model of an expanding economy considered the matrix pencil \" A − λB\" with nonnegative matrices A and B; von Neumann sought probability vectors \"p\" and \"q\" and a positive number \"λ\" that would solve the complementarity equation\nalong with two inequality systems expressing economic efficiency. In this model, the (transposed) probability vector \"p\" represents the prices of the goods while the probability vector q represents the \"intensity\" at which the production process would run. The unique solution \"λ\" represents the growth factor which is 1 plus the rate of growth of the economy; the rate of growth equals the interest rate.\n\nVon Neumann's results have been viewed as a special case of linear programming, where von Neumann's model uses only nonnegative matrices. The study of von Neumann's model of an expanding economy continues to interest mathematical economists with interests in computational economics. This paper has been called the greatest paper in mathematical economics by several authors, who recognized its introduction of fixed-point theorems, linear inequalities, complementary slackness, and saddlepoint duality. In the proceedings of a conference on von Neumann's growth model, Paul Samuelson said that many mathematicians had developed methods useful to economists, but that von Neumann was unique in having made significant contributions to economic theory itself.\nVon Neumann's famous 9-page paper started life as a talk at Princeton and then became a paper in German, which was eventually translated into English. His interest in economics that led to that paper began as follows: When lecturing at Berlin in 1928 and 1929 he spent his summers back home in Budapest, and so did the economist Nicholas Kaldor, and they hit it off. Kaldor recommended that von Neumann read a book by the mathematical economist Léon Walras. Von Neumann found some faults in that book and corrected them, for example, replacing equations by inequalities. He noticed that Walras's General Equilibrium Theory and Walras' Law, which led to systems of simultaneous linear equations, could produce the absurd result that the profit could be maximized by producing and selling a negative quantity of a product. He replaced the equations by inequalities, introduced dynamic equilibria, among other things, and eventually produced the paper.\n\nBuilding on his results on matrix games and on his model of an expanding economy, von Neumann invented the theory of duality in linear programming, after George Dantzig described his work in a few minutes, when an impatient von Neumann asked him to get to the point. Then, Dantzig listened dumbfounded while von Neumann provided an hour lecture on convex sets, fixed-point theory, and duality, conjecturing the equivalence between matrix games and linear programming.\n\nLater, von Neumann suggested a new method of linear programming, using the homogeneous linear system of Gordan (1873), which was later popularized by Karmarkar's algorithm. Von Neumann's method used a pivoting algorithm between simplices, with the pivoting decision determined by a nonnegative least squares subproblem with a convexity constraint (projecting the zero-vector onto the convex hull of the active simplex). Von Neumann's algorithm was the first interior point method of linear programming.\n\nVon Neumann made fundamental contributions to mathematical statistics. In 1941, he derived the exact distribution of the ratio of the mean square of successive differences to the sample variance for independent and identically normally distributed variables. This ratio was applied to the residuals from regression models and is commonly known as the Durbin–Watson statistic for testing the null hypothesis that the errors are serially independent against the alternative that they follow a stationary first order autoregression.\n\nSubsequently, Denis Sargan and Alok Bhargava extended the results for testing if the errors on a regression model follow a Gaussian random walk (\"i.e.\", possess a unit root) against the alternative that they are a stationary first order autoregression.\n\nVon Neumann made fundamental contributions in exploration of problems in numerical hydrodynamics. For example, with Robert D. Richtmyer he developed an algorithm defining \"artificial viscosity\" that improved the understanding of shock waves. When computers solved hydrodynamic or aerodynamic problems, they tried to put too many computational grid points at regions of sharp discontinuity (shock waves). The mathematics of \"artificial viscosity\" smoothed the shock transition without sacrificing basic physics. Von Neumann's other contributions to fluid dynamics included the discovery of the classic flow solution to blast waves, and the co-discovery (independently of Yakov Borisovich Zel'dovich and Werner Döring) of the ZND detonation model of explosives. During the 1930s, Von Neumann became an authority on the mathematics of shaped charges.\n\nStan Ulam, who knew von Neumann well, described his mastery of mathematics this way: \"Most mathematicians know one method. For example, Norbert Wiener had mastered Fourier transforms. Some mathematicians have mastered two methods and might really impress someone who knows only one of them. John von Neumann had mastered three methods.\" He went on to explain that the three methods were:\nEdward Teller wrote that \"Nobody knows all science, not even von Neumann did. But as for mathematics, he contributed to every part of it except number theory and topology. That is, I think, something unique.\"\n\nVon Neumann was asked to write an essay for the layman describing what mathematics is, and produced a beautiful analysis. He explained that mathematics straddles the world between the empirical and logical, arguing that geometry was originally empirical, but Euclid constructed a logical, deductive theory. However, he argued, that there is always the danger of straying too far from the real world and becoming irrelevant sophistry.\n\nBeginning in the late 1930s, von Neumann developed an expertise in explosions—phenomena that are difficult to model mathematically. During this period, von Neumann was the leading authority of the mathematics of shaped charges. This led him to a large number of military consultancies, primarily for the Navy, which in turn led to his involvement in the Manhattan Project. The involvement included frequent trips by train to the project's secret research facilities at the Los Alamos Laboratory in a remote part of New Mexico.\n\nVon Neumann made his principal contribution to the atomic bomb in the concept and design of the explosive lenses that were needed to compress the plutonium core of the Fat Man weapon that was later dropped on Nagasaki. While von Neumann did not originate the \"implosion\" concept, he was one of its most persistent proponents, encouraging its continued development against the instincts of many of his colleagues, who felt such a design to be unworkable. He also eventually came up with the idea of using more powerful shaped charges and less fissionable material to greatly increase the speed of \"assembly\".\n\nWhen it turned out that there would not be enough uranium-235 to make more than one bomb, the implosive lens project was greatly expanded and von Neumann's idea was implemented. Implosion was the only method that could be used with the plutonium-239 that was available from the Hanford Site. He established the design of the explosive lenses required, but there remained concerns about \"edge effects\" and imperfections in the explosives. His calculations showed that implosion would work if it did not depart by more than 5% from spherical symmetry. After a series of failed attempts with models, this was achieved by George Kistiakowsky, and the construction of the Trinity bomb was completed in July 1945.\n\nIn a visit to Los Alamos in September 1944, von Neumann showed that the pressure increase from explosion shock wave reflection from solid objects was greater than previously believed if the angle of incidence of the shock wave was between 90° and some limiting angle. As a result, it was determined that the effectiveness of an atomic bomb would be enhanced with detonation some kilometers above the target, rather than at ground level.\nVon Neumann, four other scientists, and various military personnel were included in the target selection committee that was responsible for choosing the Japanese cities of Hiroshima and Nagasaki as the first targets of the atomic bomb. Von Neumann oversaw computations related to the expected size of the bomb blasts, estimated death tolls, and the distance above the ground at which the bombs should be detonated for optimum shock wave propagation and thus maximum effect. The cultural capital Kyoto, which had been spared the bombing inflicted upon militarily significant cities, was von Neumann's first choice, a selection seconded by Manhattan Project leader General Leslie Groves. However, this target was dismissed by Secretary of War Henry L. Stimson.\n\nOn July 16, 1945, von Neumann and numerous other Manhattan Project personnel were eyewitnesses to the first test of an atomic bomb detonation, which was code-named Trinity. The event was conducted as a test of the implosion method device, at the bombing range near Alamogordo Army Airfield, southeast of Socorro, New Mexico. Based on his observation alone, von Neumann estimated the test had resulted in a blast equivalent to but Enrico Fermi produced a more accurate estimate of 10 kilotons by dropping scraps of torn-up paper as the shock wave passed his location and watching how far they scattered. The actual power of the explosion had been between 20 and 22 kilotons. It was in von Neumann's 1944 papers that the expression \"kilotons\" appeared for the first time. After the war, Robert Oppenheimer remarked that the physicists involved in the Manhattan project had \"known sin\". Von Neumann's response was that \"sometimes someone confesses a sin in order to take credit for it.\"\n\nVon Neumann continued unperturbed in his work and became, along with Edward Teller, one of those who sustained the hydrogen bomb project. He collaborated with Klaus Fuchs on further development of the bomb, and in 1946 the two filed a secret patent on \"Improvement in Methods and Means for Utilizing Nuclear Energy\", which outlined a scheme for using a fission bomb to compress fusion fuel to initiate nuclear fusion. The Fuchs–von Neumann patent used radiation implosion, but not in the same way as is used in what became the final hydrogen bomb design, the Teller–Ulam design. Their work was, however, incorporated into the \"George\" shot of Operation Greenhouse, which was instructive in testing out concepts that went into the final design. The Fuchs–von Neumann work was passed on to the Soviet Union by Fuchs as part of his nuclear espionage, but it was not used in the Soviets' own, independent development of the Teller–Ulam design. The historian Jeremy Bernstein has pointed out that ironically, \"John von Neumann and Klaus Fuchs, produced a brilliant invention in 1946 that could have changed the whole course of the development of the hydrogen bomb, but was not fully understood until after the bomb had been successfully made.\"\n\nFor his wartime services, von Neumann was awarded the Navy Distinguished Civilian Service Award in July 1946, and the Medal for Merit in October 1946.\n\nIn 1950, von Neumann became a consultant to the Weapons Systems Evaluation Group (WSEG), whose function was to advise the Joint Chiefs of Staff and the United States Secretary of Defense on the development and use of new technologies. He also became an adviser to the Armed Forces Special Weapons Project (AFSWP), which was responsible for the military aspects on nuclear weapons. Over the following two years, he became a consultant to the Central Intelligence Agency (CIA), a member of the influential General Advisory Committee of the Atomic Energy Commission, a consultant to the newly established Lawrence Livermore National Laboratory, and a member of the Scientific Advisory Group of the United States Air Force.\n\nIn 1955, von Neumann became a commissioner of the AEC. He accepted this position and used it to further the production of compact hydrogen bombs suitable for Intercontinental ballistic missile delivery. He involved himself in correcting the severe shortage of tritium and lithium 6 needed for these compact weapons, and he argued against settling for the intermediate-range missiles that the Army wanted. He was adamant that H-bombs delivered into the heart of enemy territory by an ICBM would be the most effective weapon possible, and that the relative inaccuracy of the missile wouldn't be a problem with an H-bomb. He said the Russians would probably be building a similar weapon system, which turned out to be the case. Despite his disagreement with Oppenheimer over the need for a crash program to develop the hydrogen bomb, he testified on the latter's behalf at the 1954 Oppenheimer security hearing, at which he asserted that Oppenheimer was loyal, and praised him for his helpfulness once the program went ahead.\n\nShortly before his death from cancer, von Neumann headed the United States government's top secret ICBM committee, which would sometimes meet in his home. Its purpose was to decide on the feasibility of building an ICBM large enough to carry a thermonuclear weapon. Von Neumann had long argued that while the technical obstacles were sizable, they could be overcome in time. The SM-65 Atlas passed its first fully functional test in 1959, two years after his death. The feasibility of an ICBM owed as much to improved, smaller warheads as it did to developments in rocketry, and his understanding of the former made his advice invaluable.\n\nVon Neumann is credited with developing the equilibrium strategy of mutual assured destruction (MAD). He also \"moved heaven and earth\" to bring MAD about. His goal was to quickly develop ICBMs and the compact hydrogen bombs that they could deliver to the USSR, and he knew the Soviets were doing similar work because the CIA interviewed German rocket scientists who were allowed to return to Germany, and von Neumann had planted a dozen technical people in the CIA. The Russians considered that bombers would soon be vulnerable, and they shared von Neumann's view that an H-bomb in an ICBM was the ne plus ultra of weapons; they believed that whoever had superiority in these weapons would take over the world, without necessarily using them. He was afraid of a \"missile gap\" and took several more steps to achieve his goal of keeping up with the Soviets:\nVon Neumann's assessment that the Soviets had a lead in missile technology, considered pessimistic at the time, was soon proven correct in the Sputnik crisis.\n\nVon Neumann entered government service primarily because he felt that, if freedom and civilization were to survive, it would have to be because the United States would triumph over totalitarianism from Nazism, Fascism and Soviet Communism. During a Senate committee hearing he described his political ideology as \"violently anti-communist, and much more militaristic than the norm\". He was quoted in 1950 remarking, \"If you say why not bomb [the Soviets] tomorrow, I say, why not today? If you say today at five o'clock, I say why not one o'clock?\"\n\nOn February 15, 1956, von Neumann was presented with the Medal of Freedom by President Dwight D. Eisenhower. His citation read:\n\nVon Neumann was a founding figure in computing. Donald Knuth cites von Neumann as the inventor, in 1945, of the merge sort algorithm, in which the first and second halves of an array are each sorted recursively and then merged.\nVon Neumann wrote the 23 pages long sorting program for the EDVAC in ink. On the first page, traces of the phrase \"TOP SECRET\", which was written in pencil and later erased, can still be seen. He also worked on the philosophy of artificial intelligence with Alan Turing when the latter visited Princeton in the 1930s.\n\nVon Neumann's hydrogen bomb work was played out in the realm of computing, where he and Stanisław Ulam developed simulations on von Neumann's digital computers for the hydrodynamic computations. During this time he contributed to the development of the Monte Carlo method, which allowed solutions to complicated problems to be approximated using random numbers. His algorithm for simulating a fair coin with a biased coin is used in the \"software whitening\" stage of some hardware random number generators. Because using lists of \"truly\" random numbers was extremely slow, von Neumann developed a form of making pseudorandom numbers, using the middle-square method. Though this method has been criticized as crude, von Neumann was aware of this: he justified it as being faster than any other method at his disposal, writing that \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\" Von Neumann also noted that when this method went awry it did so obviously, unlike other methods which could be subtly incorrect.\n\nWhile consulting for the Moore School of Electrical Engineering at the University of Pennsylvania on the EDVAC project, von Neumann wrote an incomplete \"First Draft of a Report on the EDVAC\". The paper, whose premature distribution nullified the patent claims of EDVAC designers J. Presper Eckert and John Mauchly, described a computer architecture in which the data and the program are both stored in the computer's memory in the same address space. This architecture is the basis of most modern computer designs, unlike the earliest computers that were \"programmed\" using a separate memory device such as a paper tape or plugboard. Although the single-memory, stored program architecture is commonly called von Neumann architecture as a result of von Neumann's paper, the architecture was based on the work of Eckert and Mauchly, inventors of the ENIAC computer at the University of Pennsylvania.\n\nJohn von Neumann consulted for the Army's Ballistic Research Laboratory, most notably on the ENIAC project, as a member of its Scientific Advisory Committee.\nThe electronics of the new ENIAC ran at one-sixth the speed, but this in no way degraded the ENIAC's performance, since it was still entirely I/O bound. Complicated programs could be developed and debugged in days rather than the weeks required for plugboarding the old ENIAC. Some of von Neumann's early computer programs have been preserved.\n\nThe next computer that von Neumann designed was the IAS machine at the Institute for Advanced Study in Princeton, New Jersey. He arranged its financing, and the components were designed and built at the RCA Research Laboratory nearby. John von Neumann recommended that the IBM 701, nicknamed \"the defense computer\", include a magnetic drum. It was a faster version of the IAS machine and formed the basis for the commercially successful IBM 704.\nStochastic computing was first introduced in a pioneering paper by von Neumann in 1953.\nHowever, the theory could not be implemented until advances in computing of the 1960s.\n\nVon Neumann's rigorous mathematical analysis of the structure of self-replication (of the semiotic relationship between constructor, description and that which is constructed), preceded the discovery of the structure of DNA.\n\nVon Neumann created the field of cellular automata without the aid of computers, constructing the first self-replicating automata with pencil and graph paper.\nThe detailed proposal for a physical non-biological self-replicating system was first put forward in lectures Von Neumann delivered in 1948 and 1949, when he first only proposed a kinematic self-reproducing automaton. While qualitatively sound, von Neumann was evidently dissatisfied with this model of a self-replicator due to the difficulty of analyzing it with mathematical rigor. He went on to instead develop a more abstract model self-replicator based on his original concept of cellular automata.\n\nSubsequently, the concept of the Von Neumann universal constructor based on the von Neumann cellular automaton was fleshed out in his posthumously published lectures \"Theory of Self Reproducing Automata\". \nUlam and von Neumann created a method for calculating liquid motion in the 1950s. The driving concept of the method was to consider a liquid as a group of discrete units and calculate the motion of each based on its neighbors' behaviors. Like Ulam's lattice network, von Neumann's cellular automata are two-dimensional, with his self-replicator implemented algorithmically. The result was a universal copier and constructor working within a cellular automaton with a small neighborhood (only those cells that touch are neighbors; for von Neumann's cellular automata, only orthogonal cells), and with 29 states per cell. Von Neumann gave an existence proof that a particular pattern would make infinite copies of itself within the given cellular universe by designing a 200,000 cell configuration that could do so. \n\nVon Neumann addressed the evolutionary growth of complexity amongst his self-replicating machines. His \"proof-of-principle\" designs showed how it is logically possible, by using a general purpose programmable (\"universal\") constructor, to exhibit an indefinitely large class of self-replicators, spanning a wide range of complexity, interconnected by a network of potential mutational pathways, including pathways from the most simple to the most complex. This is an important result, as prior to that it might have been conjectured that there is a fundamental logical barrier to the existence of such pathways; in which case, biological organisms, which do support such pathways, could not be \"machines\", as conventionally understood. Von Neumman considers the potential for conflict between his self-reproducing machines, stating that \"our models lead to such conflict situations\", indicating it as a field of further study.\n\nThe cybernetics movement highlighted the question of what it takes for self-reproduction to occur autonomously, and in 1952, John von Neumann designed an elaborate 2D cellular automaton that would automatically make a copy of its initial configuration of cells. The von Neumann neighborhood, in which each cell in a two-dimensional grid has the four orthogonally adjacent grid cells as neighbors, continues to be used for other cellular automata. Von Neumann proved that the most effective way of performing large-scale mining operations such as mining an entire moon or asteroid belt would be by using self-replicating spacecraft, taking advantage of their exponential growth.\n\nVon Neumann investigated the question of whether modelling evolution on a digital computer could solve the complexity problem in programming.\n\nBeginning in 1949, von Neumann's design for a self-reproducing computer program is considered the world's first computer virus, and he is considered to be the theoretical father of computer virology.\n\nVon Neumann's team performed the world's first numerical weather forecasts on the ENIAC computer; von Neumann published the paper \"Numerical Integration of the Barotropic Vorticity Equation\" in 1950.\nVon Neumann's interest in weather systems and meteorological prediction led him to propose manipulating the environment by spreading colorants on the polar ice caps to enhance absorption of solar radiation (by reducing the albedo), thereby inducing global warming. Von Neumann was a scientist to propose the theory of global warming, noting that the Earth was only colder during the last glacial period, he said that the burning of coal and oil would result in \"a general warming of the Earth by about one degree Fahrenheit.\"\n\nOther mathematicians were stunned by von Neumann's ability to instantaneously perform complex operations in his head. As a six-year-old, he could divide two eight-digit numbers in his head. When he was sent at the age of 15 to study advanced calculus under analyst Gábor Szegő, Szegő was so astounded with the boy's talent in mathematics that he was brought to tears on their first meeting.\n\nNobel Laureate Hans Bethe said \"I have sometimes wondered whether a brain like von Neumann's does not indicate a species superior to that of man\". Seeing von Neumann's mind at work, Eugene Wigner wrote, \"one had the impression of a perfect instrument whose gears were machined to mesh accurately to a thousandth of an inch.\" Paul Halmos states that \"von Neumann's speed was awe-inspiring.\" Israel Halperin said: \"Keeping up with him was ... impossible. The feeling was you were on a tricycle chasing a racing car.\" Edward Teller admitted that he \"never could keep up with him\". Teller also said \"von Neumann would carry on a conversation with my 3-year-old son, and the two of them would talk as equals, and I sometimes wondered if he used the same principle when he talked to the rest of us.\" Peter Lax wrote \"Von Neumann was addicted to thinking, and in particular to thinking about mathematics\".\n\nWhen George Dantzig brought von Neumann an unsolved problem in linear programming \"as I would to an ordinary mortal\", on which there had been no published literature, he was astonished when von Neumann said \"Oh, that!\", before offhandedly giving a lecture of over an hour, explaining how to solve the problem using the hitherto unconceived theory of duality.\n\nLothar Wolfgang Nordheim described von Neumann as the \"fastest mind I ever met\", and Jacob Bronowski wrote \"He was the cleverest man I ever knew, without exception. He was a genius.\" George Pólya, whose lectures at ETH Zürich von Neumann attended as a student, said \"Johnny was the only student I was ever afraid of. If in the course of a lecture I stated an unsolved problem, the chances were he'd come to me at the end of the lecture with the complete solution scribbled on a slip of paper.\" Eugene Wigner writes: \"'Jancsi,' I might say, 'Is angular momentum always an integer of \"h\"?' He would return a day later with a decisive answer: 'Yes, if all particles are at rest.'... We were all in awe of Jancsi von Neumann\".\n\nHalmos recounts a story told by Nicholas Metropolis, concerning the speed of von Neumann's calculations, when somebody asked von Neumann to solve the famous fly puzzle:\n\nEugene Wigner told a similar story, only with a swallow instead of a fly, and says it was Max Born who posed the question to von Neumann in the 1920s.\n\nVon Neumann was also noted for his eidetic memory (sometimes called photographic memory). Herman Goldstine wrote: \n\nVon Neumann was reportedly able to memorize the pages of telephone directories. He entertained friends by asking them to randomly call out page numbers; he then recited the names, addresses and numbers therein.\n\n\"It seems fair to say that if the influence of a scientist is interpreted broadly enough to include impact on fields beyond science proper, then John von Neumann was probably the most influential mathematician who ever lived,\" wrote Miklós Rédei in \"John von Neumann: Selected Letters\". James Glimm wrote: \"he is regarded as one of the giants of modern mathematics\". The mathematician Jean Dieudonné said that von Neumann \"may have been the last representative of a once-flourishing and numerous group, the great mathematicians who were equally at home in pure and applied mathematics and who throughout their careers maintained a steady production in both directions\", while Peter Lax described him as possessing the \"most scintillating intellect of this century\". In the foreword of Miklós Rédei's \"Selected Letters\", Peter Lax wrote, \"To gain a measure of von Neumann's achievements, consider that had he lived a normal span of years, he would certainly have been a recipient of a Nobel Prize in economics. And if there were Nobel Prizes in computer science and mathematics, he would have been honored by these, too. So the writer of these letters should be thought of as a triple Nobel laureate or, possibly, a -fold winner, for his work in physics, in particular, quantum mechanics\".\n\nIn 1955, von Neumann was diagnosed with what was either bone or pancreatic cancer. He was not able to accept the proximity of his own demise, and the shadow of impending death instilled great fear in him. He invited a Roman Catholic priest, Father Anselm Strittmatter, O.S.B., to visit him for consultation. Von Neumann reportedly said, \"So long as there is the possibility of eternal damnation for nonbelievers it is more logical to be a believer at the end\", essentially saying that Pascal had a point, referring to Pascal's Wager. He had earlier confided to his mother, \"There probably has to be a God. Many things are easier to explain if there is than if there isn't.\" Father Strittmatter administered the last rites to him. Some of von Neumann's friends (such as Abraham Pais and Oskar Morgenstern) said they had always believed him to be \"completely agnostic\". Of this deathbed conversion, Morgenstern told Heims, \"He was of course completely agnostic all his life, and then he suddenly turned Catholic—it doesn't agree with anything whatsoever in his attitude, outlook and thinking when he was healthy.\" Father Strittmatter recalled that even after his conversion, von Neumann did not receive much peace or comfort from it, as he still remained terrified of death.\n\nVon Neumann was on his deathbed when he entertained his brother by reciting by heart and word-for-word the first few lines of each page of Goethe's \"Faust\". He died at age 53 on February 8, 1957, at the Walter Reed Army Medical Center in Washington, D.C., under military security lest he reveal military secrets while heavily medicated. He was buried at Princeton Cemetery in Princeton, Mercer County, New Jersey.\n\n\n\nPhD students\n\n\n Books\n\nPopular periodicals\n\nVideo\n"}
{"id": "39924603", "url": "https://en.wikipedia.org/wiki?curid=39924603", "title": "Marion Cameron Gray", "text": "Marion Cameron Gray\n\nMarion Gray (26 March 1902 – 16 September 1979) was a Scottish mathematician who discovered a graph with 54 vertices and 81 edges while working at American Telephone & Telegraph. The graph is commonly known as the Gray graph.\n\nMarion Gray was born in Ayr, Scotland on 26 March 1902 to Marion (née Cameron) and James Gray. She attended Ayr Grammar School (1907–1913) and Ayr Academy (1913–1919). In 1919 she entered the University of Edinburgh where she graduated in 1922 with a first class honours in mathematics and natural philosophy. She continued on at the University for a further two years as a post doctoral student in mathematics where she was supervised by E.T. Whittaker. She joined the Edinburgh Mathematical Society where she presented several of her papers including 'The equation of telegraphy' and 'The equation of conduction of heat'. She was elected to the Committee of the Society in November 1923 and continued as a member throughout her career.\n\nIn 1924 she travelled to the United States under the assistance of both a British graduates scholarship and a Carnegie scholarship to attend Bryn Mawr College, Pennsylvania from where she gained a PhD under the supervision of Anna Johnson Pell Wheeler. Her research topic was 'A boundary value problem of ordinary self-adjoint differential equations with singularities'.\n\nAfter receiving her doctorate Gray returned to Edinburgh to take a post of university assistant in natural philosophy at the University of Edinburgh. She held the post for one year before going to London where she was an assistant in mathematics at Imperial College for three years.\n\nIn 1930 she was appointed to the post of assistant engineer at the Department of Development and Research of the American Telephone and Telegraph Company in New York. While working there she discovered an unusual cubic semi-symmetric graph graph with 54 vertices, representing the 27 points and 27 lines in a 3 × 3 × 3 three-dimensional grid, with an edge for each pair of a point and a line that meet each other. Here, \"cubic\" means that each vertex is the endpoint of three edges, and \"semi-symmetric\" means that every two edges are symmetric to each other but the same is not true for the vertices. This graph is the smallest possible cubic semi-symmetric graph. Thinking it was a theoretical discovery without practical application Gray did not publish her findings. Thirty-six years later, I. Z. Bouwer rediscovered and described the graph and explained how it could answer questions regarding types of symmetry. The graph is commonly known as the Gray graph.\n\nIn 1934 Gray joined Bell Telephone Laboratories and remained with the company for a further 30 years until her retirement.\nAs well as her own research articles Gray compiled many reviews of publications on mathematical physics and served on the US Government-related committee which produced the Handbook of Mathematical Functions. She remained an active member of various professional mathematical societies throughout her career.\n\nGray was known for her support of junior colleagues. One wrote of the time he was working in Bell Laboratories in 1957, 'In [my calculation], I was helped by a little old lady, Marion Gray, one of Bell's finest mathematicians at that time.’\n\nAfter her retirement in 1967 Gray moved back to Edinburgh where she died in 1979 aged 77 years.\n\nGray was included in the National Library of Scotland's display \"Celebrating Scottish women of science\" which ran from 1 March to 30 April 2013.\n"}
{"id": "43157170", "url": "https://en.wikipedia.org/wiki?curid=43157170", "title": "Markushevich basis", "text": "Markushevich basis\n\nIn geometry, a Markushevich basis (sometimes Markushevich bases or M-basis) is a biorthogonal system that is both \"complete\" and \"total\". It can be described by the formulation:\n\nEvery Schauder basis of a Banach space is also a Markuschevich basis; the reverse is not true in general. An example of a Markushevich basis that is not a Schauder basis can be the set\n\nin the space formula_8 of complex continuous functions in [0,1] whose values at 0 and 1 are equal, with the sup norm. It is an open problem whether or not every separable Banach space admits a Markushevich basis with formula_9 for all formula_10.\n"}
{"id": "4036008", "url": "https://en.wikipedia.org/wiki?curid=4036008", "title": "Method ringing", "text": "Method ringing\n\nMethod ringing (also known as scientific ringing) is a form of change ringing in which the ringers commit to memory the rules for generating each change of sequence, and pairs of bells are affected. This creates a form of bell music which is continually changing, but which cannot be discerned as a conventional melody. It is a way of sounding continually changing mathematical permutations.\n\nIt is distinct from call changes, where the ringers are instructed how to generate each new change by calls from a conductor, and normally only two adjacent bells swap their position at each change.\n\nIn method ringing, the ringers are guided from permutation to permutation by following the rules of a \"method.\" The underlying mathematical basis is intimately linked to group theory. The basic building block of method ringing is Plain hunt.\n\nThe first method, Grandsire, was designed around 1650, probably by Robert Roan who became master of the College Youths change ringing society in 1652. Details of the method on five bells appeared in print in 1668 in \"Tintinnalogia\", (Fabian Stedman with Richard Duckworth) and \"Campanalogia\" (1677 – written solely by Stedman) which are the first two publications on the subject.\n\nThe practice originated in England and remains most popular there today; in addition to bells in church towers, it is also often performed on handbells.\n\nIn method ringing, plain hunt is the simplest form of generating changing permutations in a continuous fashion, and is a fundamental building-block of change ringing methods.\nIt consists of a plain undeviating course of a bell between the first and last places in the striking order, with two strikes in the first and last position to enable a turn-around.\nThus each bell moves one position at each succeeding change, unless they reach the first or last position, when they remain there for two changes then proceed to the other end of the sequence.\n\nThis simple rule can be extended to any number of bells.\n\nPlain hunting is limited to a small number of possible different changes, which is numerically equal to twice the number of bells that are hunting. However, by introducing deviations from the plain hunt, by causing some of the bells to change their relationship to the others, change ringing \"methods\" were developed. These allow a large range of possible different changes to be rung; even to the extent of the full factorial sequence of changes.\n\nGrandsire, the oldest change ringing method is based on a simple deviation to the plain hunt when the treble (bell No.1) is first in the sequence or it is said to \"lead\". The treble is known as the \"hunt bell\" because it hunts continuously without ever deviating from the path. The diagram for the plain course is shown here.\n\nThe Grandsire variation on the plain hunt on odd numbers adds a second hunt bell, which is \"coursing\" the treble: that is, the second hunt bell takes its place at the front of the change immediately after the treble. The single deviation away from hunting for the rest of the bells now takes place as the two hunt bells change places at the front of the lead.\n\nFurthermore, because there are two hunt bells, not the second bell but the third remains in place:\n\nThis forces a dodge on the other bells in 4/5 positions. After this the bells immediately return to the plain hunt pattern until the next treble lead.\n\nThis rule can now be extended to any number of odd bells in changes, making Grandsire an easily extendable method. The hunt bell is changed many times during such ringing to enable the full factorial number of changes to be achieved.\n\n\"Plain Bob\" is one of the oldest change ringing and simplest of these, first named \"Grandsire Bob\". It has the added flexibility over Grandsire of being able to expand logically to both odd and even numbers of bells; also, The deviations when a plain course is extended with \"calls\", are much simpler,\n\nA \"plain course\" of plain bob minor is shown in diagrammatic form, which has the characteristics;\n\nThe red bell track shows the order of \"works\", which are deviations from the plain hunt.\nAnd then it repeats. Each bells starts at a different place in this cyclical order. \nA dodge means just that; two bells dodge round each other, thus changing their relationship to the treble, and giving rise to different changes.\n\nThe plain bob pattern can be extended beyond the constraints of the plain course, to the full unique 720 changes possible ( this is factorial 6 on 6 bells, which is 1×2×3×4×5×6 = 720 changes). To do this, at set points in the sequences one of the ringers, called the \"conductor\" calls out commands such as \"bob\" or \"single\", which introduce further variations. The conductor follows a \"composition\" which they have to commit to memory. This enables the other ringers to produce large numbers of unique changes without memorising huge quantities of data, without any written prompts.\n\nRingers can also ring different methods, with different \"works\" – so there is a huge variety of ways of ringing method changes.\n\nThe highest bell in pitch is known as the \"treble\" and the lowest the \"tenor\". The majority of bell towers have the ring of bells (or ropes) going clockwise from the treble. For convenience, the bells are referred to by number, with the treble being number 1 and the other bells numbered by their pitch (2, 3, 4, etc.) sequentially down the scale. The bells are usually tuned to a diatonic major scale, with the tenor bell being the tonic (or key) note of the scale.\n\nThe simplest way to use a set of bells is ringing \"rounds\", which is sounding the bells repeatedly in sequence from treble to tenor: 1, 2, 3, etc.. (Musicians will recognise this as a portion of a descending scale.) Ringers typically start with rounds and then begin to vary the bells' order, moving on to a series of distinct \"rows\". Each row (or \"change\") is a specific permutation of the bells (for example 123456 or 531246)—that is to say, it includes each bell rung once and only once, the difference from row to row being the order in which the bells follow one another. Plain hunt is the simplest way of creating bell permutations, or changes.\n\nSince permutations are involved, it is natural that for some people the ultimate theoretical goal of change ringing is to ring the bells in every possible permutation; this is called an \"extent\" (in the past this was sometimes referred to as a \"full peal\"). For a method on formula_1 bells, there are formula_2 (read factorial) possible permutations, a number which quickly grows as formula_1 increases. For example, while on six bells there are 720 permutations, on 8 bells there are 40,320; furthermore, 10! = 3,628,800, and 12! = 479,001,600.\n\nEstimating two seconds for each change (a reasonable pace), we find that while an extent on 6 bells can be accomplished in half an hour, a full peal on 8 bells should take nearly twenty-two and a half hours and one on 12 bells would take over thirty years! Naturally, then, except in towers with only a few bells, ringers typically can only ring a subset of the available permutations. But the key stricture of an extent, uniqueness (any row may only be rung once), is considered essential. This is called \"truth\"; to repeat any row would make the performance \"false\".\n\nAnother key limitation keeps a given bell from moving up or back more than a single place from row to row; if it rings (for instance) fourth in one row, in the next row it can only ring third, fourth, or fifth. Thus from row to row each bell either keeps its place or swaps places with one of its neighbours. This rule has its origins in the physical reality of tower bells: a bell, swinging through a complete revolution with every row, has considerable inertia and the ringer has only a limited ability to accelerate or retard its cycle.\n\nA third key rule mandates \"rounds\" as the start and end of all ringing. So to summarize: any performance must start out from rounds, visit a number of other rows (whether all possible permutations or just a subset thereof) but only once each, and then return safely to rounds, all the while making only small neighbour-swaps from row to row. These rules dramatically limit the options open to a method-maker.\n\nFor example, consider a tower with four bells. An extent includes 4! = 24 changes and there are, naturally, 24! possible orders in which to ring each change once, which is about 6.2 × 10. But once we limit ourselves to neighbour-swaps and to starting and ending with rounds, only 10,792 possible extents remain.\n\nIt is to navigate this complex terrain that various \"methods\" have been developed; they allow the ringers to plot their course ahead of time without needing to memorize it all (an impossible task) or to read it off a numbingly repetitive list of numbers. Instead, by combining a pattern short and simple enough for ringers to memorize with a few regular breaking points where simple variations can be introduced, a robust algorithm is formed. This is the essence of method ringing.\n\nA lead is part of the plain course. It commences when the method starts and lasts until the treble gets back to the same place. In the diagram of \"Plain Bob Minor\" shown, the lead starts when the treble rings in second place and lasts until the treble has rung twice at lead. It is common practice in diagrams to draw a line under the lead end to assist in understanding the method. Most methods have a plain course consisting of a number of leads where the pattern is the same, but different bells are in differing places. In the diagram given, the number 4 bell rings the same pattern as the number 2, but one lead earlier.\n\nIn \"principles\" (where the treble does the same work as other bells and is affected by calls) the definition of a lead can become more complex.\n\nTo obtain more changes than available in the plain course, a \"conductor\" makes a \"call\" directing the ringers to make a slight variation in the course. (The most common calls are called \"bobs\" and \"singles\".) These variations usually last only one change, but cause two or more ringers to swap their paths, whereupon they continue with the normal pattern. By introducing such calls appropriately, repetition can be avoided, with the peal remaining \"true\" over a large number of changes. For example, an extent in a minor method is 720 (6!) changes, so would require 12 repetitions of the plain course shown.\n\nTo know when to make calls and which ones to make, a conductor follows a plan called a \"composition\" which he or someone else devised; if properly constructed it will ensure a true performance of the desired length. Today computers make checking a composition's truth easy; but the process once involved a mix of mathematics and laborious row-by-row checking.\n\nProbably the greatest composer of the 20th century was Albert J Pitman, who composed over a hundred peals between 1910 and 1965, entirely by hand. None of his compositions was then, nor since, discovered to be false.\n\nAs well as writing out the changes longhand (as in the accompanying illustration of Plain Bob Minor) there is a shorthand called \"Place Notation\". For each row in which all bells change place, such as the first change, use an \"x\" or a \"-\". In rows where one or more bells stay in place write down the \"place numbers\" which do not change, so that the second row is written \"16\". Plain Bob Minor is therefore x16x16x16x16x16x12.\n\nMany methods are symmetrical, and so only the first half lead is given, along with possibly the lead end. Plain Bob Minor is thus: x16x16x16 le:12. Where two changes consisting of numbers follow each other, use a dot to separate them. Plain Bob Doubles (i.e. on 5 bells) is: 5.1.5.1.5 le:125, or if written at full length 5.1.5.1.5.1.5.1.5.125.\n\nMethods are generally referred to by an official name assigned to them by the Central Council of Church Bell Ringers; such names have three standard parts: the method's name proper, its \"class\", and its \"stage\".\n\nThe name proper is the method's personal name. The oldest methods have long-established names; but new methods are constantly being devised and rung, and the Central Council generally allows each to be named by the band which first rings a peal in it. Most often these methods end up with a place name, such as the band's village; but people's names and still more fanciful inventions are not uncommon.\n\nThe \"class\" describes the method, putting it in some established category of methods that work in similar ways. Methods in the simplest category omit this second name and use a simple two-part name.\n\nThe \"stage\" indicates the number of bells, using unique terminology:\n\nAs can be seen, there are different naming systems for even- and odd-bell stages. The odd-bell stage names refer to the number of possible swaps that can be made from row to row; in \"caters\" and \"cinques\" can be seen the French numbers \"quatre\" and \"cinq\" while the stage name for three-bell ringing is indeed \"singles\". Higher odd-bell stages follow the same pattern (\"sextuples\", \"septuples\", etc.) while higher even-bell stages have more prosaic names: \"fourteen\", \"sixteen\", etc.).\n\nNote that the names refer to the number of bells being permuted, which is not necessarily the same as the number being rung: for it is typical to ring triples methods not on seven bells but on eight, with the tenor \"covering\": only the seven highest bells permute; the eighth and lowest bell is simply rung last in every row. So likewise with caters, usually rung on ten bells, and other higher odd-bell stages.\n\nPut together, this system gives method names sound that is evocative, musical, and quaint: \"Kent Treble Bob Major\", \"Grandsire Caters\", \"Erin Triples\", \"Chartres Delight Royal\", \"Percy's Tea Strainer Treble Place Major\", \"Titanic Cinques\" and so forth.\n\nA short composition, lasting perhaps only a few hundred changes, is called a \"touch\", which got its name from the 16th-century expression a \"touch\" of music, meaning \"a brief piece of instrumental music\".; However many ringers look forward to the greater challenge of a \"quarter peal\" (about 1,250 changes) or a \"peal\" (about 5,000 changes), which is referred to as a \"Performance\".\n\nThis number derives from the great 17th-century quest to ring a full extent on seven bells; 7 factorial is 5,040. Sturdier bellframes and more clearly understood methods make the task easier today; but a peal still needs about 3 hours of labour and concentration.\n\nMost ringers follow the definition of a peal as regulated by the Central Council. This requires a minimum of only 5,000 changes where major or a higher stage is being rung; but demands at least the full 5,040 changes on lower stages. For triples, this ensures at least a full extent; for lower stages a full extent falls well short of the goal and ringers must complete several full extents to reach 5,040 (working out mathematically to at least 7 extents on six bells, at least 42 on five, or at least 210 on four; three-bell peals are not recognised by the Central Council).\n\nTo qualify as a peal, the ringing must meet a number of other key criteria. Among other things, each bell must be rung continuously by the same person; a ringing band cannot swap in a person to give ringers an occasional break. Likewise the ringing must be done entirely from memory; ringers cannot consult the method's blue line nor can the conductor (who must be one of the ringers) have a written reminder of the composition.\n\nMore commonly rung is the quarter peal, typically consisting of 1,260 changes and typically taking 45 minutes to ring. Half peals are more rarely rung, but have been known. One example is in Buckfast Abbey in Devon, where there are two half peal boards.\n\nRelationship of extents and peals\n\n\n\n"}
{"id": "2921675", "url": "https://en.wikipedia.org/wiki?curid=2921675", "title": "Michael D. Morley", "text": "Michael D. Morley\n\nMichael Darwin Morley (born 1930) is an American mathematician, currently professor emeritus at Cornell University.\nHis research is in advanced mathematical logic and model theory, and he is best known for Morley's categoricity theorem, which he proved in his Ph.D. thesis \"Categoricity in Power\" in 1962.\n\nHis formal Ph.D. advisor was Saunders MacLane at the University of Chicago, but he actually finished his thesis under the guidance of Robert Vaught at the University of California, Berkeley.\n\n\n"}
{"id": "44764591", "url": "https://en.wikipedia.org/wiki?curid=44764591", "title": "Nucleus (order theory)", "text": "Nucleus (order theory)\n\nIn mathematics, and especially in order theory, a nucleus is a function formula_1 on a meet-semilattice formula_2 such that (for every formula_3 in formula_2):\n\n\nEvery nucleus is evidently a monotone function.\n\nUsually, the term \"nucleus\" is used in frames and locales theory (when the semilattice formula_2 is a frame).\n\nProposition: If formula_1 is a nucleus on a frame formula_2, then the poset formula_11 of fixed points of formula_1, with order inherited from formula_2, is also a frame.\n"}
{"id": "22656", "url": "https://en.wikipedia.org/wiki?curid=22656", "title": "Operand", "text": "Operand\n\nIn mathematics an operand is the object of a mathematical operation, i.e. it is the object or quantity that is operated on.\n\nThe following arithmetic expression shows an example of operators and operands:\n\nIn the above example, '+' is the symbol for the operation called addition. \n\nThe operand '3' is one of the inputs (quantities) followed by the addition operator, and the operand '6' is the other input necessary for the operation.\n\nThe result of the operation is 9. (The number '9' is also called the sum of the augend 3 and the addend 6.)\n\nAn operand, then, is also referred to as \"one of the inputs (quantities) for an operation\".\n\nOperands may be complex, and may consist of expressions also made up of operators with operands.\n\nIn the above expression '(3 + 5)' is the first operand for the multiplication operator and '2' the second. The operand '(3 + 5)' is an expression in itself, which contains an addition operator, with the operands '3' and '5'.\n\nRules of precedence affect which values form operands for which operators:\n\nIn the above expression, the multiplication operator has the higher precedence than the addition operator, so the multiplication operator has operands of '5' and '2'. The addition operator has operands of '3' and '5 × 2'.\n\nDepending on the mathematical notation being used the position of an operator in relation to its operand(s) may vary. In everyday usage infix notation is the most common, however other notations also exist, such as the prefix and postfix notations. These alternate notations are most common within computer science.\n\nBelow is a comparison of three different notations — all represent an addition of the numbers '1' and '2'\n\nIn a mathematical expression, the order of operation is carried out from left to right. Start with the left most value and seek the first operation to be carried out in accordance with the order specified above (i.e., start with parentheses and end with the addition/subtraction group). For example, in the expression\n\nthe first operation to be acted upon is any and all expressions found inside a parenthesis. So beginning at the left and moving to the right, find the first (and in this case, the only) parenthesis, that is, (2 + 2). Within the parenthesis itself is found the expression 2. The reader is required to find the value of 2 before going any further. The value of 2 is 4. Having found this value, the remaining expression looks like this:\n\nThe next step is to calculate the value of expression inside the parenthesis itself, that is, (2 + 4) = 6. Our expression now looks like this:\n\nHaving calculated the parenthetical part of the expression, we start over again beginning with the left most value and move right. The next order of operation (according to the rules) is exponents. Start at the left most value, that is, 4, and scan your eyes to the right and search for the first exponent you come across. The first (and only) expression we come across that is expressed with an exponent is 2. We find the value of 2, which is 4. What we have left is the expression\n\nThe next order of operation is multiplication. 4 × 4 is 16. Now our expression looks like this:\n\nThe next order of operation according to the rules is division. However, there is no division operator sign (÷) in the expression, 16 − 6. So we move on to the next order of operation, i.e., addition and subtraction, which have the same precedence and are done left to right.\n\nSo the correct value for our original expression, 4 × 2 − (2 + 2), is 10. \n\nIt is important to carry out the order of operation in accordance with rules set by convention. If the reader evaluates an expression but does not follow the correct order of operation, the reader will come forth with a different value. The different value will be the incorrect value because the order of operation was not followed. The reader will arrive at the correct value for the expression if and only if each operation is carried out in the proper order.\n\nThe number of operands of an operator is called its arity. Based on arity, operators are classified as nullary (no operands), unary (1 operand), binary (2 operands), ternary (3 operands) etc.\n\nIn computer programming languages, the definitions of operator and operand are almost the same as in mathematics.\n\nIn computing, an operand is the part of a computer instruction which specifies what data is to be manipulated or operated on, while at the same time representing the data itself.\nA computer instruction describes an operation such as add or multiply X, while the operand (or operands, as there can be more than one) specify on which X to operate as well as the value of X.\n\nAdditionally, in assembly language, an operand is a value (an argument) on which the instruction, named by mnemonic, operates. The operand may be a processor register, a memory address, a literal constant, or a label. A simple example (in the x86 architecture) is\n\nMOV DS, AX\n\nwhere the value in register operand codice_1 is to be moved (codice_2) into register codice_3. Depending on the instruction, there may be zero, one, two, or more operands.\n\n"}
{"id": "7740082", "url": "https://en.wikipedia.org/wiki?curid=7740082", "title": "Parry–Sullivan invariant", "text": "Parry–Sullivan invariant\n\nIn mathematics, the Parry–Sullivan invariant (or Parry–Sullivan number) is a numerical quantity of interest in the study of incidence matrices in graph theory, and of certain one-dimensional dynamical systems. It provides a partial classification of non-trivial irreducible incidence matrices.\n\nIt is named after the English mathematician Bill Parry and the American mathematician Dennis Sullivan, who introduced the invariant in a joint paper published in the journal \"Topology\" in 1975.\n\nLet \"A\" be an \"n\" × \"n\" incidence matrix. Then the Parry–Sullivan number of \"A\" is defined to be\n\nwhere \"I\" denotes the \"n\" × \"n\" identity matrix.\n\nIt can be shown that, for nontrivial irreducible incidence matrices, flow equivalence is completely determined by the Parry–Sullivan number and the Bowen–Franks group.\n"}
{"id": "56108", "url": "https://en.wikipedia.org/wiki?curid=56108", "title": "Penrose triangle", "text": "Penrose triangle\n\nThe Penrose triangle, also known as the Penrose tribar, or the impossible tribar, is a triangular impossible object. It was first created by the Swedish artist Oscar Reutersvärd in 1934. The psychiatrist Lionel Penrose and his mathematician son Roger Penrose independently devised and popularized it in the 1950s, describing it as \"impossibility in its purest form\". It is featured prominently in the works of artist M. C. Escher, whose earlier depictions of impossible objects partly inspired it.\n\nThe tribar appears to be a solid object, made of three straight beams of square cross-section which meet pairwise at right angles at the vertices of the triangle they form. The beams may be broken, forming cubes or cuboids.\nThis combination of properties cannot be realized by any three-dimensional object in ordinary Euclidean space. Such an object can exist in certain Euclidean 3-manifolds. There also exist three-dimensional solid shapes each of which, when viewed from a certain angle, appears the same as the 2-dimensional depiction of the Penrose triangle on this page (such as – for example – the adjacent image depicting a sculpture in Perth, Australia). The term \"Penrose triangle\" can refer to the 2-dimensional depiction or the impossible object itself.\n\nM.C. Escher's lithograph \"Waterfall\" (1961) depicts a watercourse that flows in a zigzag along the long sides of two elongated Penrose triangles, so that it ends up two stories higher than it began. The resulting waterfall, forming the short sides of both triangles, drives a water wheel. Escher helpfully points out that in order to keep the wheel turning some water must occasionally be added to compensate for evaporation.\n\nIf a line is traced around the Penrose triangle, a 4-loop Möbius strip is formed..\n\nAlthough the tribar is named one of the impossible objects, there exist many more that fit into the same category. Other impossible objects include the devil's fork, the dancing elephant, and impossible arch.\n\nWhile it is possible to construct analogies to the Penrose triangle with other shapes and regular polygons to create a Penrose polygon, the visual effect is not as striking, and as the sides increase, the object seems merely to be warped or twisted.\n\n\n"}
{"id": "4519252", "url": "https://en.wikipedia.org/wiki?curid=4519252", "title": "Phase portrait", "text": "Phase portrait\n\nA phase portrait is a geometric representation of the trajectories of a dynamical system in the phase plane. Each set of initial conditions is represented by a different curve, or point.\n\nPhase portraits are an invaluable tool in studying dynamical systems. They consist of a plot of typical trajectories in the state space. This reveals information such as whether an attractor, a repellor or limit cycle is present for the chosen parameter value. The concept of topological equivalence is important in classifying the behaviour of systems by specifying when two different phase portraits represent the same qualitative dynamic behavior. An attractor is a stable point which is also called 'sink'. The repellor is considered as an unstable point, which is also known as 'source'. \n\nA phase portrait graph of a dynamical system depicts the system's trajectories (with arrows) and stable steady states (with dots) and unstable steady states (with circles) in a state space. The axes are of state variables.\n\n\nA phase portrait represents the directional behavior of a system of ODEs. The phase portrait can indicate the stability of the system. \n\nThe phase portrait behavior of a system of ODEs can be determined by the eigenvalues or the trace and determinant (trace = λ + λ, determinant = λ x λ) of the system.\n\n\n"}
{"id": "51411", "url": "https://en.wikipedia.org/wiki?curid=51411", "title": "Probability and statistics", "text": "Probability and statistics\n\nProbability and Statistics or also called Statistics and Probability are two related but separate academic disciplines. Statistical analysis often uses probability distributions, and the two topics are often studied together. However, probability theory contains much that is mostly of mathematical interest and not directly relevant to statistics. Moreover, many topics in statistics are independent of probability theory. \n\n\n"}
{"id": "19595579", "url": "https://en.wikipedia.org/wiki?curid=19595579", "title": "Q-Racah polynomials", "text": "Q-Racah polynomials\n\nIn mathematics, the \"q\"-Racah polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme, introduced by . give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \nThey are sometimes given with changes of variables as\n\nq-Racah polynomials→Racah polynomials\n\n"}
{"id": "28209555", "url": "https://en.wikipedia.org/wiki?curid=28209555", "title": "R. Leonard Brooks", "text": "R. Leonard Brooks\n\nRowland Leonard Brooks (February 6, 1916 – June 18, 1993) was an English mathematician, known for proving Brooks's theorem on the relation between the chromatic number and the degree of graphs. He was born in Lincolnshire, England, studied at Trinity College, Cambridge University, and also worked with fellow Trinity students W. T. Tutte, Cedric Smith, and Arthur Harold Stone on the problem of \"Squaring the square\" (partitioning rectangles and squares into unequal squares), both under their own names and under the pseudonym Blanche Descartes.\n\nAfter leaving Cambridge, he worked as a tax inspector.\n"}
{"id": "337457", "url": "https://en.wikipedia.org/wiki?curid=337457", "title": "Raoul Bott", "text": "Raoul Bott\n\nRaoul Bott, (September 24, 1923 – December 20, 2005) was a Hungarian-American mathematician known for numerous basic contributions to geometry in its broad sense. He is best known for his Bott periodicity theorem, the Morse–Bott functions which he used in this context, and the Borel–Bott–Weil theorem.\n\nBott was born in Budapest, Hungary, the son of Margit Kovács and Rudolph Bott. His father was of Austrian descent, and his mother was of Hungarian Jewish descent; Bott was raised a Catholic by his mother and stepfather. Bott grew up in Czechoslovakia and spent his working life in the United States. His family emigrated to Canada in 1938, and subsequently he served in the Canadian Army in Europe during World War II.\n\nBott later went to college at McGill University in Montreal, where he studied electrical engineering. He then earned a Ph.D. in mathematics from Carnegie Mellon University in Pittsburgh in 1949. His thesis, titled \"Electrical Network Theory\", was written under the direction of Richard Duffin. Afterward, he began teaching at the University of Michigan in Ann Arbor. Bott continued his study at the Institute for Advanced Study in Princeton. He was a professor at Harvard University from 1959 to 1999. In 2005 Bott died of cancer in San Diego.\n\nWith Richard Duffin at Carnegie Mellon, Bott studied existence of electronic filters corresponding to given positive-real functions. In 1949 they proved a fundamental theorem of filter synthesis. Duffin and Bott extended earlier work by Otto Brune that requisite functions of complex frequency \"s\" could be realized by a passive network of inductors and capacitors. The proof, relying on induction on the sum of the degrees of the polynomials in the numerator and denominator of the rational function, was published in Journal of Applied Physics, volume 20, page 816.\nIn his 2000 interview with Allyn Jackson of the American Mathematical Society, he explained that he sees \"networks as discrete versions of harmonic theory\", so his experience with network synthesis and electronic filter topology introduced him to algebraic topology.\n\nBott met Arnold S. Shapiro at the IAS and they worked together.\nHe studied the homotopy theory of Lie groups, using methods from Morse theory, leading to the Bott periodicity theorem (1957). In the course of this work, he introduced Morse–Bott functions, an important generalization of Morse functions.\n\nThis led to his role as collaborator over many years with Michael Atiyah, initially via the part played by periodicity in K-theory. Bott made important contributions towards the index theorem, especially in formulating related fixed-point theorems, in particular the so-called 'Woods Hole fixed-point theorem', a combination of the Riemann–Roch theorem and Lefschetz fixed-point theorem (it is named after Woods Hole, Massachusetts, the site of a conference at which collective discussion formulated it). The major Atiyah–Bott papers on what is now the Atiyah–Bott fixed-point theorem were written in the years up to 1968; they collaborated further in recovering in contemporary language Ivan Petrovsky on Petrovsky lacunas of hyperbolic partial differential equations, prompted by Lars Gårding. In the 1980s, Atiyah and Bott investigated gauge theory, using the Yang–Mills equations on a Riemann surface to obtain topological information about the moduli spaces of stable bundles on Riemann surfaces. In 1983 he spoke to the Canadian Mathematical Society in a talk he called \"A topologist marvels at Physics\".\n\nHe is also well known in connection with the Borel–Bott–Weil theorem on representation theory of Lie groups via holomorphic sheaves and their cohomology groups; and for work on foliations.\n\nHe introduced Bott–Samelson varieties and the Bott residue formula for complex manifolds and the Bott cannibalistic class.\n\nIn 1964, he was awarded the Oswald Veblen Prize in Geometry by the American Mathematical Society. In 1983, he was awarded the Jeffery–Williams Prize by the Canadian Mathematical Society. In 1987, he was awarded the National Medal of Science.\n\nIn 2000, he received the Wolf Prize. In 2005, he was elected an Overseas Fellow of the Royal Society of London.\n\nBott had 35 Ph.D. students, including Stephen Smale, Lawrence Conlon, Daniel Quillen, Peter Landweber, Robert MacPherson, Robert W. Brooks, Robin Forman, András Szenes, and Kevin Corlette.\n\n\n"}
{"id": "23195101", "url": "https://en.wikipedia.org/wiki?curid=23195101", "title": "Retirement spend-down", "text": "Retirement spend-down\n\nAt retirement, individuals stop working and no longer get employment earnings, and enter a phase of their lives, where they rely on the assets they have accumulated, to supply money for their spending needs for the rest of their lives. Retirement spend-down, or withdrawal rate, is the strategy a retiree follows to spend, decumulate or withdraw assets during retirement.\n\nRetirement planning aims to prepare individuals for retirement spend-down, because the different spend-down approaches available to retirees depend on the decisions they make during their working years. Actuaries and financial planners are experts on this topic.\n\nMore than 10,000 Post-World War II baby boomers will retire in the United States every day between now and 2027. This represents the majority of the more than 78 million Americans born between 1946 and 1964. 74% of these people are expected to be alive in 2030, which highlights that most of them will live for many years beyond retirement. By the year 2000, 1 in every 14 people was age 65 or older. By the year 2050, more than 1 in 6 people are projected to be at least 65 years old. The following statistics emphasize the importance of a well-planned retirement spend-down strategy for these people:\n\n\nIndividuals each have their own retirement aspirations, but all retirees face longevity risk – the risk of outliving their assets. This can spell financial disaster. Avoiding this risk is therefore a baseline goal that any successful retirement spend-down strategy addresses. Generally, longevity risk is greatest for low and middle income individuals.\n\nThe probabilities of a 65-year-old living to various ages are:\n\nLongevity risk is largely underestimated. Most retirees do not expect to live beyond age 85, let alone into their 90s. A 2007 study of recently retired individuals asked them to rank the following risks in order of the level of concern they present:\n\nLongevity risk was ranked as the least concerning of these risks.\n\nA portion of retirement income often comes from savings; colloquially, \"tapping into the Nest Egg\". These present a number of variables:\nOften, investments will change through retirement, becoming more conservative as one ages, but often continuing to hold risky investments, in the hope of investment gains. A number of approaches exist in target date funds, for instance.\n\nA common rule of thumb for withdrawal rate is 4%, based on 20th century American investment returns, and first articulated in , and one conclusion of the Trinity study (1998). This particular rule and approach have been heavily criticized, as have the methods of both sources, with critics arguing that withdrawal rates should vary with investment style (which they do in Bengen) and returns, and that this ignores the risk of emergencies and rising expenses (e.g., medical or long-term care). Others question the suitability of matching relatively fixed expenses with risky investment assets.\n\nNew dynamic adjustment methods for retirement withdrawal rates have been developed after Bengen's 4% withdrawal rate was proposed: constant inflation-adjusted spending, Bengen's floor-and-ceiling rule, and Guyton and Klinger's decision rules. More complex withdrawal strategies have also been created.\n\nTo decide a withdrawal rate, history shows the maximum sustainable inflation-adjusted withdrawal rate over rolling 30-year periods for three hypothetical stock and bond portfolios from 1926–2014. Stocks are represented by the S&P 500 Index, bonds by an index of five-year U.S. Treasury bonds. During the best 30-year period withdrawal rates of 10% annually could be used with a 100% success rate. The worst 30-year period had a maximum withdrawal rate of 3.5%. A 4% withdrawal rate survived most 30 year periods. The higher the stock allocation the higher rate of success. A portfolio of 75% stocks is more volatile but had higher maximum withdrawal rates. Starting with a withdrawal rate near 4% and a minimum 50% equity allocation in retirement gave a higher probability of success in historical 30 year periods.\n\nIndividuals may receive retirement income from a variety of sources:\n\n\nEach has unique risk, eligibility, tax, timing, form of payment, and distribution considerations that should be integrated into a retirement spend-down strategy.\n\nTraditional retirement spend-down approaches generally take the form of a gap analysis. Essentially, these tools collect a variety of input variables from an individual and use them to project the likelihood that the individual will meet specified retirement goals. They model the shortfall or surplus between the individual's retirement income and expected spending needs to identify whether the individual has adequate resources to retire at a particular age. Depending on their sophistication, they may be stochastic (often incorporating Monte Carlo simulation) or deterministic.\n\n\"Standard input variables\"\n\n\n\"Additional input variables that can enhance model sophistication\"\n\n\n\"Output\"\n\n\nThere are three primary approaches utilized to estimate an individual's spending needs in retirement:\n\nMarket volatility can have a significant impact on both a worker's retirement preparedness and a retiree's retirement spend-down strategy. The global financial crisis of 2008–2009 provides an example. American workers lost an estimated $2 trillion in retirement savings during this time frame. 54% of workers lost confidence in their ability to retire comfortably due the direct impact of the market turmoil on their retirement savings.\n\nAsset allocation contributed significantly to these issues. Basic investment principles recommend that individuals reduce their equity investment exposure as they approach retirement. Studies show, however, that 43% of 401(k) participants had equity exposure in excess of 70% at the beginning of 2008.\n\nWorld Pensions Council (WPC) financial economists have argued that durably low interest rates in most G20 countries will have an adverse impact on the underfunding condition of pension funds as \"without returns that outstrip inflation, pension investors face the real value of their savings declining rather than ratcheting up over the next few years\" \n\nFrom 1982 until 2011, most Western economies experienced a period of low inflation combined with relatively high returns on investments across all asset classes including government bonds. This brought a certain sense of complacency amongst some pension actuarial consultants and regulators, making it seem reasonable to use optimistic economic assumptions to calculate the present value of future pension liabilities.\n\nThe potentially long-lasting collapse in returns on government bonds is taking place against the backdrop of a protracted fall in returns for other core-assets such as blue chip stocks, and, more importantly, a silent demographic shock. Factoring in the corresponding longevity risk, pension premiums could be raised significantly while disposable incomes stagnate and employees work longer years before retiring.\n\nLongevity risk becomes more of a concern for individuals when their retirement savings are depleted by asset losses. Following the market downturn of 2008–09, 61% of working baby boomers are concerned about outliving their retirement assets. Traditional spend-down approaches generally recommend three ways they can attempt to address this risk:\n\n\nSaving more and investing more aggressively are difficult strategies for many individuals to implement due to constraints imposed by current expenses or an aversion to increased risk. Most individuals also are averse to lowering their standard of living. The closer individuals are to retirement, the more drastic these measures must be for them to have a significant impact on the individuals' retirement savings or spend-down strategies.\n\nIndividuals tend to have significantly more control over their retirement ages than they do over their savings rates, asset returns, or expenses. As a result, postponing retirement can be an attractive option for individuals looking to enhance their retirement preparedness or recover from investment losses. The relative impact that delaying retirement can have on an individual's retirement spend-down is dependent upon specific circumstances, but research has shown that delaying retirement from age 62 to age 66 can increase an average worker's retirement income by 33%.\n\nPostponing retirement minimizes the probability of running out of retirement savings in several ways:\n\n\nStudies show that nearly half of all workers expect to delay their retirement because they have accumulated fewer retirement assets than they had planned. Much of this is attributable to the market downturn of 2008–2009. Various unforeseen circumstances cause nearly half of all workers to retire earlier than they intend. In many cases, these individuals intend to work part-time during retirement. Again, however, statistics show that this is far less common than intentions would suggest.\n\nThe appeal of retirement age flexibility is the focal point of an actuarial approach to retirement spend-down that has spawned in response to the surge of baby boomers approaching retirement. The approach is based on a supply and demand model where supply and demand represent the following, which vary across different retirement ages:\n\n\nUnder this approach, individuals can specify a desired probability of retirement spend-down success. Unlike traditional spend-down approaches, retirement age is an output. It is identified by the intersection of the supply and demand curves. This framework allows individuals to quantify the impact of any adjustments to their financial circumstances on their optimal retirement age.\n\nMost approaches to retirement spend-down can be likened to individual asset/liability modeling. Regardless of the strategy employed, they seek to ensure that individuals' assets available for retirement are sufficient to fund their post-retirement liabilities and expenses. This is elaborated in dedicated portfolio theory.\n\n\n"}
{"id": "53559537", "url": "https://en.wikipedia.org/wiki?curid=53559537", "title": "SKEW", "text": "SKEW\n\nSKEW is the ticker symbol for the CBOE Skew Index, a measure of the perceived tail risk of the distribution of S&P 500 investment returns over a 30-day horizon.\nThe index values are calculated and published by the Chicago Board Options Exchange (CBOE) based on current S&P 500 options market data.\n\nSKEW is similar to the VIX index, but instead of measuring implied volatility based on a normal distribution, measures an implied risk of future returns realizing outlier behavior. The index model defines such an outlier as two or more standard deviations below the mean, which would characterize a black swan event or market crash. The index value typically reflects trading activity of portfolio managers hedging tail risk with options, to protect portfolios from a large, sudden decline in the market. A SKEW value of 100 indicates the options market perceives a low risk of outlier returns; values increasing above 100 reflect an increased perception of risk for future outlier event(s).\n\n\n"}
{"id": "897724", "url": "https://en.wikipedia.org/wiki?curid=897724", "title": "Schreier's lemma", "text": "Schreier's lemma\n\nIn mathematics, Schreier's lemma is a theorem in group theory used in the Schreier–Sims algorithm and also for finding a presentation of a subgroup. \n\nSuppose formula_1 is a subgroup of formula_2, which is finitely generated with generating set formula_3, that is, \"G\" = formula_4. \n\nLet formula_5 be a right transversal of formula_1 in formula_2. In other words, formula_5 is (the image of) a section of the quotient map formula_9, where formula_10 denotes the set of right cosets of formula_1 in formula_2.\n\nWe make the definition that given formula_13∈formula_2, formula_15 is the chosen representative in the transversal formula_5 of the coset formula_17, that is, \n\nThen formula_1 is generated by the set\n\nLet us establish the evident fact that the group Z = Z/3Z is indeed cyclic. Via Cayley's theorem, Z is a subgroup of the symmetric group \"S\". Now,\nwhere formula_23 is the identity permutation. Note \"S\" = formula_24{ \"s\"=(1 2), \"s\" = (1 2 3) }formula_25.\n\nZ has just two cosets, Z and \"S\" \\ Z, so we select the transversal { \"t\" = \"e\", \"t\"=(1 2) }, and we have \n\nFinally, \n\nThus, by Schreier's subgroup lemma, { e, (1 2 3) } generates Z, but having the identity in the generating set is redundant, so we can remove it to obtain another generating set for Z, { (1 2 3) } (as expected).\n\n"}
{"id": "53340420", "url": "https://en.wikipedia.org/wiki?curid=53340420", "title": "Single-shot multi-contrast X-ray imaging", "text": "Single-shot multi-contrast X-ray imaging\n\nSingle-shot multi-contrast x-ray imaging is an efficient and a robust x-ray imaging technique which is used to obtain three different and complimentary information, i.e. absorption, scattering, and phase contrast from a single exposure of x-rays on a detector subsequently utilizing Fourier analysis/technique. Absorption is mainly due to the attenuation and Compton scattering from the object, while phase contrast corresponds to phase shift of x-rays.\n\nThe technique obtain images from the biological and non-biological objects. The research purposes include radiography, scattering imaging, differential phase contrast, and diffraction imaging. It is also possible to adjust and modify the experiment based on what information is of most importance. Almost every application that utilize this technique have the same approach, mathematics and science behind it such as the experimental setup,complementary information and Fourier analysis.\n\nSingle-shot multi-contrast x-ray imaging gained its importance recently in contrast to Talbot–Lau interferometer because of the less optical element such as diffraction gratings being used under it and hence obtaining every information digitally.\n\nSpatial harmonic method\n\nInterferometry-based setups\n\nHybrid detectors and coded apertures\n"}
{"id": "1975821", "url": "https://en.wikipedia.org/wiki?curid=1975821", "title": "Skew lines", "text": "Skew lines\n\nIn three-dimensional geometry, skew lines are two lines that do not intersect and are not parallel. A simple example of a pair of skew lines is the pair of lines through opposite edges of a regular tetrahedron. Two lines that both lie in the same plane must either cross each other or be parallel, so skew lines can exist only in three or more dimensions. Two lines are skew if and only if they are not coplanar.\n\nIf four points are chosen at random uniformly within a unit cube, they will almost surely define a pair of skew lines. After the first three points have been chosen, the fourth point will define a non-skew line if, and only if, it is coplanar with the first three points. However, the plane through the first three points forms a subset of measure zero of the cube, and the probability that the fourth point lies on this plane is zero. If it does not, the lines defined by the points will be skew.\n\nSimilarly, in three-dimensional space a very small perturbation of any two parallel or intersecting lines will almost certainly turn them into skew lines. Therefore, any four points in general position always form skew lines.\n\nIn this sense, skew lines are the \"usual\" case, and parallel or intersecting lines are special cases.\n\nIf each line in a pair of skew lines is defined by two points that it passes through, then these four points must not be coplanar, so they must be the vertices of a tetrahedron of nonzero volume. Conversely, any two pairs of points defining a tetrahedron of nonzero volume also define a pair of skew lines. Therefore, a test of whether two pairs of points define skew lines is to apply the formula for the volume of a tetrahedron in terms of its four vertices. Denoting one point as the 1×3 vector whose three elements are the point's three coordinate values, and likewise denoting , , and for the other points, we can check if the line through and is skew to the line through and by seeing if the tetrahedron volume formula gives a non-zero result:\n\nTo calculate the distance between two skew lines the lines may be expressed using vectors:\n\nHere the 1×3 vector represents an arbitrary point on the line through particular point with representing the direction of the line and with the value of the real number formula_4 determining where the point is on the line, and similarly for arbitrary point on the line through particular point in direction .\n\nThe cross product of b and d is perpendicular to the lines, as is the unit vector\n\nThe distance between the lines is then\n\n(if |b × d| is zero the lines are parallel and this method cannot be used).\n\nExpressing the two lines as vectors:\n\nLine 1: formula_7\n\nLine 2: formula_8\n\nThe cross product of formula_9 and formula_10 is perpendicular to the lines.\n\nformula_11\n\nThe plane formed by the translations of Line 2 along formula_12 contains the point formula_13 and is perpendicular to formula_14.\n\nTherefore, the intersecting point of Line 1 with the above-mentioned plane, which is also the point on Line 1 that is nearest to Line 2 is given by\n\nformula_15\n\nSimilarly, the point on Line 2 nearest to Line 1 is given by (where formula_16\n\nformula_17\n\nNow, formula_18 and formula_19 form the shortest line segment joining Line 1 and Line 2.\n\nA \"configuration\" of skew lines is a set of lines in which all pairs are skew. Two configurations are said to be \"isotopic\" if it is possible to continuously transform one configuration into the other, maintaining throughout the transformation the invariant that all pairs of lines remain skew. Any two configurations of two lines are easily seen to be isotopic, and configurations of the same number of lines in dimensions higher than three are always isotopic, but there exist multiple non-isotopic configurations of three or more lines in three dimensions . The number of nonisotopic configurations of \"n\" lines in R, starting at \"n\" = 1, is\n\nIf one rotates a line \"L\" around another line \"M\" skew but not perpendicular to it, the surface of revolution swept out by \"L\" is a hyperboloid of one sheet. For instance, the three hyperboloids visible in the illustration can be formed in this way by rotating a line \"L\" around the central white vertical line \"M\". The copies of \"L\" within this surface form a regulus; the hyperboloid also contains a second family of lines that are also skew to \"M\" at the same distance as \"L\" from it but with the opposite angle that form the opposite regulus. The two reguli display the hyperboloid as a ruled surface.\n\nAn affine transformation of this ruled surface produces a surface which in general has an elliptical cross-section rather than the circular cross-section produced by rotating L around L'; such surfaces are also called hyperboloids of one sheet, and again are ruled by two families of mutually skew lines. A third type of ruled surface is the hyperbolic paraboloid. Like the hyperboloid of one sheet, the hyperbolic paraboloid has two families of skew lines; in each of the two families the lines are parallel to a common plane although not to each other. Any three skew lines in R lie on exactly one ruled surface of one of these types .\n\nIf three skew lines all meet three other skew lines, any transversal of the first set of three meets any transversal of the second set.\n\nIn higher-dimensional space, a flat of dimension \"k\" is referred to as a \"k\"-flat. Thus, a line may also be called a 1-flat.\n\nGeneralizing the concept of \"skew lines\" to \"d\"-dimensional space, an \"i\"-flat and a \"j\"-flat may be skew if \n. As with lines in 3-space, skew flats are those that are neither parallel nor intersect.\n\nIn affine \"d\"-space, two flats of any dimension may be parallel.\nHowever, in projective space, parallelism does not exist; two flats must either intersect or be skew.\nLet be the set of points on an \"i\"-flat, and let be the set of points on a \"j\"-flat.\nIn projective \"d\"-space, if then the intersection of and must contain a (\"i\"+\"j\"−\"d\")-flat. (A \"0\"-flat is a point.)\n\nIn either geometry, if and intersect at a \"k\"-flat, for , then the points of determine a (\"i\"+\"j\"−\"k\")-flat.\n\n\n\n"}
{"id": "3892303", "url": "https://en.wikipedia.org/wiki?curid=3892303", "title": "Tutte polynomial", "text": "Tutte polynomial\n\nThe Tutte polynomial, also called the dichromate or the Tutte–Whitney polynomial, is a graph polynomial. It is a polynomial in two variables which plays an important role in graph theory. It is defined for every undirected graph formula_1 and contains information about how the graph is connected. It is denoted by formula_2.\n\nThe importance of this polynomial stems from the information it contains about formula_1. Though originally studied in algebraic graph theory as a generalisation of counting problems related to graph coloring and nowhere-zero flow, it contains several famous other specialisations from other sciences such as the Jones polynomial from knot theory and the partition functions of the Potts model from statistical physics. It is also the source of several central computational problems in theoretical computer science.\n\nThe Tutte polynomial has several equivalent definitions. It is equivalent to Whitney’s rank polynomial, Tutte’s own dichromatic polynomial and Fortuin–Kasteleyn’s random cluster model under simple transformations. It is essentially a generating function for the number of edge sets of a given size and connected components, with immediate generalisations to matroids. It is also the most general graph invariant that can be defined by a deletion–contraction recurrence. Several textbooks about graph theory and matroid theory devote entire chapters to it.\n\nDefinition. For an undirected graph formula_4 one may define the Tutte polynomial as\n\nwhere formula_6 denotes the number of connected components of the graph formula_7. In this definition it is clear that formula_2 is well-defined and a polynomial in formula_9 and formula_10.\n\nThe same definition can be given using slightly different notation by letting formula_11 denote the rank of the graph formula_7. Then the Whitney rank generating function is defined as\n\nTheorem (Tutte).\n\nThe connection to the Tutte polynomial is given by:\n\nAt formula_19, the Tutte polynomial specialises to the all-terminal reliability polynomial studied in network theory. For a connected graph \"G\" remove every edge with probability \"p\"; this models a network subject to random edge failures. Then the reliability polynomial is a function formula_20, a polynomial in \"p\", that gives the probability that every pair of vertices in \"G\" remains connected after the edges fail. The connection to the Tutte polynomial is given by\n\nTutte also defined a closer 2-variable generalization of the chromatic polynomial, the dichromatic polynomial of a graph. This is\n\nwhere formula_6 is the number of connected components of the spanning subgraph (\"V\",\"A\"). This is related to the corank-nullity polynomial by\n\nThe dichromatic polynomial does not generalize to matroids because \"k\"(\"A\") is not a matroid property: different graphs with the same matroid can have different numbers of connected components.\n\nThe Martin polynomial formula_25 of an oriented 4-regular graph formula_26 was defined by Pierre Martin in 1977. He showed that if \"G\" is a plane graph and formula_27 is its directed medial graph, then\n\nThe deletion–contraction recurrence for the Tutte polynomial,\nimmediately yields a recursive algorithm for computing it: choose any such edge \"e\" and repeatedly apply the formula until all edges are either loops or bridges; the resulting base cases at the bottom of the evaluation are easy to compute.\n\nWithin a polynomial factor, the running time \"t\" of this algorithm can be expressed in terms of the number of vertices \"n\" and the number of edges \"m\" of the graph,\na recurrence relation that scales as the Fibonacci numbers with solution\n\nThe analysis can be improved to within a polynomial factor of the number formula_32 of spanning trees of the input graph. For sparse graphs with formula_33 this running time is formula_34. For regular graphs of degree \"k\", the number of spanning trees can be bounded by\n\nwhere\n\nso the deletion–contraction algorithm runs within a polynomial factor of this bound. For example:\n\nIn practice, graph isomorphism testing is used to avoid some recursive calls. This approach works well for graphs that are quite sparse and exhibit many symmetries; the performance of the algorithm depends on the heuristic used to pick the edge \"e\".\n\nIn some restricted instances, the Tutte polynomial can be computed in polynomial time, ultimately because Gaussian elimination efficiently computes the matrix operations determinant and Pfaffian. These algorithms are themselves important results from algebraic graph theory and statistical mechanics.\n\nformula_38 equals the number formula_32 of spanning trees of a connected graph. This is\ncomputable in polynomial time as the determinant of a maximal principal submatrix of the Laplacian matrix of \"G\", an early result in algebraic graph theory known as Kirchhoff’s Matrix–Tree theorem. Likewise, the dimension of the bicycle space at formula_40 can be computed in polynomial time by Gaussian elimination.\n\nFor planar graphs, the partition function of the Ising model, i.e., the Tutte polynomial at the hyperbola formula_41, can be expressed as a Pfaffian and computed efficiently via the FKT algorithm. This idea was developed by Fisher, Kasteleyn, and Temperley to compute the number of dimer covers of a planar lattice model.\n\nUsing a Markov chain Monte Carlo method, the Tutte polynomial can be arbitrarily well approximated along the positive branch of formula_41, equivalently, the partition function of the ferromagnetic Ising model. This exploits the close connection between the Ising model and the problem of counting matchings in a graph. The idea behind this celebrated result of Jerrum and Sinclair is to set up a Markov chain whose states are the matchings of the input graph. The transitions are defined by choosing edges at random and modifying the matching accordingly. The resulting Markov chain is rapidly mixing and leads to “sufficiently random” matchings, which can be used to recover the partition function using random sampling. The resulting algorithm is a fully polynomial-time randomized approximation scheme (fpras).\n\nSeveral computational problems are associated with the Tutte polynomial. The most straightforward one is\nIn particular, the output allows evaluating formula_45 which is equivalent to counting the number of 3-colourings of \"G\". This latter question is #P-complete, even when restricted to the family of planar graphs, so the problem of computing the coefficients of the Tutte polynomial for a given graph is #P-hard even for planar graphs.\n\nMuch more attention has been given to the family of problems called Tutteformula_46 defined for every complex pair formula_46:\nThe hardness of these problems varies with the coordinates formula_46.\n\nIf both \"x\" and \"y\" are non-negative integers, the problem formula_49 belongs to #P. For general integer pairs, the Tutte polynomial contains negative terms, which places the problem in the complexity class GapP, the closure of #P under subtraction. To accommodate rational coordinates formula_46, one can define a rational analogue of #P.\n\nThe computational complexity of exactly computing formula_49 falls into one of two classes for any formula_54. The problem is #P-hard unless formula_46 lies on the hyperbola formula_56 or is one of the points\n\nin which cases it is computable in polynomial time. If the problem is restricted to the class of planar graphs, the points on the hyperbola formula_41 become polynomial-time computable as well. All other points remain #P-hard, even for bipartite planar graphs. In his paper on the dichotomy for planar graphs, Vertigan claims (in his conclusion) that the same result holds when further restricted to graphs with vertex degree at most three, save for the point formula_59, which counts nowhere-zero Z-flows and is computable in polynomial time.\n\nThese results contain several notable special cases. For example, the problem of computing the partition function of the Ising model is #P-hard in general, even though celebrated algorithms of Onsager and Fisher solve it for planar lattices. Also, the Jones polynomial is #P-hard to compute. Finally, computing the number of four-colourings of a planar graph is #P-complete, even though the decision problem is trivial by the four colour theorem. In contrast, it is easy to see that counting the number of three-colourings for planar graphs is #P-complete because the decision problem is known to be NP-complete via a parsimonious reduction.\n\nThe question which points admit a good approximation algorithm has been very well studied. Apart from the points that can be computed exactly in polynomial time, the only approximation algorithm known for formula_49 is Jerrum and Sinclair’s FPRAS, which works for points on the “Ising” hyperbola formula_41 for \"y\" > 0. If the input graphs are restricted to dense instances, with degree formula_62, there is an FPRAS if \"x\" ≥ 1, \"y\" ≥ 1.\n\nEven though the situation is not as well understood as for exact computation, large areas of the plane are known to be hard to approximate.\n\nBollobás–Riordan polynomial\n\n\n"}
{"id": "7932644", "url": "https://en.wikipedia.org/wiki?curid=7932644", "title": "Unit tangent bundle", "text": "Unit tangent bundle\n\nIn Riemannian geometry, the unit tangent bundle of a Riemannian manifold (\"M\", \"g\"), denoted by T\"M\", UT(\"M\") or simply UT\"M\", is the unit sphere bundle for the tangent bundle T(\"M\"). It is a fiber bundle over \"M\" whose fiber at each point is the unit sphere in the tangent bundle:\n\nwhere T(\"M\") denotes the tangent space to \"M\" at \"x\". Thus, elements of UT(\"M\") are pairs (\"x\", \"v\"), where \"x\" is some point of the manifold and \"v\" is some tangent direction (of unit length) to the manifold at \"x\". The unit tangent bundle is equipped with a natural projection\n\nwhich takes each point of the bundle to its base point. The fiber \"π\"(\"x\") over each point \"x\" ∈ \"M\" is an (\"n\"−1)-sphere S, where \"n\" is the dimension of \"M\". The unit tangent bundle is therefore a sphere bundle over \"M\" with fiber S.\n\nThe definition of unit sphere bundle can easily accommodate Finsler manifolds as well. Specifically, if \"M\" is a manifold equipped with a Finsler metric \"F\" : T\"M\" → R, then the unit sphere bundle is the subbundle of the tangent bundle whose fiber at \"x\" is the indicatrix of \"F\":\n\nIf \"M\" is an infinite-dimensional manifold (for example, a Banach, Fréchet or Hilbert manifold), then UT(\"M\") can still be thought of as the unit sphere bundle for the tangent bundle T(\"M\"), but the fiber \"π\"(\"x\") over \"x\" is then the infinite-dimensional unit sphere in the tangent space.\n\nThe unit tangent bundle carries a variety of differential geometric structures. The metric on \"M\" induces a contact structure on UT\"M\". This is given in terms of a tautological one-form, defined at a point \"u\" of UT\"M\" (a unit tangent vector of \"M\") by\nwhere formula_6 is the pushforward along π of the vector \"v\" ∈ TUT\"M\". \n\nGeometrically, this contact structure can be regarded as the distribution of (2\"n\"−2)-planes which, at the unit vector \"u\", is the pullback of the orthogonal complement of \"u\" in the tangent space of \"M\". This is a contact structure, for the fiber of UT\"M\" is obviously an integral manifold (the vertical bundle is everywhere in the kernel of θ), and the remaining tangent directions are filled out by moving up the fiber of UT\"M\". Thus the maximal integral manifold of θ is (an open set of) \"M\" itself.\n\nOn a Finsler manifold, the contact form is defined by the analogous formula\nwhere \"g\" is the fundamental tensor (the hessian of the Finsler metric). Geometrically, the associated distribution of hyperplanes at the point \"u\" ∈ UT\"M\" is the inverse image under π of the tangent hyperplane to the unit sphere in T\"M\" at \"u\".\n\nThe volume form θ∧\"d\"θ defines a measure on \"M\", known as the kinematic measure, or Liouville measure, that is invariant under the geodesic flow of \"M\". As a Radon measure, the kinematic measure μ is defined on compactly supported continuous functions \"ƒ\" on UT\"M\" by\nwhere d\"V\" is the volume element on \"M\", and μ is the standard rotationally-invariant Borel measure on the Euclidean sphere UT\"M\".\n\nThe Levi-Civita connection of \"M\" gives rise to a splitting of the tangent bundle\ninto a vertical space \"V\" = kerπ and horizontal space \"H\" on which π is a linear isomorphism at each point of UT\"M\". This splitting induces a metric on UT\"M\" by declaring that this splitting be an orthogonal direct sum, and defining the metric on \"H\" by the pullback:\nand defining the metric on \"V\" as the induced metric from the embedding of the fiber UT\"M\" into the Euclidean space T\"M\". Equipped with this metric and contact form, UT\"M\" becomes a Sasakian manifold.\n"}
{"id": "23020761", "url": "https://en.wikipedia.org/wiki?curid=23020761", "title": "Variance risk premium", "text": "Variance risk premium\n\nVariance risk premium is a phenomenon on the variance swap market, of the variance swap strike being greater than the realized variance on average. For most trades, the buyer of variance ends up with a loss on the trade, while the seller profits. The amount that the buyer of variance typically loses in entering into the variance swap, is known as the variance risk premium. The variance risk premium can be naively justified by taking into account the large negative convexity of a short variance position; variance during rare times of crisis can be 50-100 times that of normal market conditions.\n\nUsing insurance as an analogy, the variance buyer typically pays a premium to be able to receive the large positive payoff of a variance swap in times of market turmoil, to \"insure\" against this.\n\nThe variance risk premium can also be analysed from the perspective of asset allocation. Carr and Wu (2007) examines whether the excess returns of selling or buying variance swaps can be explained using common factor models such as the CAPM model and the Fama-French factors, which include returns of different segments of stocks on the market. Despite the intuitive connection between stock price volatility and stock price, none of these models are able to strongly explain the excess returns on variance swaps. This implies that there is another factor that is unrelated to stock prices that affects how much, on average, one will pay to enter into a variance swap contract. This suggests that investors are willing to pay extra money to enter into variance because they dislike variance, not just because it is anti-correlated with stock prices, but on its own right. This leads to many considering variance as an asset class in and of itself.\n\nIn the years before the 2008 financial crisis, selling variance on a rolling basis was a popular trade among hedge funds and other institutional investors.\n"}
{"id": "6290771", "url": "https://en.wikipedia.org/wiki?curid=6290771", "title": "Whitehead's point-free geometry", "text": "Whitehead's point-free geometry\n\nIn mathematics, point-free geometry is a geometry whose primitive ontological notion is \"region\" rather than point. Two axiomatic systems are set out below, one grounded in mereology, the other in mereotopology and known as \"connection theory\". A point can mark a space or objects.\n\nPoint-free geometry was first formulated in Whitehead (1919, 1920), not as a theory of geometry or of spacetime, but of \"events\" and of an \"extension relation\" between events. Whitehead's purposes were as much philosophical as scientific and mathematical.\n\nWhitehead did not set out his theories in a manner that would satisfy present-day canons of formality. The two formal first order theories described in this entry were devised by others in order to clarify and refine Whitehead's theories. The domain for both theories consists of \"regions.\" All unquantified variables in this entry should be taken as tacitly universally quantified; hence all axioms should be taken as universal closures. No axiom requires more than three quantified variables; hence a translation of first order theories into relation algebra is possible. Each set of axioms has but four existential quantifiers.\n\nThe axioms G1-G7 are, but for numbering, those of Def. 2.1 in Gerla and Miranda (2008) (see also Gerla (1995)). The identifiers of the form WPn, included in the verbal description of each axiom, refer to the corresponding axiom in Simons (1987: 83).\n\nThe fundamental primitive binary relation is \"Inclusion\", denoted by infix \"≤\". (\"Inclusion\" corresponds to the binary \"Parthood\" relation that is a standard feature of all mereological theories.) The intuitive meaning of \"x\"≤\"y\" is \"\"x\" is part of \"y\".\" Assuming that identity, denoted by infix \"=\", is part of the background logic, the binary relation \"Proper Part\", denoted by infix \"<\", is defined as:\n\nformula_1\n\nThe axioms are:\n\n\n\n\n\nA model of G1–G7 is an \"inclusion space\".\n\nDefinition (Gerla and Miranda 2008: Def. 4.1). Given some inclusion space, an abstractive class is a class \"G\" of regions such that \"G\" is totally ordered by Inclusion. Moreover, there does not exist a region included in all of the regions included in \"G\".\n\nIntuitively, an abstractive class defines a geometrical entity whose dimensionality is less than that of the inclusion space. For example, if the inclusion space is the Euclidean plane, then the corresponding abstractive classes are points and lines.\n\nInclusion-based point-free geometry (henceforth \"point-free geometry\") is essentially an axiomatization of Simons's (1987: 83) system W. In turn, W formalizes a theory in Whitehead (1919) whose axioms are not made explicit. Point-free geometry is W with this defect repaired. Simons (1987) did not repair this defect, instead proposing in a footnote that the reader do so as an exercise. The primitive relation of W is Proper Part, a strict partial order. The theory of Whitehead (1919) has a single primitive binary relation \"K\" defined as \"xKy\" ↔ \"y\"<\"x\". Hence \"K\" is the converse of Proper Part. Simons's WP1 asserts that Proper Part is irreflexive and so corresponds to G1. G3 establishes that inclusion, unlike Proper Part, is anti-symmetric.\n\nPoint-free geometry is closely related to a dense linear order D, whose axioms are G1-3, G5, and the totality axiom formula_9 Hence inclusion-based point-free geometry would be a proper extension of D (namely D∪{G4, G6, G7}), were it not that the D relation \"≤\" is a total order.\n\nIn his 1929 \"Process and Reality\", A. N. Whitehead proposed a different approach, one inspired by De Laguna (1922). Whitehead took as primitive the topological notion of \"contact\" between two regions, resulting in a primitive \"connection relation\" between events. Connection theory C is a first order theory that distills the first 12 of the 31 assumptions in chpt. 2 of \"Process and Reality\" into 6 axioms, C1-C6. C is a proper fragment of the theories proposed in Clarke (1981), who noted their mereological character. Theories that, like C, feature both inclusion and topological primitives, are called mereotopologies.\n\nC has one primitive relation, binary \"connection,\" denoted by the prefixed predicate letter \"C\". That \"x\" is included in \"y\" can now be defined as \"x\"≤\"y\" ↔ ∀z[\"Czx\"→\"Czy\"]. Unlike the case with inclusion spaces, connection theory enables defining \"non-tangential\" inclusion, a total order that enables the construction of abstractive classes. Gerla and Miranda (2008) argue that only thus can mereotopology unambiguously define a point.\n\nThe axioms C1-C6 below are, but for numbering, those of Def. 3.1 in Gerla and Miranda (2008).\n\n\n\n\n\n\n\nA model of C is a \"connection space\".\n\nFollowing the verbal description of each axiom is the identifier of the corresponding axiom in Casati and Varzi (1999). Their system SMT (\"strong mereotopology\") consists of C1-C3, and is essentially due to Clarke (1981). Any mereotopology can be made atomless by invoking C4, without risking paradox or triviality. Hence C extends the atomless variant of SMT by means of the axioms C5 and C6, suggested by chpt. 2 of \"Process and Reality\". For an advanced and detailed discussion of systems related to C, see Roeper (1997).\n\nBiacino and Gerla (1991) showed that every model of Clarke's theory is a Boolean algebra, and models of such algebras cannot distinguish connection from overlap. It is doubtful whether either fact is faithful to Whitehead's intent.\n\n\n\n"}
{"id": "7455889", "url": "https://en.wikipedia.org/wiki?curid=7455889", "title": "Zero object (algebra)", "text": "Zero object (algebra)\n\nIn algebra, the zero object of a given algebraic structure is, in the sense explained below, the simplest object of such structure. As a set it is a singleton, and as a magma has a trivial structure, which is also an abelian group. The aforementioned abelian group structure is usually identified as addition, and the only element is called zero, so the object itself is typically denoted as . One often refers to \"the\" trivial object (of a specified category) since every trivial object is isomorphic to any other (under a unique isomorphism).\n\nInstances of the zero object include, but are not limited to the following:\nThese objects are described jointly not only based on the common singleton and trivial group structure, but also because of shared category-theoretical properties.\n\nIn the last three cases the scalar multiplication by an element of the base ring (or field) is defined as:\nThe most general of them, the zero module, is a finitely-generated module with an empty generating set.\n\nFor structures requiring the multiplication structure inside the zero object, such as the trivial ring, there is only one possible, , because there are no non-zero elements. This structure is associative and commutative. A ring which has both an additive and multiplicative identity is trivial if and only if , since this equality implies that for all within , \nIn this case it is possible to define division by zero, since the single element is its own multiplicative inverse. Some properties of depend on exact definition of the multiplicative identity; see the section Unital structures below.\n\nAny trivial algebra is also a trivial ring. A trivial algebra over a field is simultaneously a zero vector space considered below. Over a commutative ring, a trivial algebra is simultaneously a zero module.\n\nThe trivial ring is an example of a rng of square zero. A trivial algebra is an example of a zero algebra.\n\nThe zero-dimensional is an especially ubiquitous example of a zero object, a vector space over a field with an empty basis. It therefore has dimension zero. It is also a trivial group over addition, and a \"trivial module\" mentioned above.\n\nThe trivial ring, zero module and zero vector space are zero objects of the corresponding categories, namely Rng, -Mod and Vect.\n\nThe zero object, by definition, must be a terminal object, which means that a morphism  must exist and be unique for an arbitrary object . This morphism maps any element of  to .\n\nThe zero object, also by definition, must be an initial object, which means that a morphism  must exist and be unique for an arbitrary object . This morphism maps , the only element of , to the zero element , called the zero vector in vector spaces. This map is a monomorphism, and hence its image is isomorphic to . For modules and vector spaces, this subset  is the only empty-generated submodule (or 0-dimensional linear subspace) in each module (or vector space) .\n\nThe object is a terminal object of any algebraic structure where it exists, like it was described for examples above. But its existence and, if it exists, the property to be an initial object (and hence, a \"zero object\" in the category-theoretical sense) depend on exact definition of the multiplicative identity 1 in a specified structure.\n\nIf the definition of  requires that , then the object cannot exist because it may contain only one element. In particular, the zero ring is not a field. If mathematicians sometimes talk about a field with one element, this abstract and somewhat mysterious mathematical object is not a field.\n\nIn categories where the multiplicative identity must be preserved by morphisms, but can equal to zero, the object can exist. But not as initial object because identity-preserving morphisms from to any object where do not exist. For example, in the category of rings Ring the ring of integers Z is the initial object, not .\n\nIf an algebraic structure requires the multiplicative identity, but does not require neither its preserving by morphisms nor , then zero morphisms exist and the situation is not different from non-unital structures considered in the previous section.\n\nZero vector spaces and zero modules are usually denoted by (instead of ). This is always the case when they occur in an exact sequence.\n\n\n"}
