{"id": "407373", "url": "https://en.wikipedia.org/wiki?curid=407373", "title": "112 (number)", "text": "112 (number)\n\n112 (one hundred [and] twelve) is the natural number following 111 and preceding 113.\n\n112 is an abundant number, a heptagonal number, and a Harshad number. It is also the sum of six consecutive primes (formula_1).\n\n\n112 (emergency telephone number), used throughout the European Union and various other countries\n\n\n\n"}
{"id": "38296576", "url": "https://en.wikipedia.org/wiki?curid=38296576", "title": "Ailana Fraser", "text": "Ailana Fraser\n\nAilana Margaret Fraser is a Canadian mathematician, a professor of mathematics at the University of British Columbia. She is known for her work in geometric analysis and the theory of minimal surfaces.\n\nBorn in Toronto, Ontario, Fraser did her doctoral studies at Stanford University, receiving her Ph.D. in 1998 under the supervision of Richard Schoen. After postdoctoral studies at the Courant Institute of New York University, she taught at Brown University before moving to UBC.\n\nShe won the Krieger–Nelson Prize of the Canadian Mathematical Society in 2012, and in 2012 also became a fellow of the American Mathematical Society.\n"}
{"id": "20846707", "url": "https://en.wikipedia.org/wiki?curid=20846707", "title": "Algebraic analysis", "text": "Algebraic analysis\n\nAlgebraic analysis is an area of mathematics that deals with systems of linear partial differential equations by using sheaf theory and complex analysis to study properties and generalizations of functions such as hyperfunctions and microfunctions.\n\n\n"}
{"id": "2815449", "url": "https://en.wikipedia.org/wiki?curid=2815449", "title": "Allan Borodin", "text": "Allan Borodin\n\nAllan Bertram Borodin (born 1941) is a Canadian-American computer scientist who is a University Professor at the University of Toronto.\n\nBorodin did his undergraduate studies at Rutgers University, earning a bachelor's degree in mathematics in 1963. After earning a master's degree at the Stevens Institute of Technology in 1966 (while at the same time working P/T as a programmer at Bell Laboratories), he continued his graduate studies at Cornell University, completing a doctorate in 1969 under the supervision of Juris Hartmanis.\nHe joined the Toronto faculty in 1969 and was promoted to full professor in 1977. He served as department chair from 1980 to 1985, and became University Professor in 2011.\n\nBorodin was elected as a member of the Royal Society of Canada in 1991. In 2008 he won the CRM-Fields PIMS Prize. He became a fellow of the American Association for the Advancement of Science in 2011, and a fellow of the Association for Computing Machinery in 2014 \"For contributions to theoretical computer science in complexity, on-line algorithms, resource tradeoffs, and models of algorithmic paradigms.\"\n\n\n\n\n"}
{"id": "3300363", "url": "https://en.wikipedia.org/wiki?curid=3300363", "title": "Andreotti–Frankel theorem", "text": "Andreotti–Frankel theorem\n\nIn mathematics, the Andreotti–Frankel theorem, introduced by , states that if formula_1 is a smooth, complex affine variety of complex dimension formula_2 or, more generally, if formula_3 is any Stein manifold of dimension formula_2, then in fact formula_3 is homotopy equivalent to a CW complex of real dimension at most \"n\". In other words, formula_3 has only half as much topology.\n\nConsequently, if formula_7 is a closed connected complex submanifold of complex dimension formula_2, then formula_3 has the homotopy type of a formula_10 complex of real dimension formula_11.\nTherefore\nand\nThis theorem applies in particular to any smooth, complex affine variety of dimension formula_2.\n\n"}
{"id": "16956936", "url": "https://en.wikipedia.org/wiki?curid=16956936", "title": "Applicative universal grammar", "text": "Applicative universal grammar\n\nApplicative universal grammar, or AUG, is a universal semantic metalanguage intended for studying the semantic processes in particular languages.\nThis is a linguistic theory that views the formation of phrase in a form that is analogous to function application in an applicative programming language.\n\n\n"}
{"id": "249992", "url": "https://en.wikipedia.org/wiki?curid=249992", "title": "Binary logarithm", "text": "Binary logarithm\n\nIn mathematics, the binary logarithm () is the power to which the number must be raised to obtain the value . That is, for any real number ,\nFor example, the binary logarithm of is , the binary logarithm of is , the binary logarithm of is , and the binary logarithm of is .\n\nThe binary logarithm is the logarithm to the base . The binary logarithm function is the inverse function of the power of two function. As well as , alternative notations for the binary logarithm include , , (the notation preferred by ISO 31-11 and ISO 80000-2), and (with a prior statement that the default base is 2) .\n\nHistorically, the first application of binary logarithms was in music theory, by Leonhard Euler: the binary logarithm of a frequency ratio of two musical tones gives the number of octaves by which the tones differ. Binary logarithms can be used to calculate the length of the representation of a number in the binary numeral system, or the number of bits needed to encode a message in information theory. In computer science, they count the number of steps needed for binary search and related algorithms. Other areas\nin which the binary logarithm is frequently used include combinatorics, bioinformatics, the design of sports tournaments, and photography.\n\nBinary logarithms are included in the standard C mathematical functions and other mathematical software packages.\nThe integer part of a binary logarithm can be found using the find first set operation on an integer value, or by looking up the exponent of a floating point value.\nThe fractional part of the logarithm can be calculated efficiently.\n\nThe powers of two have been known since antiquity; for instance they appear in Euclid's \"Elements\", Props. IX.32 (on the factorization of powers of two) and IX.36 (half of the Euclid–Euler theorem, on the structure of even perfect numbers).\nAnd the binary logarithm of a power of two is just its position in the ordered sequence of powers of two.\nOn this basis, Michael Stifel has been credited with publishing the first known table of binary logarithms in 1544. His book \"Arithmetica Integra\" contains several tables that show the integers with their corresponding powers of two. Reversing the rows of these tables allow them to be interpreted as tables of binary logarithms.\n\nEarlier than Stifel, the 8th century Jain mathematician Virasena is credited with a precursor to the binary logarithm. Virasena's concept of \"ardhacheda\" has been defined as the number of times a given number can be divided evenly by two. This definition gives rise to a function that coincides with the binary logarithm on the powers of two, but it is different for other integers, giving the 2-adic order rather than the logarithm.\n\nThe modern form of a binary logarithm, applying to any number (not just powers of two) was considered explicitly by Leonhard Euler in 1739. Euler established the application of binary logarithms to music theory, long before their applications in information theory and computer science became known. As part of his work in this area, Euler published a table of binary logarithms of the integers from 1 to 8, to seven decimal digits of accuracy.\n\nThe binary logarithm function may be defined as the inverse function to the power of two function, which is a strictly increasing function over the positive real numbers and therefore has a unique inverse.\nAlternatively, it may be defined as , where is the natural logarithm, defined in any of its standard ways. Using the complex logarithm in this definition allows the binary logarithm to be extended to the complex numbers.\n\nAs with other logarithms, the binary logarithm obeys the following equations, which can be used to simplify formulas that combine binary logarithms with multiplication or exponentiation:\nFor more, see list of logarithmic identities.\n\nIn mathematics, the binary logarithm of a number is often written as . However, several other notations for this function have been used or proposed, especially in application areas.\n\nSome authors write the binary logarithm as , the notation listed in \"The Chicago Manual of Style\". Donald Knuth credits this notation to a suggestion of Edward Reingold, but its use in both information theory and computer science dates to before Reingold was active. The binary logarithm has also been written as with a prior statement that the default base for the logarithm is . Another notation that is often used for the same function (especially in the German scientific literature) is , from Latin \"logarithmus dualis\" aka \"logarithmus dyadis\".\nThe , ISO 31-11 and ISO 80000-2 standards recommend yet another notation, . According to these standards, should not be used for the binary logarithm, as it is instead reserved for the common logarithm .\n\nThe number of digits (bits) in the binary representation of a positive integer is the integral part of , i.e.\n\nIn information theory, the definition of the amount of self-information and information entropy is often expressed with the binary logarithm, corresponding to making the bit the fundamental unit of information. However, the natural logarithm and the nat are also used in alternative notations for these definitions.\n\nAlthough the natural logarithm is more important than the binary logarithm in many areas of pure mathematics such as number theory and mathematical analysis, the binary logarithm has several applications in combinatorics:\n\nThe binary logarithm also frequently appears in the analysis of algorithms, not only because of the frequent use of binary number arithmetic in algorithms, but also because binary logarithms occur in the analysis of algorithms based on two-way branching. If a problem initially has choices for its solution, and each iteration of the algorithm reduces the number of choices by a factor of two, then the number of iterations needed to select a single choice is again the integral part of . This idea is used in the analysis of several algorithms and data structures. For example, in binary search, the size of the problem to be solved is halved with each iteration, and therefore roughly iterations are needed to obtain a problem of size , which is solved easily in constant time. Similarly, a perfectly balanced binary search tree containing elements has height .\n\nThe running time of an algorithm is usually expressed in big O notation, which is used to simplify expressions by omitting their constant factors and lower-order terms. Because logarithms in different bases differ from each other only by a constant factor, algorithms that run in time can also be said to run in, say, time. The base of the logarithm in expressions such as or is therefore not important and can be omitted. However, for logarithms that appear in the exponent of a time bound, the base of the logarithm cannot be omitted. For example, is not the same as because the former is equal to and the latter to .\n\nAlgorithms with running time are sometimes called linearithmic. Some examples of algorithms with running time or are:\n\nBinary logarithms also occur in the exponents of the time bounds for some divide and conquer algorithms, such as the Karatsuba algorithm for multiplying -bit numbers in time ,\nand the Strassen algorithm for multiplying matrices in time\n. The occurrence of binary logarithms in these running times can be explained by reference to the master theorem for divide-and-conquer recurrences.\n\nIn bioinformatics, microarrays are used to measure how strongly different genes are expressed in a sample of biological material. Different rates of expression of a gene are often compared by using the binary logarithm of the ratio of expression rates: the log ratio of two expression rates is defined as the binary logarithm of the ratio of the two rates. Binary logarithms allow for a convenient comparison of expression rates: a doubled expression rate can be described by a log ratio of , a halved expression rate can be described by a log ratio of , and an unchanged expression rate can be described by a log ratio of zero, for instance.\n\nData points obtained in this way are often visualized as a scatterplot in which one or both of the coordinate axes are binary logarithms of intensity ratios, or in visualizations such as the MA plot and RA plot that rotate and scale these log ratio scatterplots.\n\nIn music theory, the interval or perceptual difference between two tones is determined by the ratio of their frequencies. Intervals coming from rational number ratios with small numerators and denominators are perceived as particularly euphonious. The simplest and most important of these intervals is the octave, a frequency ratio of . The number of octaves by which two tones differ is the binary logarithm of their frequency ratio.\n\nTo study tuning systems and other aspects of music theory that require finer distinctions between tones, it is helpful to have a measure of the size of an interval that is finer than an octave and is additive (as logarithms are) rather than multiplicative (as frequency ratios are). That is, if tones , , and form a rising sequence of tones, then the measure of the interval from to plus the measure of the interval from to should equal the measure of the interval from to . Such a measure is given by the cent, which divides the octave into equal intervals ( semitones of cents each). Mathematically, given tones with frequencies and , the number of cents in the interval from to is\nThe millioctave is defined in the same way, but with a multiplier of instead of .\n\nIn competitive games and sports involving two players or teams in each game or match, the binary logarithm indicates the number of rounds necessary in a single-elimination tournament required to determine a winner. For example, a tournament of players requires rounds to determine the winner, a tournament of teams requires rounds, etc. In this case, for players/teams where is not a power of 2, is rounded up since it is necessary to have at least one round in which not all remaining competitors play. For example, is approximately , which rounds up to , indicating that a tournament of teams requires rounds (either two teams sit out the first round, or one team sits out the second round). The same number of rounds is also necessary to determine a clear winner in a Swiss-system tournament.\n\nIn photography, exposure values are measured in terms of the binary logarithm of the amount of light reaching the film or sensor, in accordance with the Weber–Fechner law describing a logarithmic response of the human visual system to light. A single stop of exposure is one unit on a base- logarithmic scale. More precisely, the exposure value of a photograph is defined as\nwhere is the f-number measuring the aperture of the lens during the exposure, and is the number of seconds of exposure.\n\nBinary logarithms (expressed as stops) are also used in densitometry, to express the dynamic range of light-sensitive materials or digital sensors.\n\nAn easy way to calculate on calculators that do not have a function is to use the natural logarithm () or the common logarithm ( or ) functions, which are found on most scientific calculators. The specific change of logarithm base formulae for this are:\n\nor approximately\n\nThe binary logarithm can be made into a function from integers and to integers by rounding it up or down. These two forms of integer binary logarithm are related by this formula:\n\nThe definition can be extended by defining formula_11. Extended in this way, this function is related to the number of leading zeros of the 32-bit unsigned binary representation of , .\n\nThe integer binary logarithm can be interpreted as the zero-based index of the most significant bit in the input. In this sense it is the complement of the find first set operation, which finds the index of the least significant bit. Many hardware platforms include support for finding the number of leading zeros, or equivalent operations, which can be used to quickly find the binary logarithm. The codice_1 and codice_2 functions in the Linux kernel and in some versions of the libc software library also compute the binary logarithm (rounded up to an integer, plus one).\n\nFor a general positive real number, the binary logarithm may be computed in two parts.\nFirst, one computes the integer part, formula_13 (called the characteristic of the logarithm).\nThis reduces the problem to one where the argument of the logarithm is in a restricted range, the interval [1,2), simplifying the second step\nof computing the fractional part (the mantissa of the logarithm).\nFor any , there exists a unique integer such that , or equivalently . Now the integer part of the logarithm is simply , and the fractional part is . In other words:\nFor normalized floating point numbers, the integer part is given by the floating point exponent, and for integers it can be determined by performing a count leading zeros operation.\n\nThe fractional part of the result is , and can be computed iteratively, using only elementary multiplication and division.\nThe algorithm for computing the fractional part can be described in pseudocode as follows:\n\nThe result of this is expressed by the following recursive formulas, in which formula_16 is the number of squarings required in the \"i\"-th iteration of the algorithm:\n\nIn the special case where the fractional part in step 1 is found to be zero, this is a \"finite\" sequence terminating at some point. Otherwise, it is an infinite series that converges according to the ratio test, since each term is strictly less than the previous one (since every ). For practical use, this infinite series must be truncated to reach an approximate result. If the series is truncated after the th term, then the error in the result is less than .\n\nThe codice_3 function is included in the standard C mathematical functions. The default version of this function takes double precision arguments but variants of it allow the argument to be single-precision or to be a long double. In MATLAB, the argument to the codice_3 function is allowed to be a negative number, and in this case the result will be a complex number.\n\n"}
{"id": "200077", "url": "https://en.wikipedia.org/wiki?curid=200077", "title": "Binomial type", "text": "Binomial type\n\nIn mathematics, a polynomial sequence, i.e., a sequence of polynomials indexed by { 0, 1, 2, 3, ... } in which the index of each polynomial equals its degree, is said to be of binomial type if it satisfies the sequence of identities\n\nMany such sequences exist. The set of all such sequences forms a Lie group under the operation of umbral composition, explained below. Every sequence of binomial type may be expressed in terms of the Bell polynomials. Every sequence of binomial type is a Sheffer sequence (but most Sheffer sequences are not of binomial type). Polynomial sequences put on firm footing the vague 19th century notions of umbral calculus.\n\n\n\n\n\nIt can be shown that a polynomial sequence { \"p\"(x) : \"n\" = 0, 1, 2, ... } is of binomial type if and only if all three of the following conditions hold:\n\n\n\nThat linear transformation is clearly a delta operator, i.e., a shift-equivariant linear transformation on the space of polynomials in \"x\" that reduces degrees of polynomials by 1. The most obvious examples of delta operators are difference operators and differentiation. It can be shown that every delta operator can be written as a power series of the form\nwhere \"D\" is differentiation (note that the lower bound of summation is 1). Each delta operator \"Q\" has a unique sequence of \"basic polynomials\", i.e., a polynomial sequence satisfying\nIt was shown in 1973 by Rota, Kahaner, and Odlyzko, that a polynomial sequence is of binomial type if and only if it is the sequence of basic polynomials of some delta operator. Therefore, this paragraph amounts to a recipe for generating as many polynomial sequences of binomial type as one may wish.\n\nFor any sequence \"a\", \"a\", \"a\", ... of scalars, let\n\nWhere \"B\"(\"a\", ..., \"a\") is the Bell polynomial. Then this polynomial sequence is of binomial type. Note that for each \"n\" ≥ 1,\n\nHere is the main result of this section:\n\nTheorem: All polynomial sequences of binomial type are of this form.\n\nA result in Mullin and Rota, repeated in Rota, Kahaner, and Odlyzko (see \"References\" below) states that every polynomial sequence { \"p\"(\"x\") } of binomial type is determined by the sequence { \"p\"′(0) }, but those sources do not mention Bell polynomials.\n\nThis sequence of scalars is also related to the delta operator. Let\n\nThen\n\nis the delta operator of this sequence.\n\nFor sequences \"a\", \"b\", \"n\" = 0, 1, 2, ..., define a sort of convolution by\n\nLet formula_16 be the \"n\"th term of the sequence\n\nThen for any sequence \"a\", \"i\" = 0, 1, 2, ..., with \"a\" = 0, the sequence defined by \"p\"(\"x\") = 1 and\n\nfor \"n\" ≥ 1, is of binomial type, and every sequence of binomial type is of this form. This result is due to Alessandro di Bucchianico (see References below).\n\nPolynomial sequences of binomial type are precisely those whose generating functions are formal (not necessarily convergent) power series of the form\n\nwhere \"f\"(\"t\") is a formal power series whose constant term is zero and whose first-degree term is not zero. It can be shown by the use of the power-series version of Faà di Bruno's formula that\n\nThe delta operator of the sequence is \"f\"(\"D\"), so that\n\nThe coefficients in the product of two formal power series\n\nand\n\nare\n\n(see also Cauchy product). If we think of \"x\" as a parameter indexing a family of such power series, then the binomial identity says in effect that the power series indexed by \"x\" + \"y\" is the product of those indexed by \"x\" and by \"y\". Thus the \"x\" is the argument to a function that maps sums to products: an exponential function\n\nwhere \"f\"(\"t\") has the form given above.\n\nThe set of all polynomial sequences of binomial type is a group in which the group operation is \"umbral composition\" of polynomial sequences. That operation is defined as follows. Suppose { \"p\"(\"x\") : \"n\" = 0, 1, 2, 3, ... } and { \"q\"(\"x\") : \"n\" = 0, 1, 2, 3, ... } are polynomial sequences, and\n\nThen the umbral composition \"p\" o \"q\" is the polynomial sequence whose \"n\"th term is\n\n(the subscript \"n\" appears in \"p\", since this is the \"n\" term of that sequence, but not in \"q\", since this refers to the sequence as a whole rather than one of its terms).\n\nWith the delta operator defined by a power series in \"D\" as above, the natural bijection between delta operators and polynomial sequences of binomial type, also defined above, is a group isomorphism, in which the group operation on power series is formal composition of formal power series.\n\nThe sequence κ of coefficients of the first-degree terms in a polynomial sequence of binomial type may be termed the cumulants of the polynomial sequence. It can be shown that the whole polynomial sequence of binomial type is determined by its cumulants, in a way discussed in the article titled \"cumulant\". Thus\n\nand\n\nThese are \"formal\" cumulants and \"formal\" moments, as opposed to cumulants of a probability distribution and moments of a probability distribution.\n\nLet\n\nbe the (formal) cumulant-generating function. Then\n\nis the delta operator associated with the polynomial sequence, i.e., we have\n\nThe concept of binomial type has applications in combinatorics, probability, statistics, and a variety of other fields.\n\n\n\nAs the title suggests, the second of the above is explicitly about applications to combinatorial enumeration.\n\n"}
{"id": "48258", "url": "https://en.wikipedia.org/wiki?curid=48258", "title": "Bounded set", "text": "Bounded set\n\nIn mathematical analysis and related areas of mathematics, a set is called bounded, if it is, in a certain sense, of finite size. Conversely, a set which is not bounded is called unbounded. The word bounded makes no sense in a general topological space without a corresponding metric.\n\nA set \"S\" of real numbers is called \"bounded from above\" if there is a real number \"k\" such that \"k\" ≥ \" s\" for all \"s\" in \"S\". The number \"k\" is called an upper bound of \"S\". The terms \"bounded from below\" and lower bound are similarly defined.\n\nA set \"S\" is bounded if it has both upper and lower bounds. Therefore, a set of real numbers is bounded if it is contained in a finite interval.\n\nA subset \"S\" of a metric space (\"M\", \"d\") is bounded if it is contained in a ball of finite radius, i.e. if there exists \"x\" in \"M\" and \"r\" > 0 such that for all \"s\" in \"S\", we have d(\"x\", \"s\") < \"r\". (\"M\", \"d\") is a \"bounded\" metric space (or \"d\" is a \"bounded\" metric) if \"M\" is bounded as a subset of itself.\n\n\nIn topological vector spaces, a different definition for bounded sets exists which is sometimes called von Neumann boundedness. If the topology of the topological vector space is induced by a metric which is homogeneous, as in the case of a metric induced by the norm of normed vector spaces, then the two definitions coincide.\n\nA set of real numbers is bounded if and only if it has an upper and lower bound. This definition is extendable to subsets of any partially ordered set. Note that this more general concept of boundedness does not correspond to a notion of \"size\". \n\nA subset \"S\" of a partially ordered set \"P\" is called bounded above if there is an element \"k\" in \"P\" such that \"k\" ≥ \"s\" for all \"s\" in \"S\". The element \"k\" is called an upper bound of \"S\". The concepts of bounded below and lower bound are defined similarly. (See also upper and lower bounds.)\n\nA subset \"S\" of a partially ordered set \"P\" is called bounded if it has both an upper and a lower bound, or equivalently, if it is contained in an interval. Note that this is not just a property of the set \"S\" but also one of the set \"S\" as subset of \"P\".\n\nA bounded poset \"P\" (that is, by itself, not as subset) is one that has a least element and a greatest element. Note that this concept of boundedness has nothing to do with finite size, and that a subset \"S\" of a bounded poset \"P\" with as order the restriction of the order on \"P\" is not necessarily a bounded poset.\n\nA subset \"S\" of R is bounded with respect to the Euclidean distance if and only if it bounded as subset of R with the product order. However, \"S\" may be bounded as subset of R with the lexicographical order, but not with respect to the Euclidean distance.\n\nA class of ordinal numbers is said to be unbounded, or cofinal, when given any ordinal, there is always some element of the class greater than it. Thus in this case \"unbounded\" does not mean unbounded by itself but unbounded as a subclass of the class of all ordinal numbers.\n\n\n"}
{"id": "2187252", "url": "https://en.wikipedia.org/wiki?curid=2187252", "title": "Ciphertext indistinguishability", "text": "Ciphertext indistinguishability\n\nCiphertext indistinguishability is a property of many encryption schemes. Intuitively, if a cryptosystem possesses the property of indistinguishability, then an adversary will be unable to distinguish pairs of ciphertexts based on the message they encrypt. The property of indistinguishability under chosen plaintext attack is considered a basic requirement for most provably secure public key cryptosystems, though some schemes also provide indistinguishability under chosen ciphertext attack and adaptive chosen ciphertext attack. Indistinguishability under chosen plaintext attack is equivalent to the property of semantic security, and many cryptographic proofs use these definitions interchangeably.\n\nA cryptosystem is considered \"secure in terms of indistinguishability\" if no adversary, given an encryption of a message randomly chosen from a two-element message space determined by the adversary, can identify the message choice with probability significantly better than that of random guessing (). If any adversary can succeed in distinguishing the chosen ciphertext with a probability significantly greater than , then this adversary is considered to have an \"advantage\" in distinguishing the ciphertext, and the scheme is \"not\" considered secure in terms of indistinguishability. This definition encompasses the notion that in a secure scheme, the adversary should learn no information from seeing a ciphertext. Therefore, the adversary should be able to do no better than if it guessed randomly.\n\nSecurity in terms of indistinguishability has many definitions, depending on assumptions made about the capabilities of the attacker. It is normally presented as a game, where the cryptosystem is considered secure if no adversary can win the game with significantly greater probability than an adversary who must guess randomly. The most common definitions used in cryptography are indistinguishability under chosen plaintext attack (abbreviated IND-CPA), indistinguishability under (non-adaptive) chosen ciphertext attack (IND-CCA1), and indistinguishability under adaptive chosen ciphertext attack (IND-CCA2). Security under either of the latter definition implies security under the previous ones: a scheme which is IND-CCA1 secure is also IND-CPA secure, and a scheme which is IND-CCA2 secure is both IND-CCA1 and IND-CPA secure. Thus, IND-CCA2 is the strongest of the three definitions of security.\n\nFor a probabilistic asymmetric key encryption algorithm, indistinguishability under chosen plaintext attack (IND-CPA) is defined by the following game between an adversary and a challenger. For schemes based on computational security, the adversary is modeled by a probabilistic polynomial time Turing machine, meaning that it must complete the game and output a \"guess\" within a polynomial number of time steps. In this definition E(PK, \"M\") represents the encryption of a message \"M\" under the key \"PK\":\n\n\nA cryptosystem is indistinguishable under chosen plaintext attack if every probabilistic polynomial time adversary has only a negligible \"advantage\" over random guessing. An adversary is said to have a negligible \"advantage\" if it wins the above game with probability formula_4, where formula_5 is a negligible function in the security parameter \"k\", that is for every (nonzero) polynomial function formula_6 there exists formula_7 such that formula_8 for all formula_9.\n\nAlthough the adversary knows formula_10, formula_11 and PK, the probabilistic nature of E means that the encryption of formula_3 will be only one of many valid ciphertexts, and therefore encrypting formula_10, formula_11 and comparing the resulting ciphertexts with the challenge ciphertext does not afford any non-negligible advantage to the adversary.\n\nWhile the above definition is specific to an asymmetric key cryptosystem, it can be adapted to the symmetric case by replacing the public key encryption function with an encryption oracle, which retains the secret encryption key and encrypts arbitrary plaintexts at the adversary's request.\n\nThe adversarial process of performing a chosen-plaintext attack is usually outlined in the form of a Cryptographic Game. To test for symmetric IND-CPA, the game described above is defined. Let formula_15 be a key generation function, formula_16 be an encryption function, and formula_17 be a decryption function. Let formula_18 be a symmetric encryption scheme. The game formula_19 is defined as:\n\nAs many times as it would like, an adversary selects two plaintext messages of its own choosing and provides them to the LR oracle which returns a ciphertext encrypting one of the messages. An adversary's advantage is determined by its probability of guessing the value of \"b,\" a value chosen at random at the beginning of the game which determines the message that is encrypted in the LR oracle. Therefore, its advantage is defined as:\n\nIndistinguishability under non-adaptive and adaptive Chosen Ciphertext Attack (IND-CCA1, IND-CCA2) uses a definition similar to that of IND-CPA. However, in addition to the public key (or encryption oracle, in the symmetric case), the adversary is given access to a \"decryption oracle\" which decrypts arbitrary ciphertexts at the adversary's request, returning the plaintext. In the non-adaptive definition, the adversary is allowed to query this oracle only up until it receives the challenge ciphertext. In the adaptive definition, the adversary may continue to query the decryption oracle even after it has received a challenge ciphertext, with the caveat that it may not pass the challenge ciphertext for decryption (otherwise, the definition would be trivial).\n\n\nA scheme is IND-CCA1/IND-CCA2 secure if no adversary has a non-negligible advantage in winning the above game.\n\nSometimes we need encryption schemes in which the ciphertext string is indistinguishable from a random string by the adversary.\n\nIf an adversary is unable to tell if a message even exists, it gives the person who wrote the message plausible deniability.\n\nSome people building encrypted communication links prefer to make the contents of each encrypted datagram indistinguishable from random data, in order to make traffic analysis more difficult.\n\nSome people building systems to store encrypted data prefer to make the data indistinguishable from random data in order to make data hiding easier.\nFor example, some kinds of disk encryption such as TrueCrypt attempt to hide data in the innocent random data left over from some kinds of data erasure.\nAs another example, some kinds of steganography attempt to hide data by making it match the statistical characteristics of the innocent \"random\" image noise in digital photos.\n\nTo support such deniable encryption systems, a few cryptographic algorithms are specifically designed to make ciphertext messages indistinguishable from random bit strings.\n\nIf an encryption algorithm E can be designed such that an attacker (typically defined as a polynomial-time observer) who knows a plaintext message m is unable to distinguish between E(m), the encryption of that message, and a freshly-generated random bit string of the same length as E(m),\nthen it follows that when E(m1) is the same length as E(m2),\nthose two encrypted messages will be indistinguishable from each other by that attacker, even if that attacker knows the plaintext m1 and m2 (IND-CPA).\n\nMost applications don't require an encryption algorithm to produce encrypted messages that are indistinguishable from random bits.\nHowever, some authors consider such encryption algorithms to be conceptually simpler and easier to work with, and more versatile in practice—and most IND-CPA encryption algorithms apparently do, in fact, produce encrypted messages that are indistinguishable from random bits.\n\nIndistinguishability is an important property for maintaining the confidentiality of encrypted communications. However, the property of indistinguishability has in some cases been found to imply other, apparently unrelated security properties. Sometimes these implications go in both directions, making two definitions equivalent; for example, it is known that the property of indistinguishability under adaptive chosen ciphertext attack (IND-CCA2) is equivalent to the property of non-malleability under the same attack scenario (NM-CCA2). This equivalence is not immediately obvious, as non-malleability is a property dealing with message integrity, rather than confidentiality. In other cases, it has been demonstrated that indistinguishability can be combined with certain other definitions, in order to imply still other useful definitions, and vice versa. The following list summarizes a few known implications, though it is by no means complete.\n\nThe notation formula_22 means that property A implies property B. formula_23 means that properties A and B are \"equivalent\". formula_24 means that property A does not necessarily imply property B.\n\n\n\n"}
{"id": "32561183", "url": "https://en.wikipedia.org/wiki?curid=32561183", "title": "Demazure module", "text": "Demazure module\n\nIn mathematics, a Demazure module, introduced by , is a submodule of a finite-dimensional representation generated by an extremal weight space under the action of a Borel subalgebra. The Demazure character formula, introduced by , gives the characters of Demazure modules, and is a generalization of the Weyl character formula.\nThe dimension of a Demazure module is a polynomial in the highest weight, called a Demazure polynomial.\n\nSuppose that \"g\" is a complex semisimple Lie algebra, with a Borel subalgebra \"b\" containing a Cartan subalgebra \"h\". An irreducible finite-dimensional representation \"V\" of \"g\" splits as a sum of eigenspaces of \"h\", and the highest weight space is 1-dimensional and is an eigenspace of \"b\". The Weyl group \"W\" acts on the weights of \"V\", and the conjugates \"w\"λ of the highest weight vector λ under this action are the extremal weights, whose weight spaces are all 1-dimensional.\n\nA Demazure module is the \"b\"-submodule of \"V\" generated by the weight space of an extremal vector \"w\"λ, so the Demazure submodules of \"V\" are parametrized by the Weyl group \"W\".\n\nThere are two extreme cases: if \"w\" is trivial the Demazure module is just 1-dimensional, and if \"w\" is the element of maximal length of \"W\" then the Demazure module is the whole of the irreducible representation \"V\".\n\nDemazure modules can be defined in a similar way for highest weight representations of Kac–Moody algebras, except that one now has 2 cases as one can consider the submodules generated by either the Borel subalgebra \"b\" or its opposite subalgebra. In the finite-dimensional these are exchanged by the longest element of the Weyl group, but this is no longer the case in infinite dimensions as there is no longest element.\n\nThe Demazure character formula was introduced by .\nVictor Kac pointed out that Demazure's proof has a serious gap, as it depends on , which is false; see for Kac's counterexample. gave a proof of Demazure's character formula using the work on the geometry of Schubert varieties by and . gave a proof for sufficiently large dominant highest weight modules using Lie algebra techniques. proved a refined version of the Demazure character formula that conjectured (and proved in many cases).\n\nThe Demazure character formula is\nHere:\n\n"}
{"id": "5138147", "url": "https://en.wikipedia.org/wiki?curid=5138147", "title": "Discrete Poisson equation", "text": "Discrete Poisson equation\n\nIn mathematics, the discrete Poisson equation is the finite difference analog of the Poisson equation. In it, the discrete Laplace operator takes the place of the Laplace operator. The discrete Poisson equation is frequently used in numerical analysis as a stand-in for the continuous Poisson equation, although it is also studied in its own right as a topic in discrete mathematics.\n\nUsing the finite difference numerical method to discretize\nthe 2-dimensional Poisson equation (assuming a uniform spatial discretization, formula_1) on an \"m\" × \"n\" grid gives the following formula:\n\nwhere formula_3 and formula_4. The preferred arrangement of the solution vector is to use natural ordering which, prior to removing boundary elements, would look like:\n\nThis will result in an \"mn\" × \"mn\" linear system:\n\nwhere\n\nformula_8 is the \"m\" × \"m\" identity matrix, and formula_9, also \"m\" × \"m\", is given by:\n\nand formula_11 is defined by\n\nFor each formula_13 equation, the columns of formula_9 correspond to a block of formula_15 components in formula_16:\n\nwhile the columns of formula_8 to the left and right of formula_9 each correspond to other blocks of formula_15 components within formula_16:\n\nand\n\nrespectively.\n\nFrom the above, it can be inferred that there are formula_24 block columns of formula_15 in formula_26. It is important to note that prescribed values of formula_16 (usually lying on the boundary) would have their corresponding elements removed from formula_8 and formula_9. For the common case that all the nodes on the boundary are set, we have formula_30 and formula_31, and the system would have the dimensions (\"m\" − 2)(\"n\" − 2) × (\"m\" − 2)(\"n\" − 2), where formula_9 and formula_8 would have dimensions (\"m\" − 2) × (\"m\" − 2).\n\nFor a 5×5 ( formula_34 and formula_35 ) grid with all the boundary nodes prescribed,\nthe system would look like:\n\nwith\n\nand\n\nAs can be seen, the boundary formula_16's are brought to the right-hand-side\nof the equation. The entire system is 9 × 9 while formula_9 and formula_8 are 3 × 3 and given by:\n\nand\n\nBecause formula_44 is block tridiagonal and sparse, many methods of solution\nhave been developed to optimally solve this linear system for formula_45.\nAmong the methods are a generalized Thomas algorithm with a resulting computational complexity of formula_46, cyclic reduction, successive overrelaxation that has a complexity of formula_47, and Fast Fourier transforms which is formula_48. An optimal formula_49 solution can also be computed using multigrid methods. \n\nIn computational fluid dynamics, for the solution of an incompressible flow problem, the incompressibility condition acts as a constraint for the pressure. There is no explicit form available for pressure in this case due to a strong coupling of the velocity and pressure fields. In this condition, by taking the divergence of all terms in the momentum equation, one obtains the pressure poisson equation.\n\nFor an incompressible flow this constraint is given by:\n\nwhere formula_51 is the velocity in the formula_52 direction, formula_53 is\nvelocity in formula_54 and formula_55 is the velocity in the formula_56 direction. Taking divergence of the momentum equation and using the incompressibility constraint, the pressure poisson equation is formed given by:\n\nwhere formula_58 is the kinematic viscosity of the fluid and formula_59 is the velocity vector.\n\nThe discrete Poisson's equation arises in the theory of \nMarkov chains. It appears as the relative value function for the dynamic programming equation in a Markov decision process, and as the \"control variate\" for application in simulation variance reduction.\n\n"}
{"id": "35021032", "url": "https://en.wikipedia.org/wiki?curid=35021032", "title": "Erdős–Szemerédi theorem", "text": "Erdős–Szemerédi theorem\n\nIn arithmetic combinatorics, the Erdős–Szemerédi theorem, proven by Paul Erdős and Endre Szemerédi in 1983, states that, for every finite set of real numbers, either the pairwise sums or the pairwise products of the numbers in the set form a significantly larger set. More precisely, it asserts the existence of positive constants \"c\" and formula_1 such that\n\nwhenever \"A\" is a finite non-empty set of real numbers of cardinality |\"A\"|, where formula_3 is the sum-set of \"A\" with itself, and formula_4.\n\nIt is possible for \"A\" + \"A\" to be of comparable size to \"A\" if \"A\" is an arithmetic progression, and it is possible for \"A\" · \"A\" to be of comparable size to \"A\" if \"A\" is a geometric progression. The Erdős–Szemerédi theorem can thus be viewed as an assertion that it is not possible for a large set to behave like an arithmetic progression and as a geometric progression simultaneously. It can also be viewed as an assertion that the real line does not contain any set resembling a finite subring or finite subfield; it is the first example of what is now known as the \"sum-product phenomenon\", which is now known to hold in a wide variety of rings and fields, including finite fields.\n\nIt was conjectured by Erdős and Szemerédi that one can take formula_1 arbitrarily close to 1. The best result in this direction currently is by George Shakan, who showed that one can take formula_1 arbitrarily close to formula_7. Previously, Misha Rudnev, Ilya Shkredov, and Sophie Stevens had shown that one can take formula_1 arbitrarily close to formula_9, improving an earlier result by József Solymosi, who had shown that one can take it arbitrarily close to formula_10.\n"}
{"id": "13408203", "url": "https://en.wikipedia.org/wiki?curid=13408203", "title": "FRACTRAN", "text": "FRACTRAN\n\nFRACTRAN is a Turing-complete esoteric programming language invented by the mathematician John Conway. A FRACTRAN program is an ordered list of positive fractions together with an initial positive integer input \"n\". The program is run by updating the integer \"n\" as follows:\n\nIn \"The Book of Numbers\", John Conway and Richard Guy gave a formula for primes in FRACTRAN:\n\nStarting with \"n\"=2, this FRACTRAN program generates the following sequence of integers:\n\nAfter 2, this sequence contains the following powers of 2:\n\nwhich are the prime powers of 2.\n\nA FRACTRAN program can be seen as a type of register machine where the registers are stored in prime exponents in the argument \"n\".\n\nUsing Gödel numbering, a positive integer \"n\" can encode an arbitrary number of arbitrarily large positive integer variables. The value of each variable is encoded as the exponent of a prime number in the prime factorization of the integer. For example, the integer\n\nrepresents a register state in which one variable (which we will call v2) holds the value 2 and two other variables (v3 and v5) hold the value 1. All other variables hold the value 0.\n\nA FRACTRAN program is an ordered list of positive fractions. Each fraction represents an instruction that tests one or more variables, represented by the prime factors of its denominator. For example:\n\ntests v2 and v5. If formula_5 and formula_6, then it subtracts 2 from v2 and 1 from v5 and adds 1 to v3 and 1 to v7. For example:\n\nSince the FRACTRAN program is just a list of fractions, these test-decrement-increment instructions are the only allowed instructions in the FRACTRAN language. In addition the following restrictions apply:\n\nThe simplest FRACTRAN program is a single instruction such as\n\nThis program can be represented as a (very simple) algorithm as follows:\n\nGiven an initial input of the form formula_9, this program will compute the sequence formula_10, formula_11, etc., until eventually, after formula_12 steps, no factors of 2 remain and the product with formula_13 no longer yields an integer; the machine then stops with a final output of formula_14. It therefore adds two integers together.\n\nWe can create a \"multiplier\" by \"looping\" through the \"adder\". In order to do this we need to introduce states into our algorithm. This algorithm will take a number formula_9 and produce formula_16:\n\nState B is a loop that adds v3 to v5 and also moves v3 to v7, and state A is an outer control loop that repeats the loop in state B v2 times. State A also restores the value of v3 from v7 after the loop in state B has completed.\n\nWe can implement states using new variables as state indicators. The state indicators for state B will be v11 and v13. Note that we require two state control indicators for one loop; a primary flag (v11) and a secondary flag (v13). Because each indicator is consumed whenever it is tested, we need a secondary indicator to say \"continue in the current state\"; this secondary indicator is swapped back to the primary indicator in the next instruction, and the loop continues.\n\nAdding FRACTRAN state indicators and instructions to the multiplication algorithm table, we have:\n\nWhen we write out the FRACTRAN instructions, we must put the state A instructions last, because state A has no state indicators - it is the default state if no state indicators are set. So as a FRACTRAN program, the multiplier becomes:\n\nWith input 23 this program produces output 5. \n\nIn a similar way, we can create a FRACTRAN \"subtracter\", and repeated subtractions allow us to create a \"quotient and remainder\" algorithm as follows:\n\nWriting out the FRACTRAN program, we have:\n\nand input 2311 produces output 57 where \"n\" = \"qd\" + \"r\" and 0 ≤ \"r\" < \"d\".\n\nConway's prime generating algorithm above is essentially a quotient and remainder algorithm within two loops. Given input of the form formula_19 where 0 ≤ \"m\" < \"n\", the algorithm tries to divide \"n\"+1 by each number from \"n\" down to 1, until it finds the largest number \"k\" that is a divisor of \"n\"+1. It then returns 2 7 and repeats. The only times that the sequence of state numbers generated by the algorithm produces a power of 2 is when \"k\" is 1 (so that the exponent of 7 is 0), which only occurs if the exponent of 2 is a prime. A step-by-step explanation of Conway's algorithm can be found in Havil(2007).\n\nThe following FRACTRAN program:\n\ncalculates the Hamming weight H(\"a\") of the binary expansion of \"a\" i.e. the number of 1s in the binary expansion of \"a\". Given input 2, its output is 13. The program can be analysed as follows:\n\n\n"}
{"id": "5698171", "url": "https://en.wikipedia.org/wiki?curid=5698171", "title": "Fidelity of quantum states", "text": "Fidelity of quantum states\n\nIn quantum information theory, fidelity is a measure of the \"closeness\" of two quantum states. It expresses the probability that one state will pass a test to identify as the other. The fidelity is not a metric on the space of density matrices, but it can be used to define the Bures metric on this space.\n\nGiven two states formula_1 and formula_2, the fidelity is generally defined as the quantity formula_3. In the special case of formula_1 and formula_2 pure states, the definition reduces to the squared overlap between the states: formula_6 if formula_7 and formula_8. While not obvious from the general definition, the fidelity is symmetric: formula_9.\n\nGiven two random variables \"X\", \"Y\" with values (1, ..., n) (categorical random variables) and probabilities \"p\" = (\"p\"...\"p\") and \"q\" = (\"q\"...\"q\"). The fidelity of \"X\" and \"Y\" is defined to be the quantity\n\nThe fidelity deals with the marginal distribution of the random variables. It says nothing about the joint distribution of those variables. In other words, the fidelity \"F(X,Y)\" is the square of the inner product of formula_11 and formula_12 viewed as vectors in Euclidean space. Notice that \"F(X,Y)\" = 1 if and only if \"p\" = \"q\". In general, formula_13. The measure formula_14 is known as the Bhattacharyya coefficient.\n\nGiven a classical measure of the distinguishability of two probability distributions, one can motivate a measure of distinguishability of two quantum states as follows. If an experimenter is attempting to determine whether a quantum state is either of two possibilities formula_1 or formula_2, the most general possible measurement they can make on the state is a POVM, which is described by a set of Hermitian positive semidefinite operators formula_17. If the state given to the experimenter is formula_1, they will witness outcome formula_19 with probability formula_20, and likewise with probability formula_21 for formula_2. Their ability to distinguish between the quantum states formula_1 and formula_2 is then equivalent to their ability to distinguish between the classical probability distributions formula_25 and formula_26. Naturally, the experimenter will choose the best POVM he can find, so this motivates defining the quantum fidelity as the squared Bhattacharyya coefficient when extremized over all possible POVMs formula_17:\n\nIt was shown by Fuchs and Caves that this manifestly symmetric definition is equivalent to the simple asymmetric formula given in the next section.\n\nGiven two density matrices \"ρ\" and \"σ\", the fidelity is defined by\n\nwhere, for a positive semidefinite matrix formula_30, formula_31 denotes its unique positive square root, as given by the spectral theorem. The Euclidean inner product from the classical definition is replaced by the Hilbert–Schmidt inner product.\n\nSome of the important properties of the quantum state fidelity are:\n\n\nSome authors use an alternative definition formula_60 and call this quantity fidelity. The definition of formula_61 however is more common. To avoid confusion, formula_62 could be called “square root fidelity”. In any case it is advisable to clarify the adopted definition whenever the fidelity is employed.\n\nDirect calculation shows that the fidelity is preserved by unitary evolution, i.e.\n\nfor any unitary operator formula_64.\n\nWe saw that for two pure states, their fidelity coincides with the overlap. Uhlmann's theorem generalizes this statement to mixed states, in terms of their purifications:\n\nTheorem Let ρ and σ be density matrices acting on C. Let ρ be the unique positive square root of ρ and\n\nformula_65\n\nbe a purification of ρ (therefore formula_66 is an orthonormal basis), then the following equality holds:\n\nwhere formula_68 is a purification of σ. Therefore, in general, the fidelity is the maximum overlap between purifications.\n\nA simple proof can be sketched as follows. Let formula_69 denote the vector\n\nand σ be the unique positive square root of σ. We see that, due to the unitary freedom in square root factorizations and choosing orthonormal bases, an arbitrary purification of σ is of the form\n\nwhere \"V\"'s are unitary operators. Now we directly calculate\n\nBut in general, for any square matrix \"A\" and unitary \"U\", it is true that |Tr(\"AU\")| ≤ Tr ((\"A\"\"A\")). Furthermore, equality is achieved if \"U\" is the unitary operator in the polar decomposition of \"A\". From this follows directly Uhlmann's theorem.\n\nWe will here provide an alternative, explicit way to prove Uhlmann's theorem.\n\nLet formula_73 and formula_74 be purifications of formula_1 and formula_2, respectively. To start, let us show that formula_77.\n\nThe general form of the purifications of the states is:formula_78were formula_79 are the eigenvectors of formula_80, and formula_81 are arbitrary orthonormal bases. The overlap between the purifications isformula_82where the unitary matrix formula_64 is defined asformula_84The conclusion is now reached via using the inequality formula_85: formula_86Note that this inequality is the triangle inequality applied to the singular values of the matrix. Indeed, for a generic matrix formula_87and unitary formula_88, we haveformula_89where formula_90 are the (always real and non-negative) singular values of formula_91, as in the singular value decomposition. The inequality is saturated and becomes an equality when formula_92, that is, when formula_93 and thus formula_94. The above shows that formula_95 when the purifications formula_73 and formula_74 are such that formula_98. Because this choice is possible regardless of the states, we can finally conclude thatformula_99\n\nSome immediate consequences of Uhlmann's theorem are\n\nSo we can see that fidelity behaves almost like a metric. This can be formalized and made useful by defining\n\nAs the angle between the states formula_1 and formula_2. It follows from the above properties that formula_103 is non-negative, symmetric in its inputs, and is equal to zero if and only if formula_104. Furthermore, it can be proved that it obeys the triangle inequality, so this angle is a metric on the state space: the Fubini–Study metric.\n\nLet formula_105 be an arbitrary positive operator-valued measure (POVM); that is, a set of operators formula_106 satisfying formula_107, formula_108, and formula_109. Then, for any pair of states formula_1 and formula_2, we have\nformula_112\nwhere in the last step we denoted with formula_40 the probability distributions obtained by measuring formula_80 with the POVM formula_105.\n\nThis shows that the square root of the fidelity between two quantum states is upper bounded by the Bhattacharyya coefficient between the corresponding probability distributions in any possible POVM. Indeed, it is more generally true that formula_116 where formula_117, and the minimum is taken over all possible POVMs.\n\nAs was previously shown, the square root of the fidelity can be written as formula_118which is equivalent to the existence of a unitary operator formula_64 such that\n\nformula_120Remembering that formula_107 holds true for any POVM, we can then writeformula_122where in the last step we used Cauchy-Schwarz inequality as in formula_123.\n\nThe fidelity between two states can be shown to never decrease when a non-selective quantum operation formula_124 is applied to the states:formula_125 for any trace-preserving completely positive map formula_124.\n\nWe can define the trace distance between two matrices A and B in terms of the trace norm by\n\nWhen A and B are both density operators, this is a quantum generalization of the statistical distance. This is relevant because the trace distance provides upper and lower bounds on the fidelity as quantified by the \"Fuchs–van de Graaf inequalities\",\n\nOften the trace distance is easier to calculate or bound than the fidelity, so these relationships are quite useful. In the case that at least one of the states is a pure state Ψ, the lower bound can be tightened.\n\n"}
{"id": "726014", "url": "https://en.wikipedia.org/wiki?curid=726014", "title": "Filtration (mathematics)", "text": "Filtration (mathematics)\n\nIn mathematics, a filtration formula_1 is an indexed set formula_2 of subobjects of a given algebraic structure formula_3, with the index formula_4 running over some index set formula_5 that is a totally ordered set, subject to the condition that\n\nIf the index formula_4 is the time parameter of some stochastic process, then the filtration can be interpreted as representing all historical but not future information available about the stochastic process, with the algebraic object formula_2 gaining in complexity with time. Hence, a process that is adapted to a filtration formula_1, is also called non-anticipating, i.e. one that cannot see into the future.\n\nSometimes, as in a filtered algebra, there is instead the requirement that the formula_2 be subalgebras with respect to some operations (say, vector addition), but not with respect to other operations (say, multiplication), that satisfy formula_13, where the index set is the natural numbers; this is by analogy with a graded algebra.\n\nSometimes, filtrations are supposed to satisfy the additional requirement that the union of the formula_2 be the whole formula_3, or (in more general cases, when the notion of union does not make sense) that the canonical homomorphism from the direct limit of the formula_2 to formula_3 is an isomorphism. Whether this requirement is assumed or not usually depends on the author of the text and is often explicitly stated. This article does \"not\" impose this requirement.\n\nThere is also the notion of a descending filtration, which is required to satisfy formula_18 in lieu of formula_19 (and, occasionally, formula_20 instead of formula_21). Again, it depends on the context how exactly the word \"filtration\" is to be understood. Descending filtrations are not to be confused with cofiltrations (which consist of quotient objects rather than subobjects).\n\nThe concept dual to a filtration is called a \"cofiltration\".\n\nFiltrations are widely used in abstract algebra, homological algebra (where they are related in an important way to spectral sequences), and in measure theory and probability theory for nested sequences of σ-algebras. In functional analysis and numerical analysis, other terminology is usually used, such as scale of spaces or nested spaces.\n\nIn algebra, filtrations are ordinarily indexed by formula_22, the set of natural numbers. A \"filtration\" of a group formula_23, is then a nested sequence formula_24 of normal subgroups of formula_23 (that is, for any formula_26 we have formula_27). Note that this use of the word \"filtration\" corresponds to our \"descending filtration\".\n\nGiven a group formula_23 and a filtration formula_24, there is a natural way to define a topology on formula_23, said to be \"associated\" to the filtration. A basis for this topology is the set of all translates of subgroups appearing in the filtration, that is, a subset of formula_23 is defined to be open if it is a union of sets of the form formula_32, where formula_33 and formula_26 is a natural number.\n\nThe topology associated to a filtration on a group formula_23 makes formula_23 into a topological group.\n\nThe topology associated to a filtration formula_24 on a group formula_23 is Hausdorff if and only if formula_39.\n\nIf two filtrations formula_24 and formula_41 are defined on a group formula_23, then the identity map from formula_23 to formula_23, where the first copy of formula_23 is given the formula_24-topology and the second the formula_41-topology, is continuous if and only if for any formula_26 there is an formula_49 such that formula_50, that is, if and only if the identity map is continuous at 1. In particular, the two filtrations define the same topology if and only if for any subgroup appearing in one there is a smaller or equal one appearing in the other.\n\nGiven a ring formula_51 and an formula_51-module formula_53, a \"descending filtration\" of formula_53 is a decreasing sequence of submodules formula_55. This is therefore a special case of the notion for groups, with the additional condition that the subgroups be submodules. The associated topology is defined as for groups.\n\nAn important special case is known as the formula_5-adic topology (or formula_57-adic, etc.). Let formula_51 be a commutative ring, and formula_5 an ideal of formula_51.\n\nGiven an formula_51-module formula_53, the sequence formula_63 of submodules of formula_53 forms a filtration of formula_53. The \"formula_5-adic topology\" on formula_53 is then the topology associated to this filtration. If formula_53 is just the ring formula_51 itself, we have defined the \"formula_5-adic topology\" on formula_51.\n\nWhen formula_51 is given the formula_5-adic topology, formula_51 becomes a topological ring. If an formula_51-module formula_53 is then given the formula_5-adic topology, it becomes a topological formula_51-module, relative to the topology given on formula_51.\n\nGiven a ring formula_51 and an formula_51-module formula_53, an \"ascending filtration\" of formula_53 is an increasing sequence of submodules formula_55. In particular, if formula_51 is a field, then an ascending filtration of the formula_51-vector space formula_53 is an increasing sequence of vector subspaces of formula_53. Flags are one important class of such filtrations.\n\nA maximal filtration of a set is equivalent to an ordering (a permutation) of the set. For instance, the filtration formula_89 corresponds to the ordering formula_90. From the point of view of the field with one element, an ordering on a set corresponds to a maximal flag (a filtration on a vector space), considering a set to be a vector space over the field with one element.\n\nIn measure theory, in particular in martingale theory and the theory of stochastic processes, a filtration is an increasing sequence of formula_91-algebras on a measurable space. That is, given a measurable space formula_92, a filtration is a sequence of formula_91-algebras formula_94 with formula_95 where each formula_96 is a non-negative real number and\n\nThe exact range of the \"times\" \"formula_96\" will usually depend on context: the set of values for formula_96 might be discrete or continuous, bounded or unbounded. For example,\n\nSimilarly, a filtered probability space (also known as a stochastic basis) formula_101, is a probability space equipped with the filtration formula_102 of its formula_91-algebra formula_1. A filtered probability space is said to satisfy the \"usual conditions\" if it is complete (i.e., formula_105 contains all formula_106-null sets) and right-continuous (i.e. formula_107 for all times formula_96).\n\nIt is also useful (in the case of an unbounded index set) to define formula_109 as the formula_91-algebra generated by the infinite union of the formula_111's, which is contained in formula_1:\n\nA \"σ\"-algebra defines the set of events that can be measured, which in a probability context is equivalent to events that can be discriminated, or \"questions that can be answered at time formula_96\". Therefore, a filtration is often used to represent the change in the set of events that can be measured, through gain or loss of information. A typical example is in mathematical finance, where a filtration represents the information available up to and including each time formula_96, and is more and more precise (the set of measurable events is staying the same or increasing) as more information from the evolution of the stock price becomes available.\n\nLet formula_101 be a filtered probability space. A random variable formula_117 is a stopping time with respect to the filtration formula_118, if formula_119 for all formula_120. \nThe \"stopping time\" formula_91-algebra is now defined as\n\nIt is not difficult to show that formula_123 is indeed a formula_91-algebra.\nThe set formula_123 encodes information up to the \"random\" time formula_126 in the sense that, if the filtered probability space is interpreted as a random experiment, the maximum information that can be found out about it from arbitrarily often repeating the experiment until the random time formula_126 is formula_123. In particular, if the underlying probability space is finite (i.e. formula_1 is finite), the minimal sets of formula_123 (with respect to set inclusion) are given by the union over all formula_120 of the sets of minimal sets of formula_111 that lie in formula_133.\n\nIt can be shown that formula_126 is formula_123-measurable. However, simple examples show that, in general, formula_136. If formula_137 and formula_138 are stopping times on formula_101, and formula_140 almost surely, then formula_141.\n\n"}
{"id": "2549522", "url": "https://en.wikipedia.org/wiki?curid=2549522", "title": "Finite character", "text": "Finite character\n\nIn mathematics, a family formula_1 of sets is of finite character provided it has the following properties:\n\nA family formula_1 of sets of finite character enjoys the following properties:\n\n\nLet \"V\" be a vector space, and let \"F\" be the family of linearly independent subsets of \"V\". Then \"F\" is a family of finite character (because a subset \"X\" ⊆ \"V\" is linearly dependent iff \"X\" has a finite subset which is linearly dependent). \nTherefore, in every vector space, there exists a maximal family of linearly independent elements. As a maximal family is a vector basis, every vector space has a (possibly infinite) vector basis.\n"}
{"id": "483126", "url": "https://en.wikipedia.org/wiki?curid=483126", "title": "GNU Multiple Precision Arithmetic Library", "text": "GNU Multiple Precision Arithmetic Library\n\nGNU Multiple Precision Arithmetic Library (GMP) is a free library for arbitrary-precision arithmetic, operating on signed integers, rational numbers, and floating point numbers. There are no practical limits to the precision except the ones implied by the available memory in the machine GMP runs on (operand dimension limit is 2-1 bits on 32-bit machines and 2 bits on 64-bit machines). GMP has a rich set of functions, and the functions have a regular interface. The basic interface is for C but wrappers exist for other languages including Ada, C++, C#, Julia, .NET, OCaml, Perl, PHP, Python, R, Ruby and the Wolfram Language. In the past, Kaffe, a Java virtual machine, used GMP to support Java built-in arbitrary precision arithmetic. This feature has been removed from recent releases, causing protests from people who claim that they used Kaffe solely for the speed benefits afforded by GMP. As a result, GMP support has been added to GNU Classpath.\n\nThe main target applications of GMP are cryptography applications and research, Internet security applications, and computer algebra systems.\n\nGMP aims to be faster than any other bignum library for all operand sizes. Some important factors in doing this are:\n\nThe first GMP release was made in 1991. It is constantly developed and maintained.\n\nGMP is part of the GNU project (although its website being off gnu.org may cause confusion), and is distributed under the GNU Lesser General Public License (LGPL).\n\nGMP is used for integer arithmetic in many computer algebra systems such as Mathematica and Maple. It is also used in the Computational Geometry Algorithms Library (CGAL) because geometry algorithms tend to 'explode' when using ordinary floating point CPU math.\n\nGMP is needed to build the GNU Compiler Collection (GCC).\n\nHere is an example of C code showing the use of the GMP library to multiply and print large numbers:\n\nThis code calculates the value of 7612058254738945 × 9263591128439081.\n\nCompiling and running this program gives this result. (The -lgmp flag is used if compiling on Unix-type systems.)\n\nFor comparison, one can write instead the following equivalent C++ program. (The -lgmpxx -lgmp flags are used if compiling on Unix-type systems.)\n\n"}
{"id": "31073672", "url": "https://en.wikipedia.org/wiki?curid=31073672", "title": "Gerald J. Toomer", "text": "Gerald J. Toomer\n\nGerald James Toomer (born 23 November 1934) is a historian of astronomy and mathematics who has written numerous books and papers on ancient Greek and medieval Islamic astronomy. In particular, he translated Ptolemy's \"Almagest\" into English.\n\nFormerly a fellow of Corpus Christi College, Cambridge University, he moved to Brown University as a special student in 1959 to study \"the history of mathematics in antiquity and the transmission of these systems through Arabic into medieval Europe.\" He joined the History of Mathematics department in 1963, became an associate professor in 1965, and was the chairman from 1980 to 1986.\n\n\n\n"}
{"id": "5076058", "url": "https://en.wikipedia.org/wiki?curid=5076058", "title": "Hemicompact space", "text": "Hemicompact space\n\nIn mathematics, in the field of topology, a topological space is said to be hemicompact if it has a sequence of compact subsets such that every compact subset of the space lies inside some compact set in the sequence. Clearly, this forces the union of the sequence to be the whole space, because every point is compact and hence must lie in one of the compact sets.\n\n\nEvery hemicompact space is σ-compact and if in addition it is first countable then it is locally compact.\n\nIf formula_1 is a hemicompact space, then the space formula_2 of all continuous functions formula_3 to a metric space formula_4 with the compact-open topology is metrizable. To see this, take a sequence formula_5 of compact subsets of formula_1 such that every compact subset of formula_1 lies inside some compact set in this sequence (the existence of such a sequence follows from the hemicompactness of formula_1). Define psedometrics\n\nThen\n\ndefines a metric on formula_11 which induces the compact-open topology.\n\n\n"}
{"id": "6853329", "url": "https://en.wikipedia.org/wiki?curid=6853329", "title": "History of ancient numeral systems", "text": "History of ancient numeral systems\n\nNumeral systems have progressed from the use of tally marks, more than 40,000 years ago, through to the use of sets of glyphs to efficiently represent any conceivable number.\n\nThe first method of counting has been argued to be counting on fingers. This evolved into sign language for the hand-to-eye-to-elbow communication of numbers which, while not writing, gave way to written numbers.\n\nTallies made by carving notches in wood, bone, and stone were used for at least forty thousand years. These tally marks may have been used for counting elapsed time, such as numbers of days, lunar cycles or keeping records of quantities, such as of animals.\n\nLebombo bone is a baboon fibula with incised markings discovered in the Lebombo Mountains located between South Africa and Swaziland. The bone is between 44,230 and 43,000 years old, according to two dozen radiocarbon datings. According to \"The Universal Book of Mathematics\" the Lebombo bone's 29 notches suggest \"it may have been used as a lunar phase counter, in which case African women may have been the first mathematicians, because keeping track of menstrual cycles requires a lunar calendar.\" But the bone is clearly broken at one end, so the 29 notches can only be a minimum number. Furthermore, in the many more notched bones since found there is no consistent notch tally, many being in the 1–10 range.\n\nIshango bone is an artifact with a sharp piece of quartz affixed to one end, perhaps for engraving. It was first thought to be a tally stick, as it has a series of what has been interpreted as tally marks carved in three columns running the length of the tool. But some scientists have suggested that the groupings of notches indicate a mathematical understanding that goes beyond counting. It has also been suggested that the scratches might have been to create a better grip on the handle or for some other non-mathematical reason. It is believed that the bone is more than 20,000 years old.\n\nThe earliest known writing for record keeping evolved from a system of counting using small clay tokens. The earliest tokens now known are those from two sites in the Zagros region of Iran: Tepe Asiab and Ganj-i-Dareh Tepe.\n\nTo create a record that represented \"two sheep\", they selected two round clay tokens each having a + sign baked into it. Each token represented one sheep. Representing a hundred sheep with a hundred tokens would be impractical, so they invented different clay tokens to represent different numbers of each specific commodity, and by 4000 BC strung the tokens like beads on a string. There was a token for one sheep, a different token for ten sheep, a different token for ten goats, etc. Thirty-two sheep would be represented by three ten-sheep tokens followed on the string by two one-sheep tokens.\n\nTo ensure that nobody could alter the number and type of tokens, they invented a clay envelope shaped like a hollow ball into which the tokens on a string were placed, sealed, and baked. If anybody disputed the number, they could break open the clay envelope and do a recount. To avoid unnecessary damage to the record, they pressed archaic number signs and witness seals on the outside of the envelope before it was baked, each sign similar in shape to the tokens they represented. Since there was seldom any need to break open the envelope, the signs on the outside became the first written language for writing numbers in clay. An alternative method was to seal the knot in each string of tokens with a solid oblong bulla of clay having impressed symbols, while the string of tokens dangled outside of the bulla.\n\nBeginning about 3500 BC the tokens and envelopes were replaced by numerals impressed with a round stylus at different angles in flat clay tablets which were then baked. A sharp stylus was used to carve pictographs representing various tokens. Each sign represented both the commodity being counted and the quantity or volume of that commodity.\n\nAbstract numerals, dissociated from the thing being counted, were invented about 3100 BC. The things being counted were indicated by pictographs carved with a sharp stylus next to round-stylus numerals.\n\nThe Sumerians had a complex assortment of incompatible number systems, and each city had its own local way of writing numerals. For instance, at about 3100 BC in the city of Uruk, there were more than a dozen different numeric systems. In this city, there were separate number systems for counting discrete objects (such as animals, tools, and containers), cheese and grain products, volumes of grain (including fractions), beer ingredients, weights, land areas, and time and calendar units. Furthermore, these systems changed over time; for instance, numbers for counting volumes of grain changed when the size of the baskets changed.\n\nThe Sumerians invented arithmetic. People who added and subtracted volumes of grain every day used their arithmetic skills to count other things that were unrelated to volume measurements. Multiplication and division were done with multiplication tables baked in clay tablets.\n\nBetween 2700 BC and 2000 BC, the round stylus was gradually replaced by a reed stylus that had been used to press wedge shaped cuneiform signs in clay. To represent numbers that previously had been pressed with a round stylus, these cuneiform number signs were pressed in a circular pattern and they retained the additive sign-value notation that originated with tokens on a string. Cuneiform numerals and archaic numerals were ambiguous because they represented various numeric systems that differed depending on what was being counted. About 2100 BC in Sumer, these proto-sexagesimal sign-value systems gradually converged on a common sexagesimal number system that was a place-value system consisting of only two impressed marks, the vertical wedge and the chevron, which could also represent fractions. This sexagesimal number system was fully developed at the beginning of the Old Babylonian period (about 1950 BC) and became standard in Babylonia.\n\nSexagesimal numerals were a mixed radix system that retained the alternating base 10 and base 6 in a sequence of cuneiform vertical wedges and chevrons. Sexagesimal numerals became widely used in commerce, but were also used in astronomical and other calculations. This system was exported from Babylonia and used throughout Mesopotamia, and by every Mediterranean nation that used standard Babylonian units of measure and counting, including the Greeks, Romans and Syrians. In Arabic numerals, we still use sexagesimal to count time (minutes per hour), and angles (degrees).\n\nRoman numerals evolved from this primitive system of cutting notches. It was once believed that they came from alphabetic symbols or from pictographs, but these theories have been disproved.\n\n\n\n"}
{"id": "8062003", "url": "https://en.wikipedia.org/wiki?curid=8062003", "title": "Hypercone", "text": "Hypercone\n\nIn geometry, a hypercone (or spherical cone) is the figure in the 4-dimensional Euclidean space represented by the equation\n\nIt is a quadric surface, and is one of the possible 3-manifolds which are 4-dimensional equivalents of the conical surface in 3 dimensions. It is also named spherical cone because its intersections with hyperplanes perpendicular to the \"w\"-axis are spheres. A four-dimensional right spherical hypercone can be thought of as a sphere which expands with time, starting its expansion from a single point source, such that the center of the expanding sphere remains fixed. An oblique spherical hypercone would be a sphere which expands with time, again starting its expansion from a point source, but such that the center of the expanding sphere moves with a uniform velocity.\n\nA right spherical hypercone can be described by the function\nwith vertex at the origin and expansion speed \"s\".\n\nAn oblique spherical hypercone could then be described by the function\nwhere formula_4 is the 3-velocity of the center of the expanding sphere.\nAn example of such a cone would be an expanding sound wave as seen from the point of view of a moving reference frame: e.g. the sound wave of a jet aircraft as seen from the jet's own reference frame.\n\nNote that the 3D-surfaces above enclose 4D-hypervolumes, which are the 4-cones proper.\n\nThe spherical cone consists of two unbounded \"nappes\", which meet at the origin and are the analogues of the nappes of the 3-dimensional conical surface. The \"upper nappe\" corresponds with the half with positive \"w\"-coordinates, and the \"lower nappe\" corresponds with the half with negative \"w\"-coordinates.\n\nIf it is restricted between the hyperplanes \"w\" = 0 and \"w\" = \"r\" for some nonzero \"r\", then it may be closed by a 3-ball of radius \"r\", centered at (0,0,0,\"r\"), so that it bounds a finite 4-dimensional volume. This volume is given by the formula \"r\", and is the 4-dimensional equivalent of the solid cone. The ball may be thought of as the 'lid' at the base of the 4-dimensional cone's nappe, and the origin becomes its 'apex'.\n\nThis shape may be projected into 3-dimensional space in various ways. If projected onto the \"xyz\" hyperplane, its image is a ball. If projected onto the \"xyw\", \"xzw\", or \"yzw\" hyperplanes, its image is a solid cone. If projected onto an oblique hyperplane, its image is either an ellipsoid or a solid cone with an ellipsoidal base (resembling an ice cream cone). These images are the analogues of the possible images of the solid cone projected to 2 dimensions.\n\nThe (half) hypercone may be constructed in a manner analogous to the construction of a 3D cone. A 3D cone may be thought of as the result of stacking progressively smaller discs on top of each other until they taper to a point. Alternatively, a 3D cone may be regarded as the volume swept out by an upright isosceles triangle as it rotates about its base.\n\nA 4D hypercone may be constructed analogously: by stacking progressively smaller balls on top of each other in the 4th direction until they taper to a point, or taking the hypervolume swept out by a tetrahedron standing upright in the 4th direction as it rotates freely about its base in the 3D hyperplane on which it rests.\n\nIf the \"w\"-coordinate of the equation of the spherical cone is interpreted as the distance \"ct\", where \"t\" is coordinate time and \"c\" is the speed of light (a constant), then it is the shape of the light cone in special relativity. In this case, the equation is usually written as:\n\nwhich is also the equation for spherical wave fronts of light. The upper nappe is then the \"future light cone\" and the lower nappe is the \"past light cone\".\n\n"}
{"id": "29957562", "url": "https://en.wikipedia.org/wiki?curid=29957562", "title": "Infinite expression", "text": "Infinite expression\n\nIn mathematics, an infinite expression is an expression in which some operators take an infinite number of arguments, or in which the nesting of the operators continues to an infinite depth. A generic concept for infinite expression can lead to ill-defined or self-inconsistent constructions (much like a set of all sets), but there are several instances of infinite expressions that are well defined.\n\nExamples of well-defined infinite expressions include infinite sums, whether expressed using summation notation or as an infinite series, such as\n\ninfinite products, whether expressed using product notation or expanded, such as\n\ninfinite nested radicals, such as\n\ninfinite power towers, such as \n\nand infinite continued fractions, whether expressed using Gauss's Kettenbruch notation or expanded, such as\n\nIn infinitary logic, one can use infinite conjunctions and infinite disjunctions.\n\nEven for well-defined infinite expressions, the \"value\" of the infinite expression may be ambiguous or not well defined; for instance, there are multiple summation rules available for assigning values to series, and the same series may have different values according to different summation rules if the series is not absolutely convergent.\n\nFrom the point of view of the hyperreals, such an infinite expression formula_6 is obtained in every case from the sequence formula_7 of finite expressions, by evaluating the sequence at a hypernatural value formula_8 of the index \"n\", and applying the standard part, so that formula_9.\n\n"}
{"id": "1430989", "url": "https://en.wikipedia.org/wiki?curid=1430989", "title": "Ingrid Daubechies", "text": "Ingrid Daubechies\n\nIngrid Daubechies ( ; ; born 17 August 1954) is a Belgian physicist and mathematician. Between 2004 and 2011 she was the William R. Kenan, Jr. Professor in the mathematics and applied mathematics departments at Princeton University. She taught at Princeton for 16 years. In January 2011 she moved to Duke University as a James B. Duke Professor of mathematics. \n\nDaubechies was the first woman to be president of the International Mathematical Union (2011–2014). She is best known for her work with wavelets in image compression.\n\nDaubechies was born in Houthalen, Belgium, as the daughter of Marcel Daubechies (a civil mining engineer) and Simonne Duran (then a homemaker, later a criminologist). She remembers that when she was a little girl and could not sleep, she did not count numbers, as you would expect from a child, but started to multiply numbers by two from memory. Thus, as a child, she already familiarized herself with the properties of exponential growth. Her parents found out that mathematical conceptions, like cone and tetrahedron, were familiar to her before she reached the age of 6. She excelled at the primary school, moved up a class after only 3 months. After completing the Lyceum in Hasselt she entered the Vrije Universiteit Brussel at 17. \n\nDaubechies completed her undergraduate studies in physics at the Vrije Universiteit Brussel in 1975. During the next few years, she visited the CNRS Center for Theoretical Physics in Marseille several times, where she collaborated with Alex Grossmann; this work was the basis for her doctorate in quantum mechanics. She obtained her Ph.D. in theoretical physics in 1980. \n\nDaubechies continued her research career at the Vrije Universiteit Brussel until 1987, rising through the ranks to positions roughly equivalent with research assistant-professor in 1981 and research associate-professor 1985, funded by a fellowship from the NFWO (Nationaal Fonds voor Wetenschappelijk Onderzoek).\n\nDaubechies spent most of 1986 as a guest-researcher at the Courant Institute of Mathematical Sciences. At Courant she made her best-known discovery: based on quadrature mirror filter-technology she constructed compactly supported continuous wavelets that would require only a finite amount of processing, in this way enabling wavelet theory to enter the realm of digital signal processing.\n\nIn July 1987, Daubechies joined the Murray Hill AT&T Bell Laboratories' New Jersey facility. In 1988 she published the result in Communications on Pure and Applied Mathematics.\n\nFrom 1994 to 2010, Daubechies was a professor at Princeton University, where she was active within the Program in Applied and Computational Mathematics. She was the first female full professor of mathematics at Princeton. In January 2011 she moved to Duke University to serve as a professor of mathematics. She is currently the James B. Duke professor in the department of mathematics and electrical and computer engineering at Duke University. In 2016, she and Heekyoung Hahn founded Duke Summer Workshop in Mathematics for female rising high school seniors.\n\nThe name Daubechies is widely associated with the orthogonal Daubechies wavelet and the biorthogonal CDF wavelet. A wavelet from this family of wavelets is now used in the JPEG 2000 standard.\nHer research involves the use of automatic methods from both mathematics, technology and biology to extract information from samples like bones and teeth. She also developed sophisticated image processing techniques used to help establish the authenticity and age of some of the world’s most famous works of art including paintings by Vincent van Gogh and Rembrandt.\n\nDaubechies is one of the world’s most cited mathematicians recognized for her study of the mathematical methods that enhance image-compression technology. She is a member of the National Academy of Engineering, the National Academy of Sciences and the American Academy of Arts and Sciences.\n\nDaubechies received the Louis Empain Prize for Physics in 1984, awarded once every five years to a Belgian scientist on the basis of work done before the age of 29. Between 1992 and 1997 she was a fellow of the MacArthur Foundation and in 1993 was elected to the American Academy of Arts and Sciences. In 1994 she received the American Mathematical Society Steele Prize for Exposition for her book \"Ten Lectures on Wavelets\" and was invited to give a plenary lecture at the International Congress of Mathematicians in Zurich. In 1997 she was awarded the AMS Ruth Lyttle Satter prize. In 1998, she was elected to the United States National Academy of Sciences and won the Golden Jubilee Award for Technological Innovation from the IEEE Information Theory Society\nShe became a foreign member of the Royal Netherlands Academy of Arts and Sciences in 1999.\n\nIn 2000, Daubechies became the first woman to receive the National Academy of Sciences Award in Mathematics, presented every 4 years for excellence in published mathematical research. The award honored her \"for fundamental discoveries on wavelets and wavelet expansions and for her role in making wavelets methods a practical basic tool of applied mathematics\". She was awarded the Basic Research Award, German Eduard Rhein Foundation and the NAS Award in Mathematics.\n\nIn January 2005, Daubechies became the third woman since 1924 to give the Josiah Willard Gibbs Lecture sponsored by the American Mathematical Society. Her talk was on \"The Interplay Between Analysis and Algorithm\".\n\nDaubechies was the 2006 Emmy Noether Lecturer at the San Antonio Joint Mathematics Meetings. In September 2006, the Pioneer Prize from the International Council for Industrial and Applied Mathematics was awarded jointly to Daubechies and Heinz Engl.\n\nIn 2010 she was awarded an honorary doctorate by The Norwegian University of Science and Technology (NTNU).\n\nIn 2011, Daubechies was the SIAM John von Neumann Lecturer, and was awarded the IEEE Jack S. Kilby Signal Processing Medal, the Leroy P. Steele Prize for Seminal Contribution to Research from the American Mathematical Society, and the Benjamin Franklin Medal in Electrical Engineering from the Franklin Institute.\n\nIn 2012, King Albert II of Belgium granted Daubechies the title of Baroness. She also won the 2012 Nemmers Prize in Mathematics, Northwestern University,\nand the 2012 BBVA Foundation Frontiers of Knowledge Award in the Basic Sciences category (jointly with David Mumford).\n\nIn 2015, Daubechies gave the Gauss Lecture of the German Mathematical Society. The Simons Foundation, a private foundation based in New York City that funds research in mathematics and the basic sciences, gave Daubechies the Math + X Investigator award, which provides money to professors at American and Canadian universities to encourage new partnerships between mathematicians and researchers other fields of science. She was the one to suggest Simons that the foundation should fund not new research but better mechanisms for interpreting existing data.\n\nIn 2018, Daubechies won the William Benter Prize in Applied Mathematics from City University of Hong Kong (CityU). She is the first female recipient of the award. Prize officials cited Professor Daubechies's pioneering work in wavelet theory and her \"exceptional contributions to a wide spectrum of scientific and mathematical subjects...her work in enabling the mobile smartphone revolution is truly symbolic of the era.\"\n\nShe is part of the 2019 class of fellows of the Association for Women in Mathematics.\n\nIn 1985, Daubechies met mathematician Robert Calderbank, then on a 3-month exchange visit from AT&T Bell Laboratories, New Jersey to the Brussels-based mathematics division of Philips Research; they married in 1987. They have two children, Michael and Carolyn Calderbank.\n\n\n"}
{"id": "24256229", "url": "https://en.wikipedia.org/wiki?curid=24256229", "title": "Jure Zupan", "text": "Jure Zupan\n\nJure Zupan is a Slovenian physicist and founder of chemomectrics research in Slovenia, known for his work in applications and development of artificial neural networks in chemistry.\n\nZupan was born in Ljubljana, Slovenia in 1943. He studied Physics at the University of Ljubljana and graduated in 1966. He obtained his PhD in Chemistry in 1972. He did his first research on the magnetic properties of solids at the Josef Stefan Institute, Ljubljana (1963–1973). In 1974 he has joined National Institute of Chemistry in Ljubljana to work on Computerized Databases, Chemometrics, and Artificial Intelligence. He did his post doctoral research at ETH Zürich (1975) and at NIH, Bethesda (1978).\n\nSince 1985 he is a Full professor at the University of Ljubljana. He was Visiting Professor at the Arizona State University in Tempe, USA (1982), at the Vrije Universiteit Brussel, Belgium (1988), for 3 consecutive years (each year for three months) at the Technical University Munich, Germany (1990–1992), and at the University Rovira i Virgili, Tarragona, Spain (1995). After 1988 his research focused to the field of Artificial Neural Networks. He is now mostly interested in the multi-dimensional data representation and context extraction from large assembles of multi-dimensional data. He is member of the European Academy of Science (Salzburg) and member of the Engineering Academy of Slovenia.\n\nZupan is author and editor of 10 books and monographs and has co-authored more than 200 articles. With Johann Gasteiger he co-authored \"Neural Networks in Chemistry and Drug Design\". The book received more than 500 citations and was nominated the book of the month in 1993.\n\n\n"}
{"id": "13213701", "url": "https://en.wikipedia.org/wiki?curid=13213701", "title": "Kōmura's theorem", "text": "Kōmura's theorem\n\nIn mathematics, Kōmura's theorem is a result on the differentiability of absolutely continuous Banach space-valued functions, and is a substantial generalization of Lebesgue's theorem on the differentiability of the indefinite integral, which is that Φ : [0, \"T\"] → R given by\n\nis differentiable at \"t\" for almost every 0 < \"t\" < \"T\" when \"φ\" : [0, \"T\"] → R lies in the \"L\" space \"L\"([0, \"T\"]; R).\n\nLet (\"X\", || ||) be a reflexive Banach space and let \"φ\" : [0, \"T\"] → \"X\" be absolutely continuous. Then \"φ\" is (strongly) differentiable almost everywhere, the derivative \"φ\"′ lies in the Bochner space \"L\"([0, \"T\"]; \"X\"), and, for all 0 ≤ \"t\" ≤ \"T\",\n\n"}
{"id": "48330848", "url": "https://en.wikipedia.org/wiki?curid=48330848", "title": "Lior Pachter", "text": "Lior Pachter\n\nLior Samuel Pachter is a computational biologist. He works at the California Institute of Technology, where he is the Bren Professor of Computational Biology. He has widely varied research interests including genomics, combinatorics, computational geometry, machine learning, scientific computing, and statistics.\n\nPachter was born in Israel and grew up in South Africa.\nHe earned a bachelor's degree in mathematics from the California Institute of Technology in 1994. He completed his doctorate in mathematics from the Massachusetts Institute of Technology in 1999, supervised by Bonnie Berger, with Eric Lander and Daniel Kleitman as co-advisors. \n\nPachter joined the University of California, Berkeley faculty in 1999 and was given the Sackler Chair in 2012.\n\nAs well as for his technical contributions, Pachter is known for using new media to promote open science and for a thought experiment he posted on his blog according to which 'the nearest neighbor to the \"perfect human\"' is from Puerto Rico. This received considerable media attention, and a response was published in \"Scientific American\".\nIn 2017, Pachter was elected a Fellow of the International Society for Computational Biology (ISCB).\n"}
{"id": "24700145", "url": "https://en.wikipedia.org/wiki?curid=24700145", "title": "Lupanov representation", "text": "Lupanov representation\n\nLupanov's (\"k\", \"s\")-representation, named after Oleg Lupanov, is a way of representing Boolean circuits so as to show that the reciprocal of the Shannon effect. Shannon had showed that almost all Boolean functions of \"n\" variables need a circuit of size at least 2\"n\". The reciprocal is that:\nAll Boolean functions of \"n\" variables can be computed with a circuit of at most 2\"n\" + o(2\"n\") gates.\n\nThe idea is to represent the values of a boolean function \"ƒ\" in a table of 2 rows, representing the possible values of the \"k\" first variables \"x\", ..., ,\"x\", and 2 columns representing the values of the other variables.\n\nLet \"A\", ..., \"A\" be a partition of the rows of this table such that for \"i\" < \"p\", |\"A\"| = \"s\" and formula_1.\nLet \"ƒ\"(\"x\") = \"ƒ\"(\"x\") iff \"x\" ∈ \"A\".\n\nMoreover, let formula_2 be the set of the columns whose intersection with formula_3 is formula_4.\n\n"}
{"id": "25011970", "url": "https://en.wikipedia.org/wiki?curid=25011970", "title": "Michael Saks (mathematician)", "text": "Michael Saks (mathematician)\n\nMichael Ezra Saks is an American mathematician. He is currently the Department Chair of the Mathematics Department at Rutgers University (2017-) and from (2006–2010) was director of the Mathematics Graduate Program at Rutgers University. Saks received his Ph.D from the Massachusetts Institute of Technology in 1980 after completing his dissertation entitled \"Duality Properties of Finite Set Systems\" under his advisor Daniel J. Kleitman.\n\nA list of his publications and collaborations may be found at DBLP.\n\nIn 2016 he became a Fellow of the Association for Computing Machinery.\n\nSaks research in computational complexity theory, combinatorics, and graph theory has contributed to the study of lower bounds in order theory, randomized computation, and space–time tradeoff.\n\nIn Kahn and Saks (1984) it was shown there exist a tight information-theoretical lower bound for sorting under partially ordered information up to a multiplicative constant.\n\nIn the first super-linear lower bound for the noisy broadcast problem was proved. In a noisy broadcast model, formula_1 processors formula_2 are assigned a local input bit formula_3. Each processor may perform a \"noisy broadcast\" to all other processors where the received bits may be independently flipped with a fixed probability. The problem is for processor formula_4 to determine formula_5 for some function formula_6. Saks et al. showed that an existing protocol by Gallager was indeed optimal by a reduction from a generalized noisy decision tree and produced a formula_7 lower bound on the depth of the tree that learns the input.\n\nIn Beame et al. (2003) the first time–space lower bound trade-off for randomized computation of decision problems was proved.\n\nSaks holds positions in the following journal editorial boards:\n\n"}
{"id": "1096396", "url": "https://en.wikipedia.org/wiki?curid=1096396", "title": "Morley's categoricity theorem", "text": "Morley's categoricity theorem\n\nIn model theory, a branch of mathematical logic, a theory is κ-categorical (or categorical in κ) if it has exactly one model of cardinality κ up to isomorphism.\nMorley's categoricity theorem is a theorem of , which states that if a first-order theory in a countable language is categorical in some uncountable cardinality, then it is categorical in all uncountable cardinalities.\n\nOswald Veblen in 1904 defined a theory to be categorical if all of its models are isomorphic. It follows from the definition above and the Löwenheim–Skolem theorem that any first-order theory with a model of infinite cardinality cannot be categorical. One is then immediately led to the more subtle notion of κ-categoricity, which asks: for which cardinals κ is there exactly one model of cardinality κ of the given theory \"T\" up to isomorphism? This is a deep question and significant progress was only made in 1954 when Jerzy Łoś noticed that, at least for complete theories \"T\" over countable languages with at least one infinite model, he could only find three ways for \"T\" to be κ-categorical at some κ:\n\n\nIn other words, he observed that, in all the cases he could think of, κ-categoricity at any one uncountable cardinal implied κ-categoricity at all other uncountable cardinals. This observation spurred a great amount of research into the 1960s, eventually culminating in Michael Morley's famous result that these are in fact the only possibilities. The theory was subsequently extended and refined by Saharon Shelah in the 1970s and beyond, leading to stability theory and Shelah's more general programme of classification theory.\n\nThere are not many natural examples of theories that are categorical in some uncountable cardinal. The known examples include:\n\nThere are also examples of theories that are categorical in ω but not categorical in uncountable cardinals. \nThe simplest example is the theory of an equivalence relation with exactly two equivalence classes both of which are infinite. Another example is the theory of dense linear orders with no endpoints; Cantor proved that any such countable linear order is isomorphic to the rational numbers.\n\nAny theory \"T\" categorical in some infinite cardinal κ is very close to being complete. More precisely, the Łoś–Vaught test states that if a satisfiable theory has no finite models and is categorical in some infinite cardinal κ at least equal to the cardinality of its language, then the theory is complete. The reason is that all infinite models are equivalent to some model of cardinal κ by the Löwenheim–Skolem theorem, and so are all equivalent as the theory is categorical in κ. Therefore, the theory is complete as all models are equivalent. The assumption that the theory have no finite models is necessary.\n\n\n"}
{"id": "2782164", "url": "https://en.wikipedia.org/wiki?curid=2782164", "title": "Mostow rigidity theorem", "text": "Mostow rigidity theorem\n\nIn mathematics, Mostow's rigidity theorem, or strong rigidity theorem, or Mostow–Prasad rigidity theorem, essentially states that the geometry of a complete, finite-volume hyperbolic manifold of dimension greater than two is determined by the fundamental group and hence unique. The theorem was proven for closed manifolds by and extended to finite volume manifolds by in 3 dimensions, and by in all dimensions at least 3. gave an alternate proof using the Gromov norm.\n\nWhile the theorem shows that the deformation space of (complete) hyperbolic structures on a finite volume hyperbolic formula_1-manifold (for \"n\" > 2) is a point, for a hyperbolic surface of genus \"g\" > 1 there is a moduli space of dimension 6\"g\" − 6 that parameterizes all metrics of constant curvature (up to diffeomorphism), a fact essential for Teichmüller theory. There is also a rich theory of deformation spaces of hyperbolic structures on \"infinite\" volume manifolds in three dimensions.\n\nThe theorem can be given in a geometric formulation (pertaining to finite-volume, complete manifolds), and in an algebraic formulation (pertaining to lattices in Lie groups).\n\nLet formula_2 be the formula_1-dimensional hyperbolic space. A complete hyperbolic manifold can be defined as a quotient of formula_2 by a group of isometries acting freely and properly discontinuously (it is equivalent to define it as a Riemannian manifold with sectional curvature -1 which is complete). It is of finite volume if its volume is finite (for example if it is compact). The Mostow rigidity theorem may be stated as:\n\nHere formula_11 is the fundamental group of a manifold formula_12. If formula_12 is an hyperbolic manifold obtained as the quotient of formula_2 by a group formula_15 then formula_16.\n\nAn equivalent statement is that any homotopy equivalence from formula_5 to formula_6 can be homotoped to a unique isometry. The proof actually shows that if formula_6 has greater dimension than formula_5 then there can be no homotopy equivalence between them.\n\nThe group of isometries of hyperbolic space formula_2 can be identified with the Lie group formula_22 (the projective orthogonal group of a quadratic form of signature formula_23. Then the following statement is equivalent to the one above.\n\nMostow rigidity holds (in its geometric formulation) more generally for fundamental groups of all complete, finite volume locally symmetric spaces of dimension at least 3, or in its algebraic formulation for all lattices in simple Lie groups not locally isomorphic to formula_34.\n\nIt follows from the Mostow rigidity theorem that the group of isometries of a finite-volume hyperbolic \"n\"-manifold \"M\" (for \"n\">2) is finite and isomorphic to Out(\"π\"(\"M\")).\n\nMostow rigidity was also used by Thurston to prove the uniqueness of circle packing representations of triangulated planar graphs.\n\nA consequence of Mostow rigidity of interest in geometric group theory is that there exist hyperbolic groups which are quasi-isometric but not commensurable to each other.\n\n\n"}
{"id": "372397", "url": "https://en.wikipedia.org/wiki?curid=372397", "title": "Multimagic cube", "text": "Multimagic cube\n\nIn mathematics, a \"P\"-multimagic cube is a magic cube that remains magic even if all its numbers are replaced by their \"k\"-th power for 1 ≤ \"k\" ≤ \"P\". Thus, a magic cube is bimagic when it is 2-multimagic, and trimagic when it is 3-multimagic, tetramagic when it is 4-multimagic. A \"P\"-multimagic cube is said to be semi-perfect if the \"k\"-th power cubes are perfect for 1 ≤ \"k\" < \"P\", and the \"P\"-th power cube is semiperfect. If all \"P\" of the power cubes are perfect, the multimagic cube is said to be perfect. \n\nThe first known example of a bimagic cube was given by John Hendricks in 2000; it is a semiperfect cube of order 25 and magic constant 195325. In 2003, C. Bower discovered two semi-perfect bimagic cubes of order 16, and a perfect bimagic cube of order 32. \n\nMathWorld reports that only two trimagic cubes are known, discovered by C. Bower in 2003; a semiperfect cube of order 64 and a perfect cube of order 256. It also reports that he discovered the only two known tetramagic cubes, a semiperfect cube of order 1024, and perfect cube of order 8192.\n\n"}
{"id": "39245484", "url": "https://en.wikipedia.org/wiki?curid=39245484", "title": "Nonlinear realization", "text": "Nonlinear realization\n\nIn mathematical physics, nonlinear realization of a Lie group \"G\" possessing a Cartan subgroup \"H\" is a particular induced representation of \"G\". In fact, it is a representation of a Lie algebra formula_1 of \"G\" in a neighborhood of its origin.\nA nonlinear realization, when restricted to the subgroup \"H\" reduces to a linear representation.\n\nA nonlinear realization technique is part and parcel of many field theories with spontaneous symmetry breaking, e.g., chiral models, chiral symmetry breaking, Goldstone boson theory, classical Higgs field theory, gauge gravitation theory and supergravity.\n\nLet \"G\" be a Lie group and \"H\" its Cartan subgroup which admits a linear representation in a vector space \"V\". A Lie\nalgebra formula_1 of \"G\" splits into the sum formula_3 of the Cartan subalgebra formula_4 of \"H\" and its supplement formula_5, such that\n\nThere exists an open neighborhood \"U\" of the unit of \"G\" such\nthat any element formula_9 is uniquely brought into the form\n\nLet formula_11 be an open neighborhood of the unit of \"G\" such that\nformula_12, and let formula_13 be an open neighborhood of the\n\"H\"-invariant center formula_14 of the quotient \"G/H\" which consists of elements\n\nThen there is a local section formula_16 of formula_17\nover formula_13. \n\nWith this local section, one can define the induced representation, called the nonlinear realization, of elements formula_19 on formula_20 given by the expressions\n\nThe corresponding nonlinear realization of a Lie algebra\nformula_1 of \"G\" takes the following form.\n\nLet formula_23, formula_24 be the bases for formula_5 and formula_4, respectively, together with the commutation relations\n\nThen a desired nonlinear realization of formula_1 in formula_29 reads\n\nup to the second order in formula_32. \n\nIn physical models, the coefficients formula_32 are treated as Goldstone fields. Similarly, nonlinear realizations of Lie superalgebras are considered.\n\n\n"}
{"id": "11240093", "url": "https://en.wikipedia.org/wiki?curid=11240093", "title": "Nontransitive game", "text": "Nontransitive game\n\nA non-transitive game is a game for which the various strategies produce one or more \"loops\" of preferences. In a non-transitive game in which strategy A is preferred over strategy B, and strategy B is preferred over strategy C, strategy A is \"not\" necessarily preferred over strategy C.\n\nA prototypical example non-transitive game is the game rock, paper, scissors which is explicitly constructed as a non-transitive game. In probabilistic games like Penney's game, the violation of transitivity results in a more subtle way, and is often presented as a probability paradox.\n\n\n"}
{"id": "47926105", "url": "https://en.wikipedia.org/wiki?curid=47926105", "title": "Open Energy Modelling Initiative", "text": "Open Energy Modelling Initiative\n\nThe Open Energy Modelling Initiative (openmod) is a grass roots community of energy system modellers from universities and research institutes across Europe and elsewhere. The initiative promotes the use of open-source software and open data in energy system modelling for research and policy advice. The Open Energy Modelling Initiative documents a variety of open-source energy models and addresses practical and conceptual issues regarding their development and application. The initiative runs an email list, an internet forum, and a wiki and hosts occasional academic workshops. A statement of aims is available.\n\nThe application of open-source development to energy modelling dates back to around 2010. This section provides some background for the growing interest in open methods.\n\nJust two active open energy modelling projects were cited in a 2011 paper: OSeMOSYS and TEMOA. Balmorel was also open at that time, having been made public in 2001.\n, the openmod wiki lists 24 such undertakings.\n\nAn innovative 2012 paper presents the case for using \"open, publicly accessible software and data as well as crowdsourcing techniques to develop robust energy analysis tools\". The paper claims that these techniques can produce high-quality results and are particularly relevant for developing countries.\n\nThere is an increasing call for the energy models and datasets used for energy policy analysis and advice to be made public in the interests of transparency and quality. A 2010 paper concerning energy efficiency modeling argues that \"an open peer review process can greatly support model verification and validation, which are essential for model development\". One 2012 study argues that the source code and datasets used in such models should be placed under publicly accessible version control to enable third-parties to run and check specific models. Another 2014 study argues that the public trust needed to underpin a rapid transition in energy systems can only be built through the use of transparent open-source energy models. The UK TIMES project (UKTM) is open source, according to a 2014 presentation, because \"energy modelling must be replicable and verifiable to be considered part of the scientific process\" and because this fits with the \"drive towards clarity and quality assurance in the provision of policy insights\". In 2016, the Deep Decarbonization Pathways Project (DDPP) is seeking to improve its modelling methodologies, a key motivation being \"the intertwined goals of transparency, communicability and policy credibility.\" A 2016 paper argues that model-based energy scenario studies, wishing to influence decision-makers in government and industry, must become more comprehensible and more transparent. To these ends, the paper provides a checklist of transparency criteria that should be completed by modelers. The authors note however that they \"consider open source approaches to be an extreme case of transparency that does not automatically facilitate the comprehensibility of studies for policy advice.\" An editorial from 2016 opines that closed energy models providing public policy support \"are inconsistent with the open access movement [and] funded research\". A 2017 paper lists the benefits of open data and models and the reasons that many projects nonetheless remain closed. The paper makes a number of recommendations for projects wishing to transition to a more open approach. The authors also conclude that, in terms of openness, energy research has lagged behind other fields, most notably physics, biotechnology, and medicine. Moreover:\n\nA one-page opinion piece in \"Nature News\" from 2017 advances the case for using open energy data and modeling to build public trust in policy analysis. The article also argues that scientific journals have a responsibility to require that data and code be submitted alongside text for scrutiny, currently only \"Energy Economics\" makes this practice mandatory within the energy domain.\n\nIssues surrounding copyright remain at the forefront with regard to open energy data. Most energy datasets are collated and published by official or semi-official sources, for example, national statistics offices, transmission system operators, and electricity market operators. The doctrine of open data requires that these datasets be available under free licenses (such as ) or be in the public domain. But most published energy datasets carry proprietary licenses, limiting their reuse in numerical and statistical models, open or otherwise. Measures to enforce market transparency have not helped because the associated information is normally licensed to preclude downstream usage. Recent transparency measures include the 2013 European energy market transparency regulation 543/2013 and a 2016 amendment to the German Energy Industry Act to establish a nation energy information platform, slated to launch on 1July 2017. Energy databases are protected under general database law, irrespective of the copyright status of the information they hold.\n\nIn December 2017, participants from the Open Energy Modelling Initiative and allied research communities made a written submission to the European Commission on the of public sector information. The document provides a comprehensive account of the data issues faced by researchers engaged in open energy system modeling and energy market analysis and quoted extensively from a German legal opinion.\n\nIn May 2016 the European Union announced that \"all scientific articles in Europe must be freely accessible as of 2020\". This is a step in the right direction, but the new policy makes no mention of open software and its importance to the scientific process. In August 2016, the United States government announced a new federal source code policy which mandates that at least 20% of custom source code developed by or for any agency of the federal government be released as open-source software (OSS). The US Department of Energy (DOE) is participating in the program. The project is hosted on a dedicated website and subject to a three-year pilot. Open-source campaigners are using the initiative to advocate that European governments adopt similar practices. In 2017 the Free Software Foundation Europe (FSFE) issued a position paper calling for free software and open standards to be central to European science funding, including the flagship EU program Horizon2020. The position paper focuses on open data and open data processing and the question of open modeling is not traversed perse.\n\nThe Open Energy Modelling Initiative participants take turns to host regular academic workshops.\n\n\nRelated to openmod\n\n\nOpen energy data\n\n\nSimilar initiatives\n\n\nOther\n\n"}
{"id": "6793679", "url": "https://en.wikipedia.org/wiki?curid=6793679", "title": "Pointwise mutual information", "text": "Pointwise mutual information\n\nPointwise mutual information (PMI), or point mutual information, is a measure of association used in information theory and statistics. In contrast to mutual information (MI) which builds upon PMI, it refers to single events, whereas MI refers to the average of all possible events.\n\nThe PMI of a pair of outcomes \"x\" and \"y\" belonging to discrete random variables \"X\" and \"Y\" quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions, assuming independence. Mathematically:\n\nThe mutual information (MI) of the random variables \"X\" and \"Y\" is the expected value of the PMI over all possible outcomes (with respect to the joint distribution formula_2).\n\nThe measure is symmetric (formula_3). It can take positive or negative values, but is zero if \"X\" and \"Y\" are independent. Note that even though PMI may be negative or positive, its expected outcome over all joint events (MI) is positive. PMI maximizes when \"X\" and \"Y\" are perfectly associated (i.e. formula_4 or formula_5), yielding the following bounds:\n\nFinally, formula_7 will increase if formula_4 is fixed but formula_9 decreases.\n\nHere is an example to illustrate:\n\nUsing this table we can marginalize to get the following additional table for the individual distributions:\n\nWith this example, we can compute four values for formula_10. Using base-2 logarithms:\n\nPointwise Mutual Information has many of the same relationships as the mutual information. In particular,\n\nformula_12\n\nWhere formula_13 is the self-information, or formula_14.\n\nPointwise mutual information can be normalized between [-1,+1] resulting in -1 (in the limit) for never occurring together, 0 for independence, and +1 for complete co-occurrence.\n\nformula_15\n\nIn addition to the above-mentioned npmi, PMI has many other interesting variants. A comparative study of these variants can be found in \n\nLike mutual information, point mutual information follows the chain rule, that is,\n\nThis is easily proven by:\n\nIn computational linguistics, PMI has been used for finding collocations and associations between words. For instance, countings of occurrences and co-occurrences of words in a text corpus can be used to approximate the probabilities formula_9 and formula_2 respectively. The following table shows counts of pairs of words getting the most and the least PMI scores in the first 50 millions of words in Wikipedia (dump of October 2015) filtering by 1,000 or more co-occurrences. The frequency of each count can be obtained by dividing its value by 50,000,952. (Note: natural log is used to calculate the PMI values in this example, instead of log base 2)\nGood collocation pairs have high PMI because the probability of co-occurrence is only slightly lower than the probabilities of occurrence of each word. Conversely, a pair of words whose probabilities of occurrence are considerably higher than their probability of co-occurrence gets a small PMI score.\n\n\n"}
{"id": "35411073", "url": "https://en.wikipedia.org/wiki?curid=35411073", "title": "Raymond Clare Archibald", "text": "Raymond Clare Archibald\n\nRaymond Clare Archibald (7 October 1875 – 26 July 1955) was a prominent Canadian-American mathematician. He is known for his work as a historian of mathematics, his editorships of mathematical journals and his contributions to the teaching of mathematics.\n\nRaymond Clare Archibald was born in South Branch, Stewiacke, Nova Scotia on 7 October 1875. He was the son of Abram Newcomb Archibald (1849—1883) and Mary Mellish Archibald (1849—1901). He was the fourth cousin twice removed of the famous Canadian-American astronomer and mathematician Simon Newcomb (1835—1909).\n\nArchibald graduated in 1894 from Mount Allison College with B.A. degree in mathematics and teacher's certificate in violin. After teaching mathematics and violin for a year at the Mount Allison Ladies’ College he went to Harvard where he received a B.A. 1896 and a M.A. in 1897. He then traveled to Europe where he attended the University of Berlin during 1898 and received a Ph.D.cum laude from the University of Strassburg in 1900. His advisor was Karl Theodor Reye and title of his dissertation was The Cardioide and Some of its Related Curves.\n\nHe returned to Canada in 1900 and taught mathematics and violin at the Mount Allison Ladies’ College until 1907. After a one-year appointment at Acadia University he accepted an invitation of join the mathematics department at Brown University. He stayed at Brown for the rest of his career becoming a Professor Emeritus in 1943. While at Brown he created one of the finest mathematical libraries in the western hemisphere.\n\nArchibald returned to Mount Allison in 1954 to curate the Mary Mellish Archibald Memorial Library, the library he had founded in 1905 to honor his mother. At his death the library contained 23,000 volumes, 2,700 records, and 70,000 songs in American and English poetry and drama.\n\nRaymond Clare Archibald was a world-renowned historian of mathematics with a lifelong concern for the teaching of mathematics in secondary schools. At the presentation of his portrait to Brown University the head of the mathematics department, Professor Clarence Raymond Adams (1898–1965) said of him:\n\n\"The instincts of the bibliophile were also his from early years. Possessing a passion for accurate detail, systematic by nature and blessed with a memory that was the marvel of his friends, he gradually acquired a knowledge of mathematical books and their values which has scarcely been equalled. This knowledge and an untiring energy he dedicated to the upbuilding of the mathematical library at Brown University. From modest beginnings he has developed this essential equipment of the mathematical investigator to a point where it has no superior, in completeness and in convenience for the user.\"\n\nArchibald received honorary degrees from the University of Padua (LL.D., 1922), Mount Allison University (LL.D., 1923) and from Brown University (M.A. ad eundem, 1943).\n\n\nArchibald’s bibliography contains over 1,000 entries. He contributed to over 20 different journals, mathematical, scientific, educational and literary. The following are the books of which he is an author:\n\n\n\n"}
{"id": "35307933", "url": "https://en.wikipedia.org/wiki?curid=35307933", "title": "Resolution proof compression by splitting", "text": "Resolution proof compression by splitting\n\nIn mathematical logic, proof compression by splitting is an algorithm that operates as a post-process on resolution proofs. It was proposed by Scott Cotton in his paper \"Two Techniques for Minimizing Resolution Proof\".\n\nThe Splitting algorithm is based on the following observation:\n\nGiven a proof of unsatisfiability formula_1 and a variable formula_2, it is easy to re-arrange (split) the proof in a proof of formula_2 and a proof of formula_4 and the recombination of these two proofs (by an additional resolution step) may result in a proof smaller than the original.\n\nNote that applying Splitting in a proof formula_1 using a variable formula_2 does not invalidates a latter application of the algorithm using a differente variable formula_7. Actually, the method proposed by Cotton generates a sequence of proofs formula_8, where each proof formula_9 is the result of applying Splitting to formula_10. During the construction of the sequence, if a proof formula_11 happens to be too large, formula_12 is set to be the smallest proof in formula_13.\n\nFor achieving a better compression/time ratio, a heuristic for variable selection is desirable. For this purpose, Cotton defines the \"additivity\" of a resolution step (with antecedents formula_14 and formula_15 and resolvent formula_16):\n\nThen, for each variable formula_18, a score is calculated summing the additivity of all the resolution steps in formula_1 with pivot formula_18 together with the number of these resolution steps. Denoting each score calculated this way by formula_21, each variable is selected with a probability proportional to its score:\n\nTo split a proof of unsatisfiability formula_1 in a proof formula_24 of formula_2 and a proof formula_26 of formula_27, Cotton proposes the following:\n\nLet formula_28 denote a literal and formula_29 denote the resolvent of clauses formula_14 and formula_15 where formula_32 and formula_33. Then, define the map formula_34 on nodes in the resolution dag of formula_1:\n\nAlso, let formula_37 be the empty clause in formula_1. Then, formula_24 and formula_26 are obtained by computing formula_41 and formula_42, respectively.\n"}
{"id": "880754", "url": "https://en.wikipedia.org/wiki?curid=880754", "title": "Restricted representation", "text": "Restricted representation\n\nIn mathematics, restriction is a fundamental construction in representation theory of groups. Restriction forms a representation of a subgroup from a representation of the whole group. Often the restricted representation is simpler to understand. Rules for decomposing the restriction of an irreducible representation into irreducible representations of the subgroup are called branching rules, and have important applications in physics. For example, in case of explicit symmetry breaking, the symmetry group of the problem is reduced from the whole group to one of its subgroups. In quantum mechanics, this reduction in symmetry appears as a splitting of degenerate energy levels into multiplets, as in the Stark or Zeeman effect.\n\nThe induced representation is a related operation that forms a representation of the whole group from a representation of a subgroup. The relation between restriction and induction is described by Frobenius reciprocity and the Mackey theorem. Restriction to a normal subgroup behaves particularly well and is often called Clifford theory after the theorem of A. H. Clifford. Restriction can be generalized to other group homomorphisms and to other rings.\n\nFor any group \"G\", its subgroup \"H\", and a linear representation \"ρ\" of \"G\", the restriction of \"ρ\" to \"H\", denoted\n\nis a representation of \"H\" on the same vector space by the same operators:\n\nClassical branching rules describe the restriction of an irreducible representation (, \"V\") of a classical group \"G\" to a classical subgroup \"H\", i.e. the multiplicity with which an irreducible representation (\"σ\", \"W\") of \"H\" occurs in . By Frobenius reciprocity for compact groups, this is equivalent to finding the multiplicity of in the unitary representation induced from σ. Branching rules for the classical groups were determined by\n\nThe results are usually expressed graphically using Young diagrams to encode the signatures used classically to label irreducible representations, familiar from classical invariant theory. Hermann Weyl and Richard Brauer discovered a systematic method for determining the branching rule when the groups \"G\" and \"H\" share a common maximal torus: in this case the Weyl group of \"H\" is a subgroup of that of \"G\", so that the rule can be deduced from the Weyl character formula. A systematic modern interpretation has been given by in the context of his theory of dual pairs. The special case where σ is the trivial representation of \"H\" was first used extensively by Hua in his work on the Szegő kernels of bounded symmetric domains in several complex variables, where the Shilov boundary has the form \"G\"/\"H\". More generally the Cartan-Helgason theorem gives the decomposition when \"G\"/\"H\" is a compact symmetric space, in which case all multiplicities are one; a generalization to arbitrary σ has since been obtained by . Similar geometric considerations have also been used by to rederive Littlewood's rules, which involve the celebrated Littlewood–Richardson rules for tensoring irreducible representations of the unitary groups.\n\nExample. The unitary group \"U\"(\"N\") has irreducible representations labelled by signatures\n\nwhere the \"f\" are integers. In fact if a unitary matrix \"U\" has eigenvalues \"z\", then the character of the corresponding irreducible representation is given by\n\nThe branching rule from \"U\"(\"N\") to \"U\"(\"N\" – 1) states that\n\nExample. The unitary symplectic group or quaternionic unitary group, denoted Sp(\"N\") or \"U\"(\"N\", H), is the group of all transformations of\nH which commute with right multiplication by the quaternions H and preserve the H-valued hermitian inner product\n\non H, where \"q\"* denotes the quaternion conjugate to \"q\". Realizing quaternions as 2 x 2 complex matrices, the group Sp(\"N\") is just the group of block matrices (\"q\") in SU(2\"N\") with\n\nwhere \"α\" and \"β\" are complex numbers.\n\nEach matrix \"U\" in Sp(\"N\") is conjugate to a block diagonal matrix with entries\n\nwhere |\"z\"| = 1. Thus the eigenvalues of \"U\" are (\"z\"). The irreducible representations of Sp(\"N\") are labelled by signatures\n\nwhere the \"f\" are integers. The character of the corresponding irreducible representation \"σ\" is given by\n\nThe branching rule from Sp(\"N\") to Sp(\"N\" – 1) states that\n\nHere \"f\" = 0 and the multiplicity \"m\"(f, g) is given by\n\nwhere\n\nis the non-increasing rearrangement of the 2\"N\" non-negative integers (\"f\"), (\"g\") and 0.\n\nExample. The branching from U(2\"N\") to Sp(\"N\") relies on two identities of Littlewood:\n\nwhere Π is the irreducible representation of \"U\"(2\"N\") with signature \"f\" ≥ ··· ≥ \"f\" ≥ 0 ≥ ··· ≥ 0.\n\nwhere \"f\" ≥ 0.\n\nThe branching rule from U(2\"N\") to Sp(\"N\") is given by\n\nwhere all the signature are non-negative and the coefficient \"M\" (g, h; k) is the multiplicity of the irreducible representation of \"U\"(\"N\") in the tensor product formula_14 . It is given combinatorially by the Littlewood–Richardson rule, the number of lattice permutations of the skew diagram k/h of weight g.\n\nThere is an extension of Littelwood's branching rule to arbitrary signatures due to . The Littlewood–Richardson coefficients \"M\" (g, h; f) are extended to allow the signature f to have 2\"N\" parts but restricting g to have even column-lengths (\"g\" = \"g\"). In this case the formula reads\n\nwhere \"M\" (g, h; f) counts the number of lattice permutations of f/h of weight g are counted for which 2\"j\" + 1 appears no lower than row \"N\" + \"j\" of f for 1 ≤ \"j\" ≤ |\"g\"|/2.\n\nExample. The special orthogonal group SO(\"N\") has irreducible ordinary and spin representations labelled by signatures\n\n\nThe \"f\" are taken in Z for ordinary representations and in ½ + Z for spin representations. In fact if an orthogonal matrix \"U\" has eigenvalues \"z\" for 1 ≤ \"i\" ≤ \"n\", then the character of the corresponding irreducible representation is given by\n\nfor \"N\" = 2\"n\" and by\n\nfor \"N\" = 2\"n\"+1.\n\nThe branching rules from SO(\"N\") to SO(\"N\" – 1) state that\n\nfor \"N\" = 2\"n\" + 1 and\n\nfor \"N\" = 2\"n\", where the differences \"f\" − \"g\" must be integers.\n\nSince the branching rules from \"U\"(\"N\") to U(\"N\" – 1) or SO(\"N\") to SO(\"N\" – 1) have multiplicity one, the irreducible summands corresponding to smaller and smaller \"N\" will eventually terminate in one-dimensional subspaces. In this way Gelfand and Tsetlin were able to obtain a basis of any irreducible representation of U(\"N\") or SO(\"N\") labelled by a chain of interleaved signatures, called a Gelfand–Tsetlin pattern.\nExplicit formulas for the action of the Lie algebra on the Gelfand–Tsetlin basis are given in .\n\nFor the remaining classical group Sp(\"N\"), the branching is no longer multiplicity free, so that if \"V\" and \"W\" are irreducible representation of Sp(\"N\" – 1) and Sp(\"N\") the space of intertwiners Hom(\"V\",\"W\") can have dimension greater than one. It turns out that the Yangian \"Y\"(formula_19), a Hopf algebra introduced by Ludwig Faddeev and collaborators, acts irreducibly on this multiplicity space, a fact which enabled to extend the construction of Gelfand–Tsetlin bases to Sp(\"N\").\n\nIn 1937 Alfred H. Clifford proved the following result on the restriction of finite-dimensional irreducible representations from a group \"G\" to a normal subgroup \"N\" of finite index:\n\nTheorem. Let : \"G\" formula_20 GL(\"n\",\"K\") be an irreducible representation with \"K\" a field. Then \nthe restriction of to \"N\" breaks up into a direct sum of inequivalent irreducible representations of \"N\" of equal dimensions. These irreducible representations of \"N\" lie in one orbit for the action of \"G\" by conjugation on the equivalence classes of irreducible representations of \"N\". In particular the number of distinct summands is no greater than the index of \"N\" in \"G\".\n\nTwenty years later George Mackey found a more precise version of this result for the restriction of irreducible unitary representations of locally compact groups to closed normal subgroups in what has become known as the \"Mackey machine\" or \"Mackey normal subgroup analysis\".\n\nFrom the point of view of category theory, restriction is an instance of a forgetful functor. This functor is exact, and its left adjoint functor is called \"induction\". The relation between restriction and induction in various contexts is called the Frobenius reciprocity. Taken together, the operations of induction and restriction form a powerful set of tools for analyzing representations. This is especially true whenever the representations have the property of complete reducibility, for example, in representation theory of finite groups over a field of characteristic zero.\n\nThis rather evident construction may be extended in numerous and significant ways. For instance we may take any group homomorphism φ from \"H\" to \"G\", instead of the inclusion map, and define the restricted representation of \"H\" by the composition\n\nWe may also apply the idea to other categories in abstract algebra: associative algebras, rings, Lie algebras, Lie superalgebras, Hopf algebras to name some. Representations or modules \"restrict\" to subobjects, or via homomorphisms.\n\n"}
{"id": "289450", "url": "https://en.wikipedia.org/wiki?curid=289450", "title": "Risch algorithm", "text": "Risch algorithm\n\nIn symbolic computation (or computer algebra), at the intersection of mathematics and computer science, the Risch algorithm is an algorithm for indefinite integration. It is used in some computer algebra systems to find antiderivatives. It is named after the American mathematician Robert Henry Risch, a specialist in computer algebra who developed it in 1968.\n\nThe algorithm transforms the problem of integration into a problem in algebra. It is based on the form of the function being integrated and on methods for integrating rational functions, radicals, logarithms, and exponential functions. Risch called it a decision procedure, because it is a method for deciding whether a function has an elementary function as an indefinite integral, and if it does, for determining that indefinite integral.\n\nThe complete description of the Risch algorithm takes over 100 pages. The Risch–Norman algorithm is a simpler, faster, but less powerful variant that was developed in 1976 by A. C. Norman.\n\nThe Risch algorithm is used to integrate elementary functions. These are functions obtained by composing exponentials, logarithms, radicals, trigonometric functions, and the four arithmetic operations (). Laplace solved this problem for the case of rational functions, as he showed that the indefinite integral of a rational function is a rational function and a finite number of constant multiples of logarithms of rational functions. The algorithm suggested by Laplace is usually described in calculus textbooks; as a computer program, it was finally implemented in the 1960s.\n\nLiouville formulated the problem that is solved by the Risch algorithm. Liouville proved by analytical means that if there is an elementary solution to the equation then there exist constants and functions and in the field generated by such that the solution is of the form\n\nRisch developed a method that allows one to consider only a finite set of functions of Liouville's form.\n\nThe intuition for the Risch algorithm comes from the behavior of the exponential and logarithm functions under differentiation. For the function , where and are differentiable functions, we have\n\nso if were in the result of an indefinite integration, it should be expected to be inside the integral. Also, as\n\nthen if were in the result of an integration, then only a few powers of the logarithm should be expected.\n\nFinding an elementary antiderivative is very sensitive to details. For instance, the following algebraic function has an elementary antiderivative:\n\nnamely:\n\nBut if the constant term 71 is changed to 72, it is not possible to represent the antiderivative in terms of elementary functions. Some computer algebra systems may here return an antiderivative in terms of \"non-elementary\" functions (i.e. elliptic integrals), which however are outside the scope of the Risch algorithm.\n\nThe following is a more complex example that involves both algebraic and transcendental functions:\n\nIn fact, the antiderivative of this function has a fairly short form:\n\nTransforming Risch's theoretical algorithm into an algorithm that can be effectively executed by a computer was a complex task which took a long time.\n\nThe case of the purely transcendental functions (which do not involve roots of polynomials) is relatively easy and was implemented early in most computer algebra systems. The first implementation was done by Joel Moses in Macsyma soon after the publication of Risch's paper.\n\nThe case of purely algebraic functions was solved and implemented in Reduce by James H. Davenport.\n\nThe general case was solved and implemented in Scratchpad, a precursor of Axiom, by Manuel Bronstein.\n\nThe Risch algorithm applied to general elementary functions is not an algorithm but a semi-algorithm because it needs to check, as a part of its operation, if certain expressions are equivalent to zero (constant problem), in particular in the constant field. For expressions that involve only functions commonly taken to be elementary it is not known whether an algorithm performing such a check exists or not (current computer algebra systems use heuristics); moreover, if one adds the absolute value function to the list of elementary functions, it is known that no such algorithm exists; see Richardson's theorem.\n\nNote that this issue also arises in the polynomial division algorithm; this algorithm will fail if it cannot correctly determine whether coefficients vanish identically. Virtually every non-trivial algorithm relating to polynomials uses the polynomial division algorithm, the Risch algorithm included. If the constant field is computable, i.e., for elements not dependent on , the problem of zero-equivalence is decidable, then the Risch algorithm is a complete algorithm. Examples of computable constant fields are and , i.e., rational numbers and rational functions in with rational number coefficients, respectively, where is an indeterminate that does not depend on .\n\nThis is also an issue in the Gaussian elimination matrix algorithm (or any algorithm that can compute the nullspace of a matrix), which is also necessary for many parts of the Risch algorithm. Gaussian elimination will produce incorrect results if it cannot correctly determine if a pivot is identically zero.\n\n"}
{"id": "22913531", "url": "https://en.wikipedia.org/wiki?curid=22913531", "title": "Science, Technology, Engineering and Mathematics Network", "text": "Science, Technology, Engineering and Mathematics Network\n\nThe Science, Technology, Engineering and Mathematics Network or STEMNET is an educational charity in the United Kingdom that seeks to encourage participation at school and college in science and engineering-related subjects and (eventually) work.\n\nIt is based at Woolgate Exchange near Moorgate tube station in London and was established in 1996. The chief executive is Kirsten Bodley. The STEMNET offices are housed within the Engineering Council.\n\nIts chief aim is to interest children in science, technology, engineering and mathematics. Primary school children can start to have an interest in these subjects, leading secondary school pupils to choose science A levels, which will lead to a science career. It supports the After School Science and Engineering Clubs at schools. There are also nine regional Science Learning Centres.\n\nSTEMNET used to receive funding from the Department for Education and Skills. Since June 2007, it receives funding from the Department for Children, Schools and Families and Department for Innovation, Universities and Skills, since STEMNET sits on the chronological dividing point (age 16) of both of the new departments.\n\n\n"}
{"id": "2456297", "url": "https://en.wikipedia.org/wiki?curid=2456297", "title": "Seiberg duality", "text": "Seiberg duality\n\nIn quantum field theory, Seiberg duality, conjectured by Nathan Seiberg, is an S-duality relating two different supersymmetric QCDs. The two theories are not identical, but they agree at low energies. More precisely under a renormalization group flow they flow to the same IR fixed point, and so are in the same universality class.\n\nIt was first presented in Seiberg's 1994 article Electric-Magnetic Duality in Supersymmetric Non-Abelian Gauge Theories. It is an extension to nonabelian gauge theories with N=1 supersymmetry of Montonen–Olive duality in N=4 theories and electromagnetic duality in abelian theories.\n\nSeiberg duality is an equivalence of the IR fixed points in an \"N\"=1 theory with SU(N) as the gauge group and N flavors of fundamental chiral multiplets and N flavors of antifundamental chiral multiplets in the chiral limit (no bare masses) and an N=1 chiral QCD with N-N colors and N flavors, where N and N are positive integers satisfying\n\nA stronger version of the duality relates not only the chiral limit but also the full deformation space of the theory. In the special case in which\n\nthe IR fixed point is a nontrivial interacting superconformal field theory. For a superconformal field theory, the anomalous scaling dimension of a chiral superfield formula_3 where R is the R-charge. This is an exact result.\n\nThe dual theory contains a fundamental \"meson\" chiral superfield M which is color neutral but transforms as a bifundamental under the flavor symmetries.\n\nThe dual theory contains the superpotential formula_4.\n\nBeing an S-duality, Seiberg duality relates the strong coupling regime with the weak coupling regime, and interchanges chromoelectric fields (gluons) with chromomagnetic fields (gluons of the dual gauge group), and chromoelectric charges (quarks) with nonabelian 't Hooft–Polyakov monopoles. In particular, the Higgs phase is dual to the confinement phase as in the dual superconducting model.\n\nThe mesons and baryons are preserved by the duality. However in the electric theory the meson is a quark bilinear (formula_5), while in the magnetic theory it is a fundamental field. In both theories the baryons are constructed from quarks, but the number of quarks in one baryon is the rank of the gauge group, which differs in the two dual theories.\n\nThe gauge symmetries of the theories do not agree, which is not problematic as the gauge symmetry is a feature of the formulation and not of the fundamental physics. The global symmetries relate distinct physical configurations and so they need to agree in any dual description.\n\nThe moduli spaces of the dual theories are identical.\n\nThe global symmetries agree, as do the charges of the mesons and baryons.\n\nIn certain cases it reduces to ordinary electromagnetic duality.\n\nIt may be embedded in string theory via Hanany-Witten brane cartoons consisting of intersecting D-branes. There it is realized as the motion of an NS5-brane which is conjectured to preserve the universality class.\n\nSix nontrivial anomalies may be computed on both sides of the duality, and they agree as they must in accordance with Gerard 't Hooft's anomaly matching conditions. The role of the additional fundamental meson superfield M in the dual theory is very crucial in matching the anomalies. The global gravitational anomalies also match up as the parity of the number of chiral fields is the same in both theories. The R-charge of the Weyl fermion in a chiral superfield is one less than the R-charge of the superfield. The R-charge of a gaugino is +1.\n\nAnother evidence for Seiberg duality comes from identifying the superconformal index, which is a generalization of the Witten index, for the electric and the magnetic phase. The identification gives rise to complicated integral identities which have been studied in the mathematical literature.\n\nSeiberg duality has been generalized in many directions. One generalization applies to quiver gauge theories, in which the flavor symmetries are also gauged. The simplest of these is a super QCD with the flavor group gauged and an additional term in the superpotential. It leads to a series of Seiberg dualities known as a duality cascade. It was introduced by Matthew Strassler and Igor Klebanov in Supergravity and a Confining Gauge Theory: Duality Cascades and formula_6SB-Resolution of Naked Singularities.\n\nIt is not known whether Seiberg duality exists in 3-dimensional nonabelian gauge theories with only 4 supercharges, although a conjecture has appeared in Fractional M2-branes in some special cases with Chern–Simons terms.\n\nElectric-Magnetic Duality in Supersymmetric Non-Abelian Gauge Theories by Nathan Seiberg.\n"}
{"id": "23850488", "url": "https://en.wikipedia.org/wiki?curid=23850488", "title": "Soundness (interactive proof)", "text": "Soundness (interactive proof)\n\nSoundness is a property of interactive proof systems that requires that no prover can make the verifier accept for a wrong statement formula_1 except with some small probability. The upper bound of this probability is referred to as the soundness error of a proof system.\n\nMore formally, for every prover formula_2, and every formula_1:\n\nfor some formula_5.\nAs long as the soundness error is bounded by a polynomial fraction of the potential running time of the verifier (i.e. formula_6), it is always possible to amplify soundness until the soundness error becomes negligible function relative to the running time of the verifier. This is achieved by repeating the proof and accepting only if all proofs verify. After formula_7 repetitions, a soundness error formula_8 will be reduced to formula_9.\n\n"}
{"id": "3973645", "url": "https://en.wikipedia.org/wiki?curid=3973645", "title": "Taylor expansions for the moments of functions of random variables", "text": "Taylor expansions for the moments of functions of random variables\n\nIn probability theory, it is possible to approximate the moments of a function \"f\" of a random variable \"X\" using Taylor expansions, provided that \"f\" is sufficiently differentiable and that the moments of \"X\" are finite. \n\nSince formula_2 the second term disappears. Also formula_3 is formula_4. Therefore,\n\nwhere formula_6 and formula_7 are the mean and variance of X respectively.\n\nIt is possible to generalize this to functions of more than one variable using multivariate Taylor expansions. For example,\n\nSimilarly,\n\nThe above is using a first order approximation unlike for the method used in estimating the first moment. It will be a poor approximation in cases where formula_10 is highly non-linear. This is a special case of the delta method. For example,\n\nThe second order approximation is:\n\n"}
{"id": "11090262", "url": "https://en.wikipedia.org/wiki?curid=11090262", "title": "Tom (pattern matching language)", "text": "Tom (pattern matching language)\n\nTom is a programming language particularly well-suited for programming various transformations on tree structures and XML based documents. Tom is a language extension which adds new matching primitives to C and Java as well as support for rewrite rules systems. The rules can be controlled using a strategy language.\n\nTom is good for:\n\n"}
{"id": "22284121", "url": "https://en.wikipedia.org/wiki?curid=22284121", "title": "Variable (computer science)", "text": "Variable (computer science)\n\nIn computer programming, a variable or scalar is a storage location (identified by a memory address) paired with an associated symbolic name (an \"identifier\"), which contains some known or unknown quantity of information referred to as a \"value\". The variable name is the usual way to reference the stored value, in addition to referring to the variable itself, depending on the context. This separation of name and content allows the name to be used independently of the exact information it represents. The identifier in computer source code can be bound to a value during run time, and the value of the variable may thus change during the course of program execution.\nVariables in programming may not directly correspond to the concept of variables in mathematics. The latter is abstract, having no reference to a physical object such as storage location. The value of a computing variable is not necessarily part of an equation or formula as in mathematics. Variables in computer programming are frequently given long names to make them relatively descriptive of their use, whereas variables in mathematics often have terse, one- or two-character names for brevity in transcription and manipulation.\n\nA variable's storage location may be referred by several different identifiers, a situation known as aliasing. Assigning a value to the variable using one of the identifiers will change the value that can be accessed through the other identifiers.\n\nCompilers have to replace variables' symbolic names with the actual locations of the data. While a variable's name, type, and location often remain fixed, the data stored in the location may be changed during program execution.\n\nIn imperative programming languages, values can generally be accessed or changed at any time. In pure functional and logic languages, variables are bound to expressions and keep a single value during their entire lifetime due to the requirements of referential transparency. In imperative languages, the same behavior is exhibited by (named) constants (symbolic constants), which are typically contrasted with (normal) variables.\n\nDepending on the type system of a programming language, variables may only be able to store a specified datatype (e.g. integer or string). Alternatively, a datatype may be associated only with the current value, allowing a single variable to store anything supported by the programming language.\n\nVariables and scope:-\n(A) Automatic variables: - Each local variable in a function comes into existence only when the function is called, and disappears when the function is exited. Such variables are known as automatic variables. \n(B) External variables: - These are variables that are external to a function on and can be accessed by name by any function. These variables remain in existence permanently; rather that appearing and disappearing as functions are called and exited, retain their values even after the functions that set them have returned.\n\nAn identifier referencing a variable can be used to access the variable in order to read out the value, or alter the value, or edit other attributes of the variable, such as access permission, locks, semaphores, etc.\n\nFor instance, a variable might be referenced by the identifier \"codice_1\" and the variable can contain the number 1956. If the same variable is referenced by the identifier \"codice_2\" as well, and if using this identifier \"codice_2\", the value of the variable is altered to 2009, then reading the value using the identifier \"codice_4\" will yield a result of 2009 and not 1956.\n\nIf a variable is only referenced by a single identifier that can simply be called \"the name of the variable\". Otherwise, we can speak of \"one of the names of the variable\". For instance, in the previous example, the \"codice_5\" is a name of the variable in question, and \"codice_2\" is another name of the same variable.\n\nThe \"scope\" of a variable describes where in a program's text the variable may be used, while the \"extent\" (or \"lifetime\") describes when in a program's execution a variable has a (meaningful) value. The scope of a variable is actually a property of the name of the variable, and the extent is a property of the variable itself. These should not be confused with \"context\" (also called \"environment\"), which is a property of the program, and varies by point in the source code or execution – see scope: overview. Further, object lifetime may coincide with variable lifetime, but in many cases is not tied to variable lifetime.\n\nA variable name's \"scope\" affects its \"extent\".\n\n\"Scope\" is an important part of the name resolution of a variable. Most languages define a specific \"scope\" for each variable (as well as any other named entity), which may differ within a given program. The scope of a variable is the portion of the program code for which the variable's name has meaning and for which the variable is said to be \"visible\". Entrance into that scope typically begins a variable's lifetime (as it comes into context) and exit from that scope typically ends its lifetime (as it goes out of context). For instance, a variable with \"lexical scope\" is meaningful only within a certain function/subroutine, or more finely within a block of expressions/statements (accordingly with function scope or block scope); this is static resolution, performable at parse-time or compile-time. Alternatively, a variable with dynamic scope is resolved at run-time, based on a global binding stack that depends on the specific control flow. Variables only accessible within a certain functions are termed \"local variables\". A \"global variable\", or one with indefinite scope, may be referred to anywhere in the program.\n\n\"Extent\", on the other hand, is a runtime (dynamic) aspect of a variable. Each binding of a variable to a value can have its own \"extent\" at runtime. The extent of the binding is the portion of the program's execution time during which the variable continues to refer to the same value or memory location. A running program may enter and leave a given extent many times, as in the case of a closure.\n\nUnless the programming language features garbage collection, a variable whose extent permanently outlasts its scope can result in a memory leak, whereby the memory allocated for the variable can never be freed since the variable which would be used to reference it for deallocation purposes is no longer accessible. However, it can be permissible for a variable binding to extend beyond its scope, as occurs in Lisp closures and C static local variables; when execution passes back into the variable's scope, the variable may once again be used. A variable whose scope begins before its extent does is said to be \"uninitialized\" and often has an undefined, arbitrary value if accessed (see wild pointer), since it has yet to be explicitly given a particular value. A variable whose extent ends before its scope may become a dangling pointer and deemed uninitialized once more since its value has been destroyed. Variables described by the previous two cases may be said to be \"out of extent\" or \"unbound\". In many languages, it is an error to try to use the value of a variable when it is out of extent. In other languages, doing so may yield unpredictable results. Such a variable may, however, be assigned a new value, which gives it a new extent.\n\nFor space efficiency, a memory space needed for a variable may be allocated only when the variable is first used and freed when it is no longer needed. A variable is only needed when it is in scope, thus beginning each variable's lifetime when it enters scope may give space to unused variables. To avoid wasting such space, compilers often warn programmers if a variable is declared but not used.\n\nIt is considered good programming practice to make the scope of variables as narrow as feasible so that different parts of a program do not accidentally interact with each other by modifying each other's variables. Doing so also prevents action at a distance. Common techniques for doing so are to have different sections of a program use different name spaces, or to make individual variables \"private\" through either dynamic variable scoping or lexical variable scoping.\n\nMany programming languages employ a reserved value (often named \"null\" or \"nil\") to indicate an invalid or uninitialized variable.\n\nIn statically typed languages such as Java or ML, a variable also has a \"type\", meaning that only certain kinds of values can be stored in it. For example, a variable of type \"integer\" is prohibited from storing text values.\n\nIn dynamically typed languages such as Python, it is values, not variables, which carry type. In Common Lisp, both situations exist simultaneously: A variable is given a type (if undeclared, it is assumed to be codice_7, the universal supertype) which exists at compile time. Values also have types, which can be checked and queried at runtime.\n\nTyping of variables also allows polymorphisms to be resolved at compile time. However, this is different from the polymorphism used in object-oriented function calls (referred to as \"virtual functions\" in C++) which resolves the call based on the value type as opposed to the supertypes the variable is allowed to have.\n\nVariables often store simple data, like integers and literal strings, but some programming languages allow a variable to store values of other datatypes as well. Such languages may also enable functions to be parametric polymorphic. These functions operate like variables to represent data of multiple types. For example, a function named codice_8 may determine the length of a list. Such a codice_8 function may be parametric polymorphic by including a type variable in its type signature, since the amount of elements in the list is independent of the elements' types.\n\nThe \"formal parameters\" (or \"formal arguments\") of functions are also referred to as variables. For instance, in this Python code segment,\n\nthe variable named codice_10 is a \"parameter\" because it is given a value when the function is called. The integer 5 is the \"argument\" which gives codice_10 its value. In most languages, function parameters have local scope. This specific variable named codice_10 can only be referred to within the codice_13 function (though of course other functions can also have variables called codice_10).\n\nThe specifics of variable allocation and the representation of their values vary widely, both among programming languages and among implementations of a given language. Many language implementations allocate space for \"local variables\", whose extent lasts for a single function call on the \"call stack\", and whose memory is automatically reclaimed when the function returns. More generally, in \"name binding\", the name of a variable is bound to the address of some particular block (contiguous sequence) of bytes in memory, and operations on the variable manipulate that block. Referencing is more common for variables whose values have large or unknown sizes when the code is compiled. Such variables reference the location of the value instead of storing the value itself, which is allocated from a pool of memory called the \"heap\".\n\nBound variables have values. A value, however, is an abstraction, an idea; in implementation, a value is represented by some \"data object\", which is stored somewhere in computer memory. The program, or the runtime environment, must set aside memory for each data object and, since memory is finite, ensure that this memory is yielded for reuse when the object is no longer needed to represent some variable's value.\n\nObjects allocated from the heap must be reclaimed—especially when the objects are no longer needed. In a garbage-collected language (such as C#, Java, Python, Golang and Lisp), the runtime environment automatically reclaims objects when extant variables can no longer refer to them. In non-garbage-collected languages, such as C, the program (and the programmer) must explicitly allocate memory, and then later free it, to reclaim its memory. Failure to do so leads to memory leaks, in which the heap is depleted as the program runs, risks eventual failure from exhausting available memory.\n\nWhen a variable refers to a data structure created dynamically, some of its components may be only indirectly accessed through the variable. In such circumstances, garbage collectors (or analogous program features in languages that lack garbage collectors) must deal with a case where only a portion of the memory reachable from the variable needs to be reclaimed.\n\nUnlike their mathematical counterparts, programming variables and constants commonly take multiple-character names, e.g. codice_15 or codice_16. Single-character names are most commonly used only for auxiliary variables; for instance, codice_17, codice_18, codice_19 for array index variables.\n\nSome naming conventions are enforced at the language level as part of the language syntax which involves the format of valid identifiers. In almost all languages, variable names cannot start with a digit (0–9) and cannot contain whitespace characters. Whether or not punctuation marks are permitted in variable names varies from language to language; many languages only permit the underscore (\"_\") in variable names and forbid all other punctuation. In some programming languages, sigils (symbols or punctuation) are affixed to variable identifiers to indicate the variable's datatype or scope.\n\nCase-sensitivity of variable names also varies between languages and some languages require the use of a certain case in naming certain entities; Most modern languages are case-sensitive; some older languages are not. Some languages reserve certain forms of variable names for their own internal use; in many languages, names beginning with two underscores (\"__\") often fall under this category.\n\nHowever, beyond the basic restrictions imposed by a language, the naming of variables is largely a matter of style. At the machine code level, variable names are not used, so the exact names chosen do not matter to the computer. Thus names of variables identify them, for the rest they are just a tool for programmers to make programs easier to write and understand. Using poorly chosen variable names can make code more difficult to review than non-descriptive names, so names which are clear are often encouraged.\n\nProgrammers often create and adhere to code style guidelines which offer guidance on naming variables or impose a precise naming scheme. Shorter names are faster to type but are less descriptive; longer names often make programs easier to read and the purpose of variables easier to understand. However, extreme verbosity in variable names can also lead to less comprehensible code.\n\nIn terms of the classifications of variables, we can classify variables based on the lifetime of them. The different types of variables are static, stack-dynamic, explicit heap-dynamic, and implicit heap-dynamic. A static variable is also known as global variable, it is bound to a memory cell before execution begins and remains to the same memory cell until termination. A typical example is the static variables in C and C++. A Stack-dynamic variable is known as local variable, which is bound when the declaration statement is executed, and it is deallocated when the procedure returns. The main examples are local variables in C subprograms and Java methods. Explicit Heap-Dynamic variables are nameless (abstract) memory cells that are allocated and deallocated by explicit run-time instructions specified by the programmer. The main examples are dynamic objects in C++ (via new and delete) and all objects in Java. Implicit Heap-Dynamic variables are bound to heap storage only when they are assigned values. Allocation and release occur when values are reassigned to variables. As a result, Implicit heap-dynamic variables have the highest degree of flexibility. The main examples are some variables in JavaScript, PHP and all variables in APL.\n\n"}
{"id": "348029", "url": "https://en.wikipedia.org/wiki?curid=348029", "title": "Virasoro algebra", "text": "Virasoro algebra\n\nIn mathematics, the Virasoro algebra (named after the physicist Miguel Angel Virasoro) is a complex Lie algebra, the unique central extension of the Witt algebra. It is widely used in two-dimensional conformal field theory and in string theory.\n\nThe Virasoro algebra is spanned by generators for and the central charge .\nThese generators satisfy formula_1 and \n\nThe factor of is merely a matter of convention. For a derivation of the algebra as the unique central extension of the Witt algebra, see derivation of the Virasoro algebra.\n\nA lowest weight representation of the Virasoro algebra is a representation generated by a primary state: a vector formula_2 such that \nwhere the number called the conformal dimension or conformal weight of formula_2.\n\nA lowest weight representation is spanned by eigenstates of formula_5. The eigenvalues take the form formula_6, where the integer formula_7 is called the level of the corresponding eigenstate. \n\nMore precisely, a lowest weight representation is spanned by formula_5-eigenstates of the type formula_9 with formula_10 and formula_11, whose levels are formula_12. Any state whose level is not zero is called a descendant state of .\n\nFor any pair of complex numbers and , the Verma module formula_13 is \nthe largest possible lowest weight representation. (The same letter, , is used for both the element of the Virasoro algebra and its eigenvalue in a representation.) \n\nThe states formula_9 with formula_10 and formula_11 form a basis of the Verma module. The Verma module is indecomposable, and for generic values of and it is also irreducible. When it is reducible, there exist other lowest weight representations with these values of and , called degenerate representations, which are cosets of the Verma module. In particular,\nthe unique irreducible lowest weight representation with these values of and is the quotient of the Verma module by its maximal submodule.\n\nA Verma module is irreducible if and only if it has no singular vectors.\n\nA singular vector or null vector of a lowest weight representation is a state that is both descendent and primary. \n\nA sufficient condition for the Verma module formula_13 to have a singular vector at the level is formula_18 for some positive integers such that =, with\nIn particular, formula_20, and the reducible Verma module formula_21 has a singular vector formula_22 at the level =1. Then formula_23, and the corresponding reducible Verma module has a singular vector formula_24 at the level =2.\n\nA lowest weight representation with a real value of formula_25 has a unique Hermitian form such that the adjoint of formula_26 is formula_27, and the norm of the primary state is one.\nThe representation is called unitary if that Hermitian form is positive definite. \nSince any singular vector has zero norm, all unitary lowest weight representations are irreducible. \n\nThe Gram determinant of a basis of the level formula_28 is given by the Kac determinant formula, \nwhere the function \"p\"(\"N\") is the partition function, and \"A\" is some constant. \nThe Kac determinant formula was stated by V. Kac (1978), and its first published proof was given by Feigin and Fuks (1984).\n\nThe irreducible lowest weight representation with values and is unitary if and only if either ≥1 and ≥0, or is one of the values\nfor \"m\" = 2, 3, 4, ..., and \"h\" is one of the values \nfor \"r\" = 1, 2, 3, ..., \"m\"−1 and \"s\"= 1, 2, 3, ..., \"r\".\n\nDaniel Friedan, Zongan Qiu, and Stephen Shenker (1984) showed that these conditions are necessary, and Peter Goddard, Adrian Kent, and David Olive (1986) used the coset construction or GKO construction (identifying unitary representations of the Virasoro algebra within tensor products of unitary representations of affine Kac–Moody algebras) to show that they are sufficient.\n\nIn two dimensions, the algebra of local conformal transformations is made of two copies of the Witt algebra.\nIt follows that the symmetry algebra of two-dimensional conformal field theory is the Virasoro algebra. Technically, the conformal bootstrap approach to two-dimensional CFT relies on Virasoro conformal blocks, special functions that include and generalize the characters of representations of the Virasoro algebra.\n\nSince the Virasoro algebra comprises the generators of the conformal group of the worldsheet, the stress tensor in string theory obeys the commutation relations of (two copies of) the Virasoro algebra. This is because the conformal group decomposes into separate diffeomorphisms of the forward and back lightcones. Diffeomorphism invariance of the worldsheet implies additionally that the stress tensor vanishes. This is known as the Virasoro constraint, and in the quantum theory, cannot be applied to all the states in the theory, but rather only on the physical states (compare Gupta–Bleuler formalism).\n\nThere are two supersymmetric N=1 extensions of the Virasoro algebra, called the Neveu-Schwarz algebra and the Ramond algebra. Their theory is similar to that of the Virasoro algebra, now involving Grassmann numbers. There are further extensions of these algebras with more supersymmetry, such as the N = 2 superconformal algebra.\n\nThe Virasoro algebra is a central extension of the Lie algebra of meromorphic vector fields on a genus 0 Riemann surface that are holomorphic except at two fixed points. \nI V Krichever and S P Novikov (1987) found a central extension of the Lie algebra of meromorphic vector fields on a higher genus compact Riemann surface that are holomorphic except at two fixed points, and their analysis has been extended to supermanifolds by J Rabin (1995).\n\nThe Virasoro algebra also has vertex algebraic and conformal algebraic counterparts, which basically come from arranging all the basis elements into generating series and working with single objects. Unsurprisingly these are called the vertex Virasoro and conformal Virasoro algebras respectively.\n\nThe Virasoro algebra may also be specified as a presentation. This is to say that all of its generators may be determined recursively (\"generated\") out of merely two judiciously chosen generators (e.g. and ), and six equations (constraint conditions) among them, by systematic use of the Jacobi identity. (D Fairlie, J Nuyts, and C Zachos, 1988. Shortly thereafter, J Uretsky discovered the original 8 conditions could be pared down to six.)\n\nCorrespondingly, for the super Virasoro algebra extension, the Ramond algebra follows from two generating generators and five conditions; and the Neveu-Schwarz algebra out of two such and nine conditions.\n\nThe Witt algebra (the Virasoro algebra without the central extension) was discovered by É. Cartan (1909). Its analogues over finite fields were studied by E. Witt in about the 1930s. The central extension of the Witt algebra that gives the Virasoro algebra was first found (in characteristic \"p\">0) by R. E. Block (1966, page 381) and independently rediscovered (in characteristic 0) by I. M. Gelfand and (1968). Virasoro (1970) wrote down some operators generating the Virasoso algebra (later known as the Virasoro operators) while studying dual resonance models, though he did not find the central extension. The central extension giving the Virasoro algebra was rediscovered in physics shortly after by J. H. Weis, according to Brower and Thorn (1971, footnote on page 167).\n\n\n"}
