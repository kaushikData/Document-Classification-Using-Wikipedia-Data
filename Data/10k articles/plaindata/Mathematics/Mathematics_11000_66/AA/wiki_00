{"id": "6860708", "url": "https://en.wikipedia.org/wiki?curid=6860708", "title": "Acta Mathematica", "text": "Acta Mathematica\n\nActa Mathematica is a peer-reviewed open-access scientific journal covering research in all fields of mathematics. The journal was established by Gösta Mittag-Leffler in 1882 and is published by Institut Mittag-Leffler, a research institute for mathematics belonging to the Royal Swedish Academy of Sciences. The journal has been printed and distributed by Springer Science+Business Media from 2006 to 2016. Since 2017, Acta Mathematica is published electronically and in print by International Press. Its electronic version is open access without publishing fees.\n\nAccording to Cédric Villani, this journal is \"considered by many to be the most prestigious of all mathematical research journals\". The journal's \"most famous episode\" (according to Villani) concerns Henri Poincaré, who won a prize offered in 1887 by Oscar II of Sweden for the best mathematical work concerning the stability of the Solar System by purporting to prove the stability of a special case of the three-body problem. This episode was (re)discovered in the 1990's by Daniel Goroff, in his preface to the english translation of \"Les méthodes nouvelles de la mécanique céleste\". by June Barrow-Green and K.G. Andersson (, )\nThe prized or lauded paper was to be published in \"Acta Mathematica\", but after the issue containing the paper was printed, Poincaré found an error that invalidated his proof. He paid more than the prize money to destroy the print run and reprint the issue without his paper, and instead published a corrected paper a year later in the same journal that demonstrated that the system could be unstable. This paper later became one of the foundational works of chaos theory.\n\n"}
{"id": "30609033", "url": "https://en.wikipedia.org/wiki?curid=30609033", "title": "Adequality", "text": "Adequality\n\nAdequality is a technique developed by Pierre de Fermat in his treatise \"Methodus ad disquirendam maximam et minimam\" (a Latin treatise circulated in France c. 1636) to calculate maxima and minima of functions, tangents to curves, area, center of mass, least action, and other problems in calculus. According to André Weil, Fermat \"introduces the technical term adaequalitas, adaequare, etc., which he says he has borrowed from Diophantus. As Diophantus V.11 shows, it means an approximate equality, and this is indeed how Fermat explains the word in one of his later writings.\" (Weil 1973). Diophantus coined the word παρισότης (\"parisotēs\") to refer to an approximate equality. Claude Gaspard Bachet de Méziriac translated Diophantus's Greek word into Latin as \"adaequalitas\". Paul Tannery's French translation of Fermat’s Latin treatises on maxima and minima used the words \"adéquation\" and \"adégaler\".\n\nFermat used \"adequality\" first to find maxima of functions, and then adapted it to find tangent lines to curves.\n\nTo find the maximum of a term formula_1, Fermat equated (or more precisely adequated) formula_1 and formula_3 and after doing algebra he could cancel out a factor of formula_4 and then discard any remaining terms involving formula_5 To illustrate the method by Fermat's own example, consider the problem of finding the maximum of formula_6 (In Fermat's words, it is to divide a line formula_7 at a point formula_8, such that the product of the two parts formula_9 be a maximum.) Fermat \"adequated\" formula_10 with formula_11. That is (using the notation formula_12 to denote adequality, introduced by Paul Tannery):\nCanceling terms and dividing by formula_14 Fermat arrived at\nRemoving the terms that contained formula_14 Fermat arrived at the desired result that the maximum occurred when formula_17.\n\nFermat also used his principle to give a mathematical derivation of Snell's laws of refraction directly from the principle that light takes the quickest path.\n\nFermat's method was highly criticized by his contemporaries, particularly Descartes. V. Katz suggests this is because Descartes had independently discovered the same new mathematics, known as his method of normals, and Descartes was quite proud of his discovery. Katz also notes that while Fermat's methods were closer to the future developments in calculus, Descartes' methods had a more immediate impact on the development.\n\nBoth Newton and Leibniz referred to Fermat's work as an antecedent of infinitesimal calculus. Nevertheless, there is disagreement amongst modern scholars about the exact meaning of Fermat's adequality. Fermat's \"adequality\" was analyzed in a number of scholarly studies. In 1896, Paul Tannery published a French translation of Fermat’s Latin treatises on maxima and minima (Fermat, Œuvres, Vol. III, pp. 121–156). Tannery translated Fermat's term as “adégaler” and adopted Fermat’s “adéquation”. Tannery also introduced the symbol formula_12 for adequality in mathematical formulas.\n\nHeinrich Wieleitner (1929) wrote:Fermat replaces \"A\" with \"A\"+\"E\". Then he sets the new expression roughly equal (angenähert gleich) to the old one, cancels equal terms on both sides, and divides by the highest possible power of \"E\". He then cancels all terms which contain \"E\" and sets those that remain equal to each other. From that [the required] \"A\" results. That \"E\" should be as small as possible is nowhere said and is at best expressed by the word \"adaequalitas\". (Wieleitner uses the symbol formula_19.)\n<br>\n\nMax Miller (1934) wrote:Thereupon one should put the both terms, which express the maximum and the minimum, approximately equal (näherungsweise gleich), as Diophantus says.(Miller uses the symbol formula_20.)\n<br>\n\nJean Itard (1948) wrote:One knows that the expression \"adégaler\" is adopted by Fermat from Diophantus, translated by Xylander and by Bachet. It is about an approximate equality (égalité approximative) \". (Itard uses the symbol formula_21.)\n<br>\n\nJoseph Ehrenfried Hofmann (1963) wrote:Fermat chooses a quantity \"h\", thought as sufficiently small, and puts \"f\"(\"x\" + \"h\") roughly equal (ungefähr gleich) to \"f\"(\"x\"). His technical term is \"adaequare\".(Hofmann uses the symbol formula_20.)\n<br>\n\nPeer Strømholm (1968) wrote:The basis of Fermat's approach was the comparition of two expressions which, though they had the same form, were not exactly equal. This part of the process he called \"\"comparare par adaequalitatem\" or \"comparer per adaequalitatem\"\", and it implied that the otherwise strict identity between the two sides of the \"equation\" was destroyed by the modification of the variable by a \"small\" amount:\n\nformula_23.\n\nThis, I believe, was the real significance of his use of Diophantos' πἀρισον, stressing the \"smallness\" of the variation. The ordinary translation of 'adaequalitas' seems to be \"approximate equality\", but I much prefer \"pseudo-equality\" to present Fermat's thought at this point.He further notes that \"there was never in M1 (Method 1) any question of the variation \"E\" being put equal to zero. The words Fermat used to express the process of suppressing terms containing \"E\" was 'elido', 'deleo', and 'expungo', and in French 'i'efface' and 'i'ôte'. We can hardly believe that a sane man wishing to express his meaning and searching for words, would constantly hit upon such tortuous ways of imparting the simple fact that the terms vanished because \"E\" was zero.(p. 51)\n<br>\n\nClaus Jensen (1969) wrote:Moreover, in applying the notion of \"adégalité\" – which constitutes the basis of Fermat's general method of constructing tangents, and by which is meant a comparition of two magnitudes as if they were equal, although they are in fact not (\"tamquam essent aequalia, licet revera aequalia non sint\") – I will employ the nowadays more usual symbol formula_20. The Latin quotation comes from Tannery's 1891 edition of Fermat, volume 1, page 140.\n<br>\n\nMichael Sean Mahoney (1971) wrote:Fermat's Method of maxima and minima, which is clearly applicable to any polynomial 'P(x)\", originally rested on purely \"finitistic\" algebraic foundations. It assumed, counterfactually, the inequality of two equal roots in order to determine, by Viete's theory of equations, a relation between those roots and one of the coefficients of the polynomial, a relation that was fully general. This relation then led to an extreme-value solution when Fermat removed his counterfactual assumption and set the roots equal. Borrowing a term from Diophantus, Fermat called this counterfactual equality 'adequality'.(Mahoney uses the symbol formula_25.) On p. 164, end of footnote 46, Mahoney notes that one of the meanings of adequality is \"approximate equality\" or \"equality in the limiting case\".\n<br>\n\nCharles Henry Edwards, Jr. (1979) wrote:For example, in order to determine how to subdivide a segment of length formula_26 into two segments formula_27 and formula_28 whose product formula_29 is maximal, that is to find the rectangle with perimeter formula_30 that has the maximal area, he [Fermat] proceeds as follows. First he substituted formula_31 (he used \"A\", \"E\" instead of \"x\", \"e\") for the unknown \"x\", and then wrote down the following \"pseudo-equality\" to compare the resulting expression with the original one:\n\nAfter canceling terms, he divided through by \"e\" to obtain formula_33 Finally he discarded the remaining term containing \"e\", transforming the pseudo-equality into the true equality formula_34 that gives the value of \"x\" which makes formula_35 maximal. Unfortunately, Fermat never explained the logical basis for this method with sufficient clarity or completeness to prevent disagreements between historical scholars as to precisely what he meant or intended.\"\n\nKirsti Andersen (1980) wrote:The two expressions of the maximum or minimum are made \"adequal\", which means something like as nearly equal as possible.(Andersen uses the symbol formula_25.)\n<br>\n\nHerbert Breger (1994) wrote:I want to put forward my hypothesis: \"Fermat used the word \"adaequare\" in the sense of\" \"to put equal\" ... In a mathematical context, the only difference between \"aequare\" and \"adaequare\" seems to be that the latter gives more stress on the fact that the equality is achieved.(Page 197f.)\n<br>\n\nJohn Stillwell (Stillwell 2006 p. 91) wrote:Fermat introduced the idea of adequality in 1630s but he was ahead of his time. His successors were unwilling to give up the convenience of ordinary equations, preferring to use equality loosely rather than to use adequality accurately. The idea of adequality was revived only in the twentieth century, in the so-called non-standard analysis.\n<br>\n\nEnrico Giusti (2009) cites Fermat's letter to Marin Mersenne where Fermat wrote:Cette comparaison par adégalité produit deux termes inégaux qui enfin produisent l'égalité (selon ma méthode) qui nous donne la solution de la question\" (\"This comparison by adequality produces two unequal terms which finally produce the equality (following my method) which gives us the solution of the problem\").. Giusti notes in a footnote that this letter seems to have escaped Breger's notice.\n<br>\n\nKlaus Barner (2011) asserts that Fermat uses two different Latin words (aequabitur and adaequabitur) to replace the nowadays usual equals sign, \"aequabitur\" when the equation concerns a valid identity between two constants, a universally valid (proved) formula, or a conditional equation, \"adaequabitur\", however, when the equation describes a relation between two variables, which are \"not independent\" (and the equation is no valid formula). On page 36, Barner writes: \"Why did Fermat continually repeat his inconsistent procedure for all his examples for the method of tangents? Why did he never mention the secant, with which he in fact operated? I do not know.\"\n\nKatz, Schaps, Shnider (2013) argue that Fermat's application of the technique to transcendental curves such as the cycloid shows that Fermat's technique of adequality goes beyond a purely algebraic algorithm, and that, contrary to Breger's interpretation, the technical terms \"parisotes\" as used by Diophantus and \"adaequalitas\" as used by Fermat both mean \"approximate equality\". They develop a formalisation of Fermat's technique of adequality in modern mathematics as the standard part function which rounds off a finite hyperreal number to its nearest real number.\n\n\n"}
{"id": "32848670", "url": "https://en.wikipedia.org/wiki?curid=32848670", "title": "Affine q-Krawtchouk polynomials", "text": "Affine q-Krawtchouk polynomials\n\nIn mathematics, the affine \"q\"-Krawtchouk polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme, introduced by Carlitz and Hodges. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \n\nAffine q-Krawtchouk polynomials → Little q-Laguerre polynomials：\n\n"}
{"id": "11268035", "url": "https://en.wikipedia.org/wiki?curid=11268035", "title": "Alloy (specification language)", "text": "Alloy (specification language)\n\nIn computer science and software engineering, Alloy is a declarative specification language for expressing complex structural constraints and behavior in a software system. Alloy provides a simple structural modeling tool based on first-order logic. Alloy is targeted at the creation of \"micro-models\" that can then be automatically checked for correctness. Alloy specifications can be checked using the alloy analyzer.\n\nAlthough Alloy is designed with automatic analysis in mind, Alloy differs from many specification languages designed for model-checking in that it permits the definition of infinite models. The Alloy Analyzer is designed to perform finite scope checks even on infinite models.\n\nThe Alloy language and analyzer are developed by a team led by Daniel Jackson at the Massachusetts Institute of Technology in the United States.\n\nThe first version of the Alloy language appeared in 1997. It was a rather limited object modeling language. Succeeding iterations of the language \"added quantifiers, higher arity relations, polymorphism, subtyping, and signatures\".\n\nThe mathematical underpinnings of the language were heavily influenced by the Z notation, and the syntax of Alloy owes more to languages such as Object Constraint Language.\n\nThe Alloy Analyzer was specifically developed to support so-called \"lightweight formal methods\". As such, it is intended to provide fully automated analysis, in contrast to the interactive theorem proving techniques commonly used with specification languages similar to Alloy. Development of the Analyzer was originally inspired by the automated analysis provided by model checkers. However, model-checking is ill-suited to the kind of models that are typically developed in Alloy, and as a result the core of the Analyzer was eventually implemented as a model-finder built atop a boolean SAT solver.\n\nThrough version 3.0, the Alloy Analyzer incorporated an integral SAT-based model-finder based on an off-the-shelf SAT-solver. However, as of version 4.0 the Analyzer makes use of the Kodkod model-finder, for which the Analyzer acts as a front-end. Both model-finders essentially translate a model expressed in relational logic into a corresponding boolean logic formula, and then invoke an off-the-shelf SAT-solver on the boolean formula. In the event that the solver finds a solution, the result is translated back into a corresponding binding of constants to variables in the relational logic model.\n\nIn order to ensure the model-finding problem is decidable, the Alloy Analyzer performs model-finding over restricted scopes consisting of a user-defined finite number of objects. This has the effect of limiting the generality of the results produced by the Analyzer. However, the designers of the Alloy Analyzer justify the decision to work within limited scopes through an appeal to the \"small scope hypothesis\": that a high proportion of bugs can be found by testing a program for all test inputs within some small scope.\n\nAlloy models are relational in nature, and are composed of several different kinds of statements:\n\n\n\nBecause Alloy is a declarative language the meaning of a model is unaffected by the order of statements.\n\n"}
{"id": "44688", "url": "https://en.wikipedia.org/wiki?curid=44688", "title": "Almost everywhere", "text": "Almost everywhere\n\nIn measure theory (a branch of mathematical analysis), a property holds almost everywhere if, in a technical sense, the set for which the property holds takes up nearly all possibilities. The notion of almost everywhere is a companion notion to the concept of measure zero. In the subject of probability, which is largely based in measure theory, the notion is referred to as \"almost surely\".\n\nMore specifically, a property holds almost everywhere if the set of elements for which the property does not hold is a set of measure zero (Halmos 1974), or equivalently if the set of elements for which the property holds is conull. In cases where the measure is not complete, it is sufficient that the set is contained within a set of measure zero. When discussing sets of real numbers, the Lebesgue measure is assumed unless otherwise stated.\n\nThe term \"almost everywhere\" is abbreviated \"a.e.\"; in older literature \"p.p.\" is used, to stand for the equivalent French language phrase \"presque partout\".\n\nA set with full measure is one whose complement is of measure zero. In probability theory, the terms \"almost surely\", \"almost certain\" and \"almost always\" refer to events with probability 1, which are exactly the sets of full measure in a probability space.\n\nOccasionally, instead of saying that a property holds almost everywhere, it is said that the property holds for almost all elements (though the term almost all also has other meanings).\n\nIf formula_1 is a measure space, a property formula_2 is said to hold almost everywhere in formula_3 if there exists a set formula_4 with formula_5, and for formula_6, formula_7 has property formula_2. Another common way of expressing the same thing is to say that \"almost every point satisfies formula_2\" or \"for almost every formula_7, formula_11 holds\".\n\nIt is \"not\" required that the set formula_12 has measure 0; it may not belong to formula_13.\n\n\nAs a consequence of the first two properties, it is often possible to reason about \"almost every point\" of a measure space as though it were an ordinary point rather than an abstraction. This is often done implicitly in informal mathematical arguments. However, one must be careful with this mode of reasoning because of the third bullet above: universal quantification over uncountable families of statements is valid for ordinary points but not for \"almost every point\".\n\nOutside of the context of real analysis, the notion of a property true almost everywhere is sometimes defined in terms of an ultrafilter. An ultrafilter on a set \"X\" is a maximal collection \"F\" of subsets of \"X\" such that:\nA property \"P\" of points in \"X\" holds almost everywhere, relative to an ultrafilter \"F\", if the set of points for which \"P\" holds is in \"F\".\n\nFor example, one construction of the hyperreal number system defines a hyperreal number as an equivalence class of sequences that are equal almost everywhere as defined by an ultrafilter.\n\nThe definition of \"almost everywhere\" in terms of ultrafilters is closely related to the definition in terms of measures, because each ultrafilter defines a finitely-additive measure taking only the values 0 and 1, where a set has measure 1 if and only if it is included in the ultrafilter.\n\n"}
{"id": "2152465", "url": "https://en.wikipedia.org/wiki?curid=2152465", "title": "Antiisomorphism", "text": "Antiisomorphism\n\nIn Category theory, a branch of formal mathematics, an antiisomorphism (or anti-isomorphism) between structured sets \"A\" and \"B\" is an isomorphism from \"A\" to the opposite of \"B\" (or equivalently from the opposite of \"A\" to \"B\"). If there exists an antiisomorphism between two structures, they are said to be \"antiisomorphic.\"\n\nIntuitively, to say that two mathematical structures are \"antiisomorphic\" is to say that they are basically opposites of one another.\n\nThe concept is particularly useful in an algebraic setting, as, for instance, when applied to rings.\n\nLet \"A\" be the binary relation (or directed graph) consisting of elements {1,2,3} and binary relation formula_1 defined as follows:\n\nLet \"B\" be the binary relation set consisting of elements {\"a\",\"b\",\"c\"} and binary relation formula_5 defined as follows:\n\nNote that the opposite of \"B\" (denoted \"B\") is the same set of elements with the opposite binary relation formula_9 (that is, reverse all the arcs of the directed graph):\n\nIf we replace \"a\", \"b\", and \"c\" with 1, 2, and 3 respectively, we will see that each rule in \"B\" is the same as some rule in \"A\". That is, we can define an isomorphism formula_13 from \"A\" to \"B\" by\n\nformula_14\n\nThis formula_13 is an antiisomorphism between \"A\" and \"B\".\n\nSpecializing the general language of category theory to the algebraic topic of rings, we have:\nLet \"R\" and \"S\" be rings and \"f\": \"R\" → \"S\" a bijection between them, then if\n\"f\" will be called a \"ring anti-isomorphism\". If \"R\" = \"S\" then \"f\" will be called a ring \"anti-automorphism\".\n\nAn example of a ring anti-automorphism is given by the conjugate mapping of quaternions:\n\n"}
{"id": "89425", "url": "https://en.wikipedia.org/wiki?curid=89425", "title": "Arrow's impossibility theorem", "text": "Arrow's impossibility theorem\n\nIn social choice theory, Arrow's impossibility theorem, the general possibility theorem or Arrow's paradox is an impossibility theorem stating that when voters have three or more distinct alternatives (options), no ranked voting electoral system can convert the ranked preferences of individuals into a community-wide (complete and transitive) ranking while also meeting a specified set of criteria: \"unrestricted domain\", \"non-dictatorship\", \"Pareto efficiency\", and \"independence of irrelevant alternatives\". The theorem is often cited in discussions of voting theory as it is further interpreted by the Gibbard–Satterthwaite theorem. The theorem is named after economist and Nobel laureate Kenneth Arrow, who demonstrated the theorem in his doctoral thesis and popularized it in his 1951 book \"Social Choice and Individual Values\". The original paper was titled \"A Difficulty in the Concept of Social Welfare\".\n\nIn short, the theorem states that no rank-order electoral system can be designed that always satisfies these three \"fairness\" criteria:\n\n\n\n\nCardinal voting electoral systems are not covered by the theorem, as they convey more information than rank orders. However, Gibbard's theorem extends Arrows theorem for that case. The theorem can also be sidestepped by weakening the notion of independence.\n\nThe axiomatic approach Arrow adopted can treat all conceivable rules (that are based on preferences) within one unified framework. In that sense, the approach is qualitatively different from the earlier one in voting theory, in which rules were investigated one by one. One can therefore say that the contemporary paradigm of social choice theory started from this theorem.\n\nThe practical consequences of the theorem are debatable: Arrow has said \"Most systems are not going to work badly all of the time. All I proved is that all can work badly at times.\"\n\nThe need to aggregate preferences occurs in many disciplines: in welfare economics, where one attempts to find an economic outcome which would be acceptable and stable; in decision theory, where a person has to make a rational choice based on several criteria; and most naturally in electoral systems, which are mechanisms for extracting a decision from a multitude of voters' preferences.\n\nThe framework for Arrow's theorem assumes that we need to extract a preference order on a given set of options (outcomes). Each individual in the society (or equivalently, each decision criterion) gives a particular order of preferences on the set of outcomes. We are searching for a ranked voting electoral system, called a \"social welfare function\" (\"preference aggregation rule\"), which transforms the set of preferences (\"profile\" of preferences) into a single global societal preference order. Arrow's theorem says that if the decision-making body has at least two members and at least three options to decide among, then it is impossible to design a social welfare function that satisfies all these conditions (assumed to be a reasonable requirement of a fair electoral system) at once:\n\nA later (1963) version of Arrow's theorem replaced the monotonicity and non-imposition criteria with:\n\nThis later version is more general, having weaker conditions. The axioms of monotonicity, non-imposition, and IIA together imply Pareto efficiency, whereas Pareto efficiency (itself implying non-imposition) and IIA together do not imply monotonicity.\n\nThe IIA condition has three purposes (or effects):\n\nArrow's death-of-a-candidate example (1963, page 26) suggests that the agenda (the set of feasible alternatives) shrinks from, say, X = {a, b, c} to S = {a, b} because of the death of candidate c. This example is misleading since it can give the reader an impression that IIA is a condition involving \"two\" agenda and \"one\" profile. The fact is that IIA involves just \"one\" agendum ({x, y} in case of pairwise independence) but \"two\" profiles. If the condition is applied to this confusing example, it requires this: Suppose an aggregation rule satisfying IIA chooses b from the agenda {a, b} when the profile is given by (cab, cba), that is, individual 1 prefers c to a to b, 2 prefers c to b to a. Then, it must still choose b from {a, b} if the profile were, say: (abc, bac); (acb, bca); (acb, cba); or (abc, cba).\n\nIn different words, Arrow defines IIA as saying that the social preferences between alternatives x and y depend only on the individual preferences between x and y (not on those involving other candidates).\n\nLet be a set of outcomes, a number of voters or decision criteria. We shall denote the set of all full linear orderings of by .\n\nA (strict) social welfare function (preference aggregation rule) is a function\nwhich aggregates voters' preferences into a single preference order on .\n\nAn -tuple of voters' preferences is called a \"preference profile\". In its strongest and simplest form, Arrow's impossibility theorem states that whenever the set of possible alternatives has more than 2 elements, then the following three conditions become incompatible:\n\n\nBased on two proofs appearing in \"Economic Theory\". For simplicity we have presented all rankings as if ties are impossible. A complete proof taking possible ties into account is not essentially different from the one given here, except that one ought to say \"not above\" instead of \"below\" or \"not below\" instead of \"above\" in some cases. Full details are given in the original articles.\n\nWe will prove that any social choice system respecting unrestricted domain, unanimity, and independence of irrelevant alternatives (IIA) is a dictatorship. The key idea is to identify a \"pivotal voter\" whose ballot swings the societal outcome. We then prove that this voter is a partial dictator (in a specific technical sense, described below). Finally we conclude by showing that all of the partial dictators are the same person, hence this voter is a dictator.\n\nSay there are three choices for society, call them A, B, and C. Suppose first that everyone prefers option B the least: everyone prefers A to B, and everyone prefers C to B. By unanimity, society must also prefer both A and C to B. Call this situation \"profile 0\".\n\nOn the other hand, if everyone preferred B to everything else, then society would have to prefer B to everything else by unanimity. Now arrange all the voters in some arbitrary but fixed order, and for each \"i\" let \"profile i\" be the same as \"profile 0\", but move B to the top of the ballots for voters 1 through \"i\". So \"profile 1\" has B at the top of the ballot for voter 1, but not for any of the others. \"Profile 2\" has B at the top for voters 1 and 2, but no others, and so on.\n\nSince B eventually moves to the top of the societal preference, there must be some profile, number \"k\", for which B moves above A in the societal rank. We call the voter whose ballot change causes this to happen the pivotal voter for B over A. Note that the pivotal voter for B over A is not, a priori, the same as the pivotal voter for A over B. In part three of the proof we will show that these do turn out to be the same.\n\nAlso note that by IIA the same argument applies if \"profile 0\" is any profile in which A is ranked above B by every voter, and the pivotal voter for B over A will still be voter \"k\". We will use this observation below.\n\nIn this part of the argument we refer to voter \"k\", the pivotal voter for B over A, as \"pivotal voter\" for simplicity. We will show that pivotal voter dictates society's decision for B over C. That is, we show that no matter how the rest of society votes, if Pivotal Voter ranks B over C, then that is the societal outcome. Note again that the dictator for B over C is not a priori the same as that for C over B. In part three of the proof we will see that these turn out to be the same too.\nIn the following, we call voters 1 through \"k − 1\", \"segment one\", and voters \"k + 1\" through \"N\", \"segment two\". To begin, suppose that the ballots are as follows:\n\nThen by the argument in part one (and the last observation in that part), the societal outcome must rank A above B. This is because, except for a repositioning of C, this profile is the same as \"profile k − 1\" from part one. Furthermore, by unanimity the societal outcome must rank B above C. Therefore, we know the outcome in this case completely.\n\nNow suppose that pivotal voter moves B above A, but keeps C in the same position and imagine that any number (or all!) of the other voters change their ballots to move B below C, without changing the position of A. Then aside from a repositioning of C this is the same as \"profile k\" from part one and hence the societal outcome ranks B above A. Furthermore, by IIA the societal outcome must rank A above C, as in the previous case. In particular, the societal outcome ranks B above C, even though Pivotal Voter may have been the \"only\" voter to rank B above C. By IIA, this conclusion holds independently of how A is positioned on the ballots, so pivotal voter is a dictator for B over C.\n\nIn this part of the argument we refer back to the original ordering of voters, and compare the positions of the different pivotal voters (identified by applying parts one and two to the other pairs of candidates). First, the pivotal voter for B over C must appear earlier (or at the same position) in the line than the dictator for B over C: As we consider the argument of part one applied to B and C, successively moving B to the top of voters' ballots, the pivot point where society ranks B above C must come at or before we reach the dictator for B over C. Likewise, reversing the roles of B and C, the pivotal voter for C over B must be at or later in line than the dictator for B over C. In short, if \"k\" denotes the position of the pivotal voter for X over Y (for any two candidates X and Y), then we have shown\n\nNow repeating the entire argument above with B and C switched, we also have\n\nTherefore, we have\n\nand the same argument for other pairs shows that all the pivotal voters (and hence all the dictators) occur at the same position in the list of voters. This voter is the dictator for the whole election.\n\nAlthough Arrow's theorem is a mathematical result, it is often expressed in a non-mathematical way with a statement such as \"no voting method is fair\", \"every ranked voting method is flawed\", or \"the only voting method that isn't flawed is a dictatorship\". These statements are simplifications of Arrow's result which are not universally considered to be true. What Arrow's theorem does state is that a deterministic preferential voting mechanism—that is, one where a preference order is the only information in a vote, and any possible set of votes gives a unique result—cannot comply with all of the conditions given above simultaneously.\n\nVarious theorists have suggested weakening the IIA criterion as a way out of the paradox. Proponents of ranked voting methods contend that the IIA is an unreasonably strong criterion. It is the one breached in most useful electoral systems. Advocates of this position point out that failure of the standard IIA criterion is trivially implied by the possibility of cyclic preferences. If voters cast ballots as follows:\nthen the pairwise majority preference of the group is that A wins over B, B wins over C, and C wins over A: these yield rock-paper-scissors preferences for any pairwise comparison. In this circumstance, \"any\" aggregation rule that satisfies the very basic majoritarian requirement that a candidate who receives a majority of votes must win the election, will fail the IIA criterion, if social preference is required to be transitive (or acyclic). To see this, suppose that such a rule satisfies IIA. Since majority preferences are respected, the society prefers A to B (two votes for A > B and one for B > A), B to C, and C to A. Thus a cycle is generated, which contradicts the assumption that social preference is transitive.\n\nSo, what Arrow's theorem really shows is that any majority-wins electoral system is a non-trivial game, and that game theory should be used to predict the outcome of most voting mechanisms. This could be seen as a discouraging result, because a game need not have efficient equilibria; e.g., a ballot could result in an alternative nobody really wanted in the first place, yet everybody voted for.\n\nThe IIA property might not be satisfied in human decision-making of realistic complexity because the \"scalar\" preference ranking is effectively derived from the weighting—not usually explicit—of a \"vector\" of attributes (one book dealing with the Arrow theorem invites the reader to consider the related problem of creating a scalar measure for the track and field decathlon event—e.g. how does one make scoring 600 points in the discus event \"commensurable\" with scoring 600 points in the 1500 m race) and this scalar ranking can depend sensitively on the weighting of different attributes, with the tacit weighting itself affected by the context and contrast created by apparently \"irrelevant\" choices. Edward MacNeal discusses this sensitivity problem with respect to the ranking of \"most livable city\" in the chapter \"Surveys\" of his book \"MathSemantics: making numbers talk sense\" (1994).\n\nIn an attempt to escape from the negative conclusion of Arrow's theorem, social choice theorists have investigated various possibilities (\"ways out\"). These investigations can be divided into the following two:\n\nThis section includes approaches that deal with\nSince these two approaches often overlap, we discuss them at the same time. What is characteristic of these approaches is that they investigate various possibilities by eliminating or weakening or replacing one or more conditions (criteria) that Arrow imposed.\n\nSeveral theorists (e.g., Kirman and Sondermann) point out that when one drops the assumption that there are only finitely many individuals, one can find aggregation rules that satisfy all of Arrow's other conditions.\n\nHowever, such aggregation rules are practically of limited interest, since they are based on ultrafilters, highly non-constructive mathematical objects. In particular, Kirman and Sondermann argue that there is an \"invisible dictator\" behind such a rule. Mihara shows that such a rule violates algorithmic computability. These results can be seen to establish the robustness of Arrow's theorem.\n\nWhen there are only two alternatives to choose from, May's theorem shows that only simple majority rule satisfies a certain set of criteria (e.g., equal treatment of individuals and of alternatives; increased support for a winning alternative should not make it into a losing one). On the other hand, when there are at least three alternatives, Arrow's theorem points out the difficulty of collective decision making. Why is there such a sharp difference between the case of less than three alternatives and that of at least three alternatives?\n\n\"Nakamura's theorem\" (about the core of simple games) gives an answer more generally. It establishes that if the number of alternatives is less than a certain integer called the Nakamura number, then the rule in question will identify \"best\" alternatives without any problem; if the number of alternatives is greater or equal to the Nakamura number, then the rule will not always work, since for some profile a voting paradox (a cycle such as alternative A socially preferred to alternative B, B to C, and C to A) will arise. Since the Nakamura number of majority rule is 3 (except the case of four individuals), one can conclude from Nakamura's theorem that majority rule can deal with up to two alternatives rationally. Some super-majority rules (such as those requiring 2/3 of the votes) can have a Nakamura number greater than 3, but such rules violate other conditions given by Arrow.\n\nA common way \"around\" Arrow's paradox is limiting the alternative set to two alternatives. Thus, whenever more than two alternatives should be put to the test, it seems very tempting to use a mechanism that pairs them and votes by pairs. As tempting as this mechanism seems at first glance, it is generally far from satisfying even Pareto efficiency, not to mention IIA. The specific order by which the pairs are decided strongly influences the outcome. This is not necessarily a bad feature of the mechanism. Many sports use the tournament mechanism—essentially a pairing mechanism—to choose a winner. This gives considerable opportunity for weaker teams to win, thus adding interest and tension throughout the tournament. This means that the person controlling the order by which the choices are paired (the agenda maker) has great control over the outcome. In any case, when viewing the entire voting process as one game, Arrow's theorem still applies.\n\nAnother approach is relaxing the universality condition, which means restricting the domain of aggregation rules. The best-known result along this line assumes \"single peaked\" preferences.\n\nDuncan Black has shown that if there is only one dimension on which every individual has a \"single-peaked\" preference, then all of Arrow's conditions are met by majority rule. Suppose that there is some predetermined linear ordering of the alternative set. An individual's preference is \"single-peaked\" with respect to this ordering if he has some special place that he likes best along that line, and his dislike for an alternative grows larger as the alternative goes further away from that spot (i.e., the graph of his utility function has a single peak if alternatives are placed according to the linear ordering on the horizontal axis). For example, if voters were voting on where to set the volume for music, it would be reasonable to assume that each voter had their own ideal volume preference and that as the volume got progressively too loud or too quiet they would be increasingly dissatisfied.\nIf the domain is restricted to profiles in which every individual has a single peaked preference with respect to the linear ordering, then \"simple\" aggregation rules, which includes majority rule, have an \"acyclic\" (defined below) social preference, hence \"best\" alternatives. In particular, when there are odd number of individuals, then the social preference becomes transitive, and the socially \"best\" alternative is equal to the median of all the peaks of the individuals (Black's median voter theorem). Under single-peaked preferences, the majority rule is in some respects the most natural voting mechanism.\n\nOne can define the notion of \"single-peaked\" preferences on higher-dimensional sets of alternatives. However, one can identify the \"median\" of the peaks only in exceptional cases. Instead, we typically have the destructive situation suggested by McKelvey's Chaos Theorem: for any \"x\" and \"y\", one can find a sequence of alternatives such that \"x\" is beaten by \"x\" by a majority, \"x\" by \"x\", up to \"x\" by \"y\".\n\nBy relaxing the transitivity of social preferences, we can find aggregation rules that satisfy Arrow's other conditions. If we impose \"neutrality\" (equal treatment of alternatives) on such rules, however, there exists an individual who has a \"veto\". So the possibility provided by this approach is also very limited.\n\nFirst, suppose that a social preference is \"quasi-transitive\" (instead of transitive); this means that the strict preference formula_2 (\"better than\") is transitive: if formula_3 and formula_4, then formula_5. Then, there do exist non-dictatorial aggregation rules satisfying Arrow's conditions, but such rules are \"oligarchic\". This means that there exists a coalition L such that L is \"decisive\" (if every member in L prefers x to y, then the society prefers x to y), and each member in L \"has a veto\" (if she prefers x to y, then the society cannot prefer y to x).\n\nSecond, suppose that a social preference is \"acyclic\" (instead of transitive): there do not exist alternatives formula_6 that form a \"cycle\" (formula_7). Then, provided that there are at least as many alternatives as individuals, an aggregation rule satisfying Arrow's other conditions is \"collegial\". This means that there are individuals who belong to the intersection (\"collegium\") of all decisive coalitions. If there is someone who has a veto, then he belongs to the collegium. If the rule is assumed to be neutral, then it does have someone who has a veto.\n\nFinally, Brown's theorem left open the case of acyclic social preferences where the number of alternatives is less than the number of individuals. One can give a definite answer for that case using the \"Nakamura number\". See limiting the number of alternatives.\n\nThere are numerous examples of aggregation rules satisfying Arrow's conditions except IIA. The Borda rule is one of them. These rules, however, are susceptible to \"strategic manipulation\" by individuals.\n\nSee also Interpretations of the theorem above.\n\nWilson (1972) shows that if an aggregation rule is non-imposed and non-null, then there is either a dictator or an inverse dictator, provided that Arrow's conditions other than Pareto are also satisfied. Here, an \"inverse dictator\" is an individual \"i\" such that whenever \"i\" prefers \"x\" to \"y\", then the society prefers \"y\" to \"x\".\n\nAmartya Sen offered both relaxation of transitivity and removal of the Pareto principle. He demonstrated another interesting impossibility result, known as the \"impossibility of the Paretian Liberal\" (see liberal paradox for details). Sen went on to argue that this demonstrates the futility of demanding Pareto optimality in relation to voting mechanisms.\n\nIn social decision making, to rank all alternatives is not usually a goal. It often suffices to find some alternative. The approach focusing on choosing an alternative investigates either \"social choice functions\" (functions that map each preference profile into an alternative) or \"social choice rules\" (functions that map each preference profile into a subset of alternatives).\n\nAs for social choice functions, the Gibbard–Satterthwaite theorem is well-known, which states that if a social choice function whose range contains at least three alternatives is strategy-proof, then it is dictatorial.\n\nAs for social choice rules, we should assume there is a social preference behind them. That is, we should regard a rule as choosing the maximal elements (\"best\" alternatives) of some social preference. The set of maximal elements of a social preference is called the \"core\". Conditions for existence of an alternative in the core have been investigated in two approaches. The first approach assumes that preferences are at least \"acyclic\" (which is necessary and sufficient for the preferences to have a maximal element on any \"finite\" subset). For this reason, it is closely related to relaxing transitivity. The second approach drops the assumption of acyclic preferences. Kumabe and Mihara adopt this approach. They make a more direct assumption that individual preferences have maximal elements, and examine conditions for the social preference to have a maximal element. See Nakamura number for details of these two approaches.\n\nArrow originally rejected cardinal utility as a meaningful tool for expressing social welfare, and so focused his theorem on preference rankings, but later stated that a cardinal score system with three or four classes \"is probably the best\".\n\nArrow's framework assumes that individual and social preferences are \"orderings\" (i.e., satisfy completeness and transitivity) on the set of alternatives. This means that if the preferences are represented by a utility function, its value is an \"ordinal\" utility in the sense that it is meaningful so far as the greater value indicates the better alternative. For instance, having ordinal utilities of 4, 3, 2, 1 for alternatives a, b, c, d, respectively, is the same as having 1000, 100.01, 100, 0, which in turn is the same as having 99, 98, 1, .997. They all represent the ordering in which a is preferred to b to c to d. The assumption of \"ordinal\" preferences, which precludes \"interpersonal comparisons\" of utility, is an integral part of Arrow's theorem.\n\nFor various reasons, an approach based on cardinal utility, where the utility has a meaning beyond just giving a ranking of alternatives, is not common in contemporary economics. However, once one adopts that approach, one can take intensities of preferences into consideration, or one can compare (i) gains and losses of utility or (ii) levels of utility, across different individuals. In particular, Harsanyi (1955) gives a justification of utilitarianism (which evaluates alternatives in terms of the sum of individual utilities), originating from Jeremy Bentham. Hammond (1976) gives a justification of the maximin principle (which evaluates alternatives in terms of the utility of the worst-off individual), originating from John Rawls.\n\nNot all voting methods use, as input, only an ordering of all candidates. Methods which don't, often called \"rated\" or \"cardinal\" (as opposed to \"ranked\", \"ordinal\", or \"preferential\") electoral system, can be viewed as using information that only cardinal utility can convey. In that case, it is not surprising if some of them satisfy all of Arrow's conditions that are reformulated.\nRange voting is such a method.\nWhether such a claim is correct depends on how each condition is reformulated. Other rated electoral system which pass certain generalizations of Arrow's criteria include approval voting and majority judgment. Note that Arrow's theorem does not apply to single-winner methods such as these, but Gibbard's theorem still does: no non-defective electoral system is fully strategy-free, so the informal dictum that \"no electoral system is perfect\" still has a mathematical basis.\n\nFinally, though not an approach investigating some kind of rules, there is a criticism by James M. Buchanan, Charles Plott, and others. It argues that it is silly to think that there might be \"social\" preferences that are analogous to \"individual\" preferences. Arrow (1963, Chapter 8) answers this sort of criticism seen in the early period, which come at least partly from misunderstanding.\n\nA multi-pronged refutation of Arrow's theorem was published by philosophy Professor Howard DeLong in 1991. He challenges the theorem on the basis that Arrow wrongly assumes Preference is transitive property and that Collective Preference is the same as summing up individual preferences. He also claims that Arrow's model fails to model democracy as it exists in the real world as the model ignores the possibility of consensual temporary dictatorships (ie: Greek tyrants in times of war) and the effect of allowing lotteries to decide tie breakers and to avoid the problem of the tyranny of the majority, the example used being a group of campers at a summer camp, 8 of whom prefer cake, 7 who prefer ice cream, but funds are limited to one choice or the other on a weekly basis. Under the collective preference of majority rule each week the group would select cake. By drawing lots the choices would more accurately reflect the preferences of the collective than the majority. DeLong also criticizes Arrow's model for being hedonistic and not incorporating a sense of morality and social justice.\n\n\n\n"}
{"id": "24808024", "url": "https://en.wikipedia.org/wiki?curid=24808024", "title": "Bhama Srinivasan", "text": "Bhama Srinivasan\n\nBhama Srinivasan (born 22 April 1935) is a mathematician known for her work in the representation theory of finite groups. Her contributions were honored with the 1990 Noether Lecture. She served as President of the Association for Women in Mathematics from 1981 to 1983. She earned her Ph.D. in physics in 1959 with her dissertation \"Problems on Modular Representations of Finite Groups\" under J. A. Green at the University of Manchester. She currently is a professor at the University of Illinois at Chicago. She has had five doctoral students. She has co-authored a number of papers with Paul Fong in modular representation theory and Deligne–Lusztig theory.\n\nSrinivasan was born in Madras,India. She attended the University of Madras, where she earned her bachelor of arts degree in 1954 and her master of science degree in 1955. She traveled to England for her doctoral study. She remained in England to commence her professional academic career as a lecturer in mathematics at the University of Keele from 1960 through 1964. She then pursued a postdoctoral fellowship at the University of British Columbia through the National research Council of Canada from 1965 through 1966.She returned home to India to teach at the Ramanujan Institute of Mathematics of her Alma mater, the University of Madras, from 1966 though 1970.\nSrinivasan then immigrated to the United States, where she taught for the next decade at Clark University in Worcester, Massachusetts, as an associate professor. In 1977, she became a naturalized citizen of the United States. That year, she was a member of the Institute for Advanced Studies at Princeton. In 1980, she commenced her longstanding tenure at the University of Illinois as a professor of mathematics at the Chicago Circle campus.\n\nSrinivasan has distinguished herself in her field throughout her career. In January, 1979, she delivered the Invited Address to the American Mathematical Society(AMS) at the Joint Mathematics Meetings in Biloxi, Mississippi. She has also been invited to fill visiting professorships internationally at the Ecole Normale Superieure in Paris, the University of Essen in Germany, Sydney University,and the Science University of Tokyo in Japan. She has served as an editor for several journals in her field: \"Proceedings of the AMS\" (from 1983 through 1987); \"Communications in Algebra\" (from 1978 through 1984); \"Mathematical Surveys and Monographs\" (from 1991 through 1993). From 1991 through 1994, she served on the Editorial Boards Committee of the AMS.\n\nSrinivasan collaborated with Paul Fong on finite groups of the Lie type, and this work has been linked to Lusztig's research on quantum groups, thus crossing over between mathematics and physics. Although Srinivasan generally advocates pure mathematical research, resisting the temptation to find a practical application for all mathematics, she nevertheless got excited by the application of her research to physics.\n\nIn 2012 she became a fellow of the American Mathematical Society. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n"}
{"id": "49746051", "url": "https://en.wikipedia.org/wiki?curid=49746051", "title": "Bronshtein and Semendyayev", "text": "Bronshtein and Semendyayev\n\nBronshtein and Semendyayev (often just Bronshtein or Bronstein) is the informal name of a comprehensive handbook of fundamental working knowledge of mathematics and table of formulas originally compiled by the Russian mathematician Ilya Nikolaevich Bronshtein and engineer Konstantin Adolfovic Semendyayev.\n\nThe work was first published in 1945 in Russia and soon became a \"standard\" and frequently used guide for scientists, engineers, and technical university students. Over the decades, high popularity and a string of translations, extensions, re-translations and major revisions by various editors led to a complex international publishing history centered around the significantly expanded German version. Legal hurdles following the fall of the Iron Curtain caused the development to split into several independent branches maintained by different publishers and editors to the effect that there are now two considerably different publications associated with the original title – and both of them are available in several languages.\n\nWith some slight variations, the English version of the book was originally named A Guide-Book to Mathematics, but changed its name to Handbook of Mathematics. This name is still maintained up to the present by one of the branches. The other line is meanwhile named Users' Guide to Mathematics to help avoid confusion.\n\n\"Bronshtein and Semendyayev\" is a comprehensive handbook of fundamental working knowledge of mathematics and table of formulas based on the Russian book (\"Spravochnik po matematike dlya inzhenerov i uchashchikhsya vtuzov\", literally: \"Handbook of mathematics for engineers and students of technical universities\") compiled by the Russian mathematician Ilya Nikolaevich Bronshtein (Russian: Илья Николаевич Бронштейн, German: Ilja Nikolajewitsch Bronstein) and engineer Konstantin Adolfovic Semendyayev (Russian: Константин Адольфович Семендяев, German: Konstantin Adolfowitsch Semendjajew).\n\nThe scope is the concise discussion of all major fields of applied mathematics by definitions, tables and examples with a focus on practicability and with limited formal rigour. The work also contains a comprehensive list of analytically solvable integrals, that is, those integrals which can be described in closed form with antiderivatives.\n\nThe two Russian authors originally wrote the book in 1939/1940. Hot lead typesetting had already started when the Siege of Leningrad prohibited further development and the print matrices were relocated. After the war, they were considered lost, but could be found again years later, so that the first edition of could finally be published in 1945.\n\nThe expanded German translation Taschenbuch der Mathematik (literally: \"Pocketbook of mathematics\") by Viktor Ziegler was first published in 1958 by B. G. Teubner in Leipzig.\n\nBased on the German translation, an English translation became available as well under the title A Guide-Book to Mathematics in 1964, first by Pergamon Press and The Macmillan Company, later by Springer Verlag and Verlag Harri Deutsch.\n\nIn 1966 it became apparent that the title needed considerable updates to meet new requirements. The original authors felt too old to continue the work and the Russian publisher Nauka (Наука) seemed to have had lost interest in the further development as well for some while. Therefore, in 1970 a consortium of East-German mathematicians were contracted by Teubner Verlag to start to expand and revise the work. This was coordinated by Viktor Ziegler, Dorothea Ziegler and Günter Grosche (Universität Leipzig). While Semendyayev contributed some work, he did not want some other new chapters to be included in the manuscript in 1976, therefore they had to be split out into a new volume II. Finally, after almost a decade of work, the major new revision could be published in 1979, legally as a cooperation of Teubner and Nauka (Наука).\n\nThe reworked two-volume German edition was well received and again became a \"standard\" in higher mathematics education in Germany. This led to a string of high-volume revisions and various translations (including into Russian, English, Japanese, and Spanish) to meet the international demand. The English version was published by Van Nostrand Reinhold Company and Verlag Harri Deutsch as Handbook of Mathematics.\n\nA decade later the German 'Wende' and the later reunification led to considerable changes in the publishing environment in Germany between 1989 and 1991. The East-German publisher Teubner Leipzig was integrated into the West-German publisher Teubner Stuttgart. These changes put an end to the cooperation of the East-German Teubner Verlag with the West-German Verlag Harri Deutsch, who had been licensing this and other titles for distribution in Germany and Switzerland, a business model no longer working in a free market. Licensing issues caused the development to split into two independent branches by the two publishing houses:\n\nVerlag Harri Deutsch contracted and Heiner Mühlig to translate the last non-Teubner influenced edition (officially the latest revision (1977) of the third Russian edition (1953), which is actually the eleventh Russian edition (1967)) into German for a major rework of Taschenbuch der Mathematik as a single-volume title. This was first published in 1992/1993. When Verlag Harri Deutsch closed two decades later, took over in 2013, and they continue to maintain this work up to the present. The new English translation (now by Springer Verlag) is still called Handbook of Mathematics.\n\nIn a parallel development, became editor for the latest existing German edition by Teubner (1990), the version which had previously also been distributed by Verlag Harri Deutsch, and updated it significantly to become the Teubner-Taschenbuch der Mathematik (literally: \"Teubner pocketbook of mathematics\") for Teubner. This was first published in 1995/1996 – still as a two-volume work. The work was continued by Vieweg+Teubner Verlag after the merger with in 2003. When Vieweg+Teubner was bought by Springer and renamed Springer Vieweg Verlag, several new chapters were added and some more advanced contents stripped out for the single-volume Springer-Taschenbuch der Mathematik (literally: \"Springer pocketbook of mathematics\") in 2012/2013. This is now accompanied by a completely reworked and considerably expanded four-volume series named Springer-Handbuch der Mathematik (literally: \"Springer handbook of mathematics\") by Zeidler also based on the former \"Bronshtein and Semendyayev\". So far, this latest revision of the alternative development branch isn't available in English, but volume I of the former \"Teubner-Taschenbuch der Mathematik\" has been translated and published by Oxford University Press as Oxford Users' Guide to Mathematics already.\n\nAuthors: Bronshtein, Ilya Nikolaevich (Бронштейн, Илья Николаевич); Semendyayev, Konstantin Adolfovic (Семендяев, Константин Адольфович).\n\n\n\nAuthors: Bronshtein, Ilya Nikolaevich; Semendyayev, Konstantin Adolfovic; .\nTranslator: Ziegler, Viktor.\n\n\nTranslator: \n\n\nEditor: Blanuša, Danilo.\nTranslators: Vistrička, Zvonko; Uremović, Ivan\n\n\nTranslators: Jaworowski, Jan W.; Bleicher, Michael N.\n\n\nEditors: Grosche, Günter; Ziegler, Viktor; Ziegler, Dorothea.\nAuthors: ; Belger, Martin; Benker, Hans; Denkmann, Norbert; Deweß, Monika; Erfurth, Horst; Gentemann, Helmut; Göthner, Peter; Gottwald, Siegfried; Grosche, Günter; Hilbig, Harald; Hofmann, Reinhard; ; ; ; Semendjajew, Konstantin Adolfowitsch; Vettermann, Theodor; Wünsch, Volkmar; .\n\n\nEditors: Grosche, Günter; Ziegler, Viktor; Ziegler, Dorothea.\nAuthors: Bär, Gunter; Deweß, Günter; Deweß, Monika; ; Göhde, Dietrich; Jentsch, Lothar; ; Piehler, Joachim; .\n\n\nEditor: .\nTranslator: Hirsch, Kurt August\n\n\n\nEditors: Grosche, Günter; Ziegler, Viktor; Ziegler, Dorothea.\n\nTranslators: Miyamoto, Toshio (宮本 敏雄); Matsuda, Nobuyuki (松田 信行).\n\n\nEditor: Aparicio Bernardo, Emiliano.\nTranslator: Harding Rojas, Inés.\n\n\nEditor (part I): .\nAuthors (part I): Hackbusch, Wolfgang; Schwarz, Hans Rudolf; .\n\nEditors (part II): Grosche, Günter; Ziegler, Viktor; Ziegler, Dorothea; .\nAuthors (part II): ; Deweß, Günter; Deweß, Monika; Diekert, Volker; Fuchssteiner, Benno; Gottwald, Siegfried; Gündel, Susanne; Hoschek, Josef; Olderog, Ernst-Rüdiger; Richter, Michael M.; Schenke, Michael; Widmayer, Peter; .\n\n\nEditors: .\nAuthors: Hackbusch, Wolfgang; Hromkovič, Juraj; ; Schwarz, Hans Rudolf; Blath, Jochen; Schied, Alexander; Dempe, Stephan; ; Gottwald, Siegfried; .\n\n\nEditors: .\nAuthors: Hackbusch, Wolfgang; Hromkovič, Juraj; ; Schwarz, Hans Rudolf; Blath, Jochen; Schied, Alexander; Dempe, Stephan; ; Gottwald, Siegfried; .\n\n\nEditor: .\nAuthors: Hackbusch, Wolfgang; Schwarz, Hans Rudolf; .\nTranslator: Hunt, Bruce.\n\n\nEditors: ; Mühlig, Heiner.\nAuthors: Baumann, Ulrike; Brunner, Jürgen; ; Fleischer, Norbert Mozesovich (); Grauel, Adolf; Reif, Roland; Reitmann, Volker (); Steinert, I.; Marsolek, Lothar; ; Mühlig, Heiner; Nickel, Heinz; Weber, Matthias; .\n\n\nEditors: ; Mühlig, Heiner.\nAuthors: Baumann, Ulrike; Bernstein, Swanhild; Brand, Joachim; Brunner, Jürgen; Buchleitner, Andreas (CD-ROM only); ; Fleischer, Norbert Mozesovich (); Grauel, Adolf; Reif, Roland; Reitmann, Volker (); Rumpf, Benno (CD-ROM only); Steinert, I.; Tiersch, Markus (CD-ROM only); Marsolek, Lothar; Mulansky, Bernd; ; Mühlig, Heiner; Nickel, Heinz; Weber, Matthias; Wellens, Thomas (CD-ROM only); .\n\n\nContributors: Barbič, Janez; Dolinar, Gregor; Jurčič-Zlobec, Borut; Mramor Kosta, Neža.\nTranslator: Barbič, Janez.\n\n\nEditors: ;Mühlig, Heiner.\nAuthors: Baumann, Ulrike; Brunner, Jürgen; ; Fleischer, Norbert Mozesovich (); Grauel, Adolf; Reif, Roland; Reitmann, Volker (); Steinert, I.; Marsolek, Lothar; ; Mühlig, Heiner; Nickel, Heinz; Weber, Matthias; .\nTranslator: Szép, Gabriella.\n\n\nEditors: ; Mühlig, Heiner.\nAuthors: Baumann, Ulrike; Bernstein, Swanhild; Brand, Joachim; Brunner, Jürgen; Buchleitner, Andreas (CD-ROM only); ; Fleischer, Norbert Mozesovich (); Grauel, Adolf; Reif, Roland; Reitmann, Volker (); Rumpf, Benno (CD-ROM only); Steinert, I.; Tiersch, Markus (CD-ROM only); Tóth, János; Marsolek, Lothar; Mulansky, Bernd; ; Mühlig, Heiner; Nickel, Heinz; Weber, Matthias; Wellens, Thomas (CD-ROM only); .\n\n\n\nTranslators: Šućur, Miljenko; Valčić Trkulja, Milena.\n\n\nDue to \"Bronshtein and Semendyayev\" containing a comprehensive table of analytically solvable integrals, integrals are sometimes referred to as being \"Bronshtein-integrable\" in German universities if they can be looked up in the book (in playful analogy to terms like \"Riemann-integrability\" and \"Lebesgue-integrability\").\n\n\n"}
{"id": "1603001", "url": "https://en.wikipedia.org/wiki?curid=1603001", "title": "Complete information", "text": "Complete information\n\nIn economics and game theory, complete information is an economic situation or game in which knowledge about other market participants or players is available to all participants. The utility functions, payoffs, strategies and \"types\" of players are thus common knowledge.\n\nInversely, in a game with incomplete information, players do not possess full information about their opponents. Some players possess private information, a fact that the others should take into account when forming expectations about how those players will behave. A typical example is an auction: each player knows his own utility function (= valuation for the item), but does not know the utility function of the other players. See for more examples.\n\nGames of incomplete information arise most frequently in social science rather than as games in the narrow sense. For instance, John Harsanyi was motivated by consideration of arms control negotiations, where the players may be uncertain both of the capabilities of their opponents and of their desires and beliefs.\n\nIt is often assumed that the players have some statistical information about the other players. E.g., in an auction, each player knows that the valuations of the other players are drawn from some probability distribution. In this case, the game is called a Bayesian game.\n\nComplete information is importantly different from perfect information.\nIn a game of complete information, the structure of the game and the payoff functions of the players are commonly known but players may not see all of the moves made by other players (for instance, the initial placement of ships in Battleship); there may also be a chance element (as in most card games). Conversely, in games of perfect information, every player observes other players' moves, but may lack some information on others' payoffs, or on the structure of the game. A game with complete information may or may not have perfect information, and vice versa.\n\n\nGames of incomplete information can be converted into games of complete but imperfect information under the \"common prior assumption.\" This assumption is commonly made for pragmatic reasons, but its justification remains controversial among economists.\n\n\n"}
{"id": "5938019", "url": "https://en.wikipedia.org/wiki?curid=5938019", "title": "Complexity theory and organizations", "text": "Complexity theory and organizations\n\nComplexity theory and organizations, also called complexity strategy or complex adaptive organizations, is the use of the study of complexity systems in the field of strategic management and organizational studies.\n\nComplexity theory is an interdisciplinary theory that grew out of systems theory in the 1960s. It draws from research in the natural sciences that examines uncertainty and non-linearity. Complexity theory emphasizes interactions and the accompanying feedback loops that constantly change systems. While it proposes that systems are unpredictable, they are also constrained by order-generating rules.\n\nComplexity theory has been used in the fields of strategic management and organizational studies. Application areas include understanding how organizations or firms adapt to their environments and how they cope with conditions of uncertainty. Organisations have complex structures in that they are dynamic networks of interactions, and their relationships are not aggregations of the individual static entities. They are adaptive; in that the individual and collective behavior mutate and self-organize corresponding to a change-initiating micro-event or collection of events.\n\nOrganizations can be treated as complex adaptive systems (CAS) as they exhibit fundamental CAS principles like self-organization, complexity, emergence, interdependence, space of possibilities, co-evolution, chaos, and self-similarity.\n\nCAS are contrasted with ordered and chaotic systems by the relationship that exists between the system and the agents which act within it. In an ordered system the level of constraint means that all agent behaviour is limited to the rules of the system. In a chaotic system the agents are unconstrained and susceptible to statistical and other analysis. In a CAS, the system and the agents co-evolve; the system lightly constrains agent behaviour, but the agents modify the system by their interaction with it. This self-organizing nature is an important characteristic of CAS; and its ability to learn to adapt, differentiate it from other self organizing systems.\n\nCAS approaches to strategy seek to understand the nature of system constraints and agent interaction and generally takes an evolutionary or naturalistic approach to strategy. Some research integrates computer simulation and organizational studies.\n\nComplexity theory also relates to knowledge management (KM) and organizational learning (OL). \"Complex systems are, by any other definition, learning organizations.\" Complexity Theory, KM, and OL are all complimentary and co-dependent. “KM and OL each lack a theory of how cognition happens in human social systems – complexity theory offers this missing piece”.\n\nComplexity theory is also being used to better understand new ways of doing project management, as traditional models have been found lacking to current challenges. This approaches advocates forming a \"culture of trust\" that \"welcomes outsiders, embraces new ideas, and promotes cooperation.\"\n\nComplexity Theory implies approaches that focus on flatter, more flexible organizations, rather than top-down, command-and-control styles of management.\n\nA typical example for an organization behaving as CAS, is Wikipedia – collaborated and managed by a loosely organized management structure, composed of a complex mix of human–computer interactions. By managing behavior, and not only mere content, Wikipedia uses simple rules to produce a complex, evolving knowledge base which has largely replaced older sources in popular use.\n\nOther examples include the complex global macroeconomic network within a country or group of countries; stock market and complex web of cross-border holding companies; manufacturing businesses; and any human social group-based endeavour in a particular ideology and social system such as political parties, communities, geopolitical organisations, and terrorist networks of both hierarchical and leaderless nature. This new macro level state may create difficulty for an observer in explaining and describing the collective behaviour in terms of its constituent parts; as a result of the complex dynamic networks of interactions, outlined earlier.\n\n\n"}
{"id": "572382", "url": "https://en.wikipedia.org/wiki?curid=572382", "title": "Continuity correction", "text": "Continuity correction\n\nIn probability theory, a continuity correction is an adjustment that is made when a discrete distribution is approximated by a continuous distribution.\n\nIf a random variable \"X\" has a binomial distribution with parameters \"n\" and \"p\", i.e., \"X\" is distributed as the number of \"successes\" in \"n\" independent Bernoulli trials with probability \"p\" of success on each trial, then\n\nfor any \"x\" ∈ {0, 1, 2, ... \"n\"}. If \"np\" and \"np\"(1 − \"p\") are large (sometimes taken to mean ≥ 5), then the probability above is fairly well approximated by\n\nwhere \"Y\" is a normally distributed random variable with the same expected value and the same variance as \"X\", i.e., E(\"Y\") = \"np\" and var(\"Y\") = \"np\"(1 − \"p\"). This addition of 1/2 to \"x\" is a continuity correction. \n\nA continuity correction can also be applied when other discrete distributions supported on the integers are approximated by the normal distribution. For example, if \"X\" has a Poisson distribution with expected value λ then the variance of \"X\" is also λ, and\n\nif \"Y\" is normally distributed with expectation and variance both λ.\n\nBefore the ready availability of statistical software having the ability to evaluate probability distribution functions accurately, continuity corrections played an important role in the practical application of statistical tests in which the test statistic has a discrete distribution: it had a special importance for manual calculations. A particular example of this is the binomial test, involving the binomial distribution, as in checking whether a coin is fair. Where extreme accuracy is not necessary, computer calculations for some ranges of parameters may still rely on using continuity corrections to improve accuracy while retaining simplicity.\n\n\n"}
{"id": "6620", "url": "https://en.wikipedia.org/wiki?curid=6620", "title": "Cotangent space", "text": "Cotangent space\n\nIn differential geometry, one can attach to every point \"x\" of a smooth (or differentiable) manifold a vector space called the cotangent space at \"x\". Typically, the cotangent space is defined as the dual space of the tangent space at \"x\", although there are more direct definitions (see below). The elements of the cotangent space are called cotangent vectors or tangent covectors.\n\nAll cotangent spaces on a connected manifold have the same dimension, equal to the dimension of the manifold. All the cotangent spaces of a manifold can be \"glued together\" (i.e. unioned and endowed with a topology) to form a new differentiable manifold of twice the dimension, the cotangent bundle of the manifold.\n\nThe tangent space and the cotangent space at a point are both real vector spaces of the same dimension and therefore isomorphic to each other via many possible isomorphisms. The introduction of a Riemannian metric or a symplectic form gives rise to a natural isomorphism between the tangent space and the cotangent space at a point, associating to any tangent covector a canonical tangent vector.\n\nLet \"M\" be a smooth manifold and let \"x\" be a point in \"M\". Let \"T\"\"M\" be the tangent space at \"x\". Then the cotangent space at \"x\" is defined as the dual space of \"T\"\"M\":\nConcretely, elements of the cotangent space are linear functionals on \"T\"\"M\". That is, every element α ∈ \"T\"\"M\" is a linear map\nwhere F is the underlying field of the vector space being considered. For example, the field of real numbers. The elements of \"T\"\"M\" are called cotangent vectors.\n\nIn some cases, one might like to have a direct definition of the cotangent space without reference to the tangent space. Such a definition can be formulated in terms of equivalence classes of smooth functions on \"M\". Informally, we will say that two smooth functions \"f\" and \"g\" are equivalent at a point \"x\" if they have the same first-order behavior near \"x\", analogous to their linear Taylor polynomials; two functions \"f\" and \"g\" have the same first order behaviour near \"x\" if and only if the derivative of the function \"f\"-\"g\" vanishes at \"x\". The cotangent space will then consist of all the possible first-order behaviors of a function near \"x\".\n\nLet \"M\" be a smooth manifold and let \"x\" be a point in \"M\". Let \"I\" be the ideal of all functions in C(\"M\") vanishing at \"x\", and let \"I\" be the set of functions of the form formula_1, where \"f\", \"g\" ∈ \"I\". Then \"I\" and \"I\" are real vector spaces and the cotangent space is defined as the quotient space \"T\"\"M\" = \"I\" / \"I\".\n\nThis formulation is analogous to the construction of the cotangent space to define the Zariski tangent space in algebraic geometry. The construction also generalizes to locally ringed spaces.\n\nLet \"M\" be a smooth manifold and let \"f\" ∈ C(\"M\") be a smooth function. The differential of \"f\" at a point \"x\" is the map\nwhere \"X\" is a tangent vector at \"x\", thought of as a derivation. That is formula_2 is the Lie derivative of \"f\" in the direction \"X\", and one has d\"f\"(\"X\")=\"X\"(\"f\"). Equivalently, we can think of tangent vectors as tangents to curves, and write\nIn either case, d\"f\" is a linear map on \"T\"\"M\" and hence it is a tangent covector at \"x\".\n\nWe can then define the differential map d : C(\"M\") → \"T\"\"M\" at a point \"x\" as the map which sends \"f\" to d\"f\". Properties of the differential map include:\n\n\nThe differential map provides the link between the two alternate definitions of the cotangent space given above. Given a function \"f\" ∈ \"I\" (a smooth function vanishing at \"x\") we can form the linear functional d\"f\" as above. Since the map d restricts to 0 on \"I\" (the reader should verify this), d descends to a map from \"I\" / \"I\" to the dual of the tangent space, (\"T\"\"M\"). One can show that this map is an isomorphism, establishing the equivalence of the two definitions.\n\nJust as every differentiable map \"f\" : \"M\" → \"N\" between manifolds induces a linear map (called the \"pushforward\" or \"derivative\") between the tangent spaces\nevery such map induces a linear map (called the \"pullback\") between the cotangent spaces, only this time in the reverse direction:\nThe pullback is naturally defined as the dual (or transpose) of the pushforward. Unraveling the definition, this means the following:\nwhere θ ∈ \"T\"\"N\" and \"X\" ∈ \"T\"\"M\". Note carefully where everything lives.\n\nIf we define tangent covectors in terms of equivalence classes of smooth maps vanishing at a point then the definition of the pullback is even more straightforward. Let \"g\" be a smooth function on \"N\" vanishing at \"f\"(\"x\"). Then the pullback of the covector determined by \"g\" (denoted d\"g\") is given by\nThat is, it is the equivalence class of functions on \"M\" vanishing at \"x\" determined by \"g\" o \"f\".\n\nThe \"k\"-th exterior power of the cotangent space, denoted Λ(\"T\"\"M\"), is another important object in differential geometry. Vectors in the \"k\"th exterior power, or more precisely sections of the \"k\"-th exterior power of the cotangent bundle, are called differential \"k\"-forms. They can be thought of as alternating, multilinear maps on \"k\" tangent vectors. \nFor this reason, tangent covectors are frequently called \"one-forms\".\n\n"}
{"id": "34227270", "url": "https://en.wikipedia.org/wiki?curid=34227270", "title": "David Raymond Curtiss", "text": "David Raymond Curtiss\n\nDavid Raymond Curtiss (January 12, 1878 – April 29, 1953) was an American mathematician. He served as president of the Mathematical Association of America from 1935 to 1936. He was also vice president of the American Mathematical Society and the American Association for the Advancement of Science.\n\nCurtiss was born in Derby, Connecticut. He attended the University of California, earning a bachelor's degree in 1899 and a master's degree in 1901. He earned a doctorate at Harvard University under Maxime Bôcher and William Fogg Osgood in 1903. He completed a postdoctoral fellowship at École Normale Supérieure in 1904.\n\nIn 1904, Curtiss taught at Yale University for one year. He then served as a professor at Northwestern University from 1905 to 1943, including 20 years as Chair of the Mathematics Department. Curtiss authored textbooks on trigonometry and analytic geometry with Elton James Moulton. He also published the second Carus Mathematical Monograph, \"Analytic Functions of a Complex Variable.\"\n\nHis brother was astrophysicist Ralph Hamilton Curtiss. His son was computer pioneer John Hamilton Curtiss. He and his wife, who was seriously ill, committed suicide by carbon monoxide poisoning in the garage of their home in Redlands, California.\n\n"}
{"id": "48130890", "url": "https://en.wikipedia.org/wiki?curid=48130890", "title": "Denjoy–Riesz theorem", "text": "Denjoy–Riesz theorem\n\nIn topology, the Denjoy–Riesz theorem describes a class of sets of points in the Euclidean plane that can be covered by a continuous image of the unit interval, without self-intersections (a Jordan arc). A topological space is zero-dimensional according to the Lebesgue covering dimension if every finite open cover has a refinement that is also an open cover by disjoint sets. A topological space is totally disconnected if it has no nontrivial connected subsets; for points in the plane, being totally disconnected is equivalent to being zero-dimensional. The Denjoy–Riesz theorem states that every compact totally disconnected subset of the plane is a subset of a Jordan arc.\n\nBy applying this theorem to a two-dimensional version of the Smith–Volterra–Cantor set, it is possible to find an Osgood curve, a Jordan arc or closed Jordan curve whose Lebesgue measure is positive.\n\nA related result is the analyst's traveling salesman theorem, describing the point sets that form subsets of curves of finite arc length. Not every compact totally disconnected set has this property, because some compact totally disconnected sets require any arc that covers them to have infinite length.\n"}
{"id": "8023522", "url": "https://en.wikipedia.org/wiki?curid=8023522", "title": "Dependency (UML)", "text": "Dependency (UML)\n\nIn the Unified Modeling Language (UML), a Dependency is a relationship that shows that an element, or set of elements, requires other model elements for their specification or implementation. The element is \"dependent\" upon the independent element, called the supplier. Two or more elements in this relationship are called tuples.\n\nIn the UML, this is indicated by a dashed line pointing from the dependent (or client) to the independent (or supplier) element. The arrow representing a Dependency specifies the direction of a relationship, not the direction of a process.\n\nUML - defined dependencies include :\n\nA UML link is run-time relationship between instances of classifiers, while a dependency is a model-time relationship between definitions. \n\nA typical uni-directional link requires the one instance to know about, and thus depend, upon the other, but this is not required. A uni-directional link only requires the possibility of navigation not dependency. \n\nLikewise, a bi-directional link requires that both instances may traverse to each other, but this also does not require dependency.\n\nHowever, after the traversal of the association, if some property of the target instance's definition is accessed, such as an operation or attribute, a dependency is created.\n\n\n"}
{"id": "6811610", "url": "https://en.wikipedia.org/wiki?curid=6811610", "title": "Equivalence of metrics", "text": "Equivalence of metrics\n\nIn the study of metric spaces in mathematics, there are various notions of two metrics on the same underlying space being \"the same\", or equivalent.\n\nIn the following, formula_1 will denote a non-empty set and formula_2 and formula_3 will denote two metrics on formula_1.\n\nThe two metrics formula_2 and formula_3 are said to be topologically equivalent if they generate the same topology on formula_1. The adjective \"topological\" is often dropped. There are multiple ways of expressing this condition:\n\nThe following are sufficient but not necessary conditions for topological equivalence:\n\nTwo metrics formula_2 and formula_3 are strongly equivalent if and only if there exist positive constants formula_21 and formula_22 such that, for every formula_29,\nIn contrast to the sufficient condition for topological equivalence listed above, strong equivalence requires that there is a single set of constants that holds for every pair of points in formula_1, rather than potentially different constants associated with each point of formula_1.\n\nStrong equivalence of two metrics implies topological equivalence, but not vice versa. An intuitive reason why topological equivalence does not imply strong equivalence is that bounded sets under one metric are also bounded under a strongly equivalent metric, but not necessarily under a topologically equivalent metric.\n\nWhen the two metrics formula_33 are those induced by norms formula_34 respectively, then strong equivalence is equivalent to the condition that, for all formula_11,\n\nIn finite dimensional spaces, all metrics induced by the p-norm, including the euclidean metric, the taxicab metric, and the Chebyshev distance, are strongly equivalent.\n\nEven if two metrics are strongly equivalent, not all properties of the respective metric spaces are preserved. For instance, a function from the space to itself might be a contraction mapping under one metric, but not necessarily under a strongly equivalent one.\n\n\n"}
{"id": "30208106", "url": "https://en.wikipedia.org/wiki?curid=30208106", "title": "Erdős–Gallai theorem", "text": "Erdős–Gallai theorem\n\nThe Erdős–Gallai theorem is a result in graph theory, a branch of combinatorial mathematics. It provides one of two known approaches to solving the graph realization problem, i.e. it gives a necessary and sufficient condition for a finite sequence of natural numbers to be the degree sequence of a simple graph. A sequence obeying these conditions is called \"graphic\". The theorem was published in 1960 by Paul Erdős and Tibor Gallai, after whom it is named.\n\nA sequence of non-negative integers formula_1 can be represented as the degree sequence of a finite simple graph on \"n\" vertices if and only if formula_2 is even and\n\nholds for every k in formula_4.\n\nIt is not difficult to show that the conditions of the Erdős–Gallai theorem are necessary for a sequence of numbers to be graphic. The requirement that the sum of the degrees be even is the handshaking lemma, already used by Euler in his 1736 paper on the bridges of Königsberg. The inequality between the sum of the formula_5 largest degrees and the sum of the remaining degrees can be established by double counting: the left side gives the numbers of edge-vertex adjacencies among the formula_5 highest-degree vertices, each such adjacency must either be on an edge with one or two high-degree endpoints, the formula_7 term on the right gives the maximum possible number of edge-vertex adjacencies in which both endpoints have high degree, and the remaining term on the right upper bounds the number of edges that have exactly one high degree endpoint. Thus, the more difficult part of the proof is to show that, for any sequence of numbers obeying these conditions, there exists a graph for which it is the degree sequence.\n\nThe original proof of was long and involved. cites a shorter proof by Claude Berge, based on ideas of network flow. Choudum instead provides a proof by mathematical induction on the sum of the degrees: he lets formula_8 be the first index of a number in the sequence for which formula_9 (or the penultimate number if all are equal), uses a case analysis to show that the sequence formed by subtracting one from formula_10 and from the last number in the sequence (and removing the last number if this subtraction causes it to become zero) is again graphic, and forms a graph representing the original sequence by adding an edge between the two positions from which one was subtracted.\n\n describe close connections between the Erdős–Gallai theorem and the theory of integer partitions.\nLet formula_11; then the sorted integer sequences summing to formula_12 may be interpreted as the partitions of formula_12. Under majorization of their prefix sums, the partitions form a lattice, in which the minimal change between an individual partition and another partition lower in the partition order is to subtract one from one of the numbers formula_14 and add it to a number formula_15 that is smaller by at least two (formula_15 could be zero). As Aigner and Triesch show, this operation preserves the property of being graphic, so to prove the Erdős–Gallai theorem it suffices to characterize the graphic sequences that are maximal in this majorization order. They provide such a characterization, in terms of the Ferrers diagrams of the corresponding partitions, and show that it is equivalent to the Erdős–Gallai theorem.\n\nSimilar theorems describe the degree sequences of simple directed graphs, simple directed graphs with loops, and simple bipartite graphs . The first problem is characterized by the Fulkerson–Chen–Anstee theorem. The latter two cases, which are equivalent, are characterized by the Gale–Ryser theorem.\n\n proved that it suffices to consider the formula_5th inequality such that formula_18 with formula_19 and for formula_20. restrict the set of inequalities for graphs in an opposite thrust. If an even-summed positive sequence d has no repeated entries other than the maximum and the minimum (and the length exceeds\nthe largest entry), then it suffices to check only the formula_21th inequality, where formula_22.\n\nA finite sequences of nonnegative integers formula_23 with formula_24 is graphic if formula_25 is even and there exists a sequence formula_26 that is graphic and majorizes formula_23. This result was given by . reinvented it and gave a more direct proof.\n\n\n"}
{"id": "21496085", "url": "https://en.wikipedia.org/wiki?curid=21496085", "title": "Fock–Lorentz symmetry", "text": "Fock–Lorentz symmetry\n\nLorentz invariance follows from two independent postulates: the principle of relativity and the principle of constancy of the speed of light. Dropping the latter while keeping the former leads to a new invariance, known as Fock–Lorentz symmetry or the projective Lorentz transformation. The general study of such theories began with Fock, who was motivated by the search for the general symmetry group preserving relativity without assuming the constancy of \"c\".\n\nThis invariance does not distinguish between inertial frames (and therefore satisfies the principle of relativity) but it allows for a varying speed of light in space, \"c\"; indeed it allows for a non-invariant \"c\". According to Maxwell's equations, the speed of light satisfies\nwhere \"ε\" and \"μ\" are the electric constant and the magnetic constant. If the speed of light depends upon the space–time coordinates of the medium, say \"x\", then\nwhere formula_3 represents the vacuum as a variable medium.\n\n\n"}
{"id": "52722478", "url": "https://en.wikipedia.org/wiki?curid=52722478", "title": "Fractal expressionism", "text": "Fractal expressionism\n\nThe term fractal expressionism was coined by physicist-artist Richard Taylor and co-authors to distinguish fractal art generated directly by artists from fractal art generated using mathematics and/or computers. Fractals are patterns that repeat at increasingly fine scales and are prevalent in natural scenery (examples include clouds, rivers, and mountains). Fractal expressionism implies a direct expression of nature's patterns in an art work.\n\nThe initial studies of fractal expressionism focused on the poured paintings by the American artist Jackson Pollock (1912-1956), whose work has traditionally been associated with the abstract expressionist movement. Pollock’s patterns had previously been referred to as “natural” and “organic”, inviting speculation by author John Briggs in 1992 that Pollock's work featured fractals. In 1997, Taylor built a pendulum device called the Pollockizer which painted fractal patterns bearing a similarity to Pollock’s work. Computer analysis of Pollock's work published by Taylor et al. in a 1999 Nature article found that Pollock's painted patterns have characteristics that match those displayed by nature's fractals (specifically, they demonstrate a statistical self-similarity quantified by a non-integer dimension over a magnification range of 1.5-2 orders of magnitude). This analysis supported clues (see below) that Pollock's patterns are fractal and reflect \"the fingerprint of nature\".\n\nTaylor noted several similarities between Pollock's painting style and the processes used by nature to construct its landscapes. For instance, he cites Pollock's propensity to revisit paintings that he had not adjusted in several weeks as being comparable to cyclic processes in nature, such as the seasons or the tides. Furthermore, Taylor observed several visual similarities between the patterns produced by nature and those produced by Pollock as he painted. He points out that Pollock abandoned the use of a traditional frame for his paintings, preferring instead to roll out his canvas on the floor; this, Taylor asserts, is more compatible with how nature works than traditional painting techniques because the patterns in nature's scenery are not artificially bounded.\n\nThe perceived similarities between the processes and patterns involved in Pollock's paintings and those of nature compelled Taylor to posit that the same \"basic trademark\" of nature's pattern construction also appears in Pollock's work. Since some natural fractals are generated by a process known as \"chaos\", including fractals in human physiology, Taylor believed that Pollock's painting process might also have been chaotic, and could therefore leave behind a fractal pattern. Taylor's hypothesis seems to be reflected in Pollock's statement \"I am nature\", which he made when asked if nature was a source of inspiration for his work. Furthermore, Pollock is also quoted as stating \"No chaos, damn it\", in response to a Time magazine article that referred to his paintings as \"chaotic\". However, chaos theory was not understood until after Pollock's death, so he could not have been referring to the chaotic systems in nature but rather its common usage to mean disorder. In the famous film footage of Hans Namuth, Pollock says his paintings are no accident, and that he was able to control the flow of paint onto the canvas.\n\nTaylor points to two aspects of Pollock's painting process that have the potential to introduce fractal patterns. The first is Pollock's motion as he moved around the canvas, which Taylor hypothesized followed a Levy flight, a type of chaotic motion that is known to leave behind a fractal pattern. More specifically, a number of studies have shown that the motions associated with human balance have fractal characteristics. The second source of chaos could be introduced through Pollock's pouring technique. Falling fluid has the capability of changing from a non-chaotic to a chaotic flow, meaning that Pollock could have introduced a chaotic flow of paint as he dripped it onto the canvas. Although the fractal characteristics of human balance and falling liquid are generated on Pollock's painting time and length scales, physicist Predrag Cvitanovic notes that it would be quite an artistic challenge to control them: such parameters \"are in no sense observable and measurable on the length-scales and time-scales dominated by chaotic dynamics\".\n\nSince Taylor's initial Pollock analysis in 1999, more than ten research groups have used various forms of fractal analysis to successfully quantify Pollock’s work. In addition to analyzing Pollock's work for fractal content, some groups such as that of computer scientist Bruce Gooch, have used computers to generate Pollock-like images by varying their fractal characteristics. Mathematician Benoit Mandelbrot (who invented the term fractal) and art theorist Francis O’Connor (the chief Pollock scholar) are well known advocates of fractal expressionism.\n\nFractal expressionism is related to fractal fluency because the latter provides an appealing motivation for why artists such as Pollock might aspire to Fractal Expressionism. Fractal fluency is a neuroscience model that proposes that, through exposure to nature’s fractal scenery, people’s visual systems have adapted to efficiently process fractals with ease. This adaptation occurs at many stages of the visual system, from the way people’s eyes move to which regions of the brain get activated. Fluency puts the viewer in a ‘comfort zone’ so inducing an aesthetic experience. Neuroscience experiments have shown that Pollock’s paintings induce the same positive physiological responses in the observer as nature’s fractals and mathematical fractals.\n\nIn light of fractal fluency and the associated aesthetics, other artists might be expected to display fractal expressionism. One year before Taylor’s publication, mathematician Richard Voss quantified Chinese art using fractal analysis. Subsequently, other groups have used computer analysis to identify fractal content in a number of Western and Eastern artists, most recently in Pollock’s colleague Willem De Kooning’s work.\n\nIn addition to the above analyzed works, symbolic representations of fractals can be found in cultures across the continents spanning several centuries, including Roman, Egyptian, Aztec, Incan and Mayan civilizations. They frequently predate patterns named after the mathematicians who subsequently developed their visual characteristics. For example, although von Koch is famous for developing The Koch Curve in 1904, a similar shape featuring repeating triangles was first used to depict waves in friezes by Hellenic artists (300 B.C.E.). In the 13th century, repetition of triangles in Cosmati Mosaics generated a shape later known in mathematics as The Sierpinski Triangle (named after Sierpinski’s 1915 pattern). Triangular repetitions are also found in the 12th century pulpit of The Ravello Cathedral in Italy. The lavish artwork within The Book of Kells (circa 800 C.E.) and the sculpted arabesques in The Jain Dilwara Temple in Mount Abu, India (1031 C.E.) also both reveal stunning examples of exact fractals. The artistic works of Leonardo da Vinci and Katsushika Hokusai serve as more recent examples from Europe and Asia, each reproducing the recurring patterns that they saw in nature. Da Vinci’s sketch of turbulence in water, The Deluge (1571–1518), was composed of small swirls within larger swirls of water. In The Great Wave off Kanagawa (1830–1833), Hokusai portrayed a wave crashing on a shore with small waves on top of a large wave. Other woodcuts from the same period also feature repeating patterns at several size scales: The Ghost of Kohada Koheiji shows fissures in a skull and The Falls At Mt. Kurokami features branching channels in a waterfall.\n\nVoss's 1998 study of Chinese art was the first demonstration of using fractal analysis to distinguish between the works of different artists. Following Taylor's 1999 Pollock publication, Art conservator Jim Coddington proposed that fractal analysis should be explored as a technique to help authenticate Pollock paintings. In 2005, Taylor and colleagues published a fractal analysis of 14 authentic and 37 imitation Pollocks suggesting that, when combined with other techniques, fractal analysis might be useful for authenticating Pollock's work. In the same year, The Pollock-Krasner Foundation requested a fractal analysis to be used for the first time in an authenticity dispute, The analysis identified “significant deviations from Pollock’s characteristics.” Taylor cautioned that the results should be “coupled with other important information such as provenance, connoisseurship and materials analysis.” Two years later, materials scientists showed that pigments on the paintings dated from after Pollock’s death.\nIn 2006, the use of fractals to authenticate Pollocks stirred controversy. This controversy was triggered by physicists Katherine Jones-Smith and Harsh Mathur who claimed that the fractal characteristics identified by Taylor et al. are also present in crude sketches made in Adobe Photoshop, and deliberately fraudulent poured paintings made by other artists Thus, according to Jones-Smith and Mathur, labeling Pollock's paintings as \"fractal\" is meaningless, because the same characteristics are found in other non-fractal images. However, Taylor's rebuttal published in Nature showed that Taylor's group's fractal analysis could distinguish between Pollock paintings and the crude sketches, and identified further limitations in Jones-Smith and Mathur's analysis.\n\nJones-Smith and Mathur raised a valid concern applicable to all forms of fractal expressionism: are art works too small for the painted patterns to repeat over sufficient magnifications to assume the visual characteristics of fractals? In the case of Pollock paintings, the largest range used by Taylor et al. to determine each fractal parameter in a Pollock painting is less than two orders of magnitude in magnification. Nature's fractals repeat over limited magnification ranges (typically just over one order of magnitude), prompting scientists to debate what range is required to reliably establish fractal behavior.<ref name=\"Ave/Man\">[Avnir, David, Ofer Biham, Daniel M. Lidar, and Ofer Malcai. \"Is the Geometry of Nature Fractal?\" Science 279.5347 (1998): 39-40. Print.]</ref> Mandelbrot refused to include a required magnification range in his definition of fractals and instead noted that it is the range necessary to generate the properties associated with fractal repetition. In the case of Pollock's work, this would be the magnification range necessary for the patterns to generate the fractal aesthetics. Neuroscience experiments have shown that this magnification range is less than two orders and that Pollock’s paintings do indeed induce the same physiological responses as nature’s fractals and mathematical fractals Mandelbrot concluded \"I do believe that Pollocks are fractal.\"\n\nAt the time of the controversy, Coddington summarized as follows: “Fractal geometry has begun to play an important role in the authentication of the work of Jackson Pollock. We believe such analyses are necessary for pushing the field forward.” The most recent results, In 2015, by computer scientist Lior Shamir showed that, when combined with other pattern parameters, fractal analysis can be used to distinguish between real and imitation Pollocks with 93% accuracy. He found that the fractal parameters were the most powerful contributors to the detection accuracy\n"}
{"id": "11527", "url": "https://en.wikipedia.org/wiki?curid=11527", "title": "Fundamental theorem on homomorphisms", "text": "Fundamental theorem on homomorphisms\n\nIn abstract algebra, the fundamental theorem on homomorphisms, also known as the fundamental homomorphism theorem, relates the structure of two objects between which a homomorphism is given, and of the kernel and image of the homomorphism.\n\nThe homomorphism theorem is used to prove the isomorphism theorems.\n\nGiven two groups \"G\" and \"H\" and a group homomorphism \"f\" : \"G\"→\"H\", let \"K\" be a normal subgroup in \"G\" and φ the natural surjective homomorphism \"G\"→\"G\"/\"K\" (where \"G\"/\"K\" is a quotient group). If \"K\" is a subset of ker(\"f\") then there exists a unique homomorphism \"h\":\"G\"/\"K\"→\"H\" such that \"f\" = \"h\" φ.\n\nIn other words, the natural projection φ is universal among homomorphisms on \"G\" that map \"K\" to the identity element.\n\nThe situation is described by the following commutative diagram:\n\nBy setting \"K\" = ker(\"f\") we immediately get the first isomorphism theorem.\n\nSimilar theorems are valid for monoids, vector spaces, modules, and rings.\n\n\n"}
{"id": "170353", "url": "https://en.wikipedia.org/wiki?curid=170353", "title": "Gauss–Markov theorem", "text": "Gauss–Markov theorem\n\nIn statistics, the Gauss–Markov theorem, named after Carl Friedrich Gauss and Andrey Markov, states that in a linear regression model in which the errors have expectation zero, are uncorrelated and have equal variances, the best linear unbiased estimator (BLUE) of the coefficients is given by the ordinary least squares (OLS) estimator, provided it exists. Here \"best\" means giving the lowest variance of the estimate, as compared to other unbiased, linear estimators. The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, the James–Stein estimator (which also drops linearity) or ridge regression.\n\nSuppose we have in matrix notation,\nexpanding to,\n\nwhere formula_3 are non-random but unobservable parameters, formula_4 are non-random and observable (called the \"explanatory variables\"), formula_5 are random, and so formula_6 are random. The random variables formula_5 are called the \"disturbance\", \"noise\" or simply \"error\" (will be contrasted with \"residual\" later in the article; see errors and residuals in statistics). Note that to include a constant in the model above, one can choose to introduce the constant as a variable formula_8 with a newly introduced last column of X being unity i.e., formula_9 for all formula_10.\n\nThe Gauss–Markov assumptions concern the set of error random variables, formula_5:\n\n\nA linear estimator of formula_15 is a linear combination\n\nin which the coefficients formula_17 are not allowed to depend on the underlying coefficients formula_3, since those are not observable, but are allowed to depend on the values formula_19, since these data are observable. (The dependence of the coefficients on each formula_4 is typically nonlinear; the estimator is linear in each formula_21 and hence in each random formula_22 which is why this is \"linear\" regression.) The estimator is said to be unbiased if and only if\n\nregardless of the values of formula_19. Now, let formula_25 be some linear combination of the coefficients. Then the mean squared error of the corresponding estimation is\n\nin other words it is the expectation of the square of the weighted sum (across parameters) of the differences between the estimators and the corresponding parameters to be estimated. (Since we are considering the case in which all the parameter estimates are unbiased, this mean squared error is the same as the variance of the linear combination.) The best linear unbiased estimator (BLUE) of the vector formula_27 of parameters formula_28 is one with the smallest mean squared error for every vector formula_29 of linear combination parameters. This is equivalent to the condition that\n\nis a positive semi-definite matrix for every other linear unbiased estimator formula_31.\n\nThe ordinary least squares estimator (OLS) is the function\n\nof formula_33 and formula_34 (where formula_35 denotes the transpose of formula_36) that minimizes the sum of squares of residuals (misprediction amounts):\n\nThe theorem now states that the OLS estimator is a BLUE. The main idea of the proof is that the least-squares estimator is uncorrelated with every linear unbiased estimator of zero, i.e., with every linear combination formula_38 whose coefficients do not depend upon the unobservable formula_27 but whose expected value is always zero.\n\nLet formula_40 be another linear estimator of formula_27 with formula_42 where formula_43 is a formula_44 non-zero matrix. As we're restricting to \"unbiased\" estimators, minimum mean squared error implies minimum variance. The goal is therefore to show that such an estimator has a variance no smaller than that of formula_45 the OLS estimator. We calculate:\n\nTherefore, formula_47 is unbiased if and only if formula_48. Then:\n\nSince \"DD\"' is a positive semidefinite matrix, formula_50 exceeds formula_51 by a positive semidefinite matrix.\n\nAs it has been stated before, the condition of formula_52 is equivalent to the property that the best linear unbiased estimator of formula_53 is formula_54 (best in the sense that it has minimum variance). To see this, let formula_55 another linear unbiased estimator of formula_53.\n\nMoreover, equality holds if and only if formula_58. We calculate\n\nThis proves that the equality holds if and only if formula_60 which gives the uniqueness of the OLS estimator as a BLUE.\n\nThe generalized least squares (GLS), developed by Aitken, extends the Gauss–Markov theorem to the case where the error vector has a non-scalar covariance matrix. The Aitken estimator is also a BLUE.\n\nIn most treatments of OLS, the regressors (parameters of interest) in the design matrix formula_61 are assumed to be fixed in repeated samples. This assumption is considered inappropriate for a predominantly nonexperimental science like econometrics. Instead, the assumptions of the Gauss–Markov theorem are stated conditional on formula_61.\n\nThe dependent variable is assumed to be a linear function of the variables specified in the model. The specification must be linear in its parameters. This does not mean that there must be a linear relationship between the independent and dependent variables. The independent variables can take non-linear forms as long as the parameters are linear. The equation formula_63 qualifies as linear while formula_64 can be transformed to be linear by replacing formula_65 by another parameter, say formula_66. An equation with a parameter dependent on an independent variable does not qualify as linear, for example formula_67, where formula_68 is a function of formula_69.\n\nData transformations are often used to convert an equation into a linear form. For example, the Cobb–Douglas function—often used in economics—is nonlinear:\n\nBut it can be expressed in linear form by taking the natural logarithm of both sides:\n\nThis assumption also covers specification issues: assuming that the proper functional form has been selected and there are no omitted variables.\n\nOne should be aware, however, that the parameters that minimize the residuals of the transformed equation not necessarily minimize the residuals of the original equation.\n\nFor all formula_72 observations, the expectation—conditional on the regressors—of the error term is zero:\nwhere formula_74 is the data vector of regressors for the \"i\"th observation, and consequently formula_75 is the data matrix or design matrix.\n\nGeometrically, this assumption implies that formula_76 and formula_77 are orthogonal to each other, so that their inner product (i.e., their cross moment) is zero.\nThis assumption is violated if the explanatory variables are stochastic, for instance when they are measured with error, or are endogenous. Endogeneity can be the result of simultaneity, where causality flows back and forth between both the dependent and independent variable. Instrumental variable techniques are commonly used to address this problem.\n\nThe sample data matrix formula_61 must have full column rank.\nOtherwise formula_81 is not invertible and the OLS estimator cannot be computed.\n\nA violation of this assumption is perfect multicollinearity, i.e. some explanatory variables are linearly dependent. One scenario in which this will occur is called \"dummy variable trap,\" when a base dummy variable is not omitted resulting in perfect correlation between the dummy variables and the constant term.\n\nMulticollinearity (as long as it is not \"perfect\") can be present resulting in a less efficient, but still unbiased estimate. The estimates will be less precise and highly sensitive to particular sets of data. Multicollinearity can be detected from condition number or the variance inflation factor, among other tests.\n\nThe outer product of the error vector must be spherical.\nThis implies the error term has uniform variance (homoscedasticity) and no serial dependence. If this assumption is violated, OLS is still unbiased, but inefficient. The term \"spherical errors\" will describe the multivariate normal distribution: if formula_83 in the multivariate normal density, then the equation formula_84 is the formula for a ball centered at μ with radius σ in n-dimensional space.\n\nHeteroskedasticity occurs when the amount of error is correlated with an independent variable. For example, in a regression on food expenditure and income, the error is correlated with income. Low income people generally spend a similar amount on food, while high income people may spend a very large amount or as little as low income people spend. Heteroskedastic can also be caused by changes in measurement practices. For example, as statistical offices improve their data, measurement error decreases, so the error term declines over time.\n\nThis assumption is violated when there is autocorrelation. Autocorrelation can be visualized on a data plot when a given observation is more likely to lie above a fitted line if adjacent observations also lie above the fitted regression line. Autocorrelation is common in time series data where a data series may experience \"inertia.\" If a dependent variable takes a while to fully absorb a shock. Spatial autocorrelation can also occur geographic areas are likely to have similar errors. Autocorrelation may be the result of misspecification such as choosing the wrong functional form. In these cases, correcting the specification is one possible way to deal with autocorrelation.\n\nIn the presence of spherical errors, the generalized least squares estimator can be shown to be BLUE.\n\n\n\nUse of BLUE in physics\n"}
{"id": "24674758", "url": "https://en.wikipedia.org/wiki?curid=24674758", "title": "Goldner–Harary graph", "text": "Goldner–Harary graph\n\nIn the mathematical field of graph theory, the Goldner–Harary graph is a simple undirected graph with 11 vertices and 27 edges. It is named after A. Goldner and Frank Harary, who proved in 1975 that it was the smallest non-Hamiltonian maximal planar graph. The same graph had already been given as an example of a non-Hamiltonian simplicial polyhedron by Branko Grünbaum in 1967.\n\nThe Goldner–Harary graph is a planar graph: it can be drawn in the plane with none of its edges crossing. When drawn on a plane, all its faces are triangular, making it a maximal planar graph. As with every maximal planar graph, it is also 3-vertex-connected: the removal of any two of its vertices leaves a connected subgraph.\n\nThe Goldner–Harary graph is also non-hamiltonian. The smallest possible number of vertices for a non-hamiltonian polyhedral graph is 11. Therefore, the Goldner–Harary graph is a minimal example of graphs of this type. However, the Herschel graph, another non-Hamiltonian polyhedron with 11 vertices, has fewer edges.\n\nAs a non-Hamiltonian maximal planar graph, the Goldner–Harary graph provides an example of a planar graph with book thickness greater than two. Based on the existence of such examples, Bernhart and Kainen conjectured that the book thickness of planar graphs could be made arbitrarily large, but it was subsequently shown that all planar graphs have book thickness at most four.\n\nIt has book thickness 3, chromatic number 4, chromatic index 8, girth 3, radius 2, diameter 2 and is a 3-edge-connected graph.\n\nIt is also a 3-tree, and therefore it has treewidth 3. Like any \"k\"-tree, it is a chordal graph. As a planar 3-tree, it forms an example of an Apollonian network.\n\nBy Steinitz's theorem, the Goldner–Harary graph is a polyhedral graph: it is planar and 3-connected, so there exists a convex polyhedron having the Goldner–Harary graph as its skeleton.\n\nGeometrically, a polyhedron representing the Goldner–Harary graph may be formed by gluing a tetrahedron onto each face of a triangular dipyramid, similarly to the way a triakis octahedron is formed by gluing a tetrahedron onto each face of an octahedron. That is, it is the Kleetope of the triangular dipyramid. The dual graph of the Goldner–Harary graph is represented geometrically by the truncation of the triangular prism.\n\nThe automorphism group of the Goldner–Harary graph is of order 12 and is isomorphic to the dihedral group D, the group of symmetries of a regular hexagon, including both rotations and reflections.\n\nThe characteristic polynomial of the Goldner–Harary graph is : formula_1.\n"}
{"id": "154584", "url": "https://en.wikipedia.org/wiki?curid=154584", "title": "Hilbert's problems", "text": "Hilbert's problems\n\nHilbert's problems are twenty-three problems in mathematics published by German mathematician David Hilbert in 1900. The problems were all unsolved at the time, and several of them were very influential for 20th-century mathematics. Hilbert presented ten of the problems (1, 2, 6, 7, 8, 13, 16, 19, 21 and 22) at the Paris conference of the International Congress of Mathematicians, speaking on August 8 in the Sorbonne. The complete list of 23 problems was published later, most notably in English translation in 1902 by Mary Frances Winston Newson in the \"Bulletin of the American Mathematical Society\".\n\nHilbert's problems ranged greatly in topic and precision. Some of them are propounded precisely enough to enable a clear affirmative or negative answer, like the 3rd problem, which was the first to be solved, or the 8th problem (the Riemann hypothesis). For other problems, such as the 5th, experts have traditionally agreed on a single interpretation, and a solution to the accepted interpretation has been given, but closely related unsolved problems exist. Sometimes Hilbert's statements were not precise enough to specify a particular problem but were suggestive enough so that certain problems of more contemporary origin seem to apply, e.g. most modern number theorists would probably see the 9th problem as referring to the conjectural Langlands correspondence on representations of the absolute Galois group of a number field. Still other problems, such as the 11th and the 16th, concern what are now flourishing mathematical subdisciplines, like the theories of quadratic forms and real algebraic curves.\n\nThere are two problems that are not only unresolved but may in fact be unresolvable by modern standards. The 6th problem concerns the axiomatization of physics, a goal that twentieth-century developments of physics (including its recognition as a discipline independent from mathematics) seem to render both more remote and less important than in Hilbert's time. Also, the 4th problem concerns the foundations of geometry, in a manner that is now generally judged to be too vague to enable a definitive answer.\n\nThe other twenty-one problems have all received significant attention, and late into the twentieth century work on these problems was still considered to be of the greatest importance. Paul Cohen received the Fields Medal during 1966 for his work on the first problem, and the negative solution of the tenth problem during 1970 by Yuri Matiyasevich (completing work of Martin Davis, Hilary Putnam and Julia Robinson) generated similar acclaim. Aspects of these problems are still of great interest today.\n\nFollowing Gottlob Frege and Bertrand Russell, Hilbert sought to define mathematics logically using the method of formal systems, i.e., finitistic proofs from an agreed-upon set of axioms. One of the main goals of Hilbert's program was a finitistic proof of the consistency of the axioms of arithmetic: that is his second problem.\n\nHowever, Gödel's second incompleteness theorem gives a precise sense in which such a finitistic proof of the consistency of arithmetic is provably impossible. Hilbert lived for 12 years after Kurt Gödel published his theorem, but does not seem to have written any formal response to Gödel's work.\n\nHilbert's tenth problem does not ask whether there exists an algorithm for deciding the solvability of Diophantine equations, but rather asks for the \"construction\" of such an algorithm: \"to devise a process according to which it can be determined in a finite number of operations whether the equation is solvable in rational integers.\" That this problem was solved by showing that there cannot be any such algorithm contradicted Hilbert's philosophy of mathematics.\n\nIn discussing his opinion that every mathematical problem should have a solution, Hilbert allows for the possibility that the solution could be a proof that the original problem is impossible. He stated that the point is to know one way or the other what the solution is, and he believed that we always can know this, that in mathematics there is not any \"ignorabimus\" (statement whose truth can never be known). It seems unclear whether he would have regarded the solution of the tenth problem as an instance of ignorabimus: what is proved not to exist is not the integer solution, but (in a certain sense) the ability to discern in a specific way whether a solution exists.\n\nOn the other hand, the status of the first and second problems is even more complicated: there is not any clear mathematical consensus as to whether the results of Gödel (in the case of the second problem), or Gödel and Cohen (in the case of the first problem) give definitive negative solutions or not, since these solutions apply to a certain formalization of the problems, which is not necessarily the only possible one.\n\nHilbert originally included 24 problems on his list, but decided \"against\" including one of them in the published list. The \"24th problem\" (in proof theory, on a criterion for simplicity and general methods) was rediscovered in Hilbert's original manuscript notes by German historian in 2000.\n\nSince 1900, mathematicians and mathematical organizations have announced problem lists, but, with few exceptions, these collections have not had nearly as much influence nor generated as much work as Hilbert's problems.\n\nOne of the exceptions is furnished by three conjectures made by André Weil during the late 1940s (the Weil conjectures). In the fields of algebraic geometry, number theory and the links between the two, the Weil conjectures were very important . The first of the Weil conjectures was proved by Bernard Dwork, and a completely different proof of the first two conjectures via l-adic cohomology was given by Alexander Grothendieck. The last and deepest of the Weil conjectures (an analogue of the Riemann hypothesis) was proven by Pierre Deligne. Both Grothendieck and Deligne were awarded the Fields medal. However, the Weil conjectures in their scope are more like a single Hilbert problem, and Weil never intended them as a programme for all mathematics. This is somewhat ironic, since arguably Weil was the mathematician of the 1940s and 1950s who best played the Hilbert role, being conversant with nearly all areas of (theoretical) mathematics and having figured importantly in the development of many of them.\n\nPaul Erdős posed hundreds, if not thousands, of mathematical problems, many of them profound. Erdős often offered monetary rewards; the size of the reward depended on the perceived difficulty of the problem.\n\nThe end of the millennium, being also the centennial of Hilbert's announcement of his problems, was a natural occasion to propose \"a new set of Hilbert problems.\" Several mathematicians accepted the challenge, notably Fields Medalist Steve Smale, who responded to a request of Vladimir Arnold by proposing a list of 18 problems.\nSmale's problems have thus far not received much attention from the media, and it is unclear how much serious attention they are getting from the mathematical community.\n\nAt least in the mainstream media, the \"de facto\" 21st century analogue of Hilbert's problems is the list of seven Millennium Prize Problems chosen during 2000 by the Clay Mathematics Institute. Unlike the Hilbert problems, where the primary award was the admiration of Hilbert in particular and mathematicians in general, each prize problem includes a million dollar bounty. As with the Hilbert problems, one of the prize problems (the Poincaré conjecture) was solved relatively soon after the problems were announced.\n\nNoteworthy for its appearance on the list of Hilbert problems, Smale's list and the list of Millennium Prize Problems — and even, in its geometric guise, in the Weil Conjectures — is the Riemann hypothesis. Notwithstanding some famous recent assaults from major mathematicians of our day, many experts believe that the Riemann hypothesis will be included in problem lists for centuries yet. Hilbert himself declared: \"If I were to awaken after having slept for a thousand years, my first question would be: has the Riemann hypothesis been proven?\"\n\nIn 2008, DARPA announced its own list of 23 problems which it hoped could cause major mathematical breakthroughs, \"thereby strengthening the scientific and technological capabilities of DoD.\"\n\nOf the cleanly formulated Hilbert problems, problems 3, 7, 10, 11, 13, 14, 17, 19, 20, and 21 have a resolution that is accepted by consensus. On the other hand, problems 1, 2, 5, 9, 15, 18, and 22 have solutions that have partial acceptance, but there exists some controversy as to whether they resolve the problems.\n\nThat leaves 8 (the Riemann hypothesis), 12 and 16 unresolved, and 4 and 23 as too vague to ever be described as solved. The withdrawn 24 would also be in this class. 6 is considered as a problem in physics rather than in mathematics.\n\nHilbert's twenty-three problems are (for details on the solutions and references, see the detailed articles that are linked to in the first column):\n\n\n"}
{"id": "20236074", "url": "https://en.wikipedia.org/wiki?curid=20236074", "title": "Information–action ratio", "text": "Information–action ratio\n\nIn a speech to the German Informatics Society (Gesellschaft für Informatik) on October 11, 1990 in Stuttgart, sponsored by IBM-Germany, Postman said the following: \"The tie between information and action has been severed. Information is now a commodity that can be bought and sold, or used as a form of entertainment, or worn like a garment to enhance one's status. It comes indiscriminately, directed at no one in particular, disconnected from usefulness; we are glutted with information, drowning in information, have no control over it, don't know what to do with it.\"\n\nIn \"Amusing Ourselves to Death\" Postman frames the information-action ratio in the context of the telegraph's invention. Prior to the telegraph, Postman says people received information relevant to their lives, creating a high correlation between information and action: \"The information-action ratio was sufficiently close so that most people had a sense of being able to control some of the contingencies in their lives” (p. 69).\n\nThe telegraph allowed bits of information to travel long distances, and so Postman claims \"the local and the timeless ... lost their central position in newspapers, eclipsed by the dazzle of distance and speed ... Wars, crimes, crashes, fires, floods—much of it the social and political equivalent of Adelaide's whooping coughs—became the content of what people called 'the news of the day'\" (pp. 66–67).\n\nA high information-action ratio, therefore, refers to the helplessness people confront when faced with decontextualized information. Someone may know Adelaide has the whooping cough, but what could anyone do about it? Postman said that this kind of access to decontextualized information \"made the relationship between information and action both abstract and remote.\" Information consumers were \"faced with the problem of a diminished social and political potency.\"\nThe term was referenced in Arctic Monkeys' song \"Four Out of Five\" off the band's 2018 album Tranquility Base Hotel & Casino, where the Information Action Ratio is the name of a fictional taqueria on the moon.\n\n\n"}
{"id": "48289630", "url": "https://en.wikipedia.org/wiki?curid=48289630", "title": "José Bonet Solves", "text": "José Bonet Solves\n\nJosé Bonet Solves (Valencia, June 18, 1955) is a Spanish mathematician specialist in functional analysis and its applications to complex analysis and linear partial differential equations.\n\nJosé Bonet graduated in Mathematics at the University of Valencia in 1977. In 1980 he defended his Ph.D. thesis in that University under the supervision of Professor Manuel Valdivia Ureña. Bonet was assistant in the University of Valencia from 1977 to 1983; between 1983 and 1987 he was Associate Professor, in the Polytechnic University of Valencia. Since 1987, Bonet is Full Professor in the Applied Mathematics Department at the Polytechnic University of Valencia. He was also Visiting Professor in the University of Paderborn, Germany in 1989 and in 2002. He had an Alexander Von Humboldt fellowship in Düsseldorf (1994), Paderborn (1995, 2008) and Eichstätt (2008,2013); Germany.\n\nHe has been the Director of the institute of mathematics \"Instituto Universitario de Matemática Pura y Aplicada IUMPA\"\n\nJosé Bonet has been Principal Investigator of several research projects of the Spanish Ministry of Education since 1988, integrated actions between Spain and Germany and between Spain and Italy, and of the project of excellence in research PROMETEO of the Generalitat Valenciana (2008-2012) and (2013-2016). He also organized several international meetings on functional analysis.\n\nHe supervised fourteen Ph.D. thesis.\n\n\n\n"}
{"id": "29010830", "url": "https://en.wikipedia.org/wiki?curid=29010830", "title": "Károly Bezdek", "text": "Károly Bezdek\n\nKároly Bezdek (born May 28, 1955 in Budapest, Hungary), is a Hungarian-Canadian mathematician. He is a professor as well as a Canada Research Chair of mathematics and the director of the Centre for Computational and Discrete Geometry at the University of Calgary in Calgary, Alberta, Canada. Also he is a professor (on leave) of mathematics at the University of Pannonia in Veszprém, Hungary. His main research interests are in geometry in particular, in combinatorial, computational, convex, and discrete geometry. He has authored 2 books and more than 120 research papers. He is a founding Editor-in-Chief of the e-journal Contributions to Discrete Mathematics (CDM).\n\nKároly Bezdek was born in Budapest, Hungary, but grew up in Dunaújváros, Hungary. His parents are Károly Bezdek, Sr. (mechanical engineer) and Magdolna Cserey. His brother András Bezdek is also a mathematician. Károly and his brother have scored at the top level in several Mathematics and Physics competitions for high school and university students in Hungary. Károly's list of awards include winning the first prize in the traditional KöMal (Hungarian Math. Journal for Highschool Students) contest in the academic year 1972–1973, as well as winning the first prize for the research results presented at the National Science Conference for Hungarian Undergraduate Students (TDK) in 1978. Károly entered Eötvös Loránd University in Hungary, and completed his Diploma in Mathematics in 1978. Bezdek is married to Éva Bezdek, and has three sons: Dániel, Máté and Márk.\n\nKároly Bezdek received his Ph.D. (1980) as well as his Habilitation degree (1997) in mathematics from Eötvös Loránd University, in Budapest, Hungary and his Candidate of Mathematical Sciences degree (1985) as well as his Doctor of \nMathematical Sciences degree (1995) from the Hungarian Academy of Sciences. He has been a faculty member of the Department of Geometry at Eötvös Loránd University in Budapest since 1978. In particular, he has been the chair of that department between 1999-2006 and a full professor between 1998-2012. During 1978–2003, while being on a number of special leaves from Eötvös Loránd University, he has held numerous visiting positions at research institutions in Canada, Germany, the Netherlands, and United States. This included a period of about 7 years at the Department of Mathematics of Cornell University in Ithaca, New York. Between 1998-2001 Bezdek was appointed a Széchenyi Professor of mathematics at Eötvös Loránd University, in Budapest, Hungary. From 2003 Károly Bezdek is the Canada Research Chair of computational and discrete geometry at the Department of Mathematics and Statistics of the University of Calgary and is the director of the Center for Computational and Discrete Geometry at the University of Calgary. Between 2006-2010 Bezdek was an associated member of the Alfréd Rényi Institute of Mathematics in Budapest, Hungary. From 2010 Bezdek is a full professor (on leave) at the Department of Mathematics of the University of Pannonia in Veszprém, Hungary. Between July–December, 2011 Bezdek was a program co-chair of the 6 month thematic program on discrete geometry and its applications at the Fields Institute in Toronto, Ontario, Canada. Also, he is one of the three founding editors-in-chief of the free peer-reviewed electronic journal Contributions to Discrete Mathematics.\n\nHis research interests are in combinatorial, computational, convex and discrete geometry including some aspects of geometric analysis, rigidity and optimization. He is the author of more than 120 research papers and has written two research monographs. In particular, he is known for resolving the following problems in geometry:\n\n\nHis research monographs \"Classical Topics in Discrete Geometry\", CMS Books in Mathematics, Springer, New York, 2010 and \"Lectures on Sphere Arrangements - the Discrete Geometric Side\", Fields Institute Monographs, Springer, New York, 2013 are centered at the research results mentioned above and lead the reader to the frontiers of discrete geometry. The conference proceedings \"Discrete Geometry and Optimization\", Fields Institute Communications, Springer, New York, 2013, edited jointly by him, Antoine Deza (McMaster University) and Yinyu Ye (Stanford University) reflects and stimulates the fruitful interplay between discrete geometry and optimization.\n\n15 May 2017: 2017 Research Excellence Award of the University of Calgary\n\n19 June 2015: 2015 László Fejes Tóth Prize (Hungarian: Fejes Tóth László-díj)\n\n"}
{"id": "6548718", "url": "https://en.wikipedia.org/wiki?curid=6548718", "title": "Learnable evolution model", "text": "Learnable evolution model\n\nThe learnable evolution model (LEM) is a non-Darwinian methodology for evolutionary computation that employs machine learning to guide the generation of new individuals (candidate problem solutions). Unlike standard, Darwinian-type evolutionary computation methods that use random or semi-random operators for generating new individuals (such as mutations and/or recombinations), LEM employs hypothesis generation and instantiation operators.\nThe hypothesis generation operator applies a machine learning program to induce descriptions that distinguish between high-fitness and low-fitness individuals in each consecutive population. Such descriptions delineate areas in the search space that most likely contain the desirable solutions. Subsequently the instantiation operator samples these areas to create new individuals.\nLEM has been modified from optimization domain to classification domain by augmented LEM with ID3. (February 2013 by M. Elemam Shehab, K. Badran, M. Zaki and Gouda I. Salama.\n\n"}
{"id": "10134881", "url": "https://en.wikipedia.org/wiki?curid=10134881", "title": "Math Country", "text": "Math Country\n\nMath Country is an instructional television program produced by Kentucky Educational Television, in the late 1970s.\n\nThe show taught elementary math concepts and featured actor Ray Walston as a ghost named Lionel Hardway who inhabits the family farm, now lived in and ran by his descendants, helping them with various math problems, and sometimes getting involved in side stories involving the living members of the Hardway family.\n\nEpisodes were roughly 15 minutes in length (design for use during limited classroom time) and were broadcast on educational and public television channels during the school year.\n\nEach broadcast was usually followed by a short called \"Math Country Plus\", which usually dealt with how a girl in school figured out how to solve problems on her own, using her own creativity and intellect, played by two actors who interacted with the girl on a fantasy set to represent the inside of the girl's head. \n"}
{"id": "9148277", "url": "https://en.wikipedia.org/wiki?curid=9148277", "title": "Mathematical descriptions of the electromagnetic field", "text": "Mathematical descriptions of the electromagnetic field\n\nThere are various mathematical descriptions of the electromagnetic field that are used in the study of electromagnetism, one of the four fundamental interactions of nature. In this article, several approaches are discussed, although the equations are in terms of electric and magnetic fields, potentials, and charges with currents, generally speaking.\n\nThe most common description of the electromagnetic field uses two three-dimensional vector fields called the electric field and the magnetic field. These vector fields each have a value defined at every point of space and time and are thus often regarded as functions of the space and time coordinates. As such, they are often written as (electric field) and (magnetic field).\n\nIf only the electric field (E) is non-zero, and is constant in time, the field is said to be an electrostatic field. Similarly, if only the magnetic field (B) is non-zero and is constant in time, the field is said to be a magnetostatic field. However, if either the electric or magnetic field has a time-dependence, then both fields must be considered together as a coupled electromagnetic field using Maxwell's equations.\n\nThe behaviour of electric and magnetic fields, whether in cases of electrostatics, magnetostatics, or electrodynamics (electromagnetic fields), is governed by Maxwell's equations:\n\nwhere \"ρ\" is the charge density, which can (and often does) depend on time and position, \"ε\" is the electric constant, \"μ\" is the magnetic constant, and J is the current per unit area, also a function of time and position. The equations take this form with the International System of Quantities.\n\nWhen dealing with only nondispersive isotropic linear materials, Maxwell's equations are often modified to ignore bound charges by replacing the permeability and permittivity of free space with the permeability and permittivity of the linear material in question. For some materials that have more complex responses to electromagnetic fields, these properties can be represented by tensors, with time-dependence related to the material's ability to respond to rapid field changes (dispersion (optics), Green–Kubo relations), and possibly also field dependencies representing nonlinear and/or nonlocal material responses to large amplitude fields (nonlinear optics).\n\nMany times in the use and calculation of electric and magnetic fields, the approach used first computes an associated potential: the electric potential, \"formula_1\", for the electric field, and the magnetic potential, A, for the magnetic field. The electric potential is a scalar field, while the magnetic potential is a vector field. This is why sometimes the electric potential is called the scalar potential and the magnetic potential is called the vector potential. These potentials can be used to find their associated fields as follows:\n\nThese relations can be substituted into Maxwell's equations to express the latter in terms of the potentials. Faraday's law and Gauss's law for magnetism reduce to identities (e.g., in the case of Gauss's Law for magnetism, ). The other two of Maxwell's equations turn out less simply.\n\nThese equations taken together are as powerful and complete as Maxwell's equations. Moreover, the problem has been reduced somewhat, as the electric and magnetic fields together had six components to solve for. In the potential formulation, there are only four components: the electric potential and the three components of the vector potential. However, the equations are messier than Maxwell's equations using the electric and magnetic fields.\n\nThese equations can be simplified by taking advantage of the fact that only the electric and magnetic fields are physically meaningful quantities that can be measured; the potentials are not. There is a freedom to constrain the form of the potentials provided that this does not affect the resultant electric and magnetic fields, called gauge freedom. Specifically for these equations, for any choice of a twice-differentiable scalar function of position and time \"λ\", if is a solution for a given system, then so is another potential given by:\n\nThis freedom can be used to simplify the potential formulation. Either of two such scalar functions is typically chosen: the Coulomb gauge and the Lorenz gauge.\n\nThe Coulomb gauge is chosen in such a way that formula_6, which corresponds to the case of magnetostatics. In terms of \"λ\", this means that it must satisfy the equation\n\nThis choice of function results in the following formulation of Maxwell's equations:\n\nSeveral features about Maxwell's equations in the Coulomb gauge are as follows. Firstly, solving for the electric potential is very easy, as the equation is a version of Poisson's equation. Secondly, solving for the magnetic vector potential is particularly difficult. This is the big disadvantage of this gauge. The third thing to note, and something which is not immediately obvious, is that the electric potential changes instantly everywhere in response to a change in conditions in one locality.\n\nFor instance, if a charge is moved in New York at 1 pm local time, then a hypothetical observer in Australia who could measure the electric potential directly would measure a change in the potential at 1 pm New York time. This seemingly goes violates causality in special relativity, i.e. the impossibility of information, signals, or anything travelling faster than the speed of light. The resolution to this apparent problem lies in the fact that, as previously stated, no observers can measure the potentials; they measure the electric and magnetic fields. So, the combination of ∇\"φ\" and ∂A/∂\"t\" used in determining the electric field restores the speed limit imposed by special relativity for the electric field, making all observable quantities consistent with relativity.\n\nA gauge that is often used is the Lorenz gauge condition. In this, the scalar function \"λ\" is chosen such that\n\nmeaning that \"λ\" must satisfy the equation\n\nThe Lorenz gauge results in the following form of Maxwell's equations:\n\nThe operator formula_14 is called the d'Alembertian (some authors denote this by only the square formula_15). These equations are inhomogeneous versions of the wave equation, with the terms on the right side of the equation serving as the source functions for the wave. As with any wave equation, these equations lead to two types of solution: advanced potentials (which are related to the configuration of the sources at future points in time), and retarded potentials (which are related to the past configurations of the sources); the former are usually disregarded where the field is to analyzed from a causality perspective.\n\nAs pointed out above, the Lorenz gauge is no more valid than any other gauge since the potentials cannot be measured. Despite this, there are certain quantum mechanical phenomena in which potentials appear to affect particles in regions where the observable field vanishes throughout the region, for example as in the Aharonov–Bohm effect. However, these phenomena do not provide a means to directly measure the potentials nor to detect a difference between different but mutually gauge equivalent potentials. The Lorenz gauge has the further advantage of the equations being Lorentz invariant.\n\nCanonical quantization of the electromagnetic fields proceeds by elevating the scalar and vector potentials; \"φ\"(x), A(x), from fields to field operators. Substituting into the previous Lorenz gauge equations gives:\n\nHere, J and \"ρ\" are the current and charge density of the \"matter field\". If the matter field is taken so as to describe the interaction of electromagnetic fields with the Dirac electron given by the four-component Dirac spinor field \"ψ\", the current and charge densities have form:\n\nwhere α are the first three Dirac matrices. Using this, we can re-write Maxwell's equations as:\n\nwhich is the form used in quantum electrodynamics.\n\nAnalogous to the tensor formulation, two objects, one for the field and one for the current, are introduced. In geometric algebra (GA) these are multivectors. The field multivector, known as the Riemann–Silberstein vector, is\n\nand the current multivector is\n\nwhere, in the algebra of physical space (APS) formula_21 with the vector basis formula_22. The unit pseudoscalar is formula_23 (assuming an orthonormal basis). Orthonormal basis vectors share the algebra of the Pauli matrices, but are usually not equated with them. After defining the derivative\n\nMaxwell's equations are reduced to the single equation\n\nIn three dimensions, the derivative has a special structure allowing the introduction of a cross product:\n\nfrom which it is easily seen that Gauss's law is the scalar part, the Ampère–Maxwell law is the vector part, Faraday's law is the pseudovector part, and Gauss's law for magnetism is the pseudoscalar part of the equation. After expanding and rearranging, this can be written as\n\nWe can identify APS as a subalgebra of the spacetime algebra (STA) formula_27, defining formula_28 and formula_29. The formula_30s have the same algebraic properties of the gamma matrices but their matrix representation is not needed. The derivative is now\n\nThe Riemann–Silberstein becomes a bivector\n\nformula_32\n\nand the charge and current density become a vector\n\nOwing to the identity\n\nMaxwell's equations reduce to the single equation\n\nIn free space, where and are constant everywhere, Maxwell's equations simplify considerably once the language of differential geometry and differential forms is used. In what follows, cgs-Gaussian units, not SI units are used. (To convert to SI, see here.) The electric and magnetic fields are now jointly described by a 2-form F in a 4-dimensional spacetime manifold. The Faraday tensor formula_35 (electromagnetic tensor) can be written as a 2-form in Minkowski space with metric signature as\n\nwhich, as the curvature form, is the exterior derivative of the electromagnetic four-potential,\n\nThe source free equations can be written by the action of the exterior derivative on this 2-form. But for the equations with source terms (Gauss's law and the Ampère-Maxwell equation), the Hodge dual of this 2-form is needed. The Hodge 'star' dual takes a \"p\"-form to a ()-form, where \"n\" is the number of dimensions. Here, it takes the 2-form (\"F\") and gives another 2-form (in four dimensions, ). For the basis cotangent vectors, the Hodge dual is given as (see here)\n\nand so on. Using these relations, the dual of the Faraday 2-form is the Maxwell tensor,\n\nHere, the 3-form J is called the \"electric current form\" or \"current 3-form\":\n\nwith the corresponding dual 1-form:\n\nMaxwell's equations then reduce to the Bianchi identity and the source equation, respectively:\n\nwhere d denotes the exterior derivative – a natural coordinate- and metric-independent differential operator acting on forms, and the (dual) Hodge star operator formula_42 is a linear transformation from the space of 2-forms to the space of (4 − 2)-forms defined by the metric in Minkowski space (in four dimensions even by any metric conformal to this metric). The fields are in natural units where .\n\nSince d = 0, the 3-form J satisfies the conservation of current (continuity equation):\nThe current 3-form can be integrated over a 3-dimensional space-time region. The physical interpretation of this integral is the charge in that region if it is spacelike, or the amount of charge that flows through a surface in a certain amount of time if that region is a spacelike surface cross a timelike interval.\nAs the exterior derivative is defined on any manifold, the differential form version of the Bianchi identity makes sense for any 4-dimensional manifold, whereas the source equation\nis defined if the manifold is oriented and has a Lorentz metric. In particular the differential form version of the Maxwell equations are a convenient and intuitive formulation of the\nMaxwell equations in general relativity.\n\n\"Note:\" In much of the literature, the notations formula_44 and formula_45 are switched, so that formula_44 is a 1-form called the current and formula_45 is a 3-form called the dual current.\n\nIn a linear, macroscopic theory, the influence of matter on the electromagnetic field is described through more general linear transformation in the space of 2-forms. We call\nthe constitutive transformation. The role of this transformation is comparable to the Hodge duality transformation. The Maxwell equations in the presence of matter then become:\nwhere the current 3-form J still satisfies the continuity equation .\n\nWhen the fields are expressed as linear combinations (of exterior products) of basis forms \"θ\",\nthe constitutive relation takes the form\nwhere the field coefficient functions are antisymmetric in the indices and the constitutive coefficients are antisymmetric in the corresponding pairs. In particular, the Hodge duality transformation leading to the vacuum equations discussed above are obtained by taking\nwhich up to scaling is the only invariant tensor of this type that can be defined with the metric.\n\nIn this formulation, electromagnetism generalises immediately to any 4-dimensional oriented manifold or with small adaptations any manifold.\n\nMatter and energy generate curvature of spacetime. This is the subject of general relativity. Curvature of spacetime affects electrodynamics. An electromagnetic field having energy and momentum also generates curvature in spacetime. Maxwell's equations in curved spacetime can be obtained by replacing the derivatives in the equations in flat spacetime with covariant derivatives. (Whether this is the appropriate generalization requires separate investigation.) The sourced and source-free equations become (cgs-Gaussian units):\n\nand\n\nHere,\n\nis a Christoffel symbol that characterizes the curvature of spacetime and ∇ is the covariant derivative.\n\nThe formulation of the Maxwell equations in terms of differential forms can be used without change in general relativity. The equivalence of the more traditional general relativistic formulation using the covariant derivative with the differential form formulation can be seen as follows. Choose local coordinates \"x\" which gives a basis of 1-forms d\"x\" in every point of the open set where the coordinates are defined. Using this basis and cgs-Gaussian units we define\n\n\nThe epsilon tensor contracted with the differential 3-form produces 6 times the number of terms required.\n\nHere \"g\" is as usual the determinant of the matrix representing the metric tensor, \"g\". A small computation that uses the symmetry of the Christoffel symbols (i.e., the torsion-freeness of the Levi-Civita connection) and the covariant constantness of the Hodge star operator then shows that in this coordinate neighborhood we have:\n\n\nAn elegant and intuitive way to formulate Maxwell's equations is to use complex line bundles or principal bundles with fibre U(1). The connection ∇ on the line bundle has a curvature F = ∇ which is a two-form that automatically satisfies and can be interpreted as a field-strength. If the line bundle is trivial with flat reference connection \"d\" we can write ∇ = d + A and with A the 1-form composed of the electric potential and the magnetic vector potential.\n\nIn quantum mechanics, the connection itself is used to define the dynamics of the system. This formulation allows a natural description of the Aharonov–Bohm effect. In this experiment, a static magnetic field runs through a long magnetic wire (e.g., an iron wire magnetized longitudinally). Outside of this wire the magnetic induction is zero, in contrast to the vector potential, which essentially depends on the magnetic flux through the cross-section of the wire and does not vanish outside. Since there is no electric field either, the Maxwell tensor throughout the space-time region outside the tube, during the experiment. This means by definition that the connection ∇ is flat there.\n\nHowever, as mentioned, the connection depends on the magnetic field through the tube since the holonomy along a non-contractible curve encircling the tube is the magnetic flux through the tube in the proper units. This can be detected quantum-mechanically with a double-slit electron diffraction experiment on an electron wave traveling around the tube. The holonomy corresponds to an extra phase shift, which leads to a shift in the diffraction pattern.\n\nFollowing are the reasons for using each of such formulations.\n\nIn advanced classical mechanics it is often useful, and in quantum mechanics frequently essential, to express Maxwell's equations in a \"potential formulation\" involving the electric potential (also called scalar potential) \"φ\", and the magnetic potential (also called vector potential) A. For example, the analysis of radio antennas makes full use of Maxwell's vector and scalar potentials to separate the variables, a common technique used in formulating the solutions of differential equations. The potentials can be introduced by using the Poincaré lemma on the homogeneous equations to solve them in a universal way (this assumes that we consider a topologically simple, e.g. contractible space). The potentials are defined as in the table above. Alternatively, these equations define E and B in terms of the electric and magnetic potentials which then satisfy the homogeneous equations for E and B as identities. Substitution gives the non-homogeneous Maxwell equations in potential form.\n\nMany different choices of A and \"φ\" are consistent with given observable electric and magnetic fields E and B, so the potentials seem to contain more, (classically) unobservable information. The non uniqueness of the potentials is well understood, however. For every scalar function of position and time , the potentials can be changed by a gauge transformation as \nwithout changing the electric and magnetic field. Two pairs of gauge transformed potentials and are called \"gauge equivalent\", and the freedom to select any pair of potentials in its gauge equivalence class is called gauge freedom. Again by the Poincaré lemma (and under its assumptions), gauge freedom is the only source of indeterminacy, so the field formulation is equivalent to the potential formulation if we consider the potential equations as equations for gauge equivalence classes.\n\nThe potential equations can be simplified using a procedure called gauge fixing. Since the potentials are only defined up to gauge equivalence, we are free to impose additional equations on the potentials, as long as for every pair of potentials there is a gauge equivalent pair that satisfies the additional equations (i.e. if the gauge fixing equations define a slice to the gauge action). The gauge-fixed potentials still have a gauge freedom under all gauge transformations that leave the gauge fixing equations invariant. Inspection of the potential equations suggests two natural choices. In the Coulomb gauge, we impose which is mostly used in the case of magneto statics when we can neglect the \"c\"∂A/∂\"t\" term. In the Lorenz gauge (named after the Dane Ludvig Lorenz), we impose \nThe Lorenz gauge condition has the advantage of being Lorentz invariant and leading to Lorentz-invariant equations for the potentials.\n\nMaxwell's equations are exactly consistent with special relativity—i.e., if they are valid in one inertial reference frame, then they are automatically valid in every other inertial reference frame. In fact, Maxwell's equations were crucial in the historical development of special relativity. However, in the usual formulation of Maxwell's equations, their consistency with special relativity is not obvious; it can only be proven by a laborious calculation.\n\nFor example, consider a conductor moving in the field of a magnet. In the frame of the magnet, that conductor experiences a \"magnetic\" force. But in the frame of a conductor moving relative to the magnet, the conductor experiences a force due to an \"electric\" field. The motion is exactly consistent in these two different reference frames, but it mathematically arises in quite different ways.\n\nFor this reason and others, it is often useful to rewrite Maxwell's equations in a way that is \"manifestly covariant\"—i.e. \"obviously\" consistent with special relativity, even with just a glance at the equations—using covariant and contravariant four-vectors and tensors. This can be done using the EM tensor F, or the 4-potential A, with the 4-current J – see covariant formulation of classical electromagnetism.\n\nGauss's law for magnetism and the Faraday–Maxwell law can be grouped together since the equations are homogeneous, and be seen as geometric \"identities\" expressing the \"field\" F (a 2-form), which can be derived from the \"4-potential\" A. Gauss's law for electricity and the Ampere–Maxwell law could be seen as the \"dynamical equations of motion\" of the fields, obtained via the Lagrangian principle of least action, from the \"interaction term\" AJ (introduced through gauge covariant derivatives), coupling the field to matter. For the field formulation of Maxwell's equations in terms of a principle of extremal action, see electromagnetic tensor.\n\nOften, the time derivative in the Faraday–Maxwell equation motivates calling this equation \"dynamical\", which is somewhat misleading in the sense of the preceding analysis. This is rather an artifact of breaking relativistic covariance by choosing a preferred time direction. To have physical degrees of freedom propagated by these field equations, one must include a kinetic term for A, and take into account the non-physical degrees of freedom that can be removed by gauge transformation . See also gauge fixing and Faddeev–Popov ghosts.\n\nThis formulation uses the algebra that spacetime generates through the introduction of a distributive, associative (but not commutative) product called the geometric product. Elements and operations of the algebra can generally be associated with geometric meaning. The members of the algebra may be decomposed by grade (as in the formalism of differential forms) and the (geometric) product of a vector with a \"k\"-vector decomposes into a -vector and a -vector. The -vector component can be identified with the inner product and the -vector component with the outer product. It is of algebraic convenience that the geometric product is invertible, while the inner and outer products are not. The derivatives that appear in Maxwell's equations are vectors and electromagnetic fields are represented by the Faraday bivector F. This formulation is as general as that of differential forms for manifolds with a metric tensor, as then these are naturally identified with \"r\"-forms and there are corresponding operations. Maxwell's equations reduce to one equation in this formalism. This equation can be separated into parts as is done above for comparative reasons.\n\n\n"}
{"id": "321157", "url": "https://en.wikipedia.org/wiki?curid=321157", "title": "Model checking", "text": "Model checking\n\nIn computer science, model checking or property checking refers to the following problem:\nGiven a model of a system, exhaustively and automatically check whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of \"finite-state\" systems.\n\nIn order to solve such a problem algorithmically, both the model of the system and the specification are formulated in some precise mathematical language. To this end, the problem is formulated as a task in logic, namely to\ncheck whether a given structure satisfies a given logical formula.\nThis general concept applies to many kinds of logics and suitable structures. A simple model checking problem is verifying whether a given formula in the propositional logic is satisfied by a given structure.\n\nProperty checking is used for verification instead of equivalence checking when two descriptions are not functionally equivalent. Particularly, during refinement, the specification is complemented with the details that are unnecessary in the higher level specification. Yet, there is no need to verify the newly introduced properties against the original specification. It is not even possible. Therefore, the strict bi-directional equivalence check is relaxed to one-way property checking. The implementation or design is regarded a model of the circuit whereas the specifications are properties that the model must satisfy.\n\nAn important class of model checking methods have been developed for checking models of hardware and software designs\nwhere the specification is given by a temporal logic formula.\nPioneering work in temporal logic specification was done by Amir Pnueli, who received the 1996 Turing award for \"[...] seminal work introducing temporal logic into computing science\". Model checking began with\nthe pioneering work by E. M. Clarke and E. A. Emerson and by J. P. Queille and J. Sifakis. Clarke, Emerson, and Sifakis shared the 2007 Turing Award for their seminal work founding and\ndeveloping the field of model checking.\n\nModel checking is most often applied to hardware designs. For software, because of undecidability (see computability theory) the approach cannot be fully algorithmic; typically it may fail to prove or disprove a given property. In embedded systems hardware designs it is possible to validate (verify against some specified requirements) a specification delivered i.e. by means of UML activity diagrams or control interpreted Petri nets.\n\nThe structure is usually given as a source code description in an industrial hardware description language or a special-purpose language. Such a program corresponds to a finite state machine (FSM), i.e., a directed graph consisting of nodes (or vertices) and edges. A set of atomic propositions is associated with each node, typically stating which memory elements are one. The nodes represent states of a system, the edges represent possible transitions which may alter the state, while the atomic propositions represent the basic properties that hold at a point of execution.\n\nFormally, the problem can be stated as follows: given a desired property, expressed as a temporal logic formula \"p\", and a structure \"M\" with initial state \"s\", decide if formula_1. If M is finite, as it is in hardware, model checking reduces to a graph search.\n\nstate space enumeration, symbolic state space enumeration, abstract interpretation, symbolic simulation, symbolic trajectory evaluation, symbolic execution\n\nInstead of enumerating reachable states one at a time, the state space can sometimes be traversed more efficiently by considering large numbers of states at a single step.\nWhen such state space traversal is based on representations of set of states and transition relations as logical formulas, binary decision diagrams (BDD) or other related data structures,\nthe model-checking method is symbolic.\n\nHistorically, the first symbolic methods used BDDs.\nAfter the success of propositional satisfiability in solving the planning problem in artificial intelligence (see satplan) in 1996,\nthe same approach was generalized to model checking for the Linear Temporal Logic LTL (the planning problem corresponds to model-checking for safety properties).\nThis method is known as bounded model checking. The success of Boolean satisfiability solvers in bounded model checking led to the widespread use of satisfiability solvers in symbolic model checking.\n\nModel checking tools face a combinatorial blow up of the state-space, commonly known as the state explosion problem, that must be addressed to solve most real-world problems. There are several approaches to combat this problem.\n\n\nModel checking tools were initially developed to reason about the logical correctness of discrete state systems, but have since been extended to deal with real-time and limited forms of hybrid systems.\n\nHere is a partial list of model checking tools that have a Wikipedia page:\n\n\n\n\n"}
{"id": "2126854", "url": "https://en.wikipedia.org/wiki?curid=2126854", "title": "Moving-knife procedure", "text": "Moving-knife procedure\n\nIn the mathematics of social science, and especially game theory, a moving-knife procedure is a type of solution to the fair division problem. The canonical example is the division of a cake using a knife.\n\nThe simplest example is a moving-knife equivalent of the I cut, you choose scheme, first described by A.K.Austin as a prelude to his own procedure: \n(This procedure is not necessarily efficient.)\n\nGeneralizing this scheme to more than two players cannot be done by a discrete procedure without sacrificing envy-freeness.\n\nExamples of moving-knife procedures include\n\n"}
{"id": "46975535", "url": "https://en.wikipedia.org/wiki?curid=46975535", "title": "Multimodal learning", "text": "Multimodal learning\n\nThe information in real world usually comes as different modalities. For example, images are usually associated with tags and text explanations; texts contain images to more clearly express the main idea of the article. Different modalities are characterized by very different statistical properties. For instance, images are usually represented as pixel intensities or outputs of feature extractors, while texts are represented as discrete word count vectors. Due to the distinct statistical properties of different information resources, it is very important to discover the relationship between different modalities. Multimodal learning is a good model to represent the joint representations of different modalities. The multimodal learning model is also capable to fill missing modality given the observed ones. The multimodal learning model combines two deep Boltzmann machines each corresponds to one modality. An additional hidden layer is placed on top of the two Boltzmann Machines to give the joint representation.\n\nA lot of models/algorithms have been implemented to retrieve and classify a certain type of data, e.g. image or text (where humans who interacts with machines can extract images in a form of pictures and text that could be any message etc). However, data usually comes with different modalities (it is the degree to which a system's components may be separated or combined) which carry different information. For example, it is very common to caption an image to convey the information not presented by this image. Similarly, sometimes it is more straightforward to use an image to describe the information which may not be obvious from texts. As a results, if some different words appear in similar images, these words are very likely used to describe the same thing. Conversely, if some words are used in different images, these images may represent the same object. Thus, it is important to invite a novel model which is able to jointly represent the information such that the model can capture the correlation structure between different modalities. Moreover, it should also be able to recover missing modalities given observed ones, e.g. predicting possible image object according to text description. The Multimodal Deep Boltzmann Machine model satisfies the above purposes.\n\nA Boltzmann machine is a type of stochastic neural network invented by Geoffrey Hinton and Terry Sejnowski in 1985. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets. They are named after the Boltzmann distribution in statistical mechanics. The units in Boltzmann machines are divided into two groups-visible units and hidden units. General Boltzmann machines allow connection between any units. However, learning is impractical using general Boltzmann Machines because the computational time is exponential to the size of the machine. A more efficient architecture is called restricted Boltzmann machine where connection is only allowed between hidden unit and visible unit, which is described in the next section.\n\nA restricted Boltzmann machine is an undirected graphical model with stochastic visible variable and stochastic hidden variables. Each visible variable is connected to each hidden variable. The energy function of the model is defined as \nwhere formula_2 are model parameters: formula_3 represents the symmetric interaction term between visible unit formula_4 and hidden unit formula_5; formula_6 and formula_7 are bias terms. The joint distribution of the system is defined as \nwhere formula_9 is a normalizing constant.\nThe conditional distribution over hidden formula_10 and formula_11 can be derived as logistic function in terms of model parameters.\nwhere formula_16 is the logistic function.\nThe derivative of the log-likelihood with respect to the model parameters can be decomposed as the difference between the \"model's expectation\" and \"data-dependent expectation\".\n\nGaussian-Bernoulli RBMs are a variant of restricted Boltzmann machine used for modeling real-valued vectors such as pixel intensities. It is usually used to model the image data. The energy of the system of the Gaussian-Bernoulli RBM is defined as \nwhere formula_18 are the model parameters. The joint distribution is defined the same as the one in restricted Boltzmann machine. The conditional distributions now become \nIn Gaussian-Bernoulli RBM, the visible unit conditioned on hidden units is modeled as a Gaussian distribution.\n\nThe Replicated Softmax Model is also an variant of restricted Boltzmann machine and commonly used to model word count vectors in a document. In a typical text mining problem, let formula_23 be the dictionary size, and formula_24 be the number of words in the document. Let formula_25 be a formula_26 binary matrix with formula_27 only when the formula_28 word in the document is the formula_29 word in the dictionary. formula_30 denotes the count for the formula_29 word in the dictionary. The energy of the state formula_32 for a document contains formula_24 words is defined as\nThe conditional distributions are given by \n\nA deep Boltzmann machine has a sequence of layers of hidden units.There are only connections between adjacent hidden layers, as well as between visible units and hidden units in the first hidden layer. The energy function of the system adds layer interaction terms to the energy function of general restricted Boltzmann machine and is defined by \nformula_37\n\nThe joint distribution is \n\nMultimodal deep Boltzmann machine uses an image-text bi-modal DBM where the image pathway is modeled as Gaussian-Bernoulli DBM and text pathway as Replicated Softmax DBM, and each DBM has two hidden layers and one visible layer. The two DBMs join together at an additional top hidden layer. The joint distribution over the multi-modal inputs defined as \nformula_39\n\nThe conditional distributions over the visible and hidden units are \n\nExact maximum likelihood learning in this model is intractable, but approximate learning of DBMs can be carried out by using a variational approach, where mean-field inference is used to estimate data-dependent expectations and an MCMC based stochastic approximation procedure is used to approximate the model’s expected sufficient statistics.\n\nMultimodal deep Boltzmann machines is successfully used in classification and missing data retrieval. The classification accuracy of multimodal deep Boltzmann machine outperforms support vector machines, latent Dirichlet allocation and deep belief network, when models are tested on data with both image-text modalities or with single modality. Multimodal deep Boltzmann machine is also able to predict the missing modality given the observed ones with reasonably good precision.\n\n"}
{"id": "12267066", "url": "https://en.wikipedia.org/wiki?curid=12267066", "title": "Narayana number", "text": "Narayana number\n\nIn combinatorics, the Narayana numbers \"N\"(\"n\", \"k\"), \"n\" = 1, 2, 3 ..., 1 ≤ \"k\" ≤ \"n\", form a triangular array of natural numbers, called Narayana triangle, that occur in various counting problems. They are named after Canadian mathematician T. V. Narayana (1930–1987).\n\nThe Narayana numbers can be expressed in terms of binomial coefficients:\n\nThe first eight rows of the Narayana triangle read:\n\nAn example of a counting problem whose solution can be given in terms of the Narayana numbers \"N\"(\"n\", \"k\"), is the number of expressions containing \"n\" pairs of parentheses, which are correctly matched and which contain \"k\" distinct nestings. For instance, \"N\"(4, 2) = 6 as with four pairs of parentheses six sequences can be created which each contain two times the sub-pattern '()':\n\nFrom this example it should be obvious that \"N\"(\"n\", 1) = 1, since the only way to get a single sub-pattern '()' is to have all the opening parentheses in the first n positions, followed by all the closing parentheses. Also \"N\"(\"n\", \"n\") = 1, as distinct nestings can be achieved only by the repetitive pattern ()()() ... (). More generally, it can be shown that the Narayana triangle is symmetric: \"N\"(\"n\", \"k\") = \"N\"(\"n\", \"n\" − \"k\" + \"1\").\n\nThe sum of the rows in this triangle equal the Catalan numbers:\n\nTo illustrate this relationship, the Narayana numbers also count the number of paths from (0, 0) to (2\"n\", 0), with steps only northeast and southeast, not straying below the \"x\"-axis, with \"k\" peaks.\n\nThe following figures represent the Narayana numbers \"N\"(4, \"k\"):\n\nThe sum of \"N\"(4, \"k\") is 1 + 6 + 6 + 1, or 14, which is the same as Catalan number \"C\". This sum coincides with the interpretation of Catalan numbers as the number of monotonic paths along the edges of an \"n\" × \"n\" grid that do not pass above the diagonal.\n\nIn the study of partitions, we see that in a set containing n elements, we may partition that set in formula_3 different ways, where formula_3 is the n Bell number. Furthermore, the number of ways to partition a set into exactly k blocks we use the Stirling numbers formula_5. Both of these concepts are a bit off-topic, but a necessary foundation for understanding the use of the Narayana numbers. In both of the above two notions crossing partitions are accounted for.\n\nTo reject the crossing partitions and count only the noncrossing partitions, we may use the Catalan numbers to count the non-crossing partitions of all n elements of the set, formula_6. To count the non-crossing partitions in which the set is partitioned in exactly k blocks, we use the Narayana number formula_7.\n\nThe generating function of the Narayana numbers is\n\n"}
{"id": "30970275", "url": "https://en.wikipedia.org/wiki?curid=30970275", "title": "No small subgroup", "text": "No small subgroup\n\nIn mathematics, especially in topology, a topological group \"G\" is said to have no small subgroup if there exists a neighborhood \"U\" of the identity that contains no nontrivial subgroup of \"G\". An abbreviation '\"NSS\"' is sometimes used. A basic example of a topological group with no small subgroup is the general linear group over the complex numbers.\n\nA locally compact, separable metric, locally connected group with no small subgroup is a Lie group. (cf. Hilbert's fifth problem.)\n\n"}
{"id": "8731687", "url": "https://en.wikipedia.org/wiki?curid=8731687", "title": "Pre-math skills", "text": "Pre-math skills\n\n\"Pre-math skills\" (referred to in British English as pre-maths skills) is a term used in some countries to refer to math skills learned by preschoolers and kindergarten students, including learning to count numbers (usually from 1 to 10 but occasionally including 0), learning the proper sequencing of numbers, learning to determine which shapes are bigger or smaller, and learning to count objects on a screen or book. Pre-math skills are also tied into literacy skills to learn the correct pronunciations of numbers.\n\n"}
{"id": "41600", "url": "https://en.wikipedia.org/wiki?curid=41600", "title": "Pulse", "text": "Pulse\n\nIn medicine, a pulse represents the tactile arterial palpation of the heartbeat by trained fingertips. The pulse may be palpated in any place that allows an artery to be compressed near the surface of the body, such as at the neck (carotid artery), wrist (radial artery), at the groin (femoral artery), behind the knee (popliteal artery), near the ankle joint (posterior tibial artery), and on foot (dorsalis pedis artery). Pulse (or the count of arterial pulse per minute) is equivalent to measuring the heart rate. The heart rate can also be measured by listening to the heart beat by auscultation, traditionally using a stethoscope and counting it for a minute. The radial pulse is commonly measured using three fingers. This has a reason: the finger closest to the heart is used to occlude the pulse pressure, the middle finger is used get a crude estimate of the blood pressure, and the finger most distal to the heart (usually the ring finger) is used to nullify the effect of the ulnar pulse as the two arteries are connected via the palmar arches (superficial and deep). \nThe study of the pulse is known as sphygmology.\n\nClaudius Galen was perhaps the first physiologist to describe the pulse. The pulse is an expedient tactile method of determination of systolic blood pressure to a trained observer. Diastolic blood pressure is non-palpable and unobservable by tactile methods, occurring between heartbeats.\n\nPressure waves generated by the heart in systole move the arterial walls. Forward movement of blood occurs when the boundaries are pliable and compliant. These properties form enough to create a palpable pressure wave.\n\nPulse velocity, pulse deficits and much more physiologic data are readily and simplistically visualized by the use of one or more arterial catheters connected to a transducer and oscilloscope. This invasive technique has been commonly used in intensive care since the 1970s.\n\nThe rate of the pulse is observed and measured by tactile or visual means on the outside of an artery and is recorded as beats per minute or BPM.\n\nThe pulse may be further indirectly observed under light absorbances of varying wavelengths with assigned and inexpensively reproduced mathematical ratios. Applied capture of variances of light signal from the blood component hemoglobin under oxygenated vs. deoxygenated conditions allows the technology of pulse oximetry.\n\nNormal pulse rates at rest, in beats per minute (BPM):\n\nThe pulse rate can be used to check overall heart health and fitness level. Generally lower is better, but bradycardias can be dangerous. Symptoms of a dangerously slow heartbeat include weakness, loss of energy and fainting.\n\nA normal pulse is regular in rhythm and force. An irregular pulse may be due to sinus arrhythmia, ectopic beats, atrial fibrillation, paroxysmal atrial tachycardia, atrial flutter, partial heart block etc. Intermittent dropping out of beats at pulse is called \"intermittent pulse\". Examples of \"regular\" intermittent (regularly irregular) pulse include pulsus bigeminus, second-degree atrioventricular block. An example of \"irregular\" intermittent (irregularly irregular) pulse is atrial fibrillation.\n\nThe degree of expansion displayed by artery during diastolic and systolic state is called volume. It is also known as amplitude, expansion or size of pulse.\n\nA weak pulse signifies narrow pulse pressure. It may be due to low cardiac output (as seen in shock, congestive cardiac failure), hypovolemia, valvular heart disease (such as aortic outflow tract obstruction, mitral stenosis, aortic arch syndrome) etc.\n\nA bounding pulse signifies high pulse pressure. It may be due to low peripheral resistance (as seen in fever, anemia, thyrotoxicosis, , A-V fistula, Paget's disease, beriberi, liver cirrhosis), increased cardiac output, increased stroke volume (as seen in anxiety, exercise, complete heart block, aortic regurgitation), decreased distensibility of arterial system (as seen in atherosclerosis, hypertension and coarctation of aorta).\n\nThe strength of the pulse can also be reported:\n\nAlso known as compressibility of pulse. It is a rough measure of systolic blood pressure.\n\nIt corresponds to diastolic blood pressure. A low tension pulse (pulsus mollis), the vessel is soft or impalpable between beats. In high tension pulse (pulsus durus), vessels feel rigid even between pulse beats.\n\nA form or contour of a pulse is palpatiory estimation of arteriogram. A quickly rising and quickly falling pulse (pulsus celer) is seen in aortic regurgitation. A slow rising and slowly falling pulse (pulsus tardus) is seen in aortic stenosis.\n\nComparing pulses and different places gives valuable clinical information.\n\nA discrepant or unequal pulse between left and right radial artery is observed in anomalous or aberrant course of artery, coarctation of aorta, aortitis, dissecting aneurysm, peripheral embolism etc. An unequal pulse between upper and lower extremities is seen in coarctation to aorta, aortitis, block at bifurcation of aorta, dissection of aorta, iatrogenic trauma and arteriosclerotic obstruction.\n\nA normal artery is not palpable after flattening by digital pressure. A thick radial artery which is palpable 7.5–10 cm up the forearm is suggestive of arteriosclerosis.\n\nIn coarctation of aorta, femoral pulse may be significantly delayed as compared to radial pulse (unless there is coexisting aortic regurgitation). The delay can also be observed in supravalvar aortic stenosis.\n\nSeveral pulse patterns can be of clinically significance. These include:\n\nChinese medicine has focused on the pulse in the upper limbs for several centuries. The concept of pulse diagnosis is essentially based on palpation and observations of the radial and ulnar volar pulses at the readily accessible wrist.\n\n\n\nAlthough the pulse can be felt in multiple places in the head, people should not normally hear their heartbeats within the head. This is called pulsatile tinnitus, and it can indicate several medical disorders.\n\n\nThe first person to accurately measure the pulse rate was Santorio Santorii who invented the \"pulsilogium\", a form of pendulum, based on the work by Galileo Galilei. A century later another physician, de Lacroix, used the pulsilogium to test cardiac function.\n\n\n"}
{"id": "26115282", "url": "https://en.wikipedia.org/wiki?curid=26115282", "title": "Quantum clock", "text": "Quantum clock\n\nA quantum clock is a type of atomic clock with laser cooled single ions confined together in an electromagnetic ion trap. Developed in 2010 by National Institute of Standards and Technology physicists, the clock was 37 times more precise than the then-existing international standard. The quantum logic clock is based on an aluminium spectroscopy ion with a logic atom.\n\nBoth the aluminium-based quantum clock and the mercury-based optical atomic clock track time by the ion vibration at an optical frequency using a UV laser, that is 100,000 times higher than the microwave frequencies used in NIST-F1 and other similar time standards around the world. Quantum clocks like this are able to be far more precise than microwave standards.\n\nThe NIST team are not able to measure clock ticks per second because the definition of a second is based on the NIST-F1 which cannot measure a more precise machine. However the aluminium ion clock's measured frequency to the current standard is 1121015393207857.4(7)Hz. NIST have attributed the clock's accuracy to the fact that it is insensitive to background magnetic and electric fields, and unaffected by temperature.\n\nIn March 2008, physicists at NIST described an experimental quantum logic clock based on individual ions of beryllium and aluminium. This clock was compared to NIST's mercury ion clock. These were the most accurate clocks that had been constructed, with neither clock gaining nor losing time at a rate that would exceed a second in over a billion years.\n\nIn February 2010, NIST physicists described a second, enhanced version of the quantum logic clock based on individual ions of magnesium and aluminium. Considered the world's most precise clock in 2010 with a fractional frequency inaccuracy of , it offers more than twice the precision of the original.\n\nIn terms of standard deviation, the quantum logic clock deviates one second every 3.68 billion () years, while the then current international standard NIST-F1 caesium fountain atomic clock uncertainty was about 3.1 × 10 expected to neither gain nor lose a second in more than 100 million () years.\nIn 2010 an experiment placed two aluminium-ion quantum clocks close to each other, but with the second elevated compared to the first, making the gravitational time dilation effect visible in everyday lab scales.\n\nThe accuracy of quantum clocks has since been superseded by optical lattice clocks based on strontium-87 and ytterbium-171. An experimental optical lattice clock was described in a 2014 Nature paper.\nIn 2015 JILA evaluated the absolute frequency uncertainty of their latest strontium-87 optical lattice clock at , which corresponds to a measurable gravitational time dilation for an elevation change of on planet Earth that according to JILA/NIST Fellow Jun Ye is \"getting really close to being useful for relativistic geodesy\".\nAt this frequency uncertainty, this JILA optical lattice optical clock is expected to neither gain nor lose a second in more than 15 billion () years.\n\n"}
{"id": "12747259", "url": "https://en.wikipedia.org/wiki?curid=12747259", "title": "Quasi-isometry", "text": "Quasi-isometry\n\nIn mathematics, quasi-isometry is an equivalence relation on metric spaces that ignores their small-scale details in favor of their coarse structure. The concept is especially important in geometric group theory following the work of Gromov.\n\nSuppose that formula_1 is a (not necessarily continuous) function from one metric space formula_2 to a second metric space formula_3. Then formula_1 is called a \"quasi-isometry\" from formula_2 to formula_3 if there exist constants formula_7, formula_8, and formula_9 such that the following two properties both hold:\n\nThe two metric spaces formula_2 and formula_3 are called quasi-isometric if there exists a quasi-isometry formula_1 from formula_2 to formula_3.\n\nA map is called a quasi-isometric embedding if it satisfies the first condition but not necessarily the second (i.e. it is coarsely Lipschitz but may fail to be coarsely surjective).\n\nThe map between the Euclidean plane and the plane with the Manhattan distance that sends every point to itself is a quasi-isometry: in it, distances are multiplied by a factor of at most formula_24.\n\nThe map formula_25 (both with the Euclidean metric) that sends every formula_26-tuple of integers to itself is a quasi-isometry: distances are preserved exactly, and every real tuple is within distance formula_27 of an integer tuple. In the other direction, the discontinuous function that rounds every tuple of real numbers to the nearest integer tuple is also a quasi-isometry: each point is taken by this map to a point within distance formula_27 of it, so rounding changes the distance between pairs of points by adding or subtracting at most formula_29.\n\nEvery pair of finite or bounded metric spaces is quasi-isometric. In this case, every function from one space to the other is a quasi-isometry.\n\nIf formula_30 is a quasi-isometry, then there exists a quasi-isometry formula_31. Indeed, formula_32 may be defined by letting formula_11 be any point in the image of formula_1 that is within distance formula_17 of formula_10, and letting formula_32 be any point in formula_38.\n\nSince the identity map is a quasi-isometry, and the composition of two quasi-isometries is a quasi-isometry, it follows that the relation of being quasi-isometric is an equivalence relation on the class of metric spaces.\n\nGiven a finite generating set \"S\" of a finitely generated group \"G\", we can form the corresponding Cayley graph of \"S\" and \"G\". This graph becomes a metric space if we declare the length of each edge to be 1. Taking a different finite generating set \"T\" results in a different graph and a different metric space, however the two spaces are quasi-isometric. This quasi-isometry class is thus an invariant of the group \"G\". Any property of metric spaces that only depends on a space's quasi-isometry class immediately yields another invariant of groups, opening the field of group theory to geometric methods.\n\nMore generally, the Švarc–Milnor lemma states that if a group \"G\" acts properly discontinuously with compact quotient on a proper geodesic space \"X\" then \"G\" is quasi-isometric to \"X\" (meaning that any Cayley graph for \"G\" is). This gives new examples of groups quasi-isometric to each other: \n\nA \"quasi-geodesic\" in a metric space formula_39 is a quasi-isometric embedding of formula_40 into formula_41. More precisely a map formula_42 such that there exists formula_43 so that \nis called a formula_45-quasi-geodesic. Obviously geodesics (parametrised by arclength) are quasi-geodesics. The fact that in some spaces the converse is coarsely true, i.e. that every quasi-geodesic stays within bounded distance of a true geodesic, is called the \"Morse Lemma\" (not to be confused with the perhaps more widely known Morse lemma in differential topology). Formally the statement is:\n\nIt is an important tool in geometric group theory. An immediate application is that any quasi-isometry between proper hyperbolic spaces induces a homeomorphism between their boundaries. This result is the first step in the proof of the Mostow rigidity theorem.\n\nThe following are some examples of properties of group Cayley graphs that are invariant under quasi-isometry:\n\nA group is called \"hyperbolic\" if one of its Cayley graphs is a δ-hyperbolic space for some δ. When translating between different definitions of hyperbolicity, the particular value of δ may change, but the resulting notions of a hyperbolic group turn out to be equivalent.\n\nHyperbolic groups have a solvable word problem. They are biautomatic and automatic.: indeed, they are strongly geodesically automatic, that is, there is an automatic structure on the group, where the language accepted by the word acceptor is the set of all geodesic words.\n\nThe growth rate of a group with respect to a symmetric generating set describes the size of balls in the group. Every element in the group can be written as a product of generators, and the growth rate counts the number of elements that can be written as a product of length \"n\".\n\nAccording to Gromov's theorem, a group of polynomial growth is virtually nilpotent, i.e. it has a nilpotent subgroup of finite index. In particular, the order of polynomial growth formula_54 has to be a natural number and in fact formula_55.\n\nIf formula_56 grows more slowly than any exponential function, \"G\" has a subexponential growth rate. Any such group is amenable.\n\nThe ends of a topological space are, roughly speaking, the connected components of the “ideal boundary” of the space. That is, each end represents a topologically distinct way to move to infinity within the space. Adding a point at each end yields a compactification of the original space, known as the end compactification.\n\nThe ends of a finitely generated group are defined to be the ends of the corresponding Cayley graph; this definition is insensitive to the choice of generating set. Every finitely-generated infinite group has either 1, 2, or infinitely many ends, and Stallings theorem about ends of groups provides a decomposition for groups with more than one end.\n\nAn amenable group is a locally compact topological group \"G\" carrying a kind of averaging operation on bounded functions that is invariant under translation by group elements. The original definition, in terms of a finitely additive invariant measure (or mean) on subsets of \"G\", was introduced by John von Neumann in 1929 under the German name \"messbar\" (\"measurable\" in English) in response to the Banach–Tarski paradox. In 1949 Mahlon M. Day introduced the English translation \"amenable\", apparently as a pun.\n\nIn discrete group theory, where \"G\" has the discrete topology, a simpler definition is used. In this setting, a group is amenable if one can say what proportion of \"G\" any given subset takes up.\n\nIf a group has a Følner sequence then it is automatically amenable.\n\nAn ultralimit is a geometric construction that assigns to a sequence of metric spaces \"X\" a limiting metric space. An important class of ultralimits are the so-called \"asymptotic cones\" of metric spaces. Let (\"X\",\"d\") be a metric space, let \"ω\" be a non-principal ultrafilter on formula_57 and let \"p\" ∈ \"X\" be a sequence of base-points. Then the \"ω\"–ultralimit of the sequence formula_58 is called the asymptotic cone of \"X\" with respect to \"ω\" and formula_59 and is denoted formula_60. One often takes the base-point sequence to be constant, \"p\" = \"p\" for some \"p ∈ X\"; in this case the asymptotic cone does not depend on the choice of \"p ∈ X\" and is denoted by formula_61 or just formula_62.\n\nThe notion of an asymptotic cone plays an important role in geometric group theory since asymptotic cones (or, more precisely, their topological types and bi-Lipschitz types) provide quasi-isometry invariants of metric spaces in general and of finitely generated groups in particular. Asymptotic cones also turn out to be a useful tool in the study of relatively hyperbolic groups and their generalizations.\n\n"}
{"id": "2775268", "url": "https://en.wikipedia.org/wiki?curid=2775268", "title": "Refinement calculus", "text": "Refinement calculus\n\nThe refinement calculus is a formalized approach to stepwise refinement for program construction. The required behaviour of the final executable program is specified as an abstract and perhaps non-executable \"program\", which is then refined by a series of correctness-preserving transformations into an efficiently executable program.\n\nProponents include Ralph-Johan Back, who originated the approach in his 1978 PhD thesis \"On the Correctness of Refinement Steps in Program Development\", and Carroll Morgan, especially with his book \"Programming from Specifications\" (Prentice Hall, 2nd edition, 1994, ). In the latter case, the motivation was to link Abrial's specification notation Z, via a rigorous relation of behaviour-preserving program refinement, to an executable programming notation based on Dijkstra's language of guarded commands. \"Behaviour-preserving\" in this case means that any Hoare triple satisfied by a program should also be satisfied by any refinement of it, which notion leads directly to \"specification statements\" as pre- and postconditions standing, on their own, for any program that could soundly be placed between them.\n\n"}
{"id": "6069215", "url": "https://en.wikipedia.org/wiki?curid=6069215", "title": "Reversible-jump Markov chain Monte Carlo", "text": "Reversible-jump Markov chain Monte Carlo\n\nIn computational statistics, reversible-jump Markov chain Monte Carlo is an extension to standard Markov chain Monte Carlo (MCMC) methodology that allows simulation of the posterior distribution on spaces of varying dimensions.\nThus, the simulation is possible even if the number of parameters in the model is not known. \n\nLet \n\nbe a model indicator and formula_2 the parameter space whose number of dimensions formula_3 depends on the model formula_4. The model indication need not be . The stationary distribution is the joint posterior distribution of formula_5 that takes the values formula_6. \n\nThe proposal formula_7 can be constructed with a mapping formula_8 of formula_9 and formula_10, where formula_10 is drawn from a random component\nformula_12 with density formula_13 on formula_14. The move to state formula_15 can thus be formulated as\n\nThe function\n\nmust be \"one to one\" and differentiable, and have a non-zero support:\n\nso that there exists an inverse function \n\nthat is differentiable. Therefore, the formula_20 and formula_21 must be of equal dimension, which is the case if the dimension criterion \n\nis met where formula_23 is the dimension of formula_10. This is known as \"dimension matching\". \n\nIf formula_25 then the dimensional matching\ncondition can be reduced to \n\nwith\n\nThe acceptance probability will be given by\n\nwhere formula_29 denotes the absolute value and formula_30 is the joint posterior probability\n\nwhere formula_32 is the normalising constant.\nThere is an experimental RJ-MCMC tool available for the open source BUGs package.\n"}
{"id": "56669979", "url": "https://en.wikipedia.org/wiki?curid=56669979", "title": "Rudin's conjecture", "text": "Rudin's conjecture\n\nRudin's conjecture is a mathematical hypothesis (in additive combinatorics and elementary number theory) concerning an upper bound for the number of squares in finite arithmetic progressions. The conjecture, which has applications in the theory of trigonometric series, was first stated by Walter Rudin in his 1960 paper \"Trigonometric series with gaps\".\n\nFor positive integers formula_1 define the expression formula_2 to be the number of perfect squares in the arithmetic progression formula_3, for formula_4, and define formula_5 to be the maximum of the set . Rudin’s conjecture asserts (in big O notation) that formula_6 and in its stronger form that, if formula_7, formula_8.\n"}
{"id": "45550317", "url": "https://en.wikipedia.org/wiki?curid=45550317", "title": "STEM pipeline", "text": "STEM pipeline\n\nThe STEM pipeline is a term used to describe the educational pathway for students in the STEM fields, (science, technology, engineering, and mathematics). The start and end of this STEM pipeline are disputed, but it is often considered to begin in early education and extend into graduation or an adult career in STEM.\n\nThe \"pipeline\" metaphor is based on the idea that having sufficient graduates requires both having sufficient input of students at the beginning of their studies, and retaining these students through completion of their academic program. The STEM pipeline is a key component of workplace diversity and of workforce development that ensures sufficient qualified candidates are available to fill scientific and technical positions.\n\nThe STEM pipeline was promoted in the United States from the 1970s onwards, as “the push for STEM (science, technology, engineering, and mathematics) education appears to have grown from a concern for the low number of future professionals to fill STEM jobs and careers and economic and educational competitiveness.”\n\nToday, this metaphor is commonly used to describe retention problems in STEM fields, called “leaks” in the pipeline. For example, the White House reported in 2012 that 80% of minority groups and women who enroll in a STEM field switch to a non-STEM field or drop out during their undergraduate education. These leaks often vary by field, gender, ethnic and racial identity, socioeconomic background, and other factors, drawing attention to structural inequities involved in STEM education and careers.\n\nThe STEM pipeline concept is a useful tool for programs aiming at increasing the total number of graduates, and is especially important in efforts to increase the number of underrepresented minorities and women in STEM fields. Using STEM methodology, educational policymakers can examine the quantity and retention of students at all stages of the K–12 educational process and beyond, and devise programs and interventions to improve educational processes and outcomes. STEM programs focus on increasing social and academic supports for students. STEM programs may also focus on bringing students together with professionals in their field, to provide mentoring, role models and learning opportunities in industry.\n\nMaintaining a healthy and diverse STEM pipeline has been a concern in several developed countries, such as the United Kingdom, the United States, and Germany.\n\nIn the United States, although efforts to increase the number of women and African Americans in STEM fields have been ongoing, as recently as 2010 the results have been evaluated as \"poor\". In 2014, one report declared that \"traditionally underrepresented groups remain underrepresented\", while another article commented, \"You can go through your entire scholarly trajectory in computer science without seeing one face of color\", where \"of color\" refers to African Americans.\n\nSTEM pipeline programs in the US have been created at various levels. Examples include: the Technology Leadership Institute at the University of Pittsburgh at the college and university level, the Nevada STEM pipeline at the state level and the Broadening Participation in Computing Alliances at the national level.\n\nHigh Schools in the United States implement a STEM pipeline program that combines a dual pathway that enhances mathematical, engineering, and scientific skills along with a supportive group that aims to help underrepresented students aspire to become leaders in the STEM field. Students benefit from the moral support and motivational skills that mentors implement for their correct academic preparation. Teachers, and college mentors become in the life of the students in the program a guidance in their path to be the next generation of leaders. Furthermore, staff members aid students to feel integrated and cared for their well-being. In their path to pursue higher education in the STEM fields, underrepresented students are awarded scholarships that aid them throughout their college years. Scholarships are a factor that allows underrepresented to focus on their academic and allows them to be persistent throughout their years in college.\n\nThe STEM pipeline program provides a multitude of workshops, and extracurricular activities to work on social and professional development. Moreover, it arranges networking with minorities that went through the program and now currently work in the science, or health field. In addition, to the benefits as an alumnus of the program, every student is invited to become advocates within their community with the goal of increasing the amount of underrepresented students in STEM fields \n\nThe support of programs such as the STEM pipeline program aims to increase diversity in the workplace with the ambition to create an inclusive safe area where all the members of the team can contribute to the development of innovative ideas in their respective field. Additionally, the diversity of collaborating with different ideas enhances the outcome of the team's desired goal, and facilitates better planning of the timeline.\n\nThe concept of the STEM pipeline has been met with resistance for its pragmatic overtones. National Science Board Vice Chairman Kelvin Droegemeier calls for a movement away from thinking about the necessary number of STEM workers, in favor of considering the necessary knowledge and skills for the success of all workers.\n\nThe linearity of the STEM pipeline concept has been criticized as neglecting the wide variety of possible career pathways, including interdisciplinary studies, intermittent careers, and STEM-informed work in non-technical fields. A 2015 commentary in Inside Higher Ed suggested that the \"leaky pipeline\" metaphor may be viewed as pejorative towards individuals who leave the academic track for employment, or use their technical background as the basis for a career in a non-technical field.\n\nA 2015 commentary in \"Science\" observed that Margaret Thatcher and Angela Merkel could be considered two \"leaks\" in the pipeline.\n\nSome have said that increasing the STEM pipeline is not enough to promote workplace diversity. Women and minorities in STEM such as Tracy Chou have argued that STEM companies must also focus on internal reforms, such as reevaluating inequitable hiring practices and unsupportive workplace culture.\n\n"}
{"id": "914901", "url": "https://en.wikipedia.org/wiki?curid=914901", "title": "Sard's theorem", "text": "Sard's theorem\n\nSard's theorem, also known as Sard's lemma or the Morse–Sard theorem, is a result in mathematical analysis that asserts that the set of critical values (that is, the image of the set of critical points) of a smooth function \"f\" from one Euclidean space or manifold to another is a null set, i.e., it has Lebesgue measure 0. This makes the set of critical values \"small\" in the sense of a generic property. The theorem is named for Anthony Morse and Arthur Sard.\n\nMore explicitly (; ), let\n\nbe formula_2, (that is, formula_3 times continuously differentiable), where formula_4. Let formula_5 denote the \"critical set\" of formula_6 which is the set of points formula_7 at which the Jacobian matrix of formula_8 has rank formula_9. Then the image formula_10 has Lebesgue measure 0 in formula_11.\n\nIntuitively speaking, this means that although formula_5 may be large, its image must be small in the sense of Lebesgue measure: while formula_8 may have many critical \"points\" in the domain formula_14, it must have few critical \"values\" in the image formula_11.\n\nMore generally, the result also holds for mappings between second countable differentiable manifolds formula_16 and formula_17 of dimensions formula_18 and formula_19, respectively. The critical set formula_5 of a formula_2 function \nconsists of those points at which the differential \nhas rank less than formula_18 as a linear transformation. If formula_25, then Sard's theorem asserts that the image of formula_5 has measure zero as a subset of formula_16. This formulation of the result follows from the version for Euclidean spaces by taking a countable set of coordinate patches. The conclusion of the theorem is a local statement, since a countable union of sets of measure zero is a set of measure zero, and the property of a subset of a coordinate patch having zero measure is invariant under diffeomorphism.\n\nThere are many variants of this lemma, which plays a basic role in singularity theory among other fields. The case formula_28 was proven by Anthony P. Morse in 1939 , and the general case by Arthur Sard in 1942 .\n\nA version for infinite-dimensional Banach manifolds was proven by Stephen Smale .\n\nThe statement is quite powerful, and the proof involves analysis. In topology it is often quoted — as in the Brouwer fixed point theorem and some applications in Morse theory — in order to prove the weaker corollary that “a non-constant smooth map has at least one regular value”.\n\nIn 1965 Sard further generalized his theorem to state that if formula_22 is formula_2 for formula_4 and if formula_32 is the set of points formula_33 such that formula_34 has rank strictly less than formula_35, then the r-dimensional Hausdorff measure of formula_36 is zero. In particular the Hausdorff dimension of formula_36 is at most r. Caveat: The Hausdorff dimension of formula_36 can be arbitrarly close to r.\n\n"}
{"id": "32652788", "url": "https://en.wikipedia.org/wiki?curid=32652788", "title": "Semi-inner-product", "text": "Semi-inner-product\n\nIn mathematics, the semi-inner-product is a generalization of inner products formulated by Günter Lumer for the purpose of extending Hilbert space type arguments to Banach spaces in functional analysis. Fundamental properties were later explored by Giles.\n\nThe definition presented here is different from that of the \"semi-inner product\" in standard functional analysis textbooks, where a \"semi-inner product\" satisfies all the properties of inner products (including conjugate symmetry) except that it is not required to be strictly positive.\n\nA semi-inner-product for a linear vector space formula_1 over the field formula_2 of complex numbers is a function from formula_3 to formula_2, usually denoted by formula_5, such that\n\nA semi-inner-product is different from inner products in that it is in general not conjugate symmetric, i.e.,\n\ngenerally. This is equivalent to saying that \n\nIn other words, semi-inner-products are generally nonlinear about its second variable.\n\n\ndefines a norm on formula_1.\n\n\n\nhas the consistent semi-inner-product:\n\nwhere\n\n\npossesses the consistent semi-inner-product:\n\n"}
{"id": "2802848", "url": "https://en.wikipedia.org/wiki?curid=2802848", "title": "Temperley–Lieb algebra", "text": "Temperley–Lieb algebra\n\nIn statistical mechanics, the Temperley–Lieb algebra is an algebra from which are built certain transfer matrices, invented by Neville Temperley and Elliott Lieb. It is also related to integrable models, knot theory and the braid group, quantum groups and subfactors of von Neumann algebras.\n\nLet formula_1 be a commutative ring and fix formula_2. The Temperley–Lieb algebra formula_3 is the formula_1-algebra generated by the elements formula_5, subject to the Jones relations: \n\nformula_3 may be represented diagrammatically as the vector space over noncrossing pairings on a rectangle with \"n\" points on two opposite sides. The five basis elements of formula_16 are the following:\n\nMultiplication on basis elements can be performed by placing two rectangles side by side, and replacing any closed loops by a factor of formula_17, for example:\n\nThe identity element is the diagram in which each point is connected to the one directly across the rectangle from it, and the generator formula_19 is the diagram in which the \"i\"th point is connected to the \"i+1\"th point, the \"2n − i + 1\"th point is connected to the \"2n − i\"th point, and all other points are connected to the point directly across the rectangle. The generators of formula_20 are:\n\nFrom left to right, the unit 1 and the generators U, U, U, U.\n\nThe Jones relations can be seen graphically:\n\nConsider an interaction-round-a-face model e.g. a square lattice model and let formula_22 be the number of sites on the lattice. Following Temperley and Lieb we define the Temperley-Lieb hamiltonian (the TL hamiltonian) as\n\nformula_23\n\nwhere formula_24, for some spectral parameter formula_25.\n\nWe will firstly consider the case formula_26. The TL hamiltonian is formula_27, namely \n\nformula_28 = 2 - - .\n\nWe have two possible states,\n\nIn acting by formula_28 on these states, we find\n\nformula_28 = 2 - - = - ,\n\nand\n\nformula_28 = 2 - - = - + .\n\nWriting formula_28 as a matrix in the basis of possible states we have,\n\nformula_33\n\nThe eigenvector of formula_28 with the \"lowest\" eigenvalue is known as the ground state. In this case, the lowest eigenvalue formula_35 for formula_28 is formula_37. The corresponding eigenvector is formula_38. As we vary the number of sites formula_22 we find the following table\n\nwhere we have used the notation formula_40 formula_41-times e.g. formula_42.\n\nAn interesting observation is that the largest components of the ground state of formula_28 have a combinatorial enumeration as we vary the number of sites, as was first observed by Murray Batchelor, Jan de Gier and Bernard Nienhuis. Using the resources of the on-line encyclopedia of integer sequences, Batchelor \"et al.\" found, for an even numbers of sites \n\nformula_44\n\nand for an odd numbers of sites \n\nformula_45\n\nSurprisingly, these sequences corresponded to well known combinatorial objects. For formula_22 even, this corresponds to cyclically symmetric transpose complement plane partitions and for formula_22 odd, , these correspond to formula_48 alternating sign matrices symmetric about the vertical axis.\n\n"}
{"id": "48358", "url": "https://en.wikipedia.org/wiki?curid=48358", "title": "Transposition cipher", "text": "Transposition cipher\n\nIn cryptography, a transposition cipher is a method of encryption by which the positions held by units of plaintext (which are commonly characters or groups of characters) are shifted according to a regular system, so that the ciphertext constitutes a permutation of the plaintext. That is, the order of the units is changed (the plaintext is reordered). Mathematically a bijective function is used on the characters' positions to encrypt and an inverse function to decrypt.\n\nFollowing are some implementations.\n\nThe Rail Fence cipher is a form of transposition cipher that gets its name from the way in which it is encoded. In the rail fence cipher, the plaintext is written downwards on successive \"rails\" of an imaginary fence, then moving up when we get to the bottom. The message is then read off in rows. For example, using three \"rails\" and a message of 'WE ARE DISCOVERED. FLEE AT ONCE', the cipherer writes out:\n\nThen reads off:\n\nThe rail fence cipher was used by the ancient Greeks in the scytale, a mechanical system of producing a transposition cipher. The system consisted of a cylinder and a ribbon that was wrapped around the cylinder. The message to be encrypted was written on the coiled ribbon. The letters of the original message would be rearranged when the ribbon was uncoiled from the cylinder. However, the message was easily decrypted when the ribbon was recoiled on a cylinder of the same diameter as the encrypting cylinder.\n\nIn a route cipher, the plaintext is first written out in a grid of given dimensions, then read off in a pattern given in the key. For example, using the same plaintext that we used for rail fence:\nThe key might specify \"spiral inwards, clockwise, starting from the top right\". That would give a cipher text of:\n\nRoute ciphers have many more keys than a rail fence. In fact, for messages of reasonable length, the number of possible keys is potentially too great to be enumerated even by modern machinery. However, not all keys are equally good. Badly chosen routes will leave excessive chunks of plaintext, or text simply reversed, and this will give cryptanalysts a clue as to the routes.\n\nA variation of the route cipher was the Union Route Cipher, used by Union forces during the American Civil War. This worked much like an ordinary route cipher, but transposed whole words instead of individual letters. Because this would leave certain highly sensitive words exposed, such words would first be concealed by code. The cipher clerk may also add entire null words, which were often chosen to make the ciphertext humorous. \n\nIn a columnar transposition, the message is written out in rows of a fixed length, and then read out again column by column, and the columns are chosen in some scrambled order. Both the width of the rows and the permutation of the columns are usually defined by a keyword. For example, the keyword ZEBRAS is of length 6 (so the rows are of length 6), and the permutation is defined by the alphabetical order of the letters in the keyword. In this case, the order would be \"6 3 2 4 1 5\".\n\nIn a regular columnar transposition cipher, any spare spaces are filled with nulls; in an irregular columnar transposition cipher, the spaces are left blank. Finally, the message is read off in columns, in the order specified by the keyword. For example, suppose we use the keyword ZEBRAS and the message WE ARE DISCOVERED. FLEE AT ONCE. In a regular columnar transposition, we write this into the grid as follows:\nproviding five nulls (QKJEU), these letters can be randomly selected as they just fill out the incomplete columns and are not part of the message. The ciphertext is then read off as:\n\nIn the irregular case, the columns are not completed by nulls:\nThis results in the following ciphertext:\n\nTo decipher it, the recipient has to work out the column lengths by dividing the message length by the key length. Then he can write the message out in columns again, then re-order the columns by reforming the key word.\n\nIn a variation, the message is blocked into segments that are the key length long and to each segment the same permutation (given by the key) is applied. This is equivalent to a columnar transposition where the read-out is by rows instead of columns.\n\nColumnar transposition continued to be used for serious purposes as a component of more complex ciphers at least into the 1950s.\n\nA single columnar transposition could be attacked by guessing possible column lengths, writing the message out in its columns (but in the wrong order, as the key is not yet known), and then looking for possible anagrams. Thus to make it stronger, a double transposition was often used. This is simply a columnar transposition applied twice. The same key can be used for both transpositions, or two different keys can be used.\n\nAs an example, we can take the result of the irregular columnar transposition in the previous section, and perform a second encryption with a different keyword, STRIPE, which gives the permutation \"564231\":\n\nAs before, this is read off columnwise to give the ciphertext:\n\nIf multiple messages of exactly the same length are encrypted using the same keys, they can be anagrammed simultaneously. This can lead to both recovery of the messages, and to recovery of the keys (so that every other message sent with those keys can be read).\n\nDuring World War I, the German military used a double columnar transposition cipher, changing the keys infrequently. The system was regularly solved by the French, naming it Übchi, who were typically able to quickly find the keys once they'd intercepted a number of messages of the same length, which generally took only a few days. However, the French success became widely known and, after a publication in \"Le Matin\", the Germans changed to a new system on 18 November 1914.\n\nDuring World War II, the double transposition cipher was used by Dutch Resistance groups, the French Maquis and the British Special Operations Executive (SOE), which was in charge of managing underground activities in Europe. It was also used by agents of the American Office of Strategic Services and as an emergency cipher for the German Army and Navy.\n\nUntil the invention of the VIC cipher, double transposition was generally regarded as the most complicated cipher that an agent could operate reliably under difficult field conditions.\n\nThe double transposition cipher can be treated as a single transposition with a key as long as the product of the lengths of the two keys.\n\nIn late 2013, a double transposition challenge, regarded by its author as undecipherable, was solved by George Lasry using a divide-and-conquer approach where each transposition was attacked individually.\n\nA variant form of columnar transposition, proposed by Émile Victor Théodore Myszkowski in 1902, requires a keyword with recurrent letters. In usual practice, subsequent occurrences of a keyword letter are treated as if the next letter in alphabetical order, \"e.g.,\" the keyword TOMATO yields a numeric keystring of \"532164.\"\n\nIn Myszkowski transposition, recurrent keyword letters are numbered identically, TOMATO yielding a keystring of \"432143.\"\n\nPlaintext columns with unique numbers are transcribed downward;\nthose with recurring numbers are transcribed left to right:\n\nIn a disrupted transposition, certain positions in a grid are blanked out, and not used when filling in the plaintext. This breaks up regular patterns and makes the cryptanalyst's job more difficult.\n\nAnother form of transposition cipher uses \"grilles\", or physical masks with cut-outs. This can produce a highly irregular transposition over the period specified by the size of the grille, but requires the correspondents to keep a physical key secret. Grilles were first proposed in 1550, and were still in military use for the first few months of World War One.\n\nA cipher from ancient Greek times that was used to make encryptions. The device used to make these ciphers was a rod with a polygon base, which was wrapped in paper. People then, could write on the paper horizontally. When the paper was removed from the device, it would make a strip of letters that seemed randomized. The only way to read the message would be to have a Scytale machine of your own.\n\nSince transposition does not affect the frequency of individual symbols, simple transposition can be easily detected by the cryptanalyst by doing a frequency count. If the ciphertext exhibits a frequency distribution very similar to plaintext, it is most likely a transposition. This can then often be attacked by anagramming—sliding pieces of ciphertext around, then looking for sections that look like anagrams of English words, and solving the anagrams. Once such anagrams have been found, they reveal information about the transposition pattern, and can consequently be extended.\n\nSimpler transpositions also often suffer from the property that keys very close to the correct key will reveal long sections of legible plaintext interspersed by gibberish. Consequently, such ciphers may be vulnerable to optimum seeking algorithms such as genetic algorithms.\n\nA detailed description of the cryptanalysis of a German transposition cipher\ncan be found in chapter 7 of Herbert Yardley's \"The American Black Chamber.\"\n\nTransposition is often combined with other techniques such as evaluation methods. For example, a simple substitution cipher combined with a columnar transposition avoids the weakness of both. Replacing high frequency ciphertext symbols with high frequency plaintext letters does not reveal chunks of plaintext because of the transposition. Anagramming the transposition does not work because of the substitution. The technique is particularly powerful if combined with fractionation (see below). A disadvantage is that such ciphers are considerably more laborious and error prone than simpler ciphers.\n\nTransposition is particularly effective when employed with fractionation - that is, a preliminary stage that divides each plaintext symbol into several ciphertext symbols. For example, the plaintext alphabet could be written out in a grid, then every letter in the message replaced by its co-ordinates (see Polybius square and Straddling checkerboard). Another method of fractionation is to simply convert the message to Morse code, with a symbol for spaces as well as dots and dashes.\n\nWhen such a fractionated message is transposed, the components of individual letters become widely separated in the message, thus achieving Claude E. Shannon's diffusion. Examples of ciphers that combine fractionation and transposition include the bifid cipher, the trifid cipher, the ADFGVX cipher and the VIC cipher.\n\nAnother choice would be to replace each letter with its binary representation, transpose that, and then convert the new binary string into the corresponding ASCII characters. Looping the scrambling process on the binary string multiple times before changing it into ASCII characters would likely make it harder to break. Many modern block ciphers use more complex forms of transposition related to this simple idea.\n\n\n"}
{"id": "279693", "url": "https://en.wikipedia.org/wiki?curid=279693", "title": "Type signature", "text": "Type signature\n\nIn computer science, a type signature or type annotation defines the inputs and outputs for a function, subroutine or method. A type signature includes the number of arguments, the types of arguments and the order of the arguments contained by a function. A type signature is typically used during overload resolution for choosing the correct definition of a function to be called among many overloaded forms.\n\nIn C and C++, the type signature is declared by what is commonly known as a function prototype. In C/C++, a function declaration reflects its use; for example, a function pointer that would be invoked as:\nhas the signature:\n\nIn Erlang, type signatures may be optionally declared, as:\nFor example:\nA type signature in the Haskell programming language is generally written in the following format:\nNotice that the type of the result can be regarded as everything past the first supplied argument. This is a consequence of currying, which is made possible by Haskell's support for first-class functions; this function requires two inputs where one argument supplied and the function is \"curried\" to produce a function for the argument not supplied. Thus calling , where , yields a new function that can be called to produce .\n\nThe actual type specifications can consist of an actual type, such as , or a general type variable that is used in parametric polymorphic functions, such as , or , or . So we can write something like:\nSince Haskell supports higher-order functions, functions can be passed as arguments. This is written as:\nThis function takes in a function with type signature and returns data of type out.\n\nIn the Java virtual machine, \"internal type signatures\" are used to identify methods and classes at the level of the virtual machine code.\n\nExample: The method is represented in bytecode as . The signature of codice_1 method looks like this:\n\nAnd in the disassembled bytecode, it takes the form of \n\nThe method signature for the codice_1 method contains three modifiers:\n\n\nA function signature consists of the function prototype. It specifies the general information about a function like the name, scope and parameters. Many programming languages use name mangling in order to pass along more semantic information from the compilers to the linkers. In addition to mangling, there is an excess of information in a function signature (stored internally to most compilers) which is not readily available, but may be accessed.\n\nUnderstanding the notion of a function signature is an important concept for all computer science studies.\n\nThe practice of multiple inheritance requires consideration of the function signatures to avoid unpredictable results.\n\nComputer science theory, and the concept of polymorphism in particular, make much use of the concept of function signature.\n\nIn the C programming language signature is roughly equivalent to its prototype definition.\n\nThe term \"signature\" may carry other meanings in computer science. For example:\n\nIn computer programming, especially object-oriented programming, a method is commonly identified by its unique method signature, which usually includes the method name, and the number, types and order of its parameters. A method signature is the smallest type of a method.\n\nIn C/C++, the method signature is the method name and the number and type of its parameters, but it is possible to have a last parameter that consists of an array of values:\nManipulation of these parameters can be done by using the routines in the standard library header .\n\nSimilar to the C syntax, C# sees as the method signature its name and the number and type of its parameters, where the last parameter may be an array of values:\n\nIn the Java programming language, a method signature is the method name and the number, type and order of its parameters. Return types and thrown exceptions are not considered to be a part of the method signature. \nFor example, the following two methods have distinct signatures:\nThe following three methods do have the same signatures and are considered the same, as only the return value differs. The name of the parameter is not part of the method signature and is ignored by the compiler for checking method uniqueness.\nIn the Objective-C programming language, method signatures for an object are declared in the interface header file. For example,\ndefines a method that returns a general object (an ) and takes one integer argument. Objective-C only requires a type in a signature to be explicit when the type is not ; this signature is equivalent:\n"}
{"id": "52311684", "url": "https://en.wikipedia.org/wiki?curid=52311684", "title": "Weighted planar stochastic lattice", "text": "Weighted planar stochastic lattice\n\nPhysicists often uses various lattices to apply their favorite models in them. For instance, the most favorite lattice is perhaps the square lattice. There are 14 Bravais space lattice where every cell has exactly the same number of nearest, next nearest, nearest of next nearest etc neighbors and hence they are called regular lattice. Often physicists and mathematicians studies phenomena which requires disordered lattice where each cell do not have exactly the same number of neighbors rather the number of neighbors can vary wildly. For instance, if one want to study spread of disease, viruses, rumors etc then the last thing one would look for is the square lattice. In such cases a disordered lattice is necessary. One way of constructing a disordered lattice is by doing the following.\n\nStarting with a square, say of unit area, and dividing randomly at each step only one block, after\npicking it preferentially with respect to ares, into four smaller blocks creates weighted planar stochastic lattice (WPSL). Essentially it is a disordered planar lattice as its block size and their coordination number are random.\n\nIn applied mathematics, a weighted planar stochastic lattice (WPSL) is a structure that has properties in common with those of lattices and those of graphs. In general, space-filling planar cellular structures can be useful in a wide variety of seemingly disparate physical and biological systems.\nExamples include grain in polycrystalline structures, cell texture and tissues in biology, acicular texture in martensite growth, tessellated pavement on ocean shores, soap froths\nand agricultural land division according to ownership etc. The question of how these structures appear and the understanding of their topological and geometrical properties have always been an interesting proposition among scientists in general and physicists in particular.\nSeveral models prescribe how to generate cellular structures. Often these\nstructures can mimic directly the structures found in nature and they are able to capture the essential properties that we find in natural structures. \nIn general, cellular structures appear through random tessellation, tiling, or \nsubdivision of a plane into contiguous and non-overlapping cells. For instance, Voronoi diagram and Apollonian packing are \nformed by partitioning or tiling of a plane into contiguous and non-overlapping convex polygons and disks respectively.\n\nRegular planar lattices like square lattices, triangular lattices, honeycomb lattices, etc., are the simplest example of the cellular structure in which every cell has exactly the same size and the same coordination number. The planar Voronoi diagram, on the other hand, has neither a fixed cell size nor a fixed coordination number. Its coordination number distribution is rather Poissonian in nature. That is, the distribution is peaked about the mean where it is almost impossible to find cells which have significantly higher or fewer coordination number than the mean. Recently, Hassan \"et al\" proposed a lattice, namely the weighted planar stochastic lattice. For instance, unlike a network or a graph, it has properties of lattices as its sites are spatially embedded. On the other hand, unlike lattices, its dual (obtained by considering the center of each block of the lattice as node and the common border between blocks as links) display the property of networks as its degree distribution follows a power law. Besides, unlike regular lattices, the sizes of its cells are not equal; rather, the distribution of the area size of its blocks obeys dynamic scaling, whose coordination number distribution follows a power-law.\n\nThe construction process of the WPSL can be described as follows. It starts with a square of unit area which we regard as an initiator. The generator then divides the initiator, in the first step, randomly with uniform probability into four smaller blocks. In the second step and thereafter, the generator is applied to only one of the blocks. The question is: How do we pick that block when there is more than one block? The most generic choice would be to pick preferentially according to their areas so that the higher the area the higher the probability to be picked. For instance, in step one, the generator divides the initiator randomly into four smaller blocks. Let us label their areas starting from the top left corner and moving clockwise as formula_1 and formula_2. But of course the way we label is totally arbitrary and will bear no consequence to the final results of any observable quantities. Note that formula_3 is the area of the formula_4th block which can be well regarded as the probability of picking the formula_4th block. These probabilities are naturally normalized formula_6 since we choose the area of the initiator equal to one. In step two, we pick one of the four blocks preferentially with respect to their areas. Consider that we pick the block formula_7 and apply the generator onto it to divide it randomly into four smaller blocks. Thus the label formula_7 is now redundant and hence we recycle it to label the top left corner while the rest of three new blocks are labelled formula_9 and formula_10 in a clockwise fashion. In general, in the formula_11th step, we pick one out of formula_12 blocks preferentially with respect to area and divide randomly into four blocks. The detailed algorithm can be found in Dayeen and Hassan and Hassan, Hassan, and Pavel.\n\nThis process of lattice generation can also be described as follows. Consider that the substrate is a square of unit area and at each time step a seed is nucleated from which two orthogonal partitioning lines parallel to the sides of the substrate are grown until intercepted by existing lines. It results in partitioning the square into ever smaller mutually exclusive rectangular blocks. Note that the higher the area of a block, the higher is the probability that the seed will be nucleated in it to divide that into four smaller blocks since seeds are sown at random on the substrate. It can also describes kinetics of fragmentation of two-dimensional objects.\n\n\nBefore 2000 epidemic models, for instance, were studying by applying them on regular lattices like square lattice assuming that everyone can infect everyone else in the same way.\nThe emergence of a network-based framework has brought a fundamental change, offering a much much better pragmatic skeleton than any time before. Today epidemic models is one of the most active applications of network science, being used to foresee the spread of influenza or to contain Ebola. The WPSL can be a good candidate for applying epidemic like models since it has the properties \nof graph or network and the properties of traditional lattice as well.\n"}
{"id": "309392", "url": "https://en.wikipedia.org/wiki?curid=309392", "title": "Wigner's classification", "text": "Wigner's classification\n\nIn mathematics and theoretical physics, Wigner's classification\nis a classification of the nonnegative (\"E\" ≥ 0) energy irreducible unitary representations of the Poincaré group which have sharp mass eigenvalues. (Since this group is noncompact, these unitary representations are infinite-dimensional.)\nIt was introduced by Eugene Wigner, to classify particles and fields in physics—see the article particle physics and representation theory. It relies on the stabilizer subgroups of that group, dubbed the Wigner little groups of various mass states.\n\nThe Casimir invariants of the Poincaré group are , where is the 4-momentum operator, and , where is the Pauli–Lubanski pseudovector. The eigenvalues of these operators serve to label the representations. The first is associated with mass-squared and the second with helicity or spin. \n\nThe physically relevant representations may thus be classified according to whether ; but ; and with . Wigner found that massless particles are fundamentally different from massive particles.\n\n\nAs an example, let us visualize the irreducible unitary representation with and . It corresponds to the space of massive scalar fields.\n\nLet be the hyperboloid sheet defined by:\n\nThe Minkowski metric restricts to a Riemannian metric on , giving the metric structure of a hyperbolic space, in particular it is the hyperboloid model of hyperbolic space, see geometry of Minkowski space for proof. The Poincare group acts on because (forgetting the action of the translation subgroup with addition inside ) it preserves the Minkowski inner product, and an element of the translation subgroup of the Poincare group acts on by multiplication by suitable phase multipliers , where . These two actions can be combined in a clever way using induced representations to obtain an action \nof on that combines motions of and phase multiplication.\n\nThis yields an action of the Poincare group on the space of square-integrable functions defined on the hypersurface in Minkowski space. These may be viewed as measures defined on Minkowski space that are concentrated on the set defined by\n\nThe Fourier transform (in all four variables) of such measures yields positive-energy, finite-energy solutions of the Klein–Gordon equation defined on Minkowski space, namely\n\nwithout physical units. In this way, the irreducible representation of the Poincare group is realized by its action on a suitable space of solutions of a linear wave equation.\n\nPhysically, one is interested in irreducible \"projective\" unitary representations of the Poincaré group. After all, two vectors in the quantum Hilbert space that differ by multiplication by a constant represent the same physical state. Thus, two unitary operators that differ by a multiple of the identity have the same action on physical states. Therefore the unitary operators that represent Poincaré symmetry are only defined up to a constant—and therefore the group composition law need only hold up to a constant. \n\nAccording to , every projective unitary representation of the Poincaré group comes for an ordinary unitary representation of its universal cover, which is a double cover. (Bargmann's theorem applies because the double cover of the Poincaré group admits no non-trivial one-dimensional central extensions.) \n\nPassing to the double cover is important because it allows for fractional spin cases. In the positive mass case, for example, the little group is SU(2) rather than SO(3); the representations of SU(2) then include both integer and fractional spin cases.\n\nSince the general criterion in Bargmann's theorem was not known when Wigner did his classification, he needed to show by hand (Section 5 of the paper) that the phases can be chosen in the operators to reflect the composition law in the group, up to a sign, which is then accounted for by passing the to double cover of the Poincaré group.\n\nLeft out from this classification are tachyonic solutions, solutions with no fixed mass, infraparticles with no fixed mass, etc. Such solutions are of physical importance, when considering virtual states. A celebrated example is the case of Deep inelastic scattering, in which a virtual space-like photon is exchanged between the incoming lepton and the incoming hadron. This justifies the introduction of transversely and longitudinally-polarized photons, and of the related concept of transverse and longitudinal structure functions, when considering these virtual states as effective probes of the internal quark and gluon contents of the hadrons. From a mathematical point of view, one considers the SO(2,1) group instead of the usual SO(3) group encountered in the usual massive case discussed above. This explain the occurrence of two transverse polarization vectors formula_6 and formula_7\nwhich satisfy formula_8 and formula_9, to be compared with the usual case of a free formula_10 boson which has three polarization vectors formula_11, each of them satisfying formula_8.\n\n\n"}
