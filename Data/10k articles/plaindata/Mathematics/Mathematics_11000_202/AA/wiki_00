{"id": "2497875", "url": "https://en.wikipedia.org/wiki?curid=2497875", "title": "Alternating algebra", "text": "Alternating algebra\n\nIn mathematics, an alternating algebra is a -graded algebra for which for all nonzero homogeneous elements and (i.e. it is an anticommutative algebra) and has the further property that for every homogeneous element of odd degree.\n\n\n"}
{"id": "1264", "url": "https://en.wikipedia.org/wiki?curid=1264", "title": "Anisotropy", "text": "Anisotropy\n\nAnisotropy , is the property of being directionally dependent, which implies different properties in different directions, as opposed to isotropy. It can be defined as a difference, when measured along different axes, in a material's physical or mechanical properties (absorbance, refractive index, conductivity, tensile strength, etc.) \n\nAn example of anisotropy is light coming through a polarizer. Another is wood, which is easier to split along its grain than across it.\n\nIn the field of computer graphics, an anisotropic surface changes in appearance as it rotates about its geometric normal, as is the case with velvet.\n\nAnisotropic filtering (AF) is a method of enhancing the image quality of textures on surfaces that are far away and steeply angled with respect to the point of view. Older techniques, such as bilinear and trilinear filtering, do not take into account the angle a surface is viewed from, which can result in aliasing or blurring of textures. By reducing detail in one direction more than another, these effects can be reduced.\n\nA chemical anisotropic filter, as used to filter particles, is a filter with increasingly smaller interstitial spaces in the direction of filtration so that the proximal regions filter out larger particles and distal regions increasingly remove smaller particles, resulting in greater flow-through and more efficient filtration.\n\nIn NMR spectroscopy, the orientation of nuclei with respect to the applied magnetic field determines their chemical shift. In this context, anisotropic systems refer to the electron distribution of molecules with abnormally high electron density, like the pi system of benzene. This abnormal electron density affects the applied magnetic field and causes the observed chemical shift to change.\n\nIn fluorescence spectroscopy, the fluorescence anisotropy, calculated from the polarization properties of fluorescence from samples excited with plane-polarized light, is used, e.g., to determine the shape of a macromolecule.\nAnisotropy measurements reveal the average angular displacement of the fluorophore that occurs between absorption and subsequent emission of a photon.\n\nImages of a gravity-bound or man-made environment are particularly anisotropic in the orientation domain, with more image structure located at orientations parallel with or orthogonal to the direction of gravity (vertical and horizontal).\n\nPhysicists from University of California, Berkeley reported about their detection of the cosine anisotropy in cosmic microwave background radiation in 1977. Their experiment demonstrated the Doppler shift caused by the movement of the earth with respect to the early Universe matter, the source of the radiation. Cosmic anisotropy has also been seen in the alignment of galaxies' rotation axes and polarisation angles of quasars.\n\nPhysicists use the term anisotropy to describe direction-dependent properties of materials. Magnetic anisotropy, for example, may occur in a plasma, so that its magnetic field is oriented in a preferred direction. Plasmas may also show \"filamentation\" (such as that seen in lightning or a plasma globe) that is directional.\n\nAn \"anisotropic liquid\" has the fluidity of a normal liquid, but has an average structural order relative to each other along the molecular axis, unlike water or chloroform, which contain no structural ordering of the molecules. Liquid crystals are examples of anisotropic liquids.\n\nSome materials conduct heat in a way that is isotropic, that is independent of spatial orientation around the heat source. Heat conduction is more commonly anisotropic, which implies that detailed geometric modeling of typically diverse materials being thermally managed is required. The materials used to transfer and reject heat from the heat source in electronics are often anisotropic.\n\nMany crystals are anisotropic to light (\"optical anisotropy\"), and exhibit properties such as birefringence. Crystal optics describes light propagation in these media. An \"axis of anisotropy\" is defined as the axis along which isotropy is broken (or an axis of symmetry, such as normal to crystalline layers). Some materials can have multiple such optical axes.\n\nSeismic anisotropy is the variation of seismic wavespeed with direction. Seismic anisotropy is an indicator of long range order in a material, where features smaller than the seismic wavelength (e.g., crystals, cracks, pores, layers or inclusions) have a dominant alignment. This alignment leads to a directional variation of elasticity wavespeed. Measuring the effects of anisotropy in seismic data can provide important information about processes and mineralogy in the Earth; indeed, significant seismic anisotropy has been detected in the Earth's crust, mantle and inner core.\n\nGeological formations with distinct layers of sedimentary material can exhibit electrical anisotropy; electrical conductivity in one direction (e.g. parallel to a layer), is different from that in another (e.g. perpendicular to a layer). This property is used in the gas and oil exploration industry to identify hydrocarbon-bearing sands in sequences of sand and shale. Sand-bearing hydrocarbon assets have high resistivity (low conductivity), whereas shales have lower resistivity. Formation evaluation instruments measure this conductivity/resistivity and the results are used to help find oil and gas in wells.\n\nThe hydraulic conductivity of aquifers is often anisotropic for the same reason. When calculating groundwater flow to drains or to wells, the difference between horizontal and vertical permeability must be taken into account, otherwise the results may be subject to error.\n\nMost common rock-forming minerals are anisotropic, including quartz and feldspar. Anisotropy in minerals is most reliably seen in their optical properties. An example of an isotropic mineral is garnet.\n\nAnisotropy is also a well-known property in medical ultrasound imaging describing a different resulting echogenicity of soft tissues, such as tendons, when the angle of the transducer is changed. Tendon fibers appear hyperechoic (bright) when the transducer is perpendicular to the tendon, but can appear hypoechoic (darker) when the transducer is angled obliquely. This can be a source of interpretation error for inexperienced practitioners.\n\nAnisotropy, in Material Science, is a material's directional dependence of a physical property. Most materials exhibit anisotropic behavior. An example would be the dependence of Young's modulus on the direction of load.\nIn such a case anisotropy could be effectively measured directly from its stiffness tensor independently of its origin which may for instance be its texture, randomness of internal composition or defects.\n\nTexture patterns are often produced during manufacturing of the material. In the case of rolling, \"stringers\" of texture are produced in the direction of rolling, which can lead to vastly different properties in the rolling and transverse directions. \nSome materials, such as wood and fibre-reinforced composites are very anisotropic, being much stronger along the grain/fibre than across it. Metals and alloys tend to be more isotropic, though they can sometimes exhibit significant anisotropic behaviour. This is especially important in processes such as deep-drawing.\n\nWood is a naturally anisotropic (but often simplified to be transversely isotropic) material. Its properties vary widely when measured with or against the growth grain. For example, wood's strength and hardness is different for the same sample measured in different orientations.\n\nIn the Mechanics of Continuum Materials, isotropy and anisotropy are rigorously described through the symmetry group of the constitutive relation.\n\nAnisotropic etching techniques (such as deep reactive ion etching) are used in microfabrication processes to create well defined microscopic features with a high aspect ratio. These features are commonly used in MEMS and microfluidic devices, where the anisotropy of the features is needed to impart desired optical, electrical, or physical properties to the device. Anisotropic etching can also refer to certain chemical etchants used to etch a certain material preferentially over certain crystallographic planes (e.g., KOH etching of silicon [100] produces pyramid-like structures)\n\nDiffusion tensor imaging is an MRI technique that involves measuring the fractional anisotropy of the random motion (Brownian motion) of water molecules in the brain. Water molecules located in fiber tracts are more likely to be anisotropic, since they are restricted in their movement (they move more in the dimension parallel to the fiber tract rather than in the two dimensions orthogonal to it), whereas water molecules dispersed in the rest of the brain have less restricted movement and therefore display more isotropy. This difference in fractional anisotropy is exploited to create a map of the fiber tracts in the brains of the individual.\n\nRadiance fields (see BRDF) from a reflective surface are often not isotropic in nature. This makes calculations of the total energy being reflected from any scene a difficult quantity to calculate. In remote sensing applications, anisotropy functions can be derived for specific scenes, immensely simplifying the calculation of the net reflectance or (thereby) the net irradiance of a scene.\nFor example, let the BRDF be formula_1 where 'i' denotes incident direction and 'v' denotes viewing direction (as if from a satellite or other instrument). And let P be the Planar Albedo, which represents the total reflectance from the scene.\n\nIt is of interest because, with knowledge of the anisotropy function as defined, a measurement of the BRDF from a single viewing direction (say, formula_4) yields a measure of the total scene reflectance (Planar Albedo) for that specific incident geometry (say, formula_5).\n\n\n"}
{"id": "39400139", "url": "https://en.wikipedia.org/wiki?curid=39400139", "title": "Beck–Fiala theorem", "text": "Beck–Fiala theorem\n\nIn mathematics, the Beck–Fiala theorem is a major theorem in discrepancy theory due to József Beck and Tibor Fiala. Discrepancy is concerned with coloring elements of a ground set such that each set in a certain set system is as balanced as possible, i.e., has approximately the same number of elements of each color. The Beck–Fiala theorem is concerned with the case where each element doesn't appear many times across all sets. The theorem guarantees that if each element appears at most times, then the elements can be colored so that the imbalance is at most .\n\nFormally, given a universe\n\nand a collection of subsets\n\nsuch that for each formula_3,\n\nthen one can find an assignment\n\nsuch that\n\nThe proof is based on a simple linear-algebraic argument. Start with formula_7 for all elements and call all variables active in the beginning.\n\nConsider only sets with formula_8. Since each element appears at most formula_9 times in a set, there are less than formula_10 such sets. Now, enforce linear constraints formula_11 for them. Since it is a non-trivial linear subspace of formula_12 with fewer constraints than variables, there is a non-zero solution. Normalize this solution, and at least one of the values is either formula_13. Set this value and inactivate this variable. Now, ignore the sets with less than formula_9 active variables. And repeat the same procedure enforcing the linear constraints that the sum of active variables of each remaining set is still the same. By the same counting argument, there is a non-trivial solution, so one can take linear combinations of this with the original one until some element becomes formula_13. Repeat until all variables are set.\n\nOnce a set is ignored, the sum of the values of its variables is zero and there are at most formula_9 unset variables. The change in those can increase formula_17 to at most formula_18.\n\n"}
{"id": "25371909", "url": "https://en.wikipedia.org/wiki?curid=25371909", "title": "Ben Andrews (mathematician)", "text": "Ben Andrews (mathematician)\n\nBen Andrews is a Senior Fellow at the Centre for Mathematics and its Applications at the Australian National University.\nHis PhD Thesis, 1993, was on \"Evolving Convex Hypersurfaces\" at Australian National University. In 2003, he received the Australian Mathematical Society Medal, along with Andrew Hassell, for distinguished research in the mathematical sciences. In 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "6318542", "url": "https://en.wikipedia.org/wiki?curid=6318542", "title": "Boolean algebras canonically defined", "text": "Boolean algebras canonically defined\n\nBoolean algebra is a mathematically rich branch of abstract algebra. Just as group theory deals with groups, and linear algebra with vector spaces, Boolean algebras are models of the equational theory of the two values 0 and 1 (whose interpretation need not be numerical). Common to Boolean algebras, groups, and vector spaces is the notion of an algebraic structure, a set closed under zero or more operations satisfying certain equations.\n\nJust as there are basic examples of groups, such as the group Z of integers and the permutation group \"S\" of permutations of \"n\" objects, there are also basic examples of Boolean algebra such as the following.\nBoolean algebra thus permits applying the methods of abstract algebra to mathematical logic, digital logic, and the set-theoretic foundations of mathematics.\n\nUnlike groups of finite order, which exhibit complexity and diversity and whose first-order theory is decidable only in special cases, all finite Boolean algebras share the same theorems and have a decidable first-order theory. Instead the intricacies of Boolean algebra are divided between the structure of infinite algebras and the algorithmic complexity of their syntactic structure.\n\nBoolean algebra treats the equational theory of the maximal two-element finitary algebra, called the Boolean prototype, and the models of that theory, called Boolean algebras. These terms are defined as follows.\n\nAn algebra is a family of operations on a set, called the underlying set of the algebra. We take the underlying set of the Boolean prototype to be {0,1}.\n\nAn algebra is finitary when each of its operations takes only finitely many arguments. For the prototype each argument of an operation is either 0 or 1, as is the result of the operation. The maximal such algebra consists of all finitary operations on {0,1}.\n\nThe number of arguments taken by each operation is called the arity of the operation. An operation on {0,1} of arity \"n\", or \"n\"-ary operation, can be applied to any of 2 possible values for its \"n\" arguments. For each choice of arguments the operation may return 0 or 1, whence there are 2 \"n\"-ary operations.\n\nThe prototype therefore has two operations taking no arguments, called zeroary or nullary operations, namely zero and one. It has four unary operations, two of which are constant operations, another is the identity, and the most commonly used one, called \"negation\", returns the opposite of its argument: 1 if 0, 0 if 1. It has sixteen binary operations; again two of these are constant, another returns its first argument, yet another returns its second, one is called \"conjunction\" and returns 1 if both arguments are 1 and otherwise 0, another is called \"disjunction\" and returns 0 if both arguments are 0 and otherwise 1, and so on. The number of (\"n\"+1)-ary operations in the prototype is the square of the number of \"n\"-ary operations, so there are 16 = 256 ternary operations, 256 = 65,536 quaternary operations, and so on.\n\nA family is indexed by an index set. In the case of a family of operations forming an algebra, the indices are called operation symbols, constituting the language of that algebra. The operation indexed by each symbol is called the denotation or interpretation of that symbol. Each operation symbol specifies the arity of its interpretation, whence all possible interpretations of a symbol have the same arity. In general it is possible for an algebra to interpret distinct symbols with the same operation, but this is not the case for the prototype, whose symbols are in one-one correspondence with its operations. The prototype therefore has 2 \"n\"-ary operation symbols, called the Boolean operation symbols and forming the language of Boolean algebra. Only a few operations have conventional symbols, such as ¬ for negation, ∧ for conjunction, and ∨ for disjunction. It is convenient to consider the \"i\"-th \"n\"-ary symbol to be \"f\" as done below in the section on truth tables.\n\nAn equational theory in a given language consists of equations between terms built up from variables using symbols of that language. Typical equations in the language of Boolean algebra are \"x\"∧\"y\" = \"y\"∧\"x\", \"x\"∧\"x\" = \"x\", \"x\"∧¬\"x\" = \"y\"∧¬\"y\", and \"x\"∧\"y\" = \"x\".\n\nAn algebra satisfies an equation when the equation holds for all possible values of its variables in that algebra when the operation symbols are interpreted as specified by that algebra. The laws of Boolean algebra are the equations in the language of Boolean algebra satisfied by the prototype. The first three of the above examples are Boolean laws, but not the fourth since 1∧0 ≠ 1.\n\nThe equational theory of an algebra is the set of all equations satisfied by the algebra. The laws of Boolean algebra therefore constitute the equational theory of the Boolean prototype.\n\nA model of a theory is an algebra interpreting the operation symbols in the language of the theory and satisfying the equations of the theory.\n\nThat is, a Boolean algebra is a set and a family of operations thereon interpreting the Boolean operation symbols and satisfying the same laws as the Boolean prototype.\n\nIf we define a homologue of an algebra to be a model of the equational theory of that algebra, then a Boolean algebra can be defined as any homologue of the prototype.\n\nExample 1. The Boolean prototype is a Boolean algebra, since trivially it satisfies its own laws. It is thus the prototypical Boolean algebra. We did not call it that initially in order to avoid any appearance of circularity in the definition.\n\nThe operations need not be all explicitly stated. A \"basis\" is any set from which the remaining operations can be obtained by composition. A \"Boolean algebra\" may be defined from any of several different bases. Three bases for Boolean algebra are in common use, the lattice basis, the ring basis, and the Sheffer stroke or NAND basis. These bases impart respectively a logical, an arithmetical, and a parsimonious character to the subject.\n\nThe common elements of the lattice and ring bases are the constants 0 and 1, and an associative commutative binary operation, called meet \"x\"∧\"y\" in the lattice basis, and multiplication \"xy\" in the ring basis. The distinction is only terminological. The lattice basis has the further operations of join, \"x\"∨\"y\", and complement, ¬\"x\". The ring basis has instead the arithmetic operation \"x\"⊕\"y\" of addition (the symbol ⊕ is used in preference to + because the latter is sometimes given the Boolean reading of join).\n\nTo be a basis is to yield all other operations by composition, whence any two bases must be intertranslatable. The lattice basis translates \"x\"∨\"y\" to the ring basis as \"x\"⊕\"y\"⊕\"xy\", and ¬\"x\" as \"x\"⊕1. Conversely the ring basis translates \"x\"⊕\"y\" to the lattice basis as (\"x\"∨\"y\")∧¬(\"x\"∧\"y\").\n\nBoth of these bases allow Boolean algebras to be defined via a subset of the equational properties of the Boolean operations. For the lattice basis, it suffices to define a Boolean algebra as a distributive lattice satisfying \"x\"∧¬\"x\" = 0 and \"x\"∨¬\"x\" = 1, called a complemented distributive lattice. The ring basis turns a Boolean algebra into a Boolean ring, namely a ring satisfying \"x\" = \"x\".\n\nEmil Post gave a necessary and sufficient condition for a set of operations to be a basis for the nonzeroary Boolean operations. A \"nontrivial\" property is one shared by some but not all operations making up a basis. Post listed five nontrivial properties of operations, identifiable with the five Post's classes, each preserved by composition, and showed that a set of operations formed a basis if, for each property, the set contained an operation lacking that property. (The converse of Post's theorem, extending \"if\" to \"if and only if,\" is the easy observation that a property from among these five holding of every operation in a candidate basis will also hold of every operation formed by composition from that candidate, whence by nontriviality of that property the candidate will fail to be a basis.) Post's five properties are:\nThe NAND (dually NOR) operation lacks all these, thus forming a basis by itself.\n\nThe finitary operations on {0,1} may be exhibited as truth tables, thinking of 0 and 1 as the truth values false and true. They can be laid out in a uniform and application-independent way that allows us to name, or at least number, them individually. These names provide a convenient shorthand for the Boolean operations. The names of the \"n\"-ary operations are binary numbers of 2 bits. There being 2 such operations, one cannot ask for a more succinct nomenclature. Note that each finitary operation can be called a switching function.\n\nThis layout and associated naming of operations is illustrated here in full for arities from 0 to 2.\n\nThese tables continue at higher arities, with 2 rows at arity \"n\", each row giving a valuation or binding of the \"n\" variables \"x\"...\"x\" and each column headed \"f\" giving the value \"f\"(\"x\"...,\"x\") of the \"i\"-th \"n\"-ary operation at that valuation. The operations include the variables, for example \"f\" is \"x\" while \"f\" is \"x\" (as two copies of its unary counterpart) and \"f\" is \"x\" (with no unary counterpart). Negation or complement ¬\"x\" appears as \"f\" and again as \"f\", along with \"f\" (¬\"x\", which did not appear at arity 1), disjunction or union \"x\"∨\"x\" as \"f\", conjunction or intersection \"x\"∧\"x\" as \"f\", implication \"x\"→\"x\" as \"f\", exclusive-or symmetric difference \"x\"⊕\"x\" as \"f\", set difference \"x\"−\"x\" as \"f\", and so on.\n\nAs a minor detail important more for its form than its content, the operations of an algebra are traditionally organized as a list. Although we are here indexing the operations of a Boolean algebra by the finitary operations on {0,1}, the truth-table presentation above serendipitously orders the operations first by arity and second by the layout of the tables for each arity. This permits organizing the set of all Boolean operations in the traditional list format. The list order for the operations of a given arity is determined by the following two rules.\n\nWhen programming in C or Java, bitwise disjunction is denoted \"x\"|\"y\", conjunction \"x\"&\"y\", and negation ~\"x\". A program can therefore represent for example the operation \"x\"∧(\"y\"∨\"z\") in these languages as \"x\"&(\"y\"|\"z\"), having previously set \"x\" = 0xaa, \"y\" = 0xcc, and \"z\" = 0xf0 (the \"0x\" indicates that the following constant is to be read in hexadecimal or base 16), either by assignment to variables or defined as macros. These one-byte (eight-bit) constants correspond to the columns for the input variables in the extension of the above tables to three variables. This technique is almost universally used in raster graphics hardware to provide a flexible variety of ways of combining and masking images, the typical operations being ternary and acting simultaneously on source, destination, and mask bits.\n\nExample 2. All bit vectors of a given length form a Boolean algebra \"pointwise\", meaning that any \"n\"-ary Boolean operation can be applied to \"n\" bit vectors one bit position at a time. For example, the ternary OR of three bit vectors each of length 4 is the bit vector of length 4 formed by oring the three bits in each of the four bit positions, thus 0100∨1000∨1001 = 1101. Another example is the truth tables above for the \"n\"-ary operations, whose columns are all the bit vectors of length 2 and which therefore can be combined pointwise whence the \"n\"-ary operations form a Boolean algebra.\nThis works equally well for bit vectors of finite and infinite length, the only rule being that the bit positions all be indexed by the same set in order that \"corresponding position\" be well defined.\n\nThe atoms of such an algebra are the bit vectors containing exactly one 1. In general the atoms of a Boolean algebra are those elements \"x\" such that \"x\"∧\"y\" has only two possible values, \"x\" or 0.\n\nExample 3. The power set algebra, the set 2 of all subsets of a given set \"W\". This is just Example 2 in disguise, with \"W\" serving to index the bit positions. Any subset \"X\" of \"W\" can be viewed as the bit vector having 1's in just those bit positions indexed by elements of \"X\". Thus the all-zero vector is the empty subset of \"W\" while the all-ones vector is \"W\" itself, these being the constants 0 and 1 respectively of the power set algebra. The counterpart of disjunction \"x\"∨\"y\" is union \"X\"∪\"Y\", while that of conjunction \"x\"∧\"y\" is intersection \"X\"∩\"Y\". Negation ¬\"x\" becomes ~\"X\", complement relative to \"W\". There is also set difference \"X\"∖\"Y\" = \"X\"∩~\"Y\", symmetric difference (\"X\"∖\"Y\")∪(\"Y\"∖\"X\"), ternary union \"X\"∪\"Y\"∪\"Z\", and so on. The atoms here are the singletons, those subsets with exactly one element.\n\nExamples 2 and 3 are special cases of a general construct of algebra called direct product, applicable not just to Boolean algebras but all kinds of algebra including groups, rings, etc. The direct product of any family \"B\" of Boolean algebras where \"i\" ranges over some index set \"I\" (not necessarily finite or even countable) is a Boolean algebra consisting of all \"I\"-tuples (...\"x\"...) whose \"i\"-th element is taken from \"B\". The operations of a direct product are the corresponding operations of the constituent algebras acting within their respective coordinates; in particular operation \"f\" of the product operates on \"n\" \"I\"-tuples by applying operation \"f\" of \"B\" to the \"n\" elements in the \"i\"-th coordinate of the \"n\" tuples, for all \"i\" in \"I\".\n\nWhen all the algebras being multiplied together in this way are the same algebra \"A\" we call the direct product a \"direct power\" of \"A\". The Boolean algebra of all 32-bit bit vectors is the two-element Boolean algebra raised to the 32nd power, or power set algebra of a 32-element set, denoted 2. The Boolean algebra of all sets of integers is 2. All Boolean algebras we have exhibited thus far have been direct powers of the two-element Boolean algebra, justifying the name \"power set algebra\".\n\nIt can be shown that every finite Boolean algebra is isomorphic to some power set algebra. Hence the cardinality (number of elements) of a finite Boolean algebra is a power of 2, namely one of 1,2,4,8...,2... This is called a representation theorem as it gives insight into the nature of finite Boolean algebras by giving a representation of them as power set algebras.\n\nThis representation theorem does not extend to infinite Boolean algebras: although every power set algebra is a Boolean algebra, not every Boolean algebra need be isomorphic to a power set algebra. In particular, whereas there can be no countably infinite power set algebras (the smallest infinite power set algebra is the power set algebra 2 of sets of natural numbers, shown by Cantor to be uncountable), there exist various countably infinite Boolean algebras.\n\nTo go beyond power set algebras we need another construct. A subalgebra of an algebra \"A\" is any subset of \"A\" closed under the operations of \"A\". Every subalgebra of a Boolean algebra \"A\" must still satisfy the equations holding of \"A\", since any violation would constitute a violation for \"A\" itself. Hence every subalgebra of a Boolean algebra is a Boolean algebra.\n\nA subalgebra of a power set algebra is called a field of sets; equivalently a field of sets is a set of subsets of some set \"W\" including the empty set and \"W\" and closed under finite union and complement with respect to \"W\" (and hence also under finite intersection). Birkhoff's [1935] representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to a field of sets. Now Birkhoff's HSP theorem for varieties can be stated as, every class of models of the equational theory of a class \"C\" of algebras is the Homomorphic image of a Subalgebra of a direct Product of algebras of \"C\". Normally all three of H, S, and P are needed; what the first of these two Birkhoff theorems shows is that for the special case of the variety of Boolean algebras Homomorphism can be replaced by Isomorphism. Birkhoff's HSP theorem for varieties in general therefore becomes Birkhoff's ISP theorem for the variety of Boolean algebras.\n\nIt is convenient when talking about a set \"X\" of natural numbers to view it as a sequence \"x\",\"x\",\"x\"... of bits, with \"x\" = 1 if and only if \"i\" ∈ \"X\". This viewpoint will make it easier to talk about subalgebras of the power set algebra 2, which this viewpoint makes the Boolean algebra of all sequences of bits. It also fits well with the columns of a truth table: when a column is read from top to bottom it constitutes a sequence of bits, but at the same time it can be viewed as the set of those valuations (assignments to variables in the left half of the table) at which the function represented by that column evaluates to 1.\n\nExample 4. \"Ultimately constant sequences\". Any Boolean combination of ultimately constant sequences is ultimately constant; hence these form a Boolean algebra. We can identify these with the integers by viewing the ultimately-zero sequences as nonnegative binary numerals (bit 0 of the sequence being the low-order bit) and the ultimately-one sequences as negative binary numerals (think two's complement arithmetic with the all-ones sequence being −1). This makes the integers a Boolean algebra, with union being bit-wise OR and complement being \"−x−1\". There are only countably many integers, so this infinite Boolean algebra is countable. The atoms are the powers of two, namely 1,2,4... Another way of describing this algebra is as the set of all finite and cofinite sets of natural numbers, with the ultimately all-ones sequences corresponding to the cofinite sets, those sets omitting only finitely many natural numbers.\n\nExample 5. \"Periodic sequence\". A sequence is called \"periodic\" when there exists some number \"n\" > 0, called a witness to periodicity, such that \"x\" = \"x\" for all \"i\" ≥ 0. The period of a periodic sequence is its least witness. Negation leaves period unchanged, while the disjunction of two periodic sequences is periodic, with period at most the least common multiple of the periods of the two arguments (the period can be as small as 1, as happens with the union of any sequence and its complement). Hence the periodic sequences form a Boolean algebra.\n\nExample 5 resembles Example 4 in being countable, but differs in being atomless. The latter is because the conjunction of any nonzero periodic sequence \"x\" with a sequence of greater period is neither 0 nor \"x\". It can be shown that all countably infinite atomless Boolean algebras are isomorphic, that is, up to isomorphism there is only one such algebra.\n\nExample 6. \"Periodic sequence with period a power of two\". This is a proper subalgebra of Example 5 (a proper subalgebra equals the intersection of itself with its algebra). These can be understood as the finitary operations, with the first period of such a sequence giving the truth table of the operation it represents. For example, the truth table of \"x\" in the table of binary operations, namely \"f\", has period 2 (and so can be recognized as using only the first variable) even though 12 of the binary operations have period 4. When the period is 2 the operation only depends on the first \"n\" variables, the sense in which the operation is finitary. This example is also a countably infinite atomless Boolean algebra. Hence Example 5 is isomorphic to a proper subalgebra of itself! Example 6, and hence Example 5, constitutes the free Boolean algebra on countably many generators, meaning the Boolean algebra of all finitary operations on a countably infinite set of generators or variables.\n\nExample 7. \"Ultimately periodic sequences\", sequences that become periodic after an initial finite bout of lawlessness. They constitute a proper extension of Example 5 (meaning that Example 5 is a proper subalgebra of Example 7) and also of Example 4, since constant sequences are periodic with period one. Sequences may vary as to when they settle down, but any finite set of sequences will all eventually settle down no later than their slowest-to-settle member, whence ultimately periodic sequences are closed under all Boolean operations and so form a Boolean algebra. This example has the same atoms and coatoms as Example 4, whence it is not atomless and therefore not isomorphic to Example 5/6. However it contains an infinite atomless subalgebra, namely Example 5, and so is not isomorphic to Example 4, every subalgebra of which must be a Boolean algebra of finite sets and their complements and therefore atomic. This example is isomorphic to the direct product of Examples 4 and 5, furnishing another description of it.\n\nExample 8. The direct product of a Periodic Sequence (Example 5) with any finite but nontrivial Boolean algebra. (The trivial one-element Boolean algebra is the unique finite atomless Boolean algebra.) This resembles Example 7 in having both atoms and an atomless subalgebra, but differs in having only finitely many atoms. Example 8 is in fact an infinite family of examples, one for each possible finite number of atoms.\n\nThese examples by no means exhaust the possible Boolean algebras, even the countable ones. Indeed, there are uncountably many nonisomorphic countable Boolean algebras, which Jussi Ketonen [1978] classified completely in terms of invariants representable by certain hereditarily countable sets.\n\nThe \"n\"-ary Boolean operations themselves constitute a power set algebra 2, namely when \"W\" is taken to be the set of 2 valuations of the \"n\" inputs. In terms of the naming system of operations \"f\" where \"i\" in binary is a column of a truth table, the columns can be combined with Boolean operations of any arity to produce other columns present in the table. That is, we can apply any Boolean operation of arity \"m\" to \"m\" Boolean operations of arity \"n\" to yield a Boolean operation of arity \"n\", for any \"m\" and \"n\".\n\nThe practical significance of this convention for both software and hardware is that \"n\"-ary Boolean operations can be represented as words of the appropriate length. For example, each of the 256 ternary Boolean operations can be represented as an unsigned byte. The available logical operations such as AND and OR can then be used to form new operations. If we take \"x\", \"y\", and \"z\" (dispensing with subscripted variables for now) to be 10101010, 11001100, and 11110000 respectively (170, 204, and 240 in decimal, 0xaa, 0xcc, and 0xf0 in hexadecimal), their pairwise conjunctions are \"x\"∧\"y\" = 10001000, \"y\"∧\"z\" = 11000000, and \"z\"∧\"x\" = 10100000, while their pairwise disjunctions are \"x\"∨\"y\" = 11101110, \"y\"∨\"z\" = 11111100, and \"z\"∨\"x\" = 11111010. The disjunction of the three conjunctions is 11101000, which also happens to be the conjunction of three disjunctions. We have thus calculated, with a dozen or so logical operations on bytes, that the two ternary operations\n\nand\n\nare actually the same operation. That is, we have proved the equational identity\n\nfor the two-element Boolean algebra. By the definition of \"Boolean algebra\" this identity must therefore hold in every Boolean algebra.\n\nThis ternary operation incidentally formed the basis for Grau's [1947] ternary Boolean algebras, which he axiomatized in terms of this operation and negation. The operation is symmetric, meaning that its value is independent of any of the 3! = 6 permutations of its arguments. The two halves of its truth table 11101000 are the truth tables for ∨, 1110, and ∧, 1000, so the operation can be phrased as if \"z\" then \"x\"∨\"y\" else \"x\"∧\"y\". Since it is symmetric it can equally well be phrased as either of if \"x\" then \"y\"∨\"z\" else \"y\"∧\"z\", or if \"y\" then \"z\"∨\"x\" else \"z\"∧\"x\". Viewed as a labeling of the 8-vertex 3-cube, the upper half is labeled 1 and the lower half 0; for this reason it has been called the median operator, with the evident generalization to any odd number of variables (odd in order to avoid the tie when exactly half the variables are 0).\n\nThe technique we just used to prove an identity of Boolean algebra can be generalized to all identities in a systematic way that can be taken as a sound and complete axiomatization of, or axiomatic system for, the equational laws of Boolean logic. The customary formulation of an axiom system consists of a set of axioms that \"prime the pump\" with some initial identities, along with a set of inference rules for inferring the remaining identities from the axioms and previously proved identities. In principle it is desirable to have finitely many axioms; however as a practical matter it is not necessary since it is just as effective to have a finite axiom schema having infinitely many instances each of which when used in a proof can readily be verified to be a legal instance, the approach we follow here.\n\nBoolean identities are assertions of the form \"s\" = \"t\" where \"s\" and \"t\" are \"n\"-ary terms, by which we shall mean here terms whose variables are limited to \"x\" through \"x\". An \"n\"-ary term is either an atom or an application. An application \"f\"(\"t\"...,\"t\") is a pair consisting of an \"m\"-ary operation \"f\" and a list or \"m\"-tuple (\"t\"...,\"t\") of \"m\" \"n\"-ary terms called operands.\n\nAssociated with every term is a natural number called its height. Atoms are of zero height, while applications are of height one plus the height of their highest operand.\n\nNow what is an atom? Conventionally an atom is either a constant (0 or 1) or a variable \"x\" where 0 ≤ \"i\" < \"n\". For the proof technique here it is convenient to define atoms instead to be \"n\"-ary operations \"f\", which although treated here as atoms nevertheless mean the same as ordinary terms of the exact form \"f\"(\"x\"...,\"x\") (exact in that the variables must listed in the order shown without repetition or omission). This is not a restriction because atoms of this form include all the ordinary atoms, namely the constants 0 and 1, which arise here as the \"n\"-ary operations \"f\" and \"f\" for each \"n\" (abbreviating 2−1 to −1), and the variables \"x\"...,\"x\" as can be seen from the truth tables where \"x\" appears as both the unary operation \"f\" and the binary operation \"f\" while \"x\" appears as \"f\".\n\nThe following axiom schema and three inference rules axiomatize the Boolean algebra of \"n\"-ary terms.\n\nThe meaning of the side condition on A1 is that \"i\"\"ĵ\" is that 2-bit number whose \"v\"-th bit is the \"ĵ\"-th bit of \"i\", where the ranges of each quantity are \"u\": \"m\", \"v\": 2, \"j\": 2, and \"ĵ\": 2. (So \"j\" is an \"m\"-tuple of 2-bit numbers while \"ĵ\" as the transpose of \"j\" is a 2-tuple of \"m\"-bit numbers. Both \"j\" and \"ĵ\" therefore contain \"m\"2 bits.)\n\nA1 is an axiom schema rather than an axiom by virtue of containing metavariables, namely \"m\", \"i\", \"n\", and \"j\" through \"j\". The actual axioms of the axiomatization are obtained by setting the metavariables to specific values. For example, if we take \"m\" = \"n\" = \"i\" = \"j\" = 1, we can compute the two bits of \"i\"\"ĵ\" from \"i\" = 0 and \"i\" = 1, so \"i\"\"ĵ\" = 2 (or 10 when written as a two-bit number). The resulting instance, namely \"f\"(\"f\") = \"f\", expresses the familiar axiom ¬¬\"x\" = \"x\" of double negation. Rule R3 then allows us to infer ¬¬¬\"x\" = ¬\"x\" by taking \"s\" to be \"f\"(\"f\") or ¬¬\"x\", \"t\" to be \"f\" or \"x\", and \"f\" to be \"f\" or ¬.\n\nFor each \"m\" and \"n\" there are only finitely many axioms instantiating A1, namely 2 × (2). Each instance is specified by 2+\"m\"2 bits.\n\nWe treat R1 as an inference rule, even though it is like an axiom in having no premises, because it is a domain-independent rule along with R2 and R3 common to all equational axiomatizations, whether of groups, rings, or any other variety. The only entity specific to Boolean algebras is axiom schema A1. In this way when talking about different equational theories we can push the rules to one side as being independent of the particular theories, and confine attention to the axioms as the only part of the axiom system characterizing the particular equational theory at hand.\n\nThis axiomatization is complete, meaning that every Boolean law \"s\" = \"t\" is provable in this system. One first shows by induction on the height of \"s\" that every Boolean law for which \"t\" is atomic is provable, using R1 for the base case (since distinct atoms are never equal) and A1 and R3 for the induction step (\"s\" an application). This proof strategy amounts to a recursive procedure for evaluating \"s\" to yield an atom. Then to prove \"s\" = \"t\" in the general case when \"t\" may be an application, use the fact that if \"s\" = \"t\" is an identity then \"s\" and \"t\" must evaluate to the same atom, call it \"u\". So first prove \"s\" = \"u\" and \"t\" = \"u\" as above, that is, evaluate \"s\" and \"t\" using A1, R1, and R3, and then invoke R2 to infer \"s\" = \"t\".\n\nIn A1, if we view the number \"n\" as the function type \"m\"→\"n\", and \"m\" as the application \"m\"(\"n\"), we can reinterpret the numbers \"i\", \"j\", \"ĵ\", and \"i\"\"ĵ\" as functions of type \"i\": (\"m\"→2)→2, \"j\": \"m\"→((\"n\"→2)→2), \"ĵ\": (\"n\"→2)→(\"m\"→2), and \"i\"\"ĵ\": (\"n\"→2)→2. The definition (\"i\"\"ĵ\") = \"i\" in A1 then translates to (\"i\"\"ĵ\")(\"v\") = \"i\"(\"ĵ\"(\"v\")), that is, \"i\"\"ĵ\" is defined to be composition of \"i\" and \"ĵ\" understood as functions. So the content of A1 amounts to defining term application to be essentially composition, modulo the need to transpose the \"m\"-tuple \"j\" to make the types match up suitably for composition. This composition is the one in Lawvere's previously mentioned category of power sets and their functions. In this way we have translated the commuting diagrams of that category, as the equational theory of Boolean algebras, into the equational consequences of A1 as the logical representation of that particular composition law.\n\nUnderlying every Boolean algebra \"B\" is a partially ordered set or poset (\"B\",≤). The partial order relation is defined by \"x\" ≤ \"y\" just when \"x\" = \"x\"∧\"y\", or equivalently when \"y\" = \"x\"∨\"y\". Given a set \"X\" of elements of a Boolean algebra, an upper bound on \"X\" is an element \"y\" such that for every element \"x\" of \"X\", \"x\" ≤ \"y\", while a lower bound on \"X\" is an element \"y\" such that for every element \"x\" of \"X\", \"y\" ≤ \"x\".\n\nA sup (supremum) of \"X\" is a least upper bound on \"X\", namely an upper bound on \"X\" that is less or equal to every upper bound on \"X\". Dually an inf (infimum) of \"X\" is a greatest lower bound on \"X\". The sup of \"x\" and \"y\" always exists in the underlying poset of a Boolean algebra, being \"x\"∨\"y\", and likewise their inf exists, namely \"x\"∧\"y\". The empty sup is 0 (the bottom element) and the empty inf is 1 (top). It follows that every finite set has both a sup and an inf. Infinite subsets of a Boolean algebra may or may not have a sup and/or an inf; in a power set algebra they always do.\n\nAny poset (\"B\",≤) such that every pair \"x\",\"y\" of elements has both a sup and an inf is called a lattice. We write \"x\"∨\"y\" for the sup and \"x\"∧\"y\" for the inf. The underlying poset of a Boolean algebra always forms a lattice. The lattice is said to be distributive when \"x\"∧(\"y\"∨\"z\") = (\"x\"∧\"y\")∨(\"x\"∧\"z\"), or equivalently when \"x\"∨(\"y\"∧\"z\") = (\"x\"∨\"y\")∧(\"x\"∨\"z\"), since either law implies the other in a lattice. These are laws of Boolean algebra whence the underlying poset of a Boolean algebra forms a distributive lattice.\n\nGiven a lattice with a bottom element 0 and a top element 1, a pair \"x\",\"y\" of elements is called complementary when \"x\"∧\"y\" = 0 and \"x\"∨\"y\" = 1, and we then say that \"y\" is a complement of \"x\" and vice versa. Any element \"x\" of a distributive lattice with top and bottom can have at most one complement. When every element of a lattice has a complement the lattice is called complemented. It follows that in a complemented distributive lattice, the complement of an element always exists and is unique, making complement a unary operation. Furthermore, every complemented distributive lattice forms a Boolean algebra, and conversely every Boolean algebra forms a complemented distributive lattice. This provides an alternative definition of a Boolean algebra, namely as any complemented distributive lattice. Each of these three properties can be axiomatized with finitely many equations, whence these equations taken together constitute a finite axiomatization of the equational theory of Boolean algebras.\n\nIn a class of algebras defined as all the models of a set of equations, it is usually the case that some algebras of the class satisfy more equations than just those needed to qualify them for the class. The class of Boolean algebras is unusual in that, with a single exception, every Boolean algebra satisfies exactly the Boolean identities and no more. The exception is the one-element Boolean algebra, which necessarily satisfies every equation, even \"x\" = \"y\", and is therefore sometimes referred to as the inconsistent Boolean algebra.\n\nA Boolean homomorphism is a function \"h\": \"A\"→\"B\" between Boolean algebras \"A\", \"B\" such that for every Boolean operation \"f\",\n\nThe category Bool of Boolean algebras has as objects all Boolean algebras and as morphisms the Boolean homomorphisms between them.\n\nThere exists a unique homomorphism from the two-element Boolean algebra 2 to every Boolean algebra, since homomorphisms must preserve the two constants and those are the only elements of 2. A Boolean algebra with this property is called an initial Boolean algebra. It can be shown that any two initial Boolean algebras are isomorphic, so up to isomorphism 2 is \"the\" initial Boolean algebra.\n\nIn the other direction, there may exist many homomorphisms from a Boolean algebra \"B\" to 2. Any such homomorphism partitions \"B\" into those elements mapped to 1 and those to 0. The subset of \"B\" consisting of the former is called an ultrafilter of \"B\". When \"B\" is finite its ultrafilters pair up with its atoms; one atom is mapped to 1 and the rest to 0. Each ultrafilter of \"B\" thus consists of an atom of \"B\" and all the elements above it; hence exactly half the elements of \"B\" are in the ultrafilter, and there as many ultrafilters as atoms.\n\nFor infinite Boolean algebras the notion of ultrafilter becomes considerably more delicate. The elements greater or equal than an atom always form an ultrafilter but so do many other sets; for example in the Boolean algebra of finite and cofinite sets of integers the cofinite sets form an ultrafilter even though none of them are atoms. Likewise the powerset of the integers has among its ultrafilters the set of all subsets containing a given integer; there are countably many of these \"standard\" ultrafilters, which may be identified with the integers themselves, but there are uncountably many more \"nonstandard\" ultrafilters. These form the basis for nonstandard analysis, providing representations for such classically inconsistent objects as infinitesimals and delta functions.\n\nRecall the definition of sup and inf from the section above on the underlying partial order of a Boolean algebra. A complete Boolean algebra is one every subset of which has both a sup and an inf, even the infinite subsets. Gaifman [1964] and Hales [1964] independently showed that infinite free complete Boolean algebras do not exist. This suggests that a logic with set-sized-infinitary operations may have class-many terms—just as a logic with finitary operations may have infinitely many terms.\n\nThere is however another approach to introducing infinitary Boolean operations: simply drop \"finitary\" from the definition of Boolean algebra. A model of the equational theory of the algebra of \"all\" operations on {0,1} of arity up to the cardinality of the model is called a complete atomic Boolean algebra, or \"CABA\". (In place of this awkward restriction on arity we could allow any arity, leading to a different awkwardness, that the signature would then be larger than any set, that is, a proper class. One benefit of the latter approach is that it simplifies the definition of homomorphism between CABAs of different cardinality.) Such an algebra can be defined equivalently as a complete Boolean algebra that is atomic, meaning that every element is a sup of some set of atoms. Free CABAs exist for all cardinalities of a set \"V\" of generators, namely the power set algebra 2, this being the obvious generalization of the finite free Boolean algebras. This neatly rescues infinitary Boolean logic from the fate the Gaifman–Hales result seemed to consign it to.\n\nThe nonexistence of free complete Boolean algebras can be traced to failure to extend the equations of Boolean logic suitably to all laws that should hold for infinitary conjunction and disjunction, in particular the neglect of distributivity in the definition of complete Boolean algebra. A complete Boolean algebra is called completely distributive when arbitrary conjunctions distribute over arbitrary disjunctions and vice versa. A Boolean algebra is a CABA if and only if it is complete and completely distributive, giving a third definition of CABA. A fourth definition is as any Boolean algebra isomorphic to a power set algebra.\n\nA complete homomorphism is one that preserves all sups that exist, not just the finite sups, and likewise for infs. The category CABA of all CABAs and their complete homomorphisms is dual to the category of sets and their functions, meaning that it is equivalent to the opposite of that category (the category resulting from reversing all morphisms). Things are not so simple for the category Bool of Boolean algebras and their homomorphisms, which Marshall Stone showed in effect (though he lacked both the language and the conceptual framework to make the duality explicit) to be dual to the category of totally disconnected compact Hausdorff spaces, subsequently called Stone spaces.\n\nAnother infinitary class intermediate between Boolean algebras and complete Boolean algebras is the notion of a sigma-algebra. This is defined analogously to complete Boolean algebras, but with sups and infs limited to countable arity. That is, a sigma-algebra is a Boolean algebra with all countable sups and infs. Because the sups and infs are of bounded cardinality, unlike the situation with complete Boolean algebras, the Gaifman-Hales result does not apply and free sigma-algebras do exist. Unlike the situation with CABAs however, the free countably generated sigma algebra is not a power set algebra.\n\nWe have already encountered several definitions of Boolean algebra, as a model of the equational theory of the two-element algebra, as a complemented distributive lattice, as a Boolean ring, and as a product-preserving functor from a certain category (Lawvere). Two more definitions worth mentioning are:.\n\n\n\nTo put this in perspective, infinite sets arise as filtered colimits of finite sets, infinite CABAs as filtered limits of finite power set algebras, and infinite Stone spaces as filtered limits of finite sets. Thus if one starts with the finite sets and asks how these generalize to infinite objects, there are two ways: \"adding\" them gives ordinary or inductive sets while \"multiplying\" them gives Stone spaces or profinite sets. The same choice exists for finite power set algebras as the duals of finite sets: addition yields Boolean algebras as inductive objects while multiplication yields CABAs or power set algebras as profinite objects.\n\nA characteristic distinguishing feature is that the underlying topology of objects so constructed, when defined so as to be Hausdorff, is discrete for inductive objects and compact for profinite objects. The topology of finite Hausdorff spaces is always both discrete and compact, whereas for infinite spaces \"discrete\"' and \"compact\" are mutually exclusive. Thus when generalizing finite algebras (of any kind, not just Boolean) to infinite ones, \"discrete\" and \"compact\" part company, and one must choose which one to retain. The general rule, for both finite and infinite algebras, is that finitary algebras are discrete, whereas their duals are compact and feature infinitary operations. Between these two extremes, there are many intermediate infinite Boolean algebras whose topology is neither discrete nor compact.\n\n\n\n<!-- The following alternatives do not work; still searching for good template\n"}
{"id": "24392847", "url": "https://en.wikipedia.org/wiki?curid=24392847", "title": "Codes for electromagnetic scattering by cylinders", "text": "Codes for electromagnetic scattering by cylinders\n\nCodes for electromagnetic scattering by cylinders – this article list codes for electromagnetic scattering by a cylinder.\n\nMajority of existing codes for calculation of electromagnetic scattering by a single cylinder are based on Mie theory, which is an analytical solution of Maxwell's equations in terms of infinite series.\n\nThe compilation contains information about the electromagnetic scattering by cylindrical particles, relevant links, and applications.\n\n\n\n"}
{"id": "82289", "url": "https://en.wikipedia.org/wiki?curid=82289", "title": "Composite number", "text": "Composite number\n\nA composite number is a positive integer that can be formed by multiplying two smaller positive integers. Equivalently, it is a positive integer that has at least one divisor other than 1 and itself. Every positive integer is composite, prime, or the unit 1, so the composite numbers are exactly the numbers that are not prime and not a unit.\n\nFor example, the integer 14 is a composite number because it is the product of the two smaller integers 2 × 7. Likewise, the integers 2 and 3 are not composite numbers because each of them can only be divided by one and itself.\n\nThe composite numbers up to 150 are\n\nEvery composite number can be written as the product of two or more (not necessarily distinct) primes. For example, the composite number 299 can be written as 13 × 23, and the composite number 360 can be written as 2 × 3 × 5; furthermore, this representation is unique up to the order of the factors. This fact is called the fundamental theorem of arithmetic.\n\nThere are several known primality tests that can determine whether a number is prime or composite, without necessarily revealing the factorization of a composite input.\n\nOne way to classify composite numbers is by counting the number of prime factors. A composite number with two prime factors is a semiprime or 2-almost prime (the factors need not be distinct, hence squares of primes are included). A composite number with three distinct prime factors is a sphenic number. In some applications, it is necessary to differentiate between composite numbers with an odd number of distinct prime factors and those with an even number of distinct prime factors. For the latter \n\n(where μ is the Möbius function and \"x\" is half the total of prime factors), while for the former\n\nHowever, for prime numbers, the function also returns −1 and formula_3. For a number \"n\" with one or more repeated prime factors,\n\nIf \"all\" the prime factors of a number are repeated it is called a powerful number (All perfect powers are powerful numbers). If \"none\" of its prime factors are repeated, it is called squarefree. (All prime numbers and 1 are squarefree.)\n\nFor example, 72 = 2 × 3, all the prime factors are repeated, so 72 is a powerful number. 42 = 2 × 3 × 7, none of the prime factors are repeated, so 42 is squarefree.\n\nAnother way to classify composite numbers is by counting the number of divisors. All composite numbers have at least three divisors. In the case of squares of primes, those divisors are formula_5. A number \"n\" that has more divisors than any \"x\" < \"n\" is a highly composite number (though the first two such numbers are 1 and 2).\n\nComposite numbers have also been called \"rectangular numbers\", but that name can also refer to the pronic numbers, numbers that are the product of two consecutive integers.\n\nYet another way to classify composite numbers is to determine whether all prime factors are either all below or all above some fixed (prime) number. Such numbers are called smooth numbers and rough numbers, respectively.\n\n\n\n"}
{"id": "24628685", "url": "https://en.wikipedia.org/wiki?curid=24628685", "title": "Connection (algebraic framework)", "text": "Connection (algebraic framework)\n\nGeometry of quantum systems (e.g.,\nnoncommutative geometry and supergeometry) is mainly\nphrased in algebraic terms of modules and\nalgebras. Connections on modules are\ngeneralization of a linear connection on a smooth vector bundle formula_1 written as a Koszul connection on the\nformula_2-module of sections of formula_1.\n\nLet formula_4 be a commutative ring\nand formula_5 an \"A\"-module. There are different equivalent definitions\nof a connection on formula_5. Let formula_7 be the module of derivations of a ring formula_4. A\nconnection on an \"A\"-module formula_5 is defined\nas an \"A\"-module morphism\n\nsuch that the first order differential operators formula_11 on\nformula_5 obey the Leibniz rule\n\nConnections on a module over a commutative ring always exist.\n\nThe curvature of the connection formula_14 is defined as\nthe zero-order differential operator\n\non the module formula_5 for all formula_17.\n\nIf formula_18 is a vector bundle, there is one-to-one\ncorrespondence between linear\nconnections formula_19 on formula_18 and the\nconnections formula_14 on the\nformula_2-module of sections of formula_1. Strictly speaking, formula_14 corresponds to\nthe covariant differential of a\nconnection on formula_18.\n\nThe notion of a connection on modules over commutative rings is\nstraightforwardly extended to modules over a graded\ncommutative algebra. This is the case of\nsuperconnections in supergeometry of\ngraded manifolds and supervector bundles.\nSuperconnections always exist.\n\nIf formula_4 is a noncommutative ring, connections on left\nand right \"A\"-modules are defined similarly to those on\nmodules over commutative rings. However\nthese connections need not exist.\n\nIn contrast with connections on left and right modules, there is a\nproblem how to define a connection on an\n\"R\"-\"S\"-bimodule over noncommutative rings\n\"R\" and \"S\". There are different definitions\nof such a connection. Let us mention one of them. A connection on an\n\"R\"-\"S\"-bimodule formula_5 is defined as a bimodule\nmorphism\n\nwhich obeys the Leibniz rule\n\n\n\n"}
{"id": "6292", "url": "https://en.wikipedia.org/wiki?curid=6292", "title": "Convex set", "text": "Convex set\n\nIn convex geometry, a convex set is a subset of an affine space that is closed under convex combinations. More specifically, in a Euclidean space, a convex region is a region where, for every pair of points within the region, every point on the straight line segment that joins the pair of points is also within the region. For example, a solid cube is a convex set, but anything that is hollow or has an indent, for example, a crescent shape, is not convex.\n\nThe boundary of a convex set is always a convex curve. The intersection of all convex sets containing a given subset of Euclidean space is called the convex hull of . It is the smallest convex set containing .\n\nA convex function is a real-valued function defined on an interval with the property that its epigraph (the set of points on or above the graph of the function) is a convex set. Convex minimization is a subfield of optimization that studies the problem of minimizing convex functions over convex sets. The branch of mathematics devoted to the study of properties of convex sets and convex functions is called convex analysis.\n\nThe notion of a convex set can be generalized as described below.\n\nLet be a vector space over the real numbers, or, more generally, over some ordered field. This includes Euclidean spaces. A set in is said to be convex if, for all and in and all in the interval , the point also belongs to . In other words, every point on the line segment connecting and is in . This implies that a convex set in a real or complex topological vector space is path-connected, thus connected.\nFurthermore, is strictly convex if every point on the line segment connecting and other than the endpoints is inside the interior of .\n\nA set is called absolutely convex if it is convex and balanced.\n\nThe convex subsets of (the set of real numbers) are the intervals of . Some examples of convex subsets of the Euclidean plane are solid regular polygons, solid triangles, and intersections of solid triangles. Some examples of convex subsets of a Euclidean 3-dimensional space are the Archimedean solids and the Platonic solids. The Kepler-Poinsot polyhedra are examples of non-convex sets.\n\nA set that is not convex is called a \"non-convex set\". A polygon that is not a convex polygon is sometimes called a concave polygon, and some sources more generally use the term \"concave set\" to mean a non-convex set, but most authorities prohibit this usage.\n\nThe complement of a convex set, such as the epigraph of a concave function, is sometimes called a \"reverse convex set\", especially in the context of mathematical optimization.\n\nIf is a convex set in -dimensional space, then for any collection of , , -dimensional vectors in , and for any nonnegative numbers such that , then one has:\nA vector of this type is known as a convex combination of .\n\nThe collection of convex subsets of a vector space has the following properties:\n\nClosed convex sets are convex sets that contain all their limit points. They can be characterised as the intersections of \"closed half-spaces\" (sets of point in space that lie on and to one side of a hyperplane).\n\nFrom what has just been said, it is clear that such intersections are convex, and they will also be closed sets. To prove the converse, i.e., every convex set may be represented as such intersection, one needs the supporting hyperplane theorem in the form that for a given closed convex set and point outside it, there is a closed half-space that contains and not . The supporting hyperplane theorem is a special case of the Hahn–Banach theorem of functional analysis.\n\nLet \"C\" be a convex body in the plane. We can inscribe a rectangle \"r\" in \"C\" such that a homothetic copy \"R\" of \"r\" is circumscribed about \"C\". The positive homothety ratio is at most 2 and:\n\nEvery subset of the vector space is contained within a smallest convex set (called the convex hull of ), namely the intersection of all convex sets containing . The convex-hull operator Conv() has the characteristic properties of a hull operator:\nThe convex-hull operation is needed for the set of convex sets to form a lattice, in which the \"join\" operation is the convex hull of the union of two convex sets\nThe intersection of any collection of convex sets is itself convex, so the convex subsets of a (real or complex) vector space form a complete lattice.\n\nIn a real vector-space, the \"Minkowski sum\" of two (non-empty) sets, and , is defined to be the set formed by the addition of vectors element-wise from the summand-sets\nMore generally, the \"Minkowski sum\" of a finite family of (non-empty) sets is the set formed by element-wise addition of vectors\n\nFor Minkowski addition, the \"zero set\"  containing only the zero vector  has special importance: For every non-empty subset S of a vector space\nin algebraic terminology, is the identity element of Minkowski addition (on the collection of non-empty sets).\n\nMinkowski addition behaves well with respect to the operation of taking convex hulls, as shown by the following proposition:\n\nLet be subsets of a real vector-space, the convex hull of their Minkowski sum is the Minkowski sum of their convex hulls\n\nThis result holds more generally for each finite collection of non-empty sets:\n\nIn mathematical terminology, the operations of Minkowski summation and of forming convex hulls are commuting operations.\n\nThe Minkowski sum of two compact convex sets is compact. The sum of a compact convex set and a closed convex set is closed.\n\nThe notion of convexity in the Euclidean space may be generalized by modifying the definition in some or other aspects. The common name \"generalized convexity\" is used, because the resulting objects retain certain properties of convex sets.\n\nLet be a set in a real or complex vector space. is star convex (star-shaped) if there exists an in such that the line segment from to any point in is contained in . Hence a non-empty convex set is always star-convex but a star-convex set is not always convex.\n\nAn example of generalized convexity is orthogonal convexity.\n\nA set in the Euclidean space is called orthogonally convex or ortho-convex, if any segment parallel to any of the coordinate axes connecting two points of lies totally within . It is easy to prove that an intersection of any collection of orthoconvex sets is orthoconvex. Some other properties of convex sets are valid as well.\n\nThe definition of a convex set and a convex hull extends naturally to geometries which are not Euclidean by defining a geodesically convex set to be one that contains the geodesics joining any two points in the set.\n\nConvexity can be extended for a space endowed with the order topology, using the total order of the space.\n\nLet . The subspace is a convex set if for each pair of points in such that , the interval is contained in . That is, is convex if and only if for all in , implies .\n\nThe notion of convexity may be generalised to other objects, if certain properties of convexity are selected as axioms.\n\nGiven a set , a convexity over is a collection of subsets of satisfying the following axioms:\n\n\nThe elements of are called convex sets and the pair is called a convexity space. For the ordinary convexity, the first two axioms hold, and the third one is trivial.\n\nFor an alternative definition of abstract convexity, more suited to discrete geometry, see the \"convex geometries\" associated with antimatroids.\n\n"}
{"id": "3687308", "url": "https://en.wikipedia.org/wiki?curid=3687308", "title": "Covariant classical field theory", "text": "Covariant classical field theory\n\nIn mathematical physics, covariant classical field theory represents classical fields by sections of fiber bundles, and their dynamics is phrased in the context of a finite-dimensional space of fields. Nowadays, it is well known that jet bundles and the variational bicomplex are the correct domain for such a description. The Hamiltonian variant of covariant classical field theory is the covariant Hamiltonian field theory where momenta correspond to derivatives of field variables with respect to all world coordinates. Non-autonomous mechanics is formulated as covariant classical field theory on fiber bundles over the time axis ℝ.\n\n\n"}
{"id": "4152503", "url": "https://en.wikipedia.org/wiki?curid=4152503", "title": "Credibility theory", "text": "Credibility theory\n\nCredibility theory is a form of statistical inference used to forecast an uncertain future event developed by Thomas Bayes. It may be used when you have multiple estimates of a future event, and you would like to combine these estimates in such a way to get a more accurate and relevant estimate. This is typically used by actuaries working for insurance companies when determining the premium values. For example, in group health insurance an insurer is interested in calculating the risk premium, formula_1, (i.e. the theoretical expected claims amount) for a particular employer in the coming year. The insurer will likely have an estimate of historic overall claims experience, formula_2, as well as a more specific estimate for the employer in question, formula_3. Assigning a credibility factor, formula_4, to the overall claims experience (and the reciprocal to employer experience) allows the insurer to get a more accurate estimate of the risk premium in the following manner: \n\nformula_5The credibility factor is derived by calculating the maximum likelihood estimate which would minimise the error of estimate. Assuming the variance of formula_2and formula_3are known quantities taking on the values formula_8and formula_9respectively, it can be shown that formula_4should be equal to: \n\nformula_11Therefore, the more uncertainty the estimate has, the lower is its credibility. \n\nIn Bayesian credibility, we separate each class (B) and assign them a probability (Probability of B). Then we find how likely our experience (A) is within each class (Probability of A given B). Next, we find how likely our experience was over all classes (Probability of A). Finally, we can find the probability of our class given our experience. So going back to each class, we weight each statistic with the probability of the particular class given the experience.\n\nBühlmann credibility works by looking at the Variance across the population. More specifically, it looks to see how much of the Total Variance is attributed to the Variance of the Expect Values of each class (Variance of the Hypothetical Mean), and how much is attributed to the Expected Variance over all classes (Expected Value of the Process Variance). Say we have a basketball team with a high number of points per game. Sometimes they get 128 and other times they get 130 but always one of the two. Compared to all basketball teams this is a relatively low variance, meaning that they will contribute very little to the Expect Value of the Process Variance. Also, their unusually high point totals greatly increases the variance of the population, meaning that if the league booted them out, they'd have a much more predictable point total for each team (lower variance). So, this team is definitely unique (they contribute greatly to the Variance of the Hypothetical Mean). So we can rate this team's experience with a fairly high credibility. They often/always score a lot (low Expected Value of Process Variance) and not many teams score as much as them (high Variance of Hypothetical Mean).\n\nSuppose there are two coins in a box. One has heads on both sides and the other is a normal coin with 50:50 likelihood of heads or tails. You need to place a wager on the outcome after one is randomly drawn and flipped.\n\nThe odds of heads is .5 * 1 + .5 * .5 = .75. This is because there is a .5 chance of selecting the heads-only coin with 100% chance of heads and .5 chance of the fair coin with 50% chance.\n\nNow the same coin is reused and you are asked to bet on the outcome again.\n\nIf the first flip was tails, there is a 100% chance you are dealing with a fair coin, so the next flip has a 50% chance of heads and 50% chance of tails.\n\nIf the first flip was heads, we must calculate the conditional probability that the chosen coin was heads-only as well as the conditional probability that the coin was fair, after which we can calculate the conditional probability of heads on the next flip. The probability that it came from a heads-only coin given that the first flip was heads is the probability of selecting a heads-only coin times the probability of heads for that coin divided by the initial probability of heads on the first flip, or .5 * 1 / .75 = 2/3. The probability that it came from a fair coin given that the first flip was heads is the probability of selecting a fair coin times the probability of heads for that coin divided by the initial probability of heads on the first flip, or .5 * .5 / .75 = 1/3. Finally, The conditional probability of heads on the next flip given that the first flip was heads is the conditional probability of a heads-only coin times the probability of heads for a heads-only coin plus the conditional probability of a fair coin times the probability of heads for a fair coin, or 2/3 * 1 + 1/3 * .5 = 5/6 ≈ .8333\n\nActuarial credibility describes an approach used by actuaries to improve statistical estimates. Although the approach can be formulated in either a frequentist or Bayesian statistical setting, the latter is often preferred because of the ease of recognizing more than one source of randomness through both \"sampling\" and \"prior\" information. In a typical application, the actuary has an estimate X based on a small set of data, and an estimate M based on a larger but less relevant set of data. The credibility estimate is ZX + (1-Z)M, where Z is a number between 0 and 1 (called the \"credibility weight\" or \"credibility factor\") calculated to balance the sampling error of X against the possible lack of relevance (and therefore modeling error) of M.\n\nWhen an insurance company calculates the premium it will charge, it divides the policy holders into groups. For example, it might divide motorists by age, sex, and type of car; a young man driving a fast car being considered a high risk, and an old woman driving a small car being considered a low risk. The division is made balancing the two requirements that the risks in each group are sufficiently similar and the group sufficiently large that a meaningful statistical analysis of the claims experience can be done to calculate the premium. This compromise means that none of the groups contains only identical risks. The problem is then to devise a way of combining the experience of the group with the experience of the individual risk to calculate the premium better. Credibility theory provides a solution to this problem.\n\nFor actuaries, it is important to know credibility theory in order to calculate a premium for a group of insurance contracts. The goal is to set up an experience rating system to determine next year's premium, taking into account not only the individual experience with the group, but also the collective experience.\n\nThere are two extreme positions. One is to charge everyone the same premium estimated by the overall mean formula_12 of the data. This makes sense only if the portfolio is homogeneous, which means that all risks cells have identical mean claims. However, if the portfolio is heterogeneous, it is not a good idea to charge a premium in this way (overcharging \"good\" people and undercharging \"bad\" risk people) since the \"good\" risks will take their business elsewhere, leaving the insurer with only \"bad\" risks. This is an example of adverse selection.\n\nThe other way around is to charge to group formula_13 its own average claims, being formula_14 as premium charged to the insured. These methods are used if the portfolio is heterogeneous, provided a fairly large claim experience. To compromise these two extreme positions, we take the weighted average of the two extremes:\n\nformula_16 has the following intuitive meaning: it expresses how \"credible\" (acceptability) the individual of cell formula_13 is. If it is high, then use higher formula_16 to attach a larger weight to charging the formula_14, and in this case, formula_16 is called a credibility factor, and such a premium charged is called a credibility premium.\n\nIf the group were completely homogeneous then it would be reasonable to set formula_21, while if the group were completely heterogeneous then it would be reasonable to set formula_22. Using intermediate values is reasonable to the extent that both individual and group history is useful in inferring future individual behavior.\n\nFor example, an actuary has an accident and payroll historical data for a shoe factory suggesting a rate of 3.1 accidents per million dollars of payroll. She has industry statistics (based on all shoe factories) suggesting that the rate is 7.4 accidents per million. With a credibility, Z, of 30%, she would estimate the rate for the factory as 30%(3.1) + 70%(7.4) = 6.1 accidents per million.\n\n"}
{"id": "35205725", "url": "https://en.wikipedia.org/wiki?curid=35205725", "title": "Drinfeld upper half plane", "text": "Drinfeld upper half plane\n\nIn mathematics, the Drinfeld upper half plane is a rigid analytic space analogous to the usual upper half plane for function fields, introduced by . \nIt is defined to be P(C)\\P(F), where F is a function field of a curve over a finite field, F its completion at ∞, and C the completion of the algebraic closure of F.\n\nThe analogy with the usual upper half plane arises from the fact that the global function field F is analogous to the rational numbers Q. Then, F is the real numbers R and the algebraic closure of F is the complex numbers C (which are already complete). Finally, P(C) is the Riemann sphere, so P(C)\\P(R) is the upper half plane \"together with\" the lower half plane.\n\n"}
{"id": "28094731", "url": "https://en.wikipedia.org/wiki?curid=28094731", "title": "Enrico Arbarello", "text": "Enrico Arbarello\n\nEnrico Arbarello is an Italian mathematician who is a leading expert in algebraic geometry.\n\nHe earned a Ph.D. at Columbia University in New York in 1973. He was a visiting scholar at the Institute for Advanced Study from 1993-94. He is now a Mathematics Professor at Sapienza University of Rome.\nIn 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "1463006", "url": "https://en.wikipedia.org/wiki?curid=1463006", "title": "Examples of vector spaces", "text": "Examples of vector spaces\n\nThis page lists some examples of vector spaces. See vector space for the definitions of terms used on this page. See also: dimension, basis.\n\n\"Notation\". We will let F denote an arbitrary field such as the real numbers R or the complex numbers C. See also: table of mathematical symbols.\n\nThe simplest example of a vector space is the trivial one: {0}, which contains only the zero vector (see axiom 3 of vector spaces). Both vector addition and scalar multiplication are trivial. A basis for this vector space is the empty set, so that {0} is the 0-dimensional vector space over F. Every vector space over F contains a subspace isomorphic to this one.\n\nThe zero vector space is different from the null space of a linear operator F, which is the kernel of F.\n\nThe next simplest example is the field F itself. Vector addition is just field addition, and scalar multiplication is just field multiplication. This property can be used to prove that a field is a vector space. Any non-zero element of F serves as a basis so F is a 1-dimensional vector space over itself.\n\nThe field is a rather special vector space; in fact it is the simplest example of a commutative algebra over F. Also, F has just two subspaces: {0} and F itself.\n\nThe original example of a vector space, which the axiomatic definition generalizes, is the following. For any positive integer \"n\", the set of all \"n\"-tuples of elements of F forms an \"n\"-dimensional vector space over F sometimes called \"coordinate space\" and denoted F. An element of F is written\nwhere each \"x\" is an element of F. The operations on F are defined by\nCommonly, F is the field of real numbers, in which case we obtain real coordinate space R. The field of complex numbers gives complex coordinate space C. The \"a + bi\" form of a complex number shows that C itself is a two-dimensional real vector space with coordinates (\"a\",\"b\"). Similarly, the quaternions and the octonions are respectively four- and eight-dimensional real vector spaces, and C is a \"2n\"-dimensional real vector space.\n\nThe vector space F has a standard basis:\nwhere 1 denotes the multiplicative identity in F.\n\nLet F denote the space of infinite sequences of elements from F such that only \"finitely\" many elements are nonzero. That is, if we write an element of F as\nthen only a finite number of the \"x\" are nonzero (i.e., the coordinates become all zero after a certain point). Addition and scalar multiplication are given as in finite coordinate space. The dimensionality of F is countably infinite. A standard basis consists of the vectors \"e\" which contain a 1 in the \"i\"-th slot and zeros elsewhere. This vector space is the coproduct (or direct sum) of countably many copies of the vector space F.\n\nNote the role of the finiteness condition here. One could consider arbitrary sequences of elements in F, which also constitute a vector space with the same operations, often denoted by F - see below. F is the \"product\" of countably many copies of F.\n\nBy Zorn's lemma, F has a basis (there is no obvious basis). There are uncountably infinite elements in the basis. Since the dimensions are different, F is \"not\" isomorphic to F. It is worth noting that F is (isomorphic to) the dual space of F, because a linear map \"T\" from F to F is determined uniquely by its values \"T\"(\"e\") on the basis elements of F, and these values can be arbitrary. Thus one sees that a vector space need not be isomorphic to its dual if it is infinite dimensional, in contrast to the finite dimensional case.\n\nStarting from \"n\" vector spaces, or a countably infinite collection of them, each with the same field, we can define the product space like above.\n\nLet F denote the set of \"m\"×\"n\" matrices with entries in F. Then F is a vector space over F. Vector addition is just matrix addition and scalar multiplication is defined in the obvious way (by multiplying each entry by the same scalar). The zero vector is just the zero matrix. The dimension of F is \"mn\". One possible choice of basis is the matrices with a single entry equal to 1 and all other entries 0.\n\nWhen \"m\" = \"n\" the matrix is square and matrix multiplication of two such matrices produces a third. This vector space of dimension \"n\" forms an algebra over a field.\n\nThe set of polynomials with coefficients in F is a vector space over F, denoted F[\"x\"]. Vector addition and scalar multiplication are defined in the obvious manner. If the degree of the polynomials is unrestricted then the dimension of F[\"x\"] is countably infinite. If instead one restricts to polynomials with degree less than or equal to \"n\", then we have a vector space with dimension \"n\" + 1.\n\nOne possible basis for F[\"x\"] is a monomial basis: the coordinates of a polynomial with respect to this basis are its coefficients, and the map sending a polynomial to the sequence of its coefficients is a linear isomorphism from F[\"x\"] to the infinite coordinate space F.\n\nThe vector space of polynomials with real coefficients and degree less than or equal to \"n\" is denoted by P.\n\nThe set of polynomials in several variables with coefficients in F is vector space over F denoted F[\"x\", \"x\", …, \"x\"]. Here \"r\" is the number of variables.\n\nLet \"X\" be a non-empty arbitrary set and \"V\" an arbitrary vector space over F. The space of all functions from \"X\" to \"V\" is a vector space over F under pointwise addition and multiplication. That is, let \"f\" : \"X\" → \"V\" and \"g\" : \"X\" → \"V\" denote two functions, and let \"α\"∈F. We define\nwhere the operations on the right hand side are those in \"V\". The zero vector is given by the constant function sending everything to the zero vector in \"V\". The space of all functions from \"X\" to \"V\" is commonly denoted \"V\".\n\nIf \"X\" is finite and \"V\" is finite-dimensional then \"V\" has dimension |\"X\"|(dim \"V\"), otherwise the space is infinite-dimensional (uncountably so if \"X\" is infinite).\n\nMany of the vector spaces that arise in mathematics are subspaces of some function space. We give some further examples.\n\nLet \"X\" be an arbitrary set. Consider the space of all functions from \"X\" to F which vanish on all but a finite number of points in \"X\". This space is a vector subspace of F, the space of all possible functions from \"X\" to F. To see this, note that the union of two finite sets is finite, so that the sum of two functions in this space will still vanish outside a finite set.\n\nThe space described above is commonly denoted (F) and is called \"generalized coordinate space\" for the following reason. If \"X\" is the set of numbers between 1 and \"n\" then this space is easily seen to be equivalent to the coordinate space F. Likewise, if \"X\" is the set of natural numbers, N, then this space is just F.\n\nA canonical basis for (F) is the set of functions {δ | \"x\" ∈ \"X\"} defined by\nThe dimension of (F) is therefore equal to the cardinality of \"X\". In this manner we can construct a vector space of any dimension over any field. Furthermore, \"every vector space is isomorphic to one of this form\". Any choice of basis determines an isomorphism by sending the basis onto the canonical one for (F).\n\nGeneralized coordinate space may also be understood as the direct sum of |\"X\"| copies of F (i.e. one for each point in \"X\"):\nThe finiteness condition is built into the definition of the direct sum. Contrast this with the direct product of |\"X\"| copies of F which would give the full function space F.\n\nAn important example arising in the context of linear algebra itself is the vector space of linear maps. Let \"L\"(\"V\",\"W\") denote the set of all linear maps from \"V\" to \"W\" (both of which are vector spaces over F). Then \"L\"(\"V\",\"W\") is a subspace of \"W\" since it is closed under addition and scalar multiplication.\n\nNote that L(F,F) can be identified with the space of matrices F in a natural way. In fact, by choosing appropriate bases for finite-dimensional spaces V and W, L(V,W) can also be identified with F. This identification normally depends on the choice of basis.\n\nIf \"X\" is some topological space, such as the unit interval [0,1], we can consider the space of all continuous functions from \"X\" to R. This is a vector subspace of R since the sum of any two continuous functions is continuous and scalar multiplication is continuous.\n\nThe subset of the space of all functions from R to R consisting of (sufficiently differentiable) functions that satisfy a certain differential equation is a subspace of R if the equation is linear. This is because differentiation is a linear operation, i.e., <nowiki>(a f + b g)' = a f' + b g', where ' is the differentiation operator.</nowiki>\n\nSuppose K is a subfield of F (cf. field extension). Then F can be regarded as a vector space over K by restricting scalar multiplication to elements in K (vector addition is defined as normal). The dimension of this vector space is called the \"degree\" of the extension. For example the complex numbers C form a two-dimensional vector space over the real numbers R. Likewise, the real numbers R form an (uncountably) infinite-dimensional vector space over the rational numbers Q.\n\nIf \"V\" is a vector space over F it may also be regarded as vector space over K. The dimensions are related by the formula\nFor example C, regarded as a vector space over the reals, has dimension 2\"n\".\n\nApart from the trivial case of a zero-dimensional space over any field, a vector space over a field F has a finite number of elements if and only if F is a finite field and the vector space has a finite dimension. Thus we have F, the unique finite field (up to isomorphism) with \"q\" elements. Here \"q\" must be a power of a prime (\"q\" = \"p\" with \"p\" prime). Then any \"n\"-dimensional vector space \"V\" over F will have \"q\" elements. Note that the number of elements in \"V\" is also the power of a prime (because a power of a prime power is again a prime power). The primary example of such a space is the coordinate space (F).\n\nThese vector spaces are of critical importance in the representation theory of finite groups, number theory, and cryptography.\n"}
{"id": "22999896", "url": "https://en.wikipedia.org/wiki?curid=22999896", "title": "Exponential mechanism (differential privacy)", "text": "Exponential mechanism (differential privacy)\n\nThe exponential mechanism is a technique for designing differentially private algorithms. It was developed by Frank McSherry and Kunal Talwar. Differential privacy is a technique for releasing statistical information about a database without revealing information about its individual entries.\n\nMost of the initial research in the field of differential privacy revolved around real-valued functions which have relatively low sensitivity to change in the data of a single individual and whose usefulness is not hampered by small additive perturbations. A natural question is what happens in the situation when one wants to preserve more general sets of properties. The exponential mechanism helps to extend the notion of differential privacy to address these issues. Moreover, it describes a class of mechanisms that includes all possible differentially private mechanisms.\n\nIn very generic terms a privacy mechanism maps a set of formula_1 inputs from domain formula_2, to a range formula_3. The map may be randomized, in which case each element of the domain formula_4 corresponds to the probability distribution over the range formula_5. The privacy mechanism makes no assumption about the nature of formula_2 and formula_3 apart from a base measure formula_8 on formula_3. Let us define a function formula_10. Intuitively this function assigns a score to the pair formula_11, where formula_12 and formula_13. The score reflects the appeal of the pair formula_11, i.e. the higher the score, the more appealing the pair is. \nGiven the input formula_12, the mechanism's objective is to return an formula_13 such that the function formula_17 is approximately maximized. To achieve this, set up the mechanism formula_18 as follows: <br>\nDefinition: For any function formula_19, and a base measure formula_8 over formula_3, define:\nThis definition implies the fact that the probability of returning an formula_23 increases exponentially with the increase in the value of formula_17. Ignoring the base measure formula_8 then the value formula_23 which maximizes formula_17 has the highest probability. Moreover, this mechanism is differentially private. Proof of this claim will follow. One technicality that should be kept in mind is that in order to properly define formula_18 the formula_32 should be finite.\n\nTheorem (differential privacy): formula_18 gives formula_34-differential privacy.\n\nProof: The probability density of formula_18 at formula_23 equals\n\nNow, if a single change in formula_38 changes formula_39 by at most formula_40 then the numerator can change at most by a factor of formula_41 and the denominator minimum by a factor of formula_42. Thus, the ratio of the new probability density (i.e. with new formula_38) and the earlier one is at most formula_44.\n\nWe would ideally want the random draws of formula_23 from the mechanism formula_18 to nearly maximize formula_17. If we consider formula_48 to be formula_49 then we can show that the probability of the mechanism deviating from formula_49 is low, as long as there is a sufficient mass (in terms of formula_51) of values formula_23 with value formula_39 close to the optimum.\n\nLemma: Let formula_54 and formula_55, we have formula_56 is at most formula_57. The probability is taken over formula_5.\n\nProof: The probability formula_56 is at most formula_60, as the denominator can be at most one. Since both the probabilities have the same normalizing term so,\n\nThe value of formula_62 is at most one, and so this bound implies the lemma statement.\n\nTheorem (Accuracy): For those values of formula_63, we have formula_64.\n\nProof: It follows from the previous lemma that the probability of the score being at least formula_65 is formula_66. By hypothesis, formula_67. Substituting the value of formula_68 we get this probability to be at least formula_69. Multiplying with formula_65 yields the desired bound.\n\nWe can assume formula_71 for formula_72 to be less than or equal to one in all the computations, because we can always normalize with formula_73 .\n\nBefore we get into the details of the example let us define some terms which we will be using extensively throughout our discussion.\n\nDefinition (global sensitivity): The global sensitivity of a query formula_74 is its maximum difference when evaluated on two neighbouring datasets formula_75:\n\nDefinition: A predicate query formula_77 for any predicate formula_78 is defined to be\n\nNote that formula_80 for any predicate formula_78.\n\nThe following is due to Avrim Blum, Katrina Ligett and Aaron Roth.\n\nDefinition (Usefulness): A mechanism formula_82 is formula_83-useful for queries in class formula_84 with probability formula_85, if formula_86 and every dataset formula_4, for formula_88, formula_89.\n\nInformally, it means that with high probability the query formula_90 will behave in a similar way on the original dataset formula_4 and on the synthetic dataset formula_92. <br>\nLet us consider a common problem in Data Mining. Assume there is a database formula_4 with formula_1 entries. Each entry consist of formula_95-tuples of the form formula_96 where formula_97. Now, a user wants to learn a linear halfspace of the form formula_98. In essence the user wants to figure out the values of formula_99 such that maximum number of tuples in the database satisfy the inequality. The algorithm we describe below can generate a synthetic database formula_92 which will allow the user to learn (approximately) the same linear half-space while querying on this synthetic database. The motivation for such an algorithm being that the new database will be generated in a differentially private manner and thus assure privacy to the individual records in the database formula_4.\n\nIn this section we show that it is possible to release a dataset which is useful for concepts from a polynomial VC-Dimension class and at the same time adhere to formula_102-differential privacy as long as the size of the original dataset is at least polynomial on the VC-Dimension of the concept class. To state formally:\n\nTheorem: For any class of functions formula_84 and any dataset formula_104 such that\nwe can output an formula_83-useful dataset formula_92 that preserves formula_102-differential privacy. As we had mentioned earlier the algorithm need not be efficient.\n\nOne interesting fact is that the algorithm which we are going to develop generates a synthetic dataset whose size is independent of the original dataset; in fact, it only depends on the VC-dimension of the concept class and the parameter formula_109. The algorithm outputs a dataset of size formula_110\n\nWe borrow the Uniform Convergence Theorem from combinatorics and state a corollary of it which aligns to our need.\n\nLemma: Given any dataset formula_4 there exists a dataset formula_92 of size formula_113 such that formula_114.\n\nProof:\n\nWe know from the uniform convergence theorem that\n\nwhere probability is over the distribution of the dataset. \nThus, if the RHS is less than one then we know for sure that the data set formula_92 exists. To bound the RHS to less than one we need formula_117, where formula_118 is some positive constant. Since we stated earlier that we will output a dataset of size formula_110, so using this bound on formula_120 we get formula_121. Hence the lemma.\n\nNow we invoke the exponential mechanism.\n\nDefinition: For any function formula_122 and input dataset formula_4, the exponential mechanism outputs each dataset formula_92 with probability proportional to formula_125.\n\nFrom the exponential mechanism we know this preserves formula_126-differential privacy. Let's get back to the proof of the Theorem.\n\nWe define formula_127.\n\nTo show that the mechanism satisfies the formula_83-usefulness, we should show that it outputs some dataset formula_92 with formula_130 with probability formula_85. \nThere are at most formula_132 output datasets and the probability that formula_133 is at most proportional to formula_134. Thus by union bound, the probability of outputting any such dataset formula_92 is at most proportional to formula_136. \nAgain, we know that there exists some dataset formula_137 for which formula_138. Therefore, such a dataset is output with probability at least proportional to formula_139.\n\nLet formula_140 the event that the exponential mechanism outputs some dataset formula_92 such that formula_142.\n\nformula_143 the event that the exponential mechanism outputs some dataset formula_92 such that formula_145.\nNow setting this quantity to be at least formula_147, we find that it suffices to have\n\nAnd hence we prove the theorem.\n\nIn the above example of the usage of exponential mechanism, one can output a synthetic dataset in a differentially private manner and can use the dataset to answer queries with good accuracy. Other private mechanisms, such as posterior sampling, which returns parameters rather than datasets, can be made equivalent to the exponential one.\n\nApart from the setting of privacy, the exponential mechanism has also been studied in the context of auction theory and classification algorithms. In the case of auctions the exponential mechanism helps to achieve a \"truthful\" auction setting.\n\n"}
{"id": "41568237", "url": "https://en.wikipedia.org/wiki?curid=41568237", "title": "Gisbert Hasenjaeger", "text": "Gisbert Hasenjaeger\n\nGisbert F. R. Hasenjaeger (June 1, 1919 – September 2, 2006) was a German mathematical logician. Independently and simultaneously with Leon Henkin in 1949, he developed a new proof of the completeness theorem of Kurt Gödel for predicate logic. He worked as an assistant to Heinrich Scholz at Section IVa of Oberkommando der Wehrmacht Chiffrierabteilung, and was responsible for the security of the Enigma machine.\n\nGisbert Hasenjaeger went to high school in Mülheim, where his father was a lawyer and local politician. After completing school in 1936, Gisbert volunteered for labor service. He was drafted for military service in World War II, and fought as an artillerist in the Russian campaign, where he was badly wounded in January 1942. After his recovery, in October 1942 Heinrich Scholz got him an employment in the Cipher Department of the High Command of the Wehrmacht (OKW/Chi), where he was the youngest member at 24. He attended a cryptography training course by Erich Hüttenhain, and was put into the recently founded Section IVa \"Security check of own Encoding Procedures\" under Karl Stein, who assigned him the security check of the Enigma machine. At the end of the war as OKW/Chi disintegrated, Hasenjaeger managed to escape TICOM, the United States effort to roundup and seize captured German intelligence people and material.\n\nFrom the end of 1945, he studied mathematics and especially mathematical logic with Heinrich Scholz at the Westfälische Wilhelms-Universität University in Münster. In 1950 received his doctorate \"Topological studies on the semantics and syntax of an extended predicate calculus\" and completed his habilitation in 1953.\n\nIn Münster, he worked as an assistant to Scholz and later co-author, to write the textbook \"Fundamentals of Mathematical Logic\" in \"Springer's Grundlehren series\" (Yellow series of Springer-Verlag), which he published in 1961 fully 6 years after Scholz's death. In 1962 he became professor at the University of Bonn, where he was Director of the newly created Department of Logic.\n\nIn 1962, Dr. Hasenjaeger left Münster University to take a full professorship at Bonn University, where he was established Director of the newly established Department of Logic and Basic Research. In 1964/65 he spent a year at Princeton University at the Institute for Advanced Study His doctoral students at Bonn included Ronald B. Jensen, his most famous pupil.\n\nHe became professor emeritus in 1984.\n\nIn Oct 1942, after starting work at OKW/Chi, Hasenjaeger was trained in cryptology, given by the mathematician, Erich Hüttenhain, who was widely considered the most important German cryptologist of his time. Hasenjaeger was put into a newly formed department, whose principal responsibility was the defensive testing and security control of their own methods and devices. Hasenjaeger was ordered, by the mathematician Karl Stein who was also conscripted at OKW/Chi, to examine the Enigma machine for cryptologic weaknesses, while Stein was to examine the Siemens and Halske T52 and the Lorenz SZ-42. The Enigma machine that Hasenjaeger examined was a variation that worked with 3 rotors and had no plug board. Germany sold this version to neutral countries to accrue foreign exchange. Hasenjaeger was presented with a 100 character encrypted message for analysis and found a weakness which enabled the identification of the correct wiring rotors and also the appropriate rotor positions, to decrypt the messages. Further success eluded him however. He crucially failed to identify the most important weakness of the Enigma machine: the lack of fixed points (letters encrypting to themselves) due to the reflector. Hasenjaeger could take some comfort from the fact that even Alan Turing missed this weakness. Instead the honour was attributed to Gordon Welchman, who used the knowledge to decrypt several hundred thousand Enigma messages during the war. In fact fixed points were earlier used by Polish codebreaker, Henryk Zygalski, as the basis for his method of attack on Enigma cipher, referred to by the Poles as \"Zygalski sheets\" (Zygalski sheets) (płachty Zygalskiego) and by the British as the \"Netz method\".\n\nIt was while Hasenjaeger was working at Westfälische Wilhelms-Universität University in Münster in the period between 1946 and 1953 that Hasenjaeger made a most amazing discovery - a proof of Kurt Gödel's Gödel's completeness theorem for full predicate logic with identity and function symbols. Gödel's proof of 1930 for predicate logic did not automatically establish a procedure for the general case. When he had solved the problem in late 1949, he was frustrated to find that a young American mathematician Leon Henkin, had also created a proof. Both construct from extension of a term model, which is then the model for the initial theory. Although the Henkin proof was considered by Hasenjaeger and his peers to more flexible, Hasenjaeger' is considered simpler and more transparent.\n\nHasenjaeger continued to refine his proof through to 1953 when he made a breakthrough. According to the mathematicians Alfred Tarski, Stephen Cole Kleene and Andrzej Mostowski, the Arithmetical hierarchy of formulas is the set of arithmetical propositions that are true in the standard model, but not arithmetically definable. So, what does the concept of truth for the term model mean, the results for the recursively axiomatized Peano arithmetic from the Hasenjaeger method? The result was the truth predicate is well arithmetically, it is even formula_1. So far down in the arithmetic hierarchy, and that goes for any recursively axiomatized (countable, consistent) theories. Even if you are true in all the natural numbers formula_2 formulas to the axioms.\n\nThis classic proof is a very early, original application of the arithmetic hierarchy theory to a general-logical problem. It appeared in 1953 in the \"Journal of Symbolic Logic\".\n\nIn 1963, Hasenjaeger built an Universal Turing machine out of old telephone relays. Although Hasenjaeger's work on UTMs was largely unknown and he never published any details of the machinery during his lifetime, his family decided to donate the machine to the Heinz Nixdorf Museum in Paderborn, Germany, after his death.\"In a academic paper presented at the International Conference of History and Philosophy of Computing\" Rainer Glaschick, Turlough Neary, Damien Woods, Niall Murphy had examined Hasenjaeger's UTM machine at the request of Hasenjaeger family and found that the UTM was remarkably small and efficiently universal. Hasenjaeger UTM contained 3-tapes, 4 states, 2 symbols and was an evolution of ideas from Edward F. Moore's first universal machine and Hao Wang's B-machine. Hasenjaeger went on to build a small efficient Wang B-machine simulator. This was again proven by the team assembled by Rainer Glaschick to be efficiently universal.\n\nIt was only in the 1970s that Hasenjaeger learned that the Enigma Machine had been so comprehensively broken. It impressed him that Alan Turing himself, considered one of the greatest mathematicians of the 20th century, had worked on breaking the device. The fact that the Germans had so comprehensively underestimated the weaknesses of the device, in contrast to Turing and Welchmans work, was seen by Hasenjaeger today as entirely positive. Hasenjaeger stated:\n\n"}
{"id": "1433747", "url": "https://en.wikipedia.org/wiki?curid=1433747", "title": "Harnack's principle", "text": "Harnack's principle\n\nIn complex analysis, Harnack's principle or Harnack's theorem is one of several closely related theorems about the convergence of sequences of harmonic functions, that follow from Harnack's inequality.\n\nIf the functions formula_1, formula_2, ... are harmonic in an open connected subset formula_3 of the complex plane C, and\nin every point of formula_3, then the limit\neither is infinite in every point of the domain formula_3 or it is finite in every point of the domain, in both cases uniformly in each compact subset of formula_3. In the latter case, the function\nis harmonic in the set formula_10.\n\n"}
{"id": "14468160", "url": "https://en.wikipedia.org/wiki?curid=14468160", "title": "Henry Ainslie", "text": "Henry Ainslie\n\nHenry Ainslie (21 March 1760 – 1834) was a physician. Educated at Hawkshead Grammar School and then Pembroke College, Cambridge (where he graduated Senior Wrangler and was second in the Smith Prize), he became a fellow of Pembroke in 1782, and a Fellow of the Royal College of Physicians in 1795. He was a Junior Commissioner for madhouses in 1797 and 1798, and a Senior Commissioner in 1809 and 1817. \n\nIn 1785 he married Agnes Ford of Monk Coniston (an estate near Coniston Water in the English Lake District) in the church at Colton. Agnes Ford was the daughter of Richard Ford, founder of the Newland Company, later known as Harrison Ainslie. The couple owned Ford Lodge at Grizedale and planted many thousands of larch trees in the valley and on the surrounding hills and moorland which effectively started Grizedale Forest. They had a son Montague Ainslie.\n\n"}
{"id": "38818738", "url": "https://en.wikipedia.org/wiki?curid=38818738", "title": "Influential observation", "text": "Influential observation\n\nIn statistics, an influential observation is an observation for a statistical calculation whose deletion from the dataset would noticeably change the result of the calculation. In particular, in regression analysis an influential point is one whose deletion has a large effect on the parameter estimates.\n\nVarious methods have been proposed for measuring influence. Assume an estimated regression formula_1, where formula_2 is an \"n\"×1 column vector for the response variable, formula_3 is the \"n\"×\"k\" design matrix of explanatory variables (including a constant), formula_4 is the \"n\"×1 residual vector, and formula_5 is a \"k\"×1 vector of estimates of some population parameter formula_6. Also define formula_7, the projection matrix of formula_3. Then we have the following measures of influence:\n\n\nAn outlier may be defined as a surprising data point. Leverage is a measure of how much the estimated value of the dependent variable changes when the point is removed. There is one value of leverage for each data point. Data points with high leverage force the regression line to be close to the point. In Anscombe's quartet, only the bottom right image has a point with high leverage.\n\n\n"}
{"id": "147252", "url": "https://en.wikipedia.org/wiki?curid=147252", "title": "Integration by parts", "text": "Integration by parts\n\nIn calculus, and more generally in mathematical analysis, integration by parts or partial integration is a process that finds the integral of a product of functions in terms of the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be derived in one line simply by integrating the product rule of differentiation.\n\nIf and , while and , then integration by parts states that:\n\nor more compactly:\n\nMathematician Brook Taylor discovered integration by parts, first publishing the idea in 1715. More general formulations of integration by parts exist for the Riemann–Stieltjes and Lebesgue–Stieltjes integrals. The discrete analogue for sequences is called summation by parts.\n\nThe theorem can be derived as follows. Suppose \"u\"(\"x\") and \"v\"(\"x\") are two continuously differentiable functions. The product rule states (in Leibniz's notation):\n\nIntegrating both sides with respect to \"x\",\n\nthen applying the definition of indefinite integral,\n\ngives the formula for integration by parts.\n\nSince \"du\" and \"dv\" are differentials of a function of one variable \"x\",\n\nThe original integral ∫\"uv\"′ \"dx\" contains \"v\"′ (derivative of \"v\"); in order to apply the theorem, \"v\" (antiderivative of \"v\"′) must be found, and then the resulting integral ∫\"vu\"′ \"dx\" must be evaluated.\n\nIt is not actually necessary for \"u\" and \"v\" to be continuously differentiable. Integration by parts works if \"u\" is absolutely continuous and the function designated ' is Lebesgue integrable (but not necessarily continuous). (If \"v has a point of discontinuity then its antiderivative \"v\" may not have a derivative at that point.)\n\nIf the interval of integration is not compact, then it is not necessary for \"u\" to be absolutely continuous in the whole interval or for \"v\" ' to be Lebesgue integrable in the interval, as a couple of examples (in which \"u\" and \"v\" are continuous and continuously differentiable) will show. For instance, if\n\n\"u\" is not absolutely continuous on the interval , but nevertheless\n\nso long as formula_11 is taken to mean the limit of formula_12 as formula_13 and so long as the two terms on the right-hand side are finite. This is only true if we choose formula_14 Similarly, if\n\n\"v\"' is not Lebesgue integrable on the interval , but nevertheless\n\nwith the same interpretation.\n\nOne can also easily come up with similar examples in which \"u\" and \"v\" are not continuously differentiable.\nFurther, if formula_17 is a function of bounded variation on the segment formula_18 and formula_19 is differentiable on formula_18 then\n\nformula_21\n\nwhere by formula_22 I denoted the signed measure corresponding to the function of bounded variation formula_23, and functions formula_24 are extensions of formula_25\nto formula_26 which are respectively of bounded variation and differentiable.\n\nIntegrating the product rule for three multiplied functions, \"u\"(\"x\"), \"v\"(\"x\"), \"w\"(\"x\"), gives a similar result:\n\nIn general, for \"n\" factors\n\nwhich leads to\n\nwhere the product is of all functions except for the one differentiated in the same term.\n\nConsider a parametric curve by (\"x\", \"y\") = (\"f\"(\"t\"), \"g\"(\"t\")). Assuming that the curve is locally one-to-one and integrable, we can define\n\nThe area of the blue region is\n\nSimilarly, the area of the red region is\n\nThe total area \"A\" + \"A\" is equal to the area of the bigger rectangle, \"x\"\"y\", minus the area of the smaller one, \"x\"\"y\":\n\nOr, in terms of \"t\",\nOr, in terms of indefinite integrals, this can be written as\nRearranging:\nThus integration by parts may be thought of as deriving the area of the blue region from the area of rectangles and that of the red region.\n\nThis visualization also explains why integration by parts may help find the integral of an inverse function \"f\"(\"x\") when the integral of the function \"f\"(\"x\") is known. Indeed, the functions \"x\"(\"y\") and \"y\"(\"x\") are inverses, and the integral ∫\"x dy\" may be calculated as above from knowing the integral ∫\"y dx\". In particular, this explains use of integration by parts to integrate logarithm and inverse trigonometric functions.\n\nIntegration by parts is a heuristic rather than a purely mechanical process for solving integrals; given a single function to integrate, the typical strategy is to carefully separate this single function into a product of two functions \"u\"(\"x\")\"v\"(\"x\") such that the residual integral from the integration by parts formula is easier to evaluate than the single function. The following form is useful in illustrating the best strategy to take:\n\nNote that on the right-hand side, \"u\" is differentiated and \"v\" is integrated; consequently it is useful to choose \"u\" as a function that simplifies when differentiated, or to choose \"v\" as a function that simplifies when integrated. As a simple example, consider:\n\nSince the derivative of ln(\"x\") is , one makes (ln(\"x\")) part \"u\"; since the antiderivative of is -, one makes \"dx\" part \"dv\". The formula now yields:\n\nThe antiderivative of − can be found with the power rule and is .\n\nAlternatively, one may choose \"u\" and \"v\" such that the product \"u\"' (∫\"v dx\") simplifies due to cancellation. For example, suppose one wishes to integrate:\n\nIf we choose \"u\"(\"x\") = ln(|sin(\"x\")|) and \"v\"(\"x\") = secx, then \"u\" differentiates to 1/ tan \"x\" using the chain rule and \"v\" integrates to tan \"x\"; so the formula gives:\n\nThe integrand simplifies to 1, so the antiderivative is \"x\". Finding a simplifying combination frequently involves experimentation.\n\nIn some applications, it may not be necessary to ensure that the integral produced by integration by parts has a simple form; for example, in numerical analysis, it may suffice that it has small magnitude and so contributes only a small error term. Some other special techniques are demonstrated in the examples below.\n\nIn order to calculate\n\nlet:\n\nthen:\n\nwhere \"C\" is a constant of integration.\n\nFor higher powers of \"x\" in the form\n\nrepeatedly using integration by parts can evaluate integrals such as these; each application of the theorem lowers the power of \"x\" by one.\n\nAn example commonly used to examine the workings of integration by parts is\n\nHere, integration by parts is performed twice. First let\n\nthen:\n\nNow, to evaluate the remaining integral, we use integration by parts again, with:\n\nThen:\n\nPutting these together,\n\nThe same integral shows up on both sides of this equation. The integral can simply be added to both sides to get\n\nwhich rearranges to:\n\nwhere again \"C\" (and \"C\"<nowiki>'</nowiki> = \"C\"/2) is a constant of integration.\n\nA similar method is used to find the integral of secant cubed.\n\nTwo other well-known examples are when integration by parts is applied to a function expressed as a product of 1 and itself. This works if the derivative of the function is known, and the integral of this derivative times \"x\" is also known.\n\nThe first example is ∫ ln(\"x\") d\"x\". We write this as:\n\nLet:\n\nthen:\n\nwhere \"C\" is the constant of integration.\n\nThe second example is the inverse tangent function arctan(\"x\"):\n\nRewrite this as\n\nNow let:\n\nthen\n\nusing a combination of the inverse chain rule method and the natural logarithm integral condition.\n\nA rule of thumb proposed by Herbert Kasube advises that whichever function comes first in the following list should be chosen as \"u\":\n\nThe function which is to be \"dv\" is whichever comes last in the list: functions lower on the list have easier antiderivatives than the functions above them. The rule is sometimes written as \"DETAIL\" where \"D\" stands for \"dv\".\n\nTo demonstrate the LIATE rule, consider the integral\n\nFollowing the LIATE rule, \"u\" = \"x\", and \"dv\" = cos(\"x\") \"dx\", hence \"du\" = \"dx\", and \"v\" = sin(\"x\"), which makes the integral become\nwhich equals\n\nIn general, one tries to choose \"u\" and \"dv\" such that \"du\" is simpler than \"u\" and \"dv\" is easy to integrate. If instead cos(\"x\") was chosen as \"u\", and \"x dx\" as \"dv\", we would have the integral\n\nwhich, after recursive application of the integration by parts formula, would clearly result in an infinite recursion and lead nowhere.\n\nAlthough a useful rule of thumb, there are exceptions to the LIATE rule. A common alternative is to consider the rules in the \"ILATE\" order instead. Also, in some cases, polynomial terms need to be split in non-trivial ways. For example, to integrate\n\none would set\n\nso that\n\nThen\n\nFinally, this results in\n\nIntegration by parts is often used as a tool to prove theorems in mathematical analysis.\n\nThe gamma function is an example of a special function, defined as an improper integral for . Integration by parts illustrates it to be an extension of the factorial:\n\nSince\n\nfor integer , applying this formula repeatedly gives the factorial (denoted by the !):\n\nIntegration by parts is often used in harmonic analysis, particularly Fourier analysis, to show that quickly oscillating integrals with sufficiently smooth integrands decay quickly. The most common example of this is its use in showing that the decay of function's Fourier transform depends on the smoothness of that function, as described below.\n\nIf \"f\" is a \"k\"-times continuously differentiable function and all derivatives up to the \"k\"th one decay to zero at infinity, then its Fourier transform satisfies\n\nwhere is the \"k\"th derivative of \"f\". (The exact constant on the right depends on the convention of the Fourier transform used.) This is proved by noting that\n\nso using integration by parts on the Fourier transform of the derivative we get\n\nApplying this inductively gives the result for general \"k\". A similar method can be used to find the Laplace transform of a derivative of a function.\n\nThe above result tells us about the decay of the Fourier transform, since it follows that if \"f\" and are integrable then\n\nIn other words, if \"f\" satisfies these conditions then its Fourier transform decays at infinity at least as quickly as . In particular, if then the Fourier transform is integrable.\n\nThe proof uses the fact, which is immediate from the definition of the Fourier transform, that\nUsing the same idea on the equality stated at the start of this subsection gives\nSumming these two inequalities and then dividing by gives the stated inequality.\n\nOne use of integration by parts in operator theory is that it shows that the (where ∆ is the Laplace operator) is a positive operator on (see \"L\" space). If \"f\" is smooth and compactly supported then, using integration by parts, we have\n\n\nConsidering a second derivative of formula_92 in the integral on the LHS of the formula for partial integration suggests a repeated application to the integral on the RHS:\n\nExtending this concept of repeated partial integration to derivatives of degree leads to\n\nThis concept may be useful when the successive integrals of formula_95 are readily available (e.g., plain exponentials or sine and cosine, as in Laplace or Fourier transforms), and when the th derivative of formula_96 vanishes (e.g., as a polynomial function with degree formula_97). The latter condition stops the repeating of partial integration, because the RHS-integral vanishes.\n\nIn the course of the above repetition of partial integrations the integrals \nget related. This may be interpreted as arbitrarily \"shifting\" derivatives between formula_92 and formula_96 within the integrand, and proves useful, too (see Rodrigues' formula).\n\nThe essential process of the above formula can be summarized in a table; the resulting method is called \"tabular integration\" and was featured in the film \"Stand and Deliver\".\n\nFor example, consider the integral\n\nBegin to list in column A the function formula_105 and its subsequent derivatives formula_106 until zero is reached. Then list in column B the function formula_107 and its subsequent integrals formula_108 until the size of column B is the same as that of column A. The result is as follows:\n\nThe product of the entries in of columns A and B together with the respective sign give the relevant integrals in in the course of repeated integration by parts. yields the original integral. For the complete result in the must be added to all the previous products () of the of column A and the of column B (i.e., multiply the 1st entry of column A with the 2nd entry of column B, the 2nd entry of column A with the 3rd entry of column B, etc ...) with the given This process comes to a natural halt, when the product, which yields the integral, is zero ( in the example). The complete result is the following (notice the alternating signs in each term):\n\nThis yields\n\nThe repeated partial integration also turns out useful, when in the course of respectively differentiating and integrating the functions formula_106 and formula_108 their product results in a multiple of the original integrand. In this case the repetition may also be terminated with this index This can happen, expectably, with exponentials and trigonometric functions. As an example consider\n\nIn this case the product of the terms in columns A and B with the appropriate sign for index yields the negative of the original integrand (compare \n\nObserving that the integral on the RHS can have its own constant of integration formula_115, and bringing the abstract integral to the other side, gives\n\nand finally:\n\nwhere \"C\" = \"C\"′/2.\n\nThe formula for integration by parts can be extended to functions of several variables. These derivations are analogous to the one given above: a fundamental theorem of calculus is substituted into an appropriate product rule. There are several such pairings possible in multivariate calculus. For example, we may begin with a product rule for divergence followed by the divergence theorem.\n\nA product rule for divergence:\n\nInstead of an interval we integrate over an \"n\"-dimensional domain formula_119: \n\nAfter substitution using the divergence theorem we arrive at:\n\nMore specifically, suppose Ω is an open bounded subset of ℝ with a piecewise smooth boundary Γ. If \"u\" and \"v\" are two continuously differentiable functions on the closure of Ω, then the formula for integration by parts is\nwhere formula_123 is the outward unit surface normal to Γ, formula_124 is its \"i\"-th component, and \"i\" ranges from 1 to \"n\". In vector form, the equation reads\n\nReplacing \"v\" in the component formula with \"v\" and summing over \"i\" gives the vector formula\nwhere v is a vector-valued function with components \"v\", ..., \"v\"\n\nFor formula_127 where formula_128, one gets\nwhich is the first Green's identity.\n\nThe regularity requirements of the theorem can be relaxed. For instance, the boundary Γ need only be Lipschitz continuous. In the first formula above, only \"u\", \"v\" ∈ \"H\"(Ω) is necessary (where \"H\" is a Sobolev space); the other formulas have similarly relaxed requirements.\n\n\n\n"}
{"id": "40752010", "url": "https://en.wikipedia.org/wiki?curid=40752010", "title": "Lagrangian (field theory)", "text": "Lagrangian (field theory)\n\nLagrangian field theory is a formalism in classical field theory. It is the field-theoretic analogue of Lagrangian mechanics. Lagrangian mechanics is used for discrete particles each with a finite number of degrees of freedom. Lagrangian field theory applies to continua and fields, which have an infinite number of degrees of freedom.\n\nThis article uses formula_1 for the Lagrangian density, and \"L\" for the Lagrangian. \n\nThe Lagrangian mechanics formalism was generalized further to handle field theory. In field theory, the independent variable is replaced by an event in spacetime (\"x\", \"y\", \"z\", \"t\"), or more generally still by a point \"s\" on a manifold. The dependent variables (\"q\") are replaced by the value of a field at that point in spacetime formula_2 so that the equations of motion are obtained by means of an action principle, written as:\n\nwhere the \"action\", formula_4, is a functional of the dependent variables formula_5 with their derivatives and \"s\" itself\n\nwhere the brackets denote formula_7;\nand \"s\" = {\"s\"} denotes the set of \"n\" independent variables of the system, including the time variable, and is indexed by \"α\" = 1, 2, 3..., \"n\". Notice that the calligraphic typeface, formula_1, is used to denote volume density, where volume is the integral measure of the domain of the field function, i.e. formula_9.\n\nIn Lagrangian field theory, the Lagrangian as a function of generalized coordinates is replaced by a Lagrangian density, a function of the fields in the system and their derivatives, and possibly the space and time coordinates themselves. In field theory, the independent variable \"t\" is replaced by an event in spacetime (\"x\", \"y\", \"z\", \"t\") or still more generally by a point \"s\" on a manifold. \n\nOften, a \"Lagrangian density\" is simply referred to as a \"Lagrangian\".\n\nFor one scalar field formula_10, the Lagrangian density will take the form: \n\nFor many scalar fields\n\nThe above can be generalized for vector fields, tensor fields, and spinor fields. In physics fermions are described by spinor fields and bosons by tensor fields.\n\nThe time integral of the Lagrangian is called the action denoted by \"S\". In field theory, a distinction is occasionally made between the Lagrangian \"L\", of which the time integral is the action\n\nand the Lagrangian density formula_1, which one integrates over all spacetime to get the action:\n\nThe spatial volume integral of the Lagrangian density is the Lagrangian, in 3d\n\nNote, in the presence of gravity or when using general curvilinear coordinates, the Lagrangian density formula_1 will include a factor of , making it a scalar density. This procedure ensures that the action formula_4 is invariant under general coordinate transformations.\n\nSuppose we have an \"n\"-dimensional manifold, \"M\", and a target manifold, \"T\". Let formula_19 be the configuration space of smooth functions from \"M\" to \"T\".\n\nIn field theory, \"M\" is the spacetime manifold and the target space is the set of values the fields can take at any given point. For example, if there are formula_20 real-valued scalar fields, formula_21, then the target manifold is formula_22. If the field is a real vector field, then the target manifold is isomorphic to formula_23. Note that there is also an elegant formalism for this, using tangent bundles over \"M\".\n\nConsider a functional, \ncalled the action.\n\nIn order for the action to be local, we need additional restrictions on the action. If formula_25, we assume formula_26 is the integral over \"M\" of a function of formula_10, its derivatives and the position called the Lagrangian, formula_28. In other words,\n\nIt is assumed below, in addition, that the Lagrangian depends on only the field value and its first derivative but not the higher derivatives.\n\nGiven boundary conditions, basically a specification of the value of formula_10 at the boundary if \"M\" is compact or some limit on formula_10 as \"x\" → ∞ (this will help in doing integration by parts), the subspace of formula_19 consisting of functions, formula_10, such that all functional derivatives of \"S\" at formula_10 are zero and formula_10 satisfies the given boundary conditions is the subspace of on shell solutions.\n\nFrom this we get: \n\nThe left hand side is the functional derivative of the action with respect to formula_10.\n\nHence we get the Euler–Lagrange equations (due to the boundary conditions):\n\nTo go with the section on test particles above, here are the equations for the fields in which they move. The equations below pertain to the fields in which the test particles described above move and allow the calculation of those fields. The equations below will not give you the equations of motion of a test particle in the field but will instead give you the potential (field) induced by quantities such as mass or charge density at any point formula_39. For example, in the case of Newtonian gravity, the Lagrangian density integrated over spacetime gives you an equation which, if solved, would yield formula_40. This formula_40, when substituted back in equation (), the Lagrangian equation for the test particle in a Newtonian gravitational field, provides the information needed to calculate the acceleration of the particle.\n\nThe density formula_1 has units of J·m. The interaction term \"mΦ\" is replaced by a term involving a continuous mass density \"ρ\" in kg·m. This is necessary because using a point source for a field would result in mathematical difficulties. The resulting Lagrangian for the classical gravitational field is:\n\nwhere \"G\" in m·kg·s is the gravitational constant. Variation of the integral with respect to \"Φ\" gives:\n\nIntegrate by parts and discard the total integral. Then divide out by \"δΦ\" to get:\n\nand thus\n\nwhich yields Gauss's law for gravity.\n\nThe Lagrange density for general relativity in the presence of matter fields is\nformula_48 is the curvature scalar, which is the Ricci tensor contracted with the metric tensor, and the Ricci tensor is the Riemann tensor contracted with a Kronecker delta. The integral of formula_49 is known as the Einstein-Hilbert action. The Riemann tensor is the tidal force tensor, and is constructed out of Christoffel symbols and derivatives of Christoffel symbols, which are the gravitational force field. Plugging this Lagrangian into the Euler-Lagrange equation and taking the metric tensor formula_50 as the field, we obtain the Einstein field equations \nThe last tensor is the energy momentum tensor and is defined by\nformula_53 is the determinant of the metric tensor when regarded as a matrix. formula_54 is the cosmological constant. Generally, in general relativity, the integration measure of the action of Lagrange density is formula_55. This makes the integral coordinate independent, as the root of the metric determinant is equivalent to the Jacobian determinant. The minus sign is a consequence of the metric signature (the determinant by itself is negative).\n\nThe interaction terms \nare replaced by terms involving a continuous charge density ρ in A·s·m and current density formula_57 in A·m. The resulting Lagrangian for the electromagnetic field is:\n\nVarying this with respect to ϕ, we get\n\nwhich yields Gauss' law.\n\nVarying instead with respect to formula_60, we get\n\nwhich yields Ampère's law.\n\nUsing tensor notation, we can write all this more compactly. The term formula_62 is actually the inner product of two four-vectors. We package the charge density into the current 4-vector and the potential into the potential 4-vector. These two new vectors are\nWe can then write the interaction term as\nAdditionally, we can package the E and B fields into what is known as the electromagnetic tensor formula_65.\nWe define this tensor as\nThe term we are looking out for turns out to be\nWe have made use of the Minkowski metric to raise the indices on the EMF tensor. In this notation, Maxwell's equations are\nwhere ε is the Levi-Civita tensor. So the Lagrange density for electromagnetism in special relativity written in terms of Lorentz vectors and tensors is\nIn this notation it is apparent that classical electromagnetism is a Lorentz-invariant theory. By the equivalence principle, it becomes simple to extend the notion of electromagnetism to curved spacetime.\n\nThe Lagrange density of electromagnetism in general relativity also contains the Einstein-Hilbert action from above. The pure electromagnetic Lagrangian is precisely a matter Lagrangian formula_70. The Lagrangian is\n\nThis Lagrangian is obtained by simply replacing the Minkowski metric in the above flat Lagrangian with a more general (possibly curved) metric formula_72. We can generate the Einstein Field Equations in the presence of an EM field using this lagrangian. The energy-momentum tensor is\nIt can be shown that this energy momentum tensor is traceless, i.e. that\nIf we take the trace of both sides of the Einstein Field Equations, we obtain\nSo the tracelessness of the energy momentum tensor implies that the curvature scalar in an electromagnetic field vanishes. The Einstein equations are then\nAdditionally, Maxwell's equations are\nwhere formula_78 is the covariant derivative. For free space, we can set the current tensor equal to zero, formula_79. Solving both Einstein and Maxwell's equations around a spherically symmetric mass distribution in free space leads to the Reissner–Nordström charged black hole, with the defining line element (written in natural units and with charge Q):\n\nOne possible way of unifying the electromagnetic and gravitational Lagrangians (by using a fifth dimension) is given by Kaluza-Klein theory.\n\nUsing differential forms, the electromagnetic action \"S\" in vacuum on a (pseudo-) Riemannian manifold formula_81 can be written (using natural units, ) as\nHere, A stands for the electromagnetic potential 1-form, J is the current 1-form, F is the field strength 2-form and the star denotes the Hodge star operator. This is exactly the same Lagrangian as in the section above, except that the treatment here is coordinate-free; expanding the integrand into a basis yields the identical, lengthy expression. Note that with forms, an additional integration measure is not necessary because forms have coordinate differentials built in. Variation of the action leads to\nThese are Maxwell's equations for the electromagnetic potential. Substituting immediately yields the equation for the fields,\nbecause F is an exact form.\n\nThe Lagrangian density for a Dirac field is:\n\nwhere \"ψ\" is a Dirac spinor (annihilation operator), formula_86 is its Dirac adjoint (creation operator), and formula_87 is Feynman slash notation for formula_88.\n\nThe Lagrangian density for QED is:\n\nwhere formula_90 is the electromagnetic tensor, \"D\" is the gauge covariant derivative, and formula_91 is Feynman notation for formula_92 with formula_93 where formula_94 is the electromagnetic four-potential.\n\nThe Lagrangian density for quantum chromodynamics is:\n\nwhere \"D\" is the QCD gauge covariant derivative, \n\"n\" = 1, 2, ...6 counts the quark types, and formula_96 is the gluon field strength tensor.\n"}
{"id": "39747050", "url": "https://en.wikipedia.org/wiki?curid=39747050", "title": "Lapped transform", "text": "Lapped transform\n\nIn signal processing, a lapped transform is a type of linear discrete block transformation where the basis functions of the transformation overlap the block boundaries, yet the number of coefficients overall resulting from a series of overlapping block transforms remains the same as if a non-overlapping block transform had been used.\n\nLapped transforms substantially reduce the blocking artifacts that otherwise occur with block transform coding techniques, in particular those using the discrete cosine transform. The best known example is the modified discrete cosine transform used in the MP3, Vorbis, AAC, and Opus audio codecs.\n\nAlthough the best-known application of lapped transforms has been for audio coding, they have also been used for video and image coding and various other applications. They are used in video coding for coding I-frames in VC-1 and for image coding in the JPEG XR format. More recently, a form of lapped transform has also been used in the development of the Daala video coding format.\n"}
{"id": "9620408", "url": "https://en.wikipedia.org/wiki?curid=9620408", "title": "Laurence Chisholm Young", "text": "Laurence Chisholm Young\n\nLaurence Chisholm Young (14 July 1905 – 24 December 2000) was an American mathematician known for his contributions to measure theory, the calculus of variations, optimal control theory, and potential theory. He was the son of William Henry Young and Grace Chisholm Young, both prominent mathematicians.\n\nThe concept of Young measure is named after him: he also introduced the concept of the generalized curve and a concept of generalized surface which later evolved in the concept of varifold. The Young integral also is named after him and has now been generalised in the theory of rough paths.\n\nLaurence Chisholm Young was born in Göttingen, the fifth of the six children of William Henry Young and Grace Chisholm Young.\n\n\n\n\n\n"}
{"id": "7049330", "url": "https://en.wikipedia.org/wiki?curid=7049330", "title": "Leabra", "text": "Leabra\n\nLeabra stands for local, error-driven and associative, biologically realistic algorithm. It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in \"emergent\" (successor of PDP++) when making a new project, and is extensively used in various simulations.\n\nHebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.\n\nError-driven learning is performed using GeneRec, which is a generalization of the recirculation algorithm, and approximates Almeida–Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.\n\nThe activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output.\n\nLayer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations.\n\nThe net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections.\n\nDocumentation about this algorithm can be found in the book \"Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain\" published by MIT press. and in the Emergent Documentation\n\nThe pseudocode for Leabra is given here, showing exactly how the\npieces of the algorithm described in more detail in the subsequent\nsections fit together.\n\ncodice_1\n\nEmergent is the original implementation of Leabra, written in C++ and highly optimized. This is the fastest implementation, suitable for constructing large networks. Although \"emergent\" has a graphical user interface, it is very complex and has a steep learning curve.\n\nIf you want to understand the algorithm in detail, it will be easier to read non-optimzed code. For this purpose, check out the MATLAB version. There is also an R version available, that can be easily installed via codice_2 in R and has a short introduction to how the package is used. The MATLAB and R versions are not suited for constructing very large networks, but they can be installed quickly and (with some programming background) are easy to use. Furthermore, they can also be adapted easily.\n\n\n"}
{"id": "1976420", "url": "https://en.wikipedia.org/wiki?curid=1976420", "title": "Littlewood–Offord problem", "text": "Littlewood–Offord problem\n\nIn mathematical field of combinatorial geometry, the Littlewood–Offord problem is the problem of determining the number of subsums of a set of vectors that fall in a given convex set. More formally, if \"V\" is a vector space of dimension \"d\", the problem is to determine, given a finite subset of vectors \"S\" and a convex subset \"A\", the number of subsets of \"S\" whose summation is in \"A\".\n\nThe first upper bound for this problem was proven (for \"d\" = 1 and \"d\" = 2) in 1938 by John Edensor Littlewood and A. Cyril Offord. This Littlewood–Offord lemma states that if \"S\" is a set of \"n\" real or complex numbers of absolute value at least one and \"A\" is any disc of radius one, then not more than formula_1 of the 2 possible subsums of \"S\" fall into the disc.\n\nIn 1945 Paul Erdős improved the upper bound for \"d\" = 1 to\nusing Sperner's theorem. This bound is sharp; equality is attained when all vectors in \"S\" are equal. In 1966, Kleitman showed that the same bound held for complex numbers. In 1970, he extended this to the setting when \"V\" is a normed space.\n\nSuppose \"S\" = {\"v\", …, \"v\"}. By subtracting\nfrom each possible subsum (that is, by changing the origin and then scaling by a factor of 2), the Littlewood–Offord problem is equivalent to the problem of determining the number of sums of the form\nthat fall in the target set \"A\", where formula_5 takes the value 1 or −1. This makes the problem into a probabilistic one, in which the question is of the distribution of these \"random vectors\", and what can be said knowing nothing more about the \"v\".\n"}
{"id": "8481660", "url": "https://en.wikipedia.org/wiki?curid=8481660", "title": "Loss–DiVincenzo quantum computer", "text": "Loss–DiVincenzo quantum computer\n\nThe Loss–DiVincenzo quantum computer (or spin-qubit quantum computer) is a scalable semiconductor-based quantum computer proposed by Daniel Loss and David P. DiVincenzo in 1997. The proposal was to use as qubits the intrinsic spin-1/2 degree of freedom of individual electrons confined to quantum dots. This was done in a way that fulfilled DiVincenzo Criteria for a scalable quantum computer, namely:\n\n\nA candidate for such a quantum computer is a lateral quantum dot system.\n\nThe Loss–DiVincenzo quantum computer operates, basically, using inter-dot gate voltage for implementing Swap (computer science) operations and local magnetic fields (or any other local spin manipulation) for implementing the Controlled NOT gate (CNOT gate). \n\nThe Swap operation is achieved by applying a pulsed inter-dot gate voltage, so the exchange constant in the Heisenberg Hamiltonian becomes time-dependent:\n\nThis description is only valid if:\n\n\nFrom the pulsed Hamiltonian follows the time evolution operator\n\nWe can choose a specific duration of the pulse such that the integral in time over formula_9 gives formula_10 and formula_11 becomes the Swap operator formula_12.\n\nThe XOR gate may be achieved by combining formula_13 (square root of Swap) operations with individual spin operations:\n\nThis operator gives a conditional phase for the state in the basis of formula_15.\n\n"}
{"id": "2164767", "url": "https://en.wikipedia.org/wiki?curid=2164767", "title": "Mathemagician", "text": "Mathemagician\n\nA mathemagician is a mathematician who is also a magician.\n\nThe name \"mathemagician\" was probably first applied to Martin Gardner, but has since been used to describe many mathematician/magicians, including Arthur T. Benjamin, Persi Diaconis, and Colm Mulcahy. Diaconis has suggested that the reason so many mathematicians are magicians is that \"inventing a magic trick and inventing a theorem are very similar activities.\"\n\nA great number of self-working mentalism tricks rely on mathematical principles. Max Maven often utilizes this type of magic in his performance.\n\n\n"}
{"id": "47304362", "url": "https://en.wikipedia.org/wiki?curid=47304362", "title": "Mathematical sculpture", "text": "Mathematical sculpture\n\nA mathematical sculpture is a sculpture which uses mathematics as an essential conception. Helaman Ferguson, George W. Hart, Bathsheba Grossman, Peter Forakis and Jacobus Verhoeff are well-known mathematical sculptors.\n"}
{"id": "4889209", "url": "https://en.wikipedia.org/wiki?curid=4889209", "title": "Mike Young (Neighbours)", "text": "Mike Young (Neighbours)\n\nMike Young is a fictional character from the Australian soap opera \"Neighbours\", played by Guy Pearce. He made his first on-screen appearance on 20 January 1986. Mike's storylines included being physically abused by his father, moving in with Des and Daphne Clarke, making friends with Charlene Mitchell and Scott Robinson, his relationship with Jane Harris and becoming a teacher. Mike departed Erinsborough to be with his mother on 6 December 1989.\n\nWhen he was 18, Pearce's drama teacher advised him to write to TV companies asking for auditions and he wrote to the Grundy Organisation, which produced \"Neighbours\" at the time. Pearce was then cast as the troubled and lonely Mike Young. Following his last year 12 exam, he began filming on 3 December 1985. Pearce's debut was in episode number 171, the first episode broadcast on Network Ten following the show's move from Seven Network.\n\nIn 1989, the \"Neighbours\" producers did not want Pearce to play Errol Flynn in a biopic film and Pearce decided to leave the show. Of being a part of \"Neighbours\", Pearce said \"I experienced hysteria at a pretty high pitch with that show\". Pearce also added \"I'm not embarrassed by having done it now, not at all. It was an amazing experience, an amazing opportunity. And I was only 18.\"\n\nMike and his mother, Barbara (Rona McLeod; Diana Greentree), lived in fear of being beaten and abused by Mike's father, David (Stewart Faichney), throughout Mike's young life. This meant Mike grew up into a lonely, quiet young man who did not socialise much until he began attending Erinsborough High School and becomes good friends with Charlene Mitchell (Kylie Minogue) and Scott Robinson (Jason Donovan).\n\nThrough his friendship with Scott, Mike comes to Ramsay Street and befriends Scott's neighbour, Daphne Clarke (Elaine Smith), who cares for him, later giving him a part-time job at her coffee shop. Mike opens up to Daphne and Des (Paul Keane) about his problems at home with his father and Daphne tries to get Mike and Barbara away from David. However, Barbara is too scared of her husband and she does not want to leave, but Mike is determined to go and moves in with Des and Daphne when they offer to become his legal guardians. Mike briefly dates Scott's cousin Nikki Dennison (Charlene Fenn), but he later falls for Jane Harris (Annie Jones) when she moves in with her grandmother, Nell Mangel (Vivean Gray). Jane falls for Mike straight away, but it takes a while before Mike realises his feelings due to Jane's plain image. Helen Daniels (Anne Haddy) and Daphne give her a makeover for a school dance, which consists of replacing her glasses with contacts, a new haircut and makeup. Mike likes her new image and they begin dating. Mrs Mangel is not happy that her granddaughter is dating Mike and when she receives letters about Mike's reputation with other girls, Mrs Mangel stops Jane from seeing him. Daphne eventually catches school bully, Sue Parker (Kate Gorman), posting the letters and Sue explains that she is jealous of Mike and Jane's relationship. Mrs Mangel then lets Jane and Mike continue dating. When Nikki returns to Ramsay Street, Mike helps comfort her when she discovers her mother is ill, leading Jane to become jealous of their friendship. Mike becomes jealous when Shane Ramsay (Peter O'Brien) shows an attraction to Jane, but she tells Mike that he is the only one for her.\n\nFollowing their final exams, Jane focuses on her modelling career and Mike decides to become a teacher. As they are leading separate lives, Jane and Mike split up amicably and remain friends. Not long after, Daphne is killed in a car crash and Mike is left feeling guilty as he had not been around for a few weeks. Mike is angry and upset and he finds the two men who had crashed into Daphne's car. He attacks them and is later arrested. When Mike finishes university, he gets a job teaching Maths at Erinsborough High. Mike becomes close to one of his students, Jessie Ross (Michelle Kearley), who admits that she has an abusive father too. Mike confronts her father, Ted (Doug Bennett), but soon learns that her mother Adele (Marian Sinclair) is the one abusing her. Mike becomes close to Jessie and they share a kiss, which is witnessed by principal Kenneth Muir (Roger Boyce). Mr Muir suspends Mike, who decides to leave Erinsborough for a while.\n\nOn his return, Mike finds Des and Jane have forged a strong friendship. Mike becomes moody as he settles back in. When Jenny Owens (Danielle Carter) comes to see him, it is revealed that Mike and Jenny took a ride on his motorbike and had an accident. Jenny fell from the bike and was left paralysed and using a wheelchair for the rest of her life. Mike blames himself for Jenny's condition and could not bring himself to accept that the event was an accident. Jenny eventually convinces him that it was not his fault. At the same time, Des and Jane begin dating and Mike is disgusted with the both of them. He refuses to accept the relationship and leaves Erinsborough again. He returns in the middle of Des and Jane's engagement party and he interrupts it. Mike eventually accepts that Des and Jane love each other and gives them his blessing. Mike begins to feel like there is not much left for him in Erinsborough and when he hears that his mother had been in a plane crash, and with his father long dead, Mike decides to leave Ramsay Street and join her to help her recovery.\n\nA writer for the BBC's \"Neighbours\" website stated that Mike's most notable moment was when he saw Jane's \"transformation at the school dance.\" In 2010, to celebrate Neighbours' 25th anniversary Sky, a British satellite broadcasting company, profiled 25 characters of which they believed were the most memorable in the series history. Mike is in the list and describing him they state: \"The Erinsborough economy is studied the world over for being the only one in the world that can sustained on just four professions: you can work in 'business', or be a journalist, a doctor, or a teacher. Mike chose the latter, probably because he was the nice, fairly quiet boy out of the legendary original set of teens on the show. His romance with plain Jane Harris unfortunately ended up with her engaged to his father figure of a friend, Des, one of many reasons why he left the show to care for his sick mother.\" \"Heat\" magazine called Mike \"cheesy, but gorgeous\". Lorna Cooper of MSN TV has listed Mike as one of soap opera's forgotten characters and claims he is a favourite out of the golden era of the serial. Orange UK describe Mike as one of the serial's \"hottest spunks\". LoveFilm describe Mike's storylines as serious and give him the nickname \"motorbike Mike\".\n\n"}
{"id": "13099198", "url": "https://en.wikipedia.org/wiki?curid=13099198", "title": "Model-driven integration", "text": "Model-driven integration\n\nIn software design, model-driven integration is a subset of model-driven architecture (MDA) which focuses purely on solving Application Integration problems using executable Unified Modeling Language (UML).\n\n"}
{"id": "766409", "url": "https://en.wikipedia.org/wiki?curid=766409", "title": "Network theory", "text": "Network theory\n\nNetwork theory is the study of graphs as a representation of either symmetric relations or asymmetric relations between discrete objects. In computer science and network science, network theory is a part of graph theory: a network can be defined as a graph in which nodes and/or edges have attributes (e.g. names).\n\nNetwork theory has applications in many disciplines including statistical physics, particle physics, computer science, electrical engineering, biology, economics, finance, operations research, climatology and sociology. Applications of network theory include logistical networks, the World Wide Web, Internet, gene regulatory networks, metabolic networks, social networks, epistemological networks, etc.; see List of network theory topics for more examples.\n\nEuler's solution of the Seven Bridges of Königsberg problem is considered to be the first true proof in the theory of networks.\n\nNetwork problems that involve finding an optimal way of doing something are studied under the name combinatorial optimization. Examples include network flow, shortest path problem, transport problem, transshipment problem, location problem, matching problem, assignment problem, packing problem, routing problem, critical path analysis and PERT (Program Evaluation & Review Technique). In order to break a NP-hard task of network optimization down into subtasks the network is decomposed into relatively independent subnets.\n\nThe electric power systems analysis could be conducted using network theory from two main points of view: \n\n(1) an abstract perspective (i.e., as a graph consists from nodes and edges), regardless of the electric power aspects (e.g., transmission line impedances). Most of these studies focus only on the abstract structure of the power grid using node degree distribution and betweenness distribution, which introduces substantial insight regarding the vulnerability assessment of the grid. Through these types of studies, the category of the grid structure could be identified from the complex network perspective (e.g., single-scale, scale-free). This classification might help the electric power system engineers in the planning stage or while upgrading the infrastructure (e.g., add a new transmission line) to maintain a proper redundancy level in the transmission system.\n\n(2) weighted graphs that blend an abstract understanding of complex network theories and electric power systems properties.\n\nSocial network analysis examines the structure of relationships between social entities. These entities are often persons, but may also be groups, organizations, nation states, web sites, or scholarly publications.\n\nSince the 1970s, the empirical study of networks has played a central role in social science, and many of the mathematical and statistical tools used for studying networks have been first developed in sociology. Amongst many other applications, social network analysis has been used to understand the diffusion of innovations, news and rumors. Similarly, it has been used to examine the spread of both diseases and health-related behaviors. It has also been applied to the study of markets, where it has been used to examine the role of trust in exchange relationships and of social mechanisms in setting prices. Similarly, it has been used to study recruitment into political movements and social organizations. It has also been used to conceptualize scientific disagreements as well as academic prestige. More recently, network analysis (and its close cousin traffic analysis) has gained a significant use in military intelligence, for uncovering insurgent networks of both hierarchical and leaderless nature.\n\nWith the recent explosion of publicly available high throughput biological data, the analysis of molecular networks has gained significant interest. The type of analysis in this context is closely related to social network analysis, but often focusing on local patterns in the network. For example, network motifs are small subgraphs that are over-represented in the network. Similarly, activity motifs are patterns in the attributes of nodes and edges in the network that are over-represented given the network structure. The analysis of biological networks with respect to diseases has led to the development of the field of network medicine. Recent examples of application of network theory in biology include applications to understanding the cell cycle.\nThe interactions between physiological systems like brain, heart, eyes, etc. can be regarded as a physiological network.\n\nThe automatic parsing of \"textual corpora\" has enabled the extraction of actors and their relational networks on a vast scale. The resulting narrative networks, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by Quantitative Narrative Analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.\n\nLink analysis is a subset of network analysis, exploring associations between objects. An example may be examining the addresses of suspects and victims, the telephone numbers they have dialed and financial transactions that they have partaken in during a given timeframe, and the familial relationships between these subjects as a part of police investigation. Link analysis here provides the crucial relationships and associations between very many objects of different types that are not apparent from isolated pieces of information. Computer-assisted or fully automatic computer-based link analysis is increasingly employed by banks and insurance agencies in fraud detection, by telecommunication operators in telecommunication network analysis, by medical sector in epidemiology and pharmacology, in law enforcement investigations, by search engines for relevance rating (and conversely by the spammers for spamdexing and by business owners for search engine optimization), and everywhere else where relationships between many objects have to be analyzed. Links are also derived from similarity of time behavior in both nodes. Examples include climate networks where the links between two locations (nodes) are determined for example, by the similarity of the rainfall or temperature fluctuations in both sites.\n\nThe structural robustness of networks is studied using percolation theory. When a critical fraction of nodes (or links) is removed the network becomes fragmented into small disconnected clusters. This phenomenon is called percolation, and it represents an order-disorder type of phase transition with critical exponents. Percolation theory can predict the size of the largest component (called giant component), the critical threshold and the critical exponents.\n\nSeveral Web search ranking algorithms use link-based centrality metrics, including Google's PageRank, Kleinberg's HITS algorithm, the CheiRank and TrustRank algorithms. Link analysis is also conducted in information science and communication science in order to understand and extract information from the structure of collections of web pages. For example, the analysis might be of the interlinking between politicians' web sites or blogs. Another use is for classifying pages according to their mention in other pages.\n\nInformation about the relative importance of nodes and edges in a graph can be obtained through centrality measures, widely used in disciplines like sociology. For example, eigenvector centrality uses the eigenvectors of the adjacency matrix corresponding to a network, to determine nodes that tend to be frequently visited. Formally established measures of centrality are degree centrality, closeness centrality, betweenness centrality, eigenvector centrality, subgraph centrality and Katz centrality. The purpose or objective of analysis generally determines the type of centrality measure to be used. For example, if one is interested in dynamics on networks or the robustness of a network to node/link removal, often the dynamical importance of a node is the most relevant centrality measure.For a centrality measure based on k-core analysis see ref.\n\nThese concepts are used to characterize the linking preferences of hubs in a network. Hubs are nodes which have a large number of links. Some hubs tend to link to other hubs while others avoid connecting to hubs and prefer to connect to nodes with low connectivity. We say a hub is assortative when it tends to connect to other hubs. A disassortative hub avoids connecting to other hubs. If hubs have connections with the expected random probabilities, they are said to be neutral. There are three methods to quantify degree correlations.\n\nThe recurrence matrix of a recurrence plot can be considered as the adjacency matrix of an undirected and unweighted network. This allows for the analysis of time series by network measures. Applications range from detection of regime changes over characterizing dynamics to synchronization analysis.\n\nContent in a complex network can spread via two major methods: conserved spread and non-conserved spread. In conserved spread, the total amount of content that enters a complex network remains constant as it passes through. The model of conserved spread can best be represented by a pitcher containing a fixed amount of water being poured into a series of funnels connected by tubes . Here, the pitcher represents the original source and the water is the content being spread. The funnels and connecting tubing represent the nodes and the connections between nodes, respectively. As the water passes from one funnel into another, the water disappears instantly from the funnel that was previously exposed to the water. In non-conserved spread, the amount of content changes as it enters and passes through a complex network. The model of non-conserved spread can best be represented by a continuously running faucet running through a series of funnels connected by tubes. Here, the amount of water from the original source is infinite. Also, any funnels that have been exposed to the water continue to experience the water even as it passes into successive funnels. The non-conserved model is the most suitable for explaining the transmission of most infectious diseases, neural excitation, information and rumors, etc.\n\nAn interdependent network is a system of coupled networks where nodes of one or more networks depend on nodes in other networks. Such dependencies are enhanced by the developments in modern technology. Dependencies may lead to cascading failures between the networks and a relatively small failure can lead to a catastrophic breakdown of the system. Blackouts are a fascinating demonstration of the important role played by the dependencies between networks. A recent study developed a framework to study the cascading failures in an interdependent networks system.\n\n\n\n"}
{"id": "1145820", "url": "https://en.wikipedia.org/wiki?curid=1145820", "title": "Online codes", "text": "Online codes\n\nIn computer science, online codes are an example of rateless erasure codes. These codes can encode a message into a number of symbols such that knowledge of any fraction of them allows one to recover the original message (with high probability). \"Rateless\" codes produce an arbitrarily large number of symbols which can be broadcast until the receivers have enough symbols.\n\nThe online encoding algorithm consists of several phases. First the message is split into \"n\" fixed size message blocks. Then the \"outer encoding\" is an erasure code which produces auxiliary blocks that are appended to the message blocks to form a composite message.\n\nFrom this the inner encoding generates check blocks. Upon receiving a certain number of check blocks some fraction of the composite message can be recovered. Once enough has been recovered the outer decoding can be used to recover the original message.\n\nOnline codes are parameterised by the block size and two scalars, \"q\" and \"ε\". The authors suggest \"q\"=3 and ε=0.01. These parameters set the balance between the complexity and performance of the encoding. A message of \"n\" blocks can be recovered, with high probability, from (1+3ε)\"n\" check blocks. The probability of failure is (ε/2).\n\nAny erasure code may be used as the outer encoding, but the author of online codes suggest the following.\n\nFor each message block, pseudo-randomly choose \"q\" auxiliary blocks \n(from a total of 0.55\"q\"ε\"n\" auxiliary blocks) to attach it to. Each auxiliary block is then the XOR of all the message blocks which have been attached to it.\n\nThe inner encoding takes the composite message and generates a stream of check blocks. A check block is the XOR of all the blocks from the composite message that it is attached to.\n\nThe \"degree\" of a check block is the number of blocks that it is attached to. The degree is determined by sampling a random distribution, \"p\", which is defined as:\n\nOnce the degree of the check block is known, the blocks from the composite message which it is attached to are chosen uniformly.\n\nObviously the decoder of the inner stage must hold check blocks which it cannot currently decode. A check block can only be decoded when all but one of the blocks which it is attached to are known. The graph to the left shows the progress of an inner decoder. The x-axis plots the number of check blocks received and the dashed line shows the number of check blocks which cannot currently be used. This climbs almost linearly at first as many check blocks with degree > 1 are received but unusable. At a certain point, some of the check blocks are suddenly usable, resolving more blocks which then causes more check blocks to be usable. Very quickly the whole file can be decoded.\n\nAs the graph also shows the inner decoder falls just shy of decoding everything for a little while after having received \"n\" check blocks. The outer encoding ensures that a few elusive blocks from the inner decoder are not an issue, as the file can be recovered without them.\n\n"}
{"id": "31776869", "url": "https://en.wikipedia.org/wiki?curid=31776869", "title": "Palm calculus", "text": "Palm calculus\n\nIn the study of stochastic processes, Palm calculus, named after Swedish teletrafficist Conny Palm, is the study of the relationship between probabilities conditioned on a specified event and time-average probabilities. A Palm probability or Palm expectation, often denoted formula_1 or formula_2, is a probability or expectation conditioned on a specified event occurring at time 0.\n\nA simple example of a formula from Palm calculus is Little's law formula_3, which states that the time-average number of users (\"L\") in a system is equal to the product of the rate (formula_4) at which users arrive and the Palm-average waiting time (\"W\") that a user spends in the system. That is, the average \"W\" gives equal weight to the waiting time of all customers, rather than being the time-average of \"the waiting times of the customers currently in the system\".\n\nAn important example of the use of Palm probabilities is Feller's paradox, often associated with the analysis of an M/G/1 queue. This states that the (time-)average time between the previous and next points in a point process is greater than the expected interval between points. The latter is the Palm expectation of the former, conditioning on the event that a point occurs at the time of the observation. This paradox occurs because large intervals are given greater weight in the time average than small intervals.\n"}
{"id": "47661313", "url": "https://en.wikipedia.org/wiki?curid=47661313", "title": "Parareal", "text": "Parareal\n\nParareal is a parallel algorithm from numerical analysis and used for the solution of initial value problems.\nIt has been introduced in 2001 by Lions, Maday and Turinici. Since then, it has become one of the most widely studied parallel-in-time integration methods .\n\nIn contrast to e.g. Runge-Kutta or multi-step methods, some of the computations in Parareal can be performed in parallel and\nParareal is therefore one example of a \"parallel-in-time\" integration method.\nWhile historically most efforts to parallelize the numerical solution of partial differential equations focussed on the spatial discretization, in view of the challenges from exascale computing, parallel methods for temporal discretization have been identified as a possible way to increase concurrency in numerical software.\nBecause Parareal computes the numerical solution for multiple time steps in parallel, it is categorized as a \"parallel across the steps\" method.\nThis is in contrast to approaches using \"parallelism across the method\" like parallel Runge-Kutta or extrapolation methods, where independent stages can be computed in parallel or \"parallel across the system\" methods like waveform relaxation.\n\nParareal can be derived as both a multigrid method in time method or as multiple shooting along the time axis.\nBoth ideas, multigrid in time as well as adopting multiple shooting for time integration, go back to the 1980s and 1990s.\nParareal is a widely studied method and has been used and modified for a range of different applications.\nIdeas to parallelize the solution of initial value problems go back even further: the first paper proposing a parallel-in-time integration method appeared in 1964.\n\nParareal solves an initial value problem of the form\n\nformula_1\n\nHere, the right hand side formula_2 can correspond to the spatial discretization of a partial differential equation in a method of lines approach.\n\nParareal now requires a decomposition of the time interval formula_3 into formula_4 so-called time slices formula_5 such that\n\nformula_6\n\nEach time slice is assigned to one processing unit when parallelizing the algorithm, so that formula_4 is equal to the number of processing units used for Parareal: in an MPI based code for example, this would be the number of processes, while in an OpenMP based code, formula_4 would be equal to the number of threads.\n\nParareal is based on the iterative application of two methods for integration of ordinary differential equations.\nOne, commonly labelled formula_9, should be of high accuracy and computational cost while the other, typically labelled formula_10, must be computationally cheap but can be much less accurate. \nTypically, some form of Runge-Kutta method is chosen for both coarse and fine integrator, where formula_10 might be of lower order and use a larger time step than formula_9.\nIf the initial value problem stems from the discretization of a PDE, formula_10 can also use a coarser spatial discretization, but this can negatively impact convergence unless high order interpolation is used.\nThe result of numerical integration with one of these methods over a time slice formula_14 for some starting value formula_15 given at formula_16 is then written as\n\nformula_17 or formula_18.\n\nSerial time integration with the fine method would then correspond to a step-by-step computation of\n\nformula_19\n\nParareal instead uses the following iteration\n\nformula_20\n\nwhere formula_21 is the iteration counter. \nAs the iteration converges and formula_22, the terms from the coarse method cancel out and Parareal reproduces the solution that is obtained by the serial execution of the fine method only.\nIt can be shown that Parareal converges after a maximum of formula_4 iterations.\nFor Parareal to provide speedup, however, it has to converge in a number of iterations significantly smaller than the number of time slices, that is formula_24.\n\nIn the Parareal iteration, the computationally expensive evaluation of formula_25 can be performed in parallel on formula_4 processing units.\nBy contrast, the dependency of formula_27 on formula_28 means that the coarse correction has to be computed in serial order.\n\nUnder some assumptions, a simple theoretical model for the speedup of Parareal can be derived.\nAlthough in applications these assumptions can be too restrictive, the model still is useful to illustrate the trade offs that are involved in obtaining speedup with Parareal.\n\nFirst, assume that every time slice formula_5 consists of exactly formula_30 steps of the fine integrator and of formula_31 steps of the coarse integrator.\nThis includes in particular the assumption that all time slices are of identical length and that both coarse and fine integrator use a constant step size over the full simulation.\nSecond, denote by formula_32 and formula_33 the computing time required for a single step of the fine and coarse methods, respectively, and assume that both are constant.\nThis is typically not exactly true when an implicit method is used, because then runtimes vary depending on the number of iterations required by the iterative solver.\n\nUnder these two assumptions, the runtime for the fine method integrating over formula_4 time slices can be modelled as\n\nformula_35\n\nThe runtime of Parareal using formula_4 processing units and performing formula_37 iterations is\n\nformula_38\n\nSpeedup of Parareal then is\n\nformula_39\n\nThese two bounds illustrate the trade off that has to be made in choosing the coarse method: on the one hand, it has to be cheap and/or use a much larger time step to make the first bound as large as possible, on the other hand the number of iterations formula_37 has to be kept low to keep the second bound large.\nIn particular, Parareal's parallel efficiency is bounded by\n\nformula_41\n\nthat is by the inverse of the number of required iterations.\n\nThe vanilla version of Parareal has issues for problems with imaginary eigenvalues. It typically only converges toward the very last iterations, that is as formula_21 approaches formula_4, and the speedup formula_44 is always going to be smaller than one. So either the number of iterations is small and Parareal is unstable or, if formula_21 is large enough to make Parareal stable, no speedup is possible. This also means that Parareal is typically unstable for hyperbolic equations. Even though the formal analysis by Gander and Vandewalle covers only linear problems with constant coefficients, the problem also arises when Parareal is applied to the nonlinear Navier-Stokes equations when the viscosity coefficient becomes too small and the Reynolds number too large. Different approaches exist to stabilise Parareal, one being Krylov-subspace enhanced Parareal.\n\nThere are multiple algorithms that are directly based or at least inspired by the original Parareal algorithm.\n\nEarly on it was recognised that for linear problems information generated by the fine method formula_46 can be used to improve the accuracy of the coarse method formula_47. Originally, the idea was formulated for the parallel implicit time-integrator PITA, a method closely related to Parareal but with small differences in how the correction is done. In every iteration formula_21 the result formula_49 is computed for values formula_50 for formula_51. Based on this information, the subspace\n\nformula_52\n\nis defined and updated after every Parareal iteration. Denote as formula_53 the orthogonal projection from formula_54 to formula_55. Then, replace the coarse method with the improved integrator formula_56.\n\nAs the number of iterations increases, the space formula_55 will grow and the modified propagator formula_58 will become more accurate. This will lead to faster convergence. This version of Parareal can also stably integrate linear hyperbolic partial differential equations. An extension to nonlinear problems based on the reduced basis method exists as well.\n\nA method with improved parallel efficiency based on a combination of Parareal with spectral deferred corrections (SDC) has been proposed by M. Minion. It limits the choice for coarse and fine integrator to SDC, sacrificing flexibility for improved parallel efficiency. Instead of the limit of formula_59, the bound on parallel efficiency in the hybrid method becomes\n\nformula_60\n\nwith formula_61 being the number of iterations of the serial SDC base method and formula_62 the typically greater number of iterations of the parallel hybrid method. The Parareal-SDC hybrid has been further improved by addition of a \"full approximation scheme\" as used in nonlinear multigrid. This led to the development of the \"parallel full approximation scheme in space and time\" (PFASST). Performance of PFASST has been studied for PEPC, a Barnes-Hut tree code based particle solver developed at Juelich Supercomputing Centre. Simulations using all 262,144 cores on the IBM BlueGene/P system JUGENE showed that PFASST could produce additional speedup beyond saturation of the spatial tree parallelisation.\n\nThe multigrid reduction in time method (MGRIT) generalises the interpretation of Parareal as a multigrid-in-time algorithms to multiple levels using different smoothers. It is a more general approach but for a specific choice of parameters it is equivalent to Parareal. The XBraid library implementing MGRIT is being developed by Lawrence Livermore National Laboratory.\n\nParaExp uses exponential integrators within Parareal. While limited to linear problems, it can produce almost optimal parallel speedup.\n\n"}
{"id": "1490148", "url": "https://en.wikipedia.org/wiki?curid=1490148", "title": "Perturbation (astronomy)", "text": "Perturbation (astronomy)\n\nIn astronomy, perturbation is the complex motion of a massive body subject to forces other than the gravitational attraction of a single other massive body. The other forces can include a third (fourth, fifth, etc.) body, resistance, as from an atmosphere, and the off-center attraction of an oblate or otherwise misshapen body.\n\nThe study of perturbations began with the first attempts to predict planetary motions in the sky. In ancient times the causes were a mystery. Newton, at the time he formulated his laws of motion and of gravitation, applied them to the first analysis of perturbations, recognizing the complex difficulties of their calculation. Many of the great mathematicians since then have given attention to the various problems involved; throughout the 18th and 19th centuries there was demand for accurate tables of the position of the Moon and planets for marine navigation.\n\nThe complex motions of gravitational perturbations can be broken down. The hypothetical motion that the body follows under the gravitational effect of one other body only is typically a conic section, and can be readily described with the methods of geometry. This is called a two-body problem, or an unperturbed Keplerian orbit. The differences between that and the actual motion of the body are perturbations due to the additional gravitational effects of the remaining body or bodies. If there is only one other significant body then the perturbed motion is a three-body problem; if there are multiple other bodies it is an \"n\"-body problem. A general analytical solution (a mathematical expression to predict the positions and motions at any future time) exists for the two-body problem; when more than two bodies are considered analytic solutions exist only for special cases. Even the two-body problem becomes insoluble if one of the bodies is irregular in shape.\n\nMost systems that involve multiple gravitational attractions present one primary body which is dominant in its effects (for example, a star, in the case of the star and its planet, or a planet, in the case of the planet and its satellite). The gravitational effects of the other bodies can be treated as perturbations of the hypothetical unperturbed motion of the planet or satellite around its primary body.\n\nIn methods of general perturbations, general differential equations, either of motion or of change in the orbital elements, are solved analytically, usually by series expansions. The result is usually expressed in terms of algebraic and trigonometric functions of the orbital elements of the body in question and the perturbing bodies. This can be applied generally to many different sets of conditions, and is not specific to any particular set of gravitating objects. Historically, general perturbations were investigated first. The classical methods are known as \"variation of the elements\", \"variation of parameters\" or \"variation of the constants of integration\". In these methods, it is considered that the body is always moving in a conic section, however the conic section is constantly changing due to the perturbations. If all perturbations were to cease at any particular instant, the body would continue in this (now unchanging) conic section indefinitely; this conic is known as the osculating orbit and its orbital elements at any particular time are what are sought by the methods of general perturbations.\n\nGeneral perturbations takes advantage of the fact that in many problems of celestial mechanics, the two-body orbit changes rather slowly due to the perturbations; the two-body orbit is a good first approximation. General perturbations is applicable only if the perturbing forces are about one order of magnitude smaller, or less, than the gravitational force of the primary body. In the Solar System, this is usually the case; Jupiter, the second largest body, has a mass of about 1/1000 that of the Sun.\n\nGeneral perturbation methods are preferred for some types of problems, as the source of certain observed motions are readily found. This is not necessarily so for special perturbations; the motions would be predicted with similar accuracy, but no information on the configurations of the perturbing bodies (for instance, an orbital resonance) which caused them would be available.\n\nIn methods of special perturbations, numerical datasets, representing values for the positions, velocities and accelerative forces on the bodies of interest, are made the basis of numerical integration of the differential equations of motion. In effect, the positions and velocities are perturbed directly, and no attempt is made to calculate the curves of the orbits or the orbital elements.\n\nSpecial perturbations can be applied to any problem in celestial mechanics, as it is not limited to cases where the perturbing forces are small. Once applied only to comets and minor planets, special perturbation methods are now the basis of the most accurate machine-generated planetary ephemerides of the great astronomical almanacs. Special perturbations are also used for modeling an orbit with computers.\n\nCowell's formulation (so named for Philip H. Cowell, who, with A.C.D. Cromellin, used a similar method to predict the return of Halley's comet) is perhaps the simplest of the special perturbation methods. In a system of formula_1 mutually interacting bodies, this method mathematically solves for the Newtonian forces on body formula_2 by summing the individual interactions from the other formula_3 bodies:\n\nwhere formula_5 is the acceleration vector of body formula_2, formula_7 is the gravitational constant, formula_8 is the mass of body formula_3, formula_10 and formula_11 are the position vectors of objects formula_2 and formula_3 respectively, and formula_14 is the distance from object formula_2 to object formula_3. All vectors being referred to the barycenter of the system. This equation is resolved into components in formula_17, formula_18, and formula_19 and these are integrated numerically to form the new velocity and position vectors. This process is repeated as many times as necessary. The advantage of Cowell's method is ease of application and programming. A disadvantage is that when perturbations become large in magnitude (as when an object makes a close approach to another) the errors of the method also become large. \nHowever, for many problems in celestial mechanics, this is never the case. Another disadvantage is that in systems with a dominant central body, such as the Sun, it is necessary to carry many significant digits in the arithmetic because of the large difference in the forces of the central body and the perturbing bodies, although with modern computers this is not nearly the limitation it once was.\n\nEncke's method begins with the osculating orbit as a reference and integrates numerically to solve for the variation from the reference as a function of time. \nIts advantages are that perturbations are generally small in magnitude, so the integration can proceed in larger steps (with resulting lesser errors), and the method is much less affected by extreme perturbations. Its disadvantage is complexity; it cannot be used indefinitely without occasionally updating the osculating orbit and continuing from there, a process known as \"rectification\". Encke's method is similar to the general perturbation method of variation of the elements, except the rectification is performed at discrete intervals rather than continuously.\n\nLetting formula_20 be the radius vector of the osculating orbit, formula_21 the radius vector of the perturbed orbit, and formula_22 the variation from the osculating orbit,\n\nformula_23 and formula_24 are just the equations of motion of formula_21 and formula_26\n\nwhere formula_27 is the gravitational parameter with formula_28 and formula_29 the masses of the central body and the perturbed body, formula_30 is the perturbing acceleration, and formula_31 and formula_32 are the magnitudes of formula_21 and formula_20.\n\nSubstituting from equations () and () into equation (),\n\nwhich, in theory, could be integrated twice to find formula_22. Since the osculating orbit is easily calculated by two-body methods, formula_20 and formula_22 are accounted for and formula_21 can be solved. In practice, the quantity in the brackets, formula_39, is the difference of two nearly equal vectors, and further manipulation is necessary to avoid the need for extra significant digits. \nEncke's method was more widely used before the advent of modern computers, when much orbit computation was performed on mechanical calculating machines.\n\nIn the Solar System, many of the disturbances of one planet by another are periodic, consisting of small impulses each time a planet passes another in its orbit. This causes the bodies to follow motions that are periodic or quasi-periodic – such as the Moon in its strongly perturbed orbit, which is the subject of lunar theory. This periodic nature led to the discovery of Neptune in 1846 as a result of its perturbations of the orbit of Uranus.\n\nOn-going mutual perturbations of the planets cause long-term quasi-periodic variations in their orbital elements, most apparent when two planets' orbital periods are nearly in sync. For instance, five orbits of Jupiter (59.31 years) is nearly equal to two of Saturn (58.91 years). This causes large perturbations of both, with a period of 918 years, the time required for the small difference in their positions at conjunction to make one complete circle, first discovered by Laplace. Venus currently has the orbit with the least eccentricity, i.e. it is the closest to circular, of all the planetary orbits. In 25,000 years' time, Earth will have a more circular (less eccentric) orbit than Venus. It has been shown that long-term periodic disturbances within the Solar System can become chaotic over very long time scales; under some circumstances one or more planets can cross the orbit of another, leading to collisions.\n\nThe orbits of many of the minor bodies of the Solar System, such as comets, are often heavily perturbed, particularly by the gravitational fields of the gas giants. While many of these perturbations are periodic, others are not, and these in particular may represent aspects of chaotic motion. For example, in April 1996, Jupiter's gravitational influence caused the period of Comet Hale–Bopp's orbit to decrease from 4,206 to 2,380 years, a change that will not revert on any periodic basis.\n\n\n\n"}
{"id": "243627", "url": "https://en.wikipedia.org/wiki?curid=243627", "title": "Physical information", "text": "Physical information\n\nPhysical information is a form of information. In physics, it refers to the information of a physical system. Physical information is an important concept used in a number of fields of study in physics. For example, in quantum mechanics, the form of physical information known as quantum information is used in many descriptions of quantum phenomena, such as quantum observation, quantum entanglement and the causal relationship between quantum objects that carry out either or both close and long-range interactions with one another.\n\nIn a somewhat general sense, the information of a given entity can be interpreted as its identity. As such, its information can be perceived to be the representation of the specification of its existence and thus, to be serving as the full description of each of the properties (real or potentialized) that are responsible for the entity’s existence. This description, of course, is one that, in a sense, is completely divorced from both any and all forms of language.\n\nWhen clarifying the subject of information, care should be taken to distinguish between the following specific cases:\n\nAs the above usages are all conceptually distinct from each other, overloading the word \"information\" (by itself) to denote (or connote) several of these concepts simultaneously can lead to confusion. Accordingly, this article uses more detailed phrases, such as those shown in bold above, whenever the intended meaning is not made clear by the context.\n\nThe instance of information that is contained in a physical system is generally considered to specify\nthat system's \"true\" \"state\". (A realist would assert that a physical system \"always\" has a true state of some sort—whether classical or quantum—even though, in many practical situations, the system's true state may be largely unknown.)\n\nWhen discussing the information that is contained in physical systems according to modern quantum physics, we must distinguish between classical information and quantum information. Quantum information specifies the complete quantum state vector (or equivalently, wavefunction) of a system, whereas classical information, roughly speaking, only picks out a definite (pure) quantum state if we are already given a prespecified set of distinguishable (orthogonal) quantum states to choose from; such a set forms a basis for the vector space of all the possible pure quantum states (see pure state). Quantum information could thus be expressed by providing (1) a choice of a basis such that the actual quantum state is equal to one of the basis vectors, together with (2) the classical information specifying which of these basis vectors is the actual one. (However, the quantum information by itself does not include a specification of the basis, indeed, an uncountable number of different bases will include any given state vector.)\n\nNote that the amount of classical information in a quantum system gives the maximum amount of information that can actually be measured and extracted from that quantum system for use by external classical (decoherent) systems, since only basis states are operationally distinguishable from each other. The impossibility of differentiating between non-orthogonal states is a fundamental principle of quantum mechanics, equivalent to Heisenberg's uncertainty principle. Because of its more general utility, the remainder of this article will deal primarily with classical information, although quantum information theory does also have some potential applications (quantum computing, quantum cryptography, quantum teleportation) that are currently being actively explored by both theorists and experimentalists.\n\nAn amount of (classical) physical information may be quantified, as in information theory, as follows. For a system \"S\", defined abstractly in such a way that it has \"N\" distinguishable states (orthogonal quantum states) that are consistent with its description, the amount of information \"I\"(\"S\") contained in the system's state can be said to be log(\"N\"). The logarithm is selected for this definition since it has the advantage that this measure of information content is additive when concatenating independent, unrelated subsystems; e.g., if subsystem \"A\" has \"N\" distinguishable states (\"I\"(\"A\") = log(\"N\") information content) and an independent subsystem \"B\" has \"M\" distinguishable states (\"I\"(\"B\") = log(\"M\") information content), then the concatenated system has \"NM\" distinguishable states and an information content \"I\"(\"AB\") = log(\"NM\") = log(\"N\") + log(\"M\") = \"I\"(\"A\") + \"I\"(\"B\"). We expect information to be additive from our everyday associations with the meaning of the word, e.g., that two pages of a book can contain twice as much information as one page.\n\nThe base of the logarithm used in this definition is arbitrary, since it affects the result by only a multiplicative constant, which determines the unit of information that is implied. If the log is taken base 2, the unit of information is the binary digit or bit (so named by John Tukey); if we use a natural logarithm instead, we might call the resulting unit the \"nat.\" In magnitude, a nat is apparently identical to Boltzmann's constant \"k\" or the ideal gas constant \"R\", although these particular quantities are usually reserved to measure physical information that happens to be entropy, and that are expressed in physical units such as joules per kelvin, or kilocalories per mole-kelvin.\n\nAn easy way to understand the underlying unity between physical (as in thermodynamic) entropy and information-theoretic entropy is as follows: Entropy is simply that portion of the (classical) physical information contained in a system of interest (whether it is an entire physical system, or just a subsystem delineated by a set of possible messages) whose identity (as opposed to amount) is unknown (from the point of view of a particular knower).This informal characterization corresponds to both von Neumann's formal definition of the entropy of a mixed quantum state (which is just a statistical mixture of pure states; see von Neumann entropy), as well as Claude Shannon's definition of the entropy of a probability distribution over classical signal states or messages (see information entropy). Incidentally, the credit for Shannon's entropy formula (though not for its use in an information theory context) really belongs to Boltzmann, who derived it much earlier for use in his H-theorem of statistical mechanics. (Shannon himself references Boltzmann in his monograph.)\n\nFurthermore, even when the state of a system \"is\" known, we can say that the information in the system is still \"effectively\" entropy if that information is effectively incompressible, that is, if there are no known or feasibly determinable correlations or redundancies between different pieces of information within the system. Note that this definition of entropy can even be viewed as equivalent to the previous one (unknown information) if we take a meta-perspective, and say that for observer \"A\" to \"know\" the state of system \"B\" means simply that there is a definite correlation between the state of observer \"A\" and the state of system \"B\"; this correlation could thus be used by a meta-observer (that is, whoever is discussing the overall situation regarding A's state of knowledge about B) to compress his own description of the joint system \"AB\".\n\nDue to this connection with algorithmic information theory, entropy can be said to be that portion of a system's information capacity which is \"used up,\" that is, unavailable for storing new information (even if the existing information content were to be compressed). The rest of a system's information capacity (aside from its entropy) might be called \"extropy\", and it represents the part of the system's information capacity which is potentially still available for storing newly derived information. The fact that physical entropy is basically \"used-up storage capacity\" is a direct concern in the engineering of computing systems; e.g., a computer must first remove the entropy from a given physical subsystem (eventually expelling it to the environment, and emitting heat) in order for that subsystem to be used to store some newly computed information.\n\nIn a theory developed by B. Roy Frieden, \"physical information\" is defined as the loss of Fisher information that is incurred during the observation of a physical effect. Thus, if the effect has an intrinsic information level \"J\" but is observed at information level \"I\", the physical information is defined to be the difference \"I\" − \"J\". This defines an \"information Lagrangian\". Frieden's \"principle of extreme physical information\" or EPI states that extremalizing \"I\" − \"J\" by varying the system probability amplitudes gives the correct amplitudes for most or even all physical theories. The EPI principle was recently proven. It follows from a system of mathematical axioms of L. Hardy defining all known physics.\n\n\n"}
{"id": "422786", "url": "https://en.wikipedia.org/wiki?curid=422786", "title": "Primorial", "text": "Primorial\n\nIn mathematics, and more particularly in number theory, primorial is a function from natural numbers to natural numbers similar to the factorial function, but rather than successively multiplying positive integers, only prime numbers are multiplied.\n\nThere are two conflicting definitions that differ in the interpretation of the argument: the first interprets the argument as an index into the sequence of prime numbers (so that the function is strictly increasing), while the second interprets the argument as a bound on the prime numbers to be multiplied (so that the function value at any composite number is the same as at its predecessor). The rest of this article uses the former interpretation.\n\nThe name \"primorial\", coined by Harvey Dubner, draws an analogy to \"primes\" similar to the way the name \"factorial\" relates to \"factors\".\n\nFor the th prime number , the primorial is defined as the product of the first primes:\n\nwhere is the th prime number. For instance, signifies the product of the first 5 primes:\n\nThe first five primorials are:\n\nThe sequence also includes as empty product. Asymptotically, primorials grow according to:\n\nwhere is Little O notation.\n\nIn general for a positive integer , a primorial can also be defined, namely as the product of those primes ≤ :\n\nwhere is the prime-counting function , giving the number of primes ≤ . This is equivalent to:\n\nFor example, 12# represents the product of those primes ≤ 12:\n\nSince , this can be calculated as:\n\nConsider the first 12 values of :\n\nWe see that for composite every term simply duplicates the preceding term , as given in the definition. In the above example we have since 12 is a composite number.\n\nPrimorials are related to the first Chebyshev function, written or according to:\n\nSince asymptotically approaches for large values of , primorials therefore grow according to:\n\nThe idea of multiplying all known primes occurs in some proofs of the infinitude of the prime numbers, where it is used to derive the existence of another prime.\n\nPrimorials play a role in the search for prime numbers in additive arithmetic progressions. For instance, \n + 23# results in a prime, beginning a sequence of thirteen primes found by repeatedly adding 23#, and ending with . 23# is also the common difference in arithmetic progressions of fifteen and sixteen primes.\n\nEvery highly composite number is a product of primorials (e.g. 360 = ).\n\nPrimorials are all square-free integers, and each one has more distinct prime factors than any number smaller than it. For each primorial , the fraction is smaller than it for any lesser integer, where is the Euler totient function.\n\nAny completely multiplicative function is defined by its values at primorials, since it is defined by its values at primes, which can be recovered by division of adjacent values.\n\nBase systems corresponding to primorials (such as base 30, not to be confused with the primorial number system) have a lower proportion of repeating fractions than any smaller base.\n\nEvery primorial is a sparsely totient number.\n\nThe -compositorial of a composite number is the product of all composite numbers up to and including . The -compositorial is equal to the -factorial divided by the primorial . The compositorials are\n\nThe Riemann zeta function at positive integers greater than one can be expressed by using the primorial function and Jordan's totient function :\n\n"}
{"id": "54252064", "url": "https://en.wikipedia.org/wiki?curid=54252064", "title": "Resource selection function", "text": "Resource selection function\n\nResource selection functions (RSFs) are a class of functions that are used in spatial ecology to assess which habitat characteristics are important to a specific population or species of animal, by assessing the a probability of that animal using a certain resource proportional to the availability of that resource in the environment.\n\nResource Selection Functions require two types of data: location information for the wildlife in question, and data on the resources available across the study area. Resources can include a broad range of environmental and geographical variables, including categorical variables such as land cover type, or continuous variables such as average rainfall over a given time period. A variety of methods are used for modeling RSFs, with logistic regression being commonly used.\n\nRSFs can be fit to data where animal presence is known, but absence is not, such as for species where several individuals within a study area are fitted with a GPS collar, but some individuals may be present without collars.\nWhen this is the case, buffers of various distances are generated around known presence points, with a number of available points generated within each buffer, which represent areas where the animal could have been, but it is unknown whether they actually were. These models can be fit using binomial generalized linear models or binomial generalized linear mixed models, with the resources, or environmental and geographic data, as explanatory variables.\n\nResource selection functions can be modeled at a variety of spatial scales, depending on the species and the scientific question being studied. (insert one more sentence on scale)\n\nMost RSFs address one of the following scales, which were defined by Douglas Johnson in 1980 and are still used today:\n"}
{"id": "39759555", "url": "https://en.wikipedia.org/wiki?curid=39759555", "title": "SWAG (silver, wine, art and gold)", "text": "SWAG (silver, wine, art and gold)\n\nSWAG is an asset class comprising silver, wine, art and gold, identified by economist Joe Roseman in his 2011 \"Investment Week\" article, \"SWAG: The Industry's Latest Acronym\". He describes these as transportable and easy to store physical assets with no income stream (and so no exposure to income tax) and no incumbent debt, whose performance appears unrelated to the performance of equity markets; and he notes that none of these factors would be affected by sovereign default.\n\nBecause of their scarcity, desirability, durability and stability and the independence of their price from stock market prices they can add genuine diversity to an asset portfolio.\n\n"}
{"id": "855345", "url": "https://en.wikipedia.org/wiki?curid=855345", "title": "Sartaj Sahni", "text": "Sartaj Sahni\n\nProfessor Sartaj Kumar Sahni (born July 22, 1949, in Poona, India) is a computer scientist based in the United States, and is one of the pioneers in the field of data structures. He is a distinguished professor in the Department of Computer and Information Science and Engineering at the University of Florida.\n\nSahni received his BTech degree in electrical engineering from the Indian Institute of Technology Kanpur. Following this, he undertook his graduate studies at Cornell University in the USA, earning a PhD degree in 1973, under the supervision of Ellis Horowitz.\n\nSahni has published over 280 research papers and written 15 textbooks. His research publications are on the design and analysis of efficient algorithms, data structures, parallel computing, interconnection networks, design automation, and medical algorithms.\n\nWith his advisor Ellis Horowitz, Sahni wrote two widely used textbooks, \"Fundamentals of Computer Algorithms\" and \"Fundamentals of Data Structures\". He has also written highly cited research papers on the NP-completeness of approximately solving certain optimization problems, on open shop scheduling, on parallel algorithms for matrix multiplication and their application in graph theory, and on improved exponential time exact algorithms for the subset sum problem, among his many other research results.\n\nIn 1997, Sahni was awarded the IEEE Computer Society's Taylor L. Booth Education Award and in 2003 he was awarded the IEEE Computer Society McDowell Award. Sahni was also awarded the 2003 Karl V. Karlstrom Outstanding Educator Award of the Association for Computing Machinery.\n\nProf. Sahni is a member of the European Academy of Sciences. He was elected as a Fellow of the Institute of Electrical and Electronics Engineers in 1988, and of the Association for Computing Machinery in 1996; he is also a fellow of the American Association for the Advancement of Science. He is a Distinguished Alumnus of the Indian Institute of Technology, Kanpur.\n\nSahni was given the Honorary Professor Award of Asia University (Taiwan) in 2009.\n\n"}
{"id": "24673785", "url": "https://en.wikipedia.org/wiki?curid=24673785", "title": "Schreier coset graph", "text": "Schreier coset graph\n\nIn the area of mathematics called combinatorial group theory, the Schreier coset graph is a graph associated with a group \"G\", a generating set { \"x\" : \"i\" in \"I\" }, and a subgroup \"H\" ≤ \"G\". \n\nThe graph is named after Otto Schreier, who used the term “Nebengruppenbild”. An equivalent definition was made in an early paper of Todd and Coxeter. \n\nThe vertices of the graph are the right cosets \"Hg\" = { \"hg\" : \"h\" in \"H\" } for \"g\" in \"G\". \n\nThe edges of the graph are of the form (\"Hg\",\"Hgx\"). \n\nThe Cayley graph of the group \"G\" with { \"x\" : \"i\" in \"I\" } is the Schreier coset graph for \"H\" = { 1 }. \n\nA spanning tree of a Schreier coset graph corresponds to a Schreier transversal, as in Schreier's subgroup lemma, .\n\nThe book \"Categories and Groupoids\" listed below relates this to the theory of covering morphisms of groupoids. A subgroup \"H\" of a group \"G\" determines a covering morphism of groupoids formula_1 and if \"X\" is a generating set for \"G\" then its inverse image under \"p\" is the Schreier graph of \"(G,X)\".\n\nThe graph is useful to understand coset enumeration and the Todd–Coxeter algorithm. \n\nCoset graphs can be used to form large permutation representations of groups and were used by Graham Higman to show that the alternating groups of large enough degree are Hurwitz groups, .\n\nEvery vertex-transitive graph is a coset graph.\n\n"}
{"id": "32542224", "url": "https://en.wikipedia.org/wiki?curid=32542224", "title": "Shmuel Agmon", "text": "Shmuel Agmon\n\nShmuel Agmon () (born 2 February 1922 in Tel-Aviv) is an Israeli mathematician. He is known for his work in analysis and partial differential equations.\n\nAgmon's contributions to partial differential equations include Agmon's method for proving exponential decay of eigenfunctions for elliptic operators.\n\nAgmon was awarded the 1991 Israel Prize in mathematics. He received the 2007 EMET Prize \"for paving new paths in the study of partial-elliptical differential equations and their problematic language and for advancing the knowledge in the field, as well as his essential contribution to the development of the Spectral Theory and the Distribution Theory of Schrödinger Operators.\" He has also received the Weizmann Prize and the Rothschild Prize. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "436912", "url": "https://en.wikipedia.org/wiki?curid=436912", "title": "Short-time Fourier transform", "text": "Short-time Fourier transform\n\nThe short-time Fourier transform (STFT), is a Fourier-related transform used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time. In practice, the procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment. This reveals the Fourier spectrum on each shorter segment. One then usually plots the changing spectra as a function of time.\n\nSimply, in the continuous-time case, the function to be transformed is multiplied by a window function which is nonzero for only a short period of time. The Fourier transform (a one-dimensional function) of the resulting signal is taken as the window is slid along the time axis, resulting in a two-dimensional representation of the signal. Mathematically, this is written as:\n\nwhere \"w\"(\"t\") is the window function, commonly a Hann window or Gaussian window centered around zero, and \"x\"(\"t\") is the signal to be transformed (note the difference between \"w\" and ω). \"X\"(τ,ω) is essentially the Fourier Transform of \"x\"(\"t\")\"w\"(\"t\"-τ), a complex function representing the phase and magnitude of the signal over time and frequency. Often phase unwrapping is employed along either or both the time axis, τ, and frequency axis, ω, to suppress any jump discontinuity of the phase result of the STFT. The time index τ is normally considered to be \"slow\" time and usually not expressed in as high resolution as time \"t\".\n\nIn the discrete time case, the data to be transformed could be broken up into chunks or frames (which usually overlap each other, to reduce artifacts at the boundary). Each chunk is Fourier transformed, and the complex result is added to a matrix, which records magnitude and phase for each point in time and frequency. This can be expressed as:\n\nlikewise, with signal \"x\"[\"n\"] and window \"w\"[\"n\"]. In this case, \"m\" is discrete and ω is continuous, but in most typical applications the STFT is performed on a computer using the Fast Fourier Transform, so both variables are discrete and quantized.\n\nThe magnitude squared of the STFT yields the spectrogram representation of the Power Spectral Density of the function:\n\nSee also the modified discrete cosine transform (MDCT), which is also a Fourier-related transform that uses overlapping windows.\n\nIf only a small number of ω are desired, or if the STFT is desired to be evaluated for every shift \"m\" of the window, then the STFT may be more efficiently evaluated using a sliding DFT algorithm.\n\nThe STFT is invertible, that is, the original signal can be recovered from the transform by the Inverse STFT. The most widely accepted way of inverting the STFT is by using the overlap-add (OLA) method, which also allows for modifications to the STFT complex spectrum. This makes for a versatile signal processing method, referred to as the \"overlap and add with modifications\" method.\n\nGiven the width and definition of the window function \"w\"(\"t\"), we initially require the area of the window function to be scaled so that\n\nIt easily follows that\n\nand\n\nThe continuous Fourier Transform is\n\nSubstituting \"x\"(\"t\") from above:\n\nSwapping order of integration:\n\nSo the Fourier Transform can be seen as a sort of phase coherent sum of all of the STFTs of \"x\"(\"t\"). Since the inverse Fourier transform is\n\nthen \"x\"(\"t\") can be recovered from \"X\"(τ,ω) as\n\nor\n\nIt can be seen, comparing to above that windowed \"grain\" or \"wavelet\" of \"x\"(\"t\") is\n\nthe inverse Fourier transform of \"X\"(τ,ω) for τ fixed.\n\nOne of the pitfalls of the STFT is that it has a fixed resolution. The width of the windowing function relates to how the signal is represented—it determines whether there is good frequency resolution (frequency components close together can be separated) or good time resolution (the time at which frequencies change). A wide window gives better frequency resolution but poor time resolution. A narrower window gives good time resolution but poor frequency resolution. These are called narrowband and wideband transforms, respectively.\n\nThis is one of the reasons for the creation of the wavelet transform and multiresolution analysis, which can give good time resolution for high-frequency events and good frequency resolution for low-frequency events, the combination best suited for many real signals.\n\nThis property is related to the Heisenberg uncertainty principle, but not directly – see Gabor limit for discussion. The product of the standard deviation in time and frequency is limited. The boundary of the uncertainty principle (best simultaneous resolution of both) is reached with a Gaussian window function, as the Gaussian minimizes the Fourier uncertainty principle. This is called the Gabor transform (and with modifications for multiresolution becomes the Morlet wavelet transform).\n\nOne can consider the STFT for varying window size as a two-dimensional domain (time and frequency), as illustrated in the example below, which can be calculated by varying the window size. However, this is no longer a strictly time–frequency representation – the kernel is not constant over the entire signal.\n\nUsing the following sample signal formula_17 that is composed of a set of four sinusoidal waveforms joined together in sequence. Each waveform is only composed of one of four frequencies (10, 25, 50, 100 Hz). The definition of formula_17 is:\n\nThen it is sampled at 400 Hz. The following spectrograms were produced:\n\nThe 25 ms window allows us to identify a precise time at which the signals change but the precise frequencies are difficult to identify. At the other end of the scale, the 1000 ms window allows the frequencies to be precisely seen but the time between frequency changes is blurred.\n\nIt can also be explained with reference to the sampling and Nyquist frequency.\n\nTake a window of \"N\" samples from an arbitrary real-valued signal at sampling rate \"f\" . Taking the Fourier transform produces \"N\" complex coefficients. Of these coefficients only half are useful (the last \"N/2\" being the complex conjugate of the first \"N/2\" in reverse order, as this is a real valued signal).\n\nThese \"N/2\" coefficients represent the frequencies 0 to \"f\"/2 (Nyquist) and two consecutive coefficients are spaced apart by\n\"f\"/\"N\" Hz.\n\nTo increase the frequency resolution of the window the frequency spacing of the coefficients needs to be reduced. There are only two variables, but decreasing \"f\" (and keeping \"N\" constant) will cause the window size to increase — since there are now fewer samples per unit time. The other alternative is to increase \"N\", but this again causes the window size to increase. So any attempt to increase the frequency resolution causes a larger window size and therefore a reduction in time resolution—and vice versa.\n\nAs the Nyquist frequency is a limitation in the maximum frequency that can be meaningfully analysed, so is the Rayleigh frequency a limitation on the minimum frequency.\n\nRayleigh frequency is the minimum frequency that can be resolved by a finite duration time window.\n\nGiven a time window that is Τ seconds long, the minimum frequency that can be resolved is 1/Τ Hz.\n\nRayleigh frequency is important to consider in applications of the short-time Fourier transform (STFT), such as in analysing neural signals\n\nSTFTs as well as standard Fourier transforms and other tools are frequently used to analyze music. The spectrogram can, for example, show frequency on the horizontal axis, with the lowest frequencies at left, and the highest at the right. The height of each bar (augmented by color) represents the amplitude of the frequencies within that band. The depth dimension represents time, where each new bar was a separate distinct transform. Audio engineers use this kind of visual to gain information about an audio sample, for example, to locate the frequencies of specific noises (especially when used with greater frequency resolution) or to find frequencies which may be more or less resonant in the space where the signal was recorded. This information can be used for equalization or tuning other audio effects.\n\nOriginal function\n\nConverting into the discrete form:\n\nSuppose that \n\nThen we can write the original function into\n\na. Nyquist criterion(Avoiding the aliasing effect):\n\na. formula_28, where formula_29 is an integer\n\nb. formula_30\n\nc. Nyquist criterion (Avoiding the aliasing effect):\n\na. formula_28, where formula_29 is an integer\n\nb. formula_30\n\nc. Nyquist criterion (Avoiding the aliasing effect):\n\nd. Only for implementing the rectangular-STFT\n\nRectangular window imposes the constraint\nSubstitution gives:\n\nChange of variable for :\n\nCalculate formula_47 by the \"N\"-point FFT:\n\nwhere\n\nApplying the recursive formula to calculate formula_50\n\nso \n\n\nOther time-frequency transforms:\n\n"}
{"id": "1708671", "url": "https://en.wikipedia.org/wiki?curid=1708671", "title": "Spatial network", "text": "Spatial network\n\nA spatial network (sometimes also geometric graph) is a graph in which the vertices or edges are \"spatial elements\" associated with geometric objects, i.e. the nodes are located in a space equipped with a certain metric. The simplest mathematical realization is a lattice or a random geometric graph, where nodes are distributed uniformly at random over a two-dimensional plane; a pair of nodes are connected if the Euclidean distance is smaller than a given neighborhood radius. Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks and neural networks are all examples where the underlying space is relevant and where the graph's topology alone does not contain all the information. Characterizing and understanding the structure, resilience and the evolution of spatial networks is crucial for many different fields ranging from urbanism to epidemiology.\n\nAn urban spatial network can be constructed by abstracting intersections as nodes and streets as links, which is referred to as, transportation network. Beijing traffic was studied as a dynamic network and its percolation properties have been found useful to identify systematic bottlenecks.\n\nOne might think of the 'space map' as being the negative image of the standard map, with the open space cut out of the background buildings or walls.\n\nThe following aspects are some of the characteristics to examine a spatial network:\nIn many applications, such as rail, roads, and other transportation networks the network is assumed to be planar. Planar networks build up an important group out of the spatial networks, but not all spatial networks are planar. Indeed, the airline passenger\nnetworks is a non-planar example: All airports in the world are connected through direct flights. \nThere are examples of networks, which seem to be not \"directly\" embedded in space. Social networks for instance\nconnect individuals through friendship relations. But in this case, space intervenes in the fact that the connection\nprobability between two individuals usually decreases with the distance between them.\nA spatial network can be represented by a Voronoi diagram, which is a way of dividing space into a number of regions. The dual graph for a Voronoi diagram corresponds to the Delaunay triangulation for the same set of points.\nVoronoi tessellations are interesting for spatial networks in the sense that they provide a natural representation model\nto which one can compare a real world network.\n\nExamining the topology of the nodes and edges itself is another way to characterize networks. The distribution of degree of the nodes is often considered, regarding the structure of edges it is useful to find the Minimum spanning tree, or the generalization, the Steiner tree and the relative neighborhood graph.\nLattice networks (see Fig. 1) are useful models for spatial embedded networks. Many physical phenomenas have been studied on these structures. Examples include Ising model for spontaneous magnetization, diffusion phenomena modeled as random walks\nand percolation. Recently to model the resilience of interdependent infrastructures which are spatially embedded a model of interdependent lattice networks was introduced (see Fig. 2) and analyzed\n. A spatial multiplex model was introduced\nby Danziger et al and was analyzed further by Vaknin et al. For the model see Fig. 3.\n\nIn the \"real\" world many aspects of networks are not deterministic - randomness plays an important role. For example, new links, representing friendships, in social networks are in a certain manner random. Modelling spatial networks in respect of stochastic operations is consequent. In many cases the spatial Poisson process is used to approximate data sets of processes on spatial networks. Other stochastic aspects of interest are: \n\nAnother definition of spatial network derives from the theory of space syntax. It can be notoriously difficult to decide what a spatial element should be in complex spaces involving large open areas or many interconnected paths. The originators of space syntax, Bill Hillier and Julienne Hanson use axial lines and convex spaces as the spatial elements. Loosely, an axial line is the 'longest line of sight and access' through open space, and a convex space the 'maximal convex polygon' that can be drawn in open space. Each of these elements is defined by the geometry of the local boundary in different regions of the space map. Decomposition of a space map into a complete set of intersecting axial lines or overlapping convex spaces produces the axial map or overlapping convex map respectively. Algorithmic definitions of these maps exist, and this allows the mapping from an arbitrary shaped space map to a network amenable to graph mathematics to be carried out in a relatively well defined manner. Axial maps are used to analyse urban networks, where the system generally comprises linear segments, whereas convex maps are more often used to analyse building plans where space patterns are often more convexly articulated, however both convex and axial maps may be used in either situation.\n\nCurrently, there is a move within the space syntax community to integrate better with geographic information systems (GIS), and much of the software they produce interlinks with commercially available GIS systems.\n\nWhile networks and graphs were already for a long time the subject\nof many studies in mathematics, physics, mathematical sociology,\ncomputer science, spatial networks have been studied intensively during the 1970s in quantitative geography. Objects of studies in geography are inter alia locations, activities and flows of individuals, but also networks evolving in time and space. Most of the important problems such\nas the location of nodes of a network, the evolution of\ntransportation networks and their interaction with population\nand activity density are addressed in these earlier\nstudies. On the other side, many important points still remain unclear, partly because at that time datasets of large networks and larger computer capabilities were lacking. \nRecently, spatial networks have been the subject of studies in Statistics, to connect probabilities and stochastic processes with networks in the real world.\n\n\n"}
{"id": "26077527", "url": "https://en.wikipedia.org/wiki?curid=26077527", "title": "TNSDL", "text": "TNSDL\n\nTNSDL stands for TeleNokia Specification and Description Language. TNSDL is based on the ITU-T SDL-88 language. It is used exclusively at Nokia Networks, primarily for developing applications for telephone exchanges.\n\nTNSDL is a general-purpose procedural programming language. It is especially well-suited for developing highly concurrent, distributed systems.\n\nIt was originally designed for programming circuit switched exchanges. As the world shifted towards packet-switched and internet-based telecommunication, TNSDL turned out to be an excellent fit for developing internet servers, too.\n\nTNSDL is a very simple, easy-to-learn programming language.\n\nTNSDL is a strongly typed procedural programming language. Its basic capabilities are comparable to the C and Pascal languages.\n\nIn TNSDL processes are created by the CREATE command. (It is somewhat similar to the POSIX fork or pthread_create commands.) The CREATE command creates either an operating system process or a cooperative task.\n\nThe process model can be selected by configuration. The source code itself does not reflect which scheduling method is used. Still, to avoid certain race conditions, developers may need to be prepared for parallel execution. TNSDL explicitly supports critical sections to be marked in the code.\n\nIn case of cooperative multitasking a program is scheduled as one operating system process. When a cooperative thread enters the state of waiting for asynchronous input, another thread of the program may run.\n\nThe feature of TNSDL is the actor model. Processes are meant to be designed as event-driven finite state machines. Inter-process communication is done by asynchronous message passing. The OUTPUT command sends a message, while INPUT statements define the expected messages.\n\nTimers, from TNSDL perspective, are delayed messages. Just like ordinary messages, timer expiration is handled by the INPUT statement. The SET command starts and the RESET command cancels a timer.\n\nState machines can be optionally used, for example, to prevent accepting certain input messages at some stage of the processing.\n\nThe following code piece demonstrates a server, which receives a query signal (message), contacts a database process to obtain the necessary data and finally sends an answer signal.\n\nComments:\n\nTNSDL allows inputs to be tied to several or all of the states. An input signal may have state-specific behavior, if needed.\n\nNokia has made several modifications to the language, mainly including simplifications and additions, such as:\n\n\nTNSDL is not directly compiled to machine code. Instead, TNSDL programs are translated to C language source code. The responsibility of TNSDL is to allow message handling, state machine definitions, synchronizing parallel execution, \"data warming\" etc. easily and safely coded. The task of processor-specific code generation and low-level optimization is delegated to the C compiler used.\n\nAfter translating TNSDL to C, any standard-compliant C compiler, linker, coverage measurement and profiling tool can be used. To make source-level debugging possible TNSDL puts line number references to the generated C code.\n\nTNSDL code can call routines implemented in other languages, if objects or libraries are present for them. Even C language macros can be used, if C header files are present. External declarations must be made available to the TNSDL translator.\n\nThe TNSDL translator is a proprietary tool. A source code (reachability) analyser has also been developed specifically for TNSDL.\n\nTNSDL is commonly used on the DX 200, IPA 2800 and Linux platforms for high-performance, high-availability applications.\n\nTNSDL is an actively used and developed programming language used by thousands of developers (in 2010).\n\nTNSDL is mainly used at Nokia Networks for developing software for SGSNs, BSCs, mobile switching centers, application servers both in traditional setups and as virtual network functions (VNF) of NFV solutions.\n\nDespite the difference in syntax, probably one of the closest relative of TNSDL is Go language. Both languages has light-weight processes in their focus. Go's channel are similar to TNSDL INPUTs and Go's select statement on channels allows very similar program design. There are differences, though. TNSDL uses asynchronous message passing between actors, while channels in Go can either be synchronous or asynchronous (buffered). TNSDL allows message passing between processes running on the same or separate computer nodes. In that aspect TNSDL is a relative of Erlang.\n\nEven though in TNSDL one can define operators for types and protect structure attributes to be accessible via those operators only, TNSDL is not an object-oriented language. In that aspect it belongs to the family of non-OOP procedural programming languages, such as C language.\n\n1980s: In the beginning, ITU-T SDL had a graphical syntax. Textual syntax was introduced later. A corresponding graphical tool and code generator were developed within Nokia.\n\n1990: ITU-T SDL shifted towards text-based representation. Based on the SDL-88 specification TNSDL was born. TNSDL is a simplified and heavily customized variant of SDL-88.\n"}
{"id": "35258497", "url": "https://en.wikipedia.org/wiki?curid=35258497", "title": "Telephone number (mathematics)", "text": "Telephone number (mathematics)\n\nIn mathematics, the telephone numbers or the involution numbers are a sequence of integers that count the ways telephone lines can be connected to each other, where each line can be connected to at most one other line. These numbers also describe the number of matchings (the Hosoya index) of a complete graph on vertices, the number of permutations on elements that are involutions, the sum of absolute values of coefficients of the Hermite polynomials, the number of standard Young tableaux with cells, and the sum of the degrees of the irreducible representations of the symmetric group. Involution numbers were first studied in 1800 by Heinrich August Rothe, who gave a recurrence equation by which they may be calculated, giving the values (starting from )\n\nJohn Riordan provides the following explanation for these numbers: suppose that a telephone service has subscribers, any two of whom may be connected to each other by a telephone call. How many different patterns of connection are possible? For instance, with three subscribers, there are three ways of forming a single telephone call, and one additional pattern in which no calls are being made, for a total of four patterns. For this reason, the numbers counting how many patterns are possible are sometimes called the telephone numbers.\n\nEvery pattern of pairwise connections between subscribers defines an involution, a permutation of the subscribers that is its own inverse, in which two subscribers who are making a call to each other are swapped with each other and all remaining subscribers stay in place. Conversely, every possible involution has the form of a set of pairwise swaps of this type. Therefore, the telephone numbers also count involutions. The problem of counting involutions was the original combinatorial enumeration problem studied by Rothe in 1800 and these numbers have also been called involution numbers.\n\nIn graph theory, a subset of the edges of a graph that touches each vertex at most once is called a matching. The number of different matchings of a given graph is important in chemical graph theory, where the graphs model molecules and the number of matchings is known as the Hosoya index. The largest possible Hosoya index of an -vertex graph is given by the complete graphs, for which any pattern of pairwise connections is possible; thus, the Hosoya index of a complete graph on vertices is the same as the th telephone number.\nA Ferrers diagram is a geometric shape formed by a collection of squares in the plane, grouped into a polyomino with a horizontal top edge, a vertical left edge, and a single monotonic chain of horizontal and vertical bottom and right edges. A standard Young tableau is formed by placing the numbers from 1 to into these squares in such a way that the numbers increase from left to right and from top to bottom throughout the tableau.\nAccording to the Robinson–Schensted correspondence, permutations correspond one-for-one with ordered pairs of standard Young tableaux. Inverting a permutation corresponds to swapping the two tableaux, and so the self-inverse permutations correspond to single tableaux, paired with themselves. Thus, the telephone numbers also count the number of Young tableaux with squares. In representation theory, the Ferrers diagrams correspond to the irreducible representations of the symmetric group of permutations, and the Young tableaux with a given shape form a basis of the irreducible representation with that shape. Therefore, the telephone numbers give the sum of the degrees of the irreducible representations.\nIn the mathematics of chess, the telephone numbers count the number of ways to place rooks on an chessboard in such a way that no two rooks attack each other (the so-called eight rooks puzzle), and in such a way that the configuration of the rooks is symmetric under a diagonal reflection of the board. Via the Pólya enumeration theorem, these numbers form one of the key components of a formula for the overall number of \"essentially different\" configurations of mutually non-attacking rooks, where two configurations are counted as essentially different if there is no symmetry of the board that takes one into the other.\n\nThe telephone numbers satisfy the recurrence relation\nfirst published in 1800 by Heinrich August Rothe, by which they may easily be calculated.\nOne way to explain this recurrence is to partition the connection patterns of the subscribers to a telephone system into the patterns in which the first subscriber is not calling anyone else, and the patterns in which the first subscriber is making a call. There are connection patterns in which the first subscriber is disconnected, explaining the first term of the recurrence. If the first subscriber is connected to someone else, there are choices for which other subscriber they are connected to, and patterns of connection for the remaining subscribers, explaining the second term of the recurrence.\n\nThe telephone numbers may be expressed exactly as a summation\nIn each term of this sum, gives the number of matched pairs, the binomial coefficient formula_3 counts the number of ways of choosing the elements to be matched, and the double factorial is the product of the odd integers up to its argument and counts the number of ways of completely matching the selected elements. It follows from the summation formula and Stirling's approximation that, asymptotically, \n\nThe exponential generating function of the telephone numbers is\nIn other words, the telephone numbers may be read off as the coefficients of the Taylor series of , and the th telephone number is the value at zero of the th derivative of this function.\nThis function is closely related to the exponential generating function of the Hermite polynomials, which are the matching polynomials of the complete graphs.\nThe sum of absolute values of the coefficients of the th (probabilist) Hermite polynomial is the th telephone number, and the telephone numbers can also be realized as certain special values of the Hermite polynomials:\n\nFor large values of , the th telephone number is divisible by a large power of two, .\n\nMore precisely, the 2-adic order (the number of factors of two in the prime factorization) of and of is ; for it is , and for it is .\n\nFor any prime number , one can test whether there exists a telephone number divisible by by computing the recurrence for the sequence of telephone numbers, modulo , until either reaching zero or detecting a cycle. The primes that divide at least one telephone number are\n"}
{"id": "32177562", "url": "https://en.wikipedia.org/wiki?curid=32177562", "title": "Ulam's game", "text": "Ulam's game\n\nUlam's game, or the Rényi–Ulam game, is a mathematical game similar to the popular game of twenty questions where one attempts to guess an unnamed object with yes–no questions, but where some of the answers may be wrong. introduced the game, though his paper was overlooked for many years. Rényi [69] reported the following story about the Jew Bar Kochba in 135 CE, who defended his fortress against the Romans. It is also said that Bar Kochba sent out a scout to the Roman camp who was captured and tortured, having his tongue cut out. He escaped from captivity and reported back to Bar Kochba, but being unable to talk, he could not tell in words what he had seen. Bar Kochba accordingly asked him questions which he could answer by nodding or shaking his head. Thus he acquired from his mute scout the information he needed to defend the fortress. rediscovered the game, presenting the idea that there are a million objects and the answer to one question can be wrong. gave a survey of similar games and their relation to information theory.\n\n"}
{"id": "18732683", "url": "https://en.wikipedia.org/wiki?curid=18732683", "title": "X-ray transform", "text": "X-ray transform\n\nIn mathematics, the X-ray transform (also called John transform) is an integral transform introduced by Fritz John in 1938 that is one of the cornerstones of modern integral geometry. It is very closely related to the Radon transform, and coincides with it in two dimensions. In higher dimensions, the X-ray transform of a function is defined by integrating over lines rather than over hyperplanes as in the Radon transform. The X-ray transform derives its name from X-ray tomography because the X-ray transform of a function \"ƒ\" represents the attenuation data of a tomographic scan through an inhomogeneous medium whose density is represented by the function \"ƒ\". Inversion of the X-ray transform is therefore of practical importance because it allows one to reconstruct an unknown density \"ƒ\" from its known attenuation data.\n\nIn detail, if \"ƒ\" is a compactly supported continuous function on the Euclidean space R, then the X-ray transform of \"ƒ\" is the function \"Xƒ\" defined on the set of all lines in R by\nwhere \"x\" is an initial point on the line and θ is a unit vector giving the direction of the line \"L\". The latter integral is not regarded in the oriented sense: it is the integral with respect to the 1-dimensional Lebesgue measure on the Euclidean line \"L\".\n\nThe X-ray transform satisfies an ultrahyperbolic wave equation called John's equation.\n\nThe Gauss hypergeometric function can be written as an X-ray transform .\n\n"}
{"id": "50758843", "url": "https://en.wikipedia.org/wiki?curid=50758843", "title": "Yurii Vladimirovich Egorov", "text": "Yurii Vladimirovich Egorov\n\nYurii (or Yuri) Vladimirovich Egorov (Юрий Владимирович Егоров, born 14 July 1938 in Moscow, died October 2018 in Toulouse) was a Russian-Soviet mathematician who specializes in differential equations.\n\nIn 1960 he completed his undergraduate studies at the Mechanics and Mathematics Faculty of Moscow State University (MSU). In 1963 from MSU he received his Ph.D. with the thesis \"Некоторые задачи теории оптимального управления в бесконечномерных пространствах\" (\"Some Problems of Optimal Control Theory in Infinite-Dimensional Spaces\"). In 1970 from MSU he received his Russian doctorate of sciences (Doctor Nauk) with thesis: \"О локальных свойствах псевдодифференциальных операторов главного типа\" (\"Local Properties of Pseudodifferential Operators of Principal Type\"). He was employed at MSU from 1961 to 1992, and he was a full professor in the Department of Differential Equations of the Mechanics and Mathematics Faculty there from 1973 to 1992. Since 1992 he has been a professor of mathematics at Paul Sabatier University (Toulouse III).\n\nEgorov's research deals with differential equations and applications in mathematical physics, spectral theory, and optimal control theory. In 1970 he was an Invited Speaker of the ICM in Nice.\n\n\n\n"}
{"id": "14650127", "url": "https://en.wikipedia.org/wiki?curid=14650127", "title": "Zygmunt Zawirski", "text": "Zygmunt Zawirski\n\nZygmunt Zawirski (29 September 1882 – 2 April 1948) was a Polish philosopher and logician.\n\nHis main field of study was philosophy of physics, history of science, multi-valued logic and relation of multi-valued logic to calculus of probability.\n\nZawirski was born on 29 September 1882 in the village of Berezowica Mała (Berzovytsia Mala) near Zbarazh (now Ukraine). In 1928 he became a professor of the Adam Mickiewicz University in Poznań and in 1937 professor of the Jagiellonian University in Kraków. In 1936 he became an editor of \"Kwartalnik Filozoficzny\" (\"Philosophical Quarterly\"). After 1945, he was president of the Krakowskie Towarzystwo Filozoficzne (\"Kraków Philosophical Society\").\n\nHe died on 2 April 1948 in Końskie, Poland.\n\n\n"}
