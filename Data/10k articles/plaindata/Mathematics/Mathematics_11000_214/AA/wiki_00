{"id": "407365", "url": "https://en.wikipedia.org/wiki?curid=407365", "title": "106 (number)", "text": "106 (number)\n\n106 (one hundred [and] six) is the natural number following 105 and preceding 107.\n\n106 is the thirty-first distinct biprime and the fifteenth of the form (2.q). The aliquot sum of 106 is 56 within the aliquot sequence (106,56,64,63,41,1) 106 being the eleventh composite number in the 41-aliquot tree.\n106 is a centered pentagonal number, a centered heptagonal number, and a regular 19-gonal number. There are 106 distinct mathematical trees with ten vertices.\n\n106 is also:\n\n"}
{"id": "1728688", "url": "https://en.wikipedia.org/wiki?curid=1728688", "title": "194 (number)", "text": "194 (number)\n\n194 (one hundred [and] ninety-four) is the natural number following 193 and preceding 195.\n\n\nBruce Sutter set the National League record in 1982 with his 194th save, surpassing the mark held by Roy Face\n\n\n\n 194 is also:\n\n\n"}
{"id": "8200947", "url": "https://en.wikipedia.org/wiki?curid=8200947", "title": "Alternating multilinear map", "text": "Alternating multilinear map\n\nIn mathematics, more specifically in multilinear algebra, an alternating multilinear map is a multilinear map with all arguments belonging to the same space (e.g., a bilinear form or a multilinear form) that is zero whenever any two adjacent arguments are equal.\n\nThe notion of alternatization (or alternatisation in British English) is used to derive an alternating multilinear map from any multilinear map with all arguments belonging to the same space.\n\nA multilinear map of the form formula_1 is said to be alternating if it satisfies the following equivalent conditions:\n\n\n\n\nGiven a multilinear map of the form formula_1, the alternating multilinear map formula_11 defined by formula_12 is said to be the alternatization of formula_13.\n\n\n\n\n"}
{"id": "2489512", "url": "https://en.wikipedia.org/wiki?curid=2489512", "title": "Anamorphosis", "text": "Anamorphosis\n\nAnamorphosis is a distorted projection or perspective requiring the viewer to use special devices or occupy a specific vantage point (or both) to view a recognizable image. Some of the media it is used in are painting, photography, sculpture and installation, toys, and film special effects. The word \"anamorphosis\" is derived from the Greek prefix \"ana‑\", meaning \"back\" or \"again\", and the word \"morphe\", meaning \"shape\" or \"form\". An optical anamorphism is the visualization of a mathematical operation called an affine transformation. The process of extreme anamorphosis has been used by artists to disguise caricatures, erotic and scatological scenes, and other furtive images from a casual viewer, while revealing an undistorted image to the knowledgeable spectator.\n\nThere are two main types of anamorphosis: \"perspective\" (oblique) and \"mirror\" (catoptric). More-complex anamorphoses can be devised using distorted lenses, mirrors, or other optical transformations.\n\nExamples of perspectival anamorphosis date to the early Renaissance (fifteenth century). Examples of mirror anamorphosis were first seen in the late Renaissance (sixteenth century).\n\nWith mirror anamorphosis, a conical or cylindrical mirror is placed on the drawing or painting to transform a flat distorted image into an apparently undistorted picture that can be viewed from many angles. The deformed image is painted on a plane surface surrounding the mirror. By looking into the mirror, a viewer can see the image undeformed.\n\nWith \"Channel anamorphosis\" or \"turning pictures\" two different images are on different sides of a corrugated carrier. A straight frontal view shows an unclear mix of the images, while each image can be viewed correctly from a certain angle.\n\nThe prehistoric cave paintings at Lascaux may make use of anamorphic technique, because the oblique angles of the cave would otherwise result in distorted figures from a viewer's perspective. The ancient historians Pliny and Tzetzes both record a sculpture competition between Alcamenes and Phidias to create an image of Minerva. Alcamenes' sculpture was beautiful, while Phidias' had grotesque proportions. Yet once both had been mounted on pillars, the decelerated perspective made Phidias' Minerva beautiful and Alcamenes' ugly.\n\nDuring the Renaissance, artists' experimentation with optics and perspective lead to more advanced development of anamorphic imagery. At this time, religious thought and science were equally important to the technique's growth in Europe. The earliest known example, known as \"Leonardo's Eye\", was executed by Leonardo da Vinci and is included in the \"Codex Atlanticus\" (1483-1518). He later completed several large-scale anamorphic commissions for the King of France. Vignolo credited Tommasso Lauretti as the originator of a perspectival anamorphic technique in one of the earliest written descriptions in \"Two Rules\", compiled between 1530 and 1540 but not published until 1583. Without access to Vignolo's work, many other descriptions and examples were created before 1583.\n\n\"The Ambassadors\" (c. 1533) by Hans Holbein the Younger is well known for the prominent oblique anamorphic transformation in the painting. In this artwork, a distorted shape lies diagonally across the bottom of the frame. Viewing this from an acute angle transforms it into the plastic image of a human skull. The painting is regarded as a \"vanitas\" - a meditation on the transience of life - with the skull a symbolic \"memento mori\". The altered perspective required to see the image reflects the contemporary practice of painting skulls on the reverse of otherwise tranquil paintings. Four centuries later, the painting inspired the psychoanalyst Jaques Lacan to note in ‘Of the Gaze as \"Objet Petit a\"’ (1973) that the use of anamorphism, particularly in this painting, is one of the few methods for making viewers aware of their gaze. \n\nBy the 17th century, a revival of fantastical anamorphic imagery occurred. Mirror anamorphosis emerged at this time in Italy and China. It remains uncertain whether Jesuit missionaries imported or exported the technique. Two major works on perspective were published: \"Perspective\"(1612) by Salomon de Caus, and \"Curious Perspective\" (1638) by Jean-Francois Niceron. Each contained extensive scientific and practical infromation on anamorphic imagery. In Niceron's work, three types of large-scale anamorphism are explained: 'optical' (looking horizontally); 'anoptric' (looking upwards); and 'catoptric' (looking down ie. from a mezzanine). A conical perspective is also described.\nBaroque \"trompe l'oeil\" murals often used anamorphism to combine actual architectural elements with illusory painted elements to create a seamless effect when viewed from a specific location. The dome and vault of the Church of St. Ignazio in Rome, painted by Andrea Pozzo, represented the pinnacle of illusion. Due to neighboring monks complaining about blocked light, Pozzo was commissioned to paint the ceiling to look like the inside of a dome, instead of building a real dome. As the ceiling is flat, there is only one spot where the illusion is perfect and a dome looks undistorted.\n\nAnamorphosis could be used to conceal images for privacy or personal safety, and many secret portraits were created of deposed royalty. A well-known anamorphic portrait of the English King Edward VI was completed a year before his execution in 1546, only visible when viewed through a hole in the frame. It was later hung at Whitehall Palace, and may have influenced Shakespeare during the writing of \"Richard II\". Many anamorphic portraits of King Charles I were created and shared following his 1649 execution. A secret mirror anamorphosis portrait of Bonnie Prince Charlie can only be recognized when a polished cylinder is placed in the correct position. To possess such an image would have been seen as treason in the aftermath of the 1746 Battle of Culloden.\n\nIn the eighteenth and nineteenth centuries, anamorphic images had come to be used more as children's games than fine art. By the twentieth century, some artists wanted to renew the technique of anamorphosis for aesthetic and conceptual effect. During the First World War Arthur Mole, an American commercial photographer, used anamorphic techniques to create patriotic images from massive assembled groups of soldiers and reservists. When seen from high above, the gathered people resolved into recognizable pictures.\n\nMarcel Duchamp was interested in anamorphosis. His last work \"\" (1946-66) used mild anamorphosis to force viewers into the position of peep-hole voyeurs in order to see a nude, anonymous human body.\n\nSurrealist artist Salvador Dalí used extreme foreshortening and anamorphism in his paintings and works, and had a glass floor installed in a room next to his studio to enable radical perspective studies from above and below. His Dalí Theatre and Museum in Figueres, Spain features a 3-dimensional anamorphic living-room installation with custom furniture that looks like the face of Mae West when seen from a certain viewpoint. Interestingly, Lacan also compared Holbein's 16th century painting to Dali's imagery, rather than the other way around.\n\nIn the twentieth century, artists began to play with perspective by drawing \"impossible objects\". These objects included stairs that always ascend, or cubes where the back meets the front. Such works were popularized by the artist M. C. Escher and the mathematician Roger Penrose. Although referred to as \"impossible objects\", such objects as the Necker Cube and the Penrose triangle can be sculpted in 3-D by using anamorphic illusion. When viewed at a certain angle, such sculptures appear as the so-called impossible objects.\n\nThe Ames room was invented by American scientist Adelbert Ames, Jr. in 1946. When viewed through a peephole, the room appears to have normal perspective. However, all other viewpoints reveal that the room is constructed of irregular trapezoids. Similar effects had been achieved during the Renaissance through the use of \"accelerated perspective\" in stage design. These included productions by Scamozzi (1588-9), Furtenbach (1625), Sabbattini (1637) and Troili (1672).\n\nOne of the most interesting effects of an Ames room is that the distorted perspective can make people and objects looks much bigger or smaller than they really are. For this reason, Ames rooms are widely used in cinema for practical special effects. A well-known example is the homes in the Shire from the Lord of the Rings and The Hobbit films. Through the use of forced perspective, the character of Gandalf appeared much larger than the characters of Frodo and Bilbo, without the use of digital effects.\n\nCinemascope, Panavision, Technirama, and other widescreen formats use anamorphosis to project a wider image from a narrower film frame. The IMAX company uses even more extreme anamorphic transformations to project moving images from a flat film frame onto the inside of a hemispheric dome, in its \"Omnimax\" or \"IMAX Dome\" process.\nThe technique of anamorphic projection can be seen quite commonly on text written at a very flat angle on roadways, such as \"Bus Lane\" or \"Children Crossing\", to make it easily read by drivers who otherwise would have difficulty reading obliquely as the vehicle approaches the text; when the vehicle is nearly above the text, its true abnormally elongated shape can be seen. Similarly, in many sporting stadiums, especially in Rugby football in Australia, it is used to promote company brands which are painted onto the playing surface; from the television camera angle, the writing appear as signs standing vertically within the field of play.\n\nMuch writing on shop windows is in principle anamorphic, as it was written mirror-reversed on the inside of the window glass.\n\nWhile not a widespread in contemporary art, anamorphosis as a technique has been used by contemporary artists in painting, photography, printmaking, sculpture, film and video, digital art and games, holography, street art and installation. The latter two art forms are largely practised in public areas such as parks, city centres and transit stations.\n\nIn 1975 a major exhibition was held focusing exclusively on anamorphic imagery - \"Anamorphoses: Games of Perception and Illusion in Art\". The artist Jan Beutener created \"The Room\" - a major new installation - specifically for the exhibit.\n\nThe \"Presence of Absence\" is an ongoing exhibition focusing on anamorphic art. It features work by Patrick Ireland.\n\nSince the mid-20th century, many artists have made use of anamorphosis in public artworks. American Land art pioneer Michael Heizer's \"Complex One\" (1972-1974), a massive earth and concrete structure in the Nevada desert, creates a rectangular frame for a mastaba when viewed from a specific location. Inspired by Luxor and other ancient monumental sites, it is part of the larger work \"City\", an enormous sculpture running a mile and a half long. The entire work will not be completed until 2020.\n\nThe artist Marcus Raetz created anamorphic sculptures ranging in size from large-scale public installation to something that could fit in your hand. One of his largest pieces, \"Kopf\", revealed the form of a person's head in profile when viewed from a specific vantage-point. It was installed in a public park in Basel, Switzerland.\n\nShigeo Fukuda, a Japanese artist and designer globally renowned for his satirical posters on anti-war and environmental advocacy, created posters and sculptures making use of both types of anamorphosis in the 1970s and 1980s. He also wrote multiple books on the topic of optical illusions.\n\nWhile anamorphic images were not his exclusive area of focus, the American artist Jonathan Borofsky created installations in the 1980's using anamorphic techniques, exhibiting at institutions such as the Museum of Modern Art. \nJean-Max Albert created three public sculptures using anamorphosis:\"Un carré pour un square\", \"Cube fantôme\" and \"Reflet anamorphose\" (1985). \n\nJonty Hurwitz, pioneered the use of a mathematical technique to create Catoptric sculptures that resolve in a cylinder. In 2013 he produced a public work for the Savoy Hotel's River Room. \n\nIn 2011, French artist François Abélanet's anamorphic garden work \"Qui croire? (Who to believe?)\" was featured in front of the Hôtel de Ville in Paris.\n\nFelici Varini's 2014 work \"Three Ellipses for Three Locks\" in Hasselt, Belgium is an image of three loops that are made up of segments painted on to over 100 buildings. It is only visible from a specific vantage point over the city.\n\nThe Swedish artist Hans Hamngren produced and exhibited many examples of mirror anamorphosis in the 1960s and 1970s. \n\nBeginning in 1967, Dutch artist Jan Dibbets based an entire series of photographic work titled \"Perspective Corrections\" on the distortion of reality through perspective anamorphosis. This involved the incorporation of land art into his work, where areas dug out of the Earth formed squares from specific perspectives.\n\nAnamorphic effects are popular in street art, sometimes called \"Slant Art\" when accomplished on sidewalks. Examples are the sidewalk chalk drawings of Kurt Wenner and Julian Beever, where the chalked image, the pavement, and the architectural surroundings all become part of an illusion. Art of this style can be produced by taking a photograph of an object or setting at a sharp oblique angle, then putting a grid over the photograph. Another elongated grid is placed on the sidewalk based on a specific perspective, and visual elements of one are transcribed into the other, one grid square at a time.\n\nIn 2016, the street artist JR completed a massive temporary anamorphic illusion over the Louvre's pyramid, making the modern structure disappear and the original building appear as though it was still in the 17th century.\n\nSince 1993, Myrna Hoffman’s company, OOZ & OZ, has been producing mirror anamorphosis art kits and activities for children, such as the \"Morph-O-Scopes\" kit. Hoffman's kits have earned more than two dozen national toy awards.\n\nAnamorphic portraits were produced of members of the group Steeleye Span for the sleeve of their album All Around My Hat in 1975 by John O'Connor, a friend of one of them. The album packaging included a lyric sheet with holes punched in the side through which to view the portraits in their correct perspective.\n\nRick Wakeman's 1976 album \"No Earthly Connection\" featured front and back cover photographs that are mirror anamorphoses. The original vinyl release included a mirrored Mylar sheet which could be curled into a cylinder for viewing the images.\n\nIn the 2007 detective film \"Anamorph\", the plot line revolves around the solution of gruesome anamorphically distorted images.\n\nSome 0.5 liter Sprite bottles in Europe were imprinted with what appeared to be an extra \"bar code\". When the bottle was tilted towards the mouth while drinking, the bar code resolved into writing, due to the oblique perspective.\n\nThe 2009 video game \"\" has a series of riddles posed by the classic Batman antagonist The Riddler, the solution of which is based on perspective anamorphosis.\n\nIn 2013, Honda released a commercial which incorporated a series of illusions based on anamorphosis.\n\n\n\n\n"}
{"id": "51477178", "url": "https://en.wikipedia.org/wiki?curid=51477178", "title": "Approximate group", "text": "Approximate group\n\nIn mathematics, an approximate group is a subset of a group which behaves like a subgroup \"up to a constant error\", in a precise quantitative sense (so the term approximate subgroup may be more correct). For example, it is required that the set of products of elements in the subset be not much bigger than the subset itself (while for a subgroup it is required that they be equal). The notion was introduced in the 2010s but can be traced to older sources in additive combinatorics.\n\nLet formula_1 be a group and formula_2; for two subsets formula_3 we denote by formula_4 the set of all products formula_5. A non-empty subset formula_6 is a \"formula_7-approximate subgroup\" of formula_1 if: \nIt is immediately verified that a 1-approximate subgroup is the same thing as a genuine subgroup. Of course this definition is only interesting when formula_7 is small compared to formula_15 (in particular, any subset formula_16 is a formula_17-approximate subgroup). In applications it is often used with formula_18 being fixed and formula_15 going to infinity.\n\nExamples of approximate subgroups which are not groups are given by symmetric intervals and more generally arithmetic progressions in the integers. Indeed, for all formula_20 the subset formula_21 is a 2-approximate subgroup: the set formula_22 is contained in the union of the two translates formula_23 and formula_24 of formula_25. A \"generalised arithmetic progression\" in formula_26 is a subset in formula_26 of the form formula_28, and it is a formula_29-approximate subgroup.\n\nA more general example is given by balls in the word metric in finitely generated nilpotent groups.\n\nApproximate subgroups of the integer group formula_26 were completely classified by Imre Z. Ruzsa and Freiman. The result is stated as follows: \nThe constants formula_39 can be estimated sharply. In particular formula_40 is contained in at most formula_41translates of formula_35: this means that approximate subgroups of formula_26 are \"almost\" generalised arithmetic progressions.\n\nThe work of Breuillard–Green–Tao (the culmination of an effort started a few years earlier by various other people) is a vast generalisation of this result. In a very general form its statement is the following: \nThe statement also gives some information on the characteristics (rank and step) of the nilpotent group formula_52.\n\nIn the case where formula_1 is a finite matrix group the results can be made more precise, for instance: \nThe theorem applies for example to formula_71; the point is that the constant does not depend on the cardinality formula_72 of the field. In some sense this says that there are no interesting approximate subgroups (besides genuine subgroups) in finite simple linear groups (they are either \"trivial\", that is very small, or \"not proper\", that is almost equal to the whole group).\n\nThe Breuillard–Green–Tao theorem on classification of approximate groups can be used to give a new proof of Gromov's theorem on groups of polynomial growth. The result obtained is actually a bit stronger since it establishes that there exists a \"growth gap\" between virtually nilpotent groups (of polynomial growth) and other groups; that is, there exists a (superpolynomial) function formula_73 such that any group with growth function bounded by a multiple of formula_73 is virtually nilpotent.\n\nOther applications are to the construction of expander graphs from the Cayley graphs of finite simple groups, and to the related topic of superstrong approximation.\n\n"}
{"id": "54603387", "url": "https://en.wikipedia.org/wiki?curid=54603387", "title": "Archives of American Mathematics", "text": "Archives of American Mathematics\n\nThe Archives of American Mathematics, located at the University of Texas at Austin, aims to collect, preserve, and provide access to the papers principally of American mathematicians and the records of American mathematical organizations.\n\nThe Archives began in 1975 at the University of Texas at Austin with the preservation of the papers of Texas mathematicians R.L. Moore and H.S. Vandiver.\n\nIn 1978, the Mathematical Association of America established the university as the official repository for its archival records and the name \"Archives of American Mathematics\" was adopted to encompass all of the mathematical archival collections at the university. Originally a part of the Harry Ransom Center, in 1984, the Archives was added to the special collections of the Briscoe Center for American History at the University of Texas at Austin.\n\nThe AAM includes approximately 120 collections.\n\n\nSignificant archives of American mathematicians and their organizations are held by other repositories. The following are examples which include a few Canadian collections with substantial United States connections. For the complete holdings, the catalogs of the individual repositories would need to be consulted. In addition, the archives of academic institutions will typically include administrative records of mathematics departments and clubs as well as the papers of faculty.\n\n\n"}
{"id": "5753249", "url": "https://en.wikipedia.org/wiki?curid=5753249", "title": "Artin–Hasse exponential", "text": "Artin–Hasse exponential\n\nIn mathematics, the Artin–Hasse exponential, introduced by , is the power series given by\n\nOne motivation for considering this series to be analogous to the exponential function comes from infinite products. In the ring of formal power series Q<nowiki></nowiki>\"x\"<nowiki></nowiki> we have the identity\n\nwhere μ(n) is the Möbius function. This identity can be verified by showing the logarithmic derivative of the two sides are equal and that both sides have the same constant term. In a similar way, one can verify a product expansion for the Artin–Hasse exponential:\n\nSo passing from a product over all \"n\" to a product over only \"n\" prime to \"p\", which is a typical operation in \"p\"-adic analysis, leads from \"e\" to \"E\"(\"x\").\n\nThe coefficients of \"E\"(\"x\") are rational. We can use either formula for \"E\"(\"x\") to prove that, unlike \"e\", all of its coefficients are \"p\"-integral; in other words, the denominators of the coefficients of \"E\"(\"x\") are not divisible by \"p\". A first proof uses the definition of \"E\"(\"x\") and Dwork's lemma, which says that a power series \"f\"(\"x\") = 1 + ... with rational coefficients has \"p\"-integral coefficients if and only if \"f\"(\"x\")/\"f\"(\"x\") ≡ 1 mod \"pZ<nowiki></nowiki>\"x\"<nowiki></nowiki>. When \"f\"(\"x\") = \"E\"(\"x\"), we have \"f\"(\"x\")/\"f\"(\"x\") = \"e\", whose constant term is 1 and all higher coefficients are in \"pZ.\nA second proof comes from the infinite product for \"E\"(\"x\"): each exponent -μ(\"n\")/\"n\" for \"n\" not divisible by \"p\" is a \"p\"-integral, and when a rational number \"a\" is \"p\"-integral all coefficients in the binomial expansion of (1 - \"x\") are \"p\"-integral by \"p\"-adic continuity of the binomial coefficient polynomials \"t\"(\"t\"-1)...(\"t\"-\"k\"+1)/\"k\"! in \"t\" together with their obvious integrality when \"t\" is a nonnegative integer (\"a\" is a \"p\"-adic limit of nonnegative integers) . Thus each factor in the product of \"E\"(\"x\") has \"p\"-integral coefficients, so \"E\"(\"x\") itself has \"p\"-integral coefficients.\n\nThe Artin–Hasse exponential is the generating function for the probability a uniformly randomly selected element of \"S\" (the symmetric group with \"n\" elements) has \"p\"-power order (the number of which is denoted by \"t\"):\n\nThis gives a third proof that the coefficients of \"E\"(\"x\") are \"p\"-integral, using the theorem of Frobenius that in a finite group of order divisible by \"d\" the number of elements of order dividing \"d\" is also divisible by \"d\". Apply this theorem to the \"n\"th symmetric group with \"d\" equal to the highest power of \"p\" dividing \"n\"!.\n\nMore generally, for any topologically finitely generated profinite group \"G\" there is an identity\nwhere \"H\" runs over open subgroups of \"G\" with finite index (there are finitely many of each index since \"G\" is topologically finitely generated) and \"a\" is the number of continuous homomorphisms from \"G\" to \"S\". Two special cases are worth noting. (1) If \"G\" is the \"p\"-adic integers, it has exactly one open subgroup of each \"p\"-power index and a continuous homomorphism from \"G\" to \"S\" is essentially the same thing as choosing an element of \"p\"-power order in \"S\", so we have recovered the above combinatorial interpretation of the Taylor coefficients in the Artin–Hasse exponential series. (2) If \"G\" is a finite group then the sum in the exponential is a finite sum running over all subgroups of \"G\", and continuous homomorphisms from \"G\" to \"S\" are simply homomorphisms from \"G\" to \"S\". The result in this case is due to Wohlfahrt (1977). The special case when \"G\" is a finite cyclic group is due to Chowla, Herstein, and Scott (1952), and takes the form\nwhere \"a\" is the number of solutions to \"g\" = 1 in \"S\".\n\nDavid Roberts provided a natural combinatorial link between the Artin–Hasse exponential and the regular exponential in the spirit of the ergodic perspective (linking the \"p\"-adic and regular norms over the rationals) by showing that the Artin–Hasse exponential is also the generating function for the probability that an element of the symmetric group is unipotent in characteristic \"p\", whereas the regular exponential is the probability that an element of the same group is unipotent in characteristic zero.\n\nAt the 2002 PROMYS program, Keith Conrad conjectured that the coefficients of formula_7 are uniformly distributed in the p-adic integers with respect to the normalized Haar measure, with supporting computational evidence. The problem is still open.\n\nDinesh Thakur has also posed the problem of whether the Artin–Hasse exponential reduced mod \"p\" is transcendental over formula_8.\n\n\n"}
{"id": "44355177", "url": "https://en.wikipedia.org/wiki?curid=44355177", "title": "Audrey Ruth Briggs", "text": "Audrey Ruth Briggs\n\nAudrey Ruth Churchill (née Briggs) (1920–2005) was a cryptanalyst at Bletchley Park during the Second World War. She graduated in Modern Languages from Cambridge and from 1942-1945, as an expert in German, worked at Bletchley Park as a member of the Z Watch, which translated the decrypted messages. She worked variously in Huts 4 and 5, Block A(N), and Naval Section NS I - German Cryptography.\n\nIn 1946 she married former SOE Officer Major Oliver Churchill DSO MC in Worcester Cathedral where her father GW Briggs was a Canon, and her oldest son is Toby Churchill.\n"}
{"id": "1188184", "url": "https://en.wikipedia.org/wiki?curid=1188184", "title": "Block design", "text": "Block design\n\nIn combinatorial mathematics, a block design is a set together with a family of subsets (repeated subsets are allowed at times) whose members are chosen to satisfy some set of properties that are deemed useful for a particular application. These applications come from many areas, including experimental design, finite geometry, software testing, cryptography, and algebraic geometry. Many variations have been examined, but the most intensely studied are the balanced incomplete block designs (BIBDs or 2-designs) which historically were related to statistical issues in the design of experiments.\n\nA block design in which all the blocks have the same size is called \"uniform\". The designs discussed in this article are all uniform. Pairwise balanced designs (PBDs) are examples of block designs that are not necessarily uniform.\n\nGiven a finite set \"X\" (of elements called \"points\") and integers \"k\", \"r\", \"λ\" ≥ 1, we define a \"2-design\" (or \"BIBD\", standing for balanced incomplete block design) \"B\" to be a family of \"k\"-element subsets of \"X\", called \"blocks\", such that any \"x\" in \"X\" is contained in \"r\" blocks, and any pair of distinct points \"x\" and \"y\" in \"X\" is contained in \"λ\" blocks.\n\n\"Family\" in the above definition can be replaced by \"set\" if repeated blocks are not allowed. Designs in which repeated blocks are not allowed are called \"simple\".\n\nHere \"v\" (the number of elements of \"X\", called points), \"b\" (the number of blocks), \"k\", \"r\", and λ are the \"parameters\" of the design. (To avoid degenerate examples, it is also assumed that \"v\" > \"k\", so that no block contains all the elements of the set. This is the meaning of \"incomplete\" in the name of these designs.) In a table:\n\nThe design is called a (\"v\", \"k\", \"λ\")-design or a (\"v\", \"b\", \"r\", \"k\", \"λ\")-design. The parameters are not all independent; \"v\", \"k\", and λ determine \"b\" and \"r\", and not all combinations of \"v\", \"k\", and \"λ\" are possible. The two basic equations connecting these parameters are\nobtained by counting the number of pairs (\"B\", \"p\") where \"B\" is a block and \"p\" is a point in that block, and\nobtained from the count of triples (\"p\", \"q\", \"B\") where \"p\" and \"q\" are distinct points and \"B\" is a block that contains them both, and dividing this count by \"v\".\n\nThese conditions are not sufficient as, for example, a (43,7,1)-design does not exist.\n\nThe \"order\" of a 2-design is defined to be \"n\" = \"r\" − \"λ\". The complement of a 2-design is obtained by replacing each block with its complement in the point set \"X\". It is also a 2-design and has parameters \"v\"′ = \"v\", \"b\"′ = \"b\", \"r\"′ = \"b\" − \"r\", \"k\"′ = \"v\" − \"k\", \"λ\"′ = \"λ\" + \"b\" − 2\"r\". A 2-design and its complement have the same order.\n\nA fundamental theorem, Fisher's inequality, named after the statistician Ronald Fisher, is that \"b\" ≥ \"v\" in any 2-design.\n\nThe unique (6,3,2)-design has 10 blocks (\"b\" = 10) and each element is repeated 5 times (\"r\" = 5). Using the symbols 0 − 5, the blocks are the following triples:\n\nOne of four nonisomorphic (8,4,3)-designs has 14 blocks with each element repeated 7 times. Using the symbols 0 − 7 the blocks are the following 4-tuples:\n\nThe unique (7,3,1)-design has 7 blocks with each element repeated 3 times. Using the symbols 0 − 6, the blocks are the following triples:\n\nThe case of equality in Fisher's inequality, that is, a 2-design with an equal number of points and blocks, is called a symmetric design. Symmetric designs have the smallest number of blocks among all the 2-designs with the same number of points.\n\nIn a symmetric design \"r\" = \"k\" holds as well as \"b\" = \"v\", and, while it is generally not true in arbitrary 2-designs, in a symmetric design every two distinct blocks meet in \"λ\" points. A theorem of Ryser provides the converse. If \"X\" is a \"v\"-element set, and \"B\" is a \"v\"-element set of \"k\"-element subsets (the \"blocks\"), such that any two distinct blocks have exactly λ points in common, then (\"X, B\") is a symmetric block design.\n\nThe parameters of a symmetric design satisfy\nThis imposes strong restrictions on \"v\", so the number of points is far from arbitrary. The Bruck–Ryser–Chowla theorem gives necessary, but not sufficient, conditions for the existence of a symmetric design in terms of these parameters.\n\nThe following are important examples of symmetric 2-designs:\n\nFinite projective planes are symmetric 2-designs with \"λ\" = 1 and order \"n\" > 1. For these designs the symmetric design equation becomes:\n\nSince \"k\" = \"r\" we can write the \"order of a projective plane\" as \"n\" = \"k\" − 1 and, from the displayed equation above, we obtain \"v\" = (\"n\" + 1)\"n\"  +  1 = \"n\"  +  \"n\"  +  1 points in a projective plane of order \"n\".\n\nAs a projective plane is a symmetric design, we have \"b\" = \"v\", meaning that \"b\" = \"n\"  +  \"n\"  +  1 also. The number \"b\" is the number of \"lines\" of the projective plane. There can be no repeated lines since λ = 1, so a projective plane is a simple 2-design in which the number of lines and the number of points are always the same. For a projective plane, \"k\" is the number of points on each line and it is equal to \"n\" + 1. Similarly, \"r\" = \"n\" + 1 is the number of lines with which a given point is incident.\n\nFor \"n\" = 2 we get a projective plane of order 2, also called the Fano plane, with \"v\" = 4 + 2 + 1 = 7 points and 7 lines. In the Fano plane, each line has \"n\" + 1 = 3 points and each point belongs to \"n\" + 1 = 3 lines.\n\nProjective planes are known to exist for all orders which are prime numbers or powers of primes. They form the only known infinite family (with respect to having a constant λ value) of symmetric block designs.\n\nA biplane or biplane geometry is a symmetric 2-design with \"λ\" = 2; that is, every set of two points is contained in two blocks (\"lines\"), while any two lines intersect in two points. They are similar to finite projective planes, except that rather than two points determining one line (and two lines determining one point), two points determine two lines (respectively, points). A biplane of order \"n\" is one whose blocks have \"k\" = \"n\" + 2 points; it has \"v\" = 1 + (\"n\" + 2)(\"n\" + 1)/2 points (since \"r\" = \"k\").\n\nThe 18 known examples are listed below.\n\nAn Hadamard matrix of size \"m\" is an \"m\" × \"m\" matrix H whose entries are ±1 such that HH  = mI, where H is the transpose of H and I is the \"m\" × \"m\" identity matrix. An Hadamard matrix can be put into \"standardized form\" (that is, converted to an equivalent Hadamard matrix) where the first row and first column entries are all +1. If the size \"m\" > 2 then \"m\" must be a multiple of 4.\n\nGiven an Hadamard matrix of size 4\"a\" in standardized form, remove the first row and first column and convert every −1 to a 0. The resulting 0–1 matrix M is the incidence matrix of a symmetric 2-(4\"a\" − 1, 2\"a\" − 1, \"a\" − 1) design called an Hadamard 2-design. \nIt contains formula_5 blocks/points; each contains/is contained in formula_6 points/blocks. Each pair of points is contained in exactly formula_7 blocks.\n\nThis construction is reversible, and the incidence matrix of a symmetric 2-design with these parameters can be used to form an Hadamard matrix of size 4\"a\".\n\nA resolvable 2-design is a BIBD whose blocks can be partitioned into sets (called \"parallel classes\"), each of which forms a partition of the point set of the BIBD. The set of parallel classes is called a \"resolution\" of the design.\n\nIf a 2-(\"v\",\"k\",λ) resolvable design has \"c\" parallel classes, then \"b\"  ≥ \"v\" + \"c\" − 1.\n\nConsequently, a symmetric design can not have a non-trivial (more than one parallel class) resolution.\n\nArchetypical resolvable 2-designs are the finite affine planes. A solution of the famous 15 schoolgirl problem is a resolution of a 2-(15,3,1) design.\n\nGiven any positive integer \"t\", a \"t\"-design \"B\" is a class of \"k\"-element subsets of \"X\", called \"blocks\", such that every point \"x\" in \"X\" appears in exactly \"r\" blocks, and every \"t\"-element subset \"T\" appears in exactly λ blocks. The numbers \"v\" (the number of elements of \"X\"), \"b\" (the number of blocks), \"k\", \"r\", λ, and \"t\" are the \"parameters\" of the design. The design may be called a \"t\"-(\"v\",\"k\",λ)-design. Again, these four numbers determine \"b\" and \"r\" and the four numbers themselves cannot be chosen arbitrarily. The equations are\n\nwhere \"λ\" is the number of blocks that contain any \"i\"-element set of points.\n\nNote that formula_9.\n\nTheorem: Any \"t\"-(\"v\",\"k\",λ)-design is also an \"s\"-(\"v\",\"k\",λ)-design for any \"s\" with 1 ≤ \"s\" ≤ \"t\". (Note that the \"lambda value\" changes as above and depends on \"s\".)\n\nA consequence of this theorem is that every \"t\"-design with \"t\" ≥ 2 is also a 2-design.\n\nA \"t\"-(\"v\",\"k\",1)-design is called a Steiner system.\n\nThe term \"block design\" by itself usually means a 2-design.\n\nLet D = (\"X\", \"B\") be a t-(\"v\",\"k\",\"λ\") design and \"p\" a point of \"X\". The \"derived design\" \"D\" has point set \"X\" − {\"p\"} and as block set all the blocks of D which contain p with p removed. It is a (\"t\" − 1)-(\"v\" − 1, \"k\" − 1, \"λ\") design. Note that derived designs with respect to different points may not be isomorphic. A design E is called an \"extension\" of D if E has a point p such that E is isomorphic to D; we call D \"extendable\" if it has an extension.\n\nTheorem: If a \"t\"-(\"v\",\"k\",\"λ\") design has an extension, then \"k\" + 1 divides \"b\"(\"v\" + 1).\n\nThe only extendable projective planes (symmetric 2-(\"n\" + \"n\" + 1, \"n\" + 1, 1) designs) are those of orders 2 and 4.\n\nEvery Hadamard 2-design is extendable (to an Hadamard 3-design).\n\nTheorem:.\nIf D, a symmetric 2-(\"v\",\"k\",λ) design, is extendable, then one of the following holds:\n\nNote that the projective plane of order two is an Hadamard 2-design; the projective plane of order four has parameters which fall in case 2; the only other known symmetric 2-designs with parameters in case 2 are the order 9 biplanes, but none of them are extendable; and there is no known symmetric 2-design with the parameters of case 3.\n\nA design with the parameters of the extension of an affine plane, i.e., a 3-(\"n\" + 1, \"n\" + 1, 1) design, is called a finite inversive plane, or Möbius plane, of order \"n\".\n\nIt is possible to give a geometric description of some inversive planes, indeed, of all known inversive planes. An \"ovoid\" in PG(3,\"q\") is a set of \"q\" + 1 points, no three collinear. It can be shown that every plane (which is a hyperplane since the geometric dimension is 3) of PG(3,\"q\") meets an ovoid \"O\" in either 1 or \"q\" + 1 points. The plane sections of size \"q\" + 1 of \"O\" are the blocks of an inversive plane of order \"q\". Any inversive plane arising this way is called \"egglike\". All known inversive planes are egglike.\n\nAn example of an ovoid is the elliptic quadric, the set of zeros of the quadratic form\nwhere f is an irreducible quadratic form in two variables over GF(\"q\"). [\"f\"(\"x\",\"y\") = \"x\" + \"xy\" + \"y\" for example].\n\nIf \"q\" is an odd power of 2, another type of ovoid is known – the Suzuki–Tits ovoid.\n\nTheorem. Let \"q\" be a positive integer, at least 2. (a) If \"q\" is odd, then any ovoid is projectively equivalent to the elliptic quadric in a projective geometry PG(3,\"q\"); so \"q\" is a prime power and there is a unique egglike inversive plane of order \"q\". (But it is unknown if non-egglike ones exist.) (b) if \"q\" is even, then \"q\" is a power of 2 and any inversive plane of order \"q\" is egglike (but there may be some unknown ovoids).\n\nAn \"n\"-class association scheme consists of a set \"X\" of size \"v\" together with a partition \"S\" of \"X\" × \"X\" into \"n\" + 1 binary relations, R, R, ..., R. A pair of elements in relation R are said to be \"i\"th–\"associates\". Each element of \"X\" has \"n\"  \"i\"th associates. Furthermore:\n\n\nAn association scheme is \"commutative\" if formula_17 for all \"i\", \"j\" and \"k\". Most authors assume this property.\n\nA partially balanced incomplete block design with \"n\" associate classes (PBIBD(\"n\")) is a block design based on a \"v\"-set X with \"b\" blocks each of size \"k\" and with each element appearing in \"r\" blocks, such that there is an association scheme with \"n\" classes defined on \"X\" where, if elements \"x\" and \"y\" are \"i\"th associates, 1 ≤ \"i\" ≤ \"n\", then they are together in precisely λ blocks.\n\nA PBIBD(\"n\") determines an association scheme but the converse is false.\n\nLet \"A\"(3) be the following association scheme with three associate classes on the set \"X\" = {1,2,3,4,5,6}. The (\"i\",\"j\") entry is \"s\" if elements \"i\" and \"j\" are in relation R.\n\nThe blocks of a PBIBD(3) based on \"A\"(3) are:\n\nThe parameters of this PBIBD(3) are: \"v\"  =  6, \"b\"  =  8, \"k\"  =  3, \"r\"  =  4 and λ = λ = 2 and λ = 1. Also, for the association scheme we have \"n\"  =  \"n\"  =  1 and \"n\"  =  \"n\"  =  2.\n\nThe parameters of a PBIBD(\"m\") satisfy:\n\nA PBIBD(1) is a BIBD and a PBIBD(2) in which λ  =  λ is a BIBD.\n\nPBIBD(2)s have been studied the most since they are the simplest and most useful of the PBIBDs. They fall into six types based on a classification of the \"then known\" PBIBD(2)s by :\n\nThe mathematical subject of block designs originated in the statistical framework of design of experiments. These designs were especially useful in applications of the technique of analysis of variance (ANOVA). This remains a significant area for the use of block designs.\n\nWhile the origins of the subject are grounded in biological applications (as is some of the existing terminology), the designs are used in many applications where systematic comparisons are being made, such as in software testing.\n\nThe incidence matrix of block designs provide a natural source of interesting block codes that are used as error correcting codes. The rows of their incidence matrices are also used as the symbols in a form of pulse-position modulation.\n\nSuppose that skin cancer researchers want to test three different sunscreens. They coat two different sunscreens on the upper sides of the hands of a test person. After a UV radiation they record the skin irritation in terms of sunburn. The number of treatments is 3 (sunscreens) and the block size is 2 (hands per person).\n\nA corresponding BIBD can be generated by the R-function \"design.bib\" of the R-package agricolae and is specified in the following table:\n\nThe investigator chooses the parameters , and for the block design which are then inserted into the R-function. Subsequently, the remaining parameters and are determined automatically.\n\nUsing the basic relations we calculate that we need blocks, that is, 3 test people in order to obtain a balanced incomplete block design. Labeling the blocks and , to avoid confusion, we have the block design,\n\nA corresponding incidence matrix is specified in the following table:\nEach treatment occurs in 2 blocks, so .\n\nJust one block () contains the treatments 1 and 2 simultaneously and the same applies to the pairs of treatments (1,3) and (2,3). Therefore, .\n\nIt is impossible to use a complete design (all treatments in each block) in this example because there are 3 sunscreens to test, but only 2 hands on each person.\n\n\n\n\n\n\n"}
{"id": "7237011", "url": "https://en.wikipedia.org/wiki?curid=7237011", "title": "CRYPTON", "text": "CRYPTON\n\nIn cryptography, CRYPTON is a symmetric block cipher submitted as a candidate for the Advanced Encryption Standard (AES). It is very efficient in hardware implementations and was designed by Chae Hoon Lim of Future Systems Inc.\n\nThe CRYPTON algorithm processes blocks of 128 bits in the form of 4×4 byte arrays. The round transformation consists of four steps: byte-wise substitution, column-wise bit permutation, column-to-row transposition and finally key addition. CRYPTON uses 12 rounds of this encryption process. Due to the algorithm's nature, the decryption process can be made identical to the encryption process using a different key.\n\n\n"}
{"id": "37966737", "url": "https://en.wikipedia.org/wiki?curid=37966737", "title": "Cohen algebra", "text": "Cohen algebra\n\nIn mathematical set theory, a Cohen algebra, named after Paul Cohen, is a type of Boolean algebra used in the theory of forcing. A Cohen algebra is a Boolean algebra whose completion is isomorphic to the completion of a free Boolean algebra . \n\n \n"}
{"id": "44771465", "url": "https://en.wikipedia.org/wiki?curid=44771465", "title": "David G. Cantor", "text": "David G. Cantor\n\nDavid Geoffrey Cantor (1935 – November 19, 2012) was an American mathematician, specializing in number theory and combinatorics. The Cantor–Zassenhaus algorithm for factoring polynomials is named after him; he and Hans Zassenhaus published it in 1981.\n\nCantor did his undergraduate studies at the California Institute of Technology, graduating in 1956, and earned his doctorate from the University of California, Los Angeles in 1960, supervised by Basil Gordon and Ernst Straus. He became an assistant professor at the University of Washington in 1962, moved back to UCLA in 1964, and retired in 1991.\n\nIn 2012, he became one of the inaugural fellows of the American Mathematical Society.\n\n"}
{"id": "1658083", "url": "https://en.wikipedia.org/wiki?curid=1658083", "title": "Direction of arrival", "text": "Direction of arrival\n\nIn signal processing literature, direction of arrival (DOA) denotes the direction from which usually a propagating wave arrives at a point, where usually a set of sensors are located. These set of sensors forms what is called a sensor array. Often there is the associated technique of beamforming which is estimating the signal from a given direction. Various engineering problems addressed in the associated literature are:\n\n"}
{"id": "185663", "url": "https://en.wikipedia.org/wiki?curid=185663", "title": "Division by zero", "text": "Division by zero\n\nIn mathematics, division by zero is division where the divisor (denominator) is zero. Such a division can be formally expressed as \"a\"/0 where \"a\" is the dividend (numerator). In ordinary arithmetic, the expression has no meaning, as there is no number which, multiplied by 0, gives \"a\" (assuming \"a\"≠0), and so division by zero is undefined. Since any number multiplied by zero is zero, the expression 0/0 is also undefined; when it is the form of a limit, it is an indeterminate form. Historically, one of the earliest recorded references to the mathematical impossibility of assigning a value to \"a\"/0 is contained in George Berkeley's criticism of infinitesimal calculus in 1734 in \"The Analyst\" (\"ghosts of departed quantities\").\n\nThere are mathematical structures in which \"a\"/0 is defined for some \"a\" such as in the Riemann sphere and the projectively extended real line; however, such structures cannot satisfy every ordinary rule of arithmetic (the field axioms).\n\nIn computing, a program error may result from an attempt to divide by zero. Depending on the programming environment and the type of number (e.g. floating point, integer) being divided by zero, it may generate positive or negative infinity by the IEEE 754 floating point standard, generate an exception, generate an error message, cause the program to terminate, result in a special not-a-number value, a freeze via infinite loop, or a crash.\n\nWhen division is explained at the elementary arithmetic level, it is often considered as splitting a set of objects into equal parts. As an example, consider having ten cookies, and these cookies are to be distributed equally to five people at a table. Each person would receive formula_1 = 2 cookies. Similarly, if there are ten cookies, and only one person at the table, that person would receive formula_2 = 10 cookies.\n\nSo, for dividing by zero, what is the number of cookies that each person receives when 10 cookies are evenly distributed amongst 0 people at a table? Certain words can be pinpointed in the question to highlight the problem. The problem with this question is the \"when\". There is no way to evenly distribute 10 cookies to nobody. In mathematical jargon, a set of 10 items cannot be partitioned into 0 subsets. So formula_3, at least in elementary arithmetic, is said to be either meaningless, or undefined.\n\nSimilar problems occur if one has 0 cookies and 0 people, but this time the problem is in the phrase \"the number\". A partition is possible (of a set with 0 elements into 0 parts), but since the partition has 0 parts, vacuously every set in our partition has a given number of elements, be it 0, 2, 5, or 1000.\n\nIf there are, say, 5 cookies and 2 people, the problem is in \"evenly distribute\". In any integer partition of a 5-set into 2 parts, one of the parts of the partition will have more elements than the other. But the problem with 5 cookies and 2 people can be solved by cutting one cookie in half. The problem with 5 cookies and 0 people cannot be solved in any way that preserves the meaning of \"divides\".\n\nAnother way of looking at division by zero is that division can always be checked using multiplication. Considering the 10/0 example above, setting x = 10/0, if \"x\" equals ten divided by zero, then \"x\" times zero equals ten, but there is no \"x\" that, when multiplied by zero, gives ten (or any other number than zero). If instead of x=10/0 we have x=0/0, then every \"x\" satisfies the question 'what number x, multiplied by zero, gives zero?'\n\nThe \"Brahmasphutasiddhanta\" of Brahmagupta (598–668) is the earliest known text to treat zero as a number in its own right and to define operations involving zero. The author could not explain division by zero in his texts: his definition can be easily proven to lead to algebraic absurdities. According to Brahmagupta,\n\nA positive or negative number when divided by zero is a fraction with the zero as denominator. Zero divided by a negative or positive number is either zero or is expressed as a fraction with zero as numerator and the finite quantity as denominator. Zero divided by zero is zero.\n\nIn 830, Mahavira tried unsuccessfully to correct Brahmagupta's mistake in his book in \"Ganita Sara Samgraha\": \"A number remains unchanged when divided by zero.\"\n\nThe four basic operations − addition, subtraction, multiplication and division − as applied to whole numbers (positive integers), with some restrictions, in elementary arithmetic are used as a framework to support the extension of the realm of numbers to which they apply. For instance, to make it possible to subtract any whole number from another, the realm of numbers must be expanded to the entire set of integers in order to incorporate the negative integers. Similarly, to support division of any integer by any other, the realm of numbers must expand to the rational numbers. During this gradual expansion of the number system, care is taken to ensure that the \"extended operations\", when applied to the older numbers, do not produce different results. Loosely speaking, since division by zero has no meaning (is \"undefined\") in the whole number setting, this remains true as the setting expands to the real or even complex numbers.\n\nAs the realm of numbers to which these operations can be applied expands there are also changes in how the operations are viewed. For instance, in the realm of integers, subtraction is no longer considered a basic operation since it can be replaced by addition of signed numbers. Similarly, when the realm of numbers expands to include the rational numbers, division is replaced by multiplication by certain rational numbers. In keeping with this change of viewpoint, the question, \"Why can't we divide by zero?\", becomes \"Why can't a rational number have a zero denominator?\". Answering this revised question precisely requires close examination of the definition of rational numbers.\n\nIn the modern approach to constructing the field of real numbers, the rational numbers appear as an intermediate step in the development that is founded on set theory. First, the natural numbers (including zero) are established on an axiomatic basis such as Peano's axiom system and then this is expanded to the ring of integers. The next step is to define the rational numbers keeping in mind that this must be done using only the sets and operations that have already been established, namely, addition, multiplication and the integers. Starting with the set of ordered pairs of integers, } with , define a binary relation on this set by if and only if . This relation is shown to be an equivalence relation and its equivalence classes are then defined to be the rational numbers. It is in the formal proof that this relation is an equivalence relation that the requirement that the second coordinate is not zero is needed (for verifying transitivity).\n\nThe above explanation may be too abstract and technical for many purposes, but if one assumes the existence and properties of the rational numbers, as is commonly done in elementary mathematics, the \"reason\" that division by zero is not allowed is hidden from view. Nevertheless, a (non-rigorous) justification can be given in this setting.\n\nIt follows from the properties of the number system we are using (that is, integers, rationals, reals, etc.), if then the equation is equivalent to . Assuming that is a number , then it must be that . However, the single number would then have to be determined by the equation , but every number satisfies this equation, so we cannot assign a numerical value to .\n\nThe concept that explains division in algebra is that it is the inverse of multiplication. For example,\n\nsince 2 is the value for which the unknown quantity in\nis true. But the expression\nrequires a value to be found for the unknown quantity in\nBut any number multiplied by 0 is 0 and so there is no number that solves the equation.\n\nThe expression\nrequires a value to be found for the unknown quantity in\nAgain, any number multiplied by 0 is 0 and so this time every number solves the equation instead of there being a single number that can be taken as the value of 0/0.\n\nIn general, a single value can't be assigned to a fraction where the denominator is 0 so the value remains undefined.\n\nA compelling reason for not allowing division by zero is that, if it were allowed, many absurd results (i.e., fallacies) would arise. When working with numerical quantities it is easy to determine when an illegal attempt to divide by zero is being made. For example, consider the following computation.\n\nWith the following assumptions:\n\nThe following must be true:\n\nDividing by zero gives:\n\nSimplified, this yields:\n\nThe fallacy here is the assumption that dividing by 0 is a legitimate operation with the same properties as dividing by any other number.\n\nHowever, it is possible to disguise a division by zero in an algebraic argument, leading to invalid proofs that, for instance, such as the following:\nThe disguised division by zero occurs since is zero when .\n\nAt first glance it seems possible to define \"a\"/0 by considering the limit of \"a\"/\"b\" as \"b\" approaches 0.\n\nFor any positive \"a\", the limit from the right is\n\nhowever, the limit from the left is\n\nand so the formula_22 is undefined (the limit is also undefined for negative \"a\").\n\nFurthermore, there is no obvious definition of 0/0 that can be derived from considering the limit of a ratio. The limit\n\ndoes not exist. Limits of the form\n\nin which both \"ƒ\"(\"x\") and \"g\"(\"x\") approach 0 as \"x\" approaches 0, may equal any real or infinite value, or may not exist at all, depending on the particular functions \"ƒ\" and \"g\". These and other similar facts show that the expression 0/0 cannot be well-defined as a limit.\n\nA formal calculation is one carried out using rules of arithmetic, without consideration of whether the result of the calculation is well-defined. Thus, it is sometimes useful to think of \"a\"/0, where \"a\" ≠ 0, as being formula_25. This infinity can be either positive, negative, or unsigned, depending on context. For example, formally:\n\nAs with any formal calculation, invalid results may be obtained. A logically rigorous (as opposed to formal) computation would assert only that\n\nSince the one-sided limits are different, the two-sided limit does not exist in the standard framework of the real numbers. Also, the fraction 1/0 is left undefined in the extended real line, therefore it and\n\nare meaningless expressions.\n\nThe set formula_29 is the projectively extended real line, which is a one-point compactification of the real line. Here formula_25 means an unsigned infinity, an infinite quantity that is neither positive nor negative. This quantity satisfies formula_31, which is necessary in this context. In this structure, formula_32 can be defined for nonzero \"a\", and formula_33. It is the natural way to view the range of the tangent function and cotangent functions of trigonometry: tan(\"x\") approaches the single point at infinity as \"x\" approaches either formula_34 or formula_35 from either direction.\n\nThis definition leads to many interesting results. However, the resulting algebraic structure is not a field, and should not be expected to behave like one. For example, formula_36 is undefined in this extension of the real line.\n\nThe set formula_37 is the Riemann sphere, which is of major importance in complex analysis. Here too formula_25 is an unsigned infinity – or, as it is often called in this context, the point at infinity. This set is analogous to the projectively extended real line, except that it is based on the field of complex numbers. In the Riemann sphere, formula_39, but formula_40 is undefined, as is formula_41.\n\nThe negative real numbers can be discarded, and infinity introduced, leading to the set [0, ∞], where division by zero can be naturally defined as \"a\"/0 = ∞ for positive \"a\". While this makes division defined in more cases than usual, subtraction is instead left undefined in many cases, because there are no negative numbers.\n\nAlthough division by zero cannot be sensibly defined with real numbers and integers, it is possible to consistently define it, or similar operations, in other mathematical structures.\n\nIn the hyperreal numbers and the surreal numbers, division by zero is still impossible, but division by non-zero infinitesimals is possible.\n\nIn distribution theory one can extend the function formula_42 to a distribution on the whole space of real numbers (in effect by using Cauchy principal values). It does not, however, make sense to ask for a 'value' of this distribution at \"x\" = 0; a sophisticated answer refers to the singular support of the distribution.\n\nIn matrix algebra (or linear algebra in general), one can define a pseudo-division, by setting \"a\"/\"b\" = \"ab\", in which \"b\" represents the pseudoinverse of \"b\". It can be proven that if \"b\" exists, then \"b\" = \"b\". If \"b\" equals 0, then b = 0.\n\nAny number system that forms a commutative ring—for instance, the integers, the real numbers, and the complex numbers—can be extended to a wheel in which division by zero is always possible; however, in such a case, \"division\" has a slightly different meaning.\n\nThe concepts applied to standard arithmetic are similar to those in more general algebraic structures, such as rings and fields. In a field, every nonzero element is invertible under multiplication; as above, division poses problems only when attempting to divide by zero. This is likewise true in a skew field (which for this reason is called a division ring). However, in other rings, division by nonzero elements may also pose problems. For example, the ring Z/6Z of integers mod 6. The meaning of the expression formula_43 should be the solution \"x\" of the equation formula_44. But in the ring Z/6Z, 2 is a zero divisor. This equation has two distinct solutions, \"x\" = 1 and \"x\" = 4, so the expression formula_43 is undefined.\n\nIn field theory, the expression formula_46 is only shorthand for the formal expression \"ab\", where \"b\" is the multiplicative inverse of \"b\". Since the field axioms only guarantee the existence of such inverses for nonzero elements, this expression has no meaning when \"b\" is zero. Modern texts, that define fields as a special type of ring, include the axiom 0 ≠ 1 for fields (or its equivalent) so that the zero ring is excluded from being a field. In the zero ring, division by zero is possible, which shows that the other field axioms are not sufficient to exclude division by zero in a field.\n\nThe IEEE floating-point standard, supported by almost all modern floating-point units, specifies that every floating point arithmetic operation, including division by zero, has a well-defined result. The standard supports signed zero, as well as infinity and NaN (\"not a number\"). There are two zeroes: +0 (\"positive zero\") and −0 (\"negative zero\") and this removes any ambiguity when dividing. In IEEE 754 arithmetic, \"a\" ÷ +0 is positive infinity when \"a\" is positive, negative infinity when \"a\" is negative, and NaN when \"a\" = ±0. The infinity signs change when dividing by −0 instead.\n\nThe justification for this definition is to preserve the sign of the result in case of arithmetic underflow. For example, in the single-precision computation 1/(\"x\"/2), where \"x\" = ±2, the computation \"x\"/2 underflows and produces ±0 with sign matching \"x\", and the result will be ±∞ with sign matching \"x\". The sign will match that of the exact result ±2, but the magnitude of the exact result is too large to represent, so infinity is used to indicate overflow.\n\nInteger division by zero is usually handled differently from floating point since there is no integer representation for the result. Some processors generate an exception when an attempt is made to divide an integer by zero, although others will simply continue and generate an incorrect result for the division. The result depends on how division is implemented, and can either be zero, or sometimes the largest possible integer.\n\nBecause of the improper algebraic results of assigning any value to division by zero, many computer programming languages (including those used by calculators) explicitly forbid the execution of the operation and may prematurely halt a program that attempts it, sometimes reporting a \"Divide by zero\" error. In these cases, if some special behavior is desired for division by zero, the condition must be explicitly tested (for example, using an if statement). Some programs (especially those that use fixed-point arithmetic where no dedicated floating-point hardware is available) will use behavior similar to the IEEE standard, using large positive and negative numbers to approximate infinities. In some programming languages, an attempt to divide by zero results in undefined behavior. The graphical programming language Scratch 2 used in many schools returns Infinity or -Infinity depending on the sign of the dividend.\n\nIn two's complement arithmetic, attempts to divide the smallest signed integer by formula_47 are attended by similar problems, and are handled with the same range of solutions, from explicit error conditions to undefined behavior.\n\nMost calculators will either return an error or state that 1/0 is undefined; however, some TI and HP graphing calculators will evaluate (1/0) to ∞.\n\nMicrosoft Math and Mathematica return \"ComplexInfinity\" for 1/0. Maple and SageMath return an error message for 1/0, and infinity for 1/0.0 (0.0 tells these systems to use floating point arithmetic instead of algebraic arithmetic).\n\n\n\n"}
{"id": "4849249", "url": "https://en.wikipedia.org/wiki?curid=4849249", "title": "Executable UML", "text": "Executable UML\n\nExecutable UML (xtUML or xUML) is both a software development method and a highly abstract software language. It was described for the first time in 2002 in the book \"Executable UML: A Foundation for Model-Driven Architecture\". The language \"combines a subset of the UML (Unified Modeling Language) graphical notation with executable semantics and timing rules.\" The Executable UML method is the successor to the Shlaer–Mellor method.\n\nExecutable UML models \"can be run, tested, debugged, and measured for performance.\", and can be compiled into a less abstract programming language to target a specific implementation. Executable UML supports model-driven architecture (MDA) through specification of platform-independent models, and the compilation of the platform-independent models into platform-specific models.\n\nExecutable UML is a higher level of abstraction than third-generation programming languages. This allows developers to develop at the level of abstraction of the application. The Executable UML aims for separation of concerns. This is supposed to increase ease of reuse and to lower the cost of software development. This also enables Executable UML domains to be cross-platform. That means it is not tied to any specific programming language, platform or technology.\n\nExecutable UML also allows for translation of platform-independent models (PIM) into platform-specific models (PSM). The Executable UML method enables valuing the model as intellectual property, since the model is a fully executable solution for the problem space.\n\nActions are specified in action language. This means that the automatic generation of implementation code from Executable UML models can be output in an optimized form.\n\nExecutable UML is intended to serve as executable code as well as documentation. The models are a graphical, executable specification of the problem space that is compiled into a target implementation. They are also intended to be human-readable.\n\nA system is composed of multiple subject matters, known as \"domains\" in Executable UML terms. Executable UML is used to model a domain at the level of abstraction of its subject matter independent of implementation concerns. The resulting domain model is represented by the following elements:\n\nExecutable UML requires identification of the domains (also known as: aspects or concerns) of the system. \"Each domain is an autonomous world inhabited by conceptual entities\" Each domain can be modeled independent of the other domains in the system, enabling a separation of concerns. As an example, domains for an automated teller system may include the following:\n\nThe \"separation of concerns\" enables each domain to be developed and verified independently of the other domains in the system by the respective domain experts.\n\nThe connections between domains are called \"bridges\". \"A bridge is a layering dependency between domains\". This means that the domains can place requirements upon other domains. It is recommended that bridges are agreed upon by the different domain experts.\n\nA domain can be marked as \"realized\" to indicate that the domain exists and does not need to be modeled. For example, a data access domain that uses a MySQL database would be marked as realized.\n\nConceptual entities, such as tangible things, roles, incidents, interactions, and specifications, specific to the domain being modeled are abstracted into \"classes\". Classes can have \"attributes\" and \"operations\".\n\nThe relationships between these classes will be indicated with \"associations\" and \"generalizations\". An association may require further abstraction as an \"Association Class\".\n\nConstraints on the class diagram can be written in both Action Language and Object Constraint Language (OCL).\n\nThe Executable UML method limits the UML elements that can be used in an Executable UML class diagram.\n\nAn Executable UML class diagram is meant to expose information about the domain. Too much complexity in the statechart diagrams is a good indicator that the class diagram should be reworked.\n\nClasses have lifecycles which are modeled in Executable UML with a statechart diagram. The statechart diagram defines the \"states\", \"transitions\", \"events\", and \"procedures\" that define a class' behaviour.\n\nEach state has only one procedure that is executed upon entry into that state. A procedure is composed of actions, which are specified in an action language.\n\nThe class and state models by themselves can only provide a static view of the domain. In order to have an executable model, there must be a way to create class instances, establish associations, perform operations on attributes, call state events, etc. In Executable UML, this is done using an action language that conforms to the UML Action Semantics.\n\nAction Semantics was added to the UML specification in 2001. The Action Semantics RFP was based on previous work in action languages supporting the Shlaer–Mellor method. Existing action languages are Object Action Language (OAL), Shlaer–Mellor Action Language (SMALL), Action Specification Language (ASL), That Action Language (TALL), Starr's Concise Relational Action Language (SCRALL), Platform-independent Action Language (PAL) and PathMATE Action Language (PAL). SCRALL is the only one that is a graphical action language.\n\nOnce a domain is modeled, it can be tested independent of the target implementation by executing the model. Each domain can be verified and validated independent of any other domain. This allows errors detected to be associated with the domain and independent of other system concerns.\n\nVerification will involve such things as human review of the models, performed by experts in the relevant domain, and automated checking of the Executable UML semantics. i.e., checking that the Executable UML model complies with the Executable UML metamodel.\n\nValidation will typically involve use of an Executable UML tool to execute the model. The execution can occur either before or after model compilation.\n\nIn order to support execution on the target implementation, the domain model must be translated into a less abstract form. This translation process is called \"model compilation\". Most model compilers target a known programming language, because this allows reuse of existing compiler technologies.\n\nOptimizing the domain models for target implementation reasons will reduce the level of abstraction, adversely affect domain independence, and increase the cost of reuse. In executable UML, optimizations are done by the model compiler either automatically or through \"marking\". Marking allows specific model elements to be targeted for specific lower-level implementations, and allows for broader architectural decisions, such as specifying that collections of objects should be implemented as a doubly linked list.\n\nIn MDA terms, the model compiler creates the PSM. The separation between the PIM and PSM in Executable UML disables the ability to round-trip engineer the model, and deters modifications to the PSM.\n\nExecutable UML defines execution semantics for a subset of the UML. Key aspects of the Executable UML subset include the following:\n\nThe Object Management Group has standardized the Foundational UML (fUML), which was strongly influenced by Executable UML.\n\nAction Language for Foundational UML (ALF), is a standard action language specification by the Object Management Group.\n\n\n\n"}
{"id": "49598229", "url": "https://en.wikipedia.org/wiki?curid=49598229", "title": "Faddeev–LeVerrier algorithm", "text": "Faddeev–LeVerrier algorithm\n\nIn mathematics (linear algebra), the Faddeev–LeVerrier algorithm is a recursive method to calculate the coefficients of the characteristic polynomial formula_1 of a square matrix, , named after Dmitry Konstantinovich Faddeev and Urbain Le Verrier. Calculation of this polynomial yields the eigenvalues of as its roots; as a matrix polynomial in the matrix itself, it vanishes by the fundamental Cayley–Hamilton theorem. Calculating determinants, however, is computationally cumbersome, whereas this efficient algorithm is computationally significantly more efficient (in NC complexity class).\n\nThe algorithm has been independently rediscovered several times, in some form or another. It was first published in 1840 by Urbain Le Verrier, subsequently redeveloped by \nP. Horst, Jean-Marie Souriau, in its present form here by Faddeev and Sominsky, and further by J. S. Frame, and others. (For historical points, see Householder. An elegant shortcut to the proof, bypassing Newton polynomials, was introduced by Hou. The bulk of the presentation here follows Gantmacher, p. 88.)\n\nThe objective is to calculate the coefficients of the characteristic polynomial of the matrix ,\nwhere, evidently, = 1 and = (−1) det .\n\nThe coefficients are determined recursively from the top down, by dint of the auxiliary matrices , \n\nThus, \netc.,\n\nObserve terminates the recursion at . This could be used to obtain the inverse or the determinant of .\n\nThe proof relies on the modes of the adjugate matrix, , the auxiliary matrices encountered.   \nThis matrix is defined by \nand is thus proportional to the resolvent \n\nIt is evidently a matrix polynomial in of degree . Thus,\nwhere one may define the harmless ≡0.\n\nInserting the explicit polynomial forms into the defining equation for the adjugate, above, \n\nNow, at the highest order, the first term vanishes by =0; whereas at the bottom order (constant in , from the defining equation of the adjugate, above),\nso that shifting the dummy indices of the first term yields \nwhich thus dictates the recursion\nfor =1.... Note that ascending index amounts to descending in powers of , but the polynomial coefficients are yet to be determined in terms of the s and .\n\nThis can be easiest achieved through the following auxiliary equation (Hou, 1998),\nThis is but the trace of the defining equation for by dint of Jacobi's formula,\n\nInserting the polynomial mode forms in this auxiliary equation yields\nso that \nand finally \nThis completes the recursion of the previous section, unfolding in descending powers of .\n\nFurther note in the algorithm that, more directly, \nand, in comportance with the Cayley–Hamilton theorem,\n\nThe final solution might be more conveniently expressed in terms of complete exponential Bell polynomials as \n\nformula_26\n\nFurthermore, formula_27, which confirms the above calculations.\n\nThe characteristic polynomial of matrix is thus formula_28; the determinant of is formula_29; the trace is 10=−\"c\"; and the inverse of is \n\nA compact determinant of an ×-matrix solution for the above Jacobi's formula may alternatively determine the coefficients ,\n"}
{"id": "16933890", "url": "https://en.wikipedia.org/wiki?curid=16933890", "title": "G2-structure", "text": "G2-structure\n\nIn differential geometry, a formula_1-structure is an important type of G-structure that can be defined on a smooth manifold. If \"M\" is a smooth manifold of dimension seven, then a G-structure is a reduction of structure group of the frame bundle of \"M\" to the compact, exceptional Lie group G.\n\nThe condition of \"M \" admitting a formula_1 structure is equivalent to any of the following conditions:\nThe last condition above correctly suggests that many manifolds admit formula_1-structures.\n\nA manifold with holonomy formula_1 was first introduced by Edmond Bonan in 1966, who constructed the parallel 3-form, the parallel 4-form and showed that this manifold was Ricci-flat. The first complete, but noncompact 7-manifolds with holonomy formula_1 were constructed by Robert Bryant and Salamon in 1989. The first compact 7-manifolds with holonomy formula_1 were constructed by Dominic Joyce in 1994, and compact formula_1 manifolds are sometimes known as \"Joyce manifolds\", especially in the physics literature. In 2013, it was shown by M. Firat Arikan, Hyunjoo Cho, and Sema Salur that any manifold with a spin structure, and, hence, a formula_1-structure, admits a compatible almost contact metric structure, and an explicit compatible almost contact structure was constructed for manifolds with formula_1-structure. In the same paper, it was shown that certain classes of formula_1-manifolds admit a contact structure.\n\nThe property of being a formula_1-manifold is much stronger than that of admitting a formula_1-structure. Indeed, a formula_1-manifold is a manifold with a formula_1-structure which is torsion-free.\n\nThe letter \"G\" occurring in the phrases \"G-structure\" and \"formula_1-structure\" refers to different things. In the first case, G-structures take their name from the fact that arbitrary Lie groups are typically denoted with the letter \"G\". On the other hand, the letter \"G\" in \"formula_1\" comes from the fact that the its Lie algebra is the seventh type (\"G\" being the seventh letter of the alphabet) in the classification of complex simple Lie algebras by Élie Cartan.\n\n"}
{"id": "6561586", "url": "https://en.wikipedia.org/wiki?curid=6561586", "title": "GEUP", "text": "GEUP\n\nGEUP is a commercial interactive geometry software program, similar to Cabri Geometry. Originally using the Spanish language, it was programmed by Ramón\nAlvarez Galván. Recent versions include support for three-dimensional geometry.\n\n"}
{"id": "2310978", "url": "https://en.wikipedia.org/wiki?curid=2310978", "title": "Gauss's lemma (number theory)", "text": "Gauss's lemma (number theory)\n\nGauss's lemma in number theory gives a condition for an integer to be a quadratic residue. Although it is not useful computationally, it has theoretical significance, being involved in some proofs of quadratic reciprocity.\n\nIt made its first appearance in Carl Friedrich Gauss's third proof (1808) of quadratic reciprocity and he proved it again in his fifth proof (1818).\n\nFor any odd prime let be an integer that is coprime to .\n\nConsider the integers\nand their least positive residues modulo . (These residues are all distinct, so there are ( of them.)\n\nLet be the number of these residues that are greater than . Then\nwhere formula_3 is the Legendre symbol.\n\nTaking = 11 and = 7, the relevant sequence of integers is\nAfter reduction modulo 11, this sequence becomes\nThree of these integers are larger than 11/2 (namely 6, 7 and 10), so = 3. Correspondingly Gauss's lemma predicts that\nThis is indeed correct, because 7 is not a quadratic residue modulo 11.\n\nThe above sequence of residues\nmay also be written\nIn this form, the integers larger than 11/2 appear as negative numbers. It is also apparent that the absolute values of the residues are a permutation of the residues\n\nA fairly simple proof, reminiscent of one of the simplest proofs of Fermat's little theorem, can be obtained by evaluating the product\nmodulo \"p\" in two different ways. On one hand it is equal to\n\nThe second evaluation takes more work. If is a nonzero residue modulo , let us define the \"absolute value\" of to be\nSince counts those multiples which are in the latter range, and since for those multiples, is in the first range, we have\nNow observe that the values are \"distinct\" for . Indeed, we have\nbecause is coprime to .\n\nThis gives = , since and are positive least residues. But there are exactly of them, so their values are a rearrangement of the integers . Therefore,\nComparing with our first evaluation, we may cancel out the nonzero factor\nand we are left with\nThis is the desired result, because by Euler's criterion the left hand side is just an alternative expression for the Legendre symbol formula_13.\n\nGauss's lemma is used in many, but by no means all, of the known proofs of quadratic reciprocity.\n\nFor example, Gotthold Eisenstein used Gauss's lemma to prove that if is an odd prime then\nand used this formula to prove quadratic reciprocity. By using elliptic rather than circular functions, he proved the cubic and quartic reciprocity laws.\n\nLeopold Kronecker used the lemma to show that\n\nSwitching and immediately gives quadratic reciprocity.\nIt is also used in what are probably the simplest proofs of the \"second supplementary law\"\n\nGeneralizations of Gauss's lemma can be used to compute higher power residue symbols. In his second monograph on biquadratic reciprocity, Gauss used a fourth-power lemma to derive the formula for the biquadratic character of in , the ring of Gaussian integers. Subsequently, Eisenstein used third- and fourth-power versions to prove cubic and quartic reciprocity.\n\nLet \"k\" be an algebraic number field with ring of integers formula_17 and let formula_18 be a prime ideal. The ideal norm formula_19 of formula_20 is defined as the cardinality of the residue class ring. Since formula_21 is prime this is a finite field formula_22, so the ideal norm is formula_23.\n\nAssume that a primitive th root of unity formula_24 and that and formula_21 are coprime (i.e. formula_26). Then no two distinct th roots of unity can be congruent modulo formula_20.\n\nThis can be proved by contradiction, beginning by assuming that formula_28 mod formula_20, . Let such that formula_30 mod formula_20, and . From the definition of roots of unity, \nand dividing by gives \n\nLetting and taking residues mod formula_20,\n\nSince and formula_36 are coprime, formula_37 mod formula_38 but under the assumption, one of the factors on the right must be zero. Therefore, the assumption that two distinct roots are congruent is false.\n\nThus the residue classes of formula_39 containing the powers of are a subgroup of order of its (multiplicative) group of units, formula_40 Therefore, the order of formula_41 is a multiple of , and \n\nThere is an analogue of Fermat's theorem in formula_43. If formula_44 for formula_45, then \nand since formula_47 mod ,\nis well-defined and congruent to a unique th root of unity ζ.\n\nThis root of unity is called the th-power residue symbol for formula_17 and is denoted by\n\nIt can be proven that\nif and only if there is an formula_52 such that mod formula_20.\n\nLet formula_54 be the multiplicative group of the th roots of unity, and let formula_55 be representatives of the cosets of formula_56 Then is called a system mod formula_57\n\nIn other words, there are formula_58 numbers in the set formula_59 and this set constitutes a representative set for formula_60\n\nThe numbers , used in the original version of the lemma, are a 1/2 system (mod ).\n\nConstructing a system is straightforward: let be a representative set for formula_60 Pick any formula_62 and remove the numbers congruent to formula_63 from . Pick from and remove the numbers congruent to formula_64 Repeat until is exhausted. Then is a system mod formula_57\n\nGauss's lemma may be extended to the th power residue symbol as follows. Let formula_66 be a primitive th root of unity, formula_18 a prime ideal, formula_68 (i.e. formula_20 is coprime to both and ) and let be a system mod formula_57\n\nThen for each , , there are integers , unique (mod ), and , unique (mod ), such that\nand the th-power residue symbol is given by the formula\n\nThe classical lemma for the quadratic Legendre symbol is the special case , , , if , if .\n\nThe proof of the th-power lemma uses the same ideas that were used in the proof of the quadratic lemma.\n\nThe existence of the integers and , and their uniqueness (mod ) and (mod ), respectively, come from the fact that is a representative set.\n\nAssume that = = , i.e.\nand\nThen\nBecause and formula_20 are coprime both sides can be divided by , giving \nwhich, since is a system, implies and , showing that is a permutation of the set .\n\nThen on the one hand, by the definition of the power residue symbol,\nand on the other hand, since is a permutation,\nso\nand since for all , and formula_20 are coprime, can be cancelled from both sides of the congruence,\nand the theorem follows from the fact that no two distinct th roots of unity can be congruent (mod formula_20).\n\nLet be the multiplicative group of nonzero residue classes in , and let be the subgroup {+1, −1}. Consider the following coset representatives of in ,\n\nApplying the machinery of the transfer to this collection of coset representatives, we obtain the transfer homomorphism\nwhich turns out to be the map that sends to , where and are as in the statement of the lemma. Gauss's lemma may then be viewed as a computation that explicitly identifies this homomorphism as being the quadratic residue character.\n\nTwo other characterizations of squares modulo a prime are Euler's criterion and Zolotarev's lemma.\n"}
{"id": "4478788", "url": "https://en.wikipedia.org/wiki?curid=4478788", "title": "HashClash", "text": "HashClash\n\nHashClash was a distributed computing project to find collisions in the MD5 hash algorithm. It was based at Department of Mathematics and Computer Science at the Eindhoven University of Technology, and Marc Stevens initiated the project as part of his master's degree thesis.\n\nThe project ended after Stevens defended his M.Sc. thesis in June 2007.\n\n\n"}
{"id": "12185345", "url": "https://en.wikipedia.org/wiki?curid=12185345", "title": "Herbert Federer", "text": "Herbert Federer\n\nHerbert Federer (July 23, 1920 – April 21, 2010) was an American mathematician. He is one of the creators of geometric measure theory, at the meeting point of differential geometry and mathematical analysis.\n\nFederer was born July 23, 1920, in Vienna, Austria. After emigrating to the US in 1938, he studied mathematics and physics at the University of California, Berkeley, earning the Ph.D. as a student of Anthony Morse in 1944. He then spent virtually his entire career as a member of the Brown University Mathematics Department, where he eventually retired with the title of Professor Emeritus.\n\nFederer wrote more than thirty research papers in addition to his book \"Geometric measure theory\". The Mathematics Genealogy Project assigns him nine Ph.D. students and well over a hundred subsequent descendants. His most productive students include the late Frederick J. Almgren, Jr. (1933–1997) a professor at Princeton for 35 years, and his last student, Robert Hardt, now at Rice University.\n\nFederer was a member of the National Academy of Sciences. In 1987, he and his Brown colleague Wendell Fleming won the American Mathematical Society's Steele Prize \"for their pioneering work in \"Normal and Integral currents\".\"\n\nFederer's mathematical work separates thematically into the periods before and after his watershed 1960 paper \"Normal and integral currents\", co-authored with Fleming. That paper provided the first satisfactory general solution to Plateau's problem — the problem of finding a (k+1)-dimensional least-area surface spanning a given k-dimensional boundary cycle in n-dimensional Euclidean space. Their solution inaugurated a new and fruitful period of research on a large class of geometric variational problems — especially minimal surfaces — via what came to be known as Geometric Measure Theory.\n\nDuring the 15 years or so years prior to that paper, Federer worked at the technical interface of geometry and measure theory. He focused particularly on surface area, rectifiability of sets, and the extent to which one could substitute rectifiability for smoothness in the analysis of surfaces. His 1947 paper on the rectifiable subsets of n-space characterized purely unrectifiable sets by their \"invisibility\" under almost all projections. A. S. Besicovitch had proven this for 1-dimensional sets in the plane, but Federer's generalization, valid for subsets of arbitrary dimension in any Euclidean space, was a major technical accomplishment, and later played a key role in \"Normal and Integral Currents\".\n\nIn 1958, Federer wrote \"Curvature Measures\", a paper that took some early steps toward understanding second-order properties of surfaces lacking the differentiability properties typically assumed in order to discuss curvature. He also developed and named what he called the coarea formula in that paper. That formula has become a standard analytical tool.\n\nFederer is perhaps best known for his treatise \"Geometric Measure Theory\", published in 1969. Intended as both a text and a reference work, the book is unusually complete, general and authoritative: its nearly 600 pages cover a substantial amount of linear and multilinear algebra, give a profound treatment of measure theory, integration and differentiation, and then move on to rectifiability, theory of currents, and finally, variational applications. Nevertheless, the book's unique style exhibits a rare and artistic economy that still inspires admiration, respect—and exasperation. A more accessible introduction may be found in F. Morgan's book listed below.\n\n\n\n"}
{"id": "53972682", "url": "https://en.wikipedia.org/wiki?curid=53972682", "title": "Hopf decomposition", "text": "Hopf decomposition\n\nIn mathematics, the Hopf decomposition, named after Eberhard Hopf, gives a canonical decomposition of a measure space (\"X\", μ) with respect to an invertible non-singular transformation \"T\", i.e. a transformation which with its inverse is measurable and carries null sets onto null sets. Up to null sets, \"X\" can be written as a disjoint union \"C\" ∐ \"D\" of \"T\"-invariant sets where the actions of \"T\" on \"C\" and \"D\" are conservative and dissipative. Thus, if τ is the automorphism of \"A\" = L(\"X\") induced by \"T\", there is a unique τ-invariant projection \"p\" in \"A\" such that \"pA\" is conservative and \"(I–p)A\" is dissipative.\n\n\n\n\n\nTheorem. If \"T\" is an invertible transformation on a measure space (\"X\",μ) preserving null sets, then the following conditions are equivalent on \"T\" (or its inverse):\n\n\nSince \"T\" is dissipative if and only if \"T\" is dissipative, it follows that \"T\" is conservative if and only if \"T\" is conservative.\n\nIf \"T\" is conservative, then \"r\" = \"q\" ∧ (τ(\"q\") ∨ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅) = \"q\" ∧ τ(1 - \"q\") ∧ τ(1 -\"q\") ∧ τ(\"q\") ∧ ... is wandering so that if \"q\" < 1, necessarily \"r\" = 0. Hence \"q\" ≤ τ(\"q\") ∨ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅, so that \"T\" is recurrent.\n\nIf \"T\" is recurrent, then \"q\" ≤ τ(\"q\") ∨ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅ Now assume by induction that \"q\" ≤ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅. Then τ(\"q\") ≤ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅ ≤ . Hence \"q\" ≤ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅. So the result holds for \"k\"+1 and thus \"T\" is infinitely recurrent. Conversely by definition an infinitely recurrent transformation is recurrent.\n\nNow suppose that \"T\" is recurrent. To show that \"T\" is incompressible it must be shown that, if τ(\"q\") ≤ \"q\", then τ(\"q\") ≤ \"q\". In fact in this case τ(\"q\") is a decreasing sequence. But by recurrence, \"q\" ≤ τ(\"q\") ∨ τ(\"q\") ∨ τ(\"q\") ∨ ⋅⋅⋅ , so \"q\" ≤ τ(\"q\") and hence \"q\" = τ(\"q\").\n\nFinally suppose that \"T\" is incompressible. If \"T\" is not conservative there is a \"p\" ≠ 0 in \"A\" with the τ(\"p\") disjoint (orthogonal). But then \"q\" = \"p\" ⊕ τ(\"p\") ⊕ τ(\"p\") ⊕ ⋅⋅⋅ satisfies τ(\"q\") < \"q\" with , contradicting incompressibility. So \"T\" is conservative.\n\nTheorem. If \"T\" is an invertible transformation on a measure space (\"X\",\"μ\") preserving null sets and inducing an automorphism \"τ\" of \"A\" = \"L\"(\"X\"), then there is a unique \"τ\"-invariant \"p\" = \"χ\" in \"A\" such that \"τ\" is conservative on \"pA\" = \"L\"(\"C\") and dissipative on (1 − \"p\")\"A\" = \"L\"(\"D\") where \"D\" = \"X\" \\ \"C\".\n\nCorollary. The Hopf decomposition for \"T\" coincides with the Hopf decomposition for \"T\".\n\nCorollary. The Hopf decomposition for \"T\" coincides with the Hopf decomposition for \"T\" for \"n\" > 1.\n\nCorollary. If an invertible transformation \"T\" acts ergodically but non-transitively on the measure space (\"X\",\"μ\") preserving null sets and \"B\" is a subset with \"μ\"(\"B\") > 0, then the complement of \"B\" ∪ \"TB\" ∪ \"T\"\"B\" ∪ ⋅⋅⋅ has measure zero.\n\nLet (\"X\",μ) be a measure space and \"S\" a non-sngular flow on \"X\" inducing a 1-parameter group of automorphisms σ of \"A\" = L(\"X\"). It will be assumed that the action is faithful, so that σ is the identity only for \"t\" = 0. For each \"S\" or equivalently σ with \"t\" ≠ 0 there is a Hopf decomposition, so a \"p\" fixed by σ such that the action is conservative on \"p\"\"A\" and dissipative on (1−\"p\")\"A\". \n\n\n\n\n"}
{"id": "905957", "url": "https://en.wikipedia.org/wiki?curid=905957", "title": "Human height", "text": "Human height\n\nHuman height or stature is the distance from the bottom of the feet to the top of the head in a human body, standing erect. It is measured using a stadiometer, usually in centimetres when using the metric system, or feet and inches when using the imperial system.\n\nA particular genetic profile in men called Y haplotype I-M170 is correlated with height. Ecological data shows that as the frequency of this genetic profile increases in the population, the average male height in a country also increases.\n\nStudies show that there is a correlation between small stature & a longer life expectancy. Individuals of small stature are also more likely to have lower blood pressure & are less likely to acquire cancer. The University of Hawaii has found that the “longevity gene” FOXO3 that reduces the effects of aging is more commonly found in individuals of a small body size. <ref>\n"}
{"id": "57373227", "url": "https://en.wikipedia.org/wiki?curid=57373227", "title": "Krauss matching wildcards algorithm", "text": "Krauss matching wildcards algorithm\n\nIn computer science, the Krauss matching wildcards algorithm is a pattern matching algorithm. Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, the algorithm provides a non-recursive mechanism for matching patterns in software applications, based on syntax simpler than that typically offered by regular expressions.\n\nThe algorithm is based on a history of development, correctness and performance testing, and programmer feedback that began with an unsuccessful search for a reliable non-recursive algorithm for matching wildcards. An initial algorithm, implemented in a single while loop, quickly prompted comments from software developers, leading to improvements. Ongoing comments and suggestions culminated in a revised algorithm still implemented in a single while loop but refined based on a collection of test cases and a performance profiler. The experience tuning the single while loop using the profiler prompted development of a two-loop strategy that achieved further performance gains, particularly in situations involving empty input strings or input containing no wildcard characters. The two-loop algorithm is available for use by the open-source software development community, under the terms of the Apache License v. 2.0, and is accompanied by test case code.\n\nThe algorithm made available under the Apache license is implemented in both pointer-based C++ and portable C++ (implemented without pointers). The test case code, also available under the Apache license, can be applied to any algorithm that provides the pattern matching operations below. The implementation as coded is unable to handle multibyte character sets and poses problems when the text being searched may contain multiple incompatible character sets.\n\nThe algorithm supports three pattern matching operations:\n\n\nThe original algorithm has been ported to the DataFlex programming language by Larry Heiges for use with Data Access Worldwide code library. It has been posted on GitHub in modified form as part of a log file reader. The 2014 algorithm is part of the Unreal Model Viewer built into the Epic Games Unreal Engine game engine.\n\n"}
{"id": "7069430", "url": "https://en.wikipedia.org/wiki?curid=7069430", "title": "Kurtosis risk", "text": "Kurtosis risk\n\nIn statistics and decision theory, kurtosis risk is the risk that results when a statistical model assumes the normal distribution, but is applied to observations that have a tendency to occasionally be much farther (in terms of number of standard deviations) from the average than is expected for a normal distribution.\n\nKurtosis risk applies to any kurtosis-related quantitative model that assumes the normal distribution for certain of its independent variables when the latter may in fact have kurtosis much greater than does the normal distribution. Kurtosis risk is commonly referred to as \"fat tail\" risk. The \"fat tail\" metaphor explicitly describes the situation of having more observations at either extreme than the tails of the normal distribution would suggest; therefore, the tails are \"fatter\".\n\nIgnoring kurtosis risk will cause any model to understate the risk of variables with high kurtosis. For instance, Long-Term Capital Management, a hedge fund cofounded by Myron Scholes, ignored kurtosis risk to its detriment. After four successful years, this hedge fund had to be bailed out by major investment banks in the late 1990s because it understated the kurtosis of many financial securities underlying the fund's own trading positions.\n\nBenoit Mandelbrot, a French mathematician, extensively researched this issue. He felt that the extensive reliance on the normal distribution for much of the body of modern finance and investment theory is a serious flaw of any related models including the Black–Scholes option model developed by Myron Scholes and Fischer Black, and the capital asset pricing model developed by William F. Sharpe. Mandelbrot explained his views and alternative finance theory in his book: \"The (Mis)Behavior of Markets: A Fractal View of Risk, Ruin, and Reward\" published on September 18, 1997.\n\n\n"}
{"id": "15782871", "url": "https://en.wikipedia.org/wiki?curid=15782871", "title": "Lattice (discrete subgroup)", "text": "Lattice (discrete subgroup)\n\nIn Lie theory and related areas of mathematics, a lattice in a locally compact group is a discrete subgroup with the property that the quotient space has finite invariant measure. In the special case of subgroups of R, this amounts to the usual geometric notion of a lattice as a periodic subset of points, and both the algebraic structure of lattices and the geometry of the space of all lattices are relatively well understood.\n\nThe theory is particularly rich for lattices in semisimple Lie groups or more generally in semisimple algebraic groups over local fields. In particular there is a wealth of rigidity results in this setting, and a celebrated theorem of Grigori Margulis states that in most cases all lattices are obtained as arithmetic groups.\n\nLattices are also well-studied in some other classes of groups, in particular groups associated to Kac-Moody algebras and automorphisms groups of regular trees (the latter are known as \"tree lattices\").\n\nLattices are of interest in many areas of mathematics: geometric group theory (as particularly nice examples of discrete groups), in differential geometry (through the construction of locally homogeneous manifolds), in number theory (through arithmetic groups), in ergodic theory (through the study of homogeneous flows on the quotient spaces) and in combinatorics (through the construction of expanding Cayley graphs and other combinatorial objects).\n\nLattices are best thought of as discrete approximations of continuous groups (such as Lie groups). For example, it is intuitively clear that the subgroup formula_1 of integer vectors \"looks like\" the real vector space formula_2 in some sense, while both groups are essentially different: one is finitely generated and countable, while the other is not (as a group) and has the cardinality of the continuum.\n\nRigorously defining the meaning of \"approximation of a continuous group by a discrete subgroup\" in the previous paragraph in order to get a notion generalising the example formula_3 is a matter of what it is designed to achieve. Maybe the most obvious idea is to say that a subgroup \"approximates\" a larger group is that the larger group can be covered by the translates of a \"small\" subset by all elements in the subgroups. In a locally compact topological group there are two immediately available notions of \"small\": topological (a compact, or relatively compact subset) or measure-theoretical (a subset of finite Haar measure). Note that since the Haar measure is a Borel measure, in particular gives finite mass to compact subsets, the second definition is more inclusive. The definition of a lattice used in mathematics relies upon the second meaning (in particular to include such examples as formula_4) but the first also has its own interest (such lattices are called uniform).\n\nLet formula_5 be a locally compact group and formula_6 a discrete subgroup (this means that there exists a neighbourhood formula_7 of the identity element formula_8 of formula_5 such that formula_10). Then formula_6 is called a lattice in formula_5 if in addition there exists a Borel measure formula_13 on the quotient space formula_14 which is finite (i.e. formula_15) and formula_5-invariant (meaning that for any formula_17 and any open subset formula_18 the equality formula_19 is satisifed).\n\nA slightly more sophisticated formulation is as follows: suppose in addition that formula_5 is unimodular, then since formula_6 is discrete it is also unimodular and by general theorems there exists a unique formula_5-invariant Borel measure on formula_14 up to scaling. Then formula_6 is a lattice if and only if this measure is finite.\n\nIn the case of discrete subgroups this invariant measure coincides locally with the Haar measure and hence a discrete subgroup in a locally compact group formula_5 being a lattice is equivalent to it having a fundamental domain (for the action on formula_5 by left-translations) of finite volume for the Haar measure.\n\nA lattice formula_27 is called uniform when the quotient space formula_28 is compact (and \"non-uniform\" otherwise). Equivalently a discrete subgroup formula_27 is a uniform lattice if and only if there exists a compact subset formula_30 with formula_31. Note that if formula_6 is any discrete subgroup in formula_5 such that formula_34 is compact then formula_6 is automatically a lattice in formula_5.\n\nThe fundamental, and simplest, example is the subgroup formula_1 which is a lattice in the Lie group formula_2. A slightly more complicated example is given by the discrete Heisenberg group inside the continuous Heisenberg group.\n\nIf formula_5 is a discrete group then a lattice in formula_5 is exactly a subgroup formula_6 of finite index (i.e. the quotient set formula_28 is finite).\n\nAll of these examples are uniform. A non-uniform example is given by the modular group formula_43 inside formula_44, and also by the higher-dimensional analogues formula_45.\n\nAny finite-index subgroup of a lattice is also a lattice in the same group. More generally, a subgroup commensurable to a lattice is a lattice.\n\nNot every locally compact group contains a lattice, and there is no general group-theoretical sufficient condition for this. On the other hand, there are plenty of more specific settings where such criteria exist. For example, the existence or non-existence of lattices in Lie groups is a well-understood topic.\n\nAs we mentioned, a necessary condition for a group to contain a lattice is that the group must be unimodular. This allows for the easy construction of groups without lattices, for example the group of invertible upper triangular matrices or the affine groups. It is also not very hard to find unimodular groups without lattices, for example certain nilpotent Lie groups as explained below.\n\nA stronger condition than unimodularity is simplicity. This is sufficient to imply the existence of a lattice in a Lie group, but in the more general setting of locally compact groups there exists simple groups without lattices, for example the \"Neretin groups\".\n\nFor nilpotent groups the theory simplifies much from the general case, and stays similar to the case of Abelian groups. All lattices in a nilpotent Lie group are uniform, and if formula_46 is a connected simply connected Lie group (equivalently it does not contain a nontrivial compact subgroup) then a discrete subgroup is a lattice if and only if it is not contained in a proper connected subgroup (this generalises the fact that a discrete subgroup in a vector space is a lattice if and only if it spans the vector space).\n\nA nilpotent Lie group contains a lattice if and only if it can be defined over the rationals, that is if and only if its structure constants are rational numbers. More precisely, in a nilpotent group satisfying this condition lattices correspond via the exponential map to lattices (in the more elementary sense of Lattice (group)) in the Lie algebra.\n\nA lattice in a nilpotent Lie group formula_46 is always finitely generated (and hence finitely presented since it is itself nilpotent); in fact it is generated by at most formula_48 elements.\n\nFinally, a nilpotent group is isomorphic to a lattice in a nilpotent Lie group if and only if it contains a subgroup of finite index which is torsion-free and of finitely generated.\n\nThe criterion for nilpotent Lie groups to have a lattice given above does not apply to more general solvable Lie groups. It remains true that any lattice in a solvable Lie group is uniform and that lattices in solvable groups are finitely presented.\n\nNot all finitely generated solvable groups are lattices in a Lie group. An algebraic criterion is that the group be polycyclic.\n\nIf formula_5 is a semisimple linear algebraic group in formula_50 which is defined over the field formula_51 of rational numbers (i.e. the polynomial equations defining formula_5 have their coefficients in formula_51) then it has a subgroup formula_54. A fundamental theorem of Armand Borel and Harish-Chandra states that formula_6 is always a lattice in formula_5; the simplest example of this is the subgroup formula_4.\n\nGeneralising the construction above one gets the notion of an \"arithmetic lattice\" in a semisimple Lie group. Since all semisimple Lie groups can be defined over formula_51 a consequence of the arithmetic construction is that any semisimple Lie group contains a lattice.\n\nWhen the Lie group formula_5 splits as a product formula_60 there is an obvious construction of lattices in formula_5 from the smaller groups: if formula_62 are lattices then formula_63 is a lattice as well. Roughly, a lattice is then said to be \"irreducible\" if it does not come from this construction.\n\nMore formally, if formula_64 is the decomposition of formula_5 into simple factors, a lattice formula_27 is said to be irreducible if either of the following equivalent conditions hold: \n\nAn example of an irreducible lattice is given by the subgroup formula_71 which we view as a subgroup formula_72 via the map formula_73 where formula_74 is the Galois map sending a matric with coefficients formula_75 to formula_76.\n\nThe real rank of a Lie group is the maximal dimension of an abelian subgroup containing only semisimple elements. The semisimple Lie groups of real rank 1 without compact factors are (up to isogeny) those in the following list (see List of simple Lie groups):\n\n\nThe real rank of a Lie group has a significant influence on the behaviour of the lattices it contains. In particularn the behaviour of lattices in the first two families of groups (and to a lesser extent that of lattices in the latter two) differs much from that of irreducible lattices in groups of higher rank. For example:\n\n\nThe property known as (T) was introduced by Kazhdan to study the algebraic structure lattices in certain Lie groups when the classical, more geometric methods failed or at least were not as efficient. The fundamental result when studying lattices is the following:\n\nUsing harmonic analysis it is possible to classify semisimple Lie groups according to whether or not they have the property. As a consequence we get the following result, further illustrating the dichotomy of the previous section:\n\n\nLattices in semisimple Lie groups are always finitely presented. For uniform lattices this is a direct consequence of cocompactness. In the non-uniform case this can be proved using reduction theory. However a much faster proof is by using Kazhdan's property (T) when possible.\n\nIf formula_5 is a Lie group then from an inner product formula_94 on the tangent space formula_95 (the Lie algebra of formula_5) one can construct a Riemannian metric on formula_5 as follows: if formula_98 belong to the tangent space at a point formula_99 put formula_100 where formula_101 indicates the tangent map (at formula_102) of the diffeomorphism formula_103 of formula_5.\n\nThe maps formula_105 for formula_99 are by definition isometries for this metric formula_107. In particular, if formula_6 is any discrete subgroup in formula_5 (so that it acts freely and properly discontinuously by left-translations on formula_5) the quotient formula_111 is a Riemannian manifold locally isometric to formula_5 with the metric formula_107.\n\nThe Riemannian volume form associated to formula_114 defines a Haar measure on formula_5 and we see that the quotient manifold is of finite Riemannian volume if and only if formula_6 is a lattice.\n\nInteresting examples in this class of Riemannian spaces include compact flat manifolds and nilmanifolds.\n\nA natural inner product on formula_95 is given by the Killing form. If formula_5 is not compact it is not definite and hence not an inner product: however when formula_5 is semisimple and formula_120 is a maximal compact subgroup it can be used to define a formula_5-invariant metric on the homogeneous space formula_122: such Riemannian manifolds are called symmetric spaces of non-compact type without Euclidean factors.\n\nA subgroup formula_27 acts freely, properly discontinuously on formula_124 if and only if it is discrete and torsion-free. The quotients formula_125 are called locally symmetric spaces. There is thus a bijective correspondence between complete locally symmetric spaces locally isomorphic to formula_124 and of finite Riemannian volume, and torsion-free lattices in formula_5. This correspondence can be extended to all lattices by adding orbifolds on the geometric side.\n\nA class of groups with similar properties (with respect to lattices) to real semisimple Lie groups are semisimple algebraic groups over local fields of characteristic 0, for example the p-adic fields formula_128. There is an arithmetic construction similar to the real case, and the dichotomy between higher rank and rank one also holds in this case, in a more marked form. Let formula_5 be an algebraic group over formula_128 of split-formula_128-rank \"r\". Then:\n\n\nIn the latter case all lattices are in fact free groups (up to finite index).\n\nMore generally one can look at lattices in groups of the form\n\nwhere formula_134 is a semisimple algebraic group over formula_128. Usually formula_136 is allowed, in which case formula_137 is a real Lie group. An example of such a lattice is given by\n\nThis arithmetic construction can be generalised to obtain the notion of an \"S-arithmetic group\". The Margulis arithmeticity theorem applies to this setting as well. In particular, if at least two of the factors formula_134 are noncompact then any irreducible lattice in formula_5 is S-arithmetic.\n\nIf formula_141 is a semisimple algebraic group over a number field formula_142 and formula_143 its adèle ring then the group formula_144 of adélic points is well-defined (modulo some technicalities) and it is a locally compact group which naturally contains the group formula_145 of formula_142-rational point as a discrete subgroup. The Borel–Harish-Chandra theorem extends to this setting, and formula_147 is a lattice.\n\nThe strong approximation theorem relates the quotient formula_148 to more classical S-arithmetic quotients. This fact makes the adèle groups very effective as tools in the theory of automorphic forms. In particular modern forms of the trace formula are usually stated and proven for adélic groups rather than for Lie groups.\n\nAnother group of phenomena concerning lattices in semisimple algebraic groups is collectively known as \"rigidity\". Here are three classical examples of results in this category.\n\nLocal rigidity results state that in most situations every subgroup which is sufficiently \"close\" to a lattice (in the intuitive sense, formalised by Chabauty topology) is actually conjugated to the original lattice by an element of the ambient Lie group. A consequence of local rigidity and the Kazhdan-Margulis theorem is Wang's theorem: in a given group (with a fixed Haar measure), for any \"v>0\" there are only finitely many (up to conjugation) lattices with covolume bounded by \"v\".\n\nThe Mostow rigidity theorem states that for lattices in simple Lie groups not locally isomorphic to formula_44 (the group of 2 by 2 matrices with determinant 1) any isomorphism of lattices is essentially induced by an isomorphism between the groups themselves. In particular, a lattice in a Lie group \"remembers\" the ambient Lie group through its group structure. The first statement is sometimes called \"strong rigidity\" and is due to George Mostow and Gopal Prasad (Mostow proved it for cocompact lattices and Prasad extended it to the general case).\n\n\"Superrigidity\" provides (for Lie groups and algebraic groups over local fields of higher rank) a generalization dealing with homomorphisms from a lattice in an algebraic group \"G\" into another algebraic group \"H\". It was proven by Grigori Margulis and is an essential ingredient in the proof of his arithmeticity theorem.\n\nThe only groups for which Mostow rigidity does not hold are all groups locally isomorphic to formula_150. In this case there are in fact continuously many lattices and they give rise to Teichmüller spaces.\n\nNonuniform lattices in the group formula_151 are not locally rigid. In fact they are accumulation points (in the Chabauty topology) of lattices of smaller covolume, as demonstrated by hyperbolic Dehn surgery.\n\nAs lattices in rank-one p-adic groups are virtually free groups they are very non-rigid.\n\nLet formula_152 be a tree with a cocompact group of automorphisms; for example, formula_152 can be a regular or biregular tree. The group of automorphismsformula_154 of formula_152 is a locally compact group (when endowed with the compact-open topology, in which a basis of neighbourhoods of the identity is given by the stabilsers of finite subtrees, which are compact). Any group which is a lattice in some formula_154 is then called a \"tree lattice\".\n\nThe discreteness in this case is easy to see from the group action on the tree: a subgroup of formula_154 is discrete if and only if all vertex stabilisers are finite groups.\n\nIt is easily seen from the basic theory of group actions on trees that uniform tree lattices are virtually free groups. Thus the more interesting tree lattices are the non-uniform ones, equivalently those for which the quotient graph formula_158 is infinite. The existence of such lattices is not easy to see.\n\nIf formula_142 is a local field of positive characteristic (i.e. a completion of a function field of a curve over a finite field, for example the field of formal Laurent power series formula_160) and formula_5 an algebraic group defined over formula_142 of formula_142-split rank one, then any lattice in formula_5 is a tree lattice through its action on the Bruhat-Tits building which in this case is a tree. In contrast to the characteristic 0 case such lattices can be nonuniform, and in this case they are never finitely generated.\n\nIf formula_6 is the fundamental group of an infinite graph of groups, all of whose vertex groups are finite, and under additional necessary assumptions on the index of the edge groups and the size of the vertex groups, then the action of formula_6 on the Bass-Serre tree associated to the graph of groups realises it as a tree lattice.\n\nMore generally one can ask the following question: if formula_167 is a closed subgroup of formula_154, under which conditions does formula_169 contain a lattice? The existence of a uniform lattice is equivalent to formula_167 being unimodular and the quotient formula_171 being finite. The general existence theorem is more subtle: it is necessary and sufficient that formula_167 be unimodular, and that the quotient formula_171 be of \"finite volume\" in a suitable sense (which can be expressed combinatorially in terms of the action of formula_167), more general than the stroger condition that the quotient be finite (as proven by the very existence of nonuniform tree lattices).\n\n"}
{"id": "21252759", "url": "https://en.wikipedia.org/wiki?curid=21252759", "title": "Line coordinates", "text": "Line coordinates\n\nIn geometry, line coordinates are used to specify the position of a line just as point coordinates (or simply coordinates) are used to specify the position of a point.\n\nThere are several possible ways to specify the position of a line in the plane. A simple way is by the pair where the equation of the line is \"y\" = \"mx\" + \"b\". Here \"m\" is the slope and \"b\" is the \"y\"-intercept. This system specifies coordinates for all lines that are not vertical. However, it is more common and simpler algebraically to use coordinates where the equation of the line is \"lx\" + \"my\" + 1 = 0. This system specifies coordinates for all lines except those that pass through the origin. The geometrical interpretations of \"l\" and \"m\" are the negative reciprocals of the \"x\" and \"y\"-intercept respectively.\n\nThe exclusion of lines passing through the origin can be resolved by using a system of three coordinates to specify the line in which the equation, \"lx\" + \"my\" + \"n\" = 0. Here \"l\" and \"m\" may not both be 0. In this equation, only the ratios between \"l\", \"m\" and \"n\" are significant, in other words if the coordinates are multiplied by a non-zero scalar then line represented remains the same. So is a system of homogeneous coordinates for the line.\n\nIf points in the real projective plane are represented by homogeneous coordinates , the equation of the line is \"lx\" + \"my\" + \"nz\" = 0, provided In particular, line coordinate represents the line \"z\" = 0, which is the line at infinity in the projective plane. Line coordinates and represent the \"x\" and \"y\"-axes respectively.\n\nJust as \"f\"(\"x\", \"y\") = 0 can represent a curve as a subset of the points in the plane, the equation φ(\"l\", \"m\") = 0 represents a subset of the lines on the plane. The set of lines on the plane may, in an abstract sense, be thought of as the set of points in a projective plane, the dual of the original plane. The equation φ(\"l\", \"m\") = 0 then represents a curve in the dual plane. \n\nFor a curve \"f\"(\"x\", \"y\") = 0 in the plane, the tangents to the curve form a curve in the dual space called the dual curve. If φ(\"l\", \"m\") = 0 is the equation of the dual curve, then it is called the tangential equation, for the original curve. A given equation φ(\"l\", \"m\") = 0 represents a curve in the original plane determined as the envelope of the lines that satisfy this equation. Similarly, if φ(\"l\", \"m\", \"n\") is a homogeneous function then φ(\"l\", \"m\", \"n\") = 0 represents a curve in the dual space given in homogeneous coordinates, and may be called the homogeneous tangential equation of the enveloped curve.\n\nTangential equations are useful in the study of curves defined as envelopes, just as Cartesian equations are useful in the study of curves defined as loci.\n\nA linear equation in line coordinates has the form \"al\" + \"bm\" + \"c\" = 0, where \"a\", \"b\" and \"c\" are constants. Suppose (\"l\", \"m\") is a line that satisfies this equation. If \"c\" is not 0 then \"lx\" + \"my\" + 1 = 0, where \"x\" = \"a\"/\"c\" and \"y\" = \"b\"/\"c\", so every line satisfying the original equation passes though the point (\"x\", \"y\"). Conversely, any line through (\"x\", \"y\") satisfies the original equation, so \"al\" + \"bm\" + \"c\" = 0 is the equation of set of lines through (\"x\", \"y\"). For a given point (\"x\", \"y\"), the equation of the set of lines though it is \"lx\" + \"my\" + 1 = 0, so this may be defined as the tangential equation of the point. Similarly, for a point (\"x\", \"y\", \"z\") given in homogeneous coordinates, the equation of the point in homogeneous tangential coordinates is \"lx\" + \"my\" + \"nz\" = 0.\n\nThe intersection of the lines (\"l\", \"m\") and (\"l\", \"m\") is the solution to the linear equations \n\nBy Cramer's rule, the solution is\n\nThe lines (\"l\", \"m\"), (\"l\", \"m\"), and (\"l\", \"m\") are concurrent when the determinant\n\nFor homogeneous coordinates, the intersection of the lines (\"l\", \"m\", \"n\") and (\"l\", \"m\", \"n\") is\n\nThe lines (\"l\", \"m\", \"n\"), (\"l\", \"m\", \"n\") and (\"l\", \"m\", \"n\") are concurrent when the determinant\n\nDually, the coordinates of the line containing (\"x\", \"y\", \"z\") and (\"x\", \"y\", \"z\") are\n\nFor two given points in the real projective plane, (\"x\", \"y\", \"z\") and (\"x\", \"y\", \"z\"), the three determinants\n\ndetermine the projective line containing them.\n\nSimilarly, for two points in RP, (\"x\", \"y\", \"z\", \"w\") and (\"x\", \"y\", \"z\", \"w\"), the line containing them is determined by the six determinants \n\nThis is the basis for a system of homogeneous line coordinates in three-dimensional space called \"Plücker coordinates\". Six numbers in a set of coordinates only represent a line when they satisfy an additional equation. This system maps the space of lines in three-dimensional space to projective space RP, but with the additional requirement the space of lines corresponds to the Klein quadric, which is a manifold of dimension four.\n\nMore generally, the lines in \"n\"-dimensional projective space are determined by a system of \"n\"(\"n\" − 1)/2 homogeneous coordinates that satisfy a set of (\"n\" − 2)(\"n\" − 3)/2 conditions, resulting in a manifold of dimension 2(\"n\" − 1).\n\nIsaak Yaglom has shown how dual numbers provide coordinates for oriented lines in the Euclidean plane, and split-complex numbers form line coordinates for the hyperbolic plane. The coordinates depend on the presence of an origin and reference line on it. Then, given an arbitrary line its coordinates are found from the intersection with the reference line. The distance \"s\" from the origin to the intersection and the angle θ of inclination between the two lines are used:\n\nSince there are lines ultraparallel to the reference line in the Lobachevski plane, they need coordinates too: There is a unique common perpendicular, say \"s\" is the distance from the origin to this perpendicular, and \"d\" is the length of the segment between reference and the given line.\n\nThe motions of the line geometry are described with linear fractional transformations on the appropriate complex planes.\n\n\n"}
{"id": "25115911", "url": "https://en.wikipedia.org/wiki?curid=25115911", "title": "Marchenko–Pastur distribution", "text": "Marchenko–Pastur distribution\n\nIn the mathematical theory of random matrices, the Marchenko–Pastur distribution, or Marchenko–Pastur law, describes the asymptotic behavior of singular values of large rectangular random matrices. The theorem is named after Ukrainian mathematicians Vladimir Marchenko and Leonid Pastur who proved this result in 1967.\n\nIf formula_1 denotes a formula_2 random matrix whose entries are independent identically distributed random variables with mean 0 and variance formula_3, let\n\nand let formula_5 be the eigenvalues of formula_6 (viewed as random variables). Finally, consider the random measure\n\nTheorem. Assume that formula_8 so that the ratio formula_9. Then formula_10 (in weak* topology in distribution), where\nand\nwith\n\nThe Marchenko–Pastur law also arises as the free Poisson law in free probability theory, having rate formula_14 and jump size formula_15.\n\n\n"}
{"id": "34080241", "url": "https://en.wikipedia.org/wiki?curid=34080241", "title": "McKay graph", "text": "McKay graph\n\nIn mathematics, the McKay graph of a finite-dimensional representation \"V\" of a finite group \"G\" is a weighted quiver encoding the structure of the representation theory of \"G\". Each node represents an irreducible representation of \"G\". If formula_1 are irreducible representations of \"G\" then there is an arrow from formula_2 to formula_3 if and only if formula_3 is a constituent of the tensor product formula_5. Then the weight \"n\" of the arrow is the number of times this constituent appears in formula_6. For finite subgroups \"H\" of GL(2, C), the McKay graph of \"H\" is the McKay graph of the canonical representation of \"H\".\n\nIf \"G\" has \"n\" irreducible characters, then the Cartan matrix \"c\" of the representation \"V\" of dimension \"d\" is defined by formula_7, where δ is the Kronecker delta. A result by Steinberg states that if \"g\" is a representative of a conjugacy class of \"G\", then the vectors formula_8 are the eigenvectors of \"c\" to the eigenvalues formula_9, where formula_10 is the character of the representation \"V\".\n\nThe McKay correspondence, named after John McKay, states that there is a one-to-one correspondence between the McKay graphs of the finite subgroups of SL(2, C) and the extended Dynkin diagrams, which appear in the ADE classification of the simple Lie Algebras.\n\nLet \"G\" be a finite group, \"V\" be a representation of \"G\" and formula_11 be its character. Let formula_12 be the irreducible representations of \"G\". If\n\nthen define the McKay graph formula_14 of \"G\" as follows:\n\n\nWe can calculate the value of \"n\" by considering the inner product. We have the following formula: \n\nwhere formula_22 denotes the inner product of the characters.\n\nThe McKay graph of a finite subgroup of GL(2, C) is defined to be the McKay graph of its canonical representation. \n\nFor finite subgroups of SL(2, C), the canonical representation is self-dual, so \"n\" = \"n\" for all \"i\", \"j\". Thus, the McKay graph of finite subgroups of SL(2, C) is undirected. \n\nIn fact, by the McKay correspondence, there is a one-to-one correspondence between the finite subgroups of SL(2, C) and the extended Coxeter-Dynkin diagrams of type A-D-E. \n\nWe define the Cartan matrix \"c\" of \"V\" as follows: \n\nwhere formula_24 is the Kronecker delta.\n\n\n\n\n\n"}
{"id": "58264058", "url": "https://en.wikipedia.org/wiki?curid=58264058", "title": "Measurable acting group", "text": "Measurable acting group\n\nIn mathematics, a measurable acting group is a special group that acts on some space in a way that is compatible with structures of measure theory. Measurable acting groups are found in the intersection of measure theory and group theory, two sub-disciplines of mathematics. Measurable acting groups are the basis for the study of invariant measures in abstract settings, most famously the Haar measure, and the study of stationary random measures.\n\nLet formula_1 be a measurable group, where formula_2 denotes the formula_3-algebra on formula_4 and formula_5 the group law. Let further formula_6 be a measurable space and let formula_7 be the product formula_3-algebra of the formula_3-algebras formula_10 and formula_11.\n\nLet formula_4 act on formula_13 with group action\n\nIf formula_15 is a measurable function from formula_16 to formula_17, then it is called a measurable group action. In this case, the group formula_4 is said to act measurable on formula_13.\n\nOne special case of measurable acting groups are measurable groups themselves. If formula_20, and the group action is the group law, then a measurable group is a group formula_4, acting measurably on formula_4.\n"}
{"id": "1554398", "url": "https://en.wikipedia.org/wiki?curid=1554398", "title": "Minimal realization", "text": "Minimal realization\n\nIn control theory, given any transfer function, any state-space model that is both controllable and observable and has the same input-output behaviour as the transfer function is said to be a minimal realization of the transfer function. The realization is called \"minimal\" because it describes the system with the minimum number of states.\n\nThe minimum number of state variables required to describe a system equals the order of the differential equation; more state variables than the minimum can be defined. For example, a second order system can be defined by two(minimal realization) or more state variables.\n"}
{"id": "11822678", "url": "https://en.wikipedia.org/wiki?curid=11822678", "title": "Multi-user MIMO", "text": "Multi-user MIMO\n\nMulti-user MIMO (MU-MIMO) is a set of multiple-input and multiple-output (MIMO) technologies for wireless communication, in which a set of users or wireless terminals, each with one or more antennas, communicate with each other. In contrast, single-user MIMO considers a single multi-antenna transmitter communicating with a single multi-antenna receiver. In a similar way that OFDMA adds multiple access (multi-user) capabilities to OFDM, MU-MIMO adds multiple access (multi-user) capabilities to MIMO. MU-MIMO has been investigated since the beginning of research into multi-antenna communication, including work by Telatar on the capacity of the MU-MIMO uplink.\n\nSDMA, massive MIMO, coordinated multipoint (CoMP), and ad hoc MIMO are all related to MU-MIMO; each of those technologies often leverage spatial degrees of freedom to separate users.\n\nMulti-user MIMO (MU-MIMO) can leverage multiple users as spatially distributed transmission resources, at the cost of somewhat more expensive signal processing. In comparison, conventional, or single-user MIMO considers only local device multiple antenna dimensions. Multi-user MIMO algorithms are developed to enhance MIMO systems when the number of users or connections is greater than one. Multi-user MIMO can be generalized into two categories: MIMO broadcast channels (MIMO BC) and MIMO multiple access channels (MIMO MAC) for downlink and uplink situations, respectively. Single-user MIMO can be represented as point-to-point, pairwise MIMO.\n\nTo remove ambiguity of the words \"receiver\" and \"transmitter\", we can adopt the terms \"access point\" (AP; or, \"base station\"), and \"user\". An AP is the transmitter and a user is the receiver for downlink environments, whereas an AP is the receiver and a user is the transmitter for uplink environments. Homogeneous networks are somewhat freed from this distinction.\n\n\"MIMO broadcast\" represents a MIMO downlink case in a single sender to multiple receiver wireless network. Examples of advanced transmit processing for MIMO BC are interference aware precoding and SDMA-based downlink user scheduling. For advanced transmit processing, the channel state information has to be known at the transmitter (CSIT). That is, knowledge of CSIT allows throughput improvement, and methods to obtain CSIT become of significant importance. MIMO BC systems have an outstanding advantage over point-to-point MIMO systems, especially when the number of transmit antennas at the transmitter, or AP, is larger than the number of receiver antennas at each receiver (user). Two categories of coding techniques for the MIMO BC include those using dirty paper coding (DPC) and linear techniques.\n\nConversely, the MIMO multiple-access-channel or \"MIMO MAC\" represents a MIMO uplink case in the multiple sender to single receiver wireless network. Examples of advanced receive processing for MIMO MAC are joint interference cancellation and SDMA-based uplink user scheduling. For advanced receive processing, the receiver has to know the channel state information at the receiver (CSIR). Knowing CSIR is generally easier than knowing CSIT. However, knowing CSIR costs a lot of uplink resources to transmit dedicated pilots from each user to the AP. MIMO MAC systems outperforms point-to-point MIMO systems especially when the number of receiver antennas at an AP is larger than the number of transmit antennas at each user.\n\n\"Cross-layer MIMO\" enhances the performance of MIMO links by solving certain cross-layer problems that may occur when MIMO configurations are employed in a system. Cross-layer techniques can be used to enhance the performance of SISO links as well. Examples of cross-layer techniques are Joint Source-Channel Coding, Adaptive Modulation and Coding (AMC, or \"Link Adaptation\"), Hybrid ARQ (HARQ), and user scheduling.\n\nThe highly interconnected wireless ad hoc network increases the flexibility of wireless networking at the cost of increased multi-user interference. To improve the interference immunity, PHY/MAC-layer protocols have evolved from competition based to cooperative based transmission and reception. Cooperative wireless communications can actually exploit interference, which includes self-interference and other user interference. In cooperative wireless communications, each node might use self-interference and other user interference to improve the performance of data encoding and decoding, whereas conventional nodes are generally directed to avoid the interference. For example, once strong interference is decodable, a node decodes and cancels the strong interference before decoding the self-signal. The mitigation of low Carrier over Interference (CoI) ratios can be implemented across PHY/MAC/Application network layers in cooperative systems.\n\n\nCO-MIMO, also known as network MIMO (net-MIMO), or ad hoc MIMO, uses distributed antennas which belong to other users, while conventional MIMO, i.e., single-user MIMO, only employs antennas belonging to the local terminal. CO-MIMO improves the performance of a wireless network by introducing multiple antenna advantages, such as diversity, multiplexing and beamforming. If the main interest hinges on the diversity gain, it is known as cooperative diversity. It can be described as a form of macro-diversity, used for example in soft handover. Cooperative MIMO corresponds to transmitter macro-diversity or simulcasting. A simple form that does not require any advanced signal processing is single frequency networks (SFN), used especially in wireless broadcasting. SFNs combined with channel adaptive or traffic adaptive scheduling is called dynamic single frequency networks (DSFN).\n\nCO-MIMO is a technique useful for future cellular networks which consider wireless mesh networking or wireless ad hoc networking. In wireless ad hoc networks, multiple transmit nodes communicate with multiple receive nodes. To optimize the capacity of ad hoc channels, MIMO concepts and techniques can be applied to multiple links between the transmit and receive node clusters. Contrasted to multiple antennas in a single-user MIMO transceiver, participating nodes and their antennas are located in a distributed manner. So, to achieve the capacity of this network, techniques to manage distributed radio resources are essential. Strategies such as autonomous interference cognition, node cooperation, and network coding with dirty paper coding have been suggested to optimize wireless network capacity.\n\n\n"}
{"id": "17699115", "url": "https://en.wikipedia.org/wiki?curid=17699115", "title": "Newton–Pepys problem", "text": "Newton–Pepys problem\n\nThe Newton–Pepys problem is a probability problem concerning the probability of throwing sixes from a certain number of dice.\n\nIn 1693 Samuel Pepys and Isaac Newton corresponded over a problem posed by Pepys in relation to a wager he planned to make. The problem was:\n\nPepys initially thought that outcome C had the highest probability, but Newton correctly concluded that outcome A actually has the highest probability.\n\nThe probabilities of outcomes A, B and C are:\n\nThese results may be obtained by applying the binomial distribution (although Newton obtained them from first principles). In general, if P(\"N\") is the probability of throwing at least \"n\" sixes with 6\"n\" dice, then:\n\nAs \"n\" grows, P(\"N\") decreases monotonically towards an asymptotic limit of 1/2.\n\nThe solution outlined above can be implemented in R as follows:\n<source lang = \"rsplus\">\nfor (s in 1:3) { # looking for s = 1, 2 or 3 sixes\n\nAlthough Newton correctly calculated the odds of each bet, he provided a separate intuitive explanation to Pepys. He imagined that B and C toss their dice in groups of six, and said that A was most favorable because it required a 6 in only one toss, while B and C required a 6 in each of their tosses. This explanation assumes that a group does not produce more than one 6, so it does not actually correspond to the original problem.\n\nA natural generalization of the problem is to consider \"n\" non-necessarily fair dice, with \"p\" the probability that each die will select the 6 face when thrown (notice that actually the number of faces of the dice and which face should be selected are irrelevant). If \"r\" is the total number of dice selecting the 6 face, then formula_5 is the probability of having at least \"k\" correct selections when throwing exactly \"n\" dice. Then the original Newton–Pepys problem can be generalized as follows:\n\nLet formula_6 be natural positive numbers s.t. formula_7. Is then formula_8 not smaller than formula_9 for all \"n, p, k\"?\n\nNotice that, with this notation, the original Newton–Pepys problem reads as: is formula_10?\n\nAs noticed in Rubin and Evans (1961), there are no uniform answers to the generalized Newton–Pepys problem since answers depend on \"k, n\" and \"p\". There are nonetheless some variations of the previous questions that admit uniform answers:\n\n(from Chaundy and Bullard (1960)):\n\nIf formula_11 are positive natural numbers, and formula_12, then formula_13.\n\nIf formula_14 are positive natural numbers, and formula_15, then formula_16.\n\n(from Varagnolo, Pillonetto and Schenato (2013)):\n\nIf formula_17 are positive natural numbers, and formula_18 then formula_19.\n"}
{"id": "21264995", "url": "https://en.wikipedia.org/wiki?curid=21264995", "title": "Node (UML)", "text": "Node (UML)\n\nA node In the Unified Modeling Language (UML) is a computational resource upon which UML artifacts may be deployed for execution.\nThere are two types of nodes: \"device\" nodes and \"execution environments\".\n\n\nExecution environments can be nested. Nodes can be interconnected through communication paths to define network structures. A \"communication path\" is an \"association between two DeploymentTargets, through which they are able to exchange signals and messages\".\n\nWhen modeling devices, it is possible to model them in several different ways:\n\n\nUse tagged values to specify characteristics of devices / execution environments, for instance \"Memory=2GB\", \"Disk Space=32GB\", \"Version=2.5.1\".\n"}
{"id": "2139226", "url": "https://en.wikipedia.org/wiki?curid=2139226", "title": "Ordinal arithmetic", "text": "Ordinal arithmetic\n\nIn the mathematical field of set theory, ordinal arithmetic describes the three usual operations on ordinal numbers: addition, multiplication, and exponentiation. Each can be defined in essentially two different ways: either by constructing an explicit well-ordered set which represents the operation or by using transfinite recursion. Cantor normal form provides a standardized way of writing ordinals. The so-called \"natural\" arithmetical operations retain commutativity at the expense of continuity. Interpreted as nimbers, ordinals are also subject to nimber arithmetic operations.\n\nThe union of two disjoint well-ordered sets \"S\" and \"T\" can be well-ordered. The order-type of that union is the ordinal which results from adding the order-types of \"S\" and \"T\". If two well-ordered sets are not already disjoint, then they can be replaced by order-isomorphic disjoint sets, e.g. replace \"S\" by {0} × \"S\" and \"T\" by {1} × \"T\". This way, the well-ordered set \"S\" is written \"to the left\" of the well-ordered set \"T\", meaning one defines an order on \"S\" formula_1 \"T\" in which every element of \"S\" is smaller than every element of \"T\". The sets \"S\" and \"T\" themselves keep the ordering they already have. This addition of the order-types is associative and generalizes the addition of natural numbers.\n\nThe first transfinite ordinal is ω, the set of all natural numbers.\nFor example, the ordinal ω + ω is obtained by two copies of the natural numbers ordered in the usual fashion and the second copy completely to the right of the first. Writing 0' < 1' < 2' < ... for the second copy, ω + ω looks like\nThis is different from ω because in ω only 0 does not have a direct predecessor while in ω + ω the two elements 0 and 0' do not have direct predecessors.\nAs another example, here are 3 + ω and ω + 3:\nAfter relabeling, the former just looks like ω itself, i.e. 3 + ω = ω, while the latter does not: is not equal to ω since has a largest element (namely, 2') and ω does not (even if ω and are equipotent, they are not isomorphic). Hence, this addition is not commutative. In fact it is quite rare for α+β to be equal to β+α: this happens if and only if α=γ\"m\", β=γ\"n\" for some ordinal γ and natural numbers \"m\" and \"n\". From this it follows that \"α commutes with β\" is an equivalence relation on the set of nonzero ordinals, and all the equivalence classes are countably infinite.\n\nHowever, addition is still associative; one can see for example that (ω + 4) + ω = ω + (4 + ω) = ω + ω.\n\nThe definition of addition can also be given inductively (the following induction is on \"β\"):\n\nUsing this definition, ω + 3 can be seen to be a successor ordinal (it is the successor of ω + 2), whereas 3 + ω is a limit ordinal, namely, the limit of 3 + 0 = 3, 3 + 1 = 4, 3 + 2 = 5, etc., which is just ω.\n\nZero is an additive identity \"α\" + 0 = 0 + \"α\" = \"α\".\n\nAddition is associative (\"α\" + \"β\") + \"γ\" = \"α\" + (\"β\" + \"γ\").\n\nAddition is strictly increasing and continuous in the right argument:\nbut the analogous relation does not hold for the left argument; instead we only have:\n\nOrdinal addition is left-cancellative: if \"α\" + \"β\" = \"α\" + \"γ\", then \"β\" = \"γ\". Furthermore, one can define left subtraction for ordinals \"β\" ≤ \"α\": there is a unique \"γ\" such that \"α\" = \"β\" + \"γ\".\nOn the other hand, right cancellation does not work:\nNor does right subtraction, even when \"β\" ≤ \"α\": for example, there does not exist any \"γ\" such that \"γ\" + 42 = ω.\n\nIf the ordinals less than α are closed under addition and contain 0 then α is occasionally called a γ-number (see additively indecomposable ordinal). These are exactly the ordinals of the form ω.\n\nThe Cartesian product, \"S×T\", of two well-ordered sets \"S\" and \"T\" can be well-ordered by a variant of lexicographical order that puts the least significant position first. Effectively, each element of \"T\" is replaced by a disjoint copy of \"S\". The order-type of the Cartesian product is the ordinal which results from multiplying the order-types of \"S\" and \"T\". Again, this operation is associative and generalizes the multiplication of natural numbers.\n\nHere is ω·2:\nwhich has the same order type as ω + ω. In contrast, 2·ω looks like this:\nand after relabeling, this looks just like ω.\nThus, ω·2 = ω+ω ≠ ω = 2·ω, showing that multiplication of ordinals is not commutative. More generally, a natural number greater than 1 never commutes with any infinite ordinal, and two infinite ordinals α, β commute if and only if α = β for some positive natural numbers \"m\" and \"n\". The relation \"α commutes with β\" is an equivalence relation on the ordinals greater than 1, and all equivalence classes are countably infinite.\n\nDistributivity partially holds for ordinal arithmetic: \"R\"(\"S\"+\"T\") = \"RS\"+\"RT\". However, the other distributive law (\"T\"+\"U\")\"R\" = \"TR\"+\"UR\" is \"not\" generally true: (1+1)·ω = 2·ω = ω while 1·ω+1·ω = ω+ω which is different. Therefore, the ordinal numbers form a left near-semiring, but do \"not\" form a ring.\n\nThe definition of multiplication can also be given inductively (the following induction is on \"β\"):\n\nThe main properties of the product are:\n\nA δ-number (see additively indecomposable ordinal#Multiplicatively indecomposable) is an ordinal greater than 1 such that αδ=δ whenever 0<α<δ. These consist of the ordinal 2 and the ordinals of the form ω.\n\nThe definition of ordinal exponentiation for finite exponents is straightforward. If the exponent is a finite number, the power is the result of iterated multiplication. For instance, ω = ω·ω using the operation of ordinal multiplication. Note that ω·ω can be defined using the set of functions from 2 = {0,1} to ω = {0,1,2...}, ordered lexicographically with the least significant position first:\nHere for brevity, we have replaced the function {(0,\"k\"), (1,\"m\")} by the ordered pair (\"k\", \"m\").\n\nSimilarly, for any finite exponent \"n\", formula_9 can be defined using the set of functions from \"n\" (the domain) to the natural numbers (the range). These functions can be abbreviated as \"n\"-tuples of natural numbers.\n\nBut for infinite exponents, the definition may not be obvious. A limit ordinal, such as ω, is the supremum of all smaller ordinals. It might seem natural to define ω using the set of all infinite sequences of natural numbers. However, we find that any absolutely defined ordering on this set is not well-ordered. To deal with this issue we can use the variant lexicographical ordering again. We restrict the set to sequences which are nonzero for only a finite number of arguments. This is naturally motivated as the limit of the finite powers of the base (similar to the concept of coproduct in algebra). This can also be thought of as the infinite union formula_10.\n\nEach of those sequences corresponds to an ordinal less than formula_11 such as formula_12 and formula_11 is the supremum of all those smaller ordinals.\n\nThe lexicographical order on this set is a well ordering that resembles the ordering of natural numbers written in decimal notation, except with digit positions reversed, and with arbitrary natural numbers instead of just the digits 0–9:\n\nIn general, any ordinal \"α\" can be raised to the power of another ordinal \"β\" in the same way to get \"α\".\n\nIt is easiest to explain this using Von Neumann's definition of an ordinal as the set of all smaller ordinals. Then, to construct a set of order type \"α\" consider all functions from \"β\" to \"α\" such that only a finite number of elements of the domain \"β\" map to a non zero element of \"α\" (essentially, we consider the functions with finite support). The order is lexicographic with the least significant position first. We find\n\nThe definition of exponentiation can also be given inductively (the following induction is on \"β\", the exponent):\n\nProperties of ordinal exponentiation:\n\nWhile the same notation is used for ordinal exponentiation and cardinal exponentiation, ordinal exponentiation is quite different from cardinal exponentiation. For example, with ordinal exponentiation formula_14, but for formula_15(aleph naught, the cardinality of formula_16), formula_17. Here, formula_18 is the cardinality of the set of all functions from the set of all natural numbers to a set with two elements. (This is the cardinality of the power set of the set of all natural numbers and is equal to formula_19, the cardinality of the continuum.) To avoid confusing ordinal exponentiation with cardinal exponentiation, one can use symbols for ordinals (e.g. ω) in the former and symbols for cardinals (e.g. formula_15) in the latter.\n\nJacobsthal showed that the only solutions of α = β with α ≤ β are given by α = β, or α = 2 and β = 4, or α is any limit ordinal and β = εα where ε is an ε-number larger than α.\n\nEvery ordinal number \"α\" can be uniquely written as formula_21, where \"k\" is a natural number, formula_22 are positive integers, and formula_23 are ordinal numbers. This decomposition of \"α\" is called the Cantor normal form of \"α\", and can be considered the base-ω positional numeral system. The highest exponent formula_24 is called the degree of formula_25, and satisfies formula_26. The equality formula_27 applies if and only if formula_28. In that case Cantor normal form does not express the ordinal in terms of smaller ones; this can happen as explained below.\n\nA minor variation of Cantor normal form, which is usually slightly easier to work with, is to set all the numbers \"c\" equal to 1 and allow the exponents to be equal. In other words, every ordinal number α can be uniquely written as formula_29, where \"k\" is a natural number, and formula_30 are ordinal numbers.\n\nAnother variation of the Cantor normal form is the \"base δ expansion\", where ω is replaced by any ordinal δ>1, and the numbers \"c\" are positive ordinals less than δ.\n\nThe Cantor normal form allows us to uniquely express—and order—the ordinals \"α\" that are built from the natural numbers by a finite number of arithmetical operations of addition, multiplication and exponentiation base-formula_16: in other words, assuming formula_32 in the Cantor normal form, we can also express the exponents formula_33 in Cantor normal form, and making the same assumption for the formula_33 as for α and so on recursively, we get a system of notation for these ordinals (for example, \n\ndenotes an ordinal).\n\nThe ordinal ε (epsilon nought) is the set of ordinal values α of the finite-length arithmetical expressions of Cantor normal form that are hereditarily non-trivial where non-trivial means β<α when 0<α. It is the smallest ordinal that does not have a finite arithmetical expression in terms of ω, and the smallest ordinal such that formula_36, i.e. in Cantor normal form the exponent is not smaller than the ordinal itself. It is the limit of the sequence \n\nThe ordinal ε is important for various reasons in arithmetic (essentially because it measures the proof-theoretic strength of the first-order Peano arithmetic: that is, Peano's axioms can show transfinite induction up to any ordinal less than ε but not up to ε itself).\n\nThe Cantor normal form also allows us to compute sums and products of ordinals: to compute the sum, for example, one need merely know that\n\nif formula_39 (if formula_40 one can apply the distributive law on the left and rewrite this as formula_41, and if formula_42 the expression is already in Cantor normal form); and to compute products, the essential facts are that when formula_43 is in Cantor normal form and formula_44, then \n\nand\n\nif n is a non-zero natural number.\n\nTo compare two ordinals written in Cantor normal form, first compare formula_24, then formula_48, then formula_49, then formula_50, etc.. At the first difference, the ordinal that has the larger component is the larger ordinal. If they are the same until one terminates before the other, then the one that terminates first is smaller.\n\nErnst Jacobsthal showed that the ordinals satisfy a form of the unique factorization theorem: every nonzero ordinal can be written as a product of a finite number of prime ordinals. This factorization into prime ordinals is in general not unique, but there is a \"minimal\" factorization into primes that is unique up to changing the order of finite prime factors .\n\nA prime ordinal is an ordinal greater than 1 that cannot be written as a product of two smaller ordinals. Some of the first primes are 2, 3, 5, ... , ω, ω+1, ω+1, ω+1, ..., ω, ω+1, ω+1, ... There are three sorts of prime ordinals:\n\nFactorization into primes is not unique: for example, 2×3=3×2, 2×ω=ω, (ω+1)×ω=ω×ω and ω×ω = ω. However, there is a unique factorization into primes satisfying the following additional conditions:\n\nThis prime factorization can easily be read off using the Cantor normal form as follows:\n\nSo the factorization of the Cantor normal form ordinal\ninto a minimal product of infinite primes and integers is\nwhere each \"n\" should be replaced by its factorization into a non-increasing sequence of finite primes and\n\nAs discussed above, the Cantor Normal Form of ordinals below formula_56 can be expressed in an alphabet containing only the function symbols for addition, multiplication and exponentiation, as well as constant symbols for each natural number and for formula_16. We can do away with the infinitely many numerals by using just the constant symbol 0 and the operation of successor, formula_58 (for example, the integer 4 may be expressed as formula_59). This describes an \"ordinal notation\": a system for naming ordinals over a finite alphabet. This particular system of ordinal notation is called the collection of \"arithmetical\" ordinal expressions, and can express all ordinals below formula_56, but cannot express formula_56. There are other ordinal notations capable of capturing ordinals well past formula_56, but because there are only countably many strings over any finite alphabet, for any given ordinal notation there will be ordinals below formula_63 (the first uncountable ordinal) that are not expressible. Such ordinals are known as large countable ordinals.\n\nThe operations of addition, multiplication and exponentiation are all examples of primitive recursive ordinal functions, and more general primitive recursive ordinal functions can be used to describe larger ordinals.\n\nThe natural sum and natural product operations on ordinals were defined in 1906 by Gerhard Hessenberg, and are sometimes called the Hessenberg sum (or product) . These are the same as the addition and multiplication (restricted to ordinals) of John Conway's field of surreal numbers. They have the advantage that they are associative and commutative, and natural product distributes over natural sum. The cost of making these operations commutative is that they lose the continuity in the right argument which is a property of the ordinary sum and product.\nThe natural sum of α and β is sometimes denoted by α # β, and the natural product by a sort of doubled × sign: α ⨳ β. \n(Other common notation is α ⊕ β and α ⊗ β).\nTo define the natural sum of two ordinals, consider once again the disjoint union formula_64 of two well-ordered sets having these order types. Start by putting a partial order on this disjoint union by taking the orders on \"S\" and \"T\" separately but imposing no relation between \"S\" and \"T\". Now consider the order types of \"all\" well-orders on formula_64 which extend this partial order: the least upper bound of all these ordinals (which is, actually, not merely a least upper bound but actually a greatest element) is the natural sum. Alternatively, we can define the natural sum of \"α\" and \"β\" inductively (by simultaneous induction on \"α\" and \"β\") as the smallest ordinal greater than the natural sum of \"α\" and \"γ\" for all \"γ\" < \"β\" and of \"γ\" and \"β\" for all \"γ\" < \"α\".\n\nThe natural sum is associative and commutative. It is always greater or equal to the usual sum, but it may be greater. For example, the natural sum of ω and 1 is ω+1 (the usual sum), but this is also the natural sum of 1 and ω.\n\nTo define the natural product of two ordinals, consider once again the cartesian product \"S\" × \"T\" of two well-ordered sets having these order types. Start by putting a partial order on this cartesian product by using just the product order (compare two pairs if and only if each of the two coordinates is comparable). Now consider the order types of \"all\" well-orders on \"S\" × \"T\" which extend this partial order: the least upper bound of all these ordinals (which is, actually, not merely a least upper bound but actually a greatest element) is the natural product. There is also an inductive definition of the natural product (by mutual induction), but it is somewhat tedious to write down and we shall not do so (see the article on surreal numbers for the definition in that context, which, however, uses surreal subtraction, something which obviously cannot be defined on ordinals).\n\nThe natural product is associative and commutative and distributes over the natural sum. It is always greater or equal to the usual product, but it may be greater. For example, the natural product of ω and 2 is ω·2 (the usual product), but this is also the natural product of 2 and ω.\n\nYet another way to define the natural sum and product of two ordinals α and β is to use the Cantor normal form: one can find a sequence of ordinals \nγ > … > γ \nand two sequences (\"k\", …, \"k\") and \n(\"j\", …, \"j\") of natural numbers (including zero, but satisfying \n\"k\" + \"j\" > 0 for all \"i\") such that \nand defines \n\nUnder natural addition, the ordinals can be identified with the elements of the free abelian group with basis the gamma numbers ω that have non-negative integer coefficients. Under natural addition and multiplication, the ordinals can be identified with the elements of the (commutative) polynomial ring generated by the delta numbers ω that have non-negative integer coefficients.\nThe ordinals do not have unique factorization into primes under the natural product. While the full polynomial ring does have unique factorization, the subset of polynomials with non-negative coefficients does not: for example, if \"x\" is any delta number, then\nhas two incompatible expressions as a natural product of polynomials with non-negative coefficients that cannot be decomposed further.\n\nThere are arithmetic operations on ordinals by virtue of the one-to-one correspondence between ordinals and nimbers. Three common operations on nimbers are nimber addition, nimber multiplication, and minimum excludance (mex). Nimber addition is a generalization of the bitwise exclusive or operation on natural numbers. The of a set of ordinals is the smallest ordinal \"not\" present in the set.\n\n\n"}
{"id": "5103096", "url": "https://en.wikipedia.org/wiki?curid=5103096", "title": "Overlap (term rewriting)", "text": "Overlap (term rewriting)\n\nIn mathematics, computer science and logic, overlap, as a property of the reduction rules in term rewriting system, describes a situation where a number of different reduction rules specify potentially contradictory ways of reducing a reducible expression (or \"redex\") within a term. More precisely, if a number of different reduction rules share function symbols on the left hand side, overlap can occur. Often we do not consider trivial overlap with a redex and itself.\n\nFor example, consider the term rewriting system defined by the reduction rules\nThe term \"f\"(\"g\"(\"x\"), \"y\") can be reduced via ρ to yield \"y\", but it can also be reduced via ρ to yield \"f\"(\"f\"(\"x\", \"x\"), \"y\"). Note how the redex \"g\"(\"x\") is contained in the redex \"f\"(\"g\"(\"x\"), \"y\"). The result of reducing different redexes is described in a \"critical pair\"—the critical pair arising out of this term rewriting system is (\"f\"(\"f\"(\"x\", \"x\"), \"y\"), \"y\").\n\nOverlap may occur with fewer than two reduction rules. Consider the term rewriting system defined by the reduction rule\nThe term \"g\"(\"g\"(\"g\"(\"x\"))) has overlapping redexes, which can be either applied to the innermost occurrence or to the outermost occurrence of the \"g\"(\"g\"(\"x\")) term.\n"}
{"id": "12226979", "url": "https://en.wikipedia.org/wiki?curid=12226979", "title": "Peano–Russell notation", "text": "Peano–Russell notation\n\nIn mathematical logic, Peano–Russell notation was Bertrand Russell's application of Giuseppe Peano's logical notation to the logical notions of Frege and was used in the writing of \"Principia Mathematica\" in collaboration with Alfred North Whitehead:\n\"The notation adopted in the present work is based upon that of Peano, and the following explanations are to some extent modelled on those which he prefixes to his \"Formulario Mathematico\".\" (Chapter I: Preliminary Explanations of Ideas and Notations, page 4)\n\nIn the notation, variables are ambiguous in denotation, preserve a recognizable identity appearing in various places in logical statements within a given context, and have a range of possible determination between any two variables which is the same or different. When the possible determination is the same for both variables, then one implies the other; otherwise, the possible determination of one given to the other produces a meaningless phrase. The alphabetic symbol set for variables includes the lower and upper case Roman letters as well as many from the Greek alphabet.\n\nThe four fundamental functions are the \"contradictory function\", the \"logical sum\", the \"logical product\", and the \"implicative function\".\n\nThe contradictory function applied to a proposition returns its negation.\n\nThe logical sum applied to two propositions returns their disjunction.\n\nThe logical product applied to two propositions returns the truth-value of both propositions being simultaneously true.\n\nThe implicative function applied to two ordered propositions returns the truth value of the first implying the second proposition.\n\n\"Equivalence\" is written as formula_5, standing for formula_6.\n\n\"Assertion\" is same as the making of a statement between two full stops.\nAn asserted proposition is either true or an error on the part of the writer.\n\n\"Inference\" is equivalent to the rule \"modus ponens\", where formula_8\n\nIn addition to the logical product, \"dots\" are also used to show groupings of functions of propositions. In the above example, the dot before the final implication function symbol groups all of the previous functions on that line together as the antecedent to the final consequent.\n\nThe notation includes \"definitions\" as complex functions of propositions, using the equals sign \"=\" to separate the defined term from its symbolic definition, ending with the letters \"Df\".\nRussell, Bertrand and Alfred North Whitehead (1910). \"Principia Mathematica\" Cambridge, England: The University Press.\n"}
{"id": "51905555", "url": "https://en.wikipedia.org/wiki?curid=51905555", "title": "ProbOnto", "text": "ProbOnto\n\nProbOnto is a knowledge base and ontology of probability distributions. ProbOnto 2.5 (released on January 16, 2017) contains over 150 uni- and multivariate distributions and alternative parameterizations, more than 220 relationships and re-parameterization formulas, supporting also the encoding of empirical and univariate mixture distributions.\n\nProbOnto was initially designed to facilitate the encoding of nonlinear-mixed effect models and their annotation in Pharmacometrics Markup Language (PharmML) developed by DDMoRe, an Innovative Medicines Initiative project. However, ProbOnto, due to its generic structure can be applied in other platforms and modeling tools for encoding and annotation of diverse models applicable to discrete (e.g. count, categorical and time-to-event) and continuous data.\n\nThe knowledge base stores for each distribution:\n\nProbOnto stores in Version 2.5 over 220 relationships between univariate distributions with re-parameterizations as a special case, see figure. While this form of relationships is often neglected in literature, and the authors concentrate one a particular form for each distribution, they are crucial from the interoperability point of view. ProbOnto focuses on this aspect and features more than 15 distributions with alternative parameterizations.\n\nMany distributions are defined with mathematically equivalent but algebraically different formulas. This leads to issues when exchanging models between software tools. The following examples illustrate that.\n\nNormal distribution can be defined in at least three ways \nformula_1\nformula_2\nformula_3\nThe following formulas can be used to re-calculate the three different forms of the normal distribution (we use abbreviations i.e. formula_4 instead of formula_5 etc.)\n\n\n\nIn the case of the log-normal distribution there are more options. This is due to the fact that it can be parameterized in terms of parameters on the natural and log scale, see figure. \nThe available forms in ProbOnto 2.0 are\nformula_9\nformula_10\nformula_11\nformula_12\nformula_13\nformula_14\nformula_15\n\nProbOnto knowledge base stores such re-parameterization formulas to allow for a correct translation of models between tools.\n\nLet's consider the situation when one would like to run a model using two different optimal design tools, e.g. PFIM and PopED. The former supports the LN2, the latter LN7 parameterization, respectively. Therefore, the re-parameterization is required, otherwise the two tools would produce different results.\n\nFor the transition formula_16 following formulas hold\nformula_17.\n\nFor the transition formula_18 following formulas hold\nformula_19.\n\nAll remaining re-parameterisation formulas can be found in the specification document on the project website.\n\nThe knowledge base is built from a simple ontological model. At its core, a probability distribution is an instance of the class thereof, a specialization of the class of mathematical objects. A distribution relates to a number of other individuals, which are instances of various categories in the ontology. For example, these are parameters and related functions associated with a given probability distribution. This strategy allows for the rich representation of attributes and relationships between domain objects. The ontology can be seen as a conceptual schema in the domain of mathematics and has been implemented as a PowerLoom knowledge base. An OWL version is generated programmatically using the Jena API.\n\nOutput for ProbOnto are provided as supplementary materials and published on or linked from the probonto.org website. The OWL version of ProbOnto is available via Ontology Lookup Service (OLS) to facilitate simple searching and visualization of the content. In addition the OLS API provides methods to programmatically access ProbOnto and to integrate it into applications. ProbOnto is also registered on the BioSharing portal.\n\nA PharmML interface is provided in form of a generic XML schema for the definition of the distributions and their parameters. Defining functions, such as probability density function (PDF), probability mass function (PMF), hazard function (HF) and survival function (SF), can be accessed via methods provided in the PharmML schema.\n\nThis example shows how the zero-inflated Poisson distribution is encoded by using its \"codename\" and declaring that of its parameters (‘rate’ and ‘probabilityOfZero’). Model parameters \"Lambda\" and \"P0\" are assigned to the parameter code names.\nTo specify any given distribution unambiguously using ProbOnto, it is sufficient to declare its code name and the code names of its parameters.\nMore examples and a detailed specification can be found on the project website.\n\n\n"}
{"id": "3129341", "url": "https://en.wikipedia.org/wiki?curid=3129341", "title": "Quasi-Lie algebra", "text": "Quasi-Lie algebra\n\nIn mathematics, a quasi-Lie algebra in abstract algebra is just like a Lie algebra, but with the usual axiom \n\nreplaced by \n\nIn characteristic other than 2, these are equivalent (in the presence of bilinearity), so this distinction doesn't arise when considering real or complex Lie algebras. It can however become important, when considering \"Lie algebras\" over the integers.\n\nIn a quasi-Lie algebra, \n\nTherefore the bracket of any element with itself is 2-torsion, if it does not actually vanish.\n\n"}
{"id": "21565753", "url": "https://en.wikipedia.org/wiki?curid=21565753", "title": "Ran Raz", "text": "Ran Raz\n\nRan Raz () is a computer scientist who works in the area of computational complexity theory. He was a professor in the faculty of mathematics and computer science at the Weizmann Institute. He is now a professor of computer science at Princeton University.\n\nRan Raz received his Ph.D. at the Hebrew University of Jerusalem in 1992 under Avi Wigderson and Michael Ben-Or.\n\nRan Raz is well known for his work on interactive proof systems. His two most-cited papers are on multi-prover interactive proofs and on probabilistically checkable proofs.\n\nRan Raz received the Erdős Prize in 2002. His work has been awarded in the top conferences in theoretical computer science. In 2004, he received the best paper award in ACM Symposium on Theory of Computing (STOC) for , and the best paper award in IEEE Conference on Computational Complexity (CCC) for . In 2008, the work received the best paper award in IEEE Symposium on Foundations of Computer Science (FOCS).\n\n"}
{"id": "956355", "url": "https://en.wikipedia.org/wiki?curid=956355", "title": "Reptiles (M. C. Escher)", "text": "Reptiles (M. C. Escher)\n\nReptiles is a lithograph print by the Dutch artist M. C. Escher first printed in March 1943.\n\nIt depicts a desk upon which is a 2D drawing of a tessellated pattern of reptiles and hexagons. The reptiles at one edge of the drawing emerge into three dimensional reality, come to life and appear to crawl over a series of symbolic objects (a book on nature, a geometers triangle, a three dimensional/pentagonal dodecahedron, a pewter bowl containing a box of matches and a box of cigarettes) to eventually re-enter the drawing at its opposite edge. Other objects on the desk that may also be symbolic are a potted cactus and yucca, a ceramic flask with a cork stopper next to a small glass of liquid, a book of JOB cigarette rolling papers, and an open handwritten note book of many pages. Although only the size of small lizards, the reptiles have protruding crocodile-like fangs, and the one atop the dodecahedron has a dragon-like puff of smoke billowing from its nostrils.\n\nLike many of Escher's works, the meaning of the imagery is enigmatic. There are, however, a number of scholarly interpretations of the work. One is that the lizard represents man, emerging from pure concept in two dimensions, becoming a three dimensional being, groping through life acquiring knowledge and wisdom, arriving at deep understanding of science and sacred geometric principals and the alchemical/spiritual nature of reality and existence, and finally descending to reunion with the realm of pure concept. In this theory the objects around the edge of the drawing are signals of the stages of emergence (cactus, biological life), the beginning of the fire-quest for knowledge (rolling papers), contemplation and distillation (a jug and partially consumed glass of spirits), deep study, understanding and wisdom (the open well-used notebook), and re-integration (matches and cigarettes symbolizing controlled/civilized fire that nonetheless consumes and returns the physical to the abstract).\n\nOnce a woman telephoned Escher and told him that she thought the image was a \"striking illustration of reincarnation\".\n\nA colorized version of the lithograph was used by rock band Mott the Hoople as the sleeve artwork for its eponymous first album, released in 1969.\n\n\n\n"}
{"id": "1705432", "url": "https://en.wikipedia.org/wiki?curid=1705432", "title": "Routh–Hurwitz stability criterion", "text": "Routh–Hurwitz stability criterion\n\nIn control system theory, the Routh–Hurwitz stability criterion is a mathematical test that is a necessary and sufficient condition for the stability of a linear time invariant (LTI) control system. The Routh test is an efficient recursive algorithm that English mathematician Edward John Routh proposed in 1876 to determine whether all the roots of the characteristic polynomial of a linear system have negative real parts. German mathematician Adolf Hurwitz independently proposed in 1895 to arrange the coefficients of the polynomial into a square matrix, called the Hurwitz matrix, and showed that the polynomial is stable if and only if the sequence of determinants of its principal submatrices are all positive. The two procedures are equivalent, with the Routh test providing a more efficient way to compute the Hurwitz determinants than computing them directly. A polynomial satisfying the Routh–Hurwitz criterion is called a Hurwitz polynomial.\n\nThe importance of the criterion is that the roots p of the characteristic equation of a linear system with negative real parts represent solutions e of the system that are stable (bounded). Thus the criterion provides a way to determine if the equations of motion of a linear system have only stable solutions, without solving the system directly. For discrete systems, the corresponding stability test can be handled by the Schur–Cohn criterion, the Jury test and the Bistritz test. With the advent of computers, the criterion has become less widely used, as an alternative is to solve the polynomial numerically, obtaining approximations to the roots directly.\n\nThe Routh test can be derived through the use of the Euclidean algorithm and Sturm's theorem in evaluating Cauchy indices. Hurwitz derived his conditions differently.\n\nThe criterion is related to Routh–Hurwitz theorem. Indeed, from the statement of that theorem, we have formula_1 where:\nBy the fundamental theorem of algebra, each polynomial of degree \"n\" must have \"n\" roots in the complex plane (i.e., for an \"ƒ\" with no roots on the imaginary line, \"p\" + \"q\" = \"n\"). Thus, we have the condition that \"ƒ\" is a (Hurwitz) stable polynomial if and only if \"p\" − \"q\" = \"n\" (the proof is given below). Using the Routh–Hurwitz theorem, we can replace the condition on \"p\" and \"q\" by a condition on the generalized Sturm chain, which will give in turn a condition on the coefficients of \"ƒ\".\n\nLet \"f\"(\"z\") be a complex polynomial. The process is as follows:\n\nNotice that we had to suppose \"b\" different from zero in the first division. The generalized Sturm chain is in this case formula_19. Putting formula_20, the sign of formula_21 is the opposite sign of \"a\" and the sign of \"by\" is the sign of \"b\". When we put formula_22, the sign of the first element of the chain is again the opposite sign of \"a\" and the sign of \"by\" is the opposite sign of \"b\". Finally, -\"c\" has always the opposite sign of \"c\".\n\nSuppose now that \"f\" is Hurwitz-stable. This means that formula_23 (the degree of \"f\"). By the properties of the function \"w\", this is the same as formula_24 and formula_25. Thus, \"a\", \"b\" and \"c\" must have the same sign. We have thus found the necessary condition of stability for polynomials of degree 2.\n\nSystems meeting the above criteria are said to be open-loop stable; otherwise, they are unstable because there are sign changes in the first-column elements.\n\nA tabular method can be used to determine the stability when the roots of a higher order characteristic polynomial are difficult to obtain. For an \"n\"th-degree polynomial\nthe table has \"n\" + 1 rows and the following structure:\n\nwhere the elements formula_34 and formula_35 can be computed as follows:\nWhen completed, the number of sign changes in the first column will be the number of non-negative roots.\n\nIn the first column, there are two sign changes (0.75 → −3, and −3 → 3), thus there are two non-negative roots where the system is unstable.\n\nSometimes the presence of poles on the imaginary axis creates a situation of marginal stability. In that case the coefficients of the \"Routh array\" in a whole row become zero and thus further solution of the polynomial for finding changes in sign is not possible. Then another approach comes into play. The row of polynomial which is just above the row containing the zeroes is called the \"auxiliary polynomial\".\n\nWe have the following table:\n\nIn such a case the auxiliary polynomial is formula_39 which is again equal to zero. The next step is to differentiate the above equation which yields the following polynomial. formula_40. The coefficients of the row containing zero now become\n\"8\" and \"24\". The process of Routh array is proceeded using these values which yield two points on the imaginary axis. These two points on the imaginary axis are the prime cause of marginal stability.\n\n\n"}
{"id": "28970", "url": "https://en.wikipedia.org/wiki?curid=28970", "title": "SECD machine", "text": "SECD machine\n\nThe SECD machine is a highly influential (\"See: #Landin's contribution\") virtual machine and abstract machine intended as a target for functional programming language compilers. The letters stand for Stack, Environment, Control, Dump, the internal registers of the machine. The registers Stack, Control, and Dump point to (some realisations of) stacks, and Environment points to (some realisation of) an associative array. \n\nThe machine was the first to be specifically designed to evaluate lambda calculus expressions. It was originally described by Peter J. Landin in \"The Mechanical Evaluation of Expressions\" in 1964. The description published by Landin was fairly abstract, and left many implementation choices open (like an operational semantics). Hence the SECD machine is often presented in a more detailed form, such as Peter Henderson's Lispkit Lisp compiler, which has been distributed since 1980. Since then it has been used as the target for several other experimental compilers.\n\nIn 1989 researchers at the University of Calgary worked on a hardware implementation of the machine.\n\nD. A. Turner (2012) points out that \"The Revised Report on Algol 60\" (Naur 1963) specifies a procedure call by a copying rule which avoids variable capture with a systematic change of identifiers. This method works in the Algol 60 implementation, but in a functional programming language where functions are first-class citizens, a free variable on a call stack might be dereferenced in error.\n\nTurner notes that Landin solved this with his SECD machine, in which a function is represented by a closure in the heap instead.\n\nWhen evaluation of an expression begins, the expression is loaded as the only element of control codice_1. The environment codice_2, stack codice_3 and dump codice_4 begin empty.\n\nDuring evaluation of codice_1 it is converted to reverse Polish notation (RPN) with codice_6 (for apply) being the only operator. For example, the expression codice_7 (a single list element) is changed to the list codice_8.\n\nEvaluation of codice_1 proceeds similarly to other RPN expressions. If the first item in codice_1 is a value, it is pushed onto the stack codice_3. More exactly, if the item is an identifier, the value pushed onto the stack will be the binding for that identifier in the current environment codice_2. If the item is an abstraction, a closure is constructed to preserve the bindings of its free variables (which are in codice_2), and it is this closure which is pushed onto the stack.\n\nIf the item is codice_6, two values are popped off the stack and the application done (first applied to second). If the result of the application is a value, it is pushed onto the stack.\n\nIf the application is of an abstraction to a value, however, it will result in a lambda calculus expression which may itself be an application (rather than a value), and so cannot be pushed onto the stack. In this case, the current contents of codice_3, codice_2, and codice_1 are pushed onto the dump codice_4 (which is a stack of these triples), codice_3 is reinitialised to empty, and codice_1 is reinitialised to the application result with codice_2 containing the environment for the free variables of this expression, augmented with the binding that resulted from the application. Evaluation then proceeds as above.\n\nCompleted evaluation is indicated by codice_1 being empty, in which case the result will be on the stack codice_3. The last saved evaluation state on codice_4 is then popped, and the result of the completed evaluation is pushed onto the stack contents restored from codice_4. Evaluation of the restored state then continues as above.\n\nIf codice_1 and codice_4 are both empty, overall evaluation has completed with the result on the stack codice_3.\n\nThe SECD machine is stack-based. Functions take their arguments from the stack. The arguments to built-in instructions are encoded immediately after them in the instruction stream.\n\nLike all internal data-structures, the stack is a list, with the codice_3 register pointing at the list's \"head\" or beginning. Due to the list structure, the stack need not be a continuous block of memory, so stack space is available as long as there is a single free memory cell. Even when all cells have been used, garbage collection may yield additional free memory. Obviously, specific implementations of the SECD structure can implement the stack as a canonical stack structure, so improving the overall efficiency of the virtual machine, provided that a strict bound be put on the dimension of the stack.\n\nThe codice_1 register points at the head of the code or instruction list that will be evaluated. Once the instruction there has been executed, the codice_1 is pointed at the next instruction in the list—it is similar to an \"instruction pointer\" (or program counter) in conventional machines, except that subsequent instructions are always specified during execution and are not by default contained in subsequent memory locations, as it is the case with the conventional machines.\n\nThe current variable environment is managed by the codice_2 register, which points at a list of lists. Each individual list represents one environment level: the parameters of the current function are in the head of the list, variables that are free in the current function, but bound by a surrounding function, are in other elements of codice_2.\n\nThe dump, at whose head the codice_4 register points, is used as temporary storage for values of the other registers, for example during function calls. It can be likened to the return stack of other machines.\n\nThe memory organization of the SECD machine is similar to the model used by most functional language interpreters: a number of memory cells, each of which can hold either an \"atom\" (a simple value, for example \"13\"), or represent an empty or non-empty list. In the latter case, the cell holds two pointers to other cells, one representing the first element, the other representing the list except for the first element. The two pointers are traditionally named \"car\" and \"cdr\" respectively—but the more modern terms \"head\" and \"tail\" are often used instead. The different types of values that a cell can hold are distinguished by a \"tag\". Often different types of atoms (integers, strings, etc.) are distinguished as well.\n\nSo, a list holding the numbers \"1\", \"2\", and \"3\", usually written as codice_35, might be represented as follows:\n\nThe memory cells 3 to 5 do not belong to our list, the cells of which can be distributed randomly over the memory. Cell 2 is the head of the list, it points to cell 1 which holds the first element's value, and the list containing only \"2\" and \"3\" (beginning at cell 6). Cell 6 points at a cell holding 2 and at cell 7, which represents the list containing only \"3\". It does so by pointing at cell 8 containing the value \"3\", and pointing at an empty list (\"nil\") as cdr. In the SECD machine, cell 0 always implicitly represents the empty list, so no special tag value is needed to signal an empty list (everything needing that can simply point to cell 0).\n\nThe principle that the cdr in a list cell must point at another list is just a convention. If both car and cdr point at atoms, that will yield a pair, usually written like codice_36\n\n\nA number of additional instructions for basic functions like car, cdr, list construction, integer addition, I/O, etc. exist. They all take any necessary parameters from the stack.\n\n\n"}
{"id": "611964", "url": "https://en.wikipedia.org/wiki?curid=611964", "title": "Sobolev space", "text": "Sobolev space\n\nIn mathematics, a Sobolev space is a vector space of functions equipped with a norm that is a combination of \"L\"-norms of the function itself and its derivatives up to a given order. The derivatives are understood in a suitable weak sense to make the space complete, thus a Banach space. Intuitively, a Sobolev space is a space of functions with sufficiently many derivatives for some application domain, such as partial differential equations, and equipped with a norm that measures both the size and regularity of a function.\n\nSobolev spaces are named after the Russian mathematician Sergei Sobolev. Their importance comes from the fact that solutions of partial differential equations are naturally found in Sobolev spaces, rather than in spaces of continuous functions and with the derivatives understood in the classical sense.\n\nIn this section and throughout the article formula_1 is an open subset of formula_2\n\nThere are many criteria for smoothness of mathematical functions. The most basic criterion may be that of continuity. A stronger notion of smoothness is that of differentiability (because functions that are differentiable are also continuous) and a yet stronger notion of smoothness is that the derivative also be continuous (these functions are said to be of class \"C\" — see Differentiability class). Differentiable functions are important in many areas, and in particular for differential equations. In the twentieth century, however, it was observed that the space \"C\" (or \"C\", etc.) was not exactly the right space to study solutions of differential equations. The Sobolev spaces are the modern replacement for these spaces in which to look for solutions of partial differential equations.\n\nQuantities or properties of the underlying model of the differential equation are usually expressed in terms of integral norms, rather than the uniform norm. A typical example is measuring the energy of a temperature or velocity distribution by an \"L\"-norm. It is therefore important to develop a tool for differentiating Lebesgue space functions.\n\nThe integration by parts formula yields that for every \"u\" ∈ \"C\"(Ω), where \"k\" is a natural number, and for all infinitely differentiable functions with compact support formula_3\n\nwhere \"α\" a multi-index of order |\"α\"| = \"k\" and we are using the notation:\n\nThe left-hand side of this equation still makes sense if we only assume \"u\" to be locally integrable. If there exists a locally integrable function \"v\", such that\n\nwe call \"v\" the weak \"α\"-th partial derivative of \"u\". If there exists a weak \"α\"-th partial derivative of \"u\", then it is uniquely defined almost everywhere, and thus it is uniquely determined as an element of a Lebesgue space. On the other hand, if \"u\" ∈ \"C\"(Ω), then the classical and the weak derivative coincide. Thus, if \"v\" is a weak \"α\"-th partial derivative of \"u\", we may denote it by \"Du\" := \"v\".\n\nFor example, the function\n\nis not continuous at zero, and not differentiable at −1, 0, or 1. Yet the function\n\nsatisfies the definition for being the weak derivative of formula_9, which then qualifies as being in the Sobolev space formula_10 (for any allowed \"p\", see definition below).\n\nThe Sobolev spaces \"W\"(Ω) combine the concepts of weak differentiability and Lebesgue norms.\n\nIn the one-dimensional case (functions on formula_11) the Sobolev space is defined to be the subset of functions in formula_12 such that the function and its weak derivatives up to some order have a finite norm, for given . As mentioned above, some care must be taken to define derivatives in the proper sense. In the one-dimensional problem it is enough to assume that , the -th derivative of the function , is differentiable almost everywhere and is equal almost everywhere to the Lebesgue integral of its derivative (this gets rid of examples such as Cantor's function which are irrelevant to what the definition is trying to accomplish).\n\nWith this definition, the Sobolev spaces admit a natural norm,\n\nEquipped with the norm , becomes a Banach space. It turns out that it is enough to take only the first and last in the sequence, i.e., the norm defined by\n\nis equivalent to the norm above (i.e. the induced topologies of the norms are the same).\n\nSobolev spaces with are especially important because of their connection with Fourier series and because they form a Hilbert space. A special notation has arisen to cover this case, since the space is a Hilbert space:\n\nThe space can be defined naturally in terms of Fourier series whose coefficients decay sufficiently rapidly, namely,\n\nwhere formula_16 is the Fourier series of , and formula_17 denotes the 1-torus. As above, one can use the equivalent norm\n\nBoth representations follow easily from Parseval's theorem and the fact that differentiation is equivalent to multiplying the Fourier coefficient by \"in\".\n\nFurthermore, the space admits an inner product, like the space . In fact, the inner product is defined in terms of the inner product:\n\nThe space becomes a Hilbert space with this inner product.\n\nSome other Sobolev spaces permit a simpler description. For example, is the space of absolutely continuous functions on (or rather, equivalence classes of functions that are equal almost everywhere to such), while is the space of Lipschitz functions on , for every interval . All spaces are (normed) algebras, i.e. the product of two elements is once again a function of this Sobolev space, which is not the case for . (E.g., functions behaving like |\"x\"| at the origin are in , but the product of two such functions is not in ).\n\nThe transition to multiple dimensions brings more difficulties, starting from the very definition. The requirement that be the integral of does not generalize, and the simplest solution is to consider derivatives in the sense of distribution theory.\n\nA formal definition now follows. Let formula_20 The Sobolev space is defined to be the set of all functions defined on such that for every multi-index with , the mixed partial derivative\n\nexists in the weak sense and is in , i.e.\n\nThat is, the Sobolev space is defined as\n\nThe natural number is called the order of the Sobolev space .\n\nThere are several choices for a norm for . The following two are common and are equivalent in the sense of equivalence of norms:\n\nand\n\nWith respect to either of these norms, is a Banach space. For , is also a separable space. It is conventional to denote by for it is a Hilbert space with the norm formula_26.\n\nIt is rather hard to work with Sobolev spaces relying only on their definition. It is therefore interesting to know that by theorem of Meyers and Serrin a function can be approximated by smooth functions. This fact often allows us to translate properties of smooth functions to Sobolev functions.\nIf is finite and is open, then there exists for any an approximating sequence of functions such that:\n\nIf has Lipschitz boundary, we may even assume that the are the restriction of smooth functions with compact support on all of formula_28 .\n\nIn higher dimensions, it is no longer true that, for example, \"W\" contains only continuous functions. For example, 1/|\"x\"| belongs to formula_29 where formula_30 is the unit ball in three dimensions. For \"k\" > \"n\"/\"p\" the space \"W\"(Ω) will contain only continuous functions, but for which \"k\" this is already true depends both on \"p\" and on the dimension. For example, as can be easily checked using spherical polar coordinates for the function formula_31 defined on the \"n\"-dimensional ball we have:\n\nIntuitively, the blow-up of \"f\" at 0 \"counts for less\" when \"n\" is large since the unit ball has \"more outside and less inside\" in higher dimensions.\n\nLet . If a function is in , then, possibly after modifying the function on a set of measure zero, the restriction to almost every line parallel to the coordinate directions in formula_28 is absolutely continuous; what's more, the classical derivative along the lines that are parallel to the coordinate directions are in . Conversely, if the restriction of to almost every line parallel to the coordinate directions is absolutely continuous, then the pointwise gradient exists almost everywhere, and is in provided and are both in . In particular, in this case the weak partial derivatives of and pointwise partial derivatives of agree almost everywhere. The ACL characterization of the Sobolev spaces was established by Otto M. Nikodym (1933); see .\n\nA stronger result holds in the case . A function in is, after modifying on a set of measure zero, Hölder continuous of exponent , by Morrey's inequality. In particular, if , then the function is Lipschitz continuous.\n\nThe Sobolev space \"W\"(Ω) is also denoted by \"H\"(Ω). It is a Hilbert space, with an important subspace formula_34 defined to be the closure in \"H\"(Ω) of the infinitely differentiable functions compactly supported in . The Sobolev norm defined above reduces here to\n\nWhen has a regular boundary, can be described as the space of functions in \"H\"(Ω) that vanish at the boundary, in the sense of traces (see below). When , if is a bounded interval, then consists of continuous functions on of the form\n\nwhere the generalized derivative is in and has 0 integral, so that .\n\nWhen is bounded, the Poincaré inequality states that there is a constant such that\n\nWhen is bounded, the injection from to is compact. This fact plays a role in the study of the Dirichlet problem, and in the fact that there exists an orthonormal basis of consisting of eigenvectors of the Laplace operator (with Dirichlet boundary condition).\n\nSobolev spaces are often considered when investigating partial differential equations. It is essential to consider boundary values of Sobolev functions. If \"u\" ∈ \"C\"(Ω), those boundary values are described by the restriction formula_38. However, it is not clear how to describe values at the boundary for \"u\" ∈ \"W\"(Ω), as the \"n\"-dimensional measure of the boundary is zero. The following theorem resolves the problem:\n\n\"Tu\" is called the trace of \"u\". Roughly speaking, this theorem extends the restriction operator to the Sobolev space \"W\"(Ω) for well-behaved Ω. Note that the trace operator \"T\" is in general not surjective, but for 1 < \"p\" < ∞ it maps onto the Sobolev-Slobodeckij space formula_41\n\nIntuitively, taking the trace costs 1/\"p\" of a derivative. The functions \"u\" in \"W\"(Ω) with zero trace, i.e. \"Tu\" = 0, can be characterized by the equality\n\nwhere\n\nIn other words, for Ω bounded with Lipschitz boundary, trace-zero functions in \"W\"(Ω) can be approximated by smooth functions with compact support.\n\nFor a natural number \"k\" and one can show (by using Fourier multipliers) that the space formula_44 can equivalently be defined as\n\nwith the norm\n\nThis motivates Sobolev spaces with non-integer order since in the above definition we can replace \"k\" by any real number \"s\". The resulting spaces\n\nare called Bessel potential spaces (named after Friedrich Bessel). They are Banach spaces in general and Hilbert spaces in the special case \"p\" = 2.\n\nformula_48 is the set of restrictions of functions from formula_49 to Ω equipped with the norm\n\nAgain, \"H\"(Ω) is a Banach space and in the case \"p\" = 2 a Hilbert space.\n\nUsing extension theorems for Sobolev spaces, it can be shown that also \"W\"(Ω) = \"H\"(Ω) holds in the sense of equivalent norms, if Ω is domain with uniform \"C\"-boundary, \"k\" a natural number and . By the embeddings\n\nthe Bessel potential spaces formula_49 form a continuous scale between the Sobolev spaces formula_53 From an abstract point of view, the Bessel potential spaces occur as complex interpolation spaces of Sobolev spaces, i.e. in the sense of equivalent norms it holds that\n\nwhere:\n\nAnother approach to define fractional order Sobolev spaces arises from the idea to generalize the Hölder condition to the \"L\"-setting. For formula_56 and formula_57 the Slobodeckij seminorm (roughly analogous to the Hölder seminorm) is defined by\n\nLet be not an integer and set formula_59. Using the same idea as for the Hölder spaces, the Sobolev–Slobodeckij space \"W\"(Ω) is defined as\n\nIt is a Banach space for the norm\n\nIf formula_1 is suitably regular in the sense that there exist certain extension operators, then also the Sobolev–Slobodeckij spaces form a scale of Banach spaces, i.e. one has the continuous injections or embeddings\n\nThere are examples of irregular Ω such that \"W\"(Ω) is not even a vector subspace of \"W\"(Ω) for 0 < \"s\" < 1.((Check Example 9.1 in the Hitchhiker guide.))\n\nFrom an abstract point of view, the spaces \"W\"(Ω) coincide with the real interpolation spaces of Sobolev spaces, i.e. in the sense of equivalent norms the following holds:\n\nSobolev–Slobodeckij spaces play an important role in the study of traces of Sobolev functions. They are special cases of Besov spaces.\n\nIf formula_1 is a domain whose boundary is not too poorly behaved (e.g., if its boundary is a manifold, or satisfies the more permissive \"cone condition\") then there is an operator \"A\" mapping functions of formula_1 to functions of formula_28 such that:\n\n\nWe will call such an operator \"A\" an extension operator for formula_70\n\nExtension operators are the most natural way to define formula_71 for non-integer \"s\" (we cannot work directly on formula_1 since taking Fourier transform is a global operation). We define formula_71 by saying that formula_74 if and only if formula_75 Equivalently, complex interpolation yields the same formula_71 spaces so long as formula_1 has an extension operator. If formula_1 does not have an extension operator, complex interpolation is the only way to obtain the formula_71 spaces.\n\nAs a result, the interpolation inequality still holds.\n\nLike above, we define formula_80 to be the closure in formula_71 of the space formula_82 of infinitely differentiable compactly supported functions. Given the definition of a trace, above, we may state the following\n\nIf formula_87 we may define its extension by zero formula_88 in the natural way, namely\n\nFor \"f\" ∈ \"L\"(Ω) its extension by zero,\n\nis an element of formula_95 Furthermore,\n\nIn the case of the Sobolev space \"W\"(Ω) for 1 ≤ p ≤ ∞, extending a function \"u\" by zero will not necessarily yield an element of formula_97 But if Ω is bounded with Lipschitz boundary (e.g. ∂Ω is C\"\"), then for any bounded open set O such that Ω⊂⊂O (i.e. Ω is compactly contained in O), there exists a bounded linear operator\n\nsuch that for each \"u\" ∈ \"W\"(Ω): \"Eu\" = \"u\" a.e. on Ω, \"Eu\" has compact support within O, and there exists a constant \"C\" depending only on p, Ω, O and the dimension \"n\", such that\nWe call \"Eu\" an extension of \"u\" to formula_28\n\nIt is a natural question to ask if a Sobolev function is continuous or even continuously differentiable. Roughly speaking, sufficiently many weak derivatives or large \"p\" result in a classical derivative. This idea is generalized and made precise in the Sobolev embedding theorem.\n\nWrite formula_101 for the Sobolev space of some compact Riemannian manifold of dimension \"n\". Here \"k\" can be any real number, and 1 ≤ \"p\" ≤ ∞. (For \"p\" = ∞ the Sobolev space formula_102 is defined to be the Hölder space \"C\" where \"k\" = \"n\" + α and 0 < α ≤ 1.) The Sobolev embedding theorem states that if formula_103 and formula_104 then\n\nand the embedding is continuous. Moreover, if formula_106 and formula_107 then the embedding is completely continuous (this is sometimes called Kondrachov's theorem or the Rellich-Kondrachov theorem). Functions in formula_108 have all derivatives of order less than \"m\" continuous, so in particular this gives conditions on Sobolev spaces for various derivatives to be continuous. Informally these embeddings say that to convert an \"L\" estimate to a boundedness estimate costs 1/\"p\" derivatives per dimension.\n\nThere are similar variations of the embedding theorem for non-compact manifolds such as formula_28 . Sobolev embeddings on formula_28 that are not compact often have a related, but weaker, property of cocompactness.\n\n\n"}
{"id": "248114", "url": "https://en.wikipedia.org/wiki?curid=248114", "title": "Superstrong cardinal", "text": "Superstrong cardinal\n\nIn mathematics, a cardinal number κ is called superstrong if and only if there exists an elementary embedding \"j\" : \"V\" → \"M\" from \"V\" into a transitive inner model \"M\" with critical point κ and formula_1 ⊆ \"M\".\n\nSimilarly, a cardinal κ is n-superstrong if and only if there exists an elementary embedding \"j\" : \"V\" → \"M\" from \"V\" into a transitive inner model \"M\" with critical point κ and formula_2 ⊆ \"M\". Akihiro Kanamori has shown that the consistency strength of an n+1-superstrong cardinal exceeds that of an n-huge cardinal for each n > 0.\n"}
{"id": "37737648", "url": "https://en.wikipedia.org/wiki?curid=37737648", "title": "Surprisal analysis", "text": "Surprisal analysis\n\nSurprisal analysis is an information-theoretical analysis technique that integrates and applies principles of thermodynamics and maximal entropy. Surprisal analysis is capable of relating the underlying microscopic properties to the macroscopic bulk properties of a system. It has already been applied to a spectrum of disciplines including engineering, physics, chemistry and biomedical engineering. Recently, it has been extended to characterize the state of living cells, specifically monitoring and characterizing biological processes in real time using transcriptional data.\n\nSurprisal analysis was formulated at the Hebrew University of Jerusalem as a joint effort between Raphael David Levine, Richard Barry Bernstein and Avinoam Ben-Shaul in 1972. Levine and colleagues had recognized a need to better understand the dynamics of non-equilibrium systems, particularly of small systems, that are not seemingly applicable to thermodynamic reasoning. Alhassid and Levine first applied surprisal analysis in nuclear physics, to characterize the distribution of products in heavy ion reactions. Since its formulation, surprisal analysis has become a critical tool for the analysis of reaction dynamics and is an official IUPAC term.*\n\nMaximum entropy methods are at the core of a new view of scientific inference, allowing analysis and interpretation of large and sometimes noisy data. Surprisal analysis extends principles of maximal entropy and of thermodynamics, where both equilibrium thermodynamics and statistical mechanics are assumed to be inferences processes. This enables surprisal analysis to be an effective method of information quantification and compaction and of providing an unbiased characterization of systems. Surprisal analysis is particularly useful to characterize and understand dynamics in small systems, where energy fluxes otherwise negligible in large systems, heavily influence system behavior.\n\nForemost, surprisal analysis identifies the state of a system when it reaches its maximal entropy, or thermodynamic equilibrium. This is known as balance state of the system because once a system reaches its maximal entropy, it can no longer initiate or participate in spontaneous processes. Following the determination of the balanced state, surprisal analysis then characterizes all the states in which the system deviates away from the balance state. These deviations are caused by constraints; these constraints on the system prevent the system from reaching its maximal entropy. Surprisal analysis is applied to both identify and characterize these constraints. In terms of the constraints, the probability formula_1 of an event formula_2 is quantified by\n\nHere formula_4 is the probability of the event formula_2 in the balanced state. It is usually called the “prior probability” because it is the probability of an event formula_2 prior to any constraints. The surprisal itself is defined as\n\nThe surprisal equals the sum over the constraints and is a measure of the deviation from the balanced state. These deviations are ranked on the degree of deviation from the balance state and ordered on the most to least influential to the system. This ranking is provided through the use of Lagrange multipliers. The most important constraint and usually the constraint sufficient to characterize a system exhibit the largest Lagrange multiplier. The multiplier for constraint formula_8 is denoted above as formula_9; larger multipliers identify more influential constraints. The event variable formula_10 is the value of the constraint formula_8 for the event formula_2. Using the method of Lagrange multipliers requires that the prior probability formula_4 and the nature of the constraints be experimentally identified. A numerical algorithm for determining Lagrange multipliers has been introduced by Agmon et al. Recently, singular value decomposition and principal component analysis of the surprisal was utilized to identify constraints on biological systems, extending surprisal analysis to better understanding biological dynamics as shown in the figure.\n\nSurprisal (a term coined in this context by Myron Tribus) was first introduced to better understand the specificity of energy release and selectivity of energy requirements of elementary chemical reactions. This gave rise to a series of new experiments which demonstrated that in elementary reactions, the nascent products could be probed and that the energy is preferentially released and not statistically distributed. Surprisal analysis was initially applied to characterize a small three molecule system that did not seemingly conform to principles of thermodynamics and a single dominant constraint was identified that was sufficient to describe the dynamic behavior of the three molecule system. Similar results were then observed in nuclear reactions, where differential states with varying energy partitioning are possible. Often chemical reactions require energy to overcome an activation barrier. Surprisal analysis is applicable to such applications as well. Later, surprisal analysis was extended to mesoscopic systems, bulk systems and to dynamical processes.\n\nSurprisal analysis was extended to better characterize and understand cellular processes, see figure, biological phenomena and human disease with reference to personalized diagnostics. Surprisal analysis was first utilized to identify genes implicated in the balance state of cells in vitro; the genes mostly present in the balance state were genes directly responsible for the maintenance of cellular homeostasis. Similarly, it has been used to discern two distinct phenotypes during the EMT of cancer cells.\n\n"}
{"id": "21235214", "url": "https://en.wikipedia.org/wiki?curid=21235214", "title": "Symposium on Parallelism in Algorithms and Architectures", "text": "Symposium on Parallelism in Algorithms and Architectures\n\nSPAA, the ACM Symposium on Parallelism in Algorithms and Architectures, is an academic conference in the fields of parallel computing and distributed computing. It is sponsored by the Association for Computing Machinery special interest groups SIGACT and SIGARCH, and it is organized in cooperation with the European Association for Theoretical Computer Science (EATCS).\n\nSPAA was first organised on 18–21 June 1989, in Santa Fe, New Mexico, United States. In 1989–2002, SPAA was known as Symposium on Parallel Algorithms and Architectures. In 2003, the name changed to \"Symposium on Parallelism in Algorithms and Architectures\" to reflect the extended scope of the conference.\n\nIn 2003 and 2007, SPAA was part of the Federated Computing Research Conference (FCRC), and in 1998, 2005, and 2009, SPAA was co-located with the ACM Symposium on Principles of Distributed Computing (PODC).\n\n\n"}
{"id": "56294750", "url": "https://en.wikipedia.org/wiki?curid=56294750", "title": "Synthetic measure", "text": "Synthetic measure\n\nA synthetic measure (or synthetic indicator) is a value that is the result of combining other metrics, which are measurements of various features.\n\nThere is a method to measure quality of service in hotels. In related study authors aggregate tourist opinions, measured on a scale from 1 to 10. Synthetic measure (indicator) of service quality in each hotel is calculated with the help of the aggregation operator.\n\nOther study proposed to use classical parameters EV, PV and AC to carry out the synthetic measure of project performance.\n\nDifferent normalized stimulants and destimulants were used in research to create synthetic measure that selects countries with the best and the worst levels of implementation of Europe 2020 targets.\n\nThere are scientific works that describe method of quality assessment of Wikipedia articles using synthetic measure. The quality can be measured on a scale between 0 and 100 based on the following formula:\n\nwhere:\n\nThe results of the calculations are available on WikiRank service.\n\n"}
{"id": "25426649", "url": "https://en.wikipedia.org/wiki?curid=25426649", "title": "Tapering (mathematics)", "text": "Tapering (mathematics)\n\nIn mathematics, physics, and theoretical computer graphics, tapering is a kind of shape deformation. Just as an affine transformation, such as scaling or shearing, is a first-order model of shape deformation, there also exist higher-order deformations such as tapering, twisting, and bending. Tapering can be thought of as non-constant scaling by a given tapering function. The resultant deformations can be linear or nonlinear.\n\nTo create a nonlinear taper, instead of scaling in \"x\" and \"y\" for all \"z\" with constants as in:\n\nlet \"a\" and \"b\" be functions of \"z\" so that:\n\nAn example of a linear taper is formula_3, and a quadratic taper formula_4.\n\nAs another example, if the parametric equation of a cube were given by \"ƒ\"(\"t\") = (\"x\"(\"t\"), \"y\"(\"t\"), \"z\"(\"t\")), a nonlinear taper could be applied so that the cube's volume slowly decreases (or tapers) as the function moves in the positive \"z\" direction. For the given cube, an example of a nonlinear taper along \"z\" would be if, for instance, the function \"T\"(\"z\") = 1/(\"a\" + \"bt\") were applied to the cube's equation such that \"ƒ\"(\"t\") = (\"T\"(\"z\")\"x\"(\"t\"), \"T\"(\"z\")\"y\"(\"t\"), \"T\"(\"z\")\"z\"(\"t\")), for some real constants \"a\" and \"b\".\n\n\n"}
