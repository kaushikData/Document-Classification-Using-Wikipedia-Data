{"id": "467754", "url": "https://en.wikipedia.org/wiki?curid=467754", "title": "169 (number)", "text": "169 (number)\n\n169 (one hundred [and] sixty-nine) is the natural number following 168 and preceding 170.\n\n169 is an odd number, a composite number, and a deficient number.\n\n169 is a square number: 13 x 13 = 169, and if each number is reversed the equation is still true: 31 x 31 = 961. 144 shares this property: 12 x 12 = 144, 21 x 21 = 441.\n\n169 is one of the few squares to also be a centered hexagonal number. Like all odd squares, it is a centered octagonal number. 169 is an odd-indexed Pell number, thus it is also a Markov number, appearing in the solutions (2, 169, 985), (2, 29, 169), (29, 169, 14701), etc. 169 is the sum of seven consecutive primes: 13 + 17 + 19 + 23 + 29 + 31 + 37. 169 is a difference in consecutive cubes, equaling formula_1\n\n\n\n\n\n\n169 is also:\n\n\n"}
{"id": "4813617", "url": "https://en.wikipedia.org/wiki?curid=4813617", "title": "Alternating permutation", "text": "Alternating permutation\n\nIn combinatorial mathematics, an alternating permutation (or zigzag permutation) of the set {1, 2, 3, ..., \"n\"} is an arrangement of those numbers so that each entry is alternately greater or less than the preceding entry. For example, the five alternating permutations of {1, 2, 3, 4} are:\nThis type of permutation was first studied by Désiré André in the 19th century.\n\nDifferent authors use the term alternating permutation slightly differently: some require that the second entry in an alternating permutation should be larger than the first (as in the examples above), others require that the alternation should be reversed (so that the second entry is smaller than the first, then the third larger than the second, and so on), while others call both types by the name alternating permutation.\n\nThe determination of the number \"A\" of alternating permutations of the set {1, ..., \"n\"} is called André's problem. The numbers \"A\" are known as Euler numbers, zigzag numbers, or up/down numbers. When \"n\" is even the number \"A\" is known as a secant number, while if \"n\" is odd it is known as a tangent number. These latter names come from the study of the generating function for the sequence.\n\nA permutation is said to be \"alternating\" if its entries alternately rise and descend. Thus, each entry other than the first and the last should be either larger or smaller than both of its neighbors. Some authors use the term alternating to refer only to the \"up-down\" permutations for which , calling the \"down-up\" permutations that satisfy by the name \"reverse alternating\". Other authors reverse this convention, or use the word \"alternating\" to refer to both up-down and down-up permutations.\n\nThere is a simple one-to-one correspondence between the down-up and up-down permutations: replacing each entry with reverses the relative order of the entries.\n\nBy convention, in any naming scheme the unique permutations of length 0 (the permutation of the empty set) and 1 (the permutation consisting of a single entry 1) are taken to be alternating.\n\nThe determination of the number \"A\" of alternating permutations of the set {1, ..., \"n\"} is called \"André's problem\". The numbers \"A\" are variously known as \"Euler numbers\", \"zigzag numbers\", \"up/down numbers\", or by some combinations of these names. The name Euler numbers in particular is sometimes used for a closely related sequence. The first few values of \"A\" are 1, 1, 1, 2, 5, 16, 61, 272, 1385, 7936, 50521, ... .\n\nThese numbers satisfy a simple recurrence, similar to that of the Catalan numbers: by splitting the set of alternating permutations (both down-up and up-down) of the set { 1, 2, 3, ..., \"n\", \"n\" + 1 } according to the position \"k\" of the largest entry , one can show that\n\nfor all . used this recurrence to give a differential equation satisfied by the exponential generating function\n\nfor the sequence . He then solved this equation, establishing that\n\nwhere and are the trigonometric functions secant and tangent. This result is known as \"André's theorem\".\n\nIt follows from André's theorem that the radius of convergence of the series is /2. This allows one to compute the asymptotic expansion\n\nof the sequence .\n\nThe odd-indexed zigzag numbers (i.e., the tangent numbers) are closely related to Bernoulli numbers. The relation is given by the formula\n\nfor \"n\" > 0.\n\nIf \"Z\" denotes the number of permutations of {1, ..., \"n\"} that are either up-down or down-up (or both, for \"n\" < 2) then it follows from the pairing given above that \"Z\" = 2\"A\" for \"n\" ≥ 2. The first few values of \"Z\" are 1, 1, 2, 4, 10, 32, 122, 544, 2770, 15872, 101042, ... .\n\nThe Euler zigzag numbers are related to Entringer numbers, from which the zigzag numbers may be computed. The Entringer numbers can be defined recursively as follows:\nThe \"n\" zigzag number is equal to the Entringer number \"E\"(\"n\", \"n\").\n\nThe numbers \"A\" with even indices are called secant numbers or zig numbers: since the secant function is even and tangent is odd, it follows from André's theorem above that they are the numerators in the Maclaurin series of . The first few values are 1, 1, 5, 61, 1385, 50521, ... .\n\nSecant numbers are related to Euler numbers by the formula \"E\" = (−1)\"A\". (\"E\" = 0 when \"n\" is odd.)\n\nCorrespondingly, the numbers \"A\" with odd indices are called tangent numbers or zag numbers. The first few values are 1, 2, 16, 272, 7936, ... .\n\n\n"}
{"id": "2342451", "url": "https://en.wikipedia.org/wiki?curid=2342451", "title": "Arithmetical set", "text": "Arithmetical set\n\nIn mathematical logic, an arithmetical set (or arithmetic set) is a set of natural numbers that can be defined by a formula of first-order Peano arithmetic. The arithmetical sets are classified by the arithmetical hierarchy.\n\nThe definition can be extended to an arbitrary countable set \"A\" (e.g. the set of n-tuples of integers, the set of rational numbers, the set of formulas in some formal language, etc.) by using Gödel numbers to represent elements of the set and declaring a subset of \"A\" to be arithmetical if the set of corresponding Gödel numbers is arithmetical.\n\nA function formula_1 is called arithmetically definable if the graph of formula_2 is an arithmetical set.\n\nA real number is called arithmetical if the set of all smaller rational numbers is arithmetical. A complex number is called arithmetical if its real and imaginary parts are both arithmetical.\n\nA set \"X\" of natural numbers is arithmetical or arithmetically definable if there is a formula φ(\"n\") in the language of Peano arithmetic such that each number \"n\" is in \"X\" if and only if φ(\"n\") holds in the standard model of arithmetic. Similarly, a \"k\"-ary relation\nformula_3 is arithmetical if there is a formula \nformula_4 such that formula_5 holds for all \"k\"-tuples formula_6 of natural numbers. \n\nA finitary function on the natural numbers is called arithmetical if its graph is an arithmetical binary relation.\n\nA set \"A\" is said to be arithmetical in a set \"B\" if \"A\" is definable by an arithmetical formula which has \"B\" as a set parameter.\n\n\n\nEach arithmetical set has an arithmetical formula which tells whether particular numbers are in the set. An alternative notion of definability allows for a formula that does not tell whether particular numbers are in the set but tells whether the set itself satisfies some arithmetical property. \n\nA set \"Y\" of natural numbers is implicitly arithmetical or implicitly arithmetically definable if it is definable with an arithmetical formula that is able to use \"Y\" as a parameter. That is, if there is a formula formula_7 in the language of Peano arithmetic with no free number variables and a new set parameter \"Z\" and set membership relation formula_8 such that \"Y\" is the unique set \"Z\" such that formula_7 holds.\n\nEvery arithmetical set is implicitly arithmetical; if \"X\" is arithmetically defined by φ(\"n\") then it is implicitly defined by the formula\nNot every implicitly arithmetical set is arithmetical, however. In particular, the truth set of first-order arithmetic is implicitly arithmetical but not arithmetical.\n\n\n"}
{"id": "1938356", "url": "https://en.wikipedia.org/wiki?curid=1938356", "title": "Axial ratio", "text": "Axial ratio\n\nAxial ratio, for any structure or shape with two or more axes, is the ratio of the length (or magnitude) of those axes to each other - the longer axis divided by the shorter.\n\nIn \"chemistry\" or \"materials science\", the axial ratio (symbol P) is used to describe rigid rod-like molecules. It is defined as the length of the rod divided by the rod diameter.\n\nIn \"physics\", the axial ratio describes electromagnetic radiation with elliptical, or circular, polarization. The axial ratio is the ratio of the magnitudes of the major and minor axis defined by the electric field vector.\n\nAny fixed polarization can be described in terms of the shape and orientation of the polarization ellipse, which is defined by two parameters: axial ratio AR and tilt angle formula_1. The axial ratio is the ratio of the lengths of the major and minor axes of the ellipse, and is always greater than or equal to one.\n\nAlternatively, polarization can be represented as a point on the surface of the Poincaré sphere, with formula_2 as the longitude and formula_3 as the latitude, where formula_4. The sign used in the argument of the formula_5 depends on the handedness of the polarization. Positive indicates left hand polarization, while negative indicates right hand polarization, as defined by IEEE.\n\nFor the special case of circular polarization, the axial ratio equals 1 (or 0 dB) and the tilt angle is undefined. For the special case of linear polarization, the axial ratio is infinite.\n\n"}
{"id": "30999724", "url": "https://en.wikipedia.org/wiki?curid=30999724", "title": "Bayesian tool for methylation analysis", "text": "Bayesian tool for methylation analysis\n\nBayesian tool for methylation analysis, also known as BATMAN, is a statistical tool for analysing methylated DNA immunoprecipitation (MeDIP) profiles. It can be applied to large datasets generated using either oligonucleotide arrays (MeDIP-chip) or next-generation sequencing (MeDIP-seq), providing a quantitative estimation of absolute methylation state in a region of interest.\n\nMeDIP (methylated DNA immunoprecipitation) is an experimental technique used to assess DNA methylation levels by using an antibody to isolate methylated DNA sequences. The isolated fragments of DNA are either hybridized to a microarray chip (MeDIP-chip) or sequenced by next-generation sequencing (MeDIP-seq). While this tells you what areas of the genome are methylated, it does not give absolute methylation levels. Imagine two different genomic regions, \"A\" and \"B\". Region \"A\" has six CpGs (DNA methylation in mammalian somatic cells generally occurs at CpG dinucleotides), three of which are methylated. Region \"B\" has three CpGs, all of which are methylated. As the antibody simply recognizes methylated DNA, it will bind both these regions equally and subsequent steps will therefore show equal signals for these two regions. This does not give the full picture of methylation in these two regions (in region \"A\" only half the CpGs are methylated, whereas in region \"B\" all the CpGs are methylated). Therefore, to get the full picture of methylation for a given region you have to normalize the signal you get from the MeDIP experiment to the number of CpGs in the region, and this is what the Batman algorithm does. Analysing the MeDIP signal of the above example would give Batman scores of 0.5 for region \"A\" (i.e. the region is 50% methylated) and 1 for region \"B\" (i.e. The region is 100% methylated). In this way Batman converts the signals from MeDIP experiments to absolute methylation levels.\n\nThe core principle of the Batman algorithm is to model the effects of varying density of CpG dinucleotides, and the effect this has on MeDIP enrichment of DNA fragments.\nThe basic assumptions of Batman:\n\nBasic parameters in Batman:\nBased on these assumptions, the signal from the MeDIP channel of the MeDIP-chip or MeDIP-seq experiment depends on the degree of enrichment of DNA fragments overlapping that probe, which in turn depends on the amount of antibody binding, and thus to the number of methylated CpGs on those fragments. In Batman model, the complete dataset from a MeDIP/chip experiment, A, can be represented by a statistical model in the form of the following probability distribution:\n\nwhere formula_2(\"x\"|\"μ\", \"σ\") is a Gaussian probability density function. Standard Bayesian techniques can be used to infer \"f\"(\"m\"|\"A\"), that is, the distribution of likely methylation states given one or more sets of MeDIP-chip/MeDIP-seq outputs. To solve this inference problem, Batman uses nested sampling (http://www.inference.phy.cam.ac.uk/bayesys/) to generate 100 independent samples from \"f\"(\"m\"|\"A\") for each tiled region of the genome, then summarizes the most likely methylation state in 100-bp windows by fitting beta distributions to these samples. The modes of the most likely beta distributions were used as final methylation calls.\n\nBatman prerequisites:\n\nRun Batman:\n\nVisualization of Batman Data:\nMore details related to Batman procedure can be found in Batman manual freely online from https://web.archive.org/web/20110304143135/http://td-blade.gurdon.cam.ac.uk/software/batman/batmanual-alpha-0.2.3.pdf\n\nIt may be useful to take the following points into account when considering using Batman:\n\n"}
{"id": "4734", "url": "https://en.wikipedia.org/wiki?curid=4734", "title": "Bernoulli's inequality", "text": "Bernoulli's inequality\n\nIn real analysis, Bernoulli's inequality (named after Jacob Bernoulli) is an inequality that approximates exponentiations of 1 + \"x\".\n\nThe inequality states that\n\nfor every integer \"r\" ≥ 0 and every real number \"x\" ≥ −1. \nIf the exponent \"r\" is even, then the inequality is valid for \"all\" real numbers \"x\". The strict version of the inequality reads\n\nfor every integer \"r\" ≥ 2 and every real number \"x\" ≥ −1 with \"x\" ≠ 0.\n\nThere is also a generalized version that says for every real number r ≥ 1 and real number x ≥ -1,\n\nwhile for 0 ≤ \"r\" ≤ 1 and real number x ≥ -1,\n\nBernoulli's inequality is often used as the crucial step in the proof of other inequalities. It can itself be proved using mathematical induction, as shown below.\n\nJacob Bernoulli first published the inequality in his treatise “Positiones Arithmeticae de Seriebus Infinitis” (Basel, 1689), where he used the inequality often.\n\nAccording to Joseph E. Hofmann, Über die Exercitatio Geometrica des M. A. Ricci (1963), p. 177, the inequality is actually due to Sluse in his Mesolabum (1668 edition), Chapter IV \"De maximis & minimis\".\n\nFor \"r\" = 0,\n\nis equivalent to 1 ≥ 1 which is true as required.\n\nNow suppose the statement is true for \"r\" = \"k\":\n\nThen it follows that\n\nBy induction we conclude the statement is true for all \"r\" ≥ 0.\n\nThe exponent \"r\" can be generalized to an arbitrary real number as follows: if \"x\" > −1, then\n\nfor \"r\" ≤ 0 or \"r\" ≥ 1, and\n\nfor 0 ≤ \"r\" ≤ 1.\n\nThis generalization can be proved by comparing derivatives.\nAgain, the strict versions of these inequalities require \"x\" ≠ 0 and \"r\" ≠ 0, 1.\n\nThe following inequality estimates the \"r\"-th power of 1 + \"x\" from the other side. For any real numbers \"x\", \"r\"  with \"r\" > 0, one has\n\nwhere \"e\" = 2.718... This may be proved using the inequality (1 + 1/\"k\") < \"e\".\n\nAn alternative form of Bernoulli's inequality for formula_11 and formula_12 is:\n\nThis can be proved (for integer t) by using the formula for geometric series: (using y=1-x)\n\nor equivalently formula_15\n\nUsing AM-GM \n\nAn elementary proof for formula_16 can be given using Weighted AM-GM. \n\nLet formula_17 be two non-negative real constants. By Weighted AM-GM on formula_18 with weights formula_17 respectively, we get\n\nformula_20\n\nNote that \n\nformula_21\n\nand\n\nformula_22\n\nso our inequality is equivalent to\n\nformula_23\n\nAfter substituting formula_24 (bearing in mind that this implies formula_16) our inequality turns into \n\nformula_26 which is Bernoulli's inequality.\n\nUsing the formula for geometric series\n\nBernoulli's inequality \n\nis equal to\nand by the formula for geometric series (using y=1+x) we get\nwhich leads to\nNow if formula_27 then by monotony of the powers each summand formula_28, therefore their sum is greater formula_29 and hence the product on the LHS of ().\n\nIf formula_30 then by the same arguments formula_31 and thus\nall addends formula_32 are non-positive and hence their sum. Since the product of two non-positive numbers is non-negative, we get again\n(), which proofs Bernoulli's inequality even for formula_33.\nUsing Binomial theorem\n\n(1) For , formula_34\nObviously, formula_35\n\nThus, formula_36\n\n(2) For , formula_37\n\n(3) For , let , then \n\nReplace with , we have formula_38\n\nAlso, according to the binomial theorem, formula_39\n\nthenformula_40\n\nNotice that formula_41\n\nTherefore, we can see that each binomial term formula_42 is multiplied by a factor formula_43 , and that will make each term smaller than the term before.\n\nFor that reason, formula_44\n\nHence, formula_45\n\nReplace with back, we get formula_36\n\nNotice that by using binomial theorem, we can only prove the cases when r is a positive integer or zero.\n\n\n"}
{"id": "2546344", "url": "https://en.wikipedia.org/wiki?curid=2546344", "title": "Blood volume", "text": "Blood volume\n\nBlood volume is the volume of blood (both red blood cells and plasma) in the circulatory system of any individual.\n\nA typical adult has a blood volume of approximately 5 liters, with females and males having approximately the same blood volume. Blood volume is regulated by the kidneys.\n\nBlood volume (BV) can be calculated given the hematocrit (HC; the fraction of blood that is red blood cells) and plasma volume (PV), with the hematocrit being regulated via the blood oxygen content regulator:\n\nBlood volume measurement may be used in people with congestive heart failure, chronic hypertension, renal failure and critical care.\n\nThe use of relative blood volume changes during dialysis is of questionable utility.\n\nTotal Blood Volume can be measured manually via the Dual Isotope or Dual Tracer Technique, a classic technique, available since the 1950s. This technique requires double labeling of the blood; that is 2 injections and 2 standards (51Cr-RBC for tagging red blood cells and I-HAS for tagging plasma volume)as well as withdrawing and re-infusing patients with their own blood for blood volume analysis results. This method may take up to 6 hours for accurate results.\n\nBlood Volume may also be measured semi-automatically. The BVA-100, a product of Daxor Corporation, consists of an automated well counter interfaced with a computer. It is able to report Total Blood Volume (TBV), Plasma Volume (PV) and Red Cell Volume (RCV) using the indicator dilution principle, microhematocrit centrifugation and the Ideal Height and Weight Method. The indicator or tracer, is an I-131 albumin injection. An equal amount of the tracer is injected into a known and unknown volume. Clinically, the unknown volume is the patient's blood volume, with the tracer having been injected into the patient's blood stream and tagged to the blood plasma. Once the tracer is injected a technician takes five blood samples which undergo microhematocrit centrifugation to extrapolate true blood volume at time 0. The concentration of the I-131 in the blood is determined from the blood radioactivity against the standard, which has a known I-131 dilution in a known volume. The unknown volume is inversely proportional to the concentration of the indicator in the known volume; the larger the unknown volume, the lower the tracer concentration, thus the unknown volume can be calculated. The microhematocrit data along with the I-131 indicator data provide a normalized hematocrit number, more accurate than hematocrit or peripheral hematocrit measurements. Measurements are taken 5 times in 6 minute intervals so that the BVA-100 can calculate the albumin transudation time to understand the flux of liquid through capillary membranes.\n\nThe table at right shows circulating blood volumes, given as volume per kilogram, for healthy adults and some animals. However, it can be 15% less in obese and old animals.\n\n"}
{"id": "37598916", "url": "https://en.wikipedia.org/wiki?curid=37598916", "title": "Bochner–Kodaira–Nakano identity", "text": "Bochner–Kodaira–Nakano identity\n\nIn mathematics, the Bochner–Kodaira–Nakano identity is an analogue of the Weitzenböck identity for hermitian manifolds, giving an expression for the antiholomorphic Laplacian of a vector bundle over a hermitian manifold in terms of its complex conjugate and the curvature of the bundle and the torsion of the metric of the manifold. It is named after Salomon Bochner, Kunihiko Kodaira, and Shigeo Nakano.\n\n"}
{"id": "4110341", "url": "https://en.wikipedia.org/wiki?curid=4110341", "title": "Cartan model", "text": "Cartan model\n\nIn mathematics, the Cartan model is a differential graded algebra that computes the equivariant cohomology of a space.\n\n"}
{"id": "44307291", "url": "https://en.wikipedia.org/wiki?curid=44307291", "title": "Chemical reaction model", "text": "Chemical reaction model\n\nChemical reaction models transform physical knowledge into a mathematical formulation that can be utilized in computational simulation of practical problems in chemical engineering. Computer simulation provides the flexibility to study chemical processes under a wide range of conditions. Modeling of a chemical reaction involves solving conservation equations describing convection, diffusion, and reaction source for each component species.\n\n\"R\" is the net rate of production of species \"i\" by chemical reaction and \"S\" is the rate of creation by addition from the dispersed phase and the user defined source. \"J\" is the diffusion flux of species \"i\", which arises due to concentration gradients and differs in both laminar and turbulent flows. In turbulent flows, computational fluid dynamics also considers the effects of turbulent diffusivity. The net source of chemical species \"i\" due to reaction, \"R\" which appeared as the source term in the species transport equation is computed as the sum of the reaction sources over the \"N\" reactions among the species.\n\nThese reaction rates \"R\" can be calculated by following models:\n\n\nThe laminar finite rate model computes the chemical source terms using the Arrhenius expressions and ignores turbulence fluctuations. This model provides with the exact solution for laminar flames but gives inaccurate solution for turbulent flames, in which turbulence highly affects the chemistry reaction rates, due to highly non-linear Arrhenius chemical kinetics. However this model may be accurate for combustion with small turbulence fluctuations, for example supersonic flames.\n\nThe eddy dissipation model, based on the work of Magnussen and Hjertager, is a turbulent-chemistry reaction model. Most fuels are fast burning and the overall rate of reaction is controlled by turbulence mixing. In the non-premixed flames, turbulence slowly mixes the fuel and oxidizer into the reaction zones where they burn quickly. In premixed flames the turbulence slowly mixes cold reactants and hot products into the reaction zones where reaction occurs rapidly. In such cases the combustion is said to be mixing-limited, and the complex and often unknown chemical kinetics can be safely neglected. In this model, the chemical reaction is governed by large eddy mixing time scale. Combustion initiates whenever there is turbulence present in the flow. It does not need an ignition source to initiate the combustion. This type of model is valid for the non-premixed combustion, but for the premixed flames the reactant is assumed to burn at the moment it enters the computation model, which is a shortcoming of this model as in practice the reactant needs some time to get to the ignition temperature to initiate the combustion.\n\nThe eddy dissipation concept (EDC) model is an extension of the eddy dissipation model to include detailed chemical mechanism in turbulent flows. The EDC model attempts to incorporate the significance of fine structures in a turbulent reacting flow in which combustion is important. EDC has been proven efficient without the need for changing the constants for a great variety of premixed and diffusion controlled combustion problems, both where the chemical kinetics is faster than the overall fine structure mixing as well as in cases where the chemical kinetics has a dominating influence.\n\n"}
{"id": "46449014", "url": "https://en.wikipedia.org/wiki?curid=46449014", "title": "Circumcenter of mass", "text": "Circumcenter of mass\n\nIn geometry, the circumcenter of mass is a center associated with a polygon which shares many of the properties of the center of mass. More generally, the circumcenter of mass may be defined for simplicial polytopes and also in the spherical and hyperbolic geometries.\n\nIn the special case when the polytope is a quadrilateral or hexagon, the circumcenter of mass has been called the \"quasicircumcenter\" and has been used to define an Euler line of a quadrilateral. The circumcenter of mass allows us to define an Euler line for simplicial polytopes.\n\nLet formula_1 be an oriented polygon (with vertices counted countercyclically) in the plane with vertices formula_2 and let formula_3 be an arbitrary point not lying on the sides (or their extensions). Consider the triangulation of formula_1 by the oriented triangles formula_5 (the index formula_6 is viewed modulo formula_7). Associate with each of these triangles its circumcenter formula_8 with weight equal to its oriented area (positive if its sequence of vertices is countercyclical; negative otherwise). The circumcenter of mass of formula_1 is the center of mass of these weighted circumcenters. The result is independent of the choice of point formula_3.\nIn the special case when the polygon is cyclic, the circumcenter of mass coincides with the circumcenter.\n\nThe circumcenter of mass satisfies an analog of Archimedes' Lemma, which states that if a polygon is decomposed into two smaller polygons, then the circumcenter of mass of that polygon is a weighted sum of the circumcenters of mass of the two smaller polygons. As a consequence, any triangulation with nondegenerate triangles may be used to define the circumcenter of mass.\n\nFor an equilateral polygon, the circumcenter of mass and center of mass coincide. More generally, the circumcenter of mass and center of mass coincide for a simplicial polytope for which each face has the sum of squares of its edges a constant.\n\nThe circumcenter of mass is invariant under the operation of \"recutting\" of polygons. and the discrete bicycle (Darboux) transformation; in other words, the image of a polygon under these operations has the same circumcenter of mass as the original polygon. The generalized Euler line makes other appearances in the theory of integrable systems.\n\nLet formula_11 be the vertices of formula_1 and let formula_13 denote its area. The circumcenter of mass formula_14 of the polygon formula_1 is given by the formula\n\nThe circumcenter of mass can be extended to smooth curves via a limiting procedure. This continuous limit coincides with the center of mass of the homogeneous lamina bounded by the curve.\n\nUnder natural assumptions, the centers of polygons which satisfy Archimedes' Lemma are precisely the points of its Euler line. In other words, the only \"well-behaved\" centers which satisfy Archimedes' Lemma are the affine combinations of the circumcenter of mass and center of mass.\n\nThe circumcenter of mass allows an Euler line to be defined for any polygon (and more generally, for a simplicial polytope). This generalized Euler line is defined as the affine span of the center of mass and circumcenter of mass of the polytope.\n\n"}
{"id": "3625890", "url": "https://en.wikipedia.org/wiki?curid=3625890", "title": "Claw-free permutation", "text": "Claw-free permutation\n\nIn mathematical and computer science field of cryptography, a group of three numbers (\"x\",\"y\",\"z\") is said to be a claw of two permutations \"f\" and \"f\" if\n\nA pair of permutations \"f\" and \"f\" are said to be claw-free if there is no efficient algorithm for computing a claw.\n\nThe terminology \"claw free\" was introduced by Goldwasser, Micali, and Rivest in their 1984 paper, \"A Paradoxical Solution to the Signature Problem\" (and later in a more complete journal paper), where they showed that the existence of claw-free pairs of trapdoor permutations implies the existence of digital signature schemes secure against adaptive chosen-message attack. This construction was later superseded by the construction of digital signatures from any one-way trapdoor permutation. The existence of trapdoor permutations does not by itself imply claw-free permutations exist; however, it has been shown that claw-free permutations do exist if factoring is hard.\n\nThe general notion of claw-free permutation (not necessarily trapdoor) was further studied by Ivan Damgård in his PhD thesis \"The Application of Claw Free Functions in Cryptography\" (Aarhus University, 1988), where he showed how to construct \nCollision Resistant Hash Functions from claw-free permutations. The notion of claw-freeness is closely related to that of collision resistance in hash functions. The distinction is that claw-free permutations are \"pairs\" of functions in which it is hard to create a collision between them, while a collision-resistant hash function is a single function in which it's hard to find a collision, i.e. a function \"H\" is collision resistant if it's hard to find a pair of distinct values \"x\",\"y\" such that\n\nIn the hash function literature, this is commonly termed a hash collision. A hash function where collisions are difficult to find is said to have collision resistance.\n\nGiven a pair of claw-free permutations \"f\" and \"f\" it is straightforward to create a commitment scheme. To commit to a bit \"b\" the sender chooses a random \"x\", and calculates \"f\"(\"x\"). Since both \"f\" and \"f\" share the same domain (and range), the bit \"b\" is statistically hidden from the receiver. To open the commitment, the sender simply sends the randomness \"x\" to the receiver. The sender is bound to his bit because opening a commitment to 1 − \"b\" is exactly equivalent to finding a claw. Notice that like the construction of Collision Resistant Hash functions, this construction does not require that the claw-free functions have a trapdoor.\n"}
{"id": "19959657", "url": "https://en.wikipedia.org/wiki?curid=19959657", "title": "Closeness centrality", "text": "Closeness centrality\n\nIn a connected graph, closeness centrality (or closeness) of a node is a measure of centrality in a network, calculated as the reciprocal of the sum of the length of the shortest paths between the node and all other nodes in the graph. Thus, the more central a node is, the \"closer\" it is to all other nodes.\n\nCloseness was defined by Bavelas (1950) as the reciprocal of the farness, that is:\n\nwhere formula_2 is the distance between vertices formula_3 and formula_4. When speaking of closeness centrality, people usually refer to its normalized form which represents the average length of the shortest paths instead of their sum. It is generally given by the previous formula multiplied by formula_5, where formula_6 is the number of nodes in the graph. For large graphs this difference becomes inconsequential so the formula_7 is dropped resulting in:\n\nThis adjustment allows comparisons between nodes of graphs of different sizes.\n\nTaking distances \"from\" or \"to\" all other nodes is irrelevant in undirected graphs, whereas it can produce totally different results in directed graphs (e.g. a website can have a high closeness centrality from outgoing link, but low closeness centrality from incoming links).\n\nWhen a graph is not strongly connected, a widespread idea is that of using the sum of reciprocal of distances, instead of the reciprocal of the sum of distances, with the convention formula_9:\n\nThe most natural modification of Bavelas's definition of closeness is following the general principle proposed by Marchiori and Latora (2000) that in graphs with infinite distances the harmonic mean behaves better than the arithmetic mean. Indeed, Bavelas's closeness can be described as the denormalized reciprocal of the arithmetic mean of distances, whereas harmonic centrality is the denormalized reciprocal of the harmonic mean of distances.\n\nThis idea was explicitly stated for undirected graphs under the name valued centrality by Dekker (2005) and under the name harmonic centrality by Rochat (2009), axiomatized by Garg (2009) and proposed once again later by Opsahl (2010). It was studied on general directed graphs by Boldi and Vigna (2014). This idea is also quite similar to market potential proposed in Harris (1954) which now often goes by the term market access.\n\nDangalchev (2006), in a work on network vulnerability proposes for undirected graphs a different definition:\n\nThis definition is used effectively for disconnected graphs and allows to create convenient formulae for graph operations. For example:\n\nIf a graph formula_12 is created by linking node formula_13 of graph formula_14 to node formula_15 of graph formula_16 then the combined closeness is:\n\nIf graph formula_18 is the thorn graph of graph formula_19, which has formula_20 nodes, then formula_18 closeness is :\n\nThe natural generalization of this definition is \n\nwhere formula_24 belongs to (0,1). As formula_24 increases from 0 to 1, the generalized closeness changes from local characteristic (degree) to global (number of connected nodes).\n\nThe \"information centrality\" of Stephenson and Zelen (1989) is another closeness measure, which computes the harmonic mean of the resistance distances towards a vertex \"x\", which is smaller if \"x\" has many paths of small resistance connecting it to other vertices.\n\nIn the classic definition of the closeness centrality, the spread of information is modeled by the use of shortest paths. This model might not be the most realistic for all types of communication scenarios. Thus, related definitions have been discussed to measure closeness, like the random walk closeness centrality introduced by Noh and Rieger (2004). It measures the speed with which randomly walking messages reach a vertex from elsewhere in the graph—a sort of random-walk version of closeness centrality. Hierarchical closeness of Tran and Kwon (2014) is an extended closeness centrality to deal still in another way with the limitation of closeness in graphs that are not strongly connected. The hierarchical closeness explicitly includes information about the range of other nodes that can be affected by the given node.\n\n"}
{"id": "522958", "url": "https://en.wikipedia.org/wiki?curid=522958", "title": "Deterministic system", "text": "Deterministic system\n\nIn mathematics, computer science and physics, a deterministic system is a system in which no randomness is involved in the development of future states of the system. A deterministic model will thus always produce the same output from a given starting condition or initial state.\n\nPhysical laws that are described by differential equations represent deterministic systems, even though the state of the system at a given point in time may be difficult to describe explicitly.\n\nIn quantum mechanics, the Schrödinger equation, which describes the continuous time evolution of a system's wave function, is deterministic. However, the relationship between a system's wave function and the observable properties of the system appears to be non-deterministic.\n\nThe systems studied in chaos theory are deterministic. If the initial state were known exactly, then the future state of such a system could theoretically be predicted. However, in practice, knowledge about the future state is limited by the precision with which the initial state can be measured, and chaotic systems are characterized by a strong dependence on the initial conditions. This sensitivity to initial conditions can be measured with Lyapunov exponents.\n\nMarkov chains and other random walks are not deterministic systems, because their development depends on random choices.\n\nA deterministic model of computation, for example a deterministic Turing machine, is a model of computation such that the successive states of the machine and the operations to be performed are completely determined by the preceding state.\n\nA deterministic algorithm is an algorithm which, given a particular input, will always produce the same output, with the underlying machine always passing through the same sequence of states. There may be non-deterministic algorithms that run on a deterministic machine, for example, an algorithm that relies on random choices. Generally, for such random choices, one uses a pseudorandom number generator, but one may also use some external physical process, such as the last digits of the time given by the computer clock.\n\nA \"pseudorandom number generator\" is a deterministic algorithm, that is designed to produce sequences of numbers that behave as random sequences. A hardware random number generator, however, may be non-deterministic.\n\nIn economics, the Ramsey–Cass–Koopmans model is deterministic. The stochastic equivalent is known as Real Business Cycle theory.\n\n"}
{"id": "25782572", "url": "https://en.wikipedia.org/wiki?curid=25782572", "title": "Dietrich Mahnke", "text": "Dietrich Mahnke\n\nDietrich Mahnke (October 17, 1884, Verden – July 25, 1939, Fürth) was a German philosopher and historian of mathematics.\n\nFrom 1902–1906, Mahnke studied at Göttingen under Edmund Husserl and David Hilbert. After serving in the First World War (stationed in Lens, France), he graduated from the University of Freiburg in 1925 with a thesis on Leibniz. The thesis was later published in the Jahrbuch für Philosophie und phänomenologische Forschung as \"Leibnizens Synthese von Universalmathematik und Individualmetaphysik\". In 1926 he habilitated at Greifswald with a thesis entitled \"Neue Einblicke in die Entdeckungsgeschichte der höheren Analysis\". In 1927 he became a professor of philosophy at Marburg.\n\nIn 1934 he became a member of the Nazi SA.\n\nMahnke's work in the history of mathematics focussed primarily on Leibniz's development of the infinitesimal calculus, and his relationship to Neo-Platonism. His last book, \"Unendliche Sphäre und Allmittelpunkt, Beiträge zur Genealogie der mathematischen Mystik\" was a study of the use of mathematical symbolism, especially the notion of \"infinite spheres\", in religious mysticism. At the time of his death, Mahnke was editing a volume of Leibniz's mathematical correspondence. This project was then taken over by Joseph Ehrenfried Hofmann.\n\nMahnke was killed in a car accident.\n\nHis \"Nachlass\" is preserved at the University of Marburg.\n\n\"Leibniz als Gegner der Gelehrteneinseitigkeit\" (1912)\n\"Der Wille der Ewigkeit\" (1917) \n\"Eine Neue Monadologie\" (1917) \n\"Die Neubelebung der Leibnizschen Weltanschauung\" (1920)\n\"Ewigkeit und Gegenwart, Eine Fichtische Zusammenschau\" (1922)\n\"Von Hilbert zu Husserl, Erste Einführung in die Phänomenologie, besonders die formale Mathematik\" (1923)\n\"Leibniz und Goethe: die Harmonie ihrer Weltansichten\" (1924)\n\"Neue Einblicke in die Entdeckungsgeschichte der höheren Analysis\" (1926) \n\"Ein unbekanntes Selbstzeugnis Leibnizens aus seiner Erziehertätigkeit\" (1931)\n\"Unendliche Sphäre und Allmittelpunkt, Beiträge zur Genealogie der mathematischen Mystik\" (1937)\n\"Die Rationalisierung der Mystik bei Leibniz und Kant\" (1939)\n\n"}
{"id": "23997203", "url": "https://en.wikipedia.org/wiki?curid=23997203", "title": "Differential of a function", "text": "Differential of a function\n\nIn calculus, the differential represents the principal part of the change in a function \"y\" = \"f\"(\"x\") with respect to changes in the independent variable. The differential \"dy\" is defined by\nwhere formula_2 is the derivative of \"f\" with respect to \"x\", and \"dx\" is an additional real variable (so that \"dy\" is a function of \"x\" and \"dx\"). The notation is such that the equation\n\nholds, where the derivative is represented in the Leibniz notation \"dy\"/\"dx\", and this is consistent with regarding the derivative as the quotient of the differentials. One also writes\n\nThe precise meaning of the variables \"dy\" and \"dx\" depends on the context of the application and the required level of mathematical rigor. The domain of these variables may take on a particular geometrical significance if the differential is regarded as a particular differential form, or analytical significance if the differential is regarded as a linear approximation to the increment of a function. Traditionally, the variables \"dx\" and \"dy\" are considered to be very small (infinitesimal), and this interpretation is made rigorous in non-standard analysis.\n\nThe differential was first introduced via an intuitive or heuristic definition by Gottfried Wilhelm Leibniz, who thought of the differential \"dy\" as an infinitely small (or infinitesimal) change in the value \"y\" of the function, corresponding to an infinitely small change \"dx\" in the function's argument \"x\". For that reason, the instantaneous rate of change of \"y\" with respect to \"x\", which is the value of the derivative of the function, is denoted by the fraction\n\nin what is called the Leibniz notation for derivatives. The quotient \"dy\"/\"dx\" is not infinitely small; rather it is a real number.\n\nThe use of infinitesimals in this form was widely criticized, for instance by the famous pamphlet The Analyst by Bishop Berkeley. Augustin-Louis Cauchy (1823) defined the differential without appeal to the atomism of Leibniz's infinitesimals. Instead, Cauchy, following d'Alembert, inverted the logical order of Leibniz and his successors: the derivative itself became the fundamental object, defined as a limit of difference quotients, and the differentials were then defined in terms of it. That is, one was free to \"define\" the differential \"dy\" by an expression\nin which \"dy\" and \"dx\" are simply new variables taking finite real values, not fixed infinitesimals as they had been for Leibniz.\n\nAccording to , Cauchy's approach was a significant logical improvement over the infinitesimal approach of Leibniz because, instead of invoking the metaphysical notion of infinitesimals, the quantities \"dy\" and \"dx\" could now be manipulated in exactly the same manner as any other real quantities\nin a meaningful way. Cauchy's overall conceptual approach to differentials remains the standard one in modern analytical treatments, although the final word on rigor, a fully modern notion of the limit, was ultimately due to Karl Weierstrass.\n\nIn physical treatments, such as those applied to the theory of thermodynamics, the infinitesimal view still prevails. reconcile the physical use of infinitesimal differentials with the mathematical impossibility of them as follows. The differentials represent finite non-zero values that are smaller than the degree of accuracy required for the particular purpose for which they are intended. Thus \"physical infinitesimals\" need not appeal to a corresponding mathematical infinitesimal in order to have a precise sense.\n\nFollowing twentieth-century developments in mathematical analysis and differential geometry, it became clear that the notion of the differential of a function could be extended in a variety of ways. In real analysis, it is more desirable to deal directly with the differential as the principal part of the increment of a function. This leads directly to the notion that the differential of a function at a point is a linear functional of an increment Δ\"x\". This approach allows the differential (as a linear map) to be developed for a variety of more sophisticated spaces, ultimately giving rise to such notions as the Fréchet or Gâteaux derivative. Likewise, in differential geometry, the differential of a function at a point is a linear function of a tangent vector (an \"infinitely small displacement\"), which exhibits it as a kind of one-form: the exterior derivative of the function. In non-standard calculus, differentials are regarded as infinitesimals, which can themselves be put on a rigorous footing (see differential (infinitesimal)).\n\nThe differential is defined in modern treatments of differential calculus as follows. The differential of a function \"f\"(\"x\") of a single real variable \"x\" is the function \"df\" of two independent real variables \"x\" and \"Δx\" given by\n\nOne or both of the arguments may be suppressed, i.e., one may see \"df\"(\"x\") or simply \"df\". If \"y\" = \"f\"(\"x\"), the differential may also be written as \"dy\". Since \"dx\"(\"x\", Δ\"x\") = Δ\"x\" it is conventional to write \"dx\" = Δ\"x\", so that the following equality holds:\n\nThis notion of differential is broadly applicable when a linear approximation to a function is sought, in which the value of the increment Δ\"x\" is small enough. More precisely, if \"f\" is a differentiable function at \"x\", then the difference in \"y\"-values\n\nsatisfies\n\nwhere the error ε in the approximation satisfies ε/Δ\"x\" → 0 as Δ\"x\" → 0. In other words, one has the approximate identity\n\nin which the error can be made as small as desired relative to Δ\"x\" by constraining \"Δx\" to be sufficiently small; that is to say,\nas Δ\"x\" → 0. For this reason, the differential of a function is known as the principal (linear) part in the increment of a function: the differential is a linear function of the increment Δ\"x\", and although the error ε may be nonlinear, it tends to zero rapidly as Δ\"x\" tends to zero.\n\nFollowing , for functions of more than one independent variable,\n\nthe partial differential of \"y\" with respect to any one of the variables \"x\" is the principal part of the change in \"y\" resulting from a change \"dx\" in that one variable. The partial differential is therefore\n\ninvolving the partial derivative of \"y\" with respect to \"x\". The sum of the partial differentials with respect to all of the independent variables is the total differential\n\nwhich is the principal part of the change in \"y\" resulting from changes in the independent variables \"x\".\n\nMore precisely, in the context of multivariable calculus, following , if \"f\" is a differentiable function, then by the definition of the differentiability, the increment\n\nwhere the error terms ε tend to zero as the increments Δ\"x\" jointly tend to zero. The total differential is then rigorously defined as\n\nSince, with this definition,\none has\n\nAs in the case of one variable, the approximate identity holds\n\nin which the total error can be made as small as desired relative to formula_21 by confining attention to sufficiently small increments.\n\nIn measurement, the total differential is used in estimating the error Δ\"f\" of a function \"f\" based on the errors Δ\"x\", Δ\"y\", ... of the parameters \"x, y, ...\". Assuming that the interval is short enough for the change to be approximately linear:\n\nand that all variables are independent, then for all variables,\n\nThis is because the derivative \"f\" with respect to the particular parameter \"x\" gives the sensitivity of the function \"f\" to a change in \"x\", in particular the error Δ\"x\". As they are assumed to be independent, the analysis describes the worst-case scenario. The absolute values of the component errors are used, because after simple computation, the derivative may have a negative sign. From this principle the error rules of summation, multiplication etc. are derived, e.g.:\n\nThat is to say, in multiplication, the total relative error is the sum of the relative errors of the parameters.\n\nTo illustrate how this depends on the function considered, consider the case where the function is \"f(a, b) = a ln b\" instead. Then, it can be computed that the error estimate is\nwith an extra 'ln \"b\"' factor not found in the case of a simple product. This additional factor tends to make the error smaller, as ln \"b\" is not as large as a bare \"b\".\n\nHigher-order differentials of a function \"y\" = \"f\"(\"x\") of a single variable \"x\" can be defined via:\nand, in general,\nInformally, this justifies Leibniz's notation for higher-order derivatives\nWhen the independent variable \"x\" itself is permitted to depend on other variables, then the expression becomes more complicated, as it must include also higher order differentials in \"x\" itself. Thus, for instance,\nand so forth.\n\nSimilar considerations apply to defining higher order differentials of functions of several variables. For example, if \"f\" is a function of two variables \"x\" and \"y\", then\nwhere formula_28 is a binomial coefficient. In more variables, an analogous expression holds, but with an appropriate multinomial expansion rather than binomial expansion.\n\nHigher order differentials in several variables also become more complicated when the independent variables are themselves allowed to depend on other variables. For instance, for a function \"f\" of \"x\" and \"y\" which are allowed to depend on auxiliary variables, one has\n\nBecause of this notational infelicity, the use of higher order differentials was roundly criticized by , who concluded:\n\nThat is: \"Finally, what is meant, or represented, by the equality [...]? In my opinion, nothing at all.\" In spite of this skepticism, higher order differentials did emerge as an important tool in analysis\n\nIn these contexts, the \"n\"th order differential of the function \"f\" applied to an increment Δ\"x\" is defined by\nor an equivalent expression, such as\nwhere formula_33 is an \"n\"th forward difference with increment \"t\"Δ\"x\".\n\nThis definition makes sense as well if \"f\" is a function of several variables (for simplicity taken here as a vector argument). Then the \"n\"th differential defined in this way is a homogeneous function of degree \"n\" in the vector increment Δ\"x\". Furthermore, the Taylor series of \"f\" at the point \"x\" is given by\nThe higher order Gâteaux derivative generalizes these considerations to infinite dimensional spaces.\n\nA number of properties of the differential follow in a straightforward manner from the corresponding properties of the derivative, partial derivative, and total derivative. These include:\n\n\nAn operation \"d\" with these two properties is known in abstract algebra as a derivation. They imply the Power rule\nIn addition, various forms of the chain rule hold, in increasing level of generality:\n\n\n\n\nA consistent notion of differential can be developed for a function \"f\" : R → R between two Euclidean spaces. Let x,Δx ∈ R be a pair of Euclidean vectors. The increment in the function \"f\" is\nIf there exists an \"m\" × \"n\" matrix \"A\" such that\nin which the vector ε → 0 as Δx → 0, then \"f\" is by definition differentiable at the point x. The matrix \"A\" is sometimes known as the Jacobian matrix, and the linear transformation that associates to the increment Δx ∈ R the vector \"A\"Δx ∈ R is, in this general setting, known as the differential \"df\"(\"x\") of \"f\" at the point \"x\". This is precisely the Fréchet derivative, and the same construction can be made to work for a function between any Banach spaces.\n\nAnother fruitful point of view is to define the differential directly as a kind of directional derivative:\n\nwhich is the approach already taken for defining higher order differentials (and is most nearly the definition set forth by Cauchy). If \"t\" represents time and x position, then h represents a velocity instead of a displacement as we have heretofore regarded it. This yields yet another refinement of the notion of differential: that it should be a linear function of a kinematic velocity. The set of all velocities through a given point of space is known as the tangent space, and so \"df\" gives a linear function on the tangent space: a differential form. With this interpretation, the differential of \"f\" is known as the exterior derivative, and has broad application in differential geometry because the notion of velocities and the tangent space makes sense on any differentiable manifold. If, in addition, the output value of \"f\" also represents a position (in a Euclidean space), then a dimensional analysis confirms that the output value of \"df\" must be a velocity. If one treats the differential in this manner, then it is known as the pushforward since it \"pushes\" velocities from a source space into velocities in a target space.\n\nAlthough the notion of having an infinitesimal increment \"dx\" is not well-defined in modern mathematical analysis, a variety of techniques exist for defining the infinitesimal differential so that the differential of a function can be handled in a manner that does not clash with the Leibniz notation. These include:\n\n\nDifferentials may be effectively used in numerical analysis to study the propagation of experimental errors in a calculation, and thus the overall numerical stability of a problem . Suppose that the variable \"x\" represents the outcome of an experiment and \"y\" is the result of a numerical computation applied to \"x\". The question is to what extent errors in the measurement of \"x\" influence the outcome of the computation of \"y\". If the \"x\" is known to within Δ\"x\" of its true value, then Taylor's theorem gives the following estimate on the error Δ\"y\" in the computation of \"y\":\nwhere ξ = \"x\" + θΔ\"x\" for some 0 < θ < 1. If Δ\"x\" is small, then the second order term is negligible, so that Δ\"y\" is, for practical purposes, well-approximated by \"dy\" = \"f\"'(\"x\")Δ\"x\".\n\nThe differential is often useful to rewrite a differential equation\n\nin the form\n\nin particular when one wants to separate the variables.\n\n\n"}
{"id": "32034345", "url": "https://en.wikipedia.org/wiki?curid=32034345", "title": "Double affine Hecke algebra", "text": "Double affine Hecke algebra\n\nIn mathematics, a double affine Hecke algebra, or Cherednik algebra, is an algebra containing the Hecke algebra of an affine Weyl group, given as the quotient of the group ring of a double affine braid group. They were introduced by Cherednik, who used them to prove Macdonald's constant term conjecture for Macdonald polynomials. Infinitesimal Cherednik algebras have significant implications in representation theory, and therefore have important applications in particle physics and in chemistry.\n\n"}
{"id": "3122600", "url": "https://en.wikipedia.org/wiki?curid=3122600", "title": "Eilenberg–Steenrod axioms", "text": "Eilenberg–Steenrod axioms\n\nIn mathematics, specifically in algebraic topology, the Eilenberg–Steenrod axioms are properties that homology theories of topological spaces have in common. The quintessential example of a homology theory satisfying the axioms is singular homology, developed by Samuel Eilenberg and Norman Steenrod.\n\nOne can define a homology theory as a sequence of functors satisfying the Eilenberg–Steenrod axioms. The axiomatic approach, which was developed in 1945, allows one to prove results, such as the Mayer–Vietoris sequence, that are common to all homology theories satisfying the axioms.\n\nIf one omits the dimension axiom (described below), then the remaining axioms define what is called an extraordinary homology theory. Extraordinary cohomology theories first arose in K-theory and cobordism.\n\nThe Eilenberg–Steenrod axioms apply to a sequence of functors formula_1 from the category of pairs (\"X\", \"A\") of topological spaces to the category of abelian groups, together with a natural transformation formula_2 called the boundary map (here \"H\"(\"A\") is a shorthand for \"H\"(\"A\",∅)). The axioms are:\n\n\nIf \"P\" is the one point space then \"H\"(\"P\") is called the coefficient group. For example, singular homology (taken with integer coefficients, as is most common) has as coefficients the integers.\n\nSome facts about homology groups can be derived directly from the axioms, such as the fact that homotopically equivalent spaces have isomorphic homology groups.\n\nThe homology of some relatively simple spaces, such as \"n\"-spheres, can be calculated directly from the axioms. From this it can be easily shown that the (\"n\" − 1)-sphere is not a retract of the \"n\"-disk. This is used in a proof of the Brouwer fixed point theorem.\n\nA \"homology-like\" theory satisfying all of the Eilenberg–Steenrod axioms except the dimension axiom is called an extraordinary homology theory (dually, extraordinary cohomology theory). Important examples of these were found in the 1950s, such as topological K-theory and cobordism theory, which are extraordinary \"co\"homology theories, and come with homology theories dual to them.\n\n\n"}
{"id": "33893714", "url": "https://en.wikipedia.org/wiki?curid=33893714", "title": "Esscher transform", "text": "Esscher transform\n\nIn actuarial science, the Esscher transform is a transform that takes a probability density \"f\"(\"x\") and transforms it to a new probability density \"f\"(\"x\"; \"h\") with a parameter \"h\". It was introduced by F. Esscher in 1932 .\n\nLet \"f\"(\"x\") be a probability density. Its Esscher transform is defined as\n\nMore generally, if \"μ\" is a probability measure, the Esscher transform of \"μ\" is a new probability measure \"E\"(\"μ\") which has density\n\nwith respect to \"μ\".\n\n\n\n\n"}
{"id": "34812675", "url": "https://en.wikipedia.org/wiki?curid=34812675", "title": "Event structure", "text": "Event structure\n\nIn mathematics and computer science, an event structure represents a set of events, some of which can only be performed after another (there is a \"dependency\" between the events) and some of which might not be performed together (there is a \"conflict\" between the events).\n\nAn event structure formula_1 consists of\nsuch that\n\n"}
{"id": "10858909", "url": "https://en.wikipedia.org/wiki?curid=10858909", "title": "Extensions of symmetric operators", "text": "Extensions of symmetric operators\n\nIn functional analysis, one is interested in extensions of symmetric operators acting on a Hilbert space. Of particular importance is the existence, and sometimes explicit constructions, of self-adjoint extensions. This problem arises, for example, when one needs to specify domains of self-adjointness for formal expressions of observables in quantum mechanics. Other applications of solutions to this problem can be seen in various moment problems.\n\nThis article discusses a few related problems of this type. The unifying theme is that each problem has an operator-theoretic characterization which gives a corresponding parametrization of solutions. More specifically, finding self-adjoint extensions, with various requirements, of symmetric operators is equivalent to finding unitary extensions of suitable partial isometries.\n\nLet \"H\" be a Hilbert space. A linear operator \"A\" acting on \"H\" with dense domain Dom(\"A\") is symmetric if\n\nIf Dom(\"A\") = \"H\", the Hellinger-Toeplitz theorem says that \"A\" is a bounded operator, in which case \"A\" is self-adjoint and the extension problem is trivial. In general, a symmetric operator is self-adjoint if the domain of its adjoint, Dom(\"A*\"), lies in Dom(\"A\").\n\nWhen dealing with unbounded operators, it is often desirable to be able to assume that the operator in question is closed. In the present context, it is a convenient fact that every symmetric operator \"A\" is \nclosable. That is, \"A\" has a smallest closed extension, called the \"closure\" of \"A\". This can\nbe shown by invoking the symmetric assumption and Riesz representation theorem. Since \"A\" and its closure have the same closed extensions, it can always be assumed that the symmetric operator of interest is closed.\n\nIn the sequel, a symmetric operator will be assumed to be densely defined and closed.\n\nProblem \"Given a densely defined closed symmetric operator A, find its self-adjoint extensions.\"\n\nThis question can be translated to an operator-theoretic one. As a heuristic motivation, notice that the Cayley transform on the complex plane, defined by\n\nmaps the real line to the unit circle. This suggests one define, for a symmetric operator \"A\",\n\non \"Ran\"(\"A\" + \"i\"), the range of \"A\" + \"i\". The operator \"U\" is in fact an isometry between closed subspaces that takes (\"A\" + \"i\")\"x\" to (\"A\" - \"i\")\"x\" for \"x\" in Dom(\"A\"). The map\n\nis also called the Cayley transform of the symmetric operator \"A\". Given \"U\", \"A\" can be recovered by\n\ndefined on \"Dom\"(\"A\") = \"Ran\"(\"U\" - 1). Now if\n\nis an isometric extension of \"U\", the operator\n\nacting on\n\nis a symmetric extension of \"A\".\n\nTheorem The symmetric extensions of a closed symmetric operator \"A\" is in one-to-one correspondence with the isometric extensions of its Cayley transform \"U\".\n\nOf more interest is the existence of \"self-adjoint\" extensions. The following is true.\n\nTheorem A closed symmetric operator \"A\" is self-adjoint if and only if Ran (\"A\" ± \"i\") = \"H\", i.e. when its Cayley transform \"U\" is a unitary operator on \"H\".\n\nCorollary The self-adjoint extensions of a closed symmetric operator \"A\" is in one-to-one correspondence with the unitary extensions of its Cayley transform \"U\".\n\nDefine the deficiency subspaces of \"A\" by\n\nand\n\nIn this language, the description of the self-adjoint extension problem given by the corollary can be restated as follows: a symmetric operator \"A\" has self-adjoint extensions if and only if its Cayley transform \"U\" has unitary extensions to \"H\", i.e. the deficiency subspaces \"K\" and \"K\" have the same dimension.\n\nConsider the Hilbert space \"L\"[0,1]. On the subspace of absolutely continuous function that vanish on the boundary, define the operator \"A\" by\n\nIntegration by parts shows \"A\" is symmetric. Its adjoint \"A*\" is the same operator with Dom(\"A*\") being the absolutely continuous functions with no boundary condition. We will see that extending \"A\" amounts to modifying the boundary conditions, thereby enlarging Dom(\"A\") and reducing Dom(\"A*\"), until the two coincide.\n\nDirect calculation shows that \"K\" and \"K\" are one-dimensional subspaces given by\n\nand\n\nwhere \"a\" is a normalizing constant. So the self-adjoint extensions of \"A\" are parametrized by the unit circle in the complex plane, . For each unitary \"U\" : \"K\" → \"K\", defined by \"U\"(\"φ\") = \"αφ\", there corresponds an extension \"A\" with domain\n\nIf \"f\" ∈ Dom(\"A\"), then \"f\" is absolutely continuous and\n\nConversely, if \"f\" is absolutely continuous and \"f\"(0) = \"γf\"(1) for some complex \"γ\" with |\"γ\"| = 1, then \"f\" lies in the above domain.\n\nThe self-adjoint operators { \"A\" } are instances of the momentum operator in quantum mechanics.\n\nEvery partial isometry can be extended, on a possibly larger space, to a unitary operator. Consequently, every symmetric operator has a self-adjoint extension, on a possibly larger space.\n\nA symmetric operator \"A\" is called positive if <\"Ax\", \"x\"> ≥ 0 for all \"x\" in \"Dom\"(\"A\"). It is known that for every such \"A\", one has dim(\"K\") = dim(\"K\"). Therefore, every positive symmetric operator has self-adjoint extensions. The more interesting question in this direction is whether \"A\" has positive self-adjoint extensions.\n\nFor two positive operators \"A\" and \"B\", we put \"A\" ≤ \"B\" if\n\nin the sense of bounded operators.\n\nWhile the extension problem for general symmetric operators is essentially that of extending partial isometries to unitaries, for positive symmetric operators the question becomes one of extending contractions: by \"filling out\" certain unknown entries of a 2 × 2 self-adjoint contraction, we obtain the positive self-adjoint extensions of a positive symmetric operator.\n\nBefore stating the relevant result, we first fix some terminology. For a contraction Γ, acting on \"H\", we define its \"defect operators\" by\n\nThe \"defect spaces\" of Γ are\n\nThe defect operators indicate the non-unitarity of Γ, while the defect spaces ensure uniqueness in some parameterizations.\nUsing this machinery, one can explicitly describe the structure of general matrix contractions. We will only need the 2 × 2 case. Every 2 × 2 contraction Γ can be uniquely expressed as\n\nwhere each Γ is a contraction.\n\nThe Cayley transform for general symmetric operators can be adapted to this special case. For every non-negative number \"a\",\n\nThis suggests we assign to every positive symmetric operator \"A\" a contraction\n\ndefined by\n\nwhich have matrix representation\n\nIt is easily verified that the Γ entry, \"C\" projected onto \"Ran\"(\"A\" + 1) = \"Dom\"(\"C\"), is self-adjoint. The operator \"A\" can be written as\n\nwith \"Dom\"(\"A\") = \"Ran\"(\"C\" - 1). If\n\nis a contraction that extends \"C\" and its projection onto its domain is self-adjoint, then it is clear that its inverse Cayley transform\n\ndefined on\n\nis a positive symmetric extension of \"A\". The symmetric property follows from its projection onto its own domain being self-adjoint and positivity follows from contractivity. The converse is also true: given a positive symmetric extension of \"A\", its Cayley transform is a contraction satisfying the stated \"partial\" self-adjoint property.\n\nTheorem The positive symmetric extensions of \"A\" are in one-to-one correspondence with the extensions of its Cayley transform where if \"C\" is such an extension, we require \"C\" projected onto \"Dom\"(\"C\") be self-adjoint.\n\nThe unitarity criterion of the Cayley transform is replaced by self-adjointness for positive operators.\n\nTheorem A symmetric positive operator \"A\" is self-adjoint if and only if its Cayley transform is a self-adjoint contraction defined on all of \"H\", i.e. when \"Ran\"(\"A\" + 1) = \"H\".\n\nTherefore, finding self-adjoint extension for a positive symmetric operator becomes a \"matrix completion problem\". Specifically, we need to embed the column contraction \"C\" into a 2 × 2 self-adjoint contraction. This can always be done and the structure of such contractions gives a parametrization of all possible extensions.\n\nBy the preceding subsection, all self-adjoint extensions of \"C\" takes the form\n\nSo the self-adjoint positive extensions of \"A\" are in bijective correspondence with the self-adjoint contractions Γ on the defect space\n\nof Γ. The contractions\n\ngive rise to positive extensions\n\nrespectively. These are the \"smallest\" and \"largest\" positive extensions of \"A\" in the sense that\n\nfor any positive self-adjoint extension \"B\" of \"A\". The operator \"A\" is the Friedrichs extension of \"A\" and \"A\" is the von Neumann-Krein extension of \"A\".\n\nSimilar results can be obtained for accretive operators.\n\n"}
{"id": "4504771", "url": "https://en.wikipedia.org/wiki?curid=4504771", "title": "Frame (linear algebra)", "text": "Frame (linear algebra)\n\nIn linear algebra, a frame of an inner product space is a generalization of a basis of a vector space to sets that may be linearly dependent. In the terminology of signal processing, a frame provides a redundant, stable way of representing a signal. Frames are used in error detection and correction and the design and analysis of filter banks and more generally in applied mathematics, computer science, and engineering..\n\nSuppose we have a set of vectors formula_1 in the vector space \"V\" and we want to express an arbitrary element formula_2 as a linear combination of the vectors formula_3, that is, we want to find coefficients formula_4 such that\n\nIf the set formula_6 does not span formula_7, then such coefficients do not exist for every such formula_8. If formula_6 spans formula_7 and also is linearly independent, this set forms a basis of formula_7, and the coefficients formula_12 are uniquely determined by formula_8. If, however, formula_3 spans formula_7 but is not linearly independent, the question of how to determine the coefficients becomes less apparent, in particular if formula_7 is of infinite dimension.\n\nGiven that formula_1 spans formula_7 and is linearly dependent, one strategy is to remove vectors from the set until it becomes linearly independent and forms a basis. There are some problems with this plan:\n\n\nLet \"V\" be an inner product space and formula_27 be a set of vectors in formula_7. These vectors satisfy the \"frame condition\" if there are positive real numbers \"A\" and \"B\" such that <math> 0 and for each formula_8 in \"V\",\nA set of vectors that satisfies the frame condition is a \"frame\" for the vector space.\n\nThe numbers \"A\" and \"B\" are called the lower and upper \"frame bounds\", respectively. The frame bounds are not unique because numbers less than \"A\" and greater than \"B\" are also valid frame bounds. The \"optimal lower bound\" is the supremum of all lower bounds and the \"optimal upper bound\" is the infimum of all upper bounds.\n\nA frame is called \"overcomplete\" (or \"redundant\") if it is not a basis for the vector space.\n\nThe operator mapping formula_2 to a sequence of coefficients formula_4 is called the \"analysis operator\" of the frame. It is defined by:\nBy using this definition we may rewrite the frame condition as\nwhere the left and right norms denote the norm in formula_7 and the middle norm is the formula_36 norm.\n\nThe adjoint operator formula_37 of the analysis operator is called the \"synthesis operator\" of the frame. \n\nWe want that any vector formula_39 can be reconstructed from the coefficients formula_40. This is satisfied if there exists a constant formula_41 such that for all formula_42 we have:\nBy setting formula_44 and applying the linearity of the analysis operator we get that this condition is equivalent to:\nfor all formula_39 which is exactly the lower frame bound condition.\n\nBecause of the various mathematical components surrounding frames, frame theory has roots in harmonic and functional analysis, operator theory, linear algebra, and matrix theory.\n\nThe Fourier transform has been used for over a century as a way of decomposing and expanding signals. However, the Fourier transform masks key information regarding the moment of emission and the duration of a signal. In 1946, Dennis Gabor was able to solve this using a technique that simultaneously reduced noise, provided resiliency, and created quantization while encapsulating important signal characteristics. This discovery marked the first concerted effort towards frame theory.\n\nThe frame condition was first described by Richard Duffin and Albert Charles Schaeffer in a 1952 article on nonharmonic Fourier series as a way of computing the coefficients in a linear combination of the vectors of a linearly dependent spanning set (in their terminology, a \"Hilbert space frame\"). In the 1980s, Stéphane Mallat, Ingrid Daubechies, and Yves Meyer used frames to analyze wavelets. Today frames are associated with wavelets, signal and image processing, and data compression.\n\nA frame satisfies a generalization of Parseval's identity, namely the frame condition, while still maintaining norm equivalence between a signal and its sequence of coefficients.\n\nIf the set formula_1 is a frame of \"V\", it spans \"V\". Otherwise there would exist at least one non-zero formula_2 which would be orthogonal to all formula_49. If we insert formula_8 into the frame condition, we obtain\ntherefore formula_52, which is a violation of the initial assumptions on the lower frame bound.\n\nIf a set of vectors spans \"V\", this is not a sufficient condition for calling the set a frame. As an example, consider formula_53 with the dot product, and the infinite set formula_1 given by\n\nThis set spans \"V\" but since formula_56, we cannot choose a finite upper frame bound \"B\". Consequently, the set formula_1 is not a frame.\n\nIn signal processing, each vector is interpreted as a signal. In this interpretation, a vector expressed as a linear combination of the frame vectors is a redundant signal. Using a frame, it is possible to create a simpler, more sparse representation of a signal as compared with a family of elementary signals (that is, representing a signal strictly with a set of linearly independent vectors may not always be the most compact form). Frames, therefore, provide \"robustness\". Because they provide a way of producing the same vector within a space, signals can be encoded in various ways. This facilitates fault tolerance and resilience to a loss of signal. Finally, redundancy can be used to mitigate noise, which is relevant to the restoration, enhancement, and reconstruction of signals.\n\nIn signal processing, it is common to assume the vector space is a Hilbert space.\n\nA frame is a \"tight frame\" if \"A\" = \"B\"; in other words, the frame satisfies a generalized version of Parseval's identity. For example, the union of \"k\" disjoint orthonormal bases of a vector space is a tight frame with \"A\" = \"B\" = \"k\". A tight frame is a \"Parseval frame\" (sometimes called a \"normalized frame\") if \"A\" = \"B\" = 1. Each orthonormal basis is a Parseval frame, but the converse is not always true.\n\nA frame formula_27 for formula_7 is tight with frame bound \"A\" if and only if\nfor all formula_61.\n\nA frame is an \"equal norm frame\" (sometimes called a \"uniform frame\" or a \"normalized frame\") if there is a constant \"c\" such that formula_62 for each \"i\". An equal norm frame is a \"unit norm frame\" if \"c\" = 1. A Parseval (or tight) unit norm frame is an orthonormal basis; such a frame satisfies Parseval's identity.\n\nA frame is an \"equiangular frame\" if there is a constant \"c\" such that formula_63 for each distinct \"i\" and \"j\".\n\nA frame is an \"exact frame\" if no proper subset of the frame spans the inner product space. Each basis for an inner product space is an exact frame for the space (so a basis is a special case of a frame).\n\nA \"Bessel Sequence\" is a set of vectors that satisfies only the upper bound of the frame condition.\n\nSuppose \"H\" is a Hilbert space, X a locally compact space , and formula_64 is a locally finite \"Borel measure\" on X. Then a set of vectors in \"H\", formula_65 with a measure formula_64 is said to be a \"Continuous Frame\" if there exists constants, formula_67 for all formula_68.\n\nGiven a discrete set formula_69 and a measure formula_70 where formula_71 is the \"Dirac measure\" then the continuous frame property:\nreduces to:\nformula_73\n\nand we see that Continuous Frames are indeed the natural generalization of the frames mentioned above.\n\nJust like in the discrete case we can define the Analysis, Synthesis, and Frame operators when dealing with continuous frames.\n\nGiven a continuous frame formula_65 the \"Continuous Analysis Operator\" is the operator mapping formula_65 to a sequence of coefficients formula_76.\n\nIt is defined as follows:\n\nformula_77 by formula_78\n\nThe adjoint operator of the Continuous Analysis Operator is the \"Continuous Synthesis Operator\" which is the map:\n\nformula_79 by formula_80\n\nThe Composition of the Continuous Analysis Operator and the Continuous Synthesis Operator is known as the \"Continuous Frame Operator\". For a continuous frame formula_65, the \"Continuous Frame Operator\" is defined as follows:\nformula_82 by formula_83\n\nGiven a continuous frame formula_65, and another continuous frame formula_85, then formula_85 is said to be a \"Continuous Dual Frame\" of formula_87 if it satisfies the following condition for all formula_88:\n\nformula_89\n\nThe frame condition entails the existence of a set of \"dual frame vectors\" formula_90 with the property that\nfor any formula_2. This implies that a frame together with its dual frame has the same property as a basis and its dual basis in terms of reconstructing a vector from scalar products.\n\nIn order to construct a dual frame, we first need the linear mapping formula_93, called the frame operator, defined as\n\nFrom this definition of formula_95 and linearity in the first argument of the inner product,\nwhich, when substituted in the frame condition inequality, yields\nfor each formula_2.\n\nThe frame operator formula_95 is self-adjoint, positive definite, and has positive upper and lower bounds. The inverse formula_100 of formula_95 exists and it, too, is self-adjoint, positive definite, and has positive upper and lower bounds.\n\nThe dual frame is defined by mapping each element of the frame with formula_100:\n\nTo see that this makes sense, let formula_8 be an element of formula_7 and let\n\nThus\n\nwhich proves that\n\nAlternatively, we can let\n\nBy inserting the above definition of formula_110 and applying the properties of formula_95 and its inverse,\n\nwhich shows that\n\nThe numbers formula_114 are called \"frame coefficients\". This derivation of a dual frame is a summary of Section 3 in the article by Duffin and Schaeffer. They use the term \"conjugate frame\" for what here is called a dual frame.\n\nThe dual frame formula_115 is called the \"canonical dual\" of formula_3 because it acts similarly as a dual basis to a basis.\n\nWhen the frame formula_3 is overcomplete, a vector formula_8 can be written as a linear combination of formula_3 in more than one way. That is, there are different choices of coefficients formula_120 such that formula_121. This allows us some freedom for the choice of coefficients formula_120 other than formula_114. It is necessary that the frame formula_3 is overcomplete for other such coefficients formula_120 to exist. If so, then there exist frames formula_126 for which \nfor all formula_2. We call formula_129 a dual frame of formula_3.\n\nCanonical duality is a reciprocity relation, i.e. if the frame formula_90 is the canonical dual frame of formula_6, then formula_6 is the canonical dual frame of formula_90.\n\n\n"}
{"id": "5094208", "url": "https://en.wikipedia.org/wiki?curid=5094208", "title": "Fritz Carlson", "text": "Fritz Carlson\n\nFritz David Carlson (23 July 1888 – 28 November 1952) was a Swedish mathematician. After the death of Torsten Carleman, he headed the Mittag-Leffler Institute.\n\nCarlson's contributions to analysis include Carlson's theorem, the Polyá–Carlson theorem on rational functions, and Carlson's inequality\n\nIn number theory, his results include Carlson's theorem on Dirichlet series.\n\nHans Rådström, Germund Dahlquist, and Tord Ganelius were among his students.\n"}
{"id": "42027498", "url": "https://en.wikipedia.org/wiki?curid=42027498", "title": "Global analysis", "text": "Global analysis\n\nIn mathematics, global analysis, also called analysis on manifolds, is the study of the global and topological properties of differential equations on manifolds and vector bundles. Global analysis uses techniques in infinite-dimensional manifold theory and topological spaces of mappings to classify behaviors of differential equations, particularly nonlinear differential equations. These spaces can include singularities and hence catastrophe theory is a part of global analysis. Optimization problems, such as finding geodesics on Riemannian manifolds, can be solved using differential equations so that the calculus of variations overlaps with global analysis. Global analysis finds application in physics in the study of dynamical systems and topological quantum field theory.\n\n\n\n"}
{"id": "2518328", "url": "https://en.wikipedia.org/wiki?curid=2518328", "title": "Herbrand's theorem", "text": "Herbrand's theorem\n\nHerbrand's theorem is a fundamental result of mathematical logic obtained by Jacques Herbrand (1930). It essentially allows a certain kind of reduction of first-order logic to propositional logic. Although Herbrand originally proved his theorem for arbitrary formulas of first-order logic, the simpler version shown here, restricted to formulas in prenex form containing only existential quantifiers, became more popular.\n\nLet \n\nbe a formula of first-order logic with formula_2 quantifier-free. Then that formula is valid if and only if there exists a finite sequence of terms formula_3, possibly in an expansion of the language, with \n\nsuch that \n\nis valid. If it is valid, it is called a \"Herbrand disjunction\" for \n\nInformally: a formula formula_8 in prenex form containing only existential quantifiers is provable (valid) in first-order logic if and only if a disjunction composed of substitution instances of the quantifier-free subformula of formula_8 is a tautology (propositionally derivable).\n\nThe restriction to formulas in prenex form containing only existential quantifiers does not limit the generality of the theorem, because formulas can be converted to prenex form and their universal quantifiers can be removed by Herbrandization. Conversion to prenex form can be avoided, if \"structural\" Herbrandization is performed. Herbrandization can be avoided by imposing additional restrictions on the variable dependencies allowed in the Herbrand disjunction.\n\nA proof of the non-trivial direction of the theorem can be constructed according to the following steps:\n\n\nHowever, sequent calculus and cut-elimination were not known at the time of Herbrand's theorem, and Herbrand had to prove his theorem in a more complicated way.\n\n\n"}
{"id": "385982", "url": "https://en.wikipedia.org/wiki?curid=385982", "title": "Irreducible representation", "text": "Irreducible representation\n\nIn mathematics, specifically in the representation theory of groups and algebras, an irreducible representation formula_1 or irrep of an algebraic structure formula_2 is a nonzero representation that has no proper subrepresentation formula_3 closed under the action of formula_4.\n\nEvery finite-dimensional unitary representation on a Hermitian vector space formula_5 is the direct sum of irreducible representations. As irreducible representations are always indecomposable (i.e. cannot be decomposed further into a direct sum of representations), these terms are often confused; however, in general there are many reducible but indecomposable representations, such as the two-dimensional representation of the real numbers acting by upper triangular unipotent matrices.\n\nGroup representation theory was generalized by Richard Brauer from the 1940s to give modular representation theory, in which the matrix operators act on a vector space over a field formula_6 of arbitrary characteristic, rather than a vector space over the field of real numbers or over the field of complex numbers. The structure analogous to an irreducible representation in the resulting theory is a simple module.\n\nLet formula_7 be a representation i.e. a homomorphism formula_8 of a group formula_9 where formula_5 is a vector space over a field formula_11. If we pick a basis formula_12 for formula_5, formula_7 can be thought of as a function (a homomorphism) from a group into a set of invertible matrices and in this context is called a matrix representation. However, it simplifies things greatly if we think of the space formula_5 without a basis.\n\nA linear subspace formula_16 is called formula_9-invariant if formula_18 for all formula_19 and all formula_20. The restriction of formula_7 to a formula_9-invariant subspace formula_16 is known as a subrepresentation. A representation formula_8 is said to be irreducible if it has only trivial subrepresentations (all representations can form a subrepresentation with the trivial formula_9-invariant subspaces, e.g. the whole vector space formula_5, and {0}). If there is a proper non-trivial invariant subspace, formula_7 is said to be reducible.\n\nGroup elements can be represented by matrices, although the term \"represented\" has a specific and precise meaning in this context. A representation of a group is a mapping from the group elements to the general linear group of matrices. As notation, let denote elements of a group with group product signified without any symbol, so is the group product of and and is also an element of , and let representations be indicated by . The representation of \"a\" is written\n\nBy definition of group representations, the representation of a group product is translated into matrix multiplication of the representations:\n\nIf is the identity element of the group (so that , etc.), then is an identity matrix, or identically a block matrix of identity matrices, since we must have\n\nand similarly for all other group elements. The last two staments correspond to the requirement that is a group homomorphism.\n\nA representation is decomposable if a similar matrix can be found for the similarity transformation:\n\nwhich diagonalizes every matrix in the representation into the same pattern of diagonal blocks – each of the blocks are representation of the group independent of each other. The representations and are said to be equivalent representations. The representation can be decomposed into a direct sum of \"k\" matrices:\n\nso is decomposable, and it is customary to label the decomposed matrices by a superscript in brackets, as in for , although some authors just write the numerical label without parantheses.\n\nThe dimension of is the sum of the dimensions of the blocks:\n\nIf this is not possible, i.e. , then the representation is indecomposable.\n\nAll groups formula_9 have a one-dimensional, irreducible trivial representation. More generally, any one-dimensional representation is irreducible by virtue of having no proper nontrivial subspaces.\n\nThe irreducible complex representations of a finite group G can be characterized using results from character theory. In particular, all such representations decompose as a direct sum of irreps, and the number of irreps of formula_9 is equal to the number of conjugacy classes of formula_9.\n\nIn quantum physics and quantum chemistry, each set of degenerate eigenstates of the Hamiltonian operator comprises a vector space for a representation of the symmetry group of the Hamiltonian, a \"multiplet\", best studied through reduction to its irreducible parts. Identifying the irreducible representations therefore allows one to label the states, predict how they will split under perturbations; or transition to other states in . Thus, in quantum mechanics, irreducible representations of the symmetry group of the system partially or completely label the energy levels of the system, allowing the selection rules to be determined.\n\nThe irreps of and , where is the generator of rotations and the generator of boosts, can be used to build to spin representations of the Lorentz group, because they are related to the spin matrices of quantum mechanics. This allows them to derive relativistic wave equations.\n\n\n\n\n\n"}
{"id": "29220164", "url": "https://en.wikipedia.org/wiki?curid=29220164", "title": "Laplace functional", "text": "Laplace functional\n\nIn probability theory, a Laplace functional refers to one of two possible mathematical functions of functions or, more precisely, functionals that serve as mathematical tools for studying either point processes or concentration of measure properties of metric spaces. One type of Laplace functional, also known as a characteristic functional is defined in relation to a point process, which can be interpreted as random counting measures, and has applications in characterizing and deriving results on point processes. Its definition is analogous to a characteristic function for a random variable.\n\nThe other Laplace functional is for probability spaces equipped with metrics, and is used to study the concentration of measure properties of the space.\n\nFor a general point process formula_1 defined on formula_2, the Laplace functional is defined as:\n\nwhere formula_4 is any measurable non-negative function on formula_2 and\n\nwhere the notation formula_7 interprets the point process as a random counting measure; see Point process notation.\n\nThe Laplace functional characterizes a point process, and if it is known for a point process, it can be used to prove various results.\n\nFor some metric probability space (\"X\", \"d\", \"μ\"), where (\"X\", \"d\") is a metric space and \"μ\" is a probability measure on the Borel sets of (\"X\", \"d\"), the Laplace functional:\n\nThe Laplace functional maps from the positive real line to the positive (extended) real line, or in mathematical notation:\n\nThe Laplace functional of (\"X\", \"d\", \"μ\") can be used to bound the concentration function of (\"X\", \"d\", \"μ\"), which is defined for \"r\" > 0 by\n\nwhere\n\nThe Laplace functional of (\"X\", \"d\", \"μ\") then gives leads to the upper bound:\n\n"}
{"id": "7758584", "url": "https://en.wikipedia.org/wiki?curid=7758584", "title": "Marshall Hall (mathematician)", "text": "Marshall Hall (mathematician)\n\nMarshall Hall, Jr. (17 September 1910 – 4 July 1990) was an American mathematician who made significant contributions to group theory and combinatorics.\n\nHe studied mathematics at Yale University, graduating in 1932. He studied for a year at Cambridge University under a Henry Fellowship working with G. H. Hardy. He returned to Yale to take his Ph.D. in 1936 under the supervision of Øystein Ore.\n\nHe worked in Naval Intelligence during World War II, including six months in 1944 at Bletchley Park, the center of British wartime code breaking. In 1946 he took a position at The Ohio State University. In 1959 he moved to the California Institute of Technology where, in 1973, he was named the first IBM Professor at Caltech, the first named chair in mathematics. After retiring from Caltech in 1981, he accepted a post at Emory University in 1985.\n\nHall died in 1990 in London on his way to a conference to mark his 80th birthday.\n\nHe wrote a number of papers of fundamental importance in group theory, including his solution of Burnside's problem for groups of exponent 6, showing that a finitely generated group in which the order of every element divides 6 must be finite.\n\nHis work in combinatorics includes an important paper of 1943 on projective planes, which for many years was one of the most cited mathematics research papers. In this paper he constructed a family of non-Desarguesian planes which are known today as Hall planes. He also worked on block designs and coding theory.\n\nHis classic book on group theory was well received when it came out and is still useful today. His book \"Combinatorial Theory\" came out in a second edition in 1986, published by John Wiley & Sons.\n\nHe proposed Hall's conjecture on the differences between perfect squares and perfect cubes, which remains an open problem as of 2015.\n\n\n\n"}
{"id": "23236792", "url": "https://en.wikipedia.org/wiki?curid=23236792", "title": "Mathematical exposure modeling", "text": "Mathematical exposure modeling\n\nMathematical exposure modeling is an indirect method of determining exposure, particularly for human exposure to environmental contaminants. It is useful when direct measurement of pollutant concentration is not feasible because direct measurement sometimes requires skilled professionals and complex, expensive laboratory equipment. The ability to make inferences in the absence of direct measurements, makes exposure modeling a powerful tool for predicting exposures by exploring hypothetical situations. It allows researchers to ask \"what if\" questions about exposure scenarios.\n\nMathematical modeling is commonly used to determine human exposure to indoor air pollution. Studies have shown that humans spend about 90% of their time indoors, and contaminant levels may be as high or higher inside than outside, due to the presence of multiple indoor contaminant sources, in combination with poor ventilation. Indoor air modeling requires information on a number of parameters including the air exchange rate, deposition rate, source emission rate, and physical volume of the indoor setting. Indoor environments can basically be thought of as closed systems, so models describing them are usually based on the \"mass balance\" equation. It is also assumed that a pollutant emitted into an indoor environment instantly spreads uniformly throughout the system, so that the concentration is the same at any point in space at any point in time. Mathematically, the total pollutant mass emitted inside a chamber during time T can be expressed as<br>\n\nThe total mass lost during time T can be expressed as<br>\n\nFollowing the principle of the \"mass balance\" equation, the total mass in the chamber at time T, is the difference between the two equations above, mass generated during time T minus mass lost during time T. This value may also be calculated from the equation<br>\n\nThere are two critical pieces of information that are needed to calculate human exposure. These include data on 1) the whereabouts of the individual or individuals being exposed and 2) the concentration of the pollutants in the different locations. This can be expressed mathematically as the sum of the products of time spent by a person in those different locations by the time-averaged air pollutant concentrations occurring in those locations.<br>\n\nAs mentioned above, knowing the whereabouts of the individual or individuals, is very important when trying to determine air pollution exposure. In the absence of data obtained from direct observation, human activity pattern data may be used. This data can be found in several reports conducted by the U.S. Environmental Protection Agency. The data was collected through the National Human Activity Pattern Survey (NHAPS), and contains a representative cross-section of 24-hour daily activity patterns. This data can be used to create inhalation exposure models which can serve as useful public health tools for epidemiology, education, intervention, risk assessment, and creation of air quality guidelines.\n\n\nOtt, W.R., Steinemann, A.C., Wallace, L.A.. Exposure Analysis. CRC Press (2007)\n\nThe Inside Story: A Guide to Indoor Air Quality. U.S. EPA (2009)\n"}
{"id": "24221954", "url": "https://en.wikipedia.org/wiki?curid=24221954", "title": "Maximum coverage problem", "text": "Maximum coverage problem\n\nThe maximum coverage problem is a classical question in computer science, computational complexity theory, and operations research.\nIt is a problem that is widely taught in approximation algorithms.\n\nAs input you are given several sets and a number formula_1. \nThe sets may have some elements in common. \nYou must select at most formula_1 of these sets such that the maximum number of elements are covered, \ni.e. the union of the selected sets has maximal size.\n\nFormally, (unweighted) Maximum Coverage \nThe maximum coverage problem is NP-hard, and can be approximated within formula_8 under standard assumptions. \nThis result essentially matches the approximation ratio achieved by the generic greedy algorithm used for maximization of submodular functions with a cardinality constraint.\n\nThe maximum coverage problem can be formulated as the following integer linear program.\n\nThe greedy algorithm for maximum coverage chooses sets according to one rule: at each stage, choose a set which contains the largest number of uncovered elements. It can be shown that this algorithm achieves an approximation ratio of formula_9. ln-approximability results show that the greedy algorithm is essentially the best-possible polynomial time approximation algorithm for maximum coverage unless formula_10.\n\nThe inapproximability results apply to all extensions of the maximum coverage problem since they hold the maximum coverage problem as a special case.\n\nThe Maximum Coverage Problem can be applied to road traffic situations; one such example is selecting which bus routes in a public transportation network should be installed with pothole detectors to maximise coverage, when only a limited number of sensors is available. This problem is a known extension of the Maximum Coverage Problem and was first explored in literature by Junade Ali and Vladimir Dyo.\n\nIn the weighted version every element formula_11 has a weight formula_12. The task is to find a maximum coverage which has maximum weight. The basic version is a special case when all weights are formula_13.\n\nThe greedy algorithm for the weighted maximum coverage at each stage chooses a set that contains the maximum weight of uncovered elements. This algorithm achieves an approximation ratio of formula_9.\n\nIn the budgeted maximum coverage version, not only does every element formula_11 have a weight formula_12, but also every set formula_25 has a cost formula_30. Instead of formula_1 that limits the number of sets in the cover a budget formula_32 is given. This budget formula_32 limits the total cost of the cover that can be chosen.\n\nA greedy algorithm will no longer produce solutions with a performance guarantee. Namely, the worst case behavior of this algorithm might be very far from the optimal solution. The approximation algorithm is extended by the following way. First, define a modified greedy algorithm, that selects the set formula_25 that has the best ratio of weighted uncovered elements to cost. Second, among covers of cardinality formula_47, find the best cover that does not violate the budget. Call this cover formula_48. Third, find all covers of cardinality formula_1 that do not violate the budget. Using these covers of cardinality formula_1 as starting points, apply the modified greedy algorithm, maintaining the best cover found so far. Call this cover formula_51. At the end of the process, the approximate best cover will be either formula_48 or formula_51. This algorithm achieves an approximation ratio of formula_54 for values of formula_55. This is the best possible approximation ratio unless formula_56.\n\nIn the generalized maximum coverage version every set formula_25 has a cost formula_30, \nelement formula_11 has a different weight and cost depending on which set covers it.\nNamely, if formula_11 is covered by set formula_25 the weight of formula_11\nis formula_63 and its cost is formula_64. \nA budget formula_65 is given for the total cost of the solution.\n\nThe algorithm uses the concept of residual cost/weight. The residual cost/weight is measured against a tentative solution and it is the difference of the cost/weight from the cost/weight gained by a tentative solution.\n\nThe algorithm has several stages. First, find a solution using greedy algorithm. In each iteration of the greedy algorithm the tentative solution is added the set which contains the maximum residual weight of elements divided by the residual cost of these elements along with the residual cost of the set. Second, compare the solution gained by the first step to the best solution which uses a small number of sets. Third, return the best out of all examined solutions. This algorithm achieves an approximation ratio of formula_81.\n\n"}
{"id": "212935", "url": "https://en.wikipedia.org/wiki?curid=212935", "title": "Measure of non-compactness", "text": "Measure of non-compactness\n\nIn functional analysis, two measures of non-compactness are commonly used; these associate numbers to sets in such a way that compact sets all get the measure 0, and other sets get measures that are bigger according to \"how far\" they are removed from compactness.\n\nThe underlying idea is the following: a bounded set can be covered by a single ball of some radius. Sometimes several balls of a smaller radius can also cover the set. A compact set in fact can be covered by finitely many balls of arbitrary small radius, because it is totally bounded. So one could ask: what is the smallest radius that allows to cover the set with finitely many balls?\n\nFormally, we start with a metric space \"M\" and a subset \"X\". The ball measure of non-compactness is defined as\nand the Kuratowski measure of non-compactness is defined as\n\nSince a ball of radius \"r\" has diameter at most 2\"r\", we have α(\"X\") ≤ β(\"X\") ≤ 2α(\"X\").\n\nThe two measures α and β share many properties, and we will use γ in the sequel to denote either one of them. Here is a collection of facts:\n\nMeasures of non-compactness are most commonly used if \"M\" is a normed vector space. In this case, we have in addition:\n\nNote that these measures of non-compactness are useless for subsets of Euclidean space R: by the Heine–Borel theorem, every bounded closed set is compact there, which means that γ(\"X\") = 0 or ∞ according to whether \"X\" is bounded or not.\n\nMeasures of non-compactness are however useful in the study of infinite-dimensional Banach spaces, for example. In this context, one can prove that any ball \"B\" of radius \"r\" has α(\"B\") = \"r\" and β(\"B\") = 2\"r\".\n\n"}
{"id": "333365", "url": "https://en.wikipedia.org/wiki?curid=333365", "title": "Modal logic", "text": "Modal logic\n\nModal logic is a type of formal logic primarily developed in the 1960s that extends classical propositional and predicate logic to include operators expressing modality. A modal—a word that expresses a modality—qualifies a statement. For example, the statement \"John is happy\" might be qualified by saying that John is usually happy, in which case the term \"usually\" is functioning as a modal. The traditional alethic modalities, or modalities of truth, include possibility (\"Possibly, \"p\"\", \"It is possible that \"p\"\"), necessity (\"Necessarily, \"p\"\", \"It is necessary that \"p\"\"), and impossibility (\"Impossibly, \"p\"\", \"It is impossible that \"p\"\"). Other modalities that have been formalized in modal logic include temporal modalities, or modalities of time (notably, \"It was the case that \"p\"\", \"It has always been that \"p\"\", \"It will be that \"p\"\", \"It will always be that \"p\"\"), deontic modalities (notably, \"It is obligatory that \"p\"\", and \"It is permissible that \"p\"\"), epistemic modalities, or modalities of knowledge (\"It is known that \"p\"\") and doxastic modalities, or modalities of belief (\"It is believed that \"p\"\").\n\nA formal modal logic represents modalities using modal operators. For example, \"It might rain today\" and \"It is possible that rain will fall today\" both contain the notion of possibility. In a modal logic this is represented as an operator, \"Possibly\", attached to the sentence \"It will rain today\".\n\nIt is fallacious to confuse necessity and possibility. In particular, this is known as the modal fallacy.\n\nThe basic unary (1-place) modal operators are usually written \"□\" for \"Necessarily\" and \"◇\" for \"Possibly\". In a classical modal logic, each can be expressed by the other with negation:\n\nThus it is \"possible\" that it will rain today if and only if it is \"not necessary\" that it will \"not\" rain today, and it is \"necessary\" that it will rain today if and only if it is \"not possible\" that it will \"not\" rain today. Alternative symbols used for the modal operators are \"L\" for \"Necessarily\" and \"M\" for \"Possibly\".\n\nIn addition to his non-modal syllogistic, Aristotle also developed a modal syllogistic in Book I of his \"Prior Analytics\" (chs 8–22), which Theophrastus attempted to improve. There are also passages in Aristotle's work, such as the famous sea-battle argument in \"De Interpretatione\" §9, that are now seen as anticipations of the connection of modal logic with potentiality and time. In the Hellenistic period, the logicians Diodorus Cronus, Philo the Dialectician and the Stoic Chrysippus each developed a modal system that accounted for the interdefinability of possibility and necessity, accepted axiom T (see below), and combined elements of modal logic and temporal logic in attempts to solve the notorious Master Argument. The earliest formal system of modal logic was developed by Avicenna, who ultimately developed a theory of \"temporally modal\" syllogistic. Modal logic as a self-aware subject owes much to the writings of the Scholastics, in particular William of Ockham and John Duns Scotus, who reasoned informally in a modal manner, mainly to analyze statements about essence and accident.\n\nC. I. Lewis founded modern modal logic in his 1910 Harvard thesis and in a series of scholarly articles beginning in 1912. This work culminated in his 1932 book \"Symbolic Logic\" (with C. H. Langford), which introduced the five systems \"S1\" through \"S5\".\n\nRuth C. Barcan (later Ruth Barcan Marcus) developed the first axiomatic systems of quantified modal logic — first and second order extensions of Lewis' \"S2\", \"S4\", and \"S5\".\n\nThe contemporary era in modal semantics began in 1959, when Saul Kripke (then only a 19-year-old Harvard University undergraduate) introduced the now-standard Kripke semantics for modal logics. These are commonly referred to as \"possible worlds\" semantics. Kripke and A. N. Prior had previously corresponded at some length. Kripke semantics is basically simple, but proofs are eased using semantic-tableaux or analytic tableaux, as explained by E. W. Beth.\n\nA. N. Prior created modern temporal logic, closely related to modal logic, in 1957 by adding modal operators [F] and [P] meaning \"eventually\" and \"previously\". Vaughan Pratt introduced dynamic logic in 1976. In 1977, Amir Pnueli proposed using temporal logic to formalise the behaviour of continually operating concurrent programs. Flavors of temporal logic include propositional dynamic logic (PDL), propositional linear temporal logic (PLTL), linear temporal logic (LTL), computation tree logic (CTL), Hennessy–Milner logic, and \"T\".\n\nThe mathematical structure of modal logic, namely Boolean algebras augmented with unary operations (often called modal algebras), began to emerge with J. C. C. McKinsey's 1941 proof that \"S2\" and \"S4\" are decidable, and reached full flower in the work of Alfred Tarski and his student Bjarni Jónsson (Jónsson and Tarski 1951–52). This work revealed that \"S4\" and \"S5\" are models of interior algebra, a proper extension of Boolean algebra originally designed to capture the properties of the interior and closure operators of topology. Texts on modal logic typically do little more than mention its connections with the study of Boolean algebras and topology. For a thorough survey of the history of formal modal logic and of the associated mathematics, see Robert Goldblatt (2006).\n\nThe semantics for modal logic are usually given as follows: First we define a \"frame\", which consists of a non-empty set, \"G\", whose members are generally called possible worlds, and a binary relation, \"R\", that holds (or not) between the possible worlds of \"G\". This binary relation is called the \"accessibility relation\". For example, \"w R u\" means that the world \"u\" is accessible from world \"w\". That is to say, the state of affairs known as \"u\" is a live possibility for \"w\". This gives a pair, formula_3. Some formulations of modal logic also include a constant term in \"G\", conventionally called \"the actual world\", which is often symbolized as formula_4.\n\nNext, the \"frame\" is extended to a \"model\" by specifying the truth-values of all propositions at each of the worlds in \"G\". We do so by defining a relation \"v\" between possible worlds and positive literals. If there is a world \"w\" such that formula_5, then \"P\" is true at \"w\". A model is thus an ordered triple, formula_6.\n\nThen we recursively define the truth of a formula at a world in a model:\n\n\nAccording to these semantics, a truth is \"necessary\" with respect to a possible world \"w\" if it is true at every world that is accessible to \"w\", and \"possible\" if it is true at some world that is accessible to \"w\". Possibility thereby depends upon the accessibility relation \"R\", which allows us to express the relative nature of possibility. For example, we might say that given our laws of physics it is not possible for humans to travel faster than the speed of light, but that given other circumstances it could have been possible to do so. Using the accessibility relation we can translate this scenario as follows: At all of the worlds accessible to our own world, it is not the case that humans can travel faster than the speed of light, but at one of these accessible worlds there is \"another\" world accessible from \"those\" worlds but not accessible from our own at which humans can travel faster than the speed of light.\n\nIt should also be noted that the definition of □ makes vacuously true certain sentences, since when it speaks of \"every world that is accessible to \"w\"\" it takes for granted the usual mathematical interpretation of the word \"every\" (see vacuous truth). Hence, if a world \"w\" doesn't have any accessible worlds, any sentence beginning with □ is true.\n\nThe different systems of modal logic are distinguished by the properties of their corresponding accessibility relations. There are several systems that have been espoused (often called \"frame conditions\"). An accessibility relation is:\n\n\nThe logics that stem from these frame conditions are:\n\nThe Euclidean property along with reflexivity yields symmetry and transitivity. (The Euclidean property can be obtained, as well, from symmetry and transitivity.) Hence if the accessibility relation \"R\" is reflexive and Euclidean, \"R\" is provably symmetric and transitive as well. Hence for models of S5, \"R\" is an equivalence relation, because \"R\" is reflexive, symmetric and transitive.\n\nWe can prove that these frames produce the same set of valid sentences as do the frames where all worlds can see all other worlds of \"W\" (\"i.e.\", where \"R\" is a \"total\" relation). This gives the corresponding \"modal graph\" which is total complete (\"i.e.\", no more edges (relations) can be added). For example, in any modal logic based on frame conditions:\n\nIf we consider frames based on the total relation we can just say that\nWe can drop the accessibility clause from the latter stipulation because in such total frames it is trivially true of all \"w\" and \"u\" that \"w R u\". But note that this does not have to be the case in all S5 frames, which can still consist of multiple parts that are fully connected among themselves but still disconnected from each other.\n\nAll of these logical systems can also be defined axiomatically, as is shown in the next section. For example, in S5, the axioms formula_24, formula_25 and formula_26 (corresponding to \"symmetry\", \"transitivity\" and \"reflexivity\", respectively) hold, whereas at least one of these axioms does not hold in each of the other, weaker logics.\n\nThe first formalizations of modal logic were axiomatic. Numerous variations with very different properties have been proposed since C. I. Lewis began working in the area in 1910. Hughes and Cresswell (1996), for example, describe 42 normal and 25 non-normal modal logics. Zeman (1973) describes some systems Hughes and Cresswell omit.\n\nModern treatments of modal logic begin by augmenting the propositional calculus with two unary operations, one denoting \"necessity\" and the other \"possibility\". The notation of C. I. Lewis, much employed since, denotes \"necessarily \"p\"\" by a prefixed \"box\" (□\"p\") whose scope is established by parentheses. Likewise, a prefixed \"diamond\" (◇\"p\") denotes \"possibly \"p\"\". Regardless of notation, each of these operators is definable in terms of the other in classical modal logic:\nHence □ and ◇ form a dual pair of operators.\n\nIn many modal logics, the necessity and possibility operators satisfy the following analogues of de Morgan's laws from Boolean algebra:\n\nPrecisely what axioms and rules must be added to the propositional calculus to create a usable system of modal logic is a matter of philosophical opinion, often driven by the theorems one wishes to prove; or, in computer science, it is a matter of what sort of computational or deductive system one wishes to model. Many modal logics, known collectively as normal modal logics, include the following rule and axiom:\n\nThe weakest normal modal logic, named \"K\" in honor of Saul Kripke, is simply the propositional calculus augmented by □, the rule N, and the axiom K. \"K\" is weak in that it fails to determine whether a proposition can be necessary but only contingently necessary. That is, it is not a theorem of \"K\" that if □\"p\" is true then □□\"p\" is true, i.e., that necessary truths are \"necessarily necessary\". If such perplexities are deemed forced and artificial, this defect of \"K\" is not a great one. In any case, different answers to such questions yield different systems of modal logic.\n\nAdding axioms to \"K\" gives rise to other well-known modal systems. One cannot prove in \"K\" that if \"\"p\" is necessary\" then \"p\" is true. The axiom T remedies this defect:\nT holds in most but not all modal logics. Zeman (1973) describes a few exceptions, such as \"S1\".\n\nOther well-known elementary axioms are:\n\nThese yield the systems (axioms in bold, systems in italics):\n\"K\" through \"S5\" form a nested hierarchy of systems, making up the core of normal modal logic. But specific rules or sets of rules may be appropriate for specific systems. For example, in deontic logic, formula_29 (If it ought to be that \"p\", then it is permitted that \"p\") seems appropriate, but we should probably not include that formula_28. In fact, to do so is to commit the naturalistic fallacy (i.e. to state that what is natural is also good, by saying that if \"p\" is the case, \"p\" ought to be permitted).\n\nThe commonly employed system \"S5\" simply makes all modal truths necessary. For example, if \"p\" is possible, then it is \"necessary\" that \"p\" is possible. Also, if \"p\" is necessary, then it is necessary that \"p\" is necessary. Other systems of modal logic have been formulated, in part because \"S5\" does not describe every kind of modality of interest.\n\nSequent calculi and systems of natural deduction have been developed for several modal logics, but it has proven hard to combine generality with other features expected of good structural proof theories, such as purity (the proof theory does not introduce extra-logical notions such as labels) and analyticity (the logical rules support a clean notion of analytic proof). More complex calculi have been applied to modal logic to achieve generality.\n\nAnalytic tableaux provide the most popular decision method for modal logics.\n\nModalities of necessity and possibility are called \"alethic\" modalities. They are also sometimes called \"special\" modalities, from the Latin \"species\". Modal logic was first developed to deal with these concepts, and only afterward was extended to others. For this reason, or perhaps for their familiarity and simplicity, necessity and possibility are often casually treated as \"the\" subject matter of modal logic. Moreover, it is easier to make sense of relativizing necessity, e.g. to legal, physical, nomological, epistemic, and so on, than it is to make sense of relativizing other notions.\n\nIn classical modal logic, a proposition is said to be\n\nIn classical modal logic, therefore, the notion of either possibility or necessity may be taken to be basic, where these other notions are defined in terms of it in the manner of De Morgan duality. Intuitionistic modal logic treats possibility and necessity as not perfectly symmetric.\n\nFor example, suppose that while walking to the convenience store we pass Friedrich's house, and observe that the lights are off. On the way back, we observe that they have been turned on.\n(Of course, this analogy does not apply alethic modality in a \"truly\" rigorous fashion; for it to do so, it would have to axiomatically make such statements as \"human beings cannot rise from the dead\", \"Socrates was a human being and not an immortal vampire\", and \"we did not take hallucinogenic drugs which caused us to falsely believe the lights were on\", \"ad infinitum\". Absolute certainty of truth or falsehood exists only in the sense of logically constructed abstract concepts such as \"it is impossible to draw a triangle with four sides\" and \"all bachelors are unmarried\".)\n\nFor those with difficulty with the concept of something being possible but not true, the meaning of these terms may be made more comprehensible by thinking of multiple \"possible worlds\" (in the sense of Leibniz) or \"alternate universes\"; something \"necessary\" is true in all possible worlds, something \"possible\" is true in at least one possible world. These \"possible world semantics\" are formalized with Kripke semantics.\n\nSomething is physically, or nomically, possible if it is permitted by the laws of physics. For example, current theory is thought to allow for there to be an atom with an atomic number of 126, even if there are no such atoms in existence. In contrast, while it is logically possible (i.e. probably via Alcubierre drive or worm holes) to accelerate beyond the speed of light, modern science stipulates that it is not physically possible for material particles or information.\n\nPhilosophers ponder the properties that objects have independently of those dictated by scientific laws. For example, it might be metaphysically necessary, as some who advocate physicalism have thought, that all thinking beings have bodies and can experience the passage of time. Saul Kripke has argued that every person necessarily has the parents they do have: anyone with different parents would not be the same person.\n\nMetaphysical possibility has been thought to be more restricting than bare logical possibility (i.e., fewer things are metaphysically possible than are logically possible). However, its exact relation (if any) to logical possibility or to physical possibility is a matter of dispute. Philosophers also disagree over whether metaphysical truths are necessary merely \"by definition\", or whether they reflect some underlying deep facts about the world, or something else entirely.\n\nEpistemic modalities (from the Greek \"episteme\", knowledge), deal with the \"certainty\" of sentences. The □ operator is translated as \"x knows that…\", and the ◇ operator is translated as \"For all x knows, it may be true that…\" In ordinary speech both metaphysical and epistemic modalities are often expressed in similar words; the following contrasts may help:\n\nA person, Jones, might reasonably say \"both\": (1) \"No, it is \"not\" possible that Bigfoot exists; I am quite certain of that\"; \"and\", (2) \"Sure, it's \"possible\" that Bigfoots could exist\". What Jones means by (1) is that, given all the available information, there is no question remaining as to whether Bigfoot exists. This is an epistemic claim. By (2) he makes the \"metaphysical\" claim that it is \"possible for\" Bigfoot to exist, \"even though he does not\": there is no physical or biological reason that large, featherless, bipedal creatures with thick hair could not exist in the forests of North America (regardless of whether or not they do). Similarly, \"it is possible for the person reading this sentence to be fourteen feet tall and named Chad\" is \"metaphysically\" true (such a person would not somehow be prevented from doing so on account of their height and name), but not \"alethically\" true unless you match that description, and not \"epistemically\" true if it's known that fourteen-foot-tall human beings have never existed.\n\nFrom the other direction, Jones might say, (3) \"It is \"possible\" that Goldbach's conjecture is true; but also \"possible\" that it is false\", and \"also\" (4) \"if it \"is\" true, then it is necessarily true, and not possibly false\". Here Jones means that it is \"epistemically possible\" that it is true or false, for all he knows (Goldbach's conjecture has not been proven either true or false), but if there \"is\" a proof (heretofore undiscovered), then it would show that it is not \"logically\" possible for Goldbach's conjecture to be false—there could be no set of numbers that violated it. Logical possibility is a form of \"alethic\" possibility; (4) makes a claim about whether it is possible (i.e., logically speaking) that a mathematical truth to have been false, but (3) only makes a claim about whether it is possible, for all Jones knows, (i.e., speaking of certitude) that the mathematical claim is specifically either true or false, and so again Jones does not contradict himself. It is worthwhile to observe that Jones is not necessarily correct: It is possible (epistemically) that Goldbach's conjecture is both true and unprovable.\n\nEpistemic possibilities also bear on the actual world in a way that metaphysical possibilities do not. Metaphysical possibilities bear on ways the world \"might have been,\" but epistemic possibilities bear on the way the world \"may be\" (for all we know). Suppose, for example, that I want to know whether or not to take an umbrella before I leave. If you tell me \"it is \"possible that\" it is raining outside\" – in the sense of epistemic possibility – then that would weigh on whether or not I take the umbrella. But if you just tell me that \"it is \"possible for\" it to rain outside\" – in the sense of \"metaphysical possibility\" – then I am no better off for this bit of modal enlightenment.\n\nSome features of epistemic modal logic are in debate. For example, if \"x\" knows that \"p\", does \"x\" know that it knows that \"p\"? That is to say, should □\"P\" → □□\"P\" be an axiom in these systems? While the answer to this question is unclear, there is at least one axiom that is generally included in epistemic modal logic, because it is minimally true of all normal modal logics (see the section on axiomatic systems):\n\nIt has been questioned whether the epistemic and alethic modalities should be considered distinct from each other. The criticism states that there is no real difference between \"the truth in the world\" (alethic) and \"the truth in an individual's mind\" (epistemic). An investigation has not found a single language in which alethic and epistemic modalities are formally distinguished, as by the means of a grammatical mood.\n\nTemporal logic is an approach to the semantics of expressions with tense, that is, expressions with qualifications of when. Some expressions, such as '2 + 2 = 4', are true at all times, while tensed expressions such as 'John is happy' are only true sometimes.\n\nIn temporal logic, tense constructions are treated in terms of modalities, where a standard method for formalizing talk of time is to use \"two\" pairs of operators, one for the past and one for the future (P will just mean 'it is presently the case that P'). For example:\n\nThere are then at least three modal logics that we can develop. For example, we can stipulate that,\n\nOr we can trade these operators to deal only with the future (or past). For example,\n\nor,\n\nThe operators F and G may seem initially foreign, but they create normal modal systems. Note that FP\" is the same as ¬G¬\"P\". We can combine the above operators to form complex statements. For example, PP\" → □P\"P\" says (effectively), \"Everything that is past and true is necessary\".\n\nIt seems reasonable to say that possibly it will rain tomorrow, and possibly it won't; on the other hand, since we can't change the past, if it is true that it rained yesterday, it probably isn't true that it may not have rained yesterday. It seems the past is \"fixed\", or necessary, in a way the future is not. This is sometimes referred to as accidental necessity. But if the past is \"fixed\", and everything that is in the future will eventually be in the past, then it seems plausible to say that future events are necessary too.\n\nSimilarly, the problem of future contingents considers the semantics of assertions about the future: is either of the propositions 'There will be a sea battle tomorrow', or 'There will not be a sea battle tomorrow' now true? Considering this thesis led Aristotle to reject the principle of bivalence for assertions concerning the future.\n\nAdditional binary operators are also relevant to temporal logics, \"q.v.\" Linear Temporal Logic.\n\nVersions of temporal logic can be used in computer science to model computer operations and prove theorems about them. In one version, ◇\"P\" means \"at a future time in the computation it is possible that the computer state will be such that P is true\"; □\"P\" means \"at all future times in the computation P will be true\". In another version, ◇\"P\" means \"at the immediate next state of the computation, \"P\" might be true\"; □\"P\" means \"at the immediate next state of the computation, P will be true\". These differ in the choice of Accessibility relation. (P always means \"P is true at the current computer state\".) These two examples involve nondeterministic or not-fully-understood computations; there are many other modal logics specialized to different types of program analysis. Each one naturally leads to slightly different axioms.\n\nLikewise talk of morality, or of obligation and norms generally, seems to have a modal structure. The difference between \"You must do this\" and \"You may do this\" looks a lot like the difference between \"This is necessary\" and \"This is possible\". Such logics are called \"deontic\", from the Greek for \"duty\".\n\nDeontic logics commonly lack the axiom T semantically corresponding to the reflexivity of the accessibility relation in Kripke semantics: in symbols, formula_40. Interpreting □ as \"it is obligatory that\", T informally says that every obligation is true. For example, if it is obligatory not to kill others (i.e. killing is morally forbidden), then T implies that people actually do not kill others. The consequent is obviously false.\n\nInstead, using Kripke semantics, we say that though our own world does not realize all obligations, the worlds accessible to it do (i.e., T holds at these worlds). These worlds are called idealized worlds. \"P\" is obligatory with respect to our own world if at all idealized worlds accessible to our world, \"P\" holds. Though this was one of the first interpretations of the formal semantics, it has recently come under criticism.\n\nOne other principle that is often (at least traditionally) accepted as a deontic principle is \"D\", formula_41, which corresponds to the seriality (or extendability or unboundedness) of the accessibility relation. It is an embodiment of the Kantian idea that \"ought implies can\". (Clearly the \"can\" can be interpreted in various senses, e.g. in a moral or alethic sense.)\n\nWhen we try and formalize ethics with standard modal logic, we run into some problems. Suppose that we have a proposition \"K\": you have stolen some money, and another, \"Q\": you have stolen a small amount of money. Now suppose we want to express the thought that \"if you have stolen some money, it ought to be a small amount of money\". There are two likely candidates,\n\nBut (1) and \"K\" together entail □\"Q\", which says that it ought to be the case that you have stolen a small amount of money. This surely isn't right, because you ought not to have stolen anything at all. And (2) doesn't work either: If the right representation of \"if you have stolen some money it ought to be a small amount\" is (2), then the right representation of (3) \"if you have stolen some money then it ought to be a large amount\" is formula_44. Now suppose (as seems reasonable) that you ought not to steal anything, or formula_45. But then we can deduce formula_44 via formula_47 and formula_48 (the contrapositive of formula_49); so sentence (3) follows from our hypothesis (of course the same logic shows sentence (2)). But that can't be right, and is not right when we use natural language. Telling someone they should not steal certainly does not imply that they should steal large amounts of money if they do engage in theft.\n\n\"Doxastic logic\" concerns the logic of belief (of some set of agents). The term doxastic is derived from the ancient Greek \"doxa\" which means \"belief\". Typically, a doxastic logic uses □, often written \"B\", to mean \"It is believed that\", or when relativized to a particular agent s, \"It is believed by s that\".\n\nSignificantly, modal logics can be developed to accommodate most of these idioms; it is the fact of their common logical structure (the use of \"intensional\" sentential operators) that make them all varieties of the same thing.\n\nIn the most common interpretation of modal logic, one considers \"logically possible worlds\". If a statement is true in all possible worlds, then it is a necessary truth. If a statement happens to be true in our world, but is not true in all possible worlds, then it is a contingent truth. A statement that is true in some possible world (not necessarily our own) is called a possible truth.\n\nUnder this \"possible worlds idiom,\" to maintain that Bigfoot's existence is possible but not actual, one says, \"There is some possible world in which Bigfoot exists; but in the actual world, Bigfoot does not exist\". However, it is unclear what this claim commits us to. Are we really alleging the existence of possible worlds, every bit as real as our actual world, just not actual? Saul Kripke believes that 'possible world' is something of a misnomer – that the term 'possible world' is just a useful way of visualizing the concept of possibility. For him, the sentences \"you could have rolled a 4 instead of a 6\" and \"there is a possible world where you rolled a 4, but you rolled a 6 in the actual world\" are not significantly different statements, and neither commit us to the existence of a possible world. David Lewis, on the other hand, made himself notorious by biting the bullet, asserting that all merely possible worlds are as real as our own, and that what distinguishes our world as \"actual\" is simply that it is indeed our world – \"this\" world. That position is a major tenet of \"modal realism\". Some philosophers decline to endorse any version of modal realism, considering it ontologically extravagant, and prefer to seek various ways to paraphrase away these ontological commitments. Robert Adams holds that 'possible worlds' are better thought of as 'world-stories', or consistent sets of propositions. Thus, it is possible that you rolled a 4 if such a state of affairs can be described coherently.\n\nComputer scientists will generally pick a highly specific interpretation of the modal operators specialized to the particular sort of computation being analysed. In place of \"all worlds\", you may have \"all possible next states of the computer\", or \"all possible future states of the computer\".\n\nModal logics have begun to be used in areas of the humanities such as literature, poetry, art and history.\n\nNicholas Rescher has argued that Bertrand Russell rejected modal logic, and that this rejection led to the theory of modal logic languishing for decades. However, Jan Dejnozka has argued against this view, stating that a modal system which Dejnozka calls \"MDL\" is described in Russell's works, although Russell did believe the concept of modality to \"come from confusing propositions with propositional functions,\" as he wrote in \"The Analysis of Matter\".\n\nArthur Norman Prior warned Ruth Barcan Marcus to prepare well in the debates concerning quantified modal logic with Willard Van Orman Quine, due to the biases against modal logic.\n\n\n\n"}
{"id": "18555870", "url": "https://en.wikipedia.org/wiki?curid=18555870", "title": "Non-compact stencil", "text": "Non-compact stencil\n\nIn numerical mathematics, a non-compact stencil is a type of discretization method, where any node surrounding the node of interest may be used in the calculation. A non-compact stencil's computational time increases with an increase of layers of nodes used. Non-compact stencils may be compared to Compact stencils.\n\n\n"}
{"id": "1396948", "url": "https://en.wikipedia.org/wiki?curid=1396948", "title": "Particle filter", "text": "Particle filter\n\nParticle filters or Sequential Monte Carlo (SMC) methods are a set of genetic, Monte Carlo algorithms used to solve filtering problems arising in signal processing and Bayesian statistical inference. The filtering problem consists of estimating the internal states in dynamical systems when partial observations are made, and random perturbations are present in the sensors as well as in the dynamical system. The objective is to compute the posterior distributions of the states of some Markov process, given some noisy and partial observations. The term \"particle filters\" was first coined in 1996 by Del Moral in reference to mean field interacting particle methods used in fluid mechanics since the beginning of the 1960s. The terminology \"sequential Monte Carlo\" was proposed by Liu and Chen in 1998.\n\nParticle filtering uses a genetic mutation-selection sampling approach, with a set of particles (also called samples) to represent the posterior distribution of some stochastic process given noisy and/or partial observations. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. Particle filter techniques provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. However, these methods do not perform well when applied to very high-dimensional systems.\n\nParticle filters implement the prediction-updating transitions of the filtering equation directly by using a genetic type mutation-selection particle algorithm. The samples from the distribution are represented by a set of particles; each particle has a likelihood weight assigned to it that represents the probability of that particle being sampled from the probability density function. Weight disparity leading to weight collapse is a common issue encountered in these filtering algorithms; however it can be mitigated by including a resampling step before the weights become too uneven. Several adaptive resampling criteria can be used, including the variance of the weights and the relative entropy with respect to the uniform distribution. In the resampling step, the particles with negligible weights are replaced by new particles in the proximity of the particles with higher weights.\n\nFrom the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures. These particle integration techniques were developed in molecular chemistry and computational physics by Theodore E. Harris and Herman Kahn in 1951, Marshall N. Rosenbluth and Arianna W. Rosenbluth in 1955 and more recently by Jack H. Hetherington in 1984. In computational physics, these Feynman-Kac type path particle integration methods are also used in Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods. Feynman-Kac interacting particle methods are also strongly related to mutation-selection genetic algorithms currently used in evolutionary computing to solve complex optimization problems.\n\nThe particle filter methodology is used to solve Hidden Markov Model (HMM) and nonlinear filtering problems. With the notable exception of linear-Gaussian signal-observation models (Kalman filter) or wider classes of models (Benes filter) Mireille Chaleyat-Maurel and Dominique Michel proved in 1984 that the sequence of posterior distributions of the random states of the signal given the observations (a.k.a. optimal filter) have no finitely recursive recursion. Various numerical methods based on fixed grid approximations, Markov Chain Monte Carlo techniques (MCMC), conventional linearization, extended Kalman filters, or determining the best linear system (in expect cost-error sense) have never really coped with large scale systems, unstable processes or when the nonlinearities are not sufficiently smooth.\n\nParticle filters and Feynman-Kac particle methodologies find application in signal and image processing, Bayesian inference, machine learning, risk analysis and rare event sampling, engineering and robotics, artificial intelligence, bioinformatics, phylogenetics, computational science, Economics and mathematical finance, molecular chemistry, computational physics, pharmacokinetic and other fields.\n\nFrom the statistical and probabilistic viewpoint, particle filters belong to the class of branching/genetic type algorithms, and mean field type interacting particle methodologies. The interpretation of these particle methods depends on the scientific discipline. In Evolutionary Computing, mean field genetic type particle methodologies are often used as a heuristic and natural search algorithms (a.k.a. Metaheuristic). In computational physics and molecular chemistry they are used to solve Feynman-Kac path integration problems, or they compute Boltzmann-Gibbs measures, top eigenvalues and ground states of Schrödinger operators. In Biology and Genetics they also represent the evolution of a population of individuals or genes in some environment.\n\nThe origins of mean field type evolutionary computational techniques can be traced to 1950 and 1954 with the seminal work of Alan Turing on genetic type mutation-selection learning machines and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton, New Jersey. The first trace of particle filters in statistical methodology dates back to the mid-50's; the 'Poor Man's Monte Carlo', that was proposed by Hammersley et al., in 1954, contained hints of the genetic type particle filtering methods used today. In 1963, Nils Aall Barricelli simulated a genetic type algorithm to mimic the ability of individuals to play a simple game. In evolutionary computing literature, genetic type mutation-selection algorithms became popular through the seminal work of John Holland in the early 1970s, and particularly his book published in 1975.\n\nIn Biology and Genetics, the Australian geneticist Alex Fraser also published in 1957 a series of papers on the genetic type simulation of artificial selection of organisms. The computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern mutation-selection genetic particle algorithms.\n\nFrom the mathematical viewpoint, the conditional distribution of the random states of a signal given some partial and noisy observations is described by a Feynman-Kac probability on the random trajectories of the signal weighted by a sequence of likelihood potential functions. Quantum Monte Carlo, and more specifically Diffusion Monte Carlo methods can also be interpreted as a mean field genetic type particle approximation of Feynman-Kac path integrals. The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions, but the first heuristic-like and genetic type particle algorithm (a.k.a. Resampled or Reconfiguration Monte Carlo methods) for estimating ground state energies of quantum systems (in reduced matrix models) is due to Jack H. Hetherington in 1984. We also quote an earlier seminal works of Theodore E. Harris and Herman Kahn in particle physics, published in 1951, using mean field but heuristic-like genetic methods for estimating particle transmission energies. In molecular chemistry, the use of genetic heuristic-like particle methodologies (a.k.a. pruning and enrichment strategies) can be traced back to 1955 with the seminal work of Marshall. N. Rosenbluth and Arianna. W. Rosenbluth.\n\nThe use of genetic particle algorithms in advanced signal processing and Bayesian inference is more recent. It was in 1993, that Gordon et al., published in their seminal work the first application of genetic type algorithm in Bayesian statistical inference. The authors named their algorithm 'the bootstrap filter', and demonstrated that compared to other filtering methods, their bootstrap algorithm does not require any assumption about that state-space or the noise of the system. We also quote another pioneering article in this field of Genshiro Kitagawa on a related \"Monte Carlo filter\", and the ones by Pierre Del Moral and Himilcon Carvalho, Pierre Del Moral, André Monin and Gérard Salut on particle filters published in the mid-1990s. Particle filters were also developed in signal processing in the early 1989-1992 by P. Del Moral, J.C. Noyer, G. Rigal, and G. Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN (Service Technique des Constructions et Armes Navales), the IT company DIGILOG, and the LAAS-CNRS (the Laboratory for Analysis and Architecture of Systems) on RADAR/SONAR and GPS signal processing problems.\n\nFrom 1950 to 1996, all the publications on particle filters, genetic algorithms, including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry, present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency, nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms.\n\nThe mathematical foundations and the first rigorous analysis of these particle algorithms are due to Pierre Del Moral in 1996. The article also contains a proof of the unbiased properties of a particle approximations of likelihood functions and unnormalized conditional probability measures. The unbiased particle estimator of the likelihood functions presented in this article is used today in Bayesian statistical inference.\n\nBranching type particle methodologies with varying population sizes were also developed toward the end of the 1990s by Dan Crisan, Jessica Gaines and Terry Lyons, and by Dan Crisan, Pierre Del Moral and Terry Lyons. Further developments in this field were developed in 2000 by P. Del Moral, A. Guionnet and L. Miclo. The first central limit theorems are due to Pierre Del Moral and Alice Guionnet in 1999 and Pierre Del Moral and Laurent Miclo in 2000. The first uniform convergence results with respect to the time parameter for particle filters were developed in the end of the 1990s by Pierre Del Moral and Alice Guionnet. The first rigorous analysis of genealogical tree based particle filter smoothers is due to P. Del Moral and L. Miclo in 2001\n\nThe theory on Feynman-Kac particle methodologies and related particle filters algorithms has been developed in 2000 and 2004 in the books. These abstract probabilistic models encapsulate genetic type algorithms, particle and bootstrap filters, interacting Kalman filters (a.k.a. Rao–Blackwellized particle filter), importance sampling and resampling style particle filter techniques, including genealogical tree based and particle backward methodologies for solving filtering and smoothing problems. Other classes of particle filtering methodologies includes genealogical tree based models, backward Markov particle models, adaptive mean field particle models, island type particle models, and particle Markov chain Monte Carlo methodologies.\n\nThe objective of a particle filter is to estimate the posterior density of the state variables given the observation variables. The particle filter is designed for a hidden Markov Model, where the system consists of hidden and observable variables. The observable variables (observation process) are related to the hidden variables (state-process) by some functional form that is known. Similarly the dynamical system describing the evolution of the state variables is also known probabilistically.\n\nA generic particle filter estimates the posterior distribution of the hidden states using the observation measurement process. Consider a state-space shown in the diagram below.\n\nThe filtering problem is to estimate sequentially the values of the hidden states formula_2, given the values of the observation process formula_3 at any time step \"k\".\n\nAll Bayesian estimates of formula_2 follow from the posterior density \"p\"(\"x\" | \"y\",\"y\",…,\"y\"). The particle filter methodology provides an approximation of these conditional probabilities using the empirical measure associated with a genetic type particle algorithm. In contrast, the MCMC or importance sampling approach would model the full posterior \"p\"(\"x\",\"x\",…,\"x\" | \"y\",\"y\",…,\"y\").\n\nParticle methods often assume formula_2 and the observations formula_6 can be modeled in this form:\n\nAn example of system with these properties is:\n\nwhere both formula_24 and formula_25 are mutually independent sequences with known probability density functions and \"g\" and \"h\" are known functions. These two equations can be viewed as state space equations and look similar to the state space equations for the Kalman filter. If the functions \"g\" and \"h\" in the above example are linear, and if both formula_24 and formula_25 are Gaussian, the Kalman filter finds the exact Bayesian filtering distribution. If not, Kalman filter based methods are a first-order approximation (EKF) or a second-order approximation (UKF in general, but if probability distribution is Gaussian a third-order approximation is possible).\n\nThe assumption that the initial distribution and the transitions of the Markov chain are absolutely continuous with respect to the Lebesgue measure can be relaxed. To design a particle filter we simply need to assume that we can sample the transitions formula_28 of the Markov chain formula_29 and to compute the likelihood function formula_30 (see for instance the genetic selection mutation description of the particle filter given below). The absolutely continuous assumption on the Markov transitions of formula_2 are only used to derive in an informal (and rather abusive) way different formulae between posterior distributions using the Bayes' rule for conditional densities.\n\nIn some important problems, the conditional distribution of the observations given the random states of the signal may fail to have a density or may be impossible or too complex to compute. In this situation, we need to resort to an additional level of approximation. One strategy is to replace the signal formula_2 by the Markov chain formula_33 and to introduce a virtual observation of the form\n\nfor some sequence of independent sequences with known probability density functions. The central idea is to observe that\n\nThe particle filter associated with the Markov process formula_33 given the partial observations formula_37 is defined in terms of particles evolving in formula_38 with a likelihood function given with some obvious abusive notation by formula_39. These probabilistic techniques are closely related to Approximate Bayesian Computation (ABC). In the context of particle filters, these ABC particle filtering techniques were introduced in 1998 by P. Del Moral, J. Jacod and P. Protter. They were further developed by P. Del Moral, A. Doucet and A. Jasra.\n\nThe Bayes' rule for conditional probability gives:\n\nwhere\n\nParticle filters are also an approximation, but with enough particles they can be much more accurate. The nonlinear filtering equation is given by the recursion\n\n&\\stackrel{\\text{prediction}}{\\longrightarrow}p(x_{k+1}|y_0,\\cdots,y_k)=\\int p(x_{k+1}|x_k) p(x_k|y_0,\\cdots,y_k) dx_k\n\\end{align}</math>\n\nwith the convention formula_42 for \"k\" = 0. The nonlinear filtering problem consists in computing these conditional distributions sequentially.\n\nWe fix a time horizon n and a sequence of observations formula_43, and for each \"k\" = 0, ..., \"n\" we set:\n\nIn this notation, for any bounded function \"F\" on the set of trajectories of formula_2 from the origin \"k\" = 0 up to time \"k\" = \"n\", we have the Feynman-Kac formula\n\nThese Feynman-Kac path integration models arise in a variety of scientific disciplines, including in computational physics, biology, information theory and computer sciences. Their interpretations depend on the application domain. For instance, if we choose the indicator function formula_47 of some subset of the state space, they represent the conditional distribution of a Markov chain given it stays in a given tube; that is, we have:\n\nand\n\nas soon as the normalizing constant is strictly positive.\n\nInitially we start with \"N\" independent random variables formula_50 with common probability density formula_12. The genetic algorithm selection-mutation transitions\n\nmimic/approximate the updating-prediction transitions of the optimal filter evolution ():\n\n\nIn the above displayed formulae formula_57 stands for the likelihood function formula_30 evaluated at formula_59, and formula_60 stands for the conditional density formula_61 evaluated at formula_62.\n\nAt each time \"k\", we have the particle approximations\n\nand\n\nA detailed proof of these convergence results can be found in, see also the more recent developments provided in the books. In Genetic algorithms and Evolutionary computing community, the mutation-selection Markov chain described above is often called the genetic algorithm with proportional selection. Several branching variants, including with random population sizes have also been proposed in the articles.\n\nParticle methods, like all sampling-based approaches (e.g., MCMC), generate a set of samples that approximate the filtering density\n\nFor example, we may have \"N\" samples from the approximate posterior distribution of formula_2, where the samples are labeled with superscripts as\n\nThen, expectations with respect to the filtering distribution are approximated by\n\nwith\n\nwhere formula_69 stands for the Dirac measure at a given state a. The function \"f\", in the usual way for Monte Carlo, can give all the moments etc. of the distribution up to some approximation error. When the approximation equation () is satisfied for any bounded function \"f\" we write\n\nParticle filters can be interpreted as a genetic type particle algorithm evolving with mutation and selection transitions. We can keep track of the ancestral lines\n\nof the particles formula_72. The random states formula_73, with the lower indices l=0...,k, stands for the ancestor of the individual formula_74 at level l=0...,k. In this situation, we have the approximation formula\n\nwith the empirical measure\n\nHere \"F\" stands for any founded function on the path space of the signal. In a more synthetic form () is equivalent to\n\nParticle filters can be interpreted in many different ways. From the probabilistic point of view they coincide with a mean field particle interpretation of the nonlinear filtering equation. The updating-prediction transitions of the optimal filter evolution can also be interpreted as the classical genetic type selection-mutation transitions of individuals. The sequential importance resampling technique provides another interpretation of the filtering transitions coupling importance sampling with the bootstrap resampling step. Last, but not least, particle filters can be seen as an acceptance-rejection methodology equipped with a recycling mechanism.\n\nThe nonlinear filtering evolution can be interpreted as a dynamical system in the set of probability measures of the following form formula_77 where formula_78 stands for some mapping from the set of probability distribution into itself. For instance, the evolution of the one-step optimal predictor formula_79\n\nsatisfies a nonlinear evolution starting with the probability distribution formula_80. One of the simplest ways to approximate these probability measures is to start with \"N\" independent random variables formula_50 with common probability distribution formula_80 . Suppose we have defined a sequence of \"N\" random variables formula_83 such that\n\nAt the next step we sample \"N\" (conditionally) independent random variables formula_85 with common law .\n\nWe illustrate this mean field particle principle in the context of the evolution of the one step optimal predictors\n\n</math>\n\nFor \"k\" = 0 we use the convention formula_87.\n\nBy the law of large numbers, we have\n\nin the sense that\n\nfor any bounded function formula_90. We further assume that we have constructed a sequence of particles formula_91 at some rank \"k\" such that\n\nin the sense that for any bounded function formula_90 we have\n\nIn this situation, replacing formula_95 by the empirical measure formula_96 in the evolution equation of the one-step optimal filter stated in () we find that\n\nNotice that the right hand side in the above formula is a weighted probability mixture\n\nwhere formula_57 stands for the density formula_100 evaluated at formula_59, and formula_102 stands for the density formula_61 evaluated at formula_59 for formula_105\n\nThen, we sample \"N\" independent random variable formula_106 with common probability density formula_107 so that\n\nIterating this procedure, we design a Markov chain such that\n\nNotice that the optimal filter is approximated at each time step k using the Bayes' formulae\n\nThe terminology \"mean field approximation\" comes from the fact that we replace at each time step the probability measure formula_111 by the empirical approximation formula_112. The mean field particle approximation of the filtering problem is far from being unique. Several strategies are developed in the books.\n\nThe analysis of the convergence of particle filters was started in 1996 and in 2000 in the book and the series of articles. More recent developments can be found in the books, When the filtering equation is stable (in the sense that it corrects any erroneous initial condition), the bias and the variance of the particle particle estimates\n\nare controlled by the non asymptotic uniform estimates\n\nfor any function \"f\" bounded by 1, and for some finite constants formula_116 In addition, for any formula_117:\n\nfor some finite constants formula_119 related to the asymptotic bias and variance of the particle estimate, and some finite constant \"c\". The same results are satisfied if we replace the one step optimal predictor by the optimal filter approximation.\n\nTracing back in time the ancestral lines\n\nof the individuals formula_121 and formula_122 at every time step \"k\", we also have the particle approximations\n\nThese empirical approximations are equivalent to the particle integral approximations\n\nfor any bounded function \"F\" on the random trajectories of the signal. As shown in the evolution of the genealogical tree coincides with a mean field particle interpretation of the evolution equations associated with the posterior densities of the signal trajectories. For more details on these path space models, we refer to the books.\n\nWe use the product formula\n\nwith\n\nand the conventions formula_127 and formula_128 for \"k\" = 0. Replacing formula_129 by the empirical approximation\n\nin the above displayed formula, we design the following unbiased particle approximation of the likelihood function\n\nwith\n\nwhere formula_57 stands for the density formula_100 evaluated at formula_59. The design of this particle estimate and the unbiasedness property has been proved in 1996 in the article. Refined variance estimates can be found in and.\n\nUsing Bayes' rule, we have the formula\n\nNotice that\n\nThis implies that\n\nReplacing the one-step optimal predictors formula_139 by the particle empirical measures\n\nwe find that\n\nWe conclude that\n\nwith the backward particle approximation\n\nThe probability measure\n\nis the probability of the random paths of a Markov chain formula_145running backward in time from time k=n to time k=0, and evolving at each time step k in the state space associated with the population of particles formula_146\n\nIn the above displayed formula, formula_153 stands for the conditional distribution formula_154 evaluated at formula_155. In the same vein, formula_156 and formula_157 stand for the conditional densities formula_158 and formula_10 evaluated at formula_155 and formula_161 These models allows to reduce integration with respect to the densities formula_162 in terms of matrix operations with respect to the Markov transitions of the chain described above. For instance, for any function formula_163 we have the particle estimates\n\nwhere\n\nThis also shows that if\n\nthen\n\nWe shall assume that filtering equation is stable, in the sense that it corrects any erroneous initial condition.\n\nIn this situation, the particle approximations of the likelihood functions are unbiased and the relative variance is controlled by\n\nfor some finite constant \"c\". In addition, for any formula_117:\n\nfor some finite constants formula_119 related to the asymptotic bias and variance of the particle estimate, and for some finite constant \"c\".\n\nThe bias and the variance of the particle particle estimates based on the ancestral lines of the genealogical trees\n\nare controlled by the non asymptotic uniform estimates\n\nfor any function \"F\" bounded by 1, and for some finite constants formula_174 In addition, for any formula_117:\n\nfor some finite constants formula_119 related to the asymptotic bias and variance of the particle estimate, and for some finite constant \"c\". The same type of bias and variance estimates hold for the backward particle smoothers. For additive functionals of the form\n\nwith\n\nwith functions formula_163 bounded by 1, we have\n\nand\n\nfor some finite constants formula_183 More refined estimates including exponentially small probability of errors are developed in.\n\n\"Sequential importance Resampling (SIR)\", the original bootstrap filtering algorithm (Gordon et al. 1993), is also a very commonly used filtering algorithm, which approximates the filtering probability density formula_184 by a weighted set of \"N\" samples\n\nThe \"importance weights\" formula_186 are approximations to the relative posterior probabilities (or densities) of the samples such that\n\nSequential importance sampling (SIS) is a sequential (i.e., recursive) version of importance sampling. As in importance sampling, the expectation of a function \"f\" can be approximated as a weighted average\n\nFor a finite set of samples, the algorithm performance is dependent on the choice of the \"proposal distribution\"\n\nThe \"\"optimal\" proposal distribution\" is given as the \"target distribution\"\n\nThis particular choice of proposal transition has been proposed by P. Del Moral in 1996 and 1998. When it is difficult to sample transitions according to the distribution formula_191 one natural strategy is to use the following particle approximation\n\nwith the empirical approximation\n\nassociated with \"N\" (or any other large number of samples) independent random samples formula_194with the conditional distribution of the random state formula_2 given formula_196. The consistency of the resulting particle filter of this approximation and other extensions are developed in. In the above display formula_69 stands for the Dirac measure at a given state a.\n\nHowever, the transition prior probability distribution is often used as importance function, since it is easier to draw particles (or samples) and perform subsequent importance weight calculations:\n\"Sequential Importance Resampling\" (SIR) filters with transition prior probability distribution as importance function are commonly known as bootstrap filter and condensation algorithm.\n\n\"Resampling\" is used to avoid the problem of degeneracy of the algorithm, that is, avoiding the situation that all but one of the importance weights are close to zero. The performance of the algorithm can be also affected by proper choice of resampling method. The \"stratified sampling\" proposed by Kitagawa (1996) is optimal in terms of variance.\n\nA single step of sequential importance resampling is as follows:\n\nThe term \"Sampling Importance Resampling\" is also sometimes used when referring to SIR filters.\n\n\nThe \"direct version\" algorithm is rather simple (compared to other particle filtering algorithms) and it uses composition and rejection. To generate a single sample \"x\" at \"k\" from formula_211:\n\nThe goal is to generate P \"particles\" at \"k\" using only the particles from formula_225. This requires that a Markov equation can be written (and computed) to generate a formula_226 based only upon formula_227. This algorithm uses composition of the P particles from formula_225 to generate a particle at \"k\" and repeats (steps 2–6) until P particles are generated at \"k\".\n\nThis can be more easily visualized if \"x\" is viewed as a two-dimensional array. One dimension is \"k\" and the other dimensions is the particle number. For example, formula_229 would be the i particle at formula_230 and can also be written formula_231 (as done above in the algorithm). Step 3 generates a \"potential\" formula_226 based on a randomly chosen particle (formula_233) at time formula_225 and rejects or accepts it in step 6. In other words, the formula_226 values are generated using the previously generated formula_227.\n\n\n\n\n"}
{"id": "23670", "url": "https://en.wikipedia.org/wiki?curid=23670", "title": "Perfect number", "text": "Perfect number\n\nIn number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum). Equivalently, a perfect number is a number that is half the sum of all of its positive divisors (including itself) i.e. \"σ\"(\"n\") = 2\"n\".\n\nThis definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called (\"perfect\", \"ideal\", or \"complete number\"). Euclid also proved a formation rule (IX.36) whereby formula_1 is an even perfect number whenever formula_2 is a prime of the form formula_3 for prime formula_4—what is now called a Mersenne prime. Two millenia later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.\n\nIt is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.\n\nThe first perfect number is 6. Its proper divisors are 1, 2, and 3, and 1 + 2 + 3 = 6. Equivalently, the number 6 is equal to half the sum of all its positive divisors: ( 1 + 2 + 3 + 6 ) ÷ 2 = 6. The next perfect number is 28: 28 = 1 + 2 + 4 + 7 + 14. This is followed by the perfect numbers 496 and 8128 .\n\nIn about 300 BC Euclid showed that if 2 − 1 is prime then 2(2 − 1) is perfect.\nThe first four perfect numbers were the only ones known to early Greek mathematics, and the mathematician Nicomachus had noted 8128 as early as 100 AD. Philo of Alexandria in his first-century book \"On the creation\" mentions perfect numbers, claiming that the world was created in 6 days and the moon orbits in 28 days because 6 and 28 are perfect. Philo is followed by Origen, and by Didymus the Blind, who adds the observation that there are only four perfect numbers that are less than 10,000. (Commentary on Genesis 1. 14-19). St Augustine defines perfect numbers in City of God (Part XI, Chapter 30) in the early 5th century AD, repeating the claim that God created the world in 6 days because 6 is the smallest perfect number. The Egyptian mathematician Ismail ibn Fallūs (1194–1252) mentioned the next three perfect numbers (33,550,336; 8,589,869,056; and 137,438,691,328) and listed a few more which are now known to be incorrect. In a manuscript written between 1456 and 1461, an unknown mathematician recorded the earliest European reference to a fifth perfect number, with 33,550,336 being correctly identified for the first time. In 1588, the Italian mathematician Pietro Cataldi also identified the sixth (8,589,869,056) and the seventh (137,438,691,328) perfect numbers, and also proved that every perfect number obtained from Euclid's rule ends with a 6 or an 8.\n\nEuclid proved that 2(2 − 1) is an even perfect number whenever 2 − 1 is prime (Elements, Prop. IX.36).\n\nFor example, the first four perfect numbers are generated by the formula 2(2 − 1), with \"p\" a prime number, as follows:\n\nPrime numbers of the form 2 − 1 are known as Mersenne primes, after the seventeenth-century monk Marin Mersenne, who studied number theory and perfect numbers. For 2 − 1 to be prime, it is necessary that \"p\" itself be prime. However, not all numbers of the form 2 − 1 with a prime \"p\" are prime; for example, 2 − 1 = 2047 = 23 × 89 is not a prime number. In fact, Mersenne primes are very rare—of the 2,610,944 prime numbers \"p\" up to 43,112,609, \n2 − 1 is prime for only 47 of them.\n\nNicomachus (60–120 AD) conjectured that every perfect number is of the form 2(2 − 1) where 2 − 1 is prime. Ibn al-Haytham (Alhazen) circa 1000 AD conjectured that every \"even\" perfect number is of that form. It was not until the 18th century that Leonhard Euler proved that the formula 2(2 − 1) will yield all the even perfect numbers. Thus, there is a one-to-one correspondence between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa. This result is often referred to as the Euclid–Euler theorem.\n\nAn exhaustive search by the GIMPS distributed computing project has shown that the first 47 even perfect numbers are 2(2 − 1) for\nThree higher perfect numbers have also been discovered, namely those for which \"p\" = 57885161, 74207281, and 77232917, though there may be others within this range. , 50 Mersenne primes are known, and therefore 50 even perfect numbers (the largest of which is 2 × (2 − 1) with 46,498,850 digits). It is not known whether there are infinitely many perfect numbers, nor whether there are infinitely many Mersenne primes.\n\nAs well as having the form 2(2 − 1), each even perfect number is the triangular number (and hence equal to the sum of the integers from 1 to ) and the hexagonal number. Furthermore, each even perfect number except for 6 is the centered nonagonal number and is equal to the sum of the first odd cubes:\n\nEven perfect numbers (except 6) are of the form\n\nwith each resulting triangular number T = 28, T = 496, T = 8128 (after subtracting 1 from the perfect number and dividing the result by 9) ending in 3 or 5, the sequence starting with T = 3, T = 55, T = 903, 3727815, ... This can be reformulated as follows: adding the digits of any even perfect number (except 6), then adding the digits of the resulting number, and repeating this process until a single digit (called the digital root) is obtained, always produces the number 1. For example, the digital root of 8128 is 1, because 8 + 1 + 2 + 8 = 19, 1 + 9 = 10, and 1 + 0 = 1. This works with all perfect numbers 2(2 − 1) with odd prime \"p\" and, in fact, with all numbers of the form 2(2 − 1) for odd integer (not necessarily prime) \"m\".\n\nOwing to their form, 2(2 − 1), every even perfect number is represented in binary form as \"p\" ones followed by \"p\" − 1  zeros; for example,\nand\nThus every even perfect number is a pernicious number.\n\nNote that every even perfect number is also a practical number (c.f. Related concepts).\n\nIt is unknown whether there is any odd perfect number, though various results have been obtained. In 1496, Jacques Lefèvre stated that Euclid's rule gives all perfect numbers, thus implying that no odd perfect number exists. Euler stated: \"Whether (...) there are any odd perfect numbers is a most difficult question\".<br>\nMore recently, Carl Pomerance has presented a heuristic argument suggesting that indeed no odd perfect number should exist. All perfect numbers are also Ore's harmonic numbers, and it has been conjectured as well that there are no odd Ore's harmonic numbers other than 1.\n\nAny odd perfect number \"N\" must satisfy the following conditions:\n\nIn 1888, Sylvester stated:\nAll even perfect numbers have a very precise form; odd perfect numbers either do not exist or are rare. There are a number of results on perfect numbers that are actually quite easy to prove but nevertheless superficially impressive; some of them also come under Richard Guy's strong law of small numbers:\n\nThe sum of proper divisors gives various other kinds of numbers. Numbers where the sum is less than the number itself are called deficient, and where it is greater than the number, abundant. These terms, together with \"perfect\" itself, come from Greek numerology. A pair of numbers which are the sum of each other's proper divisors are called amicable, and larger cycles of numbers are called sociable. A positive integer such that every smaller positive integer is a sum of distinct divisors of it is a practical number.\n\nBy definition, a perfect number is a fixed point of the restricted divisor function , and the aliquot sequence associated with a perfect number is a constant sequence. All perfect numbers are also formula_16-perfect numbers, or Granville numbers.\n\nA semiperfect number is a natural number that is equal to the sum of all or some of its proper divisors. A semiperfect number that is equal to the sum of all its proper divisors is a perfect number. Most abundant numbers are also semiperfect; abundant numbers which are not semiperfect are called weird numbers.\n\n\n\n"}
{"id": "10761147", "url": "https://en.wikipedia.org/wiki?curid=10761147", "title": "Reeb vector field", "text": "Reeb vector field\n\nIn mathematics, the Reeb vector field, named after the French mathematician Georges Reeb, is a notion that appears in various domains of contact geometry including:\n\n"}
{"id": "4281673", "url": "https://en.wikipedia.org/wiki?curid=4281673", "title": "Reich Technologies", "text": "Reich Technologies\n\nReich Technologies was one of the UML Partners, a consortium that was instrumental to the development of standards for the Unified Modeling Language (UML). The CEO for the company (Georges-Pierre Reich) represented Reich Technologies on the committee, and was involved in the development of the proposal. The proposal was submitted to the Object Management Group (OMG), which approved the proposal, circa late 1997.\n\nReich Technologies is an international group of companies, providing a coordinated suite of products and services to support object-oriented (OO) software development in large corporations. With a presence throughout Europe and North America, Reich Technologies occupies leading positions in the world markets for integrated OO CASE tools, fine-grained object repositories and OO team programming environments.\n\nThe Intelligent Software Factory (ISF) offers an integrated object-oriented CASE tool suite. It is built on the concept of model-driven development in which the work done at the beginning of a project creates an environment for configuration management and cost containment for software maintenance. ISF has been originally built by Franck Barbier, a French researcher on OO modeling.\n\nThe Intelligent Artifact Repository (IAR) provides an enterprise-wide resource for the management and reuse of Information System assets. This concept is so powerful that the development team uses ISF and IAR for production, making ISF the first CASE tool to be self-generated. Recognizing the impact of introducing tools, Reich Technologies offers success oriented services including training, consulting and tool customizations. Corporations combine tools, services and processes with their own organizations to implement a Corporate Software Ecology.\n\nReich Technologies worked with Alistair Cockburn (special advisor to the Central Bank of Norway) and Ralph Hodgson (founder of TopQuadrant) to flesh out the concept of Use Case and integrate it in the context of Responsibility-Driven Design. Several large companies have built systems upon these constructs since 1992. Structured Use Cases and detailed Responsibility models proved to be a relevant answer to the challenge of gathering and organizing thousands of requirements, defining the scope of the system, and designing an architecture for objects. A methodology with processes and identified deliverables has been created in a joint effort. \n\nAs tool builders, Reich Technologies adds the knowledge of implementing lifecycle management for the meta-model objects. Reich Technologies has also extensive experience designing the meta-models that implement in ISF the modeling notations of diverse methodologists.\n\nReich Technologies sells off-the-shelf and tailored versions of their CASE tools.\n\n"}
{"id": "44347068", "url": "https://en.wikipedia.org/wiki?curid=44347068", "title": "Reinsurance Actuarial Premium", "text": "Reinsurance Actuarial Premium\n\nActuarial reinsurance premium calculation uses the similar mathematical tools as actuarial insurance premium. Nevertheless, Catastrophe modeling, Systematic risk or risk aggregation statistics tools are more important.\n\nTypically burning cost is the estimated cost of claims in the forthcoming insurance period, calculated from previous years’ experience adjusted for changes in the numbers insured, the nature of cover and medical inflation.\n\n\n'As if' is term used to describe the recalculation of prior years of loss experience to demonstrate what the underwriting results of a particular program would have been if the proposed program had been in force during that period.\n\nLet us note formula_1 the and formula_2 the deductible of XS or XL, with the limite formula_3 (formula_1 XS formula_2).\n\nThe premium :\nformula_6\n\nwhere\nformula_7\n\nIf formula_8 and formula_9 :\nformula_10$\n\nif formula_8 and formula_12 il n'y a pas de solution.\nIf formula_13 and formula_9 :\nformula_15\nIf formula_13 and formula_12 :\nformula_18\n\nIf formula_19 follows formula_20 then formula_21 follows formula_22\n\nThen:\nformula_23\n\nformula_24\n\nWith deductible and without limit :\n\nformula_25\n\nActuarial reserves modellisation.\n\n2. [2] http://www.r-tutor.com/elementary-statistics/simple-linear-regression/estimated-simple-regression-equation"}
{"id": "252329", "url": "https://en.wikipedia.org/wiki?curid=252329", "title": "Sequent calculus", "text": "Sequent calculus\n\nSequent calculus is, in essence, a style of formal logical argumentation where every line of a proof is a conditional tautology (called a sequent by Gerhard Gentzen) instead of an unconditional tautology. Each conditional tautology is inferred from other conditional tautologies on earlier lines in a formal argument according to rules and procedures of inference, giving a better approximation to the style of natural deduction used by mathematicians than David Hilbert's earlier style of formal logic where every line was an unconditional tautology. There may be more subtle distinctions to be made; for example, there may be non-logical axioms upon which all propositions are implicitly dependent. Then sequents signify conditional theorems in a first-order language rather than conditional tautologies.\n\nSequent calculus is one of several extant styles of proof calculus for expressing line-by-line logical arguments.\nIn other words, natural deduction and sequent calculus systems are particular distinct kinds of Gentzen-style systems. Hilbert-style systems typically have a very small number of inference rules, relying more on sets of axioms. Gentzen-style systems typically have very few axioms, if any, relying more on sets of rules.\n\nGentzen-style systems have significant practical and theoretical advantages compared to Hilbert-style systems. For example, both natural deduction and sequent calculus systems facilitate the elimination and introduction of universal and existential quantifiers so that unquantified logical expressions can be manipulated according to the much simpler rules of propositional calculus. In a typical argument, quantifiers are eliminated, then propositional calculus is applied to unquantified expressions (which typically contain free variables), and then the quantifiers are reintroduced. This very much parallels the way in which mathematical proofs are carried out in practice by mathematicians. Predicate calculus proofs are generally much easier to discover with this approach, and are often shorter. Natural deduction systems are more suited to practical theorem-proving. Sequent calculus systems are more suited to theoretical analysis.\n\nIn proof theory and mathematical logic, sequent calculus is a family of formal systems sharing a certain style of inference and certain formal properties. The first sequent calculi systems, LK and LJ, were introduced in 1934/1935 by Gerhard Gentzen as a tool for studying natural deduction in first-order logic (in classical and intuitionistic versions, respectively). Gentzen's so-called \"Main Theorem\" (\"Hauptsatz\") about LK and LJ was the cut-elimination theorem, a result with far-reaching meta-theoretic consequences, including consistency. Gentzen further demonstrated the power and flexibility of this technique a few years later, applying a cut-elimination argument to give a (transfinite) proof of the consistency of Peano arithmetic, in surprising response to Gödel's incompleteness theorems. Since this early work, sequent calculi, also called Gentzen systems, and the general concepts relating to them, have been widely applied in the fields of proof theory, mathematical logic, and automated deduction.\n\nOne way to classify different styles of deduction systems is to look at the form of \"judgments\" in the system, \"i.e.\", which things may appear as the conclusion of a (sub)proof. The simplest judgment form is used in Hilbert-style deduction systems, where a judgment has the form\nwhere formula_1 is any formula of first-order-logic (or whatever logic the deduction system applies to, \"e.g.\", propositional calculus or a higher-order logic or a modal logic). The theorems are those formulae that appear as the concluding judgment in a valid proof. A Hilbert-style system needs no distinction between formulae and judgments; we make one here solely for comparison with the cases that follow.\n\nThe price paid for the simple syntax of a Hilbert-style system is that complete formal proofs tend to get extremely long. Concrete arguments about proofs in such a system almost always appeal to the deduction theorem. This leads to the idea of including the deduction theorem as a formal rule in the system, which happens in natural deduction.\n\nIn natural deduction, judgments have the shape\nwhere the formula_4's and formula_1 are again formulae and formula_6. Permutations of the formula_4's are immaterial. In other words, a judgment consists of a list (possibly empty) of formulae on the left-hand side of a turnstile symbol \"formula_8\", with a single formula on the right-hand side. The theorems are those formulae formula_1 such that formula_10 (with an empty left-hand side) is the conclusion of a valid proof.\n\nThe standard semantics of a judgment in natural deduction is that it asserts that whenever formula_12, formula_13, etc., are all true, formula_1 will also be true. The judgments\nand\nare equivalent in the strong sense that a proof of either one may be extended to a proof of the other.\n\nFinally, sequent calculus generalizes the form of a natural deduction judgment to\na syntactic object called a sequent. The formulas on left-hand side of the turnstile are called the \"antecedent\", and the formulas on right-hand side are called the \"succedent\" or \"consequent\"; together they are called \"cedents\" or \"sequents\". Again, formula_4 and formula_19 are formulae, and formula_20 and formula_21 are nonnegative integers, that is, the left-hand-side or the right-hand-side (or neither or both) may be empty. As in natural deduction, theorems are those formula_1 where formula_10 is the conclusion of a valid proof.\n\nThe standard semantics of a sequent is an assertion that whenever \"every\" formula_24 is true, \"at least one\" formula_19 will also be true. Thus the empty sequent, having both cedents empty, is false. One way to express this is that a comma to the left of the turnstile should be thought of as an \"and\", and a comma to the right of the turnstile should be thought of as an (inclusive) \"or\". The sequents\nand\nare equivalent in the strong sense that a proof of either one may be extended to a proof of the other.\n\nAt first sight, this extension of the judgment form may appear to be a strange complication — it is not motivated by an obvious shortcoming of natural deduction, and it is initially confusing that the comma seems to mean entirely different things on the two sides of the turnstile. However, in a classical context the semantics of the sequent can also (by propositional tautology) be expressed either as\n(at least one of the As is false, or one of the Bs is true) or as\n(it cannot be the case that all of the As are true and all of the Bs are false). In these formulations, the only difference between formulae on either side of the turnstile is that one side is negated. Thus, swapping left for right in a sequent corresponds to negating all of the constituent formulae. This means that a symmetry such as De Morgan's laws, which manifests itself as logical negation on the semantic level, translates directly into a left-right symmetry of sequents — and indeed, the inference rules in sequent calculus for dealing with conjunction (∧) are mirror images of those dealing with disjunction (∨).\n\nMany logicians feel that this symmetric presentation offers a deeper insight in the structure of the logic than other styles of proof system, where the classical duality of negation is not as apparent in the rules.\n\nGentzen asserted a sharp distinction between his single-output natural deduction systems (NK and NJ) and his multiple-output sequent calculus systems (LK and LJ). He wrote that the intuitionistic natural deduction system NJ was somewhat ugly. He said that the special role of the excluded middle in the classical natural deduction system NK is removed in the classical sequent calculus system LK. He said that the sequent calculus LJ gave more symmetry than natural deduction NJ in the case of intuitionistic logic, as also in the case of classical logic (LK versus NK). Then he said that in addition to these reasons, the sequent calculus with multiple succedent formulas is intended particularly for his principal theorem (\"Hauptsatz\").\n\nThe word \"sequent\" is taken from the word \"Sequenz\" in Gentzen's 1934 paper. Kleene makes the following comment on the translation into English: \"Gentzen says 'Sequenz', which we translate as 'sequent', because we have already used 'sequence' for any succession of objects, where the German is 'Folge'.\"\n\nSequent calculus can be seen as a tool for proving formulas in propositional logic, similar to the method of analytic tableaux. It gives a series of steps which allows one to reduce the problem of proving a logical formula to simpler and simpler formulas until one arrives at trivial ones.\n\nConsider the following formula:\n\nThis is written in the following form, where the proposition that needs to be proven is to the right of the turnstile symbol formula_8:\n\nNow, instead of proving this from the axioms, it is enough to assume the premise of the implication and then try to prove its conclusion. Hence one moves to the following sequent:\n\nAgain the right hand side includes an implication, whose premise can further be assumed so that only its conclusion needs to be proven:\n\nSince the arguments in the left-hand side are assumed to be related by conjunction, this can be replaced by the following:\n\nThis is equivalent to proving the conclusion in both cases of the disjunction on the first argument on the left. Thus we may split the sequent to two, where we now have to prove each separately:\n\nIn the case of the first judgment, we rewrite formula_38 as formula_39 and split the sequent again to get:\n\nThe second sequent is done; the first sequent can be further simplified into:\n\nThis process can always be continued until there are only atomic formulas in each side. \nThe process can be graphically described by a rooted tree graph, as depicted on the right. The root of the tree is the formula we wish to prove; the leaves consist of atomic formulas only. The tree is known as a reduction tree.\n\nThe items to the left of the turnstile are understood to be connected by conjunction, and those to the right by disjunction. Therefore, when both consist only of atomic symbols, the sequent is provable (and always true) if and only if at least one of the symbols on the right also appears on the left.\n\nFollowing are the rules by which one proceeds along the tree. Whenever one sequent is split into two, the tree vertex has three edges (one coming from the vertex closer to the root), and the tree is branched. Additionally, one may freely change the order of the arguments in each side; Γ and Δ stand for possible additional arguments.\n\nThe usual term for the horizontal line used in Gentzen-style layouts for natural deduction is inference line.\n\nStarting with any formula in propositional logic, by a series of steps, the right side of the turnstile can be processed until it includes only atomic symbols. Then, the same is done for the left side. Since every logical operator appears in one of the rules above, and is omitted by the rule, the process terminates when no logical operators remain: The formula has been \"decomposed\".\n\nThus, the sequents in the leaves of the trees include only atomic symbols, which are either provable by the axiom or not, according to whether one of the symbols on the right also appears on the left.\n\nIt is easy to see that the steps in the tree preserve the semantic truth value of the formulas implied by them, with conjunction understood between the tree's different branches whenever there is a split. It is also obvious that an axiom is provable if and only if it is true for every truth values of the atomic symbols. Thus this system is sound and complete in propositional logic.\n\nSequent calculus is related to other axiomatizations of propositional calculus, such as Frege's propositional calculus or Jan Łukasiewicz's axiomatization (itself a part of the standard Hilbert system): Every formula that can be proven in these has a reduction tree.\n\nThis can be shown as follows: Every proof in propositional calculus uses only axioms and the inference rules. Each use of an axiom scheme yields a true logical formula, and can thus be proven in sequent calculus; examples for these are shown below. The only inference rule in the systems mentioned above is modus ponens, which is implemented by the cut rule.\n\nThis section introduces the rules of the sequent calculus LK (which just stands for “klassische Prädikatenlogik”), as introduced by Gentzen in 1934.\nA (formal) proof in this calculus is a sequence of sequents, where each of the sequents is derivable from sequents appearing earlier in the sequence by using one of the rules below.\n\nThe following notation will be used:\n\nNote that, contrary to the rules for proceeding along the reduction tree presented above, the following rules are for moving in the opposite directions, from axioms to theorems. Thus they are exact mirror-images of the rules above, except that here symmetry is not implicitly assumed, and rules regarding quantification are added.\n\n\"Restrictions: In the rules formula_70 and formula_71, the variable formula_52 must not occur free anywhere in the respective lower sequents.\"\n\nThe above rules can be divided into two major groups: \"logical\" and \"structural\" ones. Each of the logical rules introduces a new logical formula either on the left or on the right of the turnstile formula_8. In contrast, the structural rules operate on the structure of the sequents, ignoring the exact shape of the formulae. The two exceptions to this general scheme are the axiom of identity (I) and the rule of (Cut).\n\nAlthough stated in a formal way, the above rules allow for a very intuitive reading in terms of classical logic. Consider, for example, the rule formula_74. It says that, whenever one can prove that formula_75 can be concluded from some sequence of formulae that contain formula_44, then one can also conclude formula_75 from the (stronger) assumption that formula_78 holds. Likewise, the rule formula_79 states that, if formula_80 and formula_44 suffice to conclude formula_75, then from formula_80 alone one can either still conclude formula_75 or formula_44 must be false, i.e. formula_86 holds. All the rules can be interpreted in this way.\n\nFor an intuition about the quantifier rules, consider the rule formula_70. Of course concluding that formula_88 holds just from the fact that formula_89 is true is not in general possible. If, however, the variable y is not mentioned elsewhere (i.e. it can still be chosen freely, without influencing the other formulae), then one may assume, that formula_89 holds for any value of y. The other rules should then be pretty straightforward.\n\nInstead of viewing the rules as descriptions for legal derivations in predicate logic, one may also consider them as instructions for the construction of a proof for a given statement. In this case the rules can be read bottom-up; for example, formula_91 says that, to prove that formula_78 follows from the assumptions formula_80 and formula_94, it suffices to prove that formula_44 can be concluded from formula_80 and formula_1 can be concluded from formula_94, respectively. Note that, given some antecedent, it is not clear how this is to be split into formula_80 and formula_94. However, there are only finitely many possibilities to be checked since the antecedent by assumption is finite. This also illustrates how proof theory can be viewed as operating on proofs in a combinatorial fashion: given proofs for both formula_44 and formula_1, one can construct a proof for formula_78.\n\nWhen looking for some proof, most of the rules offer more or less direct recipes of how to do this. The rule of cut is different: it states that, when a formula formula_44 can be concluded and this formula may also serve as a premise for concluding other statements, then the formula formula_44 can be \"cut out\" and the respective derivations are joined. When constructing a proof bottom-up, this creates the problem of guessing formula_44 (since it does not appear at all below). The cut-elimination theorem is thus crucial to the applications of sequent calculus in automated deduction: it states that all uses of the cut rule can be eliminated from a proof, implying that any provable sequent can be given a \"cut-free\" proof.\n\nThe second rule that is somewhat special is the axiom of identity (I). The intuitive reading of this is obvious: every formula proves itself. Like the cut rule, the axiom of identity is somewhat redundant: the completeness of atomic initial sequents states that the rule can be restricted to atomic formulas without any loss of provability.\n\nObserve that all rules have mirror companions, except the ones for implication. This reflects the fact that the usual language of first-order logic does not include the \"is not implied by\" connective formula_107 that would be the De Morgan dual of implication. Adding such a connective with its natural rules would make the calculus completely left-right symmetric.\n\nHere is the derivation of \"formula_108\", known as\nthe \"Law of excluded middle\" (\"tertium non datur\" in Latin).\nNext is the proof of a simple fact involving quantifiers. Note that the converse is not true, and its falsity can be seen when attempting to derive it bottom-up, because an existing free variable cannot be used in substitution in the rules formula_109 and formula_110.\nFor something more interesting we shall prove formula_111. It is straightforward to find the derivation, which exemplifies the usefulness of LK in automated proving.\nThese derivations also emphasize the strictly formal structure of the sequent calculus. For example, the logical rules as defined above always act on a formula immediately adjacent to the turnstile, such that the permutation rules are necessary. Note, however, that this is in part an artifact of the presentation, in the original style of Gentzen. A common simplification involves the use of multisets of formulas in the interpretation of the sequent, rather than sequences, eliminating the need for an explicit permutation rule. This corresponds to shifting commutativity of assumptions and derivations outside the sequent calculus, whereas LK embeds it within the system itself.\n\nFor certain formulations (i.e. variants) of the sequent calculus, a proof in such a calculus is isomorphic to an upside-down, closed analytic tableau.\n\nThe structural rules deserve some additional discussion.\n\nWeakening (W) allows the addition of arbitrary elements to a sequence. Intuitively, this is allowed in the antecedent because we can always restrict the scope of our proof (if all cars have wheels, then it's safe to say that all black cars have wheels); and in the succedent because we can always allow for alternative conclusions (if all cars have wheels, then it's safe to say that all cars have either wheels or wings).\n\nContraction (C) and Permutation (P) assure that neither the order (P) nor the multiplicity of occurrences (C) of elements of the sequences matters. Thus, one could instead of sequences also consider sets.\n\nThe extra effort of using sequences, however, is justified since part or all of the structural rules may be omitted. Doing so, one obtains the so-called substructural logics.\n\nThis system of rules can be shown to be both sound and complete with respect to first-order logic, i.e. a statement formula_44 follows semantically from a set of premises formula_80 formula_114 iff the sequent formula_115 can be derived by the above rules.\n\nIn the sequent calculus, the rule of cut is admissible. This result is also referred to as Gentzen's \"Hauptsatz\" (\"Main Theorem\").\n\nThe above rules can be modified in various ways:\n\nThere is some freedom of choice regarding the technical details of how sequents and structural rules are formalized. As long as every derivation in LK can be effectively transformed to a derivation using the new rules and vice versa, the modified rules may still be called LK.\n\nFirst of all, as mentioned above, the sequents can be viewed to consist of sets or multisets. In this case, the rules for permuting and (when using sets) contracting formulae are obsolete.\n\nThe rule of weakening will become admissible, when the axiom (I) is changed, such that any sequent of the form formula_116 can be concluded. This means that formula_44 proves formula_44 in any context. Any weakening that appears in a derivation can then be performed right at the start. This may be a convenient change when constructing proofs bottom-up.\n\nIndependent of these one may also change the way in which contexts are split within the rules: In the cases formula_119, and formula_120 the left context is somehow split into formula_80 and formula_94 when going upwards. Since contraction allows for the duplication of these, one may assume that the full context is used in both branches of the derivation. By doing this, one assures that no important premises are lost in the wrong branch. Using weakening, the irrelevant parts of the context can be eliminated later.\n\nOne can introduce formula_123, the absurdity constant representing \"false\", with the axiom:\n\nOr if, as described above, weakening is to be an admissible rule, then with the axiom:\n\nWith formula_123, negation can be subsumed as a special case of implication, via the definition formula_127.\n\nAlternatively, one may restrict or forbid the use of some of the structural rules. This yields a variety of substructural logic systems. They are generally weaker than LK (\"i.e.\", they have fewer theorems), and thus not complete with respect to the standard semantics of first-order logic. However, they have other interesting properties that have led to applications in theoretical computer science and artificial intelligence.\n\nSurprisingly, some small changes in the rules of LK suffice to turn it into a proof system for intuitionistic logic. To this end, one has to restrict to sequents with exactly one formula on the right-hand side, and modify the rules to maintain this invariant. For example, formula_128 is reformulated as follows (where C is an arbitrary formula):\n\nThe resulting system is called LJ. It is sound and complete with respect to intuitionistic logic and admits a similar cut-elimination proof. This can be used in proving disjunction and existence properties.\n\nIn fact, the only two rules in LK that need to be restricted to single-formula consequents are formula_130 and formula_131 (and the latter can be seen as a special case of the former, via formula_123 as described above). When multi-formula consequents are interpreted as disjunctions, all of the other inference rules of LK are actually derivable in LJ, while the offending rule is\n\nThis amounts to the propositional formula formula_134, a classical tautology that is not constructively valid.\n\n\n\n"}
{"id": "54558894", "url": "https://en.wikipedia.org/wiki?curid=54558894", "title": "Sixth power", "text": "Sixth power\n\nIn arithmetic and algebra the sixth power of a number \"n\" is the result of multiplying six instances of \"n\" together. So:\n\nSixth powers are also formed by multiplying a number by its fifth power, the square of a number by its fourth power, or the cube of a number by itself,\nby taking a square to the third power, or by squaring a cube.\n\nThe sequence of sixth powers of integers is:\n\nThey include the significant decimal numbers 10 (a million), 100 (a short-scale trillion and long-scale billion), and 1000 (a long-scale trillion).\n\nThe sixth powers of integers can be characterized as the numbers that are simultaneously squares and cubes. \nIn this way, they are related to two other classes of figurate numbers: the square triangular numbers, which are simultaneously square and triangular,\nand the solutions to the cannonball problem, which are simultaneously square and square-pyramidal.\n\nBecause of their connection to squares and cubes, sixth powers play an important role in the study of the Mordell curves, which are elliptic curves of the form\nWhen formula_2 is divisible by a sixth power, this equation can be reduced by dividing by that power to give a simpler equation of the same form.\nA well-known result in number theory, proven by Rudolf Fueter and Louis J. Mordell, states that, when formula_2 is an integer that is not divisible by a sixth power (other than the exceptional cases formula_4 and formula_5), this equation either has no rational solutions with both formula_6 and formula_7 nonzero or infinitely many of them.\n\nIn the archaic notation of Robert Recorde, the sixth power of a number was called the \"zenzicube\", meaning the square of a cube. Similarly, the notation for sixth powers used in 12th century Indian mathematics by Bhāskara II also called them either the square of a cube or the cube of a square.\n\nThere are numerous known examples of sixth powers that can be expressed as the sum of seven other sixth powers, but no examples are yet known of a sixth power expressible as the sum of just six sixth powers. This makes it unique among the powers with exponent \"k\" = 1, 2, ... , 8, the others of which can each be expressed as the sum of \"k\" other \"k\"-th powers, and some of which (in violation of Euler's sum of powers conjecture) can be expressed as a sum of even fewer \"k\"-th powers.\n\nIn connection with Waring's problem, every sufficiently large integer can be represented as a sum of at most 24 sixth powers of integers.\n\nThere are infinitely many different nontrivial solutions to the Diophantine equation\nIt has not been proven whether the equation\nhas a nontrivial solution, but the Lander, Parkin, and Selfridge conjecture would imply that it does not.\n\n"}
{"id": "34484092", "url": "https://en.wikipedia.org/wiki?curid=34484092", "title": "Skew gradient", "text": "Skew gradient\n\nIn mathematics, a skew gradient of a harmonic function over a simply connected domain with two real dimensions is a vector field that is everywhere orthogonal to the gradient of the function and that has the same magnitude as the gradient.\n\nThe skew gradient can be defined using complex analysis and the Cauchy–Riemann equations.\n\nLet formula_1 be a complex-valued analytic function, where \"u\",\"v\" are real-valued scalar functions of the real variables \"x\", \"y\".\n\nA skew gradient is defined as:\n\nand from the Cauchy–Riemann equations, it is derived that\n\nThe skew gradient has two interesting properties. It is everywhere orthogonal to the gradient of u, and of the same length:\n\n"}
{"id": "22369901", "url": "https://en.wikipedia.org/wiki?curid=22369901", "title": "Staden Package", "text": "Staden Package\n\nThe Staden Package is computer software, a set of tools for DNA sequence assembly, editing, and sequence analysis. It is open-source software, released under a BSD 3-clause license.\n\nThe Staden package consists of several different programs. The main components are:\n\n\nThe Staden Package was developed by Rodger Staden's group at the Medical Research Council (MRC) Laboratory of Molecular Biology, Cambridge, England, since 1977. The package was available free to academic users, with 2,500 licenses issued in 2003 and an estimated 10,000 users, when funding for further development ended. The package was converted to open-source in 2004, and several new versions have been released since.\n\nDuring the years of active development, the Staden group published a number of widely used file formats and ideas, including the SCF file format, the use of sequence quality scores to generate accurate consensus sequences, and the ZTR file format.\n\n"}
{"id": "44157166", "url": "https://en.wikipedia.org/wiki?curid=44157166", "title": "Suzan Rose Benedict", "text": "Suzan Rose Benedict\n\nSuzan Rose Benedict (November 29, 1873 – April 8, 1942) was the first woman awarded a Ph.D. in Mathematics from the University of Michigan and had a long teaching career at Smith College.\n\nSuzan Benedict was born in Norwalk, Ohio, the youngest of seven children of David DeForrest Benedict, MD and Harriott Melvina Benedict (née Deaver). Dr. Benedict had been a Union Surgeon in the American Civil War. She was a niece of oil magnate and philanthropist, Louis Severance.\n\nAfter graduating high school in Norwalk, Suzan Benedict entered Smith College in 1891. She graduated in 1895 with a major in Chemistry and minors in Mathematics, German, and Physics, then returned to Norwalk and taught Mathematics until 1905, when she began graduate studies at Teacher’s College, Columbia University. She received a M.A. in Mathematics from Columbia in 1906. That same year she joined the Mathematics Department at Smith College as an assistant in Mathematics and rose to become an instructor the following year.\n\nThe summers of 1911 through 1913 she resumed her graduate studies at the University of Michigan and in 1913–14 she took a leave of absence from Smith to finish her dissertation directed by Louis Charles Karpinski: “A Comparative Study of the Early Treatises Introducing into Europe the Hindu Art of Reckoning.” She received her PhD in 1914.\n\nSuzan returned to Smith as an associate professor after receiving her PhD. She was promoted to professor in 1921. From 1918 to 1928 she was Dean of Students and she served as chairman of the Mathematics department from 1928 to 1934.\n\nHer first love was teaching. In May 1940 she wrote to Helen Owens, an instructor in mathematics at Pennsylvania State College: \"it was not modesty that prevented my sending you a long list of published papers, but a scarcity of such papers. I have lost track of the very few I have written, as I have been much more interested in teaching and administration than in research.\"\n\nIn February 1942 she retired as professor emeritus, intending to support the war effort by volunteering with the Red Cross. Two months later, she was stricken with a heart attack and died.\n\nSuzan Benedict never married. From 1918 she shared a home with Susan Miller Rambo, a colleague in the Mathematics Department at Smith College and the second woman to receive a PhD from the University of Michigan.\n\n\n\nThe Suzan R. Benedict Prize was established after her death by the college president and others at Smith College to be awarded to sophomores who had done exceptional work in differential and integral calculus.\n\n"}
{"id": "39156141", "url": "https://en.wikipedia.org/wiki?curid=39156141", "title": "Symmetric cone", "text": "Symmetric cone\n\nIn mathematics, symmetric cones, sometimes called domains of positivity, are open convex self-dual cones in Euclidean space which have a transitive group of symmetries, i.e. invertible operators that take the cone onto itself. By the Koecher–Vinberg theorem these correspond to the cone of squares in finite-dimensional real Euclidean Jordan algebras, originally studied and classified by . The tube domain associated with a symmetric cone is a noncompact Hermitian symmetric space of tube type. All the algebraic and geometric structures associated with the symmetric space can be expressed naturally in terms of the Jordan algebra. The other irreducible Hermitian symmetric spaces of noncompact type correspond to Siegel domains of the second kind. These can be described in terms of more complicated structures called Jordan triple systems, which generalize Jordan algebras without identity.\n\nA convex cone \"C\" in a finite-dimensional real inner product space \"V\" is a convex set invariant under multiplication by positive scalars. It spans the subspace \"C\" – \"C\" and the largest subspace it contains is \"C\" ∩ (−\"C\"). It spans the whole space if and only if it contains a basis. Since the convex hull of the basis is a polytope with non-empty interior, this happens if and only if \"C\" has non-empty interior. The interior in this case is also a convex cone. Moreover, an open convex cone coincides with the interior of its closure, since any interior point in the closure must lie in the interior of some polytope in the original cone. A convex cone is said to be \"proper\" if its closure, also a cone, contains no subspaces.\n\nLet \"C\" be an open convex cone. Its dual is defined as\n\nIt is also an open convex cone and \"C\"** = \"C\". An open convex cone \"C\" is said to be self-dual if \"C\"* = \"C\". It is necessarily proper, since\nit does not contain 0, so cannot contain both \"X\" and −\"X\".\n\nThe automorphism group of an open convex cone is defined by\n\nClearly \"g\" lies in Aut \"C\" if and only if \"g\" takes the closure of \"C\" onto itself. So Aut \"C\" is a closed subgroup of GL(\"V\") and hence a Lie group. Moreover, Aut \"C\"* = (Aut \"C\")*, where \"g\"* is the adjoint of \"g\". \"C\" is said to be homogeneous if Aut \"C\" acts transitively on \"C\".\n\nThe open convex cone \"C\" is called a symmetric cone if it is self-dual and homogeneous.\n\n\nIn their classic paper, studied and completely classified a class of finite-dimensional Jordan algebras, that are now called either Euclidean Jordan algebras or formally real Jordan algebras.\n\nLet \"E\" be a finite-dimensional real vector space with a symmetric bilinear product operation\n\nwith an identity element 1 such that \"a\"1 = \"a\" for \"a\" in \"A\" and a real inner product (\"a\",\"b\") for which the multiplication operators \"L\"(\"a\") defined by \"L\"(\"a\")\"b\" = \"ab\" on \"E\" are self-adjoint and satisfy the Jordan relation\n\nAs will turn out below, the condition on adjoints can be replaced by the equivalent condition that\nthe trace form Tr \"L\"(\"ab\") defines an inner product. The trace form has the advantage of being manifestly invariant under automorphisms of the Jordan algebra, which is thus a closed subgroup of O(\"E\") and thus a compact Lie group. In practical examples, however, it is often easier to produce an inner product for which the \"L\"(\"a\") are self-adjoint than verify directly positive-definiteness of the trace form. (The equivalent original condition of Jordan, von Neumann and Wigner was that if a sum of squares of elements vanishes then each of those elements has to vanish.)\n\nFrom the Jordan condition it follows that the Jordan algebra is power associative, i.e. the Jordan subalgebra generated by any single element \"a\" in \"E\" is actually an associative commutative algebra. Thus, defining \"a\" inductively by \"a\" = \"a\" (\"a\"), the following associativity relation holds:\n\nso the subalgebra can be identified with R[\"a\"], polynomials in \"a\". In fact polarizing of the Jordan relation—replacing \"a\" by \"a\" + \"tb\" and taking the coefficient of \"t\"—yields\n\nThis identity implies that \"L\"(\"a\") is a polynomial in \"L\"(\"a\") and \"L\"(\"a\") for all \"m\". In fact, assuming the result for lower exponents than \"m\",\n\nSetting \"b\" = \"a\" in the polarized Jordan identity gives:\n\na recurrence relation showing inductively that \"L\"(\"a\") is a polynomial in \"L\"(\"a\") and \"L\"(\"a\").\n\nConsequently, if power-associativity holds when the first exponent is ≤ \"m\", then it also holds for \"m\"+1 since\n\nAn element \"e\" in \"E\" is called an idempotent if \"e\" = \"e\". Two idempotents are said to be orthogonal if \"ef\" = 0. This is equivalent to orthogonality with respect to the inner product, since (\"ef\",\"ef\") = (\"e\",\"f\"). In this case \"g\" = \"e\" + \"f\" is also an idempotent. An idempotent \"g\" is called \"primitive\" or \"minimal\" if it cannot be written as a sum of non-zero orthogonal idempotents. If \"e\", ..., \"e\" are pairwise orthogonal idempotents then their sum is also an idempotent and the algebra they generate consists of all linear combinations of the \"e\". It is an associative algebra. If \"e\" is an idempotent, then 1 − \"e\" is an orthogonal idempotent. An orthogonal set of idempotents with sum 1 is said to be a \"complete set\" or a \"partition of 1\". If each idempotent in the set is minimal it is called a \"Jordan frame\". Since the number of elements in any orthogonal set of idempotents is bounded by dim \"E\", Jordan frames exist. The maximal number of elements in a Jordan frame is called the rank \"r\" of \"E\".\n\nThe spectral theorem states that any element \"a\" can be uniquely written as\n\nwhere the idempotents \"e\"'s are a partition of 1 and the λ, the \"eigenvalues\" of \"a\", are real and distinct. In fact let \"E\" = R[a] and let \"T\" be the restriction of \"L\"(\"a\") to \"E\". \"T\" is self-adjoint and has 1 as a cyclic vector. So the commutant of \"T\" consists of polynomials in \"T\" (or \"a\"). By the spectral theorem for self-adjoint operators,\n\nwhere the \"P\" are orthogonal projections on \"E\" with sum \"I\" and the λ's are the distinct real eigenvalues of \"T\". Since the \"P\"'s commute with \"T\" and are self-adjoint, they are given by multiplication elements \"e\" of R[a] and thus form a partition of 1. Uniqueness follows because if \"f\" is a partition of 1 and \"a\" = ∑ μ \"f\", then with \"p\"(\"t\")=∏ (\"t\" - μ) and \"p\" = \"p\"/(\"t\" − μ), \"f\" = \"p\"(\"a\")/\"p\"(μ). So the \"f\"'s are polynomials in \"a\" and uniqueness follows from uniqueness of the spectral decomposition of \"T\".\n\nThe spectral theorem implies that the rank is independent of the Jordan frame. For a Jordan frame with \"k\" minimal idempotents can be used to construct an element \"a\" with \"k\" distinct eigenvalues. As above the minimal polynomial \"p\" of \"a\" has degree \"k\" and R[\"a\"] has dimension \"k\". Its dimension is also the largest \"k\" such that \"F\"(\"a\") ≠ 0 where \"F\"(\"a\") is the determinant of a Gram matrix:\n\nSo the rank \"r\" is the largest integer \"k\" for which \"F\" is not identically zero on \"E\". In this case, as a non-vanishing polynomial, \"F\" is non-zero on an open dense subset of \"E\". the \"regular elements\". Any other \"a\" is a limit of regular elements \"a\". Since the operator norm of \"L\"(\"x\") gives an equivalent norm on \"E\", a standard compactness argument shows that, passing to a subsequence if necessary, the spectral idempotents of the \"a\" and their corresponding eigenvalues are convergent. The limit of Jordan frames is a Jordan frame, since a limit of non-zero idempotents yields a non-zero idempotent by continuity of the operator norm. It follows that every Jordan frame is made up of \"r\" minimal idempotents.\n\nIf \"e\" and \"f\" are orthogonal idempotents, the spectral theorem shows that \"e\" and \"f\" are polynomials in \"a\" = \"e\" − \"f\", so that \"L\"(\"e\") and \"L\"(\"f\") commute. This can be seen directly from the polarized Jordan identity which implies \"L\"(\"e\")\"L\"(\"f\") = 2 \"L\"(\"e\")\"L\"(\"f\")\"L\"(\"e\"). Commutativity follows by taking adjoints.\n\nIf \"e\" is a non-zero idempotent then the eigenvalues of \"L\"(\"e\") can only be 0, 1/2 and 1, since taking \"a\" = \"b\" = \"e\" in the polarized Jordan identity yields\n\nIn particular the operator norm of \"L\"(\"e\") is 1 and its trace is strictly positive.\n\nThere is a corresponding orthogonal eigenspace decomposition of \"E\"\n\nwhere, for \"a\" in \"E\", \"E\"(\"a\") denotes the λ-eigenspace of \"L\"(\"a\"). In this decomposition \"E\"(\"e\") and \"E\"(\"e\") are Jordan algebras with identity elements \"e\" and 1 − \"e\". Their sum \"E\"(\"e\") ⊕ \"E\"(\"e\") is a direct sum of Jordan algebras in that any product between them is zero. It is the \"centralizer subalgebra\" of \"e\" and consists of all \"a\" such that \"L\"(\"a\") commutes with \"L\"(\"e\"). The subspace \"E\"(\"e\") is a module for the centralizer of \"e\", the \"centralizer module\", and the product of any two elements in it lies in the centralizer subalgebra. On the other hand, if\n\nthen \"U\" is self-adjoint equal to 1 on the centralizer algebra and −1 on the centralizer module. So \"U\" = \"I\" and the properties above show that\n\ndefines an involutive Jordan algebra automorphism σ of \"E\".\n\nThe trace form is defined by\n\nIt is an inner product since, for non-zero \"a\" = ∑ λ \"e\",\n\nThe polarized Jordan identity can be polarized again by replacing \"a\" by \"a\" + \"tc\" and taking the coefficient of \"t\". A further anyisymmetrization in \"a\" and \"c\" yields:\n\nApplying the trace to both sides\n\nso that \"L\"(\"b\") is self-adjoint for the trace form.\n\n[[File:Adolf Hurwitz.jpg|thumb|150px|[[Adolf Hurwitz]] (1855–1919), whose work on [[composition algebra]]s was published posthumously in 1923.]]\nThe classification of simple Euclidean Jordan algebras was accomplished by , with details of the one exceptional algebra provided in the article immediately following theirs by . Using the [[Peirce decomposition]], they reduced the problem to an algebraic problem involving [[Hurwitz's theorem (composition algebras)|multiplicative quadratic forms]] already solved by [[Adolf Hurwitz|Hurwitz]]. The presentation here, following , using [[composition algebra]]s or Euclidean Hurwitz algebras, is a shorter version of the original derivation.\n\nIf \"E\" is a Euclidean Jordan algebra an ideal \"F\" in \"E\" is a linear subspace closed under multiplication by elements of \"E\", i.e. \"F\" is invariant under the operators \"L\"(\"a\") for \"a\" in \"E\". If \"P\" is the orthogonal projection onto \"F\" it commutes with the operators \"L\"(\"a\"), In particular \"F\" = (\"I\" − \"P\")\"E\" is also an ideal and \"E\" = \"F\" ⊕ \"F\". Furthermore, if \"e\" = \"P\"(1), then \"P\" = \"L\"(\"e\"). In fact for \"a\" in \"E\"\n\nso that \"ea\" = \"a\" for \"a\" in \"F\" and 0 for \"a\" in \"F\". In particular \"e\" and 1 − \"e\" are orthogonal idempotents with \"L\"(\"e\") = \"P\" and \"L\"(1 − \"e\") = \"I\" − \"P\". \"e\" and 1 − \"e\" are the identities in the Euclidean Jordan algebras \"F\" and \"F\". The idempotent \"e\" is \"central\" in \"E\", where the center of \"E\" is defined to be the set of all \"z\" such that \"L\"(\"z\") commutes with \"L\"(\"a\") for all \"a\". It forms a commutative associative subalgebra.\n\nContinuing in this way \"E\" can be written as a direct sum of minimal ideals\n\nIf \"P\" is the projection onto \"E\" and \"e\" = \"P\"(1) then \"P\" = \"L\"(\"e\"). The \"e\"'s are orthogonal with sum 1 and are the identities in \"E\". Minimality forces \"E\" to be simple, i.e. to have no non-trivial ideals. For since \"L\"(\"e\") commutes with all \"L\"(\"a\")'s, any ideal \"F\" ⊂ \"E\"\nwould be invariant under \"E\" since \"F\" = \"e\"\"F\". Such a decomposition into a direct sum of simple Euclidean algebras is unique. If \"E\" = ⊕ \"F\" is another decomposition, then \"F\"=⊕ e\"F\". By minimality only one of the terms here is non-zero so equals \"F\". By minimality the corresponding \"E\" equals \"F\", proving uniqueness.\n\nIn this way the classification of Euclidean Jordan algebras is reduced to that of simple ones. For a simple algebra \"E\" all inner products for which the operators \"L\"(\"a\") are self adjoint are proportional. Indeed, any other product has the form (\"Ta\", \"b\") for some positive self-adjoint operator commuting with the \"L\"(\"a\")'s. Any non-zero eigenspace of \"T\" is an ideal in \"A\" and therefore by simplicity \"T\" must act on the whole of \"E\" as a positive scalar.\n\n\nLet \"E\" be a simple Euclidean Jordan algebra with inner product given by the trace form τ(\"a\")= Tr \"L\"(\"a\"). The proof that \"E\" has the above form rests on constructing an analogue of matrix units for a Jordan frame in \"E\". The following properties of idempotents hold in \"E\".\n\n\nLet \"E\" be a simple Euclidean Jordan algebra. From the properties of the Peirce decomposition it follows that:\n\n\nSuch an algebra \"A\" is called a Euclidean Hurwitz algebra. In \"A\" if λ(\"a\")\"b\" = \"ab\" and ρ(\"a\")\"b\" = \"ba\", then:\n\n\nBy [[Hurwitz's theorem (normed division algebras)|Hurwitz's theorem]] \"A\" must be isomorphic to R, C, H or O. The first three are associative division algebras. The octonions do not form an associative algebra, so \"H\"(O) can only give a Jordan algebra for \"r\" = 3. Because \"A\" is associative when \"A\" = R, C or H, it is immediate that \"H\"(\"A\") is a Jordan algebra for \"r\" ≥ 3. A separate argument, given originally by , is required to show that \"H\"(O) with Jordan product \"a\"∘\"b\" = ½(\"ab\" + \"ba\") satisfies the Jordan identity [\"L\"(\"a\"),\"L\"(\"a\")] = 0. There is a later more direct proof using the [[Freudenthal diagonalization theorem]] due to : he proved that given any matrix in the algebra \"H\"(A) there is an algebra automorphism carrying the matrix onto a diagonal matrix with real entries; it is then straightforward to check that [\"L\"(\"a\"),\"L\"(\"b\")] = 0 for real diagonal matrices.\n\nThe exceptional Euclidean Jordan algebra \"E\"= \"H\"(O) is called the [[Albert algebra]]. The Cohn–Shirshov theorem implies that it cannot be generated by two elements (and the identity). This can be seen directly. For by Freudenthal's diagonalization theorem one element \"X\" can be taken to be a diagonal matrix with real entries and the other \"Y\" to be orthogonal to the Jordan subalgebra generated by \"X\". If all the diagonal entries of \"X\" are distinct, the Jordan subalgebra generated by \"X\" and \"Y\" is generated by the diagonal matrices and three elements\n\nIt is straightforward to verify that the real linear span of the diagonal matrices, these matrices and similar matrices with real entries form a unital Jordan subalgebra. If the diagonal entries of \"X\" are not distinct, \"X\" can be taken to be the primitive idempotent \"e\" with diagonal entries 1, 0 and 0. The analysis in then shows that the unital Jordan subalgebra generated by \"X\" and \"Y\" is proper. Indeed, if, if 1 − \"e\" is the sum of two primitive idempotents in the subalgebra, then, after applying an automorphism of \"E\" if necessary, the subalgebra will be generated by the diagonal matrices and a matrix orthogonal to the diagonal matrices. By the previous argument it will be proper. If 1 - \"e\" is a primitive idempotent, the subalgebra must be proper, by the properties of the rank in \"E\".\n\nA Euclidean algebra is said to be \"special\" if its central decomposition contains no copies of the Albert algebra. Since the Albert algebra cannot be generated by two elements, it follows that a Euclidean Jordan algebra generated by two elements is special. This is the [[Shirshov–Cohn theorem]] for Euclidean Jordan algebras.\n\nThe classification shows that each non-exceptional simple Euclidean Jordan algebra is a subalgebra of some \"H\"(R). The same is therefore true of any special algebra.\n\nOn the other hand, as showed, the Albert algebra \"H\"(O) cannot be realized as a subalgebra of \"H\"(R) for any \"n\".\n\nIndeed, let π is a real-linear map of \"E\" = \"H\"(O) into the self-adjoint operators on \"V\" = R with π(\"ab\") = ½(π(\"a\")π(\"b\") + π(\"b\")π(\"a\")) and π(1) = \"I\". If \"e\", \"e\", \"e\" are the diagonal minimal idempotents then \"P\" = π(\"e\" are mutually orthogonal projections on \"V\" onto orthogonal subspaces \"V\". If \"i\" ≠ \"j\", the elements \"e\" of \"E\" with 1 in the (\"i\",\"j\") and (\"j\",\"i\") entries and 0 elsewhere satisfy \"e\" = \"e\" + \"e\". Moreover, \"e\"\"e\" = ½ \"e\" if \"i\", \"j\" and \"k\" are distinct. The operators \"T\" are zero on \"V\" (\"k\" ≠ \"i\", \"j\") and restrict to involutions on \"V\" ⊕ \"V\" interchanging \"V\" and \"V\". Letting \"P\" = \"P\" \"T\" \"P\" and setting \"P\" = \"P\", the (\"P\") form a system of [[matrix unit]]s on \"V\", i.e. \"P\"* = \"P\", ∑ \"P\" = \"I\" and \"P\"\"P\" = δ \"P\". Let \"E\" and \"E\" be the subspaces of the Peirce decomposition of \"E\". For \"x\" in O, set π = \"P\" π(x\"e\"), regarded as an operator on \"V\". This does not depend on \"j\" and for \"x\", \"y\" in O\n\nSince every \"x\" in O has a right inverse \"y\" with \"xy\" = 1, the map π is injective. On the other hand, it is an algebra homomorphism from the nonassociative algebra O into the associative algebra End \"V\", a contradiction.\n\n[[File:Max Koecher 2.jpeg|thumb|150px|[[Max Koecher]] pioneered the use of Jordan algebras in studying symmetric spaces]]\n\nWhen (\"e\") is a partition of 1 in a Euclidean Jordan algebra \"E\", the self-adjoint operators L(\"e\") commute and there is a decomposition into simultaneous eigenspaces. If \"a\" = ∑ λ \"e\" the eigenvalues of \"L\"(\"a\") have the form ∑ ε λ is 0, 1/2 or 1. The \"e\" themselves give the eigenvalues λ. In particular an element \"a\" has non-negative spectrum if and only if \"L\"(\"a\") has non-negative spectrum. Moreover, \"a\" has positive spectrum if and only if \"L\"(\"a\") has positive spectrum. For if \"a\" has positive spectrum, \"a\" - ε1 has non-negative spectrum for some ε > 0.\n\nThe positive cone \"C\" in \"E\" is defined to be the set of elements \"a\" such that \"a\" has positive spectrum. This condition is equivalent to the operator \"L\"(\"a\") being a [[positive operator|positive]] self-adjoint operator on \"E\".\n\n\nTo show that the positive cone \"C\" is homogeneous, i.e. has a transitive group of automorphisms, a generalization of the quadratic action of self-adjoint matrices on themselves given by \"X\" ↦ \"YXY\" has to be defined. If \"Y\" is invertible and self-adjoint, this map is invertible and carries positive operators onto positive operators.\n\nFor \"a\" in \"E\", define an endomorphism of \"E\", called the [[quadratic representation]], by\n\nNote that for self-adjoint matrices \"L\"(\"X\")\"Y\" = ½(\"XY\" + \"YX\"), so that \"Q\"(\"X\")\"Y\" = \"XYX\".\n\nAn element \"a\" in \"E\" is called \"invertible\" if it is invertible in R[\"a\"]. If \"b\" denotes the inverse, then the spectral decomposition of \"a\" shows that \"L\"(\"a\") and \"L\"(\"b\") commute.\n\nIn fact \"a\" is invertible if and only if \"Q\"(\"a\") is invertible. In that case\n\nIndeed, if \"Q\"(\"a\") is invertible it carries R[\"a\"] onto itself. On the other hand, \"Q\"(\"a\")1 = \"a\", so\n\nTaking \"b\" = \"a\" in the polarized Jordan identity, yields\n\nReplacing \"a\" by its inverse, the relation follows if \"L\"(\"a\") and \"L\"(\"a\") are invertible. If not it holds for \"a\" + ε1 with ε arbitrarily small and hence also in the limit.\n\nThese identities are easy to prove in a finite-dimensional (Euclidean) Jordan algebra (see below) or in a [[special Jordan algebra]], i.e. the Jordan algebra defined by a unital associative algebra. They are valid in any Jordan algebra. This was conjectured by [[Nathan Jacobson|Jacobson]] and proved in : [[Ian G. Macdonald|Macdonald]] showed that if a polynomial identity in three variables, linear in the third, is valid in any special Jordan algebra, then it holds in all Jordan algebras.\n\nIn fact for \"c\" in \"A\" and \"F\"(\"a\") a function on \"A\" with values in End \"A\", let\n\"D\"\"F\"(\"a\") be the derivative at \"t\" = 0 of \"F\"(\"a\" + \"tc\"). Then\n\nThe expression in square brackets simplifies to \"c\" because \"L\"(\"a\") commutes with \"L\"(\"a\").\n\nThus\n\nApplying \"D\" to \"L\"(\"a\")\"Q\"(\"a\") = \"L\"(\"a\") and acting on \"b\" = \"c\" yields\n\nOn the other hand, \"L\"(\"Q\"(\"a\")\"b\") is invertible on an open dense set where \"Q\"(\"a\")\"b\" must also be invertible with\n\nTaking the derivative \"D\" in the variable \"b\" in the expression above gives\n\nThis yields the fundamental identity for a dense set of invertible elements, so it follows in general by continuity. The fundamental identity implies that \"c\" = \"Q\"(\"a\")\"b\" is invertible if \"a\" and \"b\" are invertible and gives a formula for the inverse of \"Q\"(\"c\"). Applying it to \"c\" gives the inverse identity in full generality.\n\nFinally it can be verified immediately from the definitions that, if \"u\" = 1 − 2\"e\" for some idempotent \"e\", then \"Q\"(\"u\") is the period 2 automorphism constructed above for the centralizer algebra and module of \"e\".\n\nThe proof of this relies on elementary continuity properties of eigenvalues of self-adjoint operators.\n\nLet \"T\"(\"t\") (α ≤ \"t\" ≤ β) be a continuous family of self-adjoint operators on \"E\" with \"T\"(α) positive and \"T\"(β) having a negative eiegenvalue. Set \"S\"(\"t\")= –\"T\"(\"t\") + \"M\" with \"M\" > 0 chosen so large that \"S\"(\"t\") is positive for all \"t\". The operator norm ||\"S\"(\"t\")|| is continuous. It is less than \"M\" for \"t\" = α and greater than \"M\" for \"t\" = β. So for some α < \"s\" < β, ||\"S\"(\"s\")|| = M and there is a vector \"v\" ≠ 0 such that \"S\"(\"s\")\"v\" = \"Mv\". In particular \"T\"(\"s\")\"v\" = 0, so that \"T\"(\"s\") is not invertible.\n\nSuppose that \"x\" = \"Q\"(\"a\")\"b\" does not lie in \"C\". Let \"b\"(\"t\") = (1 − \"t\") + \"tb\" with 0 ≤ \"t\" ≤ 1. By convexity \"b\"(\"t\") lies in \"C\". Let \"x\"(\"t\") = \"Q\"(\"a\")\"b\"(\"t\") and \"X\"(\"t\") = \"L\"(\"x\"(\"t\")). If \"X\"(\"t\") is invertible for all \"t\" with 0 ≤ \"t\" ≤ 1, the eigenvalue argument gives a contradiction since it is positive at \"t\" = 0 and has negative eigenvalues at \"t\" = 1. So \"X\"(\"s\") has a zero eigenvalue for some \"s\" with 0 < \"s\" ≤ 1: \"X\"(\"s\")\"w\" = 0 with \"w\" ≠ 0. By the properties of the quadratic representation, \"x\"(\"t\") is invertible for all \"t\". Let \"Y\"(\"t\") = \"L\"(\"x\"(\"t\")). This is a positive operator since \"x\"(\"t\") lies in \"C\". Let \"T\"(\"t\") = \"Q\"(\"x\"(\"t\")), an invertible self-adjoint operator by the invertibility of \"x\"(\"t\"). On the other hand, \"T\"(\"t\") = 2\"X\"(\"t\") - \"Y\"(\"t\"). So (\"T\"(\"s\")\"w\",\"w\") < 0 since \"Y\"(\"s\") is positive and \"X\"(\"s\")\"w\" = 0. In particular \"T\"(\"s\") has some negative eigenvalues. On the other hand, the operator \"T\"(0) = \"Q\"(\"a\") = \"Q\"(\"a\") is positive. By the eigenvalue argument, \"T\"(\"t\") has eigenvalue 0 for some \"t\" with 0 < \"t\" < \"s\", a contradiction.\n\nIt follows that the linear operators \"Q\"(\"a\") with \"a\" invertible, and their inverses, take the cone \"C\" onto itself. Indeed, the inverse of \"Q\"(\"a\") is just \"Q\"(\"a\"). Since \"Q\"(\"a\")1 = \"a\", there is thus a transitive group of symmetries:\n\nLet \"C\" be a symmetric cone in the Euclidean space \"E\". As above, Aut \"C\" denotes the closed subgroup of GL(\"E\") taking \"C\" (or equivalently its closure) onto itself. Let \"G\" = Aut \"C\" be its identity component. \"K\" = \"G\" ∩ O(\"E\"). It is a maximal compact subgroup of \"G\" and the stabilizer of a point \"e\" in \"C\". It is connected. The group \"G\" is invariant under taking adjoints. Let σ\"g\" =(\"g\"*), period 2 automorphism. Thus \"K\" is the fixed point subgroup of σ. Let formula_32 be the Lie algebra of \"G\". Thus σ induces an involution of formula_32 and hence a ±1 eigenspace decomposition\n\nwhere formula_35, the +1 eigenspace, is the Lie algebra of \"K\" and formula_36 is the −1 eigenspace. Thus formula_36⋅\"e\" is an affine subspace of dimension dim formula_36. Since \"C\" = \"G\"/\"K\" is an open subspace of \"E\", it follows that dim \"E\" = dim formula_36 and hence formula_36⋅\"e\" = \"E\". For \"a\" in \"E\" let \"L\"(\"a\") be the unique element of formula_36 such that \"L\"(\"a\")\"e\" = \"a\". Define\n\"a\" ∘ \"b\" = \"L\"(\"a\")\"b\". Then \"E\" with its Euclidean structure and this bilinear product is a Euclidean Jordan algebra with identity 1 = \"e\". The convex cone coincides \"C\" with the positive cone of \"E\".\n\nSince the elements of formula_36 are self-adjoint, \"L\"(\"a\")* = \"L\"(\"a\"). The product is commutative since\n[formula_36, formula_36] ⊆ formula_35 annihilates \"e\", so that \"ab\" = \"L\"(\"a\")\"L\"(\"b\")\"e\" = \"L\"(\"b\")\"L\"(\"a\")\"e\" = \"ba\". It remains to check the Jordan identity [\"L\"(\"a\"),\"L\"(\"a\")] = 0.\n\nThe [[associator]] is given by [\"a\",\"b\",\"c\"] = [\"L\"(\"a\"),\"L\"(\"c\")]\"b\". Since [\"L\"(\"a\"),\"L\"(\"c\")] lies in\nformula_35 it follows that [[\"L\"(\"a\"),\"L\"(\"c\")],\"L\"(\"b\")] = \"L\"([\"a\",\"b\",\"c\"]). Making both sides act on \"c\" yields\n\nOn the other hand,\n\nand likewise\n\nCombining these expressions gives\n\nwhich implies the Jordan identity.\n\nFinally the positive cone of \"E\" coincides with \"C\". This depends on the fact that in any Euclidean Jordan algebra \"E\"\n\nIn fact \"Q\"(\"e\") is a positive operator,\n\"Q\"(\"e\") is a one-parameter group of positive operators: this follows by continuity for rational \"t\", where it is a consequence of the behaviour of powers So it has the form exp \"tX\" for some self-adjoint operator \"X\". Taking the derivative at 0 gives \"X\" = 2\"L\"(\"a\").\n\nHence the positive cone is given by all elements\n\nwith \"X\" in formula_36. Thus the positive cone of \"E\" lies inside \"C\". Since both are self-dual,\nthey must coincide.\n\nLet \"C\" be the positive cone in a simple Euclidean Jordan algebra \"E\". Aut \"C\" is the closed subgroup of GL(\"E\") taking \"C\" (or its closure) onto itself. Let \"G\" = Aut \"C\" be the identity component of Aut \"C\" and let \"K\" be the closed subgroup of \"G\" fixing 1. From the group theoretic properties of cones, \"K\" is a connected compact subgroup of \"G\" and equals the identity component of the compact Lie group Aut \"E\". Let formula_32 and formula_35 be the Lie algebras of \"G\" and \"K\". \"G\" is closed under taking adjoints and \"K\" is the fixed point subgroup of the period 2 automorphism σ(\"g\") = (\"g\"*). Thus \"K\" = \"G\" ∩ SO(\"E\"). Let formula_36 be the −1 eigenspace of σ.\n\n\n\nIf \"E\" has Peirce decomposition relative to the Jordan frame (\"e\")\n\nthen formula_61 is diagonalized by this decomposition with \"L\"(\"a\") acting as (α + α)/2 on \"E\", where \"a\" = ∑ α \"e\".\n\nDefine the closed subgroup \"S\" of \"G\" by\n\nwhere the ordering on pairs \"p\" ≤ \"q\" is [[lexicographic order|lexicographic]]. \"S\" contains the group \"A\", since it acts as scalars on \"E\". If \"N\" is the closed subgroup of \"S\" such that \"nx\" = \"x\" modulo ⊕ \"E\", then \"S\" = \"AN\" = \"NA\", a [[semidirect product]] with \"A\" normalizing \"N\". Moreover, \"G\" has the following [[Iwasawa decomposition]]:\n\nFor \"i\" ≠ \"j\" let\n\nThen the Lie algebra of \"N\" is\n\nTaking ordered orthonormal bases of the \"E\" gives a basis of \"E\", using the lexicographic order on pairs (\"i\",\"j\"). The group \"N\" is lower unitriangular and its Lie algebra lower triangular. In particular the exponential map is a polynomial mapping of formula_73 onto \"N\", with polynomial inverse given by the logarithm.\n\nLet \"E\" be a Euclidean Jordan algebra. The complexification \"E\" = \"E\" ⊕ \"iE\" has a natural conjugation operation (\"a\" + \"ib\")* = \"a\" − \"ib\" and a natural complex inner product and norm. The Jordan product on \"E\" extends bilinearly to \"E\", so that (\"a\" + \"ib\")(\"c\" + \"id\") = (\"ac\" − \"bd\") + \"i\"(\"ad\" + \"bc\"). If multiplication is defined by \"L\"(\"a\")\"b\" = \"ab\" then the Jordan axiom\n\nstill holds by analytic continuation. Indeed, the identity above holds when \"a\" is replaced by \"a\" + \"tb\" for \"t\" real; and since the left side is then a polynomial with values in End \"E\" vanishing for real \"t\", it vanishes also \"t\" complex. Analytic continuation also shows that all for the formulas involving power-associativity for a single element \"a\" in \"E\", including recursion formulas for \"L\"(\"a\"), also hold in \"E\". Since for \"b\" in \"E\", \"L\"(\"b\") is still self-adjoint on \"E\", the adjoint relation \"L\"(\"a\"*) = \"L\"(\"a\")* holds for \"a\" in \"E\". Similarly the symmetric bilinear form β(\"a\",\"b\") = (\"a\",\"b\"*) satisfies β(\"ab\",\"c\") = β(\"b\",\"ac\"). If the inner product comes from the trace form, then β(\"a\",\"b\") = Tr \"L\"(\"ab\").\n\nFor \"a\" in \"E\", the quadratic representation is defined as before by \"Q\"(\"a\")=2\"L\"(\"a\") − \"L\"(\"a\"). By analytic continuation the fundamental identity still holds:\n\nAn element \"a\" in \"E\" is called \"invertible\" if it is invertible in C[\"a\"]. Power associativity shows that \"L\"(\"a\") and \"L\"(\"a\") commute. Moreover, \"a\" is invertible with inverse \"a\".\n\nAs in \"E\", \"a\" is invertible if and only if \"Q\"(\"a\") is invertible. In that case\n\nIndeed, as for \"E\", if \"Q\"(\"a\") is invertible it carries C[\"a\"] onto itself, while \"Q\"(\"a\")1 = \"a\", so\n\nso \"a\" is invertible. Conversely if \"a\" is invertible, taking \"b\" = \"a\" in the fundamental identity shows that \"Q\"(\"a\") is invertible. Replacing \"a\" by \"a\" and \"b\" by \"a\" then shows that its inverse is \"Q\"(\"a\"). Finally if \"a\" and \"b\" are invertible then so is \"c\" = \"Q\"(\"a\")\"b\" and it satisfies the inverse identity:\n\nInvertibility of \"c\" follows from the fundamental formula which gives \"Q\"(\"c\") = \"Q\"(\"a\")\"Q\"(\"b\")\"Q\"(\"a\"). Hence\n\nThe formula\n\nalso follows by analytic continuation.\n\nAut \"E\" is the [[complexification (Lie group)|complexification]] of the compact Lie group Aut \"E\" in GL(\"E\"). This follows because the Lie algebras of Aut \"E\" and Aut \"E\" consist of derivations of the complex and real Jordan algebras \"E\" and \"E\". Under the isomorphism identifying End \"E\" with the complexification of End \"E\", the complex derivations is identified with the complexification of the real derivations.\n\nThe Jordan operator \"L\"(\"a\") are symmetric with respect to the trace form, so that \"L\"(\"a\") = \"L\"(\"a\") for \"a\" in \"E\". The automorphism groups of \"E\" and \"E\" consist of invertible real and complex linear operators \"g\" such that \"L\"(\"ga\") = \"gL\"(\"a\")\"g\" and \"g1\" = 1. Aut \"E\" is the complexification of Aut \"E\". Since an automorphism \"g\" preserves the trace form, \"g\" = \"g\".\n\nThe structure groups of \"E\" and \"E\" consist of invertible real and complex linear operators \"g\" such that\n\nThey form groups Γ(\"E\") and Γ(\"E\") with Γ(\"E\") ⊂ Γ(\"E\").\n\n\nThe unitary structure group Γ(\"E\") is the subgroup of Γ(\"E\") consisting of unitary operators, so that Γ(\"E\") = Γ(\"E\") ∩ U(\"E\").\n\n\nGiven a frame in a Euclidean Jordan algebra \"E\", the [[restricted Weyl group]] can be identified with the group of operators on arising from elements in the identity component of Γ(\"E\") that leave invariant.\n\nLet \"E\" be a Euclidean Jordan algebra with the inner product given by the trace form. Let (\"e\") be a fixed Jordan frame in \"E\". For given \"a\" in \"E\" choose \"u\" in Γ(\"E\") such that\n\"ua\" = ∑ α \"e\" with α ≥ 0. Then the spectral norm ||\"a\"|| = max α is independent of all choices. It is a norm on \"E\" with\n\nIn addition ||\"a\"|| is given by the [[operator norm]] of \"Q\"(\"a\") on the inner product space \"E\". The fundamental identity for the quadratic representation implies that ||\"Q\"(\"a\")\"b\"|| ≤ ||\"a\"||||\"b\"||. The spectral norm of an element \"a\" is defined in terms of C[\"a\"] so depends only on \"a\" and not the particular Euclidean Jordan algebra in which it is calculated.\n\nThe compact set \"S\" is the set of [[extreme point]]s of the closed unit ball ||\"x\"|| ≤ 1. Each \"u\" in \"S\" has norm one. Moreover, if \"u\" = \"e\" and \"v\" = \"e\", then ||\"uv\"|| ≤ 1. Indeed, by the Cohn–Shirshov theorem the unital Jordan subalgebra of \"E\" generated by \"a\" and \"b\" is special. The inequality is easy to establish in non-exceptional simple Euclidean Jordan algebras, since each such Jordan algebra and its complexification can be realized as a subalgebra of some H(R) and its complexification \"H\"(C) ⊂ \"M\"(C). The spectral norm in \"H\"(C) is the usual operator norm. In that case, for unitary matrices \"U\" and \"V\" in \"M\"(C), clearly ||½(\"UV\" + \"VU\")|| ≤ 1. The inequality therefore follows in any special Euclidean Jordan algebra and hence in general.\n\nOn the other hand, by the [[Krein–Milman theorem]], the closed unit ball is the (closed) [[convex span]] of \"S\". It follows that ||\"L\"(\"u\")|| = 1, in the operator norm corresponding to either the inner product norm or spectral norm. Hence ||\"L\"(\"a\")|| ≤ ||\"a\"|| for all \"a\", so that the spectral norm satisfies\n\nIt follows that \"E\" is a [[Jordan operator algebra#JB* algebras|Jordan C* algebra]].\n\nThe complexification of a simple Euclidean Jordan algebra is a simple complex Jordan algebra which is also separable, i.e. its trace form is non-degenerate. Conversely, using the existence of a [[complexification (Lie group)|real form]] of the Lie algebra of the structure group, it can be shown that every complex separable simple Jordan algebra is the complexification of a simple Euclidean Jordan algebra.\n\nTo verify that the complexification of a simple Euclidean Jordan algebra \"E\" has no ideals, note that if \"F\" is an ideal in \"E\" then so too is \"F\", the orthogonal complement for the trace norm. As in the real case, \"J\" = \"F\" ∩ \"F\" must equal (0). For the associativity property of the trace form shows that \"F\" is an ideal and that \"ab\" = 0 if \"a\" and \"b\" lie in \"J\". Hence \"J\" is an ideal. But if \"z\" is in \"J\", \"L\"(\"z\") takes \"E\" into \"J\" and \"J\" into (0). Hence Tr \"L\"(\"z\") = 0. Since \"J\" is an ideal and the trace form degenerate, this forces \"z\" = 0. It follows that \"E\" = \"F\" ⊕ \"F\". If \"P\" is the corresponding projection onto \"F\", it commutes with the operators \"L\"(\"a\") and \"F\" = (\"I\" − \"P\")\"E\". is also an ideal and \"E\" = \"F\" ⊕ \"F\". Furthermore, if \"e\" = \"P\"(1), then \"P\" = \"L\"(\"e\"). In fact for \"a\" in \"E\"\n\nso that \"ea\" = \"a\" for \"a\" in \"F\" and 0 for \"a\" in \"F\". In particular \"e\" and 1 − \"e\" are orthogonal \"central\" idempotents with \"L\"(\"e\") = \"P\" and \"L\"(1 − \"e\") = \"I\" − \"P\".\n\nSo simplicity follows from the fact that the center of \"E\" is the complexification of the center of \"E\".\n\nAccording to the \"elementary approach\" to bounded symmetric space of Koecher, Hermitian symmetric spaces of noncompact type can be realized in the complexification of a Euclidean Jordan algebra \"E\" as either the open unit ball for the spectral norm, a bounded domain, or as the open tube domain , where \"C\" is the positive open cone in \"E\". In the simplest case where \"E\" = R, the complexification of \"E\" is just C, the bounded domain corresponds to the open unit disk and the tube domain to the upper half plane. Both these spaces have transitive groups of biholomorphisms given by Möbius transformations, corresponding to matrices in or . They both lie in the Riemann sphere }, the standard one-point compactification of C. Moreover, the symmetry groups are all particular cases of Möbius transformations corresponding to matrices in . This complex Lie group and its maximal compact subgroup act transitively on the Riemann sphere. The groups are also algebraic. They have distinguished generating subgroups and have an explicit description in terms of generators and relations. Moreover, the Cayley transform gives an explicit Möbius transformation from the open disk onto the upper half plane. All these features generalize to arbitrary Euclidean Jordan algebras. The compactification and complex Lie group are described in the next section and correspond to the dual Hermitian symmetric space of compact type. In this section only the symmetries of and between the bounded domain and tube domain are described.\n\nJordan frames provide one of the main Jordan algebraic techniques to describe the symmetry groups. Each Jordan frame gives rise to a product of copies of R and C. The symmetry groups of the corresponding open domains and the compactification—polydisks and polyspheres—can be deduced from the case of the unit disk, the upper halfplane and Riemann sphere. All these symmetries extend to the larger Jordan algebra and its compactification. The analysis can also be reduced to this case because all points in the complex algebra (or its compactification) lie in an image of the polydisk (or polysphere) under the unitary structure group.\n\nLet be a Euclidean Jordan algebra with complexification .\n\nThe unit ball or disk \"D\" in is just the convex bounded open set of elements\n\nThe tube domain \"T\" in is the unbounded convex open set , where \"C\" is the open positive cone in .\n\nThe group SL(2,C) acts by [[Möbius transformation]]s on the [[Riemann sphere]] C ∪ {∞}, the [[one-point compactification]] of C. If \"g\" in SL(2,C) is given by the matrix\n\nthen\n\nSimilarly the group SL(2,R) acts by Möbius transformations on the circle R ∪ {∞}, the one-point compactification of R.\n\nLet \"k\" = R or C. Then SL(2,\"k\") is generated by the three subgroups of lower and upper unitriangular matrices, L and U', and the diagonal matrices D. It is also generated by the lower (or upper) unitriangular matrices, the diagonal matrices and the matrix\n\nThe matrix \"J\" corresponds to the Möbius transformation and can be written\n\nThe Möbius transformations fixing ∞ are just the upper triangular matrices B = UD = DU. If \"g\" does not fix ∞, it sends ∞ to a finite point \"a\". But then \"g\" can be composed with an upper unitriangular matrix to send \"a\" to 0 and then with \"J\" to send 0 to infinity. This argument gives the one of the simplest examples of the [[Bruhat decomposition]]:\n\nthe double coset decomposition of . In fact the union is disjoint and can be written more precisely as\n\nwhere the product occurring in the second term is direct.\n\nNow let\n\nThen\n\nIt follows is generated by the group of operators and \"J\" subject to the following relations:\n\n\nThe last relation follows from the definition of . The generator and relations above is fact gives a presentation of . Indeed, consider the free group Φ generated by \"J\" and with \"J\" of order 4 and its square central. This consists of all products\n. The set is invariant under inversion, contains operators and \"J\", so it is enough to show it is invariant under multiplication. By construction it is invariant under multiplication by B. It is invariant under multiplication by \"J\" because of the defining equation for .\n\nIn particular the center of consists of the scalar matrices and it is the only non-trivial normal subgroup of , so that } is [[simple group|simple]]. In fact if is a normal subgroup, then the Bruhat decomposition implies that is a maximal subgroup, so that either is contained in or\n. In the first case fixes one point and hence every point of }, so lies in the center. In the second case, the [[commutator subgroup]] of is the whole group, since it the group is generated by lower and upper unitriangular matrices and the fourth relation shows that all such matrices are commutators\nsince . Writing with in and in , it follows that . Since and generate the whole group, . But then . The right hand side here is Abelian while the left hand side is its own commutator subgroup. Hence this must be the trivial group and .\n\nGiven an element \"a\" in the complex Jordan algebra , the unital Jordan subalgebra is associative and commutative. Multiplication by \"a\" defines an operator on which has a spectrum, namely its set of complex eigenvalues. If is a complex polynomial, then is defined in . It is invertible in if and only if it is invertible in\n, which happen precisely when does not vanish on the spectrum of . This permits [[rational function]]s of to be defined whenever the function is defined on the spectrum of . If and are rational functions with and defined on , then\n. They leave invariant and, when defined, the group composition law holds. (In the next section complex Möbius transformations will be defined on the compactification of .)\n\nGiven a primitive idempotent in with Peirce decomposition\n\nthe action of by Möbius transformations on can be extended to an action on \"A\" so that the action leaves invariant the components and in particular acts trivially on . If is the projection onto , the action is given be the formula\n\nFor a Jordan frame of primitive idempotents , the actions of associated with different commute, thus giving an action of . The diagonal copy of gives again the action by Möbius transformations on .\n\nThe Möbius transformation defined by\n\nis called the [[Cayley transform]]. Its inverse is given by\n\nThe inverse Cayley transform carries the real line onto the circle with the point 1 omitted. It carries the upper halfplane onto the unit disk and the lower halfplane onto the complement of the closed unit disk. In [[operator theory]] the mapping takes self-adjoint operators \"T\" onto unitary operators \"U\" not containing 1 in their spectrum. For matrices this follows because unitary and self-adjoint matrices can be diagonalized and their eigenvalues lie on the unit circle or real line. In this finite-dimensional setting the Cayley transform and its inverse establish a bijection between the matrices of operator norm less than one and operators with imaginary part a positive operator. This is the special case for of the Jordan algebraic result, explained below, which asserts that the Cayley transform and its inverse establish a bijection between the bounded domain and the tube domain .\n\nIn the case of matrices, the bijection follows from resolvant formulas. In fact if the imaginary part of is positive, then is invertible since\n\nIn particular, setting ,\n\nEquivalently\n\nis a positive operator, so that ||\"P\"(\"T\")|| < 1. Conversely if ||\"U\"|| < 1 then is invertible and\n\nSince the Cayley transform and its inverse commute with the transpose, they also establish a bijection for symmetric matrices. This corresponds to the Jordan algebra of symmetric complex matrices, the complexification of .\n\nIn the above resolvant identities take the following form:\n\nand equivalently\n\nwhere the Bergman operator is defined by with . The inverses here are well defined. In fact in one direction is invertible for ||\"u\"|| < 1: this follows either using the fact that the norm satisfies ||\"ab\"|| ≤ ||\"a\"|| ||\"b\"||; or using the resolvant identity and the invertibility of (see below). In the other direction if the imaginary part of is in then the imaginary part of is positive definite so that is invertible. This argument can be applied to , so it also invertible.\n\nTo establish the correspondence, it is enough to check it when is simple. In that case it follows from the connectivity of and and because:\n\nThe first criterion follows from the fact that the eigenvalues of are exactly if the eigenvalues of are . So the are either all positive or all negative. The second criterion follows from the fact that if\n\nThe resolvant identity is a consequence of the following identity for and invertible\n\nIn fact in this case the [[Quadratic Jordan algebra#Linear Jordan algebra defined by a quadratic Jordan algebra|relations for a quadratic Jordan algebra]] imply\n\nso that\n\nThe equality of the last two terms implies the identity, replacing by .\n\nNow set and . The resolvant identity is a special case of the more following more general identity:\n\nIn fact\n\nso the identity is equivalent to\n\nUsing the identity above together with , the left hand side equals . The right hand side equals . These are equal because of the formula .\n\nIf lies in the bounded domain , then is invertible. Since is invariant under multiplication by scalars of modulus ≤ 1, it follows that\nThis is a direct consequence of the definition of the spectral norm.\nThis is already known for the Möbius transformations, i.e. the diagonal in . It follows for diagonal matrices in a fixed component in because they correspond to transformations in the unitary structure group. Conjugating by a Möbius transformation is equivalent to conjugation by a matrix in that component. Since the only non-trivial normal subgroup of is its center, every matrix in a fixed component carries onto itself.\nGiven an element in an transformation in the identity component of the unitary structure group carries it in an element in with supremum norm less than 1. An transformation in the carries it onto zero. Thus there is a transitive group of biholomorphic transformations of . The symmetry is a biholomorphic Möbius transformation fixing only 0.\nIf is a biholomorphic self-mapping of with and derivative at 0, then must be the identity. If not, has Taylor series expansion with homogeneous of degree and . But then . Let be a functional in of norm one. Then for fixed in , the holomorphic functions of a complex variable given by must have modulus less than 1 for |\"w\"| < 1. By [[Cauchy's integral formula#Consequences|Cauchy's inequality]], the coefficients of must be uniformly bounded independent of , which is not possible if .\n\nIf is a biholomorphic mapping of onto itself just fixing 0 then\nif , the mapping fixes 0 and has derivative there. It is therefore the identity map. So for any α. This implies \"g\" is a linear mapping. Since it maps onto itself it maps the closure onto itself. In particular it must map the Shilov boundary onto itself. This forces to be in the unitary structure group.\n\nThe orbit of 0 under \"A\" is the set of all points with . The orbit of these points under the unitary structure group is the whole of . The Cartan decomposition follows because is the stabilizer of 0 in .\n\nIn fact the only point fixed by (the identity component of) \"K\" in \"D\" is 0. Uniqueness implies that the [[Center (geometry)|center]] of \"G\" must fix 0. It follows that the center of \"G\" lies in \"K\". The center of \"K\" is isomorphic to the circle group: a rotation through θ corresponds to multiplication by \"e\" on \"D\" so lies in }. Since this group has trivial center, the center of \"G\" is trivial.\n\nIn fact any larger compact subgroup would intersect \"A\" non-trivially and it has no non-trivial compact subgroups.\n\nNote that \"G\" is a Lie group (see below), so that the above three statements hold with \"G\" and \"K\" replaced by their identity components, i.e. the subgroups generated by their one-parameter cubgroups. Uniqueness of the maximal compact subgroup up to conjugacy follows from [[maximal compact subgroup#Proof of uniqueness for semisimple groups|a general argument]] or can be deduced for classical domains directly using [[Sylvester's law of inertia]] following . For the example of Hermitian matrices over C, this reduces to proving that is up to conjugacy the unique maximal compact subgroup in . In fact if , then is the subgroup of preserving \"W\". The restriction of the hermitian form given by the inner product on minus the inner product on .\nOn the other hand, if is a compact subgroup of , there is a -invariant inner product on obtained by averaging any inner product with respect to Haar measure on . The Hermitian form corresponds to an orthogonal decomposition into two subspaces of dimension both invariant under with the form positive definite on one and negative definite on the other. By Sylvester's law of inertia, given two subspaces of dimension on which the Hermitian form is positive definite, one is carried onto the other by an element of . Hence there is an element of such that the positive definite subspace is given by . So leaves invariant and .\n\nA similar argument. with [[quaternion]]s replacing the complex numbers, shows uniquess for the symplectic group, which corresponds to Hermitian matrices over R. This can also been see more directly by using [[Complex manifold|complex structures]]. A complex structure is an invertible operator \"J\" with \"J\" = −\"I\". preserving the symplectic form \"B\" and such that −\"B\"(\"Jx\",\"y\") is a real inner product. The symplectic group acts transitively on complex structures by conjugation. Moreover, the subgroup commuting with \"J\" is naturally identified with the unitary group for the corresponding complex inner product space. Uniqueness follows by showing that any compact subgroup \"K\" commutes with some complex structure \"J\". In fact, averaging over Haar measure, there is a \"K\"-invariant inner product on the underlying space. The symplectic form yields an invertible skew-adjoint operator \"T\" commuting with \"K\". The operator \"S\" = −\"T\" is positive, so has a unique positive square root, which commutes with \"K\". So \"J\" = \"S\"\"T\", the phase of \"T\", has square −\"I\" and commutes with \"K\".\n\nThere is an [[Cartan decomposition]] for \"G\" corresponding to the action on the tube \"T\" = \"E\" + \"iC\":\n\n\nIn fact the Cartan decomposition for follows from the decomposition for . Given in , there is an element in , the identity component of , such that with . Since ||\"z\"|| < 1, it follows that . Taking the Cayley transform of \"z\", it follows that every in can be written , with the Cayley transform and in . Since\n, the point is of the form with in . Hence .\n\nThere is an [[Iwasawa decomposition]] for \"G\" corresponding to the action on the tube \"T\" = \"E\" + \"iC\":\n\n\nThe group \"S\" = \"AN\" acts on \"E\" linearly and conjugation on \"N\" reproduces this action. Since the group \"S\" acts simply transitively on \"C\", it follows that \"AN\"=\"S\"⋅\"N\" acts simply transitively on \"T\" = \"E\" + \"iC\". Let \"H\" be the group of [[biholomorphism]]s of the tube \"T\". The Cayley transform shows that is isomorphic to the group \"H\" of biholomorphisms of the bounded domain \"D\". Since \"AN\" acts simply transitively on the tube \"T\" while \"K\" fixes \"ic\", they have trivial intersection.\n\nGiven \"g\" in \"H\", take \"s\" in \"AN\" such that \"g\"(\"i\")=\"s\"(\"i\"). then \"gs\" fixes \"i\" and therefore lies in \"K\". Hence \"H\" = \"K\" ⋅\"A\"⋅\"N\". So the product is a group.\n\nBy a result of [[Henri Cartan]], \"H\" is a Lie group. Cartan's original proof is presented in . It can also be deduced from the fact the \"D\" is complete for the [[Bergman metric]], for which the isometries form a Lie group; by [[Montel's theorem]], the group of biholomorphisms is a closed subgroup.\n\nThat \"H\" is a Lie group can be seen directly in this case. In fact there is a finite-dimensional 3-graded Lie algebra formula_113 of vector fields with an involution σ. The Killing form is negative definite on the +1 eigenspace of σ and positive definite on the −1 eigenspace. As a group \"H\" normalizes formula_113 since the two subgroups \"K\" and \"AN\" do. The +1 eigenspace corresponds to the Lie algebra of \"K\". Similarly the Lie algebras of the linear group \"AN\" and the affine group \"N\" lie in formula_113. Since the group \"G\" has trivial center, the map into GL(formula_113) is injective. Since \"K\" is compact, its image in GL(formula_113) is compact. Since the Lie algebra formula_113 is compatible with that of \"AN\", the image of \"AN\" is closed. Hence the image of the product is closed, since the image of \"K\" is compact. Since it is a closed subgroup, it follows that \"H\" is a Lie group.\n\nEuclidean Jordan algebras can be used to construct Hermitian symmetric spaces of tube type. The remaining Hermitian symmetric spaces are Siegel domains of the second kind. They can be constructed using Euclidean [[Jordan triple system]]s, a generalization of Euclidean Jordan algebras. In fact for a Euclidean Jordan algebra \"E\" let\n\nThen \"L\"(\"a\",\"b\") gives a bilinear map into End \"E\" such that\n\nand\n\nAny such bilinear system is called a Euclidean Jordan triple system. By definition the operators \"L\"(\"a\",\"b\") form a Lie subalgebra of End \"E\".\n\nThe [[Kantor–Koecher–Tits construction]] gives a one-one correspondence between Jordan triple systems and 3-graded Lie algebras\n\nsatisfying\n\nand equipped with an involutive automorphism σ reversing the grading. In this case\n\ndefines a Jordan triple system on formula_125. In the case of Euclidean Jordan algebras or triple systems the Kantor–Koecher–Tits construction can be identified with the Lie algebra of the Lie group of all homomorphic automorphisms of the corresponding [[bounded symmetric domain]].\nThe Lie algebra is constructed by taking formula_126 to be the Lie subalgebra formula_127 of End \"E\" generated by the L(\"a\",\"b\") and formula_128 to be copies of \"E\". The Lie bracket is given by\n\nand the involution by\n\nThe [[Killing form]] is given by\n\nwhere β(\"T\",\"T\") is the symmetric bilinear form defined by\n\nThese formulas, originally derived for Jordan algebras, work equally well for Jordan triple systems.\nThe account in develops the theory of [[bounded symmetric domain]]s starting from the standpoint of 3-graded Lie algebras. For a given finite-dimensional vector space \"E\", Koecher considers finite-dimensional Lie algebras formula_32 of vector fields on \"E\" with polynomial coefficients of degree ≤ 2. formula_125 consists of the constant vector fields ∂ and formula_135 must contain the [[Euler operator]] \"H\" = ∑ \"x\"⋅∂ as a central element. Requiring the existence of an involution σ leads directly to a Jordan triple structure on \"V\" as above. As for all Jordan triple structures, fixing \"c\" in \"E\",\nthe operators \"L\"(\"a\") = \"L\"(\"a\",\"c\") give \"E\" a Jordan algebra structure, determined by \"e\". The operators \"L\"(\"a\",\"b\") themselves come from a Jordan algebra structure as above if and only if there are additional operators \"E\" in formula_128 so that \"H\", \"E\" give a copy of formula_137. The corresponding Weyl group element implements the involution σ. This case corresponds to that of Euclidean Jordan algebras.\n\nThe remaining cases are constructed uniformly by Koecher using involutions of simple Euclidean Jordan algebras. Let \"E\" be a simple Euclidean Jordan algebra and τ a Jordan algebra automorphism of \"E\" of period 2. Thus \"E\" = \"E\" ⊕ \"E\" has an eigenspace decomposition for τ with \"E\" a Jordan subalgebra and \"E\" a module. Moreover, a product of two elements in \"E\" lies in \"E\". For \"a\", \"b\", \"c\" in \"E\", set\n\nand (\"a\",\"b\")= Tr \"L\"(\"ab\"). Then \"F\" = \"E\" is a simple Euclidean Jordan triple system, obtained by restricting the triple system on \"E\" to \"F\". Koecher exhibits explicit involutions of simple Euclidean Jordan algebras directly (see below). These Jordan triple systems correspond to irreducible Hermitian symmetric spaces given by Siegel domains of the second kind. In Cartan's listing, their compact duals are SU(\"p\" + \"q\")/S(U(\"p\") × U(\"q\")) with \"p\" ≠ \"q\" (AIII), SO(2\"n\")/U(\"n\") with \"n\" odd (DIII) and E/SO(10) × U(1) (EIII).\n\nExamples\n\nThe classification of Euclidean Jordan triple systems has been achieved by generalizing the methods of Jordan, von Neumann and Wigner, but the proofs are more involved. Prior differential geometric methods of , invoking a 3-graded Lie algebra, and of , lead to a more rapid classification.\n\n\n[[Category:Convex geometry]]\n[[Category:Non-associative algebras]]\n[[Category:Lie algebras]]\n[[Category:Lie groups]]\n[[Category:Several complex variables]]"}
{"id": "5052341", "url": "https://en.wikipedia.org/wiki?curid=5052341", "title": "Tallyman", "text": "Tallyman\n\nA tallyman is an individual who keeps a numerical record with tally marks, historically often on tally sticks.\nThe heavy metal singer Udo Dirkschneider produced a song called Tallyman.\n\nIn Ireland, it is common for political parties to provide private observers when ballot boxes are opened. These \"tallymen\" keep a tally of the preferences of visible voting papers and allow an early initial estimate of which candidates are likely to win in the drawn-out single transferable vote counting process. Since the public voting process is by then complete, it is usual for tallymen from different parties to share information.\n\nAnother possible definition is a person who called to literally do a head count, presumably on behalf of either the town council or the house owners. This is rumoured to have occurred in Liverpool, in the years after the First World War. Mechanical tally counters can make such head counts easier, by removing the need to make any marks.\n\nIn poorer parts of England (including the north and the East End of London), the tallyman was the hire purchase collector, who visited each week to collect the payments for goods purchased on the 'never never', or hire purchase. These people still had such employment up until the 1960s. \n\nThe title \"tallyman\" extended to the keeper of a village pound as animals were often held against debts, and tally sticks were used to prove they could be released. \n\nThe credit information company Experian Tallyman markets debt collection management software called \"Tallyman\", a product originally purchased from Talgentra\n\nIn 1967 Graham Gouldman wrote a song called \"Tallyman\", which was recorded by Jeff Beck and reached #30 on the British charts.\n\n\"'The tallyman,' Mum told me, 'slice off the top of the stems of the bunches as they take them in. Then him count the little stubs he just sliced off and pay the farmer.'\" explains a Ms. Wade in Andrea Levy’s novel \"Fruit of the Lemon\". Harry Belafonte addresses the tallyman in his Banana Boat Song.\n\nThe \"Tally Man\" is the name of two super villains in the DC Universe, usually enemies of Batman. The original was a \"collector\" of human lives, having killed a criminal debt collecter in his boyhood.\n\n"}
{"id": "12891058", "url": "https://en.wikipedia.org/wiki?curid=12891058", "title": "Triangulation (computer vision)", "text": "Triangulation (computer vision)\n\nIn computer vision triangulation refers to the process of determining a point in 3D space given its projections onto two, or more, images. In order to solve this problem it is necessary to know the parameters of the camera projection function from 3D to 2D for the cameras involved, in the simplest case represented by the camera matrices. Triangulation is sometimes also referred to as reconstruction.\n\nThe triangulation problem is in theory trivial. Since each point in an image corresponds to a line in 3D space, all points on the line in 3D are projected to the point in the image. If a pair of corresponding points in two, or more images, can be found it must be the case that they are the projection of a common 3D point x. The set of lines generated by the image points must intersect at x (3D point) and the algebraic formulation of the coordinates of x (3D point) can be computed in a variety of ways, as is presented below.\n\nIn practice, however, the coordinates of image points cannot be measured with arbitrary accuracy. Instead, various types of noise, such as geometric noise from lens distortion or interest point detection error, lead to inaccuracies in the measured image coordinates. As a consequence, the lines generated by the corresponding image points do not always intersect in 3D space. The problem, then, is to find a 3D point which optimally fits the measured image points. In the literature there are multiple proposals for how to define optimality and how to find the optimal 3D point. Since they are based on different optimality criteria, the various methods produce different estimates of the 3D point x when noise is involved.\n\nIn the following, it is assumed that triangulation is made on corresponding image points from two views generated by pinhole cameras. Generalization from these assumptions are discussed here.\n\nThe image to the left illustrates the epipolar geometry of a pair of stereo cameras of pinhole model. A point x (3D point) in 3D space is projected onto the respective image plane along a line (green) which goes through the camera's focal point, formula_1 and formula_2, resulting in the two corresponding image points formula_3 and formula_4. If formula_3 and formula_4 are given and the geometry of the two cameras are known, the two projection lines (green lines) can be determined and it must be the case that they intersect at point x (3D point).Using basic linear algebra that intersection point can be determined in a straightforward way.\n\nThe image to the right shows the real case. The position of the image points formula_3 and formula_4 cannot be measured exactly. The reason is a combination of factors such as \n\n\nAs a consequence, the measured image points are formula_9 and formula_10 instead of formula_3 and formula_4. However, their projection lines (blue) do not have to intersect in 3D space or come close to x. In fact, these lines intersect if and only if formula_9 and formula_10 satisfy the epipolar constraint defined by the fundamental matrix. Given the measurement noise in formula_9 and formula_10 it is rather likely that the epipolar constraint is not satisfied and the projection lines do not intersect.\n\nThis observation leads to the problem which is solved in triangulation. Which 3D point x is the best estimate of x given formula_9 and formula_10 and the geometry of the cameras? The answer is often found by defining an error measure which depends on x and then minimize this error. In the following some of the various methods for computing x presented in the literature are briefly described.\n\nAll triangulation methods produce x = x in the case that formula_19 and formula_20, that is, when the epipolar constraint is satisfied (except for singular points, see below). It is what happens when the constraint is not satisfied which differs between the methods.\n\nA triangulation method can be described in terms of a function formula_21 such that\n\nwhere formula_23 are the homogeneous coordinates of the detected image points and formula_24 are the camera matrices. x (3D point) is the homogeneous representation of the resulting 3D point. The formula_25 sign implies that formula_21 is only required to produce a vector which is equal to x up to a multiplication by a non-zero scalar since homogeneous vectors are involved.\n\nBefore looking at the specific methods, that is, specific functions formula_21, there are some general concepts related to the methods that need to be explained. Which triangulation method is chosen for a particular problem depends to some extent on these characteristics.\n\nSome of the methods fail to correctly compute an estimate of x (3D point) if it lies in a certain subset of the 3D space, corresponding to some combination of formula_28. A point in this subset is then a \"singularity\" of the triangulation method. The reason for the failure can be that some equation system to be solved is under-determined or that the projective representation of x becomes the zero vector for the singular points.\n\nIn some applications, it is desirable that the triangulation is independent of the coordinate system used to represent 3D points; if the triangulation problem is formulated in one coordinate system and then transformed into another the resulting estimate x should transform in the same way. This property is commonly referred to as \"invariance\". Not every triangulation method assures invariance, at least not for general types of coordinate transformations.\n\nFor a homogeneous representation of 3D coordinates, the most general transformation is a projective transformation, represented by a formula_29 matrix formula_30. If the homogeneous coordinates are transformed according to\n\nthen the camera matrices must transform as (C)\n\nto produce the same homogeneous image coordinates (y)\n\nIf the triangulation function formula_34 is invariant to formula_30 then the following relation must be valid\n\nfrom which follows that\n\nFor each triangulation method, it can be determined if this last relation is valid. If it is, it may be satisfied only for a subset of the projective transformations, for example, rigid or affine transformations.\n\nThe function formula_34 is only an abstract representation of a computation which, in practice, may be relatively complex. Some methods result in a formula_34 which is a closed-form continuous function while others need to be decomposed into a series of computational steps involving, for example, SVD or finding the roots of a polynomial. Yet another class of methods results in formula_34 which must rely on iterative estimation of some parameters. This means that both the computation time and the complexity of the operations involved may vary between the different methods.\n\nEach of the two image points formula_9 and formula_10 has a corresponding projection line (blue in the right image above), here denoted as formula_44 and formula_45, which can be determined given the camera matrices formula_24. Let formula_47 be a distance function between a (3D line) L' and a x (3D point) such that\n\nThe midpoint method finds the point x which minimizes\n\nIt turns out that x lies exactly at the middle of the shortest line segment which joins the two projection lines.\n"}
{"id": "1077843", "url": "https://en.wikipedia.org/wiki?curid=1077843", "title": "Yutaka Taniyama", "text": "Yutaka Taniyama\n\nYutaka Taniyama (Japanese: 谷山 豊 \"Taniyama Yutaka\"; 12 November 1927, Kisai near Tokyo – 17 November 1958, Tokyo) was a Japanese mathematician known for the Taniyama–Shimura conjecture.\n\nTaniyama was best known for conjecturing, in modern language, automorphic properties of L-functions of elliptic curves over any number field. A partial and refined case of this conjecture for elliptic curves over rationals is called the Taniyama–Shimura conjecture or the modularity theorem whose statement he subsequently refined in collaboration with Goro Shimura. The names Taniyama, Shimura and Weil have all been attached to this conjecture, but the idea is essentially due to Taniyama.\n\nIn 1986 Ribet proved that if the Taniyama–Shimura conjecture held, then so would Fermat's last theorem, which inspired Andrew Wiles to work for a number of years in secrecy on it, and to prove enough of it to prove Fermat's Last Theorem. Owing to the pioneering contribution of Wiles and the efforts of a number of mathematicians the Taniyama–Shimura conjecture was finally proven in 1999. The original Taniyama conjecture for elliptic curves over arbitrary number fields remains open.\n\nIn 1958, Taniyama worked for University of Tokyo as an assistant (joshu), was engaged, and was offered a position at the Institute for Advanced Study, Princeton, New Jersey. On 17 November 1958, Taniyama committed suicide. He left a note explaining how far he had progressed with his teaching duties, and apologizing to his colleagues for the trouble he was causing them. His suicide note read:\nUntil yesterday I had no definite intention of killing myself. But more than a few must have noticed that lately I have been tired both physically and mentally. As to the cause of my suicide, I don't quite understand it myself, but it is not the result of a particular incident, nor of a specific matter. Merely may I say, I am in the frame of mind that I lost confidence in my future. There may be someone to whom my suicide will be troubling or a blow to a certain degree. I sincerely hope that this incident will cast no dark shadow over the future of that person. At any rate, I cannot deny that this is a kind of betrayal, but please excuse it as my last act in my own way, as I have been doing my own way all my life.\n\nAlthough his note is mostly enigmatic it does mention tiredness and a loss of confidence in his future. Taniyama's ideas had been criticized as unsubstantiated and his behavior had occasionally been deemed peculiar. Goro Shimura mentioned that he suffered from depression. Taniyama also mentioned in the note his concern that some might be harmed by his suicide and his hope that the act would not cast \"a dark shadow over that person.\"\n\nAbout a month later, Misako Suzuki, the woman whom he was planning to marry, also committed suicide, leaving a note reading:\n\"We promised each other that no matter where we went, we would never be separated. Now that he is gone, I must go too in order to join him.\"\n\nAfter Taniyama's death, Goro Shimura stated that:\nHe was always kind to his colleagues, especially to his juniors, and he genuinely cared about their welfare. He was the moral support of many of those who came into mathematical contact with him, including of course myself. Probably he was never conscious of this role he was playing. But I feel his noble generosity in this respect even more strongly now than when he was alive. And yet nobody was able to give him any support when he desperately needed it. Reflecting on this, I am overwhelmed by the bitterest grief.\n\nIn a 2011 TED talk by English economist Tim Harford titled, \"Trial, error and the God complex,\" Taniyama is referenced as a mathematician who was ultimately unable to prove his conjecture during his lifetime. Reflecting on Taniyama's work, Goro Shimura stated:\nHe was not a very careful person as a mathematician. He made a lot of mistakes. But he made mistakes in a good direction. I tried to imitate him. But I've realized that it's very difficult to make good mistakes. <\"Fermat's Last Theorem,\" Horizon, 1995>\n\n\n\n"}
{"id": "24632336", "url": "https://en.wikipedia.org/wiki?curid=24632336", "title": "Ψ₀(Ωω)", "text": "Ψ₀(Ωω)\n\nIn mathematics, Ψ(Ω) is a large countable ordinal that is used to measure the proof-theoretic strength of some mathematical systems. In particular, it is the proof theoretic ordinal of the subsystem formula_1-CA of second-order arithmetic; this is one of the \"big five\" subsystems studied in reverse mathematics (Simpson 1999).\n\n\n"}
