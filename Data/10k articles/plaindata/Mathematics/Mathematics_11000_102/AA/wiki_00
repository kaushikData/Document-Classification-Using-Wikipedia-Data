{"id": "57297191", "url": "https://en.wikipedia.org/wiki?curid=57297191", "title": "Alan Gaius Ramsay McIntosh", "text": "Alan Gaius Ramsay McIntosh\n\nAlan Gaius Ramsay McIntosh (* 1942 in Sydney, † August 8, 2016 ) was an Australian mathematician who dealt with analysis (harmonic analysis, partial differential equations). He was a high school teacher at the Australian National University in Canberra.\n\nMcIntosh studied at the University of New England with a bachelor's degree in 1962 (as a student he also received the University Medal ) and PhD in 1966 with Frantisek Wolf at the University of California, Berkeley, ( Representation of Accretive Bilinear Forms in Hilbert Space by Maximal Accretive Operator ). In Berkeley, he was also a student of Tosio Kato . As a post-doctoral student, he was at the Institute for Advanced Study and from 1967 he taught at Macquarie University and from 1999 at the Australian National University. In 2014 he became emeritus.\n\nMcIntosh was involved in solving the Calderon conjecture in the theory of singular integral operators.\n\nIn 2002, he solved with Pascal Auscher, Michael T. Lacey, Philippe Tchamitchian and Steve Hofmann the open Kato root problem for elliptic differential operators.\n\nHe also deals with singular integral operators, boundary value problems of partial differential equations with applications (such as scattering theory of the Maxwell equations in irregular areas), spectral theory and functional calculus of operators in Banach spaces, analysis with Clifford algebras, barriers for the heat kernel equation and functional calculus for elliptic partial differential operators.\n\nIn 1986 he became a member of the Australian Academy of Sciences, whose Hannan Medal he received in 2015. In 2002 he received the Moyal Medal from Macquarie University.\n\n"}
{"id": "1147024", "url": "https://en.wikipedia.org/wiki?curid=1147024", "title": "Buddhabrot", "text": "Buddhabrot\n\nThe Buddhabrot is a fractal rendering technique related to the Mandelbrot set. Its name reflects its pareidolic resemblance to classical depictions of Gautama Buddha, seated in a meditation pose with a forehead mark (\"tikka\") and traditional topknot (\"ushnisha\").\n\nThe \"Buddhabrot\" rendering technique was discovered by Melinda Green (then known as Dan Green), who later described it in a 1993 Usenet post to sci.fractals.\n\nPrevious researchers had come very close to finding the precise Buddhabrot technique. In 1988, Linas Vepstas relayed similar images to Cliff Pickover for inclusion in Pickover's then-forthcoming book \"Computers, Pattern, Chaos, and Beauty\". This led directly to the discovery of Pickover stalks. However, these researchers did not filter out non-escaping trajectories required to produce the ghostly forms reminiscent of Hindu art. The inverse, \"Anti-Buddhabrot\" filter produces images similar to no filtering.\n\nGreen first named this pattern Ganesh, since an Indian co-worker \"instantly recognized it as the god 'Ganesha' which is the one with the head of an elephant.\" The name \"Buddhabrot\" was coined later by Lori Gardi.\n\nMathematically, the Mandelbrot set consists of the set of points formula_1 in the complex plane for which the iteratively defined sequence\n\ndoes \"not\" tend to infinity as formula_3 goes to infinity for formula_4.\n\nThe \"Buddhabrot\" image can be constructed by first creating a 2-dimensional array of boxes, each corresponding to a final pixel in the image. Each box formula_5 for formula_6 and formula_7 has size in complex coordinates of formula_8 and formula_9, where formula_10 and formula_11 for an image of width formula_12 and height formula_13. For each box, a corresponding counter is initialized to zero. Next, a random sampling of formula_1 points are iterated through the Mandelbrot function. For points which do escape within a chosen maximum number of iterations, and therefore are \"not\" in the Mandelbrot set, the counter for each box entered during the escape to infinity is incremented by 1. In other words, for each sequence corresponding to formula_1 that escapes, for each point formula_16 during the escape, the box that formula_17 lies within is incremented by 1. Points which do not escape within the maximum number of iterations (and considered to be in the Mandelbrot set) are discarded. After a large number of formula_1 values have been iterated, grayscale shades are then chosen based on the distribution of values recorded in the array. The result is a density plot highlighting regions where formula_16 values spend the most time on their way to infinity.\n\nRendering \"Buddhabrot\" images is typically more computationally intensive than standard Mandelbrot rendering techniques. This is partly due to requiring more random points to be iterated than pixels in the image in order to build up a sharp image. Rendering highly zoomed areas requires even more computation than for standard Mandelbrot images in which a given pixel can be computed directly regardless of zoom level. Conversely, a pixel in a zoomed region of a Buddhabrot image can be affected by initial points from regions far outside the one being rendered. Without resorting to more complex probabilistic techniques, rendering zoomed portions of \"Buddhabrot\" consists of merely cropping a large full sized rendering.\n\nThe maximum number of iterations chosen affects the image — higher values give sparser more detailed appearance, as a few of the points pass through a large number of pixels before they escape, resulting in their paths being more prominent. If a lower maximum was used, these points would not escape in time and would be regarded as not escaping at all.\n\nGreen later realized that this provided a natural way to create color Buddhabrot images by taking three such grayscale images, differing only by the maximum number of iterations used, and combining them into a single color image using the same method used by astronomers to create false color images of nebula and other celestial objects. For example, one could assign a 2,000 max iteration image to the red channel, a 200 max iteration image to the green channel, and a 20 max iteration image to the blue channel of an image in an RGB color space. Some have labelled Buddhabrot images using this technique \"Nebulabrots\".\n\nThe relationship between the Mandelbrot set as defined by the iteration formula_20, and the logistic map formula_21 is well known. The two are related by the quadratic transformation:\n\nformula_22\n\nThe traditional way of illustrating this relationship is aligning the logistic map and the Mandelbrot set through the relation between formula_23 and formula_24, using a common x-axis and a different y-axis, showing a one-dimensional relationship.\n\nMelinda Green discovered 'by accident' that the Anti-Buddhabrot paradigm fully integrates the logistic map. Both are based on tracing paths from non-escaping points, iterated from a (random) starting point, and the iteration functions are related by the transformation given above. It is then easy to see that the Anti-Buddhabrot for formula_20, plotting paths with formula_26 and formula_27, simply generates the logistic map in the plane formula_28, when using the given transformation. For rendering purposes we use formula_29. In the logistic map, all formula_30 ultimately generate the same path.\n\nBecause both the Mandelbrot set and the logistic map are an integral part of the Anti-Buddhabrot we can now show a 3D relationship between both, using the 3D axes\nformula_31. The animation shows the classic Anti-Buddhabrot with formula_32 and formula_27, this is the 2D Mandelbrot set in the plane\nformula_34, and also the Anti-Buddhabrot with formula_26 and formula_27, this is the 2D logistic map in the plane formula_28. We rotate the plane formula_38 around the formula_23-axis, first showing formula_34, then rotating 90° to show formula_28, then rotating an extra 90° to show formula_42. We could rotate an extra 180° but this gives the same images, mirrored around the formula_23-axis.\n\nThe logistic map Anti-Buddhabrot is in fact a subset of the classic Anti-Buddhabrot, situated in the plane formula_28 (or formula_45) of 3D formula_31, perpendicular to the plane formula_34. We emphasize this by showing briefly, at 90° rotation, only the projected plane formula_45, not 'disturbed' by the projections of the planes with non-zero formula_49.\n"}
{"id": "1139338", "url": "https://en.wikipedia.org/wiki?curid=1139338", "title": "Computable function", "text": "Computable function\n\nComputable functions are the basic objects of study in computability theory. Computable functions are the formalized analogue of the intuitive notion of algorithm, in the sense that a function is computable if there exists an algorithm that can do the job of the function, i.e. given an input of the function domain it can return the corresponding output. Computable functions are used to discuss computability without referring to any concrete model of computation such as Turing machines or register machines. Any definition, however, must make reference to some specific model of computation but all valid definitions yield the same class of functions.\nParticular models of computability that give rise to the set of computable functions are the Turing-computable functions and the μ-recursive functions.\n\nBefore the precise definition of computable function, mathematicians often used the informal term \"effectively calculable\". This term has since come to be identified with the computable functions. Note that the effective computability of these functions does not imply that they can be \"efficiently\" computed (i.e. computed within a reasonable amount of time). In fact, for some effectively calculable functions it can be shown that any algorithm that computes them will be very inefficient in the sense that the running time of the algorithm increases exponentially (or even superexponentially) with the length of the input. The fields of feasible computability and computational complexity study functions that can be computed efficiently.\n\nAccording to the Church–Turing thesis, computable functions are exactly the functions that can be calculated using a mechanical calculation device given unlimited amounts of time and storage space. Equivalently, this thesis states that a function is computable if and only if it has an algorithm. Note that an algorithm in this sense is understood to be a sequence of steps a person with unlimited time and an unlimited supply of pen and paper could follow.\n\nThe Blum axioms can be used to define an abstract computational complexity theory on the set of computable functions. In computational complexity theory, the problem of determining the complexity of a computable function is known as a function problem.\n\nComputability of a function is an informal notion. One way to describe it is to say that a function is computable if its value can be obtained by an effective procedure. With more rigor, a function formula_1\nis computable if and only if there is an effective procedure that, given any -tuple formula_2 of natural numbers, will produce the value formula_3. In agreement with this definition, the remainder of this article presumes that computable functions take finitely many natural numbers as arguments and produce a value which is a single natural number.\n\nAs counterparts to this informal description, there exist multiple formal, mathematical definitions. The class of computable functions can be defined in many equivalent models of computation, including\nAlthough these models use different representations for the functions, their inputs and their outputs, translations exist between any two models, and so every model describes essentially the same class of functions, giving rise to the opinion that formal computability is both natural and not too narrow.\n\nFor example, one can formalize computable functions as μ-recursive functions, which are partial functions that take finite tuples of natural numbers and return a single natural number (just as above). They are the smallest class of partial functions that includes the constant, successor, and projection functions, and is closed under composition, primitive recursion, and the μ operator.\n\nEquivalently, computable functions can be formalized as functions which can be calculated by an idealized computing agent such as a Turing machine or a register machine. Formally speaking, a partial function formula_1 can be calculated if and only if there exists a computer program with the following properties:\n\nThe basic characteristic of a computable function is that there must be a finite procedure (an algorithm) telling how to compute the function. The models of computation listed above give different interpretations of what a procedure is and how it is used, but these interpretations share many properties. The fact that these models give equivalent classes of computable functions stems from the fact that each model is capable of reading and mimicking a procedure for any of the other models, much as a compiler is able to read instructions in one computer language and emit instructions in another language.\n\nEnderton [1977] gives the following characteristics of a procedure for computing a computable function; similar characterizations have been given by Turing [1936], Rogers [1967], and others.\nThus every computable function must have a finite program that completely describes how the function is to be computed. It is possible to compute the function by just following the instructions; no guessing or special insight is required.\nIntuitively, the procedure proceeds step by step, with a specific rule to cover what to do at each step of the calculation. Only finitely many steps can be carried out before the value of the function is returned.\nThus if a value for \"f(x)\" is ever found, it must be the correct value. It is not necessary for the computing agent to distinguish correct outcomes from incorrect ones because the procedure is always correct when it produces an outcome.\n\nEnderton goes on to list several clarifications of these 3 requirements of the procedure for a computable function:\n\nTo summarise, based on this view a function is computable if: (a) given an input from its domain, possibly relying on unbounded storage space, it can give the corresponding output by following a procedure (program, algorithm) that is formed by a finite number of exact unambiguous instructions; (b) it returns such output (halts) in a finite number of steps; and (c) if given an input which is not in its domain it either never halts or it gets stuck.\n\nThe field of computational complexity studies functions with prescribed bounds on the time and/or space allowed in a successful computation.\n\nA set of natural numbers is called computable (synonyms: recursive, decidable) if there is a computable, total function such that for any natural number , if is in and if is not in .\n\nA set of natural numbers is called computably enumerable (synonyms: recursively enumerable, semidecidable) if there is a computable function such that for each number , is defined if and only if is in the set. Thus a set is computably enumerable if and only if it is the domain of some computable function. The word \"enumerable\" is used because the following are equivalent for a nonempty subset of the natural numbers:\nIf a set is the range of a function then the function can be viewed as an\nenumeration of , because the list will include every element of .\n\nBecause each finitary relation on the natural numbers can be identified with a corresponding set of finite sequences of natural numbers, the notions of computable relation and computably enumerable relation can be defined from their analogues for sets.\n\nIn computability theory in computer science, it is common to consider formal languages. An alphabet is an arbitrary set. A word on an alphabet is a finite sequence of symbols from the alphabet; the same symbol may be used more than once. For example, binary strings are exactly the words on the alphabet . A language is a subset of the collection of all words on a fixed alphabet. For example, the collection of all binary strings that contain exactly 3 ones is a language over the binary alphabet.\n\nA key property of a formal language is the level of difficulty required to decide whether a given word is in the language. Some coding system must be developed to allow a computable function to take an arbitrary word in the language as input; this is usually considered routine. A language is called computable (synonyms: recursive, decidable) if there is a computable function such that for each word over the alphabet, if the word is in the language and if the word is not in the language. Thus a language is computable just in case there is a procedure that is able to correctly tell whether arbitrary words are in the language.\n\nA language is computably enumerable (synonyms: recursively enumerable, semidecidable) if there is a computable function such that is defined if and only if the word is in the language. The term \"enumerable\" has the same etymology as in computably enumerable sets of natural numbers.\n\nThe following functions are computable:\n\n\nIf \"f\" and \"g\" are computable, then so are: \"f\" + \"g\", \"f\" * \"g\", formula_10 if\n\"f\" is unary, max(\"f\",\"g\"), min(\"f\",\"g\"), arg max{\"y\" ≤ \"f\"(\"x\")} and many more combinations.\n\nThe following examples illustrate that a function may be computable though it is not known which algorithm computes it.\n\n\nThe Church–Turing thesis states that any function computable from a procedure possessing the three properties listed above is a computable function. Because these three properties are not formally stated, the Church–Turing thesis cannot be proved. The following facts are often taken as evidence for the thesis:\n\nThe Church–Turing thesis is sometimes used in proofs to justify that a particular function is computable by giving a concrete description of a procedure for the computation. This is permitted because it is believed that all such uses of the thesis can be removed by the tedious process of writing a formal procedure for the function in some model of computation.\n\nGiven a function (or, similarly, a set), one may be interested not only if it is computable, but also whether this can be \"proven\" in a particular proof system (usually first order Peano arithmetic). A function that can be proven to be computable is called provably total.\n\nThe set of provably total functions is recursively enumerable: one can enumerate all the provably total functions by enumerating all their corresponding proofs, that prove their computability. This can be done by enumerating all the proofs of the proof system and ignoring irrelevant ones.\n\nIn a function defined by a recursive definition, each value is defined by a fixed first-order formula of other, previously defined values of the same function or other functions, which might be simply constants. A subset of these is the primitive recursive functions. Every such function is provably total: For such a k-ary function \"f\", each value formula_11 can be computed by following the definition backwards, iteratively, and after finite number of iteration (as can be easily proven), a constant is reached.\n\nThe converse is not true, as not every provably total function is primitive recursive. Indeed, one can enumerate all the primitive recursive functions and define a function \"en\" such that for all \"n\", \"m\": \"en\"(\"n\",\"m\") = \"f\"(\"m\"), where \"f\" is the n-th primitive recursive function (for k-ary functions, this will be set to \"f\"(\"m\",\"m\"...\"m\")). Now, \"g\"(\"n\") = \"en\"(\"n\",\"n\")+1 is provably total but not primitive recursive, by a diagonalization argument: had there been a \"j\" such that \"g\" = \"f\", we would have got \"g\"(\"j\") = \"en\"(\"j\",\"j\")+1 = \"f\" (\"j\")+1= \"g\"(\"j\")+1, a contradiction. (It should be noted that the Gödel numbers of all primitive recursive functions \"can\" be enumerated by a primitive recursive function, though the primitive recursive functions' values cannot).\n\nOne such function, which is provable total but not primitive recursive, is Ackermann function: since it is recursively defined, it is indeed easy to prove its computability (However, a similar diagonalization argument can also be built for all functions defined by recursive definition; thus, there are provable total functions that cannot be defined recursively ).\n\nIn a sound proof system, every provably total function is indeed total, but the converse is not true: in every first-order proof system that is strong enough and sound (including Peano arithmetic), one can prove (in another proof system) the existence of total functions that cannot be proven total in the proof system.\n\nIf the total computable functions are enumerated via the Turing machines that produces them, then the above statement can be shown, if the proof system is sound, by a similar diagonalization argument to that used above, using the enumeration of provably total functions given earlier. One uses a Turing machine that enumerates the relevant proofs, and for every input \"n\" calls \"f\"(\"n\") (where \"f\" is \"n\"-th function by \"this\" enumeration) by invoking the Turing machine that computes it according to the n-th proof. Such a Turing machine is guaranteed to halt if the proof system is sound.\n\nEvery computable function has a finite procedure giving explicit, unambiguous instructions on how to compute it. Furthermore, this procedure has to be encoded in the finite alphabet used by the computational model, so there are only countably many computable functions. For example, functions may be encoded using a string of bits (the alphabet }).\n\nThe real numbers are uncountable so most real numbers are not computable. See computable number. The set of finitary functions on the natural numbers is uncountable so most are not computable. Concrete examples of such functions are Busy beaver, Kolmogorov complexity, or any function that outputs the digits of a noncomputable number, such as Chaitin's constant.\n\nSimilarly, most subsets of the natural numbers are not computable. The halting problem was the first such set to be constructed. The Entscheidungsproblem, proposed by David Hilbert, asked whether there is an effective procedure to determine which mathematical statements (coded as natural numbers) are true. Turing and Church independently showed in the 1930s that this set of natural numbers is not computable. According to the Church–Turing thesis, there is no effective procedure (with an algorithm) which can perform these computations.\n\nThe notion of computability of a function can be relativized to an arbitrary set of natural numbers \"A\". A function \"f\" is defined to be computable in \"A (equivalently A\"-computable or computable relative to \"A) when it satisfies the definition of a computable function with modifications allowing access to \"A\" as an oracle. As with the concept of a computable function relative computability can be given equivalent definitions in many different models of computation. This is commonly accomplished by supplementing the model of computation with an additional primitive operation which asks whether a given integer is a member of \"A\". We can also talk about \"f\" being computable in \"g by identifying \"g\" with its graph.\n\nHyperarithmetical theory studies those sets that can be computed from a computable ordinal number of iterates of the Turing jump of the empty set. This is equivalent to sets defined by both a universal and existential formula in the language of second order arithmetic and to some models of Hypercomputation. Even more general recursion theories have been studied, such as E-recursion theory in which any set can be used as an argument to an E-recursive function.\n\nAlthough the Church–Turing thesis states that the computable functions include all functions with algorithms, it is possible to consider broader classes of functions that relax the requirements that algorithms must possess. The field of Hypercomputation studies models of computation that go beyond normal Turing computation.\n\n\n"}
{"id": "40512332", "url": "https://en.wikipedia.org/wiki?curid=40512332", "title": "Continuum percolation theory", "text": "Continuum percolation theory\n\nIn mathematics and probability theory, continuum percolation theory is a branch of mathematics that extends discrete percolation theory to continuous space (often Euclidean space ). More specifically, the underlying points of discrete percolation form types of lattices whereas the underlying points of continuum percolation are often randomly positioned in some continuous space and form a type of point process. For each point, a random shape is frequently placed on it and the shapes overlap each with other to form clumps or components. As in discrete percolation, a common research focus of continuum percolation is studying the conditions of occurrence for infinite or giant components. Other shared concepts and analysis techniques exist in these two types of percolation theory as well as the study of random graphs and random geometric graphs.\n\nContinuum percolation arose from an early mathematical model for wireless networks, which, with the rise of several wireless network technologies in recent years, has been generalized and studied in order to determine the theoretical bounds of information capacity and performance in wireless networks. In addition to this setting, continuum percolation has gained application in other disciplines including biology, geology, and physics, such as the study of porous material and semiconductors, while becoming a subject of mathematical interest in its own right.\n\nIn the early 1960s Edgar Gilbert proposed a mathematical model in wireless networks that gave rise to the field of continuum percolation theory, thus generalizing discrete percolation. The underlying points of this model, sometimes known as the Gilbert disk model, were scattered uniformly in the infinite plane according to a homogeneous Poisson process. Gilbert, who had noticed similarities between discrete and continuum percolation, then used concepts and techniques from the probability subject of branching processes to show that a threshold value existed for the infinite or \"giant\" component.\n\nThe exact names, terminology, and definitions of these models may vary slightly depending on the source, which is also reflected in the use of point process notation.\n\nA number of well-studied models exist in continuum percolation, which are often based on homogeneous Poisson point processes.\n\nConsider a collection of points in the plane that form a homogeneous Poisson process with constant (point) density . For each point of the Poisson process (i.e. , place a disk with its center located at the point . If each disk has a random radius (from a common distribution) that is independent of all the other radii and all the underlying points , then the resulting mathematical structure is known as a random disk model.\n\nGiven a random disk model, if the set union of all the disks is taken, then the resulting structure is known as a Boolean–Poisson model (also known as simply the Boolean model), which is a commonly studied model in continuum percolation as well as stochastic geometry. If all the radii are set to some common constant, say, , then the resulting model is sometimes known as the Gilbert disk (Boolean) model.\n\nThe disk model can be generalized to more arbitrary shapes where, instead of a disk, a random compact (hence bounded and closed in ) shape is placed on each point . Again, each shape has a common distribution and independent to all other shapes and the underlying (Poisson) point process. This model is known as the germ–grain model where the underlying points are the \"germs\" and the random compact shapes are the \"grains\". The set union of all the shapes forms a Boolean germ-grain model. Typical choices for the grains include disks, random polygon and segments of random length.\n\nBoolean models are also examples of stochastic processes known as coverage processes. The above models can be extended from the plane to general Euclidean space .\n\nIn the Boolean–Poisson model, disks there can be isolated groups or clumps of disks that do not contact any other clumps of disks. These clumps are known as components. If the area (or volume in higher dimensions) of a component is infinite, one says it is an infinite or \"giant\" component. A major focus of percolation theory is establishing the conditions when giant components exist in models, which has parallels with the study of random networks. If no big component exists, the model is said to be subcritical. The conditions of giant component criticality naturally depend on parameters of the model such as the density of the underlying point process.\n\nThe excluded area of a placed object is defined as the minimal area around the object into which an additional object cannot be placed without overlapping with the first object. For example, in a system of randomly oriented homogeneous rectangles of length , width and aspect ratio , the average excluded area is given by:\n\nIn a system of identical ellipses with semi-axes and and ratio , and perimeter , the average excluded areas is given by:\n\nThe excluded area theory states that the critical number density (percolation threshold) of a system is inversely proportional to the average excluded area :\nIt has been shown via Monte-Carlo simulations that percolation threshold in both homogeneous and heterogeneous systems of rectangles or ellipses is dominated by the average excluded areas and can be approximated fairly well by the linear relation\nwith a proportionality constant in the range 3.1–3.5.\n\nThe applications of percolation theory are various and range from material sciences to wireless communication systems. Often the work involves showing that a type of phase transition occurs in the system.\n\nWireless networks are sometimes best represented with stochastic models owing to their complexity and unpredictability, hence continuum percolation have been used to develop stochastic geometry models of wireless networks. For example, the tools of continuous percolation theory and coverage processes have been used to study the coverage and connectivity of sensor networks. One of the main limitations of these networks is energy consumption where usually each node has a battery and an embedded form of energy harvesting. To reduce energy consumption in sensor networks, various sleep schemes have been suggested that entail having a subcollection of nodes go into a low energy-consuming sleep mode. These sleep schemes obviously affect the coverage and connectivity of sensor networks. Simple power-saving models have been proposed such as the simple uncoordinated 'blinking' model where (at each time interval) each node independently powers down (or up) with some fixed probability. Using the tools of percolation theory, a blinking Boolean Poisson model has been analyzed to study the latency and connectivity effects of such a simple power scheme.\n\n"}
{"id": "3989260", "url": "https://en.wikipedia.org/wiki?curid=3989260", "title": "Course-of-values recursion", "text": "Course-of-values recursion\n\nIn computability theory, course-of-values recursion is a technique for defining number-theoretic functions by recursion. In a definition of a function \"f\" by course-of-values recursion, the value of \"f\"(\"n\"+1) is computed from the sequence formula_1. \n\nThe fact that such definitions can be converted into definitions using a simpler form of recursion is often used to prove that functions defined by course-of-values recursion are primitive recursive. Contrary to course-of-values recursion, in primitive recursion the computation of a value of a function requires only the previous value; for example, for a 1-ary primitive recursive function \"g\" the value of \"g\"(\"n\"+1) is computed only from \"g\"(\"n\") and \"n\".\n\nThe factorial function \"n\"! is recursively defined by the rules\nThis recursion is a primitive recursion because it computes the next value (\"n\"+1)! of the function based on the value of \"n\" and the previous value \"n\"! of the function. On the other hand, the function Fib(\"n\"), which returns the \"n\"th Fibonacci number, is defined with the recursion equations\nIn order to compute Fib(\"n\"+2), the last \"two\" values of the Fib function are required. Finally, consider the function \"g\" defined with the recursion equations\nTo compute \"g\"(\"n\"+1) using these equations, all the previous values of \"g\" must be computed; no fixed finite number of previous values is sufficient in general for the computation of \"g\". The functions Fib and \"g\" are examples of functions defined by course-of-values recursion. \n\nIn general, a function \"f\" is defined by course-of-values recursion if there is a fixed primitive recursive function \"h\" such that for all \"n\", \nwhere formula_10 is a Gödel number encoding the indicated sequence.\nIn particular\nprovides the initial value of the recursion. The function \"h\" might test its first argument to provide explicit initial values, for instance for Fib one could use the function defined by\nwhere \"s\"[\"i\"] denotes extraction of the element \"i\" from an encoded sequence \"s\"; this is easily seen to be a primitive recursive function (assuming an appropriate Gödel numbering is used).\n\nIn order to convert a definition by course-of-values recursion into a primitive recursion, an auxiliary (helper) function is used. Suppose that one wants to have\nTo define using primitive recursion, first define the auxiliary course-of-values function that should satisfy\nwhere the right hand side is taken to be a Gödel numbering for sequences.\n\nThus formula_15 encodes the first values of . The function formula_16 can be defined by primitive recursion because formula_17 is obtained by appending to formula_15 the new element formula_19:\nwhere computes, whenever encodes a sequence of length , a new sequence of length such that and for all . This is a primitive recursive function, under the assumption of an appropriate Gödel numbering; \"h\" is assumed primitive recursive to begin with. Thus the recursion relation can be written as primitive recursion:\nwhere \"g\" is itself primitive recursive, being the composition of two such functions:\n\nGiven formula_16, the original function can be defined by formula_25, which shows that it is also a primitive recursive function.\n\nIn the context of primitive recursive functions, it is convenient to have a means to represent finite sequences of natural numbers as single natural numbers. One such method, Gödel's encoding, represents a sequence of positive integers formula_26 as\nwhere \"p\" represent the \"i\"th prime. It can be shown that, with this representation, the ordinary operations on sequences are all primitive recursive. These operations include\nUsing this representation of sequences, it can be seen that if \"h\"(\"m\") is primitive recursive then the function\nis also primitive recursive.\n\nWhen the sequence formula_26 is allowed to include zeros, it is instead represented as\nwhich makes it possible to distinguish the codes for the sequences formula_31 and formula_32.\n\nNot every recursive definition can be transformed into a primitive recursive definition. One known example is Ackermann's function, which is of the form \"A\"(\"m\",\"n\") and is provably not primitive recursive.\n\nIndeed, every new value \"A\"(\"m\"+1, \"n\") depends on the sequence of previously defined values \"A\"(\"i\", \"j\"), but the \"i\"-s and \"j\"-s for which values should be included in this sequence depend themselves on previously computed values of the function; namely (\"i\", \"j\") = (\"m\",\"A\"(\"m\"+1,\"n\")). Thus one cannot encode the previously computed sequence of values in a primitive recursive way in the manner suggested above (or at all, as it turns out this function is not primitive recursive).\n\n"}
{"id": "21452705", "url": "https://en.wikipedia.org/wiki?curid=21452705", "title": "Dedekind number", "text": "Dedekind number\n\nIn mathematics, the Dedekind numbers are a rapidly growing sequence of integers named after Richard Dedekind, who defined them in 1897. The Dedekind number \"M\"(\"n\") counts the number of monotonic Boolean functions of \"n\" variables. Equivalently, it counts the number of antichains of subsets of an \"n\"-element set, the number of elements in a free distributive lattice with \"n\" generators, or the number of abstract simplicial complexes with \"n\" elements.\n\nAccurate asymptotic estimates of \"M\"(\"n\") and an exact expression as a summation, are known. However Dedekind's problem of computing the values of \"M\"(\"n\") remains difficult: no closed-form expression for \"M\"(\"n\") is known, and exact values of \"M\"(\"n\") have been found only for \"n\" ≤ 8.\n\nA Boolean function is a function that takes as input \"n\" Boolean variables (that is, values that can be either false or true, or equivalently binary values that can be either 0 or 1), and produces as output another Boolean variable. It is monotonic if, for every combination of inputs, switching one of the inputs from false to true can only cause the output to switch from false to true and not from true to false. The Dedekind number \"M\"(\"n\") is the number of different monotonic Boolean functions on \"n\" variables.\n\nAn antichain of sets (also known as a Sperner family) is a family of sets, none of which is contained in any other set. If \"V\" is a set of \"n\" Boolean variables, an antichain \"A\" of subsets of \"V\" defines a monotone Boolean function \"f\", where the value of \"f\" is true for a given set of inputs if some subset of the true inputs to \"f\" belongs to \"A\" and false otherwise. Conversely every monotone Boolean function defines in this way an antichain, of the minimal subsets of Boolean variables that can force the function value to be true. Therefore, the Dedekind number \"M\"(\"n\") equals the number of different antichains of subsets of an \"n\"-element set.\n\nA third, equivalent way of describing the same class of objects uses lattice theory. From any two monotone Boolean functions \"f\" and \"g\" we can find two other monotone Boolean functions \"f\" ∧ \"g\" and \"f\" ∨ \"g\", their logical conjunction and logical disjunction respectively. The family of all monotone Boolean functions on \"n\" inputs, together with these two operations, forms a distributive lattice, the lattice given by Birkhoff's representation theorem from the partially ordered set of subsets of the \"n\" variables with set inclusion as the partial order. This construction produces the free distributive lattice with \"n\" generators. Thus, the Dedekind numbers count the number of elements in free distributive lattices.\n\nThe Dedekind numbers also count (one more than) the number of abstract simplicial complexes on \"n\" elements, families of sets with the property that any subset of a set in the family also belongs to the family. Any antichain determines a simplicial complex, the family of subsets of antichain members, and conversely the maximal simplices in a complex form an antichain.\n\nFor \"n\" = 2, there are six monotonic Boolean functions and six antichains of subsets of the two-element set {\"x\",\"y\"}:\n\nThe exact values of the Dedekind numbers are known for 0 ≤ \"n\" ≤ 8:\n\nThe first six of these numbers are given by . \"M\"(6) was calculated by , \"M\"(7) was calculated by and , and \"M\"(8) by .\n\nIf \"n\" is even, then \"M\"(\"n\") must also be even.\nThe calculation of the fifth Dedekind number \"M\"(5) = 7581 disproved a conjecture by Garrett Birkhoff that \"M\"(\"n\") is always divisible by (2\"n\" − 1)(2\"n\" − 2).\n\n rewrote the logical definition of antichains into the following arithmetic formula for the Dedekind numbers:\nwhere formula_2 is the formula_3th bit of the number formula_4,\nwhich can be written using the floor function as\nHowever, this formula is not helpful for computing the values of \"M\"(\"n\") for large \"n\" due to the large number of terms in the summation.\n\nThe logarithm of the Dedekind numbers can be estimated accurately via the bounds\nHere the left inequality counts the number of antichains in which each set has exactly formula_7 elements, and the right inequality was proven by .\n\nfor even \"n\", and\nfor odd \"n\", where\nand\nThe main idea behind these estimates is that, in most antichains, all the sets have sizes that are very close to \"n\"/2. For \"n\" = 2, 4, 6, 8 Korshunov's formula provides an estimate that is inaccurate by a factor of 9.8%, 10.2%, 4.1%, and -3.3%, respectively.\n\n"}
{"id": "736713", "url": "https://en.wikipedia.org/wiki?curid=736713", "title": "Disquisitiones Arithmeticae", "text": "Disquisitiones Arithmeticae\n\nThe (Latin for \"Arithmetical Investigations\") is a textbook of number theory written in Latin by Carl Friedrich Gauss in 1798 when Gauss was 21 and first published in 1801 when he was 24. It is notable for having a revolutionary impact on the field of number theory as it not only turned the field truly rigorous and systematic but also paved the path for modern number theory. In this book Gauss brought together and reconciled results in number theory obtained by mathematicians such as Fermat, Euler, Lagrange, and Legendre and added many profound and original results of his own.\n\nThe \"Disquisitiones\" covers both elementary number theory and parts of the area of mathematics now called algebraic number theory. However, Gauss did not explicitly recognize the concept of a group, which is central to modern algebra, so he did not use this term. His own title for his subject was Higher Arithmetic. In his Preface to the \"Disquisitiones\", Gauss describes the scope of the book as follows:\n\nGauss also states, \"When confronting many difficult problems, derivations have been suppressed for the sake of brevity when readers refer to this work.\" (\"Quod, in pluribus quaestionibus difficilibus, demonstrationibus syntheticis usus sum, analysinque per quam erutae sunt suppressi, imprimis brevitatis studio tribuendum est, cui quantum fieri poterat consulere oportebat\")\n\nThe book is divided into seven sections, which are:\n\nThese sections are subdivided into 366 numbered items, which sometimes state a theorem with proof, or otherwise develop a remark or thought.\n\nSections I to III are essentially a review of previous results, including Fermat's little theorem, Wilson's theorem and the existence of primitive roots. Although few of the results in these first sections are original, Gauss was the first mathematician to bring this material together and treat it in a systematic way. He also realized the importance of the property of unique factorization (assured by the fundamental theorem of arithmetic, first studied by Euclid), which he restates and proves using modern tools.\n\nFrom Section IV onwards, much of the work is original. Section IV itself develops a proof of quadratic reciprocity; Section V, which takes up over half of the book, is a comprehensive analysis of binary and ternary quadratic forms. Section VI includes two different primality tests. Finally, Section VII is an analysis of cyclotomic polynomials, which concludes by giving the criteria that determine which regular polygons are constructible i.e. can be constructed with a compass and unmarked straight edge alone.\n\nGauss started to write an eighth section on higher order congruences, but he did not complete this, and it was published separately after his death. The eighth section was finally published as a treatise entitled \"general investigations on congruences\", and in it Gauss discussed congruences of arbitrary degree. It's worth notice since Gauss attacked the problem of general congruences from a standpoint closely related to that taken later by Dedekind, Galois, and Emil Artin. The treatise paved the way for the theory of function fields over a finite field of constants. Ideas unique to that treatise are clear recognition of the importance of the\nFrobenius morphism, and a version of Hensel's lemma.\n\nThe \"Disquisitiones\" was one of the last mathematical works to be written in scholarly Latin (an English translation was not published until 1965).\n\nBefore the \"Disquisitiones\" was published, number theory consisted of a collection of isolated theorems and conjectures. Gauss brought the work of his predecessors together with his own original work into a systematic framework, filled in gaps, corrected unsound proofs, and extended the subject in numerous ways.\n\nThe logical structure of the \"Disquisitiones\" (theorem statement followed by proof, followed by corollaries) set a standard for later texts. While recognising the primary importance of logical proof, Gauss also illustrates many theorems with numerical examples.\n\nThe \"Disquisitiones\" was the starting point for the work of other nineteenth century European mathematicians including Ernst Kummer, Peter Gustav Lejeune Dirichlet and Richard Dedekind. Many of the annotations given by Gauss are in effect announcements of further research of his own, some of which remained unpublished. They must have appeared particularly cryptic to his contemporaries; they can now be read as containing the germs of the theories of L-functions and complex multiplication, in particular.\n\nGauss' \"Disquisitiones\" continued to exert influence in the 20th century. For example, in section V, article 303, Gauss summarized his calculations of class numbers of proper primitive binary quadratic forms, and conjectured that he had found all of them with class numbers 1, 2, and 3. This was later interpreted as the determination of imaginary quadratic number fields with even discriminant and class number 1,2 and 3, and extended to the case of odd discriminant. Sometimes referred to as the class number problem, this more general question was eventually confirmed in 1986, (the specific question Gauss asked was confirmed by Landau in 1902\n\n"}
{"id": "2669524", "url": "https://en.wikipedia.org/wiki?curid=2669524", "title": "Dulmage–Mendelsohn decomposition", "text": "Dulmage–Mendelsohn decomposition\n\nIn graph theory, the Dulmage–Mendelsohn decomposition is a partition of the vertices of a bipartite graph into subsets, with the property that two adjacent vertices belong to the same subset if and only if they are paired with each other in a perfect matching of the graph. It is named after A. L. Dulmage and Nathan Mendelsohn, who published it in 1958. A generalization to any graph is the Edmonds-Gallai decomposition, using the Blossom algorithm.\n\nLet \"G\" = (\"U\",\"V\",\"E\") be a bipartite graph, and let \"D\" be the set of vertices in \"G\" that are not matched in at least one maximum matching of \"G\".\nThen \"D\" is necessarily an independent set, so \"G\" can be partitioned into three parts:\nEvery maximum matching in \"G\" consists of matchings in the first and second part that match all neighbors of \"D\", together with a perfect matching of the remaining vertices.\n\nThe third set of vertices in the coarse decomposition (or all vertices in a graph with a perfect matching) may additionally be partitioned into subsets by the following steps:\n\nTo see that this subdivision into subsets characterizes the edges that belong to perfect matchings, suppose that two vertices \"u\" and \"v\" in \"G\" belong to the same subset of the decomposition, but are not already matched by the initial perfect matching. Then there exists a strongly connected component in \"H\" containing edge \"uv\". This edge must belong to a simple cycle in \"H\" (by the definition of strong connectivity) which necessarily corresponds to an alternating cycle in \"G\" (a cycle whose edges alternate between matched and unmatched edges). This alternating cycle may be used to modify the initial perfect matching to produce a new matching containing edge \"uv\".\n\nAn edge \"uv\" of the graph \"G\" belongs to all perfect matchings of \"G\", if and only if \"u\" and \"v\" are the only members of their set in the decomposition. Such an edge exists if and only if the matching preclusion number of the graph is one.\n\nAs another component of the Dulmage–Mendelsohn decomposition, Dulmage and Mendelsohn defined the \"core\" of a graph to be the union of its maximum matchings. However, this concept should be distinguished from the core in the sense of graph homomorphisms, and from the \"k\"-core formed by the removal of low-degree vertices.\n\nThis decomposition has been used to partition meshes in finite element analysis, and to determine specified, underspecified and overspecified equations in systems of nonlinear equations.\n\n\n"}
{"id": "20797876", "url": "https://en.wikipedia.org/wiki?curid=20797876", "title": "Edge cycle cover", "text": "Edge cycle cover\n\nIn mathematics, an edge cycle cover (sometimes called simply cycle cover) of a graph is a set of cycles which are subgraphs of \"G\" and contain all edges of \"G\". \n\nIf the cycles of the cover have no vertices in common, the cover is called vertex-disjoint or sometimes simply disjoint cycle cover. In this case the set of the cycles constitutes a spanning subgraph of \"G\".\n\nIf the cycles of the cover have no edges in common, the cover is called edge-disjoint or simply disjoint cycle cover.\n\nFor a weighted graph, the Minimum-Weight Cycle Cover Problem (MWCCP) is the problem to find a cycle cover with minimal sum of weights of edges in all cycles of the cover.\n\nFor bridgeless planar graphs the MWCCP can be solved in polynomial time. \n\nThe cycle double cover conjecture is an open problem in graph theory stating that in every bridgeless graph there exists a set of cycles that together cover every edge of the graph twice.\n\n"}
{"id": "58759432", "url": "https://en.wikipedia.org/wiki?curid=58759432", "title": "Edray Herber Goins", "text": "Edray Herber Goins\n\nEdray Herber Goins (born June 29, 1972, Los Angeles) is an African-American mathematician working in number theory and algebraic geometry. His interests include Selmer groups for elliptic curves using class groups of number fields, Belyi maps and Dessin d'enfants.\n\nGoins was born in Los Angeles, his teacher mother Eddi Beatrice Brown being a daughter of Marjorie Lee Browne, the third African-American woman to get a PhD in mathematics. He attended public schools in South Los Angeles and got his BSc in mathematics and physics in 1994 from Caltech, and his PhD in 1999 on “Elliptic Curves and Icosahedral Galois Representations” from Stanford University, under Daniel Bump and Karl Rubin.\n\nAfter many years on the faculty of Purdue University, in 2018 Goins took up a position at Pomona College.. At Pomona, he currently teaches Linear Algebra, aided by his trusty mentor, and future high-school math teacher, Eric Gofen. Goins is highly regarded among students in the class for his precise chalk movements, frequent usage of the word \"tilde\", and practice of double-underling words, where he first draws a horizontal line from right to left, moves his chalk down, and draws a slightly longer horizontal line from left to right.\n\nHis summers have long been focused on engaging underrepresented students in research in the mathematical sciences. He currently runs the NSF-funded Research Experience for Undergraduates (REU) \"Pomona Research in Mathematics Experience (PRiME).”. In addition, he is noted for his 2018 essay, \"Three Questions: The Journey of One Black Mathematician.\"\n\nGoins is President of the National Association of Mathematicians (NAM).. \n\n\n"}
{"id": "32651098", "url": "https://en.wikipedia.org/wiki?curid=32651098", "title": "Elements of Dynamic", "text": "Elements of Dynamic\n\nElements of Dynamic is a book published by William Kingdon Clifford in 1878. In 1887 it was supplemented by a fourth part and an appendix. The subtitle is \"An introduction to motion and rest in solid and fluid bodies\". It was reviewed positively, has remained a standard reference since its appearance, and is now available online as a \"Historical Math Monograph\" from Cornell University.\n\nOn page 95 Clifford deconstructed the quaternion product of William Rowan Hamilton into two separate \"products\" of two vectors: vector product and scalar product, anticipating the complete severance seen in \"Vector Analysis\" (1901). \"Elements of Dynamic\" was the debut of the term cross-ratio for a four-argument function frequently used in geometry.\n\nClifford uses the term \"twist\" to discuss (pages 126 to 131) the screw theory that had recently been introduced by Robert Stawell Ball.\n\nA review in the \"Philosophical Magazine\" explained for prospective readers that kinematics is the \"study of the theory of pure motion\". Noting the nature of \"progressive training\" required for mathematics, the reviewer wondered \"For what class of readers is the book designed?\"\nRichard A. Proctor noted in \"The Contemporary Review\" (33:65) that there are \"few errors in the work, and even misprints are few and far between for a treatise of this kind.\" He did not approve of Clifford’s coining of \"odd new words as squirts, sinks, twists, and whirls.\" Proctor quoted the last sentence of the book: \"Every continuous motion of an infinite body may be built up of squirts and vortices.\"\n\nIn a \"Sketch of Professor Clifford\" in June 1879 the journal \"Popular Science\" said \"It will probably not take high rank as a university text-book, for which it was intended, but is much admired by mathematicians for the elegance, freshness, and originality displayed in the treatment of mathematical problems.\"\n\nAfter Clifford had died, and \"Book IV and Appendix\" were published in 1887, the literary magazine \n\"Athenaeum\" said \"we have here Clifford pure and simple.\" It explained that he \"had entirely shaken off the concept of force as an explanatory cause.\" It also expressed \"the oft-told regret that Clifford did not live to reshape the teaching of elementary dynamics in this country, and we wait somewhat impatiently for his successor in this labour, who seems long in appearing.\"\n\nIn 1901 Alexander Macfarlane spoke at Lehigh University on Clifford. Reviewing \"Elements of Dynamic\" he said\n\nIn 2004 Gowan Dawson reviewed the situation of the book's publication. On the basis of a letter from Lucy Clifford to Alexander MacMillan, the publisher, Dawson wrote\n\n"}
{"id": "4081509", "url": "https://en.wikipedia.org/wiki?curid=4081509", "title": "Exponential type", "text": "Exponential type\n\nIn complex analysis, a branch of mathematics, a holomorphic function is said to be of exponential type C if its growth is bounded by the exponential function \"e\" for some real-valued constant \"C\" as |\"z\"| → ∞. When a function is bounded in this way, it is then possible to express it as certain kinds of convergent summations over a series of other complex functions, as well as understanding when it is possible to apply techniques such as Borel summation, or, for example, to apply the Mellin transform, or to perform approximations using the Euler–Maclaurin formula. The general case is handled by Nachbin's theorem, which defines the analogous notion of Ψ-type for a general function Ψ(\"z\") as opposed to \"e\".\n\nA function \"f\"(\"z\") defined on the complex plane is said to be of exponential type if there exist real-valued constants \"M\" and \"τ\" such that\n\nin the limit of formula_2. Here, the complex variable \"z\" was written as formula_3 to emphasize that the limit must hold in all directions θ. Letting τ stand for the infimum of all such τ, one then says that the function \"f\" is of \"exponential type τ\".\n\nFor example, let formula_4. Then one says that formula_5 is of exponential type π, since π is the smallest number that bounds the growth of formula_5 along the imaginary axis. So, for this example, Carlson's theorem cannot apply, as it requires functions of exponential type less than π. Similarly, the Euler–Maclaurin formula cannot be applied either, as it, too, expresses a theorem ultimately anchored in the theory of finite differences.\n\nA holomorphic function formula_7 is said to be of exponential type formula_8 if for every formula_9 there exists a real-valued constant formula_10 such that\n\nfor formula_12 where formula_13.\nWe say formula_7 is of exponential type if formula_7 is of exponential type formula_16 for some formula_8. The number\n\nis the exponential type of formula_7. The limit superior here means the limit of the supremum of the ratio outside a given radius as the radius goes to infinity. This is also the limit superior of the maximum of the ratio at a given radius as the radius goes to infinity. The limit superior may exist even if the maximum at radius \"r\" does not have a limit as \"r\" goes to infinity. For example, for the function\n\nthe value of\n\nat formula_22 is asymptotic to formula_23 and thus goes to zero as \"n\" goes to infinity, but \"F\"(\"z\") is nevertheless of exponential type 1, as seen by looking at the points formula_24.\n\n has given a generalization of exponential type for entire functions of several complex variables. \nSuppose formula_25 is a convex, compact, and symmetric subset of formula_26. It is known that for every such formula_25 there is an associated norm formula_28 with the property that\n\nIn other words, formula_25 is the unit ball in formula_31 with respect to formula_28. The set\n\nis called the polar set and is also a convex, compact, and symmetric subset of formula_26. Furthermore, we can write\n\nWe extend formula_28 from formula_26 to formula_38 by\n\nAn entire function formula_7 of formula_41-complex variables is said to be of exponential type with respect to formula_25 if for every formula_9 there exists a real-valued constant formula_44 such that\n\nfor all formula_46.\n\nCollections of functions of exponential type formula_47 can form a complete uniform space, namely a Fréchet space, by the topology induced by the countable family of norms\n\n"}
{"id": "1586291", "url": "https://en.wikipedia.org/wiki?curid=1586291", "title": "Fréchet filter", "text": "Fréchet filter\n\nIn mathematics, the Fréchet filter, also called the cofinite filter, on a set is a special subset of the set's power set. A member of this power set is in the Fréchet filter if and only if its complement in the power set is finite. This is of interest in topology, where filters originated, and relates to order and lattice theory because a set's power set is a partially ordered set (and more specifically, a lattice) under set inclusion.\n\nThe Fréchet filter is named after the French mathematician Maurice Fréchet (1878-1973), who worked in topology. It is alternatively called a \"cofinite filter\" because its members are exactly the cofinite sets in a power set.\n\nThe Fréchet filter \"F\" on \"X\" is the set of all subsets \"A\" of \"X\" such that the complement of \"A\" in \"X\" is finite. That is, \n\nThis makes \"F\" a filter on the lattice (P(\"X\"), ⊆), the power set of \"X\" with set inclusion, since\n\nIf the base set \"X\" is finite, then \"F\" = P(\"X\") since every subset of \"X\", and in particular every complement, is then finite. This case is sometimes excluded by definition or else called the improper filter on \"X\". Allowing \"X\" to be finite creates a single exception to the Fréchet filter's being free and non-principal since a filter on a finite set cannot be free and a non-principal filter cannot contain any singletons as members.\n\nIf \"X\" is infinite, then every member of \"F\" is infinite since it is simply \"X\" minus finitely many of its members. Additionally, \"F\" is infinite since one of its subsets is the set of all {\"x\"}, where \"x\" ∈ \"X\".\n\nThe Fréchet filter is both free and non-principal, excepting the finite case mentioned above, and is included in every free filter. It is also the dual filter of the ideal of all finite subsets of (infinite) \"X\".\n\nThe Fréchet filter is \"not\" necessarily an ultrafilter (or maximal proper filter). Consider P(N). The set of even numbers is the complement of the set of odd numbers. Since neither of these sets is finite, neither set is in the Fréchet filter on N. However, an ultrafilter is free if and only if it includes the Fréchet filter. The existence of free ultrafilters was established by Tarski in 1930, relying on a theorem equivalent to the axiom of choice and is used in the construction of the hyperreals in nonstandard analysis.\n\nOn the set N of natural numbers, the set \"B\" = { (\"n\",∞) : \"n\" ∈ N} is a Fréchet filter base, i.e., the Fréchet filter on N consists of all supersets of elements of \"B\".\n\n\n"}
{"id": "17211428", "url": "https://en.wikipedia.org/wiki?curid=17211428", "title": "Global analytic function", "text": "Global analytic function\n\nIn the mathematical field of complex analysis, a global analytic function is a generalization of the notion of an analytic function which allows for functions to have multiple branches. Global analytic functions arise naturally in considering the possible analytic continuations of an analytic function, since analytic continuations may have a non-trivial monodromy. They are one foundation for the theory of Riemann surfaces.\n\nThe following definition is in , but also found in Weyl or perhaps Weierstrass. An analytic function in an open set \"U\" is called a function element. Two function elements (\"f\", \"U\") and (\"f\", \"U\") are said to be analytic continuations of one another if \"U\" ∩ \"U\" ≠ ∅ and \"f\" = \"f\" on this intersection. A chain of analytic continuations is a finite sequence of function elements (\"f\", \"U\"), …, (\"f\",\"U\") such that each consecutive pair are analytic continuations of one another; i.e., (\"f\", \"U\") is an analytic continuation of (\"f\", \"U\") for \"i\" = 1, 2, …, \"n\" − 1.\n\nA global analytic function is a family f of function elements such that, for any (\"f\",\"U\") and (\"g\",\"V\") belonging to f, there is a chain of analytic continuations in f beginning at (\"f\",\"U\") and finishing at (\"g\",\"V\").\n\nA complete global analytic function is a global analytic function f which contains every analytic continuation of each of its elements.\n\nUsing ideas from sheaf theory, the definition can be streamlined. In these terms, a complete global analytic function is a path connected sheaf of germs of analytic functions which is \"maximal\" in the sense that it is not contained (as an etale space) within any other path connected sheaf of germs of analytic functions.\n"}
{"id": "35457783", "url": "https://en.wikipedia.org/wiki?curid=35457783", "title": "Glossary of algebraic geometry", "text": "Glossary of algebraic geometry\n\nThis is a glossary of algebraic geometry.\n\nSee also glossary of commutative algebra, glossary of classical algebraic geometry, and glossary of ring theory. For the number-theoretic applications, see glossary of arithmetic and Diophantine geometry.\n\nFor simplicity, a reference to the base scheme is often omitted; i.e., a scheme will be a scheme over some fixed base scheme \"S\" and a morphism an \"S\"-morphism.\n\n \n\n\n"}
{"id": "8549940", "url": "https://en.wikipedia.org/wiki?curid=8549940", "title": "Golomb sequence", "text": "Golomb sequence\n\nIn mathematics, the Golomb sequence, named after Solomon W. Golomb (but also called Silverman's sequence), is a non-decreasing integer sequence where \"a\" is the number of times that \"n\" occurs in the sequence, starting with \"a\" = 1, and with the property that for \"n\" > 1 each \"a\" is the unique integer which makes it possible to satisfy the condition. For example, \"a\" = 1 says that 1 only occurs once in the sequence, so \"a\" cannot be 1 too, but it can be, and therefore must be, 2. The first few values are\n\n\"a\" = 1 <br>\nTherefore, 1 occurs exactly one time in this sequence.\n\n\"a\" > 1 <br>\n\"a\" = 2\n\n2 occurs exactly 2 times in this sequence. <br>\n\"a\" = 2\n\n3 occurs exactly 2 times in this sequence.\n\n\"a\" = \"a\" = 3\n\n4 occurs exactly 3 times in this sequence. <br>\n5 occurs exactly 3 times in this sequence.\n\n\"a\" = \"a\" = \"a\" = 4 <br>\n\"a\" = \"a\" = \"a\" = 5\n\netc.\n\nColin Mallows has given an explicit recurrence relation formula_1. An asymptotic expression for \"a\" is\n\nwhere formula_3 is the golden ratio (approximately equal to 1.618034).\n\n\n"}
{"id": "13156564", "url": "https://en.wikipedia.org/wiki?curid=13156564", "title": "Graphic communication", "text": "Graphic communication\n\nGraphic communication as the name suggests is communication using graphic elements. These elements include symbols such as glyphs and icons, images such as drawings and photographs, and can include the passive contributions of substrate, color and surroundings. It is the process of creating, producing, and distributing material incorporating words and images to convey data, concepts, and emotions.\n\nThe field of graphic communications encompasses all phases of the graphic communications processes from origination of the idea (design, layout, and typography) through reproduction, finishing and distribution of two- or three-dimensional products or electronic transmission.\n\nGraphic communication involves the use of visual material to relate ideas such as drawings, photographs, slides, and sketches. The drawings of plans and refinements and a rough map sketched to show the way could be considered graphical communication.\n\nAny medium that uses a graphics to aid in conveying a message, instruction, or an idea is involved in graphical communication. One of the most widely used forms of graphical communication is the drawing.\n\nThe earliest graphics known to anthropologists studying prehistoric periods are cave paintings and markings on boulders, bone, ivory, and antlers, which were created during the Upper Paleolithic period from 40,000–10,000 B.C. or earlier. Many of these played a major role in geometry. They used graphics to represent their mathematical theories such as the Circle Theorem and the Pythagorean theorem.\n\nGraphics are visual presentations on some surface, such as a wall, canvas, computer screen, paper, or stone to brand, inform, illustrate, or entertain. Examples are photographs, drawings, line art, graphs, diagrams, typography, numbers, symbols, geometric designs, maps, engineering drawings, or other images. Graphics often combine text, illustration, and color. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style.\n\nGraphics can be functional or artistic. The latter can be a recorded version, such as a photograph, or an interpretation by a scientist to highlight essential features, or an artist, in which case the distinction with imaginary graphics may become blurred.\n\nCommunication is the process whereby information is imparted by a sender to a receiver via a medium. It requires that all parties have an area of communicative commonality. There are auditory means, such as speaking, singing and sometimes tone of voice, and nonverbal, physical means, such as body language, sign language, paralanguage, touch, eye contact, by using writing. Communication is defined as a process by which we assign and convey meaning in an attempt to create shared understanding. This process requires a vast repertoire of skills in intrapersonal and interpersonal processing, listening, observing, speaking, questioning, analyzing, and evaluating. if you use these processes it is developmental and transfers to all areas of life: home, school, community, work, and beyond. It is through communication that collaboration and cooperation occur.\n\nVisual communication as the name suggests is communication through visual aid. It is the conveyance of ideas and information in forms that can be read or looked upon. Primarily associated with two dimensional images, it includes: signs, typography, drawing, graphic design, illustration, colour and electronic resources. It solely relies on vision. It is a form of communication with visual effect. It explores the idea that a visual message with text has a greater power to inform, educate or persuade a person. It is communication by presenting information through Visual form. The evaluation of a good visual design is based on measuring comprehension by the audience, not on aesthetic or artistic preference. There are no universally agreed-upon principles of beauty and ugliness. There exists a variety of ways to present information visually, like gestures, body languages, video and TV. Here, focus is on the presentation of text, pictures, diagrams, photos, et cetera, integrated on a computer display. The term visual presentation is used to refer to the actual presentation of information. Recent research in the field has focused on web design and graphically oriented usability. Graphic designers use methods of visual communication in their professional practice.\n\nCommunication design is a mixed discipline between design and information-development which is concerned with how intermission such as printed, crafted, electronic media or presentations communicate with people. A communication design approach is not only concerned with developing the message aside from the aesthetics in media, but also with creating new media channels to ensure the message reaches the target audience. Communication design seeks to attract, inspire, create desires and motivate the people to respond to messages, with a view to making a favorable impact to the bottom line of the commissioning body, which can be either to build a brand, move sales, or for humanitarian purposes. Its process involves strategic business thinking, utilizing market research, creativity, and problem-solving.\n\nThe term graphic design can refer to a number of artistic and professional disciplines which focus on visual communication and presentation. Various methods are used to create and combine symbols, images and/or words to create a visual representation of ideas and messages. A graphic designer may use typography, visual arts and page layout techniques to produce the final result. Graphic design often refers to both the process (designing) by which the communication is created and the products (designs) which are generated.\n\nCommon uses of graphic design include magazines, advertisements, product packaging and web design. For example, a product package might include a logo or other artwork, organized text and pure design elements such as shapes and color which unify the piece. Composition is one of the most important features of graphic design especially when using pre-existing materials or diverse elements.\n\nThe term representation, according to O'Shaughnessy and Stadler (2005), can carry a range of meanings and interpretations. In literary theory representation is commonly defined in three ways.\n\nRepresentation, according to Mitchell (1995), began with early literary theory in the ideas of Plato and Aristotle, and has evolved into a significant component of language, Saussurian and communication studies. Aristotle discusses representation in three ways:\nThe \"means\" of literary representation is language. The means of graphical representation are graphics. Graphical representation of data is one of the most commonly used modes of presentation.\nThe purpose of graphical communication is transfer message or information to the receiver in effective way.when professional organizations prepare reports then usually use the mode of graphical presentations.\n\n\n\n"}
{"id": "24506903", "url": "https://en.wikipedia.org/wiki?curid=24506903", "title": "Haran's diamond theorem", "text": "Haran's diamond theorem\n\nIn mathematics, the Haran diamond theorem gives a general sufficient condition for a separable extension of a Hilbertian field to be Hilbertian. \n\nLet \"K\" be a Hilbertian field and \"L\" a separable extension of \"K\". Assume there exist two Galois extensions \n\"N\" and \"M\" of \"K\" such that \"L\" is contained in the compositum \"NM\", but is contained in neither \"N\" nor \"M\". Then \"L\" is Hilbertian.\n\nThe name of the theorem comes from the pictured diagram of fields, and was coined by Jarden.\n\nThis theorem was firstly proved using non-standard methods by Weissauer. It was reproved by Fried using standard methods. The latter proof led Haran to his diamond theorem. \n\nLet \"K\" be a Hilbertian field, \"N\" a Galois extension of \"K\", and \"L\" a finite proper extension of \"N\". Then \"L\" is Hilbertian. \n\n\nIf \"L\" is finite over \"K\", it is Hilbertian; hence we assume that \"L/K\" is infinite. Let \"x\" be a primitive element for \"L/N\", i.e., \"L\" = \"N\"(\"x\"). \n\nLet \"M\" be the Galois closure of \"K\"(\"x\"). Then all the assumptions of the diamond theorem are satisfied, hence \"L\" is Hilbertian.\n\nAnother, preceding to the diamond theorem, sufficient permanence condition was given by Haran–Jarden:\nTheorem. \nLet \"K\" be a Hilbertian field and \"N\", \"M\" two Galois extensions of \"K\". Assume that neither contains the other. Then their compositum \"NM\" is Hilbertian.\n\nThis theorem has a very nice consequence: Since the field of rational numbers, \"Q\" is Hilbertian (Hilbert's irreducibility theorem), we get that the algebraic closure of \"Q\" is not the compositum of two proper Galois extensions.\n\n"}
{"id": "634759", "url": "https://en.wikipedia.org/wiki?curid=634759", "title": "Hausdorff paradox", "text": "Hausdorff paradox\n\nThe Hausdorff paradox is a paradox in mathematics named after Felix Hausdorff. It involves the sphere \"S\" (a 2-dimensional sphere in \"R\"). It states that if a certain countable subset is removed from \"S\", then the remainder can be divided into three disjoint subsets \"A\", \"B\" and \"C\" such that \"A\", \"B\", \"C\" and \"B\" ∪ \"C\" are all congruent. In particular, it follows that on \"S\" there is no finitely additive measure defined on all subsets such that the measure of congruent sets is equal (because this would imply that the measure of \"B\" ∪ \"C\" is simultaneously 1/3 and 2/3 of the non-zero measure of the whole sphere).\n\nThe paradox was published in Mathematische Annalen in 1914 and also in Hausdorff's book, Grundzüge der Mengenlehre, the same year. The proof of the much more famous Banach–Tarski paradox uses Hausdorff's ideas. The proof of this paradox relies on the Axiom of Choice. \n\nThis paradox shows that there is no finitely additive measure on a sphere defined on \"all\" subsets which is equal on congruent pieces. (Hausdorff first showed in the same paper the easier result that there is no \"countably\" additive measure defined on all subsets.) The structure of the group of rotations on the sphere plays a crucial role here the statement is not true on the plane or the line. In fact, as was later shown by Banach, it is possible to define an \"area\" for \"all\" bounded subsets in the Euclidean plane (as well as \"length\" on the real line) in such a way that congruent sets will have equal \"area\". (This Banach measure, however, is only finitely additive, so it is not a measure in the full sense, but it equals the Lebesgue measure on sets for which the latter exists.) This implies that if two open subsets of the plane (or the real line) are equi-decomposable then they have equal area.\n\n\n\n"}
{"id": "7847320", "url": "https://en.wikipedia.org/wiki?curid=7847320", "title": "Hurst exponent", "text": "Hurst exponent\n\nThe Hurst exponent is used as a measure of long-term memory of time series. It relates to the autocorrelations of the time series, and the rate at which these decrease as the lag between pairs of values increases.\nStudies involving the Hurst exponent were originally developed in hydrology for the practical matter of determining optimum dam sizing for the Nile river's volatile rain and drought conditions that had been observed over a long period of time. The name \"Hurst exponent\", or \"Hurst coefficient\", derives from Harold Edwin Hurst (1880–1978), who was the lead researcher in these studies; the use of the standard notation \"H\" for the coefficient relates to his name also.\n\nIn fractal geometry, the generalized Hurst exponent has been denoted by \"H\" or \"H\" in honor of both Harold Edwin Hurst and Ludwig Otto Hölder (1859–1937) by Benoît Mandelbrot (1924–2010). \"H\" is directly related to fractal dimension, \"D\", and is a measure of a data series' \"mild\" or \"wild\" randomness.\n\nThe Hurst exponent is referred to as the \"index of dependence\" or \"index of long-range dependence\". It quantifies the relative tendency of a time series either to regress strongly to the mean or to cluster in a direction. A value \"H\" in the range 0.5–1 indicates a time series with long-term positive autocorrelation, meaning both that a high value in the series will probably be followed by another high value and that the values a long time into the future will also tend to be high. A value in the range 0 – 0.5 indicates a time series with long-term switching between high and low values in adjacent pairs, meaning that a single high value will probably be followed by a low value and that the value after that will tend to be high, with this tendency to switch between high and low values lasting a long time into the future. A value of \"H\"=0.5 can indicate a completely uncorrelated series, but in fact it is the value applicable to series for which the autocorrelations at small time lags can be positive or negative but where the absolute values of the autocorrelations decay exponentially quickly to zero. This in contrast to the typically power law decay for the 0.5 < \"H\" < 1 and 0 < \"H\" < 0.5 cases.\n\nThe Hurst exponent, \"H\", is defined in terms of the asymptotic behaviour of the rescaled range as a function of the time span of a time series as follows;\n\nwhere;\n\n\nFor self-similar time series,\n\"H\" is directly related to fractal dimension, \"D\", where 1 < \"D\" < 2, such that \"D\" = 2 - \"H\". The values of the Hurst exponent vary between 0 and 1, with higher values indicating a smoother trend, less volatility, and less roughness.\n\nFor more general time series or multi-dimensional process, the Hurst exponent and fractal dimension\ncan be chosen independently, as the Hurst exponent represents structure over asymptotically longer\nperiods, while fractal dimension represents structure over asymptotically shorter periods.\n\nA number of estimators of long-range dependence have been proposed in the literature. The oldest and best-known is the so-called rescaled range (R/S) analysis popularized by Mandelbrot and Wallis and based on previous hydrological findings of Hurst. Alternatives include DFA, Periodogram regression, aggregated variances, local Whittle's estimator, wavelet analysis, both in the time domain and frequency domain.\n\nTo estimate the Hurst exponent, one must first estimate the dependence of the rescaled range on the time span \"n\" of observation. A time series of full length \"N\" is divided into a number of shorter time series of length \"n\" = \"N\", \"N\"/2, \"N\"/4, ... The average rescaled range is then calculated for each value of \"n\".\n\nFor a (partial) time series of length formula_3, formula_9, the rescaled range is calculated as follows:\n\n1. Calculate the mean;\n\n2. Create a mean-adjusted series;\n\n3. Calculate the cumulative deviate series formula_12;\n\n4. Compute the range formula_14;\n\n5. Compute the standard deviation formula_16;\n\n6. Calculate the rescaled range formula_18 and average over all the partial time series of length formula_19\n\nThe Hurst exponent is estimated by fitting the power law formula_20 to the data. This can be done by plotting formula_21 as a function of formula_22, and fitting a straight line; the slope of the line gives formula_23 (a more principled approach fits the power law in a maximum-likelihood fashion). Such a graph is called a box plot. However, this approach is known to produce biased estimates of the power-law exponent. For small formula_3 there is a significant deviation from the 0.5 slope. Anis and Lloyd estimated the theoretical (i.e., for white noise) values of the R/S statistic to be:\n\nformula_25\n\nwhere formula_26 is the Euler gamma function. The Anis-Lloyd corrected R/S Hurst exponent is calculated as 0.5 plus the slope of formula_27.\n\nNo asymptotic distribution theory has been derived for most of the Hurst exponent estimators so far. However, Weron used bootstrapping to obtain approximate functional forms for confidence intervals of the two most popular methods, i.e., for the Anis-Lloyd corrected R/S analysis:\n\nand for DFA:\n\nHere formula_28 and formula_29 is the series length. In both cases only subseries of length formula_30 were considered for estimating the Hurst exponent; subseries of smaller length lead to a high variance of the R/S estimates.\n\nThe basic Hurst exponent can be related to the expected size of changes, as a function of the lag between observations, as measured by E(|\"X-X\"|). For the generalized form of the coefficient, the exponent here is replaced by a more general term, denoted by \"q\".\n\nThere are a variety of techniques that exist for estimating \"H\", however assessing the accuracy of the estimation can be a complicated issue. Mathematically, in one technique, the Hurst exponent can be estimated such that:\n\nfor a time series\n\nmay be defined by the scaling properties of its structure functions \"S\"(formula_31):\n\nwhere \"q\" > 0, formula_31 is the time lag and averaging is over the time window\n\nusually the largest time scale of the system.\n\nPractically, in nature, there is no limit to time, and thus \"H\" is non-deterministic as it may only be estimated based on the observed data; e.g., the most dramatic daily move upwards ever seen in a stock market index can always be exceeded during some subsequent day.\n\nIn the above mathematical estimation technique, the function \"H\"(\"q\") contains information about averaged generalized volatilities at scale formula_31 (only \"q\" = 1, 2 are used to define the volatility). In particular, the \"H\" exponent indicates persistent (\"H\" > ½) or antipersistent (\"H\" < ½) behavior of the trend.\n\nFor the BRW (brown noise, 1/\"f\"²) one gets\n\nand for pink noise (1/\"f\")\n\nThe Hurst exponent for white noise is dimension dependent, and for 1D and 2D it is\n\nFor the popular Lévy stable processes and truncated Lévy processes with parameter α it has been found that\n\nA method to estimate formula_36 from non-stationary time series is called detrended fluctuation analysis.\nWhen formula_36 is a non-linear function of q the time series is a multifractal system.\n\nIn the above definition two separate requirements are mixed together as if they would be one. Here are the two independent,-` requirements: (i) stationarity of the increments, x(t+T)-x(t)=x(T)-x(0) in distribution. this is the condition that yields longtime autocorrelations. (ii) Self-similarity of the stochastic process then yields variance scaling, but is not needed for longtime memory. E.g., both Markov processes (i.e., memory-free processes) and fractional Brownian motion scale at the level of 1-point densities (simple averages), but neither scales at the level of pair correlations or, correspondingly, the 2-point probability density.\n\nAn efficient market requires a martingale condition, and unless the variance is linear in the time this produces nonstationary increments, x(t+T)-x(t)≠x(T)-x(0). Martingales are Markovian at the level of pair correlations, meaning that pair correlations cannot be used to beat a martingale market. Stationary increments with nonlinear variance, on the other hand, induce the longtime pair memory of fractional Brownian motion that would make the market beatable at the level of pair correlations. Such a market would necessarily be far from \"efficient\".\n\nAn analysis of economic time series by means of the Hurst exponent using rescaled range and Detrended fluctuation analysis is conducted by econophysicist A.F. Bariviera. This paper studies the time varying character of Long-range dependency and, thus of informational efficiency.\n\nHurst exponent has also been applied to the investigation of long-range dependency in DNA, and photonic band gap materials.\n\n\n"}
{"id": "1871327", "url": "https://en.wikipedia.org/wiki?curid=1871327", "title": "Information continuum", "text": "Information continuum\n\nThe term Information continuum is used to describe the whole set of all information, in connection with information management. The term may be used in reference to the information or the information infrastructure of a people, a species, a scientific subject or an institution.\n\nThe Internet is sometimes called an 'Information continuum'.\n\n\n"}
{"id": "55663660", "url": "https://en.wikipedia.org/wiki?curid=55663660", "title": "Irène Gijbels", "text": "Irène Gijbels\n\nIrène Gijbels is a mathematical statistician at KU Leuven in Belgium, and an expert on nonparametric statistics.\nShe has also collaborated with TopSportLab, a KU Leuven spin-off, on software for risk assessment of sports injuries.\n\nGijbels earned her Ph.D. in 1990 from Limburgs Universitair Centrum. Her dissertation, supervised by Noël Veraverbeke, was \"Asymptotic Representations under Random Censoring\".\nShe joined KU Leuven after postdoctoral research as a Fulbright scholar at the University of North Carolina at Chapel Hill and the Mathematical Sciences Research Institute.\nWith Jianqing Fan, she is the author of \"Local Polynomial Modelling and Its Applications\" (CRC Press, 1996).\n\nGijbels is an elected member of the International Statistical Institute and the Royal Flemish Academy of Belgium for Science and the Arts, and a fellow of the American Statistical Association and the Institute of Mathematical Statistics.\n"}
{"id": "41803032", "url": "https://en.wikipedia.org/wiki?curid=41803032", "title": "Isotropic solid", "text": "Isotropic solid\n\nAn isotropic solid is a solid material in which physical properties do not depend on its orientation. It is an example of isotropy which can also apply to fluids, or other non-material concepts. The properties could be felt or seen such as the index of refraction or mechanical. For example an isotropic solid will expand equally in all directions when heated. Heat will conduct equally well in any direction, and sound will pass at the same speed. The physics of these solids are much easier to describe. Some examples are glass with random arrangements that average out to uniform, or the cubic crystal system. The opposite is anisotropic solids. Most crystal structures are actually anisotropic.\n\nIsotropy should not be confused with homogeneity, which is a property of independence of position. Isotropy of materials also depends on scale. For example, at micro level involving a few crystals, steel is not isotropic. But at macro level, for example a steel rod using in the building construction, steel is isotropic.\n"}
{"id": "8665621", "url": "https://en.wikipedia.org/wiki?curid=8665621", "title": "Jack function", "text": "Jack function\n\nIn mathematics, the Jack function is a generalization of the Jack polynomial, introduced by Henry Jack. The Jack polynomial is a homogeneous, symmetric polynomial which generalizes the Schur and zonal polynomials, and is in turn generalized by the Heckman–Opdam polynomials and Macdonald polynomials.\n\nThe Jack function formula_1 \nof an integer partition formula_2, parameter formula_3, and\nindefinitely many arguments formula_4 can be recursively defined as \nfollows:\n\n\n\nwhere the summation is over all partitions formula_7 such that the skew partition formula_8 is a horizontal strip, namely\n\nwhere formula_13 equals formula_14 if formula_15 and formula_16 otherwise. The expressions formula_17 and formula_18 refer to the conjugate partitions of formula_2 and formula_7, respectively. The notation formula_21 means that the product is taken over all coordinates formula_22 of boxes in the Young diagram of the partition formula_2.\n\nIn 1997, F. Knop and S. Sahi gave a purely combinatorial formula for the Jack polynomials formula_24 in \"n\" variables:\n\nThe sum is taken over all \"admissible\" tableaux of shape formula_26 and \n\nwith \n\nAn \"admissible\" tableau of shape formula_29 is a filling of the Young diagram formula_29 with numbers 1,2,…,\"n\" such that for any box (\"i\",\"j\") in the tableau,\n\nThis result can be seen as a special case of the more general combinatorial formula for Macdonald polynomials.\n\nThe Jack functions form an orthogonal basis in a space of symmetric polynomials, with inner product:\n\nThis orthogonality property is unaffected by normalization. The normalization defined above is typically referred to as the J normalization. The C normalization is defined as\n\nwhere\n\nFor formula_41 is often denoted by formula_42 and called the Zonal polynomial.\n\nThe \"P\" normalization is given by the identity formula_43, where \n\nand formula_45 and formula_46 denotes the arm and leg length respectively. Therefore, for formula_47 is the usual Schur function.\n\nSimilar to Schur polynomials, formula_48 can be expressed as a sum over Young tableaux. However, one need to add an extra weight to each tableau that depends on the parameter formula_3.\n\nThus, a formula for the Jack function formula_50 is given by\n\nwhere the sum is taken over all tableaux of shape formula_29, and formula_53 denotes the entry in box \"s\" of \"T\".\n\nThe weight formula_54 can be defined in the following fashion: Each tableau \"T\" of shape formula_29 can be interpreted as a sequence of partitions \n\nwhere formula_57 defines the skew shape with content \"i\" in \"T\". Then \n\nwhere \n\nand the product is taken only over all boxes \"s\" in formula_29 such that \"s\" has a box from formula_61 in the same row, but \"not\" in the same column.\n\nWhen formula_62 the Jack function is a scalar multiple of the Schur polynomial\n\nwhere\nis the product of all hook lengths of formula_2.\n\nIf the partition has more parts than the number of variables, then the Jack function is 0:\n\nIn some texts, especially in random matrix theory, authors have found it more convenient to use a matrix argument in the Jack function. The connection is simple. If formula_67 is a matrix with eigenvalues\nformula_4, then \n\n\n"}
{"id": "38288859", "url": "https://en.wikipedia.org/wiki?curid=38288859", "title": "John William Theodore Youngs", "text": "John William Theodore Youngs\n\nJohn William Theodore Youngs (usually cited as J. W. T. Youngs, known as Ted Youngs; 21 August 1910 Bilaspur, Chhattisgarh, India – 20 July 1970 Santa Cruz, California) was an American mathematician.\n\nYoungs was the son of a missionary. He completed his undergraduate study at Wheaton College and received his PhD from Ohio State University in 1934 under Tibor Radó. He then taught for 18 years at Indiana University, where for eight years he was chair of the mathematics department. From 1964 he was a professor at the University of California, Santa Cruz, where he developed the mathematics faculty and was chair of the academic senate of the university.\nYoungs worked in geometric topology, for example, questions on the Frechét-equivalence of topological maps. He is famous for the Ringel–Youngs theorem (\"i.e.\" Ringel and Youngs's 1968 proof of the Heawood conjecture), which is closely related to the analogue of the four-color theorem for surfaces of higher genus.\n\nJohn Youngs was a consultant for Sandia National Laboratories, the Rand Corporation and the Institute for Defense Analyses as well as a trustee for \"Carver Research Foundation Institute\" in Tuskegee. In 1946–1947 he was a Guggenheim Fellow. At the University of Santa Cruz a mathematics prize for undergraduates in named after him.\n\n\n"}
{"id": "2397362", "url": "https://en.wikipedia.org/wiki?curid=2397362", "title": "Karush–Kuhn–Tucker conditions", "text": "Karush–Kuhn–Tucker conditions\n\nIn mathematical optimization, the Karush–Kuhn–Tucker (KKT) conditions, also known as the Kuhn–Tucker conditions, are first derivative tests (sometimes called first-order) necessary conditions for a solution in nonlinear programming to be optimal, provided that some regularity conditions are satisfied. Allowing inequality constraints, the KKT approach to nonlinear programming generalizes the method of Lagrange multipliers, which allows only equality constraints. The system of equations and inequalities corresponding to the KKT conditions is usually not solved directly, except in the few special cases where a closed-form solution can be derived analytically. In general, many optimization algorithms can be interpreted as methods for numerically solving the KKT system of equations and inequalities.\n\nThe KKT conditions were originally named after Harold W. Kuhn and Albert W. Tucker, who first published the conditions in 1951. Later scholars discovered that the necessary conditions for this problem had been stated by William Karush in his master's thesis in 1939.\n\nConsider the following nonlinear minimization or maximization problem:\n\nwhere formula_4 is the optimization variable, formula_5 is the objective or utility function, formula_6 are the inequality constraint functions, and formula_7 are the equality constraint functions. The numbers of inequality and equality constraints are denoted formula_8 and formula_9, respectively.\n\nSuppose that the objective function formula_10 and the constraint functions formula_11 and formula_12 are continuously differentiable at a point formula_13. If formula_13 is a local optimum and the optimization problem satisfies some regularity conditions (see below), then there exist constants formula_15 and formula_16, called KKT multipliers, such that\n\n\n\n\n\nIn the particular case formula_25, i.e., when there are no inequality constraints, the KKT conditions turn into the Lagrange conditions, and the KKT multipliers are called Lagrange multipliers.\n\nIf some of the functions are non-differentiable, subdifferential versions of Karush–Kuhn–Tucker (KKT) conditions are available.\n\nIn order for a minimum point formula_13 to satisfy the above KKT conditions, the problem should satisfy some regularity conditions; some common examples are tabulated here:\n\nIt can be shown that\n\nand\n\n(and the converses are not true), although MFCQ is not equivalent to CRCQ.\nIn practice weaker constraint qualifications are preferred since they provide stronger optimality conditions.\n\nIn some cases, the necessary conditions are also sufficient for optimality. In general, the necessary conditions are not sufficient for optimality and additional information is necessary, such as the Second Order Sufficient Conditions (SOSC). For smooth functions, SOSC involve the second derivatives, which explains its name.\n\nThe necessary conditions are sufficient for optimality if the objective function formula_5 of a maximization problem is a concave function, the inequality constraints formula_28 are continuously differentiable convex functions and the equality constraints formula_29 are affine functions.\n\nIt was shown by Martin in 1985 that the broader class of functions in which KKT conditions guarantees global optimality are the so-called Type 1 invex functions.\n\nFor smooth, non-linear optimization problems, a second order sufficient condition is given as follows.\n\nThe solution formula_30 found in the above section is a constrained local minimum if for the Lagrangian,\n\nthen,\n\nwhere formula_33 is a vector satisfying the following,\n\nwhere only those active inequality constraints formula_35 corresponding to strict complementarity (i.e. where formula_36) are applied. The solution is a strict constrained local minimum in the case the inequality is also strict.\n\nOften in mathematical economics the KKT approach is used in theoretical models in order to obtain qualitative results. For example, consider a firm that maximizes its sales revenue subject to a minimum profit constraint. Letting formula_37 be the quantity of output produced (to be chosen), formula_38 be sales revenue with a positive first derivative and with a zero value at zero output, formula_39 be production costs with a positive first derivative and with a non-negative value at zero output, and formula_40 be the positive minimal acceptable level of profit, then the problem is a meaningful one if the revenue function levels off so it eventually is less steep than the cost function. The problem expressed in the previously given minimization form is\n\nand the KKT conditions are\n\nSince formula_45 would violate the minimum profit constraint, we have formula_46 and hence the third condition implies that the first condition holds with equality. Solving that equality gives\n\nBecause it was given that formula_48 and formula_49 are strictly positive, this inequality along with the non-negativity condition on formula_50 guarantees that formula_50 is positive and so the revenue-maximizing firm operates at a level of output at which marginal revenue formula_52 is less than marginal cost formula_53 — a result that is of interest because it contrasts with the behavior of a profit maximizing firm, which operates at a level at which they are equal.\n\nIf we reconsider the optimization problem as a maximization problem with constant inequality constraints:\n\nThe value function is defined as\n\nso the domain of formula_61 is formula_62\n\nGiven this definition, each coefficient formula_63 is the rate at which the value function increases as formula_64 increases. Thus if each formula_64 is interpreted as a resource constraint, the coefficients tell you how much increasing a resource will increase the optimum value of our function formula_5. This interpretation is especially important in economics and is used, for instance, in utility maximization problems.\n\nWith an extra multiplier formula_67, which may be zero (as long as formula_68), in front of formula_69 the KKT stationarity conditions turn into\n\nwhich are called the Fritz John conditions. This optimality conditions holds without constraint qualifications and it is equivalent to the optimality condition \"KKT or (not-MFCQ)\".\n\nThe KKT conditions belong to a wider class of the first-order necessary conditions (FONC), which allow for non-smooth functions using subderivatives.\n\n\n\n"}
{"id": "9670200", "url": "https://en.wikipedia.org/wiki?curid=9670200", "title": "Kinetic logic", "text": "Kinetic logic\n\nKinetic logic, developed by René Thomas, is a Qualitative Modeling approach feasible to model impact, feedback, and the temporal evolution of the variables. It uses symbolic descriptions and avoids continuous descriptions e.g. differential equations.The derivation of the dynamics from the interaction graphs of systems is not easy. A lot of parameters have to be inferred, for differential description, even if the type of each interaction is known in the graph. Even small modifications in parameters can lead to a strong change in the dynamics. Kinetic Logic is used to build discrete models, in which such details of the systems are not required. The information required can be derived directly from the graph of interactions or from a sufficiently explicit verbal description. It only considers the thresholds of the elements and uses logical equations to construct state tables. Through this procedure, it is a straightforward matter to determine the behavior of the system.\n\nFollowing is René Thomas’s formalism for Kinetic Logic :\n\nIn a directed graph G = (V, A), we note G− (v) and G+ (v) the set of predecessors and successors of a node v ∈ V respectively.\n\nDefinition 1: A biological regulatory network (BRN) is a tuple G = (V, A, l, s, t, K) where \n(V, A) is a directed graph denoted by G,\nl is a function from V to N,\ns is a function from A to {+, −},\nt is a function from A to N such that, for all u ∈ V, if G+(u) is not empty then {t(u, v) | v ∈ G+(u)} = { 1, . . . , l(u)}.\nK = {Kv | v ∈ V} is a set of maps: for each v ∈ V, Kv is a function from 2G− (v) to {0, . . . , l(v)} such that Kv(ω) ≤ Kv(ω_) for all ω ⊆ ω_ ⊆ G−(v).\n\nThe map l describes the domain of each variable v: if l (v) = k, the abstract concentration on v holds its value in {0, 1, . . . , k}. Similarly, the map s represents the sign of the regulation (+ for an activation, − for an inhibition). t (u, v) is the threshold of the regulation from u to v: this regulation takes place iff the abstract concentration of u is above t(u, v), in such a case the regulation is said active. The condition on these thresholds states that each variation of the level of u induces a modification of the set of active regulations starting from u. For all x ∈ [0, . . ., l(u) − 1], the set of active regulations of u, when the discrete expression level of u is x, differs from the set when the discrete expression level is x + 1. Finally, the map Kv allows us to define what is the effect of a set of regulators on the specific target v. If this set is ω ⊆ G− (v), then, the target v is subject to a set of regulations which makes it to evolve towards a particular level Kv(ω).\n\nDefinition 2 (States): \nA state μ of a BRN G = (V, A, l, s, t, K) is a function from V to N such that μ (v) ∈ {0 .., l (v)} for all variables v ∈ V. We denote EG the set of states of G.\nWhen μ (u) ≥ t (u, v) and s (u, v) = +, we say that u is a resource of v since the activation takes place. Similarly when μ (u) < t (u, v) and s (u, v) = −, u is also a resource of v since the inhibition does not take place (the absence of the inhibition is treated as an activation).\n\nDefinition 3 (Resource function): \nLet G = (V, A, l, s, t, K) be a BRN. For each v ∈ V we define the resource function ωv: EG → 2G− (v) by:\nωv (μ) = {u ∈ G−(v) | (μ(u) ≥ t(u, v) and s(u, v) = +) or (μ (u) < t (u, v) and s (u, v) = −)}.\nAs said before, at state μ, Kv (ωv(μ)) gives the level towards which the variable v tends to evolve. We consider three cases,\n\nDefinition 4 (Signs of derivatives): \nLet G = (V, A, l, s, t, K) be a BRN and v ∈ V. \nWe define αv: EG → {+1, 0, −1} by αv(μ) =\n+1 if Kv (ωv(μ)) > μ(u)\n0 if Kv (ωv(μ)) = μ(u)\n−1 if Kv (ωv(μ)) < μ(u)\n\nThe signs of derivatives show the tendency of the solution trajectories.\n\nThe state graph of BRN represents the set of the states that a BRN can adopt with transitions among them deduced from the previous rules:\n\nDefinition 5 (State graph): \nLet G = (V, A, b, s, t,K) be a BRN. The state graph of G is a directed graph G = (EG, T) with (μ, μ_) ∈ T if there exists v ∈ V such that:\nαv (μ) ≠ 0 and μ’ (v) = μ (v) + αv (μ) and μ (u) = μ’ (u), ∀u ∈ V \\ {v}.\n\nThe critical assumptions of Kinetic Logic are:\n\nFollowing are the steps of Application of Kinetic Logic (Also shown in figure A).\n\nKeeping the research problem in mind, the behavior of elements in the system and their interactions are studied. Elements of a system can interact positively or negatively, that is, the level of an element may activate or reduce the rate of production of other elements or of itself. These interactions are represented as positive (activation) or negative (inhibition). \nWhen elements are connected in a topologically circular way, they exert an influence on their own rate of synthesis and they form a feedback loop. A feedback loop is positive or negative according to whether it contains an even or odd number of negative interactions. In a positive loop, each element of the system exerts a positive effect on its own rate of synthesis, whereas in a simple negative loop, each element has a negative effect on its own rate of synthesis. A simple positive feedback loop results in epigenetic regulation and have multiple steady states and a simple negative feedback loop results in homeostatic regulation.\nAbstraction: A chain of positive interactions is equivalent to a direct positive interaction between the two extreme elements, and any two negative interactions cancel out each other’s effect. In this way, any simple feedback loop can be abridged to a one-element loop, positive or negative according to the number of negative interactions (even or odd) in the original loop. Accordingly, through extensive literature survey and the application of the above-mentioned rules, a BRN is abstracted.\n\nLogical variables are associated with the elements of the system to describe the state of the system. They consist of the logical values. For example, a system whose state is appropriately described by the levels of substances a, b, and c, each of which can be absent, present at low level, or present at high level are represented by logical values 0, 1, and 2 respectively.\nIf a product an acts to stimulate the production of b, it is a positive regulator. In this case, the rate of synthesis of b increases with increasing concentration of a, and makes a curve similar to that shown in figure B.\nThere is little effect of a, until it reaches a threshold concentration theta, and at higher concentrations a plateau is reached which shows the maximal rate of synthesis of b. Such a nonlinear, bounded curve is called a sigmoid. It can be suggested that a is \"absent\" for a < theta and \"present\" for a > theta. The sigmoid curve can be approximated by the step function, as in figure C. \nNot only logical variables (x, y, z ...) are associated to the elements, that represent their level (e.g., concentration), but also logical functions (X, Y, Z ...) whose value reflects the rate of synthesis of the element. Thus,\nx = 0 means \"gene product absent\"\nx = 1 means \"gene product present\"\nX = 0 means \"gene off\"\nX = 1 means \"gene on\"\n\nKinetic Logic has two forms depending on the following two types of descriptions:\n\nNaïve Logical Description\nConsider a simple two-element system in which product x activates gene Y and product y represses gene X as shown in figure D. Each variable takes only two values; 0 and 1. In other words,\nX = 1 if y = 0 (X \"on” if y absent)\nY = 1 if x = 1 (Y \"on\" if x present)\nThe logical relation of the system can be written: \nX =y\nY=x\n\nGeneralized Kinetic Logic\nThe naive logical description can be generalized and made to accommodate situations in which some variables take more than two values, without complicating the analysis. Any variable has a number of biologically relevant levels, determined by the number of elements regulated by the product x. There is a specific threshold for each regulatory interaction, so if x regulates n elements, it will have up to n different thresholds.\nFor the logical sum, there is a procedure that assigns a specific weight to each term in the logical relation. According to the scale of thresholds of the corresponding variable, the weighted algebraic sum is then discretized, so an n-valued variable is associated with an n-valued function. After discretization the integers of certain weights or sums of weights are called logical parameters. \nGeneralized Kinetic Logic, although maintaining the analytic simplicity of the naive description, has certain features in common with the differential description. The generalized logical relations are completely independent of the differential description and can be directly derived from the graph of interactions or from an explicit verbal description. \nConsider an example of two elements in figure E. Using a software, this graph of interactions is drawn as shown in figure F. There are two thresholds assigned to element y: Ѳ, concerning its interaction with x and Ѳ, concerning its interaction with itself. The variable y and function Y have three possible values: 0, 1, and 2. Element x have a single threshold, Ѳ, because of the interaction x to +y, so the variable x and function X will be two-valued.\n\nThe state table of graph of interactions in figure D is shown in figure G. This table states for each state of the variables (x, y) i.e. present or absent, which products are synthesized and which are not synthesized at a significant rate. Consider the state 00/10, in which both of the gene products are absent but gene X is on. As product x is absent but being synthesized so it can be expected that in near future it will be present and the logical value of x will change from 0 to 1. This can be described by the notation Ō, in which the dash above the digit is due to the fact that variable x is committed to change its value from 0 to 1. Generally, a dash over the figure representing the logical value of a variable each time this value is different from that of the corresponding function. The state just considered can thus be represented as ŌO.\n\nTime Delays\nThe movement of a system from one state to another depends on the time delays. The time delays in systems are short time shifts of arbitrary duration. In view of the relation between a function (gene on or off) and its associated variable (gene product present or absent), the time delays become real entities whose values, far from being arbitrary, reflect specific physical processes (synthesis, degradation, dilution, etc.). The values of the different time delays play an important role in determining the pathway along which the system evolves.\n\nThe temporal relation between a logical variable x which is associated with the level of an element and a logical function X which is associated with its evolution can be explained as follows.\n\nConsider a gene that is off (X = 0) for a considerable time, then is switched on (X = 1) by a signal, and then, after some time, it is switched off again (X= 0) by another signal and the product reappears but not immediately until a proper delay tx has elapsed. If a signal switches the gene off temporarily, the product is still present because it also requires a time delay t’. This can be represented graphically as shown in figure H. Using the state table the temporal sequence of states of the system can be represented as shown in figure I.\n\nCycles\nThe state table in D can be used to identify the cyclic behavior of the system. We can see that state 01 changes to 00, and 00 changes to 10, 10 changes to 11 and 11 changes back to 01. This represents a cycle as the system starts from the state 01 and returns to the same state. The system keeps oscillating between these states. \n\nDeadlocks\nConsider another example in which:\nX=y\nY=x\nThe state table for the system is shown in figure J. The states that are encircled are stable states, as they do not evolve towards any other state. The logical stable states are defined as those for which the vectors xy . .. and XY ... are equal. When we considered the time delays i.e. from ŌŌ the system will proceed to state 1 0 or to state 01, according to whether tx < ty or ty < tx, and from ĪĪ the system will proceed to state 10 or to state 0 1 according to whether ty < tx or tx < ty. The state graph representing delays is shown in figure K.\nThe sequence of states a system depends on the relative values of the time delays. It is assumed that two delays (or sums of delays) are never exactly equal, therefore that two variables will not change their values at the same instant. But do not exclude the possibility because if this rule is applied rigidly, it could occasionally lead to the loss of interesting pathways.\n\nThe cycles and the deadlock states identified by this process are then analyzed by comparing them with the invitro and invivo findings. These results can be used to make important predictions about the system. Cyclic behaviors correspond to homeostatic regulations that retain the level of a variable at or near a fixed or optimal value. Deadlocks represent epigenetic regulation in which the concentrations exist between extreme levels.\n\nThe first approach for qualitative modeling was based on extreme discretization since all genes could be either on (present) or off (absent). This Boolean approach was generalized into a multi-valued approach i.e. Kinetic Logic, in which logical identification of all steady states became possible.\n\nKinetic logic has been employed to study the factors that influence the selection of specific pathway from many different pathways that the system can follow and the factors that lead the system towards stable states and cyclic behaviors. It has been used to reveal the logic that lie behind the functional organization and kinetic behavior of molecules. Model checking techniques have also been applied to models built through kinetic logic, in order to infer their continuous behaviors.\n\nKinetic logic has been applied on many different types of systems in biology, Psychology and Psychiatry. Mostly Kinetic Logic has been used in modeling the biological networks especially the Gene Regulatory Networks (GRNs). \nFollowing are the examples in which Kinetic Logic was employed as the modeling formalism:\n\n\nAs theoretical analysis through Kinetic Logic is a time consuming process, a tool known as Genotech, for modeling and analysis of BRNs was developed on the basis of Kinetic Logic and has been used for a number of Kinetic Logic-based studies. It analyzes behaviors like stable cycles, stable steady states and paths in the state graph (discrete model) of biological systems, accelerating the process of modeling. GenoTech is extremely useful as it allows repeated experimentation by automating the whole process. This tool is available on request.\n\n"}
{"id": "646326", "url": "https://en.wikipedia.org/wiki?curid=646326", "title": "Kunihiko Kodaira", "text": "Kunihiko Kodaira\n\nKodaira was born in Tokyo. He graduated from the University of Tokyo in 1938 with a degree in mathematics and also graduated from the physics department at the University of Tokyo in 1941. During the war years he worked in isolation, but was able to master Hodge theory as it then stood. He obtained his Ph.D. from the University of Tokyo in 1949, with a thesis entitled \"Harmonic fields in Riemannian manifolds\". He was involved in cryptographic work from about 1944, while holding an academic post in Tokyo.\n\nIn 1949 he travelled to the Institute for Advanced Study in Princeton, New Jersey at the invitation of Hermann Weyl. At this time the foundations of Hodge theory were being brought in line with contemporary technique in operator theory. Kodaira rapidly became involved in exploiting the tools it opened up in algebraic geometry, adding sheaf theory as it became available. This work was particularly influential, for example on Friedrich Hirzebruch.\n\nIn a second research phase, Kodaira wrote a long series of papers in collaboration with Donald C. Spencer, founding the deformation theory of complex structures on manifolds. This gave the possibility of constructions of moduli spaces, since in general such structures depend continuously on parameters. It also identified the sheaf cohomology groups, for the sheaf associated with the holomorphic tangent bundle, that carried the basic data about the dimension of the moduli space, and obstructions to deformations. This theory is still foundational, and also had an influence on the (technically very different) scheme theory of Grothendieck. Spencer then continued this work, applying the techniques to structures other than complex ones, such as G-structures.\n\nIn a third major part of his work, Kodaira worked again from around 1960 through the classification of algebraic surfaces from the point of view of birational geometry of complex manifolds. This resulted in a typology of seven kinds of two-dimensional compact complex manifolds, recovering the five algebraic types known classically; the other two being non-algebraic. He provided also detailed studies of elliptic fibrations of surfaces over a curve, or in other language elliptic curves over algebraic function fields, a theory whose arithmetic analogue proved important soon afterwards. This work also included a characterisation of K3 surfaces as deformations of quartic surfaces in \"P\", and the theorem that they form a single diffeomorphism class. Again, this work has proved foundational. (The K3 surfaces were named after Ernst Kummer, Erich Kähler, and Kodaira).\n\nKodaira left the Institute for Advanced Study in 1961, and briefly served as chair at the Johns Hopkins University and Stanford University. In 1967, returned to the University of Tokyo. He was awarded a Wolf Prize in 1984/5. He died in Kofu on 26 July 1997.\n\n\n\n"}
{"id": "7018809", "url": "https://en.wikipedia.org/wiki?curid=7018809", "title": "Lax equivalence theorem", "text": "Lax equivalence theorem\n\nIn numerical analysis, the Lax equivalence theorem is the fundamental theorem in the analysis of finite difference methods for the numerical solution of partial differential equations. It states that for a consistent finite difference method for a well-posed linear initial value problem, the method is convergent if and only if it is stable.\n\nThe importance of the theorem is that while convergence of the solution of the finite difference method to the solution of the partial differential equation is what is desired, it is ordinarily difficult to establish because the numerical method is defined by a recurrence relation while the differential equation involves a differentiable function. However, consistency—the requirement that the finite difference method approximate the correct partial differential equation—is straightforward to verify, and stability is typically much easier to show than convergence (and would be needed in any event to show that round-off error will not destroy the computation). Hence convergence is usually shown via the Lax equivalence theorem.\n\nStability in this context means that a matrix norm of the matrix used in the iteration is at most unity, called (practical) Lax–Richtmyer stability. Often a von Neumann stability analysis is substituted for convenience, although von Neumann stability only implies Lax–Richtmyer stability in certain cases.\n\nThis theorem is due to Peter Lax. It is sometimes called the Lax–Richtmyer theorem, after Peter Lax and Robert D. Richtmyer.\n"}
{"id": "3140923", "url": "https://en.wikipedia.org/wiki?curid=3140923", "title": "Linearly disjoint", "text": "Linearly disjoint\n\nIn mathematics, algebras \"A\", \"B\" over a field \"k\" inside some field extension formula_1 of \"k\" are said to be linearly disjoint over \"k\" if the following equivalent conditions are met:\n\nNote that, since every subalgebra of formula_1 is a domain, (i) implies formula_7 is a domain (in particular reduced). Conversely if \"A\" and \"B\" are fields and either \"A\" or \"B\" is an algebraic extension of \"k\" and formula_7 is a domain then it is a field and \"A\" and \"B\" are linearly disjoint. However there are examples where formula_7 is a domain but \"A\" and \"B\" are not linearly disjoint: for example, \"A\"=\"B\"=\"k\"(\"t\"), the field of rational functions over \"k\".\n\nOne also has: \"A\", \"B\" are linearly disjoint over \"k\" if and only if subfields of formula_1 generated by formula_11, resp. are linearly disjoint over \"k\". (cf. tensor product of fields)\n\nSuppose \"A\", \"B\" are linearly disjoint over \"k\". If formula_12, formula_13 are subalgebras, then formula_14 and formula_15 are linearly disjoint over \"k\". Conversely, if any finitely generated subalgebras of algebras \"A\", \"B\" are linearly disjoint, then \"A\", \"B\" are linearly disjoint (since the condition involves only finite sets of elements.)\n\n\n"}
{"id": "5075985", "url": "https://en.wikipedia.org/wiki?curid=5075985", "title": "Machinist calculator", "text": "Machinist calculator\n\nA machinist calculator is a hand-held calculator programmed with built-in formulas making it easy and quick for machinists to establish speeds, feeds and time without guesswork or conversion charts. Formulas may include revolutions per minute (RPM), surface feet per minute (SFM), inches per minute (IPM), feed per tooth (FPT). A cut time (CT) function takes the user, step-by-step, through a calculation to determine cycle time (execution time) for a given tool motion. Other features may include a metric-English conversion function, a stop watch/timer function and a standard math calculator. \n\nThis type of calculator is useful for machinists, programmers, inspectors, estimators, supervisors, and students.\n\n"}
{"id": "2218782", "url": "https://en.wikipedia.org/wiki?curid=2218782", "title": "Mental poker", "text": "Mental poker\n\nMental poker is the common name for a set of cryptographic problems that concerns playing a fair game over distance without the need for a trusted third party. The term is also applied to the theories surrounding these problems and their possible solutions. The name comes from the card game poker which is one of the games to which this kind of problem applies. Similar problems described as two party games are Blum's flipping a coin over a distance, Yao's Millionaires' Problem, and Rabin's Oblivious transfer.\n\nThe problem can be described thus: \"How can one allow only authorized actors to have access to certain information while not using a trusted arbiter?\". (Eliminating the trusted third-party avoids the problem of trying to determine whether the third party can be trusted or not, and may also reduce the resources required.)\n\nIn poker, this could translate to: \"How can we make sure no player is stacking the deck or peeking at other players' cards when we are shuffling the deck ourselves?\". In a physical card game, this would be relatively simple if the players were sitting face to face and observing each other, at least if the possibility of conventional cheating can be ruled out. However, if the players are not sitting at the same location but instead are at widely separate locations and pass the entire deck between them (using the postal mail, for instance), this suddenly becomes very difficult. And for electronic card games, such as online poker, where the mechanics of the game are hidden from the user, this is impossible unless the method used is such that it cannot allow any party to cheat by manipulating or inappropriately observing the electronic \"deck\".\n\nSeveral protocols for doing this have been suggested, the first by Adi Shamir, Ron Rivest and Len Adleman (the creators of the RSA-encryption protocol). This protocol was the first example of two parties conducting secure computation rather than secure message transmission, employing cryptography; later on due to leaking partial information in the original protocol, this led the definition of Semantic security by Shafi Goldwasser and Silvio Micali. Multi-player mental poker was first given in \nThe area has later evolved into what is known as Secure multi-party computation protocols (for two parties, and multi parties as well).\n\nOne possible algorithm for shuffling cards without the use of a trusted third party is to use a commutative encryption scheme. A commutative scheme means that if some data is encrypted more than once, the order in which you decrypt this data will not matter.\n\nExample: Alice has a plaintext message. She encrypts this, producing a garbled ciphertext which she gives then to Bob. Bob encrypts the ciphertext again, using the same scheme as Alice but with another key. When decrypting this double encrypted message, if the encryption scheme is commutative, it will not matter who decrypts first.\n\nAn algorithm for shuffling cards using commutative encryption would be as follows:\n\n\nThe deck is now shuffled.\n\nThis algorithm may be expanded for an arbitrary number of players. Players Carol, Dave and so forth need only repeat steps 2-4 and 8-10.\n\nDuring the game, Alice and Bob will pick cards from the deck, identified in which order they are placed in the shuffled deck. When either player wants to see their cards, they will request the corresponding keys from the other player. That player, upon checking that the requesting player is indeed entitled to look at the cards, passes the individual keys for those cards to the other player. The check is to ensure that the player does not try to request keys for cards that do not belong to that player.\n\nExample: Alice has picked cards 1 to 5 in the shuffled deck. Bob has picked cards 6 to 10. Bob requests to look at his allotted cards. Alice agrees that Bob is entitled to look at cards 6 to 10 and gives him her individual card keys A to A. Bob decrypts his cards by using both Alice's keys and his own for these cards, B to B. Bob can now see the cards. Alice cannot know which cards Bob has because she does not have access to Bob's keys B to B which are required to decrypt the cards.\n\nThe encryption scheme used must be secure against known-plaintext attacks: Bob must not be able to determine Alice's original key A (or enough of it to allow him to decrypt any cards he does not hold) based on his knowledge of the unencrypted values of the cards he has drawn. This rules out some obvious commutative encryption schemes, such as simply XORing each card with the key. (Using a separate key for each card even in the initial exchange, which would otherwise make this scheme secure, doesn't work since the cards are shuffled before they're returned.)\n\nDepending on the deck agreed upon, this algorithm may be weak. When encrypting data, certain properties of this data may be preserved from the plaintext to the ciphertext. This may be used to \"tag\" certain cards. Therefore, the parties must agree on a deck where no cards have properties that are preserved during encryption.\n\nChristian Schindelhauer describes sophisticated protocols to both perform and verify a large number of useful operations on cards and stacks of cards in his 1998 paper [SCH98]. The work is concerned with general-purpose operations (masking and unmasking cards, shuffling and re-shuffling, inserting a card into a stack, etc.) that make the protocols applicable to any card game. The cryptographic protocols used by Schindelhauer are based on quadratic residuosity, and the general scheme is similar in spirit to the above protocol. The correctness of operations can be checked by using zero-knowledge proofs, so that players don't need to reveal their strategy to verify the game's correctness.\n\nThe C++ library libtmcg [STA05] provides an implementation of the Schindelhauer toolbox. It has been used to implement a secure version of the German card game Skat, achieving modest real-world performance. The game Skat is played by three players with a 32-card deck, and so is substantially less computationally intensive than a poker game in which anywhere from five to eight players use a full 52-card deck.\n\nTo date, mental poker approaches based on the standard Alice-Bob protocol (above) don't offer high enough performance for real-time online play. The requirement that each player encrypts each card imposes a substantial overhead. A recent paper by Golle [GOL05] describes a mental poker protocol that achieves significantly higher performance by exploiting the properties of the poker game to move away from the encrypt-shuffle model. Rather than shuffle the cards and then deal as needed, with the new approach, the players generate (encrypted) random numbers on the fly, which are used to select the next card. Every new card needs to be checked against all the cards that have already been dealt to detect duplicates. As a result, this method is uniquely useful in poker-style games, in which the number of cards dealt is very small compared to the size of the whole deck. However, the method needs all cards that have already been dealt to be known to all, which in most poker-style games would beat its very purpose.\n\nThe card-generation algorithm requires a cryptosystem with two key properties. The encryption E must be additively homomorphic, so that \"E(c)+E(c) = E(c + c)\". Second, collisions must be detectable, without revealing the plaintext. In other words, given \"E(c)\" and \"E(c)\", it must be possible to answer whether \"c=c\", without the players learning any other information (specifically, the identities of \"c\" and \"c\"). The Elgamal encryption scheme is just one example of a well-known system with these properties.\nThe algorithm operates as follows:\n\nIn this way, the players need only to compute encryption for the cards that are actually used in the game, plus some overhead for the collisions that is small as long as the number of cards needed is much less than the size of the deck. As a result, this scheme turns out to be 2-4 times faster (as measured by the total number of modular exponentiations) than the best-known protocol [JAK99] that does full shuffling using mix-networks.\n\nNote that the random number generation is secure as long as any one player is generating valid random numbers. Even if \"k-1\" players collude to generate the number \"r*\", as long as the \"k\"th player truthfully generates a random formula_3, the sum formula_4 is still uniformly random in {0, 51}.\n\nMeasured in terms of the number of single-agent encryptions, the algorithm in [GOL05] is optimal when no collisions occur, in the sense that any protocol that is fair to every player must perform at least as many encryption operations. At minimum, every agent must encrypt every card that is actually used. Otherwise, if any agent doesn't participate in the encryption, then that agent is susceptible to being cheated by a coalition of the remaining players. Unknown to the non-encrypting agent, the other agents may share the keys to enable them all to know the values of all the cards. Thus, any approach relying on the agents to perform the encryption must focus on schemes that minimize the effect of collisions if they are to achieve better performance.\n\nAny mental poker protocol that relies on the players to perform the encryption is bound by the requirement that every player encrypt every card that is dealt. However, by making limited assumptions about the trustworthiness of third parties, significantly more efficient protocols may be realized. The protocol for choosing cards without shuffling may be adapted so that the encryption is handled by two or more servers. Under the assumption that the servers are non-colluding, such a protocol is secure.\n\nThe basic protocol using two servers is as follows:\n\nIn this protocol, servers \"S1\" and \"S2\" must collude if either is to learn the values of any cards. Furthermore, because players ultimately decide which cards are dealt, non-trustworthy servers are unable to influence the game to the extent that is possible in traditional online poker. The scheme may be extended to allow more servers, (and thus, increased security), simply by including the additional servers in the initial encryption. Finally, step one in the protocol may be done offline, allowing for large numbers of shuffled, encrypted \"decks\" to be pre-computed and cached, resulting in excellent in-game performance.\n\n\n"}
{"id": "32743787", "url": "https://en.wikipedia.org/wiki?curid=32743787", "title": "Michèle Audin", "text": "Michèle Audin\n\nMichèle Audin is a French mathematician, writer, and a former professor. She has worked as a professor at the University of Geneva, the University of Paris-Sud and most recently at l'Institut de recherche mathématique avancée (IRMA) in University of Strasbourg, where she performed research notably in the area of symplectic geometry.\n\nBorn in 1954, Audin is a former student of l'École normale supérieure de jeunes filles within the École Normale Supérieure. She earned a Ph.D. degree in 1984 from the University of Paris-Sud, with a thesis written under the supervision of François Latour. She became a member of the Oulipo in 2009.\n\nShe is the daughter of mathematician Maurice Audin, who died under torture in 1957 in Algeria, after having been arrested by parachutists of General Jacques Massu. On January 1, 2009, she refused to receive the Legion of Honour, on the grounds that the President of France, Nicolas Sarkozy, had refused to respond to a letter written by her mother regarding the disappearance of her father.\n\n\n"}
{"id": "54458389", "url": "https://en.wikipedia.org/wiki?curid=54458389", "title": "Muriel Kennett Wales", "text": "Muriel Kennett Wales\n\nMuriel Kennett Wales (9 Jun 1913 – 8 August 2009) was an Irish-Canadian mathematician, and is believed to have been the first Irish-born woman to earn a PhD in pure mathematics.\n\nShe was born Muriel Kennett on 9 June 1913 in Belfast. In 1914, her mother moved to Vancouver and soon remarried; henceforth Muriel was known by her mother's new last name, Wales.\nShe was first educated at the University of British Columbia (BA 1934, MA 1937 with the thesis \"Determination of Bases for Certain Quartic Number Fields\"). In 1941 she was awarded the PhD from the University of Toronto for the dissertation \"Theory Of Algebraic Functions Based On The Use Of Cycles\" under Samuel Beatty (himself the first person to receive a PhD in mathematics in Canada, in 1915).\n\nShe spent most of the 1940s working in atomic energy, in Toronto and Montreal, but by 1949 had retired back to Vancouver where she worked in her step-father's shipping company.\n"}
{"id": "39782", "url": "https://en.wikipedia.org/wiki?curid=39782", "title": "N-sphere", "text": "N-sphere\n\nIn mathematics, the -sphere is the generalization of the ordinary sphere to spaces of arbitrary dimension. It is an -dimensional manifold that can be embedded in Euclidean -space.\n\nThe 0-sphere is a pair of points, the 1-sphere is a circle, and the 2-sphere is an ordinary sphere. Generally, when embedded in an -dimensional Euclidean space, an -sphere is the surface or boundary of an -dimensional ball. That is, for any natural number , an -sphere of radius may be defined in terms of an embedding in -dimensional Euclidean space as the set of points that are at distance from a central point, where the radius may be any positive real number. Thus, the -sphere would be defined by:\n\nIn particular:\n\nAn \"n\"-sphere embedded in an -dimensional Euclidean space is called a hypersphere. The -sphere of unit radius is called the unit -sphere, denoted , often referred to as \"the\" -sphere.\n\nWhen embedded as described, an -sphere is the surface or boundary of an -dimensional ball. For , the -spheres are the simply connected -dimensional manifolds of constant, positive curvature. The -spheres admit several other topological descriptions: for example, they can be constructed by gluing two -dimensional Euclidean spaces together, by identifying the boundary of an -cube with a point, or (inductively) by forming the suspension of an -sphere.\n\nFor any natural number , an -sphere of radius is defined as the set of points in -dimensional Euclidean space that are at distance from some fixed point , where may be any positive real number and where may be any point in -dimensional space. In particular:\n\nThe set of points in -space, , that define an -sphere, , is represented by the equation:\n\nwhere = is a center point, and is the radius.\n\nThe above -sphere exists in -dimensional Euclidean space and is an example of an -manifold. The volume form of an -sphere of radius is given by\n\nwhere is the Hodge star operator; see for a discussion and proof of this formula in the case . As a result,\n\nThe space enclosed by an -sphere is called an -ball. An -ball is closed if it includes the -sphere, and it is open if it does not include the -sphere.\n\nSpecifically:\n\nTopologically, an -sphere can be constructed as a one-point compactification of -dimensional Euclidean space. Briefly, the -sphere can be described as , which is -dimensional Euclidean space plus a single point representing infinity in all directions.\nIn particular, if a single point is removed from an -sphere, it becomes homeomorphic to . This forms the basis for stereographic projection.\n\n<div style=\"overflow-wrap: normal\">In general, the volume of the -ball in -dimensional Euclidean space, and the surface area of the -sphere in -dimensional Euclidean space, of radius , are proportional to the th power of the radius, (with different constants of proportionality that vary with ). We write for the volume of the -ball and for the surface area of the -sphere, both of radius , where and are the values for the unit-radius case.<div>\n\nGiven the radius , the enclosed volume and the surface area of the -sphere reach a maximum and then decrease towards zero as the dimension increases. In particular, the volume enclosed by the -sphere of constant radius embedded in dimensions reaches a maximum at the dimension satisfying and where is given in the sidebar to the right; if the last (weak) inequality holds with equality, then the same maximum also occurs at . Specifically, for unit radius the largest enclosed volume is that enclosed by a 4-dimensional sphere bounding a 5-dimensional ball.\n\nSimilarly, the surface area of the -sphere of constant radius embedded in dimensions reaches a maximum for dimension that satisfies and where is given in the sidebar to the right; if the last (weak) inequality holds with equality, then the same maximum also occurs at . Specifically, for unit radius the largest surface area occurs for the 6-dimensional sphere bounding a 7-dimensional ball.\n\nThe 0-ball consists of a single point. The 0-dimensional Hausdorff measure is the number of points in a set. So, \nThe unit 1-ball is the interval of length 2. So,\nThe 0-sphere consists of its two end-points, . So,\nThe unit 1-sphere is the unit circle in the Euclidean plane, and this has circumference (1-dimensional measure)\nThe region enclosed by the unit 1-sphere is the 2-ball, or unit disc, and this has area (2-dimensional measure)\nAnalogously, in 3-dimensional Euclidean space, the surface area (2-dimensional measure) of the unit 2-sphere is given by\nand the volume enclosed is the volume (3-dimensional measure) of the unit 3-ball, given by\n\nThe \"surface area\", or properly the -dimensional volume, of the -sphere at the boundary of the -ball of radius is related to the volume of the ball by the differential equation\nor, equivalently, representing the unit -ball as a union of concentric -sphere \"shells\",\nSo, \n\nWe can also represent the unit -sphere as a union of tori, each the product of a circle (1-sphere) with an -sphere. Let and , so that and . Then,\nSince , the equation\nholds for all .\n\nThis completes our derivation of the recurrences:\n\nCombining the recurrences, we see that\nSo it is simple to show by induction on \"k\" that,\nwhere denotes the double factorial, defined for odd integers by .\n\nIn general, the volume, in -dimensional Euclidean space, of the unit -ball, is given by\n\nwhere is the gamma function, which satisfies , , and .\n\nBy multiplying by , differentiating with respect to , and then setting , we get the closed form\n\nThe recurrences can be combined to give a \"reverse-direction\" recurrence relation for surface area, as depicted in the diagram:\n\nIndex-shifting to then yields the recurrence relations:\n\nwhere , , and .\n\nThe recurrence relation for can also be proved via integration with 2-dimensional polar coordinates:\n\nWe may define a coordinate system in an -dimensional Euclidean space which is analogous to the spherical coordinate system defined for 3-dimensional Euclidean space, in which the coordinates consist of a radial coordinate , and angular coordinates , where the angles range over radians (or over degrees) and ranges over radians (or over degrees). If are the Cartesian coordinates, then we may compute from with: \n\nExcept in the special cases described below, the inverse transformation is unique:\n\nwhere if for some but all of are zero then when , and (180 degrees) when .\n\nThere are some special cases where the inverse transform is not unique; for any will be ambiguous whenever all of are zero; in this case may be chosen to be zero.\n\nExpressing the angular measures in radians, the volume element in -dimensional Euclidean space will be found from the Jacobian of the transformation:\n\nand the above equation for the volume of the -ball can be recovered by integrating:\n\nThe volume element of the -sphere, which generalizes the area element of the 2-sphere, is given by\n\nThe natural choice of an orthogonal basis over the angular coordinates is a product of ultraspherical polynomials,\n\nfor , and the for the angle in concordance with the spherical harmonics.\n\nJust as a two-dimensional sphere embedded in three dimensions can be mapped onto a two-dimensional plane by a stereographic projection, an -sphere can be mapped onto an -dimensional hyperplane by the -dimensional version of the stereographic projection. For example, the point on a two-dimensional sphere of radius 1 maps to the point on the -plane. In other words,\n\nLikewise, the stereographic projection of an -sphere of radius 1 will map to the -dimensional hyperplane perpendicular to the -axis as\n\nTo generate uniformly distributed random points on the unit -sphere (that is, the surface of the unit -ball), gives the following algorithm.\n\nGenerate an -dimensional vector of normal deviates (it suffices to use , although in fact the choice of the variance is arbitrary), . Now calculate the \"radius\" of this point:\n\nThe vector is uniformly distributed over the surface of the unit -ball.\n\nAn alternative given by Marsaglia is to uniformly randomly select a point in the unit -cube by sampling each independently from the uniform distribution over , computing as above, and rejecting the point and resampling if (i.e., if the point is not in the -ball), and when a point in the ball is obtained scaling it up to the spherical surface by the factor ; then again is uniformly distributed over the surface of the unit -ball.\n\nWith a point selected uniformly at random from the surface of the unit -sphere (e.g., by using Marsaglia's algorithm), one needs only a radius to obtain a point uniformly at random from within the unit -ball. If is a number generated uniformly at random from the interval and is a point selected uniformly at random from the unit -sphere, then is uniformly distributed within the unit -ball.\n\nAlternatively, points may be sampled uniformly from within the unit -ball by a reduction from the unit -sphere. In particular, if is a point selected uniformly from the unit -sphere, then is uniformly distributed within the unit -ball (i.e., by simply discarding two coordinates).\n\nNote that if is sufficiently large, most of the volume of the -ball will be contained in the region very close to its surface, so a point selected from that volume will also probably be close to the surface. This is one of the phenomena leading to the so-called curse of dimensionality that arises in some numerical and other applications.\n\n\n\n"}
{"id": "43915231", "url": "https://en.wikipedia.org/wiki?curid=43915231", "title": "Nachum Dershowitz", "text": "Nachum Dershowitz\n\nNachum Dershowitz is an Israeli computer scientist, known e.g. for the Dershowitz–Manna ordering used to prove termination of term rewrite systems.\n\nHe obtained his B.Sc. summa cum laude in 1974 in Computer Science–Applied Mathematics from Bar-Ilan University, and his Ph.D. in 1979 in Applied Mathematics from the Weizmann Institute of Science.\nSince 1978, he worked at Department of Computer Science of the University of Illinois at Urbana-Champaign, until he became a full professor of the Tel Aviv University (School of Computer Science) in 1998. \nHe was a guest researcher at Weizmann Institute, INRIA, ENS Cachan, Microsoft Research, and the universities of Stanford, Paris, Jerusalem, Chicago, and Beijing.\n\n\n"}
{"id": "58139364", "url": "https://en.wikipedia.org/wiki?curid=58139364", "title": "Nira Dyn", "text": "Nira Dyn\n\nNira (Richter) Dyn () is an Israeli mathematician and an expert in geometric modeling, subdivision surfaces, approximation theory, and image compression. She is a professor emeritus of applied mathematics at Tel Aviv University, and has been called a \"pioneer and leading researcher in the subdivision community\".\n\nDyn earned a bachelor's degree from the Technion – Israel Institute of Technology in 1965. She went on to graduate study at the Weizmann Institute of Science, where she earned a master's degree in 1967 and completed her doctorate in 1970.\nHer dissertation, \"Optimal and Minimum Norm Approximations to Linear Functionals in Hilbert Spaces, and their application to Numerical Integration\", was supervised by Philip Rabinowitz.\nAfter postdoctoral research in the Institute of Fundamental Studies at the University of Rochester, she joined the Tel Aviv faculty in 1972, and retired in 2010.\n\nDyn was an invited speaker at the 2006 International Congress of Mathematicians, in the section on numerical analysis and scientific computing.\n\nDyn is the author of:\n\n"}
{"id": "56012715", "url": "https://en.wikipedia.org/wiki?curid=56012715", "title": "Nonblocker", "text": "Nonblocker\n\nIn graph theory, a nonblocker is a subset of vertices in an undirected graph, all of which are adjacent to vertices outside of the subset. Equivalently, a nonblocker is the complement of a dominating set.\n\nThe computational problem of finding the largest nonblocker in a graph was formulated by , who observed that it belongs to MaxSNP.\nAlthough computing a dominating set is not fixed-parameter tractable under standard assumptions, the complementary problem of finding a nonblocker of a given size is fixed-parameter tractable.\n\nIn graphs with no isolated vertices, every maximal nonblocker (one to which no more vertices can be added) is itself a dominating set.\n\nOne way to construct a fixed-parameter tractable algorithm for the nonblocker problem is to use kernelization, an algorithmic design principle in which a polynomial-time algorithm is used to reduce a larger problem instance to an equivalent instance whose size is bounded by a function of the parameter.\nFor the nonblocker problem, an input to the problem consists of a graph formula_1 and a parameter formula_2, and the goal is to determine whether formula_1 has a nonblocker with formula_2 or more vertices.\n\nThis problem has an easy kernelization that reduces it to an equivalent problem with at most formula_5 vertices. First, remove all isolated vertices from formula_1, as they cannot be part of any nonblocker. Once this has been done, the remaining graph must have a nonblocker that includes at least half of its vertices; for instance, if one 2-colors any spanning tree of the graph, each color class is a nonblocker and one of the two color classes includes at least half the vertices. Therefore, if the graph with isolated vertices removed still has formula_5 or more vertices, the problem can be solved immediately. Otherwise, the remaining graph is a kernel with at most formula_5 vertices.\n\nDehne et al. improved this to a kernel of size at most formula_9. Their method involves merging pairs of neighbors of degree-one vertices until all such vertices have a single neighbor, and removing all but one of the degree-one vertices, leaving an equivalent instance\nwith only one degree-one vertex. Then, they show that (except for small values of formula_2, which can be handled separately) this instance must either be smaller than the kernel size bound or contain a formula_2-vertex blocker.\n\nOnce a small kernel has been obtained, an instance of the nonblocker problem may be solved in fixed-parameter tractable time by applying a brute-force search algorithm to the kernel. Applying faster (but still exponential) time bounds leads to a time bound for the nonblocker problem of the form formula_12. Even faster algorithms are possible for certain special classes of graphs.\n"}
{"id": "41317193", "url": "https://en.wikipedia.org/wiki?curid=41317193", "title": "Nonlinear modelling", "text": "Nonlinear modelling\n\nIn mathematics, nonlinear modelling is empirical or semi-empirical modelling which takes at least some nonlinearities into account. Nonlinear modelling in practice therefore means modelling of phenomena in which independent variables affecting the system can show complex and synergetic nonlinear effects. Contrary to traditional modelling methods, such as linear regression and basic statistical methods, nonlinear modelling can be utilized efficiently in a vast number of situations where traditional modelling is impractical or impossible. The newer nonlinear modelling approaches include non-parametric methods, such as feedforward neural networks, kernel regression, multivariate splines, etc., which do not require a \"priori\" knowledge of the nonlinearities in the relations. Thus the nonlinear modelling can utilize production data or experimental results while taking into account complex nonlinear behaviours of modelled phenomena which are in most cases practically impossible to be modelled by means of traditional mathematical approaches, such as phenomenological modelling.\n\nContrary to phenomenological modelling, nonlinear modelling can be utilized in processes and systems where the theory is deficient or there is a lack of fundamental understanding on the root causes of most crucial factors on system. Phenomenological modelling describes a system in terms of laws of nature. Nonlinear modelling can be utilized in situations where the phenomena are not well understood or expressed in mathematical terms. Thus nonlinear modelling can be an efficient way to model new and complex situations where relationships of different variables are not known.\n"}
{"id": "1313432", "url": "https://en.wikipedia.org/wiki?curid=1313432", "title": "Origin (mathematics)", "text": "Origin (mathematics)\n\nIn mathematics, the origin of a Euclidean space is a special point, usually denoted by the letter \"O\", used as a fixed point of reference for the geometry of the surrounding space.\n\nIn physical problems, the choice of origin is often arbitrary, meaning any choice of origin will ultimately give the same answer. This allows one to pick an origin point that makes the mathematics as simple as possible, often by taking advantage of some kind of geometric symmetry.\n\nIn a Cartesian coordinate system, the origin is the point where the axes of the system intersect. The origin divides each of these axes into two halves, a positive and a negative semiaxis. Points can then be located with reference to the origin by giving their numerical coordinates—that is, the positions of their projections along each axis, either in the positive or negative direction. The coordinates of the origin are always all zero, for example (0,0) in two dimensions and (0,0,0) in three.\n\nIn a polar coordinate system, the origin may also be called the pole. It does not itself have well-defined polar coordinates, because the polar coordinates of a point include the angle made by the positive \"x\"-axis and the ray from the origin to the point, and this ray is not well-defined for the origin itself.\n\nIn Euclidean geometry, the origin may be chosen freely as any convenient point of reference.\n\nThe origin of the complex plane can be referred as the point where real axis and imaginary axis intersect each other. In other words, it is the complex number zero.\n\n"}
{"id": "58277245", "url": "https://en.wikipedia.org/wiki?curid=58277245", "title": "Peter Chadwick (mathematician)", "text": "Peter Chadwick (mathematician)\n\nPeter Chadwick (23 March 1931 – 12 August 2018) was a British applied mathematician and physicist.\n\nA Huddersfield native born on 23 March 1931, Chadwick attended the University of Manchester (BSc, 1952) and completed his PhD at Pembroke College, Cambridge in 1957. He was Professor of Mathematics at the University of East Anglia from 1965 to 1991, and was made a Fellow of the Royal Society in 1977. He died on 12 August 2018, aged 87.\n"}
{"id": "2836040", "url": "https://en.wikipedia.org/wiki?curid=2836040", "title": "Richard Askey", "text": "Richard Askey\n\nRichard \"Dick\" Allen Askey (born June 4, 1933) is an American mathematician, known for his expertise in the area of special functions. The Askey–Wilson polynomials (introduced by him in 1984 together with James A. Wilson) are on the top level of the (q)-Askey scheme, which organizes orthogonal polynomials of (q-)hypergeometric type into a hierarchy. The Askey–Gasper inequality for Jacobi polynomials is essential in de Brange's famous proof of the Bieberbach conjecture.\n\nAskey earned a B.A. at Washington University in 1955, an M.A. at Harvard University in 1956, and a Ph.D. at Princeton University in 1961. After working as an instructor at Washington University (1958–1961) and University of Chicago (1961–1963), he joined the faculty of the University of Wisconsin–Madison in 1963 as an Assistant Professor of Mathematics. He became a full professor at Wisconsin in 1968, and since 2003 has been a professor emeritus. Askey was a Guggenheim Fellow, 1969–1970, which academic year he spent at the \"Mathematisch Centrum\" in Amsterdam.\nIn 1983 he gave an invited lecture at the International Congress of Mathematicians (ICM) in Warszawa.\nHe was elected a Fellow of the American Academy of Arts and Sciences in 1993.\nIn 1999 he was elected to the National Academy of Sciences.\nIn 2009 he became a fellow of the Society for Industrial and Applied Mathematics (SIAM).\nIn 2012 he became a fellow of the American Mathematical Society.\nIn December 2012 he received an honorary doctorate from SASTRA University in Kumbakonam, India.\n\nAskey explained why hypergeometric functions appear so frequently in mathematical applications: \"Riemann showed that the requirement that a differential equation have regular singular points at three given points and every other complex point is a regular point is so strong a restriction that the differential equation is the hypergeometric equation with the three singularities moved to the three given points. Differential equations with four or more singular points only infrequently have a solution which can be given explicitly as a series whose coefficients are known, or have an explicit integral representation. This partly explains why the classical hypergeometric function arises in many settings that seem to have nothing to do with each other. The differential equation they satisfy is the most general one of its kind that has solutions with many nice properties\".\n\nAskey is also very much involved with commenting and writing on mathematical education at American schools. A well-known article by him on this topic is \"Good Intentions are not Enough\".\n\n\n"}
{"id": "2073462", "url": "https://en.wikipedia.org/wiki?curid=2073462", "title": "Second-order cybernetics", "text": "Second-order cybernetics\n\nSecond-order cybernetics, also known as the cybernetics of cybernetics, is the recursive application of cybernetics to itself. It was developed between approximately 1968 and 1975 by Margaret Mead, Heinz von Foerster and others. Von Foerster referred to it as the cybernetics of \"observing systems\" whereas first order cybernetics is that of \"observed systems\". It is sometimes referred to as the \"new cybernetics\", the term preferred by Gordon Pask, and is closely allied to radical constructivism, which was developed around the same time by Ernst von Glasersfeld. While it is sometimes considered a radical break from the earlier concerns of cybernetics, there is much continuity with previous work and it can be thought of as the completion of the discipline, responding to issues evident during the Macy conferences in which cybernetics was initially developed. Its concerns include epistemology, ethics, autonomy, self-consistency, self-referentiality, and self-organizing capabilities of complex systems. It has been characterised as cybernetics where \"circularity is taken seriously\".\n\nSecond-order Cybernetics can be abbreviated as C2 or SOC, and is sometimes referred to as the cybernetics of cybernetics, the new cybernetics or second cybernetics. The terms are often used interchangeably, but can also stress different aspects:\n\nSecond-order cybernetics took shape during the late 1960s and mid 1970s. The 1967 keynote address to the inaugural meeting of the American Society for Cybernetics (ASC), Margaret Mead, who had been a participant at the Macy Conferences, is a defining moment in its development. She characterised \"cybernetics as a way of looking at things and as a language for expressing what one sees\". This paper was social and ecological in focus, with Mead calling on cyberneticians to assume responsibility for how the social consequences of the language of cybernetics and the development of cybernetic systems.\n\nMead's paper concluded with a proposal directed at the ASC itself, that it organise itself in the light of the ideas with which it was concerned. That is, the practice of Cybernetics by the ASC should be subject to Cybernetic critique, an idea returned to specially by Ranulph Glanville in his time as president of the society.\n\nMead's paper was published in 1968 in a collection edited by Heinz von Foerster. With Mead uncontactable due to field work at the time, von Foerster titled the paper \"Cybernetics of Cybernetics\", a title that perhaps emphasised his concerns more than Mead's. Von Foerster promoted Second-order Cybernetics energetically, developing it as a means of renewal for Cybernetics generally and as what has been called an \"unfinished revolution\" in science. Von Foerster's developed Second-order Cybernetics as a critique of realism and objectivity and as a radically reflexive form of science, where observers enter their domains of observation, describing their own observing not the supposed causes.\n\nThe initial development of second-order cybernetics was consolidated by the mid 1970s in a series of significant developments and publications:\n\nHeinz von Foerster attributes the origin of second-order cybernetics to the attempts of cyberneticians to construct a model of the mind:\n\n... a brain is required to write a theory of a brain. From this follows that a theory of the brain, that has any aspirations for completeness, has to account for the writing of this theory. And even more fascinating, the writer of this theory has to account for her or himself. Translated into the domain of cybernetics; the cybernetician, by entering his own domain, has to account for his or her own activity. Cybernetics then becomes cybernetics of cybernetics, or \"second-order cybernetics\".\nMany cyberneticians draw a sharp distinction between first and second order cybernetics, others stress the continuity between the two, and the implicit second order qualities of earlier cybernetics.\nThe anthropologists Gregory Bateson and Margaret Mead contrasted first and second-order cybernetics with this diagram in an interview in 1973. Referring to the Macy conferences, it emphasises the requirement for a participant observer in the second order case:\n\nIn 1992, Pask summarized the differences between the old and the new cybernetics as a shift in emphasis:.\n\nSome biologists influenced by cybernetic concepts (Maturana and Varela, 1980; Varela, 1979; Atlan, 1979) realized that the cybernetic metaphors of the program upon which molecular biology had been based rendered a conception of the autonomy of the living being impossible. Consequently, these thinkers were led to invent a new cybernetics, one more suited to the organization mankind discovers in nature – organizations he has not himself invented. The possibility that this new cybernetics could also account for social forms of organization, remained an object of debate among theoreticians on self-organization in the 1980s.\n\nIn political science in the 1980s unlike its predecessor, the new cybernetics concerns itself with the interaction of autonomous political actors and subgroups and the practical reflexive consciousness of the subject who produces and reproduces the structure of political community. A dominant consideration is that of recursiveness, or self-reference of political action both with regard to the expression of political consciousness and with the ways in which systems build upon themselves.\n\nGeyer and van der Zouwen in 1978 discuss a number of characteristics of the merging \"new cybernetics\". One characteristic of new cybernetics is that it views information as constructed by an individual interacting with the environment. This provides a new epistemological foundation of science, by viewing it as observer-dependent. Another characteristic of the new cybernetics is its contribution towards bridging the \"micro-macro gap\". That is, it links the individual with the society. Geyer and van der Zouten also noted that a transition classical cybernetics to the new cybernetics involves a transition form classical problems to new problems. These shifts in the thinking involve, among others a change emphasis on the system being steered to the system doing the steering, and the factors which guide the steering decisions. And new emphasis on communication between several systems which are trying to steer each other.\n\nGeyer & J. van der Zouwen (1992) recognize four themes in both sociocybernetics and new cybernetics:\n\nOther topics where new cybernetics is developed are: \n\nOrganizational cybernetics is distinguished from management cybernetics. Both use many of the same terms but interpret them according to another philosophy of systems thinking. Organizational cybernetics by contrast offers a significant break with the assumption of the hard approach. The full flowering of organizational cybernetics is represented by Beer's viable system model.\n\nOrganizational cybernetics studies organizational design, and the regulation and self-regulation of organizations from a systems theory perspective that also takes the social dimension into consideration. Researchers in economics, public administration and political science focus on the changes in institutions, organisation and mechanisms of social steering at various levels (sub-national, national, European, international) and in different sectors (including the private, semi-private and public sectors; the latter sector is emphasised).\n\nThe reformulation of sociocybernetics as an \"actor-oriented, observer-dependent, self-steering, time-variant\" paradigm of human systems, was most clearly articulated by Geyer and van der Zouwen in 1978 and 1986. They stated that sociocybernetics is more than just social cybernetics, which could be defined as the application of the general systems approach to social science. Social cybernetics is indeed more than such a one-way knowledge transfer. It implies a feed-back loop from the area of application – the social sciences – to the theory being applied, namely cybernetics; consequently, sociocybernetics can indeed be viewed as part of the new cybernetics: as a result of its application to social science problems, cybernetics, itself, has been changed and has moved from its originally rather mechanistic point of departure to become more actor-oriented and observer-dependent.\nIn summary, the new sociocybernetics is much more subjective and the sociological approach than the classical cybernetics approach with its emphasis on control. The new approach has a distinct emphasis on steering decisions; furthermore, it can be seen as constituting a reconceptualization of many concepts which are often routinely accepted without challenge.\n\nAndrew Pickering has criticised second order cybernetics as a form of linguistic turn, moving away from the performative practices he finds valuable in earlier cybernetics. Pickering's comments seem to apply specifically to where second order cybernetics has emphasised epistemology and language, for instance in the work of von Foerster, as he approvingly references the work of figures such as Bateson and Pask and the idea of participant observers which fall within the scope of second order cybernetics more broadly considered.\n\n\n\n"}
{"id": "29618699", "url": "https://en.wikipedia.org/wiki?curid=29618699", "title": "Shearer's inequality", "text": "Shearer's inequality\n\nIn information theory, Shearer's inequality, named after James Shearer, states that if \"X\", ..., \"X\" are random variables and \"S\", ..., \"S\" are subsets of {1, 2, ..., \"d\"} such that every integer between 1 and \"d\" lies in at least \"r\" of these subsets, then\n\nwhere formula_2 is the Cartesian product of random variables formula_3 with indices \"j\" in formula_4 (so the dimension of this vector is equal to the size of formula_4).\n"}
{"id": "562695", "url": "https://en.wikipedia.org/wiki?curid=562695", "title": "Simulink", "text": "Simulink\n\nSimulink, developed by MathWorks, is a graphical programming environment for modeling, simulating and analyzing multidomain dynamical systems. Its primary interface is a graphical block diagramming tool and a customizable set of block libraries. It offers tight integration with the rest of the MATLAB environment and can either drive MATLAB or be scripted from it. Simulink is widely used in automatic control and digital signal processing for multidomain simulation and Model-Based Design.\n\nMathWorks and other third-party hardware and software products can be used with Simulink. For example, Stateflow extends Simulink with a design environment for developing state machines and flow charts.\n\nMathWorks claims that, coupled with another of their products, Simulink can automatically generate C source code for real-time implementation of systems. As the efficiency and flexibility of the code improves, this is becoming more widely adopted for production systems, in addition to being a tool for embedded system design work because of its flexibility and capacity for quick iteration. Embedded Coder creates code efficient enough for use in embedded systems.\n\nSimulink Real-Time (formerly known as xPC Target), together with x86-based real-time systems, is an environment for simulating and testing Simulink and Stateflow models in real-time on the physical system. Another MathWorks product also supports specific embedded targets. When used with other generic products, Simulink and Stateflow can automatically generate synthesizable VHDL and Verilog.\nSimulink Verification and Validation enables systematic verification and validation of models through modeling style checking, requirements traceability and model coverage analysis. Simulink Design Verifier uses formal methods to identify design errors like integer overflow, division by zero and dead logic, and generates test case scenarios for model checking within the Simulink environment.\n\nSimEvents is used to add a library of graphical building blocks for modeling queuing systems to the Simulink environment, and to add an event-based simulation engine to the time-based simulation engine in Simulink.\n\nTherefore in Simulink any type of simulation can be done and the model can be simulated at any point in this environment.\n\nDifferent type of blocks can be accessed using the Simulink library browser. And therefore the benefit could be taken out from this environment efficiently.\n\n"}
{"id": "7463064", "url": "https://en.wikipedia.org/wiki?curid=7463064", "title": "Soddy's hexlet", "text": "Soddy's hexlet\n\nIn geometry, Soddy's hexlet is a chain of six spheres (shown in grey in Figure 1), each of which is tangent to both of its neighbors and also to three mutually tangent given spheres. In Figure 1, these three spheres are shown as a central sphere (red), and two spheres (not shown) above and below the plane the centers of the hexlet spheres lie on. In addition, the hexlet spheres are tangent to a fourth sphere (blue in Figure 1), which is not tangent to the three others.\n\nAccording to a theorem published by Frederick Soddy in 1937, it is always possible to find a hexlet for any choice of mutually tangent spheres \"A\", \"B\" and \"C\". Indeed, there is an infinite family of hexlets related by rotation and scaling of the hexlet spheres (Figure 1); in this, Soddy's hexlet is the spherical analog of a Steiner chain of six circles. Consistent with Steiner chains, the centers of the hexlet spheres lie in a single plane, on an ellipse. Soddy's hexlet was also discovered independently in Japan, as shown by Sangaku tablets from 1822 in Kanagawa prefecture.\n\nSoddy's hexlet is a chain of six spheres, labeled \"S\"–\"S\", each of which is tangent to three given spheres, \"A\", \"B\" and \"C\", that are themselves mutually tangent at three distinct points. (For consistency throughout the article, the hexlet spheres will always be depicted in grey, spheres \"A\" and \"B\" in green, and sphere \"C\" in blue.) The hexlet spheres are also tangent to a fourth fixed sphere \"D\" (always shown in red) that is not tangent to the three others, \"A\", \"B\" and \"C\".\n\nEach sphere of Soddy's hexlet is also tangent to its neighbors in the chain; for example, sphere \"S\" is tangent to \"S\" and \"S\". The chain is closed, meaning that every sphere in the chain has two tangent neighbors; in particular, the initial and final spheres, \"S\" and \"S\", are tangent to one another.\n\nThe annular Soddy's hexlet is a special case (Figure 2), in which the three mutually tangent spheres consist of a single sphere of radius \"r\" (blue) sandwiched between two parallel planes (green) separated by a perpendicular distance 2\"r\". In this case, Soddy's hexlet consists of six spheres of radius \"r\" packed like ball bearings around the central sphere and likewise sandwiched. The hexlet spheres are also tangent to a fourth sphere (red), which is not tangent to the other three.\n\nThe chain of six spheres can be rotated about the central sphere without affecting their tangencies, showing that there is an infinite family of solutions for this case. As they are rotated, the spheres of the hexlet trace out a torus (a doughnut-shaped surface); in other words, a torus is the envelope of this family of hexlets.\n\nThe general problem of finding a hexlet for three given mutually tangent spheres \"A\", \"B\" and \"C\" can be reduced to the annular case using inversion. This geometrical operation always transforms spheres into spheres or into planes, which may be regarded as spheres of infinite radius. A sphere is transformed into a plane if and only if the sphere passes through the center of inversion. An advantage of inversion is that it preserves tangency; if two spheres are tangent before the transformation, they remain so after. Thus, if the inversion transformation is chosen judiciously, the problem can be reduced to a simpler case, such as the annular Soddy's hexlet. Inversion is reversible; repeating an inversion in the same point returns the transformed objects to their original size and position.\n\nInversion in the point of tangency between spheres \"A\" and \"B\" transforms them into parallel planes, which may be denoted as \"a\" and \"b\". Since sphere \"C\" is tangent to both \"A\" and \"B\" and does not pass through the center of inversion, \"C\" is transformed into another sphere \"c\" that is tangent to both planes; hence, \"c\" is sandwiched between the two planes \"a\" and \"b\". This is the annular Soddy's hexlet (Figure 2). Six spheres \"s\"–\"s\" may be packed around \"c\" and likewise sandwiched between the bounding planes \"a\" and \"b\". Re-inversion restores the three original spheres, and transforms \"s\"–\"s\" into a hexlet for the original problem. In general, these hexlet spheres \"S\"–\"S\" have different radii.\n\nAn infinite variety of hexlets may be generated by rotating the six balls \"s\"–\"s\" in their plane by an arbitrary angle before re-inverting them. The envelope produced by such rotations is the torus that surrounds the sphere \"c\" and is sandwiched between the two planes \"a\" and \"b\"; thus, the torus has an inner radius \"r\" and outer radius 3\"r\". After the re-inversion, this torus becomes a Dupin cyclide (Figure 3).\n\nThe envelope of Soddy's hexlets is a Dupin cyclide, an inversion of the torus. Thus Soddy's construction shows that a cyclide of Dupin is the envelope of a 1-parameter family of spheres in two different ways, and each sphere in either family is tangent to two spheres in same family and three spheres in the other family. This result was probably known to Charles Dupin, who discovered the cyclides that bear his name in his 1803 dissertation under Gaspard Monge.\n\nThe intersection of the hexlet with the plane of its spherical centers produces a Steiner chain of six circles.\n\nIt is assumed that spheres A and B are the same size.\n\nIn any elliptic hexlet, such as the one shown at the top of the article, there are two tangent planes to the hexlet. In order for an elliptic hexlet to exist, the radius of C must be less than one quarter that of A. If C's radius is one quarter of A's, each sphere will become a plane in the journey. The inverted image shows a normal elliptic hexlet, though, and in the parabolic hexlet, the point where a sphere turns into a plane is precisely when its inverted image passes through the centre of inversion. In such a hexlet there is only one tangent plane to the hexlet. The line of the centres of a parabolic hexlet is a parabola.\n\nIf C is even larger than that, a hyperbolic hexlet is formed, and now there are no tangent planes at all. Label the spheres \"S\" to \"S\". \"S\" thus cannot go very far until it becomes a plane (where its inverted image passes through the centre of inversion) and then reverses its concavity (where its inverted image surrounds the centre of inversion). Now the line of the centres is a hyperbola.\n\nThe limiting case is when A, B and C are all the same size. The hexlet now becomes straight. \"S\" is small as it passes through the hole between A, B and C, and grows till it becomes a plane tangent to them. The centre of inversion is now also with a point of tangency with the image of \"S\", so it is also a plane tangent to A, B and C. As \"S\" proceeds, its concavity is reversed and now it surrounds all the other spheres, tangent to A, B, C, \"S\" and \"S\". \"S\" pushes upwards and grows to become a tangent plane and \"S\" shrinks. \"S\" then obtains \"S\"'s former position as a tangent plane. It then reverses concavity again and passes through the hole again, beginning another round trip. Now the line of centres is a degenerate hyperbola, where it has collapsed into two straight lines.\n\nThe Japanese mathematicians analysed the packing problems in which circles and polygons, balls and polyhedrons come into contact and often found the relevant theorems independently before their discovery by Western mathematicians. The Sangaku about hexlet was made by Irisawa Shintarō Hiroatsu in the school of Uchida Itsumi and dedicated to Samukawa Shrine in May 1822. The original sangaku has been lost and recorded in the Uchida's book of \"Kokinsankagami\" on 1832. The replica of the sangaku was made from the record and dedicated to Hōtoku museum in Samukawa Shrine on August, 2009.\n\nThe sangaku by Irisawa consists of three problems and the third problem relates to Soddy's hexlet: \"the diameter of the outer circumscribing sphere is 30 sun. The diameters of the nucleus balls are 10 sun and 6 sun each. The diameter of one of the balls in the chain of balls is 5 sun. Then I asked for the diameters of the remaining balls. The answer is 15 sun, 10 sun, 3.75 sun, 2.5 sun and 2 + 8/11 sun.\"\n\nBy his answer, the method to calculate the diameters of the balls is written down and can consider it the following formulas to be given in the modern scale. If the ratio of the diameter of the outside ball to the nucleus balls are \"a\", \"a\", and if the ratio of the diameter to the chain balls are \"c\", ..., \"c\". I want to represent c\", ..., \"c\" by \"a\", \"a\", \"c\". If\nthen,\nThen \"c\" + \"c\" = \"c\" + \"c\" = \"c\" + \"c\". If \"r\", ..., \"r\" are the diameters of six balls, then we get the formula:\n\n\n\n"}
{"id": "924193", "url": "https://en.wikipedia.org/wiki?curid=924193", "title": "Volterra's function", "text": "Volterra's function\n\nIn mathematics, Volterra's function, named for Vito Volterra, is a real-valued function \"V\" defined on the real line R with the following curious combination of properties:\n\n\nThe function is defined by making use of the Smith–Volterra–Cantor set and \"copies\" of the function defined by formula_1 for formula_2 and formula_3. The construction of \"V\" begins by determining the largest value of \"x\" in the interval [0, 1/8] for which \"f\" ′(\"x\") = 0. Once this value (say \"x\") is determined, extend the function to the right with a constant value of \"f\"(\"x\") up to and including the point 1/8. Once this is done, a mirror image of the function can be created starting at the point 1/4 and extending downward towards 0. This function will be defined to be 0 outside of the interval [0, 1/4]. We then translate this function to the interval [3/8, 5/8] so that the resulting function, which we call \"f\", is nonzero only on the middle interval of the complement of the Smith–Volterra–Cantor set. To construct \"f\", \"f\" ′ is then considered on the smaller interval [0,1/32], truncated at the last place the derivative is zero, extended, and mirrored the same way as before, and two translated copies of the resulting function are added to \"f\" to produce the function \"f\". Volterra's function then results by repeating this procedure for every interval removed in the construction of the Smith–Volterra–Cantor set; in other words, the function \"V\" is the limit of the sequence of functions \"f\", \"f\", ...\n\nVolterra's function is differentiable everywhere just as \"f\" (as defined above) is. One can show that \"f\" ′(\"x\") = 2\"x\" sin(1/\"x\") - cos(1/\"x\") for \"x\" ≠ 0, which means that in any neighborhood of zero, there are points where \"f\" ′ takes values 1 and −1. Thus there are points where \"V\" ′ takes values 1 and −1 in every neighborhood of each of the endpoints of intervals removed in the construction of the Smith–Volterra–Cantor set \"S\". In fact, \"V\" ′ is discontinuous at every point of \"S\", even though \"V\" itself is differentiable at every point of \"S\", with derivative 0. However, \"V\" ′ is continuous on each interval removed in the construction of \"S\", so the set of discontinuities of \"V\" ′ is equal to \"S\".\n\nSince the Smith–Volterra–Cantor set \"S\" has positive Lebesgue measure, this means that \"V\" ′ is discontinuous on a set of positive measure. By Lebesgue's criterion for Riemann integrability, \"V\" ′ is not integrable. If one were to repeat the construction of Volterra's function with the ordinary measure-0 Cantor set \"C\" in place of the \"fat\" (positive-measure) Cantor set \"S\", one would obtain a function with many similar properties, but the derivative would then be discontinuous on the measure-0 set \"C\" instead of the positive-measure set \"S\", and so the resulting function would have an integrable derivative.\n\n\n"}
{"id": "12850812", "url": "https://en.wikipedia.org/wiki?curid=12850812", "title": "Wijsman convergence", "text": "Wijsman convergence\n\nWijsman convergence is a variation of Hausdorff convergence suitable for work with unbounded sets.\nIntuitively, Wijsman convergence is to convergence in the Hausdorff metric as pointwise convergence is to uniform convergence.\n\nThe convergence was defined by Robert Wijsman.\nThe same definition was used earlier by Zdeněk Frolík.\nYet earlier, Hausdorff in his book \"Grundzüge der Mengenlehre\" defined so called \"closed limits\";\nfor proper metric spaces it is the same as Wijsman convergence.\n\nLet (\"X\", \"d\") be a metric space and let Cl(\"X\") denote the collection of all \"d\"-closed subsets of \"X\". For a point \"x\" ∈ \"X\" and a set \"A\" ∈ Cl(\"X\"), set\n\nA sequence (or net) of sets \"A\" ∈ Cl(\"X\") is said to be Wijsman convergent to \"A\" ∈ Cl(\"X\") if, for each \"x\" ∈ \"X\",\n\nWijsman convergence induces a topology on Cl(\"X\"), known as the Wijsman topology.\n\n\n\n"}
{"id": "17961926", "url": "https://en.wikipedia.org/wiki?curid=17961926", "title": "William Dunham (mathematician)", "text": "William Dunham (mathematician)\n\nWilliam Wade Dunham (born 1947) is an American writer who was originally trained in topology but became interested in the history of mathematics and specializes in Leonhard Euler. He has received several awards for writing and teaching on this subject.\n\nDunham received his B.S. from the University of Pittsburgh in 1969, his M.S. from Ohio State in 1970, and his Ph.D. from the same institution in 1974.\n\nDunham won the American Association of Publishers’ award for writing the Best Mathematics Book of 1994 for his book \" The Mathematical Universe\". In his book \"Euler: The Master of Us All\", he examines Leonhard Euler's impressive mathematical work. He received a Lester R. Ford Award in 2006 for his expository article \"Touring the Calculus\".\n\nIn 2007, Dunham gave a lecture about Euler's product-sum formula and its relationship to analytic number theory, as well as discussed Euler's evaluation of a non-trivial integral at the celebration of \"Year of Euler\" by the Euler Society. He published a chapter \"Euler and the Fundamental Theorem of Algebra\" in the book \"The Genius of Euler\" published in 2007 to commemorate the 300th birthday of Euler.\n\n\n"}
{"id": "152214", "url": "https://en.wikipedia.org/wiki?curid=152214", "title": "Zermelo–Fraenkel set theory", "text": "Zermelo–Fraenkel set theory\n\nIn set theory, Zermelo–Fraenkel set theory, named after mathematicians Ernst Zermelo and Abraham Fraenkel, is an axiomatic system that was proposed in the early twentieth century in order to formulate a theory of sets free of paradoxes such as Russell's paradox. Today, Zermelo–Fraenkel set theory with the historically controversial axiom of choice (AC) included is the standard form of axiomatic set theory and as such is the most common foundation of mathematics. Zermelo–Fraenkel set theory with the axiom of choice included is abbreviated ZFC, where C stands for \"choice\", and ZF refers to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. \n\nZermelo–Fraenkel set theory is intended to formalize a single primitive notion, that of a hereditary well-founded set, so that all entities in the universe of discourse are such sets. Thus the axioms of Zermelo–Fraenkel set theory refer only to pure sets and prevent its models from containing urelements (elements of sets that are not themselves sets). Furthermore, proper classes (collections of mathematical objects defined by a property shared by their members which are too big to be sets) can only be treated indirectly. Specifically, Zermelo–Fraenkel set theory does not allow for the existence of a universal set (a set containing all sets) nor for unrestricted comprehension, thereby avoiding Russell's paradox. Von Neumann–Bernays–Gödel set theory (NBG) is a commonly used conservative extension of Zermelo–Fraenkel set theory that does allow explicit treatment of proper classes.\n\nThere are many equivalent formulations of the axioms of Zermelo–Fraenkel set theory. Most of the axioms state the existence of particular sets defined from other sets. For example, the axiom of pairing says that given any two sets \"a\" and \"b\" there is a new set {\"a\", \"b\"} containing exactly \"a\" and \"b\". Other axioms describe properties of set membership. A goal of the axioms is that each axiom should be true if interpreted as a statement about the collection of all sets in the von Neumann universe (also known as the cumulative hierarchy).\n\nThe metamathematics of Zermelo–Fraenkel set theory has been extensively studied. Landmark results in this area established the logical independence of the axiom of choice from the remaining ZFC axioms (see Axiom of choice#Independence) and of the continuum hypothesis from ZFC.The consistency of a theory such as ZFC cannot be proved within the theory itself.\n\nFormally, ZFC is a one-sorted theory in first-order logic. The signature has equality and a single primitive binary relation, set membership, which is usually denoted ∈. The formula \"a\" ∈ \"b\" means that the set \"a\" is a member of the set \"b\" (which is also read, \"\"a\" is an element of \"b\" or \"a\" is in \"b\"\").\n\nThe modern study of set theory was initiated by Georg Cantor and Richard Dedekind in the 1870s. However, the discovery of paradoxes in naive set theory, such as Russell's paradox, led to the desire for a more rigorous form of set theory that was free of these paradoxes.\n\nIn 1908, Ernst Zermelo proposed the first axiomatic set theory, Zermelo set theory. However, as first pointed out by Abraham Fraenkel in a 1921 letter to Zermelo, this theory was incapable of proving the existence of certain sets and cardinal numbers whose existence was taken for granted by most set theorists of the time, notably the cardinal number ℵ and the set {Z, ℘(Z), ℘(℘(Z))...}, where Z is any infinite set and ℘ is the power set operation. Moreover, one of Zermelo's axioms invoked a concept, that of a \"definite\" property, whose operational meaning was not clear. In 1922, Fraenkel and Thoralf Skolem independently proposed operationalizing a \"definite\" property as one that could be formulated as a first order theory whose atomic formulas were limited to set membership and identity. They also independently proposed replacing the axiom schema of specification with the axiom schema of replacement. Appending this schema, as well as the axiom of regularity (first proposed by Dimitry Mirimanoff in 1917), to Zermelo set theory yields the theory denoted by ZF. Adding to ZF either the axiom of choice (AC) or a statement that is equivalent to it yields ZFC.\n\nThere are many equivalent formulations of the ZFC axioms; for a discussion of this see . The following particular axiom set is from . The axioms per se are expressed in the symbolism of first order logic. The associated English prose is only intended to aid the intuition.\nAll formulations of ZFC imply that at least one set exists. Kunen includes an axiom that directly asserts the existence of a set, in addition to the axioms given below (although he notes that he does so only “for emphasis”). Its omission here can be justified in two ways. First, in the standard semantics of first-order logic in which ZFC is typically formalized, the domain of discourse must be nonempty. Hence, it is a logical theorem of first-order logic that something exists — usually expressed as the assertion that something is identical to itself, ∃x(x=x). Consequently, it is a theorem of every first-order theory that something exists. However, as noted above, because in the intended semantics of ZFC there are only sets, the interpretation of this logical theorem in the context of ZFC is that some \"set\" exists. Hence, there is no need for a separate axiom asserting that a set exists. Second, however, even if ZFC is formulated in so-called free logic, in which it is not provable from logic alone that something exists, the axiom of infinity (below) asserts that an \"infinite\" set exists. This implies that \"a\" set exists and so, once again, it is superfluous to include an axiom asserting as much.\n\nTwo sets are equal (are the same set) if they have the same elements.\n\nThe converse of this axiom follows from the substitution property of equality. If the background logic does not include equality \"=\", \"x\"=\"y\" may be defined as an abbreviation for the following formula:\n\nIn this case, the axiom of extensionality can be reformulated as\n\nwhich says that if \"x\" and \"y\" have the same elements, then they belong to the same sets.\n\nEvery non-empty set \"x\" contains a member \"y\" such that \"x\" and \"y\" are disjoint sets.\n\nor in modern notation:\nformula_5 \n\nThis implies, for example, that no set is an element of itself and that every set has an ordinal rank.\n\nSubsets are commonly constructed using set builder notation. For example, the even integers can be constructed as the subset of the integers formula_6 satisfying the congruence modulo predicate formula_7:\n\nIn general, the subset of a set \"z\" obeying a formula formula_9(\"x\") with one free variable \"x\" may be written as:\n\nThe axiom schema of specification states that this subset always exists (it is an axiom \"schema\" because there is one axiom for each formula_9). Formally, let formula_9 be any formula in the language of ZFC with all free variables among formula_13 (\"y\" is \"not\" free in formula_9). Then:\n\nNote that the axiom schema of specification can only construct subsets, and does not allow the construction of sets of the more general form:\n\nThis restriction is necessary to avoid Russell's paradox and its variants that accompany naive set theory with unrestricted comprehension.\n\nIn some other axiomatizations of ZF, this axiom is redundant in that it follows from the axiom schema of replacement and the axiom of the empty set.\n\nOn the other hand, the axiom of specification can be used to prove the existence of the empty set, denoted formula_17, once at least one set is known to exist (see above). One way to do this is to use a property formula_9 which no set has. For example, if \"w\" is any existing set, the empty set can be constructed as\n\nThus the axiom of the empty set is implied by the nine axioms presented here. The axiom of extensionality implies the empty set is unique (does not depend on \"w\"). It is common to make a definitional extension that adds the symbol formula_17 to the language of ZFC.\n\nIf \"x\" and \"y\" are sets, then there exists a set which contains \"x\" and \"y\" as elements.\n\nThe axiom schema of specification must be used to reduce this to a set with exactly these two elements. The axiom of pairing is part of Z, but is redundant in ZF because it follows from the axiom schema of replacement, if we are given a set with at least two elements. The existence of a set with at least two elements is assured by either the axiom of infinity, or by the axiom schema of specification and the axiom of the power set applied twice to any set.\n\nThe union over the elements of a set exists. For example, the union over the elements of the set formula_22 is formula_23.\n\nThe axiom of union states that for any set of sets formula_24 there is a set formula_25 containing every element that is a member of some member of formula_24:\nAlthough this formula doesn't directly assert the existence of formula_28, the set formula_28 can be constructed from formula_25 in the above using the axiom schema of specification:\n\nThe axiom schema of replacement asserts that the image of a set under any definable function will also fall inside a set.\n\nFormally, let formula_9 be any formula in the language of ZFC whose free variables are among formula_33, so that in particular formula_34 is not free in formula_9. Then:\n\nIn other words, if the relation formula_9 represents a definable function formula_38, formula_25 represents its domain, and formula_40 is a set for every formula_41, then the range of formula_38 is a subset of some set formula_34. The form stated here, in which formula_34 may be larger than strictly necessary, is sometimes called the axiom schema of collection.\n\nLet formula_45 abbreviate formula_46, where formula_47 is some set. (We can see that formula_48 is a valid set by applying the Axiom of Pairing with formula_49 so that the set formula_50 is formula_51). Then there exists a set \"X\" such that the empty set formula_17 is a member of \"X\" and, whenever a set \"y\" is a member of \"X\", then formula_53 is also a member of \"X\".\n\nMore colloquially, there exists a set \"X\" having infinitely many members. (It must be established, however, that these members are all different, because if two elements are the same, the sequence will loop around in a finite cycle of sets. The axiom of regularity prevents this from happening.) The minimal set \"X\" satisfying the axiom of infinity is the von Neumann ordinal ω, which can also be thought of as the set of natural numbers formula_55.\n\nBy definition a set \"z\" is a subset of a set \"x\" if and only if every element of \"z\" is also an element of \"x\":\n\nThe Axiom of Power Set states that for any set \"x\", there is a set \"y\" that contains every subset of \"x\":\n\nThe axiom schema of specification is then used to define the power set \"P(x)\" as the subset of such a \"y\" containing the subsets of \"x\" exactly:\n\nAxioms 1–8 define ZF. Alternative forms of these axioms are often encountered, some of which are listed in . Some ZF axiomatizations include an axiom asserting that the empty set exists. The axioms of pairing, union, replacement, and power set are often stated so that the members of the set \"x\" whose existence is being asserted are just those sets which the axiom asserts \"x\" must contain.\n\nThe following axiom is added to turn ZF into ZFC:\n\nFor any set \"X\", there is a binary relation \"R\" which well-orders \"X\". This means \"R\" is a linear order on \"X\" such that every nonempty subset of \"X\" has a member which is minimal under \"R\".\n\nGiven axioms 1–8, there are many statements equivalent to axiom 9, the best known of which is the axiom of choice (AC), which goes as follows. Let \"X\" be a set whose members are all non-empty. Then there exists a function \"f\" from \"X\" to the union of the members of \"X\", called a \"choice function\", such that for all one has . Since the existence of a choice function when \"X\" is a finite set is easily proved from axioms 1–8, AC only matters for certain infinite sets. AC is characterized as nonconstructive because it asserts the existence of a choice set but says nothing about how the choice set is to be \"constructed.\" Much research has sought to characterize the definability (or lack thereof) of certain sets whose existence AC asserts.\n\nOne motivation for the ZFC axioms is the cumulative hierarchy of sets introduced by John von Neumann. In this viewpoint, the universe of set theory is built up in stages, with one stage for each ordinal number. At stage 0 there are no sets yet. At each following stage, a set is added to the universe if all of its elements have been added at previous stages. Thus the empty set is added at stage 1, and the set containing the empty set is added at stage 2. The collection of all sets that are obtained in this way, over all the stages, is known as V. The sets in V can be arranged into a hierarchy by assigning to each set the first stage at which that set was added to V.\n\nIt is provable that a set is in V if and only if the set is pure and well-founded; and provable that V satisfies all the axioms of ZFC, if the class of ordinals has appropriate reflection properties. For example, suppose that a set \"x\" is added at stage α, which means that every element of \"x\" was added at a stage earlier than α. Then every subset of \"x\" is also added at stage α, because all elements of any subset of \"x\" were also added before stage α. This means that any subset of \"x\" which the axiom of separation can construct is added at stage α, and that the powerset of \"x\" will be added at the next stage after α. For a complete argument that V satisfies ZFC see .\n\nThe picture of the universe of sets stratified into the cumulative hierarchy is characteristic of ZFC and related axiomatic set theories such as Von Neumann–Bernays–Gödel set theory (often called NBG) and Morse–Kelley set theory. The cumulative hierarchy is not compatible with other set theories such as New Foundations.\n\nIt is possible to change the definition of \"V\" so that at each stage, instead of adding all the subsets of the union of the previous stages, subsets are only added if they are definable in a certain sense. This results in a more \"narrow\" hierarchy which gives the constructible universe \"L\", which also satisfies all the axioms of ZFC, including the axiom of choice. It is independent from the ZFC axioms whether \"V\" = \"L\". Although the structure of \"L\" is more regular and well behaved than that of \"V\", few mathematicians argue that \"V\" = \"L\" should be added to ZFC as an additional \"axiom of constructibility\".\n\nAs noted earlier, proper classes (collections of mathematical objects defined by a property shared by their members which are too big to be sets) can only be treated indirectly in ZF (and thus ZFC).\nAn alternative to proper classes while staying within ZF and ZFC is the \"virtual class\" notational construct introduced by , where the entire construct y ∈ { x | Fx } is simply defined as Fy. This provides a simple notation for classes that can contain sets but need not themselves be sets, while not committing to the ontology of classes (because the notation can be syntactically converted to one that only uses sets). Quine's approach built on the earlier approach of . Virtual classes are also used in , , and in the Metamath implementation of ZFC.\n\nThe axiom schemata of replacement and separation each contain infinitely many instances. included a result first proved in his 1957 Ph.D. thesis: if ZFC is consistent, it is impossible to axiomatize ZFC using only finitely many axioms. On the other hand, Von Neumann–Bernays–Gödel set theory (NBG) can be finitely axiomatized. The ontology of NBG includes proper classes as well as sets; a set is any class that can be a member of another class. NBG and ZFC are equivalent set theories in the sense that any theorem not mentioning classes and provable in one theory can be proved in the other.\n\nGödel's second incompleteness theorem says that a recursively axiomatizable system that can interpret Robinson arithmetic can prove its own consistency only if it is inconsistent. Moreover, Robinson arithmetic can be interpreted in general set theory, a small fragment of ZFC. Hence the consistency of ZFC cannot be proved within ZFC itself (unless it is actually inconsistent). Thus, to the extent that ZFC is identified with ordinary mathematics, the consistency of ZFC cannot be demonstrated in ordinary mathematics. The consistency of ZFC does follow from the existence of a weakly inaccessible cardinal, which is unprovable in ZFC if ZFC is consistent. Nevertheless, it is deemed unlikely that ZFC harbors an unsuspected contradiction; it is widely believed that if ZFC were inconsistent, that fact would have been uncovered by now. This much is certain — ZFC is immune to the classic paradoxes of naive set theory: Russell's paradox, the Burali-Forti paradox, and Cantor's paradox.\n\nIf consistent, ZFC cannot prove the existence of the inaccessible cardinals that category theory requires. Huge sets of this nature are possible if ZF is augmented with Tarski's axiom. Assuming that axiom turns the axioms of infinity, power set, and choice (7 − 9 above) into theorems.\n\nMany important statements are independent of ZFC (see list of statements undecidable in ZFC). The independence is usually proved by forcing, whereby it is shown that every countable transitive model of ZFC (sometimes augmented with large cardinal axioms) can be expanded to satisfy the statement in question. A different expansion is then shown to satisfy the negation of the statement. An independence proof by forcing automatically proves independence from arithmetical statements, other concrete statements, and large cardinal axioms. Some statements independent of ZFC can be proven to hold in particular inner models, such as in the constructible universe. However, some statements that are true about constructible sets are not consistent with hypothesized large cardinal axioms.\n\nForcing proves that the following statements are independent of ZFC:\n\n\nRemarks:\n\n\nA variation on the method of forcing can also be used to demonstrate the consistency and unprovability of the axiom of choice, i.e., that the axiom of choice is independent of ZF. The consistency of choice can be (relatively) easily verified by proving that the inner model L satisfies choice. (Thus every model of ZF contains a submodel of ZFC, so that Con(ZF) implies Con(ZFC).) Since forcing preserves choice, we cannot directly produce a model contradicting choice from a model satisfying choice. However, we can use forcing to create a model which contains a suitable submodel, namely one satisfying ZF but not C.\n\nAnother method of proving independence results, one owing nothing to forcing, is based on Gödel's second incompleteness theorem. This approach employs the statement whose independence is being examined, to prove the existence of a set model of ZFC, in which case Con(ZFC) is true. Since ZFC satisfies the conditions of Gödel's second theorem, the consistency of ZFC is unprovable in ZFC (provided that ZFC is, in fact, consistent). Hence no statement allowing such a proof can be proved in ZFC. This method can prove that the existence of large cardinals is not provable in ZFC, but cannot prove that assuming such cardinals, given ZFC, is free of contradiction.\n\nThe project to unify set theorists behind additional axioms to resolve the Continuum Hypothesis or other meta-mathematical ambiguities is sometimes known as \"Gödel's program\". Mathematicians currently debate which axioms are the most plausible or \"self-evident\", which axioms are the most useful in various domains, and about to what degree usefulness should be traded off with plausibility; some \"multiverse\" set theorists argue that usefulness should be the sole ultimate criterion in which axioms to customarily adopt. One school of thought leans on expanding the \"iterative\" concept of a set to produce a set-theoretic universe with an interesting and complex but reasonably tractable structure by adopting forcing axioms; another school advocates for a tidier, less cluttered universe, perhaps focused on a \"core\" inner model.\n\nZFC has been criticized both for being excessively strong and for being excessively weak, as well as for its failure to capture objects such as proper classes and the universal set.\n\nMany mathematical theorems can be proven in much weaker systems than ZFC, such as Peano arithmetic and second-order arithmetic (as explored by the program of reverse mathematics). Saunders Mac Lane and Solomon Feferman have both made this point. Some of \"mainstream mathematics\" (mathematics not directly connected with axiomatic set theory) is beyond Peano arithmetic and second-order arithmetic, but still, all such mathematics can be carried out in ZC (Zermelo set theory with choice), another theory weaker than ZFC. Much of the power of ZFC, including the axiom of regularity and the axiom schema of replacement, is included primarily to facilitate the study of the set theory itself.\n\nOn the other hand, among axiomatic set theories, ZFC is comparatively weak. Unlike New Foundations, ZFC does not admit the existence of a universal set. Hence the universe of sets under ZFC is not closed under the elementary operations of the algebra of sets. Unlike von Neumann–Bernays–Gödel set theory (NBG) and Morse–Kelley set theory (MK), ZFC does not admit the existence of proper classes. A further comparative weakness of ZFC is that the axiom of choice included in ZFC is weaker than the axiom of global choice included in NBG and MK.\n\nThere are numerous mathematical statements undecidable in ZFC. These include the continuum hypothesis, the Whitehead problem, and the normal Moore space conjecture. Some of these conjectures are provable with the addition of axioms such as Martin's axiom, large cardinal axioms to ZFC. Some others are decided in ZF+AD where AD is the axiom of determinacy, a strong supposition incompatible with choice. One attraction of large cardinal axioms is that they enable many results from ZF+AD to be established in ZFC adjoined by some large cardinal axiom (see projective determinacy). The Mizar system and Metamath have adopted Tarski–Grothendieck set theory, an extension of ZFC, so that proofs involving Grothendieck universes (encountered in category theory and algebraic geometry) can be formalized.\n\n\nRelated axiomatic set theories:\n\n\n\n"}
