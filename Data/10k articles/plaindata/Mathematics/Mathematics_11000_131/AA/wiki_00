{"id": "19529000", "url": "https://en.wikipedia.org/wiki?curid=19529000", "title": "Algebra tile", "text": "Algebra tile\n\nAlgebra tiles are mathematical manipulatives that allow students to better understand ways of algebraic thinking and the concepts of algebra. These tiles have proven to provide concrete models for elementary school, middle school, high school, and college-level introductory algebra students. They have also been used to prepare prison inmates for their General Educational Development (GED) tests. Algebra tiles allow both an algebraic and geometric approach to algebraic concepts. They give students another way to solve algebraic problems other than just abstract manipulation. The National Council of Teachers of Mathematics (NCTM) recommends a decreased emphasis on the memorization of the rules of algebra and the symbol manipulation of algebra in their \"Curriculum and Evaluation Standards for Mathematics\". According to the NCTM 1989 standards \"[r]elating models to one another builds a better understanding of each\".\n\nAlgebra tiles are made up of small squares, rectangles, and large squares. The small square, the unit tile, represents the number one; the rectangle represents the variable formula_1; and the large square represents formula_2. The side of the formula_2 tile is equal to the length of the formula_1 tile. The width of the formula_1 tile is the same as the side of the unit tile. Additionally, the length of the formula_1 tile is often not an integer multiple of the side of the unit tile.\n\nThe tiles consist of two colors: one to show positive values and another to show negative values. A \"zero pair\" is a negative and a positive unit tile (or a negative and a positive formula_1 tile, or a negative and a positive formula_2 tile) which together form a sum of zero.\n\nAdding Integers is the best place to start when you want to get used to the idea of representing numbers with a quantity of tiles. Any integer can be represented by using the same number of tiles in the correct color. For example for a 6 you could select six yellow tiles. For -3 you would select 3 red tiles. The tiles are usually double sided with yellow on one side and red on the other. This allows the student to grasp the powerful concept of \"taking the opposite\" A negative simply means the opposite. So one yellow tile is positive one and the opposite (flip it over) is negative one. This is idea comes in handy when dealing with a - (-2) Start with two negative ones (red side) and the extra negative means take the opposite or flip them over. - (-2) = 2.\n\nWhen adding tiles think of combining the quantities together. if you are adding 2 + 3 you combine 2 yellow tiles with 3 yellow tiles and combined you have 5 yellow tiles. The same idea works for combining negative numbers. If you are adding -3 + -1 you combine 3 red tiles with 1 red tile to get 4 red tiles. -3 + -1 = -4\n\nWhen you add positive numbers to negative numbers using algebra tiles you need to bring in the idea of \"elimination\" or \"zero pairs\" every time you add a positive one to a negative one they eliminate each other resulting in a zero. This is true for any number of tiles. As long and you have the same quantity and opposite sign they will eliminate each other (or create a zero pair). For example if you add -5 + 7 you will combine five red tiles with seven yellow tiles. You can match the red and yellow tiles up one at a time to eliminate 5 of the yellow tiles and you will be left with 2 yellow tiles and no red tiles. -5 + 7 = 2.\n\nIf you start with more yellow tiles than red, your answer will be positive. If you start with more red tiles than yellow, your answer will be negative.\n\nOne more example: -5 + 2. You are combining 5 red tiles with 2 yellow tiles. The 2 yellow tiles will eliminate (or form a zero pair) with 2 of the red tiles leaving 3 red tiles behind. -5 + 2 = -3\n\nAlgebra tiles can also be used for subtracting integers. A person can take a problem such as formula_9 and begin with a group of six unit tiles and then take three away to leave you with three left over, so then formula_10. Algebra tiles can also be used to solve problems like formula_11.get if you had the problem formula_12. Being able to relate these two problems and why they get the same answer is important because it shows that formula_13. Another way in which algebra tiles can be used for integer subtraction can be seen through looking at problems where you subtract a positive integer from a smaller positive integer, like formula_14. Here you would begin with five positive unit tiles and then you would add zero pairs to the five positive unit tiles until there were eight positive unit tiles in front of you. Adding the zero pairs will not change the value of the original five positive unit tiles you originally had. You would then remove the eight positive unit tiles and count the number of negative unit tiles left. This number of negative unit tiles would then be your answer, which would be -3.\n\nMultiplication of integers with algebra tiles is performed through forming a rectangle with the tiles. The length and width of your rectangle would be your two factors and then the total number of tiles in the rectangle would be the answer to your multiplication problem. For instance in order to determine 3×4 you would take three positive unit tiles to represent three rows in the rectangle and then there would be four positive unit tiles to represent the columns in the rectangle. This would lead to having a rectangle with four columns of three positive unit tiles, which represents 3×4. Now you can count the number of unit tiles in the rectangle, which will equal 12.\n\nModeling algebraic expressions with algebra tiles is very similar to modeling addition and subtraction of integers using algebra tiles. In an expression such as formula_15 you would group five positive x tiles together and then three negative unit tiles together to represent this algebraic expression. Along with modeling these expressions, algebra tiles can also be used to simplify algebraic expressions. For instance, if you have formula_16 you can combine the positive and negative x tiles and unit tiles to form zero pairs to leave you with the expression formula_17. Since the tiles are laid out right in front of you it is easy to combine the like terms, or the terms that represent the same type of tile.\n\nThe distributive property is modeled through the algebra tiles by demonstrating that a(b+c)=(a×b)+(a×c). You would want to model what is being represented on both sides of the equation separately and determine that they are both equal to each other. If we want to show that formula_18 then we would make three sets of one unit tile and one x tile and then combine them together to see if would have formula_19, which we would.\n\nThe linear equation formula_20 can be modeled with one positive formula_1 tile and eight negative unit tiles on the left side of a piece of paper and six positive unit tiles on the right side. To maintain equality of the sides, each action must be performed on both sides. For example, eight positive unit tiles can be added to both sides. Zero pairs of unit tiles are removed from the left side, leaving one positive formula_1 tile. The right side has 14 positive unit tiles, so formula_23.\n\nThe equation formula_24 can be modeled with one positive formula_1 tile and seven positive unit tiles on the left side and 10 positive unit tiles on the right side. Rather than adding the same number of tiles to both sides, the same number of tiles can be subtracted from both sides. For example, seven positive unit tiles can be removed from both sides. This leaves one positive formula_1 tile on the left side and three positive unit tiles on the right side, so formula_27.\n\nLinear systems of equations may be solved algebraically by isolating one of the variables and then performing a substitution. Isolating a variable can be modeled with algebra tiles in a manner similar to solving linear equations (above), and substitution can be modeled with algebra tiles by replacing tiles with other tiles.\n\nWhen using algebra tiles to multiply a monomial by a monomial you first set up a rectangle where the length of the rectangle is the one monomial and then the width of the rectangle is the other monomial, similar to when you multiply integers using algebra tiles. Once the sides of the rectangle are represented by the algebra tiles you would then try to figure out which algebra tiles would fill in the rectangle. For instance, if you had x×x the only algebra tile that would complete the rectangle would be x, which is the answer.\n\nMultiplication of binomials is similar to multiplication of monomials when using the algebra tiles . Multiplication of binomials can also be thought of as creating a rectangle where the factors are the length and width. As with the monomials, you set up the sides of the rectangle to be the factors and then you fill in the rectangle with the algebra tiles. This method of using algebra tiles to multiply polynomials is known as the area model and it can also be applied to multiplying monomials and binomials with each other. An example of multiplying binomials is (2x+1)×(x+2) and the first step you would take is set up two positive x tiles and one positive unit tile to represent the length of a rectangle and then you would take one positive x tile and two positive unit tiles to represent the width. These two lines of tiles would create a space that looks like a rectangle which can be filled in with certain tiles. In the case of this example the rectangle would be composed of two positive x tiles, five positive x tiles, and two positive unit tiles. So the solution is 2x+5x+2.\n\nIn order to factor using algebra tiles you start out with a set of tiles that you combine into a rectangle, this may require the use of adding zero pairs in order to make the rectangular shape. An example would be where you are given one positive x tile, three positive x tiles, and two positive unit tiles. You form the rectangle by having the x tile in the upper right corner, then you have two x tiles on the right side of the x tile, one x tile underneath the x tile, and two unit tiles are in the bottom right corner. By placing the algebra tiles to the sides of this rectangle we can determine that we need one positive x tile and one positive unit tile for the length and then one positive x tile and two positive unit tiles for the width. This means that the two factors are formula_28 and formula_29. In a sense this is the reverse of the procedure for multiplying polynomials.\n\nThe process of completing the square can be accomplished using algebra tiles by placing your x tiles and x tiles into a square. You will not be able to completely create the square because there will be a smaller square missing from your larger square that you made from the tiles you were given, which will be filled in by the unit tiles. In order to complete the square you would determine how many unit tiles would be needed to fill in the missing square. In order to complete the square of x+6x you start off with one positive x tile and six positive x tiles. You place the x tile in the upper left corner and then you place three positive x tiles to the right of the x tile and three positive unit x tiles under the x tile. In order to fill in the square we need nine positive unit tiles. we have now created x+6x+9, which can be factored into formula_30.\n\n\n"}
{"id": "1337579", "url": "https://en.wikipedia.org/wiki?curid=1337579", "title": "Areas of mathematics", "text": "Areas of mathematics\n\nMathematics encompasses a growing variety and depth of subjects over history, and comprehension requires a system to categorize and organize the many subjects into more general areas of mathematics. A number of different classification schemes have arisen, and though they share some similarities, there are differences due in part to the different purposes they serve. In addition, as mathematics continues to be developed, these classification schemes must change as well to account for newly created areas or newly discovered links between different areas. Classification is made more difficult by some subjects, often the most active, which straddle the boundary between different areas.\n\nA traditional division of mathematics is into pure mathematics, mathematics studied for its intrinsic interest, and applied mathematics, mathematics which can be directly applied to real world problems.\nThis division is not always clear and many subjects have been developed as pure mathematics to find unexpected applications later on. Broad divisions, such as discrete mathematics and computational mathematics, have emerged more recently.\n\nAn ideal system of classification permits adding new areas into the organization of previous knowledge, and fitting surprising discoveries and unexpected interactions into the outline.\nFor example, the Langlands program has found unexpected connections between areas previously thought unconnected, at least Galois groups, Riemann surfaces and number theory.\n\n\n\nArithmetic is the study of numbers and the properties of operations between them.\n\nThe study of structure begins with numbers, first the familiar natural numbers and integers and their arithmetical operations, which are recorded in elementary algebra. The deeper properties of these numbers are studied in number theory. The investigation of methods to solve equations leads to the field of abstract algebra, which, among other things, studies rings and fields, structures that generalize the properties possessed by everyday numbers. Long standing questions about compass and straightedge construction were finally settled by Galois theory. The physically important concept of vectors, generalized to vector spaces, is studied in linear algebra.\n\n\nWithin the world of mathematics, analysis is the branch that focuses on change: rates of change, accumulated change, and multiple things changing relative to (or independently of) one another.\n\nModern analysis is a vast and rapidly expanding branch of mathematics that touches almost every other subdivision of the discipline, finding direct and indirect applications in topics as diverse as number theory, cryptography, and abstract algebra. It is also the language of science itself and is used across chemistry, biology, and physics, from astrophysics to X-ray crystallography.\n\nCombinatorics is the study of finite or discrete collections of objects that satisfy specified criteria. In particular, it is concerned with \"counting\" the objects in those collections (enumerative combinatorics) and with deciding whether certain \"optimal\" objects exist (extremal combinatorics). It includes graph theory, used to describe inter-connected objects (a graph in this sense is a network, or collection of connected points). See also the list of combinatorics topics, list of graph theory topics and glossary of graph theory. A \"combinatorial flavour\" is present in many parts of problem-solving.\n\nGeometry deals with spatial relationships, using fundamental qualities or axioms. Such axioms can be used in conjunction with mathematical definitions for points, straight lines, curves, surfaces, and solids to draw logical conclusions. See also List of geometry topics\n\n\n\n\n\n\n\n"}
{"id": "3090", "url": "https://en.wikipedia.org/wiki?curid=3090", "title": "Arithmetic–geometric mean", "text": "Arithmetic–geometric mean\n\nIn mathematics, the arithmetic–geometric mean (AGM) of two positive real numbers and is defined as follows:\n\nCall and and :\n\nThen define the two interdependent sequences and as\n\nwhere the square root takes the principal value. These two sequences converge to the same number, the arithmetic–geometric mean of and ; it is denoted by , or sometimes by .\n\nThe arithmetic-geometric mean is used in fast algorithms for exponential and trigonometric functions, as well as some mathematical constants, in particular, computing formula_3.\n\nTo find the arithmetic–geometric mean of and , iterate as follows:\n\nThe first five iterations give the following values:\n\nThe number of digits in which and agree (underlined) approximately doubles with each iteration. The arithmetic–geometric mean of 24 and 6 is the common limit of these two sequences, which is approximately .\n\nThe first algorithm based on this sequence pair appeared in the works of Lagrange. Its properties were further analyzed by Gauss.\n\nThe geometric mean of two positive numbers is never bigger than the arithmetic mean (see inequality of arithmetic and geometric means); as a consequence, for , is an increasing sequence, is a decreasing sequence, and . These are strict inequalities if .\n\nIf , then .\n\nThere is an integral-form expression for :\n\nwhere is the complete elliptic integral of the first kind:\n\nIndeed, since the arithmetic–geometric process converges so quickly, it provides an efficient way to compute elliptic integrals via this formula. In engineering, it is used for instance in elliptic filter design.\n\nThe reciprocal of the arithmetic–geometric mean of 1 and the square root of 2 is called Gauss's constant, after Carl Friedrich Gauss.\n\nThe geometric–harmonic mean can be calculated by an analogous method, using sequences of geometric and harmonic means. The arithmetic–harmonic mean can be similarly defined, but takes the same value as the geometric mean.\n\nThe arithmetic–geometric mean can be used to compute – among others – logarithms, complete and incomplete elliptic integrals of the first and second kind, and Jacobi elliptic functions.\n\nFrom the inequality of arithmetic and geometric means we can conclude that:\n\nand thus\n\nthat is, the sequence is nondecreasing.\n\nFurthermore, it is easy to see that it is also bounded above by the larger of and (which follows from the fact that both the arithmetic and geometric means of two numbers lie between them). Thus, by the monotone convergence theorem, the sequence is convergent, so there exists a such that:\n\nHowever, we can also see that:\n\nand so:\n\nQ.E.D.\n\nThis proof is given by Gauss.\nLet\n\nChanging the variable of integration to formula_14, where\n\ngives\n\nThus, we have\n\nThe last equality comes from observing that formula_18.\n\nFinally, we obtain the desired result\n\nGauss noticed that the sequences\n\nas\n\nhave the same limit:\nthe arithmetic–geometric mean, \"agm\".\n\nIt is possible to use this fact to construct fast algorithms for calculating elementary transcendental functions and some classical constants, in particular, the constant .\n\nFor example, according to the Gauss–Salamin formula:\n\nwhere\n\nwhich can be computed without loss of precision using\n\nTaking formula_26 , yields the \"agm\",\nwhere is a complete elliptic integral of the first kind,\nThat is to say that this quarter period may be efficiently computed through the \"agm\",\n\nUsing this property of the AGM along with the ascending transformations of Landen, Richard Brent suggested the first AGM algorithms for the fast evaluation of elementary transcendental functions (\"e\", cos \"x\", sin \"x\"). Subsequently, many authors went on to study the use of the AGM algorithms.\n\n\n\n"}
{"id": "250074", "url": "https://en.wikipedia.org/wiki?curid=250074", "title": "Binomial options pricing model", "text": "Binomial options pricing model\n\nIn finance, the binomial options pricing model (BOPM) provides a generalizable numerical method for the valuation of options. The binomial model was first proposed by Cox, Ross and Rubinstein in 1979. Essentially, the model uses a \"discrete-time\" (lattice based) model of the varying price over time of the underlying financial instrument. In general, Georgiadis showed that binomial options pricing models do not have closed-form solutions.\n\nThe Binomial options pricing model approach has been widely used since it is able to handle a variety of conditions for which other models cannot easily be applied. This is largely because the BOPM is based on the description of an underlying instrument over a period of time rather than a single point. As a consequence, it is used to value American options that are exercisable at any time in a given interval as well as Bermudan options that are exercisable at specific instances of time. Being relatively simple, the model is readily implementable in computer software (including a spreadsheet).\n\nAlthough computationally slower than the Black–Scholes formula, it is more accurate, particularly for longer-dated options on securities with dividend payments. For these reasons, various versions of the binomial model are widely used by practitioners in the options markets.\n\nFor options with several sources of uncertainty (e.g., real options) and for options with complicated features (e.g., Asian options), binomial methods are less practical due to several difficulties, and Monte Carlo option models are commonly used instead. When simulating a small number of time steps Monte Carlo simulation will be more computationally time-consuming than BOPM (cf. Monte Carlo methods in finance). However, the worst-case runtime of BOPM will be O(2), where n is the number of time steps in the simulation. Monte Carlo simulations will generally have a polynomial time complexity, and will be faster for large numbers of simulation steps. Monte Carlo simulations are also less susceptible to sampling errors, since binomial techniques use discrete time units. This becomes more true the smaller the discrete units become.\n\nThe binomial pricing model traces the evolution of the option's key underlying variables in discrete-time. This is done by means of a binomial lattice (tree), for a number of time steps between the valuation and expiration dates. Each node in the lattice represents a possible price of the underlying at a given point in time.\n\nValuation is performed iteratively, starting at each of the final nodes (those that may be reached at the time of expiration), and then working backwards through the tree towards the first node (valuation date). The value computed at each stage is the value of the option at that point in time.\n\nOption valuation using this method is, as described, a three-step process:\n\nThe tree of prices is produced by working forward from valuation date to expiration.\n\nAt each step, it is assumed that the underlying instrument will move up or down by a specific factor (formula_1 or formula_2) per step of the tree (where, by definition, formula_3 and formula_4). So, if formula_5 is the current price, then in the next period the price will either be formula_6 or formula_7.\n\nThe up and down factors are calculated using the underlying volatility, formula_8, and the time duration of a step, formula_9, measured in years (using the day count convention of the underlying instrument). From the condition that the variance of the log of the price is formula_10, we have:\n\nAbove is the original Cox, Ross, & Rubinstein (CRR) method; there are other techniques for generating the lattice, such as \"the equal probabilities\" tree. The Trinomial tree is a similar model, allowing for an up, down or stable path.\n\nThe CRR method ensures that the tree is recombinant, i.e. if the underlying asset moves up and then down (u,d), the price will be the same as if it had moved down and then up (d,u)—here the two paths merge or recombine. This property reduces the number of tree nodes, and thus accelerates the computation of the option price.\n\nThis property also allows that the value of the underlying asset at each node can be calculated directly via formula, and does not require that the tree be built first. The node-value will be:\n\nWhere formula_14 is the number of up ticks and formula_15 is the number of down ticks.\n\nAt each final node of the tree—i.e. at expiration of the option—the option value is simply its intrinsic, or exercise, value.\n\nWhere formula_17 is the strike price and formula_18 is the spot price of the underlying asset at the formula_21 period.\n\nOnce the above step is complete, the option value is then found for each node, starting at the penultimate time step, and working back to the first node of the tree (the valuation date) where the calculated result is the value of the option.\n\nIn overview: the \"binomial value\" is found at each node, using the risk neutrality assumption; see Risk neutral valuation. If exercise is permitted at the node, then the model takes the greater of binomial and exercise value at the node.\n\nThe steps are as follows:\n\n(1) Under the risk neutrality assumption, today's fair price of a derivative is equal to the expected value of its future payoff discounted by the risk free rate. Therefore, expected value is calculated using the option values from the later two nodes (\"Option up\" and \"Option down\") weighted by their respective probabilities—\"probability\" p of an up move in the underlying, and \"probability\" (1−p) of a down move. The expected value is then discounted at r, the risk free rate corresponding to the life of the option.\n\n(2) This result is the \"Binomial Value\". It represents the fair price of the derivative at a particular point in time (i.e. at each node), given the evolution in the price of the underlying to that point. It is the value of the option if it were to be held—as opposed to exercised at that point.\n\n(3) Depending on the style of the option, evaluate the possibility of early exercise at each node: if (1) the option can be exercised, and (2) the exercise value exceeds the Binomial Value, then (3) the value at the node is the exercise value.\n\nIn calculating the value at the next time step calculated—i.e. one step closer to valuation—the model must use the value selected here, for “Option up”/“Option down” as appropriate, in the formula at the node.codice_1\n\nSimilar assumptions underpin both the binomial model and the Black–Scholes model, and the binomial model thus provides a discrete time approximation to the continuous process underlying the Black–Scholes model. In fact, for European options without dividends, the binomial model value converges on the Black–Scholes formula value as the number of time steps increases. The binomial model assumes that movements in the price follow a binomial distribution; for many trials, this binomial distribution approaches the lognormal distribution assumed by Black–Scholes.\n\nIn addition, when analyzed as a numerical procedure, the CRR binomial method can be viewed as a special case of the explicit finite difference method for the Black–Scholes PDE; see finite difference methods for option pricing.\n\nIn 2011, Georgiadis shows that the binomial options pricing model has a lower bound on complexity that rules out a closed-form solution.\n\n\n\n"}
{"id": "14520910", "url": "https://en.wikipedia.org/wiki?curid=14520910", "title": "Blacker (security)", "text": "Blacker (security)\n\nBlacker (styled BLACKER) is a U.S. Department of Defense computer network security project designed to achieve A1 class ratings of the Trusted Computer System Evaluation Criteria (TCSEC). The project was implemented by SDC and Burroughs. It was the first secure system with trusted End-to-end encryption on the United States' Defense Data Network.\n\n"}
{"id": "13229499", "url": "https://en.wikipedia.org/wiki?curid=13229499", "title": "Bochner identity", "text": "Bochner identity\n\nIn mathematics — specifically, differential geometry — the Bochner identity is an identity concerning harmonic maps between Riemannian manifolds. The identity is named after the American mathematician Salomon Bochner.\n\nLet \"M\" and \"N\" be Riemannian manifolds and let \"u\" : \"M\" → \"N\" be a harmonic map. Let d\"u\" denote the derivative (pushforward) of \"u\", ∇ the gradient, Δ the Laplace–Beltrami operator, Riem the Riemann curvature tensor on \"N\" and Ric the Ricci curvature tensor on \"M\". Then\n\n"}
{"id": "420555", "url": "https://en.wikipedia.org/wiki?curid=420555", "title": "Combinatorial optimization", "text": "Combinatorial optimization\n\nIn Operations Research, applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not tractable. It operates on the domain of those optimization problems, in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the travelling salesman problem (\"TSP\") and the minimum spanning tree problem (\"MST\").\n\nCombinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, and software engineering.\n\nSome research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.\n\nApplications for combinatorial optimization include, but are not limited to:\n\n\nThere is a large amount of literature on polynomial-time algorithms for certain special classes of discrete optimization, a considerable amount of it unified by the theory of linear programming. Some examples of combinatorial optimization problems that fall into this framework are shortest paths and shortest path trees, flows and circulations, spanning trees, matching, and matroid problems.\n\nFor NP-complete discrete optimization problems, current research literature includes the following topics:\n\nCombinatorial optimization problems can be viewed as searching for the best element of some set of discrete items; therefore, in principle, any sort of search algorithm or metaheuristic can be used to solve them. However, generic search algorithms are not guaranteed to find an optimal solution, nor are they guaranteed to run quickly (in polynomial time). Since some discrete optimization problems are NP-complete, such as the traveling salesman problem, this is expected unless P=NP.\n\n\n\n\n\n"}
{"id": "55997822", "url": "https://en.wikipedia.org/wiki?curid=55997822", "title": "Copying network models", "text": "Copying network models\n\nCopying network models are network generation models that use a copying mechanism to form a network, by repeatedly duplicating and mutating existing nodes of the network. Such a network model has first been proposed in 1999 to explain the network of links between web pages, but since has been used to model biological and citation networks as well.\n\nIn 1999 Jon Kleinberg and 3 co-authors published an article to Computing and combinatorics attempting to construct a network model that explains patterns found in an analysis of the world wide web. The intuition behind the model was that when a user decides to build and publish her own web page, she encounters a list of links for her topic of interest on the web and ends up copying this collection, or many such collections to her own web page. This creating a new node in the network - the new page - and copying edges from already existing nodes in some fashion.\n\nThey outlined a model very generally, but didn't analyse the predictions of an exact model in detail, mostly due to computational limitations, but suggested that copying nodes randomly is a simple, model worthy mechanism for creating Zipfian distribution networks.\nThis paper since, has been cited over 1200 times, which is a number comparable to significant papers contributing to network science, like the one describing the Erdős–Rényi model (about 8300) and includes notable network science books like Mark Newman's\n\nTo understand a general model, take a basic network growth model, which is characterized by four stochastic processes. Creation processes formula_1 and formula_2 for node- and edge-creation, and deletion processes formula_3 and formula_4 for node- and edge-deletion.\n\nTake a discrete time timeframe, where formula_1 consists of simply at each step, creating a node with probability formula_6, and similarly formula_3 is deleting a node with probability \"a\"(\"t\"). Consequently, this also means formula_4 includes removing all edges that belonged to a node that was removed.\n\nformula_2 is where the essence of the copying model is. In the original article, they characterize formula_2 with a probability distribution, that determines a node formula_11 to add edges out of, and a number of edges formula_12 that will be added. And with probability formula_13 that the \"k\" edges are either copied or added randomly. With probability formula_13, all formula_12 edges from v are drawn to nodes chosen independently and uniformly at random. With probability formula_16, the \"k\" edges are copied from a randomly chosen node formula_17. Meaning that formula_12 neighbours of formula_17 become neighbours of formula_11. If formula_17 has a degree higher than formula_12, formula_12 edges are selected randomly and if it has a lower degree formula_24, a next node is randomly selected and formula_25 of its edges are copied, and so on.\n\nIt can be shown, that such a network produces a power law degree distribution, with an exponent formula_26 formula_27 where formula_28 is the ratio of number of the randomly added edges to the number of the copied edges. So with a ratio between zero and 0.5 a power law distribution with an exponent of formula_29 can be achieved. Also note that as the ratio approaches 1, the exponent goes to infinity.\n\nAnother simple model, proposed to explain the observation that the average node degree grows with system size adds nodes one at a time. A new node randomly selects a node from the existing ones and in addition to copying all the edges the target node was assigned at its introduction, it connects to the node itself, thus slightly increasing average degree. For example if the target node is the very first one in the network, no additional edges are added, just the one between the first node and the last. Take the two extreme cases. If a new node always connects to the first node, the model forms a star graph, where each node has degree one, but the first node has increasing degree count. In this case, the average degree increases by formula_30 with each additional node, where formula_31 is the number of nodes. In the other extreme case, where the new nodes connects to the one added before it, a complete graph is formed and the average degree increases 1 with every new node.\n\nA copying model that is somewhat of a mixture of the two was introduced by Vazquez. In this model, a when a new node is added, it connects to one randomly selected node that is already present in the network, and to each of its neighbors with formula_32 possibility. So with formula_33 this graph creates a chain and formula_34 creates a complete graph. Depending on formula_32 this graph can produce a number of power-law degree distributions with certain cutoffs that characterize real world networks well.\n\nThere has been considerable interest in modeling biological networks, such as protein interaction networks and genetic regulatory networks using copying network models. Genes that contain information about how a node in a network should interact with others tend to duplicate in evolution, thus duplicating the edges that were present in the network. Also, preferential attachment networks can not really model biological networks well, both because they are not plausible, and because a number of biological networks have power law degree distribution with exponent formula_36 which is not produced by such prefernital network models.\n\nTake a node duplication process, in which initially each node has equal probability of being duplicated in a single time step. However, the probability of duplication is influenced by the history of prior duplications, and not all edges get duplicated, only a randomly selected subset of them. Such a partial duplication model, can produce power-law distributions with exponents formula_36, consistent with the degree distribution of a number of biological networks, regardless of the starting graph.\n"}
{"id": "33034771", "url": "https://en.wikipedia.org/wiki?curid=33034771", "title": "Defining equation (physical chemistry)", "text": "Defining equation (physical chemistry)\n\nIn physical chemistry, there are numerous quantities associated with chemical compounds and reactions; notably in terms of \"amounts\" of substance, \"activity\" or \"concentration\" of a substance, and the \"rate\" of reaction. This article uses SI units.\n\nTheoretical chemistry requires quantities from core physics, such as time, volume, temperature, and pressure. But the highly quantitative nature of physical chemistry, in a more specialized way than core physics, uses molar amounts of substance rather than simply counting numbers; this leads to the specialized definitions in this article. Core physics itself rarely uses the mole, except in areas overlapping thermodynamics and chemistry.\n\n\"Entity\" refers to the type of particle/s in question, such as atoms, molecules, complexes, radicals, ions, electrons etc.\n\nConventionally for concentrations and activities, square brackets [ ] are used around the chemical molecular formula. For an arbitrary atom, generic letters in upright non-bold typeface such as A, B, R, X or Y etc. are often used.\n\nNo standard symbols are used for the following quantities, as specifically applied to a substance:\n\n\nUsually the symbol for the quantity with a subscript of some reference to the quantity is used, or the quantity is written with the reference to the chemical in round brackets. For example, the mass of water might be written in subscripts as \"m\", \"m\", \"m\", \"m\" (if clear from context) etc., or simply as \"m\"(HO). Another example could be the electronegativity of the fluorine-fluorine covalent bond, which might be written with subscripts \"χ\", \"χ\" or \"χ\" etc., or brackets \"χ\"(F-F), \"χ\"(FF) etc.\n\nNeither is standard. For the purpose of this article, the nomenclature is as follows, closely (but not exactly) matching standard use.\n\nFor general equations with no specific reference to an entity, quantities are written as their symbols with an index to label the component of the mixture - i.e. \"q\". The labeling is arbitrary in initial choice, but once chosen fixed for the calculation.\n\nIf any reference to an actual entity (say hydrogen ions H) or any entity at all (say X) is made, the quantity symbol \"q\" is followed by curved ( ) brackets enclosing the molecular formula of X, i.e. \"q\"(X), or for a component \"i\" of a mixture \"q\"(X). No confusion should arise with the notation for a mathematical function.\n\nThe defining formulae for the equilibrium constants \"K\" (all reactions) and \"K\" (gaseous reactions) apply to the general chemical reaction:\n\nand the defining equation for the rate constant \"k\" applies to the simpler synthesis reaction (one product only):\n\nwhere:\n\n\nThe dummy indices on the substances \"X\" and \"Y\" \"label\" the components (arbitrary but fixed for calculation); they are not the \"numbers\" of each component molecules as in usual chemistry notation.\n\nThe units for the chemical constants are unusual since they can vary depending on the stoichiometry of the reaction, and the number of reactant and product components. The general units for equilibrium constants can be determined by usual methods of dimensional analysis. For the generality of the kinetics and equilibria units below, let the indices for the units be;\n\nFor the constant \"K\";\n\nSubstitute the concentration units into the equation and simplify:,\n\nThe procedure is exactly identical for \"K\".\n\nFor the constant \"k\"\n\nNotation for half-reaction standard electrode potentials is as follows. The redox reaction\n\nsplit into:\n\na reduction reaction: <chem> B+ + e^- <=> B </chem>\n\nand an oxidation reaction: <chem> A+ + e^- <=> A </chem>\n\n(written this way by convention) the electrode potential for the half reactions are written as formula_4 and formula_5 respectively.\n\nFor the case of a metal-metal half electrode, letting M represent the metal and \"z\" be its valency, the half reaction takes the form of a reduction reaction:\n\n\n"}
{"id": "437701", "url": "https://en.wikipedia.org/wiki?curid=437701", "title": "Dependent and independent variables", "text": "Dependent and independent variables\n\nIn mathematical modeling, statistical modeling and experimental sciences, the values of dependent variables depend on the values of independent variables. The dependent variables represent the output or outcome whose variation is being studied. The independent variables, also known in a statistical context as regressors, represent inputs or causes, that is, potential reasons for variation. In an experiment, any variable that the experimenter manipulates can be called an independent variable. Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.\n\nIn mathematics, a function is a rule for taking an input (in the simplest case, a number or set of numbers) and providing an output (which may also be a number). A symbol that stands for an arbitrary input is called an independent variable, while a symbol that stands for an arbitrary output is called a dependent variable. The most common symbol for the input is \"x\", and the most common symbol for the output is \"y\"; the function itself is commonly written formula_1.\n\nIt is possible to have multiple independent variables or multiple dependent variables. For instance, in multivariable calculus, one often encounters functions of the form formula_2, where \"z\" is a dependent variable and \"x\" and \"y\" are independent variables. Functions with multiple outputs are often referred to as vector-valued functions.\n\nIn set theory, a function between a set X and a set Y is a subset of the Cartesian product formula_3 such that every element of X appears in an ordered pair with exactly one element of Y. In this situation, a symbol representing an element of X may be called an independent variable and a symbol representing an element of Y may be called a dependent variable, such as when X is a manifold and the symbol \"x\" represents an arbitrary point in the manifold. However, many advanced textbooks do not distinguish between dependent and independent variables.\n\nIn an experiment, a variable, manipulated by an experimenter, is called an independent variable. The dependent variable is the event expected to change when the independent variable is manipulated.\n\nIn data mining tools (for multivariate statistics and machine learning), the dependent variable is assigned a \"role\" as (or in some tools as \"label attribute\"), while an independent variable may be assigned a role as \"regular variable\". Known values for the target variable are provided for the training data set and test data set, but should be predicted for other data. The target variable is used in supervised learning algorithms but not in unsupervised learning.\n\nIn mathematical modeling, the dependent variable is studied to see if and how much it varies as the independent variables vary. In the simple stochastic linear model formula_4 the term formula_5 is the \"i\" value of the dependent variable and formula_6 is the \"i\" value of the independent variable. The term formula_7 is known as the \"error\" and contains the variability of the dependent variable not explained by the independent variable.\n\nWith multiple independent variables, the model is formula_8, where \"n\" is the number of independent variables.\n\nIn simulation, the dependent variable is changed in response to changes in the independent variables.\n\nDepending on the context, an independent variable is sometimes called a \"predictor variable\", regressor, covariate, \"controlled variable\", \"manipulated variable\", \"explanatory variable\", exposure variable (see reliability theory), \"risk factor\" (see medical statistics), \"feature\" (in machine learning and pattern recognition) or \"input variable.\"\nIn econometrics, the term \"control variable\" is usually used instead of \"covariate\". \n\nDepending on the context, a dependent variable is sometimes called a \"response variable\", \"regressand\", \"criterion\", \"predicted variable\", \"measured variable\", \"explained variable\", \"experimental variable\", \"responding variable\", \"outcome variable\", \"output variable\" or \"label\".\n\n\"Explanatory variable\" is preferred by some authors over \"independent variable\" when the quantities treated as independent variables may not be statistically independent or independently manipulable by the researcher. If the independent variable is referred to as an \"explanatory variable\" then the term \"response variable\" is preferred by some authors for the dependent variable.\n\n\"Explained variable\" is preferred by some authors over \"dependent variable\" when the quantities treated as \"dependent variables\" may not be statistically dependent. If the dependent variable is referred to as an \"explained variable\" then the term \"predictor variable\" is preferred by some authors for the independent variable.\n\nVariables may also be referred to by their form: continuous, binary/dichotomous, nominal categorical, and ordinal categorical, among others.\n\nAn example is provided by the analysis of trend in sea level by . Here the dependent variable (and variable of most interest) was the annual mean sea level at a given location for which a series of yearly values were available. The primary independent variable was time. Use was made of a covariate consisting of yearly values of annual mean atmospheric pressure at sea level. The results showed that inclusion of the covariate allowed improved estimates of the trend against time to be obtained, compared to analyses which omitted the covariate.\n\nA variable may be thought to alter the dependent or independent variables, but may not actually be the focus of the experiment. So that variable will be kept constant or monitored to try to minimize its effect on the experiment. Such variables may be designated as either a \"controlled variable\", \"control variable\", or \"extraneous variable\".\n\nExtraneous variables, if included in a regression analysis as independent variables, may aid a researcher with accurate response parameter estimation, prediction, and goodness of fit, but are not of substantive interest to the hypothesis under examination. For example, in a study examining the effect of post-secondary education on lifetime earnings, some extraneous variables might be gender, ethnicity, social class, genetics, intelligence, age, and so forth. A variable is extraneous only when it can be assumed (or shown) to influence the dependent variable. If included in a regression, it can improve the fit of the model. If it is excluded from the regression and if it has a non-zero covariance with one or more of the independent variables of interest, its omission will bias the regression's result for the effect of that independent variable of interest. This effect is called confounding or omitted variable bias; in these situations, design changes and/or controlling for a variable statistical control is necessary.\n\nExtraneous variables are often classified into three types:\n\nIn modelling, variability that is not covered by the independent variable is designated by formula_9 and is known as the \"residual\", \"side effect\", \"error\", \"unexplained share\", \"residual variable\", or \"tolerance\".\n\n\n"}
{"id": "34978225", "url": "https://en.wikipedia.org/wiki?curid=34978225", "title": "Dieudonné's theorem", "text": "Dieudonné's theorem\n\nIn mathematics, Dieudonné's theorem, named after Jean Dieudonné, is a theorem on when the Minkowski sum of closed sets is closed.\n\nLet nonempty closed convex sets formula_1 a locally convex space, if either formula_2 or formula_3 is locally compact and formula_4 (where formula_5 gives the recession cone) is a linear subspace, then formula_6 is closed.\n"}
{"id": "12647205", "url": "https://en.wikipedia.org/wiki?curid=12647205", "title": "Digital Library of Mathematical Functions", "text": "Digital Library of Mathematical Functions\n\nThe Digital Library of Mathematical Functions (DLMF) is an online project at the National Institute of Standards and Technology to develop a major resource of mathematical reference data for special functions and their applications. It is intended as an update of \"Abramowitz's and Stegun's Handbook of Mathematical Functions\" (A&S). It was published online on May 7, 2010, though some chapters appeared earlier. In the same year it appeared at Cambridge University Press under the title NIST Handbook of Mathematical Functions.\n\nIn contrast to A&S, whose initial print run was done by the U.S. Government Printing Office and was in the public domain, NIST asserts that it holds copyright to the DLMF under Title 17 USC 105 of the U.S. Code.\n\n"}
{"id": "50546680", "url": "https://en.wikipedia.org/wiki?curid=50546680", "title": "Distributed tree search", "text": "Distributed tree search\n\nDistributed Tree Search (DTS) algorithm is a class of algorithms for searching values in an efficient and distributed manner. Their purpose is to iterate through a tree by working along multiple branches in parallel and merging the results of each branch into one common solution, in order to minimize time spent searching for a value in a tree-like data structure.\n\nThe original paper was written in 1988 by Chris Ferguson and Richard E. Korf, from the University of California's Computer Science Department. They used multiple other chess AIs to develop this wider range algorithm.\n\nThe Distributed Tree Search Algorithm (also known as Korf-Ferguson algorithm) was created to solve the following problem : « Given a tree with non-uniform branching factor and depth, search it in parallel with an arbitrary number of processors as fast as possible. »\n\nThe top-level part of this algorithm is general and does not use a particular existing type of tree-search, but it can be easily specialized to fit any type of non-distributed tree-search.\n\nDTS consists of using multiple processes, each with a node and a set of processors attached, with the goal of searching the sub-tree below the said node. Each process then divides itself into multiple coordinated sub-processes which recursively divide themselves again until an optimal way to search the tree has been found based on the number of processors available to each process. Once a process finishes, DTS dynamically reassigns the processors to other processes as to keep the efficiency to a maximum through good load-balancing, especially in irregular trees.\n\nOnce a process finishes searching, it recursively sends and merges a resulting signal to its parent-process, until all the different sub-answers have been merged and the entire problem has been solved.\n\nDTS is only applicable under two major conditions: the data structure to search through is a tree, and the algorithm can make use of at least one computation unit (Although it cannot be considered as distributed if there is only one).\n\nOne major example of the everyday use of DTS is network routing. The Internet can be seen as a tree of IP addresses, and an analogy to a routing protocol could be how post offices work in the real world. Since there are over 4.3 billion IP addresses currently, society heavily depends on the time the data takes to find its way to its destination. As such, IP-routing divides the work into multiple sub-units which each have different scales of calculation capabilities and use each other's result to find the route in a very efficient manner. This is an instance of DTS that affects over 43% of the world's population, for reasons going from entertainment to national security.\n\nAlthough DTS is currently one of the most widely used algorithms, many of its applications have alternatives to them which could potentially develop into more efficient, less resource-demanding solutions, were they more researched.\n\nOne of the more controversial examples is Big-Data processing. In applications like Google Search Engine, Facebook, YouTube, search needs to be optimized to keep waiting time inside a reasonable window. This could be achieved through the plain use of DTS, but other algorithms are used in place (for example data-hashing in SQL databases), or in conjunction (Facebook's Haystack algorithm groups parallel tree-search, data-hashing and memory-ordering/sorting).\n\nOne of the more important limits of DTS is the fact that it requires a tree as input. Trees are a sub-instance of a data structure known as Graphs, which means every Graph can be converted into a tree. Although there currently exists no better way to search through trees than Korf-Ferguson's algorithm, each task has different particularities and in most cases, there will exist more efficient data structures to represent the problem and solve it than through tree-search. And so there exist instances of tree structures with cycles that cannot possibly be faster than a graph-search on the same structure with the same processing power.\n\nThere are few controversies around Korf-Ferguson's DTS algorithm, since it is recognized as very complete, but simple. It is very often used as a stepping stone for students to discover the fundamentals and key concepts of distributed problem-solving.\n\nThe most important challenge to this algorithmic concept was an article by Kröll B, « Balanced Distributed Sarch Trees Do Not Exist », which does not attack the veracity or current efficiency of the algorithm, but rather the fact that DTS itself, no matter how many improvements are made to it (for example balancing the input tree before-hand), will never be able to reach optimal resolution-time. This opens a new view point : are too many resources used into the completion of DTS, which blocks new algorithms with higher efficiency-potential from getting researched and developed ? Another limit of DTS is the fact that no matter how efficient the division, coordination and merging of the solutions is, it will always be limited by the material number or processors and their processing power. Until recently, this was admitted as being a limit to nearly every computation, but new-generation algorithms like Euclideon might one day be able to crush DFS's efficiency through processing-power-independent problem resolution.\n\nTree (data structure)\n\nSearch tree\n\nBinary search tree\n\nTree traversal\n\nMonte Carlo tree search\n\nParallel computing\n"}
{"id": "340363", "url": "https://en.wikipedia.org/wiki?curid=340363", "title": "Economic statistics", "text": "Economic statistics\n\nEconomic statistics is a topic in applied statistics that concerns the collection, processing, compilation, dissemination, and analysis of economic data. It is also common to call the data themselves 'economic statistics', but for this usage see economic data. The data of concern to economic statistics may include those of an economy of region, country, or group of countries. Economic statistics may also refer to a subtopic of official statistics for data produced by official organizations (e.g. national statistical services, intergovernmental organizations such as United Nations, European Union or OECD, central banks, ministries, etc.). Analyses within economic statistics both make use of and provide the empirical data needed in economic research, whether descriptive or econometric. They are a key input for decision making as to economic policy.\nThe subject includes statistical analysis of topics and problems in microeconomics, macroeconomics, business, finance, forecasting, data quality, and policy evaluation. It also includes such considerations as what data to collect in order to quantify some particular aspect of an economy and of how best to collect in any given instance.\n\n\n\n\n"}
{"id": "10412", "url": "https://en.wikipedia.org/wiki?curid=10412", "title": "Elementary function", "text": "Elementary function\n\nIn mathematics, an elementary function is a function of one variable which is the composition of a finite number of arithmetic operations , exponentials, logarithms, constants, and solutions of algebraic equations (a generalization of \"n\"th roots).\n\nThe elementary functions include:\n\nIt follows directly from the definition that the set of elementary functions is closed under arithmetic operations and composition. It is also closed under differentiation. It is not closed under limits and infinite sums. \n\nImportantly, the elementary functions are \"not\" closed under integration, as shown by Liouville's theorem, see Nonelementary integral. The Liouvillian functions are defined as the elementary functions and, recursively, the integrals of the Liouvillian functions.\n\nSome elementary functions, such as roots, logarithms, or inverse trigonometric functions, are not entire functions and may be multivalued.\n\nElementary functions were introduced by Joseph Liouville in a series of papers from 1833 to 1841. An algebraic treatment of elementary functions was started by Joseph Fels Ritt in the 1930s.\n\nExamples of elementary functions include:\n\nThe last function is equal to formula_9, the inverse cosine, in the entire complex plane. Hence, it is an elementary function. \n\nAn example of a function that is \"not\" elementary is the error function\n\n\na fact that may not be immediately obvious, but can be proven using the Risch algorithm.\n\n\nThe mathematical definition of an elementary function, or a function in elementary form, is considered in the context of differential algebra. A differential algebra is an algebra with the extra operation of derivation (algebraic version of differentiation). Using the derivation operation new equations can be written and their solutions used in extensions of the algebra. By starting with the field of rational functions, two special types of transcendental extensions (the logarithm and the exponential) can be added to the field building a tower containing elementary functions.\n\nA differential field \"F\" is a field \"F\" (rational functions over the rationals Q for example) together with a derivation map \"u\" → ∂\"u\". (Here ∂\"u\" is a new function. Sometimes the notation \"u\"′ is used.) The derivation captures the properties of differentiation, so that for any two elements of the base field, the derivation is linear\n\nand satisfies the Leibniz product rule\n\nAn element \"h\" is a constant if \"∂h = 0\". If the base field is over the rationals, care must be taken when extending the field to add the needed transcendental constants.\n\nA function \"u\" of a differential extension \"F\"[\"u\"] of a differential field \"F\" is an elementary function over \"F\" if the function \"u\"\n(this is Liouville's theorem).\n\n\n\n\n"}
{"id": "35732417", "url": "https://en.wikipedia.org/wiki?curid=35732417", "title": "Evectant", "text": "Evectant\n\nIn mathematical invariant theory, an evectant is a contravariant constructed from an invariant by acting on it with a differential operator called an evector. Evectants and evectors were introduced by .\n\n"}
{"id": "251177", "url": "https://en.wikipedia.org/wiki?curid=251177", "title": "Existence theorem", "text": "Existence theorem\n\nIn mathematics, an existence theorem is a theorem with a statement beginning 'there exist(s) ..', or more generally 'for all , , ... there exist(s) ...'. That is, in more formal terms of symbolic logic, it is a theorem with a prenex normal form involving the existential quantifier. Many such theorems will not do so explicitly, as usually stated in standard mathematical language – for example, the statement that the sine function is continuous; or any theorem written in big O notation. The quantification can be found in the definitions of the concepts used.\n\nA controversy that goes back to the early twentieth century concerns the issue of purely theoretic existence theorems, i.e., theorems depending on non-constructive foundational material such as the axiom of infinity, the axiom of choice, or the law of excluded middle. Such theorems provide no indication as to how to exhibit, or construct, the object whose existence is claimed. From a constructivist viewpoint, by admitting them mathematics loses its concrete applicability. The opposing viewpoint is that abstract methods are far-reaching, in a way that numerical analysis cannot be.\n\nAn existence theorem is purely theoretical if the proof given of it does not also indicate a construction of the kind of object whose existence is asserted. Such a proof is non-constructive, and the point is that the whole approach may not lend itself to construction. In terms of algorithms, purely theoretical existence theorems bypass all algorithms for finding what is asserted to exist. They contrast with \"constructive\" existence theorems. Many constructivist mathematicians work in extended logics (such as intuitionistic logic) where such existence statements are intrinsically weaker than their constructive counterparts.\n\nSuch purely theoretical existence results are in any case ubiquitous in contemporary mathematics. For example, John Nash's original proof of the existence of a Nash equilibrium, from 1951, was such an existence theorem. In 1962 a constructive approach was found.\n\nFrom the other direction there has been considerable clarification of what constructive mathematics is; without the emergence of a 'master theory'. For example, according to Errett Bishop's definitions, the continuity of a function such as should be proved as a constructive bound on the modulus of continuity, meaning that the existential content of the assertion of continuity is a promise that can always be kept. One could get another explanation from type theory, in which a proof of an existential statement can come only from a \"term\" (which we can see as the computational content).\n\n"}
{"id": "35443743", "url": "https://en.wikipedia.org/wiki?curid=35443743", "title": "Fields Medal Symposium", "text": "Fields Medal Symposium\n\nThe Fields Medal Symposium is an annual event that honours one of the Fields Medal recipients from the most recent International Congress of Mathematicians. The symposium is jointly endorsed by the International Mathematical Union and the Fields Institute for Mathematical Sciences. The idea was conceived in preparation for the International Congress of Mathematicians 2010 (ICM2010) in Hyberdad, India. Professor Edward Bierstone of the University of Toronto was the director of the institute during the inaugural symposium in October 2012. All symposiums take place at the Fields Institute in Toronto, Canada. The symposia include mathematical activity that explore work related to the honoured Fields Medallist. They will include public lectures meant to spark interest in mathematics including public lectures and events for students.\n\n! style=\"width:150px\" | Year of Symposium\n! style=\"width:150px\" | Organizers\n! style=\"width:150px\" | Fields Medalist\n! style=\"width:150px\" | Symposium Speakers\n! style=\"width:250px\" | Medallists' Distinguished Work\n\nAlex Eskin\n\nDimitry Jakobson\n\nMarina Ratner\n\nRalf Spatzier\n\nYves Benoist\n\nJean Bourgain\n\nShimon Brooks\n\nManfred Einsiedler\n\nAlex Eskin\n\nHillel Furstenberg\n\nAnatole Katok\n\nElon Lindenstrauss\n\nGregory Margulis\n\nShahar Mozes\n\nHee Oh\n\nKannan Soundararajan\n\nMasaki Tsukamaoto\n\nBenjamin Weiss\n\nTamar Ziegler\nRobert McCann\n\nClement Mouhot\n\nNeil Trudinger\nLuis Caffarelli\n\nEric Carlen\n\nJeff Cheeger\n\nWilfrid Gangbo\n\nYan Guo\n\nMichel Ledoux\n\nMonika Ludwig \n\nNader Masmoudi\n\nAssaf Naor\n\nFelix Otto\n\nTristan Rivière\n\nLaure Saint-Raymond\n\nKarl-Theodor Sturm\n\nVladimir Sverak\n\nHorng-Tzer Yau\nJohn Cardy\n\nKostya Khanin\n\nGreg Lawler\n\nGordon Slade\n\nBalint Virag\n\nWendelin Werner\nDmitri Belyaev\n\nAlexei Borodin\n\nJohn Cardy\n\nDmitry Chelkak\n\nGeoffrey Grimmett\n\nJacek Graczyk\n\nClément Hongler\n\nRick Kenyon\n\nAntti Kupiainen\n\nGreg Lawler\n\nBernard Nienhuis\n\nDimitry Panchenko\n\nSteffen Rohde\n\nScott Sheffield\n\nBéatrice de Tilière\n\nAnna Zdunik\nJohn Friedlander\n\nFlorian Herzig\n\nKumar Murty\n\nSujatha Ramdorai\n\nJacob Tsimerman\nBenedict Gross\n\nPiper Harron\n\nWei Ho\n\nMelanie Matchett Wood\n\nBarry Mazur\n\nAlison Miller\n\nHee Oh\n\nPeter Sarnak\n\nArul Shankar\n\nChristopher Skinner\n\nJacob Tsimerman\n\nIla Varma\n\nXiaoheng Wang\n\n"}
{"id": "53433912", "url": "https://en.wikipedia.org/wiki?curid=53433912", "title": "François Peyrard", "text": "François Peyrard\n\nFrançois Peyrard (1760–1822) was a French mathematician, educator and librarian. During the French Revolution, he was involved in the committee that reformed the French educational system. He was one of the founder of the École Polytechnique and its first librarian.\nBorn in Velay (now Haute-Loire), he was a student at the \"Collège\" of Le Puy-en-Velay. Refusing to become priest, he first enrolled in the Gardes françaises, then arrived in Paris in 1784 to start a career as mathematics teacher.\n\nIn 1793, he joined the committee in charge of reforming the educational system, where he met Vandermonde, Monge, Lagrange. He helped to found the École Polytechnique, for which he was the first librarian.\n\nAs mathematician, his main contributions are translations of Euclidean geometry due to his great skills in Greek, Latin, and mathematics. His translations of \"Euclid's Elements\" are still considered as the best existing in French.\n\nDue to many personal conflicts, he left the Polytechnique, in 1804, just after the release of his first \"Euclid's Elements\" translation. In less than 10 years, he had purchased as librarian approximately ten thousand books.\n\nHe then became a teacher at the Lycée Condorcet (Lycée Bonaparte at that time) in Paris. In 1815 he published a revised edition of Bézout's arithmetical opus \"Cours de mathématiques, à l'usage de la marine et de l'artillerie\".\n\nHe was the first to identify in 1808 a previously unknown manuscript of Euclid called \"Vaticanus graecus 190\", a missing part of Euclid’s works, that he dug out of Napoléon’s booty from the Vatican. He released in 1814 a revised edition of the \"Elements\", reviewed by Delambre, Lagrange and Legendre.\n\nHe died destitute in 1822.\n\nCitations\nSources\n"}
{"id": "17893372", "url": "https://en.wikipedia.org/wiki?curid=17893372", "title": "Grothendieck space", "text": "Grothendieck space\n\nIn mathematics, a Grothendieck space, named after Alexander Grothendieck, is a Banach space \"X\" in which every weakly* convergent sequence in the dual space \"X\"* converges with respect to the weak topology of \"X\"*.\n\nLet \"X\" be a Banach space. Then the following conditions are equivalent:\n\n\n\n"}
{"id": "58863", "url": "https://en.wikipedia.org/wiki?curid=58863", "title": "Gödel's incompleteness theorems", "text": "Gödel's incompleteness theorems\n\nGödel's incompleteness theorems are two theorems of mathematical logic that demonstrate the inherent limitations of every formal axiomatic system capable of modelling basic arithmetic. These results, published by Kurt Gödel in 1931, are important both in mathematical logic and in the philosophy of mathematics. The theorems are widely, but not universally, interpreted as showing that Hilbert's program to find a complete and consistent set of axioms for all mathematics is impossible.\n\nThe first incompleteness theorem states that no consistent system of axioms whose theorems can be listed by an effective procedure (i.e., an algorithm) is capable of proving all truths about the arithmetic of the natural numbers. For any such consistent formal system, there will always be statements about the natural numbers that are true, but that are unprovable within the system. The second incompleteness theorem, an extension of the first, shows that the system cannot demonstrate its own consistency.\n\nEmploying a diagonal argument, Gödel's incompleteness theorems were the first of several closely related theorems on the limitations of formal systems. They were followed by Tarski's undefinability theorem on the formal undefinability of truth, Church's proof that Hilbert's Entscheidungsproblem is unsolvable, and Turing's theorem that there is no algorithm to solve the halting problem.\n\nThe incompleteness theorems apply to formal systems that are of sufficient complexity to express the basic arithmetic of the natural numbers and which are consistent, and effectively axiomatized, these concepts being detailed below. Particularly in the context of first-order logic, formal systems are also called \"formal theories\". In general, a formal system is a deductive apparatus that consists of a particular set of axioms along with rules of symbolic manipulation (or rules of inference) that allow for the derivation of new theorems from the axioms. One example of such a system is first-order Peano arithmetic, a system in which all variables are intended to denote natural numbers. In other systems, such as set theory, only some sentences of the formal system express statements about the natural numbers. The incompleteness theorems are about formal provability within these systems, rather than about \"provability\" in an informal sense.\n\nThere are several properties that a formal system may have, including completeness, consistency, and the existence of an effective axiomatization. The incompleteness theorems show that systems which contain a sufficient amount of arithmetic cannot possess all three of these properties.\n\nA formal system is said to be \"effectively axiomatized\" (also called \"effectively generated\") if its set of theorems is a recursively enumerable set (Franzén 2005, p. 112).\n\nThis means that there is a computer program that, in principle, could enumerate all the theorems of the system without listing any statements that are not theorems. Examples of effectively generated theories include Peano arithmetic and Zermelo–Fraenkel set theory (ZFC).\n\nThe theory known as true arithmetic consists of all true statements about the standard integers in the language of Peano arithmetic. This theory is consistent, and complete, and contains a sufficient amount of arithmetic. However it does not have a recursively enumerable set of axioms, and thus does not satisfy the hypotheses of the incompleteness theorems.\n\nA set of axioms is (\"syntactically\", or \"negation\"-) complete if, for any statement in the axioms' language, that statement or its negation is provable from the axioms (Smith 2007, p.  24). This is the notion relevant for Gödel's first Incompleteness theorem. It is not to be confused with \"semantic\" completeness, which means that the set of axioms proves all the semantic tautologies of the given language. In his completeness theorem, Gödel proved that first order logic is \"semantically\" complete. But it is not syntactically complete, since there are sentences expressible in the language of first order logic that can be neither proved nor disproved from the axioms of logic alone.\n\nIn a mere system of logic it would be absurd to expect syntactic completeness. But in a system of mathematics, thinkers such as Hilbert had believed that it is just a matter of time to find such an axiomatization that would allow one to either prove or disprove (by proving its negation) each and every mathematical formula.\n\nA formal system might be syntactically incomplete by design, such as logics generally are. Or it may be incomplete simply because not all the necessary axioms have been discovered or included. For example, Euclidean geometry without the parallel postulate is incomplete, because some statements in the language (such as the parallel postulate itself) can not be proved from the remaining axioms. Similarly, the theory of dense linear orders is not complete, but becomes complete with an extra axiom stating that there are no endpoints in the order. The continuum hypothesis is a statement in the language of ZFC that is not provable within ZFC, so ZFC is not complete. In this case, there is no obvious candidate for a new axiom that resolves the issue.\n\nThe theory of first-order Peano arithmetic is consistent, has an infinite but recursively enumerable set of axioms, and can encode enough arithmetic for the hypotheses of the incompleteness theorem. Thus, by the first incompleteness theorem, Peano Arithmetic is not complete. The theorem gives an explicit example of a statement of arithmetic that is neither provable nor disprovable in Peano's arithmetics. Moreover, this statement is true in the usual model. Moreover, no effectively axiomatized, consistent extension of Peano arithmetic can be complete.\n\nA set of axioms is (simply) consistent if there is no statement such that both the statement and its negation are provable from the axioms, and \"inconsistent\" otherwise.\n\nPeano arithmetic is provably consistent from ZFC, but not from within itself. Similarly, ZFC is not provably consistent from within itself, but ZFC + \"there exists an inaccessible cardinal\" proves ZFC is consistent because if is the least such cardinal, then sitting inside the von Neumann universe is a model of ZFC, and a theory is consistent if and only if it has a model.\n\nIf one takes all statements in the language of Peano arithmetic as axioms, then this theory is complete, has a recursively enumerable set of axioms, and can describe addition and multiplication. However, it is not consistent.\n\nAdditional examples of inconsistent theories arise from the paradoxes that result when the axiom schema of unrestricted comprehension is assumed in set theory.\n\nThe incompleteness theorems apply only to formal systems which are able to prove a sufficient collection of facts about the natural numbers. One sufficient collection is the set of theorems of Robinson arithmetic \"Q\". Some systems, such as Peano arithmetic, can directly express statements about natural numbers. Others, such as ZFC set theory, are able to interpret statements about natural numbers into their language. Either of these options is appropriate for the incompleteness theorems.\n\nThe theory of algebraically closed fields of a given characteristic is complete, consistent, and has an infinite but recursively enumerable set of axioms. However it is not possible to encode the integers into this theory, and the theory cannot describe arithmetic of integers. A similar example is the theory of real closed fields, which is essentially equivalent to Tarski's axioms for Euclidean geometry. So Euclidean geometry itself (in Tarski's formulation) is an example of a complete, consistent, effectively axiomatized theory.\n\nThe system of Presburger arithmetic consists of a set of axioms for the natural numbers with just the addition operation (multiplication is omitted). Presburger arithmetic is complete, consistent, and recursively enumerable and can encode addition but not multiplication of natural numbers, showing that for Gödel's theorems one needs the theory to encode not just addition but also multiplication.\n\nDan Willard (2001) has studied some weak families of arithmetic systems which allow enough arithmetic as relations to formalise Gödel numbering, but which are not strong enough to have multiplication as a function, and so fail to prove the second incompleteness theorem; these systems are consistent and capable of proving their own consistency (see self-verifying theories).\n\nIn choosing a set of axioms, one goal is to be able to prove as many correct results as possible, without proving any incorrect results. For example, we could imagine a set of true axioms which allow us to prove every true arithmetical claim about the natural numbers (Smith 2007, p 2). In the standard system of first-order logic, an inconsistent set of axioms will prove every statement in its language (this is sometimes called the principle of explosion), and is thus automatically complete. A set of axioms that is both complete and consistent, however, proves a maximal set of non-contradictory theorems (Hinman 2005, p. 143).\n\nThe pattern illustrated in the previous sections with Peano arithmetic, ZFC, and ZFC + \"there exists an inaccessible cardinal\" cannot generally be broken. Here ZFC + \"there exists an inaccessible cardinal\" cannot from itself, be proved consistent. It is also not complete, as illustrated by the in ZFC + \"there exists an inaccessible cardinal\" theory unresolved continuum hypothesis.\n\nThe first incompleteness theorem shows that, in formal systems that can express basic arithmetic, a complete and consistent finite list of axioms can never be created: each time an additional, consistent statement is added as an axiom, there are other true statements that still cannot be proved, even with the new axiom. If an axiom is ever added that makes the system complete, it does so at the cost of making the system inconsistent. It is not even possible for an infinite list of axioms to be complete, consistent, and effectively axiomatized.\n\nGödel's first incompleteness theorem first appeared as \"Theorem VI\" in Gödel's 1931 paper \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems I\". The hypotheses of the theorem were improved shortly thereafter by J. Barkley Rosser (1936) using Rosser's trick. The resulting theorem (incorporating Rosser's improvement) may be paraphrased in English as follows, where \"formal system\" includes the assumption that the system is effectively generated.\n\nFirst Incompleteness Theorem: \"Any consistent formal system \"F\" within which a certain amount of elementary arithmetic can be carried out is incomplete; i.e., there are statements of the language of \"F\" which can neither be proved nor disproved in \"F\".\" (Raatikainen 2015)\n\nThe unprovable statement \"G\" referred to by the theorem is often referred to as \"the Gödel sentence\" for the system \"F\". The proof constructs a particular Gödel sentence for the system \"F\", but there are infinitely many statements in the language of the system that share the same properties, such as the conjunction of the Gödel sentence and any logically valid sentence.\n\nEach effectively generated system has its own Gödel sentence. It is possible to define a larger system \"F’\" that contains the whole of \"F\" plus \"G\" as an additional axiom. This will not result in a complete system, because Gödel's theorem will also apply to \"F’\", and thus \"F’\" also cannot be complete. In this case, \"G\" is indeed a theorem in \"F’\", because it is an axiom. Because \"G\" states only that it is not provable in \"F\", no contradiction is presented by its provability within \"F’\". However, because the incompleteness theorem applies to \"F’\", there will be a new Gödel statement \"G\" for \"F’\", showing that \"F’\" is also incomplete. \"G\" will differ from \"G\" in that \"G\" will refer to \"F’\", rather than \"F\".\n\nThe Gödel sentence is designed to refer, indirectly, to itself. The sentence states that, when a particular sequence of steps is used to construct another sentence, that constructed sentence will not be provable in \"F\". However, the sequence of steps is such that the constructed sentence turns out to be \"G\" itself. In this way, the Gödel sentence \"G\" indirectly states its own unprovability within \"F\" (Smith 2007, p. 135).\n\nTo prove the first incompleteness theorem, Gödel demonstrated that the notion of provability within a system could be expressed purely in terms of arithmetical functions that operate on Gödel numbers of sentences of the system. Therefore, the system, which can prove certain facts about numbers, can also indirectly prove facts about its own statements, provided that it is effectively generated. Questions about the provability of statements within the system are represented as questions about the arithmetical properties of numbers themselves, which would be decidable by the system if it were complete.\n\nThus, although the Gödel sentence refers indirectly to sentences of the system \"F\", when read as an arithmetical statement the Gödel sentence directly refers only to natural numbers. It asserts that no natural number has a particular property, where that property is given by a primitive recursive relation (Smith 2007, p. 141). As such, the Gödel sentence can be written in the language of arithmetic with a simple syntactic form. In particular, it can be expressed as a formula in the language of arithmetic consisting of a number of leading universal quantifiers followed by a quantifier-free body (these formulas are at level formula_1 of the arithmetical hierarchy). Via the MRDP theorem, the Gödel sentence can be re-written as a statement that a particular polynomial in many variables with integer coefficients never takes the value zero when integers are substituted for its variables (Franzén 2005, p. 71).\n\nThe first incompleteness theorem shows that the Gödel sentence \"G\" of an appropriate formal theory \"F\" is unprovable in \"F\". Because, when interpreted as a statement about arithmetic, this unprovability is exactly what the sentence (indirectly) asserts, the Gödel sentence is, in fact, true (Smoryński 1977 p. 825; also see Franzén 2005 pp. 28–33). For this reason, the sentence \"G\" is often said to be \"true but unprovable.\" (Raatikainen 2015). However, since the Gödel sentence cannot itself formally specify its intended interpretation, the truth of the sentence \"G\" may only be arrived at via a meta-analysis from outside the system. In general, this meta-analysis can be carried out within the weak formal system known as primitive recursive arithmetic, which proves the implication Con(\"F\")→\"G\", where Con(\"F\") is a canonical sentence asserting the consistency of \"F\" (Smoryński 1977 p. 840, Kikuchi and Tanaka 1994 p. 403).\n\nAlthough the Gödel sentence of a consistent theory is true as a statement about the intended interpretation of arithmetic, the Gödel sentence will be false in some nonstandard models of arithmetic, as a consequence of Gödel's completeness theorem (Franzén 2005, p. 135). That theorem shows that, when a sentence is independent of a theory, the theory will have models in which the sentence is true and models in which the sentence is false. As described earlier, the Gödel sentence of a system \"F\" is an arithmetical statement which claims that no number exists with a particular property. The incompleteness theorem shows that this claim will be independent of the system \"F\", and the truth of the Gödel sentence follows from the fact that no standard natural number has the property in question. Any model in which the Gödel sentence is false must contain some element which satisfies the property within that model. Such a model must be \"nonstandard\" – it must contain elements that do not correspond to any standard natural number (Raatikainen 2015, Franzén 2005, p. 135).\n\nGödel specifically cites Richard's paradox and the liar paradox as semantical analogues to his syntactical incompleteness result in the introductory section of \"On Formally Undecidable Propositions in Principia Mathematica and Related Systems I\". The liar paradox is the sentence \"This sentence is false.\" An analysis of the liar sentence shows that it cannot be true (for then, as it asserts, it is false), nor can it be false (for then, it is true). A Gödel sentence \"G\" for a system \"F\" makes a similar assertion to the liar sentence, but with truth replaced by provability: \"G\" says \"\"G\" is not provable in the system \"F\".\" The analysis of the truth and provability of \"G\" is a formalized version of the analysis of the truth of the liar sentence.\n\nIt is not possible to replace \"not provable\" with \"false\" in a Gödel sentence because the predicate \"Q is the Gödel number of a false formula\" cannot be represented as a formula of arithmetic. This result, known as Tarski's undefinability theorem, was discovered independently both by Gödel, when he was working on the proof of the incompleteness theorem, and by the theorem's namesake, Alfred Tarski.\n\nCompared to the theorems stated in Gödel's 1931 paper, many contemporary statements of the incompleteness theorems are more general in two ways. These generalized statements are phrased to apply to a broader class of systems, and they are phrased to incorporate weaker consistency assumptions.\n\nGödel demonstrated the incompleteness of the system of \"Principia Mathematica\", a particular system of arithmetic, but a parallel demonstration could be given for any effective system of a certain expressiveness. Gödel commented on this fact in the introduction to his paper, but restricted the proof to one system for concreteness. In modern statements of the theorem, it is common to state the effectiveness and expressiveness conditions as hypotheses for the incompleteness theorem, so that it is not limited to any particular formal system. The terminology used to state these conditions was not yet developed in 1931 when Gödel published his results.\n\nGödel's original statement and proof of the incompleteness theorem requires the assumption that the system is not just consistent but \"ω-consistent\". A system is ω-consistent if it is not ω-inconsistent, and is ω-inconsistent if there is a predicate \"P\" such that for every specific natural number \"m\" the system proves ~\"P\"(\"m\"), and yet the system also proves that there exists a natural number \"n\" such that \"P\"(\"n\"). That is, the system says that a number with property \"P\" exists while denying that it has any specific value. The ω-consistency of a system implies its consistency, but consistency does not imply ω-consistency. J. Barkley Rosser (1936) strengthened the incompleteness theorem by finding a variation of the proof (Rosser's trick) that only requires the system to be consistent, rather than ω-consistent. This is mostly of technical interest, because all true formal theories of arithmetic (theories whose axioms are all true statements about natural numbers) are ω-consistent, and thus Gödel's theorem as originally stated applies to them. The stronger version of the incompleteness theorem that only assumes consistency, rather than ω-consistency, is now commonly known as Gödel's incompleteness theorem and as the Gödel–Rosser theorem.\n\nFor each formal system \"F\" containing basic arithmetic, it is possible to canonically define a formula Cons(\"F\") expressing the consistency of \"F\". This formula expresses the property that \"there does not exist a natural number coding a formal derivation within the system \"F\" whose conclusion is a syntactic contradiction.\" The syntactic contradiction is often taken to be \"0=1\", in which case Cons(\"F\") states \"there is no natural number that codes a derivation of '0=1' from the axioms of \"F\".\"\n\nGödel's second incompleteness theorem shows that, under general assumptions, this canonical consistency statement Cons(\"F\") will not be provable in \"F\". The theorem first appeared as \"Theorem XI\" in Gödel's 1931 paper \"On Formally Undecidable Propositions in Principia Mathematica and Related Systems I\". In the following statement, the term \"formalized system\" also includes an assumption that \"F\" is effectively axiomatized. \n\nSecond Incompleteness Theorem: \"Assume \"F\" is a consistent formalized system which contains elementary arithmetic. Then formula_2.\" (Raatikainen 2015)\nThis theorem is stronger than the first incompleteness theorem because the statement constructed in the first incompleteness theorem does not directly express the consistency of the system. The proof of the second incompleteness theorem is obtained by formalizing the proof of the first incompleteness theorem within the system \"F\" itself.\n\nThere is a technical subtlety in the second incompleteness theorem regarding the method of expressing the consistency of \"F\" as a formula in the language of \"F\". There are many ways to express the consistency of a system, and not all of them lead to the same result. The formula Cons(\"F\") from the second incompleteness theorem is a particular expression of consistency.\n\nOther formalizations of the claim that \"F\" is consistent may be inequivalent in \"F\", and some may even be provable. For example, first-order Peano arithmetic (PA) can prove that \"the largest consistent subset of PA\" is consistent. But, because PA is consistent, the largest consistent subset of PA is just PA, so in this sense PA \"proves that it is consistent\". What PA does not prove is that the largest consistent subset of PA is, in fact, the whole of PA. (The term \"largest consistent subset of PA\" is meant here to be the largest consistent initial segment of the axioms of PA under some particular effective enumeration).\n\nThe standard proof of the second incompleteness theorem assumes that the provability predicate Prov(\"P\") satisfies the Hilbert–Bernays provability conditions. Letting #(\"P\") represent the Gödel number of a formula \"P\", the derivability conditions say:\n\n\nThere are systems, such as Robinson arithmetic, which are strong enough to meet the assumptions of the first incompleteness theorem, but which do not prove the Hilbert—Bernays conditions. Peano arithmetic, however, is strong enough to verify these conditions, as are all theories stronger than Peano arithmetic.\n\nGödel's second incompleteness theorem also implies that a system \"F\" satisfying the technical conditions outlined above cannot prove the consistency of any system \"F\" that proves the consistency of \"F\". This is because such a system \"F\" can prove that if \"F\" proves the consistency of \"F\", then \"F\" is in fact consistent. For the claim that \"F\" is consistent has form \"for all numbers \"n\", \"n\" has the decidable property of not being a code for a proof of contradiction in \"F\"\". If \"F\" were in fact inconsistent, then \"F\" would prove for some \"n\" that \"n\" is the code of a contradiction in \"F\". But if \"F\" also proved that \"F\" is consistent (that is, that there is no such n), then it would itself be inconsistent. This reasoning can be formalized in \"F\" to show that if \"F\" is consistent, then \"F\" is consistent. Since, by second incompleteness theorem, \"F\" does not prove its consistency, it cannot prove the consistency of \"F\" either.\n\nThis corollary of the second incompleteness theorem shows that there is no hope of proving, for example, the consistency of Peano arithmetic using any finitistic means that can be formalized in a system the consistency of which is provable in Peano arithmetic (PA). For example, the system of primitive recursive arithmetic (PRA), which is widely accepted as an accurate formalization of finitistic mathematics, is provably consistent in PA. Thus PRA cannot prove the consistency of PA. This fact is generally seen to imply that Hilbert's program, which aimed to justify the use of \"ideal\" (infinitistic) mathematical principles in the proofs of \"real\" (finitistic) mathematical statements by giving a finitistic proof that the ideal principles are consistent, cannot be carried out (Franzén 2005, p. 106).\n\nThe corollary also indicates the epistemological relevance of the second incompleteness theorem. It would actually provide no interesting information if a system \"F\" proved its consistency. This is because inconsistent theories prove everything, including their consistency. Thus a consistency proof of \"F\" in \"F\" would give us no clue as to whether \"F\" really is consistent; no doubts about the consistency of \"F\" would be resolved by such a consistency proof. The interest in consistency proofs lies in the possibility of proving the consistency of a system \"F\" in some system \"F’\" that is in some sense less doubtful than \"F\" itself, for example weaker than \"F\". For many naturally occurring theories \"F\" and \"F’\", such as \"F\" = Zermelo–Fraenkel set theory and \"F’\" = primitive recursive arithmetic, the consistency of \"F’\" is provable in \"F\", and thus \"F’\" cannot prove the consistency of \"F\" by the above corollary of the second incompleteness theorem.\n\nThe second incompleteness theorem does not rule out consistency proofs altogether, only consistency proofs that can be formalized in the system that is proved consistent. For example, Gerhard Gentzen proved the consistency of Peano arithmetic in a different system that includes an axiom asserting that the ordinal called ε is wellfounded; see Gentzen's consistency proof. Gentzen's theorem spurred the development of ordinal analysis in proof theory.\n\nThere are two distinct senses of the word \"undecidable\" in mathematics and computer science. The first of these is the proof-theoretic sense used in relation to Gödel's theorems, that of a statement being neither provable nor refutable in a specified deductive system. The second sense, which will not be discussed here, is used in relation to computability theory and applies not to statements but to decision problems, which are countably infinite sets of questions each requiring a yes or no answer. Such a problem is said to be undecidable if there is no computable function that correctly answers every question in the problem set (see undecidable problem).\n\nBecause of the two meanings of the word undecidable, the term independent is sometimes used instead of undecidable for the \"neither provable nor refutable\" sense.\n\nUndecidability of a statement in a particular deductive system does not, in and of itself, address the question of whether the truth value of the statement is well-defined, or whether it can be determined by other means. Undecidability only implies that the particular deductive system being considered does not prove the truth or falsity of the statement. Whether there exist so-called \"absolutely undecidable\" statements, whose truth value can never be known or is ill-specified, is a controversial point in the philosophy of mathematics.\n\nThe combined work of Gödel and Paul Cohen has given two concrete examples of undecidable statements (in the first sense of the term): The continuum hypothesis can neither be proved nor refuted in ZFC (the standard axiomatization of set theory), and the axiom of choice can neither be proved nor refuted in ZF (which is all the ZFC axioms \"except\" the axiom of choice). These results do not require the incompleteness theorem. Gödel proved in 1940 that neither of these statements could be disproved in ZF or ZFC set theory. In the 1960s, Cohen proved that neither is provable from ZF, and the continuum hypothesis cannot be proved from ZFC.\n\nIn 1973, Saharon Shelah showed that the Whitehead problem in group theory is undecidable, in the first sense of the term, in standard set theory.\n\nGregory Chaitin produced undecidable statements in algorithmic information theory and proved another incompleteness theorem in that setting. Chaitin's incompleteness theorem states that for any system that can represent enough arithmetic, there is an upper bound \"c\" such that no specific number can be proved in that system to have Kolmogorov complexity greater than \"c\". While Gödel's theorem is related to the liar paradox, Chaitin's result is related to Berry's paradox.\n\nThese are natural mathematical equivalents of the Gödel \"true but undecidable\" sentence. They can be proved in a larger system which is generally accepted as a valid form of reasoning, but are undecidable in a more limited system such as Peano Arithmetic.\n\nIn 1977, Paris and Harrington proved that the Paris–Harrington principle, a version of the infinite Ramsey theorem, is undecidable in (first-order) Peano arithmetic, but can be proved in the stronger system of second-order arithmetic. Kirby and Paris later showed that Goodstein's theorem, a statement about sequences of natural numbers somewhat simpler than the Paris–Harrington principle, is also undecidable in Peano arithmetic.\n\nKruskal's tree theorem, which has applications in computer science, is also undecidable from Peano arithmetic but provable in set theory. In fact Kruskal's tree theorem (or its finite form) is undecidable in a much stronger system codifying the principles acceptable based on a philosophy of mathematics called predicativism. The related but more general graph minor theorem (2003) has consequences for computational complexity theory.\n\nThe incompleteness theorem is closely related to several results about undecidable sets in recursion theory.\n\nStephen Cole Kleene (1943) presented a proof of Gödel's incompleteness theorem using basic results of computability theory. One such result shows that the halting problem is undecidable: there is no computer program that can correctly determine, given any program \"P\" as input, whether \"P\" eventually halts when run with a particular given input. Kleene showed that the existence of a complete effective system of arithmetic with certain consistency properties would force the halting problem to be decidable, a contradiction. This method of proof has also been presented by Shoenfield (1967, p. 132); Charlesworth (1980); and Hopcroft and Ullman (1979).\n\nFranzén (2005, p. 73) explains how Matiyasevich's solution to Hilbert's 10th problem can be used to obtain a proof to Gödel's first incompleteness theorem. Matiyasevich proved that there is no algorithm that, given a multivariate polynomial p(x, x...,x) with integer coefficients, determines whether there is an integer solution to the equation \"p\" = 0. Because polynomials with integer coefficients, and integers themselves, are directly expressible in the language of arithmetic, if a multivariate integer polynomial equation \"p\" = 0 does have a solution in the integers then any sufficiently strong system of arithmetic \"T\" will prove this. Moreover, if the system \"T\" is ω-consistent, then it will never prove that a particular polynomial equation has a solution when in fact there is no solution in the integers. Thus, if \"T\" were complete and ω-consistent, it would be possible to determine algorithmically whether a polynomial equation has a solution by merely enumerating proofs of \"T\" until either \"\"p\" has a solution\" or \"\"p\" has no solution\" is found, in contradiction to Matiyasevich's theorem. Moreover, for each consistent effectively generated system \"T\", it is possible to effectively generate a multivariate polynomial \"p\" over the integers such that the equation \"p\" = 0 has no solutions over the integers, but the lack of solutions cannot be proved in \"T\" (Davis 2006:416, Jones 1980).\n\nSmorynski (1977, p. 842) shows how the existence of recursively inseparable sets can be used to prove the first incompleteness theorem. This proof is often extended to show that systems such as Peano arithmetic are essentially undecidable (see Kleene 1967, p. 274).\n\nChaitin's incompleteness theorem gives a different method of producing independent sentences, based on Kolmogorov complexity. Like the proof presented by Kleene that was mentioned above, Chaitin's theorem only applies to theories with the additional property that all their axioms are true in the standard model of the natural numbers. Gödel's incompleteness theorem is distinguished by its applicability to consistent theories that nonetheless include statements that are false in the standard model; these theories are known as ω-inconsistent.\n\nThe proof by contradiction has three essential parts. To begin, choose a formal system that meets the proposed criteria:\n\n\nThe main problem in fleshing out the proof described above is that it seems at first that to construct a statement \"p\" that is equivalent to \"\"p\" cannot be proved\", \"p\" would somehow have to contain a reference to \"p\", which could easily give rise to an infinite regress. Gödel's ingenious technique is to show that statements can be matched with numbers (often called the arithmetization of syntax) in such a way that \"proving a statement\" can be replaced with \"testing whether a number has a given property\". This allows a self-referential formula to be constructed in a way that avoids any infinite regress of definitions. The same technique was later used by Alan Turing in his work on the Entscheidungsproblem.\n\nIn simple terms, a method can be devised so that every formula or statement that can be formulated in the system gets a unique number, called its Gödel number, in such a way that it is possible to mechanically convert back and forth between formulas and Gödel numbers. The numbers involved might be very long indeed (in terms of number of digits), but this is not a barrier; all that matters is that such numbers can be constructed. A simple example is the way in which English is stored as a sequence of numbers in computers using ASCII or Unicode:\n\nIn principle, proving a statement true or false can be shown to be equivalent to proving that the number matching the statement does or doesn't have a given property. Because the formal system is strong enough to support reasoning about \"numbers in general\", it can support reasoning about \"numbers that represent formulae and statements\" as well. Crucially, because the system can support reasoning about \"properties of numbers\", the results are equivalent to reasoning about \"provability of their equivalent statements\".\n\nHaving shown that in principle the system can indirectly make statements about provability, by analyzing properties of those numbers representing statements it is now possible to show how to create a statement that actually does this.\n\nA formula \"F\"(\"x\") that contains exactly one free variable \"x\" is called a \"statement form\" or \"class-sign\". As soon as \"x\" is replaced by a specific number, the statement form turns into a \"bona fide\" statement, and it is then either provable in the system, or not. For certain formulas one can show that for every natural number n, F(n) is true if and only if it can be proved (the precise requirement in the original proof is weaker, but for the proof sketch this will suffice). In particular, this is true for every specific arithmetic operation between a finite number of natural numbers, such as \"2×3=6\".\n\nStatement forms themselves are not statements and therefore cannot be proved or disproved. But every statement form \"F\"(\"x\") can be assigned a Gödel number denoted by G(\"F\"). The choice of the free variable used in the form \"F\"(\"x\") is not relevant to the assignment of the Gödel number G(\"F\").\n\nThe notion of provability itself can also be encoded by Gödel numbers, in the following way: since a proof is a list of statements which obey certain rules, the Gödel number of a proof can be defined. Now, for every statement \"p\", one may ask whether a number \"x\" is the Gödel number of its proof. The relation between the Gödel number of \"p\" and \"x\", the potential Gödel number of its proof, is an arithmetical relation between two numbers. Therefore, there is a statement form Bew(\"y\") that uses this arithmetical relation to state that a Gödel number of a proof of \"y\" exists:\nThe name Bew is short for \"beweisbar\", the German word for \"provable\"; this name was originally used by Gödel to denote the provability formula just described. Note that \"Bew(\"y\")\" is merely an abbreviation that represents a particular, very long, formula in the original language of \"T\"; the string \"Bew\" itself is not claimed to be part of this language.\n\nAn important feature of the formula Bew(\"y\") is that if a statement \"p\" is provable in the system then Bew(G(\"p\")) is also provable. This is because any proof of \"p\" would have a corresponding Gödel number, the existence of which causes Bew(G(\"p\")) to be satisfied.\n\nThe next step in the proof is to obtain a statement which, indirectly, asserts its own unprovability. Although Gödel constructed this statement directly, the existence of at least one such statement follows from the diagonal lemma, which says that for any sufficiently strong formal system and any statement form \"F\" there is a statement \"p\" such that the system proves\nBy letting \"F\" be the negation of Bew(\"x\"), we obtain the theorem\nand the \"p\" defined by this roughly states that its own Gödel number is the Gödel number of an unprovable formula.\n\nThe statement \"p\" is not literally equal to ~Bew(G(\"p\")); rather, \"p\" states that if a certain calculation is performed, the resulting Gödel number will be that of an unprovable statement. But when this calculation is performed, the resulting Gödel number turns out to be the Gödel number of \"p\" itself. This is similar to the following sentence in English:\nThis sentence does not directly refer to itself, but when the stated transformation is made the original sentence is obtained as a result, and thus this sentence indirectly asserts its own unprovability. The proof of the diagonal lemma employs a similar method.\n\nNow, assume that the axiomatic system is ω-consistent, and let \"p\" be the statement obtained in the previous section.\n\nIf \"p\" were provable, then Bew(G(\"p\")) would be provable, as argued above. But \"p\" asserts the negation of Bew(G(\"p\")). Thus the system would be inconsistent, proving both a statement and its negation. This contradiction shows that \"p\" cannot be provable.\n\nIf the negation of \"p\" were provable, then Bew(G(\"p\")) would be provable (because \"p\" was constructed to be equivalent to the negation of Bew(G(\"p\"))). However, for each specific number \"x\", \"x\" cannot be the Gödel number of the proof of \"p\", because \"p\" is not provable (from the previous paragraph). Thus on one hand the system proves there is a number with a certain property (that it is the Gödel number of the proof of \"p\"), but on the other hand, for every specific number \"x\", we can prove that it does not have this property. This is impossible in an ω-consistent system. Thus the negation of \"p\" is not provable.\n\nThus the statement \"p\" is undecidable in our axiomatic system: it can neither be proved nor disproved within the system.\n\nIn fact, to show that \"p\" is not provable only requires the assumption that the system is consistent. The stronger assumption of ω-consistency is required to show that the negation of \"p\" is not provable. Thus, if \"p\" is constructed for a particular system:\n\nIf one tries to \"add the missing axioms\" to avoid the incompleteness of the system, then one has to add either \"p\" or \"not \"p\"\" as axioms. But then the definition of \"being a Gödel number of a proof\" of a statement changes. which means that the formula Bew(\"x\") is now different. Thus when we apply the diagonal lemma to this new Bew, we obtain a new statement \"p\", different from the previous one, which will be undecidable in the new system if it is ω-consistent.\n\nGeorge Boolos (1989) sketches an alternative proof of the first incompleteness theorem that uses Berry's paradox rather than the liar paradox to construct a true but unprovable formula. A similar proof method was independently discovered by Saul Kripke (Boolos 1998, p. 383). Boolos's proof proceeds by constructing, for any computably enumerable set \"S\" of true sentences of arithmetic, another sentence which is true but not contained in \"S\". This gives the first incompleteness theorem as a corollary. According to Boolos, this proof is interesting because it provides a \"different sort of reason\" for the incompleteness of effective, consistent theories of arithmetic (Boolos 1998, p. 388).\n\nThe incompleteness theorems are among a relatively small number of nontrivial theorems that have been transformed into formalized theorems that can be completely verified by proof assistant software. Gödel's original proofs of the incompleteness theorems, like most mathematical proofs, were written in natural language intended for human readers.\n\nComputer-verified proofs of versions of the first incompleteness theorem were announced by Natarajan Shankar in 1986 using Nqthm (Shankar 1994), by Russell O'Connor in 2003 using Coq (O'Connor 2005) and by John Harrison in 2009 using HOL Light (Harrison 2009). A computer-verified proof of both incompleteness theorems was announced by Lawrence Paulson in 2013 using Isabelle (Paulson 2014).\n\nThe main difficulty in proving the second incompleteness theorem is to show that various facts about provability used in the proof of the first incompleteness theorem can be formalized within the system using a formal predicate for provability. Once this is done, the second incompleteness theorem follows by formalizing the entire proof of the first incompleteness theorem within the system itself.\n\nLet \"p\" stand for the undecidable sentence constructed above, and assume that the consistency of the system can be proved from within the system itself. The demonstration above shows that if the system is consistent, then \"p\" is not provable. The proof of this implication can be formalized within the system, and therefore the statement \"\"p\" is not provable\", or \"not \"P\"(\"p\")\" can be proved in the system.\n\nBut this last statement is equivalent to \"p\" itself (and this equivalence can be proved in the system), so \"p\" can be proved in the system. This contradiction shows that the system must be inconsistent.\n\nThe incompleteness results affect the philosophy of mathematics, particularly versions of formalism, which use a single system of formal logic to define their principles.\n\nThe incompleteness theorem is sometimes thought to have severe consequences for the program of logicism proposed by Gottlob Frege and Bertrand Russell, which aimed to define the natural numbers in terms of logic (Hellman 1981, p. 451–468). Bob Hale and Crispin Wright argue that it is not a problem for logicism because the incompleteness theorems apply equally to first order logic as they do to arithmetic. They argue that only those who believe that the natural numbers are to be defined in terms of first order logic have this problem.\n\nMany logicians believe that Gödel's incompleteness theorems struck a fatal blow to David Hilbert's second problem, which asked for a finitary consistency proof for mathematics. The second incompleteness theorem, in particular, is often viewed as making the problem impossible. Not all mathematicians agree with this analysis, however, and the status of Hilbert's second problem is not yet decided (see \"Modern viewpoints on the status of the problem\").\n\nAuthors including the philosopher J. R. Lucas and physicist Roger Penrose have debated what, if anything, Gödel's incompleteness theorems imply about human intelligence. Much of the debate centers on whether the human mind is equivalent to a Turing machine, or by the Church–Turing thesis, any finite machine at all. If it is, and if the machine is consistent, then Gödel's incompleteness theorems would apply to it.\n\nHilary Putnam (1960) suggested that while Gödel's theorems cannot be applied to humans, since they make mistakes and are therefore inconsistent, it may be applied to the human faculty of science or mathematics in general. Assuming that it is consistent, either its consistency cannot be proved or it cannot be represented by a Turing machine.\n\nAvi Wigderson (2010) has proposed that the concept of mathematical \"knowability\" should be based on computational complexity rather than logical decidability. He writes that \"when \"knowability\" is interpreted by modern standards, namely via computational complexity, the Gödel phenomena are very much with us.\"\n\nDouglas Hofstadter, in his books Gödel, Escher, Bach and I Am a Strange Loop, cites Gödel's theorems as an example of what he calls a \"strange loop\", a hierarchical, self-referential structure existing within an axiomatic formal system. He argues that this is the same kind of structure which gives rise to consciousness, the sense of \"I\", in the human mind. While the self-reference in Gödel's theorem comes from the Gödel sentence asserting its own unprovability within the formal system of Principia Mathematica, the self-reference in the human mind comes from the way in which the brain abstracts and categorises stimuli into \"symbols\", or groups of neurons which respond to concepts, in what is effectively also a formal system, eventually giving rise to symbols modelling the concept of the very entity doing the perception.\nHofstadter argues that a strange loop in a sufficiently complex formal system can give rise to a \"downward\" or \"upside-down\" causality, a situation in which the normal hierarchy of cause-and-effect is flipped upside-down. In the case of Gödel's theorem, this manifests, in short, as the following:\n\n\"Merely from knowing the formula's meaning, one can infer its truth or falsity without any effort to derive it in the old-fashioned way, which requires one to trudge methodically \"upwards\" from the axioms. This is not just peculiar; it is astonishing. Normally, one cannot merely look at what a mathematical conjecture says and simply appeal to the content of that statement on its own to deduce whether the statement is true or false.\" (\"I Am a Strange Loop.\")\n\nIn the case of the mind, a far more complex formal system, this \"downward causality\" manifests, in Hofstadter's view, as the ineffable human instinct that the causality of our minds lies on the high level of desires, concepts, personalities, thoughts and ideas, rather than on the low level of interactions between neurons or even fundamental particles, even though according to physics the latter seems to possess the causal power.\n\n\"There is thus a curious upside-downness to our normal human way of perceiving the world: we are built to perceive “big stuff” rather than “small stuff”, even though the domain of the tiny seems to be where the actual motors driving reality reside.\" (\"I Am a Strange Loop.\")\n\nAlthough Gödel's theorems are usually studied in the context of classical logic, they also have a role in the study of paraconsistent logic and of inherently contradictory statements (\"dialetheia\"). Graham Priest (1984, 2006) argues that replacing the notion of formal proof in Gödel's theorem with the usual notion of informal proof can be used to show that naive mathematics is inconsistent, and uses this as evidence for dialetheism. The cause of this inconsistency is the inclusion of a truth predicate for a system within the language of the system (Priest 2006:47). Stewart Shapiro (2002) gives a more mixed appraisal of the applications of Gödel's theorems to dialetheism.\n\nAppeals and analogies are sometimes made to the incompleteness theorems in support of arguments that go beyond mathematics and logic. Several authors have commented negatively on such extensions and interpretations, including Torkel Franzén (2005); Panu Raatikainen (2005); Alan Sokal and Jean Bricmont (1999); and Ophelia Benson and Jeremy Stangroom (2006). Bricmont and Stangroom (2006, p. 10), for example, quote from Rebecca Goldstein's comments on the disparity between Gödel's avowed Platonism and the anti-realist uses to which his ideas are sometimes put. Sokal and Bricmont (1999, p. 187) criticize Régis Debray's invocation of the theorem in the context of sociology; Debray has defended this use as metaphorical (ibid.).\n\nAfter Gödel published his proof of the completeness theorem as his doctoral thesis in 1929, he turned to a second problem for his habilitation. His original goal was to obtain a positive solution to Hilbert's second problem (Dawson 1997, p. 63). At the time, theories of the natural numbers and real numbers similar to second-order arithmetic were known as \"analysis\", while theories of the natural numbers alone were known as \"arithmetic\".\n\nGödel was not the only person working on the consistency problem. Ackermann had published a flawed consistency proof for analysis in 1925, in which he attempted to use the method of ε-substitution originally developed by Hilbert. Later that year, von Neumann was able to correct the proof for a system of arithmetic without any axioms of induction. By 1928, Ackermann had communicated a modified proof to Bernays; this modified proof led Hilbert to announce his belief in 1929 that the consistency of arithmetic had been demonstrated and that a consistency proof of analysis would likely soon follow. After the publication of the incompleteness theorems showed that Ackermann's modified proof must be erroneous, von Neumann produced a concrete example showing that its main technique was unsound (Zach 2006, p. 418, Zach 2003, p. 33).\n\nIn the course of his research, Gödel discovered that although a sentence which asserts its own falsehood leads to paradox, a sentence that asserts its own non-provability does not. In particular, Gödel was aware of the result now called Tarski's indefinability theorem, although he never published it. Gödel announced his first incompleteness theorem to Carnap, Feigel and Waismann on August 26, 1930; all four would attend a key conference in Königsberg the following week.\n\nThe 1930 Königsberg conference was a joint meeting of three academic societies, with many of the key logicians of the time in attendance. Carnap, Heyting, and von Neumann delivered one-hour addresses on the mathematical philosophies of logicism, intuitionism, and formalism, respectively (Dawson 1996, p. 69). The conference also included Hilbert's retirement address, as he was leaving his position at the University of Göttingen. Hilbert used the speech to argue his belief that all mathematical problems can be solved. He ended his address by saying,\nThis speech quickly became known as a summary of Hilbert's beliefs on mathematics (its final six words, \"\"Wir müssen wissen. Wir werden wissen!\", were used as Hilbert's epitaph in 1943). Although Gödel was likely in attendance for Hilbert's address, the two never met face to face (Dawson 1996, p. 72).\n\nGödel announced his first incompleteness theorem at a roundtable discussion session on the third day of the conference. The announcement drew little attention apart from that of von Neumann, who pulled Gödel aside for conversation. Later that year, working independently with knowledge of the first incompleteness theorem, von Neumann obtained a proof of the second incompleteness theorem, which he announced to Gödel in a letter dated November 20, 1930 (Dawson 1996, p. 70). Gödel had independently obtained the second incompleteness theorem and included it in his submitted manuscript, which was received by \"Monatshefte für Mathematik\" on November 17, 1930.\n\nGödel's paper was published in the \"Monatshefte\" in 1931 under the title \"Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I\"\" (\"On Formally Undecidable Propositions in Principia Mathematica and Related Systems I\"). As the title implies, Gödel originally planned to publish a second part of the paper in the next volume of the \"Monatshefte\"; the prompt acceptance of the first paper was one reason he changed his plans (van Heijenoort 1967:328, footnote 68a).\n\nGödel gave a series of lectures on his theorems at Princeton in 1933–1934 to an audience that included Church, Kleene, and Rosser. By this time, Gödel had grasped that the key property his theorems required is that the system must be effective (at the time, the term \"general recursive\" was used). Rosser proved in 1936 that the hypothesis of ω-consistency, which was an integral part of Gödel's original proof, could be replaced by simple consistency, if the Gödel sentence was changed in an appropriate way. These developments left the incompleteness theorems in essentially their modern form.\n\nGentzen published his consistency proof for first-order arithmetic in 1936. Hilbert accepted this proof as \"finitary\" although (as Gödel's theorem had already shown) it cannot be formalized within the system of arithmetic that is being proved consistent.\n\nThe impact of the incompleteness theorems on Hilbert's program was quickly realized. Bernays included a full proof of the incompleteness theorems in the second volume of \"Grundlagen der Mathematik\" (1939), along with additional results of Ackermann on the ε-substitution method and Gentzen's consistency proof of arithmetic. This was the first full published proof of the second incompleteness theorem.\n\nPaul Finsler (1926) used a version of Richard's paradox to construct an expression that was false but unprovable in a particular, informal framework he had developed. Gödel was unaware of this paper when he proved the incompleteness theorems (Collected Works Vol. IV., p. 9). Finsler wrote to Gödel in 1931 to inform him about this paper, which Finsler felt had priority for an incompleteness theorem. Finsler's methods did not rely on formalized provability, and had only a superficial resemblance to Gödel's work (van Heijenoort 1967:328). Gödel read the paper but found it deeply flawed, and his response to Finsler laid out concerns about the lack of formalization (Dawson:89). Finsler continued to argue for his philosophy of mathematics, which eschewed formalization, for the remainder of his career.\n\nIn September 1931, Ernst Zermelo wrote to Gödel to announce what he described as an \"essential gap\" in Gödel's argument (Dawson:76). In October, Gödel replied with a 10-page letter (Dawson:76, Grattan-Guinness:512-513), where he pointed out that Zermelo mistakenly assumed that the notion of truth in a system is definable in that system (which is not true in general by Tarski's undefinability theorem). But Zermelo did not relent and published his criticisms in print with \"a rather scathing paragraph on his young competitor\" (Grattan-Guinness:513). Gödel decided that to pursue the matter further was pointless, and Carnap agreed (Dawson:77). Much of Zermelo's subsequent work was related to logics stronger than first-order logic, with which he hoped to show both the consistency and categoricity of mathematical theories.\n\nLudwig Wittgenstein wrote several passages about the incompleteness theorems that were published posthumously in his 1953 \"Remarks on the Foundations of Mathematics\", in particular one section sometimes called the \"notorious paragraph\" where he seems to confuse the notions of \"true\" and \"provable\" in Russell's system. Gödel was a member of the Vienna Circle during the period in which Wittgenstein's early ideal language philosophy and Tractatus Logico-Philosophicus dominated the circle's thinking. There has been some controversy about whether Wittgenstein misunderstood the incompleteness theorem or just expressed himself unclearly. Writings in Gödel's Nachlass express the belief that Wittgenstein misread his ideas.\n\nMultiple commentators have read Wittgenstein as misunderstanding Gödel (Rodych 2003), although Juliet Floyd and Hilary Putnam (2000), as well as Graham Priest (2004) have provided textual readings arguing that most commentary misunderstands Wittgenstein. On their release, Bernays, Dummett, and Kreisel wrote separate reviews on Wittgenstein's remarks, all of which were extremely negative (Berto 2009:208). The unanimity of this criticism caused Wittgenstein's remarks on the incompleteness theorems to have little impact on the logic community. In 1972, Gödel stated: \"Has Wittgenstein lost his mind? Does he mean it seriously? He intentionally utters trivially nonsensical statements\" (Wang 1996:179), and wrote to Karl Menger that Wittgenstein's comments demonstrate a misunderstanding of the incompleteness theorems writing:\n\nSince the publication of Wittgenstein's \"Nachlass\" in 2000, a series of papers in philosophy have sought to evaluate whether the original criticism of Wittgenstein's remarks was justified. Floyd and Putnam (2000) argue that Wittgenstein had a more complete understanding of the incompleteness theorem than was previously assumed. They are particularly concerned with the interpretation of a Gödel sentence for an ω-inconsistent system as actually saying \"I am not provable\", since the system has no models in which the provability predicate corresponds to actual provability. Rodych (2003) argues that their interpretation of Wittgenstein is not historically justified, while Bays (2004) argues against Floyd and Putnam's philosophical analysis of the provability predicate. Berto (2009) explores the relationship between Wittgenstein's writing and theories of paraconsistent logic.\n\n\n\nNone of the following agree in all translated words and in typography. The typography is a serious matter, because Gödel expressly wished to emphasize \"those metamathematical notions that had been defined in their usual sense before . . .\" (van Heijenoort 1967:595). Three translations exist. Of the first John Dawson states that: \"The Meltzer translation was seriously deficient and received a devastating review in the \"Journal of Symbolic Logic\"; \"Gödel also complained about Braithwaite's commentary (Dawson 1997:216). \"Fortunately, the Meltzer translation was soon supplanted by a better one prepared by Elliott Mendelson for Martin Davis's anthology \"The Undecidable\" . . . he found the translation \"not quite so good\" as he had expected . . . [but because of time constraints he] agreed to its publication\" (ibid). (In a footnote Dawson states that \"he would regret his compliance, for the published volume was marred throughout by sloppy typography and numerous misprints\" (ibid)). Dawson states that \"The translation that Gödel favored was that by Jean van Heijenoort\" (ibid). For the serious student another version exists as a set of lecture notes recorded by Stephen Kleene and J. B. Rosser \"during lectures given by Gödel at to the Institute for Advanced Study during the spring of 1934\" (cf commentary by Davis 1965:39 and beginning on p. 41); this version is titled \"On Undecidable Propositions of Formal Mathematical Systems\". In their order of publication:\n\n\n\n\n\n"}
{"id": "1698977", "url": "https://en.wikipedia.org/wiki?curid=1698977", "title": "Homogeneous polynomial", "text": "Homogeneous polynomial\n\nIn mathematics, a homogeneous polynomial is a polynomial whose nonzero terms all have the same degree. For example, formula_1 is a homogeneous polynomial of degree 5, in two variables; the sum of the exponents in each term is always 5. The polynomial formula_2 is not homogeneous, because the sum of exponents does not match from term to term. A polynomial is homogeneous if and only if it defines a homogeneous function. An algebraic form, or simply form, is a function defined by a homogeneous polynomial. A binary form is a form in two variables. A \"form\" is also a function defined on a vector space, which may be expressed as a homogeneous function of the coordinates over any basis.\n\nA polynomial of degree 0 is always homogeneous; it is simply an element of the field or ring of the coefficients, usually called a constant or a scalar. A form of degree 1 is a linear form. A form of degree 2 is a quadratic form. In geometry, the Euclidean distance is the square root of a quadratic form.\n\nHomogeneous polynomials are ubiquitous in mathematics and physics. They play a fundamental role in algebraic geometry, as a projective algebraic variety is defined as the set of the common zeros of a set of homogeneous polynomials.\n\nA homogeneous polynomial defines a homogeneous function. This means that, if a multivariate polynomial \"P\" is homogeneous of degree \"d\", then\nfor every formula_4 in any field containing the coefficients of \"P\". Conversely, if the above relation is true for infinitely many formula_4 then the polynomial is homogeneous of degree \"d\".\n\nIn particular, if \"P\" is homogeneous then \nfor every formula_7 This property is fundamental in the definition of a projective variety.\n\nAny nonzero polynomial may be decomposed, in a unique way, as a sum of homogeneous polynomials of different degrees, which are called the homogeneous components of the polynomial. \n\nGiven a polynomial ring formula_8 over a field (or, more generally, a ring) \"K\", the homogeneous polynomials of degree \"d\" form\na vector space (or a module), commonly denoted formula_9 The above unique decomposition means that formula_10 is the direct sum of the formula_11 (sum over all nonnegative integers).\n\nThe dimension of the vector space (or free module) formula_11 is the number of different monomials of degree \"d\" in \"n\" variables (that is the maximal number of nonzero terms in a homogeneous polynomial of degree \"d\" in \"n\" variables). It is equal to the binomial coefficient\n\nHomogeneous polynomial satisfy Euler's identity for homogeneous functions. That is, if is a homogeneous polynomial of degree in the indeterminates formula_14 one has, whichever is the commutative ring of the coefficients,\nwhere formula_16 denotes the formal partial derivative of with respect to formula_17\n\nA non-homogeneous polynomial \"P\"(\"x\"...,\"x\") can be homogenized by introducing an additional variable \"x\" and defining the homogeneous polynomial sometimes denoted \"P\":\nwhere \"d\" is the degree of \"P\". For example, if\nthen\n\nA homogenized polynomial can be dehomogenized by setting the additional variable \"x\" = 1. That is\n\n"}
{"id": "5273654", "url": "https://en.wikipedia.org/wiki?curid=5273654", "title": "Jacob Klein (philosopher)", "text": "Jacob Klein (philosopher)\n\nJacob Klein (March 3, 1899 – July 16, 1978) was a Russian-American philosopher and interpreter of Plato, who worked extensively on the nature and historical origin of modern symbolic mathematics.\n\nKlein was born in Libava, Russian Empire. He studied at Berlin and Marburg, where he received his Ph.D. in 1922. A student of Nicolai Hartmann, Martin Heidegger, and Edmund Husserl, he later taught at St. John's College in Annapolis, Maryland from 1937 until his death. He served as dean from 1949 to 1958.\n\nKlein was affectionately known as Jasha. He was one of the world's preeminent interpreters of Plato and the Platonic tradition. As one of many Jewish scholars who were no longer safe in Europe, he fled the Nazis. .\n\nThe central thesis of his work \"Greek Mathematical Thought and the Origin of Algebra\" is that the modern concept of mathematics is based on the symbolic interpretation of the Greek concept of number (\"arithmos\").\n\nKlein died in 1978 in Annapolis, Maryland.\n\n"}
{"id": "17961670", "url": "https://en.wikipedia.org/wiki?curid=17961670", "title": "Kempner series", "text": "Kempner series\n\nThe Kempner series is a modification of the harmonic series, formed by omitting all terms whose denominator expressed in base 10 contains the digit 9. That is, it is the sum\nwhere the prime indicates that \"n\" takes only values whose decimal expansion has no nines. The series was first studied by A. J. Kempner in 1914. The series is counter-intuitive because, unlike the harmonic series, it converges. Kempner showed the sum of this series is less than 80. Baillie showed that, rounded to 20 decimals, the actual sum is 22.92067 66192 64150 34816\n\nHeuristically, this series converges because most large integers contain all digits. For example, a random 100-digit integer is very likely to contain at least one '9', causing it to be excluded from the above sum.\n\nSchmelzer and Baillie found an efficient algorithm for the more general problem of any omitted string of digits. For example, the sum of 1/\"n\" where \"n\" has no \"42\" is about 228.44630 41592 30813 25415. Another example: the sum of 1/\"n\" where \"n\" has no occurrence of the digit string \"314159\" is about 2302582.33386 37826 07892 02376. (All values are rounded in the last decimal place).\n\nKempner's proof of convergence is repeated in many textbooks, for example Hardy and Wright and Apostol. We group the terms of the sum by the number of digits in the denominator. The number of \"n\"-digit positive integers that have no digit equal to '9' is 8(9) because there are 8 choices (1 through 8) for the first digit, and 9 independent choices (0 through 8) for each of the other \"n\"−1 digits. Each of these numbers having no '9' is greater than or equal to 10, so the reciprocal of each of these numbers is less than or equal to 10. Therefore, the contribution of this group to the sum of reciprocals is less than 8(9/10). Therefore the whole sum of reciprocals is at most\n\nThe same argument works for any omitted non-zero digit. The number of \"n\"-digit positive integers that have no '0' is 9, so the sum of 1/\"n\" where \"n\" has no digit '0' is at most\n\nThe series also converge if strings of \"k\" digits are omitted, for example if we omit all denominators that have a decimal substring of 42. This can be proved in almost the same way. First we observe that we can work with numbers in base 10 and omit all denominators that have the given string as a \"digit\". The analogous argument to the base 10 case shows that this series converges. Now switching back to base 10, we see that this series contains all denominators that omit the given string, as well as denominators that include it if it is not on a \"\"k\"-digit\" boundary. For example, if we are omitting 42, the base-100 series would omit 4217 and 1742, but not 1427, so it is larger than the series that omits all 42s.\n\nFarhi considered generalized Kempner series, namely, the sums \"S\"(\"d\", \"n\") of the reciprocals of the positive integers that have exactly \"n\" instances of the digit \"d\" where 0 ≤ \"d\" ≤ 9 (so that the original Kempner series is \"S\"(9, 0)). He showed that for each \"d\" the sequence of values \"S\"(\"d\", \"n\") for \"n\" ≥ 1 is decreasing and converges to 10 ln 10. The sequence is not in general decreasing starting with \"n\" = 0; for example, for the original Kempner series we have \"S\"(9, 0) ≈ 22.921 < 23.026 ≈ 10 ln 10 < \"S\"(9, \"n\") for \"n\" ≥ 1.\n\nThe series converges extremely slowly. Baillie remarks that after summing 10 terms the remainder is still larger than 1.\n\nThe upper bound of 80 is very crude, and Irwin showed by a slightly finer analysis of the bounds that the value of the Kempner series is near 23, since refined to about 22.92067.\n\nBaillie developed a recursion that expresses the contribution from each \"k\"+1-digit block in terms of the contributions of the \"k\"-digit blocks for all choices of omitted digit. This permits a very accurate estimate with a small amount of computation.\n\nMost authors do not name this series. The name \"Kempner series\" is used in MathWorld and in Havil's book \"Gamma\" on the Euler–Mascheroni constant.\n\n\n"}
{"id": "25303350", "url": "https://en.wikipedia.org/wiki?curid=25303350", "title": "Lance Fortnow", "text": "Lance Fortnow\n\nLance Jeremy Fortnow (born August 15, 1963) is a computer scientist known for major results in computational complexity and interactive proof systems. He currently chairs the School of Computer Science at Georgia Tech.\n\nLance Fortnow received a doctorate in Applied Mathematics from MIT in 1989, supervised by Michael Sipser. Since graduation, he's served on the faculty of the University of Chicago (1989-1999, 2003-2007), Northwestern University (2008-2012), and most recently the Georgia Institute of Technology (2012–present) as chair of the School of Computer Science.\n\nFortnow was the founding editor-in-chief of the journal ACM Transactions on Computation Theory. He was the chair of ACM SIGACT and succeeded by Paul Beame. He was the chair of the IEEE Conference on Computational Complexity from 2000 to 2006. In 2003, Fortnow began one of the first blogs devoted to theoretical computer science and has written for it since then; since 2007 he has had a co-blogger, William Gasarch. In September 2009, Fortnow brought mainstream attention to complexity theory when he published an article surveying the progress made in the P versus NP problem in the Communications of the Association of Computing Machinery.\n\nIn his many publications, Fortnow has contributed important results to the field of computational complexity. While still a graduate student at MIT, Fortnow showed that there are no perfect zero-knowledge protocols for NP-complete languages unless the polynomial hierarchy collapses. With Michael Sipser, he also demonstrated that relative to a specific oracle there exists a language in co-NP that does not have an interactive protocol.\n\nIn November 1989, Fortnow received an email from Noam Nisan showing that co-NP had multiple prover interactive proofs (MIP). With Carsten Lund and Howard Karloff, he used this result to develop an algebraic technique for the construction of interactive proof systems and prove that every language in the polynomial-time hierarchy has an interactive proof system. Their work was hardly two weeks old when Adi Shamir employed it to prove that IP=PSPACE. Quickly following up on this (January 17, 1990 less than two months after receiving Nisan's email) Fortnow, along with László Babai and Carsten Lund, proved that MIP=NEXP. These algebraic techniques were expanded further by Fortnow, Babai, Leonid Levin, and Mario Szegedy when they presented a new generic mechanism for checking computations.\n\nSince this time Fortnow has continued to publish on a variety of topics in the field of computational complexity including derandomization, sparse languages, and oracle machines. Fortnow has also published on quantum computing, game theory, genome sequencing, and economics.\n\nLance Fortnow's work in economics includes work in game theory, optimal strategies, and prediction. With Duke Whang, Fortnow has examined the classic game theory problem of the prisoner's dilemma, extending the problem so that the dilemma is posed sequentially an infinite number of times. They investigated what strategies the players should take given the constraints that they draw their strategies from computationally bounded sets and introduce “grace periods” to prevent the dominance of vengeful strategies. Fortnow also examined the logarithmic market scoring rule (LMSR) with market makers. He helped to show that LMSR pricing is #P-hard and propose an approximation technique for pricing permutation markets. He has also contributed to an examination the behavior of informed traders working with LMSR market makers.\n\nFortnow has also authored a popular science book, titled \"The Golden Ticket: P, NP and the Search for the Impossible\"., which was loosely based on an article he previously wrote for CACM in 2009. In his book, Fortnow provides a non-technical introduction to the P versus NP problem and its algorithmic limitations. He further describes his book and illustrates why NP problems are so important on the \"Data Skeptic podcast\".\n\n\n"}
{"id": "946975", "url": "https://en.wikipedia.org/wiki?curid=946975", "title": "Line (geometry)", "text": "Line (geometry)\n\nThe notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. Lines are an idealization of such objects. Until the 17th century, lines were defined in this manner: \"The [straight or curved] line is the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which […] will leave from its imaginary moving some vestige in length, exempt of any width. […] The straight line is that which is equally extended between its points.\"\n\nEuclid described a line as \"breadthless length\" which \"lies equally with respect to the points on itself\"; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called Euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the 19th century (such as non-Euclidean, projective and affine geometry).\n\nIn modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.\n\nWhen a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). The properties of lines are then determined by the axioms which refer to them. One advantage to this approach is the flexibility it gives to users of the geometry. Thus in differential geometry a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries a line is a 2-dimensional vector space (all linear combinations of two independent vectors). This flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line.\n\nAll definitions are ultimately circular in nature since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. To avoid this vicious circle certain concepts must be taken as primitive concepts; terms which are given no definition. In geometry, it is frequently the case that the concept of line is taken as a primitive. In those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. When the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy.\n\nIn a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. In this circumstance it is possible that a \"description\" or \"mental image\" of a primitive notion is provided to give a foundation to build the notion on which would formally be based on the (unstated) axioms. Descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. These are not true definitions and could not be used in formal proofs of statements. The \"definition\" of line in Euclid's Elements falls into this category. Even in the case where a specific geometry is being considered (for example, Euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally.\n\nWhen geometry was first formalised by Euclid in the \"Elements\", he defined a general line (straight or curved) to be \"breadthless length\" with a straight line being a line \"which lies evenly with the points on itself\". These definitions serve little purpose since they use terms which are not, themselves, defined. In fact, Euclid did not use these definitions in this work and probably included them just to make it clear to the reader what was being discussed. In modern geometry, a line is simply taken as an undefined object with properties given by axioms, but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined.\n\nIn an axiomatic formulation of Euclidean geometry, such as that of Hilbert (Euclid's original axioms contained various flaws which have been corrected by modern mathematicians), a line is stated to have certain properties which relate it to other lines and points. For example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point. In two dimensions, i.e., the Euclidean plane, two lines which do not intersect are called parallel. In higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not.\n\nAny collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines.\n\nLines in a Cartesian plane or, more generally, in affine coordinates, can be described algebraically by linear equations. \n\nIn two dimensions, the equation for non-vertical lines is often given in the \"slope-intercept form\":\nwhere:\n\nThe slope of the line through points formula_2 and formula_3, when formula_4, is given by formula_5 and the equation of this line can be written formula_6.\n\nIn formula_7, every line formula_8 (including vertical lines) is described by a linear equation of the form\n\nwith fixed real coefficients \"a\", \"b\" and \"c\" such that \"a\" and \"b\" are not both zero. Using this form, vertical lines correspond to the equations with \"b\" = 0.\n\nThere are many variant ways to write the equation of a line which can all be converted from one to another by algebraic manipulation. These forms (see Linear equation for other forms) are generally named by the type of information (data) about the line that is needed to write down the form. Some of the important data of a line is its slope, x-intercept, known points on the line and y-intercept.\n\nThe equation of the line passing through two different points formula_10 and formula_11 may be written as\nIf \"x\" ≠ \"x\", this equation may be rewritten as\nor\n\nIn three dimensions, lines can \"not\" be described by a single linear equation, so they are frequently described by parametric equations:\nwhere:\n\nThey may also be described as the simultaneous solutions of two linear equations\nsuch that formula_20 and formula_21 are not proportional (the relations formula_22 imply formula_23). This follows since in three dimensions a single linear equation typically describes a plane and a line is what is common to two distinct intersecting planes.\n\nThe \"normal form\" (also called the \"Hesse normal form\", after the German mathematician Ludwig Otto Hesse), is based on the \"normal segment\" for a given line, which is defined to be the line segment drawn from the origin perpendicular to the line. This segment joins the origin with the closest point on the line to the origin. The normal form of the equation of a straight line on the plane is given by:\nwhere \"θ\" is the angle of inclination of the normal segment (the oriented angle from the unit vector of the \"x\" axis to this segment), and \"p\" is the (positive) length of the normal segment. The normal form can be derived from the general form formula_25 by dividing all of the coefficients by\n\nUnlike the slope-intercept and intercept forms, this form can represent any line but also requires only two finite parameters, \"θ\" and \"p\", to be specified. If \"p\" > 0, then \"θ\" is uniquely defined modulo 2. On the other hand, if the line is through the origin (\"c\" = 0, \"p\" = 0), one drops the term to compute sin\"θ\" and cos\"θ\", and \"θ\" is only defined modulo .\n\nIn polar coordinates on the Euclidean plane the slope-intercept form of the equation of a line is expressed as:\nwhere \"m\" is the slope of the line and b is the \"y\"-intercept. When \"θ\" = 0 the graph will be undefined. The equation can be rewritten to eliminate discontinuities in this manner:\nIn polar coordinates on the Euclidean plane, the intercept form of the equation of a line that is non-horizontal, non-vertical, and does not pass through pole may be expressed as,\nwhere formula_30 and formula_31 represent the \"x\" and \"y\" intercepts respectively.\nThe above equation is not applicable for vertical and horizontal lines because in these cases one of the intercepts does not exist. Moreover, it is not applicable on lines passing through the pole since in this case, both \"x\" and \"y\" intercepts are zero (which is not allowed here since formula_30 and formula_31 are denominators).\nA vertical line that doesn't pass through the pole is given by the equation\nSimilarly, a horizontal line that doesn't pass through the pole is given by the equation\nThe equation of a line which passes through the pole is simply given as:\nwhere \"m\" is the slope of the line.\n\nThe vector equation of the line through points A and B is given by formula_37 (where λ is a scalar).\n\nIf a is vector OA and b is vector OB, then the equation of the line can be written: formula_38.\n\nA ray starting at point \"A\" is described by limiting λ. One ray is obtained if λ ≥ 0, and the opposite ray comes from λ ≤ 0.\n\nIn three-dimensional space, a first degree equation in the variables \"x\", \"y\", and \"z\" defines a plane, so two such equations, provided the planes they give rise to are not parallel, define a line which is the intersection of the planes. More generally, in \"n\"-dimensional space \"n\"-1 first-degree equations in the \"n\" coordinate variables define a line under suitable conditions.\n\nIn more general Euclidean space, R (and analogously in every other affine space), the line \"L\" passing through two different points \"a\" and \"b\" (considered as vectors) is the subset\nThe direction of the line is from \"a\" (\"t\" = 0) to \"b\" (\"t\" = 1), or in other words, in the direction of the vector \"b\" − \"a\". Different choices of \"a\" and \"b\" can yield the same line.\n\nThree points are said to be \"collinear\" if they lie on the same line. Three points \"usually\" determine a plane, but in the case of three collinear points this does \"not\" happen.\n\nIn affine coordinates, in \"n\"-dimensional space the points \"X\"=(\"x\", \"x\", ..., \"x\"), \"Y\"=(\"y\", \"y\", ..., \"y\"), and \"Z\"=(\"z\", \"z\", ..., \"z\") are collinear if the matrix\nhas a rank less than 3. In particular, for three points in the plane (\"n\" = 2), the above matrix is square and the points are collinear if and only if its determinant is zero.\n\nEquivalently for three points in a plane, the points are collinear if and only if the slope between one pair of points equals the slope between any other pair of points (in which case the slope between the remaining pair of points will equal the other slopes). By extension, \"k\" points in a plane are collinear if and only if any (\"k\"–1) pairs of points have the same pairwise slopes.\n\nIn Euclidean geometry, the Euclidean distance \"d\"(\"a\",\"b\") between two points \"a\" and \"b\" may be used to express the collinearity between three points by:\nHowever, there are other notions of distance (such as the Manhattan distance) for which this property is not true.\n\nIn the geometries where the concept of a line is a primitive notion, as may be the case in some synthetic geometries, other methods of determining collinearity are needed.\n\nIn a sense, all lines in Euclidean geometry are equal, in that, without coordinates, one can not tell them apart from one another. However, lines may play special roles with respect to other objects in the geometry and be divided into types according to that relationship. For instance, with respect to a conic (a circle, ellipse, parabola, or hyperbola), lines can be:\n\nIn the context of determining parallelism in Euclidean geometry, a transversal is a line that intersects two other lines that may or not be parallel to each other.\n\nFor more general algebraic curves, lines could also be:\nWith respect to triangles we have:\n\nFor a convex quadrilateral with at most two parallel sides, the Newton line is the line that connects the midpoints of the two diagonals.\n\nFor a hexagon with vertices lying on a conic we have the Pascal line and, in the special case where the conic is a pair of lines, we have the Pappus line.\n\nParallel lines are lines in the same plane that never cross. Intersecting lines share a single point in common. Coincidental lines coincide with each other—every point that is on either one of them is also on the other.\n\nPerpendicular lines are lines that intersect at right angles.\n\nIn three-dimensional space, skew lines are lines that are not in the same plane and thus do not intersect each other.\n\nIn many models of projective geometry, the representation of a line rarely conforms to the notion of the \"straight curve\" as it is visualised in Euclidean geometry. In elliptic geometry we see a typical example of this. In the spherical representation of elliptic geometry, lines are represented by great circles of a sphere with diametrically opposite points identified. In a different model of elliptic geometry, lines are represented by Euclidean planes passing through the origin. Even though these representations are visually distinct, they satisfy all the properties (such as, two points determining a unique line) that make them suitable representations for lines in this geometry.\n\nGiven a line and any point \"A\" on it, we may consider \"A\" as decomposing this line into two parts.\nEach such part is called a ray (or half-line) and the point \"A\" is called its \"initial point\". The point A is considered to be a member of the ray. Intuitively, a ray consists of those points on a line passing through \"A\" and proceeding indefinitely, starting at \"A\", in one direction only along the line. However, in order to use this concept of a ray in proofs a more precise definition is required.\n\nGiven distinct points \"A\" and \"B\", they determine a unique ray with initial point \"A\". As two points define a unique line, this ray consists of all the points between \"A\" and \"B\" (including \"A\" and \"B\") and all the points \"C\" on the line through \"A\" and \"B\" such that \"B\" is between \"A\" and \"C\". This is, at times, also expressed as the set of all points \"C\" such that \"A\" is not between \"B\" and \"C\". A point \"D\", on the line determined by \"A\" and \"B\" but not in the ray with initial point \"A\" determined by \"B\", will determine another ray with initial point \"A\". With respect to the \"AB\" ray, the \"AD\" ray is called the \"opposite ray\".\nThus, we would say that two different points, \"A\" and \"B\", define a line and a decomposition of this line into the disjoint union of an open segment and two rays, \"BC\" and \"AD\" (the point \"D\" is not drawn in the diagram, but is to the left of \"A\" on the line \"AB\"). These are not opposite rays since they have different initial points.\n\nIn Euclidean geometry two rays with a common endpoint form an angle.\n\nThe definition of a ray depends upon the notion of betweenness for points on a line. It follows that rays exist only for geometries for which this notion exists, typically Euclidean geometry or affine geometry over an ordered field. On the other hand, rays do not exist in projective geometry nor in a geometry over a non-ordered field, like the complex numbers or any finite field.\n\nIn topology, a ray in a space \"X\" is a continuous embedding R → \"X\". It is used to define the important concept of end of the space.\n\nA line segment is a part of a line that is bounded by two distinct end points and contains every point on the line between its end points. Depending on how the line segment is defined, either of the two end points may or may not be part of the line segment. Two or more line segments may have some of the same relationships as lines, such as being parallel, intersecting, or skew, but unlike lines they may be none of these, if they are coplanar and either do not intersect or are collinear.\n\nThe \"shortness\" and \"straightness\" of a line, interpreted as the property that the distance along the line between any two of its points is minimized (see triangle inequality), can be generalized and leads to the concept of geodesics in metric spaces.\n\n\n\n"}
{"id": "73360", "url": "https://en.wikipedia.org/wiki?curid=73360", "title": "Lyapunov fractal", "text": "Lyapunov fractal\n\nIn mathematics, Lyapunov fractals (also known as Markus–Lyapunov fractals) are bifurcational fractals derived from an extension of the logistic map in which the degree of the growth of the population, \"r\", periodically switches between two values \"A\" and \"B\".\n\nA Lyapunov fractal is constructed by mapping the regions of stability and chaotic behaviour (measured using the Lyapunov exponent formula_1) in the \"a\"−\"b\" plane for given periodic sequences of \"a\" and \"b\". In the images, yellow corresponds to formula_2 (stability), and blue corresponds to formula_3 (chaos).\n\nLyapunov fractals are generally drawn for values of \"A\" and \"B\" in the interval formula_4. For larger values, the interval [0,1] is no longer stable, and the sequence is likely to be attracted by infinity, although convergent cycles of finite values continue to exist for some parameters. For all iteration sequences, the diagonal \"a = b\" is always the same as for the standard one parameter logistic function.\n\nThe sequence is usually started at the value 0.5, which is a critical point of the iterative function. The other (even complex valued) critical points of the iterative function during one entire round are those that pass through the value 0.5 in the first round. A convergent cycle must attract at least one critical point; therefore all convergent cycles can be obtained by just shifting the iteration sequence, and keeping the starting value 0.5. In practice, shifting this sequence leads to changes in the fractal, as some branches get covered by others; notice for instance how the Lyapunov fractal for the iteration sequence AB is not perfectly symmetric with respect to \"a\" and \"b\".\n\nAn algorithm, for computing the fractal is summarized as follows.\n\n\n"}
{"id": "2287691", "url": "https://en.wikipedia.org/wiki?curid=2287691", "title": "Mathematical joke", "text": "Mathematical joke\n\nA mathematical joke is a form of humor which relies on aspects of mathematics or a stereotype of mathematicians to derive humor. The humor may come from a pun, or from a double meaning of a mathematical term, or from a lay person's misunderstanding of a mathematical concept. Mathematician and author John Allen Paulos in his book \"Mathematics and Humor\" described several ways that mathematics, generally considered a dry, formal activity, overlaps with humor, a loose, irreverent activity: both are forms of \"intellectual play\"; both have \"logic, pattern, rules, structure\"; and both are \"economical and explicit\".\n\nSome performers combine mathematics and jokes to entertain and/or teach math.\nHumor of mathematicians may be classified into the esoteric and exoteric categories. Esoteric jokes rely on the intrinsic knowledge of mathematics and its terminology. Exoteric jokes are intelligible to the outsiders, and most of them compare mathematicians with representatives of other disciplines or with common folk.\n\nSome jokes use a mathematical term with a second non-technical meaning as the punchline of a joke.\n\nOccasionally, multiple mathematical puns appear in the same jest:\n\nThis invokes four double meanings: adder (snake) vs. addition (algebraic operation); multiplication (biological reproduction) vs. multiplication (algebraic operation); log (cut tree trunk) vs. log (logarithm); multiplication is done by adding logs (mathematical logarithm); finally, table (set of facts) vs. table (piece of furniture).\n\nOther jokes create a double meaning from a direct calculation involving facetious variable names, such as this retold from \"Gravity's Rainbow\":\n\nThe first part of this joke relies on the fact that the primitive (formed when finding the antiderivative) of the function 1/\"x\" is log(\"x\"). The second part is then based on the fact that the antiderivative is actually a class of functions, requiring the inclusion of a constant of integration, usually denoted as \"C\"—something which calculus students may forget. Thus, the indefinite integral of 1/cabin is \"log(cabin) + \"C\"\", or \"A log cabin plus the sea\", i.e., \"A houseboat\".\n\nSome jokes depend on ambiguity of numeral bases. \n\nThis joke mocks phrases that begin with \"there are two types of people in the world...\" and relies on an ambiguous meaning of the expression 10, which in the binary numeral system is equal to the decimal number 2. There are many extensions to the joke, such as \"There are 10 types of people in the world: those who understand binary, those who don't and those who didn't realize the joke was in base three.\"\n\nAnother pun using different radices, asks:\n\nThe play on words lies in the similarity of the abbreviation for October/Octal and December/Decimal, and the coincidence that both equal the same amount (formula_1).\n\nIn mathematics, the imaginary number \"i\" is a number such that:\n\nformula_2\n\nA complex number is a quantity which can be written as the sum of a real number plus an imaginary number.\n\nWhile a pun has been based upon the constant's name (\"complex numbers are all fun and games until someone loses an i\"), most jokes treat an imaginary number as if it were a fictional entity. A telephone intercept message of \"you have dialed an imaginary number, please rotate your handset ninety degrees and try again\" is a typical example. Another popular example is \"What did i say to pi? Be rational. What did pi say to i? Get real.\"\n\nSome jokes are based on stereotypes of mathematicians tending to think in complicated, abstract terms, causing them to lose touch with the \"real world\". These compare mathematicians to physicists, engineers, or the \"soft\" sciences in a form similar to an Englishman, an Irishman and a Scotsman, showing the other scientist doing something practical, while the mathematician proposes a theoretically valid but physically nonsensical solution.\n\nMathematicians are also shown as averse to making hasty generalizations from a small amount of data, even if some form of generalization seems plausible:\n\nA classic joke involving stereotypes is the \"Dictionary of Definitions of Terms Commonly Used in Math Lectures\". Examples include \"Trivial: If I have to show you how to do this, you're in the wrong class\" and \"Similarly: At least one line of the proof of this case is the same as before.\"\n\nThis category of jokes comprises those that exploit common misunderstandings of mathematics, or the expectation that most people have only a basic mathematical education, if any.\n\nThe joke is that the employee fails to understand the scientist's implication of the uncertainty in the age of the fossil and uses false precision.\n\nA form of mathematical humor comes from using mathematical tools (both abstract symbols and physical objects such as calculators) in various ways which transgress their intended scope. These constructions are generally devoid of any substantial mathematical content, besides some basic arithmetic.\n\nA set of equivocal jokes applies mathematical reasoning to situations where it is not entirely valid. Many of these are based on a combination of well-known quotes and basic logical constructs such as syllogisms:\n\nAnother set of jokes relate to the absence of mathematical reasoning, or misinterpretation of conventional notation:\n\nformula_3\n\nThat is, the limit as \"x\" goes to 8 from above is a sideways 8 or the infinity sign, in the same way that the limit as \"x\" goes to three from above is a sideways 3 or the Greek letter omega (conventionally used to notate the smallest infinite ordinal number).\n\nAn anomalous cancellation is a kind of arithmetic procedural error that gives a numerically correct answer:\n\n\nA number of mathematical fallacies are part of mathematical humorous folklore. For example:\nformula_6\nThis appears to prove that , but uses division by zero to produce the result.\n\nSome jokes attempt a seemingly plausible, but effectively impossible, mathematical operation. For example:\n\nPi goes on and on and on ...<br>\nAnd e is just as cursed.<br>\nI wonder: Which is larger<br>\nWhen their digits are reversed? \n\nTo reverse the digits of a number's decimal expansion, we have to start at the last digit and work backwards. However, that's not practical if the expansion never ends, as for formula_7 and formula_8.\n\nMany numbers have been given humorous names, either as pure numbers or as units of measurement. Some examples:\n\nSagan has been defined as \"billions and billions\", a metric of the number of stars in the observable universe.\n\n\"Jenny's constant\" has been defined as formula_9 , from the pop song 867-5309/Jenny, which concerns the telephone number 867-5309.\n\nThe mathematical constant 42 appears throughout the Douglas Adams trilogy \"The Hitchhiker's Guide to the Galaxy\", where it is portrayed as \"the answer to the ultimate question of life, the universe and everything\". This number appears as a fixed value in the TIFF image file format and its derivatives (including for example the ISO standard TIFF/EP) where the content of bytes 2–3 is defined as 42: \"An arbitrary but carefully chosen number that further identifies the file as a TIFF file\".\n\nCalculator spelling is the formation of words and phrases by displaying a number and turning the calculator upside down. The jest may be formulated as a mathematical problem where the result, when read upside down, appears to be an identifiable phrase like \"ShELL OIL\" or \"Esso\" using seven-segment display character representations where the open-top \"4\" is an inverted 'h' and '5' looks like 'S'. Other letters can be used as numbers too with 8 and 9 representing B and G, respectively.\n\nA mathematical limerick is an expression which, when read aloud, matches the form of a limerick. The following example is attributed to Leigh Mercer:\nformula_10\nThis is read as follows:\n\nAn oft-repeated joke is that topologists cannot tell a coffee cup from a doughnut, since a sufficiently pliable doughnut could be reshaped (by a homeomorphism) to the form of a cup by creating a dimple and progressively enlarging it, while shrinking the hole into a handle.\n\n\n"}
{"id": "14204445", "url": "https://en.wikipedia.org/wiki?curid=14204445", "title": "Mazur manifold", "text": "Mazur manifold\n\nIn differential topology, a branch of mathematics, a Mazur manifold is a contractible, compact, smooth 4-dimensional manifold (with boundary) which is not diffeomorphic to the standard 4-ball. The boundary of a Mazur manifold is necessarily a homology 3-sphere.\n\nFrequently the term Mazur manifold is restricted to a special class of the above definition: 4-manifolds that have a handle decomposition containing exactly three handles: a single 0-handle, a single 1-handle and single 2-handle. This is equivalent to saying the manifold must be of the form formula_1 union a 2-handle. An observation of Mazur's shows that the double of such manifolds is diffeomorphic to formula_2 with the standard smooth structure.\n\nBarry Mazur and Valentin Poenaru discovered these manifolds simultaneously. Akbulut and Kirby showed that the Brieskorn homology spheres formula_3, formula_4 and formula_5 are boundaries of Mazur manifolds. This results were later generalized to other contractible manifolds by Casson, Harer and Stern. One of the Mazur manifolds is also an example of an Akbulut cork which can be used to construct exotic 4-manifolds.\n\nMazur manifolds have been used by Fintushel and Stern to construct exotic actions of a group of order 2 on the 4-sphere.\n\nMazur's discovery was surprising for several reasons:\n\nLet formula_14 be a Mazur manifold that is constructed as formula_1 union a 2-handle. Here is a sketch of Mazur's argument that the double of such a Mazur manifold is formula_2. formula_17 is a contractible 5-manifold constructed as formula_18 union a 2-handle. The 2-handle can be unknotted since the attaching map is a framed knot in the 4-manifold formula_19. So formula_18 union the 2-handle is diffeomorphic to formula_10. The boundary of formula_10 is formula_2. But the boundary of formula_17 is the double of formula_14.\n"}
{"id": "24256106", "url": "https://en.wikipedia.org/wiki?curid=24256106", "title": "Milan Randić", "text": "Milan Randić\n\nMilan Randić (born 1930) is a Croatian American scientist who is one of the leading experts in the field of computational chemistry.\n\nRandić was born in the city of Belgrade, where his parents, originally from Kostrena (Croatian Primorje – Region in the northern Adriatic), lived at the time. Kostrena is well known by its maritime tradition, shipowners and seamen. Randic's ancestors were sailing ship owners as well as ship captains. His parents moved to Zagreb in 1941, where he continued his education.\nAfter finishing Gymnasium in Zagreb, he studied Theoretical Physics at the University of Zagreb during 1949–1953 and studied for Ph. D degree at the University of Cambridge, England (1954–1958).\n\nFrom 1960 to 1970 he was at the Ruđer Bošković Institute in Zagreb, Croatia, where he founded the Theoretical Chemistry Group. During 1971–1980 he was visiting various universities in USA including Johns Hopkins, MIT, Harvard, Tufts, and Cornell. With 1973 his research oriented towards application of Discrete Mathematics and Graph Theory in particular to characterization of molecules and bio-molecules. During 1980 to 1997 he was professor in the Department of Mathematics and Computer Science at Drake University, Des Moines, Iowa. During the past 15 years he is spending three months each year at the National Institute of Chemistry, Ljubljana, Slovenia collaborating with scientists from its Laboratory for Chemometrics.\n\nRandić has been a major contributor to the development of mathematical chemistry, in particular to the development and use of molecular descriptors based on the use of Graph Theory. In 1975 he introduced the Randić index, the first connectivity index.\n\nHe is a corresponding member of the Croatian Academy of Sciences and Arts and founder of the International Academy of Mathematical Chemistry, the seat of which is in Dubrovnik, Croatia. With year 2000 his research has been mostly shifted towards Bioinformatics, with particular emphasis on graphical representation and numerical characterization of DNA, proteins and proteome, though his fascination with Kekulé valence structures and aromaticity remains undiminished. He is Honorary Member of The Croatian Chemical Society, The International Academy of Mathematical Chemistry and The National Institute of Chemistry, Ljubljana, Slovenia.\nHis other interests include development of \"Nobel\", a universal ideographic writing system.\n\n\n"}
{"id": "11017807", "url": "https://en.wikipedia.org/wiki?curid=11017807", "title": "Morrie's law", "text": "Morrie's law\n\nMorrie's law is the trigonometric identity\n\nIt is a special case of the more general identity\n\nwith \"n\" = 3 and α = 20° and the fact that \n\nsince \n\nThe name is due to the physicist Richard Feynman, who used to refer to the identity under that name. Feynman picked that name because he learned it during his childhood from a boy with the name Morrie Jacobs and afterwards remembered it for all of his life.\n\nA similar identity for the sine function also holds:\n\nMoreover, dividing the second identity by the first, the following identity is evident:\n\nRecall the double angle formula for the sine function\n\nSolve for formula_8\n\nIt follows that:\n\nMultiplying all of these expressions together yields:\n\nThe intermediate numerators and denominators cancel leaving only the first denominator, a power of 2 and the final numerator. Note that there are \"n\" terms in both sides of the expression. Thus,\n\nwhich is equivalent to the generalization of Morrie's law.\n"}
{"id": "1678822", "url": "https://en.wikipedia.org/wiki?curid=1678822", "title": "Perceptual control theory", "text": "Perceptual control theory\n\nPerceptual control theory (PCT) is a model of behavior based on the principles of negative feedback, but differing in important respects from engineering control theory. Results of PCT experiments have demonstrated that an organism controls neither its own behavior, nor external environmental variables, but rather its own perceptions of those variables. Actions are not controlled, they are varied so as to cancel the effects that unpredictable environmental disturbances would otherwise have on controlled perceptions. As a catch-phrase of the field puts it, \"behavior is the control of perception.\" PCT demonstrates circular causation in a negative feedback loop closed through the environment. This fundamentally contradicts the classical notion of linear causation of behavior by stimuli, in which environmental stimuli are thought to cause behavioral responses, mediated (according to cognitive psychology) by intervening cognitive processes. \n\nNumerous computer simulations of specific behavioral situations demonstrate its efficacy, with extremely high correlations to observational data (0.95 or better), which are vanishingly rare in the so-called 'soft' sciences. While the adoption of PCT in the scientific community has not been widespread, it has been applied not only in experimental psychology and neuroscience, but also in sociology, linguistics, and a number of other fields, and has led to a method of psychotherapy called the method of levels.\n\nA tradition from Aristotle through William James recognizes that behavior is \"purposeful\" rather than merely reactive. However, the only evidence for intentions was subjective. Behaviorists following Wundt, Thorndyke, Watson, and others rejected introspective reports as data for an objective science of psychology. Only observable behavior could be admitted as data.\n\nThere follows from this stance the assumption that environmental events (stimuli) cause behavioral actions (responses). This assumption persists in cognitive psychology, which interposes cognitive maps and other postulated information processing between stimulus and response, but otherwise retains the assumption of linear causation from environment to behavior.\n\nAnother, more specific reason for psychologists' rejecting notions of purpose or intention was that they could not see how a goal (a state that did not yet exist) could cause the behavior that led to it. PCT resolves these philosophical arguments about teleology because it provides a model of the functioning of organisms in which purpose has objective status without recourse to introspection, and in which causation is circular around feedback loops.\n\nPCT has roots in insights of Claude Bernard and 20th century control systems engineering and cybernetics. It was originated as such, and given its present form and experimental methodology, by William T. Powers.\n\nPowers recognized that to be purposeful implies control, and that the concepts and methods of engineered control systems could be applied to biological control systems. A key insight is that the variable that is controlled is not the output of the system (the behavioral actions), but its input, that is, a sensed and transformed function of some state of the environment that could be affected by the control system's output. Because some of these sensed and transformed inputs appear as consciously perceived aspects of the environment, Powers labelled the controlled variable \"perception\". The theory came to be known as \"Perceptual Control Theory\" or PCT rather than \"Control Theory Applied to Psychology\" because control theorists often assert or assume that it is the system's output that is controlled. In PCT it is the internal representation of the state of some variable in the environment—a \"perception\" in everyday language—that is controlled. The basic principles of PCT were first published by Powers, Clark, and MacFarland as a \"general feedback theory of behavior\" in 1960, with credits to cybernetic authors Wiener and Ashby, and has been systematically developed since then in the research community that has gathered around it. Initially, it received little general recognition, but is now better known.\n\nA simple negative feedback control system is a cruise control system for a car. A cruise control system has a sensor which \"perceives\" speed as the rate of spin of the drive shaft directly connected to the wheels. It also has a driver-adjustable 'goal' specifying a particular speed. The sensed speed is continuously compared against the specified speed by a device (called a \"comparator\") which subtracts the currently sensed input value from the stored goal value. The difference (the error signal) determines the throttle setting (the accelerator depression), so that the engine output is continuously varied to prevent the speed of the car from increasing or decreasing from that desired speed as environmental conditions change. This type of classical negative feedback control was worked out by engineers in the 1930s and 1940s.\n\nIf the speed of the car starts to drop below the goal-speed, for example when climbing a hill, the small increase in the error signal, amplified, causes engine output to increase, which keeps the error very nearly at zero. If the speed begins to exceed the goal, e.g. when going down a hill, the engine is throttled back so as to act as a brake, so again the speed is kept from departing more than a barely detectable amount from the goal speed (brakes being needed only if the hill is too steep). The result is that the cruise control system maintains a speed close to the goal as the car goes up and down hills, and as other disturbances such as wind affect the car's speed. This is all done without any planning of specific actions, and without any blind reactions to stimuli. Indeed, the cruise control system does not sense disturbances such as wind pressure at all, it only senses the controlled variable, speed. Nor does it control the power generated by the engine, it uses the 'behavior' of engine power as its means to control the sensed speed.\n\nThe same principles of negative feedback control (including the ability to nullify effects of unpredictable external or internal disturbances) apply to living control systems. The thesis of PCT is that animals and people do not control their behavior; rather, they vary their behavior as their means for controlling their perceptions, with or without external disturbances. This directly contradicts the historical and still widespread assumption that behavior is the final result of stimulus inputs and cognitive plans.\n\nThe principal datum in PCT methodology is the controlled variable. The fundamental step of PCT research, the test for controlled variables, begins with the slow and gentle application of disturbing influences to the state of a variable in the environment which the researcher surmises is already under control by the observed organism. It is essential not to overwhelm the organism's ability to control, since that is what is being investigated. If the organism changes its actions just so as to prevent the disturbing influence from having the expected effect on that variable, that is strong evidence that the experimental action disturbed a controlled variable. It is crucially important to distinguish the perceptions and point of view of the observer from those of the observed organism. It may take a number of variations of the test to isolate just which aspect of the environmental situation is under control, as perceived by the observed organism.\n\nPCT employs a black box methodology. The controlled variable as measured by the observer corresponds quantitatively to a reference value for a perception that the organism is controlling. The controlled variable is thus an objective index of the purpose or intention of those particular behavioral actions by the organism—the goal which those actions consistently work to attain despite disturbances. With few exceptions, in the current state of neuroscience this internally maintained reference value is seldom directly observed as such (e.g. as a rate of firing in a neuron), since few researchers trace the relevant electrical and chemical variables by their specific pathways while a living organism is engaging in what we externally observe as behavior. However, when a working negative feedback system simulated on a digital computer performs essentially identically to observed organisms, then the well understood negative feedback structure of the simulation or model (the white box) is understood to demonstrate the unseen negative feedback structure within the organism (the black box).\n\nData for individuals are not aggregated for statistical analysis; instead, a generative model is built which replicates the data observed for individuals with very high fidelity (0.95 or better). To build such a model of a given behavioral situation requires careful measurements of three observed variables:\n\nA fourth value, the internally maintained reference \"r\" (a variable ′setpoint′), is deduced from the value at which the organism is observed to maintain \"q\", as determined by the test for controlled variables (described at the beginning of this section).\n\nWith two variables specified, the controlled input \"q\" and the reference \"r\", a properly designed control system, simulated on a digital computer, produces outputs \"q\" that almost precisely oppose unpredictable disturbances \"d\" to the controlled input. Further, the variance from perfect control accords well with that observed for living organisms. Perfect control would result in zero effect of the disturbance, but living organisms are not perfect controllers, and the aim of PCT is to model living organisms. When a computer simulation performs with >95% conformity to experimentally measured values, opposing the effect of unpredictable changes in \"d\" by generating (nearly) equal and opposite values of \"q\", it is understood to model the behavior and the internal control-loop structure of the organism.\n\nBy extension, the elaboration of the theory constitutes a general model of cognitive process and behavior. With every specific model or simulation of behavior that is constructed and tested against observed data, the general model that is presented in the theory is exposed to potential challenge that could call for revision or could lead to refutation.\n\nTo illustrate the mathematical calculations employed in a PCT simulation, consider a pursuit tracking task in which the participant keeps a mouse cursor aligned with a moving target on a computer monitor.\n\nThe model assumes that a perceptual signal within the participant represents the magnitude of the input quantity \"q\". (This has been demonstrated to be a rate of firing in a neuron, at least at the lowest levels.) In the tracking task, the input quantity is the vertical distance between the target position \"T\" and the cursor position \"C\", and the random variation of the target position acts as the disturbance \"d\" of that input quantity. This suggests that the perceptual signal \"p\" quantitatively represents the cursor position \"C\" minus the target position T, as expressed in the equation \"p\"=\"C\"–\"T\".\n\nBetween the perception of target and cursor and the construction of the signal representing the distance between them there is a delay of \"Τ\" milliseconds, so that the working perceptual signal at time \"t\" represents the target-to-cursor distance at a prior time, \"t\" – \"Τ\". Consequently, the equation used in the model is\n\n1. \"p\"(\"t\") = \"C\"(\"t–Τ\") – \"T\"(\"t–Τ\")\n\nThe negative feedback control system receives a reference signal \"r\" which specifies the magnitude of the given perceptual signal which is currently intended or desired. (For the origin of \"r\" within the organism, see under \"A hierarchy of control\", below.) Both \"r\" and \"p\" are input to a simple neural structure with \"r\" excitatory and \"p\" inhibitory. This structure is called a \"comparator\". The effect is to subtract \"p\" from \"r\", yielding an error signal \"e\" that indicates the magnitude and sign of the difference between the desired magnitude \"r\" and the currently input magnitude \"p\" of the given perception. The equation representing this in the model is:\n\n2. \"e\" = \"r–p\"\n\nThe error signal \"e\" must be transformed to the output quantity \"q\" (representing the participant's muscular efforts affecting the mouse position). Experiments have shown that in the best model for the output function, the mouse velocity \"V\" is proportional to the error signal \"e\" by a gain factor \"G\" (that is, \"V\" = \"G\"*\"e\"). Thus, when the perceptual signal \"p\" is smaller than the reference signal \"r\", the error signal \"e\" has a positive sign, and from it the model computes an upward velocity of the cursor that is proportional to the error.\n\nThe next position of the cursor \"C\" is the current position \"C\" plus the velocity \"V\" times the duration \"dt\" of one iteration of the program. By simple algebra, we substitute \"G\"*\"e\" (as given above) for \"V\", yielding a third equation:\n\n3. \"C\" = \"C\" + \"G\"*\"e\"*\"dt\"\n\nThese three simple equations or program steps constitute the simplest form of the model for the tracking task. When these three simultaneous equations are evaluated over and over with the same random disturbances \"d\" of the target position that the human participant experienced, the output positions and velocities of the cursor duplicate the participant's actions in the tracking task above within 4.0% of their peak-to-peak range, in great detail.\n\nThis simple model can be refined with a damping factor \"d\" which reduces the discrepancy between the model and the human participant to 3.6% when the disturbance \"d\" is set to maximum difficulty.\n\n3'. \"C\" = \"C\" + [(\"G\"*\"e\")–(\"d\"*\"C\")]*\"dt\"\n\nDetailed discussion of this model in (Powers 2008) includes both source and executable code, with which the reader can verify how well this simple program simulates real behavior. No consideration is needed of possible nonlinearities such as the Weber-Fechner law, potential noise in the system, continuously varying angles at the joints, and many other factors that could afflict performance if this were a simple linear model. No inverse kinematics or predictive calculations are required. The model simply reduces the discrepancy between input \"p\" and reference \"r\" continuously as it arises in real time, and that is all that is required—as predicted by the theory.\n\n[Symbolic_artificial_intelligence|Good Old Fashioned Artificial Intelligence (GOFAI)] is an early (1960s) approach to building intelligent systems. GOFAI assumes that AI programs are more complex versions of existing software designs, and that they can be built by extending existing procedural approaches and standard software methodologies. Objections to GOFAI can be separated into those based on program semantics, and those based on algorithmic complexity.\n\nImplicit in the theoretical approach to GOFAI is the belief that minds (animal and human) are essentially syntactic, that is, made from symbols combined into expressions, a stance known as the Physical Symbol Systems Hypothesis. In recent years, this belief has been challenged by those who believe that minds are essentially semantic, i.e. they process embodied and situated meaning, not abstract data symbols. The 'poster child' of this oppositional stance is a thought experiment (Gedankenexperiment) called Searle's Chinese Room (SCR), named after its inventor, philosopher John Searle. PCT is inherently situated, embodied, and dynamic.\n\nPractical attempts to building GOFAI soon run into asymptotic increases in algorithmic complexity, so-called 'complexity explosions'. They arise from the control paradigm assumed by GOFAI practitioners, which views governance (= feedforward command + feedback control) as modeling. In other words, for a GOFAI system to 'reason' about the world, it must first build an internal symbolic model of that world upon which it can apply sequences of symbolic manipulations in accord with the principles of Turing and Von Neumann abstract machines. Even small changes in the world must be updated by the GOFAI program, in case they might be critical to its logical output. But any world model which contains sufficient detail to ensure correct GOFAI output is inevitably too complex, with too many moving parts to be practical, so that by the time world changes are updated, the world has already moved on.\n\nPCT avoids the complexity issues of GOFAI by basing its real-time governance on a perceptual window (dimensionally reduced projection) of the world, which is much more tractable than the entire world. Uexküll, Dyer, and Powers arrived independently at this insight, which is important for robotics.\n\nIn the artificial systems that are specified by engineering control theory, the reference signal is considered to be an external input to the 'plant'. In engineering control theory, the reference signal or set point is public; in PCT, it is not, but rather must be deduced from the results of the test for controlled variables, as described above in the methodology section. This is because in living systems a reference signal is not an externally accessible input, but instead originates within the system. In the hierarchical model, error output of higher-level control loops, as described in the next section below, evokes the reference signal \"r\" from synapse-local memory, and the strength of \"r\" is proportional to the (weighted) strength of the error signal or signals from one or more higher-level systems.\n\nIn engineering control systems, in the case where there are several such reference inputs, a 'Controller' is designed to manipulate those inputs so as to obtain the effect on the output of the system that is desired by the system's designer, and the task of a control theory (so conceived) is to calculate those manipulations so as to avoid instability and oscillation. The designer of a PCT model or simulation specifies no particular desired effect on the output of the system, except that it must be whatever is required to bring the input from the environment (the perceptual signal) into conformity with the reference. In Perceptual Control Theory, the input function for the reference signal is a weighted sum of internally generated signals (in the canonical case, higher-level error signals), and loop stability is determined locally for each loop in the manner sketched in the preceding section on the mathematics of PCT (and elaborated more fully in the referenced literature). The weighted sum is understood to result from reorganization.\n\nEngineering control theory is computationally demanding, but as the preceding section shows, PCT is not. For example, contrast the implementation of a model of an inverted pendulum in engineering control theory with the PCT implementation as a hierarchy of five simple control systems.\n\nPerceptions, in PCT, are constructed and controlled in a hierarchy of levels. For example, visual perception of an object is constructed from differences in light intensity or differences in sensations such as color at its edges. Controlling the shape or location of the object requires altering the perceptions of sensations or intensities (which are controlled by lower-level systems). This organizing principle is applied at all levels, up to the most abstract philosophical and theoretical constructs.\n\nThe Russian physiologist Nicolas Bernstein independently came to the same conclusion that behavior has to be multiordinal—organized hierarchically, in layers. A simple problem led to this conclusion at about the same time both in PCT and in Bernstein's work. The spinal reflexes act to stabilize limbs against disturbances. Why do they not prevent centers higher in the brain from using those limbs to carry out behavior? Since the brain obviously does use the spinal systems in producing behavior, there must be a principle that allows the higher systems to operate by incorporating the reflexes, not just by overcoming them or turning them off. The answer is that the reference value (setpoint) for a spinal reflex is not static; rather, it is varied by higher-level systems as their means of moving the limbs. This principle applies to higher feedback loops, as each loop presents the same problem to subsystems above it.\n\nWhereas an engineered control system has a reference value or setpoint adjusted by some external agency, the reference value for a biological control system cannot be set in this way. The setpoint must come from some internal process. If there is a way for behavior to affect it, any perception may be brought to the state momentarily specified by higher levels and then be maintained in that state against unpredictable disturbances. In a hierarchy of control systems, higher levels adjust the goals of lower levels as their means of approaching their own goals set by still-higher systems. This has important consequences for any proposed external control of an autonomous living control system (organism). At the highest level, reference values (goals) are set by heredity or adaptive processes.\n\nIf an organism controls inappropriate perceptions, or if it controls some perceptions to inappropriate values, then it is less likely to bring progeny to maturity, and may die. Consequently, by natural selection successive generations of organisms evolve so that they control those perceptions that, when controlled with appropriate setpoints, tend to maintain critical internal variables at optimal levels, or at least within non-lethal limits. Powers called these critical internal variables \"intrinsic variables\" (Ashby's \"essential variables\").\n\nThe mechanism that influences the development of structures of perceptions to be controlled is termed \"reorganization\", a process within the individual organism that is subject to natural selection just as is the evolved structure of individuals within a species.\n\nThis \"reorganization system\" is proposed to be part of the inherited structure of the organism. It changes the underlying parameters and connectivity of the control hierarchy in a random-walk manner. There is a basic continuous rate of change in intrinsic variables which proceeds at a speed set by the total error (and stops at zero error), punctuated by random changes in direction in a hyperspace with as many dimensions as there are critical variables. This is a more or less direct adaptation of Ashby's \"homeostat\", first adopted into PCT in the 1960 paper and then changed to use E. coli's method of navigating up gradients of nutrients, as described by Koshland (1980).\n\nReorganization may occur at any level when loss of control at that level causes intrinsic (essential) variables to deviate from genetically determined set points. This is the basic mechanism that is involved in trial-and-error learning, which leads to the acquisition of more systematic kinds of learning processes.\n\nIn a hierarchy of interacting control systems, different systems at one level can send conflicting goals to one lower system. When two systems are specifying different goals for the same lower-level variable, they are in conflict. Protracted conflict is experienced by human beings as many forms of psychological distress such as anxiety, obsession, depression, confusion, and vacillation. Severe conflict prevents the affected systems from being able to control, effectively destroying their function for the organism.\n\nHigher level control systems often are able to use known strategies (which are themselves acquired through prior reorganizations) to seek perceptions that don't produce the conflict. Normally, this takes place without notice. If the conflict persists and systematic \"problem solving\" by higher systems fails, the reorganization system may modify existing systems until they bypass the conflict or until they produce new reference signals (goals) that are not in conflict at lower levels.\n\nWhen reorganization results in an arrangement that reduces or eliminates the error that is driving it, the process of reorganization slows or stops with the new organization in place. (This replaces the concept of reinforcement learning.) New means of controlling the perceptions involved, and indeed new perceptual constructs subject to control, may also result from reorganization. In simplest terms, the reorganization process varies things until something works, at which point we say that the organism has learned. When simulations of this method are done in the right way, they demonstrate that this method is surprisingly efficient.\n\nThe reorganization concept has led to a method of psychotherapy called the method of levels (MOL). Using MOL, the therapist aims to help the patient shift his or her awareness to higher levels of perception in order to resolve conflicts and allow reorganization to take place.\n\nCurrently, no one theory has been agreed upon to explain the synaptic, neuronal or systemic basis of learning. Prominent since 1973, however, is the idea that long-term potentiation (LTP) of populations of synapses induces learning through both pre- and postsynaptic mechanisms (Bliss & Lømo, 1973; Bliss & Gardner-Medwin, 1973). LTP is a form of Hebbian learning, which proposed that high-frequency, tonic activation of a circuit of neurones increases the efficacy with which they are activated and the size of their response to a given stimulus as compared to the standard neurone (Hebb, 1949). These mechanisms are the principles behind Hebb's famously simple explanation: \"Those that fire together, wire together\" (Hebb, 1949).\n\nLTP has received much support since it was first observed by Terje Lømo in 1966 and is still the subject of many modern studies and clinical research. However, there are possible alternative mechanisms underlying LTP, as presented by Enoki, Hu, Hamilton and Fine in 2009, published in the journal \"Neuron\". They concede that LTP is the basis of learning. However, they firstly propose that LTP occurs in individual synapses, and this plasticity is graded (as opposed to in a binary mode) and bidirectional (Enoki et al., 2009). Secondly, the group suggest that the synaptic changes are expressed solely presynaptically, via changes in the probability of transmitter release (Enoki et al., 2009). Finally, the team predict that the occurrence of LTP could be age-dependent, as the plasticity of a neonatal brain would be higher than that of a mature one. Therefore, the theories differ, as one proposes an on/off occurrence of LTP by pre- and postsynaptic mechanisms and the other proposes only presynaptic changes, graded ability, and age-dependence.\n\nThese theories do agree on one element of LTP, namely, that it must occur through physical changes to the synaptic membrane/s, i.e. synaptic plasticity. Perceptual control theory encompasses both of these views. It proposes the mechanism of 'reorganisation' as the basis of learning. Reorganisation occurs within the inherent control system of a human or animal by restructuring the inter- and intraconnections of its hierarchical organisation, akin to the neuroscientific phenomenon of neural plasticity. This reorganisation initially allows the trial-and-error form of learning, which is seen in babies, and then progresses to more structured learning through association, apparent in infants, and finally to systematic learning, covering the adult ability to learn from both internally and externally generated stimuli and events. In this way, PCT provides a valid model for learning that combines the biological mechanisms of LTP with an explanation of the progression and change of mechanisms associated with developmental ability (Plooij 1984, 1987, 2003, Plooij & Plooij (1990), 2013).\n\nPowers (2008) produced a simulation of arm co-ordination. He suggested that in order to move your arm, fourteen control systems that control fourteen joint angles are involved, and they reorganise simultaneously and independently. It was found that for optimum performance, the output functions must be organised in a way so as each control system's output only affects the one environmental variable it is perceiving. In this simulation, the reorganising process is working as it should, and just as Powers suggests that it works in humans, reducing outputs that cause error and increasing those that reduce error. Initially, the disturbances have large effects on the angles of the joints, but over time the joint angles match the reference signals more closely due to the system being reorganised. Powers (2008) suggests that in order to achieve coordination of joint angles to produce desired movements, instead of calculating how multiple joint angles must change to produce this movement the brain uses negative feedback systems to generate the joint angles that are required. A single reference signal that is varied in a higher-order system can generate a movement that requires several joint angles to change at the same time.\n\nBotvinick (2008) proposed that one of the founding insights of the cognitive revolution was the recognition of hierarchical structure in human behavior. Despite decades of research, however, the computational mechanisms underlying hierarchically organized behavior are still not fully understood. Bedre, Hoffman, Cooney & D'Esposito (2009) propose that the fundamental goal in cognitive neuroscience is to characterize the functional organization of the frontal cortex that supports the control of action.\n\nRecent neuroimaging data has supported the hypothesis that the frontal lobes are organized hierarchically, such that control is supported in progressively caudal regions as control moves to more concrete specification of action. However, it is still not clear whether lower-order control processors are differentially affected by impairments in higher-order control when between-level interactions are required to complete a task, or whether there are feedback influences of lower-level on higher-level control (Bedre, Hoffman, Cooney & D'Esposito 2009).\n\nBotvinik (2008) found that all existing models of hierarchically structured behavior share at least one general assumption – that the hierarchical, part–whole organization of human action is mirrored in the internal or neural representations underlying it. Specifically, the assumption is that there exist representations not only of low-level motor behaviors, but also separable representations of higher-level behavioral units. The latest crop of models provides new insights, but also poses new or refined questions for empirical research, including how abstract action representations emerge through learning, how they interact with different modes of action control, and how they sort out within the prefrontal cortex (PFC).\n\nPerceptual control theory (PCT) can provide an explanatory model of neural organisation that deals with the current issues. PCT describes the hierarchical character of behavior as being determined by control of hierarchically organized perception. Control systems in the body and in the internal environment of billions of interconnected neurons within the brain are responsible for keeping perceptual signals within survivable limits in the unpredictably variable environment from which those perceptions are derived. PCT does not propose that there is an internal model within which the brain simulates behavior before issuing commands to execute that behavior. Instead, one of its characteristic features is the principled lack of cerebral organisation of behavior. Rather, behavior is the organism's variable means to reduce the discrepancy between perceptions and reference values which are based on various external and internal inputs (Cools, 1985). Behavior must constantly adapt and change for an organism to maintain its perceptual goals. In this way, PCT can provide an explanation of abstract learning through spontaneous reorganisation of the hierarchy. PCT proposes that conflict occurs between disparate reference values for a given perception rather than between different responses (Mansell 2011), and that learning is implemented as trial-and-error changes of the properties of control systems (Marken & Powers 1989), rather than any specific response being \"reinforced\". In this way, behavior remains adaptive to the environment as it unfolds, rather than relying on learned action patterns that may not fit.\n\nHierarchies of perceptual control have been simulated in computer models and have been shown to provide a close match to behavioral data. For example, Marken conducted an experiment comparing the behavior of a perceptual control hierarchy computer model with that of six healthy volunteers in three experiments. The participants were required to keep the distance between a left line and a centre line equal to that of the centre line and a right line. They were also instructed to keep both distances equal to 2 cm. They had 2 paddles in their hands, one controlling the left line and one controlling the middle line. To do this, they had to resist random disturbances applied to the positions of the lines. As the participants achieved control, they managed to nullify the expected effect of the disturbances by moving their paddles. The correlation between the behavior of subjects and the model in all the experiments approached .99. It is proposed that the organization of models of hierarchical control systems such as this informs us about the organization of the human subjects whose behavior it so closely reproduces.\n\nPCT has significant implications for Robotics and Artificial Intelligence. The architecture, of a hierarchy of perceptual controllers, is an ideal and comparatively simple implementation for artificial systems by avoiding the need to generate specific actions whether by complex models of the external world or the computation from input-output mappings. \nThis is in stark contrast to traditional methodologies, such as the computational approach and Behavior-based Robotics.\n\nThe application of perceptual control to Robotics was outlined in a seminal paper in the Artificial Life journal by Rupert Young of Perceptual Robots in 2017. The architecture has been applied to a number of real-world robotic systems including robotic rovers, balancing robot and robot arms.\n\nTraditional approaches to robotics, which generally depend upon the computation of actions in specific situations, result in inflexible, clumsy robots unable to cope with the dynamic nature of the world. PCT robots, on the other hand, demonstrate robots that inherently resist and counter the chaotic, unpredictable world.\n\nThe preceding explanation of PCT principles provides justification of how this theory can provide a valid explanation of neural organisation and how it can explain some of the current issues of conceptual models.\n\nPerceptual control theory currently proposes a hierarchy of 11 levels of perceptions controlled by systems in the human mind and neural architecture. These are: intensity, sensation, configuration, transition, event, relationship, category, sequence, program, principle, and system concept. Diverse perceptual signals at a lower level (e.g. visual perceptions of intensities) are combined in an input function to construct a single perception at the higher level (e.g. visual perception of a color sensation). The perceptions that are constructed and controlled at the lower levels are passed along as the perceptual inputs at the higher levels. The higher levels in turn control by adjusting the reference levels (goals) of the lower levels, in effect telling the lower levels what to perceive.\n\nWhile many computer demonstrations of principles have been developed, the proposed higher levels are difficult to model because too little is known about how the brain works at these levels. Isolated higher-level control processes can be investigated, but models of an extensive hierarchy of control are still only conceptual, or at best rudimentary.\n\nPerceptual control theory has not been widely accepted in mainstream psychology, but has been effectively used in a considerable range of domains in human factors, clinical psychology, and psychotherapy (the \"Method of Levels\"), it is the basis for a considerable body of research in sociology, and it has formed the conceptual foundation for the reference model used by a succession of NATO research study groups. It is being taught in several universities worldwide and is the subject of a number of PhD dissertations.\n\n\n\n\n\n"}
{"id": "8531265", "url": "https://en.wikipedia.org/wiki?curid=8531265", "title": "Perfect spline", "text": "Perfect spline\n\nIn the mathematical subfields function theory and numerical analysis, a univariate polynomial spline of order formula_1 is called a perfect spline if its formula_1-th derivative is equal to formula_3 or formula_4 between knots and changes its sign at every knot.\n\nThe term was coined by Isaac Jacob Schoenberg.\n\nPerfect splines often give solutions to various extremal problems in mathematics. For example, norms of periodic perfect splines (they are sometimes called Euler perfect splines) are equal to Favard's constants.\n"}
{"id": "1225337", "url": "https://en.wikipedia.org/wiki?curid=1225337", "title": "Principal part", "text": "Principal part\n\nIn mathematics, the principal part has several independent meanings, but usually refers to the negative-power portion of the Laurent series of a function.\n\nThe principal part at formula_1 of a function\nis the portion of the Laurent series consisting of terms with negative degree. That is,\nis the principal part of formula_4 at formula_5.\nIf the Laurent series has an inner radius of convergence of 0 , then formula_6 has an essential singularity at formula_7, if and only if the principal part is an infinite sum. If the inner radius of convergence is not 0, then formula_6 may be regular at formula_7 despite the Laurent series having an infinite principal part.\n\nConsider the difference between the function differential and the actual increment:\nThe differential \"dy\" is sometimes called the principal (linear) part of the function increment \"Δy\".\n\nThe term principal part is also used for certain kinds of distributions having a singular support at a single point.\n\n\n"}
{"id": "38368658", "url": "https://en.wikipedia.org/wiki?curid=38368658", "title": "Profit model", "text": "Profit model\n\nThe profit model is the linear, deterministic algebraic model used implicitly by most cost accountants. Starting with, profit equals sales minus costs, it provides a structure for modeling cost elements such as materials, losses, multi-products, learning, depreciation etc. It provides a mutable conceptual base for spreadsheet modelers. This enables them to run deterministic simulations or 'what if' modelling to see the impact of price, cost or quantity changes on profitability.\n\nwhere:\n\nFor an expansion of the model see below.\n\nThe justification for wanting to express profit as an algebraic model is given by Mattessich in 1961:\n\nMost of the definitions in cost accounting are in an unclear narrative form, not readily associated with other definitions of accounting calculations. For example, preparing a comparison of fixed cost variances in stock under different stock valuation methods can be confusing. Another example is modelling labour variances with learning curve corrections and stock level changes. With the absence of a basic profit model in an algebraic form, confident development of such models is difficult.\n\nThe development of spreadsheets has led to a decentralisation of financial modelling. This has often resulted in model builders lacking training in model construction. Before any professional model is built it is usually considered wise to start by developing a mathematical model for analysis. The profit model provides a general framework plus some specific examples of how such an a priori profit model might be constructed.\n\nThe presentation of a profit model in an algebraic form is not new. Mattessich's model, while large, does not include many costing techniques such as learning curves and different stock valuation methods. Also, it was not presented in a form that most accountants were willing or able to read. This paper presents a more extended model analysing profit but it does not, unlike Mattessich, extend to the balance sheet model. Its form, of starting with the basic definition of profit and becoming more elaborate, may make it more accessible to accountants.\n\nMost cost accounting textbooks explain basic Cost Volume Profit modeling in an algebraic form, but then revert to an 'illustrative' approach. This 'illustrative' approach uses examples or narrative to explain management accounting procedures. This format, though useful when communicating with humans, can be difficult to translate into an algebraic form, suitable for computer model building. Mepham extended the algebraic, or deductive, approach to cost accounting to cover many more techniques. He develops his model to integrate with the optimizing models in operations research. The profit model comes out of Mephams work, extending it but only in a descriptive, linear form.\n\nThe basic profit model is sales minus costs. Sales are made up of quantity sold multiplied by their price. Costs are usually divided between Fixed costs and variable costs.\n\nUsing:\n\nThus the profit can be calculated from:\n\nNotice that \"w\" (average unit production cost) includes the fixed and variable costs.\nThe square brackets contain the cost of goods sold, \"wq\" not cost of good made \"wx\" where \"x\" = cost of good sold.\n\nTo show cost of good sold, the opening and closing finished goods stocks need to be included\nThe profit model would then be:\n\n\nPresenting the profit calculation in this form immediately demands that some of the costs be more carefully defined.\n\nThe unit production costs (\"w\") can be separated into fixed and variable costs:\n\nwhere\n\n\nThe introduction of this separation of \"w\" allows for consideration of the behaviour of costs for different levels of production. A linear cost curve is assumed here, divided between the constant (\"F\") and its slope (\"v\"). If the modeller has access to the details of non-linear cost curves then \"w\" will need to be defined by the appropriate function.\n\nReplacing \"wx\" in (equation 2) and making \"F\" = \"F\" + \"F\":\n\nMoving on to other extensions of the basic model, the cost elements such as direct materials, direct labour and variable overheads can be included. If a non-linear function is available and thought useful such functions can be substituted for the functions used here.\n\nThe material cost of sales = m * µ * q, where\n\nm is the amount of material in one unit of finished goods.\n\nµ is the cost per unit of the raw material.\n\nThe labour cost of sales = \"l λ q\", where\n\n\nThe variable overhead cost of sales = \"nq\" where \"n\" is the variable overhead cost per unit.\n\nThis is not here subdivided between quantity per finished goods units and cost per unit.\n\nThus the variable cost v * q can now be elaborated into:\n\nIf the production quantity is required the finished goods stock will need to be added.\n\nIn a simple case two materials can be accommodated in the model by simply adding another m * µ. In more realistic situations a matrix and a vector will be necessary (see later).\n\nIf material cost of purchases is to be used rather than material cost of production it will be necessary to adjust for material stocks. That is,\n\nwhere \n\nAll depreciation rules can be stated as equations representing their curve over time. The reducing balance method provides one of the more interesting examples.\n\nUsing c = cost, t = time, L = life, s = scrap value, Fd = time based depreciation:\n\nThis equation is better known as the rule: Depreciation per year = Last year's written down value multiplied by a constant %\n\nThe limits are 0 < t < L, and the scrap value has to be greater than zero. (For zero use 0.1).\n\nRemembering that time-based depreciation is a fixed cost and usage-based depreciation can be a variable cost, depreciation can easily be added into the model (equation 5).\n\nThus, the profit model becomes:\n\nwhere, nd = usage (as q) based depreciation and π = annual profit.\n\nIn the above, the value of the unit finished goods cost ‘w' was left undefined. There are numerous alternatives to how stock (w) is valued but only two will be compared here.\n\nThe marginal versus absorption costing debate, includes the question of the valuation of stock (w).\n\nShould w = v or as (3) w = (Fm + v x)/x.\n\n(i) Under marginal costing: w = v. Inserting in (4),\n\nBecomes\n\nThis can be simplified by taking v out and noting, opening stock quantity + production - closing stock quantity = sales quantity (q) so,\n\nNote, v q = variable cost of goods sold.\n\n(ii) Using full (absorption) costing\nUsing (equation 3), where xp = planned production, x1 = period production\nw = (Fm + v xp)/xp = Fm/xp + v.\nThis can be shown to result in:\n\nNote the strange presence of 'x' in the model.\nNotice also that the absorption model (equation 10) is the same as the marginal costing model (equation 9) except for the end part:\n\nThis part represents the fixed costs in stock. This is better seen by remem¬bering q — x= go—g1 so it could be written\n\nThe model form with 'q' and 'x' in place of' g and g allows profits to be calculated when only the sales and production figures are known.\n\nA spreadsheet could be prepared for a company with increasing then decreasing levels of sales and constant production. It could have another column showing profit under increasing sales and constant production. Thus the effects of carrying fixed costs in stock can be simulated. Such modelling thus provides a very useful tool in the marginal versus full costing debate.\n\nOne way of modeling for losses is to use: \n\nThe model, with all these losses together will look like,\n\nNote that labour and variable overhead losses could also have been included.\n\nSo far the model has assumed very few products and/or cost elements. As many firms are multi-product the model they use must be able to handle this problem. Whilst the mathematics here is straightforward the accounting problems introduced are enormous: the cost allocation problem being a good example. Other examples include calculation of break-even points, productivity measures and the optimisation of limited resources. Here only the mechanics of building a multi-dimension model will be outlined.\n\nIf a firm sells two products (a and b) then the profit model (equation 9),\n\nAll fixed costs have been combined in F\n\nTherefore, for multiple products\n\nWhere Σ = the sum of. Which can be drafted as a vector or matrix in a spreadsheet\n\nor\n\nThe profit model may represent actual data (c), planned data (p)or standard data (s) which is the actual sales quantities at the planned costs.\n\nThe actual data model will be (using equation 8):\n\nThe planned data model will be (using equation 8):\n\nThe standard data model will be (using equation 8):\n\nOperating variances are obtained by subtracting the actual model from the standard model.\n\nIt is possible to add non linear cost curves to the Profit model. For example, if with learning, the labour time per unit will decrease exponentially over time as more product is made, then the time per unit is:\n\nwhere \nr = average time. \nb = learning rate.\nq = quantity.\n\nInserting into equation 8\n\nThis equation is best solved by trial and error, the Newton Raphson method or graphing. Like depreciation within the model, the adjustment for learning does provide a form of non-linear sub-modelling.\n\nRather than the variable be absolute amounts, they might be percentage changes. This represents a major change in approach from the model above. The model is often used in the 'now that ... (say) the cost of labour has gone up by 10%' format. If a model can be developed that only uses such percentage changes then the cost of collecting absolute quantities will be saved.\n\nThe notation used below is of attaching a % sign to variables to indicate the change of that variable, for example, p% = 0.10 if the selling price is assumed to change by 10%,\n\nLet x = q and C = contribution\n\nStarting with the absolute form of the contribution model (equation (9) rearranged):\n\nThe increase in the contribution which results from an increase in p, v and/or q can be calculated thus:\n\nrearranging and using α = (p — v)/p,\n\nThis model might look messy but it is very powerful. It makes very few demands on data, especially if some of the variables do not change. It is possible to develop most of the models presented above in this percentage change format.\n\n\n"}
{"id": "42158532", "url": "https://en.wikipedia.org/wiki?curid=42158532", "title": "Redundant proof", "text": "Redundant proof\n\nIn mathematical logic, a redundant proof is a proof that has a subset that is a shorter proof of the same result. That is, a proof formula_1 of formula_2 is considered redundant if there exists another proof formula_3 of formula_4 such that formula_5 (i.e. formula_6) and formula_7 where formula_8 is the number of nodes in formula_9.\n\nA proof containing a subproof of the shapes (here omitted pivots indicate that the resolvents must be uniquely defined)\n\nis locally redundant.\n\nIndeed, both of these subproofs can be equivalently replaced by the shorter subproof formula_11. In the case of local redundancy, the pairs of redundant inferences having the same pivot occur close to each other in the proof. However, redundant inferences can also occur far apart in the proof.\n\nThe following definition generalizes local redundancy by considering inferences with the same pivot that occur within different contexts. We write formula_12 to denote a proof-context formula_13 with a single placeholder replaced by the subproof formula_14.\n\nA proof\n\nis potentially (globally) redundant. Furthermore, it is (globally) redundant if it can be rewritten to one of the following shorter proofs:\n\nThe proof\n\nis locally redundant as it is an instance of the first pattern in the definition formula_18\n\n\nBut it is not globally redundant because the replacement terms according to the definition contain formula_21 in all the cases and formula_22 does not correspond to a proof. In particular, neither formula_23 nor formula_24 can be resolved with formula_25, as they do not contain the literal formula_26.\n\nThe second pattern of potentially globally redundant proofs appearing in global redundancy definition is related to the well-known notion of regularity. Informally, a proof is irregular if there is a path from a node to the root of the proof such that a literal is used more than once as a pivot in this path.\n"}
{"id": "17695790", "url": "https://en.wikipedia.org/wiki?curid=17695790", "title": "Rising sun lemma", "text": "Rising sun lemma\n\nIn mathematical analysis, the rising sun lemma is a lemma due to Frigyes Riesz, used in the proof of the Hardy–Littlewood maximal theorem. The lemma was a precursor in one dimension of the Calderón–Zygmund lemma.\n\nThe lemma is stated as follows:\n\nThe colorful name of the lemma comes from imagining the graph of the function \"g\" as a mountainous landscape, \nwith the sun shining horizontally from the right. The set \"E\" consist of points that are in the shadow.\n\nWe need a lemma: Suppose [\"c\",\"d\") ⊂ \"S\", but d ∉ \"S\". Then \"g\"(\"c\") < \"g\"(\"d\").\nTo prove this, suppose \"g\"(\"c\") ≥ \"g\"(\"d\").\nThen \"g\" achieves its maximum on [\"c\",\"d\"] at some point \"z\" < \"d\".\nSince \"z\" ∈ \"S\", there is a \"y\" in (\"z\",\"b\"] with \"g\"(\"z\") < \"g\"(\"y\"). \nIf \"y\" ≤ \"d\", then \"g\" would not reach its maximum on [\"c\",\"d\"] at \"z\".\nThus, \"y\" ∈ (\"d\",\"b\"], and \"g\"(\"d\") ≤ \"g\"(\"z\") < \"g\"(\"y\").\nThis means that \"d\" ∈ \"S\", which is a contradiction, thus establishing the lemma.\n\nThe set \"E\" is open, so it is composed of a countable union of disjoint intervals (\"a\",\"b\").\n\nIt follows immediately from the lemma that \"g\"(\"x\") < \"g\"(\"b\") for \"x\" in \n(\"a\",\"b\").\nSince \"g\" is continuous, we must also have \"g\"(\"a\") ≤ \"g\"(\"b\"). \n\nIf \"a\" ≠ \"a\" or \"a\" ∉ \"S\", then \"a\" ∉ \"S\", \nso \"g\"(\"a\") ≥ \"g\"(\"b\"), for otherwise \"a\" ∈ \"S\". \nThus, \"g\"(\"a\") = \"g\"(\"b\") in these cases.\n\nFinally, if \"a\" = \"a\" ∈ \"S\", the lemma tells us that \"g\"(\"a\") < \"g\"(\"b\").\n\n"}
{"id": "18696734", "url": "https://en.wikipedia.org/wiki?curid=18696734", "title": "Stellation diagram", "text": "Stellation diagram\n\nIn geometry, a stellation diagram or stellation pattern is a two-dimensional diagram in the plane of some face of a polyhedron, showing lines where other face planes intersect with this one. The lines cause 2D space to be divided up into regions. Regions not intersected by any further lines are called elementary regions. Usually infinite regions are excluded from the diagram, along with any infinite portions of the lines. Each elementary region represents a top face of one cell, and a bottom face of another.\n\nA collection of these diagrams, one for each face type, can be used to represent any stellation of the polyhedron, by shading the regions which should appear in that stellation.\n\nA stellation diagram exists for every face of a given polyhedron. In face transitive polyhedra, symmetry can be used to require all faces have the same diagram shading. Semiregular polyhedra like the Archimedean solids will have different stellation diagrams for different kinds of faces.\n\n\n\n"}
{"id": "2607053", "url": "https://en.wikipedia.org/wiki?curid=2607053", "title": "Tetraview", "text": "Tetraview\n\nA tetraview is an attempt to graph a complex function of a complex variable, by a method invented by Davide P. Cervone.\n\nA graph of a real function of a real variable is the set of ordered pairs (x,y) such that y = f(x). This is the ordinary two-dimensional Cartesian graph studied in school algebra.\n\nEvery complex number has both a real part and an imaginary part, so one complex variable is two-dimensional and a pair of complex variables is four-dimensional. A tetraview is an attempt to give a picture of a four-dimensional object using a two-dimensional representation—either on a piece of paper or on a computer screen, showing a still picture consisting of five views, one in the center and one at each corner. This is roughly analogous to a picture of a three-dimensional object by giving a front view, a side view, and a view from above.\n\nA picture of a three-dimensional object is a projection of that object from three dimensions into two dimensions. A tetraview is set of five projections, first from four dimensions into three dimensions, and then from three dimensions into two dimensions.\n\nA complex function w = f(z), where z = a + b\"i\" and w = c + d\"i\" are complex numbers, has a graph in four-space (four dimensional space) R consisting of all points (a, b, c, d) such that c + d\"i\" = f(a + b\"i\").\n\nTo construct a tetraview, we begin with the four points (1,0,0,0), (0, 1, 0, 0), (0, 0, 1, 0), and (0, 0, 0, 1), which are vertices of a spherical tetrahedron on the unit three-sphere S in R.\n\nWe project the four-dimensional graph onto the three-dimensional sphere along one of the four coordinate axes, and then give a two-dimensional picture of the resulting three-dimensional graph. This provides the four corner graph. The graph in the center is a similar picture \"taken\" from the point of view of the origin.\n\n"}
{"id": "3397404", "url": "https://en.wikipedia.org/wiki?curid=3397404", "title": "Thomae's function", "text": "Thomae's function\n\nThomae's function, named after Carl Johannes Thomae, has many names: the popcorn function, the raindrop function, the countable cloud function, the modified Dirichlet function, the ruler function, the Riemann function, or the Stars over Babylon (John Horton Conway's name). This real-valued function \"f\"(\"x\") of the real variable \"x\" is defined as:\n\nSince every rational number has a unique representation with relatively prime formula_2 and formula_3, the function is well-defined. Note that formula_4 is the only number in formula_5 which is coprime to formula_6\n\nIt is a modification of the Dirichlet function, which is 1 at rational numbers and 0 elsewhere.\n\n\nFor all formula_11 since formula_12\n\nFor all formula_13 there exist formula_2 and formula_15 such that formula_16 and formula_17\nAccording to Bézout's identity, for some formula_18 holds\n\nand thus formula_20\nAssume an arbitrary rational formula_22 with formula_23 and formula_24 coprime.\n\nThis establishes formula_25\n\nLet formula_26 be any irrational number and define for all formula_27 formula_28\n\nThese formula_29 are all irrational, and so formula_30\n\nThis implies formula_31 and formula_32\n\nTaking formula_33 and formula_34 selects an formula_35 such that\n\nfor all formula_37formula_38\n\nwhich is exactly the definition of discontinuity of formula_7 at formula_40.\n\nThis shows that formula_7 is discontinuous on formula_42\nSince formula_7 is periodic with period formula_45 and formula_46 it suffices to check all irrational points in formula_47 Assume now formula_48 and formula_49 According to the Archimedean property of the reals, there exists formula_50 with formula_51 and there exist formula_52 such that\n\nfor formula_53 formula_54\n\nThe minimal distance of formula_40 to its i-th lower and upper bounds equals\n\nWe define formula_57 as the minimum of all the finitely many formula_58\n\nfor all formula_60 and formula_61\n\nThis is to say, that all these rational numbers formula_62 are outside the \n\nNow let formula_63 with the unique representation formula_64 where formula_65 are coprime. Then, necessarily, formula_66 and therefore, \n\nLikewise, for all irrational formula_68 and thus, if formula_69 then any choice of (sufficiently small) formula_70 gives \n\nTherefore, formula_7 is continuous on formula_73\n\n\n\nA natural follow-up question one might ask is if there is a function which is continuous on the rational numbers and discontinuous on the irrational numbers. This turns out to be impossible; the set of discontinuities of any function must be an set. If such a function existed, then the irrationals would be an set. The irrationals would then be the countable union of closed sets formula_90, but since the irrationals do not contain an interval, nor can any of the formula_91. Therefore, each of the formula_91 would be nowhere dense, and the irrationals would be a meager set. It would follow that the real numbers, being a union of the irrationals and the rationals (which is evidently meager), would also be a meager set. This would contradict the Baire category theorem: because the reals form a complete metric space, they form a Baire space, which cannot be meager in itself.\n\nA variant of Thomae's function can be used to show that any subset of the real numbers can be the set of discontinuities of a function. If formula_93 is a countable union of closed sets formula_94, define\n\nThen a similar argument as for Thomae's function shows that formula_96 has \"A\" as its set of discontinuities.\n\nFor a general construction on arbitrary metric space, see this article Kim, Sung Soo. \"A Characterization of the Set of Points of Continuity of a Real Function.\" American Mathematical Monthly 106.3 (1999): 258-259.\n\nEmpirical probability distributions related to Thomae's function appear in DNA sequencing. The human genome is diploid, having two strands per chromosome. When sequenced, small pieces (\"reads\") are generated: for each spot on the genome, an integer number of reads overlap with it. Their ratio is a rational number, and typically distributed similarly to Thomae's function.\n\nIf pairs of positive integers formula_97 are sampled from a distribution formula_98 and used to generate ratios formula_99, this gives rise to a distribution formula_100 on the rational numbers. If the integers are independent the distribution can be viewed as a convolution over the rational numbers, formula_101. Closed form solutions exist for power-law distributions with a cut-off. If formula_102 (where formula_103 is the polylogarithm function) then formula_104. In the case of uniform distributions on the set formula_105 formula_106, which is very similar to Thomae's function. Both their graphs have fractal dimension 3/2.\n\nFor integers, the exponent of the highest power of 2 dividing formula_107 gives 0, 1, 0, 2, 0, 1, 0, 3, 0, 1, 0, 2, 0, 1, 0, ... . If 1 is added, or if the 0's are removed, 1, 2, 1, 3, 1, 2, 1, 4, 1, 2, 1, 3, 1, 2, 1, ... . The values resemble tick-marks on a 1/16th graduated ruler, hence the name. These values correspond to the restriction of the Thomae function to those rational numbers whose denominators are powers of 2.\n\n\n"}
{"id": "621230", "url": "https://en.wikipedia.org/wiki?curid=621230", "title": "Tolerant sequence", "text": "Tolerant sequence\n\nIn mathematical logic, a tolerant sequence is a sequence \n\nof formal theories such that there are consistent extensions \n\nof these theories with each formula_5 interpretable in formula_6. Tolerance naturally generalizes from sequences of theories to trees of theories. Weak interpretability can be shown to be a special, binary case of tolerance. \n\nThis concept, together with its dual concept of cotolerance, was introduced by Japaridze in 1992, who also proved that, for Peano arithmetic and any stronger theories with effective axiomatizations, tolerance is equivalent to formula_7-consistency.\n\n\n"}
{"id": "35173610", "url": "https://en.wikipedia.org/wiki?curid=35173610", "title": "Torsion conjecture", "text": "Torsion conjecture\n\nIn algebraic geometry and number theory, the torsion conjecture or uniform boundedness conjecture for abelian varieties states that the order of the torsion group of an abelian variety over a number field can be bounded in terms of the dimension of the variety and the number field. A stronger version of the conjecture is that the torsion is bounded in terms of the dimension of the variety and the degree of the number field.\n\nThe (strong) torsion conjecture has been completely resolved in the case of elliptic curves. Barry Mazur proved uniform boundedness for elliptic curves over the rationals. His techniques were generalized by and , who obtained uniform boundedness for quadratic fields and number fields of degree at most 8 respectively. Finally, Loïc Merel () proved the conjecture for elliptic curves over any number field. The proof centers around a careful study of the rational points on classical modular curves. An effective bound for the size of the torsion group in terms of the degree of the number field was given by .\n\nMazur provided a complete list of possible torsion subgroups for rational elliptic curves. If \"C\" denotes the cyclic group of order \"n\", then the possible torsion subgroups are \"C\" with 1 ≤ \"n\" ≤ 10, and also \"C\"; and the direct sum of \"C\" with \"C\", \"C\", \"C\" or \"C\". In the opposite direction, all these torsion structures occur infinitely often over Q, since the corresponding modular curves are all genus zero curves with a rational point. A complete list of possible torsion groups is also available for elliptic curves over quadratic number fields, and there are substantial partial results for cubic and quartic number fields .\n\n"}
{"id": "1693123", "url": "https://en.wikipedia.org/wiki?curid=1693123", "title": "Triple bar", "text": "Triple bar\n\nThe triple bar, ≡, is a symbol with multiple, context-dependent meanings. It has the appearance of an \"=\" sign with a third line. The triple bar character in Unicode is code point . The closely related code point is the same symbol with a slash through it, indicating the negation of its mathematical meaning. In LaTeX mathematical formulas, the code codice_1 produces the triple bar symbol and codice_2 produces the negated triple bar symbol as output.\n\nIn logic, it is used with two different but related meanings. It can refer to the if and only if connective, also called material equivalence. This is a binary operation whose value is true when its two arguments have the same value as each other. Alternatively, in some texts ⇔ is used with this meaning, while ≡ is used for the higher-level metalogical notion of logical equivalence, according to which two formulas are logically equivalent when all models give them the same value. Gottlob Frege used a triple bar for a more philosophical notion of identity, in which two statements (not necessarily in mathematics or formal logic) are identical if they can be freely substituted for each other without change of meaning.\n\nIn mathematics, the triple bar is sometimes used as a symbol of identity or an equivalence relation (although not the only one; other common choices include ~ and ≈). Particularly, in geometry, it may be used either to show that two figures are congruent or that they are identical. In number theory, it has been used beginning with Carl Friedrich Gauss (who first used it with this meaning in 1801) to mean modular congruence: formula_1 if \"N\" divides \"a\" − \"b\". It is also used for \"identical equality\" of functions; one writes formula_2 for two functions \"f\", \"g\" if we have formula_3 for all \"x\".\n\nIn category theory, triple bars may be used to connect objects in a commutative diagram, indicating that they are actually the same object rather than being connected by an arrow of the category.\n\nThis symbol is also sometimes used in place of an equal sign for equations that define the symbol on the left-hand side of the equation, to contrast them with equations in which the terms on both sides of the equation were already defined. An alternative notation for this usage is to typeset the letters \"def\" above an ordinary equality sign, formula_4.\n\nIn botanical nomenclature, the triple bar denotes homotypic synonyms (those based on the same type specimen), to distinguish them from heterotypic synonyms (those based on different type specimens), which are marked with an equals sign.\n\nIn chemistry, the triple bar can be used to represent a triple bond between atoms. For example, HC≡CH is a common shorthand for acetylene (systematic name: ethyne).\n\nIn mobile, web, and general application design, a similar symbol is sometimes used as an interface element, where it is called a hamburger icon. The element typically indicates that a navigation menu can be accessed when the element is activated; the bars of the symbol may be seen as stylized menu items, and some variations of this symbols add more bars, or bullet points to each bar, to enhance this visual similarity. Usage of this symbol dates back to the early computer interfaces developed at Xerox PARC in the 1980s. It is also similar to the icon frequently used to indicate justified text alignment. It is an oft-used component of Google's Material Design guidelines and many Android apps and web apps that follow these guidelines make use of the hamburger menu.\n"}
{"id": "2673165", "url": "https://en.wikipedia.org/wiki?curid=2673165", "title": "Ultraviolet fixed point", "text": "Ultraviolet fixed point\n\nIn a quantum field theory, one may calculate an effective or running coupling constant that defines the coupling of the theory measured at a given momentum scale. One example of such a coupling constant is the electric charge. In approximate calculations in several quantum field theories, notably quantum electrodynamics and theories of the Higgs particle, the running coupling appears to become infinite at a finite momentum scale. This is sometimes called the Landau pole problem. It is not known whether the appearance of these inconsistencies is an artifact of the approximation, or a real fundamental problem in the theory. However, the problem can be avoided if an ultraviolet or UV fixed point appears in the theory. A quantum field theory has a UV fixed point if its renormalization group flow approaches a fixed point in the ultraviolet (i.e. short length scale/large energy) limit. This is related to zeroes of the beta-function appearing in the Callan-Symanzik equation. The large length scale/small energy limit counterpart is the infrared fixed point.\n\nAmong other things, it means that a theory possessing a UV fixed point may not be an effective field theory, because it is well-defined at arbitrarily small distance scales. At the UV fixed point itself, the theory can behave as a conformal field theory.\n\nThe converse statement, that any QFT which is valid at all distance scales (i.e. isn't an effective field theory) has a UV fixed point is false. See, for example, cascading gauge theory.\n\nNoncommutative quantum field theories have a UV cutoff even though they are not effective field theories.\n\nPhysicists distinguish between trivial and nontrivial fixed points. If a UV fixed point is trivial (generally known as Gaussian fixed point), the theory is said to be asymptotically free. On the other hand, a scenario, where a non-Gaussian (i.e. nontrival) fixed point is approached in the UV limit, is referred to as asymptotic safety. Asymptotically safe theories may be well defined at all scales despite being nonrenormalizable in perturbative sense (according to the classical scaling dimensions).\n\nSteven Weinberg has proposed that the problematic UV divergences appearing in quantum theories of gravity may be cured by means of a nontrivial UV fixed point. Such an asymptotically safe theory is renormalizable in a nonperturbative sense, and due to the fixed point physical quantities are free from divergences. As yet, a general proof for the existence of the fixed point is still lacking, but there is mounting evidence for this scenario.\n\n"}
{"id": "27072071", "url": "https://en.wikipedia.org/wiki?curid=27072071", "title": "Unit circle", "text": "Unit circle\n\nIn mathematics, a unit circle is a circle with a radius of one. Frequently, especially in trigonometry, the unit circle is the circle of radius one centered at the origin (0, 0) in the Cartesian coordinate system in the Euclidean plane. The unit circle is often denoted ; the generalization to higher dimensions is the unit sphere.\n\nIf is a point on the unit circle's circumference, then and are the lengths of the legs of a right triangle whose hypotenuse has length 1. Thus, by the Pythagorean theorem, and satisfy the equation\n\nSince for all , and since the reflection of any point on the unit circle about the - or -axis is also on the unit circle, the above equation holds for all points on the unit circle, not only those in the first quadrant.\n\nThe interior of the unit circle is called the open unit disk, while the interior of the unit circle combined with the unit circle itself is called the closed unit disk.\n\nOne may also use other notions of \"distance\" to define other \"unit circles\", such as the Riemannian circle; see the article on mathematical norms for additional examples.\n\nThe unit circle can be considered as the unit complex numbers, i.e., the set of complex numbers of the form \nfor all (see also: cis). This relation represents Euler's formula.\nIn quantum mechanics, this is referred to as phase factor.\n\nThe trigonometric functions cosine and sine of angle may be defined on the unit circle as follows: If is a point on the unit circle, and if the ray from the origin (0, 0) to makes an angle from the positive -axis, (where counterclockwise turning is positive), then\n\nThe equation gives the relation\n\nThe unit circle also demonstrates that sine and cosine are periodic functions, with the identities\n\nfor any integer .\n\nTriangles constructed on the unit circle can also be used to illustrate the periodicity of the trigonometric functions. First, construct a radius OA from the origin to a point on the unit circle such that an angle with is formed with the positive arm of the -axis. Now consider a point and line segments . The result is a right triangle with . Because has length , length , and length 1, and . Having established these equivalences, take another radius OR from the origin to a point on the circle such that the same angle is formed with the negative arm of the -axis. Now consider a point and line segments . The result is a right triangle with . It can hence be seen that, because , is at in the same way that P is at . The conclusion is that, since is the same as and is the same as , it is true that and . It may be inferred in a similar manner that , since and . A simple demonstration of the above can be seen in the equality .\n\nWhen working with right triangles, sine, cosine, and other trigonometric functions only make sense for angle measures more than zero and less than . However, when defined with the unit circle, these functions produce meaningful values for any real-valued angle measure – even those greater than 2. In fact, all six standard trigonometric functions – sine, cosine, tangent, cotangent, secant, and cosecant, as well as archaic functions like versine and exsecant – can be defined geometrically in terms of a unit circle, as shown at right.\n\nUsing the unit circle, the values of any trigonometric function for many angles other than those labeled can be calculated without the use of a calculator by using the angle sum and difference formulas.\n\nComplex numbers can be identified with points in the Euclidean plane, namely the number is identified with the point . Under this identification, the unit circle is a group under multiplication, called the \"circle group\"; it is usually denoted . On the plane, multiplication by gives a counterclockwise rotation by . This group has important applications in mathematics and science.\n\nJulia set of discrete nonlinear dynamical system with evolution function:\n\nis a unit circle. It is a simplest case so it is widely used in study of dynamical systems.\n\n"}
{"id": "20983231", "url": "https://en.wikipedia.org/wiki?curid=20983231", "title": "Urysohn universal space", "text": "Urysohn universal space\n\nThe Urysohn universal space is a certain metric space that contains all separable metric spaces in a particularly nice manner. This mathematics concept is due to Pavel Samuilovich Urysohn.\n\nA metric space (\"U\",\"d\") is called \"Urysohn universal\" if it is separable and complete and has the following property: \n\nIf \"U\" is Urysohn universal and \"X\" is any separable metric space, then there exists an isometric embedding \"f\":\"X\" → \"U\". (Other spaces share this property: for instance, the space \"l\" of all bounded real sequences with the supremum norm admits isometric embeddings of all separable metric spaces (\"Fréchet embedding\"), as does the space C[0,1] of all continuous functions [0,1]→R, again with the supremum norm, a result due to Stefan Banach.)\n\nFurthermore, every isometry between finite subsets of \"U\" extends to an isometry of \"U\" onto itself. This kind of \"homogeneity\" actually characterizes Urysohn universal spaces: A separable complete metric space that contains an isometric image of every separable metric space is Urysohn universal if and only if it is homogeneous in this sense.\n\nUrysohn proved that an Urysohn universal space exists, and that any two Urysohn universal spaces are isometric. This can be seen as follows. Take formula_1, two Urysohn spaces. These are separable, so fix in the respective spaces countable dense subsets formula_2. These must be properly infinite, so by a back-and-forth argument, one can step-wise construct partial isometries formula_3 whose domain (resp. range) contains formula_4 (resp. formula_5). The union of these maps defines a partial isometry formula_6 whose domain resp. range are dense in the respective spaces. And such maps extend (uniquely) to isometries, since a Urysohn space is required to be complete.\n"}
{"id": "20679744", "url": "https://en.wikipedia.org/wiki?curid=20679744", "title": "Whittaker model", "text": "Whittaker model\n\nIn representation theory, a branch of mathematics, the Whittaker model is a realization of a representation of a reductive algebraic group such as \"GL\" over a finite or local or global field on a space of functions on the group. It is named after E. T. Whittaker even though he never worked in this area, because pointed out that for the group SL(R) some of the functions involved in the representation are Whittaker functions.\n\nIrreducible representations without a Whittaker model are sometimes called \"degenerate\", and those with a Whittaker model are sometimes called \"generic\". The representation θ of the symplectic group Sp is the simplest example of a degenerate representation.\n\nIf \"G\" is the algebraic group \"GL\" and F is a local field,\nand τ is a fixed non-trivial character of the additive group of F and π is an irreducible representation of a general linear group \"G\"(F), then the Whittaker model for π is a representation π on a space of functions \"f\" on \"G\"(F) satisfying\n\nLet formula_2 be the general linear group formula_3, formula_4 a smooth complex valued non-trivial additive character of formula_5 and formula_6 the subgroup of formula_3 consisting of unipotent upper triangular matrices. A non-degenerate character on formula_6 is of the form\n\nfor formula_10 ∈ formula_6 and non-zero formula_12, ..., formula_13 ∈ formula_5. If formula_15 is a smooth representation of formula_16, a Whittaker functional formula_17 is a continuous linear functional on formula_18 such that formula_19 for all formula_20 ∈ formula_6, formula_22 ∈ formula_18. Multiplicity one states that, for formula_24 unitary irreducible, the space of Whittaker functionals has dimension at most equal to one.\n\nIf \"G\" is a split reductive group and \"U\" is the unipotent radical of a Borel subgroup \"B\", then a Whittaker model for a representation is an embedding of it into the induced (Gelfand–Graev) representation Ind(χ), where χ is a non-degenerate character of \"U\", such as the sum of the characters corresponding to simple roots.\n\n\n"}
{"id": "1260119", "url": "https://en.wikipedia.org/wiki?curid=1260119", "title": "Zeitschrift für Angewandte Mathematik und Physik", "text": "Zeitschrift für Angewandte Mathematik und Physik\n\nThe Zeitschrift für Angewandte Mathematik und Physik (English: \"Journal of Applied Mathematics and Physics\") is a bimonthly peer-reviewed scientific journal published by Birkhäuser Verlag. The editor-in-chief is Kaspar Nipp (ETH Zurich). It was established in 1950 and covers the fields of fluid mechanics, solid mechanics, differential equations/applied mathematics, and related topics. According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 1.711.\n"}
{"id": "3014017", "url": "https://en.wikipedia.org/wiki?curid=3014017", "title": "Zeta function regularization", "text": "Zeta function regularization\n\nIn mathematics and theoretical physics, zeta function regularization is a type of regularization or summability method that assigns finite values to divergent sums or products, and in particular can be used to define determinants and traces of some self-adjoint operators. The technique is now commonly applied to problems in physics, but has its origins in attempts to give precise meanings to ill-conditioned sums appearing in number theory.\n\nThere are several different summation methods called zeta function regularization for defining the sum of a possibly divergent series \n\nOne method is to define its zeta regularized sum to be ζ(−1) if this is defined, where the zeta function is defined for Re(\"s\") large by\n\nif this sum converges, and by analytic continuation elsewhere.\n\nIn the case when \"a\" = \"n\", the zeta function is the ordinary Riemann zeta function, and this method was used by Euler to \"sum\" the series 1 + 2 + 3 + 4 + ... to ζ(−1) = −1/12.\n\nOther values of \"s\" can also be used to assign values for the divergent sums\n\nand in general\n\nwhere \"B\" is a Bernoulli number.\n\nAnother method defines the possibly divergent infinite product \"a\"\"a\"... to be exp(−ζ′(0)). used this to define the determinant of a positive self-adjoint operator \"A\" (the Laplacian of a Riemannian manifold in their application) with eigenvalues \"a\", \"a\", ..., and in this case the zeta function is formally the trace of \"A\". showed that if \"A\" is the Laplacian of a compact Riemannian manifold then the Minakshisundaram–Pleijel zeta function converges and has an analytic continuation as a meromorphic function to all complex numbers, and extended this to elliptic pseudo-differential operators \"A\" on compact Riemannian manifolds. So for such operators one can define the determinant using zeta function regularization. See \"analytic torsion.\"\n\nThe first example in which zeta function regularization is available appears in the Casimir effect, which is in a flat space with the bulk contributions of the quantum field in three space dimensions. In this case we must calculate the value of Riemann zeta function at \"-3\", which diverges explicitly. However, it can be analytically continued to \"s=-3\" where hopefully there is no pole, thus giving a finite value to the expression. A detailed example of this regularization at work is given in the article on the detail example of the Casimir effect, where the resulting sum is very explicitly the Riemann zeta-function (and where the seemingly legerdemain analytic continuation removes an additive infinity, leaving a physically significant finite number).\n\nAn example of zeta-function regularization is the calculation of the vacuum expectation value of the energy of a particle field in quantum field theory. More generally, the zeta-function approach can be used to regularize the whole energy-momentum tensor in curved spacetime. \n\nThe unregulated value of the energy is given by a summation over the zero-point energy of all of the excitation modes of the vacuum:\n\nHere, formula_4 is the zeroth component of the energy-momentum tensor and the sum (which may be an integral) is understood to extend over all (positive and negative) energy modes formula_5; the absolute value reminding us that the energy is taken to be positive. This sum, as written, is usually infinite (formula_5 is typically linear in n). The sum may be regularized by writing it as\n\nwhere \"s\" is some parameter, taken to be a complex number. For large, real \"s\" greater than 4 (for three-dimensional space), the sum is manifestly finite, and thus may often be evaluated theoretically.\n\nThe zeta-regularization is useful as it can often be used in a way such that the various symmetries of the physical system are preserved. Zeta-function regularization is used in conformal field theory, renormalization and in fixing the critical spacetime dimension of string theory.\n\nWe can ask if are there any relation to the dimensional regularization originated by the Feynman diagram. But now we may say they are equivalent each other, see. However the main advantage of the zeta regularization is that it can be used whenever the dimensional regularization fails, for example if there are matrices or tensors inside the calculations formula_8\n\nZeta-function regularization gives an analytic structure to any sums over an arithmetic function \"f\"(\"n\"). Such sums are known as Dirichlet series. The regularized form\n\nconverts divergences of the sum into simple poles on the complex \"s\"-plane. In numerical calculations, the zeta-function regularization is inappropriate, as it is extremely slow to converge. For numerical purposes, a more rapidly converging sum is the exponential regularization, given by\n\nThis is sometimes called the Z-transform of \"f\", where \"z\" = exp(−\"t\"). The analytic structure of the exponential and zeta-regularizations are related. By expanding the exponential sum as a Laurent series\n\none finds that the zeta-series has the structure\n\nThe structure of the exponential and zeta-regulators are related by means of the Mellin transform. The one may be converted to the other by making use of the integral representation of the Gamma function:\n\nwhich lead to the identity\n\nrelating the exponential and zeta-regulators, and converting poles in the s-plane to divergent terms in the Laurent series.\n\nThe sum\n\nis sometimes called a heat kernel or a heat-kernel regularized sum; this name stems from the idea that the formula_5 can sometimes be understood as eigenvalues of the heat kernel. In mathematics, such a sum is known as a generalized Dirichlet series; its use for averaging is known as an Abelian mean. It is closely related to the Laplace–Stieltjes transform, in that\n\nwhere formula_18 is a step function, with steps of formula_19 at formula_20. A number of theorems for the convergence of such a series exist. For example, by the Hardy-Littlewood Tauberian theorem, if \n\nthen the series for formula_22 converges in the half-plane formula_23 and is uniformly convergent on every compact subset of the half-plane formula_23. In almost all applications to physics, one has formula_25\n\nMuch of the early work establishing the convergence and equivalence of series regularized with the heat kernel and zeta function regularization methods was done by G. H. Hardy and J. E. Littlewood in 1916 and is based on the application of the Cahen–Mellin integral. The effort was made in order to obtain values for various ill-defined, conditionally convergent sums appearing in number theory.\n\nIn terms of application as the regulator in physical problems, before , J. Stuart Dowker and Raymond Critchley in 1976 proposed a zeta-function regularization method for quantum physical problems. Emilio Elizalde and others have also proposed a method based on the zeta regularization for the integrals formula_26, here formula_27 is a regulator and the divergent integral depends on the numbers formula_28 in the limit formula_29 see renormalization. Also unlike other regularizations such as dimensional regularization and analytic regularization, zeta regularization has no counterterms and gives only finite results.\n\n\n"}
