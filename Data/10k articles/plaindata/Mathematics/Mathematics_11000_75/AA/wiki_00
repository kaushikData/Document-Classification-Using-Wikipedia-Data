{"id": "4347699", "url": "https://en.wikipedia.org/wiki?curid=4347699", "title": "171 (number)", "text": "171 (number)\n\n171 (one hundred [and] seventy-one) is the natural number following 170 and preceding 172.\n\n171 is an odd number, a composite number, and a deficient number.\nIt is also a triangular number, a tridecagonal number and a 58-gonal number.\n\n171 is a Harshad number, a palindromic number, and an undulating number. 171 is a repdigit in base 7 (333), and also in bases 18, 56, and 170.\n\n\n\n\n\n\n171 is also:\n\n\n"}
{"id": "13251077", "url": "https://en.wikipedia.org/wiki?curid=13251077", "title": "Arborescence (graph theory)", "text": "Arborescence (graph theory)\n\nIn graph theory, an arborescence is a directed graph in which, for a vertex \"u\" called the root and any other vertex \"v\", there is exactly one directed path from \"u\" to \"v\". An arborescence is thus the directed-graph form of a rooted tree, understood here as an undirected graph.\n\nEquivalently, an arborescence is a directed, rooted tree in which all edges point away from the root; a number of other equivalent characterizations exist. Every arborescence is a directed acyclic graph (DAG), but not every DAG is an arborescence.\n\nAn arborescence can equivalently be defined as a rooted digraph in which the path from the root to any other vertex is unique.\n\nThe term \"arborescence\" comes from French. Some authors object to it on grounds that it is cumbersome to spell. There is a large number of synonyms for arborescence in graph theory, including directed rooted tree out-arborescence, out-tree, and even branching being used to denote the same concept. \"Rooted tree\" itself has been defined by some authors as a directed graph.\n\nFurthermore, some authors define an arborescence to be a spanning directed tree of a given digraph. The same can be said about some its synonyms, especially \"branching\". Other authors use \"branching\" to denote a forest of arborescences, with the latter notion defined in broader sense given at beginning of this article, but a variation with both notions of the spanning flavor is also encountered.\n\nIt's also possible to define a useful notion by reversing all the arcs of an arborescence, i.e. making them all point to the root rather than away from it. Such digraphs are also designated by a variety of terms such as in-tree or anti-arborescence etc. W. T. Tutte distinguishes between the two cases by using the phrases \"arborescence diverging from\" [some root] and \"arborescence converging to\" [some root].\n\nThe number of rooted trees (or arborescences) with \"n\" nodes is given by the sequence:\n\n\n"}
{"id": "23579022", "url": "https://en.wikipedia.org/wiki?curid=23579022", "title": "Artur Avila", "text": "Artur Avila\n\nArtur Avila Cordeiro de Melo (born 29 June 1979) is a French-Brazilian mathematician working primarily on dynamical systems and spectral theory. He is one of the winners of the 2014 Fields Medal, being the first Latin American to win such award. He is a researcher at both the IMPA and the CNRS (working a half-year in each one).\n\nAt the age of 16, Avila won a gold medal at the 1995 International Mathematical Olympiad and received a scholarship for the Instituto Nacional de Matemática Pura e Aplicada (IMPA) to M.S. while still attending high school in Colégio de São Bento and Colégio Santo Agostinho in Rio de Janeiro. Later he enrolled in the Federal University of Rio de Janeiro (UFRJ), earning his B.S in mathematics.\n\nAt the age of 19, Avila began making his doctoral thesis on the theory of dynamical systems. In 2001 he finished it and received his PhD from IMPA. That same year he moved abroad to France to do postdoctoral research. He works with one-dimensional dynamics and holomorphic functions. Since 2003 he has worked as a researcher for the Centre National de la Recherche Scientifique (CNRS) in France, later becoming a research director in 2008. His post-doctoral supervisor was Jean-Christophe Yoccoz.\n\nIn 2005, at age 26, Avila became known amongst mathematicians for proving the \"Conjecture of the ten martinis\", a problem proposed in 1980 by the American mathematical physicist Barry Simon. Simon promised to pay ten martini doses to whoever explained his theory about the behavior of \"Schrödinger operators\", mathematical tools related to quantum physics. Artur solved the problem along with mathematician Svetlana Jitomirskaya and was rewarded with a few rounds of martini.\n\nLater, as a research mathematician, he received in 2006 a CNRS Bronze Medal as well as the Salem Prize, and was a Clay Research Fellow. He became the youngest professorial fellow (\"directeur de recherches\") at the CNRS in 2008. The same year, he was awarded one of the ten prestigious European Mathematical Society prizes, and in 2009 he won the from the French Academy of Sciences. In 2017 he gave the Łojasiewicz Lecture (on the \"One-frequency Schrödinger operators and the almost reducibility conjecture\") at the Jagiellonian University in Kraków.\n\nHe was a plenary speaker at the International Congress of Mathematicians in 2010.\nIn 2011, he was awarded the Michael Brin Prize in Dynamical Systems. He received the Early Career Award from the International Association of Mathematical Physics in 2012, TWAS Prize in 2013 and the Fields Medal in 2014.\n\nIn 2005, together with Svetlana Jitomirskaya, he solved the ten martini problem, and together with Marcelo Viana, he proved the Zorich–Kontsevich conjecture.\n\n\n"}
{"id": "55228538", "url": "https://en.wikipedia.org/wiki?curid=55228538", "title": "Co-Hopfian group", "text": "Co-Hopfian group\n\nIn the mathematical subject of group theory, a co-Hopfian group is a group that is not isomorphic to any of its proper subgroups. The notion is dual to that of a Hopfian group, named after Heinz Hopf. \n\nA group \"G\" is called co-Hopfian if whenever formula_1 is an injective group homomorphism then formula_2 is bijective, that is formula_3.\n\n\n\n\n\n "}
{"id": "40901", "url": "https://en.wikipedia.org/wiki?curid=40901", "title": "Comma-free code", "text": "Comma-free code\n\nA comma-free code is block code in which no concatenation of two code words contains a valid code word that overlaps both.\n\nComma-free codes are also known as \"self-synchronizing block codes\" because no synchronization is required to find the beginning of a code word.\n"}
{"id": "6680683", "url": "https://en.wikipedia.org/wiki?curid=6680683", "title": "Cubic form", "text": "Cubic form\n\nIn mathematics, a cubic form is a homogeneous polynomial of degree 3, and a cubic hypersurface is the zero set of a cubic form. In the case of a cubic form in three variables, the zero set is a cubic plane curve.\n\nIn , Boris Delone and Dmitry Faddeev showed that binary cubic forms with integer coefficients can be used to parametrize orders in cubic fields. Their work was generalized in to include all cubic rings, giving a discriminant-preserving bijection between orbits of a GL(2, Z)-action on the space of integral binary cubic forms and cubic rings up to isomorphism.\n\nThe classification of real cubic forms formula_1 is linked to the classification of umbilical points of surfaces. The equivalence classes of such cubics form a three-dimensional real projective space and the subset of parabolic forms define a surface – the umbilic torus or umbilic bracelet.\n\n\n"}
{"id": "33872518", "url": "https://en.wikipedia.org/wiki?curid=33872518", "title": "De Donder–Weyl theory", "text": "De Donder–Weyl theory\n\nIn mathematical physics, the De Donder–Weyl theory is a generalization of the Hamiltonian formalism in the calculus of variations and classical field theory over spacetime which treats the space and time coordinates on equal footing. In this framework, the Hamiltonian formalism in mechanics is generalized to field theory in the way that a field is represented as a system that varies both in space and in time. This generalization is different from the canonical Hamiltonian formalism in field theory which treats space and time variables differently and describes classical fields as infinite-dimensional systems evolving in time. \n\nThe De Donder–Weyl theory is based on a change of variables known as Legendre transformation. Let \"x\" be spacetime coordinates, for \"i\" = 1 to \"n\" (with \"n\" = 4 representing 3 + 1 dimensions of space and time), and \"y\" field variables, for \"a\" = 1 to \"m\", and \"L\" the Lagrangian density \nWith the polymomenta \"p\" defined as\nand the De Donder–Weyl Hamiltonian function \"H\" defined as\nthe De Donder–Weyl equations are:\n\nThis De Donder-Weyl Hamiltonian form of field equations is covariant and it is equivalent to the Euler-Lagrange equations when the Legendre transformation to the variables \"p\" and \"H\" is not singular. The theory is a formulation of a covariant Hamiltonian field theory which is different from the canonical Hamiltonian formalism and for \"n\" = 1 it reduces to Hamiltonian mechanics (see also action principle in the calculus of variations). \n\nHermann Weyl in 1935 has developed the Hamilton-Jacobi theory for the De Donder–Weyl theory.\n\nSimilarly to the Hamiltonian formalism in mechanics formulated using the symplectic geometry of phase space \nthe De Donder-Weyl theory can be formulated using the multisymplectic geometry or polysymplectic geometry and the geometry \nof jet bundles. \n\nA generalization of the Poisson brackets to the De Donder–Weyl theory \nand the representation of De Donder–Weyl equations in terms of generalized Poisson brackets satisfying the Gerstenhaber algebra \nwas found by Kanatchikov in 1993.\n\nThe formalism, now known as De Donder–Weyl (DW) theory, was developed by Théophile De Donder and Hermann Weyl. Hermann Weyl made his proposal in 1934 being inspired by the work of Constantin Carathéodory, which in turn was founded on the work of Vito Volterra. The work of De Donder on the other hand started from the theory of integral invariants of Élie Cartan. The De Donder–Weyl theory has been a part of the calculus of variations since the 1930s and initially it found very few applications in physics. Recently it was applied in theoretical physics in the context of quantum field theory and quantum gravity.\n\nIn 1970, Jedrzej Śniatycki, the author of \"Geometric quantization and quantum mechanics\", developed an invariant geometrical formulation of jet bundles, building on the work of De Donder and Weyl. In 1999 Igor Kanatchikov has shown that the De Donder–Weyl covariant Hamiltonian field equations can be formulated in terms of Duffin–Kemmer–Petiau matrices.\n\n\n"}
{"id": "8276156", "url": "https://en.wikipedia.org/wiki?curid=8276156", "title": "Dehn–Sommerville equations", "text": "Dehn–Sommerville equations\n\nIn mathematics, the Dehn–Sommerville equations are a complete set of linear relations between the numbers of faces of different dimension of a simplicial polytope. For polytopes of dimension 4 and 5, they were found by Max Dehn in 1905. Their general form was established by Duncan Sommerville in 1927. The Dehn–Sommerville equations can be restated as a symmetry condition for the \"h\"-vector\" of the simplicial polytope and this has become the standard formulation in recent combinatorics literature. By duality, analogous equations hold for simple polytopes.\n\nLet \"P\" be a \"d\"-dimensional simplicial polytope. For \"i\" = 0, 1, ..., \"d\"−1, let \"f\" denote the number of \"i\"-dimensional faces of \"P\". The sequence\n\nis called the \"f\"-vector of the polytope \"P\". Additionally, set\n\nThen for any \"k\" = −1, 0, …, \"d\"−2, the following Dehn–Sommerville equation holds:\n\nWhen \"k\" = −1, it expresses the fact that Euler characteristic of a (\"d\" − 1)-dimensional simplicial sphere is equal to 1 + (−1).\n\nDehn–Sommerville equations with different \"k\" are not independent. There are several ways to choose a maximal independent subset consisting of formula_4 equations. If \"d\" is even then the equations with \"k\" = 0, 2, 4, …, \"d\"−2 are independent. Another independent set consists of the equations with \"k\" = −1, 1, 3, …, \"d\"−3. If \"d\" is odd then the equations with \"k\" = −1, 1, 3, …, \"d\"−2 form one independent set and the equations with \"k\" = −1, 0, 2, 4, …, \"d\"−3 form another.\n\nSommerville found a different way to state these equations:\n\nformula_5\n\nwhere 0 ≤ k ≤ ½(d−1). This can be further facilitated introducing the notion of \"h\"-vector of \"P\". For \"k\" = 0, 1, …, \"d\", let\n\nThe sequence\n\nis called the \"h\"-vector of \"P\". The \"f\"-vector and the \"h\"-vector uniquely determine each other through the relation\n\nThen the Dehn–Sommerville equations can be restated simply as\n\nThe equations with 0 ≤ k ≤ ½(d−1) are independent, and the others are manifestly equivalent to them.\n\nRichard Stanley gave an interpretation of the components of the \"h\"-vector of a simplicial convex polytope \"P\" in terms of the projective toric variety \"X\" associated with (the dual of) \"P\". Namely, they are the dimensions of the even intersection cohomology groups of \"X\":\n\n(the odd intersection cohomology groups of \"X\" are all zero). In this language, the last form of the Dehn–Sommerville equations, the symmetry of the \"h\"-vector, is a manifestation of the Poincaré duality in the intersection cohomology of \"X\".\n\n"}
{"id": "12155912", "url": "https://en.wikipedia.org/wiki?curid=12155912", "title": "Discriminative model", "text": "Discriminative model\n\nDiscriminative models, also refer to conditional models, are a class of models used in statistics statistical classification, especially in supervised machine learning. The approaches used in supervised learning can be categorized into discriminative models or generative models. The typical discriminative learning approaches include Logistic Regression(LR), Support Vector Machine(SVM), conditional random fields(CRFs)(specified over an undirected graph), and etc. The typical generative model approaches contain Naive Bayes, Gaussian Mixture Model, and etc. \n\nUnlike the generative modelling, which studies from the joint probability formula_1, the discriminative modeling studies the formula_2or the direct maps the given unobserved variable (target) formula_3 a class label formula_4 depended on the observed variables (training samples). In practical field of object recognition, formula_3is likely to be a vector (i.e. raw pixels, features extracted from image or so). Within a probabilistic framework, this is done by modeling the conditional probability distribution formula_2, which can be used for predicting formula_4 from formula_3. Notice that there is still distinction between the conditional model and discriminative model, though most of the time, they are just categorised as discriminative model.\n\nConditional Model models the conditional probability distribution while the traditional discriminative model tries to optimize on mapping the input around the most similar trained samples.\n\nThe following approach is based on the assumption that given the training data-set formula_9, where formula_10is the corresponding output for the input formula_11.\n\nUsing the Linear Classifier method, we wish to use the function formula_12to simulate the behavior of what we observed from the training data-set. Using the joint feature vector formula_13, the decision function is defined as:formula_14According to Memisevic's interpretation, formula_15, which is also formula_16computes a score which measures the computability of the input formula_3with the potential output formula_4. Then the formula_19determines the class with the highest score.\n\nSince the 0-1 loss function is a commonly used one in the decision theory, the conditional probability distribution formula_20, where formula_21is a parameter vector for optimizing the training data, could be reconsidered as following for the logistics regression model:formula_22, withformula_23The equation above is the Logistic Regression. Notice that a major distinction between models is their way of introducing posterior probability. Posterior probability is inferred from the parametric model. We then can maximize the parameter by following equation:formula_24It could also be replaced by the log-loss equation below:formula_25Since the log-loss is differentiable, gradient-based method is a possible way to optimize the model. Global optimum is guaranteed because the objective function is convex. Gradient of log likelihood is presented by: formula_26where formula_27is the expectation of formula_28.\n\nThe above method will provide efficient computation for the relative small number of classification.\n\nLet's say we are given the formula_29 class labels(classification) and formula_30 feature variables, formula_31, as the training samples. \n\nGenerative model takes the joint probability formula_1, where formula_3 is the input and formula_4 the label, and predicts the most possible known label formula_35 for the unknown variable formula_36 using Bayes Rules.\n\nDiscriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of observed and target variables. However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance (in part because they have fewer variables to compute). On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily support unsupervised learning. Application-specific details ultimately dictate the suitability of selecting a discriminative versus generative model.\n\nDiscriminative models and generative models also differ in introducing the posterior possibility. To maintain the least expected loss, the minimization of result's misclassification should be acquired. In the discriminative model, the posterior probabilities, formula_37, is inferred from a parametric model, where the parameters come from the training data. Points of estimation of the parameters are obtained from the maximization of likelihood or distribution computation over the parameters. On the other hand, considering the generative models focuses on the joint probability, the class posterior possibility formula_38 is considered in Bayes' Theorem, which is formula_39.\n\nIn the repeated experiments, Logistics Regression and Naive Bayes are applied here for different models on binary classification task, discriminative learning results lower asymptotic errors, while generative one results in higher asymptotic errors faster. However, Ulusoy and Bishop's joint work, \"Comparison of Generative and Discriminative Techniques for Object Detection and Classification\", they state that the above statement is true only when the model is the appropriate one for data(i.e.the data distribution is correctly modeled by the generative model).\n\nSignificant advantages of using discriminative modeling are:\n\n\nCompared with the advantages of using generative modeling:\n\n\n\nSince both advantages and disadvantages present on the two way of modeling, combining both approaches will be a good modeling in practice. For example, in Marras' article \"A Joint Discriminative Generative Model for Deformable Model Construction and Classification\", they applied the combination of two modeling on face classification of the models, and receive a higher accuracy than the traditional approach. \n\nSimilarly, Kelm also proposed the combination of two modeling for pixel classification in his article \"Combining Generative and Discriminative Methods for Pixel Classification with Multi-Conditional Learning\". \n\nDuring the process of extracting the discriminative features prior to the clustering, Principal Component Analysis(PCA), though commonly used, is not a necessarily discriminative approach. In contrast, LDA is a discriminative one. Linear discriminant analysis(LDA), provides an efficient way of eliminating the disadvantage we list above. As we know, the discriminative model needs a combination of multiple subtasks before classification, and LDA  provides appropriate solution towards this problem by reducing dimension. \n\nIn \"Beyerlein\"'s paper, \"DISCRIMINATIVE MODEL COMBINATION\", the discriminative model combination provides a new approach in auto speech recognition. It not only helps to optimize the integration of all kinds of models into one log-linear posterior probability distribution. The combination also aims at minimizing the empirical word error rate of training samples. \n\nIn the article, A Unified and Discriminative Model for Query Refinement, Guo and his partners used a unified discriminative model in query refinement using linear classifier successfully obtained a much higher accuracy rate. The experiment they designed also consider generative model as a comparison with the unified model. Just as expected in the real-world application, the generative model performed the poorest in among of the models, including the models without their improvement. \n\nExamples of discriminative models used in machine learning include:\n\n"}
{"id": "20852937", "url": "https://en.wikipedia.org/wiki?curid=20852937", "title": "Erdős–Nagy theorem", "text": "Erdős–Nagy theorem\n\nThe Erdős–Nagy theorem is a result in discrete geometry stating that a non-convex simple polygon can be made into a convex polygon by a finite sequence of flips. The \"flips\" are defined by taking a convex hull of a polygon and reflecting a pocket with respect to the boundary edge. The theorem is named after mathematicians Paul Erdős and Béla Szőkefalvi-Nagy.\n\nPaul Erdős conjectured the result in 1935 as a problem in the \"American Mathematical Monthly\", and Szőkefalvi-Nagy published a proof in 1939. The problem has a curious history and had been repeatedly rediscovered, until Branko Grünbaum surveyed the results in 1995. As it turns out, the original proof had a delicate mistake, which has been since corrected.\n\n\n"}
{"id": "22844355", "url": "https://en.wikipedia.org/wiki?curid=22844355", "title": "Erdős–Rado theorem", "text": "Erdős–Rado theorem\n\nIn partition calculus, part of combinatorial set theory, which is a branch of mathematics, the Erdős–Rado theorem is a basic result, extending Ramsey's theorem to uncountable sets.\n\nIf \"r\" ≥ 0 is finite, \"κ\" is an infinite cardinal, then\nwhere exp(κ) = \"κ\" and inductively exp(κ)=2. This is sharp in the sense that exp(κ) cannot be replaced by exp(κ) on the left hand side.\n\nThe above partition symbol describes the following statement. If \"f\" is a coloring of the \"r+1\"-element subsets of a set of cardinality exp(κ), in \"κ\" many colors, then there is a homogeneous set of cardinality \"κ\" (a set, all whose \"r+1\"-element subsets get the same \"f\"-value).\n\n"}
{"id": "16204398", "url": "https://en.wikipedia.org/wiki?curid=16204398", "title": "Esscher principle", "text": "Esscher principle\n\nThe Esscher principle is an insurance premium principle. It is given by formula_1, where formula_2 is a strictly positive parameter. This premium is the net premium for a risk formula_3, where formula_4 denotes the moment generating function.\n\nThe Esscher principle is a risk measure used in actuarial sciences that derives from the Esscher transform. This risk measure does not respect the positive homogeneity property of coherent risk measure for formula_5.\n"}
{"id": "57857677", "url": "https://en.wikipedia.org/wiki?curid=57857677", "title": "Euler measure", "text": "Euler measure\n\nIn measure theory, the Euler measure of a polyhedral set equals the Euler integral of its indicator function.\n\nBy induction, it is easy to show that independent of dimension, the Euler measure of a closed bounded convex polyhedron always equals 1, while the Euler measure of a \"d\"-D relative-open bounded convex polyhedron is formula_1.\n\n\n"}
{"id": "20185599", "url": "https://en.wikipedia.org/wiki?curid=20185599", "title": "Frank Morgan (mathematician)", "text": "Frank Morgan (mathematician)\n\nFrank Morgan is an American mathematician and the Webster Atwell '21 Professor of Mathematics at Williams College, specialising in geometric measure theory and minimal surfaces. He is most famous for proving the Double Bubble conjecture, that the minimum-surface-area enclosure of two given volumes is formed by three spherical patches meeting at 120-degree angles at a common circle. Morgan was a vice-president-elect of the American Mathematical Society.\n\nMorgan studied at the Massachusetts Institute of Technology and Princeton University, and received his Ph.D. from Princeton in 1977, under the supervision of Frederick J. Almgren Jr.. He taught at MIT for ten years before joining the Williams faculty.\n\nFrank Morgan is the founder of SMALL, one of the largest and best known summer undergraduate Mathematics research programs. The National Science Foundation has recently announced the award of a three-year $145,445 grant to him. Morgan and his students will research manifolds with density, a generalization of Riemannian manifolds, long prominent in probability and of rapidly growing interest in geometry. Manifolds, or topological spaces that are locally Euclidean, can be understood intuitively as surfaces. This work will build on research conducted by Morgan and his students over the summer.\n\nSpecifically, Morgan intends to approach this area by studying the isoperimetric problem for manifolds with density such as Gauss space, the premier example of a manifold with density. Isoperimetric problems, which involve finding a closed curve of fixed length, which encloses the greatest area in the plane, have applications in probability theory, in Riemannian geometry, and in Grigori Perelman’s proof of the Poincaré conjecture.\n\nFrank Morgan is also an avid dancer. He gained temporary fame for his work \"Dancing the Parkway\".\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "1643806", "url": "https://en.wikipedia.org/wiki?curid=1643806", "title": "Gabriel Andrew Dirac", "text": "Gabriel Andrew Dirac\n\nGabriel Andrew Dirac (13 March 1925 – 20 July 1984) was a mathematician who mainly worked in graph theory. He stated a sufficient condition for a graph to contain a Hamiltonian circuit. In 1951 he conjectured that n points in the plane, not all collinear, must span at least [n/2] two-point lines, where [x] is the largest integer not exceeding x. This conjecture was proven true when n is sufficiently large by Green and Tao in 2012.\n\nDirac received his Ph.D. in 1952 from the University of London under Richard Rado.\n\nDirac was professor of mathematics in the University of Aarhus in Denmark, and was also Erasmus Smith's Professor of Mathematics (1962) at Trinity College Dublin in the mid-1960s.\n\nHe was the stepson of Paul Dirac, who adopted him after marrying his mother Manci, and the nephew of Eugene Wigner. His biological father is Richard Balazs, and he had an older sister, and two younger half-sisters.\n\n\n"}
{"id": "2379792", "url": "https://en.wikipedia.org/wiki?curid=2379792", "title": "Giant component", "text": "Giant component\n\nIn network theory, a giant component is a connected component of a given random graph that contains a finite fraction of the entire graph's vertices.\n\nGiant components are a prominent feature of the Erdős–Rényi model (ER) of random graphs, in which each possible edge connecting pairs of a given set of vertices is present, independently of the other edges, with probability . In this model, if formula_1 for any constant formula_2, then with high probability all connected components of the graph have size , and there is no giant component. However, for formula_3 there is with high probability a single giant component, with all other components having size . For formula_4, intermediate between these two possibilities, the number of vertices in the largest component of the graph is with high probability proportional to formula_5.\n\nGiant component is also important in percolation theory. When a fraction of nodes, formula_6, is removed randomly from an ER network of degree formula_7, there exists a critical threshold, formula_8. Above formula_9 there exists a giant component (largest cluster) of size, formula_10. formula_10 fulfills, formula_12. For formula_13 the solution of this equation is formula_14, i.e., there is no giant component.\n\nAt formula_9, the distribution of cluster sizes behaves as a power law, formula_16 which is a feature of phase transition. Giant component appears also in percolation of lattice networks.\n\nAlternatively, if one adds randomly selected edges one at a time, starting with an empty graph, then it is not until approximately formula_17 edges have been added that the graph contains a large component, and soon after that the component becomes giant. More precisely, when formula_18 edges have been added, for values of formula_18 close to but larger than formula_17, the size of the giant component is approximately formula_21. However, according to the coupon collector's problem, formula_22 edges are needed in order to have high probability that the whole random graph is connected.\n\nA similar sharp threshold between parameters that lead to graphs with all components small and parameters that lead to a giant component also occurs in random graphs with non-uniform degree distributions.\nThe degree distribution does not define a graph uniquely. \nHowever under assumption that in all respects other than their degree distribution,\nthe graphs are treated as entirely random, many results on finite/infinite-component sizes are known. \nIn this model, the existence of the giant component depends only on the first two (mixed) moments of the degree distribution. Let a randomly chosen vertex has degree formula_23, then the giant component exists if and only ifformula_24Similar expressions are also valid for directed graphs, in which case the degree distribution is two-dimensional. There are three types of connected components in directed graphs. For a randomly chosen vertex:\n\na. out-component is a set of vertices that can be reached by recursively following all out-edges forward;\n\nb. in-component is a set of vertices that can be reached by recursively following all in-edges backward;\n\nc. weak component is a set of vertices that can be reached by recursively following all edges regardless of their direction.\n\nLet a randomly chosen vertex has formula_25 in-edges and formula_25 out edges. By definition, the average number of in- and out-edges coincides so that formula_27. The criteria for giant component existence in directed and undirected random graphs are given in the table below. \n\nFor other properties of the giant component and its relation to percolation theory and critical phenomena, see references.\n\n"}
{"id": "35323360", "url": "https://en.wikipedia.org/wiki?curid=35323360", "title": "Grossberg network", "text": "Grossberg network\n\nGrossberg network is a artificial neural network introduced by Stephen Grossberg. It is a self organizing, competitive network based on continuous time. Grossberg, a neuroscientist and a biomedical engineer, designed this network based on the human visual system.\n\nThe shunting model is one of Grossberg's neural network models, based on a Leaky integrator, described by the differential equation\nwhere formula_2 represents the activation level of a neuron, formula_3 and formula_4 represent the excitatory and inhibitory inputs to the neuron, and formula_5, formula_6, and formula_7 are constants representing the leaky decay rate and the maximum and minimum activation levels.\n\nAt equilibrium (where formula_8), the activation formula_9 reaches the value\n"}
{"id": "4326149", "url": "https://en.wikipedia.org/wiki?curid=4326149", "title": "Hann function", "text": "Hann function\n\nThe Hann function, named after the Austrian meteorologist Julius von Hann, is a discrete window function given by\n\nor\n\nor, in terms of the haversine function, \n\nThe Hann window is a linear combination of modulated rectangular windows formula_4. From Euler's formula \nDue to the basic properties of the Fourier transform, its spectrum is\nwith the spectrum of the rectangular window\nIf windows are time-shifted around 0 the modulation factor vanishes and the signs in front of the 1/4 terms change to +.\n\nHann function is the original name, in honour of von Hann; however, the erroneous \"Hanning\" function is also heard of on occasion, derived from the paper in which it was named, where the term \"hanning a signal\" was used to mean applying the Hann window to it. The confusion arose from the similar Hamming function, named after Richard Hamming.\n\nThe Hann function is typically used as a window function in digital signal processing to select a subset of a series of samples in order to perform a Fourier transform or other calculations. \n\ni.e. (using continuous version to illustrate)\n\nThe advantage of the Hann window is very low aliasing, and the tradeoff slightly is a decreased resolution (widening of the main lobe).\n\n\n\n"}
{"id": "24307466", "url": "https://en.wikipedia.org/wiki?curid=24307466", "title": "Harmonic progression (mathematics)", "text": "Harmonic progression (mathematics)\n\nIn mathematics, a harmonic progression (or harmonic sequence) is a progression formed by taking the reciprocals of an arithmetic progression. It is a sequence of the form\n\nwhere −a/\"d\" is not a natural number and \"k\" is a natural number.\n\nEquivalently, a sequence is a harmonic progression when each term is the harmonic mean of the neighboring terms.\n\nIt is not possible for a harmonic progression (other than the trivial case where \"a\" = 1 and \"k\" = 0) to sum to an integer. The reason is that, necessarily, at least one denominator of the progression will be divisible by a prime number that does not divide any other denominator.\n\n\nIf collinear points A, B, C, and D are such that D is the harmonic conjugate of C with respect to A and B, then the distances from any one of these points to the three remaining points form harmonic progression. Specifically, each of the sequences\nAC, AB, AD; BC, BA, BD; CA, CD, CB; and DA, DC, DB are harmonic progressions, where each of the distances is signed according to a fixed orientation of the line.\n\nIn a triangle, if the altitudes are in arithmetic progression, then the sides are in harmonic progression\n\n\n"}
{"id": "10677746", "url": "https://en.wikipedia.org/wiki?curid=10677746", "title": "Information exchange", "text": "Information exchange\n\nInformation exchange or information sharing are informal terms that can either refer to bidirectional \"information transfer\" in telecommunications and computer science or \"communication\" seen from a system-theoretic or information-theoretic point of view. As \"information\" in this context invariably refers to (electronic) data that encodes and represents the information at hand, a broader treatment can be found under data exchange.\n\nThe term information sharing has a long history in information technology. Traditional information sharing referred to one-to-one exchanges of data between a sender and receiver. These information exchanges are implemented via dozens of open and proprietary protocols, message and file formats. Electronic data interchange (EDI) is a successful implementation of commercial data exchanges that began in the late 1970s and remains in use today.\n\nInitiatives to standardize information sharing protocols include extensible markup language (XML), simple object access protocol (SOAP), and web services description language (WSDL).\n\nFrom the point of view of a computer scientist, the four primary information sharing design patterns are sharing information one-to-one, one-to-many, many-to-many, and many-to-one. Technologies to meet all four of these design patterns are evolving and include blogs, wikis, really simple syndication, tagging, and chat.\n\nOne example of United States government's attempt to implement one of these design patterns (one to one) is the National Information Exchange Model (NIEM). One-to-one exchange models fall short of supporting all of the required design patterns needed to fully implement data exploitation technology.\n\nAdvanced information sharing platforms provide controlled vocabularies, data harmonization, data stewardship policies and guidelines, standards for uniform data as they relate to privacy, security, and data quality.\n\nThe term information sharing gained popularity as a result of the 9/11 Commission Hearings and its report of the United States government's lack of response to information known about the planned terrorist attack on the New York City World Trade Center prior to the event. The resulting commission report led to the enactment of several executive orders by President Bush that mandated agencies implement policies to \"share information\" across organizational boundaries. In addition, an Information Sharing Environment Program Manager (PM-ISE) was appointed, tasked to implement the provisions of the Intelligence Reform and Terrorism Prevention Act of 2004. In making recommendation toward the creation of an \"Information Sharing Environment\" the 9/11 Commission based itself on the findings and recommendations made by the Markle Task Force on National Security in the Information Age.\n\nInformation exchange is also used to describe the process of learning and the efficiency of the learning.\n\n"}
{"id": "57915320", "url": "https://en.wikipedia.org/wiki?curid=57915320", "title": "Isaac ibn al-Ahdab", "text": "Isaac ibn al-Ahdab\n\nItzḥak ben Shlomo ibn al-Aḥdab (or ibn al-Ḥadib) ben Tzaddiq ha-Sefardi (, c. 1350-c. 1426) was a Jewish mathematician, astronomer, and poet.\n\nIbn al-Aḥdab was born in Castile to a prominent Jewish family. He was a student of Judah ben Asher II, the great-grandson of Asher ben Yeḥiel of Cologne, who was killed in the anti-Jewish massacres of 1391. By 1396 Ibn al-Aḥdab had fled Spain and was in Sicily, where he lived (in Syracuse and Palermo) until his death around 1426.\n\nHe studied the algebra of Maghrebi mathematician Ibn al-Bannā and published \"The Epistle of the Number\", a translation and detailed commentary on Ibn al-Bannā's 13th century treatise \"Talḵīṣ ʿAmal al-Ḥisāb\" (\"A summary of the operations of calculation\"). The work is notable in being the first known Hebrew-language treatise to include extensive algebraic theories and operations.\n\nHis main astronomical work was \"Oraḥ selulah\", a set of tables in Hebrew for conjunctions and oppositions of the Sun and the Moon.\n"}
{"id": "945225", "url": "https://en.wikipedia.org/wiki?curid=945225", "title": "Isoperimetric dimension", "text": "Isoperimetric dimension\n\nIn mathematics, the isoperimetric dimension of a manifold is a notion of dimension that tries to capture how the \"large-scale behavior\" of the manifold resembles that of a Euclidean space (unlike the topological dimension or the Hausdorff dimension which compare different \"local behaviors\" against those of the Euclidean space).\n\nIn the Euclidean space, the isoperimetric inequality says that of all bodies with the same volume, the ball has the smallest surface area. In other manifolds it is usually very difficult to find the precise body minimizing the surface area, and this is not what the isoperimetric dimension is about. The question we will ask is, what is \"approximately\" the minimal surface area, whatever the body realizing it might be.\n\nWe say about a differentiable manifold \"M\" that it satisfies a \"d\"-dimensional isoperimetric inequality if for any open set \"D\" in \"M\" with a smooth boundary one has\n\nThe notations vol and area refer to the regular notions of volume and surface area on the manifold, or more precisely, if the manifold has \"n\" topological dimensions then vol refers to \"n\"-dimensional volume and area refers to (\"n\" − 1)-dimensional volume. \"C\" here refers to some constant, which does not depend on \"D\" (it may depend on the manifold and on \"d\").\n\nThe isoperimetric dimension of \"M\" is the supremum of all values of \"d\" such that \"M\" satisfies a \"d\"-dimensional isoperimetric inequality.\n\nA \"d\"-dimensional Euclidean space has isoperimetric dimension \"d\". This is the well known isoperimetric problem — as discussed above, for the Euclidean space the constant \"C\" is known precisely since the minimum is achieved for the ball. \n\nAn infinite cylinder (i.e. a product of the circle and the line) has topological dimension 2 but isoperimetric dimension 1. Indeed, multiplying any manifold with a compact manifold does not change the isoperimetric dimension (it only changes the value of the constant \"C\"). Any compact manifold has isoperimetric dimension 0.\n\nIt is also possible for the isoperimetric dimension to be larger than the topological dimension. The simplest example is the infinite jungle gym, which has topological dimension 2 and isoperimetric dimension 3. See for pictures and Mathematica code.\n\nThe hyperbolic plane has topological dimension 2 and isoperimetric dimension infinity. In fact the hyperbolic plane has positive Cheeger constant. This means that it satisfies the inequality\n\nwhich obviously implies infinite isoperimetric dimension.\n\nThe isoperimetric dimension of graphs can be defined in a similar fashion.\nA precise definition is given in Chung's survey. \nArea and volume are measured by set sizes. For every subset \"A\" of the graph \"G\" one defines formula_3 as the set of vertices in formula_4 with a neighbor in \"A\". A \"d\"-dimensional isoperimetric inequality is now defined by\n\n(This MathOverflow question provides more details.) The graph analogs of all the examples above hold but the definition is slightly different in order to avoid that the isoperimetric dimension of any finite graph is 0: In the above formula the volume of formula_6 is replaced by formula_7 (see Chung's survey, section 7).\n\nThe isoperimetric dimension of a \"d\"-dimensional grid is \"d\". In general, the isoperimetric dimension is preserved by quasi isometries, both by quasi-isometries between manifolds, between graphs, and even by quasi isometries carrying manifolds to graphs, with the respective definitions. In rough terms, this means that a graph \"mimicking\" a given manifold (as the grid mimics the Euclidean space) would have the same isoperimetric dimension as the manifold. An infinite complete binary tree has isoperimetric dimension ∞.\n\nA simple integration over \"r\" (or sum in the case of graphs) shows that a \"d\"-dimensional isoperimetric inequality implies a \"d\"-dimensional volume growth, namely\n\nwhere \"B\"(\"x\",\"r\") denotes the ball of radius \"r\" around the point \"x\" in the Riemannian distance or in the graph distance. In general, the opposite is not true, i.e. even uniformly exponential volume growth does not imply any kind of isoperimetric inequality. A simple example can be had by taking the graph Z (i.e. all the integers with edges between \"n\" and \"n\" + 1) and connecting to the vertex \"n\" a complete binary tree of height |\"n\"|. Both properties (exponential growth and 0 isoperimetric dimension) are easy to verify.\n\nAn interesting exception is the case of groups. It turns out that a group with polynomial growth of order \"d\" has isoperimetric dimension \"d\". This holds both for the case of Lie groups and for the Cayley graph of a finitely generated group.\n\nA theorem of Varopoulos connects the isoperimetric dimension of a graph to the rate of escape of random walk on the graph. The result states\n\n\"Varopoulos' theorem: If G is a graph satisfying a d-dimensional isoperimetric inequality then\"\n\n\"where\" formula_10 \"is the probability that a random walk on G starting from x will be in y after n steps, and C is some constant.\"\n\n"}
{"id": "70085", "url": "https://en.wikipedia.org/wiki?curid=70085", "title": "J. J. Thomson", "text": "J. J. Thomson\n\nSir Joseph John Thomson (18 December 1856 – 30 August 1940) was an English physicist and Nobel Laureate in Physics, credited with the discovery and identification of the electron; and with the discovery of the first subatomic particle.\n\nIn 1897, Thomson showed that cathode rays were composed of previously unknown negatively charged particles (now called electrons), which he calculated must have bodies much smaller than atoms and a very large charge-to-mass ratio. Thomson is also credited with finding the first evidence for isotopes of a stable (non-radioactive) element in 1913, as part of his exploration into the composition of canal rays (positive ions). His experiments to determine the nature of positively charged particles, with Francis William Aston, were the first use of mass spectrometry and led to the development of the mass spectrograph.\n\nThomson was awarded the 1906 Nobel Prize in Physics for his work on the conduction of electricity in gases.\n\nJoseph John Thomson was born 18 December 1856 in Cheetham Hill, Manchester, Lancashire, England. His mother, Emma Swindells, came from a local textile family. His father, Joseph James Thomson, ran an antiquarian bookshop founded by a great-grandfather. He had a brother, Frederick Vernon Thomson, who was two years younger than he was. J. J. Thomson was a reserved yet devout Anglican.\n\nHis early education was in small private schools where he demonstrated outstanding talent and interest in science. In 1870, he was admitted to Owens College in Manchester (now University of Manchester) at the unusually young age of 14. His parents planned to enroll him as an apprentice engineer to Sharp-Stewart & Co, a locomotive manufacturer, but these plans were cut short when his father died in 1873.\n\nHe moved on to Trinity College, Cambridge, in 1876. In 1880, he obtained his Bachelor of Arts degree in mathematics (Second Wrangler in the Tripos and 2nd Smith's Prize). He applied for and became a Fellow of Trinity College in 1881. Thomson received his Master of Arts degree (with Adams Prize) in 1883.\n\nIn 1890, Thomson married Rose Elisabeth Paget, one of his former students, daughter of Sir George Edward Paget, KCB, a physician and then Regius Professor of Physic at Cambridge at the church of St. Mary the Less. They had one son, George Paget Thomson, and one daughter, Joan Paget Thomson.\n\nOn 22 December 1884, Thomson was appointed Cavendish Professor of Physics at the University of Cambridge. The appointment caused considerable surprise, given that candidates such as Osborne Reynolds or Richard Glazebrook were older and more experienced in laboratory work. Thomson was known for his work as a mathematician, where he was recognized as an exceptional talent.\n\nHe was awarded a Nobel Prize in 1906, \"in recognition of the great merits of his theoretical and experimental investigations on the conduction of electricity by gases.\" He was knighted in 1908 and appointed to the Order of Merit in 1912. In 1914, he gave the Romanes Lecture in Oxford on \"The atomic theory\". In 1918, he became Master of Trinity College, Cambridge, where he remained until his death. Joseph John Thomson died on 30 August 1940; his ashes rest in Westminster Abbey, near the graves of Sir Isaac Newton and his former student, Ernest Rutherford.\n\nOne of Thomson's greatest contributions to modern science was in his role as a highly gifted teacher. One of his students was Ernest Rutherford, who later succeeded him as Cavendish Professor of Physics. In addition to Thomson himself, six of his research assistants (Charles Glover Barkla, Niels Bohr, Max Born, William Henry Bragg, Owen Willans Richardson and Charles Thomson Rees Wilson) won Nobel Prizes in physics, and two (Francis William Aston and Ernest Rutherford) won Nobel prizes in chemistry. In addition, Thomson's son (George Paget Thomson) won the 1937 Nobel Prize in physics for proving the wave-like properties of electrons.\n\nThomson's prize-winning master's work, \"Treatise on the motion of vortex rings\", shows his early interest in atomic structure. In it, Thomson mathematically described the motions of William Thomson's vortex theory of atoms.\nThomson published a number of papers addressing both mathematical and experimental issues of electromagnetism. He examined the electromagnetic theory of light of James Clerk Maxwell, introduced the concept of electromagnetic mass of a charged particle, and demonstrated that a moving charged body would apparently increase in mass.\n\nMuch of his work in mathematical modelling of chemical processes can be thought of as early computational chemistry. In further work, published in book form as \"Applications of dynamics to physics and chemistry\" (1888), Thomson addressed the transformation of energy in mathematical and theoretical terms, suggesting that all energy might be kinetic. His next book, \"Notes on recent researches in electricity and magnetism\" (1893), built upon Maxwell's \"Treatise upon electricity and magnetism\", and was sometimes referred to as \"the third volume of Maxwell\". In it, Thomson emphasized physical methods and experimentation and included extensive figures and diagrams of apparatus, including a number for the passage of electricity through gases. His third book, \"Elements of the mathematical theory of electricity and magnetism\" (1895) was a readable introduction to a wide variety of subjects, and achieved considerable popularity as a textbook.\n\nA series of four lectures, given by Thomson on a visit to Princeton University in 1896, were subsequently published as \"Discharge of electricity through gases\" (1897). Thomson also presented a series of six lectures at Yale University in 1904.\n\nSeveral scientists, such as William Prout and Norman Lockyer, had suggested that atoms were built up from a more fundamental unit, but they envisioned this unit to be the size of the smallest atom, hydrogen. Thomson in 1897 was the first to suggest that one of the fundamental units was more than 1,000 times smaller than an atom, suggesting the subatomic particle now known as the electron. Thomson discovered this through his explorations on the properties of cathode rays. Thomson made his suggestion on 30 April 1897 following his discovery that cathode rays (at the time known as Lenard rays) could travel much further through air than expected for an atom-sized particle. He estimated the mass of cathode rays by measuring the heat generated when the rays hit a thermal junction and comparing this with the magnetic deflection of the rays. His experiments suggested not only that cathode rays were over 1,000 times lighter than the hydrogen atom, but also that their mass was the same in whichever type of atom they came from. He concluded that the rays were composed of very light, negatively charged particles which were a universal building block of atoms. He called the particles \"corpuscles\", but later scientists preferred the name electron which had been suggested by George Johnstone Stoney in 1891, prior to Thomson's actual discovery.\n\nIn April 1897, Thomson had only early indications that the cathode rays could be deflected electrically (previous investigators such as Heinrich Hertz had thought they could not be). A month after Thomson's announcement of the corpuscle, he found that he could reliably deflect the rays by an electric field if he evacuated the discharge tube to a very low pressure. By comparing the deflection of a beam of cathode rays by electric and magnetic fields he obtained more robust measurements of the mass-to-charge ratio that confirmed his previous estimates. This became the classic means of measuring the charge and mass of the electron.\n\nThomson believed that the corpuscles emerged from the atoms of the trace gas inside his cathode ray tubes. He thus concluded that atoms were divisible, and that the corpuscles were their building blocks. In 1904, Thomson suggested a model of the atom, hypothesizing that it was a sphere of positive matter within which electrostatic forces determined the positioning of the corpuscles. To explain the overall neutral charge of the atom, he proposed that the corpuscles were distributed in a uniform sea of positive charge. In this \"plum pudding\" model, the electrons were seen as embedded in the positive charge like plums in a plum pudding (although in Thomson's model they were not stationary, but orbiting rapidly).\n\nIn 1912, as part of his exploration into the composition of the streams of positively charged particles then known as canal rays, Thomson and his research assistant F. W. Aston channelled a stream of neon ions through a magnetic and an electric field and measured its deflection by placing a photographic plate in its path. They observed two patches of light on the photographic plate (see image on right), which suggested two different parabolas of deflection, and concluded that neon is composed of atoms of two different atomic masses (neon-20 and neon-22), that is to say of two isotopes. This was the first evidence for isotopes of a stable element; Frederick Soddy had previously proposed the existence of isotopes to explain the decay of certain radioactive elements.\n\nJ.J. Thomson's separation of neon isotopes by their mass was the first example of mass spectrometry, which was subsequently improved and developed into a general method by F. W. Aston and by A. J. Dempster.\n\nEarlier, physicists debated whether cathode rays were immaterial like light (\"some process in the aether\") or were \"in fact wholly material, and ... mark the paths of particles of matter charged with negative electricity\", quoting Thomson. The aetherial hypothesis was vague, but the particle hypothesis was definite enough for Thomson to test.\n\nThomson first investigated the magnetic deflection of cathode rays. Cathode rays were produced in the side tube on the left of the apparatus and passed through the anode into the main bell jar, where they were deflected by a magnet. Thomson detected their path by the fluorescence on a squared screen in the jar. He found that whatever the material of the anode and the gas in the jar, the deflection of the rays was the same, suggesting that the rays were of the same form whatever their origin.\n\nWhile supporters of the aetherial theory accepted the possibility that negatively charged particles are produced in Crookes tubes, they believed that they are a mere by-product and that the cathode rays themselves are immaterial. Thomson set out to investigate whether or not he could actually separate the charge from the rays.\n\nThomson constructed a Crookes tube with an electrometer set to one side, out of the direct path of the cathode rays. Thomson could trace the path of the ray by observing the phosphorescent patch it created where it hit the surface of the tube. Thomson observed that the electrometer registered a charge only when he deflected the cathode ray to it with a magnet. He concluded that the negative charge and the rays were one and the same.\n\nIn May–June 1897, Thomson investigated whether or not the rays could be deflected by an electric field. Previous experimenters had failed to observe this, but Thomson believed their experiments were flawed because their tubes contained too much gas.\n\nThomson constructed a Crookes tube with a better vacuum. At the start of the tube was the cathode from which the rays projected. The rays were sharpened to a beam by two metal slits – the first of these slits doubled as the anode, the second was connected to the earth. The beam then passed between two parallel aluminium plates, which produced an electric field between them when they were connected to a battery. The end of the tube was a large sphere where the beam would impact on the glass, created a glowing patch. Thomson pasted a scale to the surface of this sphere to measure the deflection of the beam. Note that any electron beam would collide with some residual gas atoms within the Crookes tube, thereby ionizing them and producing electrons and ions in the tube (space charge); in previous experiments this space charge electrically screened the externally applied electric field. However, in Thomson's Crookes tube the density of residual atoms was so low that the space charge from the electrons and ions was insufficient to electrically screen the externally applied electric field, which permitted Thomson to successfully observe electrical deflection.\n\nWhen the upper plate was connected to the negative pole of the battery and the lower plate to the positive pole, the glowing patch moved downwards, and when the polarity was reversed, the patch moved upwards.\n\nIn his classic experiment, Thomson measured the mass-to-charge ratio of the cathode rays by measuring how much they were deflected by a magnetic field and comparing this with the electric deflection. He used the same apparatus as in his previous experiment, but placed the discharge tube between the poles of a large electromagnet. He found that the mass-to-charge ratio was over a thousand times \"lower\" than that of a hydrogen ion (H), suggesting either that the particles were very light and/or very highly charged. Significantly, the rays from every cathode yielded the same mass-to-charge ratio. This is in contrast to anode rays (now known to arise from positive ions emitted by the anode), where the mass-to-charge ratio varies from anode-to-anode. Thomson himself remained critical of what his work established, in his Nobel Prize acceptance speech referring to \"corpuscles\" rather than \"electrons\".\n\nThomson's calculations can be summarised as follows (notice that we reproduce here Thomson's original notations, using F instead of E for the electric field and H instead of B for the magnetic field):\n\nThe electric deflection is given by Θ = Fel/mv where Θ is the angular electric deflection, F is applied electric intensity, e is the charge of the cathode ray particles, l is the length of the electric plates, m is the mass of the cathode ray particles and v is the velocity of the cathode ray particles.\n\nThe magnetic deflection is given by φ = Hel/mv where φ is the angular magnetic deflection and H is the applied magnetic field intensity.\n\nThe magnetic field was varied until the magnetic and electric deflections were the same, when Θ = φ and Fel/mv= Hel/mv. This can be simplified to give m/e = Hl/FΘ. The electric deflection was measured separately to give Θ and H, F and l were known, so m/e could be calculated.\nAs to the source of these particles, Thomson believed they emerged from the molecules of gas in the vicinity of the cathode.\n\nThomson imagined the atom as being made up of these corpuscles orbiting in a sea of positive charge; this was his plum pudding model. This model was later proved incorrect when his student Ernest Rutherford showed that the positive charge is concentrated in the nucleus of the atom.\n\nIn 1905, Thomson discovered the natural radioactivity of potassium.\n\nIn 1906, Thomson demonstrated that hydrogen had only a single electron per atom. Previous theories allowed various numbers of electrons.\n\nThomson was elected a Fellow of the Royal Society (FRS) and appointed to the Cavendish Professorship of Experimental Physics at the Cavendish Laboratory, University of Cambridge in 1884. Thomson won numerous awards and honours during his career including:\n\nThomson was elected a Fellow of the Royal Society on 12 June 1884 and served as President of the Royal Society from 1915 to 1920.\nIn 1991, the thomson (symbol: Th) was proposed as a unit to measure mass-to-charge ratio in mass spectrometry in his honour.\n\nJ J Thomson Avenue, on the University of Cambridge campus, is named after Thomson.\n\nIn November 1927, J.J. Thomson opened the Thomson building, named in his honour, in the Leys School, Cambridge.\n\n\n"}
{"id": "36328633", "url": "https://en.wikipedia.org/wiki?curid=36328633", "title": "Jean Taylor", "text": "Jean Taylor\n\nJean Ellen Taylor (born September 17, 1944) is an American mathematician who is currently a professor emerita at Rutgers University and visiting faculty at Courant Institute of Mathematical Sciences of New York University.\n\nTaylor was born on September 17, 1944 in San Mateo, California; her father was a lawyer, her mother a schoolteacher, and she had two siblings. She did her undergraduate studies at Mount Holyoke College, graduating summa cum laude with an A.B. in 1966. She began her graduate studies in chemistry at the University of California, Berkeley, but after receiving an M.Sc. she switched to mathematics under the mentorship of S. S. Chern and then transferred to the University of Warwick and received a second M.Sc. in mathematics there. She completed a doctorate in 1972 from Princeton University under the supervision of Frederick J. Almgren, Jr.\n\nTaylor joined the Rutgers faculty in 1973, and retired in 2002. She was president of the Association for Women in Mathematics from 1999 to 2001.\n\nShe has been married three times, to mathematician John Guckenheimer (her fellow student at Berkeley), to her advisor Fred Almgren (with whom she had a daughter and two step-children), and to financier and science advocate William T. Golden.\n\nTaylor is known for her work on the mathematics of soap bubbles and of the growth of crystals. In 1976 she published the first proof of Plateau's laws, a description of the shapes formed by soap bubble clusters that had been formulated without proof in the 19th century by Joseph Plateau.\n\nTaylor is a fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science, the Association for Women in Mathematics, the American Mathematical Society and the Society for Industrial and Applied Mathematics. In 2001, she received an honorary doctorate from Mount Holyoke. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n"}
{"id": "30370276", "url": "https://en.wikipedia.org/wiki?curid=30370276", "title": "John of Tynemouth (geometer)", "text": "John of Tynemouth (geometer)\n\nJohn of Tynemouth was a 13th-century mathematician and geometer.\n\nLittle is known of John's background, but he authored \"De curvis superficiebus\" or \"Liber de curvis superficiebus Archimenidis\", a tract about Archimedes' measurements of spheres. This is an important work in the history of medieval geometry, as it helped transmit Archimedes' ideas to other medieval scholars. The work itself follows closely Archimedes' own reasoning, but with enough differences to lead modern historians to believe that John's work was dependent on a Greek text from late antiquity.\n\n\"De curvis\" survives in over 12 manuscripts, and was used by a number of other medieval scholars, including Robert Grosseteste, Jordanus de Nemore, Gerard of Brussels, and Roger Bacon.\n\nCertain stylistic choices in \"De curvis\" suggest that John was also responsible for a number of other works:\n\n\n"}
{"id": "49645022", "url": "https://en.wikipedia.org/wiki?curid=49645022", "title": "K-regular sequence", "text": "K-regular sequence\n\nIn mathematics and theoretical computer science, a \"k\"-regular sequence is an infinite sequence of terms characterized by a finite automaton with auxiliary storage. They are a generalization of \"k\"-automatic sequences to alphabets of infinite size.\n\nThere exist several characterizations of \"k\"-regular sequences, all of which are equivalent. Some common characterizations are as follows. For each, we take \"R\"′ to be a commutative Noetherian ring and we take \"R\" to be a ring containing \"R\"′.\n\nLet \"k\" ≥ 2. The \"k-kernel\" of the sequence formula_1 is the set of subsequences\nThe sequence formula_1 is (\"R\"′, \"k\")-regular (often shortened to just \"\"k\"-regular\") if the formula_4-module generated by \"K\"(\"s\") is a finitely-generated \"R\"′-module.\n\nIn the special case when formula_5, the sequence formula_1 is formula_7-regular if formula_8 is contained in a finite-dimensional vector space over formula_9.\n\nA sequence \"s\"(\"n\") is \"k\"-regular if there exists an integer \"E\" such that, for all \"e\" > \"E\" and 0 ≤ \"r\" ≤ \"k\" − 1, every subsequence of \"s\" of the form \"s\"(\"k\"\"n\" + \"r\") is expressible as an \"R\"′-linear combination formula_10, where \"c\" is an integer, \"f\" ≤ \"E\", and 0 ≤ \"b\" ≤ \"k\" − 1.\n\nAlternatively, a sequence \"s\"(\"n\") is \"k\"-regular if there exist an integer \"r\" and subsequences \"s\"(\"n\"), ..., \"s\"(\"n\") such that, for all 1 ≤ \"i\" ≤ \"r\" and 0 ≤ \"a\" ≤ \"k\" − 1, every sequence \"s\"(\"kn\" + \"a\") in the \"k\"-kernel \"K\"(\"s\") is an \"R\"′-linear combination of the subsequences \"s\"(\"n\").\n\nLet \"x\", ..., \"x\" be a set of \"k\" non-commuting variables and let τ be a map sending some natural number \"n\" to the string \"x\" ... \"x\", where the base-\"k\" representation of \"x\" is the string \"a\"...\"a\". Then a sequence \"s\"(\"n\") is \"k\"-regular if and only if the formal series formula_11 is formula_12-rational.\n\nThe formal series definition of a \"k\"-regular sequence leads to an automaton characterization similar to Schützenberger's matrix machine.\n\nThe notion of \"k\"-regular sequences was first investigated in a pair of papers by Allouche and Shallit. Prior to this, Berstel and Reutenauer studied the theory of rational series, which is closely related to \"k\"-regular sequences.\n\nLet formula_13 be the formula_14-adic valuation of formula_15. The ruler sequence formula_16 () is formula_14-regular, and the formula_14-kernel\nis contained in the two-dimensional vector space generated by formula_1 and the constant sequence formula_21. These basis elements lead to the recurrence relations\nwhich, along with the initial conditions formula_23 and formula_24, uniquely determine the sequence.\n\nThe Thue–Morse sequence \"t\"(\"n\") () is the fixed point of the morphism 0 → 01, 1 → 10. It is known that the Thue–Morse sequence is 2-automatic. Thus, it is also 2-regular, and its 2-kernel\nconsists of the subsequences formula_26 and formula_27.\n\nThe sequence of Cantor numbers \"c\"(\"n\") () consists of numbers whose ternary expansions contain no 1s. It is straightforward to show that \nand therefore the sequence of Cantor numbers is 2-regular.\n\nA somewhat interesting application of the notion of \"k\"-regularity to the broader study of algorithms is found in the analysis of the merge sort algorithm. Given a list of \"n\" values, the number of comparisons made by the merge sort algorithm are the sorting numbers, governed by the recurrence relation\nAs a result, the sequence defined by the recurrence relation for merge sort, \"T\"(\"n\"), constitutes a 2-regular sequence.\n\nIf formula_30 is an integer-valued polynomial, then formula_31 is \"k\"-regular for every formula_32.\n\nAllouche and Shallit give a number of additional examples of \"k\"-regular sequences in their papers.\n\n\"k\"-regular sequences exhibit a number of interesting properties.\n\n"}
{"id": "8722051", "url": "https://en.wikipedia.org/wiki?curid=8722051", "title": "Laplacian smoothing", "text": "Laplacian smoothing\n\nLaplacian smoothing is an algorithm to smooth a polygonal mesh. For each vertex in a mesh, a new position is chosen based on local information (such as the position of neighbors) and the vertex is moved there. In the case that a mesh is topologically a rectangular grid (that is, each internal vertex is connected to four neighbors) then this operation produces the Laplacian of the mesh.\n\nMore formally, the smoothing operation may be described per-vertex as:\n\nWhere formula_2 is the number of adjacent vertices to node formula_3, formula_4 is the position of the formula_5-th adjacent vertex and formula_6 is the new position for node formula_3.\n\n"}
{"id": "5180277", "url": "https://en.wikipedia.org/wiki?curid=5180277", "title": "Margulis lemma", "text": "Margulis lemma\n\nIn differential geometry, a subfield of mathematics, the Margulis lemma (named after Grigory Margulis) is a result about discrete subgroups of isometries of a non-positively curved Riemannian manifolds (e.g. the hyperbolic n-space). Roughly, it states that within a fixed radius, usually called the Margulis constant, the structure of the orbits of such a group cannot be too complicated. More precisely, within this radius around a point all points in its orbit are in fact in the orbit of a nilpotent subgroup (in fact a bounded finite number of such).\n\nThe Margulis lemma can be formulated as follows.\n\nLet formula_1 be a simply-connected manifold of non-positive bounded curvature. There exist constants formula_2 with the following property. For any discrete subgroup formula_3 of the group of isometries of formula_1 and any formula_5, if formula_6 is the set:\nthen the subgroup generated by formula_6 contains a nilpotent subgroup of index less than formula_9. Here formula_10 is the distance induced by the Riemannian metric.\n\nAn immediately equivalent statement can be given as follows: for any subset formula_11 of the isometry group, if it satisfies that: \nthen formula_14 contains a nilpotent subgroup of index formula_17.\n\nThe optimal constant formula_18 in the statement can be made to depend only on the dimension and the lower bound on the curvature; usually it is normalised so that the curvature is between -1 and 0. It is usually called the Margulis constant of the dimension.\n\nOne can also consider margulis constants for specific spaces. For example there has been an important effort to determine the Margulis constant of the hyperbolic spaces (of constant curvature -1). For example:\n\nA particularly studied family of examples of negatively curved manifolds are given by the symmetric spaces associated to semisimple Lie groups. In this case the Margulis lemma can be given the following, more algebraic formulation which dates back to Hans Zassenhaus. \n\nSuch a neighbourhood is called a Zassenhaus neighbourhood.\n\nLet formula_31 be a Riemannian manifold and formula_32. The \"thin part\" of formula_31 is the subset of points formula_34 where the injectivity radius of formula_31 at formula_36 is less than formula_18, usually denoted formula_38, and the \"thick part\" its complement, usually denoted formula_39. There is a tautological decomposition into a disjoint union formula_40.\n\nWhen formula_31 is of negative curvature and formula_18 is smaller than the Margulis constant for formula_43 the structure of the components of the thin part is very simple. Let us restrict to the case of hyperbolic manifolds of finite volume. Suppose that formula_18 is smaller than the Margulis constant for formula_45 and let formula_31 be a hyperbolic formula_21-manifold of finite volume. Then its thin part has two sorts of components:\n\n\nIn particular, a complete finite-volume hyperbolic manifold is always diffeomorphic to the interior of a compact manifold with (possibly empty boundary).\n\nThe Margulis lemma is an important tool in the study of manifolds of negative curvature. Besides the thick-thin decomposition some other applications are:\n\n\n"}
{"id": "53876896", "url": "https://en.wikipedia.org/wiki?curid=53876896", "title": "Mathematical Neuroscience Prize", "text": "Mathematical Neuroscience Prize\n\nThe Mathematical Neuroscience Prize is a prize awarded biennially since 2013 by the nonprofit organization Israel Brain Technologies (IBT). It is endowed with $100,000 for each laureate and honors researchers who have significantly advanced the understanding of the neural mechanisms of perception, behavior and thought through the application of mathematical analysis and modeling.\n\n\n\n"}
{"id": "47609709", "url": "https://en.wikipedia.org/wiki?curid=47609709", "title": "Mean operation", "text": "Mean operation\n\nIn algebraic topology, a mean or mean operation on a topological space \"X\" is a continuous, commutative, idempotent binary operation on \"X\". If the operation is also associative, it defines a semilattice. A classic problem is to determine which spaces admit a mean. For example, Euclidean spaces admit a mean -- the usual average of two vectors -- but spheres of positive dimension do not, including the circle.\n\n"}
{"id": "6840149", "url": "https://en.wikipedia.org/wiki?curid=6840149", "title": "Metric derivative", "text": "Metric derivative\n\nIn mathematics, the metric derivative is a notion of derivative appropriate to parametrized paths in metric spaces. It generalizes the notion of \"speed\" or \"absolute velocity\" to spaces which have a notion of distance (i.e. metric spaces) but not direction (such as vector spaces).\n\nLet formula_1 be a metric space. Let formula_2 have a limit point at formula_3. Let formula_4 be a path. Then the metric derivative of formula_5 at formula_6, denoted formula_7, is defined by\n\nif this limit exists.\n\nRecall that AC(\"I\"; \"X\") is the space of curves \"γ\" : \"I\" → \"X\" such that\n\nfor some \"m\" in the \"L\" space \"L\"(\"I\"; R). For \"γ\" ∈ AC(\"I\"; \"X\"), the metric derivative of \"γ\" exists for Lebesgue-almost all times in \"I\", and the metric derivative is the smallest \"m\" ∈ \"L\"(\"I\"; R) such that the above inequality holds.\n\nIf Euclidean space formula_10 is equipped with its usual Euclidean norm formula_11, and formula_12 is the usual Fréchet derivative with respect to time, then\n\nwhere formula_14 is the Euclidean metric.\n"}
{"id": "19049577", "url": "https://en.wikipedia.org/wiki?curid=19049577", "title": "Neal Amundson", "text": "Neal Amundson\n\nNeal R. Amundson (January 10, 1916February 16, 2011) was an American chemical engineer and applied mathematician. He was the Chair of the Department of Chemical Engineering at the University of Minnesota for over 25 years. Later, he was the Cullen Professor of Chemical & Biomolecular Engineering and Mathematics at the University of Houston. Amundson was considered one of the most prominent chemical engineering educators and researchers in the United States.. The Chemical Engineering and Materials Science building at the University of Minnesota-Twin Cities bears his name.\n\nNeal Amundson was born and raised in Saint Paul, Minnesota, as the only child of a pipefitter and a housewife who struggled to survive the Great Depression. As a young child of 9, Neal became a scratch golfer with golf clubs provided by his father, Oscar Amundson. Neal graduated from St. Paul Central High School in 1933 and was sixth in his class of 658 students. Neal was awarded a bachelor's degree in chemical engineering in 1937 and was first in his class of 52 students that year from the University of Minnesota. He was employed as a process engineer with Standard Oil of New Jersey at a plant in Louisiana. He then returned to Minnesota where he met his wife, Shirley Dimond in 1941, and had three children. He entered graduate school and earned an MS in chemical engineering in 1941 and a Ph.D. in mathematics in 1945 from the University of Minnesota.\n\nHe taught in the mathematics department until 1947 and joined the University of Minnesota's Chemical Engineering Department, where he served as Chair from 1949 until 1977. During his 25 years as department chair, Amundson helped the department to achieve a high national ranking among chemical engineering departments, which it still retains.\n\nAmundson joined the University of Houston (UH) in 1977 as a Cullen Professor and a faculty member of the Chemical Engineering & Mathematics departments. He served as UH Provost from 1987 to 1989. Amundson is known internationally for his pioneering work applying mathematical modeling and analysis to the solution of chemical engineering problems. His technical contributions are in the areas of mathematical modeling and analysis of chemical reactors, separation systems, polymerization units, and coal gasification units. Amundson was one of the main architects of the analytical methodology practiced by chemical engineers today.\n\nAmundson wrote more than 200 technical articles as well as several books. He chaired the U.S. National Research Council committee that wrote the influential \"Frontiers in Chemical Engineering\" report. He was the U.S. editor of \"Chemical Engineering Science\" from 1955 to 1972. Amundson was elected a member of National Academy of Engineering in 1970 and the National Academy of Sciences in 1992. He was elected a Fellow of the American Academy of Arts and Sciences in 1992. The National Academy of Engineering (NAE) bestowed on Amundson the prestigious NAE Founders' Award in 1990.\n\nIn 1996, Amundson was the first recipient of the International Symposia on Chemical Reaction Engineering (ISCRE) award for excellence, an award that is also named for him. The chemical engineering building at his alma mater University of Minnesota is named in his honor. He received numerous professional awards from the American Institute of Chemical Engineers (AIChE), American Chemical Society (ACS), International Symposium on Chemical Reaction Engineering (ISCRE), and American Society for Engineering Education (ASEE).\n\nHe received honorary doctorates from the Universities of Minnesota, Notre Dame, Pennsylvania, Guadalajara, and Northwestern University. He received the highest faculty honors given by the Universities of Minnesota and Houston.\n\nNeal Amundson has authored numerous journal articles describing significant advances in chemical reaction engineering and chemical engineering which includes but is not limited to:\n\n\n\n\n\n\n\n\n\n\nNeal's vision was to combine modern advances in science together with elegant yet practical mathematical methods. In his first two decades as department head of chemical engineering at the University of Minnesota, Amundson hired several chemists, mathematicians, and chemical engineers. He encouraged young faculty to explore new topics in chemical engineering such as biological systems and focused on broadening the discipline to include materials science and engineering. Neal acknowledged the importance of emerging fields and had a goal of attracting young faculty in topics such as microbiology and polymers. In two decades, he hired several faculty that would become leaders of chemical engineering and materials science; these diverse individuals were then combined into a coherent program bound by his philosophy for team teaching and collaborative research. Neal was famous for stating, \"I never hired anybody if I thought I was smarter than they are.\" Amundson promoted the idea of teaching courses in groups, with young faculty teaching and lecturing alongside senior professors; his legacy continues at the University of Minnesota.\n\nNeal Amundson has been called the \"father of modern chemical engineering\" to recognize his invigoration of chemical engineering with mathematics and emerging research fields. His work has impacted multiple generations of students, including his mentoring of 52 PhD students throughout his career. As of 2006, Neal Amundson's academic family tree contains more than 3,000 individuals. His work on reaction engineering, transport phenomena, and complex reacting systems remains the foundation of modern reaction engineering and chemical engineering. As stated by Profssors Andreas Acrivos and Dan Luss, \"Seldom has an individual exerted such a major influence in the development of an important field as was done by Neal Amundson to chemical engineering.\" \n\nAmundson died on February 16, 2011, at the age of 95.\n"}
{"id": "36728510", "url": "https://en.wikipedia.org/wiki?curid=36728510", "title": "Neumann–Poincaré operator", "text": "Neumann–Poincaré operator\n\nIn mathematics, the Neumann–Poincaré operator or Poincaré–Neumann operator, named after Carl Neumann and Henri Poincaré, is a non-self-adjoint compact operator introduced by Poincaré to solve boundary value problems for the Laplacian on bounded domains in Euclidean space. Within the language of potential theory it reduces the partial differential equation to an integral equation on the boundary to which the theory of Fredholm operators can be applied. The theory is particularly simple in two dimensions—the case treated in detail in this article—where it is related to complex function theory, the conjugate Beurling transform or complex Hilbert transform and the Fredholm eigenvalues of bounded planar domains.\n\nGreen's theorem for a bounded region Ω in the plane with smooth boundary ∂Ω states that\n\nOne direct way to prove this is as follows. By subtraction, it is sufficient to prove the theorem for a region bounded by a simple smooth curve. Any such is diffeomorphic to the closed unit disk. By change of variables it is enough to prove the result there. Separating the \"A\" and \"B\" terms, the right hand side can be written as a double integral starting in the \"x\" or \"y\" direction, to which the fundamental theorem of calculus can be applied. This converts the integral over the disk into the integral over its boundary.\n\nLet Ω be a region bounded by a simple closed curve. Given a smooth function \"f\" on the closure of Ω its normal derivative ∂\"f\" at a boundary point is the directional derivative in the direction of the outward pointing normal vector.\nApplying Green's theorem with \"A\" = \"v\" \"u\" and \"B\" = \"v\" \"u\" gives the first of Green's identities:\n\nwhere the Laplacian Δ is given by\n\nSwapping \"u\" and \"v\" and subtracting gives the second of Green's identities:\n\nIf now \"u\" is harmonic in Ω and \"v\" = 1, then this identity implies that\n\nso the integral of the normal derivative of a harmonic function on the boundary of a region always vanishes.\n\nA similar argument shows that the average of a harmonic function on the boundary of a disk equals its value at the centre. Translating the disk can be taken to be centred at 0. Green's identity can be applied to an annulus formed of the boundary of the disk and a small circle centred on 0 with \"v\" = \"z\": it follows that the average is independent of the circle. It tends to the value at its value at 0 as the radius of the smaller circle decreases. This result also follows easily using Fourier series and the Poisson integral.\n\nFor continuous functions \"f\" on the whole plane which are smooth in Ω and the complementary region Ω, the first derivative can have a jump across the boundary of Ω. The value of the normal derivative at a boundary point can be computed from inside or outside Ω. The interior normal derivative will be denoted by ∂ and the exterior normal derivative by ∂. With this terminology the four basic problems of classical potential theory are as follows:\n\n\nFor the exterior problems the inversion map \"z\" takes harmonic functions on Ω into harmonic functions on the image of Ω under the inversion map. The transform \"v\" of \"u\" is continuous in a small disc |\"z\"| ≤ \"r\" and harmonic everywhere in the interior except possibly 0. Let \"w\" be the harmonic function given by the Poisson integral on\n\"v\" − \"w\" + ε log |\"z\"| on δ ≤ |\"z\"| ≤ \"r\", it must be negative for δ small. Hence \"v\"(\"z\") ≤ \"u\"(\"z\") for \"z\" ≠ 0. The same argument applies with \"v\" and \"w\" swapped, so \"v\" = \"w\" is harmonic in the disk. Thus the singularity at ∞ is removable.\n\nBy the maximum principle the interior and exterior Dirichlet problems have unique solutions. For the interior Neumann problem, if a solution \"u\" is harmonic in 0 and its interior normal derivative vanishes, then Green's first identity implies implies the \"u\" = 0 = \"u\", so that \"u\" is constant. This shows the interior Neumann problem has a unique solution up to adding constants. Applying inversion, the same holds for the external Neumann problem.\n\nFor both Neumann problems, a necessary condition for a solution to exist is\n\nFor the interior Neumann problem, this follows by setting \"v\" = 1 in Green's second identity. For the exterior Neumann problem, the same can be done for the intersection of Ω and a large disk |\"z\"| < \"R\", giving\n\nThe first term the last product uniformly bounded because of the smoothness of the Gauss map n(\"t\"). The second is uniformly bounded because of the approximate version of Pythagoras' theorem:\n\nContinuity of \"f\" implies that on ∂Ω\n\nwhich gives the jump formulas.\n\nIf the moment φ is smooth, the derivatives of the single and double layer potentials on Ω and Ω extend continuously to their closures.\n\nAs usual the gradient of a function \"f\" defined on an open set in R is defined by\n\nSet\n\nIf the moment φ is smooth, then\n\nIn fact\n\nso that\n\nMoreover\n\nThe second relation can be rewritten by substituting in from the first relation:\n\nRegularity of layer potentials. As a consequence of these relations, successive derivatives can all be expressed in terms of single and double layer potentials of smooth moments on the boundary. Since the layer potentials on Ω and Ω have continuous limits on the boundary it follows that they define smooth functions on the closures of Ω and Ω.\n\nContinuity of normal derivatives of double layer potentials. Just as the single layer potentials are continuous at the boundary with a jump in the normal derivative, so the double layer potentials have a jump across the boundary while their normal derivatives are continuous. In fact from the formula above\n\nIf \"s\" tends to \"s\" and λ tends to 0, the first term tends to \"T\"(\"v\"(s)) since the moments tend uniformly to a moment vanishing at \"t\" = \"s\"; the second term is continuous because it is a single layer potential.\n\nThe following properties of \"T\" = \"T\" are required to solve the boundary value problem:\n\n\nIn fact since \"a\" I + \"T\" is a Fredholm operator of index 0, it and its adjoint have kernels of equal dimension. The same applies to any power of this operator. So it suffices to verify each of the statements for either \"T\" or \"T\"*. To check that \"T\" has no generalized eigenvectors with eigenvalue 1/2 it suffices to show that\n\nhas no solutions. The definition of the double layer potential shows that it vanishes at ∞, so that it is harmonic at ∞. The equation above shows that if \"u\" = \"D\"(φ) then \"u\" = 1. On the other hand, applying the inversion map gives a contradiction; for it would produce a harmonic map in bounded region vanishing at an interior point with boundary value 1, which contradicts the fact that 1 is the only harmonic map with boundary value 1. If the eigenvalue 1/2 has multiplicity greater than 1, there is a moment φ such that \"T\"*φ = φ/2 and ∫ φ = 0. It follows that if \"u\" = \"S\"(φ) then ∂ \"u\" = 0. By uniqueness \"u\" is constant on Ω. Since \"u\" is continuous on R ∪ ∞ and is harmonic at ∞ (since ∫ φ = 0) and constant on ∂Ω, it must be zero. Hence φ = ∂ \"u\" − ∂ \"u\" = 0. Thus the eigenspace is one-dimensional and the eigenfunction ψ can be normalized so that \"S\"(ψ) = 1 on ∂Ω.\n\nIn general if\n\nthen\n\nsince\n\nIf φ satisfies\n\nit follows that ∫ φ = 0 and so \"u\" = \"S\"(φ) is harmonic at infinity. By the jump formulas, ∂\"u\" = 0. By uniqueness \"u\" is constant on Ω. By continuity it is constant on ∂Ω. Since it is harmonic on Ω and vanishes at infinity, it must vanish identically. As above this forces φ = 0.\n\nThese results on the eigenvalues of \"T\" lead to the following conclusions about the four boundary value problems:\n\n\nThe solution is obtained as follows:\n\n\nThe smoothness of the solution follows from the regularity of single and double layer potentials.\n\nThere is another consequence of the laws giverning the derivatives, which completes the symmetry of the jump relations, is that normal derivative of the double layer potential has no jump across the boundary, i.e. it has a continuous extension to a tubular neighbourhood of the boundary given by\n\n\"H\" is called a hypersingular operator. Although it takes smooth functions to smooth functions, it is not a bounded operator on L(∂Ω). In fact it is a pseudodifferential operator of order 1, so does define a bounded operator between Sobolev spaces on ∂Ω, decreasing the order by 1. It allows a 2 × 2 matrix of operators to be defined by\n\nThe matrix satisfies \"C\" = \"C\", so is an idempotent, called the Calderón projector. This identity is equivalent the following classical relations, the first of which is the symmetrization relation of Plemelj:\n\nThe operators \"T\" and \"S\" are pseudodifferential operators of order −1. The relations above follow by considering \"u\" = \"S\"(φ). It has boundary value \"S\"φ) and normal derivative \"T\"* φ − φ/2. Hence in Ω\n\nTaking the boundary values of both sides and their normal derivative yields 2 equations. Two more result by considering \"D\"(Ψ); these imply the relations for the Calderón projector.\n\nThe non-zero eigenvalues of the Neumann–Poincaré operator \"T\" are called the Fredholm eigenvalues of the region Ω. Since \"T\" is a compact operator, indeed a Hilbert–Schmidt operator, all non-zero elements in its spectrum are eigenvalues of finite multiplicity by the general theory of Fredholm operators. The solution of the boundary value requires knowledge of the spectrum at ± 1/2, namely that the constant function gives an eigenfunction with eigenvalue 1/2 and multiplicity one; that there are no corresponding generalized eigenfunctions with eigenvalue 1/2; and that -1/2 is not an eigenvalue. proved that all non-zero eigenvalues are real and contained in the interval (-1/2,1/2]. proved that the other non-zero eigenvalues have an important symmetry property, namely that if λ is an eigenvalue with 0 < |λ| < 1/2, then so is –λ, with the same multiplicity. Plemelj also showed that \"T\" = \"T\" is a symmetrizable compact operator, so that, even though it is not self-adjoint, it shares many of the properties of self-adjoint operators. In particular there are no generalized eigenfunctions for non-zero eigenvalues and there is a variational principle similar to the minimax principle for determining the non-zero eigenvalues.\n\nIf λ ≠ 1/2 is an eigenvalue of \"T\"* then λ is real, with λ ≠ ± 1/2. Let φ be a corresponding eigenfunction and, following Plemelj, set \"u\" = \"S\"(φ). Then the jump formulas imply that\n\nand hence that\n\nSince ∫ φ = 0, \"u\" is harmonic at ∞. So by Green's theorem\n\nIf both the integrals vanish then \"u\" is constant on Ω and Ω. Since it is continuous and vanishes at ∞, it must therefore be identically 0, contradicting φ = ∂ - ∂. So both integrals are strictly positive and hence λ must lie in (−½,½).\n\nLet φ be an eigenfunction of \"T\"* with real eigenvalue λ satisfying 0 < |λ| < 1/2. If \"u\" = \"S\"(φ), then on\n∂Ω\n\nThis process can be reversed. Let \"u\" be a continuous function on R ∪ ∞ which is harmonic on Ω and Ω ∪ ∞ and such that the derivatives of \"u\" on Ω and Ω extend continuously to their closures. Suppose that\n\nLet ψ be the restriction of \"u\" to ∂Ω. Then\n\nThe jump formulas for the boundary values and normal derivatives give\n\nand\n\nIt follows that\n\nso that ψ and φ are eigenfunctions of \"T\" and \"T\"* with eigenvalue λ.\n\nLet \"u\" be a real harmonic function on Ω extending to a smooth function on its closure. The harmonic conjugate \"v\" of \"u\" is the unique real function on Ω such that \"u\" + \"i\" \"v\" is holomorphic. As such it must satisfy the Cauchy–Riemann equations:\n\nIf \"a\" is a point in Ω, a solution is given by\n\nwhere the integral is taken over any path in the closure of Ω. It is easily verified that \"v\" and \"v\" exist and are given by the corresponding derivatives of \"u\". Thus \"v\" is a smooth function on the closure of Ω, vanishing at 0. By the Cauchy-Riemann equations, \"f\" = \"u\" + \"i\" \"v\" is smooth on the closure of Ω, holomorphic on\nΩ and \"f\"(a) = 0. Using the inversion map, the same result holds for a harmonic function in Ω harmonic at ∞. It has a harmonic conjugate \"v\" such that \"f\" = \"u\" + \"i\" \"v\" extends smoothly to the boundary and \"f\" is holomorphic on Ω ∪ ∞. Adjusting \"v\" by a constant it can be assumed that \"f\"(∞) = 0.\n\nFollowing , let φ be an eigenfunction of \"T\"* with real eigenvalue λ satisfying 0 < |λ| < 1/2. Let \"u\" = \"S\"(φ) and let \"v\" be the harmonic conjugates of \"u\" in Ω and Ω.\nSince on ∂Ω\n\nthe Cauchy-Riemann equations give on ∂Ω\n\nNow define\n\nThus \"U\" is continuous on R and\n\nIt follows that −λ is an eigenvalue of \"T\". Since −\"u\" is the harmonic conjugate of \"v\", the process of taking harmonic conjugates is one-one, so the multiplicity of −λ as an eigenvalue is the same as that of λ.\n\nBy Green's theorem\n\nAdding the two integrals and using the jump relations for the single layer potential, it follows that\n\nThus\n\nThis shows that the operator \"S\" is self-adjoint and non-negative on L(∂Ω).\n\nThe image of \"S\" is dense (or equivalently it has zero kernel). In fact the relation \"SH\" = ¼ \"I\" - \"T\" =(½ \"I\" – \"T\") (½ \"I\" + \"T\") shows that the closure of the image of \"S\" contains the image of ½ \"I\" – \"T\", which has codimension 1. Its orthogonal complement is given by the kernel of \"T\" – ½ \"I\", i.e. the eigenfunction ψ such that \"T\"*ψ = ½ ψ. On the other hand \"ST\"=\"T\"* \"S\". If the closure of the image is not the whole of L(∂Ω) then necessarily \"S\"ψ = 0. Hence \"S\"{ψ) is constant. But then ψ = ∂\"S\"(ψ) – ∂\"S\"(ψ) = 0, a contradiction.\n\nSince \"S\" is strictly positive and \"T\" satisfies the Plemelj symmetrization relation \"ST\"* = \"TS\", the operator \"T\"* is a symmetrizable compact operator. The operator \"S\" defines a new inner product on L(∂Ω):\n\nThe operator \"T\"* is formally self-adjoint with respect to this inner product and by general theory its restriction is bounded and it defines a self-adjoint Hilbert–Schmidt operator on the Hilbert space completion. Since \"T\"* is formally self-adjoint on this inner product space, it follows immediately that any generalized eigenfunction of \"T\"* must already be an eigenfunction. By Fredholm theory, the same is true for \"T\". By general theory the kernel of \"T\" and its non-zero eigenspaces span a dense subspace of L(∂Ω). The Fredholm determinant is defined by\n\nIt can be expressed in terms of the Fredholm eigenvalues λ with modulus less than 1/2, counted with multiplicity, as\n\nNow define the complex Hilbert transform or conjugate Beurling transform \"T\" on L(C) by\n\nThis is a conjugate-linear isometric involution.\n\nIt commutes with ∂ so carries A(Ω) ⊕ A(Ω) onto itself. The compression of \"T\" to A(Ω) is denoted \"T\".\n\nIf \"F\" is a holomorphic univalent map from the unit disk \"D\" onto Ω then the Bergman space of Ω and its conjugate can be identified with that of \"D\" and \"T\" becomes the conjugate-linear singular integral operator with kernel\n\nIt defines a contraction. On the other hand it can be checked that \"T\" = 0 by computing directly on powers using Stokes theorem to transfer the integral to the boundary.\n\nIt follows that the conjugate-linear operator with kernel\n\nacts as a contraction on the Bergman space of \"D\". It is thus a Hilbert–Schmidt operator.\n\nThe conjugate-linear operator \"T\" = \"T\" satisfies the self-adjointness relation\n\nfor \"u\", \"v\" in A(Ω).\n\nThus \"A\" = \"T\" is a compact self-adjoint linear operator on \"H\" with\n\nso that \"A\" is a positive operator. By the spectral theorem for compact self-adjoint operators, there is an orthonormal basis \"u\" of \"H\" consisting of eigenvectors of \"A\":\n\nwhere μ is non-negative by the positivity of \"A\". Hence\n\nwith λ ≥ 0. Since \"T\" commutes with \"A\", it leaves its eigenspaces invariant. The positivity relation shows that it acts trivially on the zero eigenspace. The other non-zero eigenspaces are all finite-dimensional and mutually orthogonal. Thus an orthonormal basis can be chosen on each eigenspace so that:\n\nNote also that\n\nby conjugate-linearity of \"T\".\n\nThe Neumann–Poincaré operator is defined on real functions \"f\" as\n\nwhere \"H\" is the Hilbert transform on ∂Ω. Let \"J\" denote complex conjugation. Writing \"h\" = \"f\" + \"ig\",\n\nso that\n\nThe imaginary part of the Hilbert transform can be used to establish the symmetry properties of the eigenvalues of \"T\". Let\n\nso that\n\nThen\n\nThe Cauchy idempotent \"E\" satisfies \"E\"1 = 1 = \"E\"*1. Since \"J\"1 = 1, it follows that \"E\" and \"E\"* leave invariant \nL(∂Ω), the functions orthogonal to constant functions. The same is also true of \"A\" = 2 \"T\" and \"B\". Let \"A\" and \"B\" be their restrictions. Since 1 is an eigenvector of \"T\" with eigenvalue 1/2 and multiplicity one and \"T\" + ½ \"I\" is invertible,\n\nis invertible, so that \"B\" is invertible. The equation \"A\"\"B\" = − \"B\" \"A\" implies that if λ is an eigenvalue of \"A\" then so is −λ and they have the same multiplicity.\n\nThe links between the Neumann–Poincaré operator and geometric function theory appeared first in . The precise relationship between single and double layer potentials, Fredholm eigenvalues and the complex Hilbert transform is explained in detail in . Briefly given a smooth Jordan curve, the complex derivatives of its single and double layer potentials are −1 and +1 eigenfunctions of the complex Hilbert transform.\n\nLet 𝕳 be the direct sum\n\nwhere the first space consists of functions smooth on the closure of Ω and harmonic on Ω; and the second consists of functions smooth on the closure of Ω, harmonic on Ω and at ≈. The space 𝕳 is naturally an inner product space with corresponding norm given by\n\nEach element of 𝕳 can be written uniquely as the restriction of the sum of a double layer and single layer potential, provided that the moments are normalized to have 0 integral on ∂Ω. Thus for \"f\" ⊕ \"f\" in 𝕳, there are unique φ, ψ in C(∂Ω) with integral 0 such that\n\nUnder this correspondence\n\nThe layer potentials can be identified with their images in 𝕳:\n\nThe space of double layer potentials is orthogonal to the space of single layer potentials for the inner product. In fact by Green's theorem\n\nDefine an isometric embedding of 𝕳 in L(C) by\n\nThe image lies in A(Ω) ⊕ A(Ω), the direct sum of the Bergman spaces of square integrable holomorphic functions on Ω and Ω. Since polynomials in \"z\" are dense in A(Ω) and polynomials in \"z\" without constant term are dense in A(Ω), the image of \"U\" is dense in A(Ω) ⊕ A(Ω).\n\nIt can be verified directly that for φ, ψ real\n\nIn fact for single layer potentials, applying Green's theorem on the domain Ω ∪ Ω with a small closed disk of radius ε removed around a point \"w\" of the domain, it follows that\n\nsince the mean of a harmonic function over a circle is its value at the centre. Using the fact that is the fundamental solution for ∂, this can be rewritten as\n"}
{"id": "3062721", "url": "https://en.wikipedia.org/wiki?curid=3062721", "title": "Neuroinformatics", "text": "Neuroinformatics\n\nNeuroinformatics is a research field concerned with the organization of neuroscience data by the application of computational models and analytical tools. These areas of research are important for the integration and analysis of increasingly large-volume, high-dimensional, and fine-grain experimental data. Neuroinformaticians provide computational tools, mathematical models, and create interoperable databases for clinicians and research scientists. Neuroscience is a heterogeneous field, consisting of many and various sub-disciplines (e.g., cognitive psychology, behavioral neuroscience, and behavioral genetics). In order for our understanding of the brain to continue to deepen, it is necessary that these sub-disciplines are able to share data and findings in a meaningful way; Neuroinformaticians facilitate this.\n\nNeuroinformatics stands at the intersection of neuroscience and information science. Other fields, like genomics, have demonstrated the effectiveness of freely distributed databases and the application of theoretical and computational models for solving complex problems. In Neuroinformatics, such facilities allow researchers to more easily quantitatively confirm their working theories by computational modeling. Additionally, neuroinformatics fosters collaborative research—an important fact that facilitates the field's interest in studying the multi-level complexity of the brain.\n\nThere are three main directions where neuroinformatics has to be applied:\n\n\nIn the recent decade, as vast amounts of diverse data about the brain were gathered by many research groups, the problem was raised of how to integrate the data from thousands of publications in order to enable efficient tools for further research. The biological and neuroscience data are highly interconnected and complex, and by itself, integration represents a great challenge for scientists.\n\nCombining informatics research and brain research provides benefits for both fields of science. On one hand, informatics facilitates brain data processing and data handling, by providing new electronic and software technologies for arranging databases, modeling and communication in brain research. On the other hand, enhanced discoveries in the field of neuroscience will invoke the development of new methods in information technologies (IT).\n\nStarting in 1989, the United States National Institute of Mental Health (NIMH), the National Institute of Drug Abuse (NIDA) and the National Science Foundation (NSF) provided the National Academy of Sciences Institute of Medicine with funds to undertake a careful analysis and study of the need to create databases, share neuroscientific data and to examine how the field of information technology could create the tools needed for the increasing volume and modalities of neuroscientific data. The positive recommendations were reported in 1991. This positive report enabled NIMH, now directed by Allan Leshner, to create the \"Human Brain Project\" (HBP), with the first grants awarded in 1993. The HBP was led by Koslow along with cooperative efforts of other NIH Institutes, the NSF, the National Aeronautics and Space Administration and the Department of Energy. The HPG and grant-funding initiative in this area slightly preceded the explosive expansion of the World Wide Web. From 1993 through 2004 this program grew to over 100 million dollars in funded grants.\n\nNext, Koslow pursued the globalization of the HPG and neuroinformatics through the European Union and the Office for Economic Co-operation and Development (OECD), Paris, France. Two particular opportunities occurred in 1996.\n\n\nThe two related initiates were combined to form the United States proposal on \"Biological Informatics\". This initiative was supported by the White House Office of Science and Technology Policy and presented at the OECD MSF by Edwards and Koslow. An MSF committee was established on Biological Informatics with two subcommittees: 1. Biodiversity (Chair, James Edwards, NSF), and 2. Neuroinformatics (Chair, Stephen Koslow, NIH). At the end of two years the Neuroinformatics subcommittee of the Biological Working Group issued a report supporting a global neuroinformatics effort. Koslow, working with the NIH and the White House Office of Science and Technology Policy to establishing a new Neuroinformatics working group to develop specific recommendation to support the more general recommendations of the first report. The Global Science Forum (GSF; renamed from MSF) of the OECD supported this recommendation.\n\nThis committee presented 3 recommendations to the member governments of GSF. These recommendations were:\n\n\nThe GSF neuroinformatics committee then developed a business plan for the operation, support and establishment of the INCF which was supported and approved by the GSF Science Ministers at its 2004 meeting. In 2006 the INCF was created and its central office established and set into operation at the Karolinska Institute, Stockholm, Sweden under the leadership of Sten Grillner. Sixteen countries (Australia, Canada, China, the Czech Republic, Denmark, Finland, France, Germany, India, Italy, Japan, the Netherlands, Norway, Sweden, Switzerland, the United Kingdom and the United States), and the EU Commission established the legal basis for the INCF and Programme in International Neuroinformatics (PIN). To date, eighteen countries (Australia, Belgium, Czech Republic, Finland, France, Germany, India, Italy, Japan, Malaysia, Netherlands, Norway, Poland, Republic of Korea, Sweden, Switzerland, the United Kingdom and the United States) are members of the INCF. Membership is pending for several other countries.\n\nThe goal of the INCF is to coordinate and promote international activities in neuroinformatics. The INCF contributes to the development and maintenance of database and computational infrastructure and support mechanisms for neuroscience applications. The system is expected to provide access to all freely accessible human brain data and resources to the international research community. The more general task of INCF is to provide conditions for developing convenient and flexible applications for neuroscience laboratories in order to improve our knowledge about the human brain and its disorders.\n\nOn the foundation of all of these activities, Huda Akil, the 2003 President of the Society for Neuroscience (SfN) established the Brain Information Group (BIG) to evaluate the importance of neuroinformatics to neuroscience and specifically to the SfN. Following the report from BIG, SfN also established a neuroinformatics committee.\n\nIn 2004, SfN announced the Neuroscience Database Gateway (NDG) as a universal resource for neuroscientists through which almost any neuroscience databases and tools may be reached. The NDG was established with funding from NIDA, NINDS and NIMH. The Neuroscience Database Gateway has transitioned to a new enhanced platform, the Neuroscience Information Framework. Funded by the NIH Neuroscience BLueprint, the NIF is a dynamic portal providing access to neuroscience-relevant resources (data, tools, materials) from a single search interface. The NIF builds upon the foundation of the NDG, but provides a unique set of tools tailored especially for neuroscientists: a more expansive catalog, the ability to search multiple databases directly from the NIF home page, a custom web index of neuroscience resources, and a neuroscience-focused literature search function.\n\nNeuroinformatics is formed at the intersections of the following fields:\n\nBiology is concerned with molecular data (from genes to cell specific expression); medicine and anatomy with the structure of synapses and systems level anatomy; engineering – electrophysiology (from single channels to scalp surface EEG), brain imaging; computer science – databases, software tools, mathematical sciences – models, chemistry – neurotransmitters, etc. Neuroscience uses all aforementioned experimental and theoretical studies to learn about the brain through its various levels. Medical and biological specialists help to identify the unique cell types, and their elements and anatomical connections. Functions of complex organic molecules and structures, including a myriad of biochemical, molecular, and genetic mechanisms which regulate and control brain function, are determined by specialists in chemistry and cell biology. Brain imaging determines structural and functional information during mental and behavioral activity. Specialists in biophysics and physiology study physical processes within neural cells neuronal networks. The data from these fields of research is analyzed and arranged in databases and neural models in order to integrate various elements into a sophisticated system; this is the point where neuroinformatics meets other disciplines.\n\nNeuroscience provides the following types of data and information on which neuroinformatics operates: \n\nNeuroinformatics uses databases, the Internet, and visualization in the storage and analysis of the mentioned neuroscience data.\n\n\n\n\n\n\n\n\n\n\n\n\nThe main technological tendencies in neuroinformatics are:\n\nIn order to organize and operate with neural data scientists need to use the standard terminology and atlases that precisely describe the brain structures and their relationships.\n\nAnother approach in the area of the brain mappings is the probabilistic atlases obtained from the real data from different group of people, formed by specific factors, like age, gender, diseased etc. Provides more flexible tools for brain research and allow obtaining more reliable and precise results, which cannot be achieved with the help of traditional brain atlases.\n\n\n"}
{"id": "3856935", "url": "https://en.wikipedia.org/wiki?curid=3856935", "title": "Nørlund–Rice integral", "text": "Nørlund–Rice integral\n\nIn mathematics, the Nørlund–Rice integral, sometimes called Rice's method, relates the \"n\"th forward difference of a function to a line integral on the complex plane. As such, it commonly appears in the theory of finite differences, and also has been applied in computer science and graph theory to estimate binary tree lengths. It is named in honour of Niels Erik Nørlund and Stephen O. Rice. Nørlund's contribution was to define the integral; Rice's contribution was to demonstrate its utility by applying saddle-point techniques to its evaluation.\n\nThe \"n\"th forward difference of a function \"f\"(\"x\") is given by\n\nwhere formula_2 is the binomial coefficient.\n\nThe Nörlund–Rice integral is given by\n\nwhere \"f\" is understood to be meromorphic, α is an integer, formula_4, and the contour of integration is understood to circle the poles located at the integers α, ..., \"n\", but encircles neither integers 0, ..., formula_5 nor any of the poles of \"f\". The integral may also be written as\n\nwhere \"B\"(\"a\",\"b\") is the Euler beta function. If the function formula_7 is polynomially bounded on the right hand side of the complex plane, then the contour may be extended to infinity on the right hand side, allowing the transform to be written as\n\nwhere the constant \"c\" is to the left of α.\n\nThe Poisson–Mellin–Newton cycle, noted by Flajolet et al. in 1985, is the observation that the resemblance of the Nørlund–Rice integral to the Mellin transform is not accidental, but is related by means of the binomial transform and the Newton series. In this cycle, let formula_9 be a sequence, and let \"g\"(\"t\") be the corresponding Poisson generating function, that is, let\n\nTaking its Mellin transform\n\none can then regain the original sequence by means of the Nörlund–Rice integral:\n\nwhere Γ is the gamma function.\n\nA closely related integral frequently occurs in the discussion of Riesz means. Very roughly, it can be said to be related to the Nörlund–Rice integral in the same way that Perron's formula is related to the Mellin transform: rather than dealing with infinite series, it deals with finite series.\n\nThe integral representation for these types of series is interesting because the integral can often be evaluated using asymptotic expansion or saddle-point techniques; by contrast, the forward difference series can be extremely hard to evaluate numerically, because the binomial coefficients grow rapidly for large \"n\".\n\n\n"}
{"id": "20420396", "url": "https://en.wikipedia.org/wiki?curid=20420396", "title": "One-class classification", "text": "One-class classification\n\nIn machine learning, one-class classification, also known as unary classification or class-modelling, tries to \"identify\" objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. This is different from and more difficult than the traditional classification problem, which tries to \"distinguish between\" two or more classes with the training set containing objects from all the classes. An example is the classification of the operational status of a nuclear plant as 'normal': In this scenario, there are few, if any, examples of catastrophic system states; only the statistics of normal operation are known. The term one-class classification was coined by Moya & Hush (1996) and many applications can be found in scientific literature, for example outlier detection, anomaly detection, novelty detection. A feature of one-class classification is that it uses only sample points from the assigned class, so that a representative sampling is not strictly required for non-target classes. \n\nWhile many of the above approaches focus on the case of removing a small number of outliers or anomalies, one can also learn the other extreme, where the single class covers a small coherent subset of the data, using an information bottleneck approach.\n\nA similar problem is PU learning, in which a binary classifier is learned in a semi-supervised way from only \"positive\" and \"unlabeled\" sample points.\n\nIn PU learning, two sets of examples are assumed to be available for training: the positive set formula_1 and a \"mixed set\" formula_2, which is assumed to contain both positive and negative samples, but without these being labeled as such. This contrasts with other forms of semisupervised learning, where it is assumed that a labeled set containing examples of both classes is available in addition to unlabeled samples. A variety of techniques exist to adapt supervised classifiers to the PU learning setting, including variants of the EM algorithm. PU learning has been successfully applied to text, time series, and bioinformatics tasks.\n\n"}
{"id": "45116986", "url": "https://en.wikipedia.org/wiki?curid=45116986", "title": "Polynomial Wigner–Ville distribution", "text": "Polynomial Wigner–Ville distribution\n\nIn signal processing, the polynomial Wigner–Ville distribution is a quasiprobability distribution that generalizes the Wigner distribution function. It was proposed by Boualem Boashash and Peter O'Shea in 1994.\n\nMany signals in nature and in engineering applications can be modeled as formula_1, where formula_2 is a polynomial phase and formula_3.\n\nFor example, it is important to detect signals of an arbitrary high-order polynomial phase. However, the conventional Wigner–Ville distribution have the limitation being based on the second-order statistics. Hence, the polynomial Wigner–Ville distribution was proposed as a generalized form of the conventional Wigner–Ville distribution, which is able to deal with signals with nonlinear phase.\n\nThe polynomial Wigner–Ville distribution formula_4 is defined as\n\nwhere formula_6 denotes the Fourier transform with respect to formula_7, and formula_8 is the polynomial kernel given by\n\nwhere formula_10 is the input signal and formula_11 is an even number.\nThe above expression for the kernel may be rewritten in symmetric form as\n\nThe discrete-time version of the polynomial Wigner–Ville distribution is given by the discrete Fourier transform of\n\nwhere formula_14 and formula_15 is the sampling frequency.\nThe conventional Wigner–Ville distribution is a special case of the polynomial Wigner–Ville distribution with formula_16\n\nOne of the simplest generalizations of the usual Wigner–Ville distribution kernel can be achieved by taking formula_17. The set of coefficients formula_18 and formula_19 must be found to completely specify the new kernel. For example, we set\n\nThe resulting discrete-time kernel is then given by\n\nGiven a signal formula_1, where formula_24is a polynomial function, its instantaneous frequency (IF) is formula_25.\n\nFor a practical polynomial kernel formula_8, the set of coefficients formula_27and formula_19should be chosen properly such thatformula_29formula_30\n\n\nformula_32formula_33formula_34formula_35\n\n\nformula_37formula_38\n\nNonlinear FM signals are common both in nature and in engineering applications. For example, the sonar system of some bats use hyperbolic FM and quadratic FM signals for echo location. In radar, certain pulse-compression schemes employ linear FM and quadratic signals. The Wigner–Ville distribution has optimal concentration in the time-frequency plane for linear frequency modulated signals. However, for nonlinear frequency modulated signals, optimal concentration is not obtained, and smeared spectral representations result. The polynomial Wigner–Ville distribution can be designed to cope with such problem.\n\n1. B. Boashash and P. O’Shea, “Polynomial Wigner–Ville distributions and their relationship to time varying high order spectra,”IEEE Trans. Signal Process., vol. 42, pp. 216–220, Jan. 1994.\n2. M. Benidir and B. Boashash, “On the polynomial Wigner–Ville dis-tribution,” in Proc. SPIE, June 1995, San Diego, CA, vol. 2563, pp. 69–79.\n3. “Polynomial Wigner–Ville distributions and time-varying higher spectra,” in Proc. Time-Freq. Time-Scale Anal., Victoria, B.C., Canada, Oct. 1992, pp. 31–34.\n\n4. Jian-Jiun Ding, Time frequency analysis and wavelet transform class notes, the Department of Electrical Engineering, National Taiwan University (NTU), Taipei, Taiwan, 2018.\n"}
{"id": "3314539", "url": "https://en.wikipedia.org/wiki?curid=3314539", "title": "Pythagorean means", "text": "Pythagorean means\n\nIn mathematics, the three classical Pythagorean means are the arithmetic mean (\"AM\"), the geometric mean (\"GM\"), and the harmonic mean (\"HM\"). These means were studied with proportions by Pythagoreans and later generations of Greek mathematicians because of their importance in geometry and music.\n\nThey are defined by:\n\nEach mean, formula_2, has the following properties:\n\nThe harmonic and arithmetic means are reciprocal duals of each other for positive arguments:\n\nwhile the geometric mean is its own reciprocal dual:\n\nThere is an ordering to these means (if all of the formula_11 are positive)\nwith equality holding if and only if the formula_13 are all equal. \n\nThis is a generalization of the inequality of arithmetic and geometric means and a special case of an inequality for generalized means. The proof follows from the arithmetic-geometric mean inequality, formula_14, and reciprocal duality (formula_15 and formula_16 are also reciprocal dual to each other).\n\nThe study of the Pythagorean means is closely related to the study of majorization and Schur-convex functions. The harmonic and geometric means are concave symmetric functions of their arguments, and hence Schur-concave, while the arithmetic mean is a linear function of its arguments, so both concave and convex.\n\n\n"}
{"id": "24593105", "url": "https://en.wikipedia.org/wiki?curid=24593105", "title": "Quantum spacetime", "text": "Quantum spacetime\n\nIn mathematical physics, the concept of quantum spacetime is a generalization of the usual concept of spacetime in which some variables that ordinarily commute are assumed not to commute and form a different Lie algebra. The choice of that algebra still varies from theory to theory.\nAs a result of this change some variables that are usually continuous may become discrete.\nOften only such discrete variables are called \"quantized\"; usage varies.\nThe idea of quantum spacetime was proposed in the early days of quantum theory by Heisenberg and Ivanenko\nas a way to eliminate infinities from quantum field theory.\nThe germ of the idea passed from Heisenberg to Rudolf Peierls, who noted that electrons in a magnetic field\ncan be regarded as moving in a quantum space-time, and to Robert Oppenheimer, who carried it\nto Hartland Snyder,\nwho published the first concrete example.\nSnyder's Lie algebra was made simple by C. N. Yang in the same year.\n\nPhysical reasons have been given to believe that physical spacetime is a quantum spacetime.\nIn quantum mechanics position and momentum variables formula_1 are already noncommutative, obey the Heisenberg uncertainty principle, and are continuous.\nBecause of the Heisenberg uncertainty relations, greater energy is needed to probe smaller distances.\nUltimately, according to gravity theory, the probing particles form black holes that destroy what was to be measured. The process cannot be repeated, so it cannot be counted as a measurement.\nThis limited measurability led many to expect that our usual picture of continuous commutative spacetime breaks down at Planck scale distances, if not sooner.\n\nAgain, physical spacetime is expected to be quantum because physical coordinates are already slightly noncommutative.\nThe astronomical coordinates of a star are modified by gravitational fields between us and the star, as in the deflection of light by the sun, one of the classic tests of general relativity.\nTherefore, the coordinates actually depend on gravitational field variables.\nAccording to quantum theories of gravity these field variables do not commute;\ntherefore coordinates that depend on them likely do not commute.\n\nBoth arguments are based on pure gravity and quantum theory, and they limit the measurement of time\nby the only time constant in pure quantum gravity, the Planck time.\nOur instruments, however, are not purely gravitational but are made of particles. They may set a more severe, larger, limit than the Planck time.\n\nQuantum spacetimes are often described mathematically using the noncommutative geometry of Connes,\nquantum geometry, or quantum groups.\n\nAny noncommutative algebra with at least four generators could be interpreted as a quantum spacetime, but\nthe following desiderata have been suggested:\nThis would permit wave equations for particles and fields and facilitate predictions for experimental deviations from classical spacetime physics that can then be tested experimentally.\n\nSeveral models were found in the 1990s more or less meeting most of the above criteria.\n\nThe bicrossproduct model spacetime was introduced by Shahn Majid and Henri Ruegg and has Lie algebra relations\n\nfor the spatial variables formula_6 and the time variable formula_7. Here formula_2 has dimensions of time and is therefore expected to be something like the Planck time. The Poincaré group here is correspondingly deformed, now to a certain bicrossproduct quantum group with the following characteristic features.\n\nThe momentum generators formula_9 commute among themselves but addition of momenta, reflected in the quantum group structure, is deformed (momentum space becomes a non-abelian group). Meanwhile, the Lorentz group generators enjoy their usual relations among themselves but act non-linearly on the momentum space. The orbits for this action are depicted in the figure as a cross-section of formula_10 against one of the formula_9. The on-shell region describing particles in the upper center of the image would normally be hyperboloids but these are now `squashed' into the cylinder\n\nin simplified units. The upshot is that Lorentz-boosting a momentum will never increase it above the Planck momentum. The existence of a highest momentum scale or lowest distance scale fits the physical picture. This squashing comes from the non-linearity of the Lorentz boost and is an endemic feature of bicrossproduct quantum groups known since their introduction in 1988. Some physicists dub the bicrossproduct model doubly special relativity, since it sets an upper limit to both speed and momentum.\n\nAnother consequence of the squashing is that the propagation of particles is deformed, even of light, leading to a variable speed of light. This prediction requires the particular formula_13 to be the physical energy and spatial momentum (as opposed to some other function of them). Arguments for this identification were provided in 1999 by Giovanni Amelino-Camelia and Majid through a study of plane waves for a quantum differential calculus in the model. They take the form\n\nin other words a form which is sufficiently close to classical that one might plausibly believe the interpretation. At the moment such wave analysis represents the best hope to obtain physically testable predictions from the model.\n\nPrior to this work there were a number of unsupported claims to make predictions from the model based solely on the form of the Poincaré quantum group. There were also claims based on an earlier formula_15-Poincaré quantum group introduced by Jurek Lukierski and co-workers which should be viewed as an important precursor to the bicrossproduct one, albeit without the actual quantum spacetime and with different proposed generators for which the above picture does not apply. The bicrossproduct model spacetime has also been called formula_15-deformed spacetime with formula_17.\n\nThis model was introduced independently by a team working under Julius Wess in 1990 and by Majid and coworkers in a series of papers on braided matrices starting a year later. The point of view in the second approach is that usual Minkowski spacetime has a nice description via Pauli matrices as the space of 2 x 2 hermitian matrices. In quantum group theory and using braided monoidal category methods one has a natural q-version of this defined here for real values of formula_18 as a `braided hermitian matrix' of generators and relations\n\nThese relations say that the generators commute as formula_20 thereby recovering usual Minkowski space. One can work with more familiar variables formula_21 as linear combinations of these. In particular, time\n\nis given by a natural braided trace of the matrix and commutes with the other generators (so this model has a very different flavour from the bicrossproduct one). The braided-matrix picture also leads naturally to a quantity\n\nwhich as formula_20 returns us the usual Minkowski distance (this translates to a metric in the quantum differential geometry). The parameter formula_25 or formula_26 is dimensionless and formula_2 is thought to be a ratio of the Planck scale and the cosmological length. That is, there are indications that this model relates to quantum gravity with non-zero cosmological constant, the choice of formula_18 depending on whether this is positive or negative. We have described the mathematically better understood but perhaps less physically justified positive case here.\n\nA full understanding of this model requires (and was concurrent with the development of) a full theory of `braided linear algebra' for such spaces. The momentum space for the theory is another copy of the same algebra and there is a certain `braided addition' of momentum on it expressed as the structure of a braided Hopf algebra or quantum group \"in\" a certain braided monoidal category). This theory by 1993 had provided the corresponding formula_18-deformed Poincaré group as generated by such translations and formula_18-Lorentz transformations, completing the interpretation as a quantum spacetime.\n\nIn the process it was discovered that the Poincaré group not only had to be deformed but had to be extended to include dilations of the quantum spacetime. For such a theory to be exact we would need all particles in the theory to be massless, which is consistent with experiment as masses of elementary particles are indeed vanishingly small compared to the Planck mass. If current thinking in cosmology is correct then this model is more appropriate, but it is significantly more complicated and for this reason its physical predictions have yet to be worked out\n\nThis refers in modern usage to the angular momentum algebra\n\nfamiliar from quantum mechanics but interpreted in this context as coordinates of a quantum space or spacetime. These relations were proposed by Roger Penrose in his earliest spin network theory of space. It is a toy model of quantum gravity in 3 spacetime dimensions (not the physical 4) with a Euclidean (not the physical Minkowskian) signature. It was again proposed in this context by Gerardus 't Hooft. A further development including a quantum differential calculus and an action of a certain `quantum double' quantum group as deformed Euclidean group of motions was given by Majid and E. Batista\n\nA striking feature of the noncommutative geometry here is that the smallest covariant quantum differential calculus has one dimension higher than expected, namely 4, suggesting that the above can also be viewed as the spatial part of a 4-dimensional quantum spacetime. The model should not be confused with fuzzy spheres which are finite-dimensional matrix algebras which one can think of as spheres in the spin model spacetime of fixed radius.\n\nThe quantum spacetime of Hartland Snyder proposes that\n\nwhere the formula_33 generate the Lorentz group. This quantum spacetime and that of C. N. Yang entail a radical unification of spacetime, energy-momentum, and angular momentum.\n\nThe idea was revived in a modern context by Sergio Doplicher, Klaus Fredenhagen and John Roberts in 1995\n\nAn even simpler variant of this model is to let formula_37 here be a numerical antisymmetric tensor, in which context it is usually denoted formula_38, so the relations are formula_39. In even dimensions formula_40, any nondegenerate such theta can be transformed to a normal form in which this really is just the Heisenberg algebra but the difference that the variables are being proposed as those of spacetime. This proposal was for a time quite popular because of its familiar form of relations and because it has been argued that it emerges from the theory of open strings landing on D-branes, see noncommutative quantum field theory and Moyal plane. However, it should be realised that this D-brane lives in some of the higher spacetime dimensions in the theory and hence it is not our physical spacetime that string theory suggests to be effectively quantum in this way. You also have to subscribe to D-branes as an approach to quantum gravity in the first place. Even when posited as quantum spacetime it is hard to obtain physical predictions and one reason for this is that if formula_38 is a tensor then by dimensional analysis it should have dimensions of lengthformula_42, and if this length is speculated to be the Planck length then the effects would be even harder to ever detect than for other models.\n\nAlthough not quantum spacetime in the sense above, another use of noncommutative geometry is to tack on `noncommutative extra dimensions' at each point of ordinary spacetime. Instead of invisible curled up extra dimensions as in string theory, Alain Connes and coworkers have argued that the coordinate algebra of this extra part should be replaced by a finite-dimensional noncommutative algebra. For a certain reasonable choice of this algebra, its representation and extended Dirac operator, one is able to recover the Standard Model of elementary particles. In this point of view the different kinds of matter particles are manifestations of geometry in these extra noncommutative directions. Connes's first works here date from 1989 but has been developed considerably since then. Such an approach can theoretically be combined with quantum spacetime as above.\n\n\n\n"}
{"id": "20179556", "url": "https://en.wikipedia.org/wiki?curid=20179556", "title": "Range accrual", "text": "Range accrual\n\nIn finance, a range accrual is a type of derivative product very popular among structured-note investors. It is estimated that more than US$160 billion of Range Accrual indexed on interest rates only have been sold to investors between 2004 and 2007. It is one of the most popular non-vanilla financial derivatives. In essence the investor in a range accrual is betting that the reference \"index\" - usually interest rates or currency exchange rates - will stay within a predefined range.\n\nA general expression for the payoff of a range accrual is:\n\n\nIf the observation frequency is daily, the payoff could be more easily written as\n\nwhere\n\n\nThe index could be an interest rate (e.g. USD 3 months Libor), or a FX rate (e.g. EUR/USD) or a commodity (e.g. oil price) or any other observable financial index.\nThe observation period can be different from daily (e.g. weekly, monthly,etc.), though a daily observation is the most encountered.\n\nThe receiver of the range accrual coupons is selling binary options. The value of these options is used to enhance the coupon paid.\n\nLet's take an example of a 5 years range accrual note linked to USD 3 months Libor, with range set as [1.00%; 6.00%] and a conditional coupon of 5.00%. Let's assume the note to start on January 1, 2009 and the first coupon payment to happen on July 1, 2009.\n\nAn investor who buys USD 100m of this note will have the following cash flows:\n\n\n1.00% and 6.00% for 130 days, then the rate applied for the first semester will be:\n\n\n\nThe payout (\"P\" in our notation), for each day the index is in the range, could be either a fix or variable rate.\n\nA range accrual can be seen as a strip of binary options, with a decreasing lag between fixing date and payment date. For this reason, it is important the valuation model is well calibrated to the volatility term structure of the underlying, at least at the strikes implied by the range.\n\nIf furthermore the range accrual is callable, then the valuation model also needs to take into account the dynamic between the swaption and the underlying.\n\nAccrual swaps that monitor permanence of interest rates into a range and pay a related interest rate times the permanence factor also depend on correlation across different adjacent forward rates. For the details see for example Brigo and Mercurio (2001).\n\n\"(To be completed)\"\n"}
{"id": "28088480", "url": "https://en.wikipedia.org/wiki?curid=28088480", "title": "Romanian Master of Mathematics and Sciences", "text": "Romanian Master of Mathematics and Sciences\n\nThe Romanian Master of Mathematics and Sciences (formerly known as the Romanian Masters in Mathematics) is an annual competition for students in the preuniversitary level, held in Bucharest, Romania. The contestants compete individually, in four different sections: \"mathematics\", \"physics\", \"chemistry\" and \"computer science\". The participating teams (national and local teams) can have up to six students for each section (plus two teachers: a leader and a deputy leader). The contest follows the same structure as IMO and IPhO and is usually held at the end of February.\n\nThe first Romanian Master in Mathematics was held in 2008 and has been initiated by prof. Severius Moldoveanu and prof. Radu Gologan . In 2010 Physics was also added as a section, therefore the name changed to RMMS. At the beginning, the competition structure had been 4 problems in 5 hours, but also in 2010, it was changed to 3 problems in 4 hours, two days format. The first country that won the competition was the United Kingdom. The 4th edition was held between 23–28 of February 2011 and included also Chemistry and Computer Science. The 5th edition, held in 2012 was only for Physics and Mathematics. \nThe current champions in Mathematics is South Korea.\n\nThe contest is organised by the Tudor Vianu National College of Computer Science in collaboration with the Sector 1 town council. As a host, the college has the right to have its own team entering the contest in each section, thus participating against countries.\n"}
{"id": "13345478", "url": "https://en.wikipedia.org/wiki?curid=13345478", "title": "Schilder's theorem", "text": "Schilder's theorem\n\nIn mathematics, Schilder's theorem is a result in the large deviations theory of stochastic processes. Roughly speaking, Schilder's theorem gives an estimate for the probability that a (scaled-down) sample path of Brownian motion will stray far from the mean path (which is constant with value 0). This statement is made precise using rate functions. Schilder's theorem is generalized by the Freidlin–Wentzell theorem for Itō diffusions.\n\nLet \"B\" be a standard Brownian motion in \"d\"-dimensional Euclidean space R starting at the origin, 0 ∈ R; let W denote the law of \"B\", i.e. classical Wiener measure. For \"ε\" > 0, let W denote the law of the rescaled process \"B\". Then, on the Banach space \"C\" = \"C\"([0, \"T\"]; R) of continuous functions formula_1 such that formula_2, equipped with the supremum norm ||·||, the probability measures W satisfy the large deviations principle with good rate function \"I\" : \"C\" → R ∪ {+∞} given by\n\nif \"ω\" is absolutely continuous, and \"I\"(\"ω\") = +∞ otherwise. In other words, for every open set \"G\" ⊆ \"C\" and every closed set \"F\" ⊆ \"C\",\n\nand\n\nTaking \"ε\"=, one can use Schilder's theorem to obtain estimates for the probability that a standard Brownian motion \"B\" strays further than \"c\" from its starting point over the time interval [0, \"T\"], i.e. the probability\n\nas \"c\" tends to infinity. Here B(0; ||·||) denotes the open ball of radius \"c\" about the zero function in \"C\", taken with respect to the supremum norm. First note that\n\nSince the rate function is continuous on \"A\", Schilder's theorem yields\n\nmaking use of the fact that the infimum over paths in the collection \"A\" is attained for \"ω\"(\"t\") = \"t\" ⁄ \"T\". This result can be heuristically interpreted as saying that, for large \"c\" and/or large \"T\"\n\nIn fact, the above probability can be estimated more precisely: for \"B\" a standard Brownian motion in R, and any \"T\", \"c\" and \"ε\" > 0, we have:\n\n"}
{"id": "54727", "url": "https://en.wikipedia.org/wiki?curid=54727", "title": "Scotland Yard (board game)", "text": "Scotland Yard (board game)\n\nScotland Yard is a board game in which a team of players, as police, cooperate to track down a player controlling a criminal around a board representing the streets of London. It is named after Scotland Yard, the headquarters of London's Metropolitan Police Service. \"Scotland Yard\" is an asymmetric board game, with the detective players cooperatively solving a variant of the pursuit-evasion problem. The game is published by Ravensburger in most of Europe and Canada and by Milton Bradley in the United States. It received the \"Spiel des Jahres\" (Game of the Year) award in 1983. \n\nOne player controls \"Mr. X\", a criminal whose location is only revealed periodically, and the other players each control a detective, which is always present on the board.\n\nAll players start with a number of tokens allowing them to use the following methods of transport:\n\nEach player (Mr. X and the detectives) draws one of 18 possible cards which show where a player has to start, with Mr. X always drawing first. The locations on these cards are spaced far enough apart to ensure that Mr. X cannot be caught in the first round of play. There are a total of 199 locations on the board.\n\nEach detective begins with a total of 22 tokens. Once each transport token is used by a detective, it is turned over to Mr. X, effectively giving him unlimited transport. As he makes each move, he writes down his destination (either in the log book provided with the game, or on a sheet of paper) and covers it with the token he has used, so that the detectives have clues as to his whereabouts. Mr. X also has a supply of black tokens that can be used for any mode of transport (one per detective in play; in the Milton Bradley version this is always five), and two cards that allow him to make two moves in a single turn. The water routes require a black token; when one of these is played, the detectives must consider whether or not it is being used to hide a river trip. Mr. X moves first on every turn, after which the detectives must move in the same order.\n\nAt five specific times during the game, Mr. X has to reveal his current position. Detectives will take this opportunity to refine their search and, if possible, plan ways to encircle him. From each known position, the types of transport used by Mr. X limit the number of possible locations he can reach on his next move, which provides useful information to detectives (as well as preventing some types of cheating by the fugitive player).\n\nThe game is won by the detectives if any of them lands on Mr. X's current location or vice versa. Mr. X wins by avoiding capture until all detectives can no longer move, due to either exhausting their token supplies or reaching a space for which they have no more usable tokens.\n\nAlthough the game says it is for 3-6 players many play this game with only 2 players. The police, when controlled by 1 person, are far more coordinated and have a better chance of catching Mr. X. When 3-5 people are playing as the police they have to work as a team and coordinate their moves which can be difficult, especially when one player wants to play a hunch.\n\nThe contents of the game contain:\n\nThere are two main board editions, one typically associated with Milton Bradley, and another typically associated with Ravensburger. The primary difference between these is in the numbering of the stations: five stations are numbered differently, with 108 missing from the Milton Bradley boards, and 200 missing from the Ravensburger boards. There are also minor differences in the routes, such as a bus line between stations 198 and 199 that is changed to a taxi line in later editions, and the removal of a taxi line between stations 13 and 14 sometime after the renumbering.\n\nThe game has been adapted to take place on maps of different cities. \"Scotland Yard Tokyo\", also distributed by Ravensburger, takes place on the streets of Tokyo, with the major difference being game aesthetics. \"Scotland Yard: Swiss Edition\" uses the same gameplay and is set in Switzerland, with the addition of more boat routes and ski areas available only to Mr. X.\n\n\"NY Chase\" is a version based on New York City. In this version, detectives do not hand their used tokens over, and they have access to roadblocks and a helicopter, tilting the game more in favour of those playing as detectives.\n\nA faster travel version called \"Die Jagd Nach Mister X\" exists that functions quite differently. In this version, Mr. X's location is only hidden when a black travel token is used, and the game is essentially an open chase around London. Evasion is accomplished with black tokens and using the fastest travel to distant locations. In this version, each player takes turns as Mr. X, and points collected (in the form of the detectives' used travel tokens) determine the overall winner.\n\nThree years after the game's publication, Alain Munoz and Serge Laget posted an article in the French magazine \"Jeux & Stratégie\" suggesting alternative rules to balance and expand the game.\n\nThe game was first adapted for the Nintendo Game Boy in 1990, and then as \"Scotland Yard Interactive\" for the Philips CD-I in 1993. It was subsequently adapted for Windows by Cryo Interactive in 1998, for the Nintendo DS by Sproing Interactive in 2008, and for iPhone (2012) and Android (2015) by Ravensburger Digital.\n\n"}
{"id": "2844303", "url": "https://en.wikipedia.org/wiki?curid=2844303", "title": "Spin structure", "text": "Spin structure\n\nIn differential geometry, a spin structure on an orientable Riemannian manifold allows one to define associated spinor bundles, giving rise to the notion of a spinor in differential geometry.\n\nSpin structures have wide applications to mathematical physics, in particular to quantum field theory where they are an essential ingredient in the definition of any theory with uncharged fermions. They are also of purely mathematical interest in differential geometry, algebraic topology, and K theory. They form the foundation for spin geometry.\n\nIn geometry and in field theory, mathematicians ask whether or not a given oriented Riemannian manifold (\"M\",\"g\") admits spinors. One method for dealing with this problem is to require that \"M\" has a spin structure. This is not always possible since there is potentially a topological obstruction to the existence of spin structures. Spin structures will exist if and only if the second Stiefel–Whitney class \"w\"(\"M\") ∈ H(\"M\", Z) of \"M\" vanishes. Furthermore, if \"w\"(\"M\") = 0, then the set of the isomorphism classes of spin structures on \"M\" is acted upon freely and transitively by H(\"M\", Z) . As the manifold \"M\" is assumed to be oriented, the first Stiefel–Whitney class \"w\"(\"M\") ∈ H(\"M\", Z) of \"M\" vanishes too. (The Stiefel–Whitney classes \"w\"(\"M\") ∈ H(\"M\", Z) of a manifold \"M\" are defined to be the Stiefel–Whitney classes of its tangent bundle \"TM\".)\n\nThe bundle of spinors π: \"S\" → \"M\" over \"M\" is then the complex vector bundle associated with the corresponding principal bundle π: P → \"M\" of spin frames over \"M\" and the spin representation of its structure group Spin(\"n\") on the space of spinors Δ. The bundle \"S\" is called the spinor bundle for a given spin structure on \"M\".\n\nA precise definition of spin structure on manifold was possible only after the notion of fiber bundle had been introduced; André Haefliger (1956) found the topological obstruction to the existence of a spin structure on an orientable Riemannian manifold and Max Karoubi (1968) extended this result to the non-orientable pseudo-Riemannian case.\n\nA spin structure on an orientable Riemannian manifold \"(M,g)\" is an equivariant lift of the oriented orthonormal frame bundle \"F\"(\"M\") → \"M\" with respect to the double covering ρ: Spin(\"n\") → SO(\"n\"). In other words, a pair (P,\"F\") is a spin structure on the principal bundle π: \"F\"(\"M\") → \"M\" when\n\nThe principal bundle π: P → \"M\" is also called the bundle of spin frames over \"M\".\n\nTwo spin structures (P, \"F\") and (P, \"F\") on the same oriented Riemannian manifold \"(M,g)\" are called \"equivalent\" if there exists a Spin(\"n\")-equivariant map \"f\": P → P such that\n\nOf course, in this case formula_4 and formula_5 are two equivalent double coverings of the oriented orthonormal frame SO(\"n\")-bundle \"F\"(\"M\") → \"M\" of the given Riemannian manifold \"(M,g)\".\n\nThis definition of spin structure on (\"M\",\"g\") as a spin structure on the principal bundle \"F\"(\"M\") → \"M\" is due to André Haefliger (1956).\n\nAndré Haefliger found necessary and sufficient conditions for the existence of a spin structure on an oriented Riemannian manifold (\"M\",\"g\"). The obstruction to having a spin structure is a certain element [\"k\"] of H(\"M\", Z) . For a spin structure the class [\"k\"] is the second Stiefel–Whitney class \"w\"(\"M\") ∈ H(\"M\", Z) of \"M\". Hence, a spin structure exists if and only if the second Stiefel–Whitney class \"w\"(\"M\") ∈ H(\"M\", Z) of \"M\" vanishes.\n\nLet \"M\" be a paracompact topological manifold and \"E\" an oriented vector bundle on \"M\" of dimension \"n\" equipped with a fibre metric. This means that at each point of \"M\", the fibre of \"E\" is an inner product space. A spinor bundle of \"E\" is a prescription for consistently associating a spin representation to every point of \"M\". There are topological obstructions to being able to do it, and consequently, a given bundle \"E\" may not admit any spinor bundle. In case it does, one says that the bundle \"E\" is \"spin\".\n\nThis may be made rigorous through the language of principal bundles. The collection of oriented orthonormal frames of a vector bundle form a frame bundle \"P\"(\"E\"), which is a principal bundle under the action of the special orthogonal group SO(\"n\"). A spin structure for \"P\"(\"E\") is a \"lift\" of \"P\"(\"E\") to a principal bundle \"P\"(\"E\") under the action of the spin group Spin(\"n\"), by which we mean that there exists a bundle map φ : \"P\"(\"E\") → \"P\"(\"E\") such that\nwhere is the mapping of groups presenting the spin group as a double-cover of SO(\"n\").\n\nIn the special case in which \"E\" is the tangent bundle \"TM\" over the base manifold \"M\", if a spin structure exists then one says that \"M\" is a spin manifold. Equivalently \"M\" is \"spin\" if the SO(\"n\") principal bundle of orthonormal bases of the tangent fibers of \"M\" is a Z quotient of a principal spin bundle.\n\nIf the manifold has a cell decomposition or a triangulation, a spin structure can equivalently be thought of as a homotopy-class of trivialization of the tangent bundle over the 1-skeleton that extends over the 2-skeleton. If the dimension is lower than 3, one first takes a Whitney sum with a trivial line bundle.\n\nA spin structure on a vector bundle \"E\" exists if and only if the second Stiefel–Whitney class \"w\" of \"E\" vanishes. This is a result of Armand Borel and Friedrich Hirzebruch. Note, we have assumed π: \"E\" → \"M\" is an orientable vector bundle.\n\nWhen spin structures exist, the inequivalent spin structures on a manifold have a one-to-one correspondence (not canonical) with the elements of H(\"M\",Z), which by the universal coefficient theorem is isomorphic to H(\"M\",Z). More precisely, the space of the isomorphism classes of spin structures is an affine space over H(\"M\",Z).\n\nIntuitively, for each nontrivial cycle on \"M\" a spin structure corresponds to a binary choice of whether a section of the SO(\"N\") bundle switches sheets when one encircles the loop. If \"w\" vanishes then these choices may be extended over the two-skeleton, then (by obstruction theory) they may automatically be extended over all of \"M\". In particle physics this corresponds to a choice of periodic or antiperiodic boundary conditions for fermions going around each loop. Note that on a complex manifold formula_7 the second Steifel-Whitney class can be computed as the first chern class formula_8.\n\n\n\nA spin structure is analogous to a spin structure on an oriented Riemannian manifold, but uses the Spin group, which is defined instead by the exact sequence \nTo motivate this, suppose that is a complex spinor representation. The center of U(\"N\") consists of the diagonal elements coming from the inclusion , i.e., the scalar multiples of the identity. Thus there is a homomorphism\nThis will always have the element (−1,−1) in the kernel. Taking the quotient modulo this element gives the group Spin(\"n\"). This is the twisted product\n\nwhere U(1) = SO(2) = S. In other words, the group Spin(\"n\") is a central extension of SO(\"n\") by S.\n\nViewed another way, Spin(\"n\") is the quotient group obtained from with respect to the normal Z which is generated by the pair of covering transformations for the bundles and respectively. This makes the Spin group both a bundle over the circle with fibre Spin(\"n\"), and a bundle over SO(\"n\") with fibre a circle.\n\nThe fundamental group π(Spin(\"n\")) is isomorphic to Z.\n\nIf the manifold has a cell decomposition or a triangulation, a spin structure can be equivalently thought of as a homotopy class of complex structure over the 2-skeleton that extends over the 3-skeleton. Similarly to the case of spin structures, one takes a Whitney sum with a trivial line bundle if the manifold is odd-dimensional.\n\nYet another definition is that a spin structure on a manifold \"N\" is a complex line bundle \"L\" over \"N\" together with a spin structure on .\n\nA spin structure exists when the bundle is orientable and the second Stiefel–Whitney class of the bundle \"E\" is in the image of the map (in other words, the third integral Stiefel–Whitney class vanishes). In this case one says that \"E\" is spin. Intuitively, the lift gives the Chern class of the square of the U(1) part of any obtained spin bundle.\nBy a theorem of Hopf and Hirzebruch, closed orientable 4-manifolds always admit a spin structure.\n\nWhen a manifold carries a spin structure at all, the set of spin structures forms an affine space. Moreover, the set of spin structures has a free transitive action of . Thus, spin-structures correspond to elements of although not in a natural way.\n\nThis has the following geometric interpretation, which is due to Edward Witten. When the spin structure is nonzero this square root bundle has a non-integral Chern class, which means that it fails the triple overlap condition. In particular, the product of transition functions on a three-way intersection is not always equal to one, as is required for a principal bundle. Instead it is sometimes −1.\n\nThis failure occurs at precisely the same intersections as an identical failure in the triple products of transition functions of the obstructed spin bundle. Therefore, the triple products of transition functions of the full \"spin\" bundle, which are the products of the triple product of the \"spin\" and U(1) component bundles, are either or and so the spin bundle satisfies the triple overlap condition and is therefore a legitimate bundle.\n\nThe above intuitive geometric picture may be made concrete as follows. Consider the short exact sequence , where the second arrow is multiplication by 2 and the third is reduction modulo 2. This induces a long exact sequence on cohomology, which contains\n\nwhere the second arrow is induced by multiplication by 2, the third is induced by restriction modulo 2 and the fourth is the associated Bockstein homomorphism \"β\".\n\nThe obstruction to the existence of a \"spin\" bundle is an element \"w\" of . It reflects the fact that one may always locally lift an SO(N) bundle to a \"spin\" bundle, but one needs to choose a Z lift of each transition function, which is a choice of sign. The lift does not exist when the product of these three signs on a triple overlap is −1, which yields the Čech cohomology picture of \"w\".\n\nTo cancel this obstruction, one tensors this \"spin\" bundle with a U(1) bundle with the same obstruction \"w\". Notice that this is an abuse of the word \"bundle\", as neither the \"spin\" bundle nor the U(1) bundle satisfies the triple overlap condition and so neither is actually a bundle.\n\nA legitimate U(1) bundle is classified by its Chern class, which is an element of H(\"M\",Z). Identify this class with the first element in the above exact sequence. The next arrow doubles this Chern class, and so legitimate bundles will correspond to even elements in the second , while odd elements will correspond to bundles that fail the triple overlap condition. The obstruction then is classified by the failure of an element in the second H(\"M\",Z) to be in the image of the arrow, which, by exactness, is classified by its image in H(\"M\",Z) under the next arrow.\n\nTo cancel the corresponding obstruction in the \"spin\" bundle, this image needs to be \"w\". In particular, if \"w\" is not in the image of the arrow, then there does not exist any U(1) bundle with obstruction equal to \"w\" and so the obstruction cannot be cancelled. By exactness, \"w\" is in the image of the preceding arrow only if it is in the kernel of the next arrow, which we recall is the Bockstein homomorphism β. That is, the condition for the cancellation of the obstruction is\n\nwhere we have used the fact that the third integral Stiefel–Whitney class \"W\" is the Bockstein of the second Stiefel–Whitney class \"w\" (this can be taken as a definition of \"W\").\n\nThis argument also demonstrates that second Stiefel–Whitney class defines elements not only of Z cohomology but also of integral cohomology in one higher degree. In fact this is the case for all even Stiefel–Whitney classes. It is traditional to use an uppercase \"W\" for the resulting classes in odd degree, which are called the integral Stiefel–Whitney classes, and are labeled by their degree (which is always odd).\n\n\nIn particle physics the spin–statistics theorem implies that the wavefunction of an uncharged fermion is a section of the associated vector bundle to the \"spin\" lift of an SO(\"N\") bundle \"E\". Therefore, the choice of spin structure is part of the data needed to define the wavefunction, and one often needs to sum over these choices in the partition function. In many physical theories \"E\" is the tangent bundle, but for the fermions on the worldvolumes of D-branes in string theory it is a normal bundle.\n\nIn quantum field theory charged spinors are sections of associated \"spin\" bundles, and in particular no charged spinors can exist on a space that is not \"spin\". An exception arises in some supergravity theories where additional interactions imply that other fields may cancel the third Stiefel–Whitney class. The mathematical description of spinors in supergravity and string theory is a particularly subtle open problem, which was recently addressed in references. It turns out that the standard notion of spin structure is too restrictive for applications to supergravity and string theory, and that the correct notion of spinorial structure for the mathematical formulation of these theories is a \"Lipschitz structure\".\n\n\n\n"}
{"id": "57653446", "url": "https://en.wikipedia.org/wiki?curid=57653446", "title": "Stochastic homogenization", "text": "Stochastic homogenization\n\nIn homogenization theory, a branch of mathematics, stochastic homogenization is a technique for understanding solutions to partial differential equations with oscillatory random coefficients.\n"}
{"id": "1286452", "url": "https://en.wikipedia.org/wiki?curid=1286452", "title": "Subgame", "text": "Subgame\n\nIn game theory, a subgame is any part (a subset) of a game that meets the following criteria (the following terms allude to a game described in extensive form):\n\n\nIt is a notion used in the solution concept of subgame perfect Nash equilibrium, a refinement of the Nash equilibrium that eliminates non-credible threats.\n\nThe key feature of a subgame is that it, when seen in isolation, constitutes a game in its own right. When the initial node of a subgame is reached in a larger game, players can concentrate only on that subgame; they can ignore the history of the rest of the game (provided they know what subgame they are playing). This is the intuition behind the definition given above of a subgame. It must contain an initial node that is a singleton information set since this is a requirement of a game. Otherwise, it would be unclear where the player with first move should start at the beginning of a game (but see nature's choice). Even if it is clear in the context of the larger game which node of a non-singleton information set has been reached, players could not ignore the history of the larger game once they reached the initial node of a subgame if subgames cut across information sets. Furthermore, a subgame can be treated as a game in its own right, but it must reflect the strategies available to players in the larger game of which it is a subset. This is the reasoning behind 2 and 3 of the definition. All the strategies (or subsets of strategies) available to a player at a node in a game must be available to that player in the subgame the initial node of which is that node.\n\nOne of the principal uses of the notion of a subgame is in the solution concept subgame perfection, which stipulates that an equilibrium strategy profile be a Nash equilibrium in \"every subgame\".\n\nIn a Nash equilibrium, there is some sense in which the outcome is optimal - every player is playing a best response to the other players. However, in some dynamic games this can yield implausible equilibria. Consider a two-player game in which player 1 has a strategy S to which player 2 can play B as a best response. Suppose also that S is a best response to B. Hence, {S,B} is a Nash equilibrium. Let there be another Nash equilibrium {S',B'}, the outcome of which player 1 prefers and B' is the only best response to S'. In a dynamic game, the first Nash equilibrium is implausible (if player 1 moves first) because player 1 will play S', forcing the response (say) B' from player 2 and thereby attaining the second equilibrium (regardless of the preferences of player 2 over the equilibria). The first equilibrium is subgame imperfect because B does not constitute a best response to S' once S' has been played, i.e. in the subgame reached by player 1 playing S', B is not optimal for player 2.\n\nIf not all strategies at a particular node were available in a subgame containing that node, it would be unhelpful in subgame perfection. One could trivially call an equilibrium subgame perfect by ignoring playable strategies to which a strategy was not a best response. Furthermore, if subgames cut across information sets, then a Nash equilibrium in a subgame might suppose a player had information in that subgame, he did not have in the larger game.\n"}
{"id": "25944272", "url": "https://en.wikipedia.org/wiki?curid=25944272", "title": "Table of costs of operations in elliptic curves", "text": "Table of costs of operations in elliptic curves\n\nElliptic curve cryptography is a popular form of public key encryption that is based on the mathematical theory of elliptic curves. Points on an elliptic curve can be added and form a group under this addition operation. This article describes the computational costs for this group addition and certain related operations that are used in elliptic curve cryptography algorithms.\n\nThe next section presents a table of all the time-costs of some of the possible operations in elliptic curves. The columns of the table are labelled by various computational operations. The rows of the table are for different models of elliptic curves. These are the operations considered :\n<poem>\nDBL - Doubling\nADD - Addition\nmADD - Mixed addition: addition of an input that has been scaled to have \"Z\"-coordinate 1.\nmDBL - Mixed doubling: doubling of an input that has been scaled to have \"Z\" coordinate 1.\nTPL - Tripling.\nDBL+ADD - Combined double and add step\n</poem>\n\nTo see how adding (ADD) and doubling (DBL) points on elliptic curves are defined, see The group law. The importance of doubling to speed scaler multiplication is discussed after the table. For information about other possible operations on elliptic curves see http://hyperelliptic.org/EFD/g1p/index.html.\n\nUnder different assumptions on the multiplication, addition, inversion for the elements in some fixed field, the time-cost of these operations varies.\nIn this table it is assumed that:\n\nThis means that 100 multiplications (M) are required to invert (I) an element; one multiplication is required to compute the square (S) of an element; no multiplication is needed to multiply an element by a parameter (*param), by a constant (*const), or to add two elements.\n\nFor more information about other results obtained with different assumptions, see http://hyperelliptic.org/EFD/g1p/index.html\n\nIn some applications of elliptic curve cryptography and the elliptic curve method of factorization (ECM) it is necessary to consider the scalar multiplication [\"n\"]\"P\". One way to do this is to compute successively:\n\nBut it is faster to use double-and-add method, e.g. [5]\"P\" = [2]([2]P) + \"P\".\nIn general to compute [\"k\"]\"P\", write\n\nformula_2\n\nwith \"k\" in {0,1} and formula_3, \"k\" = 1, then:\n\nformula_4.\n\nNote that, this simple algorithm takes at most \"2l\" steps and each step consists of a doubling and (if \"k\" ≠ 0) adding two points. So, this is one of the reasons why addition and doubling formulas are defined.\nFurthermore, this method is applicable to any group and if the group law is written multiplicatively, the double-and-add algorithm is instead called square-and-multiply algorithm.\n\n"}
{"id": "24885593", "url": "https://en.wikipedia.org/wiki?curid=24885593", "title": "Term (logic)", "text": "Term (logic)\n\nIn analogy to natural language, where a noun phrase refers to an object and a whole sentence refers to a fact, in mathematical logic, a term denotes a mathematical object and a formula denotes a mathematical fact. In particular, terms appear as components of a formula.\n\nA first-order term is recursively constructed from constant symbols, variables and function symbols.\nAn expression formed by applying a predicate symbol to an appropriate number of terms is called an atomic formula, which evaluates to true or false in bivalent logics, given an interpretation.\nFor example, is a term built from the constant 1, the variable , and the binary function symbols and ; it is part of the atomic formula which evaluates to true for each real-numbered value of .\n\nBesides in logic, terms play important roles in universal algebra, and rewriting systems.\n\nIn the context of polynomials, sometimes \"term\" is used for a monomial with a coefficient: to 'collect like terms' in a polynomial is the operation of making it a linear combination of distinct monomials. Terms, in this sense, are things that are added or subtracted.\nA series is often represented as the sum of a sequence of terms. \nIndividual factors in an expression representing a product are multiplicative terms. \nFor example, in , we have that , and are all terms.\n\nIn elementary mathematics, \n\nGiven a set \"V\" of variable symbols, a set \"C\" of constant symbols and sets \"F\" of \"n\"-ary function symbols, also called operator symbols, for each natural number \"n\" ≥ 1, the set of (unsorted first-order) terms \"T\" is recursively defined to be the smallest set with the following properties:\nUsing an intuitive, pseudo-grammatical notation, this is sometimes written as:\n\"t\" ::= \"x\" | \"c\" | \"f\"(\"t\", ..., \"t\").\nUsually, only the first few function symbol sets \"F\" are inhabited. Well-known examples are the unary function symbols \"sin\", \"cos\" ∈ \"F\", and the binary function symbols +, −, ⋅, / ∈ \"F\", while ternary operations are less known, let alone higher-arity functions. Many authors consider constant symbols as 0-ary function symbols \"F\", thus needing no special syntactic class for them.\n\nA term denotes a mathematical object from the domain of discourse. A constant \"c\" denotes a named object from that domain, a variable \"x\" ranges over the objects in that domain, and an \"n\"-ary function \"f\" maps \"n\"-tuples of objects to objects. For example, if \"n\" ∈ \"V\" is a variable symbol, 1 ∈ \"C\" is a constant symbol, and \"add\" ∈ \"F\" is a binary function symbol, then \"n\" ∈ \"T\", 1 ∈ \"T\", and (hence) \"add\"(\"n\", 1) ∈ \"T\" by the first, second, and third term building rule, respectively. The latter term is usually written as \"n\"+1, using infix notation and the more common operator symbol + for convenience.\n\nOriginally, logicians defined a term to be a \"character string\" adhering to certain building rules. However, since the concept of tree became popular in computer science, it turned out to be more convenient to think of a term as a tree. For example, several distinct character strings, like \", \", and \"formula_1\", denote the same term and correspond to the same tree, viz. the left tree in the above picture.\nSeparating the tree structure of a term from its graphical representation on paper, it is also easy to account for parentheses (being only representation, not structure) and invisible multiplication operators (existing only in structure, not in representation).\n\nTwo terms are said to be structurally, literally, or syntactically equal if they correspond to the same tree. For example, the left and the right tree in the above picture are structurally unequal terms, although they might be considered \"semantically equal\" as they always evaluate to the same value in rational arithmetic. While structural equality can be checked without any knowledge about the meaning of the symbols, semantic equality cannot. If the function / is e.g. interpreted not as rational but as truncating integer division, then at \"n\"=2 the left and right term evaluates to 3 and 2, respectively.\nStructural equal terms need to agree in their variable names.\n\nIn contrast, a term \"t\" is called a renaming, or a variant, of a term \"u\" if the latter resulted from consistently renaming all variables of the former, i.e. if \"u\" = \"tσ\" for some renaming substitution σ. In that case, \"u\" is a renaming of \"t\", too, since a renaming substitution σ has an inverse σ, and \"t\" = uσ. Both terms are then also said to be equal modulo renaming. In many contexts, the particular variable names in a term don't matter, e.g. the commutativity axiom for addition can be stated as \"x\"+\"y\"=\"y\"+\"x\" or as \"a\"+\"b\"=\"b\"+\"a\"; in such cases the whole formula may be renamed, while an arbitrary subterm usually may not, e.g. \"x\"+\"y\"=\"b\"+\"a\" is not a valid version of the commutativity axiom.\nThe set of variables of a term \"t\" is denoted by \"vars\"(\"t\").\nA term that doesn't contain any variables is called a ground term; a term that doesn't contain multiple occurrences of a variable is called a linear term.\nFor example, 2+2 is a ground term and hence also a linear term, \"x\"⋅(\"n\"+1) is a linear term, \"n\"⋅(\"n\"+1) is a non-linear term. These properties are important in, for example, term rewriting.\n\nGiven a signature for the function symbols, the set of all terms forms the free term algebra. The set of all ground terms forms the initial term algebra.\n\nAbbreviating the number of constants as \"f\", and the number of \"i\"-ary function symbols as \"f\", the number θ of distinct ground terms of a height up to \"h\" can be computed by the following recursion formula:\n\nGiven a set \"R\" of \"n\"-ary relation symbols for each natural number \"n\" ≥ 1, an (unsorted first-order) atomic formula is obtained by applying an \"n\"-ary relation symbol to \"n\" terms. As for function symbols, a relation symbol set \"R\" is usually non-empty only for small \"n\". In mathematical logic, more complex formulas are built from atomic formulas using logical connectives and quantifiers. For example, letting ℝ denote the set of real numbers, ∀\"x\": \"x\" ∈ ℝ ⇒ (\"x\"+1)⋅(\"x\"+1) ≥ 0 is a mathematical formula evaluating to true in the algebra of complex numbers.\nAn atomic formula is called ground if it is built entirely from ground terms; all ground atomic formulas composable from a given set of function and predicate symbols make up the Herbrand base for these symbol sets.\n\n\nWhen the domain of discourse contains elements of basically different kinds, it is useful to split the set of all terms accordingly. To this end, a sort (sometimes also called type) is assigned to each variable and each constant symbol, and a declaration of domain sorts and range sort to each function symbol. A sorted term \"f\"(\"t\"...,\"t\") may be composed from sorted subterms \"t\"...,\"t\" only if the th subterm's sort matches the declared th domain sort of \"f\". Such a term is also called well-sorted; any other term (i.e. obeying the unsorted rules only) is called ill-sorted.\n\nFor example, a vector space comes with an associated field of scalar numbers. Let \"W\" and \"N\" denote the sort of vectors and numbers, respectively, let \"V\" and \"V\" be the set of vector and number variables, respectively, and \"C\" and \"C\" the set of vector and number constants, respectively. Then e.g. formula_5 and , and the vector addition, the scalar multiplication, and the inner product is declared as , and , respectively. Assuming variable symbols formula_6 and , the term formula_7 is well-sorted, while formula_8 is not (since + doesn't accept a term of sort \"N\" as 2nd argument). In order to make formula_9 a well-sorted term, an additional declaration is required. Function symbols having several declarations are called overloaded.\n\nSee many-sorted logic for more information, including extensions of the many-sorted framework described here.\n\nMathematical notations as shown in the table do not fit into the scheme of a first-order term as defined above, as they all introduce an own local, or bound, variable that may not appear outside the notation's scope, e.g. formula_10 doesn't make sense. \nIn contrast, the other variables, referred to as free, behave like ordinary first-order term variables, e.g. formula_11 does make sense.\n\nAll these operators can be viewed as taking a function rather than a value term as one of their arguments. For example, the \"lim\" operator is applied to a sequence, i.e. to a mapping from positive integer to e.g. real numbers. As another example, a C function to implement the second example from the table, ∑, would have a function pointer argument (see box below).\n\nLambda terms can be used to denote anonymous functions to be supplied as arguments to \"lim\", ∑, ∫, etc.\n\nFor example, the function \"square\" from the C program below can be written anonymously as a lambda term λ\"i\". \"i\". The general sum operator ∑ can then be considered as a ternary function symbol taking a lower bound value, an upper bound value and a function to be summed-up. Due to its latter argument, the ∑ operator is called a second-order function symbol.\nAs another example, the lambda term λ\"n\". \"x\"/\"n\" denotes a function that maps 1, 2, 3, ... to \"x\"/1, \"x\"/2, \"x\"/3, ..., respectively, that is, it denotes the sequence (\"x\"/1, \"x\"/2, \"x\"/3, ...). The \"lim\" operator takes such a sequence and returns its limit (if defined).\n\nThe rightmost column of the table indicates how each mathematical notation example can be represented by a lambda term, also converting common infix operators into prefix form.\n\n"}
{"id": "7984007", "url": "https://en.wikipedia.org/wiki?curid=7984007", "title": "Traced monoidal category", "text": "Traced monoidal category\n\nIn category theory, a traced monoidal category is a category with some extra structure which gives a reasonable notion of feedback.\n\nA traced symmetric monoidal category is a symmetric monoidal category C together with a family of functions\ncalled a \"trace\", satisfying the following conditions (where we sometimes denote an identity morphism by the corresponding object, e.g., using \"U\" to denote formula_2):\n\n\n\n\n\n\n(where formula_20 is the symmetry of the monoidal category).\n\n"}
{"id": "59008046", "url": "https://en.wikipedia.org/wiki?curid=59008046", "title": "Ute Finckh-Krämer", "text": "Ute Finckh-Krämer\n\nUte Elisabeth Finckh-Krämer (born December 16, 1956 in Wiesbaden) is a German politician in (SPD) and pacifist. She was a board member of the Federal Social Defense League and served as one of its chairpersons from March 2005 to March 201. In September 2013, she moved to the Berlin Landesliste in the German Bundestag, which she served until October 2017.\n\nFinckh-Krämer graduated from the Altes Gymnasium (Bremen) in May 1974 and studied mathematics with a minor in physics at the University of Erlangen-Nuremberg. In the summer semester of 1977, she moved to the University of Tübingen, where she graduated in April 1981 with a degree in mathematics. In the year 1986, she obtained her doctoral degree in Tübingen with the dissertation\n\" Beiträge zur Wahrscheinlichkeitstheorie auf einer Kingman-Struktur \" under Herbert Heyer. She first worked as a lecturer in adult and further education in 1987 then worked in the German Central Register for Child Hearing Disorders from 1994 to 2000. She has been a consultant in the Press and Information Office of the Federal Government since September 2001.\n\nFinckh-Krämer has been active since her student days, having participated in, among other things, the blockade in Großengstingen in the summer of 1982 and participated in a blockade in Mutlangen in the summer of 1984. In 1989, she co-founded Minden / Westphalia with the Bund für Soziale Defense (BSV). She is active for the BSV in the Civil Conflict Transformation Platform.\n\nFinckh-Krämer joined the SPD at the age of 16. From 1996 to 2002 she was a departmental cashier, then from 2002 to 2008 she was district cashier in the SPD district of Steglitz-Zehlendorf (Berlin) and from 2000 to 2002 a member of the board of directors of this circle as well as in the year 2009 campaign officer of the same circle.\n\nIn the 2013 Bundestag elections, Finckh-Krämer joined the SPD as a direct candidate in the Constituency Steglitz-Zehlendorf. \n\nFinckh-Krämer was the head of the SPD in the Subcommittee on Civilian Crisis Prevention, Conflict Transformation and Network Action and Deputy Chair of the Subcommittee on Disarmament, Arms Control and Non-Proliferation. In addition, she was a full member of the and the . She served as secretary in the Bundestag presidency and was a member of the [Parliamentary Assembly of the Council of Europe | Parliamentary Assembly of the Council of Europe] in January 2018 until the German delegation was replaced.\n\nIn the federal election 2017, she was again unable to prevail in her constituency with 24.6% of the first votes and was defeated by the CDU candidate Thomas Heilmann (35.4%). so she left the Bundestag.\n\nFinckh-Krämer is married and has two adult sons. She has lived in the Berlin district Steglitz-Zehlendorf (until 2001 Steglitz) since 1992. She is the eldest daughter of Ulrich Finckh. Finckh-Krämer is Protestant and is a member of the Lukas parish Steglitz.\n\n\n"}
