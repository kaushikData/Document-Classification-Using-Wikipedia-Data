{"id": "399138", "url": "https://en.wikipedia.org/wiki?curid=399138", "title": "53 (number)", "text": "53 (number)\n\n53 (fifty-three) is the natural number following 52 and preceding 54. It is the 16th prime number.\n\n\n\n\nFifty-three is:\n\n"}
{"id": "5165589", "url": "https://en.wikipedia.org/wiki?curid=5165589", "title": "ADHM construction", "text": "ADHM construction\n\nIn mathematical physics, the ADHM construction or monad construction is the construction of all instantons using methods of linear algebra by Michael Atiyah, Vladimir Drinfeld, Nigel Hitchin, Yuri I. Manin in their paper \"Construction of Instantons.\"\n\nThe ADHM construction uses the following data:\n\n\nThen the ADHM construction claims that, given certain regularity conditions,\n\nIn a noncommutative gauge theory, the ADHM construction is identical but the moment map formula_4 is set equal to the self-dual projection of the noncommutativity matrix of the spacetime times the identity matrix. In this case instantons exist even when the gauge group is U(1). The noncommutative instantons were discovered by Nikita Nekrasov and Albert Schwarz in 1998.\n\nSetting \"B\" and \"J\" to zero, one obtains the classical moduli space of nonabelian vortices in a supersymmetric gauge theory with an equal number of colors and flavors, as was demonstrated in Vortices, instantons and branes. The generalization to greater numbers of flavors appeared in Solitons in the Higgs phase: The Moduli matrix approach. In both cases the Fayet-Iliopoulos term, which determines a squark condensate, plays the role of the noncommutativity parameter in the real moment map.\n\nLet \"x\" be the 4-dimensional Euclidean spacetime coordinates written in quaternionic notation\nformula_5\n\nConsider the 2\"k\" × (\"N\" + 2\"k\") matrix\n\nThen the conditions formula_7 are equivalent to the factorization condition \n\nThen a hermitian projection operator \"P\" can be constructed as \n\nThe nullspace of Δ(\"x\") is of dimension \"N\" for generic \"x\". The basis vectors for this null-space can be assembled into an (\"N\" + 2\"k\") × \"N\" matrix \"U\"(\"x\") with orthonormalization condition \"U\"\"U\" = 1.\n\nA regularity condition on the rank of Δ guarantees the completeness condition \n\nThe anti-selfdual connection is then constructed from \"U\" by the formula \n\n\n"}
{"id": "2291355", "url": "https://en.wikipedia.org/wiki?curid=2291355", "title": "Alexander S. Kechris", "text": "Alexander S. Kechris\n\nAlexander Sotirios Kechris (; born March 23, 1946) is a set theorist and logician at California Institute of Technology. \n\nKechris has made contributions to the theory of Borel equivalence relations and the theory of automorphism groups of uncountable structures. His research interests cover foundations of mathematics, mathematical logic and set theory and their interactions with analysis and dynamical systems. \n\nKechris earned his Ph.D. in 1972 under the direction of Yiannis N. Moschovakis, with a dissertation titled \"Projective Ordinals and Countable Analytic Sets\". During his academic career he advised 23 PhD and sponsored 20 postdoc students.\n\nIn 2012 he became an Inaugural Fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "11092492", "url": "https://en.wikipedia.org/wiki?curid=11092492", "title": "Amalgamation property", "text": "Amalgamation property\n\nIn the mathematical field of model theory, the amalgamation property is a property of collections of structures that guarantees, under certain conditions, that two structures in the collection can be regarded as substructures of a larger one.\n\nThis property plays a crucial role in Fraïssé's theorem, which characterises classes of finite structures that arise as\nages of countable homogeneous structures.\n\nThe diagram of the amalgamation property appears in many areas of mathematical logic. Examples include in modal logic as an incestual accessibility relation, and in lambda calculus as a manner of reduction having the Church–Rosser property.\n\nAn \"amalgam\" can be formally defined as a 5-tuple (\"A,f,B,g,C\") such that \"A,B,C\" are structures having the same signature, and \"f: A\" → \"B, g\": \"A\" → \"C\" are injective morphisms that are referred to as \"embeddings\".\n\nA class \"K\" of structures has the amalgamation property if for every amalgam with \"A,B,C\" ∈ \"K\" and \"A\" ≠ Ø, there exist both a structure \"D\" ∈ \"K\" and embeddings \"f':\" \"B\" → \"D, g':\" \"C\" → \"D\" such that\n\n\nA similar but different notion to the amalgamation property is the joint embedding property. To see the difference, first consider the class \"K\" (or simply the set) containing three models with linear orders, \"L\" of size one, \"L\" of size two, and \"L\" of size three. This class \"K\" has the joint embedding property because all three models can be embedded into \"L\". However, \"K\" does not have the amalgamation property. The counterexample for this starts with \"L\" containing a single element \"e\" and extends in two different ways to \"L\", one in which \"e\" is the smallest and the other in which \"e\" is the largest. Now any common model with an embedding from these two extensions must be at least of size five so that there are two elements on either side of \"e\".\n\nNow consider the class of algebraically closed fields. This class has the amalgamation property since any two field extensions of a prime field can be embedded into a common field. However, two arbitrary fields cannot be embedded into a common field when the characteristic of the fields differ.\n\nA class \"K\" of structures has the \"strong amalgamation property\" (SAP), also called the \"disjoint amalgamation property\" (DAP), if for every amalgam with \"A,B,C\" ∈ \"K\" there exist both a structure \"D\" ∈ \"K\" and embeddings \"f<nowiki>':</nowiki>\" \"B\" → \"D, g': C\" → \"D\" such that\n\n\n"}
{"id": "19285667", "url": "https://en.wikipedia.org/wiki?curid=19285667", "title": "Auxiliary particle filter", "text": "Auxiliary particle filter\n\nThe auxiliary particle filter is a particle filtering algorithm introduced by Pitt and Shephard in 1999 to improve some deficiencies of the sequential importance resampling (SIR) algorithm when dealing with tailed observation densities.\n\nAssume that the filtered posterior is described by the following \"M\" weighted samples:\n\nThen, each step in the algorithm consists of first drawing a sample of the particle index formula_2 which will be propagated from formula_3 into the new step formula_4. These indexes are auxiliary variables only used as an intermediary step, hence the name of the algorithm. The indexes are drawn according to the likelihood of some reference point formula_5 which in some way is related to the transition model formula_6 (for example, the mean, a sample, etc.):\n\nThis is repeated for formula_8, and using these indexes we can now draw the conditional samples:\n\nFinally, the weights are updated to account for the mismatch between the likelihood at the actual sample and the predicted point formula_10:\n"}
{"id": "35215473", "url": "https://en.wikipedia.org/wiki?curid=35215473", "title": "Bracket ring", "text": "Bracket ring\n\nIn mathematics, the bracket ring is the subring of the ring of polynomials \"k\"[\"x\"...,\"x\"] generated by the \"d\" by \"d\" minors of a generic \"d\" by \"n\" matrix (\"x\").\n\nThe bracket ring may be regarded as the ring of polynomials on the image of a Grassmannian under the Plücker embedding.\n\nFor given \"d\" ≤ \"n\" we define as formal variables the \"brackets\" [λ λ ... λ] with the λ taken from {1...,\"n\"}, subject to [λ λ ... λ] = − [λ λ ... λ] and similarly for other transpositions. The set Λ(\"n\",\"d\") of size formula_1 generates a polynomial ring \"K\"[Λ(\"n\",\"d\")] over a field \"K\". There is a homomorphism Φ(\"n\",\"d\") from \"K\"[Λ(\"n\",\"d\")] to the polynomial ring \"K\"[\"x\"] in \"nd\" indeterminates given by mapping \n[λ λ ... λ] to the determinant of the \"d\" by \"d\" matrix consisting of the columns of the \"x\" indexed by the λ. The \"bracket ring\" \"B\"(\"n\",\"d\") is the image of Φ. The kernel \"I\"(\"n\",\"d\") of Φ encodes the relations or \"syzygies\" that exist between the minors of a generic \"n\" by \"d\" matrix. The projective variety defined by the ideal \"I\" is the (\"n\"−\"d\")\"d\" dimensional Grassmann variety whose points correspond to \"d\"-dimensional subspaces of an \"n\"-dimensional space.\n\nTo compute with brackets it is necessary to determine when an expression lies in the ideal \"I\"(\"n\",\"d\"). This is achieved by a \"straightening law\" due to Young (1928).\n\n\n"}
{"id": "575641", "url": "https://en.wikipedia.org/wiki?curid=575641", "title": "Casting out nines", "text": "Casting out nines\n\nThe expression \"casting out nines\" may refer to any one of three arithmetical procedures:\n\nTo \"cast out nines\" from a single number, its decimal digits can be simply added together to obtain its so-called digit sum. The digit sum of 2946, for example is 2 + 9 + 4 + 6 = 21. Since 21 = 2946 − 325 × 9, the effect of taking the digit sum of 2946 is to \"cast out\" 325 lots of 9 from it. If the digit 9 is ignored when summing the digits, the effect is to \"cast out\" one more 9 to give the result 12.\n\nMore generally, when casting out nines by summing digits, any set of digits which add up to 9, or a multiple of 9, can be ignored. In the number 3264, for example, the digits 3 and 6 sum to 9. Ignoring these two digits, therefore, and summing the other two, we get 2 + 4 = 6. Since 6 = 3264 − 362 × 9, this computation has resulted in casting out 362 lots of 9 from 3264.\n\nFor an arbitrary number, formula_1, normally represented by the sequence of decimal digits, formula_2, the digit sum is formula_3. The difference between the original number and its digit sum is \n\nBecause numbers of the form formula_5 are always divisible by 9 (since formula_6), replacing the original number by its digit sum has the effect of casting out \n\nlots of 9.\n\nIf the procedure described in the preceding paragraph is repeatedly applied to the result of each previous application, the eventual result will be a single-digit number from which \"all\" 9s, with the possible exception of one, have been \"cast out\". The resulting single-digit number is called the \"digital root\" of the original. The exception occurs when the original number has a digital root of 9, whose digit sum is itself, and therefore will not be cast out by taking further digit sums.\n\nThe number 12565, for instance, has digit sum 1+2+5+6+5 = 19, which, in turn, has digit sum 1+9=10, which, in its turn has digit sum 1+0=1, a single-digit number. The digital root of 12565 is therefore 1, and its computation has the effect of casting out (12565 - 1)/9 = 1396 lots of 9 from 12565.\n\nTo check the result of an arithmetical calculation by casting out nines, each number in the calculation is replaced by its digital root and the same calculations applied to these digital roots. The digital root of the result of this calculation is then compared with that of the result of the original calculation. If no mistake has been made in the calculations, these two digital roots must be the same. Examples in which casting-out-nines has been used to check addition, subtraction, multiplication, and division are given below.\n\nIn each addend, cross out all 9s and pairs of digits that total 9, then add together what remains. These new values are called \"excesses\". Add up leftover digits for each addend until one digit is reached. Now process the sum and also the excesses to get a \"final\" excess.\n8 times 8 is 64; 6 and 4 are 10; 1 and 0 are 1.\n\nThe method works because the original numbers are 'decimal' (base 10), the modulus is chosen to differ by 1, and casting out is equivalent to taking a digit sum. In general any two 'large' integers, \"x\" and \"y\", expressed in any smaller \"modulus\" as \"x\"' and \"y' \" (for example, modulo 7) will always have the same sum, difference or product as their originals. This property is also preserved for the 'digit sum' where the base and the modulus differ by 1.\n\nIf a calculation was correct before casting out, casting out on both sides will preserve correctness. However, it is possible that two previously unequal integers will be identical modulo 9 (on average, a ninth of the time).\n\nThe operation does not work on fractions, since a given fractional number does not have a unique representation.\n\nA nice trick for very young children to learn to add nine is to add ten to the digit and to count back one. Since we are adding 1 to the ten's digit and subtracting one from the unit's digit, the sum of the digits should remain the same. For example, 9 + 2 = 11 with 1 + 1 = 2. When adding 9 to itself, we would thus expect the sum of the digits to be 9 as follows: 9 + 9 = 18, (1 + 8 = 9) and 9 + 9 + 9 = 27, (2 + 7 = 9). Let us look at a simple multiplication: 5×7 = 35, (3 + 5 = 8). Now consider (7 + 9)×5 = 16×5 = 80, (8 + 0 = 8) or 7×(9 + 5) = 7×14 = 98, (9 + 8 = 17, (1 + 7 = 8). \n\nAny non-negative integer can be written as 9×n + a, where 'a' is a single digit from 0 to 8, and 'n' is some non-negative integer.\nThus, using the distributive rule, (9×n + a)×(9×m + b)= 9×9×n×m + 9(am + bn) + ab. Since the first two factors are multiplied by 9, their sums will end up being 9 or 0, leaving us with 'ab'. In our example, 'a' was 7 and 'b' was 5. We would expect that in any base system, the number before that base would behave just like the nine.\n\nWhile extremely useful, casting out nines does not catch all errors made while doing calculations. For example, the casting-out-nines method would not recognize the error in a calculation of 5 × 7 which produced any of the erroneous results 8, 17, 26, etc. (that is, any result congruent to 8 modulo 9). In other words, the method only catches erroneous results whose digital root is one of the 8 digits that is different from that of the correct result.\n\nA form of casting out nines known to ancient Greek mathematicians was described by the Roman bishop Hippolytus (170–235) in \"The Refutation of all Heresies\", and more briefly by the Syrian Neoplatonist philosopher Iamblichus (c.245–c.325) in his commentary on the \"Introduction to Arithmetic\" of Nicomachus of Gerasa. Both Hippolytus's and Iamblichus's descriptions, though, were limited to an explanation of how repeated digital sums of Greek numerals were used to compute a unique \"root\" between 1 and 9. Neither of them displayed any awareness of how the procedure could be used to check the results of arithmetical computations.\n\nThe earliest known surviving work which describes how casting out nines can be used to check the results of arithmetical computations is the \"Mahâsiddhânta\", written around 950 by the Indian mathematician and astronomer, Aryabhata II (c.920–c.1000 ).\nWriting about 1020, the Persian polymath, Ibn Sina (Avicenna) (c.980–1037), also gave full details of what he called the \"Hindu method\" of checking arithmetical calculations by casting out nines.\n\nIn \"Synergetics\", R. Buckminster Fuller claims to have used casting-out-nines \"before World War I.\" Fuller explains how to cast out nines and makes other claims about the resulting 'indigs,' but he fails to note that casting out nines can result in false positives.\n\nThe method bears striking resemblance to standard signal processing and computational error detection and error correction methods, typically using similar modular arithmetic in checksums and simpler check digits.\n\nThis method can be generalized to determine the remainders of division by certain prime numbers.\n\nSince 3·3 = 9,\nSo we can use the remainder from casting out nines to get the remainder of division by three.\n\nCasting out ninety nines is done by adding groups of two digits instead just one digit.\n\nSince 11·9 = 99,\nSo we can use the remainder from casting out ninety nines to get the remainder of division by eleven. This is called casting out elevens.\n\nCasting out nine hundred ninety nines is done by adding groups of three digits.\n\nSince 37·27 = 999,\nSo we can use the remainder from casting out nine hundred ninety nines to get the remainder of division by thirty seven.\n\n\n"}
{"id": "14241236", "url": "https://en.wikipedia.org/wiki?curid=14241236", "title": "Cluster state", "text": "Cluster state\n\nIn quantum information and quantum computing, a cluster state is a type of highly entangled state of multiple qubits. Cluster states are generated in lattices of qubits with Ising type interactions. A cluster \"C\" is a connected subset of a d-dimensional lattice, and a cluster state is a pure state of the qubits located on \"C\". They are different from other types of entangled states such as GHZ states or W states in that it is more difficult to eliminate quantum entanglement (via projective measurements) in the case of cluster states. Another way of thinking of cluster states is as a particular instance of graph states, where the underlying graph is a connected subset of a d-dimensional lattice. Cluster states are especially useful in the context of the one-way quantum computer. For a comprehensible introduction to the topic see.\n\nFormally, cluster states formula_1 are states which obey the set eigenvalue equations:\n\nwhere formula_3 are the correlation operators\n\nwith formula_5 and formula_6 being Pauli matrices, formula_7 denoting the neighbourhood of formula_8 and formula_9 being a set of binary parameters specifying the particular instance of a cluster state.\n\nCluster states have been realized experimentally. They have been obtained in photonic experiments using \nparametric downconversion \n. They have been created also in optical lattices of\ncold atoms\n\n"}
{"id": "23251647", "url": "https://en.wikipedia.org/wiki?curid=23251647", "title": "Continuity set", "text": "Continuity set\n\nIn measure theory, a continuity set of a measure \"μ\" is any Borel set \"B\" such that \n\nwhere formula_2 is the boundary set of \"B\". For signed measures, one asks that\n\nThe class of all continuity sets for given measure \"μ\" forms a ring.\n\nSimilarly, for a random variable \"X\" a set \"B\" is called continuity set if\nThe continuity set \"C\"(\"f\") of a function \"f\" is the set of points where \"f\" is continuous.\n"}
{"id": "19199143", "url": "https://en.wikipedia.org/wiki?curid=19199143", "title": "Coset leader", "text": "Coset leader\n\nIn coding theory, a coset leader is a word of minimum weight in any particular coset - that is, a word with the lowest amount of non-zero entries. Sometimes there are several words of equal minimum weight in a coset, and in that case, any one of those words may be chosen to be the coset leader.\n\nCoset leaders are used in the construction of a standard array for a linear code, which can then be used to decode received vectors. For a received vector \"y\", the decoded message is \"y - e\", where \"e\" is the coset leader of \"y\". Coset leaders can also be used to construct a fast decoding strategy. For each coset leader \"u\" we calculate the syndrome \"uH′\". When we receive \"v\" we evaluate \"vH′\" and find the matching syndrome. The corresponding coset leader is the most likely error pattern and we assume that \"v\"+\"u\" was the codeword sent.\n"}
{"id": "17346860", "url": "https://en.wikipedia.org/wiki?curid=17346860", "title": "Cotangent complex", "text": "Cotangent complex\n\nIn mathematics the cotangent complex is roughly a universal linearization of a morphism of geometric or algebraic objects. Cotangent complexes were originally defined in special cases by a number of authors. Luc Illusie, Daniel Quillen, and M. André independently came up with a definition that works in all cases.\n\nSuppose that \"X\" and \"Y\" are algebraic varieties and that is a morphism between them. The cotangent complex of \"f\" is a more universal version of the relative Kähler differentials Ω. The most basic motivation for such an object is the exact sequence of Kähler differentials associated to two morphisms. If \"Z\" is another variety, and if is another morphism, then there is an exact sequence\nIn some sense, therefore, relative Kähler differentials are a right exact functor. (Literally this is not true, however, because the category of algebraic varieties is not an abelian category, and therefore right-exactness is not defined.) In fact, prior to the definition of the cotangent complex, there were several definitions of functors that might extend the sequence further to the left, such as the Lichtenbaum–Schlessinger functors \"T\" and imperfection modules. Most of these were motivated by deformation theory.\n\nThis sequence is exact on the left if the morphism \"f\" is smooth. If Ω admitted a first derived functor, then exactness on the left would imply that the connecting homomorphism vanished, and this would certainly be true if the first derived functor of \"f\", whatever it was, vanished. Therefore, a reasonable speculation is that the first derived functor of a smooth morphism vanishes. Furthermore, when any of the functors which extended the sequence of Kähler differentials were applied to a smooth morphism, they too vanished, which suggested that the cotangent complex of a smooth morphism might be equivalent to the Kähler differentials.\n\nAnother natural exact sequence related to Kähler differentials is the conormal exact sequence. If \"f\" is a closed immersion with ideal sheaf \"I\", then there is an exact sequence\nThis is an extension of the exact sequence above: There is a new term on the left, the conormal sheaf of \"f\", and the relative differentials Ω have vanished because a closed immersion is formally unramified. If \"f\" is the inclusion of a smooth subvariety, then this sequence is a short exact sequence. This suggests that the cotangent complex of the inclusion of a smooth variety is equivalent to the conormal sheaf shifted by one term.\n\nThe cotangent complex dates back at least to SGA 6 VIII 2, where Pierre Berthelot gave a definition when \"f\" is a \"smoothable\" morphism, meaning there is a scheme \"V\" and morphisms and such that , \"i\" is a closed immersion, and \"h\" is a smooth morphism. (For example, all projective morphisms are smoothable, since \"V\" can be taken to be a projective bundle over \"Y\".) In this case, he defines the cotangent complex of \"f\" as an object in the derived category of coherent sheaves \"X\" as follows:\nBerthelot proves that this definition is independent of the choice of \"V\" and that for a smoothable complete intersection morphism, this complex is perfect. Furthermore, he proves that if is another smoothable complete intersection morphism and if an additional technical condition is satisfied, then there is an exact triangle\n\nThe correct definition of the cotangent complex begins in the homotopical setting. Quillen and André worked with the simplicial commutative rings, while Illusie worked with simplicial ringed topoi. For simplicity, we will consider only the case of simplicial commutative rings. Suppose that \"A\" and \"B\" are simplicial rings and that \"B\" is an \"A\"-algebra. Choose a resolution of \"B\" by simplicial free \"A\"-algebras. Applying the Kähler differential functor to \"P\" produces a simplicial \"B\"-module. The total complex of this simplicial object is the cotangent complex \"L\". The morphism \"r\" induces a morphism from the cotangent complex to Ω called the augmentation map. In the homotopy category of simplicial \"A\"-algebras (or of simplicial ringed topoi), this construction amounts to taking the left derived functor of the Kähler differential functor.\n\nGiven a commutative square as follows:\nthere is a morphism of cotangent complexes which respects the augmentation maps. This map is constructed by choosing a free simplicial \"C\"-algebra resolution of \"D\", say . Because \"P\" is a free object, the composite \"hr\" can be lifted to a morphism . Applying functoriality of Kähler differentials to this morphism gives the required morphism of cotangent complexes. In particular, given homomorphisms , this produces the sequence\nThere is a connecting homomorphism formula_11 which turns this sequence into an exact triangle.\n\nThe cotangent complex can also be defined in any combinatorial model category \"M\". Suppose that formula_12 is a morphism in \"M\". The cotangent complex formula_13 (or formula_14) is an object in the category of spectra in formula_15. A pair of composable morphisms formula_16 induces an exact triangle in the homotopy category, formula_17.\n\nSuppose that \"B\" and \"C\" are \"A\"-algebras such that for all . Then there are quasi-isomorphisms\nIf \"C\" is a flat \"A\"-algebra, then the condition that vanishes for is automatic. The first formula then proves that the construction of the cotangent complex is local on the base in the flat topology.\n\nLet . Then:\n\nFor example, the cotangent complex of the twisted cubic formula_26 in formula_47 is given by the complex\n\n\n"}
{"id": "8709138", "url": "https://en.wikipedia.org/wiki?curid=8709138", "title": "Daniel McCartney", "text": "Daniel McCartney\n\nDaniel McCartney (September 10, 1817 – November 15, 1887) was an American who had what is now known as HSAM--Highly Superior Autobiographical Memory, formerly Hyperthymesia.\n\nMcCartney was born in Westmoreland County, Pennsylvania. He was legally blind and lived with relatives throughout his lifetime. For a large part of his life, he resided in Morrow County, Ohio, before his final days in Muscatine, Iowa. He never married.\n\nMcCartney was famous for his mental ability in two specific areas. First, he remembered every single day in his life from the age of nine until his death. Given any specific calendar date, McCartney in seconds could give the day of the week, describe the weather conditions, describe what he did during the day, describe what he ate during the day, and could provide details of local, regional and national events on that day. In a similar case as McCartney's, noted neurobiologist James McGaugh of the University of California, Irvine, one of the world's leading experts on human memory, reports on a woman, Jill Price, with the astonishing ability to clearly remember events that happened to her decades ago. McGaugh labels this one-of-a-kind ability as Hyperthymesia (National Public Radio, 2006). McCartney's mental aptitude appears to be nearly identical to this recent case reported in McGaugh's study. McCartney, however, had an additional mental ability: mathematical computation.\n\nMcCartney could mentally compute difficult mathematical computations in seconds, and extremely difficult ones in minutes. McCartney was tested several times by panels of university mathematicians in which he was given a battery of mathematical questions. On one such examination in July 1870 in Salem, Ohio, McCartney was asked to take 89 to the sixth power, which he mentally computed in ten minutes, giving the correct answer of 496,981,290,961. On another examination he was asked to provide the cube root of 4,741,632 for which he answered correctly in three minutes-(168); and 389,017 for which he answered correctly in fifteen seconds-(73).\n\nOn occasion, special sessions for the general public were held to witness mental examinations of McCartney's unique abilities. During these public exhibitions, he was always correct in his responses, and would provide a response in a matter of seconds to the amazement of audiences.\n\nHe died at the age of 70 in Wilton, Iowa. McCartney was reported to be one of a number of great mental calculators (Henkle, 1871).\n\n"}
{"id": "12255960", "url": "https://en.wikipedia.org/wiki?curid=12255960", "title": "Differential coefficient", "text": "Differential coefficient\n\nIn physics, the differential coefficient of a function \"f\"(\"x\") is what is now called its derivative \"df\"(\"x\")/\"dx\", the (not necessarily constant) multiplicative factor or \"coefficient\" of the differential \"dx\" in the differential \"df\"(\"x\"). \n\nA \"coefficient\" is usually a constant quantity, but the \"differential coefficient\" of \"f\" is a \"constant function\" only if \"f\" is a linear function. When \"f\" is \"not\" linear, its differential coefficient is a function, call it \"f\"′, \"derived\" by the differentiation of \"f\", hence, the modern term, derivative.\n\nThe older usage is now rarely seen. \n\nEarly editions of Silvanus P. Thompson's \"Calculus Made Easy\" use the older term. In his 1998 update of this text, Martin Gardner lets the first use of \"differential coefficient\" stand, along with Thompson's criticism of the term as a needlessly obscure phrase that should not intimidate students, and substitutes \"derivative\" for the remainder of the book.\n"}
{"id": "407249", "url": "https://en.wikipedia.org/wiki?curid=407249", "title": "Divisor function", "text": "Divisor function\n\nIn mathematics, and specifically in number theory, a divisor function is an arithmetic function related to the divisors of an integer. When referred to as \"the\" divisor function, it counts the \"number of divisors of an integer\" (including 1 and the number itself). It appears in a number of remarkable identities, including relationships on the Riemann zeta function and the Eisenstein series of modular forms. Divisor functions were studied by Ramanujan, who gave a number of important congruences and identities; these are treated separately in the article Ramanujan's sum.\n\nA related function is the divisor summatory function, which, as the name implies, is a sum over the divisor function.\n\nThe sum of positive divisors function σ(\"n\"), for a real or complex number \"x\", is defined as the sum of the \"x\"th powers of the positive divisors of \"n\". It can be expressed in sigma notation as\n\nwhere formula_2 is shorthand for \"\"d\" divides \"n\"\".\nThe notations \"d\"(\"n\"), ν(\"n\") and τ(\"n\") (for the German \"Teiler\" = divisors) are also used to denote σ(\"n\"), or the number-of-divisors function (). When \"x\" is 1, the function is called the sigma function or sum-of-divisors function, and the subscript is often omitted, so σ(\"n\") is the same as σ(\"n\") ().\n\nThe aliquot sum \"s\"(\"n\") of \"n\" is the sum of the proper divisors (that is, the divisors excluding \"n\" itself, ), and equals σ(\"n\") − \"n\"; the aliquot sequence of \"n\" is formed by repeatedly applying the aliquot sum function.\n\nFor example, σ(12) is the number of the divisors of 12:\n\nwhile σ(12) is the sum of all the divisors:\n\nand the aliquot sum s(12) of proper divisors is:\n\nThe cases \"x\" = 2 to 5 are listed in − , \"x\" = 6 to 24 are listed in − .\n\nFor a prime number \"p\",\n\nbecause by definition, the factors of a prime number are 1 and itself. Also, where \"p\"# denotes the primorial,\n\nsince \"n\" prime factors allow a sequence of binary selection (formula_8 or 1) from \"n\" terms for each proper divisor formed.\n\nClearly, formula_9 and σ(\"n\") > \"n\" for all \"n\" > 2.\n\nThe divisor function is multiplicative, but not completely multiplicative:\n\nThe consequence of this is that, if we write\n\nwhere \"r\" = \"ω\"(\"n\") is the number of distinct prime factors of \"n\", \"p\" is the \"i\"th prime factor, and \"a\" is the maximum power of \"p\" by which \"n\" is divisible, then we have: \n\nwhich is equivalent to the useful formula: \n\nIt follows (by setting \"x\" = 0) that \"d\"(\"n\") is: \n\nFor example, if \"n\" is 24, there are two prime factors (\"p\" is 2; \"p\" is 3); noting that 24 is the product of 2×3, \"a\" is 3 and \"a\" is 1. Thus we can calculate formula_15 as so:\n\nThe eight divisors counted by this formula are 1, 2, 4, 8, 3, 6, 12, and 24.\n\nEuler proved the remarkable recurrence:\n\nwhere we set formula_18 for formula_19,\nwe use the Kronecker delta formula_20, and formula_21 are the pentagonal numbers. Indeed, Euler proved this by logarithmic differentiation of the identity in his Pentagonal number theorem.\n\nFor a non-square integer, \"n\", every divisor, \"d\", of \"n\" is paired with divisor \"n\"/\"d\" of \"n\" and formula_22 is even; for a square integer, one divisor (namely formula_23) is not paired with a distinct divisor and formula_22 is odd. Similarly, the number formula_25 is odd if and only if \"n\" is a square or twice a square.\n\nWe also note \"s\"(\"n\") = \"σ\"(\"n\") − \"n\". Here \"s\"(\"n\") denotes the sum of the proper divisors of \"n\", that is, the divisors of \"n\" excluding \"n\" itself.\nThis function is the one used to recognize perfect numbers which are the \"n\" for which \"s\"(\"n\") = \"n\". If \"s\"(\"n\") > \"n\" then \"n\" is an abundant number and if \"s\"(\"n\") < \"n\" then \"n\" is a deficient number.\n\nIf n is a power of 2, for example, formula_26, then formula_27 and \"s(n) = n - 1\", which makes \"n\" almost-perfect.\n\nAs an example, for two distinct primes \"p\" and \"q\" with \"p < q\", let\n\nThen\nand\nwhere formula_33 is Euler's totient function.\n\nThen, the roots of:\nallow us to express \"p\" and \"q\" in terms of \"σ\"(\"n\") and \"φ\"(\"n\") only, without even knowing \"n\" or \"p+q\", as:\n\nAlso, knowing n and either formula_37 or formula_33 (or knowing p+q and either formula_37 or formula_33) allows us to easily find \"p\" and \"q\".\n\nIn 1984, Roger Heath-Brown proved that the equality\n\nis true for an infinity of values of n, see .\n\nTwo Dirichlet series involving the divisor function are: \n\nwhich for \"d\"(\"n\") = \"σ\"(\"n\") gives: \n\nand \n\nA Lambert series involving the divisor function is: \n\nfor arbitrary complex |\"q\"| ≤ 1 and \"a\". This summation also appears as the Fourier series of the Eisenstein series and the invariants of the Weierstrass elliptic functions.\n\nFor formula_46 exists an explicit series representation with Ramanujan sums formula_47 as :\nThe computation of the first terms of formula_49 shows its oscillations around the \"average value\" formula_50:\n\nIn little-o notation, the divisor function satisfies the inequality:\nMore precisely, Severin Wigert showed that:\nOn the other hand, since there are infinitely many prime numbers,\n\nIn Big-O notation, Peter Gustav Lejeune Dirichlet showed that the average order of the divisor function satisfies the following inequality:\nwhere formula_56 is Euler's gamma constant. Improving the bound formula_57 in this formula is known as Dirichlet's divisor problem.\n\nThe behaviour of the sigma function is irregular. The asymptotic growth rate of the sigma function can be expressed by: \n\nwhere lim sup is the limit superior. This result is Grönwall's theorem, published in 1913 . His proof uses Mertens' 3rd theorem, which says that:\n\nwhere \"p\" denotes a prime.\n\nIn 1915, Ramanujan proved that under the assumption of the Riemann hypothesis, the inequality:\nholds for all sufficiently large \"n\" . The largest known value that violates the inequality is \"n\"=5040. In 1984, Guy Robin proved that the inequality is true for all \"n\" > 5040 if and only if the Riemann hypothesis is true . This is Robin's theorem and the inequality became known after him. Robin furthermore showed that if the Riemann hypothesis is false then there are an infinite number of values of \"n\" that violate the inequality, and it is known that the smallest such \"n\" > 5040 must be superabundant . It has been shown that the inequality holds for large odd and square-free integers, and that the Riemann hypothesis is equivalent to the inequality just for \"n\" divisible by the fifth power of a prime .\n\nRobin also proved, unconditionally, that the inequality:\nholds for all \"n\" ≥ 3.\n\nA related bound was given by Jeffrey Lagarias in 2002, who proved that the Riemann hypothesis is equivalent to the statement that:\nfor every natural number \"n\" > 1, where formula_63 is the \"n\"th harmonic number, .\n\n\n\n"}
{"id": "12830014", "url": "https://en.wikipedia.org/wiki?curid=12830014", "title": "Eight-point algorithm", "text": "Eight-point algorithm\n\nThe eight-point algorithm is an algorithm used in computer vision to estimate the essential matrix or the fundamental matrix related to a stereo camera pair from a set of corresponding image points. It was introduced by Christopher Longuet-Higgins in 1981 for the case of the essential matrix. In theory, this algorithm can be used also for the fundamental matrix, but in practice the normalized eight-point algorithm, described by Richard Hartley in 1997, is better suited for this case.\n\nThe algorithm's name derives from the fact that it estimates the essential matrix or the fundamental matrix from a set of eight (or more) corresponding image points. However, variations of the algorithm can be used for fewer than eight points.\n\nOne may express the epipolar geometry of two cameras and a point in space with an algebraic equation. Observe that, no matter where the point formula_1 is in space, the vectors formula_2, formula_3 and formula_4 belong to the same plane. Call formula_5 the coordinates of point formula_1 in the left eye's reference frame and call formula_7 the coordinates of formula_1 in the right eye's reference frame and call formula_9 the rotation and translation between the two reference frames s.t. formula_10 is the relationship between the coordinates of formula_1 in the two reference frames. The following equation always equals to zero because the vector generated from formula_12 is orthogonal to both formula_13 and formula_5 :\n\nBecause formula_16, we get \n\nReplacing formula_18 with formula_19, we get\n\nObserve that formula_21 may be thought of as a matrix; Longuet-Higgins used the symbol formula_22 to denote it. The product formula_23 is often called essential matrix and denoted with formula_24.\n\nThe vectors formula_25 are parallel to the vectors formula_26 and therefore the coplanarity constraint holds if we substitute these vectors. If we call formula_27 the coordinates of the projections of formula_1 onto the left and right image planes, then the coplanarity constraint may be written as\n\nThe basic eight-point algorithm is here described for the case of estimating the essential matrix formula_30. It consists of three steps. First, it formulates a homogeneous linear equation, where the solution is directly related to formula_30, and then solves the equation, taking into account that it may not have an exact solution. Finally, the internal constraints of the resulting matrix are managed. The first step is described in Longuet-Higgins' paper, the second and third steps are standard approaches in estimation theory.\n\nThe constraint defined by the essential matrix formula_30 is\n\nfor corresponding image points represented in normalized image coordinates formula_34. The problem which the algorithm solves is to determine formula_30 for a set of matching image points. In practice, the image coordinates of the image points are affected by noise and the solution may also be over-determined which means that it may not be possible to find formula_30 which satisfies the above constraint exactly for all points. This issue is addressed in the second step of the algorithm.\n\nWith\n\nthe constraint can also be rewritten as\n\nor\n\nwhere\n\nthat is, formula_44 represents the essential matrix in the form of a 9-dimensional vector and this vector must be orthogonal to the vector formula_45 which can be seen as a vector representation of the formula_46 matrix formula_47.\n\nEach pair of corresponding image points produces a vector formula_45. Given a set of 3D points formula_49 this corresponds to a set of vectors formula_50 and all of them must satisfy\n\nfor the vector formula_44. Given sufficiently many (at least eight) linearly independent vectors formula_50 it is possible to determine formula_44 in a straightforward way. Collect all vectors formula_50 as the columns of a matrix formula_56 and it must then be the case that\n\nThis means that formula_44 is the solution to a homogeneous linear equation.\n\nA standard approach to solving this equation implies that formula_44 is a left singular vector of formula_56 corresponding to a singular value that equals zero. Provided that at least eight linearly independent vectors formula_50 are used to construct formula_56 it follows that this singular vector is unique (disregarding scalar multiplication) and, consequently, formula_44 and then formula_30 can be determined.\n\nIn the case that more than eight corresponding points are used to construct formula_56 it is possible that it does not have any singular value equal to zero. This case occurs in practice when the image coordinates are affected by various types of noise. A common approach to deal with this situation is to describe it as a total least squares problem; find formula_44 which minimizes\n\nwhen formula_68. The solution is to choose formula_44 as the left singular vector corresponding to the \"smallest\" singular value of formula_56. A reordering of this formula_44 back into a formula_46 matrix gives the result of this step, here referred to as formula_73.\n\nAnother consequence of dealing with noisy image coordinates is that the resulting matrix may not satisfy the internal constraint of the essential matrix, that is, two of its singular values are equal and nonzero and the other is zero. Depending on the application, smaller or larger deviations from the internal constraint may or may not be a problem. If it is critical that the estimated matrix satisfies the internal constraints, this can be accomplished by finding the matrix formula_74 of rank 2 which minimizes\n\nwhere formula_73 is the resulting matrix from Step 2 and the Frobenius matrix norm is used. The solution to the problem is given by first computing a singular value decomposition of formula_73:\n\nwhere formula_79 are orthogonal matrices and formula_80 is a diagonal matrix which contains the singular values of formula_73. In the ideal case, one of the diagonal elements of formula_80 should be zero, or at least small compared to the other two which should be equal. In any case, set\n\nwhere formula_84 are the largest and second largest singular values in formula_80 respectively. Finally, formula_74 is given by\n\nThe matrix formula_74 is the resulting estimate of the essential matrix provided by the algorithm.\n\nThis topic is covered in the page on the Essential matrix (section on determining R and t from E).\n\nThe basic eight-point algorithm can in principle be used also for estimating the fundamental matrix formula_89. The defining constraint for formula_89 is\n\nwhere formula_34 are the homogeneous representations of corresponding image coordinates (not necessary normalized). This means that it is possible to form a matrix formula_56 in a similar way as for the essential matrix and solve the equation\n\nfor formula_95 which is a reshaped version of formula_89. By following the procedure outlined above, it is then possible to determine formula_89 from a set of eight matching points. In practice, however, the resulting fundamental matrix may not be useful for determining epipolar constraints.\n\nThe problem is that the resulting formula_56 often is ill-conditioned. In theory, formula_56 should have one singular value equal to zero and the rest are non-zero. In practice, however, some of the non-zero singular values can become small relative to the larger ones. If more than eight corresponding points are used to construct formula_56, where the coordinates are only approximately correct, there may not be a well-defined singular value which can be identified as approximately zero. Consequently, the solution of the homogeneous linear system of equations may not be sufficiently accurate to be useful.\n\nHartley addressed this estimation problem in his 1997 article. His analysis of the problem shows that the problem is caused by the poor distribution of the homogeneous image coordinates in their space, formula_101. A typical homogeneous representation of the 2D image coordinate formula_102 is\n\nwhere both formula_104 lie in the range 0 to 1000-2000 for a modern digital camera. This means that the first two coordinates in formula_105 vary over a much larger range than the third coordinate. Furthermore, if the image points which are used to construct formula_56 lie in a relatively small region of the image, for example at formula_107, again the vector formula_105 points in more or less the same direction for all points. As a consequence, formula_56 will have one large singular value and the remaining are small.\n\nAs a solution to this problem, Hartley proposed that the coordinate system of each of the two images should be transformed, independently, into a new coordinate system according to the following principle.\n\n\nThis principle results, normally, in a distinct coordinate transformation for each of the two images. As a result, new homogeneous image coordinates formula_111 are given by\n\nwhere formula_114 are the transformations (translation and scaling) from the old to the new \"normalized image coordinates\". This normalization is only dependent on the image points which are used in a single image and is, in general, distinct from normalized image coordinates produced by a normalized camera.\n\nThe epipolar constraint based on the fundamental matrix can now be rewritten as\n\nwhere formula_116. This means that it is possible to use the normalized homogeneous image coordinates formula_111 to estimate the transformed fundamental matrix formula_118 using the basic eight-point algorithm described above.\n\nThe purpose of the normalization transformations is that the matrix formula_119, constructed from the normalized image coordinates, in general has a better condition number than formula_56 has. This means that the solution formula_121 is more well-defined as a solution of the homogeneous equation formula_122 than formula_95 is relative to formula_56. Once formula_121 has been determined and reshaped into formula_118 the latter can be \"de-normalized\" to give formula_89 according to\n\nIn general, this estimate of the fundamental matrix is a better one than would have been obtained by estimating from the un-normalized coordinates.\n\nEach point pair contributes with one constraining equation on the element in formula_30. Since formula_30 has five degrees of freedom it should therefore be sufficient with only five point pairs to determine formula_30. Though possible from a theoretical point of view, the practical implementation of this is not straightforward and relies on solving various non-linear equations.\n"}
{"id": "11027904", "url": "https://en.wikipedia.org/wiki?curid=11027904", "title": "Epsilon calculus", "text": "Epsilon calculus\n\nHilbert's epsilon calculus is an extension of a formal language by the epsilon operator, where the epsilon operator substitutes for quantifiers in that language as a method leading to a proof of consistency for the extended formal language. The \"epsilon operator\" and \"epsilon substitution method\" are typically applied to a first-order predicate calculus, followed by a showing of consistency. The epsilon-extended calculus is further extended and generalized to cover those mathematical objects, classes, and categories for which there is a desire to show consistency, building on previously-shown consistency at earlier levels.\n\nFor any formal language \"L\", extend \"L\" by adding the epsilon operator to redefine quantification:\n\n\nThe intended interpretation of ε\"x\" \"A\" is \"some x\" that satisfies \"A\", if it exists. In other words, ε\"x\" \"A\" returns some term \"t\" such that \"A\"(\"t\") is true, otherwise it returns some default or arbitrary term. If more than one term can satisfy \"A\", then any one of these terms (which make \"A\" true) can be chosen, non-deterministically. Equality is required to be defined under \"L\", and the only rules required for \"L\" extended by the epsilon operator are modus ponens and the substitution of \"A\"(\"t\") to replace \"A\"(\"x\") for any term \"t\".\n\nIn tau-square notation from N. Bourbaki's \"Theory of Sets\", the quantifiers are defined as follows:\n\n\nwhere \"A\" is a relation in \"L\", \"x\" is a variable, and formula_5 juxtaposes a formula_6 at the front of \"A\", replaces all instances of \"x\" with formula_7, and links them back to formula_6. Then let \"Y\" be an assembly, \"(Y|x)A\" denotes the replacement of all variables \"x\" in \"A\" with \"Y\".\n\nThis notation is equivalent to the Hilbert notation and is read the same. It is used by Bourbaki to define cardinal assignment since he does not use the axiom of replacement.\n\nDefining quantifiers in this way leads to great inefficiencies. For instance, the expansion of Bourbaki's original definition of the number one, using this notation, has length approximately 4.5 × 10, and for a later edition of Bourbaki that combined this notation with the Kuratowski definition of ordered pairs, this number grows to approximately 2.4 × 10.\n\nHilbert's program for mathematics was to justify those formal systems as consistent in relation to constructive or semi-constructive systems. While Gödel's results on incompleteness mooted Hilbert's Program to a great extent, modern researchers find the epsilon calculus to provide alternatives for approaching proofs of systemic consistency as described in the epsilon substitution method.\n\nA theory to be checked for consistency is first embedded in an appropriate epsilon calculus. Second, a process is developed for re-writing quantified theorems to be expressed in terms of epsilon operations via the epsilon substitution method. Finally, the process must be shown to normalize the re-writing process, so that the re-written theorems satisfy the axioms of the theory.\n\n"}
{"id": "152900", "url": "https://en.wikipedia.org/wiki?curid=152900", "title": "Finitism", "text": "Finitism\n\nFinitism is a philosophy of mathematics that accepts the existence only of finite mathematical objects. It is best understood in comparison to the mainstream philosophy of mathematics where infinite mathematical objects (e.g., infinite sets) are accepted as legitimate.\n\nThe main idea of finitistic mathematics is not accepting the existence of infinite objects such as infinite sets. While all natural numbers are accepted as existing, the set of all natural numbers is not considered to exist as a mathematical object. Therefore quantification over infinite domains is not considered meaningful. The mathematical theory often associated with finitism is Thoralf Skolem's primitive recursive arithmetic.\n\nThe introduction of infinite mathematical objects occurred a few centuries ago when the use of infinite objects was already a controversial topic among mathematicians. The issue entered a new phase when Georg Cantor in 1874 introduced what is now called naive set theory and used it as a base for his work on transfinite numbers. When paradoxes such as Russell's paradox, Berry's paradox and the Burali-Forti paradox were discovered in Cantor's naive set theory, the issue became a heated topic among mathematicians.\n\nThere were various positions taken by mathematicians. All agreed about finite mathematical objects such as natural numbers. However there were disagreements regarding infinite mathematical objects. \nOne position was the intuitionistic mathematics that was advocated by L. E. J. Brouwer, which rejected the existence of infinite objects until they are constructed.\n\nAnother position was endorsed by David Hilbert: finite mathematical objects are concrete objects, infinite mathematical objects are ideal objects, and accepting ideal mathematical objects does not cause a problem regarding finite mathematical objects. More formally, Hilbert believed that it is possible to show that any theorem about finite mathematical objects that can be obtained using ideal infinite objects can be also obtained without them. Therefore allowing infinite mathematical objects would not cause a problem regarding finite objects. This led to Hilbert's program of proving consistency of set theory using finitistic means as this would imply that adding ideal mathematical objects is conservative over the finitistic part. Hilbert's views are also associated with formalist philosophy of mathematics. Hilbert's goal of proving the consistency of set theory or even arithmetic through finitistic means turned out to be an impossible task due to Kurt Gödel's incompleteness theorems. However, by Harvey Friedman's grand conjecture most mathematical results should be provable using finitistic means.\n\nHilbert did not give a rigorous explanation of what he considered finitistic and refer to as elementary. However, based on his work with Paul Bernays some experts such as William Tait have argued that the primitive recursive arithmetic can be considered an upper bound on what Hilbert considered finitistic mathematics.\n\nIn the years following Gödel's theorems, as it became clear that there is no hope of proving consistency of mathematics, and with development of axiomatic set theories such as Zermelo–Fraenkel set theory and the lack of any evidence against its consistency, most mathematicians lost interest in the topic. Today most classical mathematicians are considered Platonist and readily use infinite mathematical objects and a set-theoretical universe.\n\nIn her book \"Philosophy of Set Theory\", Mary Tiles characterized those who allow \"potentially infinite\" objects as classical finitists, and those who do not allow potentially infinite objects as strict finitists: for example, a classical finitist would allow statements such as \"every natural number has a successor\" and would accept the meaningfulness of infinite series in the sense of limits of finite partial sums, while a strict finitist would not. Historically, the written history of mathematics was thus classically finitist until Cantor discovered the hierarchy of transfinite cardinals at the end of the 19th century.\n\nLeopold Kronecker remained a strident opponent to Cantor's set theory:\nReuben Goodstein is another proponent of finitism. Some of his work involved building up to analysis from finitist foundations.\n\nAlthough he denied it, much of Ludwig Wittgenstein's writing on mathematics has a strong affinity with finitism.\n\nIf finitists are contrasted with transfinitists (proponents of e.g. Georg Cantor's hierarchy of infinities), then also Aristotle may be characterized as a strict finitist. Aristotle especially promoted the potential infinity as a middle option between strict finitism and actual infinity (the latter being an actualization of something never-ending in nature, in contrast with the Cantorist actual infinity consisting of the transfinite cardinal and ordinal numbers, which have nothing to do with the things in nature):\n\nUltrafinitism (also known as ultraintuitionism) has an even more conservative attitude towards mathematical objects than finitism, and has objections to the existence of finite mathematical objects when they are too large.\n\nTowards the end of the 20th century John Penn Mayberry developed a system of finitary mathematics which he called \"Euclidean Arithmetic\". The most striking tenet of his system is a complete and rigorous rejection of the special foundational status normally accorded to iterative processes, including in particular the construction of the natural numbers by the iteration \"+1\". Consequently Mayberry is in sharp dissent from those who would seek to equate finitary mathematics with Peano Arithmetic or any of its fragments such as primitive recursive arithmetic.\n\n\n"}
{"id": "205513", "url": "https://en.wikipedia.org/wiki?curid=205513", "title": "George Peacock", "text": "George Peacock\n\nGeorge Peacock FRS (9 April 1791 – 8 November 1858) was an English mathematician.\n\nPeacock was born on 9 April 1791 at Thornton Hall, Denton, near Darlington, County Durham. His father, Thomas Peacock, was a priest of the Church of England, incumbent and for 50 years curate of the parish of Denton, where he also kept a school. In early life Peacock did not show any precocity of genius, and was more remarkable for daring feats of climbing than for any special attachment to study. Initially, he received his elementary education from his father and then at Sedbergh School, and at 17 years of age, he was sent to Richmond School under James Tate, a graduate of Cambridge University. At this school he distinguished himself greatly both in classics and in the rather elementary mathematics then required for entrance at Cambridge. In 1809 he became a student of Trinity College, Cambridge.\n\nIn 1812 Peacock took the rank of Second Wrangler, and the second Smith's prize, the senior wrangler being John Herschel. Two years later he became a candidate for a fellowship in his college and won it immediately, partly by means of his extensive and accurate knowledge of the classics. A fellowship then meant about pounds 200 a year, tenable for seven years provided the Fellow did not marry meanwhile, and capable of being extended after the seven years provided the Fellow took clerical orders, which Peacock did in 1819.\n\nThe year after taking a Fellowship, Peacock was appointed a tutor and lecturer of his college, which position he continued to hold for many years. Peacock, in common with many other students of his own standing, was profoundly impressed with the need of reforming Cambridge's position ignoring the differential notation for calculus, and while still an undergraduate formed a league with Babbage and Herschel to adopt measures to bring it about. In 1815 they formed what they called the \"Analytical Society\", the object of which was stated to be to advocate the \"d\" 'ism of the Continent versus the \"dot\"-age of the University.\n\nThe first movement on the part of the Analytical Society was to translate from the French the smaller work of Lacroix on the differential and integral calculus; it was published in 1816. At that time the French language had the best manuals, as well as the greatest works on mathematics. Peacock followed up the translation with a volume containing a copious \"Collection of Examples of the Application of the Differential and Integral Calculus\", which was published in 1820. The sale of both books was rapid, and contributed materially to further the object of the Society. In that time, high wranglers of one year became the examiners of the mathematical tripos three or four years afterwards. Peacock was appointed an examiner in 1817, and he did not fail to make use of the position as a powerful lever to advance the cause of reform. In his questions set for the examination the differential notation was for the first time officially employed in Cambridge. The innovation did not escape censure, but he wrote to a friend as follows: \"I assure you that I shall never cease to exert myself to the utmost in the cause of reform, and that I will never decline any office which may increase my power to effect it. I am nearly certain of being nominated to the office of Moderator in the year 1818-1819, and as I am an examiner in virtue of my office, for the next year I shall pursue a course even more decided than hitherto, since I shall feel that men have been prepared for the change, and will then be enabled to have acquired a better system by the publication of improved elementary books. I have considerable influence as a lecturer, and I will not neglect it. It is by silent perseverance only, that we can hope to reduce the many-headed monster of prejudice and make the University answer her character as the loving mother of good learning and science.\" These few sentences give an insight into the character of Peacock: he was an ardent reformer and a few years brought success to the cause of the Analytical Society.\n\nAnother reform at which Peacock labored was the teaching of algebra. In 1830 he published a \"Treatise on Algebra\" which had for its object the placing of algebra on a true scientific basis, adequate for the development which it had received at the hands of the Continental mathematicians. To elevate astronomical science the Astronomical Society of London was founded, and the three reformers Peacock, Babbage and Herschel were again prime movers in the undertaking. Peacock was one of the most zealous promoters of an astronomical observatory at Cambridge, and one of the founders of the Philosophical Society of Cambridge.\n\nIn 1831 the British Association for the Advancement of Science (prototype of the American, French and Australasian Associations) held its first meeting in the ancient city of York. One of the first resolutions adopted was to procure reports on the state and progress of particular sciences, to be drawn up from time to time by competent persons for the information of the annual meetings, and the first to be placed on the list was a report on the progress of mathematical science. Whewell, the mathematician and philosopher, was a Vice-president of the meeting: he was instructed to select the reporter. He first asked William Rowan Hamilton, who declined; he then asked Peacock, who accepted. Peacock had his report ready for the third meeting of the Association, which was held in Cambridge in 1833; although limited to Algebra, Trigonometry, and the Arithmetic of Sines, it is one of the best of the long series of valuable reports which have been prepared for and printed by the Association.\n\nIn 1837 Peacock was appointed Lowndean Professor of Astronomy in the University of Cambridge, the chair afterwards occupied by Adams, the co-discoverer of Neptune, and later occupied by Robert Ball, celebrated for his \"Theory of Screws\". An object of reform was the statutes of the University; he worked hard at it and was made a member of a commission appointed by the Government for the purpose.\n\nHe was elected a Fellow of the Royal Society in January 1818.\n\nHe was ordained as a deacon in 1819, a priest in 1822 and appointed Vicar of Wymeswold in Leicestershire in 1826 (until 1835).\n\nIn 1839 he was appointed Dean of Ely cathedral, Cambridgeshire, a position he held for the rest of his life, some 20 years. Together with the architect George Gilbert Scott he undertook a major restoration of the cathedral building. This included the installation of the boarded ceiling.\n\nWhile holding this position he wrote a text book on algebra in two volumes, the one called \"Arithmetical Algebra\", and the other \"Symbolical Algebra\".\n\nPolitically he was a Whig.\n\nHis last public act was to attend a meeting of the university reform commission. He died in Ely on 8 November 1858 in the 68th year of his age and was buried in Ely cemetery. He had married Frances Elizabeth, the daughter of William Selwyn, but had no children.\n\nPeacock's main contribution to mathematical analysis is his attempt to place algebra on a strictly logical basis. He founded what has been called the philological or symbolical school of mathematicians; to which Gregory, De Morgan and Boole belonged. His answer to Maseres and Frend was that the science of algebra consisted of two parts—arithmetical algebra and symbolical algebra—and that they erred in restricting the science to the arithmetical part. His view of arithmetical algebra is as follows: \"In arithmetical algebra we consider symbols as representing numbers, and the operations to which they are submitted as included in the same definitions as in common arithmetic; the signs formula_1 and formula_2 denote the operations of addition and subtraction in their ordinary meaning only, and those operations are considered as impossible in all cases where the symbols subjected to them possess values which would render them so in case they were replaced by digital numbers; thus in expressions such as formula_3 we must suppose formula_4 and formula_5 to be quantities of the same kind; in others, like formula_6, we must suppose formula_4 greater than formula_5 and therefore homogeneous with it; in products and quotients, like formula_9 and formula_10 we must suppose the multiplier and divisor to be abstract numbers; all results whatsoever, including negative quantities, which are not strictly deducible as legitimate conclusions from the definitions of the several operations must be rejected as impossible, or as foreign to the science.\"\n\nPeacock's principle may be stated thus: the elementary symbol of arithmetical algebra denotes a digital, i.e., an integer number; and every combination of elementary symbols must reduce to a digital number, otherwise it is impossible or foreign to the science. If formula_4 and formula_5 are numbers, then formula_3 is always a number; but formula_6 is a number only when formula_5 is less than formula_4. Again, under the same conditions, formula_9 is always a number, but formula_10 is really a number only when formula_5 is an exact divisor of formula_4. Hence the following dilemma: Either formula_10 must be held to be an impossible expression in general, or else the meaning of the fundamental symbol of algebra must be extended so as to include rational fractions. If the former horn of the dilemma is chosen, arithmetical algebra becomes a mere shadow; if the latter horn is chosen, the operations of algebra cannot be defined on the supposition that the elementary symbol is an integer number. Peacock attempts to get out of the difficulty by supposing that a symbol which is used as a multiplier is always an integer number, but that a symbol in the place of the multiplicand may be a fraction. For instance, in formula_9, formula_4 can denote only an integer number, but formula_5 may denote a rational fraction. Now there is no more fundamental principle in arithmetical algebra than that formula_25; which would be illegitimate on Peacock's principle.\n\nOne of the earliest English writers on arithmetic is Robert Record, who dedicated his work to King Edward the Sixth. The author gives his treatise the form of a dialogue between master and scholar. The scholar battles long over this difficulty, -- that multiplying a thing could make it less. The master attempts to explain the anomaly by reference to proportion; that the product due to a fraction bears the same proportion to the thing multiplied that the fraction bears to unity. But the scholar is not satisfied and the master goes on to say: \"If I multiply by more than one, the thing is increased; if I take it but once, it is not changed, and if I take it less than once, it cannot be so much as it was before. Then seeing that a fraction is less than one, if I multiply by a fraction, it follows that I do take it less than once.\" Whereupon the scholar replies, \"Sir, I do thank you much for this reason, -- and I trust that I do perceive the thing.\"\n\nThe fact is that even in arithmetic the two processes of multiplication and division are generalized into a common multiplication; and the difficulty consists in passing from the original idea of multiplication to the generalized idea of a \"tensor\", which idea includes compressing the magnitude as well as stretching it. Let formula_26 denote an integer number; the next step is to gain the idea of the reciprocal of formula_26, not as formula_28 but simply as formula_29. When formula_26 and formula_31 are compounded we get the idea of a rational fraction; for in general formula_32 will not reduce to a number nor to the reciprocal of a number.\n\nSuppose, however, that we pass over this objection; how does Peacock lay the foundation for general algebra? He calls it symbolical algebra, and he passes from arithmetical algebra to symbolical algebra in the following manner: \"Symbolical algebra adopts the rules of arithmetical algebra but removes altogether their restrictions; thus symbolical subtraction differs from the same operation in arithmetical algebra in being possible for all relations of value of the symbols or expressions employed. All the results of arithmetical algebra which are deduced by the application of its rules, and which are general in form though particular in value, are results likewise of symbolical algebra where they are general in value as well as in form; thus the product of formula_33 and formula_34 which is formula_35 when formula_26 and formula_37 are whole numbers and therefore general in form though particular in value, will be their product likewise when formula_26 and formula_37 are general in value as well as in form; the series for formula_40 determined by the principles of arithmetical algebra when formula_37 is any whole number, \"if it be exhibited in a general form, without reference to a final term\", may be shown upon the same principle to the equivalent series for formula_42 when formula_37 is general both in form and value.\"\n\nThe principle here indicated by means of examples was named by Peacock the \"principle of the permanence of equivalent forms,\" and at page 59 of the \"Symbolical Algebra\" it is thus enunciated: \"Whatever algebraic forms are equivalent when the symbols are general in form, but specific in value, will be equivalent likewise when the symbols are general in value as well as in form.\"\n\nFor example, let formula_4, formula_5, formula_46, formula_47 denote any integer numbers, but subject to the restrictions that formula_5 is less than formula_4, and formula_47 less than formula_46; it may then be shown arithmetically that formula_52. Peacock's principle says that the form on the left side is equivalent to the form on the right side, not only when the said restrictions of being less are removed, but when formula_4, formula_5, formula_46, formula_47 denote the most general algebraic symbol. It means that formula_4, formula_5, formula_46, formula_47 may be rational fractions, or surds, or imaginary quantities, or indeed operators such as formula_61. The equivalence is not established by means of the nature of the quantity denoted; the equivalence is assumed to be true, and then it is attempted to find the different interpretations which may be put on the symbol.\n\nIt is not difficult to see that the problem before us involves the fundamental problem of a rational logic or theory of knowledge; namely, how are we able to ascend from particular truths to more general truths. If formula_4, formula_5, formula_46, formula_47 denote integer numbers, of which formula_5 is less than formula_4 and formula_47 less than formula_46, then formula_52.\n\nIt is first seen that the above restrictions may be removed, and still the above equation holds. But the antecedent is still too narrow; the true scientific problem consists in specifying the meaning of the symbols, which, and only which, will admit of the forms being equal. It is not to find \"some meanings\", but the \"most general meaning\", which allows the equivalence to be true. Let us examine some other cases; we shall find that Peacock's principle is not a solution of the difficulty; the great logical process of generalization cannot be reduced to any such easy and arbitrary procedure. When formula_4, formula_26, formula_37 denote integer numbers, it can be shown thatformula_74.\n\nAccording to Peacock the form on the left is always to be equal to the form on the right, and the meanings of formula_4, formula_26, formula_37 are to be found by interpretation. Suppose that formula_4 takes the form of the incommensurate quantity formula_79, the base of the natural system of logarithms. A number is a degraded form of a complex quantity formula_80 and a complex quantity is a degraded form of a quaternion; consequently one meaning which may be assigned to formula_26 and formula_37 is that of quaternion. Peacock's principle would lead us to suppose that formula_83, formula_26 and formula_37 denoting quaternions; but that is just what W.R. Hamilton, the inventor of the quaternion generalization, denies. There are reasons for believing that he was mistaken, and that the forms remain equivalent even under that extreme generalization of formula_26 and formula_37; but the point is this: it is not a question of conventional definition and formal truth; it is a question of objective definition and real truth. Let the symbols have the prescribed meaning, does or does not the equivalence still hold? And if it does not hold, what is the higher or more complex form which the equivalence assumes? Or does such equivalence form even exist?\n\n\n"}
{"id": "56062096", "url": "https://en.wikipedia.org/wiki?curid=56062096", "title": "Globular set", "text": "Globular set\n\nIn category theory, a branch of mathematics, a globular set is a higher-dimensional generalization of a directed graph. Precisely, it is a sequence of sets formula_1 equipped with pairs of functions formula_2 such that\n(Equivalently, it is a presheaf on the category of “globes”.) The letters \"\"s\", \"t\"\" stand for \"source\" and \"target\" and one imagines formula_5 consists of directed edges at level \"n\".\n\nA variant of the notion was used by Grothendieck to introduce the notion of an ∞-groupoid. Extending Grothendieck's work, gave a definition of a weak ∞-category in terms of globular sets.\n\n\n"}
{"id": "10094198", "url": "https://en.wikipedia.org/wiki?curid=10094198", "title": "Hardy–Littlewood maximal function", "text": "Hardy–Littlewood maximal function\n\nIn mathematics, the Hardy–Littlewood maximal operator \"M\" is a significant non-linear operator used in real analysis and harmonic analysis. It takes a locally integrable function \"f\" : R → C and returns another function \"Mf\" that, at each point \"x\" ∈ R, gives the maximum average value that \"f\" can have on balls centered at that point. More precisely,\n\nwhere \"B\"(\"x\", \"r\") is the ball of radius \"r\" centred at \"x\", and |\"E\"| denotes the \"d\"-dimensional Lebesgue measure of \"E\" ⊂ R.\n\nThe averages are jointly continuous in \"x\" and \"r\", therefore the maximal function \"Mf\", being the supremum over \"r\" > 0, is measurable. It is not obvious that \"Mf\" is finite almost everywhere. This is a corollary of the Hardy–Littlewood maximal inequality\n\nThis theorem of G. H. Hardy and J. E. Littlewood states that \"M\" is bounded as a sublinear operator from the \"L\"(R) to itself for \"p\" > 1. That is, if \"f\" ∈ \"L\"(R) then the maximal function \"Mf\" is weak \"L\"-bounded and \"Mf\" ∈ \"L\"(R). Before stating the theorem more precisely, for simplicity, let {\"f\" > \"t\"} denote the set {\"x\" | \"f\"(\"x\") > \"t\"}. Now we have:\n\nTheorem (Weak Type Estimate). For \"d\" ≥ 1 and \"f\" ∈ \"L\"(R), there is a constant \"C\" > 0 such that for all λ > 0, we have:\n\nWith the Hardy–Littlewood maximal inequality in hand, the following \"strong-type\" estimate is an immediate consequence of the Marcinkiewicz interpolation theorem:\n\nTheorem (Strong Type Estimate). For \"d\" ≥ 1, 1 < \"p\" ≤ ∞, and \"f\" ∈ \"L\"(R),\nthere is a constant \"C\" > 0 such that\n\nIn the strong type estimate the best bounds for \"C\" are unknown. However subsequently Elias M. Stein used the Calderón-Zygmund method of rotations to prove the following:\n\nTheorem (Dimension Independence). For 1 < \"p\" ≤ ∞ one can pick \"C\" = \"C\" independent of \"d\".\n\nWhile there are several proofs of this theorem, a common one is given below: For \"p\" = ∞, the inequality is trivial (since the average of a function is no larger than its essential supremum). For 1 < \"p\" < ∞, first we shall use the following version of the Vitali covering lemma to prove the weak-type estimate. (See the article for the proof of the lemma.)\n\nLemma. Let \"X\" be a separable metric space and formula_4 family of open balls with bounded diameter. Then formula_4 has a countable subfamily formula_6 consisting of disjoint balls such that\nwhere 5\"B\" is \"B\" with 5 times radius.\n\nIf \"Mf\"(\"x\") > \"t\", then, by definition, we can find a ball \"B\" centered at \"x\" such that\nBy the lemma, we can find, among such balls, a sequence of disjoint balls \"B\" such that the union of 5\"B\" covers {\"Mf\" > \"t\"}.\nIt follows:\nThis completes the proof of the weak-type estimate. We next deduce from this the \"L\" bounds. Define \"b\" by \"b\"(\"x\") = \"f\"(\"x\") if |\"f\"(\"x\")| > \"t\"/2 and 0 otherwise. By the weak-type estimate applied to \"b\", we have:\n\nwhere \"Q\" ranges over all dyadic cubes containing the point \"x\". Both of these operators satisfy the HL maximal inequality.\n\n"}
{"id": "17824450", "url": "https://en.wikipedia.org/wiki?curid=17824450", "title": "Hedonic index", "text": "Hedonic index\n\nIn econometrics, a hedonic index is any price index which uses information from hedonic regression, which describes how product price could be explained by the product's characteristics. Hedonic price indexes have proved to be very useful when applied to calculate price indices for information and communication products (e.g. personal computers) and housing, because they can successfully mitigate problems such as those that arise from there being new goods to consider and from rapid changes of quality.\n\nIn the last two decades considerable attention has been drawn to the methods of computing price indexes. The Boskin Commission in 1996 asserted that there were biases in the price index: traditional matched model indexes can substantially overestimate inflation, because they are not able to measure the impact of peculiarities of specific industries such as fast rotation of goods, huge quality differences among products on the market, and short product life cycle. The Commission showed that the usage of matched model indexes (traditional price indexes) leads to an overestimation of inflation by 0.6% per year in the US official CPI (CPI-U). Information and Communications Technology (ICT) products led both to an increase in capital stock and labor productivity growth. Similar results were obtained by Crawford for Canada, by Shiratsuka for Japan, and by Cunningham for the UK. By reversing hedonic methodology, and pending further disclosure from commercial sources, bias has also been enumerated annually over five decades, for the U.S.A. \n\nQuality adjustments are also important for understanding national accounts deflators (see GDP deflator). In the USA, for example, growth acceleration after 1995 was driven by the increased investment in ICT products that lead both to an increase in capital stock and labor productivity growth. This increases the complexity of international comparisons of deflators. Wyckoff and Eurostat show that there is a huge dispersion in ICT deflators in Organisation for Economic Co-operation and Development (OECD) and European countries, accordingly.\n\nThese differences are so huge that it cannot be explained by any means of market conditions, regulation, etc. As both studies suggest, most of the discrepancy comes from the differences in quality adjustment procedures across countries and that, in turn, makes international comparison of investment in ICT impossible (as it is calculated through deflation). This also makes it difficult to compare the impact of ICT on economies (countries, regions, etc.) that use different methods to compute GDP numbers.\n\nFor example, for a linear econometric model, assume that at each period \"t\" we have formula_1 goods, which could be described by a vector of \"k\" characteristics formula_2. Thus the hedonic (cross-sectional) regression is:\n\nwhere formula_4 is a set of coefficients and formula_5 are independent and identically distributed, having a normal distribution formula_6.\n\nThere are several ways the hedonic price indexes can be constructed. Following Triplett, two methods can be distinguished—direct and indirect. The direct method uses only information obtained from the hedonic regression, while the second method combines information derived from the hedonic regression and matched models (traditional price indexes). In indirect method, data used for estimating hedonic regression and calculating matched models indexes are different.\n\nThe \"Direct method\" could be divided into the \"Time Dummy Variable\" and \"Characteristic methods\". \n\nThe Time Dummy Variable is simpler, because it assumes implicit prices (coefficients of the hedonic regression - formula_4) to be constant over adjacent time periods. This assumption generally does not hold since implicit prices reflect both demand and supply.\n\nCharacteristic method, relaxes this assumption, based on the usage of fitted prices from hedonic regression. This method generally should lead to a more stable estimates, because ordinary least squares (OLS) estimates guarantee that the regression always passes through its mean.\n\nThe corresponding \"characteristic chain\" hedonic price index looks for period from \"0\" to \"T\",\n\nand formula_9 is an estimate of price obtained from hedonic regression at period \"t\"+1 with mean characteristics of period formula_10.\n\nThe corresponding \"characteristic base\" hedonic price index looks for period from \"0\" to \"T\":\n\nA specification of formula_12 - mean characteristics for the certain period, determines the type of index. For example, if we set formula_12 equal to the mean of the characteristics for the previous period formula_14, we would get a Laspeyres-type index. Setting formula_12 equal to formula_16 gives Paasche-type index and so on. The Fisher-type index is defined as a square root of product of Laspeyres- and Paasche-type indexes. The Edgeworth-Marshall index uses the arithmetic mean of mean characteristics of two periods \"t\" and \"t\"+1. A Walsh-type index uses the geometric average of two periods. And finally, the base quality index does not update characteristics (quality) and uses fixed base characteristics - formula_17.\n\nHedonic quality index is similar to quantity index in traditional index theory—it measures how the price of obtaining set of characteristics had changed over time. For example, if we are willing to estimate the effect that characteristic growth (or decline) has had on the price of a computer for one period - from \"t\" to \"t+1\", then the hedonic quality index would look like:\n\nwhere formula_19, as in the case with price indexes, determines the type of the index. So, the chain quality index for the period from \"0\" to \"T\" would look like:\n\nand the base index:\n\n\n"}
{"id": "25161352", "url": "https://en.wikipedia.org/wiki?curid=25161352", "title": "Homogeneous distribution", "text": "Homogeneous distribution\n\nIn mathematics, a homogeneous distribution is a distribution \"S\" on Euclidean space R or } that is homogeneous in the sense that, roughly speaking,\nfor all \"t\" > 0.\n\nMore precisely, let formula_2 be the scalar division operator on R. A distribution \"S\" on R or } is homogeneous of degree \"m\" provided that\nfor all positive real \"t\" and all test functions φ. The additional factor of \"t\" is needed to reproduce the usual notion of homogeneity for locally integrable functions, and comes about from the Jacobian change of variables. The number \"m\" can be real or complex.\n\nIt can be a non-trivial problem to extend a given homogeneous distribution from R \\ {0} to a distribution on R, although this is necessary for many of the techniques of Fourier analysis, in particular the Fourier transform, to be brought to bear. Such an extension exists in most cases, however, although it may not be unique.\n\nIf \"S\" is a homogeneous distribution on R \\ {0} of degree α, then the weak first partial derivative of \"S\"\nhas degree α−1. Furthermore, a version of Euler's homogeneous function theorem holds: a distribution \"S\" is homogeneous of degree α if and only if\n\nA complete classification of homogeneous distributions in one dimension is possible. The homogeneous distributions on } are given by various power functions. In addition to the power functions, homogeneous distributions on R include the Dirac delta function and its derivatives.\n\nThe Dirac delta function is homogeneous of degree −1. Intuitively,\nby making a change of variables \"y\" = \"tx\" in the \"integral\". Moreover, the \"k\"th weak derivative of the delta function δ is homogeneous of degree −\"k\"−1. These distributions all have support consisting only of the origin: when localized over }, these distributions are all identically zero.\n\nIn one dimension, the function\nis locally integrable on }, and thus defines a distribution. The distribution is homogeneous of degree α. Similarly formula_8 and formula_9 are homogeneous distributions of degree α.\n\nHowever, each of these distributions is only locally integrable on all of R provided Re(α) > −1. But although the function formula_10 naively defined by the above formula fails to be locally integrable for Re α ≤ −1, the mapping\nis a holomorphic function from the right half-plane to the topological vector space of tempered distributions. It admits a unique meromorphic extension with simple poles at each negative integer . The resulting extension is homogeneous of degree α, provided α is not a negative integer, since on the one hand the relation\nholds and is holomorphic in α > 0. On the other hand, both sides extend meromorphically in α, and so remain equal throughout the domain of definition.\n\nThroughout the domain of definition, \"x\" also satisfies the following properties:\n\nThere are several distinct ways to extend the definition of power functions to homogeneous distributions on R at the negative integers.\n\n\nThe poles in \"x\" at the negative integers can be removed by renormalizing. Put\nThis is an entire function of α. At the negative integers,\nThe distributions formula_17 have the properties\n\n\nA second approach is to define the distribution formula_21, for \nThese clearly retain the original properties of power functions:\nThese distributions are also characterized by their action on test functions\nand so generalize the Cauchy principal value distribution of 1/\"x\" that arises in the Hilbert transform.\n\nAnother homogeneous distribution is given by the distributional limit\nThat is, acting on test functions\nThe branch of the logarithm is chosen to be single-valued in the upper half-plane and to agree with the natural log along the positive real axis. As the limit of entire functions, is an entire function of α. Similarly,\nis also a well-defined distribution for all α\n\nWhen Re α > 0,\nwhich then holds by analytic continuation whenever α is not a negative integer. By the permanence of functional relations,\n\nAt the negative integers, the identity holds (at the level of distributions on R \\ {0})\nand the singularities cancel to give a well-defined distribution on R. The average of the two distributions agrees with formula_21:\nThe difference of the two distributions is a multiple of the delta function:\nwhich is known as the Plemelj jump relation.\n\nThe following classification theorem holds . Let \"S\" be a distribution homogeneous of degree α on }. Then formula_35 for some constants \"a\", \"b\". Any distribution \"S\" on R homogeneous of degree is of this form as well. As a result, every homogeneous distribution of degree on } extends to R.\n\nFinally, homogeneous distributions of degree −\"k\", a negative integer, on R are all of the form:\n\nHomogeneous distributions on the Euclidean space } with the origin deleted are always of the form\n\nwhere \"ƒ\" is a distribution on the unit sphere \"S\". The number λ, which is the degree of the homogeneous distribution \"S\", may be real or complex.\n\nAny homogeneous distribution of the form () on } extends uniquely to a homogeneous distribution on R provided . In fact, an analytic continuation argument similar to the one-dimensional case extends this for all .\n\n"}
{"id": "802867", "url": "https://en.wikipedia.org/wiki?curid=802867", "title": "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension", "text": "How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\n\n\"How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension\" is a paper by mathematician Benoît Mandelbrot, first published in \"Science\" in 5 May 1967. In this paper, Mandelbrot discusses self-similar curves that have Hausdorff dimension between 1 and 2. These curves are examples of \"fractals\", although Mandelbrot does not use this term in the paper, as he did not coin it until 1975. The paper is one of Mandelbrot's first publications on the topic of fractals.\n\nThe paper examines the coastline paradox: the property that the measured length of a stretch of coastline depends on the scale of measurement. Empirical evidence suggests that the smaller the increment of measurement, the longer the measured length becomes. If one were to measure a stretch of coastline with a yardstick, one would get a shorter result than if the same stretch were measured with a one-foot (30.48 cm) ruler. This is because one would be laying the ruler along a more curvilinear route than that followed by the yardstick. The empirical evidence suggests a rule which, if extrapolated, shows that the measure length increases without limit as the measurement scale decreases towards zero. \n\nThis discussion implies that it is meaningless to talk about the length of a coastline; some other means of quantifying coastlines are needed. Mandelbrot discusses an empirical law discovered by Lewis Fry Richardson, who observed that the measured length \"L\"(\"G\") of various geographic borders was a function of the measurement scale \"G\". Collecting data from several different examples, Richardson conjectured that \"L\"(\"G\") could be closely approximated by a function of the form\n\nwhere \"M\" is a positive constant and \"D\" is a constant, called the dimension, greater than or equal to 1. Intuitively, if a coastline looks smooth it should have dimension close to 1; and the more irregular the coastline looks the closer its dimension should be to 2. The examples in Richardson's research have dimensions ranging from 1.02 for the coastline of South Africa to 1.25 for the West coast of Britain. \n\nMandelbrot then describes various mathematical curves, related to the Koch snowflake, which are defined in such a way that they are strictly self-similar. Mandelbrot shows how to calculate the Hausdorff dimension of each of these curves, each of which has a dimension \"D\" between 1 and 2 (he also mentions but does not give a construction for the space-filling Peano curve, which has a dimension exactly 2). He notes that the approximation of these curves with segments of length \"G\" have lengths of the form formula_2. The resemblance with Richardson's law is striking. The paper does not claim that any coastline or geographic border actually \"has\" fractional dimension. Instead, it notes that Richardson's empirical law is compatible with the idea that geographic curves, such as coastlines, can be modelled by random self-similar figures of fractional dimension.\n\nNear the end of the paper Mandelbrot briefly discusses how one might approach the study of fractal-like objects in nature that look random rather than regular. For this he defines statistically self-similar figures and says that these are encountered in nature.\n\nThe paper is important because it is a \"turning point\" in Mandelbrot's early thinking on fractals. It is an example of the linking of mathematical objects with natural forms that was a theme of much of his later work.\n\n"}
{"id": "4492813", "url": "https://en.wikipedia.org/wiki?curid=4492813", "title": "Indiana Pi Bill", "text": "Indiana Pi Bill\n\nThe Indiana Pi Bill is the popular name for bill #246 of the 1897 sitting of the Indiana General Assembly, one of the most notorious attempts to establish mathematical truth by legislative fiat. Despite its name, the main result claimed by the bill is a method to square the circle, rather than to establish a certain value for the mathematical constant , the ratio of the circumference of a circle to its diameter. The bill, written by amateur mathematician Edward J. Goodwin, does imply various incorrect values of , such as 3.2.\n\nThe bill never became law, due to the intervention of Professor C. A. Waldo of Purdue University, who happened to be present in the legislature on the day it went up for a vote.\n\nThe impossibility of squaring the circle using only compass and straightedge constructions, suspected since ancient times, was rigorously proven in 1882 by Ferdinand von Lindemann. Better approximations of than those implied by the bill have been known since ancient times.\n\nIn 1894, Indiana physician and amateur mathematician Edward J. Goodwin (ca. 1825–1902) believed that he had discovered a correct way of squaring the circle. He proposed a bill to state representative Taylor I. Record, which Record introduced in the House under the long title \"A Bill for an act introducing a new mathematical truth and offered as a contribution to education to be used only by the State of Indiana free of cost by paying any royalties whatever on the same, provided it is accepted and adopted by the official action of the Legislature of 1897\".\n\nThe text of the bill consists of a series of mathematical claims (detailed below), followed by a recitation of Goodwin's previous accomplishments:\n\nGoodwin's \"solutions\" were indeed published in the \"American Mathematical Monthly\", though with a disclaimer of \"published by request of the author\".\n\nUpon its introduction in the Indiana House of Representatives, the bill's language and topic occasioned confusion among the membership; a member from Bloomington proposed that it be referred to the Finance Committee, but the Speaker accepted another member's recommendation to refer the bill to the Committee on Swamplands, where the bill could \"find a deserved grave\". It was transferred to the Committee on Education, which reported favorably; following a motion to suspend the rules, the bill passed on February 6, without a dissenting vote. The news of the bill occasioned an alarmed response from \"Der Tägliche Telegraph\", a German-language newspaper in Indianapolis, which viewed the event with significantly less favor than its English-speaking competitors. As this debate concluded, Purdue University Professor C. A. Waldo arrived in Indianapolis to secure the annual appropriation for the Indiana Academy of Science. An assemblyman handed him the bill, offering to introduce him to the genius who wrote it. He declined, saying that he already met as many crazy people as he cared to.\n\nWhen it reached the Indiana Senate, the bill was not treated so kindly, for Waldo had coached the senators previously. The committee to which it had been assigned reported it unfavorably, and the Senate tabled it on February 12; it was nearly passed, but opinion changed when one senator observed that the General Assembly lacked the power to define mathematical truth. Influencing some of the senators was a report that major newspapers, such as the \"Chicago Tribune\", had begun to ridicule the situation.\n\nAccording to the \"Indianapolis News\" article of February 13, page 11, column 3:\n\n... the bill was brought up and made fun of. The Senators made bad puns about it, ridiculed it and laughed over it. The fun lasted half an hour. Senator Hubbell said that it was not meet for the Senate, which was costing the State $250 a day, to waste its time in such frivolity. He said that in reading the leading newspapers of Chicago and the East, he found that the Indiana State Legislature had laid itself open to ridicule by the action already taken on the bill. He thought consideration of such a proposition was not dignified or worthy of the Senate. He moved the indefinite postponement of the bill, and the motion carried.\n\nAlthough the bill has become known as the \"Pi Bill\", its text does not mention the name \"pi\" at all, and Goodwin appears to have thought of the ratio between the circumference and diameter of a circle as distinctly secondary to his main aim of squaring the circle. Towards the end of Section 2 the following passage appears:\n\nThis comes close to an explicit claim that formula_1, and that formula_2.\n\nThis quotation is often read as three mutually incompatible assertions, but they fit together well if the statement about is taken to be about the inscribed square (with the circle's diameter as diagonal) rather than the square on the radius (with the chord of 90° as diagonal). Together they describe the circle shown in the figure, whose diameter is 10 and circumference is 32; the chord of 90° is taken to be 7. Both of the values 7 and 32 are within a few percent of the true lengths for a diameter-10 circle (which does not justify Goodwin's presentation of them as exact). The circumference should be nearer to 31.4159 and the diagonal \"7\" should be the square root of 50 (=25+25), or nearer to 7.071.\n\nGoodwin's main goal was not to measure lengths in the circle but to \"square\" it, which he interpreted literally as finding a square with the same area as the circle. He knew that Archimedes' formula for the area of a circle, which calls for multiplying the diameter by one fourth of the circumference, is not considered a solution to the ancient problem of squaring the circle. This is because the problem is to \"construct\" the area using compass and straightedge only, and Archimedes did not give a method for constructing a straight line with the same length as the circumference. Apparently, Goodwin was unaware of this central requirement; he believed that the problem with the Archimedean formula is that it gives wrong numerical results, and that a solution of the ancient problem should consist of replacing it with a \"correct\" formula. In the bill he proposed, without argument, his own method:\n\nThis appears needlessly convoluted, as an \"equilateral rectangle\" is, by definition, a square. In simple terms, the assertion is that the area of a circle is the same as that of a square with the same perimeter. This claim results in other mathematical contradictions to which Goodwin attempts to respond. For example, right after the above quote the bill goes on to say:\n\nIn the model circle above, the Archimedean area (accepting Goodwin's values for the circumference and diameter) would be 80, whereas Goodwin's proposed rule leads to an area of 64. Now, 80 exceeds 64 by one fifth \"of 80\", and Goodwin appears to confuse 64 = 80×(1−) with 80 = 64×(1+), an approximation that works only for fractions much smaller than .\n\nThe area found by Goodwin's rule is times the true area of the circle, which in many accounts of the Pi Bill is interpreted as a claim that = 4. However, there is no internal evidence in the bill that Goodwin intended to make such a claim; on the contrary, he repeatedly denies that the area of the circle has anything to do with its diameter.\n\nThe relative \"area\" error of 1− works out to about 21 percent, which is much more grave than the approximations of the \"lengths\" in the model circle of the previous section. It is unknown what made Goodwin believe that his rule could be correct. In general, figures with identical perimeters do not have identical area (see isoperimetry); the typical demonstration of this fact is to compare a long thin shape with a small enclosed area (the area approaching zero as the width decreases) to one of the same perimeter that is approximately as tall as it is wide (the area approaching the square of the width), obviously of much greater area.\n\n\n"}
{"id": "14907", "url": "https://en.wikipedia.org/wiki?curid=14907", "title": "Inverse function", "text": "Inverse function\n\nIn mathematics, an inverse function (or anti-function) is a function that \"reverses\" another function: if the function applied to an input gives a result of , then applying its inverse function to gives the result , and vice versa, i.e., if and only if .\n\nAs an example, consider the real-valued function of a real variable given by . Thinking of this as a step-by-step procedure (namely, take a number , multiply it by 5, then subtract 7 from the result), to reverse this and get back from some output value, say , we should undo each step in reverse order. In this case that means that we should add 7 to and then divide the result by 5. In functional notation this inverse function would be given by,\nWith we have that and .\n\nNot all functions have inverse functions. In order for a function to have an inverse, it must have the property that for every in there must be one, and only one in so that . This property ensures that a function will exist having the necessary relationship with .\n\nLet be a function whose domain is the set , and whose image (range) is the set . Then is \"invertible\" if there exists a function with domain and image , with the property:\n\nIf is invertible, the function is unique, which means that there is exactly one function satisfying this property (no more, no less). That function is then called \"the\" inverse of , and is usually denoted as .\n\nStated otherwise, a function, considered as a binary relation, has an inverse if and only if the converse relation is a function on the range , in which case the converse relation is the inverse function.\n\nNot all functions have an inverse. For a function to have an inverse, each element must correspond to no more than one ; a function with this property is called one-to-one or an injection. If is to be a function on , then each element must correspond to some . Functions with this property are called surjections. This property is satisfied by definition if is the image (range) of , but may not hold in a more general context. To be invertible a function must be both an injection and a surjection. Such functions are called bijections. The inverse of an injection that is not a bijection, that is, a function that is not a surjection, is only a partial function on , which means that for some , is undefined. If a function is invertible, then both it and its inverse function are bijections.\n\nThere is another convention used in the definition of functions. This can be referred to as the \"set-theoretic\" or \"graph\" definition using ordered pairs in which a codomain is never referred to. Under this convention all functions are surjections, and so, being a bijection simply means being an injection. Authors using this convention may use the phrasing that a function is invertible if and only if it is an injection. The two conventions need not cause confusion as long as it is remembered that in this alternate convention the codomain of a function is always taken to be the range of the function.\n\nThe function given by is not injective since each possible result \"y\" (except 0) corresponds to two different starting points in – one positive and one negative, and so this function is not invertible. With this type of function it is impossible to deduce an input from its output. Such a function is called non-injective or, in some applications, information-losing.\n\nIf the domain of the function is restricted to the nonnegative reals, that is, the function is redefined to be with the same \"rule\" as before, then the function is bijective and so, invertible. The inverse function here is called the \"(positive) square root function\".\nIf is an invertible function with domain and range , then\n\nUsing the composition of functions we can rewrite this statement as follows:\n\nwhere is the identity function on the set ; that is, the function that leaves its argument unchanged. In category theory, this statement is used as the definition of an inverse morphism.\n\nConsidering function composition helps to understand the notation . Repeatedly composing a function with itself is called iteration. If is applied times, starting with the value , then this is written as ; so , etc. Since , composing and yields , \"undoing\" the effect of one application of .\n\nWhile the notation might be misunderstood, certainly denotes the multiplicative inverse of and has nothing to do with the inverse function of .\n\nIn keeping with the general notation, some English authors use expressions like to denote the inverse of the sine function applied to (actually a partial inverse; see below) Other authors feel that this may be confused with the notation for the multiplicative inverse of , which can be denoted as . To avoid any confusion, an inverse trigonometric function is often indicated by the prefix \"arc\" (for Latin \"arcus\"). For instance, the inverse of the sine function is typically called the arcsine function, written as . Similarly, the inverse of a hyperbolic function is indicated by the prefix \"ar\" (for Latin \"area\"). For instance, the inverse of the hyperbolic sine function is typically written as . Other inverse special functions are sometimes prefixed with the prefix \"inv\" if the ambiguity of the notation should be avoided.\n\nSince a function is a special type of binary relation, many of the properties of an inverse function correspond to properties of converse relations.\n\nIf an inverse function exists for a given function , then it is unique. This follows since the inverse function must be the converse relation which is completely determined by .\n\nThere is a symmetry between a function and its inverse. Specifically, if is an invertible function with domain and range , then its inverse has domain and range , and the inverse of is the original function . In symbols, for functions and ,\n\nThis statement is a consequence of the implication that for to be invertible it must be bijective. The involutory nature of the inverse can be concisely expressed by \nThe inverse of a composition of functions is given by \nNotice that the order of and have been reversed; to undo followed by , we must first undo and then undo .\n\nFor example, let and let . Then the composition is the function that first multiplies by three and then adds five,\nTo reverse this process, we must first subtract five, and then divide by three,\nThis is the composition\n\nIf is a set, then the identity function on is its own inverse:\n\nMore generally, a function is equal to its own inverse if and only if the composition is equal to . Such a function is called an involution.\n\nSingle-variable calculus is primarily concerned with functions that map real numbers to real numbers. Such functions are often defined through formulas, such as:\nA surjective function from the real numbers to the real numbers possesses an inverse as long as it is one-to-one, i.e. as long as the graph of has, for each possible value only one corresponding value, and thus passes the horizontal line test.\n\nThe following table shows several standard functions and their inverses:\n\nOne approach to finding a formula for , if it exists, is to solve the equation for . For example, if is the function\n\nthen we must solve the equation for :\n\nThus the inverse function is given by the formula\n\nSometimes the inverse of a function cannot be expressed by a formula with a finite number of terms. For example, if is the function\n\nthen is a bijection, and therefore possesses an inverse function . The formula for this inverse has an infinite number of terms:\n\nIf is invertible, then the graph of the function\n\nis the same as the graph of the equation\n\nThis is identical to the equation that defines the graph of , except that the roles of and have been reversed. Thus the graph of can be obtained from the graph of by switching the positions of the and axes. This is equivalent to reflecting the graph across the line\n\nA continuous function is invertible on its range (image) if and only if it is either strictly increasing or decreasing (with no local maxima or minima). For example, the function\n\nis invertible, since the derivative\n\nIf the function is differentiable on an interval and for each , then the inverse will be differentiable on . If , the derivative of the inverse is given by the inverse function theorem,\nUsing Leibniz's notation the formula above can be written as\nThis result follows from the chain rule (see the article on inverse functions and differentiation).\n\nThe inverse function theorem can be generalized to functions of several variables. Specifically, a differentiable multivariable function is invertible in a neighborhood of a point as long as the Jacobian matrix of at is invertible. In this case, the Jacobian of at is the matrix inverse of the Jacobian of at .\n\n\n\n\nEven if a function is not one-to-one, it may be possible to define a partial inverse of by restricting the domain. For example, the function\n\nis not one-to-one, since . However, the function becomes one-to-one if we restrict to the domain , in which case\n\n(If we instead restrict to the domain , then the inverse is the negative of the square root of .) Alternatively, there is no need to restrict the domain if we are content with the inverse being a multivalued function:\n\nSometimes this multivalued inverse is called the full inverse of , and the portions (such as and −) are called \"branches\". The most important branch of a multivalued function (e.g. the positive square root) is called the \"principal branch\", and its value at is called the \"principal value\" of .\n\nFor a continuous function on the real line, one branch is required between each pair of local extrema. For example, the inverse of a cubic function with a local maximum and a local minimum has three branches (see the adjacent picture).\nThese considerations are particularly important for defining the inverses of trigonometric functions. For example, the sine function is not one-to-one, since\n\nfor every real (and more generally for every integer ). However, the sine is one-to-one on the interval\n, and the corresponding partial inverse is called the arcsine. This is considered the principal branch of the inverse sine, so the principal value of the inverse sine is always between − and . The following table describes the principal branch of each inverse trigonometric function:\n\nIf , a left inverse for (or \"retraction\" of ) is a function such that\n\nThat is, the function satisfies the rule\n\nThus, must equal the inverse of on the image of , but may take any values for elements of not in the image. A function with a left inverse is necessarily injective. In classical mathematics, every injective function with a nonempty domain necessarily has a left inverse; however, this may fail in constructive mathematics. For instance, a left inverse of the inclusion of the two-element set in the reals violates indecomposability by giving a retraction of the real line to the set .\n\nA right inverse for (or \"section\" of ) is a function such that\n\nThat is, the function satisfies the rule\n\nThus, may be any of the elements of that map to under . A function has a right inverse if and only if it is surjective (though constructing such an inverse in general requires the axiom of choice).\n\nAn inverse which is both a left and right inverse must be unique. However, if is a left inverse for , then may or may not be a right inverse for ; and if is a right inverse for , then is not necessarily a left inverse for . For example, let denote the squaring map, such that for all in , and let denote the square root map, such that for all . Then for all in ; that is, is a right inverse to . However, is not a left inverse to , since, e.g., .\n\nIf is any function (not necessarily invertible), the preimage (or inverse image) of an element is the set of all elements of that map to :\n\nThe preimage of can be thought of as the image of under the (multivalued) full inverse of the function .\n\nSimilarly, if is any subset of , the preimage of is the set of all elements of that map to :\n\nFor example, take a function , where . This function is not invertible for reasons discussed . Yet preimages may be defined for subsets of the codomain:\n\nThe preimage of a single element – a singleton set – is sometimes called the \"fiber\" of . When is the set of real numbers, it is common to refer to as a \"level set\".\n\n\n\n\n"}
{"id": "617522", "url": "https://en.wikipedia.org/wiki?curid=617522", "title": "Issai Schur", "text": "Issai Schur\n\nIssai Schur (January 10, 1875 – January 10, 1941) was a Russian mathematician who worked in Germany for most of his life. He studied at the University of Berlin. He obtained his doctorate in 1901, became lecturer in 1903 and, after a stay at the University of Bonn, professor in 1919.\n\nAs a student of Ferdinand Georg Frobenius, he worked on group representations (the subject with which he is most closely associated), but also in combinatorics and number theory and even theoretical physics. He is perhaps best known today for his result on the existence of the Schur decomposition and for his work on group representations (Schur's lemma).\n\nSchur published under the name of both I. Schur, and J. Schur, the latter especially in \"Journal für die reine und angewandte Mathematik\". This has led to some confusion.\n\nIssai Schur was born into a Jewish family, the son of the businessman Moses Schur and his wife Golde Schur (née Landau). He was born in Mogilev on the Dnieper River in what was then the Russian Empire. Schur used the name \"Schaia \"rather than \"Issai\" up in his middle twenties. Schur's father may have been a wholesale merchant.\n\nIn 1888, at the age of 13, Schur went to Liepāja (Courland, now in Latvia), where his married sister and his brother lived, 640 km north-west of Mogilev. Kurland was one of the three Baltic governorates of Tsarist Russia, and since the Middle Ages the Baltic Germans were the trend-setting social class. The local Jewish community spoke mostly German and not Yiddish.\n\nSchur attended the German-speaking Nicolai Gymnasium in Libau from 1888–1894 and reached the top grade in his final examination, and received a gold medal. Here he became fluent in German.\n\nIn October 1894, Schur attended the University of Berlin, with concentration in mathematics and physics. In 1901, he graduated \"summa cum laude\" under Frobenius and Lazarus Immanuel Fuchs with his dissertation \"On a class of matrices that can be assigned to a given matrix\", which contains a general theory of the representation of linear groups. According to Vogt, he began to use the name \"Issai\" at this time. Schur thought that his chance of success in the Russian Empire was rather poor, and because he spoke German so perfectly, he remained in Berlin. He graduated in 1903 and was a lecturer at the University of Berlin. Schur held a position as professor at the Berlin University for the ten years from 1903 to 1913.\n\nIn 1913 he accepted an appointment as associate professor and successor of Felix Hausdorff at the University of Bonn. In the following years Frobenius tried various ways to get Schur back to Berlin. Among other things, Schur's name was mentioned in a letter dated June 27, 1913 from Frobenius to Robert Gnehm (the School Board President of the ETH) as a possible successor to Carl Friedrich Geiser. Frobenius complained that they had never followed his advice before and then said: \"That is why I can't even recommend Prof. J. Schur (now in Bonn) to you. He's too good for Zurich, and should be my successor in Berlin\". Hermann Weyl got the job in Zurich. The efforts of Frobenius were finally successful in 1916, when Schur succeeded Johannes Knoblauch as adjunct professor. Frobenius died a year later, on August 3, 1917. Schur and Carathéodory were both named as the frontrunners for his successor. But they chose Constantin Carathéodory in the end. In 1919 Schur finally received a personal professorship, and in 1921 he took over the chair of the retired Friedrich Hermann Schottky. In 1922, he was also added to the Prussian Academy of Sciences.\n\nAfter the takeover by the National Socialists and the elimination of the parliamentary opposition, the Law for the Restoration of the Professional Civil Service on 7 April 1933, prescribed the release of all distinguished public servants that held unpopular political opinions or who were \"Jewish\" in origin; a subsequent regulation extended this to professors and therefore also to Schur. Schur was suspended and excluded from the university system. His colleague Erhard Schmidt fought for his reinstatement, and since Schur had been a Prussian official before the First World War, he was allowed to participate in certain special lectures on teaching in the winter semester of 1933/1934 again. Schur withdrew his application for leave from the Science Minister and passed up the offer of a visiting professorship at the University of Wisconsin-Madison for the academic year 1933–34. One element that likely played a role in the rejection of the offer was that Schur no longer felt he could cope with the requirements that would have come with a new beginning in an English-speaking environment.\n\nAlready in 1932, Schur's daughter Hilde had married the doctor Chaim Abelin in Bern. As a result, Issai Schur visited his daughter in Bern several times. In Zurich he met often with George Pólya, with whom he was on friendly terms since before the First World War.\n\nOn such a trip to Switzerland in the summer of 1935, a letter reached Schur from Ludwig Bieberbach signed on behalf of the Rector's, stating that Schur should urgently seek him out in the University of Berlin. They needed to discuss an important matter with him. It involved Schur's dismissal on September 30, 1935.\n\nSchur remained a member of the Prussian Academy of Sciences after his release as a professor, but a little later he lost this last remnant of his official position. Due to an intervention from Bieberbach in the spring of 1938 he was forced to explain his resignation from the commission of the Academy. His membership in the Advisory Board of the Mathematical Journal was ended in early 1939.\n\nSchur found himself lonely after the flight of many of his students and the expulsion of renowned scientists from his previous place of work. Only Dr. Helmut Grunsky had been friendly to him, as Schur reported in the late thirties to his expatriate student Max Menachem Schiffer. The Gestapo was everywhere. Since Schur had announced to his wife his intentions to commit suicide in case of a summons to the Gestapo, in the summer of 1938 his wife took his letters, and with them a summons from the Gestapo, sent Issai Schur to a relaxing stay in a home outside of Berlin and went with medical certificate allowing her to meet the Gestapo in place of her husband. There they flatly asked why they were still staying in Germany. But there were economic obstacles to the planned emigration: emigrating Germans had a pre-departure Reich Flight Tax to pay, which was a quarter of their assets. Now Schur's wife had inherited a mortgage on a house in Lithuania, which because of the Lithuanian foreign exchange determination could not be repaid. On the other hand, Schur was forbidden to default or leave the mortgage to the German Reich. Thus the Schurs lacked cash and cash equivalents. Finally, the missing sum of money was somehow supplied, and to this day it does not seem to be clear who were the donors.\n\nSchur was able to leave Germany in early 1939. His health, however, was already severely compromised. He traveled in the company of a nurse to his daughter in Bern, where his wife also followed a few days later. There they remained for several weeks and then emigrated to Palestine. Two years later, on his 66th birthday, on January 10, 1941, he died in Tel Aviv of a heart attack.\n\nSchur continued the work of his teacher Frobenius with many important works for group theory and representation theory. In addition, he published important results and elegant proofs of known results in almost all branches of classical algebra and number theory. His collected works are an impressive proof of this. There, his work on the theory of integral equations and infinite series can be found.\n\nIn his doctoral thesis \"Über eine Klasse von Matrizen, die sich einer gegebenen Matrix zuordnen lassen\" Issai Schur determined the polynomial representations of the general linear group formula_1 on the field formula_2 of complex numbers. The results and methods of this work are still relevant today. In his book, J.A. Green determined the polynomial representations of formula_3 over infinite fields formula_4 with arbitrary characteristic. It is mainly based on Schur's dissertation. Green writes, \"This remarkable work (of Schur) contained many very original ideas, developed with superb algebraic skill. Schur showed that these (polynomial) representations are completely reducible, that each one is \"homogeneous\" of some degree formula_5, and that es equivalence types of irreducible polynomial representations of formula_6, of fixed homogeneous degree formula_7, are in one-to-one correspondence with the partitions formula_8 of formula_7 into not more than formula_10 parts. Moreover, Schur showed that the character of an irreducible representation of type formula_11 is given by a certain symmetric function formula_12 in formula_10 variables, since described as a Schur function.\" According to Green, the methods of Schur's dissertation today are important for the theory of algebraic groups.\n\nIn 1927 Schur, in his work \"On the rational representations of the general linear group\", gave new proofs for the main results of his dissertation. If formula_14 is the natural formula_10-dimensional formula_2 vector space on which formula_1 operates, and if formula_7 is a natural number, then the formula_7-fold tensor product formula_20 over formula_2 is a formula_22-module, on which the symmetric group formula_23 of degree formula_7 also operates by permutation of the tensor factors of each generator formula_25 of formula_20. By exploiting these formula_27-bimodule actions on formula_20, Schur manages to find elegant proofs of his sentences. This work of Schur was once very well known.\n\nSchur lived in Berlin as a highly respected member of the academic world, an apolitical scholar. A leading mathematician and outstanding and very successful teacher, he held a prestigious chair at the University of Berlin for 16 years. Until 1933, his research group had an excellent reputation at the University of Berlin in Germany and beyond. With Schur in the center, his faculty worked with representation theory, which was extended by his students in different directions (including solvable groups, combinatorics, matrix theory). Schur made fundamental contributions to algebra and group theory which, according to Hermann Weyl, were comparable in scope and depth to those of Emmy Amalie Noether (1882–1935).\n\nWhen Schur's lectures were canceled in 1933, there was an outcry among the students and professors who appreciated him and liked him. By the efforts of his colleagues Erhard Schmidt Schur was allowed to continue lecturing until the end of September 1935 for the time being. Schur was the last Jewish professor who lost his job at this time.\n\nIn Switzerland, Schur's colleagues Heinz Hopf and George Pólya were informed of the dismissal of Issai Schur in 1935. They tried to help as best they could. On behalf of the Mathematical Seminars chief Michel Plancherel, on December 12, 1935 the school board president Arthur Rohn invited Isay Schur to \"une série de conférences sur la théorie de la représentation des groupes finis\". At the same time he asked that the formal invitation should come from President Rohn, \"comme le prof. Schur doit obtenir l'autorisation du ministère compétent de donner ces conférences\". George Pólya arranged from this invitation of the Mathematical Seminars the Conference of the Department of Mathematics and Physics on December 16. Meanwhile, on 14 December the official invitation letter from President Rohn had already been dispatched to Issai Schur. Schur was promised for his guest lecture a fee of CHF 500.\n\nSchur did not reply until January 28, 1936, on which day he was first in the possession of the required approval of the local authority. He declared himself willing to accept the invitation. He envisaged beginning the lecture on 4 February. Schur spent most of the month of February in Switzerland. Before his return to Germany he visited his daughter in Bern for a few days, and on 27 February he returned via Karlsruhe, where his sister lived, to Berlin. In a letter to Pólya from Berne, he concludes with the words: \"From Switzerland I take farewell with a heavy heart\".\n\nIn Berlin, meanwhile, Ludwig Bieberbach, in a letter dated February 20, 1936, informed the Reich Minister for Science, Art, and Education on the journey of Schur, and announced that he wanted to find out what was the content of the lecture in Zurich.\n\nSchur had a total of 26 graduate students, some of whom reached mathematical reputation. Among them are\n\n\nAmong others, the following concepts are named after Issai Schur:\nIn his commemorative speech, Alfred Brauer (PhD candidate Schur) spoke about Issai Schur as follows: \"As a teacher, Schur was excellent. His lectures were very clear, but not always easy and required cooperation – During the winter semester of 1930, the number of students who wanted to attend Schur's theory of numbers lecture, was such that the second largest university lecture hall with about 500 seats was too small. His most human characteristics were probably his great modesty, his helpfulness and his human interest in his students.\"\n\nHeinz Hopf, who had been in Berlin before his appointment to Zurich at the ETH Privatdozent, held – as is clear from oral statements and also from letters – Issai Schur as a mathematician and greatly appreciated man. Here, this appreciation was based entirely on reciprocity: in a letter of 1930 to George Pólya on the occasion of the re-appointment of Hermann Weyl, Schur says of Hopf: \"Hopf is a very excellent teacher, a mathematician of strong temperament and strong effect, a master's discipline, trained excellent in other areas. – If I have to characterize him as a man, it may suffice if I say that I sincerely look forward to each time I meet with him\".\n\nSchur was, however, known for putting a correct distance in personal affairs. The testimony of Hopf is in accordance with statements of Schur's former students in Berlin, by Walter Ledermann and Bernhard Neumann.\n\n\n\n"}
{"id": "43592", "url": "https://en.wikipedia.org/wiki?curid=43592", "title": "John Herschel", "text": "John Herschel\n\nSir John Frederick William Herschel, 1st Baronet (; 7 March 1792 – 11 May 1871) was an English polymath, mathematician, astronomer, chemist, inventor, experimental photographer who invented the blueprint, and did botanical work.\n\nHerschel originated the use of the Julian day system in astronomy. He named seven moons of Saturn and four moons of Uranus. He made many contributions to the science of photography, and investigated colour blindness and the chemical power of ultraviolet rays; his \"Preliminary Discourse\" (1831), which advocated an inductive approach to scientific experiment and theory building, was an important contribution to the philosophy of science.\n\nHerschel was born in Slough, Buckinghamshire, the son of Mary Baldwin and William Herschel. He was the nephew of astronomer Caroline Herschel. He studied shortly at Eton College and St John's College, Cambridge, graduating as Senior Wrangler in 1813. It was during his time as an undergraduate that he became friends with the mathematicians Charles Babbage and George Peacock. He left Cambridge in 1816 and started working with his father. He took up astronomy in 1816, building a reflecting telescope with a mirror in diameter, and with a focal length. Between 1821 and 1823 he re-examined, with James South, the double stars catalogued by his father. He was one of the founders of the Royal Astronomical Society in 1820. For his work with his father, he was presented with the Gold Medal of the Royal Astronomical Society in 1826 (which he won again in 1836), and with the Lalande Medal of the French Academy of Sciences in 1825, while in 1821 the Royal Society bestowed upon him the Copley Medal for his mathematical contributions to their Transactions. Herschel was made a Knight of the Royal Guelphic Order in 1831.\n\nHe served as President of the Royal Astronomical Society three times: 1827–29, 1839–41 and 1847–49.\n\nHis \"A preliminary discourse on the study of natural philosophy\", published early in 1831 as part of \"Dionysius Lardner's Cabinet cyclopædia\", set out methods of scientific investigation with an orderly relationship between observation and theorising. He described nature as being governed by laws which were difficult to discern or to state mathematically, and the highest aim of natural philosophy was understanding these laws through inductive reasoning, finding a single unifying explanation for a phenomenon. This became an authoritative statement with wide influence on science, particularly at the University of Cambridge where it inspired the student Charles Darwin with \"a burning zeal\" to contribute to this work.\n\nHerschel published a catalogue of his astronomical observations in 1864, as the \"General Catalogue of Nebulae and Clusters\", a compilation of his own work and that of his father's, expanding on the senior Herschel's \"Catalogue of Nebulae\". A further complementary volume was published posthumously, as the \"General Catalogue of 10,300 Multiple and Double Stars\".\n\nHerschel correctly considered astigmatism to be due to irregularity of the cornea and theorised that vision could be improved by the application of some animal jelly contained in a capsule of glass against the cornea. His views were published in an article entitled Light in 1828 and the \"Encyclopædia Metropolitana\" in 1845.\n\nHe discovered; NGC 7, NGC 10, NGC 25, NGC 28\n\nDeclining an offer from the Duke of Sussex that they travel to South Africa on a Navy ship, Herschel and his wife paid £500 for passage on the S.S. \"Mountstuart Elphinstone\", which departed from Portsmouth on 13 November 1833.\n\nThe voyage to South Africa was made in order to catalogue the stars, nebulae, and other objects of the southern skies. This was to be a completion as well as extension of the survey of the northern heavens undertaken initially by his father William Herschel. He arrived in Cape Town on 15 January 1834 and set up a private telescope at Feldhausen at Claremont, a suburb of Cape Town. Amongst his other observations during this time was that of the return of Comet Halley. Herschel collaborated with Thomas Maclear, the Astronomer Royal at the Cape of Good Hope and the members of the two families became close friends. During this time, he also witnessed the Great Eruption of Eta Carinae (December, 1837).\n\nIn addition to his astronomical work, however, this voyage to a far corner of the British empire also gave Herschel an escape from the pressures under which he found himself in London, where he was one of the most sought-after of all British men of science. While in southern Africa, he engaged in a broad variety of scientific pursuits free from a sense of strong obligations to a larger scientific community. It was, he later recalled, probably the happiest time in his life.\n\nIn an extraordinary departure from astronomy, he combined his talents with those of his wife, Margaret, and between 1834 and 1838 they produced 131 botanical illustrations of fine quality, showing the Cape flora. Herschel used a camera lucida to obtain accurate outlines of the specimens and left the details to his wife. Even though their portfolio had been intended as a personal record, and despite the lack of floral dissections in the paintings, their accurate rendition makes them more valuable than many contemporary collections. Some 112 of the 132 known flower studies were collected and published as \"Flora Herscheliana\" in 1996.\n\nAs their home during their stay in the Cape, the Herschels had selected 'Feldhausen' (\"Field Houses\"), an old estate on the south-eastern side of Table Mountain. Here John set up his reflector to begin his survey of the southern skies.\n\nHerschel, at the same time, read widely. Intrigued by the ideas of gradual formation of landscapes set out in Charles Lyell's \"Principles of Geology\", he wrote to Lyell on 20 February 1836 praising the book as a work that would bring \"a complete revolution in [its] subject, by altering entirely the point of view in which it must thenceforward be contemplated\" and opening a way for bold speculation on \"that mystery of mysteries, the replacement of extinct species by others.\" Herschel himself thought catastrophic extinction and renewal \"an inadequate conception of the Creator\" and by analogy with other intermediate causes, \"the origination of fresh species, could it ever come under our cognizance, would be found to be a natural in contradistinction to a miraculous process\". He prefaced his words with the couplet:\n\nTaking a gradualist view of development and referring to the evolution of language, he commented: \n\nThe document was circulated, and Charles Babbage incorporated extracts in his ninth and unofficial \"Bridgewater Treatise\", which postulated laws set up by a divine programmer. When HMS \"Beagle\" called at Cape Town, Captain Robert FitzRoy and the young naturalist Charles Darwin visited Herschel on 3 June 1836. Later on, Darwin would be influenced by Herschel's writings in developing his theory advanced in \"The Origin of Species\". In the opening lines of that work, Darwin writes that his intent is \"to throw some light on the origin of species — that mystery of mysteries, as it has been called by one of our greatest philosophers,\" referring to Herschel. However, Herschel ultimately rejected the theory of natural selection.\n\nHerschel returned to England in 1838, was created a baronet, of Slough in the County of Buckingham, and published \"Results of Astronomical Observations made at the Cape of Good Hope\" in 1847. In this publication he proposed the names still used today for the seven then-known satellites of Saturn: Mimas, Enceladus, Tethys, Dione, Rhea, Titan, and Iapetus. In the same year, Herschel received his second Copley Medal from the Royal Society for this work. A few years later, in 1852, he proposed the names still used today for the four then-known satellites of Uranus: Ariel, Umbriel, Titania, and Oberon.\n\nHerschel made numerous important contributions to photography. He made improvements in photographic processes, particularly in inventing the cyanotype process, which became known as blueprints. and variations, such as the chrysotype. In 1839, he made a photograph on glass, which still exists, and experimented with some color reproduction, noting that rays of different parts of the spectrum tended to impart their own color to a photographic paper. Herschel made experiments using photosensitive emulsions of vegetable juices, called phytotypes, also known as anthotypes, and published his discoveries in the Philosophical Transactions of the Royal Society of London in 1842. He collaborated in the early 1840s with Henry Collen, portrait painter to Queen Victoria. Herschel originally discovered the platinum process on the basis of the light sensitivity of platinum salts, later developed by William Willis.\n\nHerschel coined the term \"photography\" in 1839. Herschel was also the first to apply the terms \"negative\" and \"positive\" to photography.\n\nHe discovered sodium thiosulfate to be a solvent of silver halides in 1819, and informed Talbot and Daguerre of his discovery that this \"hyposulphite of soda\" (\"hypo\") could be used as a photographic fixer, to \"fix\" pictures and make them permanent, after experimentally applying it thus in early 1839.\n\nHis ground-breaking research on the subject was read at the Royal Society in London in March 1839 and January 1840.\n\nHerschel wrote many papers and articles, including entries on meteorology, physical geography and the telescope for the eighth edition of the \"Encyclopædia Britannica\". He also translated the \"Iliad\" of Homer.\nHe invented the actinometer in 1825 to measure the direct heating power of the sun's rays, and his work with the instrument is of great importance in the early history of photochemistry.\n\nHe proposed a correction to the Gregorian calendar, making years that are multiples of 4000 not leap years, thus reducing the average length of the calendar year from 365.2425 days to 365.24225. Although this is closer to the mean tropical year of 365.24219 days, his proposal has never been adopted because the Gregorian calendar is based on the mean time between vernal equinoxes (currently days).\n\nHerschel was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832, and in 1836, a foreign member of the Royal Swedish Academy of Sciences.\n\nIn 1835, the \"New York Sun\" newspaper wrote a series of satiric articles that came to be known as the Great Moon Hoax, with statements falsely attributed to Herschel about his supposed discoveries of animals living on the Moon, including batlike winged humanoids.\n\nThe village of Herschel in western Saskatchewan Canada, Mount Herschel Antarctica, the crater J. Herschel on the Moon, and the Herschel Girls' School in Cape Town South Africa, are all named after him.\n\nWhile it is commonly accepted that Herschel Island, in the Arctic Ocean, part of the Yukon Territory, was named after him, the entries in the expedition journal of Sir John Franklin state that the latter wished to honour the Herschel family, of which John Herschel's father, Sir William Herschel, and his aunt, Caroline Herschel, are as notable as John.\n\nHe married his cousin Margaret Brodie Stewart (1810–1884) on 3 March 1829 at Edinburgh and was father of the following children:\n\n\nHerschel died on 11 May 1871 at age 79 at Collingwood, his home near Hawkhurst in Kent. On his death, he was given a national funeral and buried in Westminster Abbey. His obituary by Henry W Field of London was read to the American Philosophical Society on 1 December 1871.\n\n\n\n"}
{"id": "312877", "url": "https://en.wikipedia.org/wiki?curid=312877", "title": "Jordan normal form", "text": "Jordan normal form\n\nIn linear algebra, a Jordan normal form (often called Jordan canonical form)\nof a linear operator on a finite-dimensional vector space is an upper triangular matrix of a particular form called a Jordan matrix, representing the operator with respect to some basis. Such a matrix has each non-zero off-diagonal entry equal to 1, immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them.\n\nLet \"V\" be a vector space over a field \"K\". Then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in \"K\", or equivalently if the characteristic polynomial of the operator splits into linear factors over \"K\". This condition is always satisfied if \"K\" is algebraically closed (for instance, if it is the field of complex numbers). The diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue.\n\nIf the operator is originally given by a square matrix \"M\", then its Jordan normal form is also called the Jordan normal form of \"M\". Any square matrix has a Jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. In spite of its name, the normal form for a given \"M\" is not entirely unique, as it is a block diagonal matrix formed of Jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size.\n\nThe Jordan–Chevalley decomposition is particularly simple with respect to a basis for which the operator takes its Jordan normal form. The diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the Jordan normal form.\n\nThe Jordan normal form is named after Camille Jordan, who first stated the Jordan decomposition theorem in 1870.\n\nSome textbooks have the ones on the subdiagonal, i.e., immediately below the main diagonal instead of on the superdiagonal. The eigenvalues are still on the main diagonal.\n\nAn \"n\" × \"n\" matrix \"A\" is diagonalizable if and only if the sum of the dimensions of the eigenspaces is \"n\". Or, equivalently, if and only if \"A\" has \"n\" linearly independent eigenvectors. Not all matrices are diagonalizable. Consider the following matrix:\n\nformula_1\n\nIncluding multiplicity, the eigenvalues of \"A\" are λ = 1, 2, 4, 4. The dimension of the eigenspace corresponding to the eigenvalue 4 is 1 (and not 2), so \"A\" is not diagonalizable. However, there is an invertible matrix \"P\" such that \"A\" = \"PJP\", where\n\nThe matrix J is almost diagonal. This is the Jordan normal form of \"A\". The section \"Example\" below fills in the details of the computation.\n\nIn general, a square complex matrix \"A\" is similar to a block diagonal matrix\n\nwhere each block \"J\" is a square matrix of the form\n\nSo there exists an invertible matrix \"P\" such that \"PAP\" = \"J\" is such that the only non-zero entries of \"J\" are on the diagonal and the superdiagonal. \"J\" is called the Jordan normal form of \"A\". Each \"J\" is called a Jordan block of \"A\". In a given Jordan block, every entry on the superdiagonal is 1.\n\nAssuming this result, we can deduce the following properties:\n\n\nConsider the matrix \"A\" from the example in the previous section. The Jordan normal form is obtained by some similarity transformation \"P\"\"AP\" = \"J\", i.e.,\n\nLet \"P\" have column vectors \"p\", \"i\" = 1, ..., 4, then\n\nWe see that\n\nFor \"i\" = 1,2,3 we have formula_12, i.e., \"p\" is an eigenvector of \"A\" corresponding to the eigenvalue λ. For \"i\"=4, multiplying both sides by formula_13 gives\nBut formula_15, so\nThus, formula_17\n\nVectors such as formula_18 are called generalized eigenvectors of \"A\".\n\nThus, given an eigenvalue λ, its corresponding Jordan block gives rise to a Jordan chain. The generator, or lead vector, say \"p\", of the chain is a generalized eigenvector such that (\"A\" − λ I)\"p\" = 0, where \"r\" is the size of the Jordan block. The vector \"p\" = (\"A\" − λ I)\"p\" is an eigenvector corresponding to λ. In general, \"p\" is a preimage of \"p\" under \"A\" − λ I. So the lead vector generates the chain via multiplication by (\"A\" − λ I).\n\nTherefore, the statement that every square matrix \"A\" can be put in Jordan normal form is equivalent to the claim that there exists a basis consisting only of eigenvectors and generalized eigenvectors of \"A\".\n\nWe give a proof by induction that any complex-valued matrix A may be put in Jordan normal form. The 1 × 1 case is trivial. Let \"A\" be an \"n\" × \"n\" matrix. Take any eigenvalue λ of \"A\". The range of \"A\" − λ I, denoted by Ran(\"A\" − λ I), is an invariant subspace of \"A\". Also, since λ is an eigenvalue of \"A\", the dimension of Ran(\"A\" − λ I), \"r\", is strictly less than \"n\". Let \"A' \" denote the restriction of \"A\" to Ran(\"A\" − λ I), By inductive hypothesis, there exists a basis {\"p\", ..., \"p\"} such that \"A' \", expressed with respect to this basis, is in Jordan normal form.\n\nNext consider the subspace Ker(\"A\" − λ I). If\n\nthe desired result follows immediately from the rank–nullity theorem. This would be the case, for example, if \"A\" was Hermitian.\n\nOtherwise, if\n\nlet the dimension of \"Q\" be \"s\" ≤ \"r\". Each vector in \"Q\" is an eigenvector of \"A' \" corresponding to eigenvalue \"λ\". So the Jordan form of \"A' \" must contain \"s\" Jordan chains corresponding to \"s\" linearly independent eigenvectors. So the basis {\"p\", ..., \"p\"} must contain \"s\" vectors, say {\"p\", ..., \"p\"}, that are lead vectors in these Jordan chains from the Jordan normal form of \"A. We can \"extend the chains\" by taking the preimages of these lead vectors. (This is the key step of argument; in general, generalized eigenvectors need not lie in Ran(\"A\" − λ I\"').) Let \"q\" be such that\n\nClearly no non-trivial linear combination of the \"q\" can lie in Ker(\"A\" − λ I). Furthermore, no non-trivial linear combination of the \"q\" can be in Ran(\"A\" − λ I), for that would contradict the assumption that each \"p\" is a lead vector in a Jordan chain. The set {\"q\"}, being preimages of the linearly independent set {\"p\"} under \"A\" − λ I, is also linearly independent.\n\nFinally, we can pick any linearly independent set {\"z\", ..., \"z\"} that spans\n\nBy construction, the union of the three sets {\"p\", ..., \"p\"}, {\"q\", ..., \"q\"}, and {\"z\", ..., \"z\"} is linearly independent. Each vector in the union is either an eigenvector or a generalized eigenvector of \"A\". Finally, by the rank–nullity theorem, the cardinality of the union is \"n\". In other words, we have found a basis that consists of eigenvectors and generalized eigenvectors of \"A\", and this shows \"A\" can be put in Jordan normal form.\n\nIt can be shown that the Jordan normal form of a given matrix \"A\" is unique up to the order of the Jordan blocks.\n\nKnowing the algebraic and geometric multiplicities of the eigenvalues is not sufficient to determine the Jordan normal form of \"A\". Assuming the algebraic multiplicity \"m\"(λ) of an eigenvalue λ is known, the structure of the Jordan form can be ascertained by analyzing the ranks of the powers (\"A\" − λ I). To see this, suppose an \"n\" × \"n\" matrix \"A\" has only one eigenvalue λ. So \"m\"(λ) = \"n\". The smallest integer \"k\" such that\n\nis the size of the largest Jordan block in the Jordan form of \"A\". (This number \"k\" is also called the index of λ. See discussion in a following section.) The rank of\n\nis the number of Jordan blocks of size \"k\". Similarly, the rank of\n\nis twice the number of Jordan blocks of size \"k\" plus the number of Jordan blocks of size \"k\"−1. The general case is similar.\n\nThis can be used to show the uniqueness of the Jordan form. Let \"J\" and \"J\" be two Jordan normal forms of \"A\". Then \"J\" and \"J\" are similar and have the same spectrum, including algebraic multiplicities of the eigenvalues. The procedure outlined in the previous paragraph can be used to determine the structure of these matrices. Since the rank of a matrix is preserved by similarity transformation, there is a bijection between the Jordan blocks of \"J\" and \"J\". This proves the uniqueness part of the statement.\n\nIf \"A\" is a real matrix, its Jordan form can still be non-real. Instead of representing it with complex eigenvalues and 1's on the superdiagonal, as discussed above, there exists a real invertible matrix \"P\" such that \"PAP\" = \"J\" is a real block diagonal matrix with each block being a real Jordan block. A real Jordan block is either identical to a complex Jordan block (if the corresponding eigenvalue formula_26 is real), or is a block matrix itself, consisting of 2×2 blocks (for non-real eigenvalue formula_27 with given algebraic multiplicity) of the form\n\nand describe multiplication by formula_26 in the complex plane. The superdiagonal blocks are 2×2 identity matrices and hence in this representation the matrix dimensions are larger than the complex Jordan form. The full real Jordan block is given by\n\nThis real Jordan form is a consequence of the complex Jordan form. For a real matrix the nonreal eigenvectors and generalized eigenvectors can always be chosen to form complex conjugate pairs. Taking the real and imaginary part (linear combination of the vector and its conjugate), the matrix has this form with respect to the new basis.\n\nOne can see that the Jordan normal form is essentially a classification result for square matrices, and as such several important results from linear algebra can be viewed as its consequences.\n\nUsing the Jordan normal form, direct calculation gives a spectral mapping theorem for the polynomial functional calculus: Let \"A\" be an \"n\" × \"n\" matrix with eigenvalues λ, ..., λ, then for any polynomial \"p\", \"p\"(\"A\") has eigenvalues \"p\"(λ), ..., \"p\"(λ).\n\nThe Cayley–Hamilton theorem asserts that every matrix \"A\" satisfies its characteristic equation: if is the characteristic polynomial of , then . This can be shown via direct calculation in the Jordan form, since any Jordan block for is annihilated by where is the multiplicity of the root of , the sum of the sizes of the Jordan blocks for , and therefore no less than the size of the block in question. The Jordan form can be assumed to exist over a field extending the base field of the matrix, for instance over the splitting field of ; this field extension does not change the matrix in any way.\n\nThe minimal polynomial P of a square matrix \"A\" is the unique monic polynomial of least degree, \"m\", such that \"P\"(\"A\") = 0. Alternatively, the set of polynomials that annihilate a given \"A\" form an ideal \"I\" in \"C\"[\"x\"], the principal ideal domain of polynomials with complex coefficients. The monic element that generates \"I\" is precisely \"P\".\n\nLet λ, ..., λ be the distinct eigenvalues of \"A\", and \"s\" be the size of the largest Jordan block corresponding to λ. It is clear from the Jordan normal form that the minimal polynomial of \"A\" has degree \"s\".\n\nWhile the Jordan normal form determines the minimal polynomial, the converse is not true. This leads to the notion of elementary divisors. The elementary divisors of a square matrix \"A\" are the characteristic polynomials of its Jordan blocks. The factors of the minimal polynomial \"m\" are the elementary divisors of the largest degree corresponding to distinct eigenvalues.\n\nThe degree of an elementary divisor is the size of the corresponding Jordan block, therefore the dimension of the corresponding invariant subspace. If all elementary divisors are linear, \"A\" is diagonalizable.\n\nThe Jordan form of a \"n\" × \"n\" matrix \"A\" is block diagonal, and therefore gives a decomposition of the \"n\" dimensional Euclidean space into invariant subspaces of \"A\". Every Jordan block \"J\" corresponds to an invariant subspace \"X\". Symbolically, we put\n\nwhere each \"X\" is the span of the corresponding Jordan chain, and \"k\" is the number of Jordan chains.\n\nOne can also obtain a slightly different decomposition via the Jordan form. Given an eigenvalue λ, the size of its largest corresponding Jordan block \"s\" is called the index of λ and denoted by ν(λ). (Therefore, the degree of the minimal polynomial is the sum of all indices.) Define a subspace \"Y\" by\n\nThis gives the decomposition\n\nwhere \"l\" is the number of distinct eigenvalues of \"A\". Intuitively, we glob together the Jordan block invariant subspaces corresponding to the same eigenvalue. In the extreme case where \"A\" is a multiple of the identity matrix we have \"k\" = \"n\" and \"l\" = 1.\n\nThe projection onto \"Y\" and along all the other \"Y\" ( \"j\" ≠ \"i\" ) is called the spectral projection of \"A\" at λ and is usually denoted by \"P\"(λ ; \"A\"). Spectral projections are mutually orthogonal in the sense that \"P\"(λ ; \"A\") \"P\"(λ ; \"A\") = 0 if \"i\" ≠ \"j\". Also they commute with \"A\" and their sum is the identity matrix. Replacing every λ in the Jordan matrix \"J\" by one and zeroising all other entries gives \"P\"(λ ; \"J\"), moreover if \"U J U\" is the similarity transformation such that \"A\" = \"U J U\" then \"P\"(λ ; \"A\") = \"U P\"(λ ; \"J\") \"U\". They are not confined to finite dimensions. See below for their application to compact operators, and in holomorphic functional calculus for a more general discussion.\n\nComparing the two decompositions, notice that, in general, \"l\" ≤ \"k\". When \"A\" is normal, the subspaces \"X\"'s in the first decomposition are one-dimensional and mutually orthogonal. This is the spectral theorem for normal operators. The second decomposition generalizes more easily for general compact operators on Banach spaces.\n\nIt might be of interest here to note some properties of the index, ν(\"λ\"). More generally, for a complex number λ, its index can be defined as the least non-negative integer ν(λ) such that\n\nSo ν(λ) > 0 if and only if λ is an eigenvalue of \"A\". In the finite-dimensional case, ν(λ) ≤ the algebraic multiplicity of λ.\n\nJordan reduction can be extended to any square matrix \"M\" whose entries lie in a field \"K\". The result states that any \"M\" can be written as a sum \"D\" + \"N\" where \"D\" is semisimple, \"N\" is nilpotent, and \"DN\" = \"ND\". This is called the Jordan–Chevalley decomposition. Whenever \"K\" contains the eigenvalues of \"M\", in particular when \"K\" is algebraically closed, the normal form can be expressed explicitly as the direct sum of Jordan blocks. \nSimilar to the case when \"K\" is the complex numbers, knowing the dimensions of the kernels of (\"M\" − λ\"I\") for 1 ≤ \"k\" ≤ \"m\", where \"m\" is the algebraic multiplicity of the eigenvalue λ, allows one to determine the Jordan form of \"M\". We may view the underlying vector space \"V\" as a \"K\"[\"x\"]-module by regarding the action of \"x\" on \"V\" as application of \"M\" and extending by \"K\"-linearity. Then the polynomials (\"x\" − λ) are the elementary divisors of \"M\", and the Jordan normal form is concerned with representing \"M\" in terms of blocks associated to the elementary divisors.\n\nThe proof of the Jordan normal form is usually carried out as an application to the ring \"K\"[\"x\"] of the structure theorem for finitely generated modules over a principal ideal domain, of which it is a corollary.\n\nIn a different direction, for compact operators on a Banach space, a result analogous to the Jordan normal form holds. One restricts to compact operators because every point \"x\" in the spectrum of a compact operator \"T\", the only exception being when \"x\" is the limit point of the spectrum, is an eigenvalue. This is not true for bounded operators in general. To give some idea of this generalization, we first reformulate the Jordan decomposition in the language of functional analysis.\n\nLet \"X\" be a Banach space, \"L\"(\"X\") be the bounded operators on \"X\", and σ(\"T\") denote the spectrum of \"T\" ∈ \"L\"(\"X\"). The holomorphic functional calculus is defined as follows:\n\nFix a bounded operator \"T\". Consider the family Hol(\"T\") of complex functions that is holomorphic on some open set \"G\" containing σ(\"T\"). Let Γ = {γ} be a finite collection of Jordan curves such that σ(\"T\") lies in the \"inside\" of Γ, we define \"f\"(\"T\") by\n\nThe open set \"G\" could vary with \"f\" and need not be connected. The integral is defined as the limit of the Riemann sums, as in the scalar case. Although the integral makes sense for continuous \"f\", we restrict to holomorphic functions to apply the machinery from classical function theory (e.g., the Cauchy integral formula). The assumption that σ(\"T\") lie in the inside of Γ ensures \"f\"(\"T\") is well defined; it does not depend on the choice of Γ. The functional calculus is the mapping Φ from Hol(\"T\") to \"L\"(\"X\") given by\n\nWe will require the following properties of this functional calculus:\n\nIn the finite-dimensional case, σ(\"T\") = {λ} is a finite discrete set in the complex plane. Let \"e\" be the function that is 1 in some open neighborhood of λ and 0 elsewhere. By property 3 of the functional calculus, the operator\n\nis a projection. Moreoever, let ν be the index of λ and\n\nThe spectral mapping theorem tells us\n\nhas spectrum {0}. By property 1, \"f\"(\"T\") can be directly computed in the Jordan form, and by inspection, we see that the operator \"f\"(\"T\")\"e\"(\"T\") is the zero matrix.\n\nBy property 3, \"f\"(\"T\") \"e\"(\"T\") = \"e\"(\"T\") \"f\"(\"T\"). So \"e\"(\"T\") is precisely the projection onto\nthe subspace\n\nThe relation\n\nimplies\n\nwhere the index \"i\" runs through the distinct eigenvalues of \"T\". This is exactly the invariant subspace decomposition\n\ngiven in a previous section. Each \"e\"(\"T\") is the projection onto the subspace spanned by the Jordan chains corresponding to λ and along the subspaces spanned by the Jordan chains corresponding to λ for \"j\" ≠ \"i\". In other words, \"e\"(\"T\") = \"P\"(λ;\"T\"). This explicit identification of the operators \"e\"(\"T\") in turn gives an explicit form of holomorphic functional calculus for matrices:\n\nNotice that the expression of \"f\"(\"T\") is a finite sum because, on each neighborhood of λ, we have chosen the Taylor series expansion of \"f\" centered at λ.\n\nLet \"T\" be a bounded operator λ be an isolated point of σ(\"T\"). (As stated above, when \"T\" is compact, every point in its spectrum is an isolated point, except possibly the limit point 0.)\n\nThe point λ is called a pole of operator \"T\" with order ν if the resolvent function \"R\" defined by\n\nhas a pole of order ν at λ.\n\nWe will show that, in the finite-dimensional case, the order of an eigenvalue coincides with its index. The result also holds for compact operators.\n\nConsider the annular region \"A\" centered at the eigenvalue λ with sufficiently small radius ε such that the intersection of the open disc \"B\"(λ) and σ(\"T\") is {λ}. The resolvent function \"R\" is holomorphic on \"A\".\nExtending a result from classical function theory, \"R\" has a Laurent series representation on \"A\":\n\nwhere\n\nBy the previous discussion on the functional calculus,\n\nBut we have shown that the smallest positive integer \"m\" such that\n\nis precisely the index of λ, ν(λ). In other words, the function \"R\" has a pole of order ν(λ) at λ.\n\nThis example shows how to calculate the Jordan normal form of a given matrix. As the next section explains, it is important to do the computation exactly instead of rounding the results.\n\nConsider the matrix\nwhich is mentioned in the beginning of the article.\n\nThe characteristic polynomial of \"A\" is\nThis shows that the eigenvalues are 1, 2, 4 and 4, according to algebraic multiplicity. The eigenspace corresponding to the eigenvalue 1 can be found by solving the equation \"Av\" = \"λ v\". It is spanned by the column vector \"v\" = (−1, 1, 0, 0). Similarly, the eigenspace corresponding to the eigenvalue 2 is spanned by \"w\" = (1, −1, 0, 1). Finally, the eigenspace corresponding to the eigenvalue 4 is also one-dimensional (even though this is a double eigenvalue) and is spanned by \"x\" = (1, 0, −1, 1). So, the geometric multiplicity (i.e., the dimension of the eigenspace of the given eigenvalue) of each of the three eigenvalues is one. Therefore, the two eigenvalues equal to 4 correspond to a single Jordan block, and the Jordan normal form of the matrix \"A\" is the direct sum\nThere are three chains. Two have length one: {\"v\"} and {\"w\"}, corresponding to the eigenvalues 1 and 2, respectively. There is one chain of length two corresponding to the eigenvalue 4. To find this chain, calculate\nwhere \"I\" is the 4 x 4 identity matrix. Pick a vector in the above span that is not in the kernel of \"A\" − 4\"I\", e.g., \"y\" = (1,0,0,0). Now, (\"A\" − 4\"I\")\"y\" = \"x\" and (\"A\" − 4\"I\")\"x\" = 0, so {\"y\", \"x\"} is a chain of length two corresponding to the eigenvalue 4.\n\nThe transition matrix \"P\" such that \"P\"\"AP\" = \"J\" is formed by putting these vectors next to each other as follows\nA computation shows that the equation \"P\"\"AP\" = \"J\" indeed holds.\n\nIf we had interchanged the order of which the chain vectors appeared, that is, changing the order of \"v\", \"w\" and {\"x\", \"y\"} together, the Jordan blocks would be interchanged. However, the Jordan forms are equivalent Jordan forms.\n\nIf the matrix \"A\" has multiple eigenvalues, or is close to a matrix with multiple eigenvalues, then its Jordan normal form is very sensitive to perturbations. Consider for instance the matrix\nIf ε = 0, then the Jordan normal form is simply\nHowever, for ε ≠ 0, the Jordan normal form is\nThis ill conditioning makes it very hard to develop a robust numerical algorithm for the Jordan normal form, as the result depends critically on whether two eigenvalues are deemed to be equal. For this reason, the Jordan normal form is usually avoided in numerical analysis; the stable Schur decomposition or pseudospectra are better alternatives.\n\nThe Jordan normal form is the most convenient for computation of the matrix functions (though it may be not the best choice for computer computations). Let \"f(z)\" be an analytical function of a complex argument. Applying the function on a \"n×n\" Jordan block \"J\" with eigenvalue \"λ\" results in an upper triangular matrix:\n\nso that the elements of the \"k\"-th superdiagonal of the resulting matrix are formula_63. For a matrix of general Jordan normal form\nthe above expression shall be applied to each Jordan block.\n\nThe following example shows the application to the power function \"f(z)=z\":\nwhere the binomial coefficients are defined as formula_65. For integer positive \"n\" it reduces to standard definition\nof the coefficients. For negative \"n\" the identity formula_66 may be of use.\n\n\n"}
{"id": "35216099", "url": "https://en.wikipedia.org/wiki?curid=35216099", "title": "Kazuhiko Aomoto", "text": "Kazuhiko Aomoto\n\nKazuhiko Aomoto is a Japanese mathematician who introduced the Aomoto-Gel'fand hypergeometric function and the Aomoto integral.\n\nHe was a professor at Nagoya University. In 1996 he received the Mathematical Society of Japan autumn prize for his research on complex integration.\n"}
{"id": "1091808", "url": "https://en.wikipedia.org/wiki?curid=1091808", "title": "L. T. F. Gamut", "text": "L. T. F. Gamut\n\nL. T. F. Gamut was a collective pseudonym for the Dutch logicians Johan van Benthem, Jeroen Groenendijk, Dick de Jongh, Martin Stokhof and Henk Verkuyl.\n\nGamut stands for the Dutch universities of Groningen (G), Amsterdam (am), and Utrecht (ut), then the affiliations of the authors. The initials L. T. F. stand for the discussed topics, respectively, Logic (Dutch: Logica\"), Language (Dutch: Taal\") and Philosophy (Dutch: \"Filosofie\").\n\n"}
{"id": "19089448", "url": "https://en.wikipedia.org/wiki?curid=19089448", "title": "Laver function", "text": "Laver function\n\nIn set theory, a Laver function (or Laver diamond, named after its inventor, Richard Laver) is a function connected with supercompact cardinals.\n\nIf κ is a supercompact cardinal, a Laver function is a function \"ƒ\":κ → \"V\" such that for every set \"x\" and every cardinal λ ≥ |TC(\"x\")| + κ there is a supercompact measure \"U\" on [λ] such that if \"j\" is the associated elementary embedding then \"j\"(\"ƒ\")(κ) = \"x\". (Here \"V\" denotes the κ-th level of the cumulative hierarchy, TC(\"x\") is the transitive closure of \"x\")\n\nThe original application of Laver functions was the following theorem of Laver. \nIf κ is supercompact, there is a κ-c.c. forcing notion (\"P\", ≤) such after forcing with (\"P\", ≤) the following holds: κ is supercompact and remains supercompact after forcing with any κ-directed closed forcing.\n\nThere are many other applications, for example the proof of the consistency of the proper forcing axiom.\n"}
{"id": "17637", "url": "https://en.wikipedia.org/wiki?curid=17637", "title": "Law of excluded middle", "text": "Law of excluded middle\n\nIn logic, the law of excluded middle (or the principle of excluded middle) states that for any proposition, either that proposition is true or its negation is true. It is the third of the three classic laws of thought.\n\nThe law is also known as the law (or principle) of the excluded third, in Latin principium tertii exclusi. Another Latin designation for this law is tertium non datur: \"no third [possibility] is given\".\n\nThe earliest known formulation is in Aristotle's discussion of the principle of non-contradiction, first proposed in \"On Interpretation,\" where he says that of two contradictory propositions (i.e. where one proposition is the negation of the other) one must be true, and the other false. He also states it as a principle in the \"Metaphysics\" book 3, saying that it is necessary in every case to affirm or deny, and that it is impossible that there should be anything between the two parts of a contradiction. The principle was stated as a theorem of propositional logic by Russell and Whitehead in \"Principia Mathematica\" as:\n\nformula_1.\n\nThe principle should not be confused with the semantical principle of bivalence, which states that every proposition is either true or false.\n\nThe principle of excluded middle, along with its complement, the law of non-contradiction (the second of the three classic laws of thought), are correlates of the law of identity (the first of these laws).\n\nSome systems of logic have different but analogous laws. For some finite \"n\"-valued logics, there is an analogous law called the law of excluded \"n\"+1th. If negation is cyclic and \"∨\" is a \"max operator\", then the law can be expressed in the object language by (P ∨ ~P ∨ ~~P ∨ ... ∨ ~...~P), where \"~...~\" represents \"n\"−1 negation signs and \"∨ ... ∨\" \"n\"−1 disjunction signs. It is easy to check that the sentence must receive at least one of the \"n\" truth values (and not a value that is not one of the \"n\").\n\nOther systems reject the law entirely.\n\nFor example, if \"P\" is the proposition:\n\nthen the law of excluded middle holds that the logical disjunction:\n\nis true by virtue of its form alone. That is, the \"middle\" position, that Socrates is neither mortal nor not-mortal, is excluded by logic, and therefore either the first possibility (\"Socrates is mortal\") or its negation (\"it is not the case that Socrates is mortal\") must be true.\n\nAn example of an argument that depends on the law of excluded middle follows. We seek to prove that there exist two irrational numbers formula_2 and formula_3 such that\n\nIt is known that formula_5 is irrational (see proof). Consider the number\n\nClearly (excluded middle) this number is either rational or irrational. If it is rational, the proof is complete, and\n\nBut if formula_6 is irrational, then let\n\nThen\n\nand 2 is certainly rational. This concludes the proof.\n\nIn the above argument, the assertion \"this number is either rational or irrational\" invokes the law of excluded middle. An intuitionist, for example, would not accept this argument without further support for that statement. This might come in the form of a proof that the number in question is in fact irrational (or rational, as the case may be); or a finite algorithm that could determine whether the number is rational.\n\nThe above proof is an example of a \"non-constructive\" proof disallowed by intuitionists:\n</math> is irrational but there is no known easy proof of that fact.) (Davis 2000:220)}} (Constructive proofs of the specific example above are not hard to produce; for example formula_7 and formula_14 are both easily shown to be irrational, and formula_15; a proof allowed by intuitionists).\n\nBy \"non-constructive\" Davis means that \"a proof that there actually are mathematic entities satisfying certain conditions would not have to provide a method to exhibit explicitly the entities in question.\" (p. 85). Such proofs presume the existence of a totality that is complete, a notion disallowed by intuitionists when extended to the \"infinite\"—for them the infinite can never be completed:\n\nIndeed, David Hilbert and Luitzen E. J. Brouwer both give examples of the law of excluded middle extended to the infinite. Hilbert's example: \"the assertion that either there are only finitely many prime numbers or there are infinitely many\" (quoted in Davis 2000:97); and Brouwer's: \"Every mathematical species is either finite or infinite.\" (Brouwer 1923 in van Heijenoort 1967:336).\n\nIn general, intuitionists allow the use of the law of excluded middle when it is confined to discourse over finite collections (sets), but not when it is used in discourse over infinite sets (e.g. the natural numbers). Thus intuitionists absolutely disallow the blanket assertion: \"For all propositions \"P\" concerning infinite sets \"D\": \"P\" or ~\"P\"\" (Kleene 1952:48).\n\nPutative counterexamples to the law of excluded middle include the liar paradox or Quine's Paradox. Certain resolutions of these paradoxes, particularly Graham Priest's dialetheism as formalised in LP, have the law of excluded middle as a theorem, but resolve out the Liar as both true and false. In this way, the law of excluded middle is true, but because truth itself, and therefore disjunction, is not exclusive, it says next to nothing if one of the disjuncts is paradoxical, or both true and false.\n\nAristotle wrote that ambiguity can arise from the use of ambiguous names, but cannot exist in the facts themselves:\n\nAristotle's assertion that \"...it will not be possible to be and not to be the same thing\", which would be written in propositional logic as ¬(\"P\" ∧ ¬\"P\"), is a statement modern logicians could call the law of excluded middle (\"P\" ∨ ¬\"P\"), as distribution of the negation of Aristotle's assertion makes them equivalent, regardless that the former claims that no statement is \"both\" true and false, while the latter requires that any statement is \"either\" true or false.\n\nHowever, Aristotle also writes, \"since it is impossible that contradictories should be at the same time true of the same thing, obviously contraries also cannot belong at the same time to the same thing\" (Book IV, CH 6, p. 531). He then proposes that \"there cannot be an intermediate between contradictories, but of one subject we must either affirm or deny any one predicate\" (Book IV, CH 7, p. 531). In the context of Aristotle's traditional logic, this is a remarkably precise statement of the law of excluded middle, \"P\" ∨ ¬\"P\".\n\nBertrand Russell asserts a distinction between the \"law of excluded middle\" and the \"law of noncontradiction\". In \"The Problems of Philosophy\", he cites three \"Laws of Thought\" as more or less \"self-evident\" or \"a priori\" in the sense of Aristotle:\n\nIt is correct, at least for bivalent logic—i.e. it can be seen with a Karnaugh map—that Russell's Law (2) removes \"the middle\" of the inclusive-or used in his law (3). And this is the point of Reichenbach's demonstration that some believe the \"exclusive\"-or should take the place of the \"inclusive\"-or.\n\nAbout this issue (in admittedly very technical terms) Reichenbach observes:\n\nIn line (30) the \"(x)\" means \"for all\" or \"for every\", a form used by Russell and Reichenbach; today the symbolism is usually formula_16 \"x\". Thus an example of the expression would look like this:\n\n\n\"Principia Mathematica\" (\"PM\") defines the law of excluded middle formally:\n\nSo just what is \"truth\" and \"falsehood\"? At the opening \"PM\" quickly announces some definitions:\n\nThis is not much help. But later, in a much deeper discussion, (\"Definition and systematic ambiguity of Truth and Falsehood\" Chapter II part III, p. 41 ff ) \"PM\" defines truth and falsehood in terms of a relationship between the \"a\" and the \"b\" and the \"percipient\". For example \"This 'a' is 'b'\" (e.g. \"This 'object a' is 'red'\") really means \"'object a' is a sense-datum\" and \"'red' is a sense-datum\", and they \"stand in relation\" to one another and in relation to \"I\". Thus what we really mean is: \"I perceive that 'This object a is red'\" and this is an undeniable-by-3rd-party \"truth\".\n\n\"PM\" further defines a distinction between a \"sense-datum\" and a \"sensation\":\nRussell reiterated his distinction between \"sense-datum\" and \"sensation\" in his book \"The Problems of Philosophy\" (1912) published at the same time as \"PM\" (1910–1913):\nRussell further described his reasoning behind his definitions of \"truth\" and \"falsehood\" in the same book (Chapter XII \"Truth and Falsehood\").\n\nFrom the law of excluded middle, formula ✸2.1 in \"Principia Mathematica,\" Whitehead and Russell derive some of the most powerful tools in the logician's argumentation toolkit. (In \"Principia Mathematica,\" formulas and propositions are identified by a leading asterisk and two numbers, such as \"✸2.1\".)\n\n✸2.1 ~\"p\" ∨ \"p\" \"This is the Law of excluded middle\" (\"PM\", p. 101).\n\nThe proof of ✸2.1 is roughly as follows: \"primitive idea\" 1.08 defines \"p\" → \"q\" = ~\"p\" ∨ \"q\". Substituting \"p\" for \"q\" in this rule yields \"p\" → \"p\" = ~\"p\" ∨ \"p\". Since \"p\" → \"p\" is true (this is Theorem 2.08, which is proved separately), then ~\"p\" ∨ \"p\" must be true.\n\n✸2.11 \"p\" ∨ ~\"p\" (Permutation of the assertions is allowed by axiom 1.4)<br>\n✸2.12 \"p\" → ~(~\"p\") (Principle of double negation, part 1: if \"this rose is red\" is true then it's not true that \"'this rose is not-red' is true\".)<br>\n✸2.13 \"p\" ∨ ~{~(~\"p\")} (Lemma together with 2.12 used to derive 2.14)<br>\n✸2.14 ~(~\"p\") → \"p\" (Principle of double negation, part 2)<br>\n✸2.15 (~\"p\" → \"q\") → (~\"q\" → \"p\") (One of the four \"Principles of transposition\". Similar to 1.03, 1.16 and 1.17. A very long demonstration was required here.)<br>\n✸2.16 (\"p\" → \"q\") → (~\"q\" → ~\"p\") (If it's true that \"If this rose is red then this pig flies\" then it's true that \"If this pig doesn't fly then this rose isn't red.\")<br>\n✸2.17 ( ~\"p\" → ~\"q\" ) → (\"q\" → \"p\") (Another of the \"Principles of transposition\".)<br>\n✸2.18 (~\"p\" → \"p\") → \"p\" (Called \"The complement of \"reductio ad absurdum\". It states that a proposition which follows from the hypothesis of its own falsehood is true\" (\"PM\", pp. 103–104).)\n\nMost of these theorems—in particular ✸2.1, ✸2.11, and ✸2.14—are rejected by intuitionism. These tools are recast into another form that Kolmogorov cites as \"Hilbert's four axioms of implication\" and \"Hilbert's two axioms of negation\" (Kolmogorov in van Heijenoort, p. 335).\n\nPropositions ✸2.12 and ✸2.14, \"double negation\":\nThe intuitionist writings of L. E. J. Brouwer refer to what he calls \"the \"principle of the reciprocity of the multiple species\", that is, the principle that for every system the correctness of a property follows from the impossibility of the impossibility of this property\" (Brouwer, ibid, p. 335).\n\nThis principle is commonly called \"the principle of double negation\" (\"PM\", pp. 101–102). From the law of excluded middle (✸2.1 and ✸2.11), \"PM\" derives principle ✸2.12 immediately. We substitute ~\"p\" for \"p\" in 2.11 to yield ~\"p\" ∨ ~(~\"p\"), and by the definition of implication (i.e. 1.01 p → q = ~p ∨ q) then ~p ∨ ~(~p)= p → ~(~p). QED (The derivation of 2.14 is a bit more involved.)\nMany modern logic systems replace the law of excluded middle with the concept of negation as failure. Instead of a proposition's being either true or false, a proposition is either true or not able to be proved true. These two dichotomies only differ in logical systems that are not complete. The principle of negation as failure is used as a foundation for autoepistemic logic, and is widely used in logic programming. In these systems, the programmer is free to assert the law of excluded middle as a true fact, but it is not built-in \"a priori\" into these systems.\n\nMathematicians such as L. E. J. Brouwer and Arend Heyting have also contested the usefulness of the law of excluded middle in the context of modern mathematics.\n\nIn modern mathematical logic, the excluded middle has been shown to result in possible self-contradiction. It is possible in logic to make well-constructed propositions that can be neither true nor false; a common example of this is the \"Liar's paradox\", the statement \"this statement is false\", which can itself be neither true nor false. In set theory, such a self-referential paradox can be constructed by examining the set \"the set of all sets that do not contain themselves\". This set is unambiguously defined, but leads to a Russell's paradox: does the set contain, as one of its elements, itself? Gödel's incompleteness theorem shows that contradictions of this nature are inherent in mathematical systems.\n\n\n\n"}
{"id": "4618420", "url": "https://en.wikipedia.org/wiki?curid=4618420", "title": "Leyland number", "text": "Leyland number\n\nIn number theory, a Leyland number is a number of the form\nwhere \"x\" and \"y\" are integers greater than 1. They are named after the mathematician Paul Leyland. The first few Leyland numbers are\n\nThe requirement that \"x\" and \"y\" both be greater than 1 is important, since without it every positive integer would be a Leyland number of the form \"x\" + 1. Also, because of the commutative property of addition, the condition \"x\" ≥ \"y\" is usually added to avoid double-covering the set of Leyland numbers (so we have 1 < \"y\" ≤ \"x\").\n\nA Leyland prime is a Leyland number that is also a prime. The first such primes are:\n\ncorresponding to\n\nOne can also fix the value of \"y\" and consider the sequence of \"x\" values that gives Leyland primes, for example \"x\" + 2 is prime for \"x\" = 3, 9, 15, 21, 33, 2007, 2127, 3759, ... ().\n\nBy November 2012, the largest Leyland number that had been proven to be prime was 5122 + 6753 with 25050 digits. From January 2011 to April 2011, it was the largest prime whose primality was proved by elliptic curve primality proving. In December 2012, this was improved by proving the primality of the two numbers 3110 + 63 (5596 digits) and 8656 + 2929 (30008 digits), the latter of which surpassed the previous record. There are many larger known probable primes such as 314738 + 9, but it is hard to prove primality of large Leyland numbers. Paul Leyland writes on his website: \"More recently still, it was realized that numbers of this form are ideal test cases for general purpose primality proving programs. They have a simple algebraic description but no obvious cyclotomic properties which special purpose algorithms can exploit.\"\n\nThere is a project called XYYXF to factor composite Leyland numbers.\n\nA Leyland number of the second kind is a number of the form\nwhere \"x\" and \"y\" are integers greater than 1. \n\nA Leyland prime of the second kind is a Leyland number of the second kind that is also prime. The first few such primes are:\n\nFor the probable primes, see Henri Lifchitz & Renaud Lifchitz, PRP Top Records search.\n"}
{"id": "54109664", "url": "https://en.wikipedia.org/wiki?curid=54109664", "title": "Liouville space", "text": "Liouville space\n\nIn mathematics, a Liouville space, also known as a line space or an extended Hilbert space is Cartesian product of two Hilbert spaces.\n"}
{"id": "8339650", "url": "https://en.wikipedia.org/wiki?curid=8339650", "title": "List of mathematic operators", "text": "List of mathematic operators\n\nIn mathematics, an operator or transform is a function from one space of functions to another. Operators occur commonly in engineering, physics and mathematics. Many are integral operators and differential operators.\n\nIn the following \"L\" is an operator\n\nwhich takes a function formula_2 to another function formula_3. Here, formula_4 and formula_5 are some unspecified function spaces, such as Hardy space, \"L\" space, Sobolev space, or, more vaguely, the space of holomorphic functions.\n\n"}
{"id": "18967255", "url": "https://en.wikipedia.org/wiki?curid=18967255", "title": "Mathematical economics", "text": "Mathematical economics\n\nMathematical economics is the application of mathematical methods to represent theories and analyze problems in economics. By convention, these applied methods are beyond simple geometry, such as differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, and other computational methods. Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.\n\nMathematics allows economists to form meaningful, testable propositions about wide-ranging and complex subjects which could less easily be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics. Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.\n\nBroad applications include:\n\nFormal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.\n\nThis rapid systematizing of economics alarmed critics of the discipline as well as some noted economists. John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics.\n\nThe use of mathematics in the service of social and economic analysis dates back to the 17th century. Then, mainly in German universities, a style of instruction emerged which dealt specifically with detailed presentation of data as it related to public administration. Gottfried Achenwall lectured in this fashion, coining the term statistics. At the same time, a small group of professors in England established a method of \"reasoning by figures upon things relating to government\" and referred to this practice as \"Political Arithmetick\". Sir William Petty wrote at length on issues that would later concern economists, such as taxation, Velocity of money and national income, but while his analysis was numerical, he rejected abstract mathematical methodology. Petty's use of detailed numerical data (along with John Graunt) would influence statisticians and economists for some time, even though Petty's works were largely ignored by English scholars.\n\nThe mathematization of economics began in earnest in the 19th century. Most of the economic analysis of the time was what would later be called classical economics. Subjects were discussed and dispensed with through algebraic means, but calculus was not used. More importantly, until Johann Heinrich von Thünen's \"The Isolated State\" in 1826, economists did not develop explicit and abstract models for behavior in order to apply the tools of mathematics. Thünen's model of farmland use represents the first example of marginal analysis. Thünen's work was largely theoretical, but he also mined empirical data in order to attempt to support his generalizations. In comparison to his contemporaries, Thünen built economic models and tools, rather than applying previous tools to new problems.\n\nMeanwhile, a new cohort of scholars trained in the mathematical methods of the physical sciences gravitated to economics, advocating and applying those methods to their subject, and described today as moving from geometry to mechanics.\nThese included W.S. Jevons who presented paper on a \"general mathematical theory of political economy\" in 1862, providing an outline for use of the theory of marginal utility in political economy. In 1871, he published \"The Principles of Political Economy\", declaring that the subject as science \"must be mathematical simply because it deals with quantities.\" Jevons expected that only collection of statistics for price and quantities would permit the subject as presented to become an exact science. Others preceded and followed in expanding mathematical representations of economic problems.\n\nAugustin Cournot and Léon Walras built the tools of the discipline axiomatically around utility, arguing that individuals sought to maximize their utility across choices in a way that could be described mathematically. At the time, it was thought that utility was quantifiable, in units known as utils. Cournot, Walras and Francis Ysidro Edgeworth are considered the precursors to modern mathematical economics.\n\nCournot, a professor of mathematics, developed a mathematical treatment in 1838 for duopoly—a market condition defined by competition between two sellers. This treatment of competition, first published in \"Researches into the Mathematical Principles of Wealth\", is referred to as Cournot duopoly. It is assumed that both sellers had equal access to the market and could produce their goods without cost. Further, it assumed that both goods were homogeneous. Each seller would vary her output based on the output of the other and the market price would be determined by the total quantity supplied. The profit for each firm would be determined by multiplying their output and the per unit Market price. Differentiating the profit function with respect to quantity supplied for each firm left a system of linear equations, the simultaneous solution of which gave the equilibrium quantity, price and profits. Cournot's contributions to the mathematization of economics would be neglected for decades, but eventually influenced many of the marginalists. Cournot's models of duopoly and Oligopoly also represent one of the first formulations of non-cooperative games. Today the solution can be given as a Nash equilibrium but Cournot's work preceded modern game theory by over 100 years.\n\nWhile Cournot provided a solution for what would later be called partial equilibrium, Léon Walras attempted to formalize discussion of the economy as a whole through a theory of general competitive equilibrium. The behavior of every economic actor would be considered on both the production and consumption side. Walras originally presented four separate models of exchange, each recursively included in the next. The solution of the resulting system of equations (both linear and non-linear) is the general equilibrium. At the time, no general solution could be expressed for a system of arbitrarily many equations, but Walras's attempts produced two famous results in economics. The first is Walras' law and the second is the principle of tâtonnement. Walras' method was considered highly mathematical for the time and Edgeworth commented at length about this fact in his review of \"Éléments d'économie politique pure\" (Elements of Pure Economics).\n\nWalras' law was introduced as a theoretical answer to the problem of determining the solutions in general equilibrium. His notation is different from modern notation but can be constructed using more modern summation notation. Walras assumed that in equilibrium, all money would be spent on all goods: every good would be sold at the market price for that good and every buyer would expend their last dollar on a basket of goods. Starting from this assumption, Walras could then show that if there were n markets and n-1 markets cleared (reached equilibrium conditions) that the nth market would clear as well. This is easiest to visualize with two markets (considered in most texts as a market for goods and a market for money). If one of two markets has reached an equilibrium state, no additional goods (or conversely, money) can enter or exit the second market, so it must be in a state of equilibrium as well. Walras used this statement to move toward a proof of existence of solutions to general equilibrium but it is commonly used today to illustrate market clearing in money markets at the undergraduate level.\n\nTâtonnement (roughly, French for \"groping toward\") was meant to serve as the practical expression of Walrasian general equilibrium. Walras abstracted the marketplace as an auction of goods where the auctioneer would call out prices and market participants would wait until they could each satisfy their personal reservation prices for the quantity desired (remembering here that this is an auction on \"all\" goods, so everyone has a reservation price for their desired basket of goods).\n\nOnly when all buyers are satisfied with the given market price would transactions occur. The market would \"clear\" at that price—no surplus or shortage would exist. The word \"tâtonnement\" is used to describe the directions the market takes in \"groping toward\" equilibrium, settling high or low prices on different goods until a price is agreed upon for all goods. While the process appears dynamic, Walras only presented a static model, as no transactions would occur until all markets were in equilibrium. In practice very few markets operate in this manner.\n\nEdgeworth introduced mathematical elements to Economics explicitly in \"\", published in 1881. He adopted Jeremy Bentham's felicific calculus to economic behavior, allowing the outcome of each decision to be converted into a change in utility. Using this assumption, Edgeworth built a model of exchange on three assumptions: individuals are self-interested, individuals act to maximize utility, and individuals are \"free to recontract with another independently of...any third party.\"\nGiven two individuals, the set of solutions where the both individuals can maximize utility is described by the \"contract curve\" on what is now known as an Edgeworth Box. Technically, the construction of the two-person solution to Edgeworth's problem was not developed graphically until 1924 by Arthur Lyon Bowley. The contract curve of the Edgeworth box (or more generally on any set of solutions to Edgeworth's problem for more actors) is referred to as the core of an economy.\n\nEdgeworth devoted considerable effort to insisting that mathematical proofs were appropriate for all schools of thought in economics. While at the helm of \"The Economic Journal\", he published several articles criticizing the mathematical rigor of rival researchers, including Edwin Robert Anderson Seligman, a noted skeptic of mathematical economics. The articles focused on a back and forth over tax incidence and responses by producers. Edgeworth noticed that a monopoly producing a good that had jointness of supply but not jointness of demand (such as first class and economy on an airplane, if the plane flies, both sets of seats fly with it) might actually lower the price seen by the consumer for one of the two commodities if a tax were applied. Common sense and more traditional, numerical analysis seemed to indicate that this was preposterous. Seligman insisted that the results Edgeworth achieved were a quirk of his mathematical formulation. He suggested that the assumption of a continuous demand function and an infinitesimal change in the tax resulted in the paradoxical predictions. Harold Hotelling later showed that Edgeworth was correct and that the same result (a \"diminution of price as a result of the tax\") could occur with a discontinuous demand function and large changes in the tax rate.\n\nFrom the later-1930s, an array of new mathematical tools from the differential calculus and differential equations, convex sets, and graph theory were deployed to advance economic theory in a way similar to new mathematical methods earlier applied to physics. The process was later described as moving from mechanics to axiomatics.\n\nVilfredo Pareto analyzed microeconomics by treating decisions by economic actors as attempts to change a given allotment of goods to another, more preferred allotment. Sets of allocations could then be treated as Pareto efficient (Pareto optimal is an equivalent term) when no exchanges could occur between actors that could make at least one individual better off without making any other individual worse off. Pareto's proof is commonly conflated with Walrassian equilibrium or informally ascribed to Adam Smith's Invisible hand hypothesis. Rather, Pareto's statement was the first formal assertion of what would be known as the first fundamental theorem of welfare economics. These models lacked the inequalities of the next generation of mathematical economics.\n\nIn the landmark treatise \"Foundations of Economic Analysis\" (1947), Paul Samuelson identified a common paradigm and mathematical structure across multiple fields in the subject, building on previous work by Alfred Marshall. \"Foundations\" took mathematical concepts from physics and applied them to economic problems. This broad view (for example, comparing Le Chatelier's principle to tâtonnement) drives the fundamental premise of mathematical economics: systems of economic actors may be modeled and their behavior described much like any other system. This extension followed on the work of the marginalists in the previous century and extended it significantly. Samuelson approached the problems of applying individual utility maximization over aggregate groups with comparative statics, which compares two different equilibrium states after an exogenous change in a variable. This and other methods in the book provided the foundation for mathematical economics in the 20th century.\n\nRestricted models of general equilibrium were formulated by John von Neumann in 1937. Unlike earlier versions, the models of von Neumann had inequality constraints. For his model of an expanding economy, von Neumann proved the existence and uniqueness of an equilibrium using his generalization of Brouwer's fixed point theorem. Von Neumann's model of an expanding economy considered the matrix pencil \" A - λ B \" with nonnegative matrices A and B; von Neumann sought probability vectors \"p\" and \"q\" and a positive number \"λ\" that would solve the complementarity equation\nalong with two inequality systems expressing economic efficiency. In this model, the (transposed) probability vector \"p\" represents the prices of the goods while the probability vector q represents the \"intensity\" at which the production process would run. The unique solution \"λ\" represents the rate of growth of the economy, which equals the interest rate. Proving the existence of a positive growth rate and proving that the growth rate equals the interest rate were remarkable achievements, even for von Neumann. Von Neumann's results have been viewed as a special case of linear programming, where von Neumann's model uses only nonnegative matrices. The study of von Neumann's model of an expanding economy continues to interest mathematical economists with interests in computational economics.\n\nIn 1936, the Russian–born economist Wassily Leontief built his model of input-output analysis from the 'material balance' tables constructed by Soviet economists, which themselves followed earlier work by the physiocrats. With his model, which described a system of production and demand processes, Leontief described how changes in demand in one economic sector would influence production in another. In practice, Leontief estimated the coefficients of his simple models, to address economically interesting questions. In production economics, \"Leontief technologies\" produce outputs using constant proportions of inputs, regardless of the price of inputs, reducing the value of Leontief models for understanding economies but allowing their parameters to be estimated relatively easily. In contrast, the von Neumann model of an expanding economy allows for choice of techniques, but the coefficients must be estimated for each technology.\n\nIn mathematics, mathematical optimization (or optimization or mathematical programming) refers to the selection of a best element from some set of available alternatives. In the simplest case, an optimization problem involves maximizing or minimizing a real function by selecting input values of the function and computing the corresponding values of the function. The solution process includes satisfying general necessary and sufficient conditions for optimality. For optimization problems, specialized notation may be used as to the function and its input(s). More generally, optimization includes finding the best available element of some function given a defined domain and may use a variety of different computational optimization techniques.\n\nEconomics is closely enough linked to optimization by agents in an economy that an influential definition relatedly describes economics \"qua\" science as the \"study of human behavior as a relationship between ends and scarce means\" with alternative uses. Optimization problems run through modern economics, many with explicit economic or technical constraints. In microeconomics, the utility maximization problem and its dual problem, the expenditure minimization problem for a given level of utility, are economic optimization problems. Theory posits that consumers maximize their utility, subject to their budget constraints and that firms maximize their profits, subject to their production functions, input costs, and market demand.\n\nEconomic equilibrium is studied in optimization theory as a key ingredient of economic theorems that in principle could be tested against empirical data. Newer developments have occurred in dynamic programming and modeling optimization with risk and uncertainty, including applications to portfolio theory, the economics of information, and search theory.\n\nOptimality properties for an entire market system may be stated in mathematical terms, as in formulation of the two fundamental theorems of welfare economics and in the Arrow–Debreu model of general equilibrium (also discussed below). More concretely, many problems are amenable to analytical (formulaic) solution. Many others may be sufficiently complex to require numerical methods of solution, aided by software. Still others are complex but tractable enough to allow computable methods of solution, in particular computable general equilibrium models for the entire economy.\n\nLinear and nonlinear programming have profoundly affected microeconomics, which had earlier considered only equality constraints. Many of the mathematical economists who received Nobel Prizes in Economics had conducted notable research using linear programming: Leonid Kantorovich, Leonid Hurwicz, Tjalling Koopmans, Kenneth J. Arrow, Robert Dorfman, Paul Samuelson and Robert Solow. Both Kantorovich and Koopmans acknowledged that George B. Dantzig deserved to share their Nobel Prize for linear programming. Economists who conducted research in nonlinear programming also have won the Nobel prize, notably Ragnar Frisch in addition to Kantorovich, Hurwicz, Koopmans, Arrow, and Samuelson.\n\nLinear programming was developed to aid the allocation of resources in firms and in industries during the 1930s in Russia and during the 1940s in the United States. During the Berlin airlift (1948), linear programming was used to plan the shipment of supplies to prevent Berlin from starving after the Soviet blockade.\n\nExtensions to nonlinear optimization with inequality constraints were achieved in 1951 by Albert W. Tucker and Harold Kuhn, who considered the nonlinear optimization problem:\n\nIn allowing inequality constraints, the Kuhn–Tucker approach generalized the classic method of Lagrange multipliers, which (until then) had allowed only equality constraints. The Kuhn–Tucker approach inspired further research on Lagrangian duality, including the treatment of inequality constraints. The duality theory of nonlinear programming is particularly satisfactory when applied to convex minimization problems, which enjoy the convex-analytic duality theory of Fenchel and Rockafellar; this convex duality is particularly strong for polyhedral convex functions, such as those arising in linear programming. Lagrangian duality and convex analysis are used daily in operations research, in the scheduling of power plants, the planning of production schedules for factories, and the routing of airlines (routes, flights, planes, crews).\n\n\"Economic dynamics\" allows for changes in economic variables over time, including in dynamic systems. The problem of finding optimal functions for such changes is studied in variational calculus and in optimal control theory. Before the Second World War, Frank Ramsey and Harold Hotelling used the calculus of variations to that end.\n\nFollowing Richard Bellman's work on dynamic programming and the 1962 English translation of L. Pontryagin \"et al\".'s earlier work, optimal control theory was used more extensively in economics in addressing dynamic problems, especially as to economic growth equilibrium and stability of economic systems, of which a textbook example is optimal consumption and saving. A crucial distinction is between deterministic and stochastic control models. Other applications of optimal control theory include those in finance, inventories, and production for example.\n\nIt was in the course of proving of the existence of an optimal equilibrium in his 1937 model of economic growth that John von Neumann introduced functional analytic methods to include topology in economic theory, in particular, fixed-point theory through his generalization of Brouwer's fixed-point theorem. Following von Neumann's program, Kenneth Arrow and Gérard Debreu formulated abstract models of economic equilibria using convex sets and fixed–point theory. In introducing the Arrow–Debreu model in 1954, they proved the existence (but not the uniqueness) of an equilibrium and also proved that every Walras equilibrium is Pareto efficient; in general, equilibria need not be unique. In their models, the (\"primal\") vector space represented \"quantitites\" while the \"dual\" vector space represented \"prices\".\n\nIn Russia, the mathematician Leonid Kantorovich developed economic models in partially ordered vector spaces, that emphasized the duality between quantities and prices. Kantorovich renamed \"prices\" as \"objectively determined valuations\" which were abbreviated in Russian as \"o. o. o.\", alluding to the difficulty of discussing prices in the Soviet Union.\n\nEven in finite dimensions, the concepts of functional analysis have illuminated economic theory, particularly in clarifying the role of prices as normal vectors to a hyperplane supporting a convex set, representing production or consumption possibilities. However, problems of describing optimization over time or under uncertainty require the use of infinite–dimensional function spaces, because agents are choosing among functions or stochastic processes.\n\nJohn von Neumann's work on functional analysis and topology broke new ground in mathematics and economic theory. It also left advanced mathematical economics with fewer applications of differential calculus. In particular, general equilibrium theorists used general topology, convex geometry, and optimization theory more than differential calculus, because the approach of differential calculus had failed to establish the existence of an equilibrium.\n\nHowever, the decline of differential calculus should not be exaggerated, because differential calculus has always been used in graduate training and in applications. Moreover, differential calculus has returned to the highest levels of mathematical economics, general equilibrium theory (GET), as practiced by the \"GET-set\" (the humorous designation due to Jacques H. Drèze). In the 1960s and 1970s, however, Gérard Debreu and Stephen Smale led a revival of the use of differential calculus in mathematical economics. In particular, they were able to prove the existence of a general equilibrium, where earlier writers had failed, because of their novel mathematics: Baire category from general topology and Sard's lemma from differential topology. Other economists associated with the use of differential analysis include Egbert Dierker, Andreu Mas-Colell, and Yves Balasko. These advances have changed the traditional narrative of the history of mathematical economics, following von Neumann, which celebrated the abandonment of differential calculus.\n\nJohn von Neumann, working with Oskar Morgenstern on the theory of games, broke new mathematical ground in 1944 by extending functional analytic methods related to convex sets and topological fixed-point theory to economic analysis. Their work thereby avoided the traditional differential calculus, for which the maximum–operator did not apply to non-differentiable functions. Continuing von Neumann's work in cooperative game theory, game theorists Lloyd S. Shapley, Martin Shubik, Hervé Moulin, Nimrod Megiddo, Bezalel Peleg influenced economic research in politics and economics. For example, research on the fair prices in cooperative games and fair values for voting games led to changed rules for voting in legislatures and for accounting for the costs in public–works projects. For example, cooperative game theory was used in designing the water distribution system of Southern Sweden and for setting rates for dedicated telephone lines in the USA.\n\nEarlier neoclassical theory had bounded only the \"range\" of bargaining outcomes and in special cases, for example bilateral monopoly or along the contract curve of the Edgeworth box. Von Neumann and Morgenstern's results were similarly weak. Following von Neumann's program, however, John Nash used fixed–point theory to prove conditions under which the bargaining problem and noncooperative games can generate a unique equilibrium solution. Noncooperative game theory has been adopted as a fundamental aspect of experimental economics, behavioral economics, information economics, industrial organization, and political economy. It has also given rise to the subject of mechanism design (sometimes called reverse game theory), which has private and public-policy applications as to ways of improving economic efficiency through incentives for information sharing.\n\nIn 1994, Nash, John Harsanyi, and Reinhard Selten received the Nobel Memorial Prize in Economic Sciences their work on non–cooperative games. Harsanyi and Selten were awarded for their work on repeated games. Later work extended their results to computational methods of modeling.\n\nAgent-based computational economics (ACE) as a named field is relatively recent, dating from about the 1990s as to published work. It studies economic processes, including whole economies, as dynamic systems of interacting agents over time. As such, it falls in the paradigm of complex adaptive systems. In corresponding agent-based models, agents are not real people but \"computational objects modeled as interacting according to rules\" ... \"whose micro-level interactions create emergent patterns\" in space and time. The rules are formulated to predict behavior and social interactions based on incentives and information. The theoretical assumption of mathematical \"optimization\" by agents markets is replaced by the less restrictive postulate of agents with \"bounded\" rationality \"adapting\" to market forces.\n\nACE models apply numerical methods of analysis to computer-based simulations of complex dynamic problems for which more conventional methods, such as theorem formulation, may not find ready use. Starting from specified initial conditions, the computational economic system is modeled as evolving over time as its constituent agents repeatedly interact with each other. In these respects, ACE has been characterized as a bottom-up culture-dish approach to the study of the economy. In contrast to other standard modeling methods, ACE events are driven solely by initial conditions, whether or not equilibria exist or are computationally tractable. ACE modeling, however, includes agent adaptation, autonomy, and learning. It has a similarity to, and overlap with, game theory as an agent-based method for modeling social interactions. Other dimensions of the approach include such standard economic subjects as competition and collaboration, market structure and industrial organization, transaction costs, welfare economics and mechanism design, information and uncertainty, and macroeconomics.\n\nThe method is said to benefit from continuing improvements in modeling techniques of computer science and increased computer capabilities. Issues include those common to experimental economics in general and by comparison and to development of a common framework for empirical validation and resolving open questions in agent-based modeling. The ultimate scientific objective of the method has been described as \"test[ing] theoretical findings against real-world data in ways that permit empirically supported theories to cumulate over time, with each researcher's work building appropriately on the work that has gone before.\"\n\nOver the course of the 20th century, articles in \"core journals\" in economics have been almost exclusively written by economists in academia. As a result, much of the material transmitted in those journals relates to economic theory, and \"economic theory itself has been continuously more abstract and mathematical.\" A subjective assessment of mathematical techniques employed in these core journals showed a decrease in articles that use neither geometric representations nor mathematical notation from 95% in 1892 to 5.3% in 1990. A 2007 survey of ten of the top economic journals finds that only 5.8% of the articles published in 2003 and 2004 both lacked statistical analysis of data and lacked displayed mathematical expressions that were indexed with numbers at the margin of the page.\n\nBetween the world wars, advances in mathematical statistics and a cadre of mathematically trained economists led to econometrics, which was the name proposed for the discipline of advancing economics by using mathematics and statistics. Within economics, \"econometrics\" has often been used for statistical methods in economics, rather than mathematical economics. Statistical econometrics features the application of linear regression and time series analysis to economic data.\n\nRagnar Frisch coined the word \"econometrics\" and helped to found both the Econometric Society in 1930 and the journal \"Econometrica\" in 1933. A student of Frisch's, Trygve Haavelmo published \"The Probability Approach in Econometrics\" in 1944, where he asserted that precise statistical analysis could be used as a tool to validate mathematical theories about economic actors with data from complex sources. This linking of statistical analysis of systems to economic theory was also promulgated by the Cowles Commission (now the Cowles Foundation) throughout the 1930s and 1940s.\n\nThe roots of modern econometrics can be traced to the American economist Henry L. Moore. Moore studied agricultural productivity and attempted to fit changing values of productivity for plots of corn and other crops to a curve using different values of elasticity. Moore made several errors in his work, some from his choice of models and some from limitations in his use of mathematics. The accuracy of Moore's models also was limited by the poor data for national accounts in the United States at the time. While his first models of production were static, in 1925 he published a dynamic \"moving equilibrium\" model designed to explain business cycles—this periodic variation from overcorrection in supply and demand curves is now known as the cobweb model. A more formal derivation of this model was made later by Nicholas Kaldor, who is largely credited for its exposition.\n\nMuch of classical economics can be presented in simple geometric terms or elementary mathematical notation. Mathematical economics, however, conventionally makes use of calculus and matrix algebra in economic analysis in order to make powerful claims that would be more difficult without such mathematical tools. These tools are prerequisites for formal study, not only in mathematical economics but in contemporary economic theory in general. Economic problems often involve so many variables that mathematics is the only practical way of attacking and solving them. Alfred Marshall argued that every economic problem which can be quantified, analytically expressed and solved, should be treated by means of mathematical work.\n\nEconomics has become increasingly dependent upon mathematical methods and the mathematical tools it employs have become more sophisticated. As a result, mathematics has become considerably more important to professionals in economics and finance. Graduate programs in both economics and finance require strong undergraduate preparation in mathematics for admission and, for this reason, attract an increasingly high number of mathematicians. Applied mathematicians apply mathematical principles to practical problems, such as economic analysis and other economics-related issues, and many economic problems are often defined as integrated into the scope of applied mathematics.\n\nThis integration results from the formulation of economic problems as stylized models with clear assumptions and falsifiable predictions. This modeling may be informal or prosaic, as it was in Adam Smith's \"The Wealth of Nations\", or it may be formal, rigorous and mathematical.\n\nBroadly speaking, formal economic models may be classified as stochastic or deterministic and as discrete or continuous. At a practical level, quantitative modeling is applied to many areas of economics and several methodologies have evolved more or less independently of each other.\n\n\nAccording to the Mathematics Subject Classification (MSC), mathematical economics falls into the Applied mathematics/other classification of category 91:\n\nwith MSC2010 classifications for 'Game theory' at codes 91Axx and for 'Mathematical economics' at codes 91Bxx.\n\nThe \"Handbook of Mathematical Economics\" series (Elsevier), currently 4 volumes, distinguishes between \"mathematical methods in economics\", v. 1, Part I, and \"areas of economics\" in other volumes where mathematics is employed.\n\nAnother source with a similar distinction is \"\" (1987, 4 vols., 1,300 subject entries). In it, a \"Subject Index\" includes mathematical entries under 2 headings (vol. IV, pp. 982–3):\n\nA widely used system in economics that includes mathematical methods on the subject is the JEL classification codes. It originated in the \"Journal of Economic Literature\" for classifying new books and articles. The relevant categories are listed below (simplified below to omit \"Miscellaneous\" and \"Other\" JEL codes), as reproduced from . \"The New Palgrave Dictionary of Economics\" (2008, 2nd ed.) also uses the JEL codes to classify its entries. The corresponding footnotes below have links to abstracts of \"The New Palgrave Online\" for each JEL category (10 or fewer per page, similar to Google searches).\n\nFriedrich Hayek contended that the use of formal techniques projects a scientific exactness that does not appropriately account for informational limitations faced by real economic agents. \n\nIn an interview in 1999, the economic historian Robert Heilbroner stated:\n\nHeilbroner stated that \"some/much of economics is not naturally quantitative and therefore does not lend itself to mathematical exposition.\"\n\nPhilosopher Karl Popper discussed the scientific standing of economics in the 1940s and 1950s. He argued that mathematical economics suffered from being tautological. In other words, insofar as economics became a mathematical theory, mathematical economics ceased to rely on empirical refutation but rather relied on mathematical proofs and disproof. According to Popper, falsifiable assumptions can be tested by experiment and observation while unfalsifiable assumptions can be explored mathematically for their consequences and for their consistency with other assumptions.\n\nSharing Popper's concerns about assumptions in economics generally, and not just mathematical economics, Milton Friedman declared that \"all assumptions are unrealistic\". Friedman proposed judging economic models by their predictive performance rather than by the match between their assumptions and reality.\n\nConsidering mathematical economics, J.M. Keynes wrote in \"The General Theory\":\nIn response to these criticisms, Paul Samuelson argued that mathematics is a language, repeating a thesis of Josiah Willard Gibbs. In economics, the language of mathematics is sometimes necessary for representing substantive problems. Moreover, mathematical economics has led to conceptual advances in economics. In particular, Samuelson gave the example of microeconomics, writing that \"few people are ingenious enough to grasp [its] more complex parts... \"without\" resorting to the language of mathematics, while most ordinary individuals can do so fairly easily \"with\" the aid of mathematics.\"\n\nSome economists state that mathematical economics deserves support just like other forms of mathematics, particularly its neighbors in mathematical optimization and mathematical statistics and increasingly in theoretical computer science. Mathematical economics and other mathematical sciences have a history in which theoretical advances have regularly contributed to the reform of the more applied branches of economics. In particular, following the program of John von Neumann, game theory now provides the foundations for describing much of applied economics, from statistical decision theory (as \"games against nature\") and econometrics to general equilibrium theory and industrial organization. In the last decade, with the rise of the internet, mathematical economists and optimization experts and computer scientists have worked on problems of pricing for on-line services --- their contributions using mathematics from cooperative game theory, nondifferentiable optimization, and combinatorial games.\n\nRobert M. Solow concluded that mathematical economics was the core \"infrastructure\" of contemporary economics:\n\nEconomics is no longer a fit conversation piece for ladies and gentlemen. It has become a technical subject. Like any technical subject it attracts some people who are more interested in the technique than the subject. That is too bad, but it may be inevitable. In any case, do not kid yourself: the technical core of economics is indispensable infrastructure for the political economy. That is why, if you consult [a reference in contemporary economics] looking for enlightenment about the world today, you will be led to technical economics, or history, or nothing at all.\nProminent mathematical economists include, but are not limited to, the following (by century of birth).\n\n\n\n\n\n\n\n\n\n"}
{"id": "15663294", "url": "https://en.wikipedia.org/wiki?curid=15663294", "title": "Matrix polynomial", "text": "Matrix polynomial\n\nIn mathematics, a matrix polynomial is a polynomial with square matrices as variables. Given an ordinary, scalar-valued polynomial\nthis polynomial evaluated at a matrix \"A\" is\nwhere \"I\" is the identity matrix.\n\nA matrix polynomial equation is an equality between two matrix polynomials, which holds for the specific matrices in question. A matrix polynomial identity is a matrix polynomial equation which holds for all matrices \"A\" in a specified matrix ring \"M\"(\"R\").\n\nThe characteristic polynomial of a matrix \"A\" is a scalar-valued polynomial, defined by formula_3. The Cayley–Hamilton theorem states that if this polynomial is viewed as a matrix polynomial and evaluated at the matrix \"A\" itself, the result is the zero matrix: formula_4. The characteristic polynomial is thus a polynomial which annihilates \"A\". \n\nThere is a unique monic polynomial of minimal degree which annihilates \"A\"; this polynomial is the minimal polynomial. Any polynomial which annihilates \"A\" (such as the characteristic polynomial) is a multiple of the minimal polynomial.\n\nIt follows that given two polynomials \"P\" and \"Q\", we have formula_5 if and only if \nwhere formula_7 denotes the \"j\"th derivative of \"P\" and formula_8 are the eigenvalues of \"A\" with corresponding indices formula_9 (the index of an eigenvalue is the size of its largest Jordan block).\n\nMatrix polynomials can be used to sum a matrix geometrical series as one would an ordinary geometric series,\n\nIf \"I\" − \"A\" is nonsingular one can evaluate the expression for the sum \"S\".\n\n\n"}
{"id": "57122", "url": "https://en.wikipedia.org/wiki?curid=57122", "title": "Multiplication table", "text": "Multiplication table\n\nIn mathematics, a multiplication table (sometimes, less formally, a times table) is a mathematical table used to define a multiplication operation for an algebraic system.\n\nThe decimal multiplication table was traditionally taught as an essential part of elementary arithmetic around the world, as it lays the foundation for arithmetic operations with base-ten numbers. Many educators believe it is necessary to memorize the table up to 9 × 9.\n\nThe oldest known multiplication tables were used by the Babylonians about 4000 years ago. However, they used a base of 60. The oldest known tables using a base of 10 are the Chinese decimal multiplication table on bamboo strips dating to about 305 BC, during China's Warring States period.\nThe multiplication table is sometimes attributed to the ancient Greek mathematician Pythagoras (570–495 BC). It is also called the Table of Pythagoras in many languages (for example French, Italian and at one point even Russian), sometimes in English. The Greco-Roman mathematician Nichomachus (60–120 AD), a follower of Neopythagoreanism, included a multiplication table in his \"Introduction to Arithmetic\", whereas the oldest surviving Greek multiplication table is on a wax tablet dated to the 1st century AD and currently housed in the British Museum.\n\nIn 493 AD, Victorius of Aquitaine wrote a 98-column multiplication table which gave (in Roman numerals) the product of every number from 2 to 50 times and the rows were \"a list of numbers starting with one thousand, descending by hundreds to one hundred, then descending by tens to ten, then by ones to one, and then the fractions down to 1/144.\"\n\nIn his 1820 book \"The Philosophy of Arithmetic\", mathematician John Leslie published a multiplication table up to 99 × 99, which allows numbers to be multiplied in pairs of digits at a time. Leslie also recommended that young pupils memorize the multiplication table up to 25 × 25. The illustration below shows a table up to 12 × 12, which is a size commonly used in schools.\n\nThe traditional rote learning of multiplication was based on memorization of columns in the table, in a form like\n\n<poem>\n</poem>\nThis form of writing the multiplication table in columns with complete number sentences is still used in some countries, such as Bosnia and Herzegovina, instead of the modern grid above.\n\nThere is a pattern in the multiplication table that can help people to memorize the table more easily. It uses the figures below:\n\nFigure 1 is used for multiples of 1, 3, 7, and 9. Figure 2 is used for the multiples of 2, 4, 6, and 8. These patterns can be used to memorize the multiples of any number from 0 to 10, except 5. As you would start on the number you are multiplying, when you multiply by 0, you stay on 0 (0 is external and so the arrows have no effect on 0, otherwise 0 is used as a link to create a perpetual cycle). The pattern also works with multiples of 10, by starting at 1 and simply adding 0, giving you 10, then just apply every number in the pattern to the \"tens\" unit as you would normally do as usual to the \"ones\" unit.\nFor example, to recall all the multiples of 7:\n\n\nTables can also define binary operations on groups, fields, rings, and other algebraic systems. In such contexts they can be called Cayley tables. Here are the addition and multiplication tables for the finite field Z.\n\nFor every natural number \"n\", there are also addition and multiplication tables for the ring Z.\n\nFor other examples, see group, and octonion.\n\nThe Chinese multiplication table consists of eighty-one sentences with four or five Chinese characters per sentence, making it easy for children to learn by heart. A shorter version of the table consists of only forty-five sentences, as terms such as \"nine eights beget seventy-two\" are identical to \"eight nines beget seventy-two\" so there is no need to learn them twice.\n\nA bundle of 21 bamboo slips dated 305 BC in the Warring States period in the Tsinghua Bamboo Slips (清华简) collection is the world's earliest known example of a decimal multiplication table.\nIn 1989, the National Council of Teachers of Mathematics (NCTM) developed new standards which were based on the belief that all students should learn higher-order thinking skills, and which recommended reduced emphasis on the teaching of traditional methods that relied on rote memorization, such as multiplication tables. Widely adopted texts such as Investigations in Numbers, Data, and Space (widely known as TERC after its producer, Technical Education Research Centers) omitted aids such as multiplication tables in early editions. NCTM made it clear in their 2006 Focal Points that basic mathematics facts must be learned, though there is no consensus on whether rote memorization is the best method.\n\n"}
{"id": "11566873", "url": "https://en.wikipedia.org/wiki?curid=11566873", "title": "Noether Lecture", "text": "Noether Lecture\n\nThe Noether Lecture is an award and lecture series that honors women \"who have made fundamental and sustained contributions to the mathematical sciences\". The Association for Women in Mathematics (AWM) established the annual lectures in 1980 as the Emmy Noether Lectures, in honor of one of the leading mathematicians of her time. In 2013 it was renamed the AWM-AMS Noether Lecture and since 2015 is sponsored jointly with the American Mathematical Society (AMS). The recipient delivers the lecture at the yearly American Joint Mathematics Meetings held in January.\n\nThe ICM Emmy Noether Lecture is an additional lecture series sponsored by the International Mathematical Union. Beginning in 1994 an additional Emmy Noether Lecture was delivered at the International Congress of Mathematicians held every four years. In 2010 the lecture series was made permanent.\n"}
{"id": "97168", "url": "https://en.wikipedia.org/wiki?curid=97168", "title": "Proofs of Fermat's little theorem", "text": "Proofs of Fermat's little theorem\n\nThis article collects together a variety of proofs of Fermat's little theorem, which states that\nfor every prime number \"p\" and every integer \"a\" (see modular arithmetic).\n\nSome of the proofs of Fermat's little theorem given below depend on two simplifications.\n\nThe first is that we may assume that is in the range . This is a simple consequence of the laws of modular arithmetic; we are simply saying that we may first reduce modulo . This is consistent with reducing formula_2 modulo , as one can check.\n\nSecondly, it suffices to prove that\nfor in the range . Indeed, if the previous assertion holds for such , multiplying both sides by yields the original form of the theorem,\nOn the other hand, if , the theorem holds trivially.\n\nThis is perhaps the simplest known proof, requiring the least mathematical background. It is an attractive example of a combinatorial proof (a proof that involves counting a collection of objects in two different ways).\n\nThe proof given here is an adaptation of Golomb's proof.\n\nTo keep things simple, let us assume that is a positive integer. Consider all the possible strings of symbols, using an alphabet with different symbols. The total number of such strings is , since there are possibilities for each of positions (see rule of product).\n\nFor example, if and , then we can use an alphabet with two symbols (say and ), and there are strings of length 5:\n\nWe will argue below that if we remove the strings consisting of a single symbol from the list (in our example, and ), the remaining strings can be arranged into groups, each group containing exactly strings. It follows that is divisible by .\n\nLet us think of each such string as representing a necklace. That is, we connect the two ends of the string together and regard two strings as the same necklace if we can rotate one string to obtain the second string; in this case we will say that the two strings are \"friends\". In our example, the following strings are all friends:\nSimilarly, each line of the following list corresponds to a single necklace.\nNotice that in the above list, some necklaces are represented by 5 different strings, and some only by a single string, so the list shows very clearly why is divisible by .\n\nOne can use the following rule to work out how many friends a given string has:\n\nFor example, suppose we start with the string , which is built up of several copies of the shorter string . If we rotate it one symbol at a time, we obtain the following 3 strings:\nThere aren't any others, because is exactly 3 symbols long and cannot be broken down into further repeating strings.\n\nUsing the above rule, we can complete the proof of Fermat's little theorem quite easily, as follows. Our starting pool of strings may be split into two categories:\n\nThe second category contains strings, and they may be arranged into groups of strings, one group for each necklace. Therefore, must be divisible by , as promised.\n\nThis proof uses some basic concepts from dynamical systems.\n\nWe start by considering a family of functions \"T\"(\"x\"), where \"n\" ≥ 2 is an integer, mapping the interval [0, 1] to itself by the formula\nwhere {\"y\"} denotes the fractional part of \"y\". For example, the function \"T\"(\"x\") is illustrated below:\n\nA number \"x\" is said to be a \"fixed point\" of a function \"f\"(\"x\") if \"f\"(\"x\") = \"x\"; in other words, if \"f\" leaves \"x\" fixed. The fixed points of a function can be easily found graphically: they are simply the \"x\" coordinates of the points where the graph of \"f\"(\"x\") intersects the graph of the line \"y\" = \"x\". For example, the fixed points of the function \"T\"(\"x\") are 0, 1/2, and 1; they are marked by black circles on the following diagram:\n\nWe will require the following two lemmas.\n\nLemma 1. For any \"n\" ≥ 2, the function \"T\"(\"x\") has exactly \"n\" fixed points.\n\nProof. There are 3 fixed points in the illustration above, and the same sort of geometrical argument applies for any \"n\" ≥ 2.\n\nLemma 2. For any positive integers \"n\" and \"m\", and any 0 ≤ x ≤ 1,\nIn other words, \"T\"(\"x\") is the composition of \"T\"(\"x\") and \"T\"(\"x\").\n\nProof. The proof of this lemma is not difficult, but we need to be slightly careful with the endpoint \"x\" = 1. For this point the lemma is clearly true, since\nSo let us assume that 0 ≤ \"x\" < 1. In this case,\nso \"T\"(\"T\"(\"x\")) is given by\nTherefore, what we really need to show is that\nTo do this we observe that {\"nx\"} = \"nx\" − \"k\", where \"k\" is the integer part of \"nx\"; then\nsince \"mk\" is an integer.\n\nNow let us properly begin the proof of Fermat's little theorem, by studying the function \"T\"(\"x\"). We will assume that \"a\" ≥ 2. From Lemma 1, we know that it has \"a\" fixed points. By Lemma 2 we know that\nso any fixed point of \"T\"(\"x\") is automatically a fixed point of \"T\"(\"x\").\n\nWe are interested in the fixed points of \"T\"(\"x\") that are \"not\" fixed points of \"T\"(\"x\"). Let us call the set of such points \"S\". There are \"a\" − \"a\" points in \"S\", because by Lemma 1 again, \"T\"(\"x\") has exactly \"a\" fixed points. The following diagram illustrates the situation for \"a\" = 3 and \"p\" = 2. The black circles are the points of \"S\", of which there are 3 − 3 = 6.\n\nThe main idea of the proof is now to split the set \"S\" up into its orbits under \"T\". What this means is that we pick a point \"x\" in \"S\", and repeatedly apply \"T\"(x) to it, to obtain the sequence of points\nThis sequence is called the orbit of \"x\" under \"T\". By Lemma 2, this sequence can be rewritten as\nSince we are assuming that \"x\" is a fixed point of \"T\"(\"x\"), after \"p\" steps we hit \"T\"(\"x\") = \"x\", and from that point onwards the sequence repeats itself.\n\nHowever, the sequence \"cannot\" begin repeating itself any earlier than that. If it did, the length of the repeating section would have to be a divisor of \"p\", so it would have to be 1 (since \"p\" is prime). But this contradicts our assumption that \"x\" is not a fixed point of \"T\".\n\nIn other words, the orbit contains exactly \"p\" distinct points. This holds for every orbit of \"S\". Therefore, the set \"S\", which contains \"a\" − \"a\" points, can be broken up into orbits, each containing \"p\" points, so \"a\" − \"a\" is divisible by \"p\".\n\n(This proof is essentially the same as the necklace-counting proof given above, simply viewed through a different lens: one may think of the interval [0, 1] as given by sequences of digits in base \"a\" (our distinction between 0 and 1 corresponding to the familiar distinction between representing integers as ending in \".0000...\" and \".9999...\"). \"T\" amounts to shifting such a sequence by \"n\" many digits. The fixed points of this will be sequences that are cyclic with period dividing \"n\". In particular, the fixed points of \"T\" can be thought of as the necklaces of length \"p\", with \"T\" corresponding to rotation of such necklaces by \"n\" spots.\n\nThis proof could also be presented without distinguishing between 0 and 1, simply using the half-open interval [0, 1); then \"T\" would only have \"n\" − 1 fixed points, but \"T\" − \"T\" would still work out to \"a\" − \"a\", as needed.)\n\nThis proof, due to Euler, uses induction to prove the theorem for all integers .\n\nThe base step, that , is trivial. Next, we must show that if the theorem is true for , then it is also true for . For this inductive step, we need the following lemma.\n\nLemma. For any integers and and for any prime , .\n\nThe lemma is a case of the freshman's dream. Leaving the proof for later on, we proceed with the induction.\n\nProof. Assume \"k\" ≡ \"k\" (mod \"p\"), and consider (\"k\"+1). By the lemma we have\n\nUsing the induction hypothesis, we have that \"k\" ≡ \"k\" (mod \"p\"); and, trivially, 1 = 1. Thus\n\nwhich is the statement of the theorem for \"a\" = \"k\"+1. ∎\n\nIn order to prove the lemma, we must introduce the binomial theorem, which states that for any positive integer \"n\",\n\nwhere the coefficients are the binomial coefficients,\n\ndescribed in terms of the factorial function, \"n\"! = 1×2×3×⋯×\"n\".\n\nProof of Lemma. We consider the binomial coefficient when the exponent is a prime \"p\":\n\nThe binomial coefficients are all integers. The numerator contains a factor \"p\" by the definition of factorial. When 0 < \"i\" < \"p\", neither of the terms in the denominator includes a factor of \"p\" (relying on the primality of \"p\"), leaving the coefficient itself to possess a prime factor of \"p\" from the numerator, implying that\n\nModulo \"p\", this eliminates all but the first and last terms of the sum on the right-hand side of the binomial theorem for prime \"p\". ∎\n\nThe primality of \"p\" is essential to the lemma; otherwise, we have examples like\n\nwhich is not divisible by 4.\n\nThe proof, which was first discovered by Leibniz (who did not publish it) and later rediscovered by Euler, is a very simple application of the multinomial theorem which is brought here for the sake of simplicity.\n\nThe summation is taken over all sequences of nonnegative integer indices through such the sum of all is .\n\nThus if we express as a sum of 1s (ones), we obtain\n\nClearly, if is prime, and if is not equal to for any , we have\n\nand\n\nif is equal to for some .\n\nSince there are exactly elements such that , the theorem follows.\n\n(This proof is essentially a coarser-grained variant of the necklace-counting proof given earlier; the multinomial coefficients count the number of ways a string can be permuted into arbitrary anagrams, while the necklace argument counts the number of ways a string can be rotated into cyclic anagrams. That is to say, that the nontrivial multinomial coefficients here are divisible by can be seen as a consequence of the fact that each nontrivial necklace of length can be unwrapped into a string in many ways.\n\nThis multinomial expansion is also, of course, what essentially underlies the binomial theorem-based proof above)\n\nAn additive-combinatorial proof based on formal power product expansions was given by Giedrius Alkauskas. This proof uses neither the Euclidean algorithm nor the binomial theorem, but rather it employs formal power series with rational coefficients.\n\nThis proof, discovered by James Ivory and rediscovered by Dirichlet requires some background in modular arithmetic.\n\nLet us assume that is positive and not divisible by .\n\nThe idea is that if we write down the sequence of numbers\n\nand reduce each one modulo , the resulting sequence turns out to be a rearrangement of\n\nTherefore, if we multiply together the numbers in each sequence, the results must be identical modulo :\nCollecting together the terms yields\nFinally, we may “cancel out” the numbers from both sides of this equation, obtaining\n\nThere are two steps in the above proof that we need to justify:\nWe will prove these things below; let us first see an example of this proof in action.\n\nIf and , then the sequence in question is\nreducing modulo 7 gives\nwhich is just a rearrangement of\nMultiplying them together gives\nthat is,\nCanceling out 1 × 2 × 3 × 4 × 5 × 6 yields\nwhich is Fermat's little theorem for the case and .\n\nLet us first explain why it is valid, in certain situations, to “cancel”. The exact statement is as follows. If , , and  are integers, and is not divisible by a prime number , and if\n\nthen we may “cancel” to obtain\n\nOur use of this cancellation law in the above proof of Fermat's little theorem was valid, because the numbers are certainly not divisible by (indeed they are \"smaller\" than ).\n\nWe can prove the cancellation law easily using Euclid's lemma, which generally states that if a prime divides a product (where and are integers), then must divide or . Indeed, the assertion () simply means that divides . Since is a prime which does not divide , Euclid's lemma tells us that it must divide instead; that is, () holds.\n\nNote that the conditions under which the cancellation law holds are quite strict, and this explains why Fermat's little theorem demands that is a prime. For example, , but it is not true that . However, the following generalization of the cancellation law holds: if , , , and are integers, if and are relatively prime, and if\nthen we may “cancel” to obtain\nThis follows from a generalization of Euclid's lemma.\n\nFinally, we must explain why the sequence\nwhen reduced modulo \"p\", becomes a rearrangement of the sequence\nTo start with, none of the terms , , ..., can be congruent to zero modulo , since if is one of the numbers , then is relatively prime with , and so is , so Euclid's lemma tells us that shares no factor with . Therefore, at least we know that the numbers , , ..., , when reduced modulo , must be found among the numbers .\n\nFurthermore, the numbers , , ..., must all be \"distinct\" after reducing them modulo , because if\nwhere and are one of , then the cancellation law tells us that\nSince both and are between and , they must be equal. Therefore, the terms , , ..., when reduced modulo must be distinct. \nTo summarise: when we reduce the numbers , , ..., modulo , we obtain distinct members of the sequence , , ..., . Since there are exactly of these, the only possibility is that the former are a rearrangement of the latter.\n\nThis method can also be used to prove Euler's theorem, with a slight alteration in that the numbers from to are substituted by the numbers less than and coprime with some number (not necessarily prime). Both the rearrangement property and the cancellation law (under the generalized form mentioned above) are still satisfied and can be utilized.\n\nFor example, if , then the numbers less than  and coprime with are , , , and . Thus we have:\n\nTherefore,\n\nThis proof requires the most basic elements of group theory.\n\nThe idea is to recognise that the set }, with the operation of multiplication (taken modulo ), forms a group. The only group axiom that requires some effort to verify is that each element of is invertible. Taking this on faith for the moment, let us assume that is in the range , that is, is an element of . Let be the order of , that is, is the smallest positive integer such that . Then the numbers reduced modulo  form a subgroup of  whose order is  and therefore, by Lagrange's theorem, divides the order of , which is . So for some positive integer and then\n\nTo prove that every element of is invertible, we may proceed as follows. First, is coprime to . Thus Bézout's identity assures us that there are integers and such that . Reading this equality modulo , we see that is an inverse for , since . Therefore, every element of is invertible. So, as remarked earlier, is a group.\n\nFor example, when , the inverses of each element are given as follows:\n\nIf we take the previous proof and, instead of using Lagrange's theorem, we try to prove it in this specific situation, then we get Euler's third proof, which is the one that he found more natural. Let be the set whose elements are the numbers reduced modulo . If , then and therefore divides . Otherwise, there is some .\n\nLet be the set whose elements are the numbers reduced modulo . Then has distinct elements, because otherwise there would be two distinct numbers } such that , which is impossible, since it would follow that . On the other hand, no element of can be an element of , because otherwise there would be numbers } such that , and then , which is impossible, since .\n\nSo, the set has elements. If it turns out to be equal to \"G\", then and therefore divides . Otherwise, there is some and we can start all over again, defining as the set whose elements are the numbers reduced modulo . Since is finite, this process must stop at some point and this proves that divides .\n\nFor instance, if and , then, since\nwe have and }. Clearly, }. Let be an element of ; for instance, take . Then, since\nwe have }. Clearly, . Let be an element of ; for instance, take . Then, since\nwe have }. And now .\n\nNote that the sets , , and so on are in fact the cosets of in .\n"}
{"id": "32673845", "url": "https://en.wikipedia.org/wiki?curid=32673845", "title": "Q-Hahn polynomials", "text": "Q-Hahn polynomials\n\nIn mathematics, the \"q\"-Hahn polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by \nformula_1\n\nq-Hahn polynomials→ Quantum q-Krawtchouk polynomials：\n\nformula_2\n\nq-Hahn polynomials→ Hahn polynomials\n\nmake the substitutionformula_3,formula_4 into definition of q-Hahn polynomials, and find the limit q→1, we obtain\nformula_5，which is exactly Hahn polynomials.\n\n"}
{"id": "39032314", "url": "https://en.wikipedia.org/wiki?curid=39032314", "title": "Quantum readout of PUFs", "text": "Quantum readout of PUFs\n\nQuantum Readout is a method to verify the authenticity of an object. The method is secure provided that the object cannot be copied or physically emulated.\n\nWhen authenticating an object, one can distinguish two cases.\n\nIn the hands-on scenario, Physical Unclonable Functions (PUFs) of various types can serve as great authentication tokens. Their physical unclonability, combined with the verifier's ability to detect spoofing, makes it exceedingly hard for an attacker to create an object that will pass as a PUF clone. However, hands-on authentication requires that the holder of the PUF relinquishes control of it, which may not be acceptable, especially if there is the risk that the verifier is an impostor.\n\nIn the hands-off scenario, however, reliable authentication is much more difficult to achieve. It is prudent to assume that the challenge-response behavior of each PUF is known publicly. (An attacker may get hold of a genuine PUF for a while and perform a lot of measurements on it without being discovered.) This is a \"worst case\" assumption as customary in security research. It poses no problem in the hands-on case, but in the hands-off case it means that spoofing becomes a real danger. Imagine for instance authentication of an optical PUF through a glass fiber.\nThe attacker does not have the PUF, but he knows everything about it. He receives the challenge (laser light) through the fiber. Instead of scattering the light off a physical object, he does the following:\n(i) measure the incoming wave front; \n(ii) look up the corresponding response in his database; \n(iii) prepare laser light in the correct response state and send it back to the verifier.\nThis attack is known as \"digital emulation\".\n\nFor a long time spoofing in the hands-off scenario has seemed to be a fundamental problem that\ncannot be solved.\nThe traditional approach to remote object authentication is to somehow enforce a\nhands-on environment, e.g. by having a tamper-proof trusted remote device probing the object.\nDrawbacks of this approach are (a) cost and (b) unknown degree of security in the face of ever more sophisticated attacks.\n\nThe problem of spoofing in the hands-off case can be solved using two\nfundamental information-theoretic properties of quantum physics:\n(1) A single quantum in an unknown state cannot be cloned.\n(2) When a quantum state is measured most of the information it contains\nis destroyed.\n\nBased on these principles, the following scheme was proposed.\nSteps 2-4 are repeated multiple times in order to exponentially lower the false accept probability.\n\nThe crucial point is that the attacker cannot determine what the actual challenge is, because\nthat information is packaged in a \"fragile\" quantum state. \nIf he tries to investigate the challenge state by measuring it, he destroys part of the information.\nNot knowing where exactly to look in his challenge-response database, the attacker cannot reliably produce correct responses. \n\nA continuous-variable quantum authentication of PUFs has been also proposed in the literature, which relies on standard wave-front shaping and homodyne detection techniques.\n\nThe scheme is secure only if the following conditions are met,\n\nIn multiple-scattering optical systems the above requirements can be met in practice.\n\nQuantum Readout of PUFs is \"unconditionally\" secure against digital emulation, but \n\"conditionally\" against \nphysical cloning and physical emulation.\n\nQuantum Readout of PUFs achieves\n\nImagine Alice and Bob wish to engage in Quantum key distribution on an ad hoc basis, i.e. without ever having exchanged data or matter in the past. They both have an enrolled optical PUF.\nThey look up each other's PUF enrollment data from a trusted source.\nThey run quantum key distribution \"through\" both optical PUFs;\nwith a slight modification of the protocol, they get quantum key distribution and\ntwo-way authentication. \nThe security of their key distribution is unconditional,\nbut the security of the authentication is conditional on the two assumptions mentioned above.\n\nQuantum Readout of speckle-based optical PUFs has been demonstrated in the lab.\n\nThis realization is known under the name Quantum-Secure Authentication.\n\nSecurity has been proven in the case of Challenge Estimation attacks,\nin which the attacker tries to determine the challenge as best as he can using measurements.\nThere are proofs for n=1,\nfor quadrature measurements on coherent states\n\nand for fixed number of quanta n>1.\n\nThe result for dimension K and n quanta\nis that the false acceptance probability in a single round\ncannot exceed (n+1)/(n+K).\n\nThe security of the continuous-variable quantum authentication of PUFs against an emulation attack, has been also addressed in the framework of Holevo’s bound and Fano’s inequality.\n\nhttp://theconversation.com/quantum-physics-can-fight-fraud-by-making-card-verification-unspoofable-35632\n"}
{"id": "22615301", "url": "https://en.wikipedia.org/wiki?curid=22615301", "title": "R. M. Wilson", "text": "R. M. Wilson\n\nRichard Michael Wilson (23 November 1945) is a mathematician and a professor at the California Institute of Technology. Wilson and his PhD supervisor Dijen K. Ray-Chaudhuri, solved Kirkman's schoolgirl problem in 1968. Wilson is known for his work in combinatorial mathematics.\n\nWilson was educated at Indiana University where he was awarded a Bachelor of Science degree in 1966. followed by a Master of Science degree from Ohio State University in 1968. His PhD, also from Ohio State University was awarded in 1969 for research supervised by Dijen K. Ray-Chaudhuri.\n\nHis breakthrough in pairwise balanced designs, and orthogonal Latin squares built upon the groundwork set before him, by R. C. Bose, E. T. Parker, S. S. Shrikhande, and Haim Hanani is widely referenced in Combinatorial Design Theory and Coding Theory.\n\n"}
{"id": "2581136", "url": "https://en.wikipedia.org/wiki?curid=2581136", "title": "Rationalizability", "text": "Rationalizability\n\nIn game theory, rationalizability is a solution concept. The general idea is to provide the weakest constraints on players while still requiring that players are rational and this rationality is common knowledge among the players. It is more permissive than Nash equilibrium. Both require that players respond optimally to some belief about their opponents' actions, but Nash equilibrium requires that these beliefs be correct while rationalizability does not. Rationalizability was first defined, independently, by Bernheim (1984) and Pearce (1984). \n\nGiven a normal-form game, the rationalizable set of actions can be computed as follows: Start with the full action set for each player. Next, remove all actions which are never a best reply to any belief about the opponents' actions -- the motivation for this step is that no rational player could choose such actions. Next, remove all actions which are never a best reply to any belief about the opponents' remaining actions -- this second step is justified because each player knows that the other players are rational. Continue the process until no further actions are eliminated. In a game with finitely many actions, this process always terminates and leaves a non-empty set of actions for each player. These are the rationalizable actions.\n\nConsider a simple coordination game (the payoff matrix is to the right). The row player can play \"a\" if she can reasonably believe that the column player could play \"A\", since \"a\" is a best response to \"A\". She can reasonably believe that the column player can play \"A\" if it is reasonable for the column player to believe that the row player could play \"a\". He can believe that she will play \"a\" if it is reasonable for him to believe that she could play \"a\", etc.\nThis provides an infinite chain of consistent beliefs that result in the players playing (\"a\", \"A\"). This makes (\"a\", \"A\") a rationalizable pair of actions. A similar process can be repeated for (\"b\", \"B\"). \n\nAs an example where not all strategies are rationalizable, consider a prisoner's dilemma pictured to the left. Row player would never play \"c\", since \"c\" is not a best response to any strategy by the column player. For this reason, \"c\" is not rationalizable. \n\nConversely, for two-player games, the set of all rationalizable strategies can be found by iterated elimination of strictly dominated strategies. For this method to hold however, one also needs to consider strict domination by mixed strategies. Consider the game on the right with payoffs of the column player omitted for simplicity. Notice that \"b\" is not strictly dominated by either \"t\" or \"m\" in the pure strategy sense, but it is still dominated by a strategy that would mix \"t\" and \"m\" with probability of each equal to 1/2. This is due to the fact that given any belief about the action of the column player, the mixed strategy will always yield higher expected payoff. This implies that \"b\" is not rationalizable.\n\nMoreover, \"b\" is not a best response to either \"L\" or \"R\" or any mix of the two. This is because an action that is not rationalizable can never be a best response to any opponent's strategy (pure or mixed). This would imply another version of the previous method of finding rationalizable strategies as those that survive the iterated elimination of strategies that are never a best response (in pure or mixed sense).\n\nIn games with more than two players, however, there may be strategies that are not strictly dominated, but which can never be the best response. By the iterated elimination of all such strategies one can find the rationalizable strategies for a multiplayer game.\n\nIt can be easily proved that every Nash equilibrium is a rationalizable equilibrium; however, the inverse is not true. Some rationalizable equilibria are not Nash equilibria. This makes the rationalizability concept a generalization of Nash equilibrium concept. \n\nAs an example, consider the game matching pennies pictured to the right. In this game the only Nash equilibrium is row playing \"h\" and \"t\" with equal probability and column playing \"H\" and \"T\" with equal probability. However, all the pure strategies in this game are rationalizable.\n\nConsider the following reasoning: row can play \"h\" if it is reasonable for her to believe that column will play \"H\". Column can play \"H\" if its reasonable for him to believe that row will play \"t\". Row can play \"t\" if it is reasonable for her to believe that column will play \"T\". Column can play \"T\" if it is reasonable for him to believe that row will play \"h\" (beginning the cycle again). This provides an infinite set of consistent beliefs that results in row playing \"h\". A similar argument can be given for row playing \"t\", and for column playing either \"H\" or \"T\".\n\n"}
{"id": "5660713", "url": "https://en.wikipedia.org/wiki?curid=5660713", "title": "Signed distance function", "text": "Signed distance function\n\nIn mathematics and its applications, the signed distance function (or oriented distance function) of a set \"Ω\" in a metric space determines the distance of a given point \"x\" from the boundary of \"Ω\", with the sign determined by whether \"x\" is in \"Ω\". The function has positive values at points \"x\" inside \"Ω\", it decreases in value as \"x\" approaches the boundary of \"Ω\" where the signed distance function is zero, and it takes negative values outside of \"Ω\". However, the alternative convention is also sometimes taken instead (i.e., negative inside \"Ω\" and positive outside).\n\nIf \"Ω\" is a subset of a metric space, \"X\", with metric, \"d\", then the \"signed distance function\", \"f\", is defined by\n\nwhere formula_2 denotes the boundary of formula_3. For any formula_4,\n\nwhere \"inf\" denotes the infimum.\n\nIf \"Ω\" is a subset of the Euclidean space R with piecewise smooth boundary, then the signed distance function is differentiable almost everywhere, and its gradient satisfies the eikonal equation\n\nIf the boundary of \"Ω\" is \"C\" for \"k\"≥2 (see differentiability classes) then \"d\" is \"C\" on points sufficiently close to the boundary of \"Ω\". In particular, on the boundary \"f\" satisfies\n\nwhere \"N\" is the inward normal vector field. The signed distance function is thus a differentiable extension of the normal vector field. In particular, the Hessian of the signed distance function on the boundary of \"Ω\" gives the Weingarten map.\n\nIf, further, \"Γ\" is a region sufficiently close to the boundary of \"Ω\" that \"f\" is twice continuously differentiable on it, then there is an explicit formula involving the Weingarten map \"W\" for the Jacobian of changing variables in terms of the signed distance function and nearest boundary point. Specifically, if \"T\"(∂\"Ω\",\"μ\") is the set of points within distance \"μ\" of the boundary of \"Ω\" (i.e. the tubular neighbourhood of radius \"μ\"), and \"g\" is an absolutely integrable function on \"Γ\", then\n\nwhere det indicates the determinant and \"dS\" indicates that we are taking the surface integral.\n\nAlgorithms for calculating the signed distance function include the efficient fast marching method, fast sweeping method and the more general level-set method.\n\nSigned distance functions are applied, for example, in computer vision.\n\nThey have also recently been used in a method (advanced by Valve Corporation) to render smooth fonts at large sizes (or alternatively at high DPI) using GPU acceleration. Valve's method computed signed distance fields in raster space in order to avoid the computational complexity of solving the problem in the (continuous) vector space. More recently piece-wise approximation solutions have been proposed (which for example approximate a Bézier with arc splines), but even this way the computation can be too slow for real-time rendering, and it has to be assisted by grid-based discretization techniques to approximate (and cull from the computation) the distance to points that are too far away.\n\n\n"}
{"id": "6774401", "url": "https://en.wikipedia.org/wiki?curid=6774401", "title": "Transformational theory", "text": "Transformational theory\n\nTransformational theory is a branch of music theory David Lewin developed in the 1980s, and formally introduced in his 1987 work, \"Generalized Musical Intervals and Transformations\". The theory—which models musical transformations as elements of a mathematical group—can be used to analyze both tonal and atonal music.\n\nThe goal of transformational theory is to change the focus from musical objects—such as the \"C major chord\" or \"G major chord\"—to relations between objects. Thus, instead of saying that a C major chord is followed by G major, a transformational theorist might say that the first chord has been \"transformed\" into the second by the \"Dominant operation.\" (Symbolically, one might write \"Dominant(C major) = G major.\") While traditional musical set theory focuses on the makeup of musical objects, transformational theory focuses on the intervals or types of musical motion that can occur. According to Lewin's description of this change in emphasis, \"[The transformational] attitude does not ask for some observed measure of extension between reified 'points'; rather it asks: 'If I am \"at\" s and wish to get to t, what characteristic \"gesture\" should I perform in order to arrive there?'\" (from \"Generalized Musical Intervals and Transformations\", hereafter GMIT, p. 159)\n\nThe formal setting for Lewin's theory is a set S (or \"space\") of musical objects, and a set T of transformations on that space. Transformations are modeled as functions acting on the entire space, meaning that every transformation must be applicable to every object.\n\nLewin points out that this requirement significantly constrains the spaces and transformations that can be considered. For example, if the space S is the space of diatonic triads (represented by the Roman numerals I, ii, iii, IV, V, vi, and vii°), the \"Dominant transformation\" must be defined so as to apply to each of these triads. This means, for example, that some diatonic triad must be selected as the \"dominant\" of the diminished triad on vii. Ordinary musical discourse, however, typically holds that the \"dominant\" relationship is only between the I and V chords. (Certainly, no diatonic triad is ordinarily considered the dominant of the diminished triad.) In other words, \"dominant,\" as used informally, is not a function that applies to all chords, but rather describes a particular relationship between two of them.\n\nThere are, however, any number of situations in which \"transformations\" can extend to an entire space. Here, transformational theory provides a degree of abstraction that could be a significant music-theoretical asset. One transformational network can describe the relationships among musical events in more than one musical excerpt, thus offering an elegant way of relating them. For example, figure 7.9 in Lewin's GMIT can describe the first phrases of both the first and third movements of Beethoven's Symphony No. 1 in C Major, Op. 21. In this case, the transformation graph's objects are the same in both excerpts from the Beethoven Symphony, but this graph could apply to many more musical examples when the object labels are removed. Further, such a transformational network that gives only the intervals between pitch classes in an excerpt may also describe the differences in the relative durations of another excerpt in a piece, thus succinctly relating two different domains of music analysis. Lewin's observation that only the transformations, and not the objects on which they act, are necessary to specify a transformational network is the main benefit of transformational analysis over traditional object-oriented analysis.\n\nThe \"transformations\" of transformational theory are typically modeled as functions that act over some musical space S, meaning that they are entirely defined by their inputs and outputs: for instance, the \"ascending major third\" might be modeled as a function that takes a particular pitch class as input and outputs the pitch class a major third above it.\n\nHowever, several theorists have pointed out that ordinary musical discourse often includes more information than functions. For example, a single pair of pitch classes (such as C and E) can stand in multiple relationships: E is both a major third above C and a minor sixth below it. (This is analogous to the fact that, on an ordinary clockface, the number 4 is both four steps clockwise from 12 and 8 steps counterclockwise from it.) For this reason, theorists such as Dmitri Tymoczko have proposed replacing Lewinnian \"pitch class intervals\" with \"paths in pitch class space.\" More generally, this suggests that there are situations where it might not be useful to model musical motion (\"transformations\" in the intuitive sense) using functions (\"transformations\" in the strict sense of Lewinnian theory).\n\nAnother issue concerns the role of \"distance\" in transformational theory. In the opening pages of GMIT, Lewin suggests that a subspecies of \"transformations\" (namely, musical intervals) can be used to model \"directed measurements, distances, or motions \". However, the mathematical formalism he uses—which models \"transformations\" by group elements—does not obviously represent distances, since group elements are not typically considered to have size. (Groups are typically individuated only up to isomorphism, and isomorphism does not necessarily preserve the \"sizes\" assigned to group elements.) Theorists such as Ed Gollin, Dmitri Tymoczko, and Rachel Hall, have all written about this subject, with Gollin attempting to incorporate \"distances\" into a broadly Lewinnian framework.\n\nTymoczko's \"Generalizing Musical Intervals\" contains one of the few extended critiques of transformational theory, arguing (1) that intervals are sometimes \"local\" objects that, like vectors, cannot be transported around a musical space; (2) that musical spaces often have boundaries, or multiple paths between the same points, both prohibited by Lewin's formalism; and (3) that transformational theory implicitly relies on notions of distance extraneous to the formalism as such.\n\nAlthough transformation theory is more than thirty years old, it did not become a widespread theoretical or analytical pursuit until the late 1990s. Following Lewin's revival (in GMIT) of Hugo Riemann's three contextual inversion operations on triads (parallel, relative, and Leittonwechsel) as formal transformations, the branch of transformation theory called Neo-Riemannian theory was popularized by Brian Hyer (1995), Michael Kevin Mooney (1996), Richard Cohn (1997), and an entire issue of the \"Journal of Music Theory\" (42/2, 1998). Transformation theory has received further treatment by Fred Lerdahl (2001), Julian Hook (2002), David Kopp (2002), and many others.\n\nThe status of transformational theory is currently a topic of debate in music-theoretical circles. Some authors, such as Ed Gollin, Dmitri Tymoczko and Julian Hook, have argued that Lewin's transformational formalism is too restrictive, and have called for extending the system in various ways. Others, such as Richard Cohn and Steven Rings, while acknowledging the validity of some of these criticisms, continue to use broadly Lewinnian techniques.\n\n\n\n"}
{"id": "10873846", "url": "https://en.wikipedia.org/wiki?curid=10873846", "title": "Václav Chvátal", "text": "Václav Chvátal\n\nVáclav (Vašek) Chvátal ( is a Professor Emeritus in the Department of Computer Science and Software Engineering at Concordia University in Montreal, Quebec, Canada. He has published extensively on topics in graph theory, combinatorics, and combinatorial optimization.\n\nChvátal was born in Prague in 1946 and educated in mathematics at Charles University in Prague, where he studied under the supervision of Zdeněk Hedrlín. He fled Czechoslovakia in 1968, three days after the Soviet invasion, and completed his Ph.D. in Mathematics at the University of Waterloo, under the supervision of Crispin St. J. A. Nash-Williams, in the fall of 1970. Subsequently, he took positions at McGill University (1971 and 1978-1986), Stanford University (1972 and 1974-1977), the Université de Montréal (1972-1974 and 1977-1978), and Rutgers University (1986-2004) before returning to Montreal for the \nCanada Research Chair in Combinatorial Optimization \nat Concordia (2004-2011) and the Canada Research Chair in Discrete Mathematics (2011-2014) till his retirement.\n\nChvátal first learned of graph theory in 1964, on finding a book by Claude Berge in a Pilsen bookstore and much of his research involves graph theory:\n\nSome of Chvátal's work concerns families of sets, or equivalently hypergraphs, a subject already occurring in his Ph.D. thesis, where he also studied Ramsey theory.\n\nChvátal first became interested in linear programming through the influence of Jack Edmonds while Chvátal was a student at Waterloo. He quickly recognized the importance of cutting planes for attacking combinatorial optimization problems such as computing maximum independent sets and, in particular, introduced the notion of a cutting-plane proof. At Stanford in the 1970s, he began writing his popular textbook, \"Linear Programming\", which was published in 1983.\n\nCutting planes lie at the heart of the branch and cut method used by efficient solvers for the traveling salesman problem. Between 1988 and 2005, the team of David L. Applegate, Robert E. Bixby, Vašek Chvátal, and William J. Cook developed one such solver, Concorde. The team was awarded The Beale-Orchard-Hays Prize for Excellence in Computational Mathematical Programming in 2000 for their ten-page paper enumerating some of Concorde's refinements of the branch and cut method that led to the solution of a 13,509-city instance and it was awarded the Frederick W. Lanchester Prize in 2007 for their book, \"The Traveling Salesman Problem: A Computational Study\".\nChvátal is also known for proving the art gallery theorem, for researching a self-describing digital sequence, for his work with David Sankoff on the Chvátal–Sankoff constants controlling the behavior of the longest common subsequence problem on random inputs, and for his work with Endre Szemerédi on hard instances for resolution theorem proving.\n\n\n"}
{"id": "35544539", "url": "https://en.wikipedia.org/wiki?curid=35544539", "title": "Ward's conjecture", "text": "Ward's conjecture\n\nIn mathematics, Ward's conjecture is the conjecture made by that \"many (and perhaps all?) of the ordinary and partial differential equations that are regarded as being integrable or solvable may be obtained from the self-dual gauge field equations (or its generalizations) by reduction\".\n\n explain how a variety of completely integrable equations such as the Korteweg-de Vries equation or KdV equation, the Kadomtsev–Petviashvili equation or KP equation, the nonlinear Schrödinger equation, the sine-Gordon equation, the Ernst equation and the Painlevé equations all arise as reductions or other simplifications of the self-dual Yang-Mills equation\n\nwhere formula_2 is the curvature of a connection on an oriented 4-dimensional pseudo-Riemannian metric, and formula_3 is the Hodge star operator.\n\nThey also obtain the equations of an integrable system known as the Euler–Arnold–Manakov top, a generalization of the Euler top, and they state that the Kowalevsaya top is also a reduction of the self-dual Yang-Mills equations.\n\nVia the Penrose-Ward transform these solutions give the holomorphic vector bundles often seen in the context of algebraic integrable systems.\n\n"}
{"id": "1226978", "url": "https://en.wikipedia.org/wiki?curid=1226978", "title": "X-machine", "text": "X-machine\n\nThe X-machine (\"XM\") is a theoretical model of computation introduced by Samuel Eilenberg in 1974.\nThe \"X\" in \"X-machine\" represents the fundamental data type on which the machine operates; for example, a machine that operates on databases (objects of type \"database\") would be a \"database\"-machine. The X-machine model is structurally the same as the finite state machine, except that the symbols used to label the machine's transitions denote relations of type \"X\"→\"X\". Crossing a transition is equivalent to applying the relation that labels it (computing a set of changes to the data type \"X\"), and traversing a path in the machine corresponds to applying all the associated relations, one after the other.\n\nEilenberg's original X-machine was a completely general theoretical model of computation (subsuming the Turing machine, for example), which admitted deterministic, non-deterministic and non-terminating computations. His seminal work published many variants of the basic X-machine model, each of which generalized the finite state machine in a slightly different way.\n\nIn the most general model, an X-machine is essentially a \"machine for manipulating objects of type X\". Suppose that X is some datatype, called the \"fundamental datatype\", and that Φ is a set of (partial) relations φ: X → X. An X-machine is a finite state machine whose arrows are labelled by relations in Φ. In any given state, one or more transitions may be \"enabled\" if the domain of the associated relation φ accepts (a subset of) the current values stored in X. In each cycle, all enabled transitions are assumed to be taken. Each recognised path through the machine generates a list φ ... φ of relations. We call the composition φ ... φ of these relations the \"path relation\" corresponding to that path. The \"behaviour\" of the X-machine is defined to be the union of all the behaviours computed by its path relations. In general, this is non-deterministic, since applying any relation computes a set of outcomes on X. In the formal model, all possible outcomes are considered together, in parallel.\n\nFor practical purposes, an X-machine should describe some finite computation. An encoding function α: Y → X converts from some \"input\" data type Y into the initial state of X, and a decoding function β: X → Z, converts back from the final state(s) of X into some \"output\" data type Z. Once the initial state of X is populated, the X-machine runs to completion, and the outputs are then observed. In general, a machine may deadlock (be blocked), or livelock (never halt), or perform one or more complete computations. For this reason, more recent research has focused on deterministic X-machines, whose behaviour can be controlled and observed more precisely.\n\nA compiler with a peep-hole optimizer can be thought of as a machine for optimizing program structure. In this Optimizer-machine, the encoding function α takes source code from the input-type Y (the program source) and loads it into the memory-type X (a parse tree). Suppose that the machine has several states, called FindIncrements, FindSubExprs and Completed. The machine starts in the initial state FindIncrements, which is linked to other states via the transitions:\n\nThe relation DoIncrement maps a parsed subtree corresponding to \"x := x + 1\" into the optimized subtree \"++x\". The relation DoSubExpr maps a parse tree containing multiple occurrences of the same expression \"x + y ... x + y\" into an optimized version with a local variable to store the repeated computation \"z := x + y; ... z ... z\". These relations are only enabled if X contains the domain values (subtrees) on which they operate. The remaining relations SkipIncrement and SkipSubExpr are \"nullops\" (identity relations) enabled in the complementary cases.\n\nSo, the Optimizer-machine will run to completion, first converting trivial additions into in-place increments (while in the FindIncrements state), then it will move on to the FindSubExprs state and perform a series of common sub-expression removals, after which it will move to the final state Completed. The decoding function β will then map from the memory-type X (the optimized parse-tree) into the output-type Z (optimized machine code).\n\nWhen referring to Eilenberg's original model, \"X-machine\" is typically written with a lower-case \"m\", because the sense is \"any machine for processing X\". When referring to later specific models, the convention is to use a capital \"M\" as part of the proper name of that variant.\n\nInterest in the X-machine was revived in the late 1980s by Mike Holcombe, who noticed that the model was ideal for software formal specification purposes, because it cleanly separates \"control flow\" from \"processing\". Provided one works at a sufficiently abstract level, the control flows in a computation can usually be represented as a finite state machine, so to complete the X-machine specification all that remains is to specify the processing associated with each of the machine's transitions. The structural simplicity of the model makes it extremely flexible; other early illustrations of the idea included Holcombe's specification of human-computer interfaces,\nhis modelling of processes in cell biochemistry,\nand Stannett's modelling of decision-making in military command systems.\n\nX-machines have received renewed attention since the mid-1990s, when Gilbert Laycock's deterministic Stream X-Machine was found to serve as the basis for specifying large software systems that are \"completely\" testable. Another variant,\nthe Communicating Stream X-Machine offers a useful testable model for biological processes \nand future \"swarm-based\" satellite systems.\nX-machines have been applied to lexical semantics by Andras Kornai, who models word meaning by `pointed' machines that have one member of the base set X distinguished. Application to other branches of linguistics, in particular to a contemporary reformulation of Pāṇini were pioneered by Gerard Huet and his co-workers\n\nThe X-machine is rarely encountered in its original form, but underpins several subsequent models of computation. The most influential model on theories of software testing has been the Stream X-Machine. NASA has recently discussed using a combination of Communicating Stream X-Machines and the process calculus WSCSS in the design and testing of \"swarm satellite\" systems.\n\nThe earliest variant, the continuous-time \"Analog X-Machine\" (\"AXM\"), was introduced by Mike Stannett in 1990 as a potentially \"super-Turing\" model of computation;\nit is consequently related to work in hypercomputation theory.\n\nThe most commonly encountered X-machine variant is Gilbert Laycock's 1993 \"Stream X-Machine\" (\"SXM\") model,\nwhich forms the basis for Mike Holcombe and Florentin Ipate's theory of \"complete\" software testing, which guarantees known correctness properties, once testing is over. The Stream X-Machine differs from Eilenberg's original model, in that the fundamental data type X is of the form \"Out\"* × \"Mem\" × \"In\"*, where \"In\"* is an input sequence, \"Out\"* is an output sequence, and \"Mem\" is the (rest of the) memory.\n\nThe advantage of this model is that it allows a system to be driven, one step at a time, through its states and transitions, while observing the outputs at each step. These are witness values, that guarantee that particular functions were executed on each step. As a result, complex software systems may be decomposed into a hierarchy of Stream X-Machines, designed in a top-down way and tested in a bottom-up way. This divide-and-conquer approach to design and testing is backed by Florentin Ipate's proof of correct integration, which proves how testing the layered machines independently is equivalent to testing the composed system.\n\nThe earliest proposal for connecting several X-machines in parallel is Judith Barnard's 1995 \"Communicating X-machine\" (\"CXM\" or \"COMX\") model,\nin which machines are connected via named communication channels (known as \"ports\"); this model exists in both discrete- and real-timed variants. Earlier versions of this work were not fully formal and did not show full input/output relations.\n\nA similar Communicating X-Machine approach using buffered\nchannels was developed by Petros Kefalas. The focus of this work was on expressiveness in the composition of components. The ability to reassign channels meant that some of the testing theorems from Stream X-Machines did not carry over.\n\nThese variants are discussed in more detail on a separate page.\n\nThe first fully formal model of concurrent X-machine composition was proposed in 1999 by Cristina Vertan and Horia Georgescu, based on earlier work on communicating automatata by Philip Bird and Anthony Cowling. In Vertan's model, the machines communicate indirectly, via a shared \"communication matrix\" (essentially an array of pigeonholes), rather than directly via shared channels.\n\nBălănescu, Cowling, Georgescu, Vertan and others have studied the formal properties of this CSXM model in some detail. Full input/output relations can be shown. The \"communication matrix\" establishes a protocol for synchronous communication. The advantage of this is that it decouples each machine's processing from their communication, allowing the separate testing of each behaviour. This compositional model was proven equivalent to a standard Stream X-Machine, so leveraging the earlier testing theory developed by Holcombe and Ipate.\n\nThis X-machine variant is discussed in more detail on a separate page.\n\nKirill Bogdanov and Anthony Simons developed several variants of the X-machine to model the behaviour of objects in object-oriented systems. This model differs from the Stream X-Machine approach, in that the monolithic data type X is distributed over, and encapsulated by, several objects, which are serially composed; and systems are driven by method invocations and returns, rather than by inputs and outputs. \nFurther work in this area concerned adapting the formal testing theory in the context of inheritance, which partitions the state-space of the superclass in extended subclass objects.\n\nA \"CCS-augmented X-machine\" (CCSXM) model was later developed by Simons and Stannett in 2002 to support complete behavioural testing of object-oriented systems, in the presence of asynchronous communication This is expected to bear some similarity with NASA's recent proposal; but no definitive comparison of the two models has as yet been conducted.\n\n\n\n"}
