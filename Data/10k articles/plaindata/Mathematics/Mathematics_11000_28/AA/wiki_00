{"id": "407340", "url": "https://en.wikipedia.org/wiki?curid=407340", "title": "93 (number)", "text": "93 (number)\n\n93 (ninety-three) is the natural number following 92 and preceding 94.\n\n93 is:\n\n\nThere are 93 different cyclic Gilbreath permutations on 11 elements, and therefore there are 93 different real periodic points of order 11 on the Mandelbrot set.\n\nNinety-three is:\n\nIn classical Persian finger counting, the number 93 is represented by a closed fist. Because of this, classical Arab and Persian poets around 1 CE referred to someone's lack of generosity by saying that the person's hand made \"ninety-three\".\n\n\n"}
{"id": "13629602", "url": "https://en.wikipedia.org/wiki?curid=13629602", "title": "Allegory (category theory)", "text": "Allegory (category theory)\n\nIn the mathematical field of category theory, an allegory is a category that has some of the structure of the category of sets and binary relations between them. Allegories can be used as an abstraction of categories of relations, and in this sense the theory of allegories is a generalization of relation algebra to relations between different sorts. Allegories are also useful in defining and investigating certain constructions in category theory, such as exact completions.\n\nIn this article we adopt the convention that morphisms compose from right to left, so \"RS\" means \"first do \"S\", then do \"R\".\n\nAn allegory is a category in which\nall such that\nHere, we are abbreviating using the order defined by the intersection: \"R\"⊆\"S\" means \"R\" = \"R\"∩\"S\"\".\n\nA first example of an allegory is the category of sets and relations. The objects of this allegory are sets, and a morphism \"X → Y\" is a binary relation between \"X\" and \"Y\". Composition of morphisms is composition of relations; intersection of morphisms is intersection of relations.\n\nIn a category \"C\", a relation between objects \"X\", \"Y\" is a span of morphisms \"X←R→Y\" that is jointly-monic. Two such spans \"X←S→Y\" and \"X←T→Y\" are considered equivalent when there is an isomorphism between S and T that make everything commute, and strictly speaking relations are only defined up to equivalence (one may formalise this either using equivalence classes or using bicategories). If the category \"C\" has products, a relation between \"X\" and \"Y\" is the same thing as a monomorphism into \"X\"×\"Y\" (or an equivalence class of such). In the presence of pullbacks and a proper factorization system, one can define the composition of relations. The composition of \"X←R→Y←S→Z\" is found by first pulling back the cospan \"R→Y←S\" and then taking the jointly-monic image of the resulting span \"X←R←·→S→Z\".\n\nComposition of relations will be associative if the factorization system is appropriately stable. In this case one can consider a category Rel(\"C\"), with the same objects as \"C\", but where morphisms are relations between the objects. The identity relations are the diagonals \"X\"→\"X\"×\"X\".\n\nRecall that a regular category is a category with finite limits and images in which covers are stable under pullback. A regular category has a stable regular epi/mono factorization system. The category of relations for a regular category is always an allegory. Anti-involution is defined by turning the source/target of the relation around, and intersections are intersections of subobjects, computed by pullback.\n\nA morphism \"R\" in an allegory \"A\" is called a map if it is entire (1⊆\"R\"°\"R\") and deterministic (\"RR\"°⊆1). Another way of saying this: a map is a morphism that has a right adjoint in \"A\", when \"A\" is considered, using the local order structure, as a 2-category. Maps in an allegory are closed under identity and composition. Thus there is a subcategory Map(\"A\") of \"A\", with the same objects but only the maps as morphisms. For a regular category \"C\", there is an isomorphism of categories \"C\"≅Map(Rel(\"C\")). In particular, a morphism in Map(Rel(Set)) is just an ordinary set function.\n\nIn an allegory, a morphism \"R:X→Y\" is tabulated by a pair of maps \"f\":\"Z→X\", \"g\":\"Z→Y\" if \"gf\"°=R and \"f\"°\"f\"∩\"g\"°\"g\"=1. An allegory is called tabular if every morphism has a tabulation. For a regular category \"C\", the allegory Rel(\"C\") is always tabular. On the other hand, for any tabular allegory \"A\", the category Map(\"A\") of maps is a locally regular category: it has pullbacks, equalizers and images that are stable under pullback. This is enough to study relations in Map(\"A\") and, in this setting, \"A\"≅Rel(Map(\"A\")).\n\nA unit in an allegory is an object \"U\" for which the identity is the largest morphism \"U→U\", and such that from every other object there is an entire relation to \"U\". An allegory with a unit is called unital. Given a tabular allegory \"A\", the category Map(\"A\") is a regular category (it has a terminal object) if and only if \"A\" is unital.\n\nAdditional properties of allegories can be axiomatized. Distributive allegories have a union-like operation that is suitably well-behaved, and division allegories have a generalization of the division operation of relation algebra. Power allegories are distributive division allegories with additional powerset-like structure. The connection between allegories and regular categories can be developed into a connection between power allegories and toposes.\n"}
{"id": "11417634", "url": "https://en.wikipedia.org/wiki?curid=11417634", "title": "Amenable Banach algebra", "text": "Amenable Banach algebra\n\nA Banach algebra, \"A\", is amenable if all bounded derivations from \"A\" into dual Banach \"A\"-bimodules are inner (that is of the form formula_1 for some formula_2 in the dual module).\n\nAn equivalent characterization is that \"A\" is amenable if and only if it has a virtual diagonal.\n\n\n"}
{"id": "2546", "url": "https://en.wikipedia.org/wiki?curid=2546", "title": "Automated theorem proving", "text": "Automated theorem proving\n\nAutomated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. Automated reasoning over mathematical proof was a major impetus for the development of computer science.\n\nWhile the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege's \"Begriffsschrift\" (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic. His \"Foundations of Arithmetic\", published 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential \"Principia Mathematica\", first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.\n\nIn 1929, Mojżesz Presburger showed that the theory of natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.\nHowever, shortly after this positive result, Kurt Gödel published \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems\" (1931), showing that in any sufficiently strong axiomatic system there are true statements which cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples for undecidable questions.\n\nShortly after World War II, the first general purpose computers became available. In 1954, Martin Davis programmed Presburger's algorithm for a JOHNNIAC vacuum tube computer at the Princeton Institute for Advanced Study. According to Davis, \"Its great triumph was to prove that the sum of two even numbers is even\". More ambitious was the Logic Theory Machine in 1956, a deduction system for the propositional logic of the \"Principia Mathematica\", developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theory Machine constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the \"Principia\".\n\nThe \"heuristic\" approach of the Logic Theory Machine tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle. In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore's program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.\n\nDepending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the frequent case of propositional logic, the problem is decidable but co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first order predicate calculus, Gödel's completeness theorem states that the theorems (provable statements) are exactly the logically valid well-formed formulas, so identifying valid formulas is recursively enumerable: given unbounded resources, any valid formula can eventually be proven. However, \"invalid\" formulas (those that are \"not\" entailed by a given theory), cannot always be recognized.\n\nThe above applies to first order theories, such as Peano arithmetic. However, for a specific model that may be described by a first order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel's incompleteness theorem, we know that any theory whose proper axioms are true for the natural numbers cannot prove all first order statements true for the natural numbers, even if the list of proper axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first order theory (such as the integers).\n\nA simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.\n\nSince the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial and various techniques aiming at making the prover's output smaller, and consequently more easily understandable and checkable, have been developed.\n\nProof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.\n\nAnother distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference. Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).\n\nThere are hybrid theorem proving systems which use model checking as an inference rule. There are also programs which were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof which was essentially impossible to verify by humans due to the enormous size of the program's calculation (such proofs are called non-surveyable proofs). Another example of a program-assisted proof is the one that shows that the game of Connect Four can always be won by first player.\n\nCommercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification. Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.\n\nIn the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, Java etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using J.A. Robinson's resolution Principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.\n\nFirst-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling \"fully\" automated systems. More expressive logics, such as higher order logics, allow the convenient expression of a wider range of problems than first order logic, but theorem proving for these logics is less well developed.\n\nThe quality of implemented systems has benefited from the existence of a large library of standard benchmark examples — the Thousands of Problems for Theorem Provers (TPTP) Problem Library — as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.\n\nSome important systems (all have won at least one CASC competition division) are listed below.\n\nThe Theorem Prover Museum is an initiative to conserve the sources of theorem prover systems for future analysis, since they are important cultural/scientific artefacts. It has the sources of many of the systems mentioned above.\n\n\n\n\n\n\n\n"}
{"id": "31345089", "url": "https://en.wikipedia.org/wiki?curid=31345089", "title": "BS 8888", "text": "BS 8888\n\nBS 8888 is the British standard developed by the BSI Group for technical product documentation, geometric product specification, geometric tolerance specification and engineering drawings.\n\nBS 308 was formerly the standard for engineering drawing since 1927. Over a period of 73 years it was expanded and edited until its withdrawal in 2000. The BSI Group, who produced the standard, played an important role in the development of the international standard on technical specification in conjunction with the ISO. In 2000, the BS 308 was replaced by the updated BS 8888.\n\nA significant change in the 2013 revision is that there is no longer a requirement to state whether specifications have been toleranced in accordance with either the Principle of Independency or the Principle of Dependency.\n\nThis updated version of the standard has been restructured to be more aligned to the workflow of designers and engineers to assist throughout the design process. The standard now references 3D geometry, not only as drawings but also allowing a 3D surface to be used as a datum feature. \n\nBS 8888 performs three fundamental tasks:\n\n"}
{"id": "2033586", "url": "https://en.wikipedia.org/wiki?curid=2033586", "title": "Back-and-forth method", "text": "Back-and-forth method\n\nIn mathematical logic, especially set theory and model theory, the back-and-forth method is a method for showing isomorphism between countably infinite structures satisfying specified conditions. In particular:\n\n\nSuppose that\n\n\nFix enumerations (without repetition) of the underlying sets:\n\nNow we construct a one-to-one correspondence between \"A\" and \"B\" that is strictly increasing. Initially no member of \"A\" is paired with any member of \"B\".\n\nIt still has to be checked that the choice required in step (1) and (2) can actually be made in accordance to the requirements. Using step (1) as an example:\n\nIf there are already \"a\" and \"a\" in \"A\" corresponding to \"b\" and \"b\" in \"B\" respectively such that \"a\" < \"a\" < \"a\" and \"b\" < \"b\", we choose \"b\" in between \"b\" and \"b\" using density. Otherwise, we choose a suitable large or small element of \"B\" using the fact that \"B\" has neither a maximum nor a minimum. Choices made in step (2) are dually possible. Finally, the construction ends after countably many steps because \"A\" and \"B\" are countably infinite. Note that we had to use all the prerequisites.\n\nAccording to Hodges (1993):\nWhile the theorem on countable densely ordered sets is due to Cantor (1895), the back-and-forth method with which it is now proved was developed by Huntington (1904) and Hausdorff (1914). Later it was applied in other situations, most notably by Roland Fraïssé in model theory.\n\n\n"}
{"id": "13520509", "url": "https://en.wikipedia.org/wiki?curid=13520509", "title": "Baire measure", "text": "Baire measure\n\nIn mathematics, a Baire measure is a measure on the σ-algebra of Baire sets of a topological space whose value on every compact Baire set is finite. In compact metric spaces the Borel sets and the Baire sets are the same, so Baire measures are the same as Borel measures that are finite on compact sets. In general Baire sets and Borel sets need not be the same. In spaces with non-Baire Borel sets, Baire measures are used because they connect to the properties of continuous functions more directly.\n\nThere are several inequivalent definitions of Baire sets, so correspondingly there are several inequivalent concepts of Baire measure on a topological space. These all coincide on spaces that are locally compact σ-compact Hausdorff spaces. \n\nIn practice Baire measures can be replaced by regular Borel measures. The relation between Baire measures and regular Borel measures is as follows: \n\n\n\n"}
{"id": "5203165", "url": "https://en.wikipedia.org/wiki?curid=5203165", "title": "Butterfly curve (algebraic)", "text": "Butterfly curve (algebraic)\n\nIn mathematics, the algebraic butterfly curve is a plane algebraic curve of degree six, given by the equation \n\nThe butterfly curve has a single singularity with delta invariant three, which means it is a curve of genus seven. The only plane curves of genus seven are singular, since seven is not a triangular number, and the minimum degree for such a curve is six, so the butterfly curve aside from its appearance is possibly interesting as an example.\n\nThe butterfly curve has branching number and multiplicity two, and hence the singularity link has two components, pictured at right.\n\nThe area of the algebraic butterfly curve is given by (with gamma function formula_2)\nand its arc length \"s\" by\n\n\n\n"}
{"id": "187926", "url": "https://en.wikipedia.org/wiki?curid=187926", "title": "Centroid", "text": "Centroid\n\nIn mathematics and physics, the centroid or geometric center of a plane figure is the arithmetic mean position of all the points in the figure. Informally, it is the point at which a cutout of the shape could be perfectly balanced on the tip of a pin.\n\nThe definition extends to any object in \"n\"-dimensional space: its centroid is the mean position of all the points in all of the coordinate directions.\n\nWhile in geometry the term barycenter is a synonym for \"centroid\", in astrophysics and astronomy, barycenter is the center of mass of two or more bodies that orbit each other. In physics, the center of mass is the arithmetic mean of all points weighted by the local density or specific weight. If a physical object has uniform density, then its center of mass is the same as the centroid of its shape.\n\nIn geography, the centroid of a radial projection of a region of the Earth's surface to sea level is known as the region's geographical center.\n\nThe term \"centroid\" is of recent coinage (1814). It is used as a substitute for the older terms \"center of gravity,\" and \"center of mass\", when the purely geometrical aspects of that point are to be emphasized. The term is peculiar to the English language. The French use \"centre de gravité\" on most occasions, and others use terms of similar meaning.\n\nThe center of gravity, as the name indicates, is a notion that arose in mechanics, most likely in connection with building activities. When, where, and by whom it was invented is not known, as it is a concept that likely occurred to many people individually with minor differences. \n\nWhile it is possible Euclid was still active in Alexandria during the childhood of Archimedes (287-212 BCE), it is certain that when Archimedes visited Alexandria, Euclid was no longer there. Thus Archimedes could not have learned the theorem that the medians of a triangle meet in a point—the center of gravity of the triangle directly from Euclid, as this proposition is not in Euclid's Elements. The first explicit statement of this proposition is due to Heron of Alexandria (perhaps the first century CE) and occurs in his Mechanics. It may be added, in passing, that the proposition did not become common in the textbooks on plane geometry until the nineteenth century.\n\nWhile Archimedes does not state that proposition explicitly, he makes indirect references to it, suggesting he was familiar with it. However, Jean Etienne Montucla (1725-1799), the author of the first history of mathematics (1758), declares categorically (vol. I, p. 463) that the center of gravity of solids is a subject Archimedes did not touch.\n\nIn 1802 Charles Bossut (1730-1813) published a two-volume Essai aur PhisMire generale des mathematiques. This book was highly esteemed by his contemporaries, judging from the fact that within two years after its publication it was already available in translation in Italian (1802-03), English (1803), and German (1804). Bossut credits Archimedes with having found the centroid of plane figures, but has nothing to say about solids.\n\nThe geometric centroid of a convex object always lies in the object. A non-convex object might have a centroid that is outside the figure itself. The centroid of a ring or a bowl, for example, lies in the object's central void.\n\nIf the centroid is defined, it is a fixed point of all isometries in its symmetry group. In particular, the geometric centroid of an object lies in the intersection of all its hyperplanes of symmetry. The centroid of many figures (regular polygon, regular polyhedron, cylinder, rectangle, rhombus, circle, sphere, ellipse, ellipsoid, superellipse, superellipsoid, etc.) can be determined by this principle alone.\n\nIn particular, the centroid of a parallelogram is the meeting point of its two diagonals. This is not true for other quadrilaterals.\n\nFor the same reason, the centroid of an object with translational symmetry is undefined (or lies outside the enclosing space), because a translation has no fixed point.\n\nThe centroid of a triangle is the intersection of the three medians of the triangle (each median connecting a vertex with the midpoint of the opposite side). It lies on the triangle's Euler line, which also goes through various other key points including the orthocenter and the circumcenter.\n\nAny of the three medians through the centroid divides the triangle's area in half. This is not true for other lines through the centroid; the greatest departure from the equal-area division occurs when a line through the centroid is parallel to a side of the triangle, creating a smaller triangle and a trapezoid; in this case the trapezoid's area is 5/9 that of the original triangle.\n\nLet \"P\" be any point in the plane of a triangle with vertices \"A, B,\" and \"C\" and centroid \"G\". Then the sum of the squared distances of \"P\" from the three vertices exceeds the sum of the squared distances of the centroid \"G\" from the vertices by three times the squared distance between \"P\" and \"G\":\n\nThe sum of the squares of the triangle's sides equals three times the sum of the squared distances of the centroid from the vertices:\n\nA triangle's centroid is the point that maximizes the product of the directed distances of a point from the triangle's sidelines.\n\nFor other properties of a triangle's centroid, see below.\n\nThe centroid of a uniformly dense planar lamina, such as in figure (a) below, may be determined experimentally by using a plumbline and a pin to find the collocated center of mass of a thin body of uniform density having the same shape. The body is held by the pin, inserted at a point, off the presumed centroid in such a way that it can freely rotate around the pin; the plumb line is then dropped from the pin (figure b). The position of the plumbline is traced on the surface, and the procedure is repeated with the pin inserted at any different point (or a number of points) off the centroid of the object. The unique intersection point of these lines will be the centroid (figure c). Provided that the body is of uniform density, all lines made this way will include the centroid, and all lines will cross at the exact same place.\n\nThis method can be extended (in theory) to concave shapes where the centroid may lie outside the shape, and virtually to solids (again, of uniform density), where the centroid may lie within the body. The (virtual) positions of the plumb lines need to be recorded by means other than by drawing them along the shape.\n\nFor convex two-dimensional shapes, the centroid can be found by balancing the shape on a smaller shape, such as the top of a narrow cylinder. The centroid occurs somewhere within the range of contact between the two shapes (and exactly at the point where the shape would balance on a pin). In principle, progressively narrower cylinders can be used to find the centroid to arbitrary precision. In practice air currents make this infeasible. However, by marking the overlap range from multiple balances, one can achieve a considerable level of accuracy.\n\nThe centroid of a finite set of formula_3 points formula_4 in formula_5 is\nThis point minimizes the sum of squared Euclidean distances between itself and each point in the set.\n\nThe centroid of a plane figure formula_7 can be computed by dividing it into a finite number of simpler figures formula_8, computing the centroid formula_9 and area formula_10 of each part, and then computing\n\nHoles in the figure formula_7, overlaps between the parts, or parts that extend outside the figure can all be handled using negative areas formula_10. Namely, the measures formula_10 should be taken with positive and negative signs in such a way that the sum of the signs of formula_10 for all parts that enclose a given point formula_16 is 1 if formula_16 belongs to formula_7, and 0 otherwise.\n\nFor example, the figure below (a) is easily divided into a square and a triangle, both with positive area; and a circular hole, with negative area (b).\n\nThe centroid of each part can be found in any list of centroids of simple shapes (c). Then the centroid of the figure is the weighted average of the three points. The horizontal position of the centroid, from the left edge of the figure is\nThe vertical position of the centroid is found in the same way.\n\nThe same formula holds for any three-dimensional objects, except that each formula_10 should be the volume of formula_21, rather than its area. It also holds for any subset of formula_22, for any dimension formula_23, with the areas replaced by the formula_23-dimensional measures of the parts.\n\nThe centroid of a subset \"X\" of formula_25 can also be computed by the integral\n\nwhere the integrals are taken over the whole space formula_25, and \"g\" is the characteristic function of the subset, which is 1 inside \"X\" and 0 outside it. Note that the denominator is simply the measure of the set \"X\". This formula cannot be applied if the set \"X\" has zero measure, or if either integral diverges.\n\nAnother formula for the centroid is\n\nwhere \"C\" is the \"k\"th coordinate of \"C\", and \"S\"(\"z\") is the measure of the intersection of \"X\" with the hyperplane defined by the equation \"x\" = \"z\". Again, the denominator is simply the measure of \"X\".\n\nFor a plane figure, in particular, the barycenter coordinates are\n\nwhere \"A\" is the area of the figure \"X\"; \"S\"(\"x\") is the length of the intersection of \"X\" with the vertical line at abscissa \"x\"; and \"S\"(\"y\") is the analogous quantity for the swapped axes.\n\nThe centroid formula_31 of a region bounded by the graphs of the continuous functions formula_32 and formula_33 such that formula_34 on the interval formula_35, formula_36, is given by\n\nwhere formula_39 is the area of the region (given by formula_40).\n\nThis is a method of determining the centroid of an L-shaped object.\n\n\nThe centroid of a triangle is the point of intersection of its medians (the lines joining each vertex with the midpoint of the opposite side). The centroid divides each of the medians in the ratio 2:1, which is to say it is located ⅓ of the distance from each side to the opposite vertex (see figures at right). Its Cartesian coordinates are the means of the coordinates of the three vertices. That is, if the three vertices are formula_41 formula_42 and formula_43 then the centroid (denoted \"C\" here but most commonly denoted \"G\" in triangle geometry) is\n\nThe centroid is therefore at formula_45 in barycentric coordinates.\n\nIn trilinear coordinates the centroid can be expressed in any of these equivalent ways in terms of the side lengths \"a, b, c\" and vertex angles \"L, M, N\":\n\nThe centroid is also the physical center of mass if the triangle is made from a uniform sheet of material; or if all the mass is concentrated at the three vertices, and evenly divided among them. On the other hand, if the mass is distributed along the triangle's perimeter, with uniform linear density, then the center of mass lies at the Spieker center (the incenter of the medial triangle), which does not (in general) coincide with the geometric centroid of the full triangle.\n\nThe area of the triangle is 1.5 times the length of any side times the perpendicular distance from the side to the centroid.\n\nA triangle's centroid lies on its Euler line between its orthocenter \"H\" and its circumcenter \"O\", exactly twice as close to the latter as to the former:\n\nIn addition, for the incenter \"I\" and nine-point center \"N\", we have\n\nIf G is the centroid of the triangle ABC, then:\n\nThe isogonal conjugate of a triangle's centroid is its symmedian point.\n\nThe centroid of a non-self-intersecting closed polygon defined by \"n\" vertices (\"x\",\"y\"), (\"x\",\"y\"), ..., (\"x\",\"y\") is the point (\"C\", \"C\"), where\n\nand where \"A\" is the polygon's signed area, as described by\n\nIn these formulas, the vertices are assumed to be numbered in order of their occurrence along the polygon's perimeter; furthermore, the vertex ( \"x\", \"y\" ) is assumed to be the same as ( \"x\", \"y\" ), meaning \"i + 1\" on the last case must loop around to \"i = 0\". (If the points are numbered in clockwise order, the area \"A\", computed as above, will be negative; however, the centroid coordinates will be correct even in this case.)\n\nThe centroid of a cone or pyramid is located on the line segment that connects the apex to the centroid of the base. For a solid cone or pyramid, the centroid is 1/4 the distance from the base to the apex. For a cone or pyramid that is just a shell (hollow) with no base, the centroid is 1/3 the distance from the base plane to the apex.\n\nA tetrahedron is an object in three-dimensional space having four triangles as its faces. A line segment joining a vertex of a tetrahedron with the centroid of the opposite face is called a \"median\", and a line segment joining the midpoints of two opposite edges is called a \"bimedian\". Hence there are four medians and three bimedians. These seven line segments all meet at the \"centroid\" of the tetrahedron. The medians are divided by the centroid in the ratio 3:1. The centroid of a tetrahedron is the midpoint between its Monge point and circumcenter (center of the circumscribed sphere). These three points define the \"Euler line\" of the tetrahedron that is analogous to the Euler line of a triangle.\n\nThese results generalize to any \"n\"-dimensional simplex in the following way. If the set of vertices of a simplex is formula_59, then considering the vertices as vectors, the centroid is\n\nThe geometric centroid coincides with the center of mass if the mass is uniformly distributed over the whole simplex, or concentrated at the vertices as \"n\" equal masses.\n\nThe centroid of a solid hemisphere (i.e. half of a solid ball) divides the line segment connecting the sphere's center to the hemisphere's pole in the ratio 3:8. The centroid of a hollow hemisphere (i.e. half of a hollow sphere) divides the line segment connecting the sphere's center to the hemisphere's pole in half.\n\n\n\n"}
{"id": "43768137", "url": "https://en.wikipedia.org/wiki?curid=43768137", "title": "Chaotica (software)", "text": "Chaotica (software)\n\nChaotica is a commercial fractal art editor and renderer extending flam3 and Apophysis's functionality. There is also a free version with limited render resolution and animation length.\n\nChaotica began as a personal project in 2010 but has since been handed over to Glare Technologies, the developers of Indigo Renderer.\n\nChaotica implements a generalized iterated function system and features a modern rendering engine based on advanced algorithms not found in open-source IFS implementations. It has an animation editor, selective randomization of parameters, and imaging controls such as different anti-aliasing modes and RGB-channel response curves.\n\n"}
{"id": "915449", "url": "https://en.wikipedia.org/wiki?curid=915449", "title": "Counterexamples in Topology", "text": "Counterexamples in Topology\n\nCounterexamples in Topology (1970, 2nd ed. 1978) is a book on mathematics by topologists Lynn Steen and J. Arthur Seebach, Jr.\n\nIn the process of working on problems like the metrization problem, topologists (including Steen and Seebach) have defined a wide variety of topological properties. It is often useful in the study and understanding of abstracts such as topological spaces to determine that one property does not follow from another. One of the easiest ways of doing this is to find a counterexample which exhibits one property but not the other. In Counterexamples in Topology, Steen and Seebach, together with five students in an undergraduate research project at St. Olaf College, Minnesota in the summer of 1967, canvassed the field of topology for such counterexamples and compiled them in an attempt to simplify the literature.\n\nFor instance, an example of a first-countable space which is not second-countable is counterexample #3, the discrete topology on an uncountable set. This particular counterexample shows that second-countability does not follow from first-countability.\n\nSeveral other \"Counterexamples in ...\" books and papers have followed, with similar motivations.\n\nIn her review of the first edition, Mary Ellen Rudin wrote:\nIn his submission to Mathematical Reviews C. Wayne Patty wrote:\nWhen the second edition appeared in 1978 its review in Advances in Mathematics treated topology as territory to be explored:\n\nSeveral of the naming conventions in this book differ from more accepted modern conventions, particularly with respect to the separation axioms. The authors use the terms T, T, and T to refer to regular, normal, and completely normal. They also refer to completely Hausdorff as Urysohn. This was a result of the different historical development of metrization theory and general topology; see History of the separation axioms for more.\n\n\n\n"}
{"id": "2270985", "url": "https://en.wikipedia.org/wiki?curid=2270985", "title": "Donald A. Martin", "text": "Donald A. Martin\n\nDonald A. Martin (born December 24, 1940), also known as Tony Martin, is an American set theorist and philosopher of mathematics at UCLA, where he is a member of the faculty of mathematics and philosophy.\n\nAmong Martin's most notable work are the proofs of analytic determinacy (from the existence of a measurable cardinal), Borel determinacy (from ZFC alone), the proof (with John R. Steel) of projective determinacy (from suitable large cardinal axioms), and his work on Martin's axiom. Martin measure on Turing degrees is also named after Martin.\n\nMartin is a Harvard junior fellow. In 2014, he became a Fellow of the American Mathematical Society.\n\n\n"}
{"id": "11693109", "url": "https://en.wikipedia.org/wiki?curid=11693109", "title": "Finite state machine with datapath", "text": "Finite state machine with datapath\n\nA Finite State Machine with Datapath (FSMD) is a mathematical abstraction that is sometimes used to design digital logic or computer programs.\n\nAn FSMD is a digital system composed of a finite-state machine, which controls the program flow, and a datapath, which performs data processing operations.\n\nFSMDs are essentially sequential programs in which statements have been scheduled into states, thus resulting in more complex state diagrams.\n\nHere, a program is converted into a complex state diagram in which states and arcs may include arithmetic expressions, and those expressions may use external inputs and outputs as well as variables.\n\nFSMs do not use variables or arithmetic operations/conditions, thus FSMDs are more powerful than FSMs.\n\nThe FSMD level of abstraction is often referred to as the register-transfer level.\n\nFSMD is equivalent to Turing machine in power.\n"}
{"id": "34299169", "url": "https://en.wikipedia.org/wiki?curid=34299169", "title": "Friedrich Otto Rudolf Sturm", "text": "Friedrich Otto Rudolf Sturm\n\nFriedrich Otto Rudolf Sturm (6 January 1841 – 12 April 1919) was a German mathematician. His Ph.D. advisor was Heinrich Eduard Schroeter, and Otto Toeplitz was one of his Ph.D. students. His best ever proposal type claim is commonly known as \"Sturm's Theorem\" based on finding the complex imaginary roots of an infinite arbitrary-integer series.\n\n\n"}
{"id": "38731112", "url": "https://en.wikipedia.org/wiki?curid=38731112", "title": "Gathering 4 Gardner", "text": "Gathering 4 Gardner\n\nGathering 4 Gardner (G4G) is an educational foundation and non-profit corporation (Gathering 4 Gardner, Inc.) devoted to preserving the legacy and spirit of prolific writer Martin Gardner. G4G organizes conferences where people who have been inspired by or have a strong personal connection to Martin Gardner can meet and celebrate his influence. These events explore ideas and developments in recreational mathematics, magic, illusion, puzzles, philosophy, and rationality, and foster creative work in all of these areas by enthusiasts of all ages. G4G also facilitates a related series of events called Celebration of Mind (CoM).\n\nMartin Gardner's prolific output as a columnist and writer—he authored over 100 books between 1951 and 2010—put him in contact with a large number of people on a wide range of subjects from magic, mathematics, puzzles, physics, philosophy, logic and rationality, to G. K. Chesterton, Alice in Wonderland, and the Wizard of Oz. As a result, he had a large following of amateurs and professionals eager to pay tribute to him, but many of them had only infrequent contact with each other. Moreover, Gardner was famously shy, and generally declined to appear at any events honoring him.\n\nIn the early 1990s, Atlanta-based entrepreneur and puzzle collector Thomas M. Rodgers (1943–2012), a friend of Martin Gardner's, conceived a plan to create a gathering of people who shared Gardner's interests, especially puzzles, magic, and mathematics. Rodgers invited the world's foremost puzzle composers and collectors, and enlisted magician Mark Setteducati and mathematician Elwyn Berlekamp to recruit leading magicians and recreational mathematicians, respectively. Gardner agreed to attend. Thus was born the first Gathering (G4G1), held in Atlanta, GA, in January 1993.\n\nIn 2007 board members Rodgers, Berlekamp, Setteducati, Thane Plambeck, and Scott Hudson de Tarnowsky decided that G4G should broaden its reach and expand the scope of its educational programs. To that end they formed the 501(c)(3) non-profit corporation Gathering 4 Gardner, Inc.\n\nThe logo of Gathering for Gardner, as well as the logo for the first CoM event, employs ambigrams designed by long-time Gardner associate Scott Kim.\n\nContinuing Martin Gardner's pursuit of a playful and fun approach to learning, G4Gn events explore ideas in fields of interest to Gardner. The term G4G is also used to denote the community of people who participate in these events. With the \"n\" denoting the number in the series, G4Gn is an invitation-only bi-annual conference that started with G4G1 in January 1993. A second gathering (G4G2) was held in Atlanta in 1996, and from then on the events have been held bi-annually, in the springs of even-numbered years, the most recent being G4G13 which took place in April 2018 and featured Fields medallist Manjul Bhargava and inventor of the eponymous cube Erno Rubik. To date, all G4Gn conferences have been held in Atlanta, GA. Gardner (and his wife), who disliked travel in addition to wanting to avoid the limelight, attended only the first two G4Gs.\n\nNotable presenters over the years have included\n\nActivities typically include lectures, performance art, puzzle and book displays, close-up and stage magic acts, as well as guided sculpture building. Traditionally, each conference has a Gift Exchange, in which attendees swap puzzles, magic tricks, artwork, mathematical papers, novelty items, books, and CDs/DVDs.\n\nGardner and many of his admirers were filmed at G4G2 in 1996, and this footage formed the basis for a 46-minute episode of The Nature of Things made by David Suzuki. Titled, \"Mystery and Magic of Mathematics: Martin Gardner and Friends\", it showcases Martin's numerous passions, and reminds us of the amazing panoply of people that he informally assembled and mentored over the decades.\n\nDeveloped in 2010 after Martin Gardner's death that spring, Celebration of Mind (CoM) is a worldwide series of events held on or around Gardner's birthday: October 21. Anyone, anywhere, can host one, and most of them are open to the public. These CoMs vary in size from three people meeting to perform rubber-band magic for each other, to crowds of hundreds of students in highly organized exploratory paper folding activities. The goal is to celebrate the boundless creativity and curiosity of the human mind, and they have been held in locations from Boston to Beijing, and from Riga to Rio. There have been Celebration of Mind events hosted on all seven continents, and G4G encourages organizers to register their events at the CoM website. Resources to assist with hosting events are provided through the organization's website, or participants can also just perform a magic trick, or share a puzzle or recreational mathematics problem with friends.\n\n"}
{"id": "5970951", "url": "https://en.wikipedia.org/wiki?curid=5970951", "title": "George Lane (mental calculator)", "text": "George Lane (mental calculator)\n\nGeorge Lane (born 1964 ) is a British mental calculator and author. He is a five-time gold medalist in the Mental Calculations event at the annual Mind Sports Olympiad, and is one of only three Grandmasters of Mental Calculation, as recognised by the Mind Sports Organisation. George is a regular facilitator at the annual Junior Mental Calculation World Championship. He has written two books.\n\n"}
{"id": "44168089", "url": "https://en.wikipedia.org/wiki?curid=44168089", "title": "Hilbert–Bernays paradox", "text": "Hilbert–Bernays paradox\n\nThe Hilbert–Bernays paradox is a distinctive paradox belonging to the family of the paradoxes of reference (like Berry's paradox). It is named after David Hilbert and Paul Bernays.\n\nThe paradox appears in Hilbert and Bernays' \"Grundlagen der Mathematik\" and is used by them to show that a sufficiently strong consistent theory cannot contain its own reference functor. Although it has gone largely unnoticed in the course of the 20th century, it has recently been rediscovered and appreciated for the distinctive difficulties it presents.\n\nJust as the semantic property of truth seems to be governed by the naive schema:\n\n(where we use single quotes to refer to the linguistic expression inside the quotes), the semantic property of reference seems to be governed by the naive schema:\n\nConsider however a name h for (natural) numbers satisfying:\n\nSuppose that, for some number \"n\":\n\nThen, surely, the referent of h exists, and so does (the referent of h)+1. By (R), it then follows that:\n\nand so, by (H) and the principle of indiscernibility of identicals, it is the case that:\n\nBut, again by indiscernibility of identicals, (1) and (3) yield:\n\nand, by transitivity of identity, (1) together with (4) yields:\n\nBut (5) is absurd, since no number is identical with its successor.\n\nSince every sufficiently strong theory will have to accept something like (H), absurdity can only be avoided either by rejecting the principle of naive reference (R) or by rejecting classical logic (which validates the reasoning from (R) and (H) to absurdity). On the first approach, typically whatever one says about the Liar paradox \"carries over smoothly\" to the Hilbert–Bernays paradox. The paradox presents instead \"distinctive difficulties\" for many solutions pursuing the second approach: for example, solutions to the Liar paradox that reject the law of excluded middle (which is \"not\" used by the Hilbert–Bernays paradox) have denied that there is such a thing as the referent of h; solutions to the Liar paradox that reject the law of noncontradiction (which is \"not\" used by the Hilbert–Bernays paradox) have claimed that h refers to more than one object.\n"}
{"id": "51438", "url": "https://en.wikipedia.org/wiki?curid=51438", "title": "Hypercomplex number", "text": "Hypercomplex number\n\nIn mathematics, a hypercomplex number is a traditional term for an element of a unital algebra over the field of real numbers. \nThe study of hypercomplex numbers in the late 19th century forms the basis of modern group representation theory.\n\nIn the nineteenth century number systems called quaternions, tessarines, coquaternions, biquaternions, and octonions became established concepts in mathematical literature, added to the real and complex numbers. The concept of a hypercomplex number covered them all, and called for a discipline to explain and classify them.\n\nThe cataloguing project began in 1872 when Benjamin Peirce first published his \"Linear Associative Algebra\", and was carried forward by his son Charles Sanders Peirce. Most significantly, they identified the nilpotent and the idempotent elements as useful hypercomplex numbers for classifications. The Cayley–Dickson construction used involutions to generate complex numbers, quaternions, and octonions out of the real number system. Hurwitz and Frobenius proved theorems that put limits on hypercomplexity: Hurwitz's theorem says finite-dimensional real composition algebras are the reals ℝ, the complexes ℂ, the quaternions ℍ, and the octonions 𝕆, and the Frobenius theorem says the only real associative division algebras are ℝ, ℂ, and ℍ. In 1958 J. Frank Adams published a further generalization in terms of Hopf invariants on \"H\"-spaces which still limits the dimension to 1, 2, 4, or 8.\n\nIt was matrix algebra that harnessed the hypercomplex systems. First, matrices contributed new hypercomplex numbers like 2 × 2 real matrices. Soon the matrix paradigm began to explain the others as they became represented by matrices and their operations. In 1907 Joseph Wedderburn showed that associative hypercomplex systems could be represented by matrices, or direct sums of systems of matrices. From that date the preferred term for a hypercomplex system became associative algebra as seen in the title of Wedderburn’s thesis at University of Edinburgh. Note however, that non-associative systems like octonions and hyperbolic quaternions represent another type of hypercomplex number.\n\nAs Hawkins explains, the hypercomplex numbers are stepping stones to learning about Lie groups and group representation theory. For instance, in 1929 Emmy Noether wrote on \"hypercomplex quantities and representation theory\". In 1973 Kantor and Solodovnikov published a textbook on hypercomplex numbers which was translated in 1989.\n\nKaren Parshall has written a detailed exposition of the heyday of hypercomplex numbers, including the role of such luminaries as Theodor Molien and Eduard Study. For the transition to modern algebra, Bartel van der Waerden devotes thirty pages to hypercomplex numbers in his \"History of Algebra\".\n\nA definition of a hypercomplex number is given by as an element of a finite-dimensional algebra over the real numbers that is unital and distributive (but not necessarily associative). Elements are generated with real number coefficients formula_1 for a basis formula_2. Where possible, it is conventional to choose the basis so that formula_3. A technical approach to hypercomplex numbers directs attention first to those of dimension two.\n\nTheorem: Up to isomorphism, there are exactly three 2-dimensional unital algebras over the reals: the ordinary complex numbers, the split-complex numbers, and the dual numbers.\n\nfor some real numbers \"a\" and \"a\".\nUsing the common method of completing the square by subtracting \"a\"\"u\" and adding the quadratic complement to both sides yields\nThe three cases depend on this real value:\n\nThe complex numbers are the only two-dimensional hypercomplex algebra that is a field.\nAlgebras such as the split-complex numbers that include non-real roots of 1 also contain idempotents formula_16 and zero divisors formula_17, so such algebras cannot be division algebras. However, these properties can turn out to be very meaningful, for instance in describing the Lorentz transformations of special relativity.\n\nIn a 2004 edition of Mathematics Magazine the two-dimensional real algebras have been styled the \"generalized complex numbers\". The idea of cross-ratio of four complex numbers can be extended to the two-dimensional real algebras.\n\nA Clifford algebra is the unital associative algebra generated over an underlying vector space equipped with a quadratic form. Over the real numbers this is equivalent to being able to define a symmetric scalar product, \"u\"⋅\"v\" = ½(\"uv\" + \"vu\") that can be used to orthogonalise the quadratic form, to give a set of bases {\"e\", ..., \"e\"} such that:\nImposing closure under multiplication generates a multivector space spanned by a basis of 2 elements, {1, \"e\", \"e\", \"e\", ..., \"e\"\"e\", ..., \"e\"\"e\"\"e\", ...}. These can be interpreted as the basis of a hypercomplex number system. Unlike the basis {\"e\", ..., \"e\"}, the remaining basis elements may or may not anti-commute, depending on how many simple exchanges must be carried out to swap the two factors. So , but .\n\nPutting aside the bases for which (i.e. directions in the original space over which the quadratic form was degenerate), the remaining Clifford algebras can be identified by the label \"C\"ℓ(R), indicating that the algebra is constructed from \"p\" simple basis elements with , \"q\" with , and where R indicates that this is to be a Clifford algebra over the reals—i.e. coefficients of elements of the algebra are to be real numbers.\n\nThese algebras, called geometric algebras, form a systematic set, which turn out to be very useful in physics problems which involve rotations, phases, or spins, notably in classical and quantum mechanics, electromagnetic theory and relativity.\n\nExamples include: the complex numbers \"C\"ℓ(R), split-complex numbers \"C\"ℓ(R), quaternions \"C\"ℓ(R), split-biquaternions \"C\"ℓ(R), split-quaternions (the natural algebra of two-dimensional space); \"C\"ℓ(R) (the natural algebra of three-dimensional space, and the algebra of the Pauli matrices); and the spacetime algebra \"C\"ℓ(R).\n\nThe elements of the algebra \"C\"ℓ(R) form an even subalgebra \"C\"ℓ(R) of the algebra \"C\"ℓ(R), which can be used to parametrise rotations in the larger algebra. There is thus a close connection between complex numbers and rotations in two-dimensional space; between quaternions and rotations in three-dimensional space; between split-complex numbers and (hyperbolic) rotations (Lorentz transformations) in 1+1-dimensional space, and so on.\n\nWhereas Cayley–Dickson and split-complex constructs with eight or more dimensions are not associative with respect to multiplication, Clifford algebras retain associativity at any number of dimensions.\n\nIn 1995 Ian R. Porteous wrote on \"The recognition of subalgebras\" in his book on Clifford algebras. His Proposition 11.4 summarizes the hypercomplex cases:\n\nFor extension beyond the classical algebras, see Classification of Clifford algebras.\n\nAll of the Clifford algebras \"C\"ℓ(R) apart from the real numbers, complex numbers and the quaternions contain non-real elements that square to +1; and so cannot be division algebras. A different approach to extending the complex numbers is taken by the Cayley–Dickson construction. This generates number systems of dimension 2, \"n\" = 2, 3, 4, ..., with bases formula_23, where all the non-real basis elements anti-commute and satisfy formula_24. In 8 or more dimensions (n ≥ 3) these algebras are non-associative. In 16 or more dimensions (n ≥ 4) these algebras also have zero-divisors.\n\nThe first algebras in this sequence are the four-dimensional quaternions, eight-dimensional octonions, and 16-dimensional sedenions. An algebraic symmetry is lost with each increase in dimensionality: quaternion multiplication is not commutative, octonion multiplication is non-associative, and the norm of sedenions is not multiplicative.\n\nThe Cayley–Dickson construction can be modified by inserting an extra sign at some stages. It then generates the \"split algebras\" in the collection of composition algebras instead of the division algebras: \nUnlike the complex numbers, the split-complex numbers are not algebraically closed, and further contain zero divisors and non-trivial idempotents. As with the quaternions, split-quaternions are not commutative, but further contain nilpotents; they are isomorphic to the 2 × 2 real matrices. Split-octonions are non-associative and contain nilpotents.\n\nThe tensor product of any two algebras is another algebra, which can be used to produce many more examples of hypercomplex number systems.\n\nIn particular taking tensor products with the complex numbers (considered as algebras over the reals) leads to four-dimensional tessarines formula_32, eight-dimensional biquaternions formula_33, and 16-dimensional complex octonions formula_34.\n\n\n\n\n"}
{"id": "6600793", "url": "https://en.wikipedia.org/wiki?curid=6600793", "title": "Integral representation theorem for classical Wiener space", "text": "Integral representation theorem for classical Wiener space\n\nIn mathematics, the integral representation theorem for classical Wiener space is a result in the fields of measure theory and stochastic analysis. Essentially, it shows how to decompose a function on classical Wiener space into the sum of its expected value and an Itō integral.\n\nLet formula_1 (or simply formula_2 for short) be classical Wiener space with classical Wiener measure formula_3. If formula_4, then there exists a unique Itō integrable process formula_5 (i.e. in formula_6, where formula_7 is canonical Brownian motion) such that\n\nfor formula_3-almost all formula_10.\n\nIn the above,\n\nThe proof of the integral representation theorem requires the Clark-Ocone theorem from the Malliavin calculus.\n\nLet formula_14 be a probability space. Let formula_15 be a Brownian motion (i.e. a stochastic process whose law is Wiener measure). Let formula_16 be the natural filtration of formula_17 by the Brownian motion formula_7:\nSuppose that formula_20 is formula_21-measurable. Then there is a unique Itō integrable process formula_22 such that\n\n"}
{"id": "27782728", "url": "https://en.wikipedia.org/wiki?curid=27782728", "title": "Irit Dinur", "text": "Irit Dinur\n\nIrit Dinur (Hebrew: אירית דינור) is an Israeli mathematician. She is professor of computer science at the Weizmann Institute of Science. Her research is in foundations of computer science and in combinatorics, and especially in probabilistically checkable proofs and hardness of approximation.\nIrit Dinur earned her doctorate in 2002 from the school of computer science in Tel-Aviv University, advised by Shmuel Safra; her thesis was entitled \"On the Hardness of Approximating the Minimum Vertex Cover and The Closest Vector in a Lattice\". She joined the Weizmann Institute after visiting the Institute of Advanced Studies in Princeton, New Jersey, NEC, and the University of California, Berkeley.\n\nDinur published in 2006 a new proof of the PCP theorem that was significantly simpler than previous proofs of the same result.\nIn 2007, she was given the Michael Bruno Memorial Award in Computer Science by Yad Hanadiv. She was a plenary speaker at the 2010 International Congress of Mathematicians. In 2012, she won the Anna and Lajos Erdős Prize in Mathematics, given by the Israel Mathematical Union. She was the William Bentinck-Smith Fellow at Harvard University in 2012–2013.\n\n"}
{"id": "1180190", "url": "https://en.wikipedia.org/wiki?curid=1180190", "title": "Itoh-Tsujii inversion algorithm", "text": "Itoh-Tsujii inversion algorithm\n\nThe Itoh-Tsujii inversion algorithm is used to invert elements in a finite field. It was introduced in 1988 and first used over GF(2) using the normal basis representation of elements, however the algorithm is generic and can be used for other bases, such as the polynomial basis. It can also be used in any finite field, GF(\"p\").\n\nThe algorithm is as follows:\n\nThis algorithm is fast because steps 3 and 5 both involve operations in the subfield GF(\"p\"). Similarly, if a small value of \"p\" is used a lookup table can be used for inversion in step 4. The majority of time spent in this algorithm is in step 2, the first exponentiation. This is one reason why this algorithm is well-suited for the normal basis, since squaring and exponentiation are relatively easy in that basis.\n\n"}
{"id": "147867", "url": "https://en.wikipedia.org/wiki?curid=147867", "title": "John Koza", "text": "John Koza\n\nJohn R. Koza is a computer scientist and a former adjunct professor at Stanford University, most notable for his work in pioneering the use of genetic programming for the optimization of complex problems. Koza co-founded of Scientific Games Corporation, a company which builds computer systems to run state lotteries in the United States. John Koza is also credited with being the creator of the 'scratch card' with the help of retail promotions specialist Daniel Bower.\n\nKoza was born in 1944 and earned a bachelor's degree in computer science from the University of Michigan, being the second person to ever earn a bachelor's degree in computer science. He earned a doctoral degree in computer science from the University of Michigan in 1972.\n\nKoza was featured in Popular Science for his work on evolutionary programming that alters its own code to find far more complex solutions. The machine, which he calls the \"invention machine\", has created antennae, circuits, and lenses, and has received a patent from the US Patent Office.\n\nIn the political space, Koza advocates for a plan to revamp the way states choose their electors for the Electoral College in the United States, such that candidates who win the majority of the popular vote would then win a majority of the electors through an interstate compact. He established the organization \"National Popular Vote\" to advocate for state adoption of the policy and the election of supportive candidates.\n\n\n"}
{"id": "15014170", "url": "https://en.wikipedia.org/wiki?curid=15014170", "title": "Judgment (mathematical logic)", "text": "Judgment (mathematical logic)\n\nIn mathematical logic, a judgment (or judgement) or assertion is a statement or enunciation in the metalanguage. For example, typical judgments in first-order logic would be \"that a string is a well-formed formula\", or \"that a proposition is true\". Similarly, a judgment may assert the occurrence of a free variable in an expression of the object language, or the provability of a proposition. In general, a judgment may be any inductively definable assertion in the metatheory.\n\nJudgments are used in formalizing deduction systems: a logical axiom expresses a judgment, premises of a rule of inference are formed as a sequence of judgments, and their conclusion is a judgment as well (thus, hypotheses and conclusions of proofs are judgments). A characteristic feature of the variants of Hilbert-style deduction systems is that the \"context\" is not changed in any of their rules of inference, while both natural deduction and sequent calculus contain some context-changing rules. Thus, if we are interested only in the derivability of tautologies, not hypothetical judgments, then we can formalize the Hilbert-style deduction system in such a way that its rules of inference contain only judgments of a rather simple form. The same cannot be done with the other two deductions systems: as context is changed in some of their rules of inferences, they cannot be formalized so that hypothetical judgments could be avoided—not even if we want to use them just for proving derivability of tautologies.\n\nThis basic diversity among the various calculi allows such difference, that the same basic thought (e.g. deduction theorem) must be proven as a metatheorem in Hilbert-style deduction system, while it can be declared explicitly as a rule of inference in natural deduction.\n\nIn type theory, some analogous notions are used as in mathematical logic (giving rise to connections between the two fields, e.g. Curry-Howard correspondence). The abstraction in the notion of \"judgment\" in mathematical logic can be exploited also in foundation of type theory as well.\n\nIn logic, logical assertion is a statement that asserts that a certain premise is true, and is useful for statements in proof. It is equivalent to a sequent with an empty antecedent.\n\nFor example, if \"p\" = \"\"x\" is even\", the implication\nis thus true. We can also write this using the logical assertion symbol, as\n\nIn computer programming and programming language semantics, these are used in the form of assertions; one example is a loop invariant.\n\n\n\n"}
{"id": "2984476", "url": "https://en.wikipedia.org/wiki?curid=2984476", "title": "Kaup–Kupershmidt equation", "text": "Kaup–Kupershmidt equation\n\nThe Kaup–Kupershmidt equation (named after David J. Kaup and Boris Abram Kupershmidt) is the nonlinear fifth-order partial differential equation\n\nIt is the first equation in a hierarchy of integrable equations with the Lax operator\n\nIt has properties similar (but not identical) to those of the better-known KdV hierarchy in which the Lax operator has order 2.\n"}
{"id": "6078504", "url": "https://en.wikipedia.org/wiki?curid=6078504", "title": "Kharitonov's theorem", "text": "Kharitonov's theorem\n\nKharitonov's theorem is a result used in control theory to assess the stability of a dynamical system when the physical parameters of the system are not known precisely. When the coefficients of the characteristic polynomial are known, the Routh-Hurwitz stability criterion can be used to check if the system is stable (i.e. if all roots have negative real parts). Kharitonov's theorem can be used in the case where the coefficients are only known to be within specified ranges. It provides a test of stability for a so-called interval polynomial, while Routh-Hurwitz is concerned with an ordinary polynomial.\n\nAn interval polynomial is the family of all polynomials\nwhere each coefficient formula_2 can take any value in the specified intervals\nIt is also assumed that the leading coefficient cannot be zero: formula_4.\n\nAn interval polynomial is stable (i.e. all members of the family are stable) if and only if the four so-called Kharitonov polynomials\nare stable.\n\nWhat is somewhat surprising about Kharitonov's result is that although in principle we are testing an infinite number of polynomials for stability, in fact we need to test only four. This we can do using Routh-Hurwitz or any other method. So it only takes four times more work to be informed about the stability of an interval polynomial than it takes to test one ordinary polynomial for stability.\n\nKharitonov's theorem is useful in the field of robust control, which seeks to design systems that will work well despite uncertainties in component behavior due to measurement errors, changes in operating conditions, equipment wear and so on.\n\n"}
{"id": "16807502", "url": "https://en.wikipedia.org/wiki?curid=16807502", "title": "Michigan Mathematical Journal", "text": "Michigan Mathematical Journal\n\nThe Michigan Mathematical Journal (established 1952) is published by the mathematics department at the University of Michigan. An important early editor for the Journal was George Piranian.\n\nHistorically, the Journal has been published a small number of times in a given year (currently four), in all areas of mathematics. The current Managing Editor is Mircea Mustaţă.\n"}
{"id": "1358791", "url": "https://en.wikipedia.org/wiki?curid=1358791", "title": "Nicolae Popescu", "text": "Nicolae Popescu\n\nNicolae Popescu, Ph.D., D.Phil. (; 22 September 1937 – 29 July 2010) was a Romanian mathematician and professor at the University of Bucharest. He also held a research position at the Institute of Mathematics of the Romanian Academy, and was elected a Member of the Romanian Academy in 1992. \n\nHe is best known for his contributions to algebra and the theory of Abelian categories. From 1964 to 2007 he collaborated with Pierre Gabriel on the characterization of abelian categories; their best-known result is the Gabriel–Popescu theorem. His areas of expertise were category theory, abelian categories with applications to rings and modules, adjoint functors, limits and colimits, the theory of sheaves, the theory of rings, fields and polynomials, and valuation theory. He also had interests and published in the following areas: algebraic topology, algebraic geometry, commutative algebra, K-theory, class field theory, and algebraic function theory. \n\nPopescu published between 1962 and 2008 more than 102 papers in peer-reviewed, mathematics journals, several monographs on the theory of sheaves, and also six books on abelian category theory and abstract algebra. In a Grothendieck-like, energetic style, he initiated and provided scientific leadership to several seminars on category theory, sheaves and abstract algebra which resulted in a continuous stream of high-quality mathematical publications in international, peer-reviewed mathematics journals by several members participating in his Seminar series. His book \"Abelian Categories with Applications to Rings and Modules\" continues to provide valuable information to mathematicians around the world. \n\nPopescu was married, and there is a surviving wife, Professor Dr. Elena Liliana Popescu and three children. He earned his M.S. degree in mathematics in 1964, and his Ph.D. degree in mathematics in 1967, both at the University of Bucharest. He was awarded a D. Phil. degree (Doctor Docent) in 1972 by the University of Bucharest.\n\nIn 2009 he carried out mathematics studies at the Institute of Mathematics of the Romanian Academy in the Algebra research group and also had international collaborations on three continents. He shared many moral, ethical and religious values with another famous mathematician, Alexander Grothendieck, who visited the School of Mathematics in Bucharest in 1968. Like Grothendieck, he had a long-standing interest in category theory, number theory, practicing Yoga, and supporting promising young mathematicians in his fields of interest. He also supported the early developments of category theory applications in relational biology and mathematical biophysics/mathematical biology.\n\nPopescu was appointed as a Lecturer at the University of Bucharest in 1968 where he taught graduate students until 1972. Since 1964, he also held a Research Professorship at the Institute of Mathematics of the Romanian Academy; the institute was closed in 1976 by order of Nicolae Ceaușescu, for reasons related to his daughter Zoia Ceaușescu, who had been hired at the institute two years before.\n\n\n\n"}
{"id": "37060377", "url": "https://en.wikipedia.org/wiki?curid=37060377", "title": "Oscar Goldman (mathematician)", "text": "Oscar Goldman (mathematician)\n\nOscar Goldman (1925 – 17 December 1986, Bryn Mawr) was an American mathematician, who worked on algebra and its applications to number theory.\n\nOscar Goldman received his Ph.D in 1948 under Claude Chevalley at Princeton University. He was chair of the Mathematics Department at Brandeis University from 1952 to 1960. As chair of the department his immediate successor was Maurice Auslander. \n\nIn 1962, Goldman left Brandeis to become a professor and chair of the mathematics department at the University of Pennsylvania. Murray Gerstenhaber and Chung Tao Yang had persuaded Provost David R. Goddard to hire Goldman to help improve the quality of U. Penn's mathematics department to the level of the mathematics departments of the University of Chicago, Harvard University, and Princeton University. From 1963 to 1967, Goldman served as the chair of the mathematics department of U. Penn., hired several outstanding mathematicians including Richard Kadison and Eugenio Calabi, and regularly consulted Saunders Mac Lane and Donald C. Spencer in making his decisions on hiring and curriculum improvements.\n\n\n"}
{"id": "26511174", "url": "https://en.wikipedia.org/wiki?curid=26511174", "title": "Parikh's theorem", "text": "Parikh's theorem\n\nParikh's theorem in theoretical computer science says that if one looks only at the number of occurrences of each terminal symbol in a context-free language, without regard to their order, then the language is indistinguishable from a regular language. It is useful for deciding whether or not a string with a given number of some terminals is accepted by a context-free grammar. It was first proved by Rohit Parikh in 1961 and republished in 1966.\n\nLet formula_1 be an alphabet. The \"Parikh vector\" of a word is defined as the function formula_2, given by\n\nformula_3, where formula_4 denotes the number of occurrences of the letter formula_5 in the word formula_6.\n\nA subset of formula_7 is said to be \"linear\" if it is of the form\n\nformula_8 for some vectors formula_9.\n\nA subset of formula_7 is said to be \"semi-linear\" if it is a union of finitely many linear subsets.\n\nThe formal statement of Parikh's theorem is as follows. Let formula_11 be a context-free language. Let formula_12 be the set of Parikh vectors of words in formula_11, that is, formula_14. Then formula_12 is a semi-linear set.\n\nTwo languages are said to be \"commutatively equivalent\" if they have the same set of Parikh vectors. If formula_16 is any semi-linear set, the language of words whose Parikh vectors are in formula_16 is commutatively equivalent to some regular language. Thus, every context-free language is commutatively equivalent to some regular language.\n\nThe theorem has multiple interpretations. It shows that a context-free language over a singleton alphabet must be a regular language and that some context-free languages can only have ambiguous grammars. Such languages are called \"inherently ambiguous languages\". From a formal grammar perspective, this means that some ambiguous context-free grammars cannot be converted to equivalent unambiguous context-free grammars.\n"}
{"id": "1655033", "url": "https://en.wikipedia.org/wiki?curid=1655033", "title": "Philip Hall", "text": "Philip Hall\n\nPhilip Hall FRS (11 April 1904 – 30 December 1982), was an English mathematician. His major work was on group theory, notably on finite groups and solvable groups. \n\nHe was educated first at Christ's Hospital, where he won the Thompson Gold Medal for mathematics, and later at King's College, Cambridge. He was elected a Fellow of the Royal Society in 1951 and awarded its Sylvester Medal in 1961. He was President of the London Mathematical Society in 1955–1957, and awarded its Berwick Prize in 1958 and De Morgan Medal in 1965.\n\n\n"}
{"id": "32067452", "url": "https://en.wikipedia.org/wiki?curid=32067452", "title": "Pickands–Balkema–de Haan theorem", "text": "Pickands–Balkema–de Haan theorem\n\nThe Pickands–Balkema–de Haan theorem is often called the second theorem in extreme value theory. It gives the asymptotic tail distribution of a random variable \"X\", when the true distribution \"F\" of \"X\" is unknown. Unlike for the first theorem (the Fisher–Tippett–Gnedenko theorem) in extreme value theory, the interest here is in the values above a threshold.\n\nIf we consider an unknown distribution function formula_1 of a random variable formula_2, we are interested in estimating the conditional distribution function formula_3 of the variable formula_2 above a certain threshold formula_5. This is the so-called conditional excess distribution function, defined as\n\nfor formula_7, where formula_8 is either the finite or infinite right endpoint of the underlying distribution formula_1. The function formula_3 describes the distribution of the excess value over a threshold formula_5, given that the threshold is exceeded.\n\nLet formula_12 be a sequence of independent and identically-distributed random variables, and let formula_3 be their conditional excess distribution function. Pickands (1975), Balkema and de Haan (1974) posed that for a large class of underlying distribution functions formula_1, and large formula_5, formula_3 is well approximated by the generalized Pareto distribution. That is:\n\nwhere\n\nHere \"σ\" > 0, and \"y\" ≥ 0 when \"k\" ≥ 0 and 0 ≤ \"y\" ≤ −\"σ\"/\"k\" when \"k\" < 0. Since a special case of the generalized Pareto distribution is a power-law, the Pickands–Balkema–de Haan theorem is sometimes used to justify the use of a power-law for modeling extreme events. Still, many important distributions, such as the normal and log-normal distributions, do not have extreme-value tails that are asymptotically power-law.\n\n\nStable distribution\n\n"}
{"id": "771168", "url": "https://en.wikipedia.org/wiki?curid=771168", "title": "Polynomial remainder theorem", "text": "Polynomial remainder theorem\n\nIn algebra, the polynomial remainder theorem or little Bézout's theorem is an application of Euclidean division of polynomials. It states that the remainder of the division of a polynomial formula_1 by a linear polynomial formula_2 is equal to formula_3 In particular, formula_2 is a divisor of formula_1 if and only if formula_6 a property known as the factor theorem.\n\nLet formula_7. Polynomial division of formula_1 by formula_9 gives the quotient formula_10 and the remainder formula_11. Therefore, formula_12.\n\nShow that the polynomial remainder theorem holds for an arbitrary second degree polynomial formula_13 by using algebraic manipulation:\n\nMultiplying both sides by (\"x\" − \"r\") gives\n\nSince formula_16 is our remainder, we have indeed shown that formula_17.\n\nThe polynomial remainder theorem follows from the theorem of Euclidean division, which, given two polynomials \"f\"(\"x\") (the dividend) and \"g\"(\"x\") (the divisor), asserts the existence (and the uniqueness) of a quotient \"q\"(\"x\") and a remainder \"R\"(\"x\") such that\n\nIf we take formula_19 as the divisor, either \"R\" = 0 or its degree is zero; in both cases, \"R\" is a constant that is independent of \"x\"; that is \n\nSetting formula_21 in this formula, we obtain:\n\nA slightly different proof, which may appear to some people as more elementary, starts with an observation that formula_23 is a linear combination of the terms of the form formula_24, each of which is divisible by formula_2 since formula_26.\n\nThe polynomial remainder theorem may be used to evaluate formula_27 by calculating the remainder, formula_28. Although polynomial long division is more difficult than evaluating the function itself, synthetic division is computationally easier. Thus, the function may be more \"cheaply\" evaluated using synthetic division and the polynomial remainder theorem.\n\nThe factor theorem is another application of the remainder theorem: if the remainder is zero, then the linear divisor is a factor. Repeated application of the factor theorem may be used to factorize the polynomial.\n"}
{"id": "22965731", "url": "https://en.wikipedia.org/wiki?curid=22965731", "title": "Probability Surveys", "text": "Probability Surveys\n\nProbability Surveys is an open-access electronic journal that is jointly sponsored by the Bernoulli Society and the Institute of Mathematical Statistics. It publishes review articles on topics of interest in probability theory. \n\n"}
{"id": "1648765", "url": "https://en.wikipedia.org/wiki?curid=1648765", "title": "Random matrix", "text": "Random matrix\n\nIn probability theory and mathematical physics, a random matrix is a matrix-valued random variable—that is, a matrix in which some or all elements are random variables. Many important properties of physical systems can be represented mathematically as matrix problems. For example, the thermal conductivity of a lattice can be computed from the dynamical matrix of the particle-particle interactions within the lattice.\n\nIn nuclear physics, random matrices were introduced by Eugene Wigner to model the nuclei of heavy atoms. He postulated that the spacings between the lines in the spectrum of a heavy atom nucleus should resemble the spacings between the eigenvalues of a random matrix, and should depend only on the symmetry class of the underlying evolution. In solid-state physics, random matrices model the behaviour of large disordered Hamiltonians in the mean field approximation.\n\nIn quantum chaos, the Bohigas–Giannoni–Schmit (BGS) conjecture asserts that the spectral statistics of quantum systems whose classical counterparts exhibit chaotic behaviour are described by random matrix theory.\n\nIn quantum optics, transformations described by random unitary matrices are crucial for demonstrating the advantage of quantum over classical computation (see, e.g., the boson sampling model). Moreover, such random unitary transformations can be directly implemented in an optical circuit, by mapping their parameters to optical circuit components (that is beam splitters and phase shifters).\n\nRandom matrix theory has also found applications to the chiral Dirac operator in quantum chromodynamics, quantum gravity in two dimensions, mesoscopic physics,spin-transfer torque, the fractional quantum Hall effect, Anderson localization, quantum dots, and superconductors\n\nIn multivariate statistics, random matrices were introduced by John Wishart for statistical analysis of large samples; see estimation of covariance matrices.\n\nSignificant results have been shown that extend the classical scalar Chernoff, Bernstein, and Hoeffding inequalities to the largest eigenvalues of finite sums of random Hermitian matrices. Corollary results are derived for the maximum singular values of rectangular matrices.\n\nIn numerical analysis, random matrices have been used since the work of John von Neumann and Herman Goldstine to describe computation errors in operations such as matrix multiplication. See also for more recent results.\n\nIn number theory, the distribution of zeros of the Riemann zeta function (and other L-functions) is modelled by the distribution of eigenvalues of certain random matrices. The connection was first discovered by Hugh Montgomery and Freeman J. Dyson. It is connected to the Hilbert–Pólya conjecture.\n\nIn the field of theoretical neuroscience, random matrices are increasingly used to model the network of synaptic connections between neurons in the brain. Dynamical models of neuronal networks with random connectivity matrix were shown to exhibit a phase transition to chaos when the variance of the synaptic weights crosses a critical value, at the limit of infinite system size. Relating the statistical properties of the spectrum of biologically inspired random matrix models to the dynamical behavior of randomly connected neural networks is an intensive research topic.\n\nIn optimal control theory, the evolution of \"n\" state variables through time depends at any time on their own values and on the values of \"k\" control variables. With linear evolution, matrices of coefficients appear in the state equation (equation of evolution). In some problems the values of the parameters in these matrices are not known with certainty, in which case there are random matrices in the state equation and the problem is known as one of stochastic control. A key result in the case of linear-quadratic control with stochastic matrices is that the certainty equivalence principle does not apply: while in the absence of multiplier uncertainty (that is, with only additive uncertainty) the optimal policy with a quadratic loss function coincides with what would be decided if the uncertainty were ignored, this no longer holds in the presence of random coefficients in the state equation.\n\nThe most studied random matrix ensembles are the Gaussian ensembles.\n\nThe Gaussian unitary ensemble GUE(\"n\") is described by the Gaussian measure with density\n\non the space of \"n × n\" Hermitian matrices \"H\" = (\"H\"). Here \"Z\"(\"n\") = 2  is a normalization constant, chosen so that the integral of the density is equal to one. The term \"unitary\" refers to the fact that the distribution is invariant under unitary conjugation.\nThe Gaussian unitary ensemble models Hamiltonians lacking time-reversal symmetry.\n\nThe Gaussian orthogonal ensemble GOE(\"n\") is described by the Gaussian measure with density\n\non the space of \"n × n\" real symmetric matrices \"H\" = (\"H\"). Its distribution is invariant under orthogonal conjugation, and it models Hamiltonians with time-reversal symmetry.\n\nThe Gaussian symplectic ensemble GSE(\"n\") is described by the Gaussian measure with density\n\non the space of \"n × n\" Hermitian quaternionic matrices, e.g. symmetric square matrices composed of quaternions, \"H\" = (\"H\"). Its distribution is invariant under conjugation by the symplectic group, and it models Hamiltonians with time-reversal symmetry but no rotational symmetry.\n\nThe joint probability density for the eigenvalues \"λ\",\"λ\"...,\"λ\" of GUE/GOE/GSE is given by\n\nwhere the Dyson index, \"β\" = 1 for GOE, \"β\" = 2 for GUE, and \"β\" = 4 for GSE, counts the number of real components per matrix element; \"Z\" is a normalization constant which can be explicitly computed, see Selberg integral. In the case of GUE (\"β\" = 2), the formula (1) describes a determinantal point process. Eigenvalues repel as the joint probability density has a zero (of formula_5th order) for coinciding eigenvalues formula_6.\n\nFor the distribution of the largest eigenvalue for GOE, GUE and Wishart matrices of finite dimensions, see.\n\nFrom the ordered sequence of eigenvalues formula_7, one defines the normalized spacings formula_8, where formula_9 is the mean spacing. The probability distribution of spacings is approximately given by,\n\nfor the orthogonal ensemble GOE formula_11, \nfor the unitary ensemble GUE formula_13, and \nfor the symplectic ensemble GSE formula_15.\n\nThe numerical constants are such that formula_16 is normalized:\n\nand the mean spacing is,\n\nfor formula_19.\n\n\"Wigner matrices\" are random Hermitian matrices formula_20 such that the entries\nabove the main diagonal are independent random variables with zero mean, and\nhave identical second moments.\n\n\"Invariant matrix ensembles\" are random Hermitian matrices with density on the space of real symmetric/ Hermitian/ quaternionic Hermitian matrices, which is of the form\nformula_23\nwhere the function \"V\" is called the potential.\n\nThe Gaussian ensembles are the only common special cases of these two classes of random matrices.\n\nThe spectral theory of random matrices studies the distribution of the eigenvalues as the size of the matrix goes to infinity.\n\nIn the \"global regime\", one is interested in the distribution of linear statistics of the form \"N = n\" tr \"f(H)\".\n\nThe \"empirical spectral measure\" \"μ\" of \"H\" is defined by\n\nUsually, the limit of formula_25 is a deterministic measure; this is a particular case of self-averaging. The cumulative distribution function of the limiting measure is called the integrated density of states and is denoted \"N\"(\"λ\"). If the integrated density of states is differentiable, its derivative is called the density of states and is denoted \"ρ\"(\"λ\").\n\nThe limit of the empirical spectral measure for Wigner matrices was described by Eugene Wigner; see Wigner semicircle distribution and Wigner surmise. As far as sample covariance matrices are concerned, a theory was developed by Marčenko and Pastur.\n\nThe limit of the empirical spectral measure of invariant matrix ensembles is described by a certain integral equation which arises from potential theory.\n\nFor the linear statistics \"N\" = \"n\" ∑ \"f\"(\"λ\"), one is also interested in the fluctuations about ∫ \"f\"(\"λ\") \"dN\"(\"λ\"). For many classes of random matrices, a central limit theorem of the form\nis known, see, etc.\n\nIn the \"local regime\", one is interested in the spacings between eigenvalues, and, more generally, in the joint distribution of eigenvalues in an interval of length of order 1/\"n\". One distinguishes between \"bulk statistics\", pertaining to intervals inside the support of the limiting spectral measure, and \"edge statistics\", pertaining to intervals near the boundary of the support.\n\nFormally, fix formula_27 in the interior of the support of formula_28. Then consider the point process\n\nwhere formula_30 are the eigenvalues of the random matrix.\n\nThe point process formula_31 captures the statistical properties of eigenvalues in the vicinity of formula_27. For the Gaussian ensembles, the limit of formula_31 is known; thus, for GUE it is a determinantal point process with the kernel\n\n(the \"sine kernel\").\n\nThe \"universality\" principle postulates that the limit of formula_31 as formula_36 should depend only on the symmetry class of the random matrix (and neither on the specific model of random matrices nor on formula_27). This was rigorously proved for several models of random matrices: for invariant matrix ensembles,\nfor Wigner matrices,\net cet.\n\nSee Tracy–Widom distribution.\n\n\"Wishart matrices\" are \"n × n\" random matrices of the form \"H\" = \"X\" \"X\", where \"X\" is an \"n × m\" random matrix (\"m\" ≥ \"n\") with independent entries, and \"X\" is its conjugate transpose. In the important special case considered by Wishart, the entries of \"X\" are identically distributed Gaussian random variables (either real or complex).\n\nThe limit of the empirical spectral measure of Wishart matrices was found by Vladimir Marchenko and Leonid Pastur, see Marchenko–Pastur distribution.\n\n\n"}
{"id": "5505578", "url": "https://en.wikipedia.org/wiki?curid=5505578", "title": "Recurrent point", "text": "Recurrent point\n\nIn mathematics, a recurrent point for a function \"f\" is a point that is in its own limit set by \"f\". Any neighborhood containing the recurrent point will also contain (a countable number of) iterates of it as well. \n\nLet formula_1 be a Hausdorff space and formula_2 a function. A point formula_3 is said to be recurrent (for formula_4) if formula_5, \"i.e.\" if formula_6 belongs to its formula_7-limit set. This means that for each neighborhood formula_8 of formula_6 there exists formula_10 such that formula_11.\n\nThe set of recurrent points of formula_4 is often denoted formula_13 and is called the recurrent set of formula_4. Its closure is called the Birkhoff center of formula_4, and appears in the work of George David Birkhoff on dynamical systems.\n\nEvery recurrent point is a nonwandering point, hence if formula_4 is a homeomorphism and formula_1 is compact, then formula_13 is an invariant subset of the non-wandering set of formula_4 (and may be a proper subset).\n"}
{"id": "18814716", "url": "https://en.wikipedia.org/wiki?curid=18814716", "title": "Regular chain", "text": "Regular chain\n\nIn computer algebra, a regular chain is a particular kind of triangular set in a multivariate polynomial ring over a field. It enhances the notion of characteristic set.\n\nGiven a linear system, one can convert it to a triangular system via Gaussian elimination. For the non-linear case, given a polynomial system F over a field, one can convert (decompose or triangularize) it to a finite set of triangular sets, in the sense that the algebraic variety \"V\"(F) is described by these triangular sets.\n\nA triangular set may merely describe the empty set. To fix this degenerated case, the notion of regular chain was introduced, independently by Kalkbrener (1993), Yang and Zhang (1994). Regular chains also appear in Chou and Gao (1992). Regular chains are special triangular sets which are used in different algorithms for computing unmixed-dimensional decompositions of algebraic varieties. Without using factorization, these decompositions have better properties that the ones produced by Wu's algorithm. Kalkbrener's original definition was based on the following observation: every irreducible variety is uniquely determined by one of its generic points and varieties can be represented by describing the generic points of their irreducible components. These generic points are given by regular chains.\n\nDenote Q the rational number field. In Q[x, x, x] with variable ordering x < x < x,\nis a triangular set and also a regular chain. Two generic points given by \"T\" are (a, a, a) and (a, -a, a) where \"a\" is transcendental over Q.\nThus there are two irreducible components, given by { x - x, x - x } and { x + x, x - x }, respectively.\nNote that: (1) the content of the second polynomial is x, which does not contribute to the generic points represented and thus can be removed; (2) the dimension of each component is 1, the number of free variables in the regular chain.\n\nThe variables in the polynomial ring \nare always sorted as x < ... < x. \nA non-constant polynomial \"f\" in formula_3 can be seen as a univariate polynomial in its greatest variable.\nThe greatest variable in \"f\" is called its main variable, denoted by \"mvar\"(f). Let \"u\" be \nthe main variable of \"f\" and write it as \nwhere \"e\" is the degree of \"f\" w.r.t. \"u\" and formula_5 is \nthe leading coefficient of \"f\" w.r.t. \"u\". Then the initial of \"f\"\nis formula_5 and \"e\" is its main degree.\n\n\nA non-empty subset \"T\" of formula_3 is a triangular set, \nif the polynomials in \"T\" are non-constant and have distinct main variables. \nHence, a triangular set is finite, and has cardinality at most \"n\".\n\n\nLet T = {t, ..., t} be a triangular set such that \n\"mvar\"(t) < ... < \"mvar\"(t), \nformula_8 be the initial of \"t\" and \"h\" be the product of h's. \nThen \"T\" is a \"regular chain\" if \nwhere each resultant is computed with respect to the main variable of \"t\", respectively. \nThis definition is from Yang and Zhang, which is of much algorithmic flavor.\n\n\nThe \"quasi-component\" \"W\"(\"T\") described by the regular chain \"T\" is \nthe set difference of the varieties \"V\"(\"T\") and \"V\"(\"h\"). \nThe attached algebraic object of a regular chain is its \"saturated ideal\" \nA classic result is that the Zariski closure of \"W\"(\"T\") equals the variety defined by sat(\"T\"), that is,\nand its dimension is n - |T|, the difference of the number of variables and the number of polynomials in \"T\".\n\n\nIn general, there are two ways to decompose a polynomial system \"F\". The first one is to decompose lazily, that is, only to represent its generic points in the (Kalkbrener) sense,\nThe second is to describe all zeroes in the Lazard sense,\nThere are various algorithms available for triangular decompositions in either sense.\n\nLet \"T\" be a regular chain in the polynomial ring \"R\".\n\n\n\n\n\n\n"}
{"id": "3378540", "url": "https://en.wikipedia.org/wiki?curid=3378540", "title": "Resolvent formalism", "text": "Resolvent formalism\n\nIn mathematics, the resolvent formalism is a technique for applying concepts from complex analysis to the study of the spectrum of operators on Banach spaces and more general spaces. Formal justification for the manipulations can be found in the framework of holomorphic functional calculus.\n\nThe resolvent captures the spectral properties of an operator in the analytic structure of the functional. Given an operator , the resolvent may be defined as\n\nAmong other uses, the resolvent may be used to solve the inhomogeneous Fredholm integral equations; a commonly used approach is a series solution, the Liouville-Neumann series.\n\nThe resolvent of can be used to directly obtain information about the spectral decomposition\nof . For example, suppose is an isolated eigenvalue in the \nspectrum of . That is, suppose there exists a simple closed curve formula_2 \nin the complex plane that separates from the rest of the spectrum of .\nThen the residue \ndefines a projection operator onto the eigenspace of . \n\nThe Hille-Yosida theorem relates the resolvent through a Laplace transform to an integral over the one-parameter group of transformations generated by . Thus, for example, if is Hermitian, then is a one-parameter group of unitary operators. The resolvent of \"iA\" can be expressed as their Laplace transform integral \n\nThe first major use of the resolvent operator as a series in (cf. Liouville-Neumann series) was by Ivar Fredholm, in a landmark 1903 paper in \"Acta Mathematica\" that helped establish modern operator theory. \n\nThe name \"resolvent\" was given by David Hilbert.\n\nFor all in , the resolvent set of an operator , we have that the first resolvent identity (also called Hilbert's identity) holds:\n\nThe second resolvent identity is a generalization of the first resolvent identity, above, useful for comparing the resolvents of two distinct operators. Given operators and , both defined on the same linear space, and in the following identity holds,\n\nWhen studying an unbounded operator : → on a Hilbert space , if there exists formula_7 such that formula_8 is a compact operator, we say that has compact resolvent. The spectrum formula_9 of such is a discrete subset of formula_10. If furthermore is self-adjoint, then formula_11 and there exists an orthonormal basis formula_12 of eigenvectors of with eigenvalues formula_13 respectively. Also, formula_14 has no finite accumulation point.\n\n\n"}
{"id": "1901026", "url": "https://en.wikipedia.org/wiki?curid=1901026", "title": "Robinson–Schensted correspondence", "text": "Robinson–Schensted correspondence\n\nIn mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and a further generalization to pictures by Zelevinsky.\n\nThe simplest description of the correspondence is using the Schensted algorithm , a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, while the other tableau records the evolution of the shape during construction. The correspondence had been described, in a rather different form, much earlier by Robinson , in an attempt to prove the Littlewood–Richardson rule. The correspondence is often referred to as the Robinson–Schensted algorithm, although the procedure used by Robinson is radically different from the Schensted–algorithm, and almost entirely forgotten. Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin.\n\nThe bijective nature of the correspondence relates it to the enumerative identity:\n\nwhere formula_2 denotes the set of partitions of (or of Young diagrams with squares), and denotes the number of standard Young tableaux of shape .\n\nThe Schensted algorithm starts from the permutation written in two-line notation\n\nwhere , and proceeds by constructing sequentially a sequence of (intermediate) ordered pairs of Young tableaux of the same shape:\n\nwhere are empty tableaux. The output tableaux are and . Once is constructed, one forms by \"inserting\" into , and then by adding an entry to in the square added to the shape by the insertion (so that and have equal shapes for all ). Because of the more passive role of the tableaux , the final one , which is part of the output and from which the previous are easily read off, is called the recording tableau; by contrast the tableaux are called insertion tableaux.\n\nThe basic procedure used to insert each is called Schensted insertion or row-insertion (to distinguish it from a variant procedure called column-insertion). Its simplest form is defined in terms of \"incomplete standard tableaux\": like standard tableaux they have distinct entries, forming increasing rows and columns, but some values (still to be inserted) may be absent as entries. The procedure takes as arguments such a tableau and a value not present as entry of ; it produces as output a new tableau denoted and a square by which its shape has grown. The value appears in the first row of , either having been added at the end (if no entries larger than were present), or otherwise replacing the first entry in the first row of . In the former case is the square where is added, and the insertion is completed; in the latter case the replaced entry is similarly inserted into the second row of , and so on, until at some step the first case applies (which certainly happens if an empty row of is reached).\n\nMore formally, the following pseudocode describes the row-insertion of a new value into .\n\nThe shape of grows by exactly one square, namely .\n\nThe fact that has increasing rows and columns, if the same holds for , is not obvious from this procedure (entries in the same column are never even compared). It can however be seen as follows. At all times except immediately after step 4, the square is either empty in or holds a value greater than ; step 5 re-establishes this property because now is the square immediately below the one that originally contained in . Thus the effect of the replacement in step 4 on the value is to make it smaller; in particular it cannot become greater than its right or lower neighbours. On the other hand the new value is not less than its left neighbour (if present) either, as is ensured by the comparison that just made step 2 terminate. Finally to see that the new value is larger than its upper neighbour if present, observe that holds after step 5, and that decreasing in step 2 only decreases the corresponding value .\n\nThe full Schensted algorithm applied to a permutation proceeds as follows.\n\nThe algorithm produces a pair of standard Young tableaux.\n\nIt can be seen that given any pair of standard Young tableaux of the same shape, there is an inverse procedure that produces a permutation that will give rise to by the Schensted algorithm. It essentially consists of tracing steps of the algorithm backwards, each time using an entry of to find the square where the inverse insertion should start, moving the corresponding entry of to the preceding row, and continuing upwards through the rows until an entry of the first row is replaced, which is the value inserted at the corresponding step of the construction algorithm. These two inverse algorithms define a bijective correspondence between permutations of on one side, and pairs of standard Young tableaux of equal shape and containing squares on the other side.\n\nOne of the most fundamental properties, but not evident from the algorithmic construction, is symmetry:\n\n\nThis can be proven, for instance, by appealing to Viennot's geometric construction.\n\nFurther properties, all assuming that the correspondence associates tableaux to the permutation .\n\n\n"}
{"id": "34113393", "url": "https://en.wikipedia.org/wiki?curid=34113393", "title": "Sendov's conjecture", "text": "Sendov's conjecture\n\nIn mathematics, Sendov's conjecture, sometimes also called Ilieff's conjecture, concerns the relationship between the locations of roots and critical points of a polynomial function of a complex variable. It is named after Blagovest Sendov.\n\nThe conjecture states that for a polynomial\n\nwith all roots \"r\", ..., \"r\" inside the closed unit disk |\"z\"| ≤ 1, each of the \"n\" roots is at a distance no more than 1 from at least one critical point.\n\nThe Gauss–Lucas theorem says that all of the critical points lie within the convex hull of the roots. It follows that the critical points must be within the unit disk, since the roots are.\n\nThe conjecture has been proven for \"n\" < 9. \n\nThis conjecture was first mooted by Blagovest Sendov in 1959. He proposed this conjecture to Nikola Obreschkov. In 1967 this conjecture was misattributed to Ljubomir Iliev by Walter Hayman. In 1969 Mier and Sharmaad proved the conjecture for polynomials with \"n\" < 6. In 1991 Brown proved the conjecture for \"n\" < 7. Borcea extended the proof to n < 8 in 1996. Brown and Xiang proved the conjecture for \"n\" < 9 in 1999. Degot has proven the conjecture for large \"n\" but this proof requires additional conditions.\n\n\n"}
{"id": "54558969", "url": "https://en.wikipedia.org/wiki?curid=54558969", "title": "Seventh power", "text": "Seventh power\n\nIn arithmetic and algebra the seventh power of a number \"n\" is the result of multiplying seven instances of \"n\" together. So:\n\nSeventh powers are also formed by multiplying a number by its sixth power, the square of a number by its fifth power, or the cube of a number by its fourth power.\n\nThe sequence of seventh powers of integers is:\n\nIn the archaic notation of Robert Recorde, the seventh power of a number was called the \"second sursolid\".\n\nLeonard Eugene Dickson studied generalizations of Waring's problem for seventh powers, showing that every non-negative integer can be represented as a sum of at most 258 non-negative seventh powers. All but finitely many positive integers can be expressed more simply as the sum of at most 46 seventh powers. If negative powers are allowed, only 12 powers are required.\n\nThe smallest number that can be represented in two different ways as a sum of four positive seventh powers is 2056364173794800. \n\nThe smallest seventh power that can be represented as a sum of eight distinct seventh powers is:\n\nThe two known examples of a seventh power expressible as the sum of seven seventh powers are\n\nand\n\nany example with fewer terms in the sum would be a counterexample to Euler's sum of powers conjecture, which is currently only known to be false for the powers 4 and 5.\n\n"}
{"id": "7359504", "url": "https://en.wikipedia.org/wiki?curid=7359504", "title": "Shuntarō Itō", "text": "Shuntarō Itō\n\nIto was born in Tokyo, and obtained his bachelor's degree in literature from Tokyo University in 1953, his master's in 1955, and Ph.D. from University of Wisconsin–Madison in 1964. Ito taught at the University of Tokyo, as Professor, from 1978 to 1989, and in International Research Center for Japanese Studies from 1989 to 1995, and in Reitaku University from 1995.\n\n\n"}
{"id": "1035039", "url": "https://en.wikipedia.org/wiki?curid=1035039", "title": "Smooth number", "text": "Smooth number\n\nIn number theory, a smooth (or friable) number is an integer which factors completely into small prime numbers. For example, a 7-smooth number is a number whose prime factors are all at most 7, so 49 = 7 and 15750 = 2 × 3 × 5 × 7 are both 7-smooth, while 11 and 702 = 2 × 3 × 13 are not. The term seems to have been coined by Leonard Adleman. Smooth numbers are especially important in cryptography relying on factorization. The 2-smooth numbers are just the powers of 2, while 5-smooth numbers are known as regular numbers.\n\nA positive integer is called B-smooth if none of its prime factors is greater than B. For example, 1,620 has prime factorization 2 × 3 × 5; therefore 1,620 is 5-smooth because none of its prime factors are greater than 5. This definition includes numbers that lack some of the smaller prime factors; for example, both 10 and 12 are 5-smooth, even though they miss out prime factors 3 and 5 respectively. All 5-smooth numbers are of the form 2 × 3 × 5, where a, b and c are positive integers or zero.\n\n5-smooth numbers are also called \"regular numbers\" or \"Hamming numbers\"; 7-smooth numbers are also called humble numbers, and sometimes called \"highly composite\", although this conflicts with another meaning of highly composite numbers.\n\nNote that B itself is not required to appear among the factors of a B-smooth number. If the largest prime factor of a number is p then the number is B-smooth for any B ≥ p. Usually B is prime, but composite numbers are permitted as well. A number is B-smooth if and only if it is p-smooth, where p is the largest prime less than or equal to B.\n\nAn important practical application of smooth numbers is for fast Fourier transform (FFT) algorithms (such as the Cooley–Tukey FFT algorithm) that operate by recursively breaking down a problem of a given size \"n\" into problems the size of its factors. By using \"B\"-smooth numbers, one ensures that the base cases of this recursion are small primes, for which efficient algorithms exist. (Large prime sizes require less-efficient algorithms such as Bluestein's FFT algorithm.)\n\n5-smooth or regular numbers play a special role in Babylonian mathematics. They are also important in music theory, (see Limit (music)) and the problem of generating these numbers efficiently has been used as a test problem for functional programming.\n\nSmooth numbers have a number of applications to cryptography. Although most applications involve cryptanalysis (e.g. the fastest known integer factorization algorithms), the VSH hash function is one example of a constructive use of smoothness to obtain a provably secure design.\n\nLet formula_1 denote the number of \"y\"-smooth integers less than or equal to \"x\" (the de Bruijn function).\n\nIf the smoothness bound \"B\" is fixed and small, there is a good estimate for formula_2:\n\nwhere formula_4 denotes the number of primes less than or equal to formula_5.\n\nOtherwise, define the parameter \"u\" as \"u\" = log \"x\" / log \"y\": that is, \"x\" = \"y\". Then,\n\nwhere formula_7 is the Dickman function.\n\nThe average size of the smooth part of a number of given size is known as formula_8 and it is known to decay much more slowly than formula_7.\n\nFor any \"k\", almost all natural numbers will not be \"k\"-smooth.\n\nFurther, \"m\" is called \"B\"-powersmooth (or \"B\"-ultrafriable) if all prime \"powers\" formula_10 dividing \"m\" satisfy:\n\nFor example, 720 (235) is 5-smooth but is not 5-powersmooth (because there are several prime powers greater than 5, \"e.g.\", formula_12 or formula_13). It is 16-powersmooth since its greatest prime factor power is 2 = 16. The number is also 17-powersmooth, 18-powersmooth, etc.\n\n\"B\"-smooth and \"B\"-powersmooth numbers have applications in number theory, such as in Pollard's \"p\" − 1 algorithm. Such applications are often said to work with \"smooth numbers,\" with no \"B\" specified; this means the numbers involved must be \"B\"-powersmooth for some unspecified small number \"B\"; as \"B\" increases, the performance of the algorithm or method in question degrades rapidly. For example, the Pohlig–Hellman algorithm for computing discrete logarithms has a running time of O(\"B\") for groups of \"B\"-smooth order.\n\nMoreover, \"m\" is said to be smooth over a set A if there exists a factorization of \"m\" where the factors are powers of elements in A. For example, 12=4*3, 12 is smooth over sets such as A={4,3}, A={2,3}, and formula_14; however it would not be smooth over the set A={3,5} as 12 contains the factor 4 or 2 which is not in A.\n\nNote the set A does not have to be a set of prime factors, but it is typically a proper subset of the primes as seen in the factor base of Dixon's factorization method and the Quadratic sieve. Likewise it is what the General number field sieve uses to build its notion of smoothness under the homomorphism formula_15.\n\n\n\nThe On-Line Encyclopedia of Integer Sequences (OEIS)\nlists \"B\"-smooth numbers for small \"B\"s:\n"}
{"id": "56860120", "url": "https://en.wikipedia.org/wiki?curid=56860120", "title": "Stephens' constant", "text": "Stephens' constant\n\nStephens' constant expresses the density of certain subsets of the prime numbers. Let formula_1 and formula_2 be two multiplicatively independent integers, that is, formula_3 except when both formula_4 and formula_5 equal zero. Consider the set formula_6 of prime numbers formula_7 such that formula_7 evenly divides formula_9 for some power formula_10. The density of the set formula_6 relative to the set of all primes is a rational multiple of\n\nStephens' constant is closely related to the Artin constant formula_13 that arises in the study of primitive roots.\n\n"}
{"id": "7652984", "url": "https://en.wikipedia.org/wiki?curid=7652984", "title": "Suslin representation", "text": "Suslin representation\n\nIn mathematics, a Suslin representation of a set of reals (more precisely, elements of Baire space) is a tree whose projection is that set of reals. More generally, a subset \"A\" of κ is λ-Suslin if there is a tree \"T\" on κ × λ such that \"A\" = p[\"T\"].\n\nBy a tree on κ × λ we mean here a subset \"T\" of the union of κ × λ for all \"i\" ∈ N (or \"i\" < ω in set-theoretical notation).\n\nHere, p[\"T\"] = { \"f\" | ∃\"g\" : (\"f\",\"g\") ∈ [\"T\"] } is the \"projection of \"T\",\nwhere [\"T\"] = { (\"f\", \"g\" ) | ∀\"n\" ∈ ω : (\"f\"(\"n\"), \"g\"(\"n\") ∈ \"T\" } is the set of branches through \"T\".\n\nSince [\"T\"] is a closed set for the product topology on κ × λ where κ and λ are equipped with the discrete topology (and all closed sets in κ × λ come in this way from some tree on κ × λ), λ-Suslin subsets of κ are projections of closed subsets in κ × λ.\n\nWhen one talks of \"Suslin sets\" without specifying the space, then one usually means Suslin subsets of R, which descriptive set theorists usually take to be the set ω.\n\n\n"}
{"id": "36025432", "url": "https://en.wikipedia.org/wiki?curid=36025432", "title": "Symbolic data analysis", "text": "Symbolic data analysis\n\nSymbolic data analysis (SDA) is an extension of standard data analysis where symbolic data tables are used as input and symbolic objects are made output as a result. The data units are called \"symbolic\" since they are more complex than standard ones, as they not only contain values or categories, but also include internal variation and structure. SDA is based on four spaces: the space of individuals, the space of concepts, the space of descriptions, and the space of symbolic objects. The space of descriptions models individuals, while the space of symbolic objects models concepts\n\n\nR2S : An R package to transform relational data into symbolic data \n"}
{"id": "38576605", "url": "https://en.wikipedia.org/wiki?curid=38576605", "title": "Thomas Bond Sprague Prize", "text": "Thomas Bond Sprague Prize\n\nThe Thomas Bond Sprague Prize is a prize awarded annually to the student\nor students showing the greatest distinction in actuarial science, \nfinance, insurance, mathematics of operational research, probability, risk\nand statistics in the Master of Mathematics/Master of Advanced\nStudies examinations of the University of Cambridge,\nalso known as Part III of the Mathematical\nTripos. The prize is named after\nThomas Bond Sprague, the only person to have been president\nof both the Institute of Actuaries in London and the Faculty of Actuaries in\nEdinburgh. It is awarded by the Rollo Davidson Trust of Churchill College, Cambridge,\nfollowing a donation by D. O. Forfar, MA, FFA, FRSE (alumnus of Trinity College, Cambridge), former Appointed Actuary of \nScottish Widows.\n"}
{"id": "30639352", "url": "https://en.wikipedia.org/wiki?curid=30639352", "title": "Turkish Journal of Mathematics", "text": "Turkish Journal of Mathematics\n"}
{"id": "25016871", "url": "https://en.wikipedia.org/wiki?curid=25016871", "title": "Up tack", "text": "Up tack\n\nThe up tack or falsum (⊥, codice_1 in LaTeX, U+22A5 in Unicode) is a constant symbol used to represent:\n\n\nThe glyph of the up tack appears as an upside-down tee symbol, and as such is sometimes called eet (the word \"tee\" in reverse). Tee plays a complementary or dual role in many of these theories.\n\nThe similar-looking perpendicular symbol (⟂, codice_2 in LaTeX, U+27C2 in Unicode) is a binary relation symbol used to represent:\n\n\n"}
{"id": "62641", "url": "https://en.wikipedia.org/wiki?curid=62641", "title": "Vector field", "text": "Vector field\n\nIn vector calculus and physics, a vector field is an assignment of a vector to each point in a subset of space. A vector field in the plane (for instance), can be visualised as: a collection of arrows with a given magnitude and direction, each attached to a point in the plane. Vector fields are often used to model, for example, the speed and direction of a moving fluid throughout space, or the strength and direction of some force, such as the magnetic or gravitational force, as it changes from one point to another point.\n\nThe elements of differential and integral calculus extend naturally to vector fields. When a vector field represents force, the line integral of a vector field represents the work done by a force moving along a path, and under this interpretation conservation of energy is exhibited as a special case of the fundamental theorem of calculus. Vector fields can usefully be thought of as representing the velocity of a moving flow in space, and this physical intuition leads to notions such as the divergence (which represents the rate of change of volume of a flow) and curl (which represents the rotation of a flow).\n\nIn coordinates, a vector field on a domain in \"n\"-dimensional Euclidean space can be represented as a vector-valued function that associates an \"n\"-tuple of real numbers to each point of the domain. This representation of a vector field depends on the coordinate system, and there is a well-defined transformation law in passing from one coordinate system to the other. Vector fields are often discussed on open subsets of Euclidean space, but also make sense on other subsets such as surfaces, where they associate an arrow tangent to the surface at each point (a tangent vector).\n\nMore generally, vector fields are defined on differentiable manifolds, which are spaces that look like Euclidean space on small scales, but may have more complicated structure on larger scales. In this setting, a vector field gives a tangent vector at each point of the manifold (that is, a section of the tangent bundle to the manifold). Vector fields are one kind of tensor field.\n\nGiven a subset \"S\" in R, a vector field is represented by a vector-valued function \"V\": \"S\" → R in standard Cartesian coordinates (\"x\", ..., \"x\"). If each component of \"V\" is continuous, then \"V\" is a continuous vector field, and more generally \"V\" is a \"C\" vector field if each component of \"V\" is \"k\" times continuously differentiable.\n\nA vector field can be visualized as assigning a vector to individual points within an \"n\"-dimensional space.\n\nGiven two \"C\"-vector fields \"V\", \"W\" defined on \"S\" and a real valued \"C\"-function \"f\" defined on \"S\", the two operations scalar multiplication and vector addition\n\ndefine the module of \"C\"-vector fields over the ring of C-functions where the multiplication of the functions is defined pointwise (therefore, it is commutative with the multiplicative identity being \"f(p) := 1\").\n\nIn physics, a vector is additionally distinguished by how its coordinates change when one measures the same vector with respect to a different background coordinate system. The transformation properties of vectors distinguish a vector as a geometrically distinct entity from a simple list of scalars, or from a covector.\n\nThus, suppose that (\"x\"...,\"x\") is a choice of Cartesian coordinates, in terms of which the components of the vector \"V\" are\n\nand suppose that (\"y\"...,\"y\") are \"n\" functions of the \"x\" defining a different coordinate system. Then the components of the vector \"V\" in the new coordinates are required to satisfy the transformation law\nSuch a transformation law is called contravariant. A similar transformation law characterizes vector fields in physics: specifically, a vector field is a specification of \"n\" functions in each coordinate system subject to the transformation law () relating the different coordinate systems.\n\nVector fields are thus contrasted with scalar fields, which associate a number or \"scalar\" to every point in space, and are also contrasted with simple lists of scalar fields, which do not transform under coordinate changes.\n\nGiven a differentiable manifold \"M\", a vector field on \"M\" is an assignment of a tangent vector to each point in \"M\". More precisely, a vector field \"F\" is a mapping from \"M\" into the tangent bundle \"TM\" so that formula_4 is the identity mapping\nwhere \"p\" denotes the projection from \"TM\" to \"M\". In other words, a vector field is a section of the tangent bundle.\n\nAn alternative definition: A smooth vector field \"X\" on a manifold M is a linear map formula_5 such that \"X\" is a derivation:\nformula_6 for all formula_7 .\n\nIf the manifold \"M\" is smooth or analytic—that is, the change of coordinates is smooth (analytic)—then one can make sense of the notion of smooth (analytic) vector fields. The collection of all smooth vector fields on a smooth manifold \"M\" is often denoted by Γ(T\"M\") or \"C\"(\"M\",T\"M\") (especially when thinking of vector fields as sections); the collection of all smooth vector fields is also denoted by formula_8 (a fraktur \"X\").\n\n\nVector fields can be constructed out of scalar fields using the gradient operator (denoted by the del: ∇).\n\nA vector field \"V\" defined on an open set \"S\" is called a gradient field or a conservative field if there exists a real-valued function (a scalar field) \"f\" on \"S\" such that\n\nThe associated flow is called the gradient flow, and is used in the method of gradient descent.\n\nThe path integral along any closed curve \"γ\" (\"γ\"(0) = \"γ\"(1)) in a conservative field is zero:\n\nA \"C\"-vector field over R \\ {0} is called a central field if\n\nwhere O(\"n\", R) is the orthogonal group. We say central fields are invariant under orthogonal transformations around 0.\n\nThe point 0 is called the center of the field.\n\nSince orthogonal transformations are actually rotations and reflections, the invariance conditions mean that vectors of a central field are always directed towards, or away from, 0; this is an alternate (and simpler) definition. A central field is always a gradient field, since defining it on one semiaxis and integrating gives an antigradient.\n\nA common technique in physics is to integrate a vector field along a curve, also called determining its line integral. Intuitively this is summing up all vector components in line with the tangents to the curve, expressed as their scalar products. For example, given a particle in a force field (e.g. gravitation), where each vector at some point in space represents the force acting there on the particle, the line integral along a certain path is the work done on the particle, when it travels along this path. Intuitively, it is the sum of the scalar products of the force vector and the small tangent vector in each point along the curve.\n\nThe line integral is constructed analogously to the Riemann integral and it exists if the curve is rectifiable (has finite length) and the vector field is continuous.\n\nGiven a vector field \"V\" and a curve γ, parametrized by \"t\" in [\"a\", \"b\"] (where \"a\" and \"b\" are real numbers), the line integral is defined as\n\nThe divergence of a vector field on Euclidean space is a function (or scalar field). In three-dimensions, the divergence is defined by\n\nwith the obvious generalization to arbitrary dimensions. The divergence at a point represents the degree to which a small volume around the point is a source or a sink for the vector flow, a result which is made precise by the divergence theorem.\n\nThe divergence can also be defined on a Riemannian manifold, that is, a manifold with a Riemannian metric that measures the length of vectors.\n\nThe curl is an operation which takes a vector field and produces another vector field. The curl is defined only in three-dimensions, but some properties of the curl can be captured in higher dimensions with the exterior derivative. In three-dimensions, it is defined by\n\nThe curl measures the density of the angular momentum of the vector flow at a point, that is, the amount to which the flow circulates around a fixed axis. This intuitive description is made precise by Stokes' theorem.\n\nThe index of a vector field is an integer that helps to describe the behaviour of a vector field around an isolated zero (i.e., an isolated singularity of the field). In the plane, the index takes the value -1 at a saddle singularity but +1 at a source or sink singularity.\n\nLet the dimension of the manifold on which the vector field is defined be n. Take a small sphere S around the zero so that no other zeros lie in the interior of S. A map from this sphere to a unit sphere of dimensions \"n\" − 1 can be constructed by dividing each vector on this sphere by its length to form a unit length vector, which is a point on the unit sphere S. This defines a continuous map from S to S. The index of the vector field at the point is the degree of this map. It can be shown that this integer does not depend on the choice of S, and therefore depends only on the vector field itself.\n\nThe index of the vector field as a whole is defined when it has just a finite number of zeroes. In this case, all zeroes are isolated, and the index of the vector field is defined to be the sum of the indices at all zeroes.\n\nThe index is not defined at any non-singular point (i.e., a point where the vector is non-zero). it is equal to +1 around a source, and more generally equal to (−1) around a saddle that has k contracting dimensions and n-k expanding dimensions. For an ordinary (2-dimensional) sphere in three-dimensional space, it can be shown that the index of any vector field on the sphere must be 2. This shows that every such vector field must have a zero. This implies the hairy ball theorem, which states that if a vector in R is assigned to each point of the unit sphere S in a continuous manner, then it is impossible to \"comb the hairs flat\", i.e., to choose the vectors in a continuous way such that they are all non-zero and tangent to S.\n\nFor a vector field on a compact manifold with a finite number of zeroes, the Poincaré-Hopf theorem states that the index of the vector field is equal to the Euler characteristic of the manifold.\n\nMichael Faraday, in his concept of \"lines of force,\" emphasized that the field \"itself\" should be an object of study, which it has become throughout physics in the form of field theory.\n\nIn addition to the magnetic field, other phenomena that were modeled by Faraday include the electrical field and light field.\n\nConsider the flow of a fluid through a region of space. At any given time, any point of the fluid has a particular velocity associated with it; thus there is a vector field associated to any flow. The converse is also true: it is possible to associate a flow to a vector field having that vector field as its velocity.\n\nGiven a vector field \"V\" defined on \"S\", one defines curves γ(\"t\") on \"S\" such that for each \"t\" in an interval \"I\"\n\nBy the Picard–Lindelöf theorem, if \"V\" is Lipschitz continuous there is a \"unique\" \"C\"-curve γ for each point \"x\" in \"S\" so that, for some ε > 0,\n\nThe curves γ are called integral curves or trajectories (or less commonly, flow lines) of the vector field \"V\" and partition \"S\" into equivalence classes. It is not always possible to extend the interval (−ε, +ε) to the whole real number line. The flow may for example reach the edge of \"S\" in a finite time.\nIn two or three dimensions one can visualize the vector field as giving rise to a flow on \"S\". If we drop a particle into this flow at a point \"p\" it will move along the curve γ in the flow depending on the initial point \"p\". If \"p\" is a stationary point of \"V\" (i.e., the vector field is equal to the zero vector at the point \"p\"), then the particle will remain at \"p\".\n\nTypical applications are streamline in fluid, geodesic flow, and one-parameter subgroups and the exponential map in Lie groups.\n\nBy definition, a vector field is called complete if every one of its flow curves exist for all time. In particular, compactly supported vector fields on a manifold are complete. If formula_18 is a complete vector field on formula_19, then the one-parameter group of diffeomorphisms generated by the flow along formula_18 exists for all time. On a compact manifold without boundary, every smooth vector field is complete. An example of an incomplete vector field formula_21 on the real line formula_22 is given by formula_23. For, the differential equation formula_24, with initial condition formula_25, has as its unique solution formula_26 if formula_27 (and formula_28 for all formula_29 if formula_30). Hence for formula_27, formula_32 is undefined at formula_33 so cannot be defined for all values of formula_34.\n\nGiven a smooth function between manifolds, \"f\": \"M\" → \"N\", the derivative is an induced map on tangent bundles, \"f\": \"TM\" → \"TN\". Given vector fields \"V\": \"M\" → \"TM\" and \"W\": \"N\" → \"TN\", we say that \"W\" is \"f\"-related to \"V\" if the equation \"W\" ∘ \"f\" = \"f\" ∘ \"V\" holds.\n\nIf \"V\" is \"f\"-related to \"W\", \"i\" = 1, 2, then the Lie bracket [\"V\", \"V\"] is \"f\"-related to [\"W\", \"W\"].\n\nReplacing vectors by \"p\"-vectors (\"p\"th exterior power of vectors) yields \"p\"-vector fields; taking the dual space and exterior powers yields differential \"k\"-forms, and combining these yields general tensor fields.\n\nAlgebraically, vector fields can be characterized as derivations of the algebra of smooth functions on the manifold, which leads to defining a vector field on a commutative algebra as a derivation on the algebra, which is developed in the theory of differential calculus over commutative algebras.\n\n\n"}
{"id": "54477872", "url": "https://en.wikipedia.org/wiki?curid=54477872", "title": "Young's inequality for products", "text": "Young's inequality for products\n\nIn mathematics, Young's inequality for products is a mathematical inequality about the product of two numbers. The inequality is named after William Henry Young and should not be confused with Young's convolution inequality.\n\nYoung's inequality for products can be used to prove Hölder's inequality. It is also widely used to estimate the norm of nonlinear terms in PDE theory, since it allows one to estimate a product of two terms by a sum of the same terms raised to a power and scaled.\n\nIn its standard form, the inequality states that if \"a\" and \"b\" are nonnegative real numbers and \"p\" and \"q\" are real numbers greater or equal than 1 such that 1/\"p\" + 1/\"q\" = 1, then\n\nThe equality holds if and only if . This form of Young's inequality can be proved by Jensen's inequality and can be used to prove Hölder's inequality.\n\nAn elementary case of Young's inequality is the inequality with exponent 2,\n\nwhich also gives rise to the so-called Young's inequality with \"ε\" (valid for every \"ε\" > 0), sometimes called the Peter–Paul inequality.\n\nT. Ando proved a generalization of Young's inequality for complex matrices ordered \nby Loewner ordering. It states that for any pair \"A\", \"B\" of complex matrices of order \"n\" there exists a unitary matrix \"U\" such that\n\nwhere * denotes the conjugate transpose of the matrix and formula_5.\n\nFor the standard version of the inequality,\nlet \"f\" denote a real-valued, continuous and strictly increasing function on [0, \"c\"] with \"c\" > 0 and \"f\"(0) = 0. Let \"f\" denote the inverse function of \"f\". Then, for all \"a\" ∈ [0, \"c\"] and \"b\" ∈ [0, \"f\"(\"c\")],\n\nwith equality if and only if \"b\" = \"f\"(\"a\").\n\nIf \"f\" is a convex function and its Legendre transform (convex conjugate) is denoted by \"g\", then\nThis follows immediately from the definition of the Legendre transform.\n\nMore generally, if \"f\" is a convex function defined on a real vector space formula_8 and its convex conjugate is denoted by formula_9 (and is defined on the dual space formula_10), then\nwhere formula_12 is the dual pairing.\n\n\n\n"}
