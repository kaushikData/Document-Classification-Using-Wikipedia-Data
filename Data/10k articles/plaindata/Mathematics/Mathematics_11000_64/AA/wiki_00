{"id": "448267", "url": "https://en.wikipedia.org/wiki?curid=448267", "title": "Almost disjoint sets", "text": "Almost disjoint sets\n\nIn mathematics, two sets are almost disjoint if their intersection is small in some sense; different definitions of \"small\" will result in different definitions of \"almost disjoint\".\nHg\n\nThe most common choice is to take \"small\" to mean finite. In this case, two sets are almost disjoint if their intersection is finite, i.e. if\n\n(Here, '|\"X\"|' denotes the cardinality of \"X\", and '< ∞' means 'finite'.) For example, the closed intervals [0, 1] and [1, 2] are almost disjoint, because their intersection is the finite set {1}. However, the unit interval [0, 1] and the set of rational numbers Q are not almost disjoint, because their intersection is infinite.\n\nThis definition extends to any collection of sets. A collection of sets is pairwise almost disjoint or mutually almost disjoint if any two \"distinct\" sets in the collection are almost disjoint. Often the prefix \"pairwise\" is dropped, and a pairwise almost disjoint collection is simply called \"almost disjoint\".\n\nFormally, let \"I\" be an index set, and for each \"i\" in \"I\", let \"A\" be a set. Then the collection of sets {\"A\" : \"i\" in \"I\"} is almost disjoint if for any \"i\" and \"j\" in \"I\",\n\nFor example, the collection of all lines through the origin in R is almost disjoint, because any two of them only meet at the origin. If {\"A\"} is an almost disjoint collection consisting of more than one set, then clearly its intersection is finite:\n\nHowever, the converse is not true—the intersection of the collection \nis empty, but the collection is \"not\" almost disjoint; in fact, the intersection of \"any\" two distinct sets in this collection is infinite.\n\nThe possible cardinalities of a maximal almost disjoint family on the set formula_5 of the natural numbers has been the object of intense study. The minimum of such cardinalities is one of the classical Cardinal characteristics of the continuum.\n\nSometimes \"almost disjoint\" is used in some other sense, or in the sense of measure theory or topological category. Here are some alternative definitions of \"almost disjoint\" that are sometimes used (similar definitions apply to infinite collections):\n\n\n\n"}
{"id": "14699765", "url": "https://en.wikipedia.org/wiki?curid=14699765", "title": "Argument (complex analysis)", "text": "Argument (complex analysis)\n\nIn mathematics, the argument is a multi-valued function operating on the nonzero complex numbers. With complex number \"z\" visualized as a point in the complex plane, the argument of \"z\" is the angle between the positive real axis and the line joining the point to the origin, shown as in figure 1 and denoted arg \"z\". To define a single-valued function, the principal value of the argument (sometimes denoted Arg \"z\") is used. It is chosen to be the unique value of the argument that lies within the interval (–π, π].\n\nAn argument of the complex number , denoted , is defined in two equivalent ways:\n\nThe names \"magnitude,\" for the modulus, and \"phase\", for the argument, are sometimes used equivalently.\n\nUnder both definitions, it can be seen that the argument of any non-zero complex number has many possible values: firstly, as a geometrical angle, it is clear that whole circle rotations do not change the point, so angles differing by an integer multiple of radians (a complete circle) are the same, as reflected by figure 2 on the right. Similarly, from the periodicity of and , the second definition also has this property. The argument of zero is usually left undefined.\n\nBecause a complete rotation around the origin leaves a complex number unchanged, there are many choices which could be made for by circling the origin any number of times. This is shown in figure 2, a representation of the multi-valued (set-valued) function formula_3, where a vertical line (not shown in the figure) cuts the surface at heights representing all the possible choices of angle for that point.\n\nWhen a well-defined function is required then the usual choice, known as the \"principal value\", is the value in the open-closed interval , that is from to radians, excluding rad itself (equivalently from −180 to +180 degrees, excluding −180° itself). This represents an angle of up to half a complete circle from the positive real axis in either direction.\n\nSome authors define the range of the principal value as being in the closed-open interval .\n\nThe principal value sometimes has the initial letter capitalized as in , especially when a general version of the argument is also being considered. Note that notation varies, so and may be interchanged in different texts.\n\nThe set of all possible values of the argument can be written in terms of as:\n\nLikewise\n\nIf a complex number is known in terms of its real and imaginary parts, then the function that calculates the principal value is called the two-argument arctangent function atan2:\nThe atan2 function (also called arctan2 or other synonyms) is available in the math libraries of many programming languages, and usually returns a value in the range .\n\nMany texts say the value is given by , as is slope, and converts slope to angle. This is correct only when , so the quotient is defined and the angle lies between and , but extending this definition to cases where is not positive is relatively involved. Specifically, one may define the principal value of the argument separately on the two half-planes and (separated into two quadrants if one wishes a branch cut on the negative -axis), , , and then patch together.\n\nA compact expression with 4 overlapping half-planes is\n\nFor the variant where is defined to lie in the interval , the value can be found by adding to the value above when it is negative.\n\nAlternatively, the principal value can be calculated in a uniform way using the tangent half-angle formula, the function being defined over the complex plane but excluding the origin:\nThis is based on a parametrization of the circle (except for the negative -axis) by rational functions. This version of is not stable enough for floating point computational use (it may overflow near the region ) but can be used in symbolic calculation.\n\nA variant of the last formula which avoids overflow is sometimes used in high precision computation:\n\nOne of the main motivations for defining the principal value is to be able to write complex numbers in modulus-argument form. Hence for any complex number ,\n\nThis is only really valid if is non-zero but can be considered as valid also for if is considered as being an indeterminate form rather than as being undefined.\n\nSome further identities follow. If and are two non-zero complex numbers, then\n\nIf and is any integer, then\n"}
{"id": "6494256", "url": "https://en.wikipedia.org/wiki?curid=6494256", "title": "CTL*", "text": "CTL*\n\nCTL* is a superset of computational tree logic (CTL) and linear temporal logic (LTL). It freely combines path quantifiers and temporal operators. Like CTL, CTL* is a branching time logic. The formal semantics of CTL* formulae are defined with respect to a given Kripke structure.\n\nLTL has been proposed for the verification of computer programs first by Amir Pnueli in 1977. Four years later in 1981 E. M. Clarke and E. A. Emerson invented CTL and CTL model checking. CTL* was defined by E. A. Emerson and Joseph Y. Halpern in 1986.\n\nCTL and LTL were developed independently before CTL*. Both sublogics have become standards in the model checking community, while CTL* is of practical importance because it provides an expressive testbed for representing and comparing these and other logics. This is surprising because the computational complexity of model checking in CTL* is not worse than that of LTL: they both lie in PSPACE.\n\nThe language of well-formed CTL* formulae is generated by the following unambiguous (wrt bracketing) context-free grammar:\n\nwhere formula_3 ranges over a set of atomic formulas. Valid CTL*-formulae are built using the nonterminal formula_4. These formulae are called \"state formulae\", while those created by the symbol formula_5 are called \"path formulae\". (The above grammar contains some redundancies; for example formula_6 as well as implication and equivalence can be defined as just for Boolean algebras (or propositional logic) from negation and conjunction, and the temporal operators X and U are sufficient to define the other two.)\n\nThe operators basically are the same as in CTL. However, in CTL, every temporal operator (formula_7) has to be directly preceded by a quantifier, while in CTL* this is not required. The universal path quantifier may be defined in CTL* in the same way as for classical predicate calculus formula_8, although this in not possible in the CTL fragment.\n\n\nRemark: When taking LTL as subset of CTL*, any LTL formula is implicitly prefixed with the universal path quantifier formula_13.\n\nThe semantics of CTL* are defined with respect to some Kripke structure. As the names imply, state formulae are interpreted with respect to the states of this structure, while path formulae are interpreted over paths on it.\n\nIf a state formula_14 of the Kripke structure satisfies a state formula formula_4 it is denoted formula_16. This relation is defined inductively as follows:\n\n\nThe satisfaction relation formula_30 for path formulae formula_31 and a path formula_32 is also defined inductively. For this, let formula_33 denote the sub-path formula_34:\n\n\nModel checking of CTL* is PSPACE-complete and the satisfiability problem is 2EXPTIME-complete.\n\n\n\n"}
{"id": "51223601", "url": "https://en.wikipedia.org/wiki?curid=51223601", "title": "Cap set", "text": "Cap set\n\nIn mathematics, a cap set is a subset of formula_1 (an formula_2-dimensional affine space over a three-element field) with no three elements in a line.\nThe cap set problem is the problem of finding the size of the largest possible cap set, as a function of formula_2. Cap sets may be defined more generally as subsets of finite affine or projective spaces with no three in line, where these objects are simply called caps.\nAn example of this problem comes from the game Set, a card game in which each card has four features (its number, symbol, shading, and color), each of which can take one of three values. The cards of this game can be interpreted as representing points of the four-dimensional affine space formula_4, where each coordinate of a point specifies the value of one of the features. A line, in this space, is a triple of cards that, in each feature, are either all the same as each other or all different from each other. The game play consists of finding and collecting lines among the cards that are currently face up, and a cap set describes an array of face-up cards in which no lines may be collected.\n\nOne way to construct a large cap set in the game Set would be to choose two out of the three values for each feature, and place face up each of the cards that uses only one of those two values in each of its features. The result would be a cap set of 16 cards. More generally, the same strategy would lead to cap sets in formula_1 of size formula_6. However, in 1971, Giuseppe Pellegrino proved that Set has larger cap sets, consisting of up to 20 cards, and that this is the largest possible size for these sets. Pellegrino's solution for the four-dimensional cap-set problem also leads to larger lower bounds than formula_6 for any higher dimension, which were further improved by to approximately formula_8.\nIn 1984, Tom Brown and Joe Buhler proved that the largest possible size of a cap set in formula_1 is formula_10 as formula_2 grows; loosely speaking, this means that cap sets have zero density. Péter Frankl, Ronald Graham, and Vojtěch Rödl have shown in 1987 that the result of Brown and Buhler follows easily from the Ruzsa - Szemerédi triangle removal lemma, and asked whether there exists a constant formula_12 such that, indeed, for all sufficiently large values of formula_2, any cap set in formula_1 has size at most formula_15; that is, whether any set in formula_1 of size exceeding formula_15 contains an affine line. This question also appeared in a paper published by Noga Alon and Moshe Dubiner in 1995. Same year, Roy Meshulam proved that the size of a cap set does not exceed formula_18. Determining whether Meshulam's bound can be improved to formula_15 with formula_12 was considered one of the most intriguing open problems in additive combinatorics and Ramsey theory for over 20 years; see, for instance, blog posts on this problem by the Fields medalists Timothy Gowers and Terence Tao (in the latter post, Tao refers to this problem as \"perhaps, my favorite open problem\"). It was considered a major breakthrough when in 2011, Michael Bateman and Nets Katz improved the bound to formula_21 with a positive constant formula_22. The cap set problem was solved in 2016, when Ernie Croot, Vsevolod Lev, and Péter Pál Pach posted a preprint on a tightly related problem, that was quickly used by Jordan Ellenberg and Dion Gijswijt to prove an upper bound of formula_23 on the cap set problem.\n\nThe solution to the cap set problem can also be used to prove a partial form of the sunflower conjecture, namely that if a family of subsets of an formula_2-element set has no three subsets whose pairwise intersections are all equal, then the number of subsets in the family is at most formula_15 for a constant formula_26. The new upper bounds also imply lower bounds on certain types of algorithms for matrix multiplication.\n\nThe \"cap set\" terminology should be distinguished from other unrelated mathematical objects with the same name, and in particular from sets with the compact absorption property in function spaces as well as from compact convex co-convex subsets of a convex set.\n\n"}
{"id": "3559586", "url": "https://en.wikipedia.org/wiki?curid=3559586", "title": "Chiral symmetry breaking", "text": "Chiral symmetry breaking\n\nIn particle physics, chiral symmetry breaking is the spontaneous symmetry breaking of a chiral symmetry – usually by a gauge theory such as quantum chromodynamics, the quantum field theory of the strong interaction. Yoichiro Nambu was awarded the 2008 Nobel prize in physics for describing this phenomenon (\"for the discovery of the mechanism of spontaneous broken symmetry in subatomic physics\").\n\nExperimentally, it is observed that the masses of the octet of pseudoscalar mesons (such as the pion) are much lighter than the next heavier states such as the octet of vector mesons, such as rho meson.\n\nThis is a consequence of spontaneous symmetry breaking of chiral symmetry in a fermion sector of QCD with 3 flavors of light quarks, \"u\", \"d\" and \"s\". Such a theory, for idealized massless quarks, has global chiral flavor symmetry. Under SSB, this is spontaneously broken to the diagonal flavor \"SU\"(3) subgroup, generating eight Nambu–Goldstone bosons, which are the pseudoscalar mesons transforming as an octet representation of this flavor \"SU\"(3).\n\nBeyond this idealization of massless quarks, the actual small quark masses also break the chiral symmetry explicitly as well (providing non-vanishing pieces to the divergence of chiral currents). The masses of the pseudoscalar meson octet are specified by an expansion in the quark masses which goes by the name of chiral perturbation theory. The internal consistency of this argument is further checked by lattice QCD computations, which allow one to vary the quark mass and confirm that the variation of the pseudoscalar masses with the quark masses is as dictated by chiral perturbation theory, effectively as the square-root of the quark masses.\n\nFor the three heavy quarks: the charm quark, bottom quark, and top quark, their masses, and hence the explicit breaking these amount to, are much larger than the QCD spontaneous chiral symmetry breaking scale. Thus, they cannot be treated as a small perturbation around the explicit symmetry limit.\n\nChiral symmetry breaking is most apparent in the mass generation of nucleons from more elementary light quarks, accounting for approximately 99% of their combined mass as a baryon. It thus accounts for most of the mass of all visible matter. For example, in the proton, of mass \"m\" ≈ 938 MeV, the valence quarks, two up quarks with \"m\" ≈ 2.3 MeV and one down quark with \"m\" ≈ 4.8 MeV, only contribute about 9.4 MeV to the proton's mass. The source of the bulk of the proton's mass is quantum chromodynamics binding energy, which arises out of QCD chiral symmetry breaking.\n\nThe spontaneous symmetry breaking may be described in analogy to magnetization.\n\nA vacuum condensate of bilinear expressions involving the quarks in the QCD vacuum is known as the fermion condensate.\n\nIt can be calculated as\nformed through nonperturbative action of QCD gluons, with \"v\" ≈ −(250 MeV). This cannot be preserved under an isolated \"L\" or \"R\" rotation. The pion decay constant, ≈ 93 MeV, may be viewed as a measure of the strength of the chiral symmetry breaking.\n\nFor two light quarks, the up quark and the down quark, the QCD Lagrangian provides insight. The symmetry of the QCD Lagrangian, called \"chiral symmetry\" describes invariance with respect to a symmetry group formula_2. This symmetry group amounts to\n\nThe quark condensate induced by nonperturbative strong interactions spontaneously breaks the formula_4 down to the diagonal vector subgroup \"SU(2)\", known as isospin. The resulting effective theory of baryon bound states of QCD (which describes protons and neutrons), then, has mass terms for these, disallowed by the original linear realization of the chiral symmetry, but allowed by the spontaneously broken nonlinear realization thus achieved as a result of the strong interactions.\n\nThe Nambu-Goldstone bosons corresponding to the three broken generators are the three pions, charged and neutral.\n\nPseudo-Goldstone bosons arise in a quantum field theory with \"both\" spontaneous and explicit symmetry breaking. These two types of symmetry breaking typically occur separately, and at different energy scales, and are not thought to be predicated on each other.\n\nIn the absence of explicit breaking, spontaneous symmetry breaking would engender massless Nambu–Goldstone bosons for the exact spontaneously broken chiral symmetries. The chiral symmetries discussed, however, are only approximate symmetries, given their \"small\" explicit breaking.\n\nThe explicit symmetry breaking occurs at a smaller energy scale. The properties of these pseudo-Goldstone bosons can normally be calculated using chiral perturbation theory, expanding around the exactly symmetric theory in terms of the explicit symmetry-breaking parameters. In particular, the computed mass must be small, .\n\nFor three light quarks, the up quark, down quark, and strange quark, the flavor-chiral symmetries extending those discussed above also decompose, to Gell-Mann's \n\nThe chiral symmetry generators spontaneously broken comprise the coset space formula_6. This space is not a group, and consists of the eight axial generators, corresponding to the eight light pseudoscalar mesons, the nondiagonal part of formula_7.\n\nThe remaining eight unbroken vector subgroup generators constitute the manifest standard \"Eightfold Way\" flavor symmetries, \"SU(3)\".\n\n\n"}
{"id": "6594053", "url": "https://en.wikipedia.org/wiki?curid=6594053", "title": "Cover-coding", "text": "Cover-coding\n\nCover-coding is a technique for obscuring the data that is transmitted over an insecure link, to reduce the risks of snooping. An example of cover-coding would be for the sender to perform a bitwise XOR (exclusive OR) of the original data with a password or random number which is known to both sender and receiver. The resulting cover-coded data is then transmitted from sender to the receiver, who uncovers the original data by performing a further bitwise XOR (exclusive OR) operation on the received data using the same password or random number.\n"}
{"id": "4982245", "url": "https://en.wikipedia.org/wiki?curid=4982245", "title": "Degen's eight-square identity", "text": "Degen's eight-square identity\n\nIn mathematics, Degen's eight-square identity establishes that the product of two numbers, each of which is a sum of eight squares, is itself the sum of eight squares.\nNamely:\n\nFirst discovered by Carl Ferdinand Degen around 1818, the identity was independently rediscovered by John Thomas Graves (1843) and Arthur Cayley (1845). The latter two derived it while working on an extension of quaternions called octonions. In algebraic terms the identity means that the norm of product of two octonions equals the product of their norms: formula_10. Similar statements are true for quaternions (Euler's four-square identity), complex numbers (the Brahmagupta–Fibonacci two-square identity) and real numbers. In 1898 Adolf Hurwitz proved that there is no similar bilinear identity for 16 squares (sedenions) or any other number of squares except for 1,2,4, and 8. However, in the 1960s, H. Zassenhaus, W. Eichhorn, and A. Pfister (independently) showed there can be a non-bilinear identity for 16 squares. \n\nNote that each quadrant reduces to a version of Euler's four-square identity:\n\nand similarly for the other three quadrants. By Pfister's theorem, a different sort of eight-square identity can be given where the formula_16, introduced below, are non-bilinear and merely rational functions of the formula_17. Thus,\n\nwhere,\n\nand,\n\nwith,\n\nIncidentally, the formula_32 obey the identity,\n\n\n"}
{"id": "26611926", "url": "https://en.wikipedia.org/wiki?curid=26611926", "title": "Dirichlet kernel", "text": "Dirichlet kernel\n\nIn mathematical analysis, the Dirichlet kernel is the collection of functions\n\nIt is named after Peter Gustav Lejeune Dirichlet.\n\nThe importance of the Dirichlet kernel comes from its relation to Fourier series. The convolution of \"D\"(\"x\") with any function \"ƒ\" of period 2 is the \"n\"th-degree Fourier series approximation to \"ƒ\", i.e., we have\n\nwhere\n\nis the \"k\"th Fourier coefficient of \"ƒ\". This implies that in order to study convergence of Fourier series it is enough to study properties of the Dirichlet kernel. \n\nOf particular importance is the fact that the \"L\" norm of \"D\" on formula_4 diverges to infinity as \"n\" → ∞. One can estimate that\n\nBy using a Riemann-sum argument to estimate the contribution in the largest neighbourhood of zero in which formula_6 is positive, and the Jensen's inequality for the remaining part, it is also possible to show that:\n\nThis lack of uniform integrability is behind many divergence phenomena for the Fourier series. For example, together with the uniform boundedness principle, it can be used to show that the Fourier series of a continuous function may fail to converge pointwise, in rather dramatic fashion. See convergence of Fourier series for further details. \n\nA precise proof of the first result that formula_8 is given by \n\nwhere we have used the Taylor series identity that formula_10 and where formula_11 are the first-order harmonic numbers.\n\nTake the periodic Dirac delta function, which is not a function of a real variable, but rather a \"generalized function\", also called a \"distribution\", and multiply by 2. We get the identity element for convolution on functions of period 2. In other words, we have\n\nfor every function \"ƒ\" of period 2. The Fourier series representation of this \"function\" is\n\nTherefore the Dirichlet kernel, which is just the sequence of partial sums of this series, can be thought of as an \"approximate identity\". Abstractly speaking it is not however an approximate identity of \"positive\" elements (hence the failures mentioned above).\n\nThe trigonometric identity\n\ndisplayed at the top of this article may be established as follows. First recall that the sum of a finite geometric series is\n\nIn particular, we have\n\nMultiply both the numerator and the denominator by \"r\", getting\n\nIn the case \"r\" = \"e\" we have\n\nas required.\n\nStart with the series\n\nMultiply both sides of the above by\n\nand use the trigonometric identity\n\nto reduce the right-hand side to\n\nIf the sum is only over non negative integers (which may arise when computing a DFT that is not centered), then using similar techniques we can show the following identity:\n\n\n"}
{"id": "8529", "url": "https://en.wikipedia.org/wiki?curid=8529", "title": "Disjunction elimination", "text": "Disjunction elimination\n\nIn propositional logic, disjunction elimination (sometimes named proof by cases, case analysis, or or elimination), is the valid argument form and rule of inference that allows one to eliminate a disjunctive statement from a logical proof. It is the inference that if a statement formula_1 implies a statement formula_2 and a statement formula_3 also implies formula_2, then if either formula_1 or formula_3 is true, then formula_2 has to be true. The reasoning is simple: since at least one of the statements P and R is true, and since either of them would be sufficient to entail Q, Q is certainly true.\n\nAn example in English:\n\nIt is the rule can be stated as:\n\nwhere the rule is that whenever instances of \"formula_9\", and \"formula_10\" and \"formula_11\" appear on lines of a proof, \"formula_2\" can be placed on a subsequent line.\n\nThe \"disjunction elimination\" rule may be written in sequent notation:\n\nwhere formula_14 is a metalogical symbol meaning that formula_2 is a syntactic consequence of formula_9, and formula_10 and formula_11 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_1, formula_2, and formula_3 are propositions expressed in some formal system.\n\n"}
{"id": "39147", "url": "https://en.wikipedia.org/wiki?curid=39147", "title": "Finite difference", "text": "Finite difference\n\nA finite difference is a mathematical expression of the form . If a finite difference is divided by , one gets a difference quotient. The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems.\n\nCertain recurrence relations can be written as difference equations by replacing iteration notation with finite differences.\n\nToday, the term \"finite difference\" is often taken as synonymous with finite difference approximations of derivatives, especially in the context of numerical methods. Finite difference approximations are finite difference quotients in the terminology employed above.\n\nFinite differences have also been the topic of study as abstract self-standing mathematical objects, such as in works by George Boole (1860), L. M. Milne-Thomson (1933), and (1939), tracing its origins back to one of Jost Bürgi's algorithms () and others including Isaac Newton. In this viewpoint, the formal calculus of finite differences is an alternative to the calculus of infinitesimals.\n\nThree forms are commonly considered: forward, backward, and central differences.\n\nA forward difference is an expression of the form\n\nDepending on the application, the spacing may be variable or constant. When omitted, is taken to be 1: .\n\nA backward difference uses the function values at and , instead of the values at and :\nFinally, the central difference is given by\n\nThe derivative of a function at a point is defined by the limit.\n\nIf has a fixed (non-zero) value instead of approaching zero, then the right-hand side of the above equation would be written\n\nHence, the forward difference divided by approximates the derivative when is small. The error in this approximation can be derived from Taylor's theorem. Assuming that is differentiable, we have\n\nThe same formula holds for the backward difference:\n\nHowever, the central (also called centered) difference yields a more accurate approximation. If is twice differentiable,\n\nThe main problem with the central difference method, however, is that oscillating functions can yield zero derivative. If for odd, and for even, then if it is calculated with the central difference scheme. This is particularly troublesome if the domain of is discrete.\n\nAuthors for whom finite differences mean finite difference approximations define the forward/backward/central differences as the quotients given in this section (instead of employing the definitions given in the previous section).\n\nIn an analogous way, one can obtain finite difference approximations to higher order derivatives and differential operators. For example, by using the above central difference formula for and and applying a central difference formula for the derivative of at , we obtain the central difference approximation of the second derivative of :\n\n\nSimilarly we can apply other differencing formulas in a recursive manner.\n\n\n\nMore generally, the th order forward, backward, and central differences are given by, respectively,\n\n\nor for ,\n\n\n\nThese equations use binomial coefficients after the summation sign shown as . Each row of Pascal's triangle provides the coefficient for each value of .\n\nNote that the central difference will, for odd , have multiplied by non-integers. This is often a problem because it amounts to changing the interval of discretization. The problem may be remedied taking the average of and .\n\nForward differences applied to a sequence are sometimes called the binomial transform of the sequence, and have a number of interesting combinatorial properties. Forward differences may be evaluated using the Nörlund–Rice integral. The integral representation for these types of series is interesting, because the integral can often be evaluated using asymptotic expansion or saddle-point techniques; by contrast, the forward difference series can be extremely hard to evaluate numerically, because the binomial coefficients grow rapidly for large .\n\nThe relationship of these higher-order differences with the respective derivatives is straightforward,\n\nHigher-order differences can also be used to construct better approximations. As mentioned above, the first-order difference approximates the first-order derivative up to a term of order . However, the combination\napproximates up to a term of order . This can be proven by expanding the above expression in Taylor series, or by using the calculus of finite differences, explained below.\n\nIf necessary, the finite difference can be centered about any point by mixing forward, backward, and central differences.\n\nUsing linear algebra one can construct finite difference approximations which utilize an arbitrary number of points to the left and a (possibly different) number of points to the right of the evaluation point, for any order derivative. This involves solving a linear system such that the Taylor expansion of the sum of those points around the evaluation point best approximates the Taylor expansion of the desired derivative. Such formulas can be represented graphically on a hexagonal or diamond-shaped grid.\n\nThis is useful for differentiating a function on a grid, where, as one approaches the edge of the grid, one must sample fewer and fewer points on one side.\n\nThe details are outlined in these notes.\n\nThe Finite Difference Coefficients Calculator constructs finite difference approximations for non-standard (and even non-integer) stencils given an arbitrary stencil and a desired derivative order.\n\n\n\nAn important application of finite differences is in numerical analysis, especially in numerical differential equations, which aim at the numerical solution of ordinary and partial differential equations respectively. The idea is to replace the derivatives appearing in the differential equation by finite differences that approximate them. The resulting methods are called finite difference methods.\n\nCommon applications of the finite difference method are in computational science and engineering disciplines, such as thermal engineering, fluid mechanics, etc.\n\nAn open Python package of the finite difference method for arbitrary accuracy and order in any dimension on uniform and non-uniform grids is the Findiff project.\n\nThe Newton series consists of the terms of the Newton forward difference equation, named after Isaac Newton; in essence, it is the Newton interpolation formula, first published in his \"Principia Mathematica\" in 1687, namely the discrete analog of the continuum Taylor expansion,\nwhich holds for any polynomial function and for most (but not all) analytic functions. Here, the expression\n\nis the binomial coefficient, and\n\nis the \"falling factorial\" or \"lower factorial\", while the empty product is defined to be 1. In this particular case, there is an assumption of unit steps for the changes in the values of of the generalization below.\n\nNote the formal correspondence of this result to Taylor's theorem. Historically, this, as well as the Chu–Vandermonde identity, \n(following from it, and corresponding to the binomial theorem), are included in the observations that matured to the system of umbral calculus.\n\nTo illustrate how one may use Newton's formula in actual practice, consider the first few terms of doubling the Fibonacci sequence One can find a polynomial that reproduces these values, by first computing a difference table, and then substituting the differences that correspond to (underlined) into the formula as follows,\n\nFor the case of nonuniform steps in the values of , Newton computes the divided differences,\nthe series of products,\nand the resulting polynomial is the scalar product,\n\nIn analysis with -adic numbers, Mahler's theorem states that the assumption that is a polynomial function can be weakened all the way to the assumption that is merely continuous.\n\nCarlson's theorem provides necessary and sufficient conditions for a Newton series to be unique, if it exists. However, a Newton series does not, in general, exist.\n\nThe Newton series, together with the Stirling series and the Selberg series, is a special case of the general difference series, all of which are defined in terms of suitably scaled forward differences.\n\nIn a compressed and slightly more general form and equidistant nodes the formula reads\n\nThe forward difference can be considered as an operator, called the , which maps the function to . This operator amounts to\nwhere is the shift operator with step \"h\", defined by , and is the identity operator.\n\nThe finite difference of higher orders can be defined in recursive manner as . Another equivalent definition is .\n\nThe difference operator is a linear operator, as such it satisfies .\n\nIt also satisfies a special Leibniz rule indicated above,\n. Similar statements hold for the backward and central differences.\n\nFormally applying the Taylor series with respect to , yields the formula\nwhere denotes the continuum derivative operator, mapping to its derivative . The expansion is valid when both sides act on analytic functions, for sufficiently small . Thus, , and formally inverting the exponential yields \nThis formula holds in the sense that both operators give the same result when applied to a polynomial.\n\nEven for analytic functions, the series on the right is not guaranteed to converge; it may be an asymptotic series. However, it can be used to obtain more accurate approximations for the derivative. For instance, retaining the first two terms of the series yields the second-order approximation to mentioned at the end of the section \"Higher-order differences\".\n\nThe analogous formulas for the backward and central difference operators are\n\nThe calculus of finite differences is related to the umbral calculus of combinatorics. This remarkably systematic correspondence is due to the identity of the commutators of the umbral quantities to their continuum analogs ( limits),\n\nA large number of formal differential relations of standard calculus involving \nfunctions thus \"map systematically to umbral finite-difference analogs\" involving .\n\nFor instance, the umbral analog of a monomial is a generalization of the above falling factorial (Pochhammer k-symbol), \nso that \nhence the above Newton interpolation formula (by matching coefficients in the expansion of an arbitrary function in such symbols), and so on.\n\nFor example, the umbral sine is\n\nAs in the continuum limit, the eigenfunction of also happens to be an exponential,\n\nand hence \"Fourier sums of continuum functions are readily mapped to umbral Fourier sums faithfully\", i.e., involving the same Fourier coefficients multiplying these umbral basis exponentials. This umbral exponential thus amounts to the exponential generating function of the Pochhammer symbols.\n\nThus, for instance, the Dirac delta function maps to its umbral correspondent, the cardinal sine function,\n\nand so forth. Difference equations can often be solved with techniques very similar to those for solving differential equations.\n\nThe inverse operator of the forward difference operator, so then the umbral integral, is the indefinite sum or antidifference operator.\n\nAnalogous to rules for finding the derivative, we have:\n\nAll of the above rules apply equally well to any difference operator, including as to .\n\n\nSee references.\n\nwhere is its coefficient vector. An infinite difference is a further generalization, where the finite sum above is replaced by an infinite series. Another way of generalization is making coefficients depend on point : , thus considering weighted finite difference. Also one may make the step depend on point : . Such generalizations are useful for constructing different modulus of continuity.\n\n\nFinite differences can be considered in more than one variable. They are analogous to partial derivatives in several variables.\n\nSome partial derivative approximations are:\n\nAlternatively, for applications in which the computation of is the most costly step, and both first and second derivatives must be computed, a more efficient formula for the last case is\n\nsince the only values to compute that are not already needed for the previous four equations are and .\n\n\n"}
{"id": "11659", "url": "https://en.wikipedia.org/wiki?curid=11659", "title": "Fourier analysis", "text": "Fourier analysis\n\nIn mathematics, Fourier analysis () is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer.\n\nToday, the subject of Fourier analysis encompasses a vast spectrum of mathematics. In the sciences and engineering, the process of decomposing a function into oscillatory components is often called Fourier analysis, while the operation of rebuilding the function from these pieces is known as Fourier synthesis. For example, determining what component frequencies are present in a musical note would involve computing the Fourier transform of a sampled musical note. One could then re-synthesize the same sound by including the frequency components as revealed in the Fourier analysis. In mathematics, the term \"Fourier analysis\" often refers to the study of both operations.\n\nThe decomposition process itself is called a Fourier transformation. Its output, the Fourier transform, is often given a more specific name, which depends on the domain and other properties of the function being transformed. Moreover, the original concept of Fourier analysis has been extended over time to apply to more and more abstract and general situations, and the general field is often known as harmonic analysis. Each transform used for analysis (see list of Fourier-related transforms) has a corresponding inverse transform that can be used for synthesis.\n\nFourier analysis has many scientific applications – in physics, partial differential equations, number theory, combinatorics, signal processing, digital image processing, probability theory, statistics, forensics, option pricing, cryptography, numerical analysis, acoustics, oceanography, sonar, optics, diffraction, geometry, protein structure analysis, and other areas.\n\nThis wide applicability stems from many useful properties of the transforms:\n\nIn forensics, laboratory infrared spectrophotometers use Fourier transform analysis for measuring the wavelengths of light at which a material will absorb in the infrared spectrum. The FT method is used to decode the measured signals and record the wavelength data. And by using a computer, these Fourier calculations are rapidly carried out, so that in a matter of seconds, a computer-operated FT-IR instrument can produce an infrared absorption pattern comparable to that of a prism instrument.\n\nFourier transformation is also useful as a compact representation of a signal. For example, JPEG compression uses a variant of the Fourier transformation (discrete cosine transform) of small square pieces of a digital image. The Fourier components of each square are rounded to lower arithmetic precision, and weak components are eliminated entirely, so that the remaining components can be stored very compactly. In image reconstruction, each image square is reassembled from the preserved approximate Fourier-transformed components, which are then inverse-transformed to produce an approximation of the original image.\n\nWhen processing signals, such as audio, radio waves, light waves, seismic waves, and even images, Fourier analysis can isolate narrowband components of a compound waveform, concentrating them for easier detection or removal. A large family of signal processing techniques consist of Fourier-transforming a signal, manipulating the Fourier-transformed data in a simple way, and reversing the transformation.\n\nSome examples include:\n\nMost often, the unqualified term Fourier transform refers to the transform of functions of a continuous real argument, and it produces a continuous function of frequency, known as a \"frequency distribution\". One function is transformed into another, and the operation is reversible. When the domain of the input (initial) function is time (), and the domain of the output (final) function is ordinary frequency, the transform of function at frequency is given by the complex number:\n\nEvaluating this quantity for all values of produces the \"frequency-domain\" function. Then can be represented as a recombination of complex exponentials of all possible frequencies:\n\nwhich is the inverse transform formula. The complex number, , conveys both amplitude and phase of frequency .\n\nSee Fourier transform for much more information, including:\n\nThe Fourier transform of a periodic function, , with period , becomes a Dirac comb function, modulated by a sequence of complex coefficients:\n\nfor all integer values of , and where is the integral over any interval of length \"P\".\n\nThe inverse transform, known as Fourier series, is a representation of in terms of a summation of a potentially infinite number of harmonically related sinusoids or complex exponential functions, each with an amplitude and phase specified by one of the coefficients:\n\nWhen , is expressed as a periodic summation of another function, :\n\nthe coefficients are proportional to samples of at discrete intervals of :\n\nA sufficient condition for recovering (and therefore ) from just these samples (i.e. from the Fourier series) is that the non-zero portion of be confined to a known interval of duration , which is the frequency domain dual of the Nyquist–Shannon sampling theorem.\n\nSee Fourier series for more information, including the historical development.\n\nThe DTFT is the mathematical dual of the time-domain Fourier series. Thus, a convergent periodic summation in the frequency domain can be represented by a Fourier series, whose coefficients are samples of a related continuous time function:\n\nwhich is known as the DTFT. Thus the DTFT of the sequence is also the Fourier transform of the modulated Dirac comb function.\n\nThe Fourier series coefficients (and inverse transform), are defined by:\n\nParameter corresponds to the sampling interval, and this Fourier series can now be recognized as a form of the Poisson summation formula. Thus we have the important result that when a discrete data sequence, , is proportional to samples of an underlying continuous function, , one can observe a periodic summation of the continuous Fourier transform, . That is a cornerstone in the foundation of digital signal processing. Furthermore, under certain idealized conditions one can theoretically recover and exactly. A sufficient condition for perfect recovery is that the non-zero portion of be confined to a known frequency interval of width . When that interval is , the applicable reconstruction formula is the Whittaker–Shannon interpolation formula.\n\nAnother reason to be interested in is that it often provides insight into the amount of aliasing caused by the sampling process.\n\nApplications of the DTFT are not limited to sampled functions. See Discrete-time Fourier transform for more information on this and other topics, including:\n\nSimilar to a Fourier series, the DTFT of a periodic sequence, , with period , becomes a Dirac comb function, modulated by a sequence of complex coefficients (see DTFT/Periodic data):\n\nThe sequence is what is customarily known as the DFT of .  It is also -periodic, so it is never necessary to compute more than coefficients. The inverse transform is given by:\n\nWhen is expressed as a periodic summation of another function:\n\nthe coefficients are proportional to samples of at discrete intervals of :\n\nConversely, when one wants to compute an arbitrary number () of discrete samples of one cycle of a continuous DTFT, , it can be done by computing the relatively simple DFT of , as defined above. In most cases, is chosen equal to the length of non-zero portion of . Increasing , known as \"zero-padding\" or \"interpolation\", results in more closely spaced samples of one cycle of . Decreasing , causes overlap (adding) in the time-domain (analogous to aliasing), which corresponds to decimation in the frequency domain. (see Sampling the DTFT) In most cases of practical interest, the sequence represents a longer sequence that was truncated by the application of a finite-length window function or FIR filter array.\n\nThe DFT can be computed using a fast Fourier transform (FFT) algorithm, which makes it a practical and important transformation on computers.\n\nSee Discrete Fourier transform for much more information, including:\n\nFor periodic functions, both the Fourier transform and the DTFT comprise only a discrete set of frequency components (Fourier series), and the transforms diverge at those frequencies. One common practice (not discussed above) is to handle that divergence via Dirac delta and Dirac comb functions. But the same spectral information can be discerned from just one cycle of the periodic function, since all the other cycles are identical. Similarly, finite-duration functions can be represented as a Fourier series, with no actual loss of information except that the periodicity of the inverse transform is a mere artifact.\n\nWe also note that it is common in practice for the duration of \"s\"(•) to be limited to the period, or .  But these formulas do not require that condition.\n\nThe Fourier variants can also be generalized to Fourier transforms on arbitrary locally compact Abelian topological groups, which are studied in harmonic analysis; there, the Fourier transform takes functions on a group to functions on the dual group. This treatment also allows a general formulation of the convolution theorem, which relates Fourier transforms and convolutions. See also the Pontryagin duality for the generalized underpinnings of the Fourier transform.\n\nMore specific, Fourier analysis can be done on cosets, even discrete cosets.\n\nIn signal processing terms, a function (of time) is a representation of a signal with perfect \"time resolution\", but no frequency information, while the Fourier transform has perfect \"frequency resolution\", but no time information.\n\nAs alternatives to the Fourier transform, in time–frequency analysis, one uses time–frequency transforms to represent signals in a form that has some time information and some frequency information – by the uncertainty principle, there is a trade-off between these. These can be generalizations of the Fourier transform, such as the short-time Fourier transform, the Gabor transform or fractional Fourier transform (FRFT), or can use different functions to represent signals, as in wavelet transforms and chirplet transforms, with the wavelet analog of the (continuous) Fourier transform being the continuous wavelet transform.\n\nLike the empirical mode decomposition (EMD), the Fourier decomposition method (FDM) proposed in 2015 which produces a time-frequency representation that is not limited by the uncertainty principle. \nFor many decades, there was a perception in the literature that Fourier theory is not suitable for nonlinear and no-stationary time-series analysis. However, proposed FDM has established that Fourier theory is a most suitable tool for nonlinear and no-stationary time-series analysis as well, and it has been applied in many applications such as biomedical data (ECG, EEG), speech signal, seismic data, and global temperature time-series analysis. \n\nA primitive form of harmonic series dates back to ancient Babylonian mathematics, where they were used to compute ephemerides (tables of astronomical positions).\n\nThe classical Greek concepts of deferent and epicycle in the Ptolemaic system of astronomy were related to Fourier series (see Deferent and epicycle: Mathematical formalism).\n\nIn modern times, variants of the discrete Fourier transform were used by Alexis Clairaut in 1754 to compute an orbit,\nwhich has been described as the first formula for the DFT,\nand in 1759 by Joseph Louis Lagrange, in computing the coefficients of a trigonometric series for a vibrating string. Technically, Clairaut's work was a cosine-only series (a form of discrete cosine transform), while Lagrange's work was a sine-only series (a form of discrete sine transform); a true cosine+sine DFT was used by Gauss in 1805 for trigonometric interpolation of asteroid orbits.\nEuler and Lagrange both discretized the vibrating string problem, using what would today be called samples.\n\nAn early modern development toward Fourier analysis was the 1770 paper \"Réflexions sur la résolution algébrique des équations\" by Lagrange, which in the method of Lagrange resolvents used a complex Fourier decomposition to study the solution of a cubic:\nLagrange transformed the roots into the resolvents:\n\nwhere is a cubic root of unity, which is the DFT of order 3.\n\nA number of authors, notably Jean le Rond d'Alembert, and Carl Friedrich Gauss used trigonometric series to study the heat equation, but the breakthrough development was the 1807 paper \"Mémoire sur la propagation de la chaleur dans les corps solides\" by Joseph Fourier, whose crucial insight was to model \"all\" functions by trigonometric series, introducing the Fourier series.\n\nHistorians are divided as to how much to credit Lagrange and others for the development of Fourier theory: Daniel Bernoulli and Leonhard Euler had introduced trigonometric representations of functions, and Lagrange had given the Fourier series solution to the wave equation, so Fourier's contribution was mainly the bold claim that an arbitrary function could be represented by a Fourier series.\n\nThe subsequent development of the field is known as harmonic analysis, and is also an early instance of representation theory.\n\nThe first fast Fourier transform (FFT) algorithm for the DFT was discovered around 1805 by Carl Friedrich Gauss when interpolating measurements of the orbit of the asteroids Juno and Pallas, although that particular FFT algorithm is more often attributed to its modern rediscoverers Cooley and Tukey.\n\nIn signal processing, the Fourier transform often takes a time series or a function of continuous time, and maps it into a frequency spectrum. That is, it takes a function from the time domain into the frequency domain; it is a decomposition of a function into sinusoids of different frequencies; in the case of a Fourier series or discrete Fourier transform, the sinusoids are harmonics of the fundamental frequency of the function being analyzed.\n\nWhen the function is a function of time and represents a physical signal, the transform has a standard interpretation as the frequency spectrum of the signal. The magnitude of the resulting complex-valued function at frequency represents the amplitude of a frequency component whose initial phase is given by the phase of .\n\nFourier transforms are not limited to functions of time, and temporal frequencies. They can equally be applied to analyze \"spatial\" frequencies, and indeed for nearly any function domain. This justifies their use in such diverse branches as image processing, heat conduction, and automatic control.\n\n"}
{"id": "349223", "url": "https://en.wikipedia.org/wiki?curid=349223", "title": "Group ring", "text": "Group ring\n\nIn algebra, a group ring is a free module and at the same time a ring, constructed in a natural way from any given ring and any given group. As a free module, its ring of scalars is the given ring, and its basis is one-to-one with the given group. As a ring, its addition law is that of the free module and its multiplication extends \"by linearity\" the given group law on the basis. Less formally, a group ring is a generalization of a given group, by attaching to each element of the group a \"weighting factor\" from a given ring.\n\nA group ring is also referred to as a group algebra, for it is indeed an algebra over the given ring. A group algebra over a field has a further structure of Hopf algebra; in this case, it is thus called a group Hopf algebra.\n\nThe apparatus of group rings is especially useful in the theory of group representations.\n\nLet \"G\" be a group, written multiplicatively, and let \"R\" be a ring. The group ring of \"G\" over \"R\", which we will denote by \"R\"[\"G\"] (or simply \"RG\"), is the set of mappings of finite support, where the module scalar product \"αf\" of a scalar \"α\" in \"R\" and a vector (or mapping) \"f\" is defined as the vector formula_1, and the module group sum of two vectors \"f\" and \"g\" is defined as the vector formula_2. To turn the additive group \"R\"[\"G\"] into a ring, we define the product of \"f\" and \"g\" to be the vector\nThe summation is legitimate because \"f\" and \"g\" are of finite support, and the ring axioms are readily verified.\n\nSome variations in the notation and terminology are in use. In particular, the mappings such as are sometimes written as what are called \"formal linear combinations of elements of \"G\", with coefficients in \"R\"\":\nor simply\nwhere this doesn't cause confusion.\n\n1. Let , the cyclic group of order 3, with generator \"a\" and identity element 1. An element \"r\" of C[\"G\"] may be written as\n\nwhere \"z\", \"z\" and \"z\" are in C, the complex numbers. Writing a different element \"s\" as\n\ntheir sum is\n\nand their product is\n\nNotice that the identity element 1 of \"G\" induces a canonical embedding of the coefficient ring (in this case C) into C[\"G\"]; however strictly speaking the multiplicative identity element of C[\"G\"] is 1⋅1 where the first \"1\" comes from C and the second from \"G\". The additive identity element is zero.\n\nWhen \"G\" is a non-commutative group, one must be careful to preserve the order of the group elements (and not accidentally commute them) when multiplying the terms.\n\n2. A different example is that of the Laurent polynomials over a ring \"R\": these are nothing more or less than the group ring of the infinite cyclic group Z over \"R\".\n\n3. Let \"Q\" be the quaternion group with elements formula_10. Consider the group ring R\"Q\", where R is the set of real numbers. An arbitrary element of this group ring is of the form\n\nwhere formula_12 is a real number.\n\nMultiplication, like in any other group ring, is defined based on the group operation. For example,\n\nNote that RQ\" is not the same as the Hamilton quaternions over R. This is because the Hamilton quaternions satisfy additional relations in the ring, such as formula_14, whereas in the group ring RQ\", formula_15 is not equal to formula_16. To be more specific, R\"Q\" has dimension 8 as a real vector space, while the Hamilton quaternions have dimension 4 as a real vector space.\n\nAssuming that the ring \"R\" has a unit element 1, and denoting the group unit by 1, the ring \"R\"[\"G\"] contains a subring isomorphic to \"R\", and its group of invertible elements contains a subgroup isomorphic to \"G\". For considering the indicator function of {1}, which is the vector \"f\" defined by\nthe set of all scalar multiples of \"f\" is a subring of \"R\"[\"G\"] isomorphic to \"R\". And if we map each element \"s\" of \"G\" to the indicator function of {\"s\"}, which is the vector \"f\" defined by\nthe resulting mapping is an injective group homomorphism (with respect to multiplication, not addition, in \"R\"[\"G\"]).\n\nIf \"R\" and \"G\" are both commutative (i.e., \"R\" is commutative and \"G\" is an abelian group), \"R\"[\"G\"] is commutative.\n\nIf \"H\" is a subgroup of \"G\", then \"R\"[\"H\"] is a subring of \"R\"[\"G\"]. Similarly, if \"S\" is a subring of \"R\", \"S\"[\"G\"] is a subring of \"R\"[\"G\"].\n\nGroup algebras occur naturally in the theory of group representations of finite groups. The group algebra \"K\"[\"G\"] over a field \"K\" is essentially the group ring, with the field \"K\" taking the place of the ring. As a set and vector space, it is the free vector space on \"G\" over the field \"K\". That is, for \"x\" in \"K\"[\"G\"],\n\nThe algebra structure on the vector space is defined using the multiplication in the group:\nwhere on the left, \"g\" and \"h\" indicate elements of the group algebra, while the multiplication on the right is the group operation (denoted by juxtaposition).\n\nBecause the above multiplication can be confusing, one can also write the basis vectors of \"K\"[\"G\"] as \"e\" (instead of \"g\"), in which case the multiplication is written as:\n\nThinking of the free vector space as \"K\"-valued functions on \"G\", the algebra multiplication is convolution of functions.\n\nWhile the group algebra of a \"finite\" group can be identified with the space of functions on the group, for an infinite group these are different. The group algebra, consisting of \"finite\" sums, corresponds to functions on the group that vanish for cofinitely many points; topologically (using the discrete topology), these correspond to functions with compact support.\n\nHowever, the group algebra \"K\"[\"G\"] and the space of functions are dual: given an element of the group algebra\n\nand a function on the group these pair to give an element of \"K\" via\n\nwhich is a well-defined sum because it is finite.\n\nThe group algebra is an algebra over itself; under the correspondence of representations over \"R\" and \"R\"[\"G\"] modules, it is the regular representation of the group.\n\nWritten as a representation, it is the representation \"g\" \"ρ\" with the action given by formula_24, or\n\nThe dimension of the vector space \"K\"[\"G\"] is just equal to the number of elements in the group. The field \"K\" is commonly taken to be the complex numbers C or the reals R, so that one discusses the group algebras C[\"G\"] or R[\"G\"].\n\nThe group algebra C[\"G\"] of a finite group over the complex numbers is a semisimple ring. This result, Maschke's theorem, allows us to understand C[\"G\"] as a finite product of matrix rings with entries in C.\n\nTaking \"K\"[\"G\"] to be an abstract algebra, one may ask for concrete representations of the algebra over a vector space \"V\". Such a representation\n\nis an algebra homomorphism from the group algebra to the set of endomorphisms on \"V\". Taking \"V\" to be an abelian group, with group addition given by vector addition, such a representation is in fact a left \"K\"[\"G\"]-module over the abelian group \"V\". This is demonstrated below, where each axiom of a module is confirmed.\n\nPick so that \nThen formula_28 is a homomorphism of abelian groups, in that\n\nfor any . Next, one notes that the set of endomorphisms of an abelian group is an endomorphism ring. The representation formula_30 is a ring homomorphism, in that one has\n\nfor any two and . Similarly, under multiplication,\n\nFinally, one has that the unit is mapped to the identity:\n\nwhere 1 is the multiplicative unit of \"K\"[\"G\"]; that is,\n\nis the vector corresponding to the identity element \"e\" in \"G\".\n\nThe last three equations show that formula_30 is a ring homomorphism from \"K\"[\"G\"] taken as a group ring, to the endomorphism ring. The first identity showed that individual elements are group homomorphisms. Thus, a representation formula_30 is a left \"K\"[\"G\"]-module over the abelian group \"V\".\n\nNote that given a general \"K\"[\"G\"]-module, a vector-space structure is induced on \"V\", in that one has an additional axiom\n\nfor scalar .\n\nAny group representation\n\nwith \"V\" a vector space over the field \"K\", can be extended to an algebra representation\n\nsimply by letting formula_40 and extending linearly. Thus, representations of the group correspond exactly to representations of the algebra, and so, in a certain sense, talking about the one is the same as talking about the other.\n\nThe center of the group algebra is the set of elements that commute with all elements of the group algebra:\n\nThe center is equal to the set of class functions, that is the set of elements that are constant on each conjugacy class\n\nIf , the set of irreducible characters of \"G\" forms an orthonormal basis of Z(\"K\"[\"G\"]) with respect to the inner product\n\nMuch less is known in the case where \"G\" is countably infinite, or uncountable, and this is an area of active research. The case where \"R\" is the field of complex numbers is probably the one best studied. In this case, Irving Kaplansky proved that if \"a\" and \"b\" are elements of C[\"G\"] with , then . Whether this is true if \"R\" is a field of positive characteristic remains unknown.\n\nA long-standing conjecture of Kaplansky (~1940) says that if \"G\" is a torsion-free group, and \"K\" is a field, then the group ring \"K\"[\"G\"] has no non-trivial zero divisors. This conjecture is equivalent to \"K\"[\"G\"] having no non-trivial nilpotents under the same hypotheses for \"K\" and \"G\".\n\nIn fact, the condition that \"K\" is a field can be relaxed to any ring that can be embedded into an integral domain.\n\nThe conjecture remains open in full generality, however some special cases of torsion-free groups have been shown to satisfy the zero divisor conjecture. These include:\n\n\nThe case of \"G\" being a topological group is discussed in greater detail in the article on group algebras.\n\nA module \"M\" over \"R\"[\"G\"] is then the same as a linear representation of \"G\" over the field \"R\". There is no particular reason to limit \"R\" to be a field here. However, the classical results were obtained first when \"R\" is the complex number field and \"G\" is a finite group, so this case deserves close attention. It was shown that \"R\"[\"G\"] is a semisimple ring, under those conditions, with profound implications for the representations of finite groups. More generally, whenever the characteristic of the field \"R\" does not divide the order of the finite group \"G\", then \"R\"[\"G\"] is semisimple (Maschke's theorem).\n\nWhen \"G\" is a finite abelian group, the group ring is commutative, and its structure is easy to express in terms of roots of unity. When \"R\" is a field of characteristic \"p\", and the prime number \"p\" divides the order of the finite group \"G\", then the group ring is \"not\" semisimple: it has a non-zero Jacobson radical, and this gives the corresponding subject of modular representation theory its own, deeper character.\n\nCategorically, the group ring construction is left adjoint to \"group of units\"; the following functors are an adjoint pair:\nwhere formula_46 takes a group to its group ring over \"R\", and formula_47 takes an \"R\"-algebra to its group of units.\n\nWhen , this gives an adjunction between the category of groups and the category of rings, and the unit of the adjunction takes a group \"G\" to a group that contains trivial units: In general, group rings contain nontrivial units. If \"G\" contains elements \"a\" and \"b\" such that formula_48 and \"b\" does not normalize formula_49 then the square of\n\nis zero, hence formula_51. The element is a unit of infinite order.\n\nThe above adjunction expresses a universal property of group rings. Let be a (commutative) ring, let be a group, and let be an -algebra. For any group homomorphism formula_52, there exists a unique -algebra homomorphism formula_53 such that formula_54 where is the inclusion \n\nIn other words, formula_56 is the unique homomorphism making the following diagram commute:\n\nAny other ring satisfying this property is canonically isomorphic to the group ring. \n\nThe group algebra generalizes to the monoid ring and thence to the category algebra, of which another example is the incidence algebra.\n\nIf a group has a length function – for example, if there is a choice of generators and one takes the word metric, as in Coxeter groups – then the group ring becomes a filtered algebra.\n\n\n\n\n"}
{"id": "35228504", "url": "https://en.wikipedia.org/wiki?curid=35228504", "title": "Gustavo Sannia", "text": "Gustavo Sannia\n\nGustavo Sannia (Napoli, 13 May 1875 – 21 December 1930) was an Italian mathematician working in differential geometry, projective geometry, and summation of series. He was the son of Achille Sannia, mathematician and senator of the Kingdom of Italy.\n\n\n\n"}
{"id": "1773852", "url": "https://en.wikipedia.org/wiki?curid=1773852", "title": "Gutmann method", "text": "Gutmann method\n\nThe Gutmann method is an algorithm for securely erasing the contents of computer hard disk drives, such as files. Devised by Peter Gutmann and Colin Plumb and presented in the paper \"Secure Deletion of Data from Magnetic and Solid-State Memory\" in July 1996, it involved writing a series of 35 patterns over the region to be erased.\n\nThe selection of patterns assumes that the user does not know the encoding mechanism used by the drive, so it includes patterns designed specifically for three types of drives. A user who knows which type of encoding the drive uses can choose only those patterns intended for their drive. A drive with a different encoding mechanism would need different patterns.\n\nMost of the patterns in the Gutmann method were designed for older MFM/RLL encoded disks. Gutmann himself has noted that more modern drives no longer use these older encoding techniques, making parts of the method irrelevant. He said \"In the time since this paper was published, some people have treated the 35-pass overwrite technique described in it more as a kind of voodoo incantation to banish evil spirits than the result of a technical analysis of drive encoding techniques\".\n\nSince about 2001, some ATA IDE and SATA hard drive manufacturer designs include support for the ATA Secure Erase standard, obviating the need to apply the Gutmann method when erasing an entire drive. However, a 2011 research found that 4 out of 8 manufacturers did not implement ATA Secure Erase correctly.\n\nOne standard way to recover data that has been overwritten on a hard drive is to capture and process the analog signal obtained from the drive's read/write head prior to this analog signal being digitized. This analog signal will be close to an ideal digital signal, but the differences will reveal important information. By calculating the ideal digital signal and then subtracting it from the actual analog signal, it is possible to amplify the obtained difference signal and use it to determine what had previously been written on the disk.\n\nFor example:\n\nThis can then be done again to see the previous data written:\n\nHowever, even when overwriting the disk repeatedly with random data it is theoretically possible to recover the previous signal. The permittivity of a medium changes with the frequency of the magnetic field. This means that a lower frequency field will penetrate deeper into the magnetic material on the drive than a high frequency one. So a low frequency signal will, in theory, still be detectable even after it has been overwritten hundreds of times by a high frequency signal.\n\nThe patterns used are designed to apply alternating magnetic fields of various frequencies and various phases to the drive surface and thereby approximate degaussing the material below the surface of the drive.\n\nAn overwrite session consists of a lead-in of four random write patterns, followed by patterns 5 to 31 (see rows of table below), executed in a random order, and a lead-out of four more random patterns.\n\nEach of patterns 5 to 31 was designed with a specific magnetic media encoding scheme in mind, which each pattern targets. The drive is written to for all the passes even though the table below only shows the bit patterns for the passes that are specifically targeted at each encoding scheme. The end result should obscure any data on the drive so that only the most advanced physical scanning (e.g., using a magnetic force microscope) of the drive is likely to be able to recover any data. \n\nThe series of patterns is as follows:\n\nEncoded bits shown in bold are what should be present in the ideal pattern, although due to the encoding the complementary bit is actually present at the start of the track.\n\nThe delete function in most operating systems simply marks the space occupied by the file as reusable (removes the pointer to the file) without immediately removing any of its contents. At this point the file can be fairly easily recovered by numerous recovery applications. However, once the space is overwritten with other data, there is no known way to use software to recover it. It cannot be done with software alone since the storage device only returns its current contents via its normal interface. Gutmann claims that intelligence agencies have sophisticated tools, including magnetic force microscopes, which together with image analysis, can detect the previous values of bits on the affected area of the media (for example hard disk).\n\nDaniel Feenberg of the National Bureau of Economic Research, an American private nonprofit research organization, criticized Gutmann's claim that intelligence agencies are likely to be able to read overwritten data, citing a lack of evidence for such claims. Nevertheless, some published government security procedures consider a disk overwritten once to still be sensitive.\n\nGutmann himself has responded to some of these criticisms and also criticized how his algorithm has been abused in an epilogue to his original paper, in which he states:\n\n\n"}
{"id": "7058047", "url": "https://en.wikipedia.org/wiki?curid=7058047", "title": "History of Lorentz transformations", "text": "History of Lorentz transformations\n\nThe history of Lorentz transformations comprises the development of linear transformations forming the Lorentz group or Poincaré group preserving the Lorentz interval formula_1 and the Minkowski inner product formula_2.\n\nIn mathematics, transformations equivalent to what was later known as Lorentz transformations in various dimensions were discussed in the 19th century in relation to the theory of quadratic forms, hyperbolic geometry, Möbius geometry, and sphere geometry, which is connected to the fact that the group of motions in hyperbolic space, the Möbius group or projective special linear group, and the Laguerre group are isomorphic to the Lorentz group.\n\nIn physics, Lorentz transformations became known at the beginning of the 20th century, when it was discovered that they exhibit the symmetry of Maxwell's equations. Subsequently, they became fundamental to all of physics, because they formed the basis of special relativity in which they exhibit the symmetry of Minkowski spacetime, making the velocity of light invariant between different inertial frames. They relate the spacetime coordinates, which specify the position \"x,y,z\" and time \"t\" of an event, relative to a particular inertial frame of reference (the \"rest system\"), and the coordinates \"x′,y′,z′\" and \"t′\" of the same event relative to another coordinate system moving in the positive \"x\"-direction at a constant speed \"v\", relative to the rest system.\n\nThe general quadratic form \"q(x)\" with coefficients of a symmetric matrix A, the associated bilinear form \"b(x,y)\", and the linear transformations of \"q(x)\" and \"b(x,y)\" into \"q(x′)\" and \"b(x′,y′)\" using the transformation matrix g, can be written as\n\nThe case \"n=1\" is the binary quadratic form introduced by Lagrange (1773) and Gauss (1798/1801), \"n=2\" is the ternary quadratic form introduced by Gauss (1798/1801), \"n=3\" is the quaternary quadratic form etc.\n\nThe general Lorentz transformation follows from () by setting A=A′=diag(-1,1...,1) and det g=1. It forms an indefinite orthogonal group called the Lorentz group , the quadratic form \"q(x)\" becomes the Lorentz interval in terms of an indefinite quadratic form in terms of pseudo-Euclidean space, and the associated bilinear form \"b(x)\" becomes the Minkowski inner product:\n\nSuch general Lorentz transformations () for various dimensions were used by Gauss (1818), Jacobi (1827, 1833), Lebesgue (1837), Bour (1856), Somov (1863), Hill (1882) in order to simplify computations of elliptic functions and integrals. They were also used by Poincaré (1881), Cox (1881/82), Picard (1882, 1884), Killing (1885, 1893), Gérard (1892), Hausdorff (1899), Woods (1901, 1903), Liebmann (1904/05) to describe hyperbolic motions (i.e. rigid motions in the hyperbolic plane or hyperbolic space), which were expressed in terms of Weierstrass coordinates of the hyperboloid model satisfying the relation formula_3 or in terms of the Cayley–Klein metric of projective geometry using the \"absolute\" form formula_4. In addition, infinitesimal transformations related to the Lie algebra of the group of hyperbolic motions were given in terms of Weierstrass coordinates formula_3 by Killing (1888-1897).\n\nIf \"x, x′\" in () are interpreted as homogeneous coordinates, then the corresponding inhomogenous coordinates \"u, u′\" follow by\n\nso that the Lorentz transformation becomes a homography leaving invariant the equation of the unit sphere, which John Lighton Synge called “the most general formula for the composition of velocities” in terms of special relativity (the transformation matrix g stays the same as in ()):\n\n\\hline -x_{0}^{2}+\\cdots+x_{n}^{2}=-x_{0}^{\\prime2}+\\dots+x_{n}^{\\prime2}=0 & \\rightarrow & -1+u_{1}^{2}+\\cdots+u_{n}^{2}=-1+u_{1}^{\\prime2}+\\cdots+u_{n}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline \\begin{align}u_{s} & =\\frac{g_{s0}+\\sum_{j=1}^{n}g_{sj}u_{j}^{\\prime}}{g_{00}+\\sum_{j=1}^{n}g_{0j}u_{j}^{\\prime}}\\\\\nu_{s}^{\\prime} & =\\frac{g_{s0}^{(-1)}+\\sum_{j=1}^{n}g_{sj}^{(-1)}u_{j}}{g_{00}^{(-1)}+\\sum_{j=1}^{n}g_{0j}^{(-1)}u_{j}}\\\\\n\\left|\\begin{align}\\sum_{i=1}^{n}g_{ij}g_{ik}-g_{0j}g_{0k} & =\\left\\{ \\begin{align}-1\\quad & (j=k=0)\\\\\n1\\quad & (j=k>0)\\\\\n0\\quad & (j\\ne k)\n\\right.\\\\\n\\sum_{j=1}^{n}g_{ij}g_{kj}-g_{i0}g_{k0} & =\\left\\{ \\begin{align}-1\\quad & (i=k=0)\\\\\n1\\quad & (i=k>0)\\\\\n0\\quad & (i\\ne k)\n\\right.\n\\right.\n\nSuch Lorentz transformations for various dimensions were directly used by Gauss (1818), Jacobi (1827–1833), Lebesgue (1837), Bour (1856), Somov (1863), Hill (1882), Callandreau (1885) in order to simplify computations of elliptic functions and integrals, by Picard (1882-1884) in relation to Hermitian quadratic forms, or by Woods (1901, 1903) in terms of the Beltrami–Klein model of hyperbolic geometry. In addition, infinitesimal transformations related to the Lie algebra of the group of hyperbolic motions leaving invariant the unit sphere formula_7 were given by Lie (1885-1893) and Werner (1889) and Killing (1888-1897).\n\nParticular forms of Lorentz transformations or relativistic velocity additions, mostly restricted to 2, 3 or 4 dimensions, have been formulated by many authors using:\n\n\nBy using the imaginary quantities formula_8 in x as well as formula_9 \"(s=1,2...n)\" in g, the Lorentz transformation () assumes the form of an orthogonal transformation, the Lorentz interval becomes the Euclidean norm, and the Minkowski inner product becomes the dot product:\n\nThe cases \"n=1,2,3,4\" of this quadratic form with real numbers and its transformation was discussed by Euler (1771) and in \"n\" dimensions by Cauchy (1829). Its interpretation as leaving invariant the equation of the sphere with imaginary radius was given by Lie (1871), its interpretation as a Lorentz transformation with \"n=3\" using one imaginary coordinate was given by Minkowski (1907) and Sommerfeld (1909).\n\nA well known example of an orthogonal transformation is spatial rotation:\n\nThis quadratic form and its transformation with real numbers and real angle was discussed by Euler (1771), its interpretation as a Lorentz transformation using one imaginary coordinate and imaginary angle was given by Minkowski (1907) and Sommerfeld (1909).\n\nThe case of a Lorentz transformation without spatial rotation is called a Lorentz boost. The simplest case can be given, for instance, by setting \"n=1\" in ():\n\nwhich resembles precisely the relations of hyperbolic functions by setting \"g=g\"=cosh(η) and \"g=g\"=sinh(η), with η as the hyperbolic angle. Thus by adding an unchanged \"x\"-axis, a Lorentz boost for \"n=2\" representing a translation in the hyperbolic plane in terms of Weierstrass coordinates of the hyperboloid model along one axis (being the same as a rotation around an imaginary angle \"iη=φ\" in ()) is given by\n\n\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}} & =\\sinh\\eta & (e)\\\\\n\\frac{\\tanh\\eta+\\tanh\\zeta}{1+\\tanh\\eta\\tanh\\zeta} & =\\tanh\\left(\\eta+\\zeta\\right) & (f)\n}\\right.\n\nwhich can also be expressed as squeeze mappings in terms of exponential functions:\n\nAll hyperbolic relations (a,b,c,d,e,f) on the right of () were given by Lambert (1768–1770). The Lorentz transformations (, see ) were given by Cox (1882), Lindemann (1890/91), Gérard (1892), Killing (1893, 1897/98), Whitehead (1897/98), Woods (1903/05) and Liebmann (1904/05). Lorentz transformations (, 1) were given by Lindemann (1890/91) and Herglotz (1909), while formulas equivalent to (, 2) by Klein (1871).\n\nIn line with equation () one can use coordinates formula_10, which in terms of hyperbolic geometry can be interpreted as changing the above Weierstrass coordinates into Beltrami coordinates inside the unit circle formula_11, thus the corresponding Lorentz transformations () obtain the form:\n\n\\hline -x_{0}^{2}+x_{1}^{2}+x_{2}^{2}=-x_{0}^{\\prime2}+x_{1}^{\\prime2}+x_{2}^{\\prime2}=0 & \\rightarrow & -1+u_{x}^{2}+u_{y}^{2}=-1+u_{x}^{\\prime2}+u_{y}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline {\\scriptstyle \\begin{align}\\frac{\\sinh\\eta}{\\cosh\\eta} & =\\tanh\\eta=v\\\\\n\\cosh\\eta & =\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\\\\nu_{1} & =\\tanh\\zeta_{1}\\\\\nu_{2} & =\\tanh\\zeta_{2}\\\\\nu_{1}^{\\prime} & =\\tanh\\zeta_{1}^{\\prime}\\\\\n}\\left|\\begin{align}u_{1} & =\\frac{\\sinh\\eta+u_{1}^{\\prime}\\cosh\\eta}{\\cosh\\eta+u_{1}^{\\prime}\\sinh\\eta} & & =\\frac{\\tanh\\zeta_{1}^{\\prime}+\\tanh\\eta}{1+\\tanh\\zeta_{1}^{\\prime}\\tanh\\eta} & & =\\frac{u_{1}^{\\prime}+v}{1+u_{1}^{\\prime}v}\\\\\nu_{2} & =\\frac{u_{2}^{\\prime}}{\\cosh\\eta+u_{1}^{\\prime}\\sinh\\eta} & & =\\frac{\\tanh\\zeta_{2}^{\\prime}\\sqrt{1-\\tanh^{2}\\eta}}{1+\\tanh\\zeta_{1}^{\\prime}\\tanh\\eta} & & =\\frac{u_{2}^{\\prime}\\sqrt{1-v^{2}}}{1+u_{1}^{\\prime}v}\\\\\nu_{1}^{\\prime} & =\\frac{-\\sinh\\eta+u_{1}\\cosh\\eta}{\\cosh\\eta-u_{1}\\sinh\\eta} & & =\\frac{\\tanh\\zeta{}_{1}-\\tanh\\eta}{1-\\tanh\\zeta{}_{1}\\tanh\\eta} & & =\\frac{u_{1}-v}{1-u_{1}v}\\\\\n\\right.\n\nThese Lorentz transformations were given by Escherich (1874) and Killing (1898) (on the left), as well as Beltrami (1868) or Schur (1885/86, 1900/02) (on the right). By using the scalar product of \"[u, u]\", the resulting Lorentz transformation can be seen as equivalent to the hyperbolic law of cosines:\n\nu^{\\prime2}=u_{1}^{\\prime2}+u_{2}^{\\prime2},\\ \\tan\\alpha'=\\frac{u_{2}^{\\prime}}{u_{1}^{\\prime}}\\\\\n\\downarrow\\\\\nu'=\\frac{\\sqrt{-v^{2}-u^{2}+2vu\\cos\\alpha+\\left(vu\\sin\\alpha\\right){}^{2}}}{1-vu\\cos\\alpha},\\quad u=\\frac{\\sqrt{v^{2}+u^{\\prime2}+2vu'\\cos\\alpha'-\\left(vu'\\sin\\alpha'\\right){}^{2}}}{1+vu'\\cos\\alpha'}\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-u^{\\prime2}}}=\\frac{1}{\\sqrt{1-v^{2}}}\\frac{1}{\\sqrt{1-u^{2}}}-\\frac{v}{\\sqrt{1-v^{2}}}\\frac{u}{\\sqrt{1-u^{2}}}\\cos\\alpha & (b)\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\tanh^{2}\\xi}}=\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{1}{\\sqrt{1-\\tanh^{2}\\zeta}}-\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{\\tanh\\zeta}{\\sqrt{1-\\tanh^{2}\\zeta}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\cosh\\xi=\\cosh\\eta\\cosh\\zeta-\\sinh\\eta\\sinh\\zeta\\cos\\alpha & (a)\n\nThe hyperbolic law of cosines (a) was given by Taurinus (1826) and Lobachevsky (1829/30) and others, while variant (b) was given by Schur (1900/02).\n\nIn the theory of relativity, Lorentz transformations exhibit the symmetry of Minkowski spacetime by using a constant \"c\" as the speed of light, and a parameter \"v\" as the relative velocity between two inertial reference frames. In particular, the hyperbolic angle η in () can be interpreted as the velocity related rapidity η=atanh(β) with β=\"v/c\", so that γ=cosh(η) is the Lorentz factor, βγ=sinh(η) the proper velocity, \"v=c\"·tanh(η) the relative velocity of two inertial frames, \"u′=c\"·tanh(ζ) the velocity of another object, \"u=c\"·tanh(η+ζ) the velocity-addition formula, thus () becomes:\n\n} & =\\gamma & (d)\\\\\n\\frac{\\beta}{\\sqrt{1-\\beta^{2}}} & =\\beta\\gamma & (e)\\\\\n\\frac{u'+v}{1+\\frac{u'v}{c^{2}}} & =u & (f)\n}\\right.\n\nOr in four dimensions and by setting \"x=ct, x=x, x=y\" and adding an unchanged \"z\" the familiar form follows\n\n\\right)\\\\\nx & =\\gamma(x'+vt')\\\\\ny & =y'\\\\\nz & =z'\n\\right|\\begin{align}t' & =\\gamma\\left(t-x\\frac{v}{c^{2}}\\right)\\\\\nx' & =\\gamma(x-vt)\\\\\ny' & =y\\\\\nz' & =z\n\nSimilar transformations were introduced by Voigt (1887) and by Lorentz (1892, 1895) who analyzed Maxwell's equations, they were completed by Larmor (1897, 1900) and Lorentz (1899, 1904), and brought into their modern form by Poincaré (1905) who gave the transformation the name of Lorentz. Eventually, Einstein (1905) showed in his development of special relativity that the transformations follow from the principle of relativity and constant light speed alone by modifying the traditional concepts of space and time, without requiring a mechanical aether in contradistinction to Lorentz and Poincaré. Minkowski (1907–1908) used them to argue that space and time are inseparably connected as spacetime. Minkowski (1907–1908) and Varićak (1910) showed the relation to imaginary and hyperbolic functions. Important contributions to the mathematical understanding of the Lorentz transformation were also made by other authors such as Herglotz (1909/10), Ignatowski (1910), Noether (1910) and Klein (1910), Borel (1913–14).\n\nIn line with equation (), one can substitute formula_12 in () or (), producing the Lorentz transformation of velocities (or velocity addition formula) in analogy to Beltrami coordinates of ():\n\n\\hline -x_{0}^{2}+x_{1}^{2}+x_{2}^{2}=-x_{0}^{\\prime2}+x_{1}^{\\prime2}+x_{2}^{\\prime2}=0 & \\rightarrow & -c^{2}+u_{x}^{2}+u_{y}^{2}=-c^{2}+u_{x}^{\\prime2}+u_{y}^{\\prime2}=0\n\\end{matrix}\\\\\n\\hline {\\scriptstyle \\begin{align}\\frac{\\sinh\\eta}{\\cosh\\eta} & =\\tanh\\eta=\\frac{v}{c}\\\\\n\\cosh\\eta & =\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\\\\nu_{x} & =c\\tanh\\zeta_{x}\\\\\nu_{y} & =c\\tanh\\zeta_{y}\\\\\nu_{x}^{\\prime} & =c\\tanh\\zeta_{x}^{\\prime}\\\\\n}\\left|\\begin{align}u_{x} & =\\frac{c^{2}\\sinh\\eta+u_{x}^{\\prime}c\\cosh\\eta}{c\\cosh\\eta+u_{x}^{\\prime}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta_{x}^{\\prime}+c\\tanh\\eta}{1+\\tanh\\zeta_{x}^{\\prime}\\tanh\\eta} & & =\\frac{u_{x}^{\\prime}+v}{1+\\frac{v}{c^{2}}u_{x}^{\\prime}}\\\\\nu_{y} & =\\frac{cy'}{c\\cosh\\eta+u_{x}^{\\prime}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta_{y}^{\\prime}\\sqrt{1-\\tanh^{2}\\eta}}{1+\\tanh\\zeta_{x}^{\\prime}\\tanh\\eta} & & =\\frac{u_{y}^{\\prime}\\sqrt{1-\\frac{v^{2}}{c^{2}}}}{1+\\frac{v}{c^{2}}u_{x}^{\\prime}}\\\\\nu_{x}^{\\prime} & =\\frac{-c^{2}\\sinh\\eta+u_{x}c\\cosh\\eta}{c\\cosh\\eta-u_{x}\\sinh\\eta} & & =\\frac{c\\tanh\\zeta{}_{x}-c\\tanh\\eta}{1-\\tanh\\zeta{}_{x}\\tanh\\eta} & & =\\frac{u_{x}-v}{1-\\frac{v}{c^{2}}u{}_{x}}\\\\\n\\right.\n\nor more generally expressed as hyperbolic law of cosines in terms of ():\n\nu'^{2}=u_{x}^{\\prime2}+u_{y}^{\\prime2},\\ \\tan\\alpha'=\\frac{u_{y}^{\\prime}}{u_{x}^{\\prime}}\\\\\n\\downarrow\\\\\nu'=\\frac{\\sqrt{-v^{2}-u^{2}+2vu\\cos\\alpha+\\left(\\frac{vu\\sin\\alpha}{c}\\right){}^{2}}}{1-\\frac{v}{c^{2}}u\\cos\\alpha},\\quad u=\\frac{\\sqrt{v^{2}+u^{\\prime2}+2vu'\\cos\\alpha'-\\left(\\frac{vu'\\sin\\alpha'}{c}\\right){}^{2}}}{1+\\frac{v}{c^{2}}u'\\cos\\alpha'}\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\frac{u^{\\prime2}}{c^{2}}}}=\\frac{1}{\\sqrt{1-\\frac{v^{2}}{c^{2}}}}\\frac{1}{\\sqrt{1-\\frac{u^{2}}{c^{2}}}}-\\frac{v/c}{\\sqrt{1-\\frac{v^{2}}{c^{2}}}}\\frac{u/c}{\\sqrt{1-\\frac{u^{2}}{c^{2}}}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\frac{1}{\\sqrt{1-\\tanh^{2}\\xi}}=\\frac{1}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{1}{\\sqrt{1-\\tanh^{2}\\zeta}}-\\frac{\\tanh\\eta}{\\sqrt{1-\\tanh^{2}\\eta}}\\frac{\\tanh\\zeta}{\\sqrt{1-\\tanh^{2}\\zeta}}\\cos\\alpha\\\\\n\\downarrow\\\\\n\\cosh\\xi=\\cosh\\eta\\cosh\\zeta-\\sinh\\eta\\sinh\\zeta\\cos\\alpha\n\nThe velocity addition formula was given by Einstein (1905), while the relations to trigonometric and hyperbolic functions were given by Sommerfeld (1909) and Varićak (1910).\n\nAlso Lorentz boosts for arbitrary directions in line with () can be given as:\n\nor in vector notation\n\nSuch transformations were formulated by Herglotz (1911) and Silberstein (1911) and others.\n\nA general sphere transformation preserving the quadratic form formula_14 is the group Con(p,1) of spacetime conformal transformations in terms of inversions or special conformal transformations, which has the property of changing spheres into spheres. One can switch between the representations by using an imaginary radius coordinate \"x=iR\" which gives formula_15 (conformal transformation), or by using a real radius coordinate \"x=R\" which gives formula_14 (spherical wave transformation). This group was studied by Lie (1871) and others in terms of contact transformations, in which \"x\" is related to the radius \"R\".\n\nIt turns out that Con(3,1) is isomorphic to the special orthogonal group SO(4,2), and contains the Lorentz group SO(3,1) as a subgroup by setting λ=1. More generally, Con(p,q) is isomorphic to SO(p+1,q+1) and contains SO(p,q) as subgroup. This implies that Con(p,0) is isomorphic to the Lorentz group of arbitrary dimensions SO(p+1,1). Consequently, the conformal group in the plane Con(2,0) – known as the group of Möbius transformations – is isomorphic to the Lorentz group SO(3,1). This can be seen using tetracyclical coordinates satisfying the form formula_17, which were discussed by Pockels (1891), Klein (1893), Bôcher (1894).\n\nA special case of Lie's geometry of oriented spheres is the Laguerre group, transforming oriented planes and lines into each other. It's generated by the Laguerre inversion introduced by Laguerre (1882) and discussed by Darboux (1887) leaving invariant \"x+y+z-R\" with \"R\" as radius, thus the Laguerre group is isomorphic to the Lorentz group\n\nStephanos (1883) argued that Lie's geometry of oriented spheres in terms of contact transformations, as well as the special case of the transformations of oriented planes into each other (such as by Laguerre), provides a geometrical interpretation of Hamilton's biquaternions.\n\nThe relation between Lie's sphere transformations and the Lorentz transformation was noted by Bateman & Cunningham (1909–1910) and others. Furthermore, the group isomorphism between the Laguerre group and Lorentz group was pointed out by Bateman (1910), Cartan (1912, 1915/55), Poincaré (1912/21) and others.\n\nThe general transformation () of any quadratic form into itself can also be given using \"arbitrary\" parameters based on the Cayley transform (I-T)·(I+T) as follows:\n\n\\cdot\\mathbf{A}\\cdot\\mathbf{x}=q'=\\mathbf{x}^{\\mathrm{\\prime T}}\\cdot\\mathbf{A}\\cdot\\mathbf{x}'\\\\\n\\hline \\\\\n\\mathbf{x}=(\\mathbf{I}-\\mathbf{T}\\cdot\\mathbf{A})^{-1}\\cdot(\\mathbf{I}+\\mathbf{T}\\cdot\\mathbf{A})\\cdot\\mathbf{x}'\\\\\n\\text{or}\\\\\n\\mathbf{x}=\\mathbf{A}^{-1}\\cdot(\\mathbf{A}-\\mathbf{T})\\cdot(\\mathbf{A}+\\mathbf{T})^{-1}\\cdot\\mathbf{A}\\cdot\\mathbf{x}'\n\nwhere A is, as above, a symmetric matrix defining the quadratic form (there is no primed A' because the coefficients are assumed to be the same on both sides), I the identity matrix, and T an arbitrary antisymmetric matrix. After Cayley (1846) introduced transformations related to sums of positive squares, Hermite (1853/54, 1854) derived transformations for arbitrary quadratic forms, whose result was reformulated in terms of matrices () by Cayley (1855a, 1855b). For instance, the choice A=diag(1,1,1) gives an orthogonal transformation which can be used to describe spatial rotations corresponding to the Euler-Rodrigues parameters \"[a,b,c,d]\" discovered by Euler (1771) and Rodrigues (1840), which can be interpreted as the coefficients of quaternions. Setting \"d=1\", the equations have the form:\n\nAlso the Lorentz interval and the Lorentz transformation can be produced by the Cayley–Hermite formalism. The Lorentz transformation in 2 dimensions follows from () by setting A=diag(-1,1):\n\n\\left[\\begin{matrix}1+a^{2} & -2a\\\\\n\\end{matrix}\\right|\\left\\{ \\mathbf{T}=\\begin{vmatrix}0 & a\\\\\n-a & 0\n\nor with A=diag(-1,1,1):\n\nor with A=diag(-1,1,1,1):\n\nEquations containing the Lorentz transformations (, , ) as special cases were given by Cayley (1855), while Lorentz transformation () was explicitly given by Bachmann (1869). In relativity, equations similar to (, ) were first employed by Borel (1913) to represent Lorentz transformations.\n\nAs described in equation (), the Lorentz interval is closely connected to the alternative form formula_18, which in terms of the Cayley–Hermite parameters is invariant under the transformation:\n\nThis transformation was given by Cayley (1884), even though he didn't relate it to the Lorentz interval but rather to formula_19. As shown in the next section in equation (), many authors (some before Cayley) expressed the invariance of formula_18 and its relation to the Lorentz interval by using the alternative Cayley–Klein parameters and Möbius transformations.\n\nThe previously mentioned Euler-Rodrigues parameter \"a,b,c,d\" (i.e. Cayley-Hermite parameter in equation () with \"d=1\") are closely related to Cayley–Klein parameter α,β,γ,δ introduced by Helmholtz (1866/67), Cayley (1879) and Klein (1884) to connect Möbius transformations formula_21 and rotations:\n\nthus () becomes:\n\nAlso the Lorentz transformation can be expressed with variants of the Cayley–Klein parameters: One relates these parameters to a spin-matrix D, the spin transformations of variables formula_23 (the overline denotes complex conjugate), and the Möbius transformation of formula_24. When defined in terms of isometries of hyperblic space (hyperbolic motions), the Hermitian matrix u associated with these Möbius transformations produces an invariant determinant formula_25 identical to the Lorentz interval. Therefore, these transformations were described by John Lighton Synge as being a \"factory for the mass production of Lorentz transformations\". It also turns out that the related spin group Spin(3, 1) or special linear group SL(2, C) acts as the double cover of the Lorentz group (one Lorentz transformation corresponds to two spin transformations of different sign), while the Möbius group Con(2, 0) or projective special linear group PSL(2, C) is isomorphic to both the Lorentz group and the group of isometries of hyperbolic space.\n\nIn four dimensions, these Lorentz transformations can be represented as follows:\n\n\\eta' & =\\gamma\\xi+\\delta\\eta\n\\right.\\\\\n\\hline \\left.\\begin{matrix}\\mathbf{u}=\\left(\\begin{matrix}X_{1} & X_{2}\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}\\bar{\\xi}\\xi & \\xi\\bar{\\eta}\\\\\n\\bar{\\xi}\\eta & \\bar{\\eta}\\eta\n\\end{matrix}\\right)=\\left(\\begin{matrix}x_{0}+x_{3} & x_{1}-ix_{2}\\\\\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\right|\\begin{matrix}\\mathbf{D}=\\left(\\begin{matrix}\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\\\\n\\hline \\mathbf{u}'=\\mathbf{D}\\cdot\\mathbf{u}\\cdot\\bar{\\mathbf{D}}^{\\mathrm{T}}=\\begin{align}X_{1}^{\\prime} & =X_{1}\\alpha\\bar{\\alpha}+X_{2}\\alpha\\bar{\\beta}+X_{3}\\bar{\\alpha}\\beta+X_{4}\\beta\\bar{\\beta}\\\\\nX_{2}^{\\prime} & =X_{1}\\bar{\\alpha}\\gamma+X_{2}\\bar{\\alpha}\\delta+X_{3}\\bar{\\beta}\\gamma+X_{4}\\bar{\\beta}\\delta\\\\\nX_{3}^{\\prime} & =X_{1}\\alpha\\bar{\\gamma}+X_{2}\\alpha\\bar{\\delta}+X_{3}\\beta\\bar{\\gamma}+X_{4}\\beta\\bar{\\delta}\\\\\n\\hline \\begin{align}X_{3}^{\\prime}X_{2}^{\\prime}-X_{1}^{\\prime}X_{4}^{\\prime} & =X_{3}X_{2}-X_{1}X_{4}=0\\\\\n\nor expressing u′ in terms of \"x...\" it follows:\n\nThe general transformation u′ in () was given by Cayley (1854). The Möbius transformation in () was used as a Lorentz transformation by Poincaré (1881) and Hausdorff (1899) for the special case \"x\"=-1 and the resulting Lorentz interval formula_26 (hyperboloid in three dimensions). The general relation between Möbius transformations and transformation u′ leaving invariant the generalized circle was pointed out by Poincaré (1883) in relation to Kleinian groups, and by Klein (1884) in relation to surfaces of second degree and the invariance of the unit sphere. The invariance of the complete Lorentz interval using u′ as Lorentz transformation was demonstrated by Klein (1889-1893), 1896/97), Bianchi (1893), Fricke (1893, 1897). Its reformulation as Lorentz transformation () was provided by Bianchi (1893) and Fricke (1893, 1897). In relativity, () was first employed by Herglotz (1909/10).\n\nIn the case of three dimensions (\"x\"=0, while \"x\" becomes the new \"x\", no complex conjugate) it simplifies to:\n\n\\eta' & =\\gamma\\xi+\\delta\\eta\n\\right.\\\\\n\\hline \\left.\\begin{matrix}\\mathbf{u}=\\left(\\begin{matrix}X_{1} & X_{2}\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}\\xi^{2} & \\xi\\eta\\\\\n\\end{matrix}\\right)=\\left(\\begin{matrix}x_{0}+x_{2} & x_{1}\\\\\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\right|\\begin{matrix}\\mathbf{D}=\\left(\\begin{matrix}\\alpha & \\beta\\\\\n\\gamma & \\delta\n\\end{matrix}\\right)\\\\\n\\end{matrix}\\\\\n\\hline \\mathbf{u}'=\\mathbf{D}\\cdot\\mathbf{u}\\cdot\\mathbf{D}^{\\mathrm{T}}=\\begin{align}X_{1}^{\\prime} & =X_{1}\\alpha^{2}+X_{2}2\\alpha\\beta+X_{3}\\beta^{2}\\\\\nX_{2}^{\\prime} & =X_{1}\\alpha\\gamma+X_{2}(\\alpha\\delta+\\beta\\gamma)+X_{3}\\beta\\delta\\\\\n\\hline \\begin{align}X_{2}^{\\prime2}-X_{1}^{\\prime}X_{3}^{\\prime} & =X_{2}^{2}-X_{1}X_{3}=0\\\\\n\nthus\n\nThe general transformation u′ and its invariant formula_18 in () was already used by Lagrange (1773) and Gauss (1798/1801) in the theory of integer binary quadratic forms. The invariant formula_18 was also studied by Klein (1871) in connection to hyperbolic plane geometry (see equation ()), while the connection between u′ and formula_18 with the Möbius transformation was analyzed by Poincaré (1886) in relation to Fuchsian groups. The adaptation to the Lorentz interval by which () becomes a Lorentz transformation was given by Bianchi (1888) and Fricke (1891). The Lorentz Transformation in the form of () was explicitly stated by Gauss around 1800 (posthumously published 1863), as well as Selling (1873), Bianchi (1888), Fricke (1891) in relation to integer indefinite ternary quadratic forms.\n\nThe Lorentz transformations can also be expressed in terms of biquaternions having one real part \"xe+xe+xe\" and one purely imaginary part \"ix\" (some authors use the opposite convention). Its general form (on the left) and the corresponding boost (on the right) are as follows (where the overline denotes Hamiltonian conjugation and * complex conjugation):\n\nCayley (1854, 1855) derived quaternion transformations leaving invariant the sum of squares formula_30. Cox (1882/83) discussed the Lorentz interval in terms of Weierstrass coordinates formula_31 in the course of adapting William Kingdon Clifford's biquaternions \"a+ωb\" to hyperbolic geometry (ω=-1 for hyperbolic geometry, ω=1 elliptic, ω=0 parabolic). Stephanos (1883) related the imaginary part of William Rowan Hamilton's biquaternions to the radius of spheres, and introduced a homography leaving invariant the equations of oriented spheres or oriented planes in terms of Lie sphere geometry. Buchheim (1884/85) discussed the Cayley absolute formula_32 and adapted Clifford's biquaternions to hyperbolic geometry similar to Cox by using all three values of ω. Eventually, the modern Lorentz transformation using biquaternions with ω=-1 as in hyperbolic geometry was given by Noether (1910), Klein (1910), Conway (1911), Silberstein (1911).\n\nOften connected with quaternionic systems is the hyperbolic number ε=1, which also allows to formulate the Lorentz transformations:\n\nAfter the trigonometric expression e=cos(x)+i·sin(x) (Euler's formula) was given by Euler (1748) and the hyperbolic analogue e by Cockle (1848) in the framework of tessarines, it was shown by Cox (1882/83) that one can identify ww′=e with associative quaternion multiplication. Here, e is the hyperbolic versor with ε=1, the elliptic one follows with ε=-1, and parabolic with ε=0 (this should not be confused with the expression ω in Clifford's biquaternions also used by Cox, in which ω=-1 is hyperbolic). The hyperbolic versor was also discussed by Macfarlane (1892, 1894, 1900) in terms of hyperbolic quaternions. The expression ε=1 for hyperbolic motions (and ε=-1 for elliptic, ε=0 for parabolic motions) also appear in \"biquaternions\" defined by Vahlen (1901/02, 1905).\n\nMore extended forms of complex and (bi-)quaternionic systems in terms of Clifford algebra can also be used to express the Lorentz transformations. For instance, using a system \"a\" of Clifford numbers one can transform the following general quadratic form into itself, in which the individual values of formula_33 can be set to +1 or -1 at will:\n\nThe Lorentz interval follows if the sign of one i differs from all others. The general definite form formula_35 as well as the general indefinite form formula_36 and their invariance under transformation (1) was discussed by Lipschitz (1885/86), while hyperbolic motions were discussed by Vahlen (1901/02, 1905) by setting ε=1 in transformation (2), while elliptic motions follow with ε=-1 and parabolic motions ε=0, all of which he also related to biquaternions.\n\nA summary of historical Lorentz boost formulas consistent with (, , , ) and velocity additions consistent with (, , , ).\n\nAfter Vincenzo Riccati introduced hyperbolic functions in 1757, Johann Heinrich Lambert (read 1767, published 1768) gave the following relations, in which \"tang \"φ or abbreviated \"tφ\" was equated by Lambert to the tangens hyperbolicus formula_37 of a variable \"u\", or in modern notation \"tφ=tanh(u)\":\n\nIn (1770) he rewrote the addition law for the hyperbolic tangens (f) or (g) as:\n\nLeonhard Euler (1771) demonstrated the invariance of quadratic forms in terms of sum of squares under a linear substitution and its coefficients, now known as orthogonal transformation, as well as under rotations using Euler angles. The case of two dimensions is given by\n\nor three dimensions\n\nThese coefficiens \"A,B,C,D,E,F,G,H,I\" were related by Euler to four arbitrary parameter \"p,q,r,s\", which where rediscovered by Olinde Rodrigues (1840) who related them to rotation angles now called Euler–Rodrigues parameters in line with equation ():\n\nThe orthogonal transformation in four dimensions was given by him as\n\nAfter the invariance of the sum of squares under linear substitutions was discussed by Euler (1771), the general expressions of a binary quadratic form and its transformation was formulated by Lagrange (1773) as follows\n\nwhich is equivalent to () \"(n=1)\". The theory of binary quadratic forms was considerably expanded by Carl Friedrich Gauss (1798, published 1801) in his Disquisitiones Arithmeticae. He rewrote Lagrange's formalism as follows using integer coefficients α,β,γ,δ:\n\nwhich is equivalent to () \"(n=1)\". As pointed out by Gauss, \"F\" and \"F' \" are called \"proper equivalent\" if αδ-βγ=1, so that \"F\" is contained in \"F' \" as well as \"F' \" is contained in \"F\". In addition, if another form \"F\"\" is contained by the same procedure in \"F' \" it is also contained in \"F\" and so forth.\n\nGauss (1798/1801) also discussed ternary quadratic forms with the general expression\n\nwhich is equivalent to () \"(n=2)\". Gauss called these forms definite when they have the same sign such as \"x+y+z\", or indefinite in the case of different signs such as \"x+y-z\". While discussing the classification of ternary quadratic forms formula_47, Gauss (1801) presented twenty special cases, among them these six variants:\n\nThe determination of all transformations of the Lorentz interval (as a special case of an integer ternary quadratic form) into itself was explicitly worked out by Gauss around 1800 (posthumously published in 1863), for which he provided a coefficient system α,β,γ,δ:\n\nGauss' result was cited by Bachmann (1869), Selling (1873), Bianchi (1888), Leonard Eugene Dickson (1923). The parameters α,β,γ,δ, when applied to spatial rotations, were later called Cayley–Klein parameters.\n\nGauss (1818) discussed planetary motions together with formulating elliptic functions. In order to simplify the integration, he transformed the expression\n\ninto\n\nin which cos \"T\", sin \"T\" are related to cos \"E\", sin \"E\" by the following transformation including an arbitrary constant \"k\", which Gauss then rewrote by setting \"k\"=1:\n\nSubsequently he showed that these relations can be reformulated using three variables \"x,y,z\" and \"u,u′,u″\", so that\n\ncan be transformed into\n\nin which \"x,y,z\" and \"u,u′,u″\" are related by the transformation:\n\nAfter the addition theorem for the tangens hyperbolicus was given by Lambert (1868), hyperbolic geometry was used by Franz Taurinus (1826), and later by Nikolai Lobachevsky (1829/30) and others, to formulate the hyperbolic law of cosines:\n\nFollowing Gauss (1818), Carl Gustav Jacob Jacobi formulated Gauss' transformation using 3 dimensions in 1827 and in 1833:\n\nIn 1832 he used the orthogonal transformation in order to derive the case of 2 dimensions:\n\nAfter Cauchy (1829) formulated the orthogonal transformation for arbitrary dimensions, Jacobi in 1833 used this result to extend his previous formulas:\n\nHe also stated the following linear substitution leaving invariant the Lorentz interval:\n\nAugustin-Louis Cauchy (1829) extended the orthogonal transformation of Euler (1771) to arbitrary dimensions\n\nVictor-Amédée Lebesgue (1837) summarized the previous work of Gauss (1818), Jacobi (1827, 1833), Cauchy 1829. He started with the orthogonal transformation\n\nIn order to achieve the invariance of the Lorentz interval\n\nhe gave the following instructions as to how the previous equations shall be modified: In equation (9) change the sign of the last term of each member. In the first \"n-1\" equations of (10) change the sign of the last term of the left-hand side, and in the one which satisfies α=\"n\" change the sign of the last term of the left-hand side as well as the sign of the right-hand side. In all equations (11) the last term will change sign. In equations (12) the last terms of the right-hand side will change sign, and so will the left-hand side of the \"n\"-th equation. In equations (13) the signs of the last terms of the left-hand side will change, moreover in the \"n\"-th equation change the sign of the right-hand side. In equations (14) the last terms will change sign.\n\nHe went on to redefine the variables of the Lorentz interval and its transformation:\n\nThe Euler–Rodrigues parameters discovered by Euler (1871) and Rodrigues (1840) leaving invariant formula_19 were extended to formula_66 by Arthur Cayley (1846) as a byproduct of what is now called the Cayley transform using the method of skew–symmetric coefficients. Following Cayley's methods, a general transformation for quadratic forms into themselves in three (1853) and arbitrary (1854) dimensions was provided by Hermite (1853, 1854). Hermite's formula was simplified and brought into matrix form equivalent to () by Cayley (1855a)\n\nwhich he abbreviated in 1858, where formula_68 is any skew-symmetric matrix:\n\nUsing the parameters of (1855a), Cayley in a subsequent paper (1855b) particularly discussed several special cases, such as:\n\nor:\n\nor:\n\nAlready in 1854, Cayley published an alternative method of transforming quadratic forms by using certain parameters α,β,γ,δ in relation to a homographic transformation of a surface of second order into itself:\n\nIn the same paper, Cayley also introduced four different parameters \"M=a+b+c+d\" in order to demonstrate the invariance of formula_74, and subsequently showed the relation to quaternions. Fricke & Klein (1897) credited Cayley by calling the above transformation the most general (real or complex) space collineation of first kind of an absolute surface of second kind into itself. Parameters α,β,γ,δ are similar to what was later called Cayley–Klein parameters in relation to spatial rotations (which was done by Cayley in 1879 and before by Hermann von Helmholtz (1866/67)).\n\nIn 1845, Cayley showed that the Euler-Rodrigues parameters in equation () representing rotations can be related to William Rowan Hamilton's quaternions by using a pre- and a postfactor\n\nand in 1848 he used the abbreviated form\n\nIn 1854 he showed how to transform the sum of four squares into itself:\n\nor in 1855:\n\nIn 1859, Cayley found out that a quadratic form or projective quadric can be used as an \"absolute\", serving as the basis of a projective metric (the Cayley–Klein metric). For instance, using the absolute \"x+y+z=0\", he defined the distance of two points as follows\n\nand he also alluded to the case of the unit sphere \"x+y+z=1\". In the hands of Klein (1871), all of this became essential for the discussion of non-Euclidean geometry (in particular the Cayley–Klein or Beltrami–Klein model of hyperbolic geometry) and associated quadratic forms and transformations, including the Lorentz interval and Lorentz transformation.\n\nCayley (1884) himself also discussed some properties of the Beltrami–Klein model and the pseudosphere, and formulated coordinate transformations using the Cayley-Hermite formalism:\n\nCharles Hermite (1853) extended the work of Gauss (1801) and others (including himself) on \"definite\" ternary quadratic forms that can be transformed into ±\"(x+y+z)\", by additionally analyzing \"indefinite\" ternary quadratic forms that can be transformed into the Lorentz interval ±\"(x+y-z)\". By using Cayley's (1846) method of skew–symmetric coefficients, he derived a transformation leaving invariant almost all types of ternary quadratic forms. This was generalized by him in 1854 to all dimensions, so Hermite arrived at a transformation scheme leaving invariant almost all types of quadratic forms:\n\nThis result was subsequently expressed in matrix form by Cayley (1855). Later, Ferdinand Georg Frobenius (1877) added some modifications in order to include some special cases of quadratic forms that cannot be dealt with by the Cayley–Hermite transformation.\n\nFollowing Gauss (1818), Edmond Bour (1856) wrote the transformations:\n\nFollowing Gauss (1818), Jacobi (1827, 1833), and Bour (1856), Osip Ivanovich Somov (1863) wrote the transformation systems:\n\nEugenio Beltrami (1868a) introduced coordinates of the Beltrami–Klein model of hyperbolic geometry, and formulated the corresponding transformations in terms of homographies:\n\nPaul Gustav Heinrich Bachmann (1869) adapted Hermite's (1853/54) transformation of ternary quadratic forms to the case of integer transformations. He particularly analyzed the Lorentz interval and its transformation, and also alluded to the analogue result of Gauss (1800) in terms of Cayley–Klein parameters, while Bachmann formulated his result in terms of the Cayley–Hermite transformation:\n\nHe described this transformation in 1898 in the first part of his \"arithmetics of quadratic forms\" as well.\n\nElaborating on Cayley's (1859) definition of an \"absolute\" (Cayley–Klein metric), Felix Klein (1871) defined a \"fundamental conic section\" in order to discuss motions such as rotation and translation in the non-Euclidean plane, and another fundamental form by using homogeneous coordinates \"x,y\" related to a circle with radius \"2c\" with measure of curvature formula_87. When \"c\" is positive, the measure of curvature is negative and the fundamental conic section is real, thus the geometry becomes hyperbolic (Beltrami–Klein model):\n\nIn (1873) he pointed out that hyperbolic geometry in terms of a surface of constant negative curvature can be related to a quadratic equation, which can be transformed into a sum of squares of which one square has a different sign, and can also be related to the interior of a surface of second degree corresponding to an ellipsoid or two-sheet hyperboloid.\n\nIn (1872) while devising the Erlangen program, Klein discussed the general relation between projective metrics, binary forms and conformal geometry transforming a sphere into itself in terms of linear transformations of the complex variable \"x+iy\". Following Klein, these relations were discussed by Ludwig Wedekind (1875) using formula_92. Klein (1875) then showed that all finite groups of motions follow by determining all finite groups of such linear transformations of \"x+iy\" into itself. In (1878), Klein classified the substitutions of formula_93 with αδ-βγ=1 into hyperbolic, elliptic, parabolic, and in (1882) he added the loxodromic substitution as the combination of elliptic and hyperbolic ones. (In 1890, Robert Fricke in his edition of Klein's lectures of elliptic functions and Modular forms, referred to the analogy of this treatment to the theory of quadratic forms as given by Gauss and in particular Dirichlet.)\n\nIn (1884) Klein related the linear fractional transformations (interpreted as rotations around the \"x+iy\"-sphere) to Cayley–Klein parameters [α,β,γ,δ], to Euler–Rodrigues parameters \"[a,b,c,d]\", and to the unit sphere by means of stereographic projection, and also discussed transformations preserving surfaces of second degree equivalent to the transformation given by Cayley (1854):\n\n\\xi^{2}+\\eta^{2}+\\zeta^{2}-1 & =x_{1}^{2}+x_{2}^{2}+x_{3}^{2}-x_{0}^{2}=0\n</math>.\n\nThe formulas on the right can be related to those on the left by setting\n\nand become equivalent to Lorentz transformation () by setting\n\nIn his lecture in the winter semester of 1889/90 (published 1892–93), he discussed the hyperbolic plane by using (as in 1871) the Lorentz interval in terms of a circle with radius \"2k\" as the basis of hyperbolic geometry, and another quadratic form to discuss the \"kinematics of hyperbolic geometry\" consisting of motions and congruent displacements of the hyperbolic plane into itself:\n\nIn his lecture in the summer semester of 1890 (published 1892–93), he discussed general surfaces of second degree, including an \"oval\" surface corresponding to hyperbolic space and its motions:\n\nIn (1896/97), Klein again defined hyperbolic motions and explicitly used \"t\" as time coordinate:\n\nKlein's work was summarized and extended Fricke (1893, 1897) (supported by Klein).\n\nIn 1890 Klein discussed a general type of Euclidean or Non-Euclidean motion in relation to a problem posed by Helmholtz (1868), with the following transformation\n\nIn 1893 he called the special case with \"a=b=0\" a \"spiral transformation\":\n\nIn relation to line geometry, Klein (1871/72) used coordinates satisfying the condition formula_102. They were introduced in 1868 (belatedly published in 1872/73) by Gaston Darboux as a system of five coordinates in \"R\" (later called \"pentaspherical\" coordinates) in which the last coordinate is imaginary. Sophus Lie (1871) more generally used \"n+2\" coordinates in \"R\" (later called \"polyspherical\" coordinates) satisfying formula_103 in which the last coordinate is imaginary, as a means to discuss conformal transformations generated by inversions. These simultaneous publications can be explained by the fact that Darboux, Lie, and Klein corresponded with each other by letter.\n\nWhen the last coordinate is defined as real, the corresponding polyspherical coordinates satisfy the form of a sphere. Initiated by lectures of Klein between 1889–1890, his student Friedrich Pockels (1891) used such real coordinates, emphasizing that all of these coordinate systems remain invariant under conformal transformations generated by inversions:\n\nSpecial cases were described by Klein (1893):\n\nBoth systems were also described by Maxime Bôcher (1894) in an expanded version of a thesis supervised by Klein.\n\nIn several papers between 1847 and 1850 it was shown by Joseph Liouville that the relation \"λ(δx+δy+δz)\" is invariant under the group of conformal transformations generated by inversions transforming spheres into spheres, which can be related special conformal transformations or Möbius transformations. (The conformal nature of the linear fractional transformation formula_107 of a complex variable formula_108 was already discussed by Euler (1777)).\n\nLiouville's theorem was extended to all dimensions by Sophus Lie (1871a). In addition, Lie described a manifold whose elements can be represented by spheres, where the last coordinate \"y\" can be related to an imaginary radius by \"iy\":\n\nIf the second equation is satisfied, two spheres \"y′\" and \"y″\" are in contact. Lie then defined the correspondence between contact transformations in \"R\" and conformal point transformations in \"R\": The sphere of space \"R\" consists of \"n+1\" parameter (coordinates plus imaginary radius), so if this sphere is taken as the element of space \"R\", it follows that \"R\" now corresponds to \"R\". Therefore, any transformation (to which he counted orthogonal transformations and inversions) leaving invariant the condition of contact between spheres in \"R\", corresponds to the transformation of points in \"R\".\n\nEventually, Lie (1871/72) pointed out that the conformal point transformations of \"R\" consist of motions (such as rigid transformations and orthogonal transformations), similarity transformations, and inversions.\n\nIn (1885/86), Lie identified the projective group of a general surface of second degree formula_110 with the group of non-Euclidean motions. In a thesis guided by Lie, Hermann Werner (1889) discussed this projective group by using the equation of a unit sphere as the surface of second degree, and also gave the corresponding infinitesimal projective transformations:\n\nformula_111\n\nMore generally, Lie (1890) defined non-Euclidean motions in terms of two forms formula_112 in which the imaginary form with formula_113 denotes the group of elliptic motions (in Klein's terminology), the real form with −1 the group of hyperbolic motions, with the latter having the same form as Werner's transformation:\n\nSummarizing, Lie (1893) discussed the real continuous groups of the conic sections representing non-Euclidean motions, which in the case of hyperbolic motions have the form:\n\nContinuing the work of Gauss (1801) on definite ternary quadratic forms and Hermite (1853) on indefinite ternary quadratic forms, Eduard Selling (1873) used the auxiliary coefficients ξ,η,ζ by which a definite form formula_118 and an indefinite form \"f\" can be rewritten in terms of three squares:\n\nIn addition, Selling showed that auxiliary coefficients ξηζ can be geometrically interpreted as point coordinates which are in motion upon one sheet of a two-sheet hyperboloid, which is related to Selling's formalism for the reduction of indefinite forms by using definite forms.\n\nSelling also reproduced the Lorentz transformation given by Gauss (1800/63), to whom he gave full credit, and called it the only example of a particular indefinite ternary form known to him that has ever been discussed:\n\nGustav von Escherich (1874) discussed the plane of constant negative curvature based on the Beltrami–Klein model of hyperbolic geometry by Beltrami (1868), as well as Christoph Gudermann's (1830) rectangular coordinates \"x\"=tan(a) and \"y\"=tan(b) and coordinate transformations using trigonometric functions in the cases of rotation and translation related to sphere geometry. By using hyperbolic functions \"x\"=tanh(a/k) and \"y\"=tanh(b/k), Escherich gave the corresponding coordinate transformations for the hyperbolic plane, which for the case of translation has the form:\n\nWilhelm Killing (1878–1880) described non-Euclidean geometry by using Weierstrass coordinates (named after Karl Weierstrass who described them in lectures in 1872 which Killing attended) obeying the form\n\nor\n\nwhere \"k\" is the reciprocal measure of curvature, \"k\"=∞ denotes Euclidean geometry, \"k\">0 elliptic geometry, and \"k\"<0 hyperbolic geometry. In (1877/78) he pointed out the possibility and some characteristics of a transformation (indicating rigid motions) preserving the above form. In (1879/80) he wrote the corresponding transformations as a general rotation matrix\n\nIn (1885) he wrote the Weierstrass coordinates and their transformation as follows:\n\nIn (1885) he also gave the transformation for \"n\" dimensions:\n\nIn (1885) he applied his transformations to mechanics and defined four-dimensional vectors of velocity and force. Regarding the geometrical interpretation of his transformations, Killing argued in (1885) that by setting \"k\"=-1 and using \"p,x,y\" as rectangular space coordinates, the hyperbolic plane is mapped on one side of a two-sheet hyperboloid \"p-x-y=1\" (known as hyperboloid model), by which the previous formulas become equivalent to Lorentz transformations and the geometry becomes that of Minkowski space. Finally, in (1893) he wrote:\n\nand for \"n\" dimensions\n\nThe case of translation was given by Killing (1893) in the form\n\nIn 1898, Killing wrote that relation in a form similar to Escherich (1874), and derived the corresponding Lorentz transformation for the two cases were \"v\" is unchanged or \"u\" is unchanged:\n\nKilling (1887/88) defined the infinitesimal projective transformations in relation to the following quadratic form of second degree by:\n\nand in (1892) he defined the infinitesimal transformation for non-Euclidean motions in terms of Weierstrass coordinates:\n\nIn (1897/98) he pointed out (see the following formulas on the left) that the corresponding group of non-Euclidean motions in terms of Weierstrass coordinates is intransitive when related to form (1) and transitive when related to form (2), and he also showed (on the right) the relation of Weierstrass coordinates to the notation of Killing (1887/88) and Werner (1889), Lie (1890):\n\nHenri Poincaré (1881) published a work which connected the work of Hermite (1853) and Selling (1873) on indefinite quadratic forms with non-Euclidean geometry (Poincaré already discussed such relations in an unpublished manuscript in 1880). He used two indefinite ternary forms in terms of three squares and then defined them in terms of Weierstrass coordinates (without using that expression) connected by a transformation with integer coefficients:\n\nHe went on to describe the properties of \"hyperbolic coordinates\". Poincaré mentioned the hyperboloid model also in (1887).\n\nPoincaré (1881a) also demonstrated the connection of his above formulas to Möbius transformations:\n\nPoincaré (1881b) also used the Möbius transformation formula_138 in relation to Fuchsian functions and the discontinuous Fuchsian group, being a special case of the hyperbolic group leaving invariant the “fundamental circle” (Poincaré disk model and Poincaré half-plane model of hyperbolic geometry). He then extended Klein's (1878-1882) study on the relation between Möbius transformations and hyperbolic, elliptic, parabolic, and loxodromic substitutions, and while formulating Kleinian groups (1883) he used the following transformation leaving invariant the generalized circle:\n\nIn 1886, Poincaré investigated the relation between indefinite ternary quadratic forms and Fuchsian functions and groups:\n\nHomersham Cox (1881/82) – referring to similar rectangular coordinates used by Gudermann (1830) and George Salmon (1862) on a sphere, and to Escherich (1874) as reported by Johannes Frischauf (1876) in the hyperbolic plane – defined the Weierstrass coordinates (without using that expression) and their transformation:\n\nCox also gave the Weierstrass coordinates and their transformation in hyperbolic space:\n\nThe case of translation was also given by him, where the \"y\"-axis remains unchanged:\n\nSubsequently, Cox (1882/83) also described hyperbolic geometry in terms of an analogue to quaternions and Hermann Grassmann's exterior algebra. To that end, he used hyperbolic numbers, which were first introduced by James Cockle (1848) in the framework of his tessarine algebra as follows:\n\nIn the hands of Cox (who doesn't mention Cockle) this expression becomes a means to transfer point P to point Q in the hyperbolic plane, which he wrote in the form:\n\nIn (1882/83a) he showed the equivalence of \"PQ\"=-cosh(θ)+ι·sinh(θ) with \"quaternion multiplication\", and in (1882/83b) he described \"QP\"=cosh(θ)+ι·sinh(θ) as being \"associative quaternion multiplication\". He also showed that the position of point P in the hyperbolic plane may be determined by three quantities in terms of Weierstrass coordinates obeying the relation \"z-x-y=1\".\n\nCox went on to develop an algebra for hyperbolic space analogous to Clifford's biquaternions. While Clifford (1873) used biquaternions of the form \"a+ωb\" in which ω=0 denotes parabolic space and ω=1 elliptic space, Cox discussed hyperbolic space using the imaginary quantity formula_148 and therefore ω=-1. He also obtained relations of quaternion multiplication in terms of Weierstrass coordinates:\n\nFollowing Gauss (1818), George William Hill (1882) formulated the equations\n\nAfter previous work by Albert Ribaucour (1870), a transformation which transforms oriented spheres into oriented spheres, oriented planes into oriented planes, and oriented lines into oriented lines, was explicitly formulated by Edmond Laguerre (1882) as \"transformation by reciprocal directions\" which was later called „Laguerre inversion/transformation\". It can be seen as a special case of the conformal group in terms of Lie's transformations of oriented spheres. In two dimensions the transformation or oriented lines has the form (\"R\" being the radius):\n\nÉmile Picard (1882) analyzed the invariance of indefinite ternary Hermitian quadratic forms with integer coefficients and their relation to discontinuous groups, extending Poincaré's Fuchsian functions of one complex variable related to a circle, to \"hyperfuchsian\" functions of two complex variables related to a hypersphere. He formulated the following special case of an Hermitian form:\n\nOr in (1884a) in relation to indefinite binary Hermitian quadratic forms:\n\nOr in (1884b):\n\nOr in (1884c):\n\nCyparissos Stephanos (1883) showed that Hamilton's biquaternion \"a+aι+aι+aι\" can be interpreted as an oriented sphere in terms of Lie's sphere geometry (1871), having the vector \"aι+aι+aι\" as its center and the scalar formula_158 as its radius. Its norm formula_159 is thus equal to the power of a point of the corresponding sphere. In particular, the norm of two quaternions \"N(Q-Q)\" (the corresponding spheres are in contact with \"N(Q-Q)=0\") is equal to the tangential distance between two spheres. The general contact transformation between two spheres then can be given by a homography using 4 arbitrary quaternions \"A,B,C,D\" and two variable quaternions \"X,Y\":\n\nStephanos pointed out that the special case \"A=0\" denotes transformations of oriented planes (see Laguerre's transformation of oriented planes (1882)).\n\nArthur Buchheim (1884, published 1885) applied Clifford's biquaternions and their operator ω to different forms of geometries (Buchheim mentions Cox (1882) as well). He defined the scalar \"ω=e\" which in the case -1 denotes hyperbolic space, 1 elliptic space, and 0 parabolic space. He derived the following relations consistent with the Cayley–Klein absolute:\n\nFollowing Gauss (1818) and Hill (1882), Octave Callandreau (1885) formulated the equations\n\nClifford algebra (which includes quaternions as special cases) was used by Rudolf Lipschitz (1885/86) who introduced the orthogonal transformation ΛX=YΛ of a definite quadratic form as a sum or squares formula_164 into itself, which he discussed both for real as well as imaginary expressions. He then discussed the general indefinite form and its transformation by using \"m\" real and \"n-m\" imaginary quantities:\n\nFriedrich Schur (1885/86) discussed spaces of constant Riemann curvature, and by following Beltrami (1868) he used the transformation\n\nIn (1900/02) he derived basic formulas of non-Eucliden geometry, including the case of translation for which he obtained the transformation similar to his previous one:\n\nwhere formula_168 can have values >0, <0 or ∞.\n\nHe also defined the triangle\n\nFollowing Laguerre (1882), Gaston Darboux (1887) presented the Laguerre inversions in four dimensions using coordinates \"x,y,z,R\":\n\nRelated to Klein's (1871) and Poincaré's (1881-1887) work on non-Euclidean geometry and indefinite quadratic forms, Luigi Bianchi (1888) analyzed the differential Lorentz interval \"ds=dx+dy-dz\", alluded to the Möbius transformations and its parameters α,β,γ,δ in order to preserve the Lorentz interval, and gave credit to Gauss (1800/63) who obtained the same coefficient system:\n\nIn 1893, Bianchi gave the coefficients in the case of four dimensions:\n\nSolving for formula_175 Bianchi obtained:\n\nFerdinand von Lindemann discussed hyperbolic geometry in his (1890/91) edition of the lectures on geometry of Alfred Clebsch. Citing Killing (1885) and Poincaré (1887) in relation to the hyperboloid model in terms of Weierstrass coordinates for the hyperbolic plane and space, he set\n\nIn addition, following Klein (1871) he employed the Cayley absolute related to surfaces of second degree, by using the following quadratic form and its transformation\n\ninto which he put\n\nFrom that, he obtained the following Cayley absolute and the corresponding most general motion in hyperbolic space comprising ordinary rotations (\"a\"=0) or translations (α=0):\n\nRobert Fricke (1891) – following the work of his teacher Klein (1878–1882) as well as Poincaré (1881–1887) on automorphic functions and group theory – obtained the following transformation for an integer ternary quadratic form\n\nAnd the general case of four dimensions in 1893:\n\nSupported by Felix Klein, Fricke summarized his and Klein's work in a treatise concerning automorphic functions (1897). Using a sphere as the absolute, in which the interior of the sphere is denoted as hyperbolic space, they defined hyperbolic motions, and stressed that any hyperbolic motion corresponds to \"circle relations\" (now called Möbius transformations):\n\nLouis Gérard (1892) – in a thesis examined by Poincaré – discussed Weierstrass coordinates (without using that name) in the plane using the following invariant and its Lorentz transformation equivalent to () \"(n=2)\":\n\nHe gave the case of translation as follows:\n\nAlexander Macfarlane (1892, 1893) – similar to Cockle (1848) and Cox (1882/83) – defined the hyperbolic versor in terms of hyperbolic numbers\n\nand in 1894 he defined the \"exspherical\" versor\n\nand used them to analyze hyperboloids of one- or two sheets. This was further extended by him in (1900) in order to express trigonometry in terms of hyperbolic quaternions \"re\", with β=+1 and formula_189, the hyperbolic number \"x+yβ\", and the hyperbolic versor \"e\".\n\nAlfred North Whitehead (1898) discussed the kinematics of hyperbolic space as part of his study of universal algebra, and obtained the following transformation:\n\nFelix Hausdorff (1899) – citing Killing (1885) – discussed Weierstrass coordinates in the plane using the following invariant and its transformation:\n\nHausdorff (1899) also discussed the relation of the above coordinates to conformal Möbius transformations:\n\nModifying Lipschitz's (1885/86) variant of Clifford numbers, Theodor Vahlen (1901/02) formulated Möbius transformations (which he called vector transformations) and biquaternions in order to discuss motions in n-dimensional non-Euclidean space, leaving the following quadratic form invariant (where \"j\"=1 represents hyperbolic motions, \"j\"=-1 elliptic motions, \"j\"=0 parabolic motions):\n\nFrederick S. Woods (1901/02) defined the following invariant quadratic form and its projective transformation (he pointed out that this can be connected to hyperbolic geometry by setting formula_194 with \"R\" as real quantity):\n\nAlternatively, Woods (1903, published 1905) – citing Killing (1885) – used the invariant quadratic form in terms of Weierstrass coordinates and its transformation (with formula_196 for hyperbolic space):\n\nand the case of translation:\n\nand the loxodromic substitution for hyperbolic space:\n\nHeinrich Liebmann (1904/05) – citing Killing (1885), Gérard (1892), Hausdorff (1899) – used the invariant quadratic form and its Lorentz transformation equivalent to () \"(n=2)\"\n\nand the case of translation:\n\nWoldemar Voigt (1887) developed a transformation in connection with the Doppler effect and an incompressible medium, being in modern notation:\n\nIf the right-hand sides of his equations are multiplied by γ they are the modern Lorentz transformation (). In Voigt's theory the speed of light is invariant, but his transformations mix up a relativistic boost together with a rescaling of space-time. Optical phenomena in free space are scale, conformal (using the factor λ discussed above), and Lorentz invariant, so the combination is invariant too. For instance, Lorentz transformations can be extended by using formula_203:\n\n\"l\"=1/γ gives the Voigt transformation, \"l\"=1 the Lorentz transformation. But scale transformations are not a symmetry of all the laws of nature, only of electromagnetism, so these transformations cannot be used to formulate a principle of relativity in general. It was demonstrated by Poincaré and Einstein that one has to set \"l\"=1 in order to make the above transformation symmetric and to form a group as required by the relativity principle, therefore the Lorentz transformation is the only viable choice.\n\nAlso Hermann Minkowski said in 1908 that the transformations which play the main role in the principle of relativity were first examined by Voigt in 1887. Voigt responded in the same paper by saying that his theory was based on an elastic theory of light, not an electromagnetic one. However, he concluded that some results were actually the same.\n\nIn 1888, Oliver Heaviside investigated the properties of charges in motion according to Maxwell's electrodynamics. He calculated, among other things, anisotropies in the electric field of moving bodies represented by this formula:\n\nConsequently, Joseph John Thomson (1889) found a way to substantially simplify calculations concerning moving charges by using the following mathematical transformation (like other authors such as Lorentz or Larmor, also Thomson implicitly used the Galilean transformation \"z-vt\" in his equation):\n\nThereby, inhomogeneous electromagnetic wave equations are transformed into a Poisson equation. Eventually, George Frederick Charles Searle noted in (1896) that Heaviside's expression leads to a deformation of electric fields which he called \"Heaviside-Ellipsoid\" of axial ratio\n\nIn order to explain the aberration of light and the result of the Fizeau experiment in accordance with Maxwell's equations, Lorentz in 1892 developed a model (\"Lorentz ether theory\") in which the aether is completely motionless, and the speed of light in the aether is constant in all directions. In order to calculate the optics of moving bodies, Lorentz introduced the following quantities to transform from the aether system into a moving system (it's unknown whether he was influenced by Voigt, Heaviside, and Thomson)\n\nwhere \"x\" is the Galilean transformation \"x-vt\". Except the additional γ in the time transformation, this is the complete Lorentz transformation (). While \"t\" is the \"true\" time for observers resting in the aether, \"t′\" is an auxiliary variable only for calculating processes for moving systems. It is also important that Lorentz and later also Larmor formulated this transformation in two steps. At first an implicit Galilean transformation, and later the expansion into the \"fictitious\" electromagnetic system with the aid of the Lorentz transformation. In order to explain the negative result of the Michelson–Morley experiment, he (1892b) introduced the additional hypothesis that also intermolecular forces are affected in a similar way and introduced length contraction in his theory (without proof as he admitted). The same hypothesis was already made by George FitzGerald in 1889 based on Heaviside's work. While length contraction was a real physical effect for Lorentz, he considered the time transformation only as a heuristic working hypothesis and a mathematical stipulation.\n\nIn 1895, Lorentz further elaborated on his theory and introduced the \"theorem of corresponding states\". This theorem states that a moving observer (relative to the ether) in his „fictitious\" field makes the same observations as a resting observers in his „real\" field for velocities to first order in \"v/c\". Lorentz showed that the dimensions of electrostatic systems in the ether and a moving frame are connected by this transformation:\n\nFor solving optical problems Lorentz used the following transformation, in which the modified time variable was called \"local time\" () by him:\n\nWith this concept Lorentz could explain the Doppler effect, the aberration of light, and the Fizeau experiment.\n\nIn 1897, Larmor extended the work of Lorentz and derived the following transformation\n\nLarmor noted that if it is assumed that the constitution of molecules is electrical then the FitzGerald–Lorentz contraction is a consequence of this transformation, explaining the Michelson–Morley experiment. It's notable that Larmor was the first who recognized that some sort of time dilation is a consequence of this transformation as well, because \"individual electrons describe corresponding parts of their orbits in times shorter for the [rest] system in the ratio 1/γ\". Larmor wrote his electrodynamical equations and transformations neglecting terms of higher order than \"(v/c)\" – when his 1897 paper was reprinted in 1929, Larmor added the following comment in which he described how they can be made valid to all orders of \"v/c\":\n\nIn line with that comment, in his book Aether and Matter published in 1900, Larmor used a modified local time \"t″=t′-εvx′/c\" instead of the 1897 expression \"t′=t-vx/c\" by replacing \"v/c\" with \"εv/c\", so that \"t″\" is now identical to the one given by Lorentz in 1892, which he combined with a Galilean transformation for the \"x′, y′, z′, t′\" coordinates:\n\nLarmor knew that the Michelson–Morley experiment was accurate enough to detect an effect of motion depending on the factor \"(v/c)\", and so he sought the transformations which were \"accurate to second order\" (as he put it). Thus he wrote the final transformations (where \"x′=x-vt\" and \"t″\" as given above) as:\n\nby which he arrived at the complete Lorentz transformation (). Larmor showed that Maxwell's equations were invariant under this two-step transformation, \"to second order in \"v/c\"\" – it was later shown by Lorentz (1904) and Poincaré (1905) that they are indeed invariant under this transformation to all orders in \"v/c\".\n\nLarmor gave credit to Lorentz in two papers published in 1904, in which he used the term \"Lorentz transformation\" for Lorentz's first order transformations of coordinates and field configurations:\n\nAlso Lorentz extended his theorem of corresponding states in 1899. First he wrote a transformation equivalent to the one from 1892 (again, \"x\"* must be replaced by \"x-vt\"):\n\nThen he introduced a factor formula_216 of which he said he has no means of determining it, and modified his transformation as follows (where the above value of \"t′\" has to be inserted):\n\nThis is equivalent to the complete Lorentz transformation () when solved for \"x″\" and \"t″\" and with ε=1. Like Larmor, Lorentz noticed in 1899 also some sort of time dilation effect in relation to the frequency of oscillating electrons \"\"that in \"S\" the time of vibrations be \"kε\" times as great as in \"S\"\"\", where \"S\" is the aether frame.\n\nIn 1904 he rewrote the equations in the following form by setting \"l\"=1/ε (again, \"x\"* must be replaced by \"x-vt\"):\n\nUnder the assumption that \"l=1\" when \"v\"=0, he demonstrated that \"l=1\" must be the case at all velocities, therefore length contraction can only arise in the line of motion. So by setting the factor \"l\" to unity, Lorentz's transformations now assumed the same form as Larmor's and are now completed. Unlike Larmor, who restricted himself to show the covariance of Maxwell's equations to second order, Lorentz tried to widen its covariance to all orders in \"v/c\". He also derived the correct formulas for the velocity dependence of electromagnetic mass, and concluded that the transformation formulas must apply to all forces of nature, not only electrical ones. However, he didn't achieve full covariance of the transformation equations for charge density and velocity. When the 1904 paper was reprinted in 1913, Lorentz therefore added the following remark:\n\nLorentz's 1904 transformation was cited and used by Alfred Bucherer in July 1904:\n\nor by Wilhelm Wien in July 1904:\n\nor by Emil Cohn in November 1904 (setting the speed of light to unity):\n\nor by Richard Gans in February 1905:\n\nNeither Lorentz or Larmor gave a clear physical interpretation of the origin of local time. However, Henri Poincaré in 1900 commented on the origin of Lorentz’s \"wonderful invention\" of local time. He remarked that it arose when clocks in a moving reference frame are synchronised by exchanging signals which are assumed to travel with the same speed formula_223 in both directions, which lead to what is nowadays called relativity of simultaneity, although Poincaré's calculation does not involve length contraction or time dilation. In order to synchronise the clocks here on Earth (the \"x*, t\"* frame) a light signal from one clock (at the origin) is sent to another (at \"x\"*), and is sent back. It's supposed that the Earth is moving with speed \"v\" in the \"x\"-direction (= \"x\"*-direction) in some rest system (\"x, t\") (\"i.e.\" the luminiferous aether system for Lorentz and Larmor). The time of flight outwards is\n\nand the time of flight back is\n\nThe elapsed time on the clock when the signal is returned is formula_226 and the time formula_227 is ascribed to the moment when the light signal reached the distant clock. In the rest frame the time formula_228 is ascribed to that same instant. Some algebra gives the relation between the different time coordinates ascribed to the moment of reflection. Thus\n\nidentical to Lorentz (1892). By dropping the factor γ under the assumption that formula_230, Poincaré gave the result formula_231, which is the form used by Lorentz in 1895.\n\nSimilar physical interpretations of local time were later given by Emil Cohn (1904) and Max Abraham (1905).\n\nOn June 5, 1905 (published June 9) Poincaré simplified the equations which are algebraically equivalent to those of Larmor and Lorentz and gave them the modern form ():\n\nApparently Poincaré was unaware of Larmor's contributions, because he only mentioned Lorentz and therefore used for the first time the name \"Lorentz transformation\". Poincaré set the speed of light to unity, pointed out the group characteristics of the transformation by setting \"l\"=1, and modified/corrected Lorentz's derivation of the equations of electrodynamics in some details in order to fully satisfy the principle of relativity, \"i.e.\" making them fully Lorentz covariant.\n\nIn July 1905 (published in January 1906) Poincaré showed in detail how the transformations and electrodynamic equations are a consequence of the principle of least action; he demonstrated in more detail the group characteristics of the transformation, which he called Lorentz group, and he showed that the combination formula_233 is invariant. He noticed that the Lorentz transformation is merely a rotation in four-dimensional space about the origin by introducing formula_234 as a fourth imaginary coordinate, and he used an early form of four-vectors.\n\nOn June 30, 1905 (published September 1905) Einstein published what is now called special relativity and gave a new derivation of the transformation, which was based only on the principle on relativity and the principle of the constancy of the speed of light. While Lorentz considered \"local time\" to be a mathematical stipulation device for explaining the Michelson-Morley experiment, Einstein showed that the coordinates given by the Lorentz transformation were in fact the inertial coordinates of relatively moving frames of reference. For quantities of first order in \"v/c\" this was also done by Poincaré in 1900, while Einstein derived the complete transformation by this method. Unlike Lorentz and Poincaré who still distinguished between real time in the aether and apparent time for moving observers, Einstein showed that the transformations concern the nature of space and time.\n\nThe notation for this transformation is equivalent to Poincaré's of 1905 and (), except that Einstein didn't set the speed of light to unity:\n\nEinstein also defined the velocity addition formula () (which also has been done by Poincaré in May 1905 in unpublished letters to Lorentz):\n\nThe work on the principle of relativity by Lorentz, Einstein, Planck, together with Poincaré's four-dimensional approach, were further elaborated and combined with the hyperboloid model by Hermann Minkowski in 1907 and 1908. Minkowski particularly reformulated electrodynamics in a four-dimensional way (Minkowski spacetime). For instance, he wrote \"x, y, z, it\" in the form \"x, x, x, x\". By defining formula_237 as the angle of rotation around the formula_108-axis, the Lorentz transformation assumes a form (with \"c\"=1) in agreement with ():\n\nEven though Minkowski used the imaginary number iψ, he for once directly used the tangens hyperbolicus in the equation for velocity\n\nMinkowski's expression can also by written as ψ=atanh(q) and was later called rapidity. He also wrote the Lorentz transformation in matrix form equivalent to () formula_242:\n\nAs a graphical representation of the Lorentz transformation he introduced the Minkowski diagram, which became a standard tool in textbooks and research articles on relativity:\n\nUsing an imaginary rapidity such as Minkowski, Arnold Sommerfeld (1909) formulated a transformation equivalent to Lorentz boost (), and the relativistc velocity addition () in terms of trigonometric functions and the spherical law of cosines:\n\nformula_244\n\nIn line with Lie's (1871) research on the relation between sphere transformations with an imaginary radius coordinate and 4D conformal transformations, it was pointed out by Bateman and Cunningham (1909–1910), that by setting \"u=ict\" as the imaginary fourth coordinates one can produce spacetime conformal transformations. Not only the quadratic form \"λ(δx+δy+δz+δu)\", but also Maxwells equations are covariant with respect to these transformations, irrespective of the choice of λ. These variants of conformal or Lie sphere transformations were called spherical wave transformations by Bateman. However, this covariance is restricted to certain areas such as electrodynamics, whereas the totality of natural laws in inertial frames is covariant under the Lorentz group. In particular, by setting λ=1 the Lorentz group can be seen as a 10-parameter subgroup of the 15-parameter spacetime conformal group .\n\nBateman (1910/12) also alluded to the identity between the Laguerre inversion and the Lorentz transformations. In general, the isomorphism between the Laguerre group and the Lorentz group was pointed out by Élie Cartan (1912, 1915/55), Henri Poincaré (1912/21) and others.\n\nFollowing Klein (1889–1897) and Fricke & Klein (1897) concerning the Cayley absolute, hyperbolic motion and its transformation, Gustav Herglotz (1909/10) classified the one-parameter Lorentz transformations as loxodromic, hyperbolic, parabolic and elliptic. The general case (on the left) equivalent to Lorentz transformation () and the hyperbolic case (on the right) equivalent to Lorentz transformation () are as follows:\n\nFollowing Sommerfeld (1909), hyperbolic functions were used by Vladimir Varićak in several papers starting from 1910, who represented the equations of special relativity on the basis of hyperbolic geometry in terms of Weierstrass coordinates. For instance, by setting \"l=ct\" and \"v/c=tanh(u)\" with \"u\" as rapidity he wrote the Lorentz transformation in agreement with ():\n\nVarićak also related the velocity addition to the hyperbolic law of cosines:\n\nSubsequently, other authors such as E. T. Whittaker (1910) or Alfred Robb (1911, who coined the name rapidity) used similar expressions, which are still used in modern textbooks.\n\nWhile earlier derivations and formulations of the Lorentz transformation relied from the outset on optics, electrodynamics, or the invariance of the speed of light, Vladimir Ignatowski (1910) showed that it is possible to use the principle of relativity (and related group theoretical principles) alone, in order to derive the following transformation between two inertial frames:\n\nThe variable \"n\" can be seen as a space-time constant whose value has to be determined by experiment or taken from a known physical law such as electrodynamics. For that purpose, Ignatowski used the above-mentioned Heaviside ellipsoid representing a contraction of electrostatic fields by \"x\"/γ in the direction of motion. It can be seen that this is only consistent with Ignatowski's transformation when \"n=1/c\", resulting in \"p\"=γ and the Lorentz transformation (). With \"n\"=0, no length changes arise and the Galilean transformation follows. Ignatowski's method was further developed and improved by Philipp Frank and Hermann Rothe (1911, 1912), with various authors developing similar methods in subsequent years.\n\nIn an appedix to Klein's and Sommerfeld's \"Theory of the top\" (1910), Fritz Noether showed how to formulate hyperbolic rotations using biquaternions with formula_249, which he also related to the speed of light by setting ω=-\"c\". He concluded that this is the principal ingredient for a rational representation of the group of Lorentz transformations equivalent to ():\n\nformula_250\n\nBesides citing quaternion related standard works such as Cayley (1854), Noether referred to the entries in Klein's encyclopedia by Eduard Study (1899) and the French version by Élie Cartan (1908). Cartan's version contains a description of Study's dual numbers, Clifford's biquaternions (including the choice formula_249 for hyperbolic geometry), and Clifford algebra, with references to Stephanos (1883), Buchheim (1884/85), Vahlen (1901/02) and others.\n\nAlready in 1908, while describing \"Drehstreckungen\" (orthogonal substitutions in terms of rotations leaving invariant a quadratic form up to a factor) by using Cayley's (1854) quaternion multiplication formalism, Felix Klein pointed out that the modern principle of relativity as provided by Minkowski is essentially only the consequent application of such Drehstreckungen, even though he didn't provide details. Citing Noether, in August 1910 Klein published the following quaternion substitutions forming the group of Lorentz transformations:\n\nor in March 1911\n\nIndependently, also Arthur W. Conway in February 1911 succeeded in combining quaternions and relativity (where formula_254 is the force and formula_255 the charge)\n\nAlso Ludwik Silberstein in November 1911 as well as in 1914, succeeded in combining quaternions and relativity\n\nSilberstein cites Cayley (1854, 1855) and Study's encyclopedia entry (in the extended French version of Cartan in 1908), as well as the appendix of Klein's and Sommerfeld's book.\n\nGustav Herglotz (1911) showed how to formulate the transformation equivalent to () in order to allow for arbitrary velocities and coordinates formula_258 and formula_259:\n\nThis was simplified using vector notation by Ludwik Silberstein (1911 on the left, 1914 on the right):\n\nEquivalent formulas were also given by Wolfgang Pauli (1921), with Erwin Madelung (1922) providing the matrix form\n\nThese formulas were called \"general Lorentz transformation without rotation\" by Christian Møller (1952), who in addition gave an even more general Lorentz transformation in which the Cartesian axes have different orientations, using a rotation operator formula_263. In this case, formula_264 is not equal to formula_265, but the relation formula_266 holds instead, with the result\n\nBorel (1913) started by demonstrating Euclidean motions using Euler-Rodrigues parameter in three dimensions, and Cayley's (1846) parameter in four dimensions. Then he demonstrated the connection to indefinite quadratic forms expressing hyperbolic motions and Lorentz transformations. In three dimensions equivalent to ():\n\nIn four dimensions equivalent to ():\n\nIn pursuing the history in years before Lorentz enunciated his expressions, one looks to the essence of the concept. In mathematical terms, Lorentz transformations are squeeze mappings, the linear transformations that turn a square into a rectangles of the same area. Before Euler, the squeezing was studied as quadrature of the hyperbola and led to the hyperbolic logarithm. In 1748 Euler issued his precalculus textbook where the number e is exploited for trigonometry in the unit circle. The first volume of Introduction to the Analysis of the Infinite had no diagrams, allowing teachers and students to draw their own illustrations.\n\nThere is a gap in Euler's text where Lorentz transformations arise. A feature of natural logarithm is its interpretation as area in hyperbolic sectors. In relativity the classical concept of velocity is replaced with rapidity, a hyperbolic angle concept built on hyperbolic sectors. A Lorentz transformation is a hyperbolic rotation which preserves differences of rapidity, just as the circular sector area is preserved with a circular rotation. Euler’s gap is the lack of hyperbolic angle and hyperbolic functions, later developed by Johann H. Lambert. Euler described some transcendental functions: exponetiation and circular functions. He used the exponential series formula_270 With the imaginary unit i = – 1, and splitting the series into even and odd terms, he obtained\nThis development misses the alternative:\nHere Euler could have noted split-complex numbers along with complex numbers.\n\nFor physics, one space dimension is insufficient. But to extend split-complex arithmetic to four dimensions leads to hyperbolic quaternions, and opens the door to abstract algebra’s \nhypercomplex numbers. Reviewing the expressions of Lorentz and Einstein, one observes that the Lorentz factor is an algebraic function of velocity. For readers uncomfortable with transcendental functions cosh and sinh, algebraic functions may be more to their liking.\n\n\n\n\n\n"}
{"id": "3526138", "url": "https://en.wikipedia.org/wiki?curid=3526138", "title": "Incomplete markets", "text": "Incomplete markets\n\nIn economics, incomplete markets are markets in which the number of Arrow–Debreu securities is less than the number of states of nature. In contrast with complete markets, this shortage of securities will likely restrict individuals from transferring the desired level of wealth among states.\n\nAn Arrow security purchased or sold at date \"t\" is a contract promising to deliver one unit of income in one of the possible contingencies which can occur at date \"t\" + 1. If at each date-event there exists a complete set of such contracts, one for each contingency that can occur at the following date, individuals will trade these contracts in order to insure against future risks, targeting a desirable and budget feasible level of consumption in each state (i.e. consumption smoothing). In most set ups when these contracts are not available, optimal risk sharing between agents will not be possible. For this scenario, agents (homeowners, workers, firms, investors, etc.) will lack the instruments to insure against future risks such as employment status, health, labor income, prices, among others.\n\nIn a competitive market, each agent makes intertemporal choices in a stochastic environment. Their attitudes toward risk, the production possibility set, and the set of available trades determine the equilibrium quantities and prices of assets that are traded. In an \"idealized\" representation agents are assumed to have costless contractual enforcement and perfect knowledge of future states and their likelihood. With a complete set of state contingent claims (also known as Arrow–Debreu securities) agents can trade these securities to hedge against undesirable or bad outcomes.\n\nWhen a market is incomplete, it typically fails to make the optimal allocation of assets. That is, the First Welfare Theorem no longer holds. The competitive equilibrium in an Incomplete Market is generally constrained suboptimal. The notion of constrained suboptimality was formalized by Geanakoplos and Polemarchakis (1986).\n\nDespite the latest ongoing innovation in financial and insurance markets, markets remain incomplete. While several contingent claims are traded routinely against many states such as insurance policies, futures, financial options, among others, the set of outcomes is far greater than the set of claims.\n\nIn practice the idea of a state contingent security for every possible realization of nature seems unrealistic. For example, if the economy lacks the institutions to guarantee that the contracts are enforced, it is unlikely that agents will either sell or buy these securities.\n\nAnother common way to motivate the absence of state contingent securities is asymmetric information between agents. For example, the realization of labor income for a given individual is private information and it cannot be known without cost by anyone else. If an insurance company cannot verify the individual's labor income, the former would always have the incentive to claim a low realization of income and the market would collapse.\n\nMany authors have argued that modeling incomplete markets and other sorts of financial frictions is crucial to explain the counterfactual predictions of the standard Complete Market models. The most notable example is the equity premium puzzle Mehra and Prescott (1985), where the Complete Market model failed to explain the historical high equity premium and low risk-free rate.\n\nAlong with the Equity premium puzzle other counterfactual implications of the Complete Market model are related to the empirical observations concerning individuals’ consumption, wealth and market transactions. For example, in a Complete Market framework, given that agents can fully insure against idiosyncratic risks, each individual’s consumption must fluctuate as much as anyone else’s, and the relative position in terms wealth distribution of an individual should not vary much over time. The empirical evidence suggests otherwise. Further, the individual consumptions are not highly correlated with each other and wealth holdings are very volatile.\n\nIn the economic and financial literature, a significant effort has been made in recent years to part from the setting of Complete Markets. Market incompleteness is modeled as an exogenous institutional structure or as an endogenous process.\n\nIn the first approach, the economic models take as given the institutions and arrangements observed in actual economies. This approach has two advantages. First the structure of the model is similar to that of the Arrow–Debreu model to make it amenable to the powerful techniques of analysis developed for that framework. Second it is easy to compare model allocations with their empirical counterpart. Among the first papers using this approach, Diamond (1967) focused directly on the “realistic” market structure consisting of the stock and bond markets.\n\nThe other set of models explicitly account for the frictions that could prevent full insurance, but derive the optimal risk-sharing endogenously. This literature has focused on information frictions. Risk sharing in private information models with asset accumulation and enforcement frictions. The advantage of this approach is that market incompleteness and the available state contingent claims respond to the economic environment, which makes the model appealing for policy experiments since it is less vulnerable to the Lucas critique.\n\nSuppose there is an economy with two agents (Robinson and Jane) with identical log utility functions. There are two equally likely states of nature. If state 1 is realized, Robinson is endowed with 1 unit of wealth and Jane with 0. In state 2, Robinson gets 0 while Jane receives 1 unit of wealth. With Complete Markets there are two state contingent claims:\n\nBefore the realization of the uncertainty, the two agents can trade the state contingent securities. In equilibrium, the two Arrow-Debreu securities have the same price and the allocation is as follows:\n\nThe main outcome in this economy is that both Robinson and Jane will end up with 0.5 units of wealth independently of the state of nature that is realized.\n\nIf the market is incomplete, meaning one or both of the securities are not available for trade, the two agents can't trade to hedge against a bad realization of nature and thus remain exposed to the possibility of the undesirable outcome of having zero wealth. In fact, with certainty, one of the agents will be 'rich' and the other 'poor'.\n\nThis example is an extreme case of market incompleteness. In practice, agents do have some type of savings or insurance instrument. The main point here is to illustrate the potential welfare losses that can arise if markets are incomplete.\n\n"}
{"id": "1300939", "url": "https://en.wikipedia.org/wiki?curid=1300939", "title": "Infinite-dimensional optimization", "text": "Infinite-dimensional optimization\n\nIn certain optimization problems the unknown optimal solution might not be a number or a vector, but rather a continuous quantity, for example a function or the shape of a body. Such a problem is an infinite-dimensional optimization problem, because, a continuous quantity cannot be determined by a finite number of certain degrees of freedom.\n\n\nInfinite-dimensional optimization problems can be more challenging than finite-dimensional ones. Typically one needs to employ methods from partial differential equations to solve such problems.\n\nSeveral disciplines which study infinite-dimensional optimization problems are calculus of variations, optimal control and shape optimization.\n\n\n"}
{"id": "14230769", "url": "https://en.wikipedia.org/wiki?curid=14230769", "title": "Integraph", "text": "Integraph\n\nAn Integraph is a mechanical analog computing device for plotting the integral of a graphically defined function.\n\nIt was invented independently about 1880 by the British physicist Sir Charles Vernon Boys and by Bruno Abdank-Abakanowicz, a Polish-Lithuanian mathematician/electrical engineer from the Russian Empire. Abakanowicz's design was constructed by Coradi of Zurich.\n\nThe input to the integraph is a tracing point that is moved to trace the input curve. The output is defined by the path a disk that rolls along the paper without slipping. The mechanism sets the angle of the output disk based on the position of the input curve: if the input is zero, the disk is angled to roll straight, parallel to the x axis on the Cartesian plane. If the input is above zero the disk is angled slightly toward the positive y direction, such that the y value of its position increases as it rolls in that direction. If the input is below zero, the disk is angled the other way such that its y position decreases as it rolls.\n\nThe hardware consists of a rectangular carriage which moves left to right on rollers. Two sides of the carriage run parallel to the x axis. The other two sides are parallel to the y axis. Along the trailing vertical (y axis) rail slides a smaller carriage holding a tracing point. Along the leading vertical rail slides a second smaller carriage to which is affixed a small, sharp disc, which rests and rolls (but does not slide) on the graphing paper. The trailing carriage is connected both with a point in the center of the carriage and the disc on the leading rail by a system of sliding crossheads and wires, such that the tracing point must follow the disc's tangential path.\n\nThe integraph plots (traces) the \"integral curve\"\nwhen we are given the \"differential curve\",\n\nThe mathematical basis of the mechanism depends on the following considerations: For any point of the differential curve, construct the auxiliary triangle with vertices and . The hypotenuse of this right triangle intersects the -axis making an angle the value of whose tangent is . This hypotenuse is parallel to the tangent line of the integral curve at that corresponds to . \n\nThe integraph may be used to obtain a quadrature of the circle. If the differential curve is the unit circle, the integral curve intersects the lines at points that are equally spaced at a distance of /2.\n\n\nGauthier-Villars, 1886 available at Google Books\n"}
{"id": "6964629", "url": "https://en.wikipedia.org/wiki?curid=6964629", "title": "Interaction information", "text": "Interaction information\n\nThe interaction information (McGill 1954), or amounts of information (Hu Kuo Ting, 1962) or co-information (Bell 2003) or information interaction (Licina 2017) is one of several generalizations of the mutual information. In fact, the definition of interaction information is identical to that of multivariate mutual information except for a change in sign in the case of an odd number of random variables except in the work of Licina where information is more like a physical entitiy.\n\nInteraction information expresses the amount information (redundancy or synergy) bound up in a set of variables, \"beyond\" that which is present in any subset of those variables. Unlike the mutual information, the interaction information can be either positive or negative. This confusing property has likely retarded its wider adoption as an information measure in machine learning and cognitive science. These functions, their negativity and minima have a direct interpretation in algebraic topology (Baudot & Bennequin, 2015).\n\nFor three variables formula_1, the interaction information formula_2 is given by\n\nwhere, for example, formula_4 is the mutual information between variables formula_5 and formula_6, and formula_7 is the conditional mutual information between variables formula_5 and formula_6 given formula_10. Formally,\n\nand\n\nIt thus follows that\n\nFor the three-variable case, the interaction information formula_2 is the difference between the information shared by formula_15 when formula_10 has been fixed and when formula_10 has not been fixed. (See also Fano's 1961 textbook.) Interaction information measures the influence of a variable formula_10 on the amount of information shared between formula_15. Because the term formula_7 can be zero — for example, when the\ndependency between formula_21 is due entirely to the influence of a common cause formula_10, the interaction information can be negative as well as positive. Negative interaction information indicates that variable formula_10 inhibits (i.e., \"accounts for\" or \"explains\" some of) the correlation between formula_15, whereas positive interaction information indicates that variable formula_10 facilitates or enhances the correlation between formula_15.\n\nInteraction information is bounded. In the three variable case, it is bounded by\n\nNegative interaction information seems much more natural than positive interaction information in the sense that such \"explanatory\" effects are typical of common-cause structures. For example, clouds cause rain and also block the sun; therefore, the correlation between rain and darkness is partly accounted for by the presence of clouds, formula_28. The result is negative interaction information formula_29.\n\nThe case of positive interaction information seems a bit less natural. A prototypical example of positive formula_2 has formula_5 as the output of an XOR gate to which formula_6 and formula_10 are the independent random inputs. In this case formula_34 will be zero, but formula_35 will be positive (1 bit) since once output formula_5 is known, the value on input formula_6 completely determines the value on input formula_10. Since formula_39, the result is positive interaction information formula_2. It may seem that this example relies on a peculiar ordering of formula_41 to obtain the positive interaction, but the symmetry of the definition for formula_2 indicates that the same positive interaction information results regardless of which variable we consider as the \"interloper\" or conditioning variable. For example, input formula_6 and output formula_5 are also independent until input formula_10 is fixed, at which time they are totally dependent (obviously), and we have the same positive interaction information as before, formula_46.\n\nThis situation is an instance where fixing the \"common effect\" formula_5 of causes formula_6 and formula_10 induces a dependency among the causes that did not formerly exist. This behavior is colloquially referred to as \"explaining away\" and is thoroughly discussed in the Bayesian Network literature (e.g., Pearl 1988). Pearl's example is auto diagnostics: A car's engine can fail to start formula_50 due either to a dead battery formula_51 or due to a blocked fuel pump formula_52. Ordinarily, we assume that battery death and fuel pump blockage are independent events, because of the essential modularity of such automotive systems. Thus, in the absence of other information, knowing whether or not the battery is dead gives us no information about whether or not the fuel pump is blocked. However, if we happen to know that the car fails to start (i.e., we fix common effect formula_5), this information induces a dependency between the two causes \"battery death\" and \"fuel blockage\". Thus, knowing that the car fails to start, if an inspection shows the battery to be in good health, we can conclude that the fuel pump must be blocked.\n\n\"Battery death\" and \"fuel blockage\" are thus dependent, conditional on their common effect \"car starting\". What the foregoing discussion indicates is that the obvious directionality in the common-effect graph belies a deep informational symmetry: If conditioning on a common effect\nincreases the dependency between its two parent causes, then conditioning on one of the causes must create the same increase in dependency between the second cause and the common effect. In Pearl's automotive example, if conditioning on \"car starts\" induces formula_2 bits of dependency between the two causes \"battery dead\" and \"fuel blocked\", then conditioning on\n\"fuel blocked\" must induce formula_2 bits of dependency between \"battery dead\" and \"car starts\". This may seem odd because \"battery dead\" and \"car starts\" are already governed by the implication \"battery dead\" formula_56 \"car doesn't start\". However, these variables are still not totally correlated because the converse is not true. Conditioning on \"fuel blocked\" removes the major alternate cause of failure to start, and strengthens the converse relation and therefore the association between \"battery dead\" and \"car starts\". A paper by Tsujishita (1995) focuses in greater depth on the third-order mutual information.\n\nIf three variables form a Markov chain formula_57, then formula_58 soformula_59\n\nOne can recursively define the \"n\"-dimensional interaction information in terms of the formula_60-dimensional interaction information. For example, the four-dimensional interaction information can be defined as\n\nor, equivalently,\n\nIt is possible to extend all of these results to an arbitrary number of dimensions. The general expression for interaction information on variable set formula_63 in terms of the marginal entropies is given by Hu Kuo Ting (1962), Jakulin & Bratko (2003).\n\nwhich is an alternating (inclusion-exclusion) sum over all subsets formula_65, where formula_66. Note\nthat this is the information-theoretic analog to the Kirkwood approximation.\n\nThe possible negativity of interaction information can be the source of some confusion (Bell 2003). As an example of this confusion, consider a set of eight independent binary variables formula_67. Agglomerate these variables as follows:\n\nBecause the formula_69's overlap each other (are redundant) on the three binary variables formula_70, we would expect the interaction information formula_71 to equal formula_72 bits, which it does. However, consider\nnow the agglomerated variables\n\nThese are the same variables as before with the addition of formula_74. Because the formula_69's now overlap each other (are redundant) on only one binary variable formula_76, we would expect the interaction information formula_77 to equal formula_78 bit. However, formula_77 in this case is actually equal to formula_80 bit,\nindicating a synergy rather than a redundancy. This is correct in the sense that\n\nbut it remains difficult to interpret.\n\n\n"}
{"id": "29094048", "url": "https://en.wikipedia.org/wiki?curid=29094048", "title": "Inverse problem in optics", "text": "Inverse problem in optics\n\nThe inverse problem in optics (or the inverse optics problem) refers to the fundamentally ambiguous mapping between sources of retinal stimulation and the retinal images that are caused by those sources.\n\nFor example, the size of an object, the orientation of the object, and its distance from the observer are conflated in the retinal image. For any given projection on the retina there are an infinite number of pairings of object size, orientation and distance that could have given rise to that projection on the retina. Because the image on the retina does not specify which pairing did in fact cause the image, this and other aspects of vision qualify as an inverse problem.\n"}
{"id": "6948446", "url": "https://en.wikipedia.org/wiki?curid=6948446", "title": "Jaroslav Nešetřil", "text": "Jaroslav Nešetřil\n\nJaroslav (Jarik) Nešetřil (; born March 13, 1946 in Brno) is a Czech mathematician, working at Charles University in Prague. His research areas include combinatorics (structural combinatorics, Ramsey theory), graph theory (coloring problems, sparse structures), algebra (representation of structures, categories, homomorphisms), posets (diagram and dimension problems), computer science (complexity, NP-completeness).\n\nNešetřil received his Ph.D. from Charles University in 1973 under the supervision of Aleš Pultr and Gert Sabidussi. He is responsible for more than 300 publications. Since 2006, he is chairman of the Committee of Mathematics of Czech Republic (the Czech partner of IMU).\n\nJaroslav Nešetřil is Editor in Chief of \"Computer Science Review\" and \"INTEGERS: the Electronic Journal of Combinatorial Number Theory\".\nHe is also honorary editor of \"Electronic Journal of Graph Theory and Applications\". Since 2008, Jaroslav Nešetřil belongs to the Advisory Board of the Academia Sinica.\n\nHe was awarded the state prize (1985 jointly with Vojtěch Rödl) for a collection of papers in Ramsey theory. The book \"Sparsity - Graphs, Structures, and Algorithms\" he co-authored with Patrice Ossona de Mendez was included in ACM Computing Reviews\nlist of \"Notable Books and Articles of 2012\".\n\nNešetřil is a corresponding member of the German Academy of Sciences since 1996 and has been declared Doctor Honoris Causa of the University of Alaska (Fairbanks) in 2002. He has also been declared Doctor Honoris Causa of the University of Bordeaux 1 in 2009; the speech he made in French at this occasion attracted a great deal of attention. He received in 2010 the Medal of Merit of Czech Republic and the Gold medal of Faculty of Mathematics and Physics, Charles University in 2011. In 2012, he has been elected to the Academia Europaea. Also, he has been\nelected honorary member of the Hungarian Academy of Sciences in 2013.\n\nHe was an invited speaker of the European Congress of Mathematics, in Amsterdam, 2008, and invited speaker (by both the Logic and Foundations and Combinatorics sections) at the Combinatorics session of the International Congress of Mathematicians, in Hyderabad, 2010.\n\nIn 2018, on the occasion of the 670th anniversary of the establishment of Charles University, Nešetřil has received from the rector of Charles university the Donatio Universitatis Carolinae prize “for his contribution to mathematics and for his leading role in establishing a world-renowned group in discrete mathematics at Charles University”. \n\n\n"}
{"id": "26271750", "url": "https://en.wikipedia.org/wiki?curid=26271750", "title": "Judgment sample", "text": "Judgment sample\n\nJudgment sample, or Expert sample, is a type of random sample that is selected based on the opinion of an expert. Results obtained from a judgment sample are subject to some degree of bias, due to the frame and population not being identical. The frame is a list of all the units, items, people, etc., that define the population to be studied.\nJudgement sampling is noble to provide detailed information about the difficulties in obtaining the distinction. A random sample would provide less bias, but potentially less raw information. \nThe downfalls of this system are significant as any non-random sample brings bias into question, which limits the types of statistical analyzes you may reasonably perform, and there are considerable limits to an experts ability to choose a good sample. \n"}
{"id": "14390402", "url": "https://en.wikipedia.org/wiki?curid=14390402", "title": "Judith Grabiner", "text": "Judith Grabiner\n\nJudith Victor Grabiner (born October 12, 1938) is an American mathematician and historian of mathematics, who is Flora Sanborn Pitzer Professor Emerita of Mathematics at Pitzer College, one of the Claremont Colleges. Her main interest is in mathematics in the eighteenth and nineteenth centuries.\n\nGrabiner completed a Bachelor of Science degree at the University of Chicago in 1960. She was a graduate student in the history of science at Harvard University, completing a Master of Arts in 1962 and a Ph.D. in 1966, under I. Bernard Cohen. Her PhD dissertation was on Italian mathematician Joseph-Louis Lagrange. Grabiner was an instructor at Harvard for several years, before she and her husband Sandy Grabiner moved to California. She was a professor of history at California State University, Dominguez Hills from 1972 to 1985.\n\nGrabiner joined the mathematics department at Pitzer College in 1985, and has been the Flora Sanborn Pitzer Professor of Mathematics since 1994. Her teaching includes courses on the history of mathematics, mathematics in different cultures, and mathematics and philosophy.\n\nIn 2003, Grabiner received one of the Mathematical Association of America's Deborah and Franklin Haimo Awards for Distinguished College or University Teaching of Mathematics. She became a fellow of the American Mathematical Society in 2012. In 2014, she was awarded the Beckenbach Book Prize.\n\nGrabiner has published journal articles in \"American Mathematical Monthly\", \"Historia Mathematica\", and \"Mathematics Magazine\". She received the Carl B. Allendoerfer Award for the best article in \"Mathematics Magazine\" in 1984, 1989, and 1996, and the Lester R. Ford Award in 1984, 1998, 2005, and 2010, for the best article in \"American Mathematical Monthly\".\n"}
{"id": "3626214", "url": "https://en.wikipedia.org/wiki?curid=3626214", "title": "KL-43", "text": "KL-43\n\nThe KL-43 is a portable, electronic cipher device used by the United States and the NATO from the early 1980s. The machine, manufactured by TRW, is an adaptation of language translator technology, and includes a keyboard for input and an LCD for output. It also contains a built-in modem, a telephone coupler, and the facility for connecting to a printer. A version of the KL-43 was famously used by Oliver North to communicate with his assistant, Fawn Hall, and others while managing clandestine operations in Nicaragua in support of the \"Contra\" rebels. The device was paraded in front of cameras during the Iran-Contra congressional hearings.\n\nThere are a number of variations of the KL-43, including the following:\n\n"}
{"id": "621769", "url": "https://en.wikipedia.org/wiki?curid=621769", "title": "List of amateur mathematicians", "text": "List of amateur mathematicians\n\nThis is a list of amateur mathematicians—people whose primary vocation did not involve mathematics (or any similar discipline) yet made notable, and sometimes important, contributions to the field of mathematics.\n"}
{"id": "31213087", "url": "https://en.wikipedia.org/wiki?curid=31213087", "title": "List of numeral systems", "text": "List of numeral systems\n\nThis is a list of numeral systems, that is, writing systems for expressing numbers.\n\nNumeral systems are classified here as to whether they use positional notation (also known as place-value notation), and further categorized by radix or base.\n\nThe common names are derived somewhat arbitrarily from a mix of Latin and Greek, in some cases including roots from both languages within a single name. There have been some proposals for standardisation.\n\nThe common names of the negative base numeral systems are formed using the prefix \"nega-\", giving names such as:\n\n\nAll known numeral systems developed before the Babylonian numerals are non-positional, as are many developed later, such as the Roman numerals.\n\n"}
{"id": "1519821", "url": "https://en.wikipedia.org/wiki?curid=1519821", "title": "Michael Artin", "text": "Michael Artin\n\nMichael Artin (; born 28 June 1934) is an American mathematician and a professor emeritus in the Massachusetts Institute of Technology mathematics department, known for his contributions to algebraic geometry.\n\nArtin was born in Hamburg, Germany, and brought up in Indiana. His parents were Natalia Naumovna Jasny (Natascha) and Emil Artin, preeminent algebraist of the 20th century. Artin's parents left Germany in 1937, because Michael Artin's maternal grandfather was Jewish.\n\nArtin did his undergraduate studies at Princeton University, receiving an A.B. in 1955; he then moved to Harvard University, where he received a Ph.D. in 1960 under the supervision of Oscar Zariski, defending a thesis about Enriques surfaces.\n\nIn the early 1960s, Artin spent time at the IHÉS in France, contributing to the SGA4 volumes of the Séminaire de géométrie algébrique, on topos theory and étale cohomology. His work on the problem of characterising the representable functors in the category of schemes has led to the Artin approximation theorem, in local algebra. This work also gave rise to the ideas of an algebraic space and algebraic stack, and has proved very influential in moduli theory. Additionally, he has made contributions to the deformation theory of algebraic varieties. He began to turn his interest from algebraic geometry to noncommutative algebra (noncommutative ring theory), especially geometric aspects, after a talk by Shimshon Amitsur and an encounter in Chicago with Claudio Procesi and Lance W. Small, \"which prompted [his] first foray into ring theory\".\n\nIn 2002, Artin won the American Mathematical Society's annual Steele Prize for Lifetime Achievement. In 2005, he was awarded the Harvard Centennial Medal. In 2013, he won the Wolf Prize in Mathematics, and in 2015 was awarded the National Medal of Science. He is also a member of the National Academy of Sciences and a Fellow of the American Academy of Arts and Sciences (1969), the American Association for the Advancement of Science, the Society for Industrial and Applied Mathematics, and the American Mathematical Society.\n\n\n\n\n"}
{"id": "33755616", "url": "https://en.wikipedia.org/wiki?curid=33755616", "title": "Noisy-storage model", "text": "Noisy-storage model\n\nThe noisy-storage model refers to a cryptographic model employed in quantum cryptography. It assumes that the quantum memory device of an attacker (adversary) trying to break the protocol is imperfect (noisy). \nThe main goal of this model is to enable the secure implementation of two-party cryptographic primitives, such as bit commitment, oblivious transfer and secure identification.\n\nQuantum communication has proven to be extremely useful when it comes to distributing encryption keys. It allows two distant parties Alice and Bob to expand a small initial secret key into an arbitrarily long secret key by sending qubits (quantum bits) to each other. Most importantly, it can be shown that any eavesdropper trying to listen into their communication cannot intercept any information about the long key. This is known as quantum key distribution (QKD).\n\nYet, it has been shown that even quantum communication does not allow the secure implementation of many other two-party cryptographic tasks. These all form instances of secure function evaluation. An example is oblivious transfer. What sets these tasks apart from key distribution is that they aim to solve problems between two parties, Alice and Bob, who do \"not\" trust each other. That is, there is no outside party like an eavesdropper, only Alice and Bob. Intuitively, it is this lack of trust that makes the problem hard. Unlike in quantum key distribution, Alice and Bob cannot collaborate to try and detect any eavesdropping activity. Instead, each party has to fend for himself.\n\nSince tasks like secure identification are of practical interest, one is willing to make assumptions on how powerful the adversary can be. Security then holds as long as these assumptions are satisfied. In classical cryptography, i.e., without the use of quantum tools, most of these are computational assumptions. Such assumptions consists of two parts. First, one assumes that a particular problem is difficult to solve. For example, one might assume that it is hard to factor a large integer into its prime factors (e.g. 15=5x3). Second, one assumes that the adversary has a limited amount of computing power, namely less than what is (thought to be) required to solve the chosen problem.\n\nIn information theoretic cryptography physical assumptions appear, which do not rely on any hardness assumptions, but merely assume a limit on some other resource. In classical cryptography, the bounded-storage model introduced by Ueli Maurer assumes that the adversary can only store a certain number of classical bits. Protocols are known that do (in principle) allow the secure implementation of any cryptographic task as long as the adversary's storage is small. Very intuitively, security becomes possible under this assumption since the adversary has to make a choice which information to keep. That is, the protocol effectively overflows his memory device leading to an inevitable lack on information for the adversary. It was later discovered that any classical protocol which requires the honest parties to store formula_1 bits in order to execute it successfully can be broken by an adversary that can store more than about formula_2 bits. That is, the gap between what is required to execute the protocol, and what is required to break the security is relatively small.\n\nThis gap changes dramatically when using quantum communication\n. That is, Alice and Bob can send qubits to each other as part of the protocol. Likewise, one now assumes that the adversary's quantum storage is limited to a certain number of qubits. There is no restriction on how many classical bits the adversary can store. This is known as the bounded-\"quantum\"-storage model. It was shown that there exist quantum protocols in which the honest parties need \"no\" quantum storage at all to execute them, but are nevertheless secure as long as Alice transmits more than twice the number of qubits than the adversary can store.\n\nMore generally, security is possible as long as the amount of information that the adversary can store in his memory device is limited. This intuition is captured by the noisy-storage model, which includes the bounded-quantum-storage model as a special case. Such a limitation can, for example, come about if the memory device is extremely large, but very imperfect. In information theory such an imperfect memory device is also called a noisy channel. The motivation for this more general model is threefold. First, it allows one to make statements about much more general memory devices that the adversary may have available. Second, security statements could be made when the signals transmitted, or the storage device itself, uses continuous variables whose dimension is infinite and thus cannot be captured by a bounded storage assumption without additional constraints. Third, even if the dimension of the signals itself is small, the noisy-storage analysis allows security beyond the regime where bounded-storage itself can make any security statement. For example, if the storage channel is entanglement breaking, security is possible even if the storage device is arbitrarily large (i.e., not bounded in any way).\n\nThe assumption of the noisy-storage model is that during waiting times formula_3 introduced into the protocol, the adversary can only store quantum information in his noisy memory device. Such a device is simply a quantum channel formula_4 that takes input states formula_5 to some noisy output states formula_6. Otherwise, the adversary is all powerful. For example, he can store an unlimited amount of classical information and perform any computation instantaneously.\nThe latter assumption also implies that he can perform any form of error correcting encoding before and after using the noisy memory device, even if it is computationally very difficult to do (i.e., it requires a long time). In this context, this is generally referred to as an encoding attack formula_7 and a decoding attack formula_8. Since the adversary's classical memory can be arbitrarily large, the encoding formula_7 may not only generate some quantum state as input to the storage device formula_10 but also output classical information. The adversary's decoding attack formula_8 can make use of this extra classical information, as well as any additional information that the adversary may gain after the waiting time has passed.\n\nIn practise, one often considers storage devices that consist of formula_12 memory cells, each of which is subject to noise. In information-theoretic terms, this means that the device has the form formula_13, where formula_14 is a noisy quantum channel acting on a memory cell of dimension formula_15.\n\n\nMost protocols proceed in two steps. First, Alice and Bob exchange formula_1 qubits encoded in two or three mutually unbiased bases. These are the same encodings which are used in the BB84 or six-state protocols of quantum key distribution. Typically, this takes the form of Alice sending such qubits to Bob, and Bob measuring them immediately on arrival. This has the advantage that Alice and Bob need no quantum storage to execute the protocol. It is furthermore experimentally relatively easy to create such qubits, making it possible to implement such protocols using currently available technology.\n\nThe second step is to perform classical post-processing of the measurement data obtained in step one. Techniques used depend on the protocol in question and include privacy amplification, error-correcting codes, min-entropy sampling, and interactive hashing.\n\nTo demonstrate that all two-party cryptographic tasks can be implemented securely, a common approach is to show that a simple cryptographic primitive can be implemented that is known to be \"universal\" for secure function evaluation. That is, once one manages to build a protocol for such a cryptographic primitive all other tasks can be implemented by using this primitive as a basic building block. One such primitive is oblivious transfer. In turn, oblivious transfer can be constructed from an even simpler building block known as weak string erasure in combination with cryptographic techniques such as privacy amplification.\n\nAll protocols proposed to date allow one of the parties (Alice) to have even an unlimited amount of noise-free quantum memory. I.e., the noisy-storage assumption is applied to only one of the parties (Bob). For storage devices of the form formula_13 it is known that any two-party cryptographic task can be implemented securely by means of weak string erasure and oblivious transfer whenever any of the following conditions hold.\n\n\n\n\nThe three mutually unbiased bases are the same encodings as in the six-state protocol of quantum key distribution. The last condition does form the best known condition for most channels, yet the quantum capacity as well as the strong converse parameter are generally not easy to determine.\n\nUsing such basic primitives as building blocks is not always the most efficient way to solve a cryptographic task. Specialized protocols targeted to solve specific problems are generally more efficient. Examples of known protocols are\n\n\nThe assumption of bounded-quantum-storage has also been applied outside the realm of secure function evaluation. In particular, it has been shown that if the eavesdropper in quantum key distribution is memory bounded, higher bit error rates can be tolerated in an experimental implementation.\n"}
{"id": "2533474", "url": "https://en.wikipedia.org/wiki?curid=2533474", "title": "Padovan polynomials", "text": "Padovan polynomials\n\nIn mathematics, Padovan polynomials are a generalization of Padovan sequence numbers. These polynomials are defined by:\n\nThe first few Padovan polynomials are:\n\nThe Padovan numbers are recovered by evaluating the polynomials P(\"x\") at \"x\" = 1.\n\nEvaluating P(\"x\") at \"x\" = 2 gives the \"n\"th Fibonacci number plus (-1). \n\nThe ordinary generating function for the sequence is\n\n"}
{"id": "2829628", "url": "https://en.wikipedia.org/wiki?curid=2829628", "title": "Pendent", "text": "Pendent\n\nPendent is an adjective that describes the condition of hanging, either literally, or figuratively, as in undecided or incomplete. To be distinguished from the spelling \"pendant\" which is the noun. \n\nSomething pendent may be viewed as any member of a support system (e.g. a section of a dome or, organically, a parent/guardian in a nuclear family). A pendent component of a structure or system requires one or more of the same as itself to be functional. For example, one playing card in a house of cards requires another against it in order to maintain stability. Likewise the segments of certain types of dome rely upon each other for support, as do the individual blocks or timber frames which make up a dome whether segmented or not. The whole dome may in turn be supported by pendentives (which in turn support each other). In the construction of arches and domes, the pendent condition commonly leads to special requirements for timber centring or similar expedients during construction: when the structure is completed it becomes self-supporting and the temporary structure can be removed.\n"}
{"id": "8890014", "url": "https://en.wikipedia.org/wiki?curid=8890014", "title": "Perfect totient number", "text": "Perfect totient number\n\nIn number theory, a perfect totient number is an integer that is equal to the sum of its iterated totients. That is, we apply the totient function to a number \"n\", apply it again to the resulting totient, and so on, until the number 1 is reached, and add together the resulting sequence of numbers; if the sum equals \"n\", then \"n\" is a perfect totient number. Or to put it algebraically, if\nwhere\nis the iterated totient function and \"c\" is the integer such that\nthen \"n\" is a perfect totient number.\n\nThe first few perfect totient numbers are\n\nFor example, start with 327. Then φ(327) = 216, φ(216) = 72, φ(72) = 24, φ(24) = 8, φ(8) = 4, φ(4) = 2, φ(2) = 1, and 216 + 72 + 24 + 8 + 4 + 2 + 1 = 327.\n\nIt can be observed that many perfect totient are multiples of 3; in fact, 4375 is the smallest perfect totient number that is not divisible by 3. All powers of 3 are perfect totient numbers, as may be seen by induction using the fact that\n\nVenkataraman (1975) found another family of perfect totient numbers: if \"p\" = 4×3+1 is prime, then 3\"p\" is a perfect totient number. The values of \"k\" leading to perfect totient numbers in this way are\n\nMore generally if \"p\" is a prime number greater than 3, and 3\"p\" is a perfect totient number, then \"p\" ≡ 1 (mod 4) (Mohan and Suryanarayana 1982). Not all \"p\" of this form lead to perfect totient numbers; for instance, 51 is not a perfect totient number. Iannucci et al. (2003) showed that if 9\"p\" is a perfect totient number then \"p\" is a prime of one of three specific forms listed in their paper. It is not known whether there are any perfect totient numbers of the form 3\"p\" where \"p\" is prime and \"k\" > 3.\n"}
{"id": "57498426", "url": "https://en.wikipedia.org/wiki?curid=57498426", "title": "Periodic table of topological invariants", "text": "Periodic table of topological invariants\n\nThe periodic table of topological invariants is an application of topology to physics. It indicates the group of topological invariant for topological insulators and superconductors in each dimension and in each discrete symmetry class.\n\nThere are ten discrete symmetry classes of topological insulators and superconductors, corresponding to the ten Altland–Zirnbauer classes of random matrices. They are defined by three symmetries of the Hamiltonian formula_1, (where formula_2, and formula_3, are the annihilation and creation operators of mode formula_4, in some arbitrary spatial basis) : time reversal symmetry, particle hole (or charge conjugation) symmetry, and chiral (or sublattice) symmetry.\n\nChiral symmetry is a unitary operator formula_5, that acts on formula_2, as a unitary rotation (formula_7,) and satisfies formula_8. A Hamiltonian formula_9 possesses chiral symmetry when formula_10, for some choice of formula_5 (on the level of first-quantised Hamiltonians, this means formula_12 and formula_9 are anticommuting matrices).\n\nTime reversal is an antiunitary operator formula_14, that acts on formula_15, (where formula_16, is an arbitrary complex coefficient, and formula_17, denotes complex conjugation) as formula_18. It can be written as formula_19 where formula_20 is the complex conjugation operator and formula_21 is a unitary matrix. Either formula_22 or formula_23. A Hamiltonian with time reversal symmetry satisfies formula_24, or on the level of first-quantised matrices, formula_25, for some choice of formula_21.\n\nCharge conjugation formula_27 is also an antiunitary operator which acts on formula_15 as formula_29, and can be written as formula_30 where formula_31 is unitary. Again either formula_32 or formula_33 depending on what formula_31 is. A Hamiltonian with particle hole symmetry satisfies formula_35, or on the level of first-quantised Hamiltonian matrices, formula_36, for some choice of formula_31.\n\nIn the Bloch Hamiltonian formalism for periodic crystals, where the Hamiltonian formula_38 acts on modes of crystal momentum formula_39, the chiral symmetry, TRS, and PHS conditions become formula_40, formula_41 and formula_42.\n\nIt is evident that if two of these three symmetries are present, then the third is also present, due to the relation formula_43.\n\nThe aforementioned discrete symmetries label 10 distinct discrete symmetry classes, which coincide with the Altland–Zirnbauer classes of random matrices.\n\nA bulk Hamiltonian in a particular symmetry group is restricted to be a Hermitian matrix with no zero-energy eigenvalues (i.e. so that the spectrum is \"gapped\" and the system is a bulk insulator) satisfying the symmetry constraints of the group. In the case of formula_44 dimensions, this Hamiltonian is a continuous function formula_38 of the formula_46 parameters in the Bloch momentum vector formula_47 in the Brillouin zone; then the symmetry constraints must hold for all formula_48.\n\nGiven two Hamiltonians formula_49 and formula_50, it may be possible to continuously deform formula_49 into formula_50 while maintaining the symmetry constraint and gap (that is, there exists continuous function formula_53 such that for all formula_54 the Hamiltonian has no zero eigenvalue and symmetry condition is maintained, and formula_55 and formula_56). Then we say that formula_49 and formula_50 are equivalent.\n\nHowever, it may also turn out that there is no such continuous deformation. in this case, physically if two materials with bulk Hamiltonians formula_49 and formula_50 respectively neighbour each other with an edge between them, when one continuously moves across the edge one must encounter a zero eigenvalue (as there is no continuous transformation that avoids this). This may manifest as a gapless zero energy edge mode or an electric current that only flows along the edge.\n\nAn interesting question is to ask, given a symmetry class and a dimension of the Brillouin zone, what are all the equivalence classes of Hamiltonians. Each equivalence class can be labeled by a topological invariant; two Hamiltonians whose topological invariant are different cannot be deformed into each other and belong to different equivalence classes.\n\nFor each of the symmetry classes, the question can be simplified by deforming the Hamiltonian into a \"projective\" Hamiltonian, and considering the symmetric space in which such Hamiltonians live. These classifying spaces are shown for each symmetry class:\n\nFor example, a (real symmetric) Hamiltonian in symmetry class AI can have its formula_61 positive eigenvalues deformed to +1 and its formula_62 negative eigenvalues deformed to -1; the resulting such matrices are described by the union of real Grassmannians formula_63\n\nThe strong topological invariants of a many-band system in formula_46 dimensions can be labeled by the elements of the formula_46-th homotopy group of the symmetric space. These groups are displayed in this table, called the periodic table of topological insulators:\n\nThere may also exist weak topological invariants (associated to the fact that the suspension of the Brillouin zone is in fact equivalent to a formula_66 sphere wedged with lower-dimensional spheres), which are not included in this table. Furthermore, the table assumes the limit of an infinite number of bands, i.e. involves formula_67 Hamiltonians for formula_68.\n\nThe table also is periodic in the sense that the group of invariants in formula_46 dimensions is the same as the group of invariants in formula_70 dimensions. In the case of no antiunitary symmetries, the invariant groups are periodic in dimension by 2.\n\nFor nontrivial symmetry classes, the actual invariant can be defined by one of the following integrals over all or part of the Brillouin zone: the Chern number, the Wess Zumino winding number, the Chern–Simons invariant, the Fu–Kane invariant.\n\nThe periodic table also displays a peculiar property: the invariant groups in formula_46 dimensions are identical to those in formula_72 dimensions but in a different symmetry class. Among the complex symmetry classes, the invariant group for A in formula_46 dimensions is the same as that for AIII in formula_72 dimensions, and vice versa. One can also imagine arranging each of the eight real symmetry classes on the Cartesian plane such that the formula_75 coordinate is formula_76 if time reversal symmetry is present and formula_77 if it is absent, and the formula_78 coordinate is formula_79 if particle hole symmetry is present and formula_77 if it is absent. Then the invariant group in formula_46 dimensions for a certain real symmetry class is the same as the invariant group in formula_72 dimensions for the symmetry class directly one space clockwise. This phenomenon was termed the \"Bott Clock\" by Kitaev, in reference to the Bott periodicity theorem.\n\nThe Bott Clock can be understood by considering the problem of Clifford algebra extensions. Near an interface between two inequivalent bulk materials, the Hamiltonian approaches a gap closing. To lowest order expansion in momentum slightly away from the gap closing, the Hamiltonian takes the form of a Dirac Hamiltonian formula_83. Here, formula_84 are a representation of the Clifford Algebra formula_85, while formula_86 is an added \"mass term\" that and anticommutes with the rest of the Hamiltonian and vanishes at the interface (thus giving the interface a gapless edge mode at formula_87). The formula_86 term for the Hamiltonian on one side of the interface cannot be continuously deformed into the formula_86 term for the Hamiltonian on the other side of the interface. Thus (letting formula_90 be an arbitrary positive scalar) the problem of classifying topological invariants reduces to the problem of classifying all possible inequivalent choices of formula_91 to extend the Clifford algebra to one higher dimension, while maintaining the symmetry constraints.\n"}
{"id": "58528288", "url": "https://en.wikipedia.org/wiki?curid=58528288", "title": "Poincaré Medal", "text": "Poincaré Medal\n\nThe Poincaré Medal (\"Médaille Poincaré\") is a mathematics award from the Institut de France, Academie des Sciences, Fondation Henri Poincaré. The medal recognizes an eminent mathematician and is awarded only on exceptional occasions. It was established in 1914 and was eliminated in 1997 in favor of the Grande Médaille.\n\n\n"}
{"id": "2404544", "url": "https://en.wikipedia.org/wiki?curid=2404544", "title": "Poincaré recurrence theorem", "text": "Poincaré recurrence theorem\n\nIn physics, the Poincaré recurrence theorem states that certain systems will, after a sufficiently long but finite time, return to a state very close to, if not exactly the same as, the initial state. The Poincaré recurrence time is the length of time elapsed until the recurrence; this time may vary greatly depending on the exact initial state and required degree of closeness. The result applies to isolated mechanical systems subject to some constraints, e.g., all particles must be bound to a finite volume. The theorem is commonly discussed in the context of ergodic theory, dynamical systems and statistical mechanics.\n\nThe theorem is named after Henri Poincaré, who discussed it in 1890 and proved by Constantin Carathéodory using measure theory in 1919.\n\nAny dynamical system defined by an ordinary differential equation determines a flow map \"f\" mapping phase space on itself. The system is said to be volume-preserving if the volume of a set in phase space is invariant under the flow. For instance, all Hamiltonian systems are volume-preserving because of Liouville's theorem. The theorem is then: If a flow preserves volume and has only bounded orbits, then for each open set there exist orbits that intersect the set infinitely often.\n\nThe proof, speaking qualitatively, hinges on two premises:\n\nImagine any finite starting volume of phase space and follow its path under dynamics of the system. The volume \"sweeps\" points of phase space as it evolves, and the \"front\" of this sweeping has a constant size. Over time the explored phase volume (known as a \"phase tube\") grows linearly, at least at first. But, because the accessible phase volume is finite, the phase tube volume must eventually saturate because it cannot grow larger than the accessible volume. This means that the phase tube must intersect itself. In order to intersect itself, however, it must do so by first passing through the starting volume. Therefore, at least a finite fraction of the starting volume is recurring.\n\nNow, consider the size of the non-returning portion of the starting phase volume—that portion that never returns to the starting volume. Using the principle just discussed in the last paragraph, we know that if the non-returning portion is finite, then a finite part of the non-returning portion must return. But that would be a contradiction, since any part of the non-returning portion that returns, also returns to the original starting volume. Thus, the non-returning portion of the starting volume cannot be finite and must be infinitely smaller than the starting volume itself. Q.E.D..\n\nThe theorem does not comment on certain aspects of recurrence which this proof cannot guarantee:\n\nLet \n\nbe a finite measure space and let \n\nbe a measure-preserving transformation. Below are two alternative statements of the theorem.\n\nFor any formula_3, the set of those points formula_4 of formula_5 for which there exists formula_6 such that formula_7 for all formula_8 has zero measure. That is, almost every point of formula_5 returns to formula_5. In fact, almost every point returns infinitely often; \"i.e.\"\n\nFor a proof, see .\n\nThe following is a topological version of this theorem:\n\nIf formula_12 is a second-countable Hausdorff space and formula_13 contains the Borel sigma-algebra, then the set of recurrent points of formula_14 has full measure. That is, almost every point is recurrent.\n\nFor a proof, see \n\nFor quantum mechanical systems with discrete energy eigenstates, a similar theorem holds. For every formula_15 and formula_16 there exists a time \"T\" larger than formula_17, such that formula_18, where formula_19 denotes the state vector of the system at time \"t\".\n\nThe essential elements of the proof are as follows. The system evolves in time according to:\n\nwhere the formula_21 are the energy eigenvalues (we use natural units, so formula_22 ), and the formula_23 are the energy eigenstates. The squared norm of the difference of the state vector at time \"formula_24\" and time zero, can be written as:\n\nWe can truncate the summation at some \"n\" = \"N\" independent of \"T\", because\n\nformula_26\n\nwhich can be made arbitrarily small because the summation formula_27, being the squared norm of the initial state, converges to 1.\n\nThat the finite sum\n\ncan be made arbitrarily small, follows from the existence of integers formula_29 such that \n\nfor arbitrary formula_31. This implies that there exists intervals for T on which \n\nOn such intervals, we have:\n\nThe state vector thus returns arbitrarily closely to the initial state, infinitely often.\n\n\n"}
{"id": "16018931", "url": "https://en.wikipedia.org/wiki?curid=16018931", "title": "Reports on Mathematical Physics", "text": "Reports on Mathematical Physics\n\nReports on Mathematical Physics () is a peer-reviewed scientific journal, started in 1970, which publishes papers in theoretical physics that present a rigorous mathematical approach to problems of quantum and classical mechanics and field theories, relativity and gravitation, statistical physics, and the mathematical foundations of physical theories. The editor-in-chief of this journal is A. Jamiolkowski. The impact factor of this journal was 1.042 in 2013. The SCImago Journal Rank (SJR) of this journal was 0.454 in 2013\n\n"}
{"id": "39789", "url": "https://en.wikipedia.org/wiki?curid=39789", "title": "Rotation", "text": "Rotation\n\nA rotation is a circular movement of an object around a center (or point) of rotation. A three-dimensional object can always be rotated around an infinite number of imaginary lines called \"rotation axes\" ( ). If the axis passes through the body's center of mass, the body is said to rotate upon itself, or spin. A rotation about an external point, e.g. the Earth about the Sun, is called a revolution or orbital revolution, typically when it is produced by gravity. The axis is called a pole.\n\nMathematically, a rotation is a rigid body movement which, unlike a translation, keeps a point fixed. This definition applies to rotations within both two and three dimensions (in a plane and in space, respectively.)\n\nAll rigid body movements are rotations, translations, or combinations of the two.\n\nA rotation is simply a progressive radial orientation to a common point. That common point lies within the axis of that motion. The axis is 90 degrees perpendicular to the plane of the motion. If the axis of the rotation lies external of the body in question then the body is said to orbit. There is no fundamental difference between a “rotation” and an “orbit” and or \"spin\". The key distinction is simply where the axis of the rotation lies, either within or outside of a body in question. This distinction can be demonstrated for both “rigid” and “non rigid” bodies.\n\nIf a rotation around a point or axis is followed by a second rotation around the same point/axis, a third rotation results. The reverse (inverse) of a rotation is also a rotation. Thus, the rotations around a point/axis form a group. However, a rotation around a point or axis and a rotation around a different point/axis may result in something other than a rotation, e.g. a translation.\n\nRotations around the \"x\", \"y\" and \"z\" axes are called \"principal rotations\". Rotation around any axis can be performed by taking a rotation around the \"x\" axis, followed by a rotation around the \"y\" axis, and followed by a rotation around the \"z\" axis. That is to say, any spatial rotation can be decomposed into a combination of principal rotations.\n\nIn flight dynamics, the principal rotations are known as \"yaw\", \"pitch\", and \"roll\" (known as Tait–Bryan angles). This terminology is also used in computer graphics.\n\nIn astronomy, rotation is a commonly observed phenomenon. Stars, planets and similar bodies all spin around on their axes. The rotation rate of planets in the solar system was first measured by tracking visual features. Stellar rotation is measured through Doppler shift or by tracking active surface features.\n\nThis rotation induces a centrifugal acceleration in the reference frame of the Earth which slightly counteracts the effect of gravity the closer one is to the equator. One effect is that an object weighs slightly less at the equator. Another is that the Earth is slightly deformed into an oblate spheroid.\n\nAnother consequence of the rotation of a planet is the phenomenon of precession. Like a gyroscope, the overall effect is a slight \"wobble\" in the movement of the axis of a planet. Currently the tilt of the Earth's axis to its orbital plane (obliquity of the ecliptic) is 23.44 degrees, but this angle changes slowly (over thousands of years). (See also Precession of the equinoxes and Pole star.)\n\nWhile revolution is often used as a synonym for rotation, in many fields, particularly astronomy and related fields, revolution, often referred to as orbital revolution for clarity, is used when one body moves around another while rotation is used to mean the movement around an axis. Moons revolve around their planet, planets revolve about their star (such as the Earth around the Sun); and stars slowly revolve about their galaxial center. The motion of the components of galaxies is complex, but it usually includes a rotation component.\n\nMost planets in our solar system, including Earth, spin in the same direction as they orbit the Sun. The exceptions are Venus and Uranus. Uranus rotates nearly on its side relative to its orbit. Current speculation is that Uranus started off with a typical prograde orientation and was knocked on its side by a large impact early in its history. Venus may be thought of as rotating slowly backwards (or being \"upside down\"). The dwarf planet Pluto (formerly considered a planet) is anomalous in this and other ways.\n\nThe speed of rotation is given by the angular frequency (rad/s) or frequency (turns per time), or period (seconds, days, etc.). The time-rate of change of angular frequency is angular acceleration (rad/s²), caused by torque. The ratio of the two (how heavy is it to start, stop, or otherwise change rotation) is given by the moment of inertia.\n\nThe angular velocity vector (an \"axial vector\") also describes the direction of the axis of rotation. Similarly the torque is an axial vector.\n\nThe physics of the rotation around a fixed axis is mathematically described with the axis–angle representation of rotations. According to the right-hand rule, the direction away from the observer is associated with clockwise rotation and the direction towards the observer with counterclockwise rotation, like a screw.\n\nThe laws of physics are currently believed to be invariant under any fixed rotation. (Although they do appear to change when viewed from a rotating viewpoint: see rotating frame of reference.)\n\nIn modern physical cosmology, the cosmological principle is the notion that the distribution of matter in the universe is homogeneous and isotropic when viewed on a large enough scale, since the forces are expected to act uniformly throughout the universe and have no preferred direction, and should, therefore, produce no observable irregularities in the large scale structuring over the course of evolution of the matter field that was initially laid down by the Big Bang.\n\nIn particular, for a system which behaves the same regardless of how it is oriented in space, its Lagrangian is rotationally invariant. According to Noether's theorem, if the action (the integral over time of its Lagrangian) of a physical system is invariant under rotation, then angular momentum is conserved.\n\nEuler rotations provide an alternative description of a rotation. It is a composition of three rotations defined as the movement obtained by changing one of the Euler angles while leaving the other two constant. Euler rotations are never expressed in terms of the external frame, or in terms of the co-moving rotated body frame, but in a mixture. They constitute a mixed axes of rotation system, where the first angle moves the line of nodes around the external axis \"z\", the second rotates around the line of nodes and the third one is an intrinsic rotation around an axis fixed in the body that moves.\n\nThese rotations are called precession, nutation, and \"intrinsic rotation\".\n\nIn flight dynamics, the principal rotations described with Euler angles above are known as \"pitch\", \"roll\" and \"yaw\". The term rotation is also used in aviation to refer to the upward pitch (nose moves up) of an aircraft, particularly when starting the climb after takeoff.\n\nPrincipal rotations have the advantage of modelling a number of physical systems such as gimbals, and joysticks, so are easily visualised, and are a very compact way of storing a rotation. But they are difficult to use in calculations as even simple operations like combining rotations are expensive to do, and suffer from a form of gimbal lock where the angles cannot be uniquely calculated for certain rotations.\n\nMany amusement rides provide rotation. A Ferris wheel has a horizontal central axis, and parallel axes for each gondola, where the rotation is opposite, by gravity or mechanically. As a result, at any time the orientation of the gondola is upright (not rotated), just translated. The tip of the translation vector describes a circle. A carousel provides rotation about a vertical axis. Many rides provide a combination of rotations about several axes. In Chair-O-Planes the rotation about the vertical axis is provided mechanically, while the rotation about the horizontal axis is due to the centripetal force. In roller coaster inversions the rotation about the horizontal axis is one or more full cycles, where inertia keeps people in their seats.\n\nRotation of a ball or other object, usually called \"spin\", plays a role in many sports, including topspin and backspin in tennis, \"English\", \"follow\" and \"draw\" in billiards and pool, curve balls in baseball, spin bowling in cricket, flying disc sports, etc. Table tennis paddles are manufactured with different surface characteristics to allow the player to impart a greater or lesser amount of spin to the ball.\n\nRotation of a player one or more times around a vertical axis may be called \"spin\" in figure skating, \"twirling\" (of the baton or the performer) in baton twirling, or \"360\", \"540\", \"720\", etc. in snowboarding, etc. Rotation of a player or performer one or more times around a horizontal axis may be called a flip, roll, somersault, \"heli\", etc. in gymnastics, waterskiing, or many other sports, or a \"one-and-a-half\", \"two-and-a-half\", \"gainer\" (starting facing away from the water), etc. in diving, etc. A combination of vertical and horizontal rotation (back flip with 360°) is called a \"möbius\" in waterskiing freestyle jumping.\n\nRotation of a player around a vertical axis, generally between 180 and 360 degrees, may be called a \"spin move\" and is used as a deceptive or avoidance maneuver, or in an attempt to play, pass, or receive a ball or puck, etc., or to afford a player a view of the goal or other players. It is often seen in hockey, basketball, football of various codes, tennis, etc.\n\nThe \"end result\" of any sequence of rotations of any object in 3D about a fixed point is always equivalent to a rotation about an axis. However, an object may \"physically\" rotate in 3D about a fixed point on more than one axis simultaneously, in which case there is no single fixed axis of rotation - just the fixed point. However, these two descriptions can be reconciled - such a physical motion can always be re-described in terms of a single axis of rotation, provided the orientation of that axis relative to the object is allowed to change moment by moment.\n\n2 dimensional rotations, unlike the 3 dimensional ones, possess no axis of rotation. This is equivalent, for linear transformations, with saying that there is no direction in the place which is kept unchanged by a 2 dimensional rotation, except, of course, the identity.\n\nThe question of the existence of such a direction is the question of existence of an eigenvector for the matrix A representing the rotation. Every 2D rotation around the origin through an angle formula_1 in counterclockwise direction can be quite simply represented by the following matrix:\n\nA standard eigenvalue determination leads to the characteristic equation \nwhich has \nas its eigenvalues. Therefore, there is no real eigenvalue, meaning that no real vector in the plane is kept unchanged by A.\n\nKnowing that the trace is an invariant, the rotation angle formula_5 for a proper orthogonal 3x3 rotation matrix formula_6 is found by \n\nformula_7\n\nUsing the principal arc-cosine, this formula gives a rotation angle satisfying formula_8. The corresponding rotation axis must be defined to point in a direction that limits the rotation angle to not exceed 180 degrees. (This can always be done because any rotation of more than 180 degrees about an axis formula_9 can always be written as a rotation having formula_8 if the axis is replaced with formula_11.)\n\nEvery proper rotation formula_6 in 3D space has an axis of rotation, which is defined such that any vector formula_13 that is aligned with the rotation axis will not be affected by rotation. Accordingly, formula_14, and the rotation axis therefore corresponds to an eigenvector of the rotation matrix associated with an eigenvalue of 1. As long as the rotation angle formula_5 is nonzero (i.e., the rotation is not the identity tensor), there is one and only one such direction. Because A has only real components, there is at least one real eigenvalue, and the remaining two eigenvalues must be complex conjugates of each other (see Eigenvalues and eigenvectors#Eigenvalues and the characteristic polynomial). Knowing that 1 is an eigenvalue, it follows that the remaining two eigenvalues are complex conjugates of each other, but this does not imply that they are complex -- they could be real with double multiplicity. In the degenerate case of a rotation angle formula_16, the remaining two eigenvalues are both equal to -1. In the degenerate case of a zero rotation angle, the rotation matrix is the identity, and all three eigenvalues are 1 (which is the only case for which the rotation axis is arbitrary).\n\nA spectral analysis is not required to find the rotation axis. If formula_17 denotes the unit eigenvector aligned with the rotation axis, and if formula_5 denotes the rotation angle, then it can be shown that formula_19. Consequently, the expense of an eigenvalue analysis can be avoided by simply normalizing this vector \"if it has a nonzero magnitude.\" On the other hand, if this vector has a zero magnitude, it means that formula_20. In other words, this vector will be zero if and only if the rotation angle is 0 or 180 degrees, and the rotation axis may be assigned in this case by normalizing any column of formula_21 that has a nonzero magnitude.\n\nThis discussion applies to a proper rotation, and hence formula_22. Any improper orthogonal 3x3 matrix formula_23 may be written as formula_24, in which formula_6 is proper orthogonal. That is, any improper orthogonal 3x3 matrix may be decomposed as a proper rotation (from which an axis of rotation can be found as described above) followed by an inversion (multiplication by -1). It follows that the rotation axis of formula_6 is also the eigenvector of formula_23 corresponding to an eigenvalue of -1.\n\nAs much as every tridimensional rotation has a rotation axis, also every tridimensional rotation has a plane, which is perpendicular to the rotation axis, and which is left invariant by the rotation. The rotation, restricted to this plane, is an ordinary 2D rotation.\n\nThe proof proceeds similarly to the above discussion. First, suppose that all eigenvalues of the 3D rotation matrix A are real. This means that there is an orthogonal basis, made by the corresponding eigenvectors (which are necessarily orthogonal), over which the effect of the rotation matrix is just stretching it. If we write A in this basis, it is diagonal; but a diagonal orthogonal matrix is made of just +1's and -1's in the diagonal entries. Therefore, we don't have a proper rotation, but either the identity or the result of a sequence of reflections.\n\nIt follows, then, that a proper rotation has some complex eigenvalue. Let v be the corresponding eigenvector. Then, as we showed in the previous topic, formula_28 is also an eigenvector, and formula_29 and formula_30 are such that their scalar product vanishes:\n\nbecause, since formula_32 is real, it equals its complex conjugate formula_33, and formula_34 and formula_35 are both representations of the same scalar product between formula_36 and formula_28.\n\nThis means formula_29 and formula_30 are orthogonal vectors. Also, they are both real vectors by construction. These vectors span the same subspace as formula_36 and formula_28, which is an invariant subspace under the application of A. Therefore, they span an invariant plane.\n\nThis plane is orthogonal to the invariant axis, which corresponds to the remaining eigenvector of A, with eigenvalue 1, because of the orthogonality of the eigenvectors of A.\n\n"}
{"id": "49067714", "url": "https://en.wikipedia.org/wiki?curid=49067714", "title": "Rūsiņš Mārtiņš Freivalds", "text": "Rūsiņš Mārtiņš Freivalds\n\nRūsiņš Mārtiņš Freivalds (10 November 1942 – 4 January 2016) was a Latvian computer scientist and mathematician. He was a member of Latvian Academy of Sciences from 1992. He discovered Freivalds' algorithm for checking the correctness of matrix products. He also taught at the University of Latvia with students including Daina Taimina and Andris Ambainis. He was born in Cesvaine and studied at Moscow State University (MSU).\n\nFreivalds died from a heart attack on 4 January 2016 in Riga, aged 73.\n"}
{"id": "17597677", "url": "https://en.wikipedia.org/wiki?curid=17597677", "title": "Semi-infinite programming", "text": "Semi-infinite programming\n\nIn optimization theory, semi-infinite programming (SIP) is an optimization problem with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints. In the former case the constraints are typically parameterized.\n\nThe problem can be stated simply as:\n\nwhere\n\nSIP can be seen as a special case of bilevel programs in which the lower-level variables do not participate in the objective function.\n\nIn the meantime, see external links below for a complete tutorial.\n\nIn the meantime, see external links below for a complete tutorial.\n\n\n\n"}
{"id": "9193086", "url": "https://en.wikipedia.org/wiki?curid=9193086", "title": "Skip counting", "text": "Skip counting\n\nSkip counting is a mathematics technique taught as a kind of multiplication in reform mathematics textbooks such as TERC. In older textbooks, this technique is called counting by twos (threes, fours, etc.).\n\nIn skip counting by twos, a person can count to 10 by only naming every other number: 2, 4, 6, 8, 10. Combining the base (two, in this example) with the number of groups (five, in this example) produces the standard multiplication equation: two multiplied by five equals ten.\n\nAnother similar method is coloring in squares in a 100s chart to show multiplication patterns. Critics such as Mathematically Correct however believe that it is an inadequate method to teach multiplication and ineffective for computation compared to traditional mathematics methods of multiplication.\n"}
{"id": "922382", "url": "https://en.wikipedia.org/wiki?curid=922382", "title": "Smith–Volterra–Cantor set", "text": "Smith–Volterra–Cantor set\n\nIn mathematics, the Smith–Volterra–Cantor set (SVC), fat Cantor set, or ε-Cantor set is an example of a set of points on the real line ℝ that is nowhere dense (in particular it contains no intervals), yet has positive measure. The Smith–Volterra–Cantor set is named after the mathematicians Henry Smith, Vito Volterra and Georg Cantor. The Smith-Volterra-Cantor set is topologically equivalent to the middle-thirds Cantor set.\n\nSimilar to the construction of the Cantor set, the Smith–Volterra–Cantor set is constructed by removing certain intervals from the unit interval [0, 1]. \n\nThe process begins by removing the middle 1/4 from the interval [0, 1] (the same as removing 1/8 on either side of the middle point at 1/2) so the remaining set is \n\nThe following steps consist of removing subintervals of width 1/4 from the middle of each of the 2 remaining intervals. So for the second step the intervals (5/32, 7/32) and (25/32, 27/32) are removed, leaving \n\nContinuing indefinitely with this removal, the Smith–Volterra–Cantor set is then the set of points that are never removed. The image below shows the initial set and five iterations of this process. \n\nEach subsequent iterate in the Smith–Volterra–Cantor set's construction removes proportionally less from the remaining intervals. This stands in contrast to the Cantor set, where the proportion removed from each interval remains constant. Thus, the former has positive measure while the latter has zero measure.\n\nBy construction, the Smith–Volterra–Cantor set contains no intervals and therefore has empty interior. It is also the intersection of a sequence of closed sets, which means that it is closed.\nDuring the process, intervals of total length \n\nare removed from [0, 1], showing that the set of the remaining points has a positive measure of 1/2. This makes the Smith–Volterra–Cantor set an example of a closed set whose boundary has positive Lebesgue measure.\n\nIn general, one can remove \"r\" from each remaining subinterval at the \"n\"-th step of the algorithm, and end up with a Cantor-like set. The resulting set will have positive measure if and only if the sum of the sequence is less than the measure of the initial interval. If the middle interval of length formula_4 is removed from formula_5 for each \"n\"-th iteration, where formula_6 this holds if and only if formula_7: the complement of the set obtained in this manner has Lebesgue measure\n\nHence the set itself has positive measure if and only if\n\nCartesian products of Smith–Volterra–Cantor sets can be used to find totally disconnected sets in higher dimensions with nonzero measure. By applying the Denjoy–Riesz theorem to a two-dimensional set of this type, it is possible to find a Jordan curve such that the points on the curve have positive area.\n\n\n"}
{"id": "47049443", "url": "https://en.wikipedia.org/wiki?curid=47049443", "title": "Spatiotemporal pattern", "text": "Spatiotemporal pattern\n\nSpatialtemporal patterns are patterns that occur in a wide range of natural phenoma and are characterized by a spatial and a temporal patterning. The general rules of pattern formation hold. In contrast to \"static\", pure spatial patterns, the full complexity of spatiotemporal patterns can only be recognized over time. Any kind of traveling wave is a good example of a spatiotemporal pattern. Besides the shape and amplitude of the wave (spatial part), its time-varying position (and possibly shape) in space is an essential part of the entire pattern.\n\nThe distinction between spatial and spatio-temporal patterns in nature is not clear-cut because a static, invariable pattern will never occur in the strict sense. Even rock formations will slowly change on a time-scale of 10s of millions of years, therefore the distinction lies in the time scale of change in relation to human experience. Already the snapshot state of a dune will usually be taken as an example of a purely spatial pattern although this is clearly not the case. It is thus apt to say that spatiotemporal patterns in nature are the rule rather than the exception.\n\nMany hydrodynamical systems show s.t. pattern formation:\n\nAny type of reaction-diffusion system that produces spatial patterns will also, due to the time-dependency of both reactions and diffusion, produce spatiotemporal patterns.\n\nNeural networks, both artificial and natural, produce a virtually unbounded variety of s.t. patterns, both in sensory perception, learning, thinking and reasoning as well as in spontaneous activity. It has for example been demonstrated that spiral waves, signatures of many excitable systems can occur in neocortical preparations.\nAll communication, language, relies on spatiotemporal encoding of information, producing and transmitting sound variations or any type of signal i.e. single building blocks of information that are varied over time. -Even though written language appears to exist only as a (2D) spatial concatenation of letters - strings, it must be decoded sequentially over time. Any kind of language that is understood by organisms is thus eventually a transcoding of neural s.t. signals and will - in successful communication - evoke similar patterns of neural activity in the recipient as they existed in the sender. For example, the warning call of a bird when it perceives a predator will produce a similar type and degree of alarmedness (eventually a certain kind of neural activity pattern) in other individuals even though they have not yet seen or heard the potential attacker.\nEven artificial languages, e.g. computer languages, are not read and interpreted in one step, but sequentially, thus, their meaningfully arranged vocabulary (e.g. \"computer code\") can be seen as a s.t. pattern.\n\nAs a particular type of language, the \"static\" (neglecting random transcription errors, recombination and mutation) DNA and its transcription pattern over time yields biologically essential s.t. patterns. Gene regulatory networks are responsible for regulation the time course of gene expression level which can be analyzed using expression profiling.\n"}
{"id": "42243853", "url": "https://en.wikipedia.org/wiki?curid=42243853", "title": "Stan (software)", "text": "Stan (software)\n\nStan is a probabilistic programming language for statistical inference written in C++. The Stan language is used to specify a (Bayesian) statistical model with an imperative program calculating the log probability density function.\n\nStan is licensed under the New BSD License. Stan is named in honour of Stanislaw Ulam, pioneer of the Monte Carlo method.\n\nStan was created by Andrew Gelman and Bob Carpenter, with a development team consisting of 34 members. \n\nStan can be accessed through several interfaces:\n\nStan implements gradient-based Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference, stochastic, gradient-based variational Bayesian methods for approximate Bayesian inference, and gradient-based optimization for penalized maximum likelihood estimation. \n\nStan implements reverse-mode automatic differentiation to calculate gradients of the model, which is required by HMC, NUTS, L-BFGS, BFGS, and variational inference. The automatic differentiation within Stan can be used outside of the probabilistic programming language.\n\nStan is used in fields including social science and pharmaceutical statistics.\n\n\n"}
{"id": "2728973", "url": "https://en.wikipedia.org/wiki?curid=2728973", "title": "Tensor product of Hilbert spaces", "text": "Tensor product of Hilbert spaces\n\nIn mathematics, and in particular functional analysis, the tensor product of Hilbert spaces is a way to extend the tensor product construction so that the result of taking a tensor product of two Hilbert spaces is another Hilbert space. Roughly speaking, the tensor product is the metric space completion of the ordinary tensor product. This is an example of a topological tensor product. The tensor product allows Hilbert spaces to be collected into a symmetric monoidal category.\n\nSince Hilbert spaces have inner products, one would like to introduce an inner product, and therefore a topology, on the tensor product that arise naturally from those of the factors. Let \"H\" and \"H\" be two Hilbert spaces with inner products formula_1 and formula_2, respectively. Construct the tensor product of \"H\" and \"H\" as vector spaces as explained in the article on tensor products. We can turn this vector space tensor product into an inner product space by defining\nand extending by linearity. That this inner product is the natural one is justified by the identification of scalar-valued bilinear maps on \"H\" × \"H\" and linear functionals on their vector space tensor product. Finally, take the completion under this inner product. The resulting Hilbert space is the tensor product of  \"H\" and \"H\".\n\nThe tensor product can also be defined without appealing to the metric space completion. If \"H\" and \"H\" are two Hilbert spaces, one associates to every simple tensor product formula_4 the rank one operator from \"H\" to \"H\" that maps a given formula_5 as\nThis extends to a linear identification between formula_7 and the space of finite rank operators from \"H\" to \"H\".\nThe finite rank operators are embedded in the Hilbert space \"HS\"(\"H\", \"H\") of Hilbert–Schmidt operators from \"H\" to \"H\". The scalar product in \"HS\"(\"H\", \"H\") is given by\nwhere formula_9 is an arbitrary orthonormal basis of \"H\".\n\nUnder the preceding identification, one can define the Hilbertian tensor product of \"H\" and \"H\", that is isometrically and linearly isomorphic to \"HS\"(\"H\", \"H\").\n\nThe Hilbert tensor product formula_10 is characterized by the following universal property :\n\nA weakly Hilbert-Schmidt mapping \"L\" : \"H\" × \"H\" → \"K\" is defined as a bilinear map for which a real number \"d\" exists, such that formula_11 for all \"u\" formula_12 \"K\" and one (hence all) orthonormal basis \"e\", \"e\", ... of \"H\" and \"f\", \"f\", ... of \"H\".\n\nAs with any universal property, this characterizes the tensor product \"H\" uniquely, up to isomorphism. The same universal property, with obvious modifications, also applies for the tensor product of any finite number of Hilbert spaces. It is essentially the same universal property shared by all definitions of tensor products, irrespective of the spaces being tensored: this implies that any space with a tensor product is a symmetric monoidal category, and Hilbert spaces are a particular example thereof.\n\nIf formula_13 is a collection of Hilbert spaces and formula_14 is a collection of unit vectors in these Hilbert spaces then the incomplete tensor product (or Guichardet tensor product) is the formula_15 completion of the set of all finite linear combinations of simple tensor vectors formula_16 where all but finitely many of the formula_17's equal the corresponding formula_14.\n\nLet formula_19 be the von Neumann algebra of bounded operators on formula_20 for formula_21. Then the von Neumann tensor product of the von Neumann algebras is the strong completion of the set of all finite linear combinations of simple tensor products formula_22 where formula_23 for formula_21. This is exactly equal to the von Neumann algebra of bounded operators of formula_25. Unlike for Hilbert spaces, one may take infinite tensor products of von Neumann algebras, and for that matter C*-algebras of operators, without defining reference states. This is one advantage of the \"algebraic\" method in quantum statistical mechanics.\n\nIf formula_26 and formula_27 have orthonormal bases formula_28 and formula_29, respectively, then formula_30 is an orthonormal basis for formula_25. In particular, the Hilbert dimension of the tensor product is the product (as cardinal numbers) of the Hilbert dimensions.\n\nThe following examples show how tensor products arise naturally.\n\nGiven two measure spaces formula_32 and formula_33, with measures formula_34 and formula_35 respectively, one may look at formula_36, the space of functions on formula_37 that are square integrable with respect to the product measure formula_38. If formula_39 is a square integrable function on formula_32, and formula_41 is a square integrable function on formula_33, then we can define a function formula_43 on formula_37 by formula_45. The definition of the product measure ensures that all functions of this form are square integrable, so this defines a bilinear mapping formula_46. Linear combinations of functions of the form formula_47 are also in formula_36. It turns out that the set of linear combinations is in fact dense in formula_36, ifformula_50 and formula_51 are separable. This shows that formula_52 is isomorphic toformula_36, and it also explains why we need to take the completion in the construction of the Hilbert space tensor product.\n\nSimilarly, we can show that formula_54, denoting the space of square integrable functions formula_55, is isomorphic to formula_56 if this space is separable. The isomorphism maps formula_57 to formula_58 We can combine this with the previous example and conclude that formula_52 and formula_36 are both isomorphic to formula_61\n\nTensor products of Hilbert spaces arise often in quantum mechanics. If some particle is described by the Hilbert space formula_26, and another particle is described by formula_27, then the system consisting of both particles is described by the tensor product of formula_26 and formula_27. For example, the state space of a quantum harmonic oscillator is formula_66, so the state space of two oscillators is formula_67, which is isomorphic to formula_68. Therefore, the two-particle system is described by wave functions of the form formula_69. A more intricate example is provided by the Fock spaces, which describe a variable number of particles.\n\n"}
{"id": "3524992", "url": "https://en.wikipedia.org/wiki?curid=3524992", "title": "Triangular function", "text": "Triangular function\n\nA triangular function (also known as a triangle function, hat function, or tent function) is a function whose graph takes the shape of a triangle. Often this is an isosceles triangle of height 1 and base 2 in which case it is referred to as \"the\" triangular function. Triangular functions are useful in signal processing and \"communication systems engineering\" as representations of idealized signals, and the triangular function specifically as an integral transform kernel function from which more realistic signals can be derived, for example in kernel density estimation. It also has applications in pulse code modulation as a pulse shape for transmitting digital signals and as a matched filter for receiving the signals. It is also used to define the triangular window sometimes called the Bartlett window.\n\nThe most common definition is as a piecewise function:\n\nEquivalently, it may be defined as the convolution of two identical unit rectangular functions:\n\nThe triangular function can also be represented as the product of the rectangular and absolute value functions:\n\nNote that some authors instead define the triangle function to have a base of width 1 instead of width 2:\n\nIn its most general form a triangular function is any linear B-Spline:\n\nWhereas the definition at the top is a special case\n\nA linear B-spline is the same as a continuous piecewise linear function, formula_10, and this general triangle function is useful to formally define such as:\n\nThe piecewise linear function passes through every point expressed as coordinates with ordered pair formula_14. That is\n\nFor any parameter formula_16:\n\nThe transform is easily determined using the convolution property of Fourier transforms and the Fourier transform of the rectangular function:\n\nwhere formula_19 is the normalized sinc function.\n\n"}
{"id": "9888821", "url": "https://en.wikipedia.org/wiki?curid=9888821", "title": "Trimean", "text": "Trimean\n\nIn statistics the trimean (TM), or Tukey's trimean, is a measure of a probability distribution's location defined as a weighted average of the distribution's median and its two quartiles:\n\nThis is equivalent to the average of the median and the midhinge:\n\nThe foundations of the trimean were part of Arthur Bowley's teachings, and later popularized by statistician John Tukey in his 1977 book which has given its name to a set of techniques called exploratory data analysis.\n\nLike the median and the midhinge, but unlike the sample mean, it is a statistically resistant L-estimator with a breakdown point of 25%. This beneficial property has been described as follows:\nDespite its simplicity, the trimean is a remarkably efficient estimator of population mean. More precisely, for a large data set (over 100 points) from a symmetric population, the average of the 20th, 50th, and 80th percentile is the most efficient 3 point L-estimator, with 88% efficiency. For context, the best 1 point estimate by L-estimators is the median, with an efficiency of 64% or better (for all \"n\"), while using 2 points (for a large data set of over 100 points from a symmetric population), the most efficient estimate is the 29% midsummary (mean of 29th and 71st percentiles), which has an efficiency of about 81%. Using quartiles, these optimal estimators can be approximated by the midhinge and the trimean. Using further points yield higher efficiency, though it is notable that only 3 points are needed for very high efficiency.\n\n\n"}
{"id": "34622141", "url": "https://en.wikipedia.org/wiki?curid=34622141", "title": "Tseytin transformation", "text": "Tseytin transformation\n\nThe Tseytin transformation, alternatively written Tseitin transformation takes as input an arbitrary combinatorial logic circuit and produces a boolean formula in conjunctive normal form (CNF), which can be solved by a CNF-SAT solver. The length of the formula is linear in the size of the circuit. Input vectors that make the circuit output \"true\" are in 1-to-1 correspondence with assignments that satisfy the formula. This reduces the problem of circuit satisfiability on any circuit (including any formula) to the satisfiability problem on 3-CNF formulas.\n\nThe naive approach is to write the circuit as a Boolean expression, and use De Morgan's law and the distributive property to convert it to CNF. However, this can result in an exponential increase in equation size. The Tseytin transformation outputs a formula whose size has grown linearly relative to the input circuit's.\n\nThe output equation is the constant 1 set equal to an expression. This expression is a conjunction of sub-expressions, where the satisfaction of each sub-expression enforces the proper operation of a single gate in the input circuit. The satisfaction of the entire output expression thus enforces that the entire input circuit is operating properly.\n\nFor each gate, a new variable representing its output is introduced. A small pre-calculated CNF expression that relates the inputs and outputs is appended (via the \"and\" operation) to the output expression. Note that inputs to these gates can be either the original literals or the introduced variables representing outputs of sub-gates.\n\nThough the output expression contains more variables than the input, it remains equisatisfiable, meaning that it is satisfiable if, and only if, the original input equation is satisfiable. When a satisfying assignment of variables is found, those assignments for the introduced variables can simply be discarded.\n\nA final clause is appended with a single literal: the final gate's output variable. If this literal is complemented, then the satisfaction of this clause enforces the output expression's to false; otherwise the expression is forced true.\n\nConsider the following formula formula_1 .\n\nConsider all subformulas (without variables):\n\nIntroduce a new variable for each subformula:\n\nConjunct all substitutions and the substitution for formula_1:\n\nAll substitutions can be transformed into CNF, e.g.\n\nListed are some of the possible sub-expressions that can be created for various logic gates. In an operation expression, C acts as an output; in a CNF sub-expression, C acts as a new Boolean variable. For each operation, the CNF sub-expression is true if and only if C adheres to the contract of the Boolean operation for all possible input values.\n\nThe following circuit returns true when at least some of its inputs are true, but not more than two at a time. It implements the equation . A variable is introduced for each gates' output; here each is marked in red:\nNotice that the output of the inverter with x as an input has two variables introduced. While this is redundant, it does not affect the equisatisfiability of the resulting equation. Now substitute each gate with its appropriate CNF sub-expression:\nThe final output variable is gate8 so to enforce that the output of this circuit be true, one final simple clause is appended: \n(gate8). Combining these equations results in the final instance of SAT:\n\nOne possible satisfying assignment of these variables is:\n\nThe values of the introduced variables are usually discarded, but they can be used to trace the logic path in the original circuit. Here, (x1,x2,x3) = (0,0,1) indeed meets the criteria for the original circuit to output true. To find a different answer, the clause (x1 ∨ x2 ∨ ) can be appended and the SAT solver executed again.\n\nPresented is one possible derivation of the CNF sub-expression for some chosen gates:\n\nAn OR gate with two inputs \"A\" and \"B\" and one output \"C\" is satisfies the following conditions:\nWe can express these two conditions as the conjunction of two implications:\nReplacing the implications with equivalent expressions involving only conjunctions, disjunctions, and negations yields\nwhich is nearly in conjunctive normal form already. Distributing the rightmost clause twice yields\nand applying the associativity of conjunction gives the CNF formula\n\nThe NOT gate is operating properly when its input and output oppose each other. That is:\nexpress these conditions as an expression that must be satisfied:\nThe NOR gate is operating properly when the following conditions hold: \nexpress these conditions as an expression that must be satisfied:\n\n"}
{"id": "9889145", "url": "https://en.wikipedia.org/wiki?curid=9889145", "title": "Vagal tone", "text": "Vagal tone\n\nVagal tone refers to activity of the vagus nerve, a fundamental component of the parasympathetic branch of the autonomic nervous system. This branch of the nervous system is not under conscious control and is largely responsible for the regulation of several body compartments at rest. Vagal activity results in various effects, including: heart rate reduction, vasodilation/constriction of vessels, glandular activity in the heart, lungs, and digestive tract as well as control of gastrointestinal sensitivity, motility and inﬂammation.\n\nIn this context, tone specifically refers to the continual nature of baseline parasympathetic action that the vagus nerve exerts. While baseline vagal input is constant, the degree of stimulation it exerts is regulated by a balance of inputs from sympathetic and parasympathetic divisions of the autonomic nervous system. Despite the described duality, vagal tone has been reported to mainly reflect the general level of parasympathetic activity. Vagal tone is typically considered in the context of heart function, but also has utility in assessing emotional regulation and other processes that alter, or are altered by changes and modification of the parasympathetic activity.\n\nMeasuring vagal tone along with its quantification and estimation can be performed by means of either invasive or noninvasive procedures. The former methodologies encompass the vagus nerve stimulation by manual or electrical techniques but literature reports a very limited number of experiments and clinical studies especially involving human subjects. On the other hand, noninvasive techniques are largely employed and they mainly rely on the investigation of heart rate and heart rate variability.\n\nIn the majority of cases, vagal tone is not directly measured. The most common procedure towards its quantification consist in investigating the processes altered by the vagus nerve – specifically heart rate and heart rate variability. As a general consideration, increased vagal tone (and thus vagal action) is associated with a diminished and more variable heart rate. On the opposite, during graded orthostatic tilt, vagal tone withdrawal is physiological and described as an indirect indicator of cardiovascular fitness.\n\nHeart rate is largely controlled by the heart's internal pacemaker activity. Considering a healthy heart, the main pacemaker is a collection of cells on the border of the atria and vena cava called the sinoatrial node. Heart cells exhibit automaticity which is the ability to generate electrical activity independent of external stimulation. As a result, the cells of the node spontaneously generate electrical activity that is subsequently conducted throughout the heart, resulting in a regular heart rate.\n\nIn absence of any external stimuli, sinoatrial pacing contributes to maintain the heart rate in the range of 60–100 beats per minute (bpm). At the same time, the two branches of the autonomic nervous system act in a complementary way increasing or slowing the heart rate. In this context, the vagus nerve acts on sinoatrial node slowing its conduction thus actively modulating vagal tone accordingly. This modulation is mediated by the neurotransmitter acetylcholine and downstream changes to ionic currents and calcium of heart cells.\n\nGiven the evidence that the vagus nerve plays a crucial role in heart rate regulation by modulating the response of sinoatrial node, vagal tone can be quantified by investigating heart rate modulation induced by vagal tone changes. This kind of analysis allows to investigate vagal tone by means of several noninvasive techniques based on heart rate variability.\n\nRespiratory sinus arrhythmia (RSA) is typically a benign, naturally occurring variation in heart rate that occurs during each breathing cycle. Specifically, heart rate increases during inspiration and decreases during expiration period. RSA was firstly recognized by Carl Ludwig but its genesis and understanding it is still nowadays largely discussed. RSA has been observed in humans from the early stages of life through adulthood. Moreover, RSA is a mechanism which can be consistently found in several different species.\n\nDuring inhalation intra-thoracic pressure lowers due to the contraction and downward movement of the diaphragm and the expansion of the chest cavity. Atrial pressure is also lowered as a result, enabling an increased blood flow to the heart. Such increase in blood volume towards the heart cavities triggers baroreceptors which act to diminish vagal tone. Subsequently, heart rate increases.\n\nOn the opposite during exhalation, the diaphragm relaxes, moving upward it decreases the size of the chest cavity, causing a subsequent increase in intrathoracic pressure. This increase in pressure inhibits venous return to the heart resulting in both reduced atrial expansion and minor activation of baroreceptors. Given the reduced baroreceptor activation, vagal tone is not suppressed as during inhalation so that it can exert its ability in decreasing heart rate.\n\nAs previously described, it is nowadays established that the two divisions of the autonomic nervous system influence each other reciprocally and independently so more and more measures able to discriminate the two contributions have been developed. In recent years, several studies have been published highlighting the quantification of RSA as a reliable tool to investigate vagal tone in a noninvasive way. Such investigations encompass physiological, behavioral, and several clinical studies. The main advantage in measuring of vagal tone by RSA is that such information are easily derivable from a single electrocardiography (ECG) recording. At the same time, novel methodologies started addressing RSA quantification by a multivariate approach thus not considering ECG only but the interrelationship of ECG and respiration.\n\nOn the opposite, vagal tone quantification by means of RSA has been questioned by many authors. It has been argued that RSA is unequivocally related to vagal control but it also clear that is determined by two different mechanisms namely: vagal tonic and vagal phasic. The former processes exhibit different dynamics and origins so that it is crucial to be able to differentiate their contributions to RSA. Furthermore, it has been observed that tonic and phasic components are distinct yet not completely independent one each other.\n\nDespite the nowadays limitations in RSA quantification, it is considered a promising, noninvasive and reliable index of vagal control of the heart, thus an indirect estimator of vagal tone.\n\nThe main hypothesis capable of explaining the reason behind the correlation of RSA and vagal tone describes RSA as an intrinsic resting function of the cardiopulmonary system. The theory suggest that in animals and humans RSA may eventually contribute to energy saving for both cardiac and respiratory systems thus reducing the heart rate and related heartbeats numbers. Furthermore, RSA could save energy expenditure by suppressing ineffective ventilation during the ebb of perfusion (delivery of blood from arteries to capillaries for oxygenation and nutrition).\n\nIn the physiological fields, RSA has been found to increase in subjects in resting state and to decrease in state of stress or tension. RSA is increased in supine position and decreased in prone position. RSA is on average higher and more pronounced during day time with respect to night time. RSA have also been extensively used to quantify vagal tone withdrawal in graded orthostatic tilt.\n\nTypically, expression of RSA decreases with age: it is pronounced in children and its magnitude tends to gradually disappear once a subject approach adulthood. However, adults in excellent cardiovascular health, such as endurance runners, swimmers, and cyclists, are likely to have a more pronounced RSA. Professional athletes on average maintain very high vagal tone and consequently higher RSA levels. RSA has been found to becomes less prominent in individuals with diabetes and cardiovascular disease.\n\nThe majority of vagal tone research in the physiological field (social behavior, social interactions, and human psychology) have been focused on newborns and children. The rational is to investigate children's adaptive functioning within a quantitative and reliable framework. Typically, researchers focus their attention on baseline vagal tone detection, treating it either as a potential predictor of behavior or examining its relationship with mental health (particularly emotion regulation, anxiety, and internalizing and externalizing disorders).\n\nThe Polyvagal theory by Porges is considered as the most influential model able to describe the differences between basal vagal tone during steady state and vagal reactivity as a response to external stimuli. The model describes vagal tone modifications a differential measure between vagal tone baseline and vagal tone activation during attention-demanding state. The theory states that successful vagal regulation is characterized by RSA suppression or withdrawal during attention tasks leading to increased metabolic output associated with heart rate increase.\n\nDespite the hypothesized link between vagal tone reduction and social functioning as stated by Porges' theory, researchers have been focusing mainly on the analysis of basal vagal tone. Examples are the findings reporting lower baseline RSA in children with Autism Spectrum Disorders with respect to healthy controls. Research indicates that children with more secure attachments with their mothers exhibited greater empathetic responsiveness, less social inhibition, and higher vagal tone, highlighting the vagus nerve's regulatory effect, as well as the quantification of vagal tone by means of RSA, as a predictor of emotional and social function.\n\nVagal tone estimation based on heart rate is quantifiable by several parameters rather than the use of RSA only. Examples are indexes of beat-to-beat variability such as RMSSD reported by The Task Force of the European Society of Cardiology and Heart Rhythm Society. Frequency analysis of heart rate in the range 0.15–0.4 Hz has been reported to quantify vagal tone based on heart rate variability spectrum. In the specific context of vagal tone response to head up tilt, a measure of beat-to-beat variability (RMSSD) showed significant decreases following head-up tilts as reported by Myers. Another method employed to quantify vagal activity is the computation of high frequency spectral component of heart rate variability power spectral density. An example for the latter described methodology is the change in sympatho-vagal balance during hypnosis. Results report hypnosis to affect heart rate variability, shifting the sympatho-vagal interaction toward an enhanced parasympathetic activity and reduction of the sympathetic tone.\n\n"}
{"id": "22304328", "url": "https://en.wikipedia.org/wiki?curid=22304328", "title": "Wilkie's theorem", "text": "Wilkie's theorem\n\nIn mathematics, Wilkie's theorem is a result by Alex Wilkie about the theory of ordered fields with an exponential function, or equivalently about the geometric nature of exponential varieties.\n\nIn terms of model theory, Wilkie's theorem deals with the language \"L\" = (+,−,·,<,0,1,\"e\"), the language of ordered rings with an exponential function \"e\". Suppose \"φ\"(\"x\"...,\"x\") is a formula in this language, then Wilkie's theorem states that there is an integer \"n\" ≥ \"m\" and polynomials \"f\"...,\"f\" ∈ Z[\"x\"...,\"x\",\"e\"...,\"e\"] such that \"φ\"(\"x\"...,\"x\") is equivalent to the existential formula\n\nThus, while this theory does not have full quantifier elimination, formulae can be put in a particularly simple form. This result proves that the theory of the structure R, that is the real ordered field with the exponential function, is model complete.\n\nIn terms of analytic geometry, the theorem states that any definable set in the above language — in particular the complement of an exponential variety — is in fact a projection of an exponential variety. An exponential variety over a field \"K\" is the set of points in \"K\" where a finite collection of exponential polynomials simultaneously vanish. Wilkie's theorem states that if we have any definable set in an \"L\" structure K = (\"K\",+,−,·,0,1,\"e\"), say \"X\" ⊂ \"K\", then there will be an exponential variety in some higher dimension \"K\" such that the projection of this variety down onto \"K\" will be precisely \"X\".\n\nThe result can be considered as a variation of Gabrielov's theorem. This earlier theorem, by Andrei Gabrielov, dealt with sub-analytic sets, or the language \"L\" of ordered rings with a function symbol for each proper analytic function on R restricted to the closed unit cube [0,1]. Gabrielov's theorem states that any formula in this language is equivalent to an existential one, as above. Hence the theory of the real ordered field with restricted analytic functions is model complete.\n\nGabrielov's theorem applies to the real field with all restricted analytic functions adjoined, whereas Wilkie's theorem removes the need to restrict the function, but only allows one to add the exponential function. As an intermediate result Wilkie asked when the complement of a sub-analytic set could be defined using the same analytic functions that described the original set. It turns out the required functions are the pfaffian functions. In particular the theory of the real ordered field with restricted, totally defined pfaffian functions is model complete. Wilkie's approach for this latter result is somewhat different from his proof of Wilkie's theorem, and the result that allowed him to show that the Pfaffian structure is model complete is sometimes known as Wilkie's theorem of the complement. See also \n"}
{"id": "53847553", "url": "https://en.wikipedia.org/wiki?curid=53847553", "title": "William Yslas Vélez", "text": "William Yslas Vélez\n\nWilliam \"Bill\" Yslas Vélez is an American mathematician, currently University of Arizona Distinguished Professor at the University of Arizona and an Elected Fellow of the American Association for the Advancement of Science. In 2017, he was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
