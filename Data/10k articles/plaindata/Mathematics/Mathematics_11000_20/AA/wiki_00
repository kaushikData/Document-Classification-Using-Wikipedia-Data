{"id": "44532981", "url": "https://en.wikipedia.org/wiki?curid=44532981", "title": "2-ring", "text": "2-ring\n\nIn mathematics, a categorical ring is, roughly, a category equipped with addition and multiplication. In other words, a categorical ring is obtained by replacing the underlying set of a ring by a category. For example, given a ring \"R\", let \"C\" be a category whose objects are the elements of the set \"R\" and where morphisms are only the identity morphisms. Then \"C\" is a categorical ring. But the point is that one can also consider the situation in which an element of \"R\" comes with a \"nontrivial automorphism\" (cf. Lurie).\n\nThis line of generalization of a ring eventually leads to the notion of an \"E\"-ring.\n\n\n\n"}
{"id": "362204", "url": "https://en.wikipedia.org/wiki?curid=362204", "title": "25 (number)", "text": "25 (number)\n\n25 (twenty-five) is the natural number following 24 and preceding 26.\n\nIt is a square number, being 5 = 5 × 5. It is one of two two-digit numbers whose square and higher powers of the number also ends in the same last two digits, e.g. 25 = 625, the other is 76. It is the smallest square that is also a sum of two (non-zero) squares: 25 = 3 + 4. Hence it often appears in illustrations of the Pythagorean theorem.\n\n25 is the sum of the single-digit odd natural numbers 1, 3, 5, 7 and 9, the first five odd natural numbers.\n\n25 is a centered octagonal number, a centered square number, and an automorphic number.\n\n25 percent (%) is equal to .\n\nIt is the smallest base 10 Friedman number as it can be expressed by its own digits: 5.\n\nIt is also a Cullen number. 25 is the smallest pseudoprime satisfying the congruence 7 = 7 mod \"n\".\n\n25 is the smallest aspiring number — a composite non-sociable number whose aliquot sequence does not terminate.\n\nAccording to the Shapiro inequality, 25 is the least odd integer \"n\" such that there exist \"x\", \"x\", …, \"x\" such that\n\nwhere \"x\" = \"x\", \"x\" = \"x\".\n\nWithin base 10 one can readily test for divisibility by 25 by seeing if the last two digits of the number match 00, 25, 50 or 75.\n\n\n\n\nTwenty five is:\n\n\n"}
{"id": "2379969", "url": "https://en.wikipedia.org/wiki?curid=2379969", "title": "3D Life", "text": "3D Life\n\n3D Life is a cellular automaton. It is a three-dimensional extension of Game of Life, investigated by Carter Bays. A number of different semitotalistic rules for the 3D rectangular Moore neighborhood were investigated.\n\nIt was popularized by Alexander Keewatin Dewdney in his \"Computer Recreations\" column in Scientific American magazine.\n\n\n"}
{"id": "4688641", "url": "https://en.wikipedia.org/wiki?curid=4688641", "title": "Affine curvature", "text": "Affine curvature\n\nSpecial affine curvature, also known as the equi-affine curvature or affine curvature, is a particular type of curvature that is defined on a plane curve that remains unchanged under a special affine transformation (an affine transformation that preserves area). The curves of constant equi-affine curvature \"k\" are precisely all non-singular plane conics. Those with \"k\" > 0 are ellipses, those with \"k\" = 0 are parabolas, and those with \"k\" < 0 are hyperbolas.\n\nThe usual Euclidean curvature of a curve at a point is the curvature of its osculating circle, the unique circle making second order contact (having three point contact) with the curve at the point. In the same way, \nthe special affine curvature of a curve at a point \"P\" is the special affine curvature of its hyperosculating conic, which is the unique conic making fourth order contact (having five point contact) with the curve at \"P\". In other words it is the\nlimiting position of the (unique) conic through \"P\" and four points \"P\", \"P\", \"P\", \"P\" on the curve, as each of the points approaches \"P\":\n\nIn some contexts, the affine curvature refers to a differential invariant κ of the general affine group, which may readily obtained from the special affine curvature \"k\" by κ = \"k\"d\"k\"/d\"s\", where \"s\" is the special affine arc length. Where the general affine group is not used, the special affine curvature \"k\" is sometimes also called the affine curvature .\n\nTo define the special affine curvature, it is necessary first to define the special affine arclength (also called the equi-affine arclength). Consider an affine plane curve formula_2. Choose co-ordinates for the affine plane such that the area of the parallelogram spanned by two vectors formula_3 and formula_4 is given by the determinant\n\nIn particular, the determinant\nis a well-defined invariant of the special affine group, and gives the signed area of the parallelogram spanned by the velocity and acceleration of the curve β. Consider a reparameterization of the curve β, say with a new parameter \"s\" related to \"t\" by means of a regular reparameterization \"s\" = \"s\"(\"t\"). This determinant undergoes then a transformation of the following sort, by the chain rule:\nThe reparameterization can be chosen so that\nprovided the velocity and acceleration, dβ/d\"t\" and dβ/d\"t\" are linearly independent. Existence and uniqueness of such a parameterization follows by integration:\n\nThis integral is called the special affine arclength, and a curve carrying this parameterization is said to be parameterized with respect to its special affine arclength.\n\nSuppose that β(\"s\") is a curve parameterized with its special affine arclength. Then the special affine curvature (or equi-affine curvature) is given by\n\nHere β′ denotes the derivative of β with respect to \"s\".\n\nMore generally (; ), for a plane curve with arbitrary parameterization\n\nthe special affine curvature is:\n\nprovided the first and second derivatives of the curve are linearly independent. In the special case of a graph \"y\" = \"y\"(\"x\"), these formulas reduce to\n\nwhere the prime denotes differentiation with respect to \"x\" (; ).\n\nSuppose as above that β(\"s\") is a curve parameterized by special affine arclength. There are a pair of invariants of the curve that are invariant under the full general affine group — the group of all affine motions of the plane, not just those that are area-preserving. The first of these is\n\nsometimes called the \"affine arclength\" (although this risks confusion with the special affine arclength described above). The second is referred to as the \"affine curvature\":\n\nSuppose that β(\"s\") is a curve parameterized by special affine arclength with constant affine curvature \"k\". Let\n\nNote that det \"C\", since β is assumed to carry the special affine arclength parameterization, and that\n\nIt follows from the form of \"C\" that\n\nBy applying a suitable special affine transformation, we can arrange that \"C\"(0) = \"I\" is the identity matrix. Since \"k\" is constant, it follows that \"C\" is given by the matrix exponential\n\nThe three cases are now as follows.\n\n\nIf the curvature vanishes identically, then upon passing to a limit,\n\nso β'(\"s\") = (1,s), and so integration gives\n\nup to an overall constant translation, which is the special affine parameterization of the parabola \"y\" = \"x\"/2.\n\n\nIf the special affine curvature is positive, then it follows that\n\nso that\n\nup to a translation, which is the special affine parameterization of the ellipse \"kx\" + \"k\"\"y\" = 1.\n\n\nIf \"k\" is negative, then the trigonometric functions in \"C\" give way to hyperbolic functions:\n\nThus\n\nup to a translation, which is the special affine parameterization of the hyperbola\n\nThe special affine curvature of an immersed curve is the only (local) invariant of the curve in the following sense:\n\n\nIn fact, a slightly stronger statement holds:\n\n\nThis is analogous to the fundamental theorem of curves in the classical Euclidean differential geometry of curves, in which the complete classification of plane curves up to Euclidean motion depends on a single function κ, the curvature of the curve. It follows essentially by applying the Picard–Lindelöf theorem to the system\n\nwhere \"C\" = [β′ β′′]. An alternative approach, rooted in the theory of moving frames, is to apply the existence of a primitive for the Darboux derivative.\n\nThe special affine curvature can be derived explicitly by techniques of invariant theory. For simplicity, suppose that an affine plane curve is given in the form of a graph \"y\" = \"y\"(\"x\"). The special affine group acts on the Cartesian plane via transformations of the form\nwith \"ad\" − \"bc\" = 1. The following vector fields span the Lie algebra of infinitesimal generators of the special affine group:\n\nAn affine transformation not only acts on points, but also on the tangent lines to graphs of the form \"y\" = \"y\"(\"x\"). That is, there is an action of the special affine group on triples of coordinates\n\nThe group action is generated by vector fields\ndefined on the space of three variables (\"x\",\"y\",\"y\"′). These vector fields can be determined by the following two requirements:\n\nSimilarly, the action of the group can be extended to the space of any number of derivatives\nThe prolonged vector fields generating the action of the special affine group must then inductively satisfy, for each generator \"X\" ∈ {\"T\",\"T\",\"X\",\"X\",\"H\"}:\n\nCarrying out the inductive construction up to order 4 gives\n\nThe special affine curvature\ndoes not depend explicitly on \"x\", \"y\", or \"y\"′, and so satisfies\nThe vector field \"H\" acts diagonally as a modified homogeneity operator, and it is readily verified that \"H\"\"k\" = 0. Finally,\n\nThe five vector fields\nform an involutive distribution on (an open subset of) R so that, by the Frobenius integration theorem, they integrate locally to give a foliation of R by five-dimensional leaves. Concretely, each leaf is a local orbit of the special affine group. The function \"k\" parameterizes these leaves.\n\nHuman curvilinear 2-dimensional drawing movements tend to follow the equi-affine parametrization. This is more commonly known as the two thirds power law, according to which the hand's speed is proportional to the Euclidean curvature raised to the minus third power. Namely, \nwhere formula_48 is the speed of the hand, formula_49 is the Euclidean curvature and formula_50 is a constant termed the velocity gain factor.\n\n\n"}
{"id": "294085", "url": "https://en.wikipedia.org/wiki?curid=294085", "title": "Arithmetization of analysis", "text": "Arithmetization of analysis\n\nThe arithmetization of analysis was a research program in the foundations of mathematics carried out in the second half of the 19th century. \n\nKronecker originally introduced the term \"arithmetization of analysis\", by which he meant its constructivization in the context of the natural numbers (see quotation at bottom of page). The meaning of the term later shifted to signify the set-theoretic construction of the real line. Its main proponent was Weierstrass, who argued the geometric foundations of calculus were not solid enough for rigorous work. \n\nThe highlights of this research program are:\n\nAn important spinoff of the arithmetization of analysis is set theory. Naive set theory was created by Cantor and others after arithmetization was completed as a way to study the singularities of functions appearing in calculus. \n\nThe arithmetization of analysis had several important consequences:\n\n\n"}
{"id": "30738616", "url": "https://en.wikipedia.org/wiki?curid=30738616", "title": "Carl Anton Bretschneider", "text": "Carl Anton Bretschneider\n\nCarl Anton Bretschneider (27 May 1808 – 6 November 1878) was a mathematician from Gotha, Germany. Bretschneider worked in geometry, number theory, and history of geometry. He also worked on logarithmic integrals and mathematical tables. He was one of the first mathematicians to use the symbol γ for Euler's constant when he published his 1837 paper. He is best known for his discovery of Bretschneider's formula.\n\n\n\n"}
{"id": "57182828", "url": "https://en.wikipedia.org/wiki?curid=57182828", "title": "Charles Lagrange Prize", "text": "Charles Lagrange Prize\n\nCharles Lagrange Prize, or Prix Charles Lagrange, is a monetary prize, recognizing the best mathematical or experimental work contributing to the progress of mathematical knowledge in the world. It was first awarded in 1952 by the Academie Royale de Belgique, Classe des Sciences.  Recipients may be Belgian or foreign.\n\nThe recipients of the Charles Lagrange Prize are:\n\n"}
{"id": "7713", "url": "https://en.wikipedia.org/wiki?curid=7713", "title": "Chinese remainder theorem", "text": "Chinese remainder theorem\n\nThe Chinese remainder theorem is a theorem of number theory, which states that if one knows the remainders of the Euclidean division of an integer by several integers, then one can determine uniquely the remainder of the division of by the product of these integers, under the condition that the divisors are pairwise coprime.\n\nThe earliest known statement of the theorem is by the Chinese mathematician Sunzi in \"Sunzi Suanjing\" in the 3rd century AD.\n\nThe Chinese remainder theorem is widely used for computing with large integers, as it allows replacing a computation for which one knows a bound on the size of the result by several similar computations on small integers.\n\nThe Chinese remainder theorem (expressed in terms of congruences) is true over every principal ideal domain. It has been generalized to any commutative ring, with a formulation involving ideals.\n\nThe earliest known statement of the theorem, as a problem with specific numbers, appears in the 3rd-century book \"Sunzi Suanjing\" by the Chinese mathematician Sunzi:\n\nSunzi's work contains neither a proof nor a full algorithm. What amounts to an algorithm for solving this problem was described by Aryabhata (6th century). Special cases of the Chinese remainder theorem were also known to Brahmagupta (7th century), and appear in Fibonacci's Liber Abaci (1202). The result was later generalized with a complete solution called \"Dayanshu\" () in Qin Jiushao's 1247 \"Mathematical Treatise in Nine Sections\" (, \"Shushu Jiuzhang\").\n\nThe notion of congruences was first introduced and used by Gauss in his \"Disquisitiones Arithmeticae\" of 1801. Gauss illustrates the Chinese remainder theorem on a problem involving calendars, namely, \"to find the years that have a certain period number with respect to the solar and lunar cycle and the Roman indiction.\" Gauss introduces a procedure for solving the problem that had already been used by Euler but was in fact an ancient method that had appeared several times.\n\nLet be integers greater than 1, which are often called \"moduli\" or \"divisors\". Let us denote by the product of the .\n\nThe Chinese remainder theorem asserts that if the are pairwise coprime, and if are integers such that for every , then there is one and only one integer , such that and the remainder of the Euclidean division of by is for every .\n\nThis may be restated as follows in term of congruences:\nIf the are pairwise coprime, and if are any integers, then there exists an integer such that\n\nand any two such are congruent modulo .\n\nIn abstract algebra, the theorem is often restated as: if the are pairwise coprime, the map\ndefines a ring isomorphism\n\nbetween the ring of integers modulo and the direct product of the rings of integers modulo the . This means that for doing a sequence of arithmetic operations in formula_4 one may do the same computation independently in each formula_5 and then get the result by applying the isomorphism (from the right to the left). This may be much faster than the direct computation if and the number of operations are large. This is widely used, under the name \"multi-modular computation\", for linear algebra over the integers or the rational numbers.\n\nThe theorem can also be restated in the language of combinatorics as the fact that the infinite arithmetic progressions of integers form a Helly family.\n\nThe existence and the uniqueness of the solution may be proven independently. However, the first proof of existence, given below, uses this uniqueness.\n\nSuppose that and are both solutions to all the congruences. As and give the same remainder, when divided by , their difference is a multiple of each . As the are pairwise coprime, their product divides also , and thus and are congruent modulo . If and are supposed to be non negative and less than (as in the first statement of the theorem), then their difference may be a multiple of only if .\n\nThe map\nmaps congruence classes modulo to sequences of congruence classes modulo . The proof of uniqueness shows that this map is injective. As the domain and the codomain of this map have the same number of elements, the map is also surjective, which proves the existence of the solution.\n\nThis proof is very simple but does not provide any direct way for computing a solution. Moreover, it cannot be generalized to other situations where the following proof can.\n\nExistence may be established by an explicit construction of . This construction may be split into two steps, firstly by solving the problem in the case of two moduli, and the second one by extending this solution to the general case by induction on the number of moduli.\n\nWe want to solve the system:\nwhere formula_8 and formula_9 are coprime.\n\nBézout's identity asserts the existence of two integers formula_10 and formula_11 such that \nThe integers formula_10 and formula_11 may be computed by the extended Euclidean algorithm.\n\nA solution is given by\nIndeed, \nimplying that formula_17 The second congruence is proved similarly.\n\nLet us consider a sequence of congruence equations:\nwhere the formula_19 are pairwise coprime. The two first equations have a solution formula_20 provided by the method of the previous section. The set of the solutions of these two first equations is the set of all solutions of the equation\n\nAs the other formula_19 are coprime with formula_23 this reduces solving the initial problem of equations to a similar problem with formula_24 equations. Iterating the process, one gets eventually the solutions of the initial problem.\n\nFor constructing a solution, it is not necessary to make an induction on the number of moduli. However, such a direct construction involves more computation with large numbers, which makes it less efficient and less used. Nevertheless, Lagrange interpolation is a special case of this construction, applied to polynomials instead of integers.\n\nLet formula_25 be the product of all moduli but one. As the formula_19 are pairwise coprime, formula_27 and formula_19 are coprime. Thus Bézout's identity applies, and there exist integers formula_29 and formula_30 such that\n\nA solution of the system of congruences is\nIn fact, as formula_33 is a multiple of formula_19 for formula_35\nwe have\nfor every formula_37\n\nLet us consider a system of congruences:\nwhere the formula_19 are pairwise coprime, and let formula_40 In this section several methods are described for computing the unique solution for formula_41, such that formula_42 and these methods are applied on the example:\n\nIt is easy to check whether a value of is a solution: it suffices to compute the remainder of the Euclidean division of by each . Thus, to find the solution, it suffices to check successively the integers from to until finding the solution.\n\nAlthough very simple this method is very inefficient: for the simple example considered here, integers (including ) have to be checked for finding the solution . This is an exponential time algorithm, as the size of the input is, up to a constant factor, the number of digits of , and the average number of operations is of the order of .\n\nTherefore, this method is rarely used, for hand-written computation as well on computers.\n\nThe search of the solution may be made dramatically faster by sieving. For this method, we suppose, without loss of generality, that formula_44 (if it were not the case, it would suffice to replace each formula_45 by the remainder of its division by formula_19). This implies that the solution belongs to the arithmetic progression\nBy testing the values of these numbers modulo formula_48 one eventually finds a solution formula_49 of the two first congruences. Then the solution belongs to the arithmetic progression \nTesting the values of these numbers modulo formula_51, and continuing until every modulus has been tested gives eventually the solution.\n\nThis method is faster if the moduli have been ordered by decreasing value, that is if formula_52 For the example, this gives the following computation. We consider first the numbers that are congruent to 4 modulo 5 (the largest modulus), which are 4, , , ... For each of them, compute the remainder by 4 (the second largest modulus) until getting a number congruent to 3 modulo 4. Then one can proceed by adding at each step, and computing only the remainders by 3. This gives\n\nThis method works well for hand-written computation with a product of moduli that is not too big. However, it is much slower than other methods, for very large products of moduli. Although dramatically faster than the systematic search, this method also has an exponential time complexity and is therefore not used on computers.\n\nThe constructive existence proof shows that, in the case of two moduli, the solution may be obtained by the computation of the Bézout coefficients of the moduli, followed by a few multiplications, additions and reductions modulo formula_53 (for getting a result in the interval formula_54). As the Bézout's coefficients may be computed with the extended Euclidean algorithm, the whole computation, at most, has a quadratic time complexity of formula_55 where formula_56 denotes the number of digits of formula_57\n\nFor more than two moduli, the method for two moduli allows the replacement of any two congruences by a single congruence modulo the product of the moduli. Iterating this process provides eventually the solution with a complexity, which is quadratic in the number of digits of the product of all moduli. This quadratic time complexity does not depend on the order in which the moduli are regrouped. One may regroup the two first moduli, then regroup the resulting modulus with the next one, and so on. This strategy is the easiest to implement, but it also requires more computation involving large numbers.\n\nAnother strategy consists in partitioning the moduli in pairs whose product have comparable sizes (as much as possible), applying, in parallel, the method of two moduli to each pair, and iterating with a number of moduli approximatively divided by two. This method allows an easy parallelization of the algorithm. Also, if fast algorithms (that is algorithms working in quasilinear time) are used for the basic operations, this method provides an algorithm for the whole computation that works in quasilinear time.\n\nOn the current example (which has only three moduli), both strategies are identical and work as follows.\n\nBézout's identity for 3 and 4 is\nPutting this in the formula given for proving the existence gives \nfor a solution of the two first congruences, the other solutions being obtained by adding to −9 any multiple of . One may continue with any of these solutions, but the solution is smaller (in absolute value) and thus leads probably to an easier computation\n\nBézout identity for 5 and 3×4 = 12 is\nApplying the same formula again, we get a solution of the problem:\nThe other solutions are obtained by adding any multiple of , and the smallest positive solution is .\n\nThe system of congruences solved by the Chinese remainder theorem may be rewritten as a system of simultaneous linear Diophantine equations:\nwhere the unknown integers are formula_41 and the formula_64 Therefore, every general method for solving such systems may be used for finding the solution of Chinese remainder theorem, such as the reduction of the matrix of the system to Smith normal form or Hermite normal form. However, as usual when using a general algorithm for a more specific problem, this approach is less efficient than the method of the preceding section, based on a direct use of Bézout's identity.\n\nIn , the Chinese remainder theorem has been stated in three different ways: in terms of remainders, of congruences, and of a ring isomorphism. The statement in terms of remainders does not apply, in general, to principal ideal domains, as remainders are not defined in such rings. However, the two other versions make sense over a principal ideal domain : it suffices to replace \"integer\" by \"element of the domain\" and formula_65 by . These two versions of the theorem are true in this context, because the proofs (except for the first existence proof), are based on Euclid's lemma and Bézout's identity, which are true over every principal domain.\n\nHowever, in general, the theorem is only an existence theorem and does not provide any way for computing the solution, unless one has an algorithm for computing the coefficients of Bézout's identity.\n\nThe statement in terms of remainders given in cannot be generalized to any principal ideal domain, but its generalization to Euclidean domains is straightforward. The univariate polynomials over a field is the typical example of a Euclidean domain, which is not the integers. Therefore, we state the theorem for the case of a ring of univariate domain formula_66 over a field formula_67 For getting the theorem for a general Euclidean domain, it suffices to replace the degree by the Euclidean function of the Euclidean domain.\n\nThe Chinese remainder theorem for polynomials is thus: Let formula_68 (the moduli) be, for , pairwise coprime polynomials in formula_66. Let formula_70 be the degree of formula_68, and formula_72 be the sum of the formula_73\nIf formula_74 are polynomials such that formula_75 or formula_76 for every , then, there is one and only one polynomial formula_77, such that formula_78 and the remainder of the Euclidean division of formula_77 by formula_68 is formula_81 for every .\n\nThe construction of the solution may be done as in or . However, the latter construction may be simplified by using, as follows, partial fraction decomposition instead of extended Euclidean algorithm.\n\nThus, we want to find a polynomial formula_77, which satisfies the congruences\nfor formula_84\n\nLet us consider the polynomials\n\nThe partial fraction decomposition of formula_86 gives polynomials formula_87 with degrees formula_88 such that\nand thus\n\nThen a solution of the simultaneous congruence system is given by the polynomial\n\nIn fact, we have\nfor formula_93\n\nThis solution may have a degree larger that formula_94 The unique solution of degree less than formula_72 may be deduced by considering the remainder formula_96 of the Euclidean division of formula_97 by formula_98 This solution is \n\nA special case of Chinese remainder theorem for polynomials is Lagrange interpolation. For this, let us consider monic polynomials of degree one:\n\nThey are pairwise coprime if the formula_101 are all different. The remainder of the division by formula_68 of a polynomial formula_77 is formula_104\n\nNow, let formula_105 be constants (polynomials of degree 0) in formula_67 Both Lagrange interpolation and Chinese remainder theorem assert the existence of a unique polynomial formula_107 of degree less than formula_108 such that\n\nfor every formula_37\n\nLagrange interpolation formula is exactly the result, in this case, of the above construction of the solution. More precisely, let \n\nThe partial fraction decomposition of formula_112 is \n\nIn fact, reducing the right-hand side to a common denominator one gets \n\nand the numerator is equal to one, as being a polynomial of degree less than formula_115 which takes the value one for formula_108 different values of formula_117\n\nUsing the above general formula, we get the Lagrange interpolation formula:\n\nHermite interpolation is an application of Chinese remainder theorem for univariate polynomials, which may involve moduli of arbitrary degrees (Lagrange interpolation involves only moduli of degree one).\n\nThe problem consists of finding a polynomial of the least possible degree, such that the polynomial and its first derivatives take given values at some fixed points.\n\nMore precisely, let formula_119 be formula_108 elements of the ground field formula_121 and, for formula_122 let formula_123 be the values of the first formula_124 derivatives of the sought polynomial at formula_101 (including the 0th derivative, which is the value of the polynomial itself). The problem is to find a polynomial formula_77 such that its \"j\"th derivative takes the value formula_127 at formula_128 for formula_129 and formula_130\n\nLet us consider the polynomial\nThis is the Taylor polynomial of order formula_132 at formula_101, of the unknown polynomial formula_134 Therefore, we must have\n\nConversely, any polynomial formula_136 that satisfies these formula_108 congruences, in particular verifies, for any formula_138\ntherefore formula_68 is its Taylor polynomial of order formula_141 at formula_101, that is, formula_77 solves the initial Hermite interpolation problem.\nThe Chinese remainder theorem asserts that there exists exactly one polynomial of degree less than the sum of the formula_144 which satisfies these formula_108 congruences.\n\nThere are several ways for computing the solution formula_134 One may use the method described at the beginning of . One may also use the constructions given in or .\n\nThe Chinese remainder theorem can be generalized to non-coprime moduli. Let formula_147 be any integers, let formula_148, and consider the system of congruences:\nIf formula_150, then this system of equations has a unique solution modulo formula_151. Otherwise, it has no solutions.\n\nIf we use Bézout's identity to write formula_152, then the solution is\nThis defines an integer, as divides both and . Otherwise, the proof is very similar to that for coprime moduli.\n\nThe Chinese remainder theorem can be generalized to any ring, by using coprime ideals (also called comaximal ideals). Two ideals and are coprime if there are elements formula_154 and formula_155 such that formula_156 This relation plays the role of Bézout's identity in the proofs related to this generalization, which, otherwise are very similar. The generalization may be stated as follows.\n\nLet be two-sided ideals of a ring formula_157 that are pairwise coprime, and be their intersection. Then we have the isomorphism: \nbetween the quotient ring formula_159 and the direct product of the formula_160\nwhere \"formula_161\" denotes the image of the element formula_41 in the quotient ring defined by the ideal formula_163\nMoreover, if formula_157 is commutative, then the ideal intersection is equal to the product of the ideals formula_165\n\nThe Chinese remainder theorem has been used to construct a Gödel numbering for sequences, which is involved in the proof of Gödel's incompleteness theorems.\n\nThe prime-factor FFT algorithm (also called Good-Thomas algorithm) uses the Chinese remainder theorem for reducing the computation of a fast Fourier transform of size formula_53 to the computation of two fast Fourier transforms of smaller sizes formula_8 and formula_9 (providing that formula_8 and formula_9 are coprime).\n\nMost implementations of RSA use the Chinese remainder theorem during signing of HTTPS certificates and during decryption.\n\nThe Chinese remainder theorem can also be used in secret sharing, which consists of distributing a set of shares among a group of people who, all together (but no one alone), can recover a certain secret from the given set of shares. Each of the shares is represented in a congruence, and the solution of the system of congruences using the Chinese remainder theorem is the secret to be recovered. Secret sharing using the Chinese remainder theorem uses, along with the Chinese remainder theorem, special sequences of integers that guarantee the impossibility of recovering the secret from a set of shares with less than a certain cardinality.\n\nThe range ambiguity resolution techniques used with medium pulse repetition frequency radar can be seen as a special case of the Chinese remainder theorem.\n\nDedekind's theorem on the linear independence of characters. Let be a monoid and an integral domain, viewed as a monoid by considering the multiplication on . Then any finite family of distinct monoid homomorphisms is linearly independent. In other words, every family of elements satisfying \n\nmust be equal to the family .\n\nProof. First assume that is a field, otherwise, replace the integral domain by its quotient field, and nothing will change. We can linearly extend the monoid homomorphisms to -algebra homomorphisms , where is the monoid ring of over . Then, by linearity, the condition\n\nyields\n\nNext, for the two -linear maps and are not proportional to each other. Otherwise and would also be proportional, and thus equal since as monoid homomorphisms they satisfy: , which contradicts the assumption that they are distinct.\n\nTherefore, the kernels and are distinct. Since is a field, is a maximal ideal of for every . Because they are distinct and maximal the ideals and are coprime whenever . The Chinese Remainder Theorem (for general rings) yields an isomorphism:\n\nwhere\n\nConsequently, the map\n\nis surjective. Under the isomorphisms the map corresponds to:\n\nNow,\n\nyields\n\nfor every vector in the image of the map . Since is surjective, this means that\n\nfor every vector\n\nConsequently, . QED.\n\n\n\n\n"}
{"id": "21698103", "url": "https://en.wikipedia.org/wiki?curid=21698103", "title": "Crepant resolution", "text": "Crepant resolution\n\nIn algebraic geometry, a crepant resolution of a singularity is a resolution that does not affect the canonical class of the manifold. The term \"crepant\" was coined by by removing the prefix \"dis\" from the word \"discrepant\", to indicate that the resolutions have no discrepancy in the canonical class.\n\nThe crepant resolution conjecture of states that the orbifold cohomology of a Gorenstein orbifold is isomorphic to a semiclassical limit of the quantum cohomology of a crepant resolution.\n\nIn 2 dimensions, crepant resolutions of complex Gorenstein quotient singularities (du Val singularities) always exist and are unique, in 3 dimensions they exist but need not be unique as they can be related by flops, and in dimensions greater than 3 they need not exist.\n\nA substitute for crepant resolutions which always exists is a terminal model. Namely, for every variety \"X\" over a field of characteristic zero such that \"X\" has canonical singularities (for example, rational Gorenstein singularities), there is a variety \"Y\" with Q-factorial terminal singularities and a birational projective morphism \"f\": \"Y\" → \"X\" which is crepant in the sense that \"K\" = \"f\"*\"K\". \n\n"}
{"id": "8495580", "url": "https://en.wikipedia.org/wiki?curid=8495580", "title": "De Morgan algebra", "text": "De Morgan algebra\n\nIn mathematics, a De Morgan algebra (named after Augustus De Morgan, a British mathematician and logician) is a structure \"A\" = (A, ∨, ∧, 0, 1, ¬) such that:\n\n\nIn a De Morgan algebra, the laws\n\n\ndo not always hold. In the presence of the De Morgan laws, either law implies the other, and an algebra which satisfies them becomes a Boolean algebra.\n\nRemark: It follows that ¬( x∨y) = ¬x∧¬y, ¬1 = 0 and ¬0 = 1 (e.g. ¬1 = ¬1∨0 = ¬1∨¬¬0 = ¬(1∧¬0) = ¬¬0 = 0). Thus ¬ is a dual automorphism.\n\nIf the lattice is defined in terms of the order instead, i.e. (A, ≤) is a bounded partial order with a least upper bound and greatest lower bound for every pair of elements, and the meet and join operations so defined satisfy the distributive law, then the complementation can also be defined as an involutive anti-automorphism, that is, a structure \"A\" = (A, ≤, ¬) such that:\n\n\nDe Morgan algebras were introduced by Grigore Moisil around 1935. although without the restriction of having a 0 and a 1. They were then variously called quasi-boolean algebras in the Polish school, e.g. by Rasiowa and also distributive \"i\"-lattices by J. A. Kalman. (\"i\"-lattice being an abbreviation for lattice with involution.) They have been further studied in the Argentinian algebraic logic school of Antonio Monteiro.\n\nDe Morgan algebras are important for the study of the mathematical aspects of fuzzy logic. The standard fuzzy algebra \"F\" = ([0,  1], max(\"x\", \"y\"), min(\"x\", \"y\"), 0, 1, 1 − \"x\") is an example of a De Morgan algebra where the laws of excluded middle and noncontradiction do not hold.\n\nAnother example is Dunn's 4-valued logic, in which \"false\" < \"neither-true-nor-false\" < \"true\" and \"false\" < \"both-true-and-false\" < \"true\", while \"neither-true-nor-false\" and \"both-true-and-false\" are not comparable.\n\nIf a De Morgan algebra additionally satisfies \"x\" ∧ ¬\"x\" ≤ \"y\" ∨ ¬\"y\", it is called a Kleene algebra. (This notion should not to be confused with the other Kleene algebra generalizing regular expressions.) This notion has also been called a normal \"i\"-lattice by Kalman.\n\nExamples of Kleene algebras in the sense defined above include: lattice-ordered groups, Post algebras and Łukasiewicz algebras. Boolean algebras also meet this definition of Kleene algebra. The simplest Kleene algebra that is not Boolean is Kleene's three-valued logic K. K made its first appearance in Kleene's \"On notation for ordinal numbers\" (1938). The algebra was named after Kleene by Brignole and Monteiro.\n\nDe Morgan algebra is not the only plausible way to generalize the Boolean algebra. Another way is to keep ¬\"x\" ∧ \"x\" = 0 (i.e. the law of noncontradiction) but to drop the law of the excluded middle and the law of double negation. This approach (called \"semicomplementation\") is well-defined even for a [meet] semilattice; if the set of semicomplements has a greatest element it is usually called pseudocomplement. If the pseudocomplement thus defined satisfies the law of the excluded middle, the resulting algebra is also Boolean. However, if only the weaker law ¬\"x\" ∨ ¬¬\"x\" = 1 is required, this results in Stone algebras. More generally, both De Morgan and Stone algebras are proper subclasses of Ockham algebras.\n\n\n"}
{"id": "5569486", "url": "https://en.wikipedia.org/wiki?curid=5569486", "title": "Definable set", "text": "Definable set\n\nIn mathematical logic, a definable set is an \"n\"-ary relation on the domain of a structure whose elements are precisely those elements satisfying some formula in the first-order language of that structure. A set can be defined with or without parameters, which are elements of the domain that can be referenced in the formula defining the relation.\n\nLet formula_1 be a first-order language, formula_2 an formula_1-structure with domain formula_4, formula_5 a fixed subset of formula_4, and formula_7 a natural number. Then:\n\nLet formula_25 be the structure consisting of the natural numbers with the usual ordering. Then every natural number is definable in formula_26 without parameters. The number formula_27 is defined by the formula formula_28 stating that there exist no elements less than \"x\":\n\nformula_29\n\nand a natural formula_30 is defined by the formula formula_28 stating there exist exactly formula_32 elements less than \"x\":\n\nformula_33\n\nIn contrast, one cannot define any specific integer without parameters in the structure formula_34 consisting of the integers with the usual ordering (see the section on automorphisms below).\n\nLet formula_35 be the first-order structure consisting of the natural numbers and their usual arithmetic operations and order relation. The sets definable in this structure are known as the arithmetical sets, and are classified in the arithmetical hierarchy. If the structure is considered in second-order logic instead of first-order logic, the definable sets of natural numbers in the resulting structure are classified in the analytical hierarchy. These hierarchies reveal many relationships between definability in this structure and computability theory, and are also of interest in descriptive set theory.\n\nLet formula_36 be the structure consisting of the field of real numbers. Although the usual ordering relation is not directly included in the structure, there is a formula that defines the set of nonnegative reals, since these are the only reals that possess square roots:\n\nformula_37\n\nThus any formula_38 is nonnegative if and only if formula_39. In conjunction with a formula that defines the additive inverse of a real number in formula_40, one can use formula_41 to define the usual ordering in formula_40: for formula_43, set formula_44 if and only if formula_45 is nonnegative. The enlarged structure formula_46s is called a definitional extension of the original structure. It has the same expressive power as the original structure, in the sense that a set is definable over the enlarged structure from a set of parameters if and only if it is definable over the original structure from that same set of parameters. \n\nThe theory of formula_47 has quantifier elimination. Thus the definable sets are Boolean combinations of solutions to polynomial equalities and inequalities; these are called semi-algebraic sets. Generalizing this property of the real line leads to the study of o-minimality.\n\nAn important result about definable sets is that they are preserved under automorphisms.\n\nThis result can sometimes be used to classify the definable subsets of a given structure. For example, in the case of formula_34 above, any translation of formula_62 is an automorphism preserving the empty set of parameters, and thus it is impossible to define any particular integer in this structure without parameters in formula_62. In fact, since any two integers are carried to each other by a translation and its inverse, the only sets of integers definable in formula_62 without parameters are the empty set and formula_65 itself. In contrast, there are infinitely many definable sets of pairs (or indeed \"n\"-tuples for any fixed \"n\">1) of elements of formula_62, since any automorphism (translation) preserves the \"distance\" between two elements.\n\nThe Tarski–Vaught test is used to characterize the elementary substructures of a given structure.\n\n"}
{"id": "36643448", "url": "https://en.wikipedia.org/wiki?curid=36643448", "title": "Equidissection", "text": "Equidissection\n\nIn geometry, an equidissection is a partition of a polygon into triangles of equal area. The study of equidissections began in the late 1960s with Monsky's theorem, which states that a square cannot be equidissected into an odd number of triangles. In fact, most polygons cannot be equidissected at all.\n\nMuch of the literature is aimed at generalizing Monsky's theorem to broader classes of polygons. The general question is: Which polygons can be equidissected into how many pieces? Particular attention has been given to trapezoids, kites, regular polygons, centrally symmetric polygons, polyominos, and hypercubes.\n\nEquidissections do not have many direct applications. They are considered interesting because the results are counterintuitive at first, and for a geometry problem with such a simple definition, the theory requires some surprisingly sophisticated algebraic tools. Many of the results rely upon extending \"p\"-adic valuations to the real numbers and extending Sperner's lemma to more general colored graphs.\n\nA \"dissection\" of a polygon \"P\" is a finite set of triangles that do not overlap and whose union is all of \"P\". A dissection into \"n\" triangles is called an \"n\"-dissection, and it is classified as an \"even dissection\" or an \"odd dissection\" according to whether \"n\" is even or odd.\n\nAn \"equidissection\" is a dissection in which every triangle has the same area. For a polygon \"P\", the set of all \"n\" for which an \"n\"-equidissection of \"P\" exists is called the \"spectrum\" of \"P\" and denoted \"S\"(\"P\"). A general theoretical goal is to compute the spectrum of a given polygon.\n\nA dissection is called \"simplicial\" if the triangles meet only along common edges. Some authors restrict their attention to simplicial dissections, especially in the secondary literature, since they are easier to work with. For example, the usual statement of Sperner's lemma applies only to simplicial dissections. Often simplicial dissections are called \"triangulations\", although the vertices of the triangles are not restricted to the vertices or edges of the polygon. Simplicial equidissections are therefore also called \"equal-area triangulations\".\n\nThe terms can be extended to higher-dimensional polytopes: an equidissection is set of simplexes having the same \"n\"-volume.\n\nIt is easy to find an \"n\"-equidissection of a triangle for all \"n\". As a result, if a polygon has an \"m\"-equidissection, then it also has an \"mn\"-equidissection for all \"n\". In fact, often a polygon's spectrum consists precisely of the multiples of some number \"m\"; in this case, both the spectrum and the polygon are called \"principal\" and the spectrum is denoted formula_1. For example, the spectrum of a triangle is formula_2. A simple example of a non-principal polygon is the quadrilateral with vertices (0, 0), (1, 0), (0, 1), (3/2, 3/2); its spectrum includes 2 and 3 but not 1.\n\nAffine transformations of the plane are useful for studying equidissections, including translations, uniform and non-uniform scaling, reflections, rotations, shears, and other similarities and linear maps. Since an affine transformation preserves straight lines and ratios of areas, it sends equidissections to equidissections. This means that one is free to apply any affine transformation to a polygon that might give it a more manageable form. For example, it is common to choose coordinates such that three of the vertices of a polygon are (0, 1), (0, 0), and (1, 0).\n\nThe fact that affine transformations preserve equidissections also means that certain results can be easily generalized. All results stated for a regular polygon also hold for affine-regular polygons; in particular, results concerning the unit square also apply to other parallelograms, including rectangles and rhombuses. All results stated for polygons with integer coordinates also apply to polygons with rational coordinates, or polygons whose vertices fall on any other lattice.\n\nMonsky's theorem states that a square has no odd equidissections, so its spectrum is formula_3. More generally, it is known that centrally symmetric polygons and polyominos have no odd equidissections. A conjecture by Stein proposes that no \"special polygon\" has an odd equidissection, where a special polygon is one whose equivalence classes of parallel edges each sum to the zero vector. Squares, centrally symmetric polygons, polyominos, and polyhexes are all special polygons.\n\nFor \"n\" > 4, the spectrum of a regular \"n\"-gon is formula_4. For \"n\" > 1, the spectrum of an \"n\"-dimensional cube is formula_5, where \"n\"! is the factorial of \"n\".\n\nLet \"T\"(\"a\") be a trapezoid where \"a\" is the ratio of parallel side lengths. If \"a\" is a rational number, then \"T\"(\"a\") is principal. In fact, if \"r\"/\"s\" is a fraction in lowest terms, then formula_6. More generally, all convex polygons with rational coordinates can be equidissected, although not all of them are principal; see the above example of a kite with a vertex at (3/2, 3/2).\n\nAt the other extreme, if \"a\" is a transcendental number, then \"T\"(\"a\") has no equidissection. More generally, no polygon whose vertex coordinates are algebraically independent has an equidissection. This means that almost all polygons with more than three sides cannot be equidissected. Although most polygons cannot be cut into equal-area triangles, all polygons can be cut into equal-area quadrilaterals.\n\nIf \"a\" is an algebraic irrational number, then \"T\"(\"a\") is a trickier case. If \"a\" is algebraic of degree 2 or 3 (quadratic or cubic), and its conjugates all have positive real parts, then \"S\"(\"T\"(\"a\")) contains all sufficiently large \"n\" such that \"n\"/(1 + \"a\") is an algebraic integer. It is conjectured that a similar condition involving stable polynomials may determine whether or not the spectrum is empty for algebraic numbers \"a\" of all degrees.\n\nThe idea of an equidissection seems like the kind of elementary geometric concept that should be quite old. remark of Monsky's theorem, \"one could have guessed that surely the answer must have been known for a long time (if not to the Greeks).\" But the study of equidissections did not begin until 1965, when Fred Richman was preparing a master's degree exam at New Mexico State University.\n\nRichman wanted to include a question on geometry in the exam, and he noticed that it was difficult to find (what is now called) an odd equidissection of a square. Richman proved to himself that it was impossible for 3 or 5, that the existence of an \"n\"-equidissection implies the existence of an -dissection, and that certain quadrilaterals arbitrarily close to being squares have odd equidissections. However, he did not solve the general problem of odd equidissections of squares, and he left it off the exam. Richman's friend John Thomas became interested in the problem; in his recollection,\n\nThomas proved that an odd equidissection was impossible if the coordinates of the vertices are rational numbers with odd denominators. He submitted this proof to \"Mathematics Magazine\", but it was put on hold:\n\nThe question was instead given as an Advanced Problem in the \"American Mathematical Monthly\" . When nobody else submitted a solution, the proof was published in \"Mathematics Magazine\" , three years after it was written. then built on Thomas' argument to prove that there are no odd equidissections of a square, without any rationality assumptions.\n\nMonsky's proof relies on two pillars: a combinatorial result that generalizes Sperner's lemma and an algebraic result, the existence of a 2-adic valuation on the real numbers. A clever coloring of the plane then implies that in all dissections of the square, at least one triangle has an area with what amounts to an even denominator, and therefore all equidissections must be even. The essence of the argument is found already in , but was the first to use a 2-adic valuation to cover dissections with arbitrary coordinates.\n\nThe first generalization of Monsky's theorem was , who proved that the spectrum of an \"n\"-dimensional cube is formula_5. The proof is revisited by .\n\nGeneralization to regular polygons arrived in 1985, during a geometry seminar run by G. D. Chakerian at UC Davis. Elaine Kasimatis, a graduate student, \"was looking for some algebraic topic she could slip into\" the seminar. Sherman Stein suggested dissections of the square and the cube: \"a topic that Chakerian grudgingly admitted was geometric.\" After her talk, Stein asked about regular pentagons. Kasimatis answered with , proving that for \"n\" > 5, the spectrum of a regular \"n\"-gon is formula_4. Her proof builds on Monsky's proof, extending the \"p\"-adic valuation to the complex numbers for each prime divisor of \"n\" and applying some elementary results from the theory of cyclotomic fields. It is also the first proof to explicitly use an affine transformation to set up a convenient coordinate system. then framed the problem of finding the spectrum of a general polygon, introducing the terms \"spectrum\" and \"principal\". They proved that almost all polygons lack equidissections, and that not all polygons are principal.\n\nAttempting to generalize the results for regular \"n\"-gons for even \"n\", conjectured that no centrally symmetric polygon has an odd equidissection, and he proved the \"n\" = 6 and \"n\" = 8 cases. The full conjecture was proved by . A decade later, Stein made what he describes as \"a surprising breakthrough\", conjecturing that no polyomino has an odd equidissection. He proved the result of a polyomino with an odd number of squares in . The full conjecture was proved when treated the even case.\n\nThe topic of equidissections has recently been popularized by treatments in \"The Mathematical Intelligencer\" , a volume of the Carus Mathematical Monographs , and the fourth edition of \"Proofs from THE BOOK\" .\n\n consider a variation of the problem: Given a convex polygon \"K\", how much of its area can be covered by \"n\" non-overlapping triangles of equal area inside \"K\"? The ratio of the area of the best possible coverage to the area of \"K\" is denoted \"t\"(\"K\"). If \"K\" has an \"n\"-equidissection, then \"t\"(\"K\") = 1; otherwise it is less than 1. The authors show that for a quadrilateral \"K\", \"t\"(\"K\") ≥ 4\"n\"/(4\"n\" + 1), with \"t\"(\"K\") = 8/9 if and only if \"K\" is affinely congruent to the trapezoid \"T\"(2/3). For a pentagon, \"t\"(\"K\") ≥ 2/3, \"t\"(\"K\") ≥ 3/4, and \"t\"(\"K\") ≥ 2\"n\"/(2\"n\" + 1) for \"n\" ≥ 5.\n\nGünter M. Ziegler asked the converse problem in 2003: Given a dissection of the whole of a polygon into \"n\" triangles, how close can the triangle areas be to equal? In particular, what is the smallest possible difference between the areas of the smallest and largest triangles? Let the smallest difference be \"M\"(\"n\") for a square and \"M\"(\"a\", \"n\") for the trapezoid \"T\"(\"a\"). Then \"M\"(\"n\") is 0 for even \"n\" and greater than 0 for odd \"n\". gave the asymptotic upper bound \"M\"(\"n\") = O(1/\"n\") (see Big O notation). improves the bound to \"M\"(\"n\") = O(1/\"n\") with a better dissection, and he proves that there exist values of \"a\" for which \"M\"(\"a\", \"n\") decreases arbitrarily quickly. obtain a superpolynomial upper bound, derived from an explicit construction that uses the Thue–Morse sequence.\n\n\n"}
{"id": "15895296", "url": "https://en.wikipedia.org/wiki?curid=15895296", "title": "Equisatisfiability", "text": "Equisatisfiability\n\nIn logic, two formulae are equisatisfiable if the first formula is satisfiable whenever the second is and vice versa; in other words, either both formulae are satisfiable or both are not. Equisatisfiable formulae may disagree, however, for a particular choice of variables. As a result, equisatisfiability is different from logical equivalence, as two equivalent formulae always have the same models.\n\nEquisatisfiability is generally used in the context of translating formulae, so that one can define a translation to be correct if the original and resulting formulae are equisatisfiable. Examples of translations involving this concept are Skolemization and some translations into conjunctive normal form. \n\nA translation from propositional logic into propositional logic in which every binary disjunction formula_1 is replaced by formula_2, where formula_3 is a new variable (one for each replaced disjunction) is a transformation in which satisfiability is preserved: the original and resulting formulae are equisatisfiable. Note that these two formulae are not equivalent: the first formula has the model in which formula_4 is true while formula_5 and formula_3 are false, and this is not a model of the second formula, in which formula_3 has to be true in this case.\n"}
{"id": "55711732", "url": "https://en.wikipedia.org/wiki?curid=55711732", "title": "Estelle Basor", "text": "Estelle Basor\n\nEstelle Lucille Basor (born 1947) is an American mathematician interested in operator theory and the theory of random matrices.\nShe is professor emeritus of mathematics at the California Polytechnic State University (Cal Poly), and deputy director of the American Institute of Mathematics.\n\nBasor earned a bachelor's degree in mathematics from the University of California, Santa Cruz in 1969, and completed a Ph.D. there in 1975.\nHer dissertation, supervised by Harold Widom, was \"Asymptotic Formulas for Toeplitz Determinants\".\nShe joined the Cal Poly faculty in 1976, and taught there until retiring in 2008.\n\nAt Cal Poly, she was the 2005 winner of the Distinguished Research, Creative Activity and Professional Development Award, and a colloquium in her honor was held in 2006.\nShe was elected to the 2018 class of fellows of the American Mathematical Society.\n\nBasor's husband, Kent E. Morrison, is also a mathematician who went to school with her at Santa Cruz, worked with her at Cal Poly, and is now associated with the American Institute of Mathematics.\n\n"}
{"id": "35441511", "url": "https://en.wikipedia.org/wiki?curid=35441511", "title": "Fontaine–Mazur conjecture", "text": "Fontaine–Mazur conjecture\n\nIn mathematics, the Fontaine–Mazur conjectures are some conjectures introduced by about when \"p\"-adic representations of Galois groups of number fields can be constructed from representations on étale cohomology groups of a varieties. Some cases of this conjecture in dimension 2 were already proved by .\n\n\n"}
{"id": "3660078", "url": "https://en.wikipedia.org/wiki?curid=3660078", "title": "Formal epistemology", "text": "Formal epistemology\n\nFormal epistemology uses formal methods from decision theory, logic, probability theory and computability theory to model and reason about issues of epistemological interest. Work in this area spans several academic fields, including philosophy, computer science, economics, and statistics. The focus of formal epistemology has tended to differ somewhat from that of traditional epistemology, with topics like uncertainty, induction, and belief revision garnering more attention than the analysis of knowledge, skepticism, and issues with justification.\n\nThough formally oriented epistemologists have been laboring since the emergence of formal logic and probability theory (if not earlier), only recently have they been organized under a common disciplinary title. This gain in popularity may be attributed to the organization of yearly Formal Epistemology Workshops by Branden Fitelson and Sahotra Sarkar, starting in 2004, and the PHILOG-conferences starting in 2002 (The Network for Philosophical Logic and Its Applications) organized by Vincent F. Hendricks. Carnegie Mellon University's Philosophy Department hosts an annual summer school in logic and formal epistemology. In 2010, the department founded the Center for Formal Epistemology.\n\nSome of the topics that come under the heading of formal epistemology include:\n\n\n\n\n"}
{"id": "43040208", "url": "https://en.wikipedia.org/wiki?curid=43040208", "title": "Frequent subtree mining", "text": "Frequent subtree mining\n\nIn computer science, frequent subtree mining is the problem of finding all patterns in a given database whose support (a metric related to its number of occurrences in other subtrees) is over a given threshold. It is a more general form of the maximum agreement subtree problem.\n\nFrequent subtree mining is the problem of trying to find all of the patterns whose \"support\" is over a certain user-specified level, where \"support\" is calculated as the number of trees in a database which have at least one subtree isomorphic to a given pattern.\n\nThe problem of frequent subtree mining has been formally defined as:\n\nIn 2002, Mohammed J. Zaki introduced TreeMiner, an efficient algorithm for solving the frequent subtree mining problem, which used a \"scope list\" to represent tree nodes and which was contrasted with PatternMatcher, an algorithm based on pattern matching.\n\nDomains in which frequent subtree mining is useful tend to involve complex relationships between data entities: for instance, the analysis of XML documents often requires frequent subtree mining. Another domain where this is useful is the web usage mining problem: since the actions taken by users when visiting a web site can be recorded and categorized in many different ways, complex databases of trees need to be analyzed with frequent subtree mining. Other domains in which frequent subtree mining is useful include computational biology, RNA structure analysis, pattern recognition, bioinformatics, and analysis of the KEGG GLYCAN database.\n\nChecking whether a pattern (or a transaction) supports a given subgraph is an NP-complete problem, since it is an NP-complete instance of the subgraph isomorphism problem. Furthermore, due to combinatorial explosion, according to Lei et al., \"mining all frequent subtree patterns becomes infeasible for a large and dense tree database\".\n"}
{"id": "41104382", "url": "https://en.wikipedia.org/wiki?curid=41104382", "title": "Frobenius category", "text": "Frobenius category\n\nIn category theory, a branch of mathematics, a Frobenius category is an exact category in which the notions of injective objects and projective objects coincide and there are enough injectives (or equivalently enough projectives). It is an analog of a Frobenius algebra.\n\nThe stable category of a Frobenius category is canonically a triangulated category.\n\n"}
{"id": "24774661", "url": "https://en.wikipedia.org/wiki?curid=24774661", "title": "Geoffrey Hellman", "text": "Geoffrey Hellman\n\nGeoffrey Hellman (born August 16, 1943) is an American professor and philosopher. He is Professor of Philosophy at the University of Minnesota in Minneapolis, Minnesota.\n\nHe obtained his B.A. (1965) and Ph.D. (1972) degrees in philosophy from Harvard University.\n\n\n\n\n\n"}
{"id": "614147", "url": "https://en.wikipedia.org/wiki?curid=614147", "title": "Knuth–Bendix completion algorithm", "text": "Knuth–Bendix completion algorithm\n\nThe Knuth – Bendix completion algorithm (named after Donald Knuth and Peter Bendix) is a semi-decision algorithm for transforming a set of equations (over terms) into a confluent term rewriting system. When the algorithm succeeds, it effectively solves the word problem for the specified algebra.\n\nBuchberger's algorithm for computing Gröbner bases is a very similar algorithm. Although developed independently, it may also be seen as the instantiation of Knuth–Bendix algorithm in the theory of polynomial rings.\n\nFor a set \"E\" of equations, its deductive closure () is the set of all equations that can be derived by applying equations from \"E\" in any order.\nFormally, \"E\" is considered a binary relation, () is its rewrite closure, and () is the equivalence closure of ().\nFor a set \"R\" of rewrite rules, its deductive closure ( ∘ ) is the set of all equations than can be confirmed by applying rules from \"R\" left-to-right to both sides until they are literally equal.\nFormally, \"R\" is again viewed as binary relation, () is its rewrite closure, () is its converse, and ( ∘ ) is the relation composition of their reflexive transitive closures ( and ).\n\nFor example, if are the group axioms, the derivation chain\ndemonstrates that \"a\"⋅(\"a\"⋅\"b\") \"b\" is a member of \"E\"'s deductive closure.\nIf is a \"rewrite rule\" version of \"E\", the derivation chains\ndemonstrate that (\"a\"⋅\"a\")⋅\"b\" ∘ \"b\"⋅1 is a member of \"R\"'s deductive closure.\nHowever, there is no way to derive \"a\"⋅(\"a\"⋅\"b\") ∘ \"b\" similar to above, since a right-to-left application of the rule is not allowed.\n\nThe Knuth–Bendix algorithm takes a set \"E\" of equations between terms, and a reduction ordering (>) on the set of all terms, and attempts to construct a confluent and terminating term rewriting system \"R\" that has the same deductive closure as \"E\".\nWhile proving consequences from \"E\" often requires human intuition, proving consequences from \"R\" does not.\nFor more details, see Confluence (abstract rewriting)#Motivating examples, which gives an example proof from group theory, performed both using \"E\" and using \"R\".\n\nGiven a set \"E\" of equations between terms, the following inference rules can be used to transform it into an equivalent convergent term rewrite system (if possible):\nThey are based on a user-given reduction ordering (>) on the set of all terms; it is lifted to a well-founded ordering (▻) on the set of rewrite rules by defining if \n\nThe following example run, obtained from the E theorem prover, computes a completion of the (additive) group axioms as in Knuth, Bendix (1970).\nIt starts with the three initial equations for the group (neutral element 0, inverse elements, associativity), using codice_1 for \"X\"+\"Y\", and codice_2 for −\"X\". \nThe 10 equations marked with \"final\" represent the resulting convergent rewrite system.\n\"pm\" is short for \"paramodulation\", implementing \"deduce\". Critical pair computation is an instance of paramodulation for equational unit clauses.\n\"rw\" is rewriting, implementing \"compose\", \"collapse\", and \"simplify\".\nOrienting of equations is done implicitly and not recorded.\n\nSee also Word problem (mathematics) for another presentation of this example.\n\nAn important case in computational group theory are string rewriting systems which can be used to give canonical labels to elements or cosets of a finitely presented group as products of the generators. This special case is the focus of this section.\n\nThe critical pair lemma states that a term rewriting system is locally confluent (or weakly confluent) if and only if all its critical pairs are convergent. Furthermore, we have Newman's lemma which states that if an (abstract) rewriting system is strongly normalizing and weakly confluent, then the rewriting system is confluent. So, if we can add rules to the term rewriting system in order to force all critical pairs to be convergent while maintaining the strong normalizing property, then this will force the resultant rewriting system to be confluent.\n\nConsider a finitely presented monoid formula_1 where X is a finite set of generators and R is a set of defining relations on X. Let X be the set of all words in X (i.e. the free monoid generated by X). Since the relations R generate an equivalence relation on X*, one can consider elements of M to be the equivalence classes of X under R. For each class \"{w, w, ... }\" it is desirable to choose a standard representative \"w\". This representative is called the canonical or normal form for each word \"w\" in the class. If there is a computable method to determine for each \"w\" its normal form \"w\" then the word problem is easily solved. A confluent rewriting system allows one to do precisely this.\n\nAlthough the choice of a canonical form can theoretically be made in an arbitrary fashion this approach is generally not computable. (Consider that an equivalence relation on a language can produce an infinite number of infinite classes.) If the language is well-ordered then the order < gives a consistent method for defining minimal representatives, however computing these representatives may still not be possible. In particular, if a rewriting system is used to calculate minimal representatives then the order < should also have the property:\n\nThis property is called translation invariance. An order that is both translation-invariant and a well-order is called a reduction order.\n\nFrom the presentation of the monoid it is possible to define a rewriting system given by the relations R. If A x B is in R then either A < B in which case B → A is a rule in the rewriting system, otherwise A > B and A → B. Since < is a reduction order a given word W can be reduced W > W_1 > ... > W_n where W_n is irreducible under the rewriting system. However, depending on the rules that are applied at each W → W it is possible to end up with two different irreducible reductions W ≠ W' of W. However, if the rewriting system given by the relations is converted to a confluent rewriting system via the Knuth–Bendix algorithm, then all reductions are guaranteed to produce the same irreducible word, namely the normal form for that word.\n\nSuppose we are given a presentation formula_2, where formula_3 is a set of generators and formula_4 is a set of relations giving the rewriting system. Suppose further that we have a reduction ordering formula_5 among the words generated by formula_3(e.g., shortlex order). For each relation formula_7 in formula_4, suppose formula_9. Thus we begin with the set of reductions formula_10.\n\nFirst, if any relation formula_7 can be reduced, replace formula_12 and formula_13 with the reductions.\n\nNext, we add more reductions (that is, rewriting rules) to eliminate possible exceptions of confluence. Suppose that formula_12 and formula_15, where formula_16, overlap.\n\n\nReduce the word formula_29 using formula_12 first, then using formula_15 first. Call the results formula_32, respectively. If formula_33, then we have an instance where confluence could fail. Hence, add the reduction formula_34 to formula_4.\n\nAfter adding a rule to formula_4, remove any rules in formula_4 that might have reducible left sides.\n\nRepeat the procedure until all overlapping left sides have been checked.\n\nConsider the monoid: \nWe use the shortlex order. This is an infinite monoid but nevertheless, the Knuth–Bendix algorithm is able to solve the word problem.\n\nOur beginning three reductions are therefore\n\nA suffix of formula_39 (namely formula_40) is a prefix of formula_41, so consider the word formula_42. Reducing using (), we get formula_43. Reducing using (), we get formula_44. Hence, we get formula_45, giving the reduction rule\n\nSimilarly, using formula_46 and reducing using () and (), we get formula_47. Hence the reduction\n\nBoth of these rules obsolete (), so we remove it.\n\nNext, consider formula_48 by overlapping () and (). Reducing we get formula_49, so we add the rule\nConsidering formula_50 by overlapping () and (), we get formula_51, so we add the rule\nThese obsolete rules () and (), so we remove them.\n\nNow, we are left with the rewriting system\n\nChecking the overlaps of these rules, we find no potential failures of confluence. Therefore, we have a confluent rewriting system, and the algorithm terminates successfully.\n\nThe order of the generators may crucially affect whether the Knuth–Bendix completion terminates. As an example, consider the free Abelian group by the monoid presentation:\n\nThe Knuth–Bendix completion with respect to lexicographic order formula_53 finishes with a convergent system, however considering the length-lexicographic order formula_54 it does not finish for there are no finite convergent systems compatible with this latter order.\n\nIf Knuth–Bendix does not succeed, it will either run forever, or fail when it encounters an unorientable equation (i.e. an equation that it cannot turn into a rewrite rule). The enhanced completion without failure will not fail on unorientable equations and provides a semi-decision procedure for the word problem.\n\nThe notion of logged rewriting discussed in the paper by Heyworth and Wensley listed below allows some recording or logging of the rewriting process as it proceeds. This is useful for computing identities among relations for presentations of groups.\n\n"}
{"id": "3759614", "url": "https://en.wikipedia.org/wiki?curid=3759614", "title": "Kronecker's lemma", "text": "Kronecker's lemma\n\nIn mathematics, Kronecker's lemma (see, e.g., ) is a result about the relationship between convergence of infinite sums and convergence of sequences. The lemma is often used in the proofs of theorems concerning sums of independent random variables such as the strong Law of large numbers. The lemma is named after the German mathematician Leopold Kronecker.\n\nIf formula_1 is an infinite sequence of real numbers such that \nexists and is finite, then we have for all formula_3 and formula_4 that \n\nLet formula_6 denote the partial sums of the \"x\"'s. Using summation by parts,\nPick any \"ε\" > 0. Now choose \"N\" so that formula_6 is \"ε\"-close to \"s\" for \"k\" > \"N\". This can be done as the sequence formula_6 converges to \"s\". Then the right hand side is:\nNow, let \"n\" go to infinity. The first term goes to \"s\", which cancels with the third term. The second term goes to zero (as the sum is a fixed value). Since the \"b\" sequence is increasing, the last term is bounded by formula_13.\n"}
{"id": "31714734", "url": "https://en.wikipedia.org/wiki?curid=31714734", "title": "Kummer's congruence", "text": "Kummer's congruence\n\nIn mathematics, Kummer's congruences are some congruences involving Bernoulli numbers, found by .\n\nThe simplest form of Kummer's congruence states that\nwhere \"p\" is a prime, \"h\" and \"k\" are positive even integers not divisible by \"p\"−1 and the numbers \"B\" are Bernoulli numbers.\n\nMore generally if \"h\" and \"k\" are positive even integers not divisible by \"p\" − 1, then\nwhenever\n\nwhere φ(\"p\") is the Euler totient function, evaluated at \"p\" and \"a\" is a non negative integer. At \"a\" = 0, the expression takes the simpler form, as seen above.\nThe two sides of the Kummer congruence are essentially values of the p-adic zeta function, and the Kummer congruences imply that the \"p\"-adic zeta function for negative integers is continuous, so can be extended by continuity to all \"p\"-adic integers.\n\n\n"}
{"id": "33695362", "url": "https://en.wikipedia.org/wiki?curid=33695362", "title": "List of things named after Pythagoras", "text": "List of things named after Pythagoras\n\nThis is a list of things named after Pythagoras, the ancient Greek philosopher, mystic, mathematician, and music theorist.\n\n\n\n\n"}
{"id": "29139836", "url": "https://en.wikipedia.org/wiki?curid=29139836", "title": "Lorenz asymmetry coefficient", "text": "Lorenz asymmetry coefficient\n\nThe Lorenz asymmetry coefficient (LAC) is a summary statistic of the Lorenz curve that measures the degree of asymmetry of the curve. The Lorenz curve is used to describe the inequality in the distribution of a quantity (usually income or wealth in economics, or size or reproductive output in ecology). The most common summary statistic for the Lorenz curve is the Gini coefficient, which is an overall measure of inequality within the population. The Lorenz asymmetry coefficient can be a useful supplement to the Gini coefficient. The Lorenz asymmetry coefficient is defined as\n\nwhere the functions \"F\" and \"L\" are defined as for the Lorenz curve, and \"μ\" is the mean. If \"S\" > 1, then the point where the Lorenz curve is parallel with the line of equality is above the axis of symmetry. Correspondingly, if \"S\" < 1, then the point where the Lorenz curve is parallel to the line of equality is below the axis of symmetry.\n\nIf data arise from the log-normal distribution, then \"S\" = 1, i.e., the Lorenz curve is symmetric.\n\nThe sample statistic \"S\" can be calculated from \"n\" ordered size data, formula_2, using the following equations:\n\nwhere \"m\" is the number of individuals with a size or wealth less than \"μ\" and formula_6. However, if one or more of the data size is equal to \"μ\", then S has to defined as an interval instead of a number (see #LAC interval when some data is equal to μ).\n\nThe Lorenz asymmetry coefficient characterizes an important aspect of the shape of a Lorenz curve. It tells which size or wealth classes contribute most to the population’s total inequality, as measured by the Gini coefficient. If the LAC is less than 1, the inequality is primarily due to the relatively many small or poor individuals. If the LAC is greater than 1, the inequality is primarily due to the few largest or wealthiest individuals.\n\nFor incomes distributed according to a log-normal distribution, the LAC is identically 1.\n\nThe above formulas assume that none of the data values are equal to \"μ\"; strictly speaking we assume that data sizes are continuously distributed, so that formula_7. Otherwise, if one or more of formula_8, then a section of the Lorenz curve is parallel to the diagonal, and S has to be defined as an interval instead of a number. The interval can be defined as follows:\n\nformula_9\n\nwhere \"a\" is the number of data values that are equal to \"μ\".\n\n"}
{"id": "17878029", "url": "https://en.wikipedia.org/wiki?curid=17878029", "title": "Markov information source", "text": "Markov information source\n\nIn mathematics, a Markov information source, or simply, a Markov source, is an information source whose underlying dynamics are given by a stationary finite Markov chain.\n\nAn information source is a sequence of random variables ranging over a finite alphabet Γ, having a stationary distribution.\n\nA Markov information source is then a (stationary) Markov chain \"M\", together with a function\n\nthat maps states \"S\" in the Markov chain to letters in the alphabet Γ.\n\nA unifilar Markov source is a Markov source for which the values formula_2 are distinct whenever each of the states formula_3 are reachable, in one step, from a common prior state. Unifilar sources are notable in that many of their properties are far more easily analyzed, as compared to the general case.\n\nMarkov sources are commonly used in communication theory, as a model of a transmitter. Markov sources also occur in natural language processing, where they are used to represent hidden meaning in a text. Given the output of a Markov source, whose underlying Markov chain is unknown, the task of solving for the underlying chain is undertaken by the techniques of hidden Markov models, such as the Viterbi algorithm.\n\n\n"}
{"id": "4230329", "url": "https://en.wikipedia.org/wiki?curid=4230329", "title": "Mathematics Magazine", "text": "Mathematics Magazine\n\nMathematics Magazine is a refereed bimonthly publication of the Mathematical Association of America. Its intended audience is teachers of collegiate mathematics, especially at the junior/senior level, and their students. It is explicitly a journal of mathematics rather than pedagogy. Rather than articles in the terse \"theorem-proof\" style of research journals, it seeks articles which provide a context for the mathematics they deliver, with examples, applications, illustrations, and historical background.\nPaid circulation in 2008 was 9,500 and total circulation was 10,000.\n\n\"Mathematics Magazine\" is a continuation of \"Mathematics News Letter\" (1926-1934) and \"National Mathematics Magazine\" (1934-1945.) Doris Schattschneider became the first female editor of \"Mathematics Magazine\" in 1981. \n\nThe MAA gives the Carl B. Allendoerfer Awards annually \"for articles of expository excellence\" published in \"Mathematics Magazine\".\n\n\n"}
{"id": "14188295", "url": "https://en.wikipedia.org/wiki?curid=14188295", "title": "Moving average crossover", "text": "Moving average crossover\n\nIn the statistics of time series, and in particular the analysis of financial time series for stock trading purposes, a moving-average crossover occurs when, on plotting two moving averages each based on different degrees of smoothing, the traces of these moving averages cross. It does not predict future direction but shows trends. This indicator uses two (or more) moving averages, a slower moving average and a faster moving average. The faster moving average is a short term moving average. For end-of-day stock markets, for example, it may be 5-, 10- or 25-day period while the slower moving average is medium or long term moving average (e.g. 50-, 100- or 200-day period). A short term moving average is faster because it only considers prices over short period of time and is thus more reactive to daily price changes. On the other hand, a long term moving average is deemed slower as it encapsulates prices over a longer period and is more lethargic. However, it tends to smooth out price noises which are often reflected in short term moving averages.\n\nA moving average, as a line by itself, is often overlaid in price charts to indicate price trends. A crossover occurs when a faster moving average (i.e., a shorter period moving average) crosses a slower moving average (i.e. a longer period moving average). In other words, this is when the shorter period moving average line crosses a longer period moving average line. In stock investing, this meeting point is used either to enter (buy or sell) or exit (sell or buy) the market.\n\nThe particular case where simple equally weighted moving-averages are used is sometimes called a simple moving-average (SMA) crossover. Such a crossover can be used to signal a change in trend and can be used to trigger a trade in a Black Box trading system.\n\nGolden cross-There are several types of moving average cross traders use in trading. When 200 days simple moving average cross 50 days simple moving average it is called a golden cross. The golden cross is widely accepted.\n\nSilver cross-When 50 Exponential moving average cross above or below 100 exponential moving average it is known as a silver cross. Silver cross invented by a Successful forex trader S.A.Hasib. S.A.Hasib believe that exponential moving average helps to understand current market conditions. The exponential moving average also acts as a dynamic support and resistance. \n"}
{"id": "696794", "url": "https://en.wikipedia.org/wiki?curid=696794", "title": "Nemeth Braille", "text": "Nemeth Braille\n\nThe Nemeth Braille Code for Mathematics is a Braille code for encoding mathematical and scientific notation linearly using standard six-dot Braille cells for tactile reading by the visually impaired. The code was developed by Abraham Nemeth. The Nemeth Code was first written up in 1952. It was revised in 1956, 1965, and 1972, and beginning in 1992 was integrated into Unified English Braille. It is an example of a compact human-readable markup language.\n\nNemeth Braille is just one code used to write mathematics in braille. There are many systems in use around the world.\n\nThe Nemeth Code Book (1972) opens with the following words:\n\nThis Braille Code for Mathematics and Science Notation has been prepared to provide a system of symbols which will allow technical literature to be presented and read in braille. The Code is intended to convey as accurate an impression as is possible to the braille reader of the corresponding printed text, and this is one of its principal features. When the braille reader has a clear conception of the corresponding printed text, the area of communication between himself and his teacher, his colleagues, his associates, and the world at large is greatly broadened. \"A test of the accuracy with which the Code conveys information from the print to the braille text is to effect a transcription in the reverse direction. The amount of agreement between the original printed text and one transcribed from the braille is a measure of the Code's accuracy\".\n\nOne consequence is that the braille transcriber does not need to know the underlying mathematics.\nThe braille transcriber needs to identify the inkprint symbols and know how to render them in\nNemeth Code braille. For example, if the same math symbol might have two different meanings, this would\nnot matter; both instances would be brailled the same. This is in contrast to the International\nBraille Music Code, where the braille depends on the meaning of the inkprint music. Thus a knowledge of\nmusic is required to produce braille music.\n\nGreek and Latin letters are based on the assignments of International Greek Braille.\n\n\n"}
{"id": "2284654", "url": "https://en.wikipedia.org/wiki?curid=2284654", "title": "Nonelementary problem", "text": "Nonelementary problem\n\nIn computational complexity theory, a nonelementary problem is a problem that is not a member of the class ELEMENTARY.\n\nExamples of nonelementary problems that are nevertheless decidable include:\n"}
{"id": "2544098", "url": "https://en.wikipedia.org/wiki?curid=2544098", "title": "Orientation (geometry)", "text": "Orientation (geometry)\n\nIn geometry the orientation, angular position, or attitude of an object such as a line, plane or rigid body is part of the description of how it is placed in the space it occupies.\nNamely, it is the imaginary rotation that is needed to move the object from a reference placement to its current placement. A rotation may not be enough to reach the current placement. It may be necessary to add an imaginary translation, called the object's location (or position, or linear position). The location and orientation together fully describe how the object is placed in space. The above-mentioned imaginary rotation and translation may be thought to occur in any order, as the orientation of an object does not change when it translates, and its location does not change when it rotates.\n\nEuler's rotation theorem shows that in three dimensions any orientation can be reached with a single rotation around a fixed axis. This gives one common way of representing the orientation using an axis–angle representation. Other widely used methods include rotation quaternions, Euler angles, or rotation matrices. More specialist uses include Miller indices in crystallography, strike and dip in geology and grade on maps and signs.\n\nTypically, the orientation is given relative to a frame of reference, usually specified by a Cartesian coordinate system.\n\nIn general the position and orientation in space of a rigid body are defined as the position and orientation, relative to the main reference frame, of another reference frame, which is fixed relative to the body, and hence translates and rotates with it (the body's \"local reference frame\", or \"local coordinate system\"). At least three independent values are needed to describe the orientation of this local frame. Three other values are \nAll the points of the body change their position during a rotation except for those lying on the rotation axis. If the rigid body has rotational symmetry not all orientations are distinguishable, except by observing how the orientation evolves in time from a known starting orientation. For example, the orientation in space of a line, line segment, or vector can be specified with only two values, for example two direction cosines. Another example is the position of a point on the earth, often described using the orientation of a line joining it with the earth's center, measured using the two angles of longitude and latitude. Likewise, the orientation of a plane can be described with two values as well, for instance by specifying the orientation of a line normal to that plane, or by using the strike and dip angles.\n\nFurther details about the mathematical methods to represent the orientation of rigid bodies and planes in three dimensions are given in the following sections.\n\nIn two dimensions the orientation of any object (line, vector, or plane figure) is given by a single value: the angle through which it has rotated. There is only one degree of freedom and only one fixed point about which the rotation takes place.\n\nSeveral methods to describe orientations of a rigid body in three dimensions have been developed. They are summarized in the following sections.\n\nThe first attempt to represent an orientation was owed to Leonhard Euler. He imagined three reference frames that could rotate one around the other, and realized that by starting with a fixed reference frame and performing three rotations, he could get any other reference frame in the space (using two rotations to fix the vertical axis and another to fix the other two axes). The values of these three rotations are called Euler angles.\n\nThese are three angles, also known as yaw, pitch and roll, Navigation angles and Cardan angles. Mathematically they constitute a set of six possibilities inside the twelve possible sets of Euler angles, the ordering being the one best used for describing the orientation of a vehicle such as an airplane. In aerospace engineering they are usually referred to as Euler angles.\n\nEuler also realized that the composition of two rotations is equivalent to a single rotation about a different fixed axis (Euler's rotation theorem). Therefore, the composition of the former three angles has to be equal to only one rotation, whose axis was complicated to calculate until matrices were developed.\n\nBased on this fact he introduced a vectorial way to describe any rotation, with a vector on the rotation axis and module equal to the value of the angle. Therefore, any orientation can be represented by a rotation vector (also called Euler vector) that leads to it from the reference frame. When used to represent an orientation, the rotation vector is commonly called orientation vector, or attitude vector.\n\nA similar method, called axis–angle representation, describes a rotation or orientation using a unit vector aligned with the rotation axis, and a separate value to indicate the angle (see figure).\n\nWith the introduction of matrices the Euler theorems were rewritten. The rotations were described by orthogonal matrices referred to as rotation matrices or direction cosine matrices. When used to represent an orientation, a rotation matrix is commonly called orientation matrix, or attitude matrix.\n\nThe above-mentioned Euler vector is the eigenvector of a rotation matrix (a rotation matrix has a unique real eigenvalue). \nThe product of two rotation matrices is the composition of rotations. Therefore, as before, the orientation can be given as the rotation from the initial frame to achieve the frame that we want to describe.\n\nThe configuration space of a non-symmetrical object in \"n\"-dimensional space is SO(\"n\") × R. Orientation may be visualized by attaching a basis of tangent vectors to an object. The direction in which each vector points determines its orientation.\n\nAnother way to describe rotations is using rotation quaternions, also called versors. They are equivalent to rotation matrices and rotation vectors. With respect to rotation vectors, they can be more easily converted to and from matrices. When used to represent orientations, rotation quaternions are typically called orientation quaternions or attitude quaternions.\n\nThe attitude of a lattice plane is the orientation of the line normal to the plane, and is described by the plane's Miller indices. In three-space a family of planes (a series of parallel planes) can be denoted by its Miller indices (\"hkl\"), so the family of planes has an attitude common to all its constituent planes.\n\nMany features observed in geology are planes or lines, and their orientation is commonly referred to as their \"attitude\". These attitudes are specified with two angles.\n\nFor a line, these angles are called the \"trend\" and the \"plunge\". The trend is the compass direction of the line, and the plunge is the downward angle it makes with a horizontal plane.\n\nFor a plane, the two angles are called its \"strike (angle)\" and its \"dip (angle)\". A \"strike line\" is the intersection of a horizontal plane with the observed planar feature (and therefore a horizontal line), and the strike angle is the \"bearing\" of this line (that is, relative to geographic north or from magnetic north). The dip is the angle between a horizontal plane and the observed planar feature as observed in a third vertical plane perpendicular to the strike line.\n\nThe attitude of a rigid body is its orientation as described, for example, by the orientation of a frame fixed in the body relative to a fixed reference frame. The attitude is described by \"attitude coordinates\", and consists of at least three coordinates. One scheme for orienting a rigid body is based upon body-axes rotation; successive rotations three times about the axes of the body's fixed reference frame, thereby establishing the body's Euler angles. Another is based upon roll, pitch and yaw, although these terms also refer to incremental deviations from the nominal attitude\n\n"}
{"id": "1058833", "url": "https://en.wikipedia.org/wiki?curid=1058833", "title": "Orthogonal complement", "text": "Orthogonal complement\n\nIn the mathematical fields of linear algebra and functional analysis, the orthogonal complement of a subspace \"W\" of a vector space \"V\" equipped with a bilinear form \"B\" is the set \"W\" of all vectors in \"V\" that are orthogonal to every vector in \"W\". Informally, it is called the perp, short for perpendicular complement. It is a subspace of \"V\".\n\nIn the case that \"W\" is the subspace of formula_1 (with the usual dot product) spanned by the rows of the next matrix,\n\nformula_2\n\nits orthogonal complement \"W\" is spanned by the three row-vectors of\nformula_3.\n\nThe fact that every vector on the first list is orthogonal to every vector on the second list can be checked by direct computation. The fact that the spans of these vectors are orthogonal then follows by bilinearity of the dot product. Finally, the fact that these spaces are orthogonal complements follows from the dimension relationships given below.\n\nLet formula_4 be a vector space over a field formula_5 equipped with a bilinear form formula_6. We define formula_7 to be left-orthogonal to formula_8, and formula_8 to be right-orthogonal to formula_7, when formula_11. For a subset formula_12 of formula_4 we define the left orthogonal complement formula_14 to be\n\nThere is a corresponding definition of right orthogonal complement. For a reflexive bilinear form, where formula_11 implies formula_17 for all formula_7 and formula_8 in formula_4, the left and right complements coincide. This will be the case if formula_6 is a symmetric or an alternating form.\n\nThe definition extends to a bilinear form on a free module over a commutative ring, and to a sesquilinear form extended to include any free module over a commutative ring with conjugation.\n\n\nThis section considers orthogonal complements in inner product spaces.\n\nThe orthogonal complement is always closed in the metric topology. In finite-dimensional spaces, that is merely an instance of the fact that all subspaces of a vector space are closed. In infinite-dimensional Hilbert spaces, some subspaces are not closed, but all orthogonal complements are closed. In such spaces, the orthogonal complement of the orthogonal complement of formula_12 is the closure of formula_12, i.e.,\n\nSome other useful properties that always hold are the following. Let formula_36 be a Hilbert space and let formula_37 and formula_38 be its linear subspaces. Then:\n\nThe orthogonal complement generalizes to the annihilator, and gives a Galois connection on subsets of the inner product space, with associated closure operator the topological closure of the span.\n\nFor a finite-dimensional inner product space of dimension \"n\", the orthogonal complement of a \"k\"-dimensional subspace is an -dimensional subspace, and the double orthogonal complement is the original subspace:\n\nIf \"A\" is an matrix, where , , and refer to the row space, column space, and null space of \"A\" (respectively), we have\n\nThere is a natural analog of this notion in general Banach spaces. In this case one defines the orthogonal complement of \"W\" to be a subspace of the dual of \"V\" defined similarly as the annihilator\n\nIt is always a closed subspace of \"V\". There is also an analog of the double complement property. \"W\" is now a subspace of \"V\" (which is not identical to \"V\"). However, the reflexive spaces have a natural isomorphism \"i\" between \"V\" and \"V\". In this case we have\n\nThis is a rather straightforward consequence of the Hahn–Banach theorem.\n\nIn special relativity the orthogonal complement is used to determine the simultaneous hyperplane at a point of a world line. The bilinear form η used in Minkowski space determines a pseudo-Euclidean space of events. The origin and all events on the light cone are self-orthogonal. When a time event and a space event evaluate to zero under the bilinear form, then they are hyperbolic-orthogonal. This terminology stems from the use of two conjugate hyperbolas in the pseudo-Euclidean plane: conjugate diameters of these hyperbolas are hyperbolic-orthogonal.\n\n\n\n"}
{"id": "19295866", "url": "https://en.wikipedia.org/wiki?curid=19295866", "title": "PRIMUS (journal)", "text": "PRIMUS (journal)\n\nPRIMUS: Problems, Resources, and Issues in Mathematics Undergraduate Studies (print: , online: ) is a peer-reviewed academic journal covering the teaching of undergraduate mathematics, established in 1991. The journal has been published by Taylor & Francis since March 2007. It is abstracted and indexed in Cambridge Scientific Abstracts, MathEduc, PsycINFO, and \"Zentralblatt MATH\".\n, its editors-in-chief are Jo Ellis-Monaghan and Matt Boelkins.\n"}
{"id": "33020517", "url": "https://en.wikipedia.org/wiki?curid=33020517", "title": "Pentagonal polytope", "text": "Pentagonal polytope\n\nIn geometry, a pentagonal polytope is a regular polytope in \"n\" dimensions constructed from the H Coxeter group. The family was named by H. S. M. Coxeter, because the two-dimensional pentagonal polytope is a pentagon. It can be named by its Schläfli symbol as {5, 3} (dodecahedral) or {3, 5} (icosahedral).\n\nThe family starts as 1-polytopes and ends with \"n\" = 5 as infinite tessellations of 4-dimensional hyperbolic space.\n\nThere are two types of pentagonal polytopes; they may be termed the \"dodecahedral\" and \"icosahedral\" types, by their three-dimensional members. The two types are duals of each other.\n\nThe complete family of dodecahedral pentagonal polytopes are:\n\nThe facets of each dodecahedral pentagonal polytope are the dodecahedral pentagonal polytopes of one less dimension. Their vertex figures are the simplices of one less dimension.\nThe complete family of icosahedral pentagonal polytopes are:\n\nThe facets of each icosahedral pentagonal polytope are the simplices of one less dimension. Their vertex figures are icosahedral pentagonal polytopes of one less dimension.\nThe pentagonal polytopes can be stellated to form new star regular polytopes:\n\nLike other polytopes, they can be combined with their duals to form compounds;\n\n\n"}
{"id": "7180897", "url": "https://en.wikipedia.org/wiki?curid=7180897", "title": "Per Enflo", "text": "Per Enflo\n\nPer H. Enflo (; born 20 May 1944) is a Swedish mathematician working primarily in functional analysis, a field in which he solved problems that had been considered fundamental. Three of these problems had been open for more than forty years:\n\nIn solving these problems, Enflo developed new techniques which were then used by other researchers in functional analysis and operator theory for years. Some of Enflo's research has been important also in other mathematical fields, such as number theory, and in computer science, especially computer algebra and approximation algorithms.\n\nEnflo works at Kent State University, where he holds the title of University Professor. Enflo has earlier held positions at the Miller Institute for Basic Research in Science at the University of California, Berkeley, Stanford University, École Polytechnique, (Paris) and The Royal Institute of Technology, Stockholm.\n\nEnflo is also a concert pianist.\n\nIn mathematics, Functional analysis is concerned with the study of vector spaces and operators acting upon them. It has its historical roots in the study of functional spaces, in particular transformations of functions, such as the Fourier transform, as well as in the study of differential and integral equations. In functional analysis, an important class of vector spaces consists of the complete normed vector spaces over the real or complex numbers, which are called Banach spaces. An important example of a Banach space is a Hilbert space, where the norm arises from an inner product. Hilbert spaces are of fundamental importance in many areas, including the mathematical formulation of quantum mechanics, stochastic processes, and time-series analysis. Besides studying spaces of functions, functional analysis also studies the continuous linear operators on spaces of functions.\n\nAt Stockholm University, Hans Rådström suggested that Enflo consider Hilbert's fifth problem in the spirit of functional analysis. In two years, 1969–1970, Enflo published five papers on Hilbert's fifth problem; these papers are collected in Enflo (1970), along with a short summary. Some of the results of these papers are described in Enflo (1976) and in the last chapter of Benyamini and Lindenstrauss.\n\nEnflo's techniques have found application in computer science. Algorithm theorists derive approximation algorithms that embed finite metric spaces into low-dimensional Euclidean spaces with low \"distortion\" (in Gromov's terminology for the Lipschitz category; c.f. Banach–Mazur distance). Low-dimensional problems have lower computational complexity, of course. More importantly, if the problems embed well in either the Euclidean plane or the three-dimensional Euclidean space, then geometric algorithms become exceptionally fast.\n\nHowever, such embedding techniques have limitations, as shown by Enflo's (1969) theorem:\nThis theorem, \"found by Enflo [1969], is probably the first result showing an unbounded distortion for embeddings into Euclidean spaces. Enflo considered the problem of uniform embeddability among Banach spaces, and the distortion was an auxiliary device in his proof.\"\n\nA uniformly convex space is a Banach space so that, for every formula_8 there is some formula_9 so that for any two vectors with formula_10 and formula_11\nimplies that\nIntuitively, the center of a line segment inside the unit ball must lie deep inside the unit ball unless the segment is short.\n\nIn 1972 Enflo proved that \"every super-reflexive Banach space admits an equivalent uniformly convex norm\".\nWith one paper, which was published in 1973, Per Enflo solved three problems that had stumped functional analysts for decades: The basis problem of Stefan Banach, the \"Goose problem\" of Stanislaw Mazur, and the approximation problem of Alexander Grothendieck. Grothendieck had shown that his approximation problem was the central problem in the theory of Banach spaces and continuous linear operators.\n\nThe basis problem was posed by Stefan Banach in his book, \"Theory of Linear Operators\". Banach asked whether every separable Banach space has a Schauder basis.\n\nA Schauder basis or countable basis is similar to the usual (Hamel) basis of a vector space; the difference is that for Hamel bases we use linear combinations that are \"finite\" sums, while for Schauder bases they may be \"infinite\" sums. This makes Schauder bases more suitable for the analysis of infinite-dimensional topological vector spaces including Banach spaces.\n\nSchauder bases were described by Juliusz Schauder in 1927. Let \"V\" denote a Banach space over the field \"F\". A \"Schauder basis\" is a sequence (\"b\") of elements of \"V\" such that for every element \"v\" ∈ \"V\" there exists a \"unique\" sequence (α) of elements in \"F\" so that\nwhere the convergence is understood with respect to the norm topology. Schauder bases can also be defined analogously in a general topological vector space.\n\nBanach and other Polish mathematicians would work on mathematical problems at the Scottish Café. When a problem was especially interesting and when its solution seemed difficult, the problem would be written down in the book of problems, which soon became known as the \"Scottish Book\". For problems that seemed especially important or difficult or both, the problem's proposer would often pledge to award a prize for its solution.\n\nOn 6 November 1936, Stanislaw Mazur posed a problem on representing continuous functions. Formally writing down \"problem 153\" in the \"Scottish Book\", Mazur promised as the reward a \"live goose\", an especially rich price during the Great Depression and on the eve of World War II.\n\nFairly soon afterwards, it was realized that Mazur's problem was closely related to Banach's problem on the existence of Schauder bases in separable Banach spaces. Most of the other problems in the \"Scottish Book\" were solved regularly. However, there was little progress on Mazur's problem and a few other problems, which became famous open problems to mathematicians around the world.\n\nGrothendieck's work on the theory of Banach spaces and continuous linear operators introduced the approximation property. A Banach space is said to have the approximation property, if every compact operator is a limit of finite-rank operators. The converse is always true.\n\nIn a long monograph, Grothendieck proved that if every Banach space had the approximation property, then every Banach space would have a Schauder basis. Grothendieck thus focused the attention of functional analysts on deciding whether every Banach space have the approximation property.\n\nIn 1972, Per Enflo constructed a separable Banach space that lacks the approximation property and a Schauder basis. In 1972, Mazur awarded a live goose to Enflo in a ceremony at the Stefan Banach Center in Warsaw; the \"goose reward\" ceremony was broadcast throughout Poland.\n\nIn functional analysis, one of the most prominent problems was the invariant subspace problem, which required the evaluation of the truth of the following proposition:\nFor Banach spaces, the first example of an operator without an invariant subspace was constructed by Enflo. (For Hilbert spaces, the invariant subspace problem remains open.)\n\nEnflo proposed a solution to the invariant subspace problem in 1975, publishing an outline in 1976. Enflo submitted the full article in 1981 and the article's complexity and length delayed its publication to 1987 Enflo's long \"manuscript had a world-wide circulation among mathematicians\" and some of its ideas were described in publications besides Enflo (1976). Enflo's works inspired a similar construction of an operator without an invariant subspace for example by Beauzamy, who acknowledged Enflo's ideas.\n\nIn the 1990s, Enflo developed a \"constructive\" approach to the invariant subspace problem on Hilbert spaces.\n\nAn essential idea in Enflo's construction was \"concentration of polynomials at low degrees\": For all positive integers formula_7 and formula_16, there exists formula_17 such that for all homogeneous polynomials formula_18 and formula_19 of degrees formula_7 and formula_16 (in formula_22 variables), then\nformula_23\nwhere formula_24 denotes the sum of the absolute values of the coefficients of formula_18. Enflo proved that formula_26 does not depend on the number of variables formula_22. Enflo's original proof was simplified by Montgomery.\n\nThis result was generalized to other norms on the vector space of homogeneous polynomials. Of these norms, the most used has been the Bombieri norm.\n\nThe Bombieri norm is defined in terms of the following scalar product:\nFor all formula_28 we have\n\nwhere we use the following notation:\nif formula_33, we write formula_34 and\nformula_35 and formula_36\n\nThe most remarkable property of this norm is the Bombieri inequality:\n\nLet formula_37 be two homogeneous polynomials respectively of degree formula_38 and formula_39 with formula_40 variables, then, the following inequality holds:\n\nIn the above statement, the Bombieri inequality is the left-hand side inequality; the right-hand side inequality means that the Bombieri norm is a norm of the algebra of polynomials under multiplication.\n\nThe Bombieri inequality implies that the product of two polynomials cannot be arbitrarily small, and this lower-bound is fundamental in applications like polynomial factorization (or in Enflo's construction of an operator without an invariant subspace).\n\nEnflo's idea of \"concentration of polynomials at low degrees\" has led to important publications in number theory algebraic and Diophantine geometry, and polynomial factorization.\n\nIn applied mathematics, Per Enflo has published several papers in mathematical biology, specifically in population dynamics.\n\nEnflo has also published in population genetics and paleoanthropology.\n\nToday, all humans belong to one population of \"Homo sapiens sapiens\", which is individed by species barrier. However, according to the \"Out of Africa\" model this is not the first species of hominids: the first species of genus \"Homo\", \"Homo habilis\", evolved in East Africa at least 2 Ma, and members of this species populated different parts of Africa in a relatively short time. \"Homo erectus\" evolved more than 1.8 Ma, and by 1.5 Ma had spread throughout the Old World.\n\nAnthropologists have been divided as to whether current human population evolved as one interconnected population (as postulated by the Multiregional Evolution hypothesis), or evolved only in East Africa, speciated, and then migrating out of Africa and replaced human populations in Eurasia (called the \"Out of Africa\" Model or the \"Complete Replacement\" Model).\n\nNeanderthals and modern humans coexisted in Europe for several thousand years, but the duration of this period is uncertain. Modern humans may have first migrated to Europe 40–43,000 years ago. Neanderthals may have lived as recently as 24,000 years ago in refugia on the south coast of the Iberian peninsula such as Gorham's Cave. Inter-stratification of Neanderthal and modern human remains has been suggested, but is disputed.\n\nWith Hawks and Wolpoff, Enflo published an explanation of fossil evidence on the DNA of Neanderthal and modern humans. This article tries to resolve a debate in the evolution of modern humans between theories suggesting either multiregional and single African origins. In particular,\nthe extinction of Neanderthals could have happened due to waves of modern humans entered Europe – in technical terms, due to \"the continuous influx of modern human DNA into the Neandertal gene pool.\"\n\nEnflo has also written about the population dynamics of zebra mussels in Lake Erie.\n\nPer Enflo is also a concert pianist.\n\nA child prodigy in both music and mathematics, Enflo won the Swedish competition for young pianists at age 11 in 1956, and he won the same competition in 1961. At age 12, Enflo appeared as a soloist with the Royal Opera Orchestra of Sweden. He debuted in the Stockholm Concert Hall in 1963. Enflo's teachers included Bruno Seidlhofer, Géza Anda, and Gottfried Boon (who himself was a student of Arthur Schnabel).\n\nIn 1999 Enflo competed in the first annual Van Cliburn Foundation’s International Piano Competition for Outstanding Amateurs.\n\nEnflo performs regularly around Kent and in a Mozart series in Columbus, Ohio (with the Triune Festival Orchestra). His solo piano recitals have appeared on the Classics Network of the radio station WOSU, which is sponsored by Ohio State University.\n\n\n\n\n\n"}
{"id": "50774822", "url": "https://en.wikipedia.org/wiki?curid=50774822", "title": "Principle of maximum caliber", "text": "Principle of maximum caliber\n\nThe principle of maximum caliber (MaxCal) or maximum path entropy principle, suggested by E. T. Jaynes, can be considered as a generalization of the principle of maximum entropy. It postulates that the most unbiased probability distribution of paths is the one that maximizes their Shannon entropy. This entropy of paths is sometimes called the \"caliber\" of the system, and is given by the path integral\n\nThe principle of maximum caliber was proposed by Edwin T. Jaynes in 1980, in an article titled \n\"The Minimum Entropy Production Principle\" over the context of to find a principle for to derive the non-equilibrium statistical mechanics.\n\nThe principle of maximum caliber can be considered as a generalization of the principle of maximum entropy defined over the paths space, the caliber formula_2 is of the form\n\nwhere for \"n\"-constraints\n\nit is shown that the probability functional is\n\nIn the same way, for n dynamical constraints defined in the interval formula_6 of the form\n\nit is shown that the probability functional is\n\nFollowing the hypothesis of Jaynes, there are publications in which it appears as the principle of maximum caliber it is framed in the context of creating a statistical representation of systems with many degrees of freedom.\n"}
{"id": "22610235", "url": "https://en.wikipedia.org/wiki?curid=22610235", "title": "Rational irrationality", "text": "Rational irrationality\n\nThe concept known as rational irrationality was popularized by economist Bryan Caplan in 2001 to reconcile the widespread existence of irrational behavior (particularly in the realms of religion and politics) with the assumption of rationality made by mainstream economics and game theory. The theory, along with its implications for democracy, was expanded upon by Caplan in his book \"The Myth of the Rational Voter\".\n\nThe original purpose of the concept was to explain how (allegedly) detrimental policies could be implemented in a democracy, and, unlike conventional public choice theory, Caplan posited that bad policies were selected by voters themselves. The theory has also been embraced by the ethical intuitionist philosopher Michael Huemer as an explanation for irrationality in politics. The theory has also been applied to explain religious belief.\n\nCaplan posits that there are two types of rationality:\n\n\nRational irrationality describes a situation in which it is \"instrumentally\" rational for an actor to be \"epistemically\" irrational.\n\nCaplan argues that rational irrationality is more likely in situations in which:\n\n\nIn the framework of neoclassical economics, Caplan posits that there is a \"demand for irrationality\". A person's demand curve describes the amount of irrationality that the person is willing to tolerate at any given cost of irrationality. By the law of demand, the lower the cost of irrationality, the higher the demand for it. When the cost of error is effectively zero, a person's demand for irrationality is high.\n\nRational irrationality is not doublethink and does not state that the individual deliberately chooses to believe something he or she knows to be false. Rather, the theory is that when the costs of having erroneous beliefs are low, people relax their intellectual standards and allow themselves to be more easily influenced by fallacious reasoning, cognitive biases, and emotional appeals. In other words, people do not deliberately seek to believe false things but stop putting in the intellectual effort to be open to evidence that may contradict their beliefs.\n\nFor rational irrationality to exist, people must have preferences over beliefs: certain beliefs must be appealing to people for reasons \"other than\" their truth value. In an essay on irrationality in politics Michael Huemer identifies some possible sources of preferences over beliefs:\n\n\nMany of the claims of religions are not easily verifiable in the day-to-day world. There are many competing religious theories about the origins of life, reincarnation, and paradise, but mistaken beliefs about these rarely impose real world costs upon the believers themselves. Thus, it may be instrumentally rational to be epistemically irrational about these matters. In other words, when forming or updating their religious beliefs, people may tend to relax their intellectual standards for the sake of driving popular support towards their beliefs.\n\nPolitics is a situation where rational irrationality is expected to be common, according to Caplan's theory. In typical large democracies, each individual voter has a very low probability of influencing the outcome of an election or determining whether a particular policy will be implemented. Thus, the expected cost of supporting an erroneous policy (obtained by multiplying the cost of the policy by the probability that the individual voter will have a decisive role in influencing the policy) is very low. The psychological benefits of supporting policies that feel good but are in fact harmful may be greater than these small expected costs. This creates a situation where voters may be rationally irrational for practical morale reasons.\n\nFor rational irrationality at an individual level to have an effect on political outcomes, it is necessary that there be \"systemic\" ways in which people are irrational. In other words, people need to have systemic biases: there needs to be a systemic difference between people's preferences over beliefs and true beliefs. In the absence of systemic biases, different forms of irrationality would cancel out when aggregated using the voting process.\n\nCaplan attempts to demonstrate empirically the existence of systemic biases in beliefs about economics in his book \"The Myth of the Rational Voter\".\n\nWhen a large number of individuals hold systematically biased beliefs, the \"total\" cost to the democracy of all these irrational beliefs could be significant. Thus, even though every individual voter may be behaving rationally, the voters as a whole are not acting in their collective self-interest. This is analogous to the tragedy of the commons. Another way of thinking about it is that each voter, by being rationally irrational, creates a small negative externality for other voters.\n\nCaplan believes that the rational irrationality of voters is one of the reasons why democracies choose suboptimal economic policies, particularly in the area of free trade versus protectionism. Philosopher Michael Huemer, in a TEDx talk on rational irrationality in politics, cited the war on terror and protectionism as two examples of rational irrationality in politics.\n\nAny theory of democracy must take into account the empirical fact that most voters in a democracy have very little idea about the details of politics, including the names of their elected representatives, the terms of office, and the platforms of candidates of major political parties.\n\nLike rational irrationality, some theories of democracy claim that democracies tend to choose bad policies. Other theories claim that despite the empirical observations about voter ignorance, democracies do in fact do fairly well. Below are listed some of these theories and their relation to rational irrationality.\n\nThe most famous theory of democratic failure is public choice theory. The theory, developed by James Buchanan, Gordon Tullock, and others, relies on rational ignorance. Voters have a very small probability of influencing policy outcomes, so they do not put much effort to stay up-to-date on politics. This allows special interests to manipulate the political process and engage in rent seeking. A key idea of public choice theory is that many harmful policies have concentrated benefits (experienced by special interests) and diffuse costs. The special interests experiencing the benefits are willing to lobby for the policies, while the costs are spread out very diffusely among a much larger group of people. Because these costs are diffuse, the people bearing the costs do not have enough at stake to lobby against the policies.\n\nRational irrationality and rational ignorance share some key similarities but are also different in a number of ways. The similarities are that both theories reject the claim that voters are rational and well-informed, and both theories claim that democracy does not function well. However, the theories differ in a number of ways:\n\n\nThere are two main objections to public choice theory and rational ignorance that do not apply to rational irrationality:\n\n\nBrennan and Lomasky have an alternative theory of democratic failure that is quite similar to Caplan's theory of rational irrationality. Their theory, called \"expressive voting\", states that people vote to \"express\" certain beliefs. The key difference between expressive voting and rational irrationality is that the former does not require people to actually \"hold\" systematically biased beliefs, while the latter does.\n\nLoren Lomasky, one of the proponents of expressive voting, explained some of the key differences between the theories in a critical review of Caplan's book.\n\nDonald Wittman has argued that democracy works well. Wittman's argument rests on raising a number of objections to public choice theory, such as those outlined above while contrasting public choice theory and rational irrationality. Caplan described his own work on rational irrationality as an attempt to rescue democratic failure from Wittman's attacks. After the publication of Caplan's book, Wittman and Caplan debated each other.\n"}
{"id": "26558892", "url": "https://en.wikipedia.org/wiki?curid=26558892", "title": "Reeb sphere theorem", "text": "Reeb sphere theorem\n\nIn mathematics, Reeb sphere theorem, named after Georges Reeb, states that\n\nA singularity of a foliation \"F\" is of Morse type if in its small neighborhood all leaves of the foliation are levels of a Morse function, being the singularity a critical point of the function. The singularity is a center if it is a local extremum of the function; otherwise, the singularity is a saddle.\n\nThe number of centers \"c\" and the number of saddles formula_1, specifically \"c\" − \"s\", is tightly connected with the manifold topology.\n\nWe denote ind \"p\" = min(\"k\", \"n\" − \"k\"), the index of a singularity formula_2, where \"k\" is the index of the corresponding critical point of a Morse function. In particular, a center has index 0, index of a saddle is at least 1.\n\nA Morse foliation \"F\" on a manifold \"M\" is a singular transversely oriented codimension one foliation of class \"C\" with isolated singularities such that:\n\nThis is the case \"c\" > \"s\" = 0, the case without saddles.\n\nTheorem: \"Let formula_4 be a closed oriented connected manifold of dimension formula_5. Assume that formula_4 admits a formula_7-transversely oriented codimension one foliation formula_8 with a non empty set of singularities all of them centers. Then the singular set of formula_8 consists of two points and formula_4 is homeomorphic to the sphere formula_11\".\n\nIt is a consequence of the Reeb stability theorem.\n\nMore general case is formula_12\n\nIn 1978, E. Wagneur generalized the Reeb sphere theorem to Morse foliations with saddles. He showed that the number of centers cannot be too much as compared with the number of saddles, notably, formula_13. So there are exactly two cases when formula_14:\n\nHe obtained a description of the manifold admitting a foliation with singularities that satisfy (1).\n\nTheorem: \"Let formula_4 be a compact connected manifold admitting a Morse foliation formula_8 with formula_19 centers and formula_1 saddles. Then formula_13.\" \n\"In case formula_22,\"\n\n\nFinally, in 2008, C. Camacho and B. Scardua considered the case (2), formula_26. This is possible in a small number of low dimensions.\n\nTheorem: \"Let formula_4 be a compact connected manifold and formula_8 a Morse foliation on formula_23. If formula_30, then\"\n\n"}
{"id": "253862", "url": "https://en.wikipedia.org/wiki?curid=253862", "title": "Rithmomachy", "text": "Rithmomachy\n\nRithmomachy (or Rithmomachia, also Arithmomachia, Rythmomachy, Rhythmomachy, or sundry other variants; sometimes known as The Philosophers' Game) is a highly complex, early European mathematical board game. The earliest known description of it dates from the eleventh century. A literal translation of the name is \"The Battle of the Numbers\". The game is much like chess, except most methods of capture depend on the numbers inscribed on each piece.\n\nIt has been argued that between the twelfth and sixteenth centuries, \"rithmomachia served as a practical exemplar for teaching the contemplative values of Boethian mathematical philosophy, which emphasized the natural harmony and perfection of number and proportion. The game, Moyer argues, was used both as a mnemonic drill for the study of Boethian number theory and, more importantly, as a vehicle for moral education, by reminding players of the mathematical harmony of creation.\"\n\nVery little, if anything, is known about the origin of the game. But it is known that medieval writers attributed it to Pythagoras, although no trace of it has been discovered in Greek literature, and the earliest mention of it is from the time of Hermannus Contractus (1013–1054).\n\nThe name, which appears in a variety of forms, points to a Greek origin, the more so because Greek was little known at the time when the game first appeared in literature. Based upon the Greek theory of numbers, and having a Greek name, it is still speculated by some that the origin of the game is to be sought in the Greek civilization, and perhaps in the later schools of Byzantium or Alexandria.\n\nThe first written evidence of Rithmomachia dates back to around 1030, when a monk, named Asilo, created a game that illustrated the number theory of Boëthius' \"De institutione arithmetica\", for the students of monastery schools. The rules of the game were improved shortly thereafter by the respected monk, Hermannus Contractus, from Reichenau, and in the school of Liège. In the following centuries, Rithmomachia spread quickly through schools and monasteries in the southern parts of Germany and France. It was used mainly as a teaching aid, but, gradually, intellectuals started to play it for pleasure. In the 13th century Rithmomachia came to England, where famous mathematician Thomas Bradwardine wrote a text about it. Even Roger Bacon recommended Rithmomachia to his students, while Sir Thomas More let the inhabitants of the fictitious Utopia play it for recreation.\n\nThe game was well enough known as to justify printed treatises in Latin, French, Italian, and German, in the sixteenth century, and to have public advertisements of the sale of the board and pieces under the shadow of the old Sorbonne.\n\nThe game was played on a board resembling the one used for chess or checkers, with eight squares on the shorter side, but with sixteen on the longer side. The forms used for the pieces were triangles, squares, and rounds. Pyramids could be formed by stacking pieces. The game was noteworthy in that the black and white forces were not symmetrical. Although each side had the same array of pieces, the numbers on them differed, allowing different possible captures and winning configurations to the two players.\n\nThe rules below describe the most common version of the game, played through much of the Middle Ages and Renaissance. There was also a variant propounded by Fulke in the 16th century, with significantly different (and somewhat more consistent) capture rules.\n\nThere are four types of pieces, which are Rounds, Triangles, Squares, and Pyramids.\n\nThere were a variety of capture methods. Pieces do not land on another piece to capture it, but instead remain in their square and remove the other. If a piece is captured, it changes sides.\n\n\nThere were also a variety of victory conditions for determining when a game would end and who the winner was. There were common victories, and proper victories, which were recommended for more skilled players. Proper victories required placing pieces in linear arrangements in the opponent's side of the board, with the numbers formed by the arrangement following various types of numerical progression. The types of progression required — arithmetic, geometric and harmonic — suggest a connection with the mathematical work of Boëthius.\n\nAt its peak, Rithmomachy rivalled chess for popularity in Europe. The game virtually disappeared from the 17th century until the late 19th and early 20th century, when it was rediscovered by historians.\n\n"}
{"id": "766014", "url": "https://en.wikipedia.org/wiki?curid=766014", "title": "Ruffini's rule", "text": "Ruffini's rule\n\nIn mathematics, Ruffini's rule is a practical way for paper-and-pencil computation of the Euclidean division of a polynomial by a a binomial of the form . It was described by Paolo Ruffini in 1804. Ruffini's rule is a special case of synthetic division when the divisor is a linear factor.\n\nThe rule establishes a method for dividing the polynomial\nby the binomial\nto obtain the quotient polynomial\n\nThe algorithm is in fact the long division of \"P\"(\"x\") by \"Q\"(\"x\").\n\nTo divide \"P\"(\"x\") by \"Q\"(\"x\"):\n\n\nThe \"b\" values are the coefficients of the result (\"R\"(\"x\")) polynomial, the degree of which is one less than that of \"P\"(\"x\"). The final value obtained, \"s\", is the remainder. As shown in the polynomial remainder theorem, this remainder is equal to \"P\"(\"r\"), the value of the polynomial at \"r\".\n\nRuffini's rule has many practical applications; most of them rely on simple division (as demonstrated below) or the common extensions given still further below.\n\nA worked example of polynomial division, as described above.\n\nLet:\n\nWe want to divide \"P\"(\"x\") by \"Q\"(\"x\") using Ruffini's rule. The main problem is that \"Q\"(\"x\") is not a binomial of the form \"x\" − \"r\", but rather \"x\" + \"r\". We must rewrite \"Q\"(\"x\") in this way:\nNow we apply the algorithm:\n\n1. Write down the coefficients and \"r\". Note that, as \"P\"(\"x\") didn't contain a coefficient for \"x\", we've written 0:\n\n2. Pass the first coefficient down:\n\n3. Multiply the last obtained value by \"r\":\n\n4. Add the values:\n\n5. Repeat steps 3 and 4 until we've finished:\n\nSo, if \"original number\" = \"divisor\" × \"quotient\" + \"remainder\", then\n\nThe rational root theorem tells us that for a polynomial all of whose coefficients (\"a\" through \"a\") are integers, the real rational roots are always of the form \"p\"/\"q\", where \"p\" is an integer divisor of \"a\" and \"q\" is an integer divisor of \"a\". Thus if our polynomial is\n\nthen the possible rational roots are all the integer divisors of \"a\" (−2):\n\n(This example is simple because the polynomial is monic (i.e. \"a\" = 1); for non-monic polynomials the set of possible roots will include some fractions, but only a finite number of them since \"a\" and \"a\" only have a finite number of integer divisors each.) In any case, for monic polynomials, every rational root is an integer, and so every integer root is just a divisor of the constant term (i.e. \"a\"). It can be shown that this remains true for non-monic polynomials, i.e. \"to find the integer roots of any polynomials with integer coefficients, it suffices to check the divisors of the constant term\".\n\nSo, setting \"r\" equal to each of these possible roots in turn, we will test-divide the polynomial by (). If the resulting quotient has no remainder, we have found a root.\n\nYou can choose one of the following three methods: they will all yield the same results, with the exception that only through the second method and the third method (when applying Ruffini's rule to obtain a factorization) can you discover that a given root is repeated. (Neither method will discover irrational or complex roots.)\n\nWe try to divide \"P\"(\"x\") by the binomial (\"x\" − each possible root). If the remainder is 0, the selected number is a root (and vice versa):\n\nWe start just as in Method 1 until we find a valid root. Then, instead of restarting the process with the other possible roots, we continue testing the possible roots against the result of the Ruffini on the valid root we've just found until we only have a coefficient remaining (remember that roots can be repeated: if you get stuck, try each valid root twice):\n\n\nThus, for each \"r\" in our set, \"r\" is actually a root of the polynomial if and only if \"P\"(\"r\")=0\n\nThis shows that finding \"integer and rational\" roots of a polynomial neither requires any division nor the application of Ruffini's rule.\n\nHowever, once a valid root has been found, call it \"r\":\nyou can apply Ruffini's rule to determine\nThis allows you to partially factorize the polynomial as\n\nAny additional (rational) root of the polynomial is also a root of \"Q\"(\"x\") and, of course, is still to be found among the possible roots determined earlier which have not yet been checked (any value already determined \"not\" to be a root of \"P\"(\"x\") is not a root of \"Q\"(\"x\") either; more formally, \"P\"(\"r\")≠0 → \"Q\"(\"r\")≠0 ).\n\nThus, you can proceed evaluating \"Q\"(\"r\") instead of \"P\"(\"r\"), and (as long as you can find another root, \"r\") dividing \"Q\"(\"r\") by (\"x\"–\"r\").\n\nEven if you're only searching for roots, this allows you to evaluate polynomials of successively smaller degree, as the factorization proceeds.\n\nIf, as is often the case, you are also factorizing a polynomial of degree \"n\", then:\n\n\nand the remainder of is 12\n\n\nThen, applying Ruffini's Rule:\n\nHere, \"r\"=-1 and \n\n\nAgain, applying Ruffini's Rule:\n\nAs it was possible to completely factorize the polynomial, it's clear that the last root is -2 (the previous procedure would have given the same result, with a final quotient of 1).\n\nHaving used the \"\"p\"/\"q\"\" result above (or, to be fair, any other means) to find all the real rational roots of a particular polynomial, it is but a trivial step further to partially factor that polynomial using those roots. As is well-known, each linear factor (\"x\" − \"r\") which divides a given polynomial corresponds with a root \"r\", and \"vice versa\".\n\nSo if\n\nBy the fundamental theorem of algebra, \"R\"(\"x\") should be equal to \"P\"(\"x\"), if all the roots of \"P\"(\"x\") are rational. But since we have been using a method which finds only rational roots, it is very likely that \"R\"(\"x\") is not equal to \"P\"(\"x\"); it is very likely that \"P\"(\"x\") has some irrational or complex roots not in \"R\". So consider\n\nIf \"S\"(\"x\") = 1, then we know \"R\"(\"x\") = \"P\"(\"x\") and we are done. Otherwise, \"S\"(\"x\") will itself be a polynomial; this is another factor of \"P\"(\"x\") which has no real rational roots. So write out the right-hand-side of the following equation in full:\n\nWe can call this a \"complete factorization\" of \"P\"(\"x\") over Q (the rationals) if \"S\"(\"x\") = 1. Otherwise, we only have a \"partial factorization\" of \"P\"(\"x\") over Q, which may or may not be further factorable over the rationals; but which will certainly be further factorable over the reals or at worst the complex plane. (Note: by a \"complete factorization\" of \"P\"(\"x\") over Q, we mean a factorization as a product of polynomials with rational coefficients, such that each factor is irreducible over Q, where \"irreducible over Q\" means that the factor cannot be written as the product of two non-constant polynomials with rational coefficients and smaller degree.)\n\nLet\n\nUsing the methods described above, the rational roots of \"P\"(\"x\") are:\n\nThen, the product of (\"x\" − each root) is\n\nAnd \"P\"(\"x\")/\"R\"(\"x\"):\n\nHence the factored polynomial is \"P\"(\"x\") = \"R\"(\"x\") · 1 = \"R\"(\"x\"):\n\nLet\n\nUsing the methods described above, the rational roots of \"P\"(\"x\") are:\n\nThen, the product of (\"x\" − each root) is\n\nAnd \"P\"(\"x\")/\"R\"(\"x\")\n\nAs formula_33, the factored polynomial is \"P\"(\"x\") = \"R\"(\"x\") · \"S\"(\"x\"):\n\nTo completely factor a given polynomial over C, the complex numbers, we must know all of its roots (and that could include irrational and/or complex numbers). For example, consider the polynomial above:\n\nExtracting its rational roots and factoring it, we end with:\n\nBut that is not completely factored over C. If we need to factor our polynomial to a product of linear factors, we must deal with that quadratic factor\n\nThe easiest way is to use quadratic formula, which gives us\n\nand the solutions\n\nSo the completely factored polynomial over C will be:\n\nHowever, it should be noted that we cannot in every case expect things to be so easy; the quadratic formula's analogue for fourth-order polynomials is very messy and no such analogue exists for 5th-or-higher order polynomials. See Galois theory for a theoretical explanation of why this is so, and see numerical analysis for ways to \"approximate\" roots of polynomials numerically.\n\nIt is entirely possible that, when looking for a given polynomial's roots, we might obtain a messy higher-order polynomial for S(x) which is further factorable over the \"rationals\" even before considering irrational or complex factorings. Consider the polynomial \"x\" − 3\"x\" + 3\"x\" − 9\"x\" + 2\"x\" − 6. Using Ruffini's method we will find only one root (\"x\" = 3); factoring it out gives us \"P\"(\"x\") = (\"x\" + 3\"x\" + 2)(\"x\" − 3).\n\nAs explained above, if our assignment was to \"factor into irreducibles over C\" we know that would have to find some way to dissect the quartic and look for its irrational and/or complex roots. But if we were asked to \"factor into irreducibles over Q\", we might think we are done; but it is important to realize that this might not necessarily be the case.\n\nFor in this instance the quartic is actually factorable as the product of two quadratics (\"x\" + 1)(\"x\" + 2). These, at last, are irreducible over the rationals (and, indeed, the reals as well in this example); so now we are done; \"P\"(\"x\") = (\"x\" + 1)(\"x\" + 2)(\"x\" − 3). In this instance it is in fact easy to factor our quartic by treating it as a biquadratic equation; but finding such factorings of a higher degree polynomial can be very difficult.\n\nThis method was invented by Paolo Ruffini. He took part in a competition organised by the Italian Scientific Society (of Forty). The question to be answered was a method to find the roots of any polynomial. Five submissions were received. In 1804 Ruffini's was awarded the first place and the method was published. Ruffini published refinements of the method in 1807 and 1813. \n\nHorner's method was published in 1819 and in a refined version in 1845.\n\n\n"}
{"id": "685237", "url": "https://en.wikipedia.org/wiki?curid=685237", "title": "Shaw Prize", "text": "Shaw Prize\n\nThe Shaw Prize is an annual award first presented by the Shaw Prize Foundation in 2004. Established in 2002 in Hong Kong, it honours \"individuals who are currently active in their respective fields and who have recently achieved distinguished and significant advances, who have made outstanding contributions in academic and scientific research or applications, or who in other domains have achieved excellence. The award is dedicated to furthering societal progress, enhancing quality of life, and enriching humanity's spiritual civilization.\"\n\n<nowiki> </nowiki>The prize is widely regarded as the \"Nobel of the East\". It is named after Sir Run Run Shaw (邵逸夫), who was a philanthropist and forerunner in the Hong Kong media industry.\n\nThe prize is for recent achievements in the fields of astronomy, life science and medicine, and mathematical sciences; it is not awarded posthumously. Nominations are submitted by invited individuals beginning each year in September. The award winners are then announced in the summer, and receive the prize at the ceremony in early autumn. The winners receive a medal and a certificate. The front of the medal bears a portrait of Shaw as well as the English and the Traditional Chinese name of the prize; the back bears the year, the category, the name of the winner and a Chinese quotation of philosopher Xun Zi (制天命而用之, which means \"Grasp the law of nature and make use of it\"). In addition, the winner receives a sum of money, which is worth US$1.2 million from 1 October 2015.\n\nAs of 2012, 28 prizes have been awarded to 48 individuals. The inaugural winner for the Astronomy award was Canadian Jim Peebles; he was honoured for his contributions to cosmology. Two inaugural prizes were awarded for the Life Science and Medicine category: Americans Stanley Norman Cohen, Herbert Boyer and Yuet-Wai Kan jointly won one of the prizes for their works pertaining to DNA while British physiologist Sir Richard Doll won the other for his contribution to cancer epidemiology. Shiing-Shen Chern of China won the inaugural Mathematical Sciences award for his work on differential geometry.\n\nOf note, seven of the Nobel laureates—Jules Hoffmann, Bruce Beutler, Saul Perlmutter, Adam Riess, Shinya Yamanaka, Robert Lefkowitz and Brian Schmidt—were previous laureates of the Shaw Prize.\n\nBoard of Adjudicators: Yuet Wai Kan, Kenneth Young, Peter Goldreich, Randy Schekman, Peter Sarnak\n\nAstronomy: Peter Goldreich, Ewine van Dishoeck, Reinhard Genzel, Victoria Kaspi, John A. Peacock\n\nLife Science and Medicine: Randy Schekman, Bruce Beutler, Carol W. Greider, Franz-Ulrich Hartl, Robert Lefkowitz, Eve Marder, Shinya Yamanaka\n\nMathematical Sciences: Peter Sarnak, John M. Ball, David Eisenbud, Sir Timothy Gowers, John Morgan (mathematician)\n\n\n"}
{"id": "19771879", "url": "https://en.wikipedia.org/wiki?curid=19771879", "title": "Splitting lemma (functions)", "text": "Splitting lemma (functions)\n\nIn mathematics, especially in singularity theory the splitting lemma is a useful result due to René Thom which provides a way of simplifying the local expression of a function usually applied in a neighbourhood of a degenerate critical point. \n\nLet formula_1 be a smooth function germ, with a critical point at 0 (so formula_2). Let \"V\" be a subspace of formula_3 such that the restriction \"f|V\" is non-degenerate, and write \"B\" for the Hessian matrix of this restriction. Let \"W\" be any complementary subspace to \"V\". Then there is a change of coordinates formula_4 of the form formula_5 with formula_6, and a smooth function \"h\" on \"W\" such that\n\nThis result is often referred to as the parametrized Morse lemma, which can be seen by viewing \"y\" as the parameter. It is the \"gradient version\" of the implicit function theorem.\n\nThere are extensions to infinite dimensions, to complex analytic functions, to functions invariant under the action of a compact group, . . . \n\n"}
{"id": "42682101", "url": "https://en.wikipedia.org/wiki?curid=42682101", "title": "Strichartz estimate", "text": "Strichartz estimate\n\nIn applied mathematics, Strichartz estimates are a family of inequalities for linear dispersive partial differential equations. These inequalities establish size and decay of solutions in mixed norm Lebesgue spaces. They were first noted by Robert Strichartz and arose out of contentions to the Fourier restriction problem.\n\nConsider the linear Schrödinger equation in formula_1 with \"h\" = \"m\" = 1. Then the solution for initial data formula_2 is given by formula_3. Let \"q\" and \"r\" be real numbers satisfying formula_4; formula_5; and formula_6. \n\nIn this case the homogeneous Strichartz estimates take the form:\nFurther suppose that formula_8 satisfy the same restrictions as formula_9 and formula_10 are their dual exponents, then the dual homogeneous Strichartz estimates take the form:\n\nThe inhomogeneous Strichartz estimates are:\n"}
{"id": "1550685", "url": "https://en.wikipedia.org/wiki?curid=1550685", "title": "Strong antichain", "text": "Strong antichain\n\nIn order theory, a subset \"A\" of a partially ordered set \"X\" is a strong downwards antichain if it is an antichain in which no two distinct elements have a common lower bound, that is,\n\nIn the case where \"X\" is ordered by inclusion, this is simply a family of pairwise disjoint sets.\n\nA strong upwards antichain \"B\" is a subset of \"X\" in which no two distinct elements have a common upper bound. Authors will often omit the \"upwards\" and \"downwards\" term and merely refer to strong antichains. Unfortunately, there is no common convention as to which version is called a strong antichain. In the context of forcing, authors will sometimes also omit the \"strong\" term and merely refer to antichains. To resolve ambiguities in this case, the weaker type of antichain is called a weak antichain.\n\nIf \"(X,≤)\" is a partial order and there exist distinct \"x,y ∈ X\" such that {\"x,y\"} is a strong antichain, then \"(X,≤)\" cannot be a lattice (or even a meet semilattice), since by definition, every two elements in a lattice (or meet semilattice) must have a common lower bound. Thus lattices have only trivial strong antichains (i.e., strong antichains of cardinality 1 or less).\n\n"}
{"id": "17398087", "url": "https://en.wikipedia.org/wiki?curid=17398087", "title": "Subdirectly irreducible algebra", "text": "Subdirectly irreducible algebra\n\nIn the branch of mathematics known as universal algebra (and in its applications), a subdirectly irreducible algebra is an algebra that cannot be factored as a subdirect product of \"simpler\" algebras. Subdirectly irreducible algebras play a somewhat analogous role in algebra to primes in number theory.\n\nA universal algebra \"A\" is said to be subdirectly irreducible when \"A\" has more than one element, and when any subdirect representation of \"A\" includes (as a factor) an algebra isomorphic to \"A\", with the isomorphism being given by the projection map.\n\n\nThe subdirect representation theorem of universal algebra states that every algebra is subdirectly representable by its subdirectly irreducible quotients. An equivalent definition of \"subdirect irreducible\" therefore is any algebra \"A\" that is not subdirectly representable by those of its quotients not isomorphic to \"A\". (This is not quite the same thing as \"by its proper quotients\" because a proper quotient of \"A\" may be isomorphic to \"A\", for example the quotient of the semilattice (Z, min) obtained by identifying just the two elements 3 and 4.)\n\nAn immediate corollary is that any variety, as a class closed under homomorphisms, subalgebras, and direct products, is determined by its subdirectly irreducible members, since every algebra \"A\" in the variety can be constructed as a subalgebra of a suitable direct product of the subdirectly irreducible quotients of \"A\", all of which belong to the variety because \"A\" does. For this reason one often studies not the variety itself but just its subdirect irreducibles.\n\nAn algebra \"A\" is subdirectly irreducible if and only if it contains two elements that are identified by every proper quotient, equivalently, if and only if its lattice Con \"A\" of congruences has a least nonidentity element. That is, any subdirect irreducible must contain a specific pair of elements witnessing its irreducibility in this way. Given such a witness (\"a\",\"b\") to subdirect irreducibility we say that the subdirect irreducible is (\"a\",\"b\")-irreducible.\n\nGiven any class \"C\" of similar algebras, Jónsson's Lemma (due to Bjarni Jónsson) states that if the variety HSP(\"C\") generated by \"C\" is congruence-distributive, its subdirect irreducibles are in HSP(\"C\"), that is, they are quotients of subalgebras of ultraproducts of members of \"C\". (If \"C\" is a finite set of finite algebras, the ultraproduct operation is redundant.)\n\nA necessary and sufficient condition for a Heyting algebra to be subdirectly irreducible is for there to be a greatest element strictly below 1. The witnessing pair is that element and 1, and identifying any other pair \"a\", \"b\" of elements identifies both \"a\"→\"b\" and \"b\"→\"a\" with 1 thereby collapsing everything above those two implications to 1. Hence every finite chain of two or more elements as a Heyting algebra is subdirectly irreducible.\n\nBy Jónsson's Lemma, subdirectly irreducible algebras of a congruence-distributive variety generated by a finite set of finite algebras are no larger than the generating algebras, since the quotients and subalgebras of an algebra \"A\" are never larger than \"A\" itself. For example, the subdirect irreducibles in the variety generated by a finite linearly ordered Heyting algebra \"H\" must be just the nondegenerate quotients of \"H\", namely all smaller linearly ordered nondegenerate Heyting algebras. The conditions cannot be dropped in general: for example, the variety of all Heyting algebras is generated by the set of its finite subdirectly irreducible algebras, but there exist subdirectly irreducible Heyting algebras of arbitrary (infinite) cardinality. There also exists a single finite algebra generating a (non-congruence-distributive) variety with arbitrarily large subdirect irreducibles.\n"}
{"id": "1059994", "url": "https://en.wikipedia.org/wiki?curid=1059994", "title": "Sun Zhiwei", "text": "Sun Zhiwei\n\nSun Zhiwei (, born October 16, 1965) is a Chinese mathematician, working primarily in number theory, combinatorics, and group theory. He is a professor at Nanjing University.\n\nBorn in Huai'an, Jiangsu, Sun and his twin brother Sun Zhihong proved a theorem about what are now known as the Wall–Sun–Sun primes that guided the search for counterexamples to Fermat's last theorem.\n\nIn 2003, he presented a unified approach to three famous topics of Paul Erdős in combinatorial number theory: covering systems, restricted sumsets, and zero-sum problems or EGZ Theorem.\n\nHe used q-series to prove that any natural number can be represented as a sum of an even square and two triangular numbers. He conjectured, and proved with B.-K. Oh, that each positive integer can be represented as a sum of a square, an odd square and a triangular number. In 2009, he conjectured that any natural number can be written as the sum of two squares and a pentagonal number, as the sum of a triangular number, an even square and a pentagonal number, and as the sum of a square, a pentagonal number and a hexagonal number.\nHe also raised many open conjectures on congruences \nand posed over 100 conjectural series for powers of formula_1.\n\nIn 2013 he published a paper containing many conjectures on primes, one of which states that for any positive integer formula_2 there are consecutive primes formula_3 not exceeding formula_4 such that formula_5, where formula_6 denotes the formula_7-th prime.\n\nIn the paper , he refined Lagrange's four-square theorem in various ways and posed many related conjectures one of which is Sun's 1-3-5 conjecture.\n\nHe is the Editor-in-Chief of the Journal of Combinatorics and Number Theory.\n\n\n"}
{"id": "34165143", "url": "https://en.wikipedia.org/wiki?curid=34165143", "title": "Toniann Pitassi", "text": "Toniann Pitassi\n\nToniann Pitassi is a Canadian and American mathematician and computer scientist specializing in computational complexity theory.\n\nA native of Pittsburgh, Pitassi earned bachelor's and master's degrees at Pennsylvania State University before moving to the University of Toronto for her doctoral studies; she earned her Ph.D. in 1992 from Toronto under the supervision of Stephen Cook. After postdoctoral studies at the University of California, San Diego and faculty positions at the University of Pittsburgh and University of Arizona, she returned to Toronto in 2001, and is now a professor in the University of Toronto Department of Computer Science and University of Toronto Department of Mathematics.\n\nShe was an invited speaker at International Congress of Mathematicians in Berlin in 1998. She was the program chair for the 2012 Symposium on Theory of Computing. From September through December 2017, she was a Visiting Professor at the Institute for Advanced Study.\n\nPitassi's research has largely focused on proof complexity, a branch of computational complexity theory that seeks upper and lower bounds on the lengths of mathematical proofs of logical propositions within various formalized proof systems. The goal of this study is to use these bounds to understand both the time complexity of proof-finding procedures, and the relative strengths of different proof systems.\n\nResearch contributions that she has made in this area include exponential lower bounds for Frege proofs of the pigeonhole principle, exponential lower bounds for the cutting-plane method applied to propositions derived from the maximum clique problem, exponential lower bounds for resolution proofs of dense random 3-satisfiability instances, and subexponential upper bounds for the same dense random instances using the Davis–Putnam algorithm. With Paul Beame, she also wrote a survey of proof complexity.\n\n"}
{"id": "6063370", "url": "https://en.wikipedia.org/wiki?curid=6063370", "title": "Torsten Carleman", "text": "Torsten Carleman\n\nTorsten Carleman (8 July 1892, Visseltofta, Osby Municipality – 11 January 1949, Stockholm), born Tage Gillis Torsten Carleman, was a Swedish mathematician, known for his results in classical analysis and its applications. As the director of the Mittag-Leffler Institute for more than two decades, Carleman was the most influential mathematician in Sweden.\n\nThe dissertation of Carleman under Erik Albert Holmgren, as well as his work in the early 1920s, was devoted to singular integral equations. He developed the spectral theory of integral operators with \"Carleman kernels\", that is, kernels \"K\"(\"x\", \"y\") such that \"K\"(\"y\", \"x\") = \"K\"(\"x\", \"y\") for almost every (\"x\", \"y\"), and \nfor almost every \"x\".\n\nIn the mid-1920s, Carleman developed the theory of quasi-analytic functions. He proved the necessary and sufficient condition for quasi-analyticity, now called the Denjoy–Carleman theorem. As a corollary, he obtained a sufficient condition for the determinacy of the moment problem. As one of the steps in the proof of the Denjoy–Carleman theorem in , he introduced the Carleman inequality\nvalid for any sequence of non-negative real numbers \"a\".\n\nAt about the same time, he established the \"Carleman formulae\" in complex analysis, which reconstruct an analytic function in a domain from its values on a subset of the boundary. He also proved a generalisation of Jensen's formula, now called the Jensen–Carleman formula.\n\nIn the 1930s, independently of John von Neumann, he discovered the mean ergodic theorem. Later, he worked in the theory of partial differential equations, where he introduced the \"Carleman estimates\", and found a way to study the spectral asymptotics of Schrödinger operators.\n\nIn 1932, following the work of Henri Poincaré, Erik Ivar Fredholm, and Bernard Koopman, he devised the \"Carleman embedding\" (also called \"Carleman linearization\"), a way to embed a finite-dimensional system of nonlinear differential equations  = P(u) for u: R → R, where the components of P are polynomials in u, into an infinite-dimensional system of linear differential equations.\n\nIn 1933 Carleman published a short proof of what is now called the Denjoy–Carleman–Ahlfors theorem.\nThis theorem states that the number of asymptotic values attained by an entire function of order ρ along curves in the complex plane going outwards toward infinite absolute value is less than or equal to 2ρ.\n\nIn 1935, Torsten Carleman introduced a generalisation of Fourier transform, which foreshadowed the work of Mikio Sato on hyperfunctions; his notes were published in . He considered the functions \"f\" of at most polynomial growth, and showed that every such function can be decomposed as \"f\" = \"f\" + \"f\", where \"f\" and \"f\" are analytic in the upper and lower half planes, respectively, and that this representation is essentially unique. Then he defined the Fourier transform of (\"f\", \"f\") as another such pair (\"g\", \"g\"). Though conceptually different, the definition coincides with the one given later by Laurent Schwartz for tempered distributions. Carleman's definition gave rise to numerous extensions.\n\nReturning to mathematical physics in the 1930s, Carleman gave the first proof of global existence for Boltzmann's equation in the kinetic theory of gases (his result applies to the space-homogeneous case). The results were published posthumously in .\n\nCarleman supervised the Ph.D. theses of Ulf Hellsten, Karl Persson (Dagerholm), Åke Pleijel and (jointly with Fritz Carlson) of Hans Rådström.\n\nCarleman was born in Visseltofta to Alma Linnéa Jungbeck and Karl Johan Carleman, a school teacher. He studied at Växjö Cathedral School, graduating in 1910.\n\nHe continued his studies at Uppsala University, being one of the active members of the Uppsala Mathematical Society. Kjellberg recalls: \nHe was a genius! My older friends in Uppsala used to tell me about the wonderful years they had had when Carleman was there. He was the most active speaker in the Uppsala Mathematical Society and a well-trained gymnast. When people left the seminar crossing the Fyris River, he walked on his hands on the railing of the bridge.\n\nFrom 1917 he was docent at Uppsala University, and from 1923 — a full professor at Lund University. In 1924 he was appointed professor at Stockholm University. He was elected a member of the Royal Swedish Academy of Sciences in 1926. From 1927, he was director of the Mittag-Leffler Institute and editor of Acta Mathematica.\n\nFrom 1929 to 1946 Carleman was married to Anna-Lisa Lemming (1885–1954), the half-sister of the athlete Eric Lemming who won four golden medals and three bronze at the Olympic Games.\n\nCarlson remembers Carleman as: \"secluded and taciturn, who looked at life and people with a bitter humour. In his heart, he was inclined to kindliness towards those around him, and strove to assist them swiftly.\" Towards the end of his life, he remarked to his students that \"professors ought to be shot at the age of fifty.\"\n\nDuring the last decades of his life, Carleman abused alcohol, according to Norbert Wiener and William Feller. His final years were plagued by neuralgia. At the end of 1948, he developed the liver disease jaundice; he died from complications of the disease.\n\n\n"}
{"id": "150170", "url": "https://en.wikipedia.org/wiki?curid=150170", "title": "Trigonometric tables", "text": "Trigonometric tables\n\nIn mathematics, tables of trigonometric functions are useful in a number of areas. Before the existence of pocket calculators, trigonometric tables were essential for navigation, science and engineering. The calculation of mathematical tables was an important area of study, which led to the development of the first mechanical computing devices.\n\nModern computers and pocket calculators now generate trigonometric function values on demand, using special libraries of mathematical code. Often, these libraries use pre-calculated tables internally, and compute the required value by using an appropriate interpolation method. Interpolation of simple look-up tables of trigonometric functions is still used in computer graphics, where only modest accuracy may be required and speed is often paramount.\n\nAnother important application of trigonometric tables and generation schemes is for fast Fourier transform (FFT) algorithms, where the same trigonometric function values (called \"twiddle factors\") must be evaluated many times in a given transform, especially in the common case where many transforms of the same size are computed. In this case, calling generic library routines every time is unacceptably slow. One option is to call the library routines once, to build up a table of those trigonometric values that will be needed, but this requires significant memory to store the table. The other possibility, since a regular sequence of values is required, is to use a recurrence formula to compute the trigonometric values on the fly. Significant research has been devoted to finding accurate, stable recurrence schemes in order to preserve the accuracy of the FFT (which is very sensitive to trigonometric errors).\n\nModern computers and calculators use a variety of techniques to provide trigonometric function values on demand for arbitrary angles (Kantabutra, 1996). One common method, especially on higher-end processors with floating-point units, is to combine a polynomial or rational approximation (such as Chebyshev approximation, best uniform approximation, and Padé approximation, and typically for higher or variable precisions, Taylor and Laurent series) with range reduction and a table lookup — they first look up the closest angle in a small table, and then use the polynomial to compute the correction. Maintaining precision while performing such interpolation is nontrivial, however; and methods like Gal's accurate tables, Cody and Waite reduction, and Payne and Hanek reduction algorithms can be used for this purpose. On simpler devices that lack a hardware multiplier, there is an algorithm called CORDIC (as well as related techniques) that is more efficient, since it uses only shifts and additions. All of these methods are commonly implemented in hardware for performance reasons.\n\nThe particular polynomial used to approximate a trig function is generated ahead of time using some approximation of a minimax approximation algorithm.\n\nFor very high precision calculations, when series-expansion convergence becomes too slow, trigonometric functions can be approximated by the arithmetic-geometric mean, which itself approximates the trigonometric function by the (complex) elliptic integral (Brent, 1976).\n\nTrigonometric functions of angles that are rational multiples of 2π are algebraic numbers. The values for \"a/b·2π\" can be found by applying de Moivre's identity for \"n = a\" to a \"b\" root of unity, which is also a root of the polynomial \"x - 1\" in the complex plane. For example, the cosine and sine of 2π ⋅ 5/37 are the real and imaginary parts, respectively, of the 5th power of the 37th root of unity cos(2π/37) + sin(2π/37)i, which is a root of the degree-37 polynomial \"x\" − 1. For this case, a root-finding algorithm such as Newton's method is much simpler than the arithmetic-geometric mean algorithms above while converging at a similar asymptotic rate. The latter algorithms are required for transcendental trigonometric constants, however.\n\nHistorically, the earliest method by which trigonometric tables were computed, and probably the most common until the advent of computers, was to repeatedly apply the half-angle and angle-addition trigonometric identities starting from a known value (such as sin(π/2) = 1, cos(π/2) = 0). This method was used by the ancient astronomer Ptolemy, who derived them in the \"Almagest\", a treatise on astronomy. In modern form, the identities he derived are stated as follows (with signs determined by the quadrant in which \"x\" lies;\n\nThese were used to construct Ptolemy's table of chords, which was applied to astronomical problems.\n\nVarious other permutations on these identities are possible: for example, some early trigonometric tables used not sine and cosine, but sine and versine).\n\nA quick, but inaccurate, algorithm for calculating a table of \"N\" approximations \"s\" for sin(2π\"n\"/\"N\") and \"c\" for cos(2π\"n\"/\"N\") is:\n\nfor \"n\" = 0...,\"N\" − 1, where \"d\" = 2π/\"N\".\n\nThis is simply the Euler method for integrating the differential equation:\n\nwith initial conditions \"s\"(0) = 0 and \"c\"(0) = 1, whose analytical solution is \"s\" = sin(\"t\") and \"c\" = cos(\"t\").\n\nUnfortunately, this is not a useful algorithm for generating sine tables because it has a significant error, proportional to 1/\"N\".\n\nFor example, for \"N\" = 256 the maximum error in the sine values is ~0.061 (\"s\" = −1.0368 instead of −0.9757). For \"N\" = 1024, the maximum error in the sine values is ~0.015 (\"s\" = −0.99321 instead of −0.97832), about 4 times smaller. If the sine and cosine values obtained were to be plotted, this algorithm would draw a logarithmic spiral rather than a circle.\n\nA simple recurrence formula to generate trigonometric tables is based on Euler's formula and the relation:\n\nThis leads to the following recurrence to compute trigonometric values \"s\" and \"c\" as above:\n\nfor \"n\" = 0, ..., \"N\" − 1, where \"w\" = cos(2π/\"N\") and \"w\" = sin(2π/\"N\"). These two starting trigonometric values are usually computed using existing library functions (but could also be found e.g. by employing Newton's method in the complex plane to solve for the primitive root of \"z\" − 1).\n\nThis method would produce an \"exact\" table in exact arithmetic, but has errors in finite-precision floating-point arithmetic. In fact, the errors grow as O(ε \"N\") (in both the worst and average cases), where ε is the floating-point precision.\n\nA significant improvement is to use the following modification to the above, a trick (due to Singleton, 1967) often used to generate trigonometric values for FFT implementations:\n\nwhere α = 2 sin(π/\"N\") and β = sin(2π/\"N\"). The errors of this method are much smaller, O(ε √\"N\") on average and O(ε \"N\") in the worst case, but this is still large enough to substantially degrade the accuracy of FFTs of large sizes.\n\n\n"}
