{"id": "641270", "url": "https://en.wikipedia.org/wiki?curid=641270", "title": "A Course of Pure Mathematics", "text": "A Course of Pure Mathematics\n\nA Course of Pure Mathematics is a classic textbook in introductory mathematical analysis, written by G. H. Hardy. It is recommended for people studying calculus. First published in 1908, it went through ten editions (up to 1952) and several reprints. It is now out of copyright in UK and is downloadable from various internet web sites. It remains one of the most popular books on pure mathematics.\n\nThe book contains a large number of descriptive and study materials together with a number of difficult problems with regards to number theory analysis. The book is organized into the following chapters, with each chapter further divided.\n\nI. REAL VARIABLES\n\nII. FUNCTIONS OF REAL VARIABLES\n\nIII COMPLEX NUMBERS\n\nIV LIMITS OF FUNCTIONS OF A POSITIVE INTEGRAL VARIABLE\n\nV LIMITS OF FUNCTIONS OF A CONTINUOUS VARIABLE. CONTINUOUS AND DISCONTINUOUS FUNCTIONS\n\nVI DERIVATIVES AND INTEGRALS\n\nVII ADDITIONAL THEOREMS IN THE DIFFERENTIAL AND INTEGRAL CALCULUS\n\nVIII THE CONVERGENCE OF INFINITE SERIES AND INFINITE INTEGRALS\n\nIX THE LOGARITHMIC, EXPONENTIAL AND CIRCULAR FUNCTIONS OF A REAL VARIABLE\n\nX THE GENERAL THEORY OF THE LOGARITHMIC, EXPONENTIAL AND CIRCULAR FUNCTIONS\n\nAppendices\n\nINDEX\n\nThe book was intended to help reform mathematics teaching in the UK, and more specifically in the University of Cambridge and in schools preparing to study higher mathematics. It was aimed directly at \"scholarship level\" students – the top 10% to 20% by ability. Hardy himself did not originally find a passion for mathematics, only seeing it as a way to beat other students, which he did decisively, and gain scholarships. However, his book excels in effectively explaining analytical number theory and calculus following the rigor of mathematics.\n\nWhilst his book changed the way the subject was taught at university, the content reflects the era in which the book was written. The whole book explores number theory and the author constructs real numbers theoretically. It adequately deals with single-variable calculus, sequences, number series, properties of cos, sin, log, etc. but does not refer to mathematical groups, multi-variable functions or vector calculus. Each section includes some demanding problems. Hardy combines the enthusiasm of the missionary with the rigor of the purist in his exposition of the fundamental ideas of the differential and integral calculus, of the properties of infinite series and of other topics involving the notion of limit. Hardy's presentation of mathematical analysis is as valid today as when first written: students will find that his economical and energetic style of presentation is one that modern authors rarely come close to. Despite its limitations, it is considered a classic in its field. It is probably of most use to 1st year university students of pure mathematics.\n\n\n"}
{"id": "12008116", "url": "https://en.wikipedia.org/wiki?curid=12008116", "title": "Alpha recursion theory", "text": "Alpha recursion theory\n\nIn recursion theory, α recursion theory is a generalisation of recursion theory to subsets of admissible ordinals formula_1. An admissible set is closed under formula_2 functions. If formula_3 is a model of Kripke–Platek set theory then formula_1 is an admissible ordinal. In what follows formula_1 is considered to be fixed.\n\nThe objects of study in formula_1 recursion are subsets of formula_1. A is said to be formula_1 recursively enumerable if it is formula_9 definable over formula_10. A is recursive if both A and formula_11 (its complement in formula_1) are formula_1 recursively enumerable.\n\nMembers of formula_10 are called formula_1 finite and play a similar role to the finite numbers in classical recursion theory.\n\nWe say R is a reduction procedure if it is formula_1 recursively enumerable and every member of R is of the form formula_17 where \"H\", \"J\", \"K\" are all α-finite.\n\n\"A\" is said to be α-recursive in \"B\" if there exist formula_18 reduction procedures such that:\n\nIf \"A\" is recursive in \"B\" this is written formula_21. By this definition \"A\" is recursive in formula_22 (the empty set) if and only if \"A\" is recursive. However A being recursive in B is not equivalent to A being formula_23.\n\nWe say \"A\" is regular if formula_24 or in other words if every initial portion of \"A\" is α-finite.\n\nShore's splitting theorem: Let A be formula_1 recursively enumerable and regular. There exist formula_1 recursively enumerable formula_28 such that formula_29\n\nShore's density theorem: Let \"A\", \"C\" be α-regular recursively enumerable sets such that formula_30 then there exists a regular α-recursively enumerable set \"B\" such that formula_31.\n\n"}
{"id": "31152478", "url": "https://en.wikipedia.org/wiki?curid=31152478", "title": "Aristarchus's inequality", "text": "Aristarchus's inequality\n\nAristarchus's inequality (after the Greek astronomer and mathematician Aristarchus of Samos; c. 310 – c. 230 BCE) is a law of trigonometry which states that if \"α\" and \"β\" are acute angles (i.e. between 0 and a right angle) and \"β\" < \"α\" then\n\nPtolemy used the first of these inequalities while constructing his table of chords.\n\n"}
{"id": "6178477", "url": "https://en.wikipedia.org/wiki?curid=6178477", "title": "BB84", "text": "BB84\n\nBB84 is a quantum key distribution scheme developed by Charles Bennett and Gilles Brassard in 1984. It is the first quantum cryptography protocol. The protocol is provably secure, relying on the quantum property that information gain is only possible at the expense of disturbing the signal if the two states one is trying to distinguish are not orthogonal (see no-cloning theorem) and an authenticated public classical channel. It is usually explained as a method of securely communicating a private key from one party to another for use in one-time pad encryption.\n\nIn the BB84 scheme, Alice wishes to send a private key to Bob. She begins with two strings of bits, formula_1 and formula_2, each formula_3 bits long. She then encodes these two strings as a string of formula_3 qubits:\n\nwhere formula_6 and formula_7 are the formula_8-th bits of formula_1 and formula_2 respectively. Together, formula_11 give us an index into the following four qubit states:\n\nNote that the bit formula_7 is what decides which basis formula_6 is encoded in (either in the computational basis or the Hadamard basis). The qubits are now in states that are not mutually orthogonal, and thus it is impossible to distinguish all of them with certainty without knowing formula_2.\n\nAlice sends formula_19 over a public and authenticated quantum channel formula_20 to Bob. Bob receives a state formula_21, where formula_20 represents both the effects of noise in the channel and eavesdropping by a third party we'll call Eve. After Bob receives the string of qubits, all three parties, namely Alice, Bob and Eve, have their own states. However, since only Alice knows formula_2, it makes it virtually impossible for either Bob or Eve to distinguish the states of the qubits. Also, after Bob has received the qubits, we know that Eve cannot be in possession of a copy of the qubits sent to Bob, by the no-cloning theorem, unless she has made measurements. Her measurements, however, risk disturbing a particular qubit with probability ½ if she guesses the wrong basis.\n\nBob proceeds to generate a string of random bits formula_24 of the same length as formula_2 and then measures the string he has received from Alice, formula_26. At this point, Bob announces publicly that he has received Alice's transmission. Alice then knows she can now safely announce formula_2. Bob communicates over a public channel with Alice to determine which formula_7 and formula_29 are not equal. Both Alice and Bob now discard the qubits in formula_1 and formula_26 where formula_2 and formula_24 do not match.\n\nFrom the remaining formula_34 bits where both Alice and Bob measured in the same basis, Alice randomly chooses formula_35 bits and discloses her choices over the public channel. Both Alice and Bob announce these bits publicly and run a check to see whether more than a certain number of them agree. If this check passes, Alice and Bob proceed to use information reconciliation and privacy amplification techniques to create some number of shared secret keys. Otherwise, they cancel and start over.\n\n"}
{"id": "1927952", "url": "https://en.wikipedia.org/wiki?curid=1927952", "title": "Ben Green (mathematician)", "text": "Ben Green (mathematician)\n\nBen Joseph Green FRS (born 27 February 1977) is a British mathematician, specialising in combinatorics and number theory. He is the Waynflete Professor of Pure Mathematics at the University of Oxford.\n\nBen Green was born on 27 February 1977 in Bristol, England. He studied at local schools in Bristol, Bishop Road Primary School and Fairfield Grammar School, competing in the International Mathematical Olympiad in 1994 and 1995. He entered Trinity College, Cambridge in 1995 and completed his BA in mathematics in 1998, winning the Senior Wrangler title. He stayed on for Part III and earned his doctorate under the supervision of English mathematician Timothy Gowers, with a thesis entitled \"Topics in arithmetic combinatorics\" (2003). During his PhD he spent a year as a visiting student at Princeton University. He was a research Fellow at Trinity College, Cambridge between 2001 and 2005, before becoming a Professor of Mathematics at the University of Bristol from January 2005 to September 2006 and then the first Herchel Smith Professor of Pure Mathematics at the University of Cambridge from September 2006 to August 2013. He became the Waynflete Professor of Pure Mathematics at the University of Oxford on 1 August 2013. He was also a Research Fellow of the Clay Mathematics Institute and held various positions at institutes such as Princeton University, University of British Columbia, and Massachusetts Institute of Technology.\n\nThe majority of Green's research is in the fields of analytic number theory and additive combinatorics, but he also has results in harmonic analysis and in group theory. His most well known theorem, proved jointly with his frequent collaborator Terence Tao, states that there exist arbitrarily long arithmetic progressions in the prime numbers: this is now known as the Green–Tao theorem.\n\nAmongst Green's early results in additive combinatorics are an improvement of a result of Jean Bourgain of the size of arithmetic progressions in sumsets, as well as a proof of the Cameron–Erdős conjecture on sum-free sets of natural numbers. He also proved an arithmetic regularity lemma for functions defined on the first formula_1 natural numbers, somewhat analogous to the Szemerédi regularity lemma for graphs.\n\nFrom 2004-2010, in joint work with Terence Tao and Tamar Ziegler, he developed so-called higher order Fourier analysis. This theory relates Gowers norms with objects known as nilsequences. The theory derives its name from these nilsequences, which play an analogous role to the role that characters play in classical Fourier analysis. Green and Tao used higher order Fourier analysis to present a new method for counting the number of solutions to simultaneous equations in certain sets of integers, including in the primes. This generalises the classical approach using Hardy--Littlewood circle method. Many aspects of this theory, including the quantitative aspects of the inverse theorem for the Gowers norms, are still the subject of ongoing research.\n\nGreen has also collaborated with Emmanuel Breuillard on topics in group theory. In particular, jointly with Terence Tao, they proved a structure theorem for approximate groups, generalising the Freiman-Ruzsa theorem on sets of integers with small doubling. Green also has work, joint with Kevin Ford and Sean Eberhard, on the theory of the symmetric group, in particular on what proportion of its elements fix a set of size formula_2.\n\nGreen and Tao also have a paper on algebraic combinatorial geometry, resolving the Dirac-Motzkin conjecture (see Sylvester–Gallai theorem). In particular they prove that, given any collection of formula_3 points in the plane that are not all collinear, if formula_3 is large enough then there must exist at least formula_5 lines in the plane containing exactly two of the points.\n\nKevin Ford, Ben Green, Sergei Konyagin, James Maynard and Terence Tao, initially in two separate research groups and then in combination, improved the lower bound for the size of the longest gap between two consecutive primes of size at most formula_6. The form of the previously best-known bound, essentially due to Rankin, had not been improved for 76 years.\n\nMore recently Green has considered questions in arithmetic Ramsey theory. Together with Tom Sanders he proved that, if a finite field of prime order is finitely coloured, then there exist two elements formula_7 and formula_8 in the field such that formula_7,formula_8, formula_11 and formula_12 all have the same colour.\n\nGreen has also been involved with the new developments of Croot-Lev-Pach-Ellenberg-Gijswijt on applying a polynomial method to bound the size of subsets of a finite vector space without solutions to linear equations. He adapted these methods to prove, in function fields, a strong version of Sárközy's theorem.\n\nGreen has been a Fellow of the Royal Society since 2010, and a Fellow of the American Mathematical Society since 2012. Green was chosen by the German Mathematical Society to deliver a Gauss Lectureship in 2013. He has received several awards:\n\n\n"}
{"id": "30780965", "url": "https://en.wikipedia.org/wiki?curid=30780965", "title": "Bidirectional transformation", "text": "Bidirectional transformation\n\nIn computer programming, bidirectional transformations (bx) are programs in which a single piece of code can be run in several ways, such that the same data are sometimes considered as input, and sometimes as output. For example, a bx run in the forward direction might transform input I into output O, while the same bx run backward would take as input versions of I and O and produce a new version of I as its output.\n\nBidirectional model transformations are an important special case in which a model is input to such a program.\n\nSome bidirectional languages are \"bijective\". The bijectivity of a language is a severe restriction of its bidirectionality, because a bijective language is merely relating two different ways to present the very same information.\n\nMore general is a lens language, in which there is a distinguished forward direction (\"get\") that takes a concrete input to an abstract output, discarding some information in the process: the concrete state includes all the information that is in the abstract state, and usually some more. The backward direction (\"put\") takes a concrete state and an abstract state and computes a new concrete state. Lenses are required to obey certain conditions to ensure sensible behaviour.\n\nThe most general case is that of symmetric bidirectional transformations. Here the two states that are related typically share some information, but each also includes some information that is not included in the other.\n\nBidirectional transformations can be used to:\n\n\nA bidirectional program which obeys certain round-trip laws is called a lens.\n\n\n\n"}
{"id": "27378783", "url": "https://en.wikipedia.org/wiki?curid=27378783", "title": "Birla Industrial &amp; Technological Museum", "text": "Birla Industrial &amp; Technological Museum\n\nBirla Industrial & Technological Museum (BITM), a unit under National Council of Science Museums (NCSM), Ministry of Culture, Government of India, is at Gurusaday Road, Kolkata.\n\nThe first science museum in India was set up by the industrialist Ghanshyam Das Birla at BITS, in Pilani in a hall (185 sq.mt area) of the Tower Building. The museum depicted mainly the industries and business enterprises of the Birlas. The museum was opened to the public in 1954. Ten years later the museum was shifted to the present building.\n\nThe second science museum was mooted by KS Krishnan, physicist and the then Director of National Physical Laboratory (NPL), he was inspired and encouraged by the then prime minister of India Jawaharlal Nehru. R Subramanian was appointed to develop science museum and planetarium project by NPL in 1956. The science museum of 555 sq.mr floor space in Delhi was opened for public in 1956, but it was close down by the authority after few years, although it was appreciated by general visitors.\n\nBidhan Chandra Roy, the then Chief Minister of West Bengal and physician was impressed to see Deutsches Museum of Munich. He thought to set up a science museum and a planetarium in Calcutta. Roy requested to GD Birla for a help. Birla donated his residential house to the then prime minister of India Jawaharlal Nehru. The three storied Victorian style architectural building along with five bighas land of ‘Birla Park’, where they had lived for thirty five years.\n\n\n\n\n"}
{"id": "1731976", "url": "https://en.wikipedia.org/wiki?curid=1731976", "title": "Branko Grünbaum", "text": "Branko Grünbaum\n\nBranko Grünbaum (; 2 October 1929 – 14 September 2018) was a Yugoslavian-born mathematician of Jewish descent and a professor emeritus at the University of Washington in Seattle. He received his Ph.D. in 1957 from Hebrew University of Jerusalem in Israel. \nHe authored over 200 papers, mostly in discrete geometry, an area in which he is known for various classification theorems. He wrote on the theory of abstract polyhedra.\n\nHis paper on line arrangements may have inspired a paper by N. G. de Bruijn on quasiperiodic tilings (the most famous example of which is the Penrose tiling of the plane). This paper is also cited by the authors of a monograph on hyperplane arrangements as having inspired their research.\n\nGrünbaum also devised a multi-set generalisation of Venn diagrams. He was an editor and a frequent contributor to \"Geombinatorics\".\n\nGrünbaum's classic monograph \"Convex polytopes\", first published in 1967, became the main textbook on the subject. His monograph \"Tilings and Patterns\", coauthored with G. C. Shephard, helped to rejuvenate interest in this classic field, and has proved popular with nonmathematical audiences, as well as with mathematicians.\n\nIn 1976 Grünbaum won a Lester R. Ford Award for his expository article \"Venn diagrams and independent families of sets\". In 2004, Gil Kalai and Victor Klee edited a special issue of \"Discrete and Computational Geometry\" in his honor, the \"Grünbaum Festschrift\". In 2005, Grünbaum was awarded the Leroy P. Steele Prize for Mathematical Exposition from the American Mathematical Society. He was a Guggenheim Fellow, a Fellow of the AAAS and in 2012 he became a fellow of the American Mathematical Society.\nGrünbaum supervised 19 Ph.D.s and currently has at least 200 mathematical \"descendants\".\n\n\n\n"}
{"id": "2850640", "url": "https://en.wikipedia.org/wiki?curid=2850640", "title": "Busemann function", "text": "Busemann function\n\nIn geometric topology, Busemann functions are used to study the large-scale geometry of geodesics in Hadamard spaces and in particular Hadamard manifolds (simply connected complete Riemannian manifolds of nonpositive curvature). They are named after Herbert Busemann, who introduced them; he gave an extensive treatment of the topic in his 1955 book \"The geometry of geodesics\".\n\nLet formula_1 be a metric space. A geodesic ray is a path formula_2 which minimizes distance everywhere along its length. i.e., for all formula_3, \nEquivalently, a ray is an isometry from the \"canonical ray\" (the set formula_5 equipped with the Euclidean metric) into the metric space \"X\".\n\nGiven a ray \"γ\", the Busemann function formula_6 is defined by\n\nThus, when \"t\" is very large, the distance formula_8 is approximately equal to formula_9. Given a ray \"γ\", its Busemann function is always well-defined: indeed the right hand side \"F\"(\"x\") above tends pointwise to the left hand side on compacta, since formula_10 is bounded above by formula_11 and increasing since, if formula_12,\n\nIt is immediate from the triangle inequality that\n\nso that formula_15 is uniformly continuous. More specifically, the above estimate above shows that\n\n\nBy Dini's theorem, the functions formula_16 tend to formula_17 uniformly on compacta as \"t\" tends to infinity.\n\nLet \"D\" be the unit disk in the complex plane with the Poincaré metric\n\nThen, for |z| < 1 and |ζ| = 1, the Busemann function is given by\n\nwhere the term in brackets on the right hand side is the Poisson kernel for the unit disk and ζ corresponds to the radial geodesic γ from the origin towards ζ,\nγ(\"t\") = ζ tanh(\"t\"/2). In fact the computation of \"d\"(\"x\",\"y\") can be reduced to that of \"d\"(\"z\",0) = \"d\"(|\"z\"|,0) = tanh |\"z\"| = log (1+|\"z\"|)/(1-|\"z\"|), since the metric is invariant under Möbius transformations in SU(1,1); the geodesics through 0 have the form ζ \"g\"(0) where \"g\" is the 1-parameter subgroup of SU(1,1)\n\nThe formula above also completely determines the Busemann function by Möbius invariance. Note that\n\nso that the Busemann function in this case is non-negative.\n\nIn a Hadamard space, where any two points are joined by a unique geodesic segment, the function \"F\" = \"F\" is \"convex\", i.e. convex on geodesic segments [\"x\",\"y\"]. Explicitly this means that if \"z\"(\"s\") is the point which divides [\"x\",\"y\"] in the ratio , then \"F\"(\"z\"(\"s\")) ≤ \"s\" \"F\"(\"x\") + (1 − \"s\") \"F\"(\"y\"). In fact for fixed \"a\" the function \"d\"(\"x\",\"a\") is convex and hence so are its translates; in particular, if γ is a geodesic ray in \"X\", then \"F\" is convex. Since the Busemann function \"B\" is the pointwise limit of \"F\",\n\n\nIn fact let . Since γ(\"t\") is parametrised by arclength, Alexandrov's first comparison theorem for Hadamard spaces implies that the function is convex. Hence for 0< \"s\" < \"t\"\n\nThus\n\nso that\n\nLetting \"t\" tend to ∞, it follows that\n\nso convergence is uniform on bounded sets.\n\nNote that the inequality above for formula_28 (together with its proof) also holds for geodesic segments: if Γ(\"t\") is a geodesic segment starting at \"x\" and parametrised by arclength then\n\nNext suppose that \"x\", \"y\" are points in a Hadamard space, and let δ(\"s\") be the geodesic through \"x\" with δ(0) = \"y\" and δ(\"t\") = \"x\", where \"t\" = \"d\"(\"x\",\"y\"). This geodesic cuts the boundary of the closed ball (\"y\",\"r\") at the point δ(\"r\"). Thus if , there is a point \"v\" with such that .\n\nThis condition persists for Busemann functions. The statement and proof of the property for Busemann functions relies on a fundamental theorem on closed convex subsets of a Hadamard space, which generalises orthogonal projection in a Hilbert space: if is a closed convex set in a Hadamard space , then every point in has a unique closest point in and ; moreover is uniquely determined by the property that, for in ,\n\nso that the angle at in the Euclidean comparison triangle for \"a\",\"x\",\"y\" is greater than or equal to /2.\n\n\nIn fact let \"v\" be the closest point to \"y\" in \"C\". Then \"h\"(\"v\") = \"h\"(\"y\") − \"r\" and so \"h\" is minimised by \"v\" in (\"y\",\"R\") where \"R\" = \"d\"(\"y\",\"v\") and \"v\" is the unique point where \"h\" is minimised. By the Lipschitz condition \"r\" = |\"h\"(\"y\") − \"h\"(\"v\")| ≤ \"R\". To prove the assertion, it suffices to show that \"R\" = \"r\", i.e. \"d\"(\"y\",\"v\") = \"r\". On the other hand, \"h\" is the uniform limit on any closed ball of functions \"h\". On (\"y\",\"r\"), these are minimised by points \"v\" with \"h\"(\"v\") = \"h\"(\"y\") − \"r\". Hence the infimum of \"h\" on (\"y\",\"r\") is \"h\"(\"y\") − \"r\" and \"h\"(\"v\") tends to \"h\"(\"y\") − \"r\". Thus \"h\"(\"v\") = \"h\"(\"y\") − \"r\" with \"r\" ≤ \"r\" and \"r\" tending towards \"r\". Let \"u\" be the closest point to \"y\" with \"h\"(\"u\") ≤ \"h\"(\"y\") − \"r\". Let \"R\" = \"d\"(\"y\",\"u\") ≤ \"r\". Then \"h\"(\"u\") = \"h\"(\"y\") − \"r\", and, by the Lipschitz condition on \"h\", \"R\" ≥ \"r\". In particular \"R\" tends to \"r\". Passing to a subsequence if necessary it can be assumed that \"r\" and \"R\" are both increasing (to \"r\"). The inequality for convex optimisation implies that for \"n\" > \"m\".\n\nso that \"u\" is a Cauchy sequence. If \"u\" is its limit, then \"d\"(\"y\",\"u\") = \"r\" and \"h\"(\"u\") = \"h\"(\"y\") − \"r\". By uniqueness it follows that \"u\" = \"v\" and hence \"d\"(\"y\",\"v\") = \"r\", as required.\n\nUniform limits. The above argument proves more generally that if \"d\"(\"x\",\"x\") tends to infinity and the functions \"h\"(\"x\") = \"d\"(\"x\",\"x\") – \"d\"(\"x\",\"x\") tend uniformly on bounded sets to \"h\"(\"x\"), then \"h\" is convex, Lipschitz with Lipschitz constant 1 and, given \"y\" in \"X\" and \"r\" > 0, there is a unique point \"v\" with \"d\"(\"y\",\"v\") = \"r\" such that \"h\"(\"v\") = \"h\"(\"y\") − \"r\". If on the other hand the sequence (\"x\") is bounded, then the terms all lie in some closed ball and uniform convergence there implies that (\"x\") is a Cauchy sequence so converges to some \"x\" in \"X\". So \"h\" tends uniformly to \"h\"(\"x\") = \"d\"(\"x\",\"x\") – \"d\"(\"x\",\"x\"), a function of the same form. The same argument also shows that the class of functions which satisfy the same three conditions (being convex, Lipschitz and having minima on closed balls) is closed under taking uniform limits on bounded sets.\n\nComment. Note that, since any closed convex subset of a Hadamard subset of a Hadamard space is also a Hadamard space, any closed ball in a Hadamard space is a Hadamard space. In particular it need not be the case that every geodesic segment is contained in a geodesic defined on the whole of R or even a semi-infinite interval [0,∞). The closed unit ball of a Hilbert space gives an explicit example which is not a proper metric space.\n\n\nIn fact the third condition implies that \"v\" is the closest point to \"y\" in the closed convex set \"C\" of points \"u\" such that \"h\"(\"u\") ≤ \"h\"(\"y\") – \"r\". Let δ(\"t\") for 0 ≤ \"t\" ≤ \"r\" be the geodesic joining \"y\" to \"v\". Then \"k\"(\"t\") = \"h\"(δ(\"t\")) - \"h\"(\"y\") is a convex Lipschitz function on [0,\"r\"] with Lipschitz constant 1 satisfying \"k\"(\"t\") ≤ – \"t\" and \"k\"(0) = 0 and \"k\"(\"r\") = –\"r\". So \"k\" vanishes everywhere, since if 0 < \"s\" < \"r\", \"k\"(\"s\") ≤ –\"s\" and |\"k\"(s)| ≤ \"s\". Hence \"h\"(δ(\"t\")) = \"h\"(\"y\") – \"t\". By uniqueness it follows that δ(\"t\") is the closest point to \"y\" in \"C\" and that it is the unique point minimising \"h\" in (\"y\",\"t\"). Uniqueness implies that these geodesics segments coincide for arbitrary \"r\" and therefore that δ extends to a geodesic ray with the stated property.\n\n\nTo prove the first assertion, it is enough to check this for \"t\" sufficiently large. In that case γ(\"t\") and δ(\"t\" − \"h\"(\"y\")) are the projections of \"x\" and \"y\" onto the closed convex set . Therefore, \"d\"(γ(\"t\"),δ(\"t\" − \"h\"(\"y\"))) ≤ \"d\"(\"x\",\"y\"). Hence \"d\"(γ(\"t\"),δ(\"t\")) ≤ \"d\"(γ(\"t\"),δ(\"t\" − \"h\"(\"y\"))) + \"d\"(δ(\"t\" − \"h\"(\"y\")),δ(\"t\")) ≤ \"d\"(\"x\",\"y\") + |\"h\"(\"y\")|. The second assertion follows because \"d\"(δ(\"t\"),δ(\"t\")) is convex and bounded on [0,∞), so, if it vanishes at \"t\" = 0, must vanish everywhere.\n\n\nLet \"C\" be the closed convex set of points \"z\" with \"h\"(\"z\") ≤ −\"r\". Since \"X\" is a Hadamard space for every point \"y\" in \"X\" there is a unique closest point \"P\"(\"y\") to \"y\" in \"C\". It depends continuously on \"y\" and if \"y\" lies outside \"C\", then \"P\"(\"y\") lies on the hypersurface \"h\"(\"z\") = − \"r\"—the boundary ∂\"C\" of \"C\"—and \"P\"(\"y\") satisfies the inequality of convex optimisation. Let δ(\"s\") be the geodesic ray starting at \"y\".\n\nFix \"x\" in \"X\". Let γ(\"s\") be the geodesic ray starting at \"x\". Let \"g\"(\"z\") = \"h\"(\"z\"), the Busemann function for γ with base point \"x\". In particular \"g\"(\"x\") = 0. It suffices to show that . Now take \"y\" with \"h\"(\"x\") = \"h\"(\"y\") and let δ(\"t\") be the geodesic ray starting at \"y\" corresponding to \"h\". Then\n\nOn the other hand, for any four points \"a\", \"b\", \"c\", \"d\" in a Hadamard space, the following quadrilateral inequality of Reshetnyak holds:\n\nSetting \"a\" = \"x\", \"b\" = \"y\", \"c\" = γ(\"t\"), \"d\" = δ(\"t\"), it follows that\n\nso that\n\nHence \"h\"(\"y\") = 0. Similarly \"h\"(\"x\") = 0. Hence \"h\"(\"y\") = 0 on the level surface of \"h\" containing \"x\". Now for \"t\" ≥ 0 and \"z\" in \"X\", let α(\"z\") = γ(\"t\") the geodesic ray starting at \"z\". Then and . Moreover, by boundedness, . The flow α can be used to transport this result to all the level surfaces of \"h\". In fact for general \"y\", if \"h\"(\"y\") < \"h\"(\"x\"), take \"s\" > 0 such that \"h\"(α(\"x\")) = \"h\"(\"y\") and set \"x\" = α(\"x\"). Then \"h\"(\"y\") = 0, where γ(\"t\") = α(\"x\") = γ(\"s\" + \"t\"). But then \"h\" = \"h\" – \"s\", so that \"h\"(\"y\") = \"s\". Hence , as required. Similarly if \"h\"(\"y\") > \"h\"(\"x\"), take\n\"s\" > 0 such that \"h\"(α(\"y\")) = \"h\"(\"x\"). Let \"y\" = α(\"y\"). Then\n\"h\"(\"y\") = 0, so \"h\"(\"y\") = –\"s\". Hence , as required.\n\nFinally there are necessary and sufficient conditions for two geodesics to define the same Busemann function up to constant:\n\n\nSuppose firstly that γ and δ are two geodesic rays with Busemann functions differing by a constant. Shifting the argument of one of the geodesics by a constant, it may be assumed that \"B\" = \"B\" = \"B\", say. Let \"C\" be the closed convex set on which \"B\"(\"x\") ≤ −\"r\". Then \"B\"(γ(\"t\")) = \"B\"(γ(\"t\")) = −\"t\" and similarly \"B\"(δ(\"t\")) = − \"t\". Then for \"s\" ≤ \"r\", the points γ(\"s\") and δ(\"s\") have closest points γ(\"r\") and δ(\"r\") in \"C\", so that \"d\"(γ(\"r\"), δ(\"r\")) ≤ \"d\"(γ(\"s\"), δ(\"s\")). Hence .\n\nNow suppose that . Let δ(\"t\") be the geodesic ray starting at \"y\" associated with \"h\". Then . Hence . Since δ and δ both start at \"y\", it follows that δ(\"t\") ≡ δ(\"t\"). By the previous result \"h\" and \"h\" differ by a constant; so \"h\" and \"h\" differ by a constant.\n\nTo summarise, the above results give the following characterisation of Busemann functions on a Hadamard space:\n\nTHEOREM. On a Hadamard space, the following conditions on a function f are equivalent:\n\n\nIn the previous section it was shown that if \"X\" is a Hadamard space and \"x\" is a fixed point in \"X\" then the union of the space of Busemann functions vanishing at \"x\" and the space of functions \"h\"(\"x\") = \"d\"(\"x\",\"y\") − \"d\"(\"x\",\"y\") is closed under taking uniform limits on bounded sets. This result can be formalised in the notion of bordification of \"X\". In this topology, the points \"x\" tend to a geodesic ray γ starting at \"x\" if and only if \"d\"(\"x\",\"x\") tends to ∞ and for \"t\" > 0 arbitrarily large the sequence obtained by taking the point on each segment [\"x\",\"x\"] at a distance \"t\" from \"x\" tends to γ(\"t\").\n\nIf \"X\" is a metric space, Gromov's bordification can be defined as follows. Fix a point \"x\" in \"X\" and let \"X\" = (\"x\",\"N\"). Let \"Y\" = \"C\"(\"X\") be the space of Lipschitz continuous functions on \"X\", .e. those for which |\"f\"(\"x\") – \"f\"(\"y\")| ≤ \"A\" \"d\"(\"x\",\"y\") for some constant \"A\" > 0. The space \"Y\" can be topologised by the seminorms ||\"f\"|| = sup |\"f\"|, the topology of uniform convergence on bounded sets. The seminorms are finite by the Lipschitz conditions. This is the topology induced by the natural map of \"C\"(\"X\") into the direct product of the Banach spaces \"C\"(\"X\") of continuous bounded functions on \"X\". It is give by the metric \"D\"(\"f\",\"g\") = ∑ 2 ||\"f\" − \"g\"||(1 +||\"f\" − \"g\"||).\n\nThe space \"X\" is embedded into \"Y\" by sending \"x\" to the function \"f\"(\"y\") = \"d\"(\"y\",\"x\") – \"d\"(\"x\",\"x\"). Let be the closure of \"X\" in \"Y\". Then is metrisable, since \"Y\" is, and contains \"X\" as an open subset; moreover bordifications arising from different choices of basepoint are naturally homeomorphic. Let \"h\"(\"x\") = (\"d\"(\"x\",\"x\") + 1). Then \"h\" lies in \"C\"(\"X\"). It is non-zero on \"X\" and vanishes only at ∞. Hence it extends to a continuous function on with zero set \\ \"X\". It follows that \\ \"X\" is closed in , as required. To check that = (\"x\") is independent of the basepoint, it suffices to show that \"k\"(\"x\") = \"d\"(\"x\",\"x\") − \"d\"(\"x\",\"x\") extends to a continuous function on . But \"k\"(\"x\") = \"f\"(\"x\"), so, for \"g\" in , \"k\"(\"g\") = \"g\"(\"x\"). Hence the correspondence between the compactifications for \"x\" and \"x\" is given by sending \"g\" in (\"x\") to \"g\" + \"g\"(\"x\")1 in (\"x\").\n\nWhen \"X\" is a Hadamard space, Gromov's ideal boundary ∂\"X\" = \\ \"X\" can be realised explicitly as \"asymptotic limits\" of geodesic rays using Busemann functions. In fact if \"x\" is an unbounded sequence in \"X\" with \"h\"(\"x\") = \"d\"(\"x\",\"x\") − \"d\"(\"x\",\"x\") tending to \"h\" in \"Y\", then \"h\" vanishes at \"x\", is convex, Lipschitz with Lipschitz constant 1 and has minimum \"h\"(\"y\") − \"r\" on any closed ball (\"y\",\"r\"). Hence \"h\" is a Busemann function \"B\" corresponding to a unique geodesic ray γ starting at \"x\".\n\nOn the other hand, \"h\" tends to \"B\" uniformly on bounded sets if and only if \"d\"(\"x\",\"x\") tends to ∞ and for \"t\" > 0 arbitrarily large the sequence obtained by taking the point on each segment [\"x\",\"x\"] at a distance \"t\" from \"x\" tends to γ(\"t\"). In fact for \"d\"(\"x\",\"x\") ≥ \"t\", let \"x\"(\"t\") be the point in [\"x\",\"x\"] with \"d\"(\"x\",\"x\"(\"t\")) = \"t\". Suppose first that \"h\" tends to \"B\" uniformly on (\"x\",\"R\"). Then for \"t\" ≤ \"R\",\n\nso it suffices show that on any bounded set \"h\"(\"y\") = \"d\"(\"y\",\"x\") – \"d\"(\"x\",\"x\") is uniformly close to \"F\"(\"y\") for \"n\" sufficiently large.\n\nFor a fixed ball (\"x\",\"R\"), fix \"s\" so that \"R\"/\"s\" ≤ ε. The claim is then an immediate consequence of the inequality for geodesic segments in a Hadamard space, since\n\nHence, if \"y\" in (\"x\",\"R\") and \"n\" is sufficiently large that \"d\"(\"x\"(\"s\"),γ(\"s\")) ≤ ε, then\n\nSuppose that \"x\", \"y\" are points in a Hadamard manifold and let \"γ\"(\"s\") be the geodesic through \"x\" with \"γ\"(0) = \"y\". This geodesic cuts the boundary of the closed ball (\"y\",\"r\") at the two points γ(±\"r\"). Thus if \"d\"(\"x\",\"y\") > \"r\", there are points \"u\", \"v\" with \"d\"(\"y\",\"u\") = \"d\"(\"y\",\"v\") = \"r\" such that |\"d\"(\"x\",\"u\") − \"d\"(\"x\",\"v\")| = 2\"r\". By continuity this condition persists for Busemann functions:\n\n\nIn fact taking a sequence \"t\" tending to ∞ and \"h\" = \"F\", there are points \"u\" and \"v\" which satisfy these conditions for \"h\" for \"n\" sufficiently large. Passing to a subsequence if necessary, it can be assumed that \"u\" and \"v\" tend to \"u\" and \"v\". By continuity these points satisfy the conditions for \"h\". To prove uniqueness, note that by compactness \"h\" assumes its maximum and minimum on (\"y\",\"r\"). The Lipschitz condition shows that the values of \"h\" there differ by at most 2\"r\". Hence \"h\" is minimized at \"v\" and maximized at \"u\". On the other hand, \"d\"(\"u\",\"v\") = 2\"r\" and for \"u\" and \"v\" the points \"v\" and \"u\" are the unique points in (\"y\",\"r\") maximizing this distance. The Lipschitz condition on \"h\" then immediately implies \"u\" and \"v\" must be the unique points in (\"y\",\"r\") maximizing and minimizing \"h\". Now suppose that \"y\" tends to \"y\". Then the corresponding points \"u\" and \"v\" lie in a closed ball so admit convergent subsequences. But by uniqueness of \"u\" and \"v\" any such subsequences must tend to \"u\" and \"v\", so that \"u\" and \"v\"\nmust tend to \"u\" and \"v\", establishing continuity.\n\nThe above result holds more generally in a Hadamard space.\n\n\nFrom the previous properties of \"h\", for each \"y\" there is a unique geodesic γ(\"t\") parametrised by arclength with γ(0) = \"y\" such that . It has the property that it cuts ∂\"B\"(\"y\",\"r\") at \"t\" = ±\"r\": in the previous notation γ(\"r\") = \"u\" and γ(–\"r\") = \"v\". The vector field \"V\" defined by the unit vector formula_44 at \"y\" is continuous, because \"u\" is a continuous function of \"y\" and the map sending (\"x\",\"v\") to (\"x\",exp \"v\") is a diffeomorphism from \"TX\" onto \"X\" × \"X\" by the Cartan-Hadamard theorem. Let δ(\"s\") be another geodesic parametrised by arclength through \"y\" with δ(0) = \"y\". Then \"dh\" ∘ δ (0)/ \"ds\" = formula_45. Indeed, let \"H\"(\"x\") = \"h\"(\"x\") − \"h\"(\"y\"), so that \"H\"(\"y\") = 0. Then\n\nApplying this with \"x\" = \"u\" and \"v\", it follows that for \"s\" > 0\n\nThe outer terms tend to formula_45 as \"s\" tends to 0, so the middle term has the same limit, as claimed. A similar argument applies for \"s\" < 0.\n\nThe assertion on the outer terms follows from the first variation formula for arclength, but can be deduced directly as follows. Let \nformula_49 and formula_50, both unit vectors. Then for tangent vectors \"p\" and \"q\" at \"y\" in the unit ball\n\nwith ε uniformly bounded. Let \"s\" = \"t\" and \"r\" = \"t\". Then\n\nThe right hand side here tends to (\"a\",\"b\") as \"t\" tends to 0 since\n\nThe same method works for the other terms.\n\nHence it follows that \"h\" is a C function with \"dh\" dual to the vector field \"V\", so that ||\"dh\"(\"y\")|| = 1. The vector field \"V\" is thus the gradient vector field for \"h\". The geodesics through any point are the flow lines for the flow α for \"V\", so that α is the gradient flow for \"h\".\n\nTHEOREM. On a Hadamard manifold X the following conditions on a continuous function h are equivalent:\n\nIt has already been proved that (1) implies (2).\n\nThe arguments above show \"mutatis mutandi\" that (2) implies (3).\n\nIt therefore remains to show that (3) implies (1). Fix \"x\" in \"X\". Let α be the gradient flow for \"h\". It follows that and that is a geodesic through \"x\" parametrised by arclength with . Indeed, if \"s\" < \"t\", then\n\nso that \"d\"(γ(\"s\"),γ(\"t\")) = |\"s\" − \"t\"|. Let \"g\"(\"y\") = \"h\"(\"y\"), the Busemann function for γ with base point \"x\". In particular \"g\"(\"x\") = 0. To prove (1), it suffices to show that \"g\" = \"h\" – \"h\"(\"x\")1.\n\nLet \"C\"(−\"r\") be the closed convex set of points \"z\" with \"h\"(\"z\") ≤ −\"r\". Since \"X\" is a Hadamard space for every point \"y\" in \"X\" there is a unique closest point \"P\"(\"y\") to \"y\" in \"C\"(-\"r\"). It depends continuously on \"y\" and if \"y\" lies outside \"C\"(-\"r\"), then \"P\"(\"y\") lies on the hypersurface \"h\"(\"z\") = − \"r\"—the boundary ∂\"C\"(–\"r\") of \"C\"(–\"r\")—and the geodesic from \"y\" to \"P\"(\"y\") is orthogonal to ∂\"C\"(–\"r\"). In this case the geodesic is just α(\"y\"). Indeed, the fact that α is the gradient flow of \"h\" and the conditions ||\"dh\"(\"y\")|| ≡ 1 imply that the flow lines α(\"y\") are geodesics parametrised by arclength and cut the level curves of \"h\" orthogonally. Taking \"y\" with \"h\"(\"y\") = \"h\"(\"x\") and \"t\" > 0,\n\nOn the other hand, for any four points \"a\", \"b\", \"c\", \"d\" in a Hadamard space, the following quadrilateral inequality of Reshetnyak holds:\n\nSetting \"a\" = \"x\", \"b\" = \"y\", \"c\" = α(\"x\"), \"d\" = α(\"y\"), it follows that\n\nso that\n\nHence \"h\"(\"y\") = 0 on the level surface of \"h\" containing \"x\". The flow α can be used to transport this result to all the level surfaces of \"h\". In fact for general \"y\" take \"s\" such that \"h\"(α(\"x\")) = \"h\"(\"y\") and set \"x\" = α(\"x\"). Then \"h\"(\"y\") = 0, where γ(\"t\") = α(\"x\") = γ(\"s\" + \"t\"). But then \"h\" = \"h\" – \"s\", so that \"h\"(\"y\") = \"s\". Hence , as required.\n\nNote that this argument could be shortened using the fact that two Busemann functions \"h\" and \"h\" differ by a constant if and only if the corresponding geodesic rays satisfy sup \"d\"(γ(\"t\"),δ(\"t\")) < ∞. Indeed, all the geodesics defined by the flow α satisfy the latter condition, so differ by constants. Since along any of these geodesics \"h\" is linear with derivative 1, \"h\" must differ from these Busemann functions by constants.\n\n defined a compactification of a Hadamard manifold \"X\" which uses Busemann functions. Their construction, which can be extended more generally to proper (i.e. locally compact) Hadamard spaces, gives an explicit geometric realisation of a compactification defined by Gromov—by adding an \"ideal boundary\"—for the more general class of proper metric spaces \"X\", those for which every closed ball is compact. Note that, since any Cauchy sequence is contained in a closed ball, any proper metric space is automatically complete. The ideal boundary is a special case of the ideal boundary for a metric space. In the case of Hadamard spaces, this agrees with the space of geodesic rays emanating from any fixed point described using Busemann functions in the bordification of the space.\n\nIf \"X\" is a proper metric space, Gromov's compactification can be defined as follows. Fix a point \"x\" in \"X\" and let \"X\" = (\"x\",\"N\"). Let \"Y\" = \"C\"(\"X\") be the space of Lipschitz continuous functions on \"X\", .e. those for which |\"f\"(\"x\") – \"f\"(\"y\")| ≤ \"A\" \"d\"(\"x\",\"y\") for some constant \"A\" > 0. The space \"Y\" can be topologised by the seminorms ||\"f\"|| = sup |\"f\"|, the topology of uniform convergence on compacta. This is the topology induced by the natural map of \"C\"(\"X\") into the direct product of the Banach spaces \"C\"(\"X\"). It is give by the metric \"D\"(\"f\",\"g\") = ∑ 2 ||\"f\" − \"g\"||(1 +||\"f\" − \"g\"||).\n\nThe space \"X\" is embedded into \"Y\" by sending \"x\" to the function \"f\"(\"y\") = \"d\"(\"y\",\"x\") – \"d\"(\"x\",\"x\"). Let be the closure of \"X\" in \"Y\". Then is compact (metrisable) and contains \"X\" as an open subset; moreover compactifications arising from different choices of basepoint are naturally homeomorphic. Compactness follows from the Arzelà–Ascoli theorem since the image in \"C\"(\"X\") is equicontinuous and uniformly bounded in norm by \"N\". Let \"x\" be a sequence in \"X\" ⊂ tending to \"y\" in \\ \"X\". Then all but finitely many terms must lie outside \"X\" since \"X\" is compact, so that any subsequence would converge to a point in \"X\"; so the sequence \"x\" must be unbounded in \"X\". Let \"h\"(\"x\") = (\"d\"(\"x\",\"x\") + 1). Then \"h\" lies in \"C\"(\"X\"). It is non-zero on \"X\" and vanishes only at ∞. Hence it extends to a continuous function on with zero set \\ \"X\". It follows that \\ \"X\" is closed in , as required. To check that the compactification = (\"x\") is independent of the basepoint, it suffices to show that \"k\"(\"x\") = \"d\"(\"x\",\"x\") − \"d\"(\"x\",\"x\") extends to a continuous function on . But \"k\"(\"x\") = \"f\"(\"x\"), so, for \"g\" in , \"k\"(\"g\") = \"g\"(\"x\"). Hence the correspondence between the compactifications for \"x\" and \"x\" is given by sending \"g\" in (\"x\") to \"g\" + \"g\"(\"x\")1 in (\"x\").\n\nWhen \"X\" is a Hadamard manifold (or more generally a proper Hadamard space), Gromov's ideal boundary ∂\"X\" = \\ \"X\" can be realised explicitly as \"asymptotic limits\" of geodesics by using Busemann functions. Fixing a base point \"x\", there is a unique geodesic γ(\"t\") parametrised by arclength such that γ(0) = \"x\" and formula_44 is a given unit vector. If \"B\" is the corresponding Busemann function, then\n\"B\" lies in ∂\"X\"(\"x\") and induces a homeomorphism of the unit (\"n\" − 1)-sphere onto ∂\"X\"(\"x\"), sending formula_44 to \"B\".\n\nIn the case of spaces of negative curvature, such as the Poincaré disk, CAT(-1) and hyperbolic spaces, there is a metric structure on their Gromov boundary. This structure is preserved by the group of quasi-isometries which carry geodesics rays to quasigeodesic rays. Quasigeodesics were first studied for negatively curved surfaces—in particular the hyperbolic upper halfplane and unit disk—by Morse and generalised to negatively curved symmetric spaces by Mostow, for his work on the rigidity of discrete groups. The basic result is the Morse–Mostow lemma on the stability of geodesics.\n\nBy definition a quasigeodesic Γ defined on an interval [\"a\",\"b\"] with −∞ ≤ \"a\" < \"b\" ≤ ∞ is a map Γ(\"t\") into a metric space, not necessarily continuous, for which there are constants λ ≥ 1 and ε > 0 such that for all \"s\" and \"t\":\n\nThe following result is essentially due to Marston Morse (1924).\n\nMorse's lemma on stability of geodesics. In the hyperbolic disk there is a constant \"R\" depending on λ and ε such that any quasigeodesic segment Γ defined on a finite interval [\"a\",\"b\"] is within a Hausdorff distance \"R\" of the geodesic segment [Γ(\"a\"),Γ(\"b\")].\n\nThe classical proof of Morse's lemma for the Poincaré unit disk or upper halfplane proceeds more directly by using orthogonal projection onto the geodesic segment.\n\n\n\n\n\nThe generalisation of Morse's lemma to CAT(-1) spaces is often referred to as the Morse–Mostow lemma and can be proved by a straightforward generalisation of the classical proof. There is also a generalisation for the more general class of hyperbolic metric spaces due to Gromov. Gromov's proof is given below for the Poincaré unit disk; the properties of hyperbolic metric spaces are developed in the course of the proof, so that it applies mutatis mutandi to CAT(-1) or hyperbolic metric spaces.\n\nRecall that in a Hadamard space if [\"a\",\"b\"] and [\"a\",\"b\"] are two geodesic segments and the intermediate points \"c\"(\"t\") and \"c\"(\"t\") divide them in the ratio \"t\":(1 – \"t\"), then \"d\"(\"c\"(\"t\"),\"c\"(\"t\")) is a convex function of \"t\". In particular if Γ(\"t\") and Γ(\"t\") are geodesic segments of unit speed defined on [0,\"R\"] starting at the same point then\n\nIn particular this implies the following:\n\n\nBefore discussing CAT(-1) spaces, this section will describe the Efremovich–Tikhomirova theorem for the unit disk \"D\" with the Poincaré metric. It asserts that quasi-isometries of \"D\" extend to quasi-Möbius homeomorphisms of the unit disk with the Euclidean metric. The theorem forms the prototype for the more general theory of CAT(-1) spaces. Their original theorem was proved in a slightly less general and less precise form in and applied to bi-Lipschitz homeomorphisms of the unit disk for the Poincaré metric; earlier, in the posthumous paper , the Japanese mathematician Akira Mori had proved a related result within Teichmüller theory assuring that every quasiconformal homeomorphism of the disk is Hölder continuous and therefore extends continuously to a homeomorphism of the unit circle (it is known that this extension is quasi-Möbius).\n\nIf \"X\" is the Poincaré unit disk, or more generally a CAT(-1) space, the Morse lemma on stability of quasigeodesics implies that every quasi-isometry of \"X\" extends uniquely to the boundary. By definition two self-mappings \"f\", \"g\" of \"X\" are quasi-equivalent if sup \"d\"(\"f\"(\"x\"),\"g\"(\"x\")) < ∞, so that corresponding points are at a uniformly bounded distance of each other. A quasi-isometry \"f\" of \"X\" is a self-mapping of \"X\", not necessarily continuous, which has a quasi-inverse \"f\" such that \"f\" ∘ \"f\" and \"f\" ∘ \"f\" are quasi-equivalent to he identity and such that there are constants λ ≥ 1 and ε > 0 such that for all \"x\", \"y\" in \"X\" and both mappings\n\nNote that quasi-inverses are unique up to quasi-equivalence; that equivalent definition could be given using possibly different right and left-quasi inverses, but they would necessarily be quasi-equivalent; that quasi-isometries are closed under composition which up to quasi-equivalence depends only the quasi-equivalence classes; and that, modulo quasi-equivalence, the quasi-isometries form a group.\n\nFixing a point \"x\" in \"X\", given a geodesic ray γ starting at \"x\", the image \"f\" ∘ γ under a quasi-isometry \"f\" is a quasi-geodesic ray. By the Morse-Mostow lemma it is within a bounded distance of a unique geodesic ray δ starting at \"x\". This defines a mapping ∂\"f\" on the boundary ∂\"X\" of \"X\", independent of the quasi-equivalence class of \"f\", such that ∂(\"f\" ∘ \"g\") = ∂\"f\" ∘ ∂\"g\". Thus there is a homomorphism of the group of quasi-isometries into the group of self-mappings of ∂\"X\".\n\nTo check that ∂\"f\" is continuous, note that if γ and γ are geodesic rays that are uniformly close on [0,\"R\"], within a distance η, then \"f\" ∘ γ and \"f\" ∘ γ lie within a distance λη + ε on [0,\"R\"], so that δ and δ lie within a distance λη + ε + 2\"h\"(λ,ε); hence on a smaller interval [0,\"r\"], δ and δ lie within a distance (\"r\"/\"R\")⋅[λη + ε + 2\"h\"(λ,ε)] by convexity.\n\nOn CAT(-1) spaces, a finer version of continuity asserts that ∂\"f\" is a quasi-Möbius mapping with respect to a natural class of metric on ∂\"X\", the \"visual metrics\" generalising the Euclidean metric on the unit circle and its transforms under the Möbius group. These visual metrics can be defined in terms of Busemann functions.\n\nIn the case of the unit disk, Teichmüller theory implies that the homomorphism carries quasiconformal homeomorphisms of the disk onto the group of quasi-Möbius homeomorphisms of the circle (using for example the Ahlfors–Beurling or Douady–Earle extension): it follows that the homomorphism from the quasi-isometry group into the quasi-Möbius group is surjective.\n\nIn the other direction, it is straightforward to prove that the homomorphism is injective. In fact suppose that \"f\" is a quasi-isometry of the unit disk such that ∂\"f\" is the identity. The assumption and the Morse lemma implies that if γ(R) is a geodesic line, then \"f\"(γ(R)) lies in an \"h\"-neighbourhood of γ(R). Now take a second geodesic line δ such that δ and γ intersect orthogonally at a given point in \"a\". Then \"f\"(\"a\") lies in the intersection of \"h\"-neighbourhoods of δ and γ. Applying a Möbius transformation, it can be assumed that \"a\" is at the origin of the unit disk and the geodesics are the real and imaginary axes. By convexity, the \"h\"-neighbourhoods of these axes intersect in a 3\"h\"-neighbourhood of the origin: in fact if \"z\" lies in both neighbourhoods, let \"x\" and \"y\" be the orthogonal projections of \"z\" onto the \"x\"- and \"y\"-axes; then so taking projections onto the \"y\"-axis, ; hence . Hence , so that \"f\" is quasi-equivalent to the identity, as claimed.\n\nGiven two distinct points \"z\", \"w\" on the unit circle or real axis there is a unique hyperbolic geodesic [\"z\",\"w\"] joining them. It is given by the circle (or straight line) which cuts the unit circl unit circle or real axis orthogonally at those two points. Given four distinct points \"a\", \"b\", \"c\", \"d\" in the extended complex plane their cross ratio is defined by\n\nIf \"g\" is a complex Möbius transformation then it leaves the cross ratio invariant: . Since the Möbius group acts simply transitively on triples of points, the cross ratio can alternatively be described as the complex number \"z\" in C\\{0,1} such that \"g\"(\"a\") = 0, \"g\"(\"b\") = 1, \"g\"(\"c\") = λ, \"g\"(\"d\") = ∞ for a Möbius transformation \"g\".\n\nSince \"a\", \"b\", \"c\" and \"d\" all appear in the numerator defining the cross ratio, to understand the behaviour of the cross ratio under permutations of \"a\", \"b\", \"c\" and \"d\", it suffices to consider permutations that fix \"d\", so only permute \"a\", \"b\" and \"c\". The cross ratio transforms according to the anharmonic group of order 6 generated by the Möbius transformations sending λ to 1 – λ and λ. The other three transformations send λ to 1 – λ, to λ(λ – 1) and to (1 – λ).\n\nNow let \"a\", \"b\", \"c\", \"d\" be points on the unit circle or real axis in that order. Then the geodesics [\"a\",\"b\"] and [\"c\",\"d\"] do not intersect and the distance between these geodesics is well defined: there is a unique geodesic line cutting these two geodesics orthogonally and the distance is given by the length of the geodesic segment between them. It is evidently invariant under real Möbius transformations. To compare the cross ratio and the distance between geodesics, Möbius invariance allows the calculation to be reduced to a symmetric configuration. In fact for 0 < \"r\" < \"R\", take \"a\" = –\"R\", \"b\" = −\"r\", \"c\" = \"r\", \"d\" = \"R\". Then\n\nsince log[cosh(\"x\")/exp\"x\")] = log (1 + exp(–2\"x\"))/2 is bounded above and below in \"x\" ≥ 0. Note that \"a\", \"b\",\"c\", \"d\" are in order around the unit circle if and only if (\"a\",\"b\";\"c\",\"d\") > 1.\n\nA more general and precise geometric interpretation of the cross ratio can be given using projections of ideal points on to a geodesic line; it does not depend on the order of the points on the circle and therefore whether or not geodesic lines intersect.\n\nSince both sides are invariant under Möbius transformations, it suffices to check this in the case that \"a\" = 0, \"b\" = ∞, \"c\" = \"x\" and \"d\" = 1. In this case the geodesic line is the positive imaginary axis, right hand side equals |log |\"x\"||, \"p\" = |\"x\"|\"i\" and \"q\" = \"i\". So the left hand side equals |log|\"x\"||. Note that \"p\" and \"q\" are also the points where the incircles of the ideal triangles \"abc\" and \"abd\" touch \"ab\".\n\nA homeomorphism \"F\" of the circle is \"quasisymmetric\" if there are constants \"a\", \"b\" > 0 such that\n\ndenotes the cross-ratio.\n\nIt is immediate that quasisymmetric and quasi-Möbius homeomorphisms are closed under the operations of inversion and composition.\n\nIf \"F\" is quasisymmetric then it is also quasi-Möbius, with \"c\" = \"a\" and \"d\" = \"b\": this follows by multiplying the first inequality for (\"z\",\"z\",\"z\") and (\"z\",\"z\",\"z\"). Conversely any quasi-Möbius homeomorphism \"F\" is quasisymmetric. To see this, it can be first be checked that \"F\" (and hence \"F\") is Hölder continuous. Let \"S\" be the set of cube roots of unity, so that if \"a\" ≠ \"b\" in \"S\", then |\"a\" − \"b\"| = 2 sin /3 = . To prove a Hölder estimate, it can be assumed that \"x\" – \"y\" is uniformly small. Then both \"x\" and \"y\" are greater than a fixed distance away from \"a\", \"b\" in \"S\" with \"a\" ≠ \"b\", so the estimate follows by applying the quasi-Möbius inequality to \"x\", \"a\", \"y\", \"b\". To verify that \"F\" is quasisymmetric, it suffices to find a uniform upper bound for |\"F\"(\"x\") − \"F\"(\"y\")| / |\"F\"(\"x\") − \"F\"(\"z\")| in the case of a triple with |\"x\" − \"z\"| = |\"x\" − \"y\"|, uniformly small. In this case there is a point \"w\" at a distance greater than 1 from \"x\", \"y\" and \"z\". Applying the quasi-Möbius inequality to \"x\", \"w\", \"y\" and \"z\" yields the required upper bound. To summarise:\n\n\nTo prove the theorem it suffices to prove that if \"F\" = ∂\"f\" then there are constants \"A\", \"B\" > 0 such that for \"a\", \"b\", \"c\", \"d\" distinct points on the unit circle\n\nIt has already been checked that \"F\" (and is inverse) are continuous. Composing \"f\", and hence \"F\", with complex conjugation if necessary, it can further be assumed that \"F\" preserves the orientation of the circle. In this case, if \"a\",\"b\", \"c\",\"d\" are in order on the circle, so too are there images under \"F\"; hence both (\"a\",\"b\";\"c\",\"d\") and (\"F\"(\"a\"),\"F\"(\"b\");\"F\"(\"c\"),\"F\"(\"d\")) are real and greater than one. In this case\n\nTo prove this, it suffices to show that . From the previous section it suffices show . This follows from the fact that the images under \"f\" of [\"a\",\"b\"] and [\"c\",\"d\"] lie within \"h\"-neighbourhoods of [\"F\"(\"a\"),\"F\"(\"b\")] and [\"F\"(\"c\"),\"F\"(\"d\")]; the minimal distance can be estimated using the quasi-isometry constants for \"f\" applied to the points on [\"a\",\"b\"] and [\"c\",\"d\"]\nrealising \"d\"([\"a\",\"b\"],[\"c\",\"d\"]).\n\nAdjusting \"A\" and \"B\" if necessary, the inequality above applies also to \"F\". Replacing \"a\", \"b\", \"c\" and \"d\" by their images under \"F\", it follows that\n\nif \"a\", \"b\", \"c\" and \"d\" are in order on the unit circle. Hence the same inequalities are valid for the three cyclic of the quadruple \"a\", \"b\", \"c\", \"d\". If \"a\" and \"b\" are switched then the cross ratios are sent to their inverses, so lie between 0 and 1; similarly if \"c\" and \"d\" are switched. If both pairs are switched, the cross ratio remains unaltered. Hence the inequalities are also valid in this case. Finally if \"b\" and \"c\" are interchanged, the croos ratio changes from λ to , which lies between 0 and 1. Hence again the same inequalities are valid. It is easy to check that using these transformations the inequlaties are valid for all possible permutations of \"a\", \"b\", \"c\" and \"d\", so that \"F\" and its inverse are quasi-Möbius homeomorphisms.\n\nBusemann functions can be used to determine special visual metrics on the class of CAT(-1) spaces. These are complete geodesic metric spaces in which the distances between points on the boundary of a geodesic triangle are less than or equal to the comparison triangle in the hyperbolic upper half plane or equivalently the unit disk with the Poincaré metric. In the case of the unit disk the chordal metric can be recovered directly using Busemann functions \"B\" and the special theory for the disk generalises completely to any proper CAT(-1) space \"X\". The hyperbolic upper half plane is a CAT(0) space, as lengths in a hyperbolic geodesic triangle are less than lengths in the Euclidean comparison triangle: in particular a CAT(-1) space is a CAT(0) space, so the theory of Busemann functions and the Gromov boundary applies. From the theory of the hyperbolic disk, it follows in particular that every geodesic ray in a CAT(-1) space extends to a geodesic line and given two points of the boundary there is a unique geodesic γ such that has these points as the limits γ(±∞). The theory applies equally well to any CAT(−κ) space with κ > 0 since these arise by scaling the metric on a CAT(-1) space by κ. On the hyperbolic unit disk \"D\" quasi-isometries of \"D\" induce quasi-Möbius homeomorphisms of the boundary in a functorial way. There is a more general theory of Gromov hyperbolic spaces, a similar statement holds, but with less precise control on the homeomorphisms of the boundary.\n\nMore recently Busemann functions have been used by probabilists to study asymptotic properties in models of first-passage percolation and directed last-passage percolation.\n\n"}
{"id": "1672489", "url": "https://en.wikipedia.org/wiki?curid=1672489", "title": "Cyclic number", "text": "Cyclic number\n\nA cyclic number is an integer in which cyclic permutations of the digits are successive multiples of the number. The most widely known is the six-digit number 142857, whose first six integer multiples are\n\nTo qualify as a cyclic number, it is required that consecutive multiples be cyclic permutations. Thus, the number 076923 would not be considered a cyclic number, because even though all cyclic permutations are multiples, they are not consecutive integer multiples:\n\nThe following trivial cases are typically excluded:\n\nIf leading zeros are not permitted on numerals, then 142857 is the only cyclic number in decimal, due to the necessary structure given in the next section. Allowing leading zeros, the sequence of cyclic numbers begins:\n\nCyclic numbers are related to the recurring digital representations of unit fractions. A cyclic number of length \"L\" is the digital representation of\n\nConversely, if the digital period of 1 /\"p\" (where \"p\" is prime) is\n\nthen the digits represent a cyclic number.\n\nFor example:\n\nMultiples of these fractions exhibit cyclic permutation:\n\nFrom the relation to unit fractions, it can be shown that cyclic numbers are of the form of the Fermat quotient\n\nwhere \"b\" is the number base (10 for decimal), and \"p\" is a prime that does not divide \"b\". (Primes \"p\" that give cyclic numbers in base \"b\" are called full reptend primes or long primes in base \"b\").\n\nFor example, the case \"b\" = 10, \"p\" = 7 gives the cyclic number 142857, and the case \"b\" = 12, \"p\" = 5 gives the cyclic number 2497.\n\nNot all values of \"p\" will yield a cyclic number using this formula; for example, the case \"b\" = 10, \"p\" = 13 gives 076923076923, and the case \"b\" = 12, \"p\" = 19 gives 076B45076B45076B45. These failed cases will always contain a repetition of digits (possibly several).\n\nThe first values of \"p\" for which this formula produces cyclic numbers in decimal (\"b\" = 10) are \n\nFor \"b\" = 12 (duodecimal), these \"p\"s are \n\nFor \"b\" = 2 (binary), these \"p\"s are \n\nFor \"b\" = 3 (ternary), these \"p\"s are \n\nThere are no such \"p\"s in the hexadecimal system.\n\nThe known pattern to this sequence comes from algebraic number theory, specifically, this sequence is the set of primes \"p\" such that \"b\" is a primitive root modulo \"p\". A conjecture of Emil Artin is that this sequence contains 37.395..% of the primes (for \"b\" in ).\n\nCyclic numbers can be constructed by the following procedure:\n\nLet \"b\" be the number base (10 for decimal)<br>\nLet \"p\" be a prime that does not divide \"b\".<br>\nLet \"t\" = 0.<br>\nLet \"r\" = 1.<br>\nLet \"n\" = 0.<br>\nloop:\nif \"t\" = \"p\" − 1 then \"n\" is a cyclic number.\n\nThis procedure works by computing the digits of 1 /\"p\" in base \"b\", by long division. \"r\" is the remainder at each step, and \"d\" is the digit produced.\n\nThe step\n\nserves simply to collect the digits. For computers not capable of expressing very large integers, the digits may be output or collected in another way.\n\nNote that if \"t\" ever exceeds \"p\"/2, then the number must be cyclic, without the need to compute the remaining digits.\n\n\nUsing the above technique, cyclic numbers can be found in other numeric bases. (Note that not all of these follow the second rule (all successive multiples being cyclic permutations) listed in the Special Cases section above) In each of these cases the digits across half the period add up to the base minus one. Thus for binary the sum of the bits across half the period is 1; for ternary it is 2, and so on.\n\nIn binary, the sequence of cyclic numbers begins: \n\nIn ternary: \n\nIn quaternary:\n\nIn quinary: \n\nIn senary: \n\nIn base 7: \n\nIn octal: \n\nIn nonary:\n\nIn base 11: \n\nIn duodecimal: \n\nIn base 13: \n\nIn base 14: \n\nIn base 15: \n\nIn hexadecimal:\n\nIn base 17: \n\nIn base 18: \nIn base 19: \n\nIn base 20: \n\nIn base 21: \n\nIn base 22: \n\nIn base 23: \n\nIn base 24: \n\nIn base 25:\n\nNote that in ternary (\"b\" = 3), the case \"p\" = 2 yields 1 as a cyclic number. While single digits may be considered trivial cases, it may be useful for completeness of the theory to consider them only when they are generated in this way.\n\nIt can be shown that no cyclic numbers (other than trivial single digits, i.e. \"p\" = 2) exist in any numeric base which is a perfect square, that is, base 4, 9, 16, 25, etc.\n\n\n\n"}
{"id": "30667752", "url": "https://en.wikipedia.org/wiki?curid=30667752", "title": "Differential Equations (journal)", "text": "Differential Equations (journal)\n\nThe journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\nIts 2009 MCQ was 0.12, and its 2009 impact factor was 0.339.\n\n"}
{"id": "211566", "url": "https://en.wikipedia.org/wiki?curid=211566", "title": "Direct limit", "text": "Direct limit\n\nIn mathematics, a direct limit is a way to construct a (typically large) object from many (typically smaller) objects that are put together in a specific way. These objects may be groups, rings, vector spaces or in general objects from any category. The way they are put together is specified by a system of homomorphisms (group homomorphism, ring homomorphism, or in general morphisms in the category) between those smaller objects. The direct limit of the objects formula_1, where formula_2 ranges over some directed set formula_3, is denoted by formula_4. (This is a slight abuse of notation as it suppresses the system of homomorphisms that is crucial for the structure of the limit.) \n\nDirect limits are a special case of the concept of colimit in category theory. Direct limits are dual to inverse limits which are a special case of limits in category theory.\n\nWe will first give the definition for algebraic structures like groups and modules, and then the general definition, which can be used in any category.\n\nIn this section objects are understood to consist of underlying sets with a given algebraic structure, such as groups, rings, modules (over a fixed ring), algebras (over a fixed field), etc. With this in mind, \"homomorphisms\" are understood in the corresponding setting (group homomorphisms, etc.).\n\nLet formula_5 be a directed set. Let formula_6 be a family of objects indexed by formula_7 and formula_8 be a homomorphism for all formula_9 with the following properties:\nThen the pair formula_14 is called a direct system over formula_3.\n\nThe direct limit of the direct system formula_14 is denoted by formula_17 and is defined as follows. Its underlying set is the disjoint union of the formula_11's modulo a certain :\n\nHere, if formula_20 and formula_21, then formula_22 iff there is some formula_23 with formula_24 and formula_25 and such that formula_26.\nHeuristically, two elements in the disjoint union are equivalent if and only if they \"eventually become equal\" in the direct system. An equivalent formulation that highlights the duality to the inverse limit is that an element is equivalent to all its images under the maps of the direct system, i.e. formula_27 whenever formula_9.\n\nOne naturally obtains from this definition \"canonical functions\" formula_29 sending each element to its equivalence class. The algebraic operations on formula_30 are defined such that these maps become homomorphisms. Formally, the direct limit of the direct system formula_14 consists of the object formula_17 together with the canonical homomorphisms formula_29.\n\nThe direct limit can be defined in an arbitrary category formula_34 by means of a universal property. Let formula_35 be a direct system of objects and morphisms in formula_34 (as defined above). A \"target\" is a pair formula_37 where formula_38 is an object in formula_34 and formula_40 are morphisms for each formula_41 such that formula_42 whenever formula_9. A direct limit of the direct system formula_35 is a \"universally repelling target\" formula_37 in the sense that formula_37 is a target and for each target formula_47, there is a unique morphism formula_48 such that formula_49 for each \"i\". The following diagram\n\nwill then commute for all \"i\", \"j\". \n\nThe direct limit is often denoted\nwith the direct system formula_35 and the canonical morphisms formula_52 being understood.\n\nUnlike for algebraic objects, not every direct system in an arbitrary category has a direct limit. If it does, however, the direct limit is unique in a strong sense: given another direct limit \"X\"′ there exists a \"unique\" isomorphism \"X\"′ → \"X\" that commutes with the canonical morphisms.\n\n\nDirect limits are linked to inverse limits via\n\nAn important property is that taking direct limits in the category of modules is an exact functor. This means that if you start with a directed system of short exact sequences formula_56 and form direct limits, you obtain a short exact sequence formula_57.\n\nWe note that a direct system in a category formula_34 admits an alternative description in terms of functors. Any directed set formula_59 can be considered as a small category formula_60 whose objects are the elements formula_3 and there is a morphisms formula_62 if and only if formula_63. A direct system over formula_3 is then the same as a covariant functor formula_65. The colimit of this functor is the same as the direct limit of the original direct system.\n\nA notion closely related to direct limits are the filtered colimits. Here we start with a covariant functor formula_66 from a filtered category formula_67 to some category formula_34 and form the colimit of this functor. One can show that a category has all directed limits if and only if it has all filtered colimits, and a functor defined on such a category commutes with all direct limits if and only if it commutes with all filtered colimits.\n\nGiven an arbitrary category formula_34, there may be direct systems in formula_34 which don't have a direct limit in formula_34 (consider for example the category of finite sets, or the category of finitely generated abelian groups). In this case, we can always embed formula_34 into a category formula_73 in which all direct limits exist; the objects of formula_73 are called ind-objects of formula_34. \n\nThe categorical dual of the direct limit is called the inverse limit. As above, inverse limits can be viewed as limits of certain functors and are closely related to limits over cofiltered categories.\n\nIn the literature, one finds the terms \"directed limit\", \"direct inductive limit\", \"directed colimit\", \"direct colimit\" and \"inductive limit\" for the concept of direct limit defined above. The term \"inductive limit\" is ambiguous however, as some authors use it for the general concept of colimit.\n\n\n"}
{"id": "937347", "url": "https://en.wikipedia.org/wiki?curid=937347", "title": "Double bind", "text": "Double bind\n\nA double bind is an emotionally distressing dilemma in communication in which an individual (or group) receives two or more conflicting messages, with one negating the other. This creates a situation in which a successful response to one message results in a failed response to the other (and vice versa), so that the person will automatically be wrong regardless of response. The double bind occurs when the person cannot confront the inherent dilemma, and therefore can neither resolve it nor opt out of the situation.\n\nDouble bind theory was first described by Gregory Bateson and his colleagues in the 1950s.\n\nDouble binds are often utilized as a form of control without open coercion—the use of confusion makes them both difficult to respond to as well as to resist.: 271-278.\n\nA double bind generally includes different levels of abstraction in the order of messages and these messages can either be stated explicitly or implicitly within the context of the situation, or they can be conveyed by tone of voice or body language. Further complications arise when frequent double binds are part of an ongoing relationship to which the person or group is committed.\n\nThe double bind is often misunderstood to be a simple contradictory situation, where the subject is trapped by two conflicting demands. While it's true that the core of the double bind is two conflicting demands, the difference lies in how they are imposed upon the subject, what the subject's understanding of the situation is, and who (or what) imposes these demands upon the subject. Unlike the usual no-win situation, the subject has difficulty in defining the exact nature of the paradoxical situation in which he or she is caught. The contradiction may be unexpressed in its immediate context and therefore invisible to external observers, only becoming evident when a prior communication is considered. Typically, a demand is imposed upon the subject by someone whom he or she respects (such as a parent, teacher, or doctor) but the demand itself is inherently impossible to fulfill because some broader context forbids it. For example, this situation arises when a person in a position of authority imposes two contradictory conditions but there exists an unspoken rule that one must never question authority.\n\nGregory Bateson and his colleagues defined the double bind as follows (paraphrased):\nThus, the essence of a double bind is two conflicting demands, \"each on a different logical level\", neither of which can be ignored or escaped. This leaves the subject torn both ways, so that whichever demand they try to meet, the other demand cannot be met. \"I must do it, but I can't do it\" is a typical description of the double-bind experience.\n\nFor a double bind to be effective, the subject must be unable to confront or resolve the conflict between the demand placed by the primary injunction and that of the secondary injunction. In this sense, the double bind differentiates itself from a simple contradiction to a more inexpressible internal conflict, where the subject really \"wants\" to meet the demands of the primary injunction, but fails each time through an inability to address the situation's incompatibility with the demands of the secondary injunction. Thus, subjects may express feelings of extreme anxiety in such a situation, as they attempt to fulfil the demands of the primary injunction albeit with obvious contradictions in their actions.\n\nThis was a problem in United States legal circles prior to the Fifth Amendment to the United States Constitution being applied to state action. A person could be subpoenaed to testify in a federal case and given Fifth Amendment immunity for testimony in that case. However, since the immunity did not apply to a state prosecution, the person could refuse to testify at the Federal level despite being given immunity, thus subjecting the person to imprisonment for contempt of court, or the person could testify, and the information he or she was forced to give in the Federal proceeding could then be used to convict the person in a state proceeding.\n\nThe term \"double bind\" was first used by the anthropologist Gregory Bateson and his colleagues (including Don D. Jackson, Jay Haley and John H. Weakland) in the mid-1950s in their discussions on complexity of communication in relation to schizophrenia. Bateson made clear that such complexities are common in normal circumstances, especially in \"play, humour, poetry, ritual and fiction\" (see Logical Types below). Their findings indicated that the tangles in communication often diagnosed as schizophrenia are not necessarily the result of an organic brain dysfunction. Instead, they found that destructive double binds were a frequent pattern of communication among families of patients, and they proposed that growing up amidst perpetual double binds could lead to learned patterns of confusion in thinking and communication.\n\nHuman communication is complex, and context is an essential part of it. Communication consists of the words said, tone of voice, and body language. It also includes how these relate to what has been said in the past; what is not said, but is implied; how these are modified by other nonverbal cues, such as the environment in which it is said, and so forth. For example, if someone says \"I love you\", one takes into account who is saying it, their tone of voice and body language, and the context in which it is said. It may be a declaration of passion or a serene reaffirmation, insincere and/or manipulative, an implied demand for a response, a joke, its public or private context may affect its meaning, and so forth.\n\nConflicts in communication are common and often we ask \"What do you mean?\" or seek clarification in other ways. This is called \"meta-communication:\" communication about the communication. Sometimes, asking for clarification is impossible. Communication difficulties in ordinary life often occur when meta-communication and feedback systems are lacking or inadequate or there isn't enough time for clarification.\nDouble binds can be extremely stressful and become destructive when one is trapped in a dilemma and punished for finding a way out. But making the effort to find the way out of the trap can lead to emotional growth.\n\nThe classic example given of a negative double bind is of a mother telling her child that she loves him or her, while at the same time turning away in disgust, or inflicting corporal punishment as discipline: the words are socially acceptable; the body language is in conflict with it. The child does not know how to respond to the conflict between the words and the body language and, because the child is dependent on the mother for basic needs, they are in a quandary. Small children have difficulty articulating contradictions verbally and can neither ignore them nor leave the relationship.\n\nAnother example is when one is commanded to \"be spontaneous\". The very command contradicts spontaneity, but it only becomes a double bind when one can neither ignore the command nor comment on the contradiction. \nOften, the contradiction in communication is not apparent to bystanders unfamiliar with previous communications.\n\n\n\nBateson also described positive double binds, both in relation to Zen Buddhism with its path of spiritual growth, and the use of therapeutic double binds by psychiatrists to confront their patients with the contradictions in their life in such a way that would help them heal. One of Bateson's consultants, Milton H. Erickson (5 volumes, edited by Rossi) eloquently demonstrated the productive possibilities of double binds through his own life, showing the technique in a brighter light.\n\nOne of the causes of double binds is the loss of feedback systems. Gregory Bateson and Lawrence S. Bale describe double binds that have arisen in science that have caused decades-long delays of progress in science because the scientific community had defined something as outside of its scope (or as \"not science\")—see Bateson in his \"Introduction\" to Steps to an Ecology of Mind (1972, 2000), pp. xv–xxvi; and Bale in his article, \"Gregory Bateson, Cybernetics and the Social/Behavioral Sciences\" (esp. pp. 1–8) on the paradigm of classical science vs. that of systems theory/cybernetics. (See also Bateson's description in his \"Forward\" of how the double bind hypothesis fell into place).\n\nThe Double Bind Theory was first articulated in relationship to schizophrenia, but Bateson and his colleagues hypothesized that schizophrenic thinking was not necessarily an inborn mental disorder but a learned confusion in thinking.\nIt is helpful to remember the context in which these ideas were developed. Bateson and his colleagues were working in the Veteran's Administration Hospital (1949–1962) with World War II veterans. As soldiers they'd been able to function well in combat, but the effects of life-threatening stress had affected them. At that time, 18 years before Post-Traumatic Stress Disorder was officially recognized, the veterans had been saddled with the catch-all diagnosis of schizophrenia. Bateson didn't challenge the diagnosis but he did maintain that the seeming nonsense the patients said at times did make sense within context, and he gives numerous examples in section III of \"Steps to an Ecology of Mind\", \"Pathology in Relationship\". For example, a patient misses an appointment, and when Bateson finds him later the patient says 'the judge disapproves'; Bateson responds, \"You need a defense lawyer\" see following (pp. 195–6) Bateson also surmised that people habitually caught in double binds in childhood would have greater problems—that in the case of the schizophrenic, the double bind is presented continually and habitually within the family context from infancy on. By the time the child is old enough to have identified the double bind situation, it has already been internalized, and the child is unable to confront it. The solution then is to create an escape from the conflicting logical demands of the double bind, in the world of the delusional system (see in \"Towards a Theory of Schizophrenia – Illustrations from Clinical Data\").\n\nOne solution to a double bind is to place the problem in a larger context, a state Bateson identified as Learning III, a step up from Learning II (which requires only learned responses to reward/consequence situations). In Learning III, the double bind is contextualized and understood as an impossible no-win scenario so that ways around it can be found.\n\nBateson's double bind theory was never followed up by research into whether family systems imposing systematic double binds might be a cause of schizophrenia. This complex theory has been only partly tested, and there are gaps in the current psychological and experimental evidence required to establish causation [citation?]. The current understanding of schizophrenia emphasizes the robust scientific evidence for a genetic predisposition to the disorder, with psychosocial stressors, including dysfunctional family interaction patterns, as secondary causative factors in some instances.\n\nAfter many years of research into schizophrenia, Bateson continued to explore problems of communication and learning, first with dolphins, and then with the more abstract processes of evolution. Bateson emphasised that any communicative system characterized by different logical levels might be subject to double bind problems. Especially including the communication of characteristics from one generation to another (genetics and evolution).\n\n\"...evolution always followed the pathways of viability. As Lewis Carroll has pointed out, the theory [of natural selection] explains quite satisfactorily why there are no bread-and-butter-flies today.\"\n\nBateson used the fictional Bread and Butter Fly (from \"Through the Looking Glass, and What Alice Found There\") to illustrate the double bind in terms of natural selection. The gnat points out that the insect would be doomed if he found his food (which would dissolve his own head), and starve if he did not. Alice suggests that this must happen quite often, to which the gnat replies \"it always happens\".\n\nThe pressures that drive evolution therefore represent a genuine double bind. And there is truly no escape: \"It always happens.\" No species can escape natural selection, including our own.\n\nBateson suggested that all evolution is driven by the double bind, whenever circumstances change: If any environment becomes toxic to any species, that species will die out unless it transforms into another species, in which case, the species becomes extinct anyway.\n\nMost significant here is Bateson's exploration of what he later came to call 'the pattern that connects'—that problems of communication which span more than one level (e.g., the relationship between the individual and the family) should also be expected to be found spanning other pairs of levels in the hierarchy (e.g. the relationship between the genotype and the phenotype):\n\n\"We are very far, then, from being able to pose specific questions for the geneticist; but I believe that the wider implications of what I have been saying modify somewhat the philosophy of genetics. Our approach to the problems of schizophrenia by way of a theory of levels or logical types has disclosed first that the problems of adaptation and learning and their pathologies must be considered in terms of a hierarchic system in which stochastic change occurs at the boundary points between the segments of the hierarchy. We have considered three such regions of stochastic change—the level of genetic mutation, the level of learning, and the level of change in family organization. We have disclosed the possibility of a relationship of these levels which orthodox genetics would deny, and we have disclosed that at least in human societies the evolutionary system consists not merely in the selective survival of those persons who happen to select appropriate environments but also in the modification of family environment in a direction which might enhance the phenotypic and genotypic characteristics of the individual members.\"\nRené Girard, in his literary theory of mimetic desire, proposes what he calls a \"model-obstacle\", a role model who demonstrates an object of desire and yet, in possessing that object, becomes a rival who obstructs fulfillment of the desire. According to Girard, the \"internal mediation\" of this mimetic dynamic \"operates along the same lines as what Gregory Bateson called the ‘double bind’.\" Girard found in Sigmund Freud's psychoanalytic theory, a precursor to mimetic desire. \"The individual who 'adjusts' has managed to relegate the two contradictory injunctions of the double bind—to imitate and not to imitate—to two different domains of application. This is, he divides reality in such a way as to neutralize the \"double bind\".\" While critical of Freud's doctrine of the unconscious mind, Girard sees the ancient Greek tragedy, \"Oedipus Rex\", and key elements of Freud's Oedipus complex, patricidal and incestuous desire, to serve as prototypes for his own analysis of the mimetic double bind.\n\nThe field of neuro-linguistic programming also makes use of the expression \"double bind\". Grinder and Bandler (both of whom had personal contact with Bateson and Erickson) asserted that a message could be constructed with multiple messages, whereby the recipient of the message is given the impression of choice—although both options have the same outcome at a higher level of intention. This is called a \"double bind\" in NLP terminology, and has applications in both sales and therapy. In therapy, the practitioner may seek to challenge destructive double binds that limit the client in some way and may also construct double binds in which both options have therapeutic consequences. In a sales context, the speaker may give the respondent the illusion of choice between two possibilities. For example, a salesperson might ask: \"Would you like to pay cash or by credit card?\", with both outcomes presupposing that the person will make the purchase; whereas the third option (that of not buying) is intentionally excluded from the spoken choices.\n\nNote that in the NLP context, the use of the phrase \"double bind\" does not carry the primary definition of two conflicting messages; it is about creating a false sense of choice which ultimately binds to the intended outcome. In the \"cash or credit card?\" example, this is not a \"Bateson double bind\" since there is no contradiction, although it still is an \"NLP double bind\". Similarly if a salesman were selling a book about the evils of commerce, it could perhaps be a \"Bateson double bind\" if the buyer happened to believe that commerce was evil, yet felt compelled or obliged to buy the book.\n\n\n"}
{"id": "609737", "url": "https://en.wikipedia.org/wiki?curid=609737", "title": "Duality (mathematics)", "text": "Duality (mathematics)\n\nIn mathematics, a duality, generally speaking, translates concepts, theorems or mathematical structures into other concepts, theorems or structures, in a one-to-one fashion, often (but not always) by means of an involution operation: if the dual of \"A\" is \"B\", then the dual of \"B\" is \"A\". Such involutions sometimes have fixed points, so that the dual of \"A\" is \"A\" itself. For example, Desargues' theorem is self-dual in this sense under the \"standard duality in projective geometry\".\n\nIn mathematical contexts, \"duality\" has numerous meanings although it is \"a very pervasive and important concept in (modern) mathematics\" and \"an important general theme that has manifestations in almost every area of mathematics\".\n\nMany mathematical dualities between objects of two types correspond to pairings, bilinear functions from an object of one type and another object of the second type to some family of scalars. For instance, \"linear algebra duality\" corresponds in this way to bilinear maps from pairs of vector spaces to scalars, the \"duality between distributions and the associated test functions\" corresponds to the pairing in which one integrates a distribution against a test function, and \"Poincaré duality\" corresponds similarly to intersection number, viewed as a pairing between submanifolds of a given manifold.\n\nFrom a category theory viewpoint, duality can also be seen as a functor, at least in the realm of vector spaces. This functor assigns to each space its dual space, and the pullback construction assigns to each arrow its dual .\n\nIn the words of Michael Atiyah,\nThe following list of examples shows the common features of many dualities, but also indicates that the precise meaning of duality may vary from case to case.\n\nA simple, maybe the most simple, duality arises from considering subsets of a fixed set . To any subset , the complement consists of all those elements in which are not contained in . It is again a subset of . Taking the complement has the following properties:\n\nThis duality appears in topology as a duality between open and closed subsets of some fixed topological space : a subset of is closed if and only if its complement in is open. Because of this, many theorems about closed sets are dual to theorems about open sets. For example, any union of open sets is open, so dually, any intersection of closed sets is closed. The interior of a set is the largest open set contained in it, and the closure of the set is the smallest closed set that contains it. Because of the duality, the complement of the interior of any set is equal to the closure of the complement of .\n\nA duality in geometry is provided by the dual cone construction. Given a set formula_1 of points in the plane formula_2 (or more generally points in the dual cone is defined as the set formula_3 consisting of those points formula_4 satisfying\n\nfor all points formula_5 in formula_1, as illustrated in the diagram.\nUnlike for the complement of sets mentioned above, it is not in general true that applying the dual cone construction twice gives back the original set formula_1. Instead, formula_8 is the smallest cone containing formula_1 which may be bigger than formula_1. Therefore this duality is weaker than the one above, in that\nThe other two properties carry over without change:\n\nA very important example of a duality arises in linear algebra by associating to any vector space its dual vector space . Its elements are the -linear maps formula_23, where is the field over which is defined.\nThe three properties of the dual cone carry over to this type of duality by replacing subsets of formula_2 by vector space and inclusions of such subsets by linear maps. That is:\nA particular feature of this duality is that and are isomorphic for certain objects, namely finite-dimensional vector spaces. However, this is in a sense a lucky coincidence, for giving such an isomorphism requires a certain choice, for example the choice of a basis of . This is also true in the case if is a Hilbert space, \"via\" the Riesz representation theorem.\n\nIn all the dualities discussed before, the dual of an object is of the same kind as the object itself. For example, the dual of a vector space is again a vector space. Many duality statements are not of this kind. Instead, such dualities reveal a close relation between objects of seemingly different nature. One example of such a more general duality is from Galois theory. For a fixed Galois extension , one may associate the Galois group to any intermediate field (i.e., ). This group is a subgroup of the Galois group . Conversely, to any such subgroup there is the fixed field consisting of elements fixed by the elements in .\n\nCompared to the above, this duality has the following features:\n\nGiven a poset (short for partially ordered set; i.e., a set that has a notion of ordering but in which two elements cannot necessarily be placed in order relative to each other), the dual poset comprises the same ground set but the converse relation. Familiar examples of dual partial orders include\n\n\nA \"duality transform\" is an involutive antiautomorphism of a partially ordered set , that is, an order-reversing involution . In several important cases these simple properties determine the transform uniquely up to some simple symmetries. For example, if , are two duality transforms then their composition is an order automorphism of ; thus, any two duality transforms differ only by an order automorphism. For example, all order automorphisms of a power set are induced by permutations of .\n\nA concept defined for a partial order will correspond to a \"dual concept\" on the dual poset . For instance, a minimal element of will be a maximal element of : minimality and maximality are dual concepts in order theory. Other pairs of dual concepts are upper and lower bounds, lower sets and upper sets, and ideals and filters.\n\nIn topology, open sets and closed sets are dual concepts: the complement of an open set is closed, and vice versa. In matroid theory, the family of sets complementary to the independent sets of a given matroid themselves form another matroid, called the dual matroid.\n\nThere are many distinct but interrelated dualities in which geometric or topological objects correspond to other objects of the same type, but with a reversal of the dimensions of the features of the objects. A classical example of this is the duality of the platonic solids, in which the cube and the octahedron form a dual pair, the dodecahedron and the icosahedron form a dual pair, and the tetrahedron is self-dual. The dual polyhedron of any of these polyhedra may be formed as the convex hull of the center points of each face of the primal polyhedron, so the vertices of the dual correspond one-for-one with the faces of the primal. Similarly, each edge of the dual corresponds to an edge of the primal, and each face of the dual corresponds to a vertex of the primal. These correspondences are incidence-preserving: if two parts of the primal polyhedron touch each other, so do the corresponding two parts of the dual polyhedron. More generally, using the concept of polar reciprocation, any convex polyhedron, or more generally any convex polytope, corresponds to a dual polyhedron or dual polytope, with an -dimensional feature of an -dimensional polytope corresponding to an -dimensional feature of the dual polytope. The incidence-preserving nature of the duality is reflected in the fact that the face lattices of the primal and dual polyhedra or polytopes are themselves order-theoretic duals. Duality of polytopes and order-theoretic duality are both involutions: the dual polytope of the dual polytope of any polytope is the original polytope, and reversing all order-relations twice returns to the original order. Choosing a different center of polarity leads to geometrically different dual polytopes, but all have the same combinatorial structure.\nFrom any three-dimensional polyhedron, one can form a planar graph, the graph of its vertices and edges. The dual polyhedron has a dual graph, a graph with one vertex for each face of the polyhedron and with one edge for every two adjacent faces. The same concept of planar graph duality may be generalized to graphs that are drawn in the plane but that do not come from a three-dimensional polyhedron, or more generally to graph embeddings on surfaces of higher genus: one may draw a dual graph by placing one vertex within each region bounded by a cycle of edges in the embedding, and drawing an edge connecting any two regions that share a boundary edge. An important example of this type comes from computational geometry: the duality for any finite set of points in the plane between the Delaunay triangulation of and the Voronoi diagram of . As with dual polyhedra and dual polytopes, the duality of graphs on surfaces is a dimension-reversing involution: each vertex in the primal embedded graph corresponds to a region of the dual embedding, each edge in the primal is crossed by an edge in the dual, and each region of the primal corresponds to a vertex of the dual. The dual graph depends on how the primal graph is embedded: different planar embeddings of a single graph may lead to different dual graphs. Matroid duality is an algebraic extension of planar graph duality, in the sense that the dual matroid of the graphic matroid of a planar graph is isomorphic to the graphic matroid of the dual graph.\n\nA kind of geometric duality also occurs in optimization theory, but not one that reverses dimensions. A linear program may be specified by a system of real variables (the coordinates for a point in Euclidean space a system of linear constraints (specifying that the point lie in a halfspace; the intersection of these halfspaces is a convex polytope, the feasible region of the program), and a linear function (what to optimize). Every linear program has a dual problem with the same optimal solution, but the variables in the dual problem correspond to constraints in the primal problem and vice versa.\nIn logic, functions or relations and are considered dual if , where ¬ is logical negation. The basic duality of this type is the duality of the ∃ and ∀ quantifiers in classical logic. These are dual because and are equivalent for all predicates in classical logic: if there exists an for which fails to hold, then it is false that holds for all (but the converse does not hold constructively). From this fundamental logical duality follow several others:\n\n\nOther analogous dualities follow from these:\n\n\nA group of dualities can be described by endowing, for any mathematical object , the set of morphisms into some fixed object , with a structure similar to that of . This is sometimes called internal Hom. In general, this yields a true duality only for specific choices of , in which case is referred to as the \"dual\" of . There is always a map from to the \"bidual\", that is to say, the dual of the dual,\n\nIt assigns to some the map that associates to any map (i.e., an element in ) the value .\nDepending on the concrete duality considered and also depending on the object , this map may or may not be an isomorphism.\n\nThe construction of the dual vector space\n\nmentioned in the introduction is an example of such a duality. Indeed, the set of morphisms, i.e., linear maps, forms a vector space in its own right. The map mentioned above is always injective. It is surjective, and therefore an isomorphism, if and only if the dimension of is finite. This fact characterizes finite-dimensional vector spaces without referring to a basis.\n\nA vector space is isomorphic to precisely if is finite-dimensional. In this case, such an isomorphism is equivalent to a non-degenerate bilinear form\n\nIn this case is called an inner product space.\nFor example, if is the field of real or complex numbers, any positive definite bilinear form gives rise to such an isomorphism. In Riemannian geometry, is taken to be the tangent space of a manifold and such positive bilinear forms are called Riemannian metrics. Their purpose is to measure angles and distances. Thus, duality is a foundational basis of this branch of geometry. Another application of inner product spaces is the Hodge star which provides a correspondence between the elements of the exterior algebra. For an -dimensional vector space, the Hodge star operator maps -forms to -forms. This can be used to formulate Maxwell's equations. In this guise, the duality inherent in the inner product space exchanges the role of magnetic and electric fields.\n\nIn some projective planes, it is possible to find geometric transformations that map each point of the projective plane to a line, and each line of the projective plane to a point, in an incidence-preserving way. For such planes there arises a general principle of duality in projective planes: given any theorem in such a plane projective geometry, exchanging the terms \"point\" and \"line\" everywhere results in a new, equally valid theorem. A simple example is that the statement \"two points determine a unique line, the line passing through these points\" has the dual statement that \"two lines determine a unique point, the intersection point of these two lines\". For further examples, see Dual theorems.\n\nA conceptual explanation of this phenomenon in some planes (notably field planes) is offered by the dual vector space. In fact, the points in the projective plane formula_25 correspond to one-dimensional subvector spaces formula_26 while the lines in the projective plane correspond to subvector spaces formula_27 of dimension 2. The duality in such projective geometries stems from assigning to a one-dimensional formula_28 the subspace of formula_29 consisting of those linear maps formula_30 which satisfy formula_31. As a consequence of the dimension formula of linear algebra, this space is two-dimensional, i.e., it corresponds to a line in the projective plane associated to formula_29.\n\nThe (positive definite) bilinear form\n\nyields an identification of this projective plane with the formula_25. Concretely, the duality assigns to formula_26 its orthogonal formula_35. The explicit formulas in duality in projective geometry arise by means of this identification.\n\nIn the realm of topological vector spaces, a similar construction exists, replacing the dual by the topological dual vector space. A topological vector space that is canonically isomorphic to its bidual is called reflexive space. For example, the dual of an -space is where provided that , but the dual of is bigger than . Hence is not reflexive.\n\nHilbert spaces are equipped with an inner product . As in the finite-dimensional case, it allows to define a map\n\nThe Riesz representation theorem asserts that this map is an isomorphism, i.e., every Hilbert space is isomorphic to its dual.\n\nDistributions are linear functionals on appropriate spaces of functions. They are an important technical means in the theory of partial differential equations (PDE): instead of solving a PDE directly, it may be easier to first solve the PDE in the \"weak sense\", i.e., find a distribution that satisfies the PDE and, second, to show that the solution must in fact be a function.\n\nThe dual lattice of a lattice is given by\n\nwhich is used in the construction of toric varieties. The Pontryagin dual of locally compact topological groups \"G\" is given by\n\ncontinuous group homomorphisms with values in the circle (with multiplication of complex numbers as group operation).\n\nIn another group of dualities, the objects of one theory are translated into objects of another theory and the maps between objects in the first theory are translated into morphisms in the second theory, but with direction reversed. Using the parlance of category theory, this amounts to a contravariant functor between two categories and :\n\nwhich for any two objects \"X\" and \"Y\" of \"C\" gives a map\n\nThat functor may or may not be an equivalence of categories. There are various situations, where such a functor is an equivalence between the opposite category of , and . Using a duality of this type, every statement in the first theory can be translated into a \"dual\" statement in the second theory, where the direction of all arrows has to be reversed. Therefore, any duality between categories and is formally the same as an equivalence between and ( and ). However, in many circumstances the opposite categories have no inherent meaning, which makes duality an additional, separate concept.\n\nA category that is equivalent to its dual is called \"self-dual\". An example of self-dual category is the category of Hilbert spaces.\n\nMany category-theoretic notions come in pairs in the sense that they correspond to each other while considering the opposite category. For example, Cartesian products and disjoint unions of sets are dual to each other in the sense that\n\nand\n\nfor any set . This is a particular case of a more general duality phenomenon, under which limits in a category correspond to colimits in the opposite category ; further concrete examples of this are epimorphisms vs. monomorphism, in particular factor modules (or groups etc.) vs. submodules, direct products vs. direct sums (also called coproducts to emphasize the duality aspect). Therefore, in some cases, proofs of certain statements can be halved, using such a duality phenomenon. Further notions displaying related by such a categorical duality are projective and injective modules in homological algebra, fibrations and cofibrations in topology and more generally model categories.\n\nTwo functors and are adjoint if for all objects \"c\" in \"C\" and \"d\" in \"D\"\n\nin a natural way. Actually, the correspondence of limits and colimits is an example of adjoints, since there is an adjunction\n\nbetween the colimit functor that assigns to any diagram in indexed by some category its colimit and the diagonal functor that maps any object of to the constant diagram which has at all places. Dually,\n\nGelfand duality is a duality between commutative C*-algebras \"A\" and compact Hausdorff spaces \"X\" is the same: it assigns to \"X\" the space of continuous functions (which vanish at infinity) from \"X\" to C, the complex numbers. Conversely, the space \"X\" can be reconstructed from \"A\" as the spectrum of \"A\". Both Gelfand and Pontryagin duality can be deduced in a largely formal, category-theoretic way.\n\nIn a similar vein there is a duality in algebraic geometry between commutative rings and affine schemes: to every commutative ring \"A\" there is an affine spectrum, Spec \"A\". Conversely, given an affine scheme \"S\", one gets back a ring by taking global sections of the structure sheaf O. In addition, ring homomorphisms are in one-to-one correspondence with morphisms of affine schemes, thereby there is an equivalence\nAffine schemes are the local building blocks of schemes. The previous result therefore tells that the local theory of schemes is the same as commutative algebra, the study of commutative rings.\n\nNoncommutative geometry draws inspiration from Gelfand duality and studies noncommutative C*-algebras as if they were functions on some imagined space. Tannaka–Krein duality is a non-commutative analogue of Pontryagin duality.\n\nIn a number of situations, the two categories which are dual to each other are actually arising from partially ordered sets, i.e., there is some notion of an object \"being smaller\" than another one. A duality that respects the orderings in question is known as a Galois connection. An example is the standard duality in Galois theory mentioned in the introduction: a bigger field extension corresponds—under the mapping that assigns to any extension \"L\" ⊃ \"K\" (inside some fixed bigger field Ω) the Galois group Gal (Ω / \"L\") —to a smaller group.\n\nThe collection of all open subsets of a topological space \"X\" forms a complete Heyting algebra. There is a duality, known as Stone duality, connecting sober spaces and spatial locales.\n\n\nPontryagin duality gives a duality on the category of locally compact abelian groups: given any such group \"G\", the character group\ngiven by continuous group homomorphisms from \"G\" to the circle group \"S\" can be endowed with the compact-open topology. Pontryagin duality states that the character group is again locally compact abelian and that\nMoreover, discrete groups correspond to compact abelian groups; finite groups correspond to finite groups. On the one hand, Pontryagin is a special case of Gelfand duality. On the other hand, it is the conceptual reason of Fourier analysis, see below.\n\nIn analysis, problems are frequently solved by passing to the dual description of functions and operators.\n\nFourier transform switches between functions on a vector space and its dual:\nand conversely\nIf \"f\" is an \"L\"-function on R or R, say, then so is formula_38 and formula_39. Moreover, the transform interchanges operations of multiplication and convolution on the corresponding function spaces. A conceptual explanation of the Fourier transform is obtained by the aforementioned Pontryagin duality, applied to the locally compact groups R (or R etc.): any character of R is given by ξ↦ e. The dualizing character of Fourier transform has many other manifestations, for example, in alternative descriptions of quantum mechanical systems in terms of coordinate and momentum representations.\n\n\nTheorems showing that certain objects of interest are the dual spaces (in the sense of linear algebra) of other objects of interest are often called \"dualities\". Many of these dualities are given by a bilinear pairing of two \"K\"-vector spaces\nFor perfect pairings, there is, therefore, an isomorphism of \"A\" to the dual of \"B\".\n\nPoincaré duality of a smooth compact complex manifold \"X\" is given by a pairing of singular cohomology with C-coefficients (equivalently, sheaf cohomology of the constant sheaf C)\nwhere \"n\" is the (complex) dimension of \"X\". Poincaré duality can also be expressed as a relation of singular homology and de Rham cohomology, by asserting that the map\n(integrating a differential \"k\"-form over an 2\"n\"−\"k\"-(real) -dimensional cycle) is a perfect pairing.\n\nPoincaré duality also reverses dimensions; it corresponds to the fact that, if a topological manifold is represented as a cell complex, then the dual of the complex (a higher-dimensional generalization of the planar graph dual) represents the same manifold. In Poincaré duality, this homeomorphism is reflected in an isomorphism of the \"k\"th homology group and the (\"n\" − \"k\")th cohomology group.\n\nThe same duality pattern holds for a smooth projective variety over a separably closed field, using l-adic cohomology with Q-coefficients instead. This is further generalized to possibly singular varieties, using intersection cohomology instead, a duality called Verdier duality. Serre duality or coherent duality are similar to the statements above, but applies to cohomology of coherent sheaves instead.\n\nWith increasing level of generality, it turns out, an increasing amount of technical background is helpful or necessary to understand these theorems: the modern formulation of these dualities can be done using derived categories and certain direct and inverse image functors of sheaves (with respect to the classical analytical topology on manifolds for Poincaré duality, l-adic sheaves and the étale topology in the second case, and with respect to coherent sheaves for coherent duality).\n\nYet another group of similar duality statements is encountered in arithmetics: étale cohomology of finite, local and global fields (also known as Galois cohomology, since étale cohomology over a field is equivalent to group cohomology of the (absolute) Galois group of the field) admit similar pairings. The absolute Galois group \"G\"(F) of a finite field, for example, is isomorphic to formula_41, the profinite completion of Z, the integers. Therefore, the perfect pairing (for any \"G\"-module \"M\")\nis a direct consequence of Pontryagin duality of finite groups. For local and global fields, similar statements exist (local duality and global or Poitou–Tate duality).\n\n\n\n"}
{"id": "8092698", "url": "https://en.wikipedia.org/wiki?curid=8092698", "title": "Euclidean distance matrix", "text": "Euclidean distance matrix\n\nIn mathematics, a Euclidean distance matrix is an \"n×n\" matrix representing the spacing of a set of \"n\" points in Euclidean space. If \"A\" is a Euclidean distance matrix and the points formula_1 are defined on \"m\"-dimensional space, then the elements of \"A\" are given by\n\nwhere ||.|| denotes the 2-norm on R.\n\nSimply put, the element formula_4 describes the square of the distance between the \"i\" and \"j\" points in the set. By the properties of the 2-norm (or indeed, Euclidean distance in general), the matrix \"A\" has the following properties.\n\n\n"}
{"id": "47369663", "url": "https://en.wikipedia.org/wiki?curid=47369663", "title": "Flail space model", "text": "Flail space model\n\nThe flail space model (FSM) is a model of how a car passenger moves in a vehicle that collides with a roadside feature such as a guardrail or a crash cushion. Its principal purpose is to assess the potential risk of harm to the hypothetical occupant as he or she impacts the interior of the passenger compartment and, ultimately, the efficacy of an experimental roadside feature undergoing full-scale vehicle crash testing.\n\nThe FSM eliminates the complexity and expense of using instrumented anthropometric dummies during the crash test experiments. Furthermore, while crash test dummies were developed to model collisions between vehicles, they are not accurate when used for the sorts of collision angles that occur when a vehicle collides with a roadside feature; by contrast, the FSM was designed for such collisions.\n\nThe FSM is based on research performed at Southwest Research Institute in 1980 and published in 1981 in the paper entitled \"Collision Risk Assessment Based on Occupant Flail-Space Model\" by Jarvis D. Michie. The FSM (coined by Michie) was accepted by the highway community and published as a key part of the \"Recommended Procedures for the Safety Evaluation of Highway Appurtenances\" published in 1981 in National Cooperative Highway Research Program (NCHRP) Report 230. In 1993, the NCHRP Report was updated and presented as NCHRP Report 350; in this research effort performed by the Texas Transportation Research Institute, the FSM was reexamined and was unmodified in the new publication. In 2004, Douglas Gabauer further examined the efficacy of the FSM in his PhD thesis. The American Association of State Highway and Transportation Officials (AASHTO) retained the FSM as the method of assessing the risk of harm to vehicle occupants in the 2009 \"Manual for Assessing Safety Hardware\" that replaced NCHRP Report 350, stating that the FSM had \"served its intended purpose well\".\n\nThe FSM hypothesis divides the collision into two stages. In stage one, the unrestrained occupant is propelled forward and sideways in the compartment space due to vehicle collision accelerations and then impacts one or more surfaces (including the steering wheel) with velocity \"V\". According to the model, the vehicle (instead of the occupant) is the object that is accelerating. The occupant experiences no injury-producing force prior to contact with the compartment surfaces.\n\nIn stage two, the occupant is assumed to remain in contact with the compartment surface and experiences the same accelerations as the vehicle for the rest of the collision. The occupant may sustain injury at the end of stage one based on the velocity of impact with the compartment surfaces and due to vehicle accelerations during stage two. The occupant impact velocity and acceleration are computed from the vehicle collision acceleration history and the compartment geometry. Finally, the hypothetical occupant impact velocity and acceleration are then compared to threshold values of human tolerance to these forces.\n"}
{"id": "567580", "url": "https://en.wikipedia.org/wiki?curid=567580", "title": "Gaussian integral", "text": "Gaussian integral\n\nThe Gaussian integral, also known as the Euler–Poisson integral, is the integral of the Gaussian function \"e\" over the entire real line. It is named after the German mathematician Carl Friedrich Gauss. The integral is:\n\nAbraham de Moivre originally discovered this type of integral in 1733, while Gauss published the precise integral in 1809. The integral has a wide range of applications. For example, with a slight change of variables it is used to compute the normalizing constant of the normal distribution. The same integral with finite limits is closely related to both the error function and the cumulative distribution function of the normal distribution. In physics this type of integral appears frequently, for example, in quantum mechanics, to find the probability density of the ground state of the harmonic oscillator. This integral is also used in the path integral formulation, to find the propagator of the harmonic oscillator, and in statistical mechanics, to find its partition function.\n\nAlthough no elementary function exists for the error function, as can be proven by the Risch algorithm, the Gaussian integral can be solved analytically through the methods of multivariable calculus. That is, there is no elementary \"indefinite integral\" for\nbut the definite integral\ncan be evaluated. The definite integral of an arbitrary Gaussian function is\n\nThe Gaussian integral is encountered very often in physics and numerous generalizations of the integral are encountered in quantum field theory.\n\nA standard way to compute the Gaussian integral, the idea of which goes back to Poisson, is to make use of the property that:\n\nformula_5\n\n\nComparing these two computations yields the integral, though one should take care about the improper integrals involved.\n\nformula_7\n\nwhere the factor of \"r\" is the Jacobian determinant which appears because of the transform to polar coordinates (\"r\" \"dr\" \"dθ\" is the standard measure on the plane, expressed in polar coordinates ), and the substitution involves taking \"s\" = −\"r\", so \"ds\" = −2\"r\" \"dr\".\n\nCombining these yields\n\nso\n\nTo justify the improper double integrals and equating the two expressions, we begin with an approximating function:\n\nIf the integral\nwere absolutely convergent we would have that its Cauchy principal value, that is, the limit\n\nwould coincide with\nTo see that this is the case, consider that\n\nso we can compute\nby just taking the limit\n\nTaking the square of formula_17 yields\n\nUsing Fubini's theorem, the above double integral can be seen as an area integral\n\ntaken over a square with vertices {(−\"a\", \"a\"), (\"a\", \"a\"), (\"a\", −\"a\"), (−\"a\", −\"a\")} on the \"xy\"-plane.\n\nSince the exponential function is greater than 0 for all real numbers, it then follows that the integral taken over the square's incircle must be less than formula_20, and similarly the integral taken over the square's circumcircle must be greater than formula_20. The integrals over the two disks can easily be computed by switching from cartesian coordinates to polar coordinates:\n\nIntegrating,\n\nBy the squeeze theorem, this gives the Gaussian integral\n\nA different technique, which goes back to Laplace (1812), is the following. Let\n\nSince the limits on \"s\" as \"y\" → ±∞ depend on the sign of \"x\", it simplifies the calculation to use the fact that \"e\" is an even function, and, therefore, the integral over all real numbers is just twice the integral from zero to infinity. That is,\n\nThus, over the range of integration, \"x\" ≥ 0, and the variables \"y\" and \"s\" have the same limits. This yields:\n\nTherefore, formula_31, as expected.\n\nThe integrand is an even function,\n\nThus, after the change of variable formula_33, this turns into the Euler integral\n\nwhere Γ is the gamma function. This shows why the factorial of a half-integer is a rational multiple of formula_35. More generally,\n\nThe integral of an arbitrary Gaussian function is\n\nAn alternative form is\n\nThis form is very useful in calculating mathematical expectations of some continuous probability distributions concerning normal distribution.\n\nSee, for example, the expectation of the log-normal distribution.\n\nSuppose \"A\" is a symmetric positive-definite (hence invertible) precision matrix, which is the matrix inverse of the covariance matrix. Then,\n\nwhere the integral is understood to be over R. This fact is applied in the study of the multivariate normal distribution.\n\nAlso,\n\nwhere σ is a permutation of {1, ..., 2\"N\"} and the extra factor on the right-hand side is the sum over all combinatorial pairings of {1, ..., 2\"N\"} of \"N\" copies of \"A\".\n\nAlternatively, \n\nfor some analytic function \"f\", provided it satisfies some appropriate bounds on its growth and some other technical criteria. (It works for some functions and fails for others. Polynomials are fine.) The exponential over a differential operator is understood as a power series.\n\nWhile functional integrals have no rigorous definition (or even a nonrigorous computational one in most cases), we can \"define\" a Gaussian functional integral in analogy to the finite-dimensional case. There is still the problem, though, that formula_42 is infinite and also, the functional determinant would also be infinite in general. This can be taken care of if we only consider ratios:\n\nIn the DeWitt notation, the equation looks identical to the finite-dimensional case.\n\nIf A is again a symmetric positive-definite matrix, then (assuming all are column vectors)\n\nwhere \"n\" is a positive integer and !! denotes the double factorial.\n\nAn easy way to derive these is by parameter differentiation.\n\nOne could also integrate by parts and find a recurrence relation to solve this.\n\nApplying a linear change of basis shows that the integral of the exponential of a homogeneous polynomial in \"n\" variables may depend only on SL(n)-invariants of the polynomial. One such invariant is the discriminant,\nzeros of which mark the singularities of the integral. However, the integral may also depend on other invariants.\n\nExponentials of other even polynomials can numerically be solved using series. These may be interpreted as formal calculations when there is no convergence. For example, the solution to the integral of the exponential of a quartic polynomial is\n\nThe \"n\" + \"p\" = 0 mod 2 requirement is because the integral from −∞ to 0 contributes a factor of (−1)/2 to each term, while the integral from 0 to +∞ contributes a factor of 1/2 to each term. These integrals turn up in subjects such as quantum field theory.\n\n\n"}
{"id": "31707735", "url": "https://en.wikipedia.org/wiki?curid=31707735", "title": "Generalized minimum-distance decoding", "text": "Generalized minimum-distance decoding\n\nIn coding theory, generalized minimum-distance (GMD) decoding provides an efficient algorithm for decoding concatenated codes, which is based on using an errors-and-erasures decoder for the outer code.\n\nA naive decoding algorithm for concatenated codes can not be an optimal way of decoding because it does not take into account the information that maximum likelihood decoding (MLD) gives. In other words, in the naive algorithm, inner received codewords are treated the same regardless of the difference between their hamming distances. Intuitively, the outer decoder should place higher confidence in symbols whose inner encodings are close to the received word. David Forney in 1966 devised a better algorithm called generalized minimum distance (GMD) decoding which makes use of those information better. This method is achieved by measuring confidence of each received codeword, and erasing symbols whose confidence is below a desired value. And GMD decoding algorithm was one of the first examples of soft-decision decoders. We will present three versions of the GMD decoding algorithm. The first two will be randomized algorithms while the last one will be a deterministic algorithm.\n\n\nConsider the received word formula_32 which was corrupted by a noisy channel. The following is the algorithm description for the general case. In this algorithm, we can decode y by just declaring an erasure at every bad position and running the errors and erasure decoding algorithm for formula_17 on the resulting vector.\n\nRandomized_Decoder\nGiven : formula_34.\n\nTheorem 1. \"Let y be a received word such that there exists a codeword\" formula_44 \"such that\" formula_45. \"Then the deterministic GMD algorithm outputs\" formula_46.\n\nNote that a naive decoding algorithm for concatenated codes can correct up to formula_47 errors.\n\n\"Remark.\" If formula_53, then the algorithm in Step 2 will output formula_46. The lemma above says that in expectation, this is indeed the case. Note that this is not enough to prove Theorem 1, but can be crucial in developing future variations of the algorithm.\n\nProof of lemma 1. For every formula_55 define formula_56 This implies that\n\nNext for every formula_35, we define two indicator variables:\n\nWe claim that we are done if we can show that for every formula_35:\n\nClearly, by definition \n\nFurther, by the linearity of expectation, we get \n\nTo prove (2) we consider two cases: formula_64-th block is correctly decoded (Case 1), formula_64-th block is incorrectly decoded (Case 2):\n\nCase 1: formula_66\n\nNote that if formula_67 then formula_68, and formula_69 implies formula_70 and formula_71.\n\nFurther, by definition we have\n\nCase 2: formula_73\n\nIn this case, formula_74 and formula_75\n\nSince formula_76. This follows another case analysis when formula_77 or not.\n\nFinally, this implies\n\nIn the following sections, we will finally show that the deterministic version of the algorithm above can do unique decoding of formula_79 up to half its design distance.\n\nNote that, in the previous version of the GMD algorithm in step \"3\", we do not really need to use \"fresh\" randomness for each formula_64. Now we come up with another randomized version of the GMD algorithm that uses the \"same\" randomness for every formula_64. This idea follows the algorithm below.\n\nModified_Randomized_Decoder\nGiven : formula_82, pick formula_83 at random. Then every for every formula_35:\n\nFor the proof of Lemma 1, we only use the randomness to show that\n\nIn this version of the GMD algorithm, we note that\n\nThe second equality above follows from the choice of formula_94. The proof of Lemma 1 can be also used to show formula_95 for version2 of GMD. In the next section, we will see how to get a deterministic version of the GMD algorithm by choosing formula_94 from a polynomially sized set as opposed to the current infinite set formula_97.\n\nLet formula_98 for formula_35.\n\nEvery loop of 1~4 can be run in polynomial time, the algorithm above can also be computed in polynomial time. Specifically, each call to an errors and erasures decoder of formula_111 errors takes formula_112 time. Finally, the runtime of the algorithm above is formula_113 where formula_114 is the running time of the outer errors and erasures decoder.\n\n\n"}
{"id": "31554519", "url": "https://en.wikipedia.org/wiki?curid=31554519", "title": "Gert Sabidussi", "text": "Gert Sabidussi\n\nGert Sabidussi (born 28 October 1929 in Graz) is an Austrian mathematician specializing in combinatorics and graph theory.\n\nSabidussi was born in Graz, Austria. His family later moved to Innsbruck where his father was a Protestant deacon. He graduated from the University of Vienna, where he attended lectured by Felix Ehrenhaft, Nikolaus Hofreiter, Johann Radon and Hans Thirring. In 1953, he defended his doctorate on 0-1 matrices under the supervision of Edmund Hlawka and received a two-year fellowship at Princeton University. He was then an Instructor at University of Minnesota in Minneapolis, but because of the heavy teaching load moved a year later, in 1956, to Tulane University in New Orleans. He moved to Montreal in 1963, and was instrumental in bringing to Canada a number of combinatorialists and graph theorists, including Anton Kotzig, and Jaroslav Nešetřil who wrote a thesis under Sabidussi. He first worked at McMaster University and then at University of Montreal. Over the years, he had 13 graduate students. His 60th, 70th and 80th birthdays were celebrated with large Graph Theory birthday conferences.\n\nSabidussi wrote foundational work on Cayley graphs, graph products and Frucht's theorem.\n\n\n"}
{"id": "44402565", "url": "https://en.wikipedia.org/wiki?curid=44402565", "title": "Global cascades model", "text": "Global cascades model\n\nGlobal cascades models are a class of models aiming to model large and rare cascades that are triggered by exogenous perturbations which are relatively small compared with the size of the system. The phenomenon occurs ubiquitously in various systems, like information cascades in social systems, stock market crashes in economic systems, and cascading failure in physics infrastructure networks. The models capture some essential properties of such phenomenon.\n\nTo describe and understand global cascades, a network-based threshold model has been proposed by Duncan J. Watts in 2002. The model is motivated by considering a population of individuals who must make a decision between two alternatives, and their choices depend explicitly on other people's states or choices. The model assumes that an individual will adopt a new particular opinion (product or state) if a threshold fraction of his/her neighbors have adopted the new one, else he would keep his original state. To initiate the model, a new opinion will be randomly distributed among a small fraction of individuals in the network. If the fraction satisfies a particular condition, a large cascades can be triggered.(see Global Cascades Condition) A phase transition phenomenon has been observed: when the network of interpersonal influences is sparse, the size of the cascades exhibits a power law distribution, the most highly connected nodes are critical in triggering cascades, and if the network is relatively dense, the distribution shows a bimodal form, in which nodes with average degree show more importance by serving as triggers.\n\nSeveral generalizations of the Watt's threshold model have been proposed and analyzed in the following years. For example, the original model has been combined with independent interaction models to provide a generalized model of social contagion, which classifies the behavior of the system into three universal classes. It has also been generalized on modular networks degree-correlated networks and to networks with tunable clustering. The role of the initiators has also been studied recently, shows that different initiator would influence the size of the cascades.\n\nTo derive the precise cascade condition in the original model, a generating function method could be applied. The generating function for vulnerable nodes in the network is:\n\nwhere \"p\" is the probability a node has degree \"k\", and \nand \"f\" is the distribution of the threshold fraction of individuals. The average vulnerable cluster size can be derived as:\n\nwhere \"z\" is the average degree of the network. The Global cascades occur when the average vulnerable cluster size <\"n\"> diverges\n\nThe equation could be interpreted as: When formula_5, the clusters in the network is small and global cascades will not happen since the early adopters are isolated in the system, thus no enough momentum could be generated. When formula_6, the typical size of the vulnerable cluster is infinite, which implies presence of global cascades.\n\nThe Model considers a change of state of individuals in different systems which belongs to a larger class of contagion problems. However it differs with other models in several aspects: Compared with 1) epidemic model: where contagion events between individual pairs are independent, the effect a single infected node having on an individual depends on the individual's other neighbors in the proposed model. Unlike 2) percolation or self-organized criticality models, the threshold is not expressed as the absolute number of \"infected\" neighbors around an individual, instead, a corresponding fraction of neighbors is selected. It is also different from 3) random-field ising model and majority voter model, which are frequently analyzed on regular lattices, here, however the heterogeneity of the network plays a significant role.\n\n"}
{"id": "52405621", "url": "https://en.wikipedia.org/wiki?curid=52405621", "title": "Graded-symmetric algebra", "text": "Graded-symmetric algebra\n\nIn algebra, given a commutative ring \"R\", the graded-symmetric algebra of a graded \"R\"-module \"M\" is the quotient of the tensor algebra of \"M\" by the ideal \"I\"; here the ideal \"I\" is generated by elements of the form:\nfor homogeneous elements \"x\", \"y\" in \"M\" of degree |\"x\"|, |\"y\"|. By construction, a graded-symmetric algebra is graded-commutative; i.e., formula_3 and is universal for this.\n\nIn spite of the name, the notion is a common generalization of a symmetric algebra and an exterior algebra: indeed, if \"V\" is a (non-graded) \"R\"-module, then the graded-symmetric algebra of \"V\" with trivial grading is the usual symmetric algebra of \"V\". Similarly, the graded-symmetric algebra of the graded module with \"V\" in degree one and zero elsewhere is the exterior algebra of \"V\".\n\n"}
{"id": "455398", "url": "https://en.wikipedia.org/wiki?curid=455398", "title": "Hans Eberstark", "text": "Hans Eberstark\n\nHans Eberstark (27 January 1929 in Vienna – 19 December 2001) was an Austrian linguist, translator, and mental calculator.\n\nEberstark often lectured on language and translation in Europe and was known for asking someone whose first language was a small local dialect of German (particularly Swiss German, of which there are countless dialects) to speak with him (during the lecture); after a few minutes Eberstark would suddenly start speaking fluently in that dialect.\n\nOf Viennese Jewish origin, he spent eight years in Shanghai during World War II, with many other displaced people from all over Europe. It was there that he was exposed to many different languages. He once told science writer Jeremy Bernstein that to his \"eternal disgrace\" he had not learned to speak Chinese \"properly\" while there, but Bernstein noted that Eberstark's standards for speaking a language were \"different from most people's.\"\n\nEberstark was living in Vienna, Austria, when he joined Mensa International. After he moved to Geneva, Switzerland in 1965 to work as an interpreter with the International Labour Organization, he founded a Mensa chapter in that city. He took early retirement in 1967 and became a free-lance and also taught courses in translating and interpreting at the University of Geneva.\n\nEberstark was prepared to interpret into English and German and from French, Dutch, Italian, Spanish and Catalan. He also knew how to speak Surinamese Creole, Haitian Creole and Papiamento, from the Netherlands Antilles, as well as Yiddish, several varieties of Swiss German, Albanian and Hebrew.\n\nIn the late 1960s he was married with two children. He once told a group of friends that he \"knew\" the date he would die.\n\nHe also is known for having once recited 11,944 successive digits of the mathematical quantity of pi from memory. During an earlier attempt he had intended on reciting roughly half that many but had made a mistake. He was angry with himself for the mistake so he memorized even more.\n\nEberstark once wrote that the \"external rewards\" of excelling in mental arithmetic were \"Making friends, making money, showing off, and giving pleasure.\"\n\n"}
{"id": "8776613", "url": "https://en.wikipedia.org/wiki?curid=8776613", "title": "Hexacoordinate", "text": "Hexacoordinate\n\nHexacoordinate in chemistry is a molecule with six ligands or atomic attachments arranged around a single metal atom in the centre. Most hexacoordinate species form an octahedral molecular geometry, with four ligands arranged equatorially, and two axially.\nThere is research that suggests some planar hexacoordinate structures of carbon may exist.\n\n"}
{"id": "33013259", "url": "https://en.wikipedia.org/wiki?curid=33013259", "title": "Humbert polynomials", "text": "Humbert polynomials\n\nIn mathematics, the Humbert polynomials π(\"x\") are a generalization of Pincherle polynomials introduced by given by the generating function\n\n\n"}
{"id": "46232572", "url": "https://en.wikipedia.org/wiki?curid=46232572", "title": "In silico clinical trials", "text": "In silico clinical trials\n\nAn \"in silico\" clinical trial is an individualised computer simulation used in the development or regulatory evaluation of a medicinal product, device, or intervention. While completely simulated clinical trials are not feasible with current technology and understanding of biology, its development would be expected to have major benefits over current \"in vivo\" clinical trials, and research on it is being pursued.\n\nThe term \"in silico\" indicates any use of computers in clinical trials, even if limited to management of clinical information in a database.\n\nThe traditional model for the development of medical treatments and devices begins with pre-clinical development. In laboratories, test-tube and other \"in vitro\" experiments establish the plausibility for the efficacy of the treatment. Then \"in vivo\" animal models, with different species, provide guidance on the efficacy and safety of the product for humans. With success in both \"in vitro\" and \"in vivo\" studies, scientist can propose that clinical trials test whether the product be made available for humans. Clinical trials are often divided into four phases. Phase 3 involves testing a large number of people. When a medication fails at this stage, the financial losses can be catastrophic.\nPredicting low-frequency side effects has been difficult, because such side effects need not become apparent until the treatment is adopted by many patients. The appearance of severe side-effects in phase three often causes development to stop, for ethical and economic reasons. Also, in recent years many candidate drugs failed in phase 3 trials because of lack of efficacy rather than for safety reasons. One reason for failure is that traditional trials aim to establish efficacy and safety for most subjects, rather than for individual subjects, and so efficacy is determined by a statistic of central tendency for the trial. Traditional trials do not adapt the treatment to the covariates of subjects: \n\nAccurate computer models of a treatment and its deployment, as well as patient characteristics, are necessary precursors for the development of \"in silico\" clinical trials. In such a scenario, ‘virtual’ patients would be given a ‘virtual’ treatment, enabling observation through a computer simulation of how the candidate biomedical product performs and whether it produces the intended effect, without inducing adverse effects. Such \"in silico\" clinical trials could help to reduce, refine, and partially replace real clinical trials by:\nIn addition, real clinical trials may indicate that a product is unsafe or ineffective, but rarely indicate why or suggest how it might be improved. As such, a product that fails during clinical trials may simply be abandoned, even if a small modification would solve the problem. This stifles innovation, decreasing the number of truly original biomedical products presented to the market every year, and at the same time increasing the cost of development.\nAnalysis through \"in silico\" clinical trials is expected to provide a better understanding of the mechanism that caused the product to fail in testing,<ref name=\"The Role of Modeling and Simulation in Development and Registration of Medicinal Products: Output From the EFPIA/EMA Modeling and Simulation Workshop. Manolis\"></ref> and may be able to provide information that could be used to refine the product to such a degree that it could successfully complete clinical trials.\n\n\"In silico\" clinical trials would also provide significant benefits over current pre-clinical practices. Unlike animal models, the virtual human models can be re-used indefinitely, providing significant cost savings. Compared to trials in animals or a small sample of humans, \"in silico\" trials might more effectively predict the behaviour of the drug or device in large-scale trials, identifying side effects that were previously difficult or impossible to detect, helping to prevent unsuitable candidates from progressing to the costly phase 3 trials.\n\n\n"}
{"id": "23535218", "url": "https://en.wikipedia.org/wiki?curid=23535218", "title": "Industrial engineering", "text": "Industrial engineering\n\nIndustrial engineering is an inter-disciplinary profession that is concerned with the optimization of complex processes, systems, or organizations by developing, improving and implementing integrated systems of people, money, knowledge, information, equipment, energy and materials. \n\nIndustrial engineers use specialized knowledge and skills in business administration, management, mathematics, physical sciences, social sciences and methods of engineering analysis and design to specify, predict, and evaluate the results obtained from systems or processes. From these results, they are able to create new systems, processes or situations for the useful coordination of man, materials and machines and improve the quality and productivity of systems, physical or social. Depending on the sub-specialties involved, industrial engineering may also overlap with, operations research, systems engineering, manufacturing engineering, production engineering, management science, management engineering, financial engineering, ergonomics or human factors engineering, safety engineering, or others, depending on the viewpoint or motives of the user. \n\nEven though its underlying concepts overlap considerably with certain business-oriented disciplines, such as operations management, Industrial engineering is a longstanding engineering discipline subject to (and eligible for) professional engineering licensure in most jurisdictions. \n\nThere is a general consensus among historians that the roots of the Industrial Engineering Profession date back to the Industrial Revolution. The technologies that helped mechanize traditional manual operations in the textile industry including the Flying shuttle, the Spinning jenny, and perhaps most importantly the Steam engine generated Economies of scale that made Mass production in centralized locations attractive for the first time. The concept of the production system had its genesis in the factories created by these innovations.\n\nAdam Smith's concepts of Division of Labour and the \"Invisible Hand\" of capitalism introduced in his treatise \"The Wealth of Nations\" motivated many of the technological innovators of the Industrial revolution to establish and implement factory systems. The efforts of James Watt and Matthew Boulton led to the first integrated machine manufacturing facility in the world, including the implementation of concepts such as cost control systems to reduce waste and increase productivity and the institution of skills training for craftsmen.\n\nCharles Babbage became associated with Industrial engineering because of the concepts he introduced in his book \"On the Economy of Machinery and Manufacturers\" which he wrote as a result of his visits to factories in England and the United States in the early 1800s. The book includes subjects such as the time required to perform a specific task, the effects of subdividing tasks into smaller and less detailed elements, and the advantages to be gained from repetitive tasks.\n\nEli Whitney and Simeon North proved the feasibility of the notion of Interchangeable parts in the manufacture of muskets and pistols for the US Government. Under this system, individual parts were mass-produced to tolerances to enable their use in any finished product. The result was a significant reduction in the need for skill from specialized workers, which eventually led to the industrial environment to be studied later.\n\nFrederick Taylor (1856 – 1915) is generally credited as being the father of the Industrial Engineering discipline. He earned a degree in mechanical engineering from Steven's University and earned several patents from his inventions. His books, \"Shop Management\" and \"The Principles of Scientific Management\" which were published in the early 1900s, were the beginning of Industrial Engineering. Improvements in work efficiency under his methods was based on improving work methods, developing of work standards, and reduction in time required to carry out the work. With an abiding faith in the scientific method, Taylor's contribution to \"Time Study\" sought a high level of precision and predictability for manual tasks.\n\nThe husband-and-wife team of Frank Gilbreth (1868 – 1924) and Lillian Gilbreth (1878 – 1972) was the other cornerstone of the Industrial Engineering movement whose work is housed at Purdue University School of Industrial Engineering. They categorized the elements of human motion into 18 basic elements called \"therbligs\". This development permitted analysts to design jobs without knowledge of the time required to do a job. These developments were the beginning of a much broader field known as human factors or ergonomics.\n\nin 1908, the first course on Industrial Engineering was offered as an elective at Pennsylvania State University, which became a separate program in 1909 through the efforts of Hugo Diemer. The first doctoral degree in industrial engineering was awarded in 1933 by Cornell University.\n\nIn 1912 Henry Laurence Gantt developed the Gantt chart which outlines actions the organization along with their relationships. This chart opens later form familiar to us today by Wallace Clark.\n\nAssembly lines: moving car factory of Henry Ford (1913) accounted for a significant leap forward in the field. Ford reduced the assembly time of a car more than 700 hours to 1.5 hours. In addition, he was a pioneer of the economy of the capitalist welfare (\"welfare capitalism\") and the flag of providing financial incentives for employees to increase productivity.\n\nComprehensive quality management system (Total quality management or TQM) developed in the forties was gaining momentum after World War II and was part of the recovery of Japan after the war.\n\nThe American Institute of Industrial Engineering was formed in 1948. The early work by F. W. Taylor and the Gilbreths was documented in papers presented to the American Society of Mechanical Engineers as interest grew from merely improving machine performance to the performance of the overall manufacturing process; most notably starting with the presentation by Henry R. Towne (1844 - 1924) of his paper \"The Engineer as An Economist\" (1186).\n\nIn 1960 to 1975, with the development of decision support systems in supply such as the Material requirements planning (MRP), you can emphasize the timing issue (inventory, production, compounding, transportation, etc.) of industrial organization. Israeli scientist Dr. Jacob Rubinovitz installed the CMMS program developed in IAI and Control-Data (Israel) in 1976 in South Africa and worldwide.\n\nIn the seventies, with the penetration of Japanese management theories such as Kaizen and Kanban, Japan realized very high levels of quality and productivity. These theories improved issues of quality, delivery time, and flexibility. Companies in the west realized the great impact of Kaizen and started implementing their own Continuous improvement programs.\n\nIn the nineties, following the global industry globalization process, the emphasis was on supply chain management and customer-oriented business process design. Theory of constraints developed by an Israeli scientist Eliyahu M. Goldratt (1985) is also a significant milestone in the field.\n\nEngineering is traditionally decompositional. To understand the whole, it is first broken into its parts. One then masters the parts and puts them back together, becoming the master of the whole.\nIndustrial and systems engineering's (ISE) approach is the opposite; any one part cannot be understood without the context of the whole. Changes in one part affect the whole, and the role of a part is a projection into the whole. In traditional engineering, people understand the parts first, then they can understand the whole. In ISE, they understand the whole first, and then they can understand the role of each part.\n\nAlso, Industrial engineering considers the human factor and its relation to the technical aspect of the situation and the all of the other factors that influence the entire situation, while other engineering disciplines focuses on the design of inanimate objects \n\n\"Industrial Engineers integrate combinations of people, information, materials, and equipment that produce innovative and efficient organizations. In addition to manufacturing, Industrial Engineers work and consult in every industry, including hospitals, communications, e-commerce, entertainment, government, finance, food, pharmaceuticals, semiconductors, sports, insurance, sales, accounting, banking, travel, and transportation.\" \n\n\" Industrial Engineering is the branch of Engineering most closely related to human resources in that we apply social skills to work with all types of employees, from engineers to salespeople to top management. One of the main focuses of an Industrial Engineer is to improve the working environments of people – not to change the worker, but to change the workplace.\"\n\n\"All engineers, including Industrial Engineers, take mathematics through calculus and differential equations. Industrial Engineering is different in that it is based on discrete variable math, whereas all other engineering is based on continuous variable math. We emphasize the use of linear algebra and difference equations, as opposed to the use of differential equations which are so prevalent in other engineering disciplines. This emphasis becomes evident in optimization of production systems in which we are sequencing orders, scheduling batches, determining the number of materials handling units, arranging factory layouts, finding sequences of motions, etc. As, Industrial Engineers, we deal almost exclusively with systems of discrete components.\"\nWhile originally applied to manufacturing, the use of \"industrial\" in \"industrial engineering\" can be somewhat misleading, since it has grown to encompass any methodical or quantitative approach to optimizing how a process, system, or organization operates. In fact, the \"Industrial\" in Industrial engineering means the \"industry\" in its broadest sense. People have changed the term \"industrial\" to broader terms such as Industrial and Manufacturing Engineering, Industrial and Systems Engineering, Industrial Engineering & Operations Research, Industrial Engineering & Management. \nIndustrial engineering has many sub-disciplines, the most common of which are listed below. Although there are industrial engineers who focus exclusively on one of these sub-disciplines, many deal with a combination of them such as Supply Chain and Logistics, and Facilities and Energy Management.\n\nFacilities Engineering & Energy Management\n\nFinancial Engineering\n\nHuman Factors & Safety Engineering\n\nInformation Systems Engineering & Management\n\nManufacturing Engineering\n\nOperations Engineering & ManagementOperations Research & Optimization\n\nPolicy Planning\n\nProduction Engineering\n\nQuality & Reliability Engineering\n\nSupply Chain Management & LogisticsSystem Analysis\n\nSystems Engineering\n\nSystems Simulation\n\nRelated Disciplines\n\nOrganization Development & Change Management\n\nBehavioral Economics\nIndustrial engineers study the interaction of human beings with machines, materials, information, procedures and environments in such developments and in designing a technological system.\n\nUniversities offer degrees at the bachelor, masters, and doctoral level.\n\nIn the United States, the undergraduate degree earned is the Bachelor of Science (B.S.) or Bachelor of Science and Engineering (B.S.E.) in Industrial Engineering (IE). Variations of the title include Industrial & Operations Engineering (IOE), and Industrial & Systems Engineering (ISE). The typical curriculum includes a broad math and science foundation spanning chemistry, physics, mechanics (i.e., statics, kinematics, and dynamics), materials science, computer science, electronics/circuits, engineering design, and the standard range of engineering mathematics (i.e. calculus, linear algebra, differential equations, statistics). For any engineering undergraduate program to be accredited, regardless of concentration, it must cover a largely similar span of such foundational work - which also overlaps heavily with the content tested on one or more engineering licensure exams in most jurisdictions.\n\nThe coursework specific to IE entails specialized courses in areas such as optimization, applied probability, stochastic modeling, design of experiments, statistical process control, simulation, manufacturing engineering, ergonomics/safety engineering, and engineering economics. Industrial engineering elective courses typically cover more specialized topics in areas such as manufacturing, supply chains and logistics, analytics and machine learning, production systems, human factors and industrial design, and service systems.\n\nCertain business schools may offer programs with some overlapping relevance to IE, but the engineering programs are distinguished by a much more intensely quantitative focus, required engineering science electives, and the core math and science courses required of all engineering programs.\n\nThe usual graduate degree earned is the Master of Science (MS) or Master of Science and Engineering (MSE) in Industrial Engineering or various alternative related concentration titles.\n\nTypical MS curricula may cover:\nWhile Industrial Engineering as a formal degree has been around for years, consensus on what topics should be taught and studied differs across countries. For example, Turkey focuses on a very technical degree while Denmark, Finland and the United Kingdom have a management focus degree, thus making it less technical. The United States focuses on case-studies and group problem solving.\n\nTraditionally, a major aspect of industrial engineering was planning the layouts of factories and designing assembly lines and other manufacturing paradigms. And now, in lean manufacturing systems, industrial engineers work to eliminate wastes of time, money, materials, energy, and other resources.\n\nExamples of where industrial engineering might be used include flow process charting, process mapping, designing an assembly workstation, strategizing for various operational logistics, consulting as an efficiency expert, developing a new financial algorithm or loan system for a bank, streamlining operation and emergency room location or usage in a hospital, planning complex distribution schemes for materials or products (referred to as supply-chain management), and shortening lines (or queues) at a bank, hospital, or a theme park.\n\nModern industrial engineers typically use predetermined motion time system, computer simulation (especially discrete event simulation), along with extensive mathematical tools for modeling, such as mathematical optimization and queueing theory, and computational methods for system analysis, evaluation, and optimization. Industrial engineers also use the tools of data science and machine learning in their work owing to the strong relatedness of these disciplines with the field and the similar technical background required of industrial engineers (including a strong foundation in probability theory, linear algebra, and statistics, as well as having coding skills).\n\n\n\n"}
{"id": "7343761", "url": "https://en.wikipedia.org/wiki?curid=7343761", "title": "Jeff Cheeger", "text": "Jeff Cheeger\n\nJeff Cheeger (born December 1, 1943, Brooklyn, New York City), is a mathematician. Cheeger is professor at the Courant Institute of Mathematical Sciences at New York University in New York City. His main interests are differential geometry and its connections with topology and analysis.\n\nHe graduated from Harvard University with a B.A. in 1964. He graduated from Princeton University with an M.S. in 1966 and with a Ph.D. in 1967. He is a Silver Professor at the Courant Institute at NYU where he has worked since 1993.\n\nHe worked as a teaching assistant and research assistant at Princeton from 1966–1967, an N.S.F. Postdoctoral Fellow and Instructor from 1967–1968, an assistant professor from 1968 to 1969 at the University of Michigan, and an associate professor from 1969-1971 at SUNY at Stony Brook. Cheeger was a professor at SUNY, Stony Brook from 1971 to 1985, a leading professor from 1985 to 1990, and a distinguished professor from 1990 until 1992.\n\nCheeger has also had a number of visiting positions in Brazil (1971), at the Institute for Advanced Study (1972, 1977, 1978, 1995), Harvard University (1972), the Institut des Hautes Études Scientifiques (1984–1985) and the Mathematical Sciences Research Institute (1985).\n\nHe has supervised at least 13 doctoral theses and three postdocs. He has served as a member of several AMS committees and NSF panels.\n\nCheeger delivered Invited Addresses at the International Congress of Mathematicians in 1974 and in 1986.\n\nHe received the Guggenheim Fellowship in 1984. In 1998 Cheeger was elected a foreign member of the Finnish Academy of Science and Letters.\n\nCheeger was elected a member of the United States National Academy of Sciences in 1997.\n\nHe received the Fourteenth Oswald Veblen Prize in Geometry from the American Mathematical Society in 2001.\n\n\n\n\n\n"}
{"id": "51251595", "url": "https://en.wikipedia.org/wiki?curid=51251595", "title": "Korovkin approximation", "text": "Korovkin approximation\n\nIn mathematics the Korovkin approximation is a convergence statement in which the approximation of a function is given by a certain sequence of functions. In practice a continuous function can be approximated by polynomials. With Korovkin approximations one comes a convergence for the whole approximation with examination of the convergence of the process at a finite number of functions. The Korovkin approximation is named after Pavel Korovkin.\n"}
{"id": "21449443", "url": "https://en.wikipedia.org/wiki?curid=21449443", "title": "Larry Stockmeyer", "text": "Larry Stockmeyer\n\nLarry Joseph Stockmeyer (1948 – 31 July 2004) was an American computer scientist. He was one of the pioneers in the field of computational complexity theory, and he also worked in the field of distributed computing. He died of pancreatic cancer.\n\n\n\n\n\n"}
{"id": "9335972", "url": "https://en.wikipedia.org/wiki?curid=9335972", "title": "Lipman Bers", "text": "Lipman Bers\n\nLipman \"Lipa\" Bers (Latvian: \"Lipmans Berss\"; May 22, 1914 – October 29, 1993) was an American mathematician born in Riga who created the theory of pseudoanalytic functions and worked on Riemann surfaces and Kleinian groups. He was also known for his work in human rights activism.\n\nBers was born in Riga, then under the rule of the Russian Csars, and spent several years as a child in Saint Petersburg; his family returned to Riga in approximately 1919, by which time it was part of independent Latvia. In Riga, his mother was the principal of a Jewish elementary school, and his father became the principal of a Jewish high school, both of which Bers attended, with an interlude in Berlin while his mother, by then separated from his father, attended the Berlin Psychoanalytic Institute. After high school, Bers studied at the University of Zurich for a year, but had to return to Riga again because of the difficulty of transferring money from Latvia in the international financial crisis of the time. He continued his studies at the University of Riga, where he became active in socialist politics, including giving political speeches and working for an underground newspaper. In the aftermath of the Latvian coup in 1934 by right-wing leader Kārlis Ulmanis, Bers was targeted for arrest but fled the country, first to Estonia and then to Czechoslovakia.\n\nBers received his Ph.D. in 1938 from the University of Prague. He had begun his studies in Prague with Rudolf Carnap, but when Carnap moved to the US he switched to Charles Loewner, who would eventually become his thesis advisor. In Prague, he lived with an aunt, and married his wife Mary (née Kagan) whom he had met in elementary school and who had followed him from Riga. Having applied for postdoctoral studies in Paris, he was given a visa to go to France soon after the Munich Agreement, in which Nazi Germany annexed Czechoslovakia. He and his wife Mary had a daughter in Paris. They were unable to obtain a visa there to emigrate to the US, as the Latvian quota had filled, so they escaped to the south of France ten days before the fall of Paris, and eventually obtained an emergency US visa in Marseilles, one of a group of 10,000 visas set aside for political refugees by Eleanor Roosevelt. The Bers family rejoined Bers' mother, who had by then moved to New York City and become a psychoanalyst, married to thespian Beno Tumarin. At this time, Bers worked for a small Yiddish research agency, YIVO.\n\nBers spent World War II teaching mathematics as a research associate at Brown University, where he was joined by Loewner. After the war, Bers found an assistant professorship at Syracuse University (1945–1951), before moving to New York University (1951–1964) and then Columbia University (1964–1982), where he became the Davies Professor of Mathematics, and where he chaired the mathematics department from 1972 to 1975. His move to NYU coincided with a move of his family to New Rochelle, New York, where he joined a small community of émigré mathematicians. He was a visiting scholar at the Institute for Advanced Study in 1949–51. He was a Vice-President (1963–65) and a President (1975–77) of the American Mathematical Society, chaired the Division of Mathematical Sciences of the United States National Research Council from 1969 to 1971, chaired the U.S. National Committee on Mathematics from 1977 to 1981, and chaired the Mathematics Section of the National Academy of Sciences from 1967 to 1970.\n\nLate in his life, Bers suffered from Parkinson's disease and strokes. He died on October 29, 1993.\n\nBers' doctoral work was on the subject of potential theory. While in Paris, he worked on Green's function and on integral representations. After first moving to the US, while working for YIVO, he researched Yiddish mathematics textbooks rather than pure mathematics.\n\nAt Brown, he began working on problems of fluid dynamics, and in particular on the two-dimensional subsonic flows associated with cross-sections of airfoils. At this time, he began his work with Abe Gelbart on what would eventually develop into the theory of pseudoanalytic functions. Through the 1940s and 1950s he continued to develop this theory, and to use it to study the planar elliptic partial differential equations associated with subsonic flows. Another of his major results in this time concerned the singularities of the partial differential equations defining minimal surfaces. Bers proved an extension of Riemann's theorem on removable singularities, showing that any isolated singularity of a pencil of minimal surfaces can be removed; he spoke on this result at the 1950 International Congress of Mathematicians and published it in \"Annals of Mathematics\".\n\nLater, beginning with his visit to the Institute for Advanced Study, Bers \"began\na ten-year odyssey that took him from pseudoanalytic functions and elliptic equations to quasiconformal mappings, Teichmüller theory, and\nKleinian groups\". With Lars Ahlfors, he solved the \"moduli problem\", of finding a holomorphic parameterization of the Teichmüller space, each point of which represents a compact Riemann surface of a given genus. During this period he also coined the popular phrasing of a question on eigenvalues of planar domains, \"Can one hear the shape of a drum?\", used as an article title by Mark Kac in 1966 and finally answered negatively in 1992 by an academic descendant of Bers. In the late 1950s, by way of adding a coda to his earlier work, Bers wrote several major retrospectives of flows, pseudoanalytic functions, fixed point methods, Riemann surface theory prior to his work on moduli, and the theory of several complex variables. In 1958, he presented his work on Riemann surfaces in a second talk at the International Congress of Mathematicians.\n\nBers' work on the parameterization of Teichmüller space led him in the 1960s to consider the boundary of the parameterized space, whose points corresponded to new types of Kleinian groups, eventually to be called singly-degenerate Kleinian groups. He applied Eichler cohomology, previously developed for applications in number theory and the theory of Lie groups, to Kleinian groups. He proved the Bers area inequality, an area bound for hyperbolic surfaces that became a two-dimensional precursor to William Thurston's work on geometrization of 3-manifolds and 3-manifold volume, and in this period Bers himself also studied the continuous symmetries of hyperbolic 3-space.\n\nQuasi-Fuchsian groups may be mapped to a pair of Riemann surfaces by taking the quotient by the group of one of the two connected components of the complement of the group's limit set; fixing the image of one of these two maps leads to a subset of the space of Kleinian groups called a Bers slice. In 1970, Bers conjectured that the singly degenerate Kleinian surface groups can be found on the boundary of a Bers slice; this statement, known as the Bers density conjecture, was finally proven by Namazi, Souto, and Ohshika in 2010 and 2011. The Bers compactification of Teichmüller space also dates to this period.\n\nOver the course of his career, Bers advised approximately 50 doctoral students, among them Enrico Arbarello, Irwin Kra, Linda Keen, Murray H. Protter, and Lesley Sibner. Approximately a third of Bers' doctoral students were women, a high proportion for mathematics. Having felt neglected by his own advisor, Bers met regularly for meals with his students and former students, maintained a keen interest in their personal lives as well as their professional accomplishments, and kept up a friendly competition with Lars Ahlfors over who could bring to larger number of academic descendants to mathematical gatherings.\n\nAs a small child with his mother in Saint Petersburg, Bers had cheered the Russian Revolution and the rise of the Soviet Union, but by the late 1930s he had become disillusioned with communism after the assassination of Sergey Kirov and Stalin's ensuing purges. His son Victor later said that \"His experiences in Europe motivated his activism in the human rights movement,\" and Bers himself attributed his interest in human rights to the legacy of Menshevik leader Julius Martov. He founded the Committee on Human Rights of the National Academy of Sciences, and beginning in the 1970s worked to allow the emigration of dissident soviet mathematicians including Yuri Shikhanovich, Leonid Plyushch, Valentin Turchin, and David and Gregory Chudnovsky. Within the U.S., he also opposed the American involvement in the Vietnam War and southeast Asia, and the maintenance of the U.S. nuclear arsenal during the Cold War.\n\nIn 1961, Bers was elected a Fellow of the American Academy of Arts and Sciences, and in 1965 he became a Fellow of the American Association for the Advancement of Science. He joined the National Academy of Sciences in 1964. He was a member of the Finnish Academy of Sciences, and the American Philosophical Society. He received the AMS Leroy P. Steele Prize for mathematical exposition in 1975 for his paper \"Uniformization, moduli, and Kleinian groups\". In 1986, the New York Academy of Sciences gave him their Human Rights Award. In the early 1980s, the Association for Women in Mathematics held a symposium to honor Bers' accomplishments in mentoring women mathematicians.\n\n\n"}
{"id": "1707754", "url": "https://en.wikipedia.org/wiki?curid=1707754", "title": "List of NP-complete problems", "text": "List of NP-complete problems\n\nThis is a list of some of the more commonly known problems that are NP-complete when expressed as decision problems. As there are hundreds of such problems known, this list is in no way comprehensive. Many problems of this type can be found in .\n\nGraphs occur frequently in everyday applications. Examples include biological or social networks, which contain hundreds, thousands and even billions of nodes in some cases (e.g. Facebook or LinkedIn). \n\n\n\n\n\nGeneral\n\nSpecific problems\n\n"}
{"id": "26994422", "url": "https://en.wikipedia.org/wiki?curid=26994422", "title": "List of algebraic constructions", "text": "List of algebraic constructions\n\nAn algebraic construction is a method by which an algebraic entity is defined or derived from another.\n\nInstances include:\n\n"}
{"id": "396014", "url": "https://en.wikipedia.org/wiki?curid=396014", "title": "Local zeta-function", "text": "Local zeta-function\n\nIn number theory, the local zeta function formula_1 (sometimes called the congruent zeta function) is defined as\n\nwhere formula_3 is the number of points of formula_4 defined over the degree formula_5 extension formula_6 of formula_7, and formula_4 is a non-singular formula_9-dimensional projective algebraic variety over the field formula_7 with formula_11 elements. By the variable transformation formula_12, then it is defined by\n\nas the formal power series of the variable formula_14.\n\nEquivalently, the local zeta function sometimes is defined as follows: \n\nIn other word, the local zeta function formula_17 with coefficients in the finite field formula_7 is defined as a function whose logarithmic derivative generates the numbers formula_3 of the solutions of equation, defining formula_4, in the \"m\" degree extension formula_6.\n\nGiven a finite field \"F\", there is, up to isomorphism, just one field \"F\" with\n\nfor \"k\" = 1, 2, ... . Given a set of polynomial equations — or an algebraic variety \"V\" — defined over \"F\", we can count the number\n\nof solutions in \"F\" and create the generating function\n\nThe correct definition for \"Z\"(\"t\") is to make log \"Z\" equal to \"G\", and so\n\nwe will have \"Z\"(0) = 1 since \"G\"(0) = 0, and \"Z\"(\"t\") is \"a priori\" a formal power series.\n\nNote that the logarithmic derivative\n\nequals the generating function\n\nFor example, assume all the \"N\" are 1; this happens for example if we start with an equation like \"X\" = 0, so that geometrically we are taking \"V\" a point. Then\n\nis the expansion of a logarithm (for |\"t\"| < 1). In this case we have\n\nTo take something more interesting, let \"V\" be the projective line over \"F\". If \"F\" has \"q\" elements, then this has \"q\" + 1 points, including as we must the one point at infinity. Therefore, we shall have\n\nand\n\nfor |\"t\"| small enough.\n\nIn this case we have\n\nThe first study of these functions was in the 1923 dissertation of Emil Artin. He obtained results for the case of hyperelliptic curve, and conjectured the further main points of the theory as applied to curves. The theory was then developed by F. K. Schmidt and Helmut Hasse. The earliest known non-trivial cases of local zeta-functions were implicit in Carl Friedrich Gauss's \"Disquisitiones Arithmeticae\", article 358; there certain particular examples of elliptic curves over finite fields having complex multiplication have their points counted by means of cyclotomy.\n\nFor the definition and some examples, see also.\n\nThe relationship between the definitions of \"G\" and \"Z\" can be explained in a number of ways. (See for example the infinite product formula for \"Z\" below.) In practice it makes \"Z\" a rational function of \"t\", something that is interesting even in the case of \"V\" an elliptic curve over finite field.\n\nIt is the functions \"Z\" that are designed to multiply, to get global zeta functions. Those involve different finite fields (for example the whole family of fields Z/\"p\"Z as \"p\" runs over all prime numbers). In that connection, the variable \"t\" undergoes substitution by \"p\", where \"s\" is the complex variable traditionally used in Dirichlet series. (For details see Hasse-Weil zeta-function.)\n\nWith that understanding, the products of the \"Z\" in the two cases used as examples come out as formula_33 and formula_34.\n\nFor projective curves \"C\" over \"F\" that are non-singular, it can be shown that\n\nwith \"P\"(\"t\") a polynomial, of degree 2\"g\" where \"g\" is the genus of \"C\". Rewriting\n\nthe Riemann hypothesis for curves over finite fields states\n\nFor example, for the elliptic curve case there are two roots, and it is easy to show the absolute values of the roots are \"q\". Hasse's theorem is that they have the same absolute value; and this has immediate consequences for the number of points.\n\nAndré Weil proved this for the general case, around 1940 (\"Comptes Rendus\" note, April 1940): he spent much time in the years after that writing up the algebraic geometry involved. This led him to the general Weil conjectures, Alexander Grothendieck developed the scheme theory for the sake of resolving it and finally, Pierre Deligne had proved a generation later. See étale cohomology for the basic formulae of the general theory.\n\nIt is a consequence of the Lefschetz trace formula for the Frobenius morphism that\n\nHere formula_39 is a separated scheme of finite type over the finite field \"F\" with formula_11 elements, and Frob is the geometric Frobenius acting on formula_41-adic étale cohomology with compact supports of formula_42, the lift of formula_39 to the algebraic closure of the field \"F\". This shows that the zeta function is a rational function of formula_44.\n\nAn infinite product formula for formula_45 is\n\nHere, the product ranges over all closed points \"x\" of \"X\" and deg(\"x\") is the degree of \"x\".\nThe local zeta function \"Z(X, t)\" is viewed as a function of the complex variable \"s\" via the change of \nvariables \"q\".\n\nIn the case where \"X\" is the variety \"V\" discussed above, the closed points \nare the equivalence classes \"x=[P]\" of points \"P\" on formula_47, where two points are equivalent if they are conjugates over \"F\". The degree of \"x\" is the degree of the field extension of \"F\"\ngenerated by the coordinates of \"P\". The logarithmic derivative of the infinite product \"Z(X, t)\" is easily seen to be the generating function discussed above, namely\n\n"}
{"id": "19453961", "url": "https://en.wikipedia.org/wiki?curid=19453961", "title": "Mathematical object", "text": "Mathematical object\n\nA mathematical object is an abstract object arising in mathematics. The concept is studied in philosophy of mathematics.\n\nIn mathematical practice, an \"object\" is anything that has been (or could be) formally defined, and with which one may do deductive reasoning and mathematical proofs. Commonly encountered mathematical objects include numbers, permutations, partitions, matrices, sets, functions, and relations. Geometry as a branch of mathematics has such objects as hexagons, points, lines, triangles, circles, spheres, polyhedra, topological spaces and manifolds. Another branch—algebra—has groups, rings, fields, group-theoretic lattices, and order-theoretic lattices. Categories are simultaneously homes to mathematical objects and mathematical objects in their own right. In proof theory, proofs and theorems are also mathematical objects.\n\nThe ontological status of mathematical objects has been the subject of much investigation and debate by philosophers of mathematics.\n\nOne view that emerged around the turn of the 20th century with the work of Cantor is that all mathematical objects can be defined as sets. The set {0,1} is a relatively clear-cut example. On the face of it the group Z of integers mod 2 is also a set with two elements. However, it cannot simply be the set {0,1}, because this does not mention the additional structure imputed to Z by the operations of addition and negation mod 2: how are we to tell which of 0 or 1 is the additive identity, for example? To organize this group as a set it can first be coded as the quadruple ({0,1},+,−,0), which in turn can be coded using one of several conventions as a set representing that quadruple, which in turn entails encoding the operations + and − and the constant 0 as sets.\n\nSets may include ordered denotation of the particular identities and operations that apply to them, indicating a group, abelian group, ring, field, or other mathematical object. These types of mathematical objects are commonly studied in abstract algebra.\n\nIf, however, the goal of mathematical ontology is taken to be the internal consistency of mathematics, it is more important that mathematical objects be definable in some uniform way (for example, as sets) regardless of actual practice, in order to lay bare the essence of its paradoxes. This has been the viewpoint taken by foundations of mathematics, which has traditionally accorded the management of paradox higher priority than the faithful reflection of the details of mathematical practice as a justification for defining mathematical objects to be sets.\n\nMuch of the tension created by this foundational identification of mathematical objects with sets can be relieved without unduly compromising the goals of foundations by allowing two kinds of objects into the mathematical universe, sets and relations, without requiring that either be considered merely an instance of the other. These form the basis of model theory as the domain of discourse of predicate logic. From this viewpoint, mathematical objects are entities satisfying the axioms of a formal theory expressed in the language of predicate logic.\n\nA variant of this approach replaces relations with operations, the basis of universal algebra. In this variant the axioms often take the form of equations, or implications between equations.\n\nA more abstract variant is category theory, which abstracts sets as objects and the operations thereon as morphisms between those objects. At this level of abstraction mathematical objects reduce to mere vertices of a graph whose edges as the morphisms abstract the ways in which those objects can transform and whose structure is encoded in the composition law for morphisms. Categories may arise as the models of some axiomatic theory and the homomorphisms between them (in which case they are usually concrete, meaning equipped with a faithful forgetful functor to the category Set or more generally to a suitable topos), or they may be constructed from other more primitive categories, or they may be studied as abstract objects in their own right without regard for their provenance.\n\n\n\n"}
{"id": "8410911", "url": "https://en.wikipedia.org/wiki?curid=8410911", "title": "Mian–Chowla sequence", "text": "Mian–Chowla sequence\n\nIn mathematics, the Mian–Chowla sequence is an integer sequence defined\nrecursively in the following way. The sequence starts with\n\nThen for formula_2, formula_3 is the smallest integer such that every pairwise sum\n\nis distinct, for all formula_5 and formula_6 less than or equal to formula_7.\n\nInitially, with formula_8, there is only one pairwise sum, 1 + 1 = 2. The next term in the sequence, formula_9, is 2 since the pairwise sums then are 2, 3 and 4, i.e., they are distinct. Then, formula_10 can't be 3 because there would be the non-distinct pairwise sums 1 + 3 = 2 + 2 = 4. We find then that formula_11, with the pairwise sums being 2, 3, 4, 5, 6 and 8. The sequence thus begins \n\nIf we define formula_12, the resulting sequence is the same except each term is one less (that is, 0, 1, 3, 7, 12, 20, 30, 44, 65, 80, 96, ... ).\n\nThe sequence was invented by Abdul Majid Mian and Sarvadaman Chowla.\n\n"}
{"id": "4716117", "url": "https://en.wikipedia.org/wiki?curid=4716117", "title": "Model transformation", "text": "Model transformation\n\nA model transformation, in model-driven engineering, is an automated way of modifying and creating models. An example use of model transformation is ensuring that a family of models is consistent, in a precise sense which the software engineer can define. The aim of using a model transformation is to save effort and reduce errors by automating the building and modification of models where possible.\n\nModel transformations can be thought of as programs that take models as input. There is a wide variety of kinds of model transformation and uses of them, which differ in their inputs and outputs and also in the way they are expressed.\n\nA model transformation usually specifies which models are acceptable as input, and if appropriate what models it may produce as output, by specifying the metamodel to which a model must conform.\n\nModel transformations and languages for them have been classified in many ways.\nSome of the more common distinctions drawn are:\n\nIn principle a model transformation may have many inputs and outputs of various types; the only absolute limitation is that a model transformation will take at least one model as input. However, a model transformation that did not produce any model as output would more commonly be called a model analysis or model query.\n\nEndogenous transformations are transformations between models expressed in the same language. Exogenous transformations are transformations between models expressed using different languages. For example, in a process conforming to the OMG Model Driven Architecture, a platform-independent model might be transformed into a platform-specific model by an exogenous model transformation.\n\nA unidirectional model transformation has only one mode of execution: that is, it always takes the same type of input and produces the same type of output. Unidirectional model transformations are useful in compilation-like situations, where any output model is read-only. The relevant notion of consistency is then very simple: the input model is consistent with the model that the transformation would produce as output, only.\n\nFor a bidirectional model transformation, the same type of model can sometimes be input and other times be output. Bidirectional transformations are necessary in situations where people are working on more than one model and the models must be kept consistent. Then a change to either model might necessitate a change to the other, in order to maintain consistency between the models. Because each model can incorporate information which is not reflected in the other, there may be many models which are consistent with a given model. Important special cases are:\n\n\nIt is particularly important that a bidirectional model transformation has appropriate properties to make it behave sensibly: for example, not making changes unnecessarily, or discarding deliberately made changes.\n\nA model transformation may be written in a general purpose programming language, but specialised model transformation languages are also available. Bidirectional transformations, in particular, are best written in a language that ensures the directions are appropriately related. The OMG-standardised model transformation languages are collectively known as QVT.\n\nIn some model transformation languages, for example the QVT languages, a model transformation is itself a model, that is, it conforms to a metamodel which is part of the model transformation language's definition. This facilitates the definition of Higher Order Transformations (HOTs), i.e. transformations which have other transformations as input and/or output.\n\n\n"}
{"id": "5442846", "url": "https://en.wikipedia.org/wiki?curid=5442846", "title": "Multiple rule-based problems", "text": "Multiple rule-based problems\n\nMultiple rule-based problems are problems containing various conflicting rules and restrictions. Such problems typically have an \"optimal\" solution, found by striking a balance between the various restrictions, without directly defying any of the aforementioned restrictions.\n\nSolutions to such problems can either require complex, non-linear thinking processes, or can instead require mathematics-based solutions in which an optimal solution is found by setting the various restrictions as equations, and finding an appropriate maximum value when all equations are added. These problems may thus require more working information as compared to causal relationship problem solving or single rule-based problem solving. The multiple rule-based problem solving is more likely to increase cognitive load than are the other two types of problem solving.\n"}
{"id": "52947580", "url": "https://en.wikipedia.org/wiki?curid=52947580", "title": "Nauruan navigational system", "text": "Nauruan navigational system\n\nThe Nauruan navigational system is a way of expressing direction, similar to North, South, East and West, but limitations in the system mean that it is unable to be used outside of Nauru.\n\nThe four main directions are pago, poe, pawa and pwiju (pwijiuw). Other directions include Gankoro and Arijeijen.\n"}
{"id": "5783978", "url": "https://en.wikipedia.org/wiki?curid=5783978", "title": "Paranormal space", "text": "Paranormal space\n\nIn mathematics, in the realm of topology, a paranormal space is a topological space in which every countable discrete collection of closed sets has a locally finite open expansion.\n\n\n"}
{"id": "2128500", "url": "https://en.wikipedia.org/wiki?curid=2128500", "title": "Pascal's rule", "text": "Pascal's rule\n\nIn mathematics, Pascal's rule is a combinatorial identity about binomial coefficients. It states that for any natural number \"n\" we have\n\nwhere formula_2 is a binomial coefficient. This is also commonly written\n\nPascal's rule has an intuitive combinatorial meaning. Recall that formula_4 counts in how many ways can we pick a subset with \"b\" elements out from a set with \"a\" elements. Therefore, the right side of the identity formula_2 is counting how many ways can we get a \"k\"-subset out from a set with \"n\" elements.\n\nNow, suppose you distinguish a particular element 'X' from the set with \"n\" elements. Thus, every time you choose \"k\" elements to form a subset there are two possibilities: \"X\" belongs to the chosen subset or not.\n\nIf \"X\" is in the subset, you only really need to choose \"k\" − 1 more objects (since it is known that \"X\" will be in the subset) out from the remaining \"n\" − 1 objects. This can be accomplished in formula_6 ways.\n\nWhen \"X\" is not in the subset, you need to choose all the \"k\" elements in the subset from the \"n\" − 1 objects that are not \"X\". This can be done in formula_7 ways.\n\nWe conclude that the numbers of ways to get a \"k\"-subset from the \"n\"-set, which we know is formula_2, is also the number\nformula_9\n\nSee also Bijective proof.\n\nWe need to show\n\nLet formula_12 and formula_13. Then\n\n\n\n"}
{"id": "326298", "url": "https://en.wikipedia.org/wiki?curid=326298", "title": "Power center (geometry)", "text": "Power center (geometry)\n\nIn geometry, the power center of three circles, also called the radical center, is the intersection point of the three radical axes of the pairs of circles. If the radical center lies outside of all three circles, then it is the center of the unique circle (the radical circle) that intersects the three given circles orthogonally; the construction of this orthogonal circle corresponds to Monge's problem. This is a special case of the three conics theorem.\n\nThe three radical axes meet in a single point, the radical center, for the following reason. The radical axis of a pair of circles is defined as the set of points that have equal power \"h\" with respect to both circles. For example, for every point P on the radical axis of circles 1 and 2, the powers to each circle are equal, \"h\" = \"h\". Similarly, for every point on the radical axis of circles 2 and 3, the powers must be equal, \"h\" = \"h\". Therefore, at the intersection point of these two lines, all three powers must be equal, \"h\" = \"h\" = \"h\". Since this implies that \"h\" = \"h\", this point must also lie on the radical axis of circles 1 and 3. Hence, all three radical axes pass through the same point, the radical center.\n\nThe radical center has several applications in geometry. It has an important role in a solution to Apollonius' problem published by Joseph Diaz Gergonne in 1814. In the power diagram of a system of circles, all of the vertices of the diagram are located at radical centers of triples of circles. The Spieker center of a triangle is the radical center of its excircles. Several types of radical circles have been defined as well, such as the radical circle of the Lucas circles.\n\n\n"}
{"id": "1557634", "url": "https://en.wikipedia.org/wiki?curid=1557634", "title": "Propositional formula", "text": "Propositional formula\n\nIn propositional logic, a propositional formula is a type of syntactic formula which is well formed and has a truth value. If the values of all variables in a propositional formula are given, it determines a unique truth value. A propositional formula may also be called a propositional expression, a sentence, or a sentential formula.\n\nA propositional formula is constructed from simple propositions, such as \"five is greater than three\" or propositional variables such as \"P\" and \"Q\", using connectives such as NOT, AND, OR, or IMPLIES; for example:\n\nIn mathematics, a propositional formula is often more briefly referred to as a \"proposition\", but, more precisely, a propositional formula is not a proposition but a formal expression that \"denotes\" a proposition, a formal object under discussion, just like an expression such as \"\" is not a value, but denotes a value. In some contexts, maintaining the distinction may be of importance.\n\nFor the purposes of the propositional calculus, propositions (utterances, sentences, assertions) are considered to be either simple or compound. Compound propositions are considered to be linked by sentential connectives, some of the most common of which are \"AND\", \"OR\", \"IF … THEN …\", \"NEITHER … NOR …\", \"… IS EQUIVALENT TO …\" . The linking semicolon \";\", and connective \"BUT\" are considered to be expressions of \"AND\". A sequence of discrete sentences are considered to be linked by \"AND\"s, and formal analysis applies a recursive \"parenthesis rule\" with respect to sequences of simple propositions (see more below about well-formed formulas).\n\nSimple propositions are declarative in nature, that is, they make assertions about the condition or nature of a \"particular\" object of sensation e.g. \"This cow is blue\", \"There's a coyote!\" (\"That coyote IS \"there\", behind the rocks.\"). Thus the simple \"primitive\" assertions must be about specific objects or specific states of mind. Each must have at least a subject (an immediate object of thought or observation), a verb (in the active voice and present tense preferred), and perhaps an adjective or adverb. \"Dog!\" probably implies \"I see a dog\" but should be rejected as too ambiguous.\n\nFor the purposes of the propositional calculus a compound proposition can usually be reworded into a series of simple sentences, although the result will probably sound stilted.\n\nThe predicate calculus goes a step further than the propositional calculus to an \"analysis of the \"inner structure\" of propositions\" It breaks a simple sentence down into two parts (i) its subject (the object (singular or plural) of discourse) and (ii) a predicate (a verb or possibly verb-clause that asserts a quality or attribute of the object(s)). The predicate calculus then generalizes the \"subject|predicate\" form (where | symbolizes concatenation (stringing together) of symbols) into a form with the following blank-subject structure \" ___|predicate\", and the predicate in turn generalized to all things with that property.\n\nThe generalization of \"this pig\" to a (potential) member of two classes \"winged things\" and \"blue things\" means that it has a truth-relationship with both of these classes. In other words, given a domain of discourse \"winged things\", either we find p to be a member of this domain or not. Thus we have a relationship W (wingedness) between p (pig) and { T, F }, W(p) evaluates to { T, F } where { T, F } is the set of the boolean values \"true\" and \"false\". Likewise for B (blueness) and p (pig) and { T, F }: B(p) evaluates to { T, F }. So we now can analyze the connected assertions \"B(p) AND W(p)\" for its overall truth-value, i.e.:\n\nIn particular, simple sentences that employ notions of \"all\", \"some\", \"a few\", \"one of\", etc. are treated by the predicate calculus. Along with the new function symbolism \"F(x)\" two new symbols are introduced: ∀ (For all), and ∃ (There exists …, At least one of … exists, etc.). The predicate calculus, but not the propositional calculus, can establish the formal validity of the following statement:\n\nTarski asserts that the notion of IDENTITY (as distinguished from LOGICAL EQUIVALENCE) lies outside the propositional calculus; however, he notes that if a logic is to be of use for mathematics and the sciences it must contain a \"theory\" of IDENTITY. Some authors refer to \"predicate logic with identity\" to emphasize this extension. See more about this below.\n\nAn algebra (and there are many different ones), loosely defined, is a method by which a collection of symbols called variables together with some other symbols such as parentheses (, ) and some sub-set of symbols such as *, +, ~, &, ∨, =, ≡, ∧, ￢ are manipulated within a system of rules. These symbols, and well-formed strings of them, are said to represent objects, but in a specific algebraic system these objects do not have meanings. Thus work inside the algebra becomes an exercise in obeying certain laws (rules) of the algebra's syntax (symbol-formation) rather than in semantics (meaning) of the symbols. The meanings are to be found outside the algebra.\n\nFor a well-formed sequence of symbols in the algebra —a formula— to have some usefulness outside the algebra the symbols are assigned meanings and eventually the variables are assigned values; then by a series of rules the formula is evaluated.\n\nWhen the values are restricted to just two and applied to the notion of simple sentences (e.g. spoken utterances or written assertions) linked by propositional connectives this whole algebraic system of symbols and rules and evaluation-methods is usually called the propositional calculus or the sentential calculus.\n\nWhile some of the familiar rules of arithmetic algebra continue to hold in the algebra of propositions (e.g. the commutative and associative laws for AND and OR), some do not (e.g. the distributive laws for AND, OR and NOT).\n\nAnalysis: In deductive reasoning, philosophers, rhetoricians and mathematicians reduce arguments to formulas and then study them (usually with truth tables) for correctness (soundness). For example: Is the following argument sound?\n\nEngineers analyze the logic circuits they have designed using synthesis techniques and then apply various reduction and minimization techniques to simplify their designs.\n\nSynthesis: Engineers in particular synthesize propositional formulas (that eventually end up as circuits of symbols) from truth tables. For example, one might write down a truth table for how binary addition should behave given the addition of variables \"b\" and \"a\" and \"carry_in\" \"ci\", and the results \"carry_out\" \"co\" and \"sum\" Σ:\nThe simplest type of propositional formula is a propositional variable. Propositions that are simple (atomic), symbolic expressions are often denoted by variables named \"a\", \"b\", or \"A\", \"B\", etc. A propositional variable is intended to represent an atomic proposition (assertion), such as \"It is Saturday\" = \"a\" (here the symbol = means \" … is assigned the variable named …\") or \"I only go to the movies on Monday\" = \"b\".\n\nEvaluation of a propositional formula begins with assignment of a truth value to each variable. Because each variable represents a simple sentence, the truth values are being applied to the \"truth\" or \"falsity\" of these simple sentences.\n\nTruth values in rhetoric, philosophy and mathematics: The truth values are only two: { TRUTH \"T\", FALSITY \"F\" }. An empiricist puts all propositions into two broad classes: \"analytic\"—true no matter what (e.g. tautology), and \"synthetic\"—derived from experience and thereby susceptible to confirmation by third parties (the verification theory of meaning). Empiricits hold that, in general, to arrive at the truth-value of a synthetic proposition, meanings (pattern-matching templates) must first be applied to the words, and then these meaning-templates must be matched against whatever it is that is being asserted. For example, my utterance \"That cow is \"\"!\" Is this statement a TRUTH? Truly I said it. And maybe I \"am\" seeing a blue cow—unless I am lying my statement is a TRUTH relative to the object of my (perhaps flawed) perception. But is the blue cow \"really there\"? What do you see when you look out the same window? In order to proceed with a verification, you will need a prior notion (a template) of both \"cow\" and \"\", and an ability to match the templates against the object of sensation (if indeed there is one).\n\nTruth values in engineering: Engineers try to avoid notions of truth and falsity that bedevil philosophers, but in the final analysis engineers must trust their measuring instruments. In their quest for robustness, engineers prefer to pull known objects from a small library—objects that have well-defined, predictable behaviors even in large combinations, (hence their name for the propositional calculus: \"combinatorial logic\"). The fewest behaviors of a single object are two (e.g. { OFF, ON }, { open, shut }, { UP, DOWN } etc.), and these are put in correspondence with { 0, 1 }. Such elements are called digital; those with a continuous range of behaviors are called analog. Whenever decisions must be made in an analog system, quite often an engineer will convert an analog behavior (the door is 45.32146% UP) to digital (e.g. DOWN=0 ) by use of a comparator.\n\nThus an assignment of meaning of the variables and the two value-symbols { 0, 1 } comes from \"outside\" the formula that represents the behavior of the (usually) compound object. An example is a garage door with two \"limit switches\", one for UP labelled SW_U and one for DOWN labelled SW_D, and whatever else is in the door's circuitry. Inspection of the circuit (either the diagram or the actual objects themselves—door, switches, wires, circuit board, etc.) might reveal that, on the circuit board \"node 22\" goes to +0 volts when the contacts of switch \"SW_D\" are mechanically in contact (\"closed\") and the door is in the \"down\" position (95% down), and \"node 29\" goes to +0 volts when the door is 95% UP and the contacts of switch SW_U are in mechanical contact (\"closed\"). The engineer must define the meanings of these voltages and all possible combinations (all 4 of them), including the \"bad\" ones (e.g. both nodes 22 and 29 at 0 volts, meaning that the door is open and closed at the same time). The circuit mindlessly responds to whatever voltages it experiences without any awareness of TRUTH or FALSEHOOD, RIGHT or WRONG, SAFE or DANGEROUS.\n\nArbitrary propositional formulas are built from propositional variables and other propositional formulas using propositional connectives. Examples of connectives include:\n\nThe following are the connectives common to rhetoric, philosophy and mathematics together with their truth tables. The symbols used will vary from author to author and between fields of endeavor. In general the abbreviations \"T\" and \"F\" stand for the evaluations TRUTH and FALSITY applied to the variables in the propositional formula (e.g. the assertion: \"That cow is blue\" will have the truth-value \"T\" for Truth or \"F\" for Falsity, as the case may be.).\n\nThe connectives go by a number of different word-usages, e.g. \"a IMPLIES b\" is also said \"IF a THEN b\". Some of these are shown in the table.\n\nIn general, the engineering connectives are just the same as the mathematics connectives excepting they tend to evaluate with \"1\" = \"T\" and \"0\" = \"F\". This is done for the purposes of analysis/minimization and synthesis of formulas by use of the notion of \"minterms\" and Karnaugh maps (see below). Engineers also use the words logical product from Boole's notion (a*a = a) and logical sum from Jevons' notion (a+a = a).\n\nThe IF … THEN … ELSE … connective appears as the simplest form of CASE operator of recursion theory and computation theory and is the connective responsible for conditional goto's (jumps, branches). From this one connective all other connectives can be constructed (see more below). Although \" IF c THEN b ELSE a \" sounds like an implication it is, in its most reduced form, a switch that makes a decision and offers as outcome only one of two alternatives \"a\" or \"b\" (hence the name switch statement in the C programming language).\n\nThe following three propositions are equivalent (as indicated by the logical equivalence sign ≡ ):\n\n\nThus IF … THEN … ELSE—unlike implication—does not evaluate to an ambiguous \"TRUTH\" when the first proposition is false i.e. c = F in (c → b). For example, most people would reject the following compound proposition as a nonsensical \"non sequitur\" because the second sentence is \"not connected in meaning\" to the first.\n\nIn recognition of this problem, the sign → of formal implication in the propositional calculus is called material implication to distinguish it from the everyday, intuitive implication.\n\nThe use of the IF … THEN … ELSE construction avoids controversy because it offers a completely deterministic choice between two stated alternatives; it offers two \"objects\" (the two alternatives b and a), and it \"selects\" between them exhaustively and unambiguously. In the truth table below, d1 is the formula: ( (IF c THEN b) AND (IF NOT-c THEN a) ). Its fully reduced form d2 is the formula: ( (c AND b) OR (NOT-c AND a). The two formulas are equivalent as shown by the columns \"=d1\" and \"=d2\". Electrical engineers call the fully reduced formula the AND-OR-SELECT operator. The CASE (or SWITCH) operator is an extension of the same idea to \"n\" possible, but mutually exclusive outcomes. Electrical engineers call the CASE operator a multiplexer.\n\nThe first table of this section stars *** the entry logical equivalence to note the fact that \"Logical equivalence\" is not the same thing as \"identity\". For example, most would agree that the assertion \"That cow is blue\" is identical to the assertion \"That cow is blue\". On the other hand, \"logical\" equivalence sometimes appears in speech as in this example: \" 'The sun is shining' means 'I'm biking' \" Translated into a propositional formula the words become: \"IF 'the sun is shining' THEN 'I'm biking', AND IF 'I'm biking' THEN 'the sun is shining'\":\n\nDifferent authors use different signs for logical equivalence: ↔ (e.g. Suppes, Goodstein, Hamilton), ≡ (e.g. Robbin), ⇔ (e.g. Bender and Williamson). Typically identity is written as the equals sign =. One exception to this rule is found in \"Principia Mathematica\". For more about the philosophy of the notion of IDENTITY see Leibniz's law.\n\nAs noted above, Tarski considers IDENTITY to lie outside the propositional calculus, but he asserts that without the notion, \"logic\" is insufficient for mathematics and the deductive sciences. In fact the sign comes into the propositional calculus when a formula is to be evaluated.\n\nIn some systems there are no truth tables, but rather just formal axioms (e.g. strings of symbols from a set { ~, →, (, ), variables p, p, p, … } and formula-formation rules (rules about how to make more symbol strings from previous strings by use of e.g. substitution and modus ponens). the result of such a calculus will be another formula (i.e. a well-formed symbol string). Eventually, however, if one wants to use the calculus to study notions of validity and truth, one must add axioms that define the behavior of the symbols called \"the truth values\" {T, F} ( or {1, 0}, etc.) relative to the other symbols.\n\nFor example, Hamilton uses two symbols = and ≠ when he defines the notion of a valuation v of any wffs \"A\" and \"B\" in his \"formal statement calculus\" L. A valuation v is a \"function\" from the wffs of his system L to the range (output) { T, F }, given that each variable p, p, p in a wff is assigned an arbitrary truth value { T, F }.\n\nThe two definitions () and () define the equivalent of the truth tables for the ~ (NOT) and → (IMPLICATION) connectives of his system. The first one derives F ≠ T and T ≠ F, in other words \" v(\"A\") does not mean v(~\"A\")\". Definition () specifies the third row in the truth table, and the other three rows then come from an application of definition (). In particular () assigns the value F (or a meaning of \"F\") to the entire expression. The definitions also serve as formation rules that allow substitution of a value previously derived into a formula:\nSome formal systems specify these valuation axioms at the outset in the form of certain formulas such as the law of contradiction or laws of identity and nullity. The choice of which ones to use, together with laws such as commutation and distribution, is up to the system's designer as long as the set of axioms is complete (i.e. sufficient to form and to evaluate any well-formed formula created in the system).\n\nAs shown above, the CASE (IF c THEN b ELSE a ) connective is constructed either from the 2-argument connectives IF … THEN … and AND or from OR and AND and the 1-argument NOT. Connectives such as the n-argument AND (a & b & c & … & n), OR (a ∨ b ∨ c ∨ … ∨ n) are constructed from strings of two-argument AND and OR and written in abbreviated form without the parentheses. These, and other connectives as well, can then used as building blocks for yet further connectives. Rhetoricians, philosophers, and mathematicians use truth tables and the various theorems to analyze and simplify their formulas.\n\nElectrical engineering uses drawn symbols and connect them with lines that stand for the mathematicals act of substitution and replacement. They then verify their drawings with truth tables and simplify the expressions as shown below by use of Karnaugh maps or the theorems. In this way engineers have created a host of \"combinatorial logic\" (i.e. connectives without feedback) such as \"decoders\", \"encoders\", \"mutifunction gates\", \"majority logic\", \"binary adders\", \"arithmetic logic units\", etc.\n\nA definition creates a new symbol and its behavior, often for the purposes of abbreviation. Once the definition is presented, either form of the equivalent symbol or formula can be used. The following symbolism = is following the convention of Reichenbach. Some examples of convenient definitions drawn from the symbol set { ~, &, (, ) } and variables. Each definition is producing a logically equivalent formula that can be used for substitution or replacement.\n\nThe definitions above for OR, IMPLICATION, XOR, and logical equivalence are actually schemas (or \"schemata\"), that is, they are \"models\" (demonstrations, examples) for a general formula \"format\" but shown (for illustrative purposes) with specific letters a, b, c for the variables, whereas any variable letters can go in their places as long as the letter substitutions follow the rule of substitution below.\n\nSubstitution: The variable or sub-formula to be substituted with another variable, constant, or sub-formula must be replaced in all instances throughout the overall formula.\n\nReplacement: (i) the formula to be replaced must be within a tautology, i.e. \"logically equivalent\" ( connected by ≡ or ↔) to the formula that replaces it, and (ii) unlike substitution its permissible for the replacement to occur only in one place (i.e. for one formula).\n\nThe classical presentation of propositional logic (see Enderton 2002) uses the connectives formula_8. The set of formulas over a given set of propositional variables is inductively defined to be the smallest set of expressions such that:\nThis inductive definition can be easily extended to cover additional connectives.\n\nThe inductive definition can also be rephrased in terms of a closure operation (Enderton 2002). Let \"V\" denote a set of propositional variables and let \"X\" denote the set of all strings from an alphabet including symbols in \"V\", left and right parentheses, and all the logical connectives under consideration. Each logical connective corresponds to a formula building operation, a function from \"XX\" to \"XX\":\nThe set of formulas over \"V\" is defined to be the smallest subset of \"XX\" containing \"V\" and closed under all the formula building operations.\n\nThe following \"laws\" of the propositional calculus are used to \"reduce\" complex formulas. The \"laws\" can be verified easily with truth tables. For each law, the principal (outermost) connective is associated with logical equivalence ≡ or identity =. A complete analysis of all 2 combinations of truth-values for its \"n\" distinct variables will result in a column of 1's (T's) underneath this connective. This finding makes each law, by definition, a tautology. And, for a given law, because its formula on the left and right are equivalent (or identical) they can be substituted for one another.\nEnterprising readers might challenge themselves to invent an \"axiomatic system\" that uses the symbols { ∨, &, ~, (, ), variables a, b, c }, the formation rules specified above, and as few as possible of the laws listed below, and then derive as theorems the others as well as the truth-table valuations for ∨, &, and ~. One set attributed to Huntington (1904) (Suppes:204) uses eight of the laws defined below.\n\nNote that if used in an axiomatic system, the symbols 1 and 0 (or T and F) are considered to be wffs and thus obey all the same rules as the variables. Thus the laws listed below are actually axiom schemas, that is, they stand in place of an infinite number of instances. Thus ( x ∨ y ) ≡ ( y ∨ x ) might be used in one instance, ( p ∨ 0 ) ≡ ( 0 ∨ p ) and in another instance ( 1 ∨ q ) ≡ ( q ∨ 1 ), etc.\n\nIn general, to avoid confusion during analysis and evaluation of propositional formulas make liberal use parentheses. However, quite often authors leave them out. To parse a complicated formula one first needs to know the seniority, or rank, that each of the connectives (excepting *) has over the other connectives. To \"well-form\" a formula, start with the connective with the highest rank and add parentheses around its components, then move down in rank (paying close attention to the connective's scope over which the it is working). From most- to least-senior, with the predicate signs ∀x and ∃x, the IDENTITY = and arithmetic signs added for completeness:\n\nThus the formula can be parsed—but note that, because NOT does not obey the distributive law, the parentheses around the inner formula (~c & ~d) is mandatory:\n\nBoth AND and OR obey the commutative law and associative law:\n\nOmitting parentheses in strings of AND and OR: The connectives are considered to be unary (one-variable, e.g. NOT) and binary (i.e. two-variable AND, OR, IMPLIES). For example:\nHowever, a truth-table demonstration shows that the form without the extra parentheses is perfectly adequate.\n\nOmitting parentheses with regards to a single-variable NOT: While ~(a) where a is a single variable is perfectly clear, ~a is adequate and is the usual way this literal would appear. When the NOT is over a formula with more than one symbol, then the parentheses are mandatory, e.g. ~(a ∨ b).\n\nOR distributes over AND and AND distributes over OR. NOT does not distribute over AND or OR. See below about De Morgan's law:\n\nNOT, when distributed over OR or AND, does something peculiar (again, these can be verified with a truth-table):\n\nAbsorption, in particular the first one, causes the \"laws\" of logic to differ from the \"laws\" of arithmetic:\n\nThe sign \" = \" (as distinguished from logical equivalence ≡, alternately ↔ or ⇔) symbolizes the assignment of value or meaning. Thus the string (a & ~(a)) symbolizes \"1\", i.e. it means the same thing as symbol \"1\" \". In some \"systems\" this will be an axiom (definition) perhaps shown as ( (a & ~(a)) = 1 ); in other systems, it may be derived in the truth table below:\n\n\nA key property of formulas is that they can be uniquely parsed to determine the structure of the formula in terms of its propositional variables and logical connectives. When formulas are written in infix notation, as above, unique readability is ensured through an appropriate use of parentheses in the definition of formulas. Alternatively, formulas can be written in Polish notation or reverse Polish notation, eliminating the need for parentheses altogether.\n\nThe inductive definition of infix formulas in the previous section can be converted to a formal grammar in Backus-Naur form:\nIt can be shown that any expression matched by the grammar has a balanced number of left and right parentheses, and any nonempty initial segment of a formula has more left than right parentheses. This fact can be used to give an algorithm for parsing formulas. For example, suppose that an expression \"x\" begins with formula_23. Starting after the second symbol, match the shortest subexpression \"y\" of \"x\" that has balanced parentheses. If \"x\" is a formula, there is exactly one symbol left after this expression, this symbol is a closing parenthesis, and \"y\" itself is a formula. This idea can be used to generate a recursive descent parser for formulas.\n\nExample of parenthesis counting:\n\nThis method locates as \"1\" the principal connective the connective under which the overall evaluation of the formula occurs for the outer-most parentheses (which are often omitted). It also locates the inner-most connective where one would begin evaluatation of the formula without the use of a truth table, e.g. at \"level 6\".\nThe notion of valid argument is usually applied to inferences in arguments, but arguments reduce to propositional formulas and can be evaluated the same as any other propositional formula. Here a valid inference means: \"The formula that represents the inference evaluates to \"truth\" beneath its principal connective, no matter what truth-values are assigned to its variables\", i.e. the formula is a tautology.\nQuite possibly a formula will be \"well-formed\" but not valid. Another way of saying this is: \"Being well-formed is \"necessary\" for a formula to be valid but it is not \"sufficient\".\" The only way to find out if it is \"both\" well-formed \"and\" valid is to submit it to verification with a truth table or by use of the \"laws\":\n\n\nA set of logical connectives is called complete if every propositional formula is tautologically equivalent to a formula with just the connectives in that set. There are many complete sets of connectives, including formula_24, formula_25, and formula_26. There are two binary connectives that are complete on their own, corresponding to NAND and NOR, respectively. Some pairs are not complete, for example formula_27.\n\nThe binary connective corresponding to NAND is called the Sheffer stroke, and written with a vertical bar | or vertical arrow ↑. The completeness of this connective was noted in \"Principia Mathematica\" (1927:xvii). Since it is complete on its own, all other connectives can be expressed using only the stroke. For example, where the symbol \" ≡ \" represents \"logical equivalence\":\nIn particular, the zero-ary connectives formula_28 (representing truth) and formula_29 (representing falsity) can be expressed using the stroke:\n\nThis connective together with { 0, 1 }, ( or { F, T } or { formula_29, formula_28 } ) forms a complete set. In the following the IF...THEN...ELSE relation (c, b, a) = d represents ( (c → b) ∨ (~c → a) ) ≡ ( (c & b) ∨ (~c & a) ) = d\n\nExample: The following shows how a theorem-based proof of \"(c, b, 1) ≡ (c → b)\" would proceed, below the proof is its truth-table verification. ( Note: (c → b) is \"defined\" to be (~c ∨ b) ):\n\nIn the following truth table the column labelled \"taut\" for tautology evaluates logical equivalence (symbolized here by ≡) between the two columns labelled d. Because all four rows under \"taut\" are 1's, the equivalence indeed represents a tautology.\n\nAn arbitrary propositional formula may have a very complicated structure. It is often convenient to work with formulas that have simpler forms, known as normal forms. Some common normal forms include conjunctive normal form and disjunctive normal form. Any propositional formula can be reduced to its conjunctive or disjunctive normal form.\n\nReduction to normal form is relatively simple once a truth table for the formula is prepared. But further attempts to minimize the number of literals (see below) requires some tools: reduction by De Morgan's laws and truth tables can be unwieldy, but Karnaugh maps are very suitable a small number of variables (5 or less). Some sophisticated tabular methods exist for more complex circuits with multiple outputs but these are beyond the scope of this article; for more see Quine–McCluskey algorithm.\n\nIn electrical engineering a variable x or its negation ~(x) is lumped together into a single notion called a literal. A string of literals connected by ANDs is called a term. A string of literals connected by OR is called an alterm. Typically the literal ~(x) is abbreviated ~x. Sometimes the &-symbol is omitted altogether in the manner of algebraic multiplication.\n\n\nIn the same way that a 2-row truth table displays the evaluation of a propositional formula for all 2 possible values of its variables, n variables produces a 2-square Karnaugh map (even though we cannot draw it in its full-dimensional realization). For example, 3 variables produces 2 = 8 rows and 8 Karnaugh squares; 4 variables produces 16 truth-table rows and 16 squares and therefore 16 minterms. Each Karnaugh-map square and its corresponding truth-table evaluation represents one minterm.\n\nAny propositional formula can be reduced to the \"logical sum\" (OR) of the active (i.e. \"1\"- or \"T\"-valued) minterms. When in this form the formula is said to be in disjunctive normal form. But even though it is in this form, it is not necessarily minimized with respect to either the number of terms or the number of literals.\n\nIn the following table, observe the peculiar numbering of the rows: (0, 1, 3, 2, 6, 7, 5, 4, 0). The first column is the decimal equivalent of the binary equivalent of the digits \"cba\", in other words:\n\nThis numbering comes about because as one moves down the table from row to row only one variable at a time changes its value. Gray code is derived from this notion. This notion can be extended to three and four-dimensional hypercubes called Hasse diagrams where each corner's variables change only one at a time as one moves around the edges of the cube. Hasse diagrams (hypercubes) flattened into two dimensions are either Veitch diagrams or Karnaugh maps (these are virtually the same thing).\n\nWhen working with Karnaugh maps one must always keep in mind that the top edge \"wrap arounds\" to the bottom edge, and the left edge wraps around to the right edge—the Karnaugh diagram is really a three- or four- or n-dimensional flattened object.\n\nVeitch improved the notion of Venn diagrams by converting the circles to abutting squares, and Karnaugh simplified the Veitch diagram by converting the minterms, written in their literal-form (e.g. ~abc~d) into numbers. The method proceeds as follows:\n\nProduce the formula's truth table. Number its rows using the binary-equivalents of the variables (usually just sequentially 0 through n-1) for n variables.\n\nExample: ((c & d) ∨ (p & ~(c & (~d)))) = q in conjunctive normal form is:\n\nHowever, this formula be reduced both in the number of terms (from 4 to 3) and in the total count of its literals (12 to 6).\n\nUse the values of the formula (e.g. \"p\") found by the truth-table method and place them in their into their respective (associated) Karnaugh squares (these are numbered per the Gray code convention). If values of \"d\" for \"don't care\" appear in the table, this adds flexibility during the reduction phase.\n\nMinterms of adjacent (abutting) 1-squares (T-squares) can be reduced with respect to the number of their literals, and the number terms also will be reduced in the process. Two abutting squares (2 x 1 horizontal or 1 x 2 vertical, even the edges represent abutting squares) lose one literal, four squares in a 4 x 1 rectangle (horizontal or vertical) or 2 x 2 square (even the four corners represent abutting squares) lose two literals, eight squares in a rectangle lose 3 literals, etc. (One seeks out the largest square or rectangles and ignores the smaller squares or rectangles contained totally within it. ) This process continues until all abutting squares are accounted for, at which point the propositional formula is minimized.\n\nFor example, squares #3 and #7 abut. These two abutting squares can lose one literal (e.g. \"p\" from squares #3 and #7), four squares in a rectangle or square lose two literals, eight squares in a rectangle lose 3 literals, etc. (One seeks out the largest square or rectangles.) This process continues until all abutting squares are accounted for, at which point the propositional formula is said to be minimized.\n\nExample: The map method usually is done by inspection. The following example expands the algebraic method to show the \"trick\" behind the combining of terms on a Karnaugh map:\n\nObserve that by the Idempotency law (A ∨ A) = A, we can create more terms. Then by association and distributive laws the variables to disappear can be paired, and then \"disappeared\" with the Law of contradiction (x & ~x)=0. The following uses brackets [ and ] only to keep track of the terms; they have no special significance:\n\nGiven the following examples-as-definitions, what does one make of the subsequent reasoning:\n\nThen assign the variable \"s\" to the left-most sentence \"This sentence is simple\". Define \"compound\" c = \"not simple\" ~s, and assign c = ~s to \"This sentence is compound\"; assign \"j\" to \"It [this sentence] is conjoined by AND\". The second sentence can be expressed as:\n\nIf truth values are to be placed on the sentences c = ~s and j, then all are clearly FALSEHOODS: e.g. \"This sentence is complex\" is a FALSEHOOD (it is \"simple\", by definition). So their conjunction (AND) is a falsehood. But when taken in its assembed form, the sentence a TRUTH.\n\nThis is an example of the paradoxes that result from an impredicative definition—that is, when an object m has a property P, but the object m is defined in terms of property P. The best advice for a rhetorician or one involved in deductive analysis is avoid impredicative definitions but at the same time be on the lookout for them because they can indeed create paradoxes. Engineers, on the other hand, put them to work in the form of propositional formulas with feedback.\n\nThe notion of a propositional formula appearing as one of its own variables requires a formation rule that allows the assignment of the formula to a variable. In general there is no stipulation (either axiomatic or truth-table systems of objects and relations) that forbids this from happening.\n\nThe simplest case occurs when an OR formula becomes one its own inputs e.g. p = q. Begin with (p ∨ s) = q, then let p = q. Observe that q's \"definition\" depends on itself \"q\" as well as on \"s\" and the OR connective; this definition of q is thus impredicative.\nEither of two conditions can result: oscillation or memory.\n\nIt helps to think of the formula as a black box. Without knowledge of what is going on \"inside\" the formula-\"box\" from the outside it would appear that the output is no longer a function of the inputs alone. That is, sometimes one looks at q and sees 0 and other times 1. To avoid this problem one has to know the state (condition) of the \"hidden\" variable p inside the box (i.e. the value of q fed back and assigned to p). When this is known the apparent inconsistency goes away.\n\nTo understand [predict] the behavior of formulas with feedback requires the more sophisticated analysis of sequential circuits. Propositional formulas with feedback lead, in their simplest form, to state machines; they also lead to memories in the form of Turing tapes and counter-machine counters. From combinations of these elements one can build any sort of bounded computational model (e.g. Turing machines, counter machines, register machines, Macintosh computers, etc.).\n\nIn the abstract (ideal) case the simplest oscillating formula is a NOT fed back to itself: ~(~(p=q)) = q. Analysis of an abstract (ideal) propositional formula in a truth-table reveals an inconsistency for both p=1 and p=0 cases: When p=1, q=0, this cannot be because p=q; ditto for when p=0 and q=1.\n\nOscillation with delay: If an delay (ideal or non-ideal) is inserted in the abstract formula between p and q then p will oscillate between 1 and 0: 101010...101... \"ad infinitum\". If either of the delay and NOT are not abstract (i.e. not ideal), the type of analysis to be used will be dependent upon the exact nature of the objects that make up the oscillator; such things fall outside mathematics and into engineering.\n\nAnalysis requires a delay to be inserted and then the loop cut between the delay and the input \"p\". The delay must be viewed as a kind of proposition that has \"qd\" (q-delayed) as output for \"q\" as input. This new proposition adds another column to the truth table. The inconsistency is now between \"qd\" and \"p\" as shown in red; two stable states resulting:\n\nWithout delay, inconsistencies must be eliminated from a truth table analysis. With the notion of \"delay\", this condition presents itself as a momentary inconsistency between the fed-back output variable q and p = q.\n\nA truth table reveals the rows where inconsistencies occur between p = q at the input and q at the output. After \"breaking\" the feed-back, the truth table construction proceeds in the conventional manner. But afterwards, in every row the output q is compared to the now-independent input p and any inconsistencies between p and q are noted (i.e. p=0 together with q=1, or p=1 and q=0); when the \"line\" is \"remade\" both are rendered impossible by the Law of contradiction ~(p & ~p)). Rows revealing inconsistencies are either considered transient states or just eliminated as inconsistent and hence \"impossible\".\n\nAbout the simplest memory results when the output of an OR feeds back to one of its inputs, in this case output \"q\" feeds back into \"p\". Given that the formula is first evaluated (initialized) with p=0 & q=0, it will \"flip\" once when \"set\" by s=1. Thereafter, output \"q\" will sustain \"q\" in the \"flipped\" condition (state q=1). This behavior, now time-dependent, is shown by the state diagram to the right of the once-flip.\n\nThe next simplest case is the \"set-reset\" flip-flop shown below the once-flip. Given that r=0 & s=0 and q=0 at the outset, it is \"set\" (s=1) in a manner similar to the once-flip. It however has a provision to \"reset\" q=0 when \"r\"=1. And additional complication occurs if both set=1 and reset=1. In this formula, the set=1 \"forces\" the output q=1 so when and if (s=0 & r=1) the flip-flop will be reset. Or, if (s=1 & r=0) the flip-flop will be set. In the abstract (ideal) instance in which s=1 ⇒ s=0 & r=1 ⇒ r=0 simultaneously, the formula q will be indeterminate (undecidable). Due to delays in \"real\" OR, AND and NOT the result will be unknown at the outset but thereafter predicable.\n\nThe formula known as \"clocked flip-flop\" memory (\"c\" is the \"clock\" and \"d\" is the \"data\") is given below. It works as follows: When c = 0 the data d (either 0 or 1) cannot \"get through\" to affect output q. When c = 1 the data d \"gets through\" and output q \"follows\" d's value. When c goes from 1 to 0 the last value of the data remains \"trapped\" at output \"q\". As long as c=0, d can change value without causing q to change.\n\n\nThe state diagram is similar in shape to the flip-flop's state diagram, but with different labelling on the transitions.\n\nBertrand Russell (1912:74) lists three laws of thought that derive from Aristotle: (1) The law of identity: \"Whatever is, is.\", (2) The law of contradiction: \"Nothing cannot both be and not be\", and (3) The law of excluded middle: \"Everything must be or not be.\"\n\nThe use of the word \"everything\" in the law of excluded middle renders Russell's expression of this law open to debate. If restricted to an expression about BEING or QUALITY with reference to a finite collection of objects (a finite \"universe of discourse\") -- the members of which can be investigated one after another for the presence or absence of the assertion—then the law is considered intuitionistically appropriate. Thus an assertion such as: \"This object must either BE or NOT BE (in the collection)\", or \"This object must either have this QUALITY or NOT have this QUALITY (relative to the objects in the collection)\" is acceptable. See more at Venn diagram.\n\nAlthough a propositional calculus originated with Aristotle, the notion of an \"algebra\" applied to propositions had to wait until the early 19th century. In an (adverse) reaction to the 2000 year tradition of Aristotle's syllogisms, John Locke's \"Essay concerning human understanding (1690)\" used the word semiotics (theory of the use of symbols). By 1826 Richard Whately had critically analyzed the syllogistic logic with a sympathy toward Locke's semiotics. George Bentham's work (1827) resulted in the notion of \"quantification of the predicate\" (1827) (nowadays symbolized as ∀ ≡ \"for all\"). A \"row\" instigated by William Hamilton over a priority dispute with Augustus De Morgan \"inspired George Boole to write up his ideas on logic, and to publish them as MAL [Mathematical Analysis of Logic] in 1847\" (Grattin-Guinness and Bornet 1997:xxviii).\n\nAbout his contribution Grattin-Guinness and Bornet comment:\n\nGottlob Frege's massive undertaking (1879) resulted in a formal calculus of propositions, but his symbolism is so daunting that it had little influence excepting on one person: Bertrand Russell. First as the student of Alfred North Whitehead he studied Frege's work and suggested a (famous and notorious) emendation with respect to it (1904) around the problem of an antinomy that he discovered in Frege's treatment ( cf Russell's paradox ). Russell's work led to a collatoration with Whitehead that, in the year 1912, produced the first volume of \"Principia Mathematica\" (PM). It is here that what we consider \"modern\" propositional logic first appeared. In particular, PM introduces NOT and OR and the assertion symbol ⊦ as primitives. In terms of these notions they define IMPLICATION → ( def. *1.01: ~p ∨ q ), then AND (def. *3.01: ~(~p ∨ ~q) ), then EQUIVALENCE p ←→ q (*4.01: (p → q) & ( q → p ) ).\n\n\nComputation and switching logic:\n\n"}
{"id": "27553", "url": "https://en.wikipedia.org/wiki?curid=27553", "title": "Set theory", "text": "Set theory\n\nSet theory is a branch of mathematical logic that studies sets, which informally are collections of objects. Although any type of object can be collected into a set, set theory is applied most often to objects that are relevant to mathematics. The language of set theory can be used to define nearly all mathematical objects.\n\nThe modern study of set theory was initiated by Georg Cantor and Richard Dedekind in the 1870s. After the discovery of paradoxes in naive set theory, such as Russell's paradox, numerous axiom systems were proposed in the early twentieth century, of which the Zermelo–Fraenkel axioms, with or without the axiom of choice, are the best-known.\n\nSet theory is commonly employed as a foundational system for mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Beyond its foundational role, set theory is a branch of mathematics in its own right, with an active research community. Contemporary research into set theory includes a diverse collection of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.\n\nMathematical topics typically emerge and evolve through interactions among many researchers. Set theory, however, was founded by a single paper in 1874 by Georg Cantor: \"On a Property of the Collection of All Real Algebraic Numbers\".\n\nSince the 5th century BC, beginning with Greek mathematician Zeno of Elea in the West and early Indian mathematicians in the East, mathematicians had struggled with the concept of infinity. Especially notable is the work of Bernard Bolzano in the first half of the 19th century. Modern understanding of infinity began in 1870–1874 and was motivated by Cantor's work in real analysis. An 1872 meeting between Cantor and Richard Dedekind influenced Cantor's thinking and culminated in Cantor's 1874 paper.\n\nCantor's work initially polarized the mathematicians of his day. While Karl Weierstrass and Dedekind supported Cantor, Leopold Kronecker, now seen as a founder of mathematical constructivism, did not. Cantorian set theory eventually became widespread, due to the utility of Cantorian concepts, such as one-to-one correspondence among sets, his proof that there are more real numbers than integers, and the \"infinity of infinities\" (\"Cantor's paradise\") resulting from the power set operation. This utility of set theory led to the article \"Mengenlehre\" contributed in 1898 by Arthur Schoenflies to Klein's encyclopedia.\n\nThe next wave of excitement in set theory came around 1900, when it was discovered that some interpretations of Cantorian set theory gave rise to several contradictions, called antinomies or paradoxes. Bertrand Russell and Ernst Zermelo independently found the simplest and best known paradox, now called Russell's paradox: consider \"the set of all sets that are not members of themselves\", which leads to a contradiction since it must be a member of itself and not a member of itself. In 1899 Cantor had himself posed the question \"What is the cardinal number of the set of all sets?\", and obtained a related paradox. Russell used his paradox as a theme in his 1903 review of continental mathematics in his \"The Principles of Mathematics\".\n\nIn 1906 English readers gained the book \"Theory of Sets of Points\" by husband and wife William Henry Young and Grace Chisholm Young, published by Cambridge University Press.\n\nThe momentum of set theory was such that debate on the paradoxes did not lead to its abandonment. The work of Zermelo in 1908 and the work of Abraham Fraenkel and Thoralf Skolem in 1922 resulted in the set of axioms ZFC, which became the most commonly used set of axioms for set theory. The work of analysts such as Henri Lebesgue demonstrated the great mathematical utility of set theory, which has since become woven into the fabric of modern mathematics. Set theory is commonly used as a foundational system, although in some areas—such as algebraic geometry and algebraic topology—category theory is thought to be a preferred foundation.\n\nSet theory begins with a fundamental binary relation between an object and a set . If is a member (or element) of , the notation is used. Since sets are objects, the membership relation can relate sets as well.\n\nA derived binary relation between two sets is the subset relation, also called set inclusion. If all the members of set are also members of set , then is a subset of , denoted . For example, is a subset of , and so is but is not. As insinuated from this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the term proper subset is defined. is called a proper subset of if and only if is a subset of , but is not equal to . Note also that 1, 2, and 3 are members (elements) of the set but are not subsets of it; and in turn, the subsets, such as {1}, are not members of the set {1, 2, 3}.\n\nJust as arithmetic features binary operations on numbers, set theory features binary operations on sets. The:\n\nSome basic sets of central importance are the empty set (the unique set containing no elements; occasionally called the \"null set\" though this name is ambiguous), the set of natural numbers, and the set of real numbers.\n\nA set is pure if all of its members are sets, all members of its members are sets, and so on. For example, the set containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to the von Neumann universe of pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into a cumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (by transfinite recursion) an ordinal number α, known as its rank. The rank of a pure set X is defined to be the least upper bound of all successors of ranks of members of X. For example, the empty set is assigned rank 0, while the set containing only the empty set is assigned rank 1. For each ordinal α, the set \"V\" is defined to consist of all pure sets with rank less than α. The entire von Neumann universe is denoted \"V\".\n\nElementary set theory can be studied informally and intuitively, and so can be taught in primary schools using Venn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which are Russell's paradox and the Burali-Forti paradox. Axiomatic set theory was originally devised to rid set theory of such paradoxes.\n\nThe most widely studied systems of axiomatic set theory imply that all sets form a cumulative hierarchy. Such systems come in two flavors, those whose ontology consists of:\nThe above systems can be modified to allow urelements, objects that can be members of sets but that are not themselves sets and do not have any members.\n\nThe systems of New Foundations NFU (allowing urelements) and NF (lacking them) are not based on a cumulative hierarchy. NF and NFU include a \"set of everything, \" relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which the axiom of choice does not hold.\n\nSystems of constructive set theory, such as CST, CZF, and IZF, embed their set axioms in intuitionistic instead of classical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These include rough set theory and fuzzy set theory, in which the value of an atomic formula embodying the membership relation is not simply True or False. The Boolean-valued models of ZFC are a related subject.\n\nAn enrichment of ZFC called internal set theory was proposed by Edward Nelson in 1977.\n\nMany mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse as graphs, manifolds, rings, and vector spaces can all be defined as sets satisfying various (axiomatic) properties. Equivalence and order relations are ubiquitous in mathematics, and the theory of mathematical relations can be described in set theory.\n\nSet theory is also a promising foundational system for much of mathematics. Since the publication of the first volume of \"Principia Mathematica\", it has been claimed that most or even all mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, using first or second order logic. For example, properties of the natural and real numbers can be derived within set theory, as each number system can be identified with a set of equivalence classes under a suitable equivalence relation whose field is some infinite set.\n\nSet theory as a foundation for mathematical analysis, topology, abstract algebra, and discrete mathematics is likewise uncontroversial; mathematicians accept that (in principle) theorems in these areas can be derived from the relevant definitions and the axioms of set theory. Few full derivations of complex mathematical theorems from set theory have been formally verified, however, because such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project, Metamath, includes human-written, computer‐verified derivations of more than 12,000 theorems starting from ZFC set theory, first order logic and propositional logic.\n\nSet theory is a major area of research in mathematics, with many interrelated subfields.\n\nCombinatorial set theory concerns extensions of finite combinatorics to infinite sets. This includes the study of cardinal arithmetic and the study of extensions of Ramsey's theorem such as the Erdős–Rado theorem.\n\nDescriptive set theory is the study of subsets of the real line and, more generally, subsets of Polish spaces. It begins with the study of pointclasses in the Borel hierarchy and extends to the study of more complex hierarchies such as the projective hierarchy and the Wadge hierarchy. Many properties of Borel sets can be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.\n\nThe field of effective descriptive set theory is between set theory and recursion theory. It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (\"relativizing\") it to make it more broadly applicable.\n\nA recent area of research concerns Borel equivalence relations and more complicated definable equivalence relations. This has important applications to the study of invariants in many fields of mathematics.\n\nIn set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. In fuzzy set theory this condition was relaxed by Lotfi A. Zadeh so an object has a \"degree of membership\" in a set, a number between 0 and 1. For example, the degree of membership of a person in the set of \"tall people\" is more flexible than a simple yes or no answer and can be a real number such as 0.75.\n\nAn inner model of Zermelo–Fraenkel set theory (ZF) is a transitive class that includes all the ordinals and satisfies all the axioms of ZF. The canonical example is the constructible universe \"L\" developed by Gödel.\nOne reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a model \"V\" of ZF satisfies the continuum hypothesis or the axiom of choice, the inner model \"L\" constructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.\n\nThe study of inner models is common in the study of determinacy and large cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).\n\nA large cardinal is a cardinal number with an extra property. Many such properties are studied, including inaccessible cardinals, measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable in Zermelo-Fraenkel set theory.\n\nDeterminacy refers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. The axiom of determinacy (AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that the Wadge degrees have an elegant structure.\n\nPaul Cohen invented the method of forcing while searching for a model of ZFC in which the continuum hypothesis fails, or a model of ZF in which the axiom of choice fails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. \"forced\") by the construction and the original model. For example, Cohen's construction adjoins additional subsets of the natural numbers without changing any of the cardinal numbers of the original model. Forcing is also one of two methods for proving relative consistency by finitistic methods, the other method being Boolean-valued models.\n\nA cardinal invariant is a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection of meagre sets of reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.\n\nSet-theoretic topology studies questions of general topology that are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.\n\nFrom set theory's inception, some mathematicians have objected to it as a foundation for mathematics. The most common objection to set theory, one Kronecker voiced in set theory's earliest years, starts from the constructivist view that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both in naive and in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased by Errett Bishop's influential book \"Foundations of Constructive Analysis\".\n\nA different objection put forth by Henri Poincaré is that defining sets using the axiom schemas of specification and replacement, as well as the axiom of power set, introduces impredicativity, a type of circularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo-Fraenkel theory, is much greater than that of constructive mathematics, to the point that Solomon Feferman has said that \"all of scientifically applicable analysis can be developed [using predicative methods]\".\n\nLudwig Wittgenstein condemned set theory. He wrote that \"set theory is wrong\", since it builds on the \"nonsense\" of fictitious symbolism, has \"pernicious idioms\", and that it is nonsensical to talk about \"all numbers\". Wittgenstein's views about the foundations of mathematics were later criticised by Georg Kreisel and Paul Bernays, and investigated by Crispin Wright, among others.\n\nCategory theorists have proposed topos theory as an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory. Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework for pointless topology and Stone spaces.\n\nAn active area of research is the univalent foundations and related to it homotopy type theory. Within homotopy type theory, a set may be regarded as a homotopy 0-type, with universal properties of sets arising from the inductive and recursive properties of higher inductive types. Principles such as the axiom of choice and the law of the excluded middle can be formulated in a manner corresponding to the classical formulation in set theory or perhaps in a spectrum of distinct ways unique to type theory. Some of these principles may be proven to be a consequence of other principles. The variety of formulations of these axiomatic principles allows for a detailed analysis of the formulations required in order to derive various mathematical results.\n\n\n\n"}
{"id": "25264092", "url": "https://en.wikipedia.org/wiki?curid=25264092", "title": "Slow-growing hierarchy", "text": "Slow-growing hierarchy\n\nIn computability theory, computational complexity theory and proof theory, the slow-growing hierarchy is an ordinal-indexed family of slowly increasing functions \"g\": N → N (where N is the set of natural numbers, {0, 1, ...}). It contrasts with the fast-growing hierarchy.\n\nLet μ be a large countable ordinal such that a fundamental sequence is assigned to every limit ordinal less than μ. The slow-growing hierarchy of functions \"g\": N → N, for α < μ, is then defined as follows:\n\n\nHere α[\"n\"] denotes the \"n\" element of the fundamental sequence assigned to the limit ordinal α.\n\nThe article on the Fast-growing hierarchy describes a standardized choice for fundamental sequence for all α < ε.\n\nThe slow-growing hierarchy grows much more slowly than the fast-growing hierarchy. Even \"g\" is only equivalent to \"f\" and \"g\" only attains the growth of \"f\" (the first function that Peano arithmetic cannot prove total in the hierarchy) when α is the Bachmann–Howard ordinal.\n\nHowever, Girard proved that the slow-growing hierarchy eventually \"catches up\" with the fast-growing one. Specifically, that there exists an ordinal α such that for all integers \"n\"\nwhere \"f\" are the functions in the fast-growing hierarchy. He further showed that the first α this holds for is the ordinal of the theory \"ID\" of arbitrary finite iterations of an inductive definition. However, for the assignment of fundamental sequences found in the first match up occurs at the level ε. For Buchholz style tree ordinals it could be shown that the first match up even occurs at formula_4.\n\nExtensions of the result proved to considerably larger ordinals show that there are very few ordinals below the ordinal of transfinitely iterated formula_5-comprehension where the slow- and fast-growing hierarchy match up.\n\nThe slow-growing hierarchy depends extremely sensitively on the choice of the underlying fundamental sequences.\n\nCichon provided an interesting connection between the slow-growing hierarchy and derivation length for term rewriting.\n\n"}
{"id": "36971277", "url": "https://en.wikipedia.org/wiki?curid=36971277", "title": "Strangulated graph", "text": "Strangulated graph\n\nIn graph theoretic mathematics, a strangulated graph is a graph in which deleting the edges of any induced cycle of length greater than three would disconnect the remaining graph. That is, they are the graphs in which every peripheral cycle is a triangle.\n\nIn a maximal planar graph, or more generally in every polyhedral graph, the peripheral cycles are exactly the faces of a planar embedding of the graph, so a polyhedral graph is strangulated if and only if all the faces are triangles, or equivalently it is maximal planar. Every chordal graph is strangulated, because the only induced cycles in chordal graphs are triangles, so there are no longer cycles to delete.\n\nA clique-sum of two graphs is formed by identifying together two equal-sized cliques in each graph, and then possibly deleting some of the clique edges. For the version of clique-sums relevant to strangulated graphs, the edge deletion step is omitted. A clique-sum of this type between two strangulated graphs results in another strangulated graph, for every long induced cycle in the sum must be confined to one side or the other (otherwise it would have a chord between the vertices at which it crossed from one side of the sum to the other), and the disconnected parts of that side formed by deleting the cycle must remain disconnected in the clique-sum. Every chordal graph can be decomposed in this way into a clique-sum of complete graphs, and every maximal planar graph can be decomposed into a clique-sum of 4-vertex-connected maximal planar graphs.\n\nAs show, these are the only possible building blocks of strangulated graphs: the strangulated graphs are exactly the graphs that can be formed as clique-sums of complete graphs and maximal planar graphs.\n\n"}
{"id": "4688530", "url": "https://en.wikipedia.org/wiki?curid=4688530", "title": "The Book of Squares", "text": "The Book of Squares\n\nThe Book of Squares, (Liber Quadratorum in the original Latin) is a book on algebra by Leonardo Fibonacci, published in 1225. It was dedicated to Frederick II, Holy Roman Emperor. Fibonacci's identity, establishing that the set of all sums of two squares is closed under multiplication, appears in it. The book anticipated the works of later mathematicians like Fermat and Euler. The book examines several topics in number theory, among them an inductive method for finding Pythagorean triples based on the sequence of odd integers, the fact that the sum of the first formula_1 odd integers is formula_2, and the solution to the congruum problem.\n\n"}
{"id": "32542924", "url": "https://en.wikipedia.org/wiki?curid=32542924", "title": "The Code (2011 TV series)", "text": "The Code (2011 TV series)\n\nThe Code is a mathematics-based documentary for BBC Two presented by Marcus du Sautoy, beginning on 27 July 2011 and ended on 10 August 2011. Each episode covers a different branch of mathematics. As well as being a documentary, \"The Code\" is also a series of online challenges forming a treasure hunt, with clues to finding the treasure being included in the episodes, online games and other challenges.\n\nThe treasure hunt is a series of online mathematical challenges. The BBC planned to offer the challenges to 1000 participants selected from among people who applied to participate via Twitter or email. There are three stages to the treasure hunt: The Codebreakers, the Ultimate Challenge, and the Finale.\n\nThe first puzzles are \"The Codebreakers\". These consist of three wheels, one relating to each episode. Each Codebreaker has six different questions and challenges relating to it, the answers to which surround the wheel. Answers to these questions can be found by watching the episode for clues, completing a flash game, solving a puzzle on the programme's blog or reaching a milestone in a mass community challenge, which involves trying to find examples of all the prime numbers between 2 and 2011 in the real world. Once a clue is found, the challenger can enter it into the Codebreaker by moving the correct \"hand\" around the Codebreaker. Each time a hand is moved, the password given changes. Each possible outcome of the Codebreaker produces a different password to the ultimate challenge. Entering the correct solutions into each of the three Codebreakers will result in the challenger getting the three correct passwords. Once these are entered, the Ultimate Challenge is accessible.\n\nThe Ultimate Challenge will be made accessible after all three episodes have been broadcast, and can only be accessed if the challenger enters all three passwords correctly. Once it is accessed, the first three eligible people to solve the Ultimate Challenge go through to the Finale.\n\nThe Finale took place at Bletchley Park during the weekend of 10 September 2011. The prize, a specially commissioned mathematical sculpture of the platonic solids , was won by Pete Ryland.\n\n"}
{"id": "29407873", "url": "https://en.wikipedia.org/wiki?curid=29407873", "title": "Vojtěch Rödl", "text": "Vojtěch Rödl\n\nVojtěch Rödl is a Czech mathematician, currently the Samuel Candler Dobbs Professor at Emory University in Atlanta, known for his work in combinatorics. \n\nRödl received his Ph.D. from Charles University, Prague in 1976; his advisor was Zdenek Hedrlin. Significant contributions include his work with Jaroslav Nešetřil on Ramsey theory, his proof of the Erdős–Hanani conjecture on hypergraph packing and his development, together with Brendan Nagle, Mathias Schacht, and Jozef Skokan (and independently of Timothy Gowers), of the hypergraph regularity lemma. \n\nIn 2012, Rödl and his former student Schacht were awarded the George Pólya Prize by the Society for Industrial and Applied Mathematics, for their work on hypergraph regularity.\n\n\n"}
{"id": "42316777", "url": "https://en.wikipedia.org/wiki?curid=42316777", "title": "Well-separated pair decomposition", "text": "Well-separated pair decomposition\n\nIn computational geometry, a well-separated pair decomposition (WSPD) of a set of points formula_1, is a sequence of pairs of sets formula_2, such that each pair is well-separated, and for each two distinct points formula_3, there exists precisely one pair which separates the two.\n\nThe graph induced by a well-separated pair decomposition can serve as a k-spanner of the complete Euclidean graph, and is useful in approximating solutions to several problems pertaining to this.\n\nLet formula_4 be two disjoint sets of points in formula_5, formula_6 denote the axis-aligned minimum bounding box for the points in formula_7, and formula_8 denote the separation factor.\n\nWe consider formula_9 and formula_10 to be well-separated, if for each of formula_11 and formula_12 there exists a d-ball of radius formula_13 containing it, such that the two spheres have a minimum distance of at least formula_14.\n\nWe consider a sequence of well-separated pairs of subsets of formula_15, formula_16 to be a well-separated pair decomposition (WSPD) of formula_15 if for any two distinct points formula_3, there exists precisely one formula_19, formula_20, such that either\n\n\nBy way of constructing a fair split tree, it is possible to construct a WSPD of size formula_25 in formula_26 time.\n\nThe general principle of the split tree of a point set is that each node of the tree represents a set of points and that the bounding box of is split along its longest side in two equal parts which form the two children of and their point set. It is done recursively until there is only one point in the set.\n\nLet denote the size of the longest interval of the bounding hyperrectangle of point set and let denote the size of the \"i\"-th dimension of the bounding hyperrectangle of point set . We give pseudocode for the Split tree computation below.\n\nThis algorithm runs in formula_27 time.\n\nWe give a more efficient algorithm that runs in formula_26 time below. The goal is to loop over the list in only formula_29 operations per step of the recursion but only call the recursion on at most half the points each time.\n\nLet be the \"j\"-th coordinate of the \"i\"-th point in such that is sorted for each dimension and be the point. Also, let be the hyperplane that splits the longest side of in two. Here is the algorithm in pseudo-code:\n\nTo be able to maintain the sorted lists for each node, linked lists are used. Cross-pointers are kept for each list to the others to be able to retrieve a point in constant time. In the algorithm above, in each iteration of the loop, a call to the recursion is done. In reality, to be able to reconstruct the list without the overhead of resorting the points, it is necessary to rebuild the sorted lists once all points have been assigned to their nodes. To do the rebuilding, walk along each list for each dimension, add each point to the corresponding list of its nodes, and add cross-pointers in the original list to be able to add the cross-pointers for the new lists. Finally, call the recursion on each node and his set.\n\nThe WSPD can be extracted from such a split tree by calling the recursive function on the children of \"every\" node in the split tree. Let / denote the children of the node . We give pseudocode for the function below.\n\nWe give pseudocode for the function below.\n\nCombining the -well-separated pairs from all the calls of gives the WSPD for separation .\nIt is clear that the pairs returned by the algorithm are well-separated because of the return condition of the function .\n\nNow, we have to prove that for any distinct points formula_30 and formula_31 in formula_15, there is a unique pair formula_33 so that (i) formula_34 and formula_35 or (ii) formula_36 and formula_37. Assume without loss of generality that (i) holds.\n\nLet formula_38 be the lowest common ancestor of formula_30 and formula_31 in the split tree and let formula_41 and formula_42 be the children of formula_38. Because of the last assumption, formula_30 is in the subtree of formula_41 and formula_31 in the subtree of formula_42. A call to is necessarily done in . Because, each time there is a recursion, the recursion tree creates two branches that contain all the points of the current recursion call, there will be a sequence of call to leading to having formula_30 in formula_9 and formula_31 in formula_10.\n\nBecause formula_38 is the lowest common ancestor of formula_30 and formula_31, calling on the children of a higher node would result of formula_30 and formula_31 not being in a pair and calling on the children in one of the nodes of one of the subtrees of formula_38 would result by formula_30 or formula_31 not being in any pair. Thus, the pair formula_33 is the unique one separating formula_30 and formula_31.\nEach time the recursion tree split in two, there is one more pair added to the decomposition. So, the algorithm run-time is in the number of pairs in the final decomposition.\n\nCallahan and Kosaraju proved that this algorithm finds a Well-separated pair decomposition (WSPD) of size formula_25.\n\nLemma 1: Let formula_33 be a well-separated pair with respect to formula_65. Let formula_66 and formula_35. Then, formula_68.\n\nProof: Because formula_30 and formula_70 are in the same set, we have that formula_71 where formula_13 is the radius of the enclosing circle of formula_9 and formula_10. Because formula_30 and formula_31 are in two well-separated sets, we have that formula_77. We obtain that:\n\nformula_78\n\nLemma 2: Let formula_33 be a well-separated pair with respect to formula_65. Let formula_66 and formula_82. Then, formula_83.\n\nProof: By the triangle inequality, we have:\n\nformula_84\n\nFrom Lemma 1, we obtain:\n\nformula_85\n\nThe well-separated pair decomposition has application in solving a number of problems. WSPD can be used to:\n\n"}
