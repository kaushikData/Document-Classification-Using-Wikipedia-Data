{"id": "667063", "url": "https://en.wikipedia.org/wiki?curid=667063", "title": "Antimatroid", "text": "Antimatroid\n\nIn mathematics, an antimatroid is a formal system that describes processes in which a set is built up by including elements one at a time, and in which an element, once available for inclusion, remains available until it is included. Antimatroids are commonly axiomatized in two equivalent ways, either as a set system modeling the possible states of such a process, or as a formal language modeling the different sequences in which elements may be included.\nDilworth (1940) was the first to study antimatroids, using yet another axiomatization based on lattice theory, and they have been frequently rediscovered in other contexts; see Korte et al. (1991) for a comprehensive survey of antimatroid theory with many additional references.\n\nThe axioms defining antimatroids as set systems are very similar to those of matroids, but whereas matroids are defined by an \"exchange axiom\" (e.g., the \"basis exchange\", or \"independent set exchange\" axioms), antimatroids are defined instead by an \"anti-exchange axiom\", from which their name derives.\nAntimatroids can be viewed as a special case of greedoids and of semimodular lattices, and as a generalization of partial orders and of distributive lattices. \nAntimatroids are equivalent, by complementation, to convex geometries, a combinatorial abstraction of convex sets in geometry.\n\nAntimatroids have been applied to model precedence constraints in scheduling problems, potential event sequences in simulations, task planning in artificial intelligence, and the states of knowledge of human learners.\n\nAn antimatroid can be defined as a finite family \"F\" of sets, called \"feasible sets\", with the following two properties:\n\nAntimatroids also have an equivalent definition as a formal language, that is, as a set of strings defined from a finite alphabet of symbols. A language \"L\" defining an antimatroid must satisfy the following properties:\n\nIf \"L\" is an antimatroid defined as a formal language, then the sets of symbols in strings of \"L\" form an accessible union-closed set system. In the other direction, if \"F\" is an accessible union-closed set system, and \"L\" is the language of strings \"s\" with the property that the set of symbols in each prefix of \"s\" is feasible, then \"L\" defines an antimatroid. Thus, these two definitions lead to mathematically equivalent classes of objects.\n\n\nIn the set theoretic axiomatization of an antimatroid there are certain special sets called \"paths\" that determine the whole antimatroid, in the sense that the sets of the antimatroid are exactly the unions of paths. If \"S\" is any feasible set of the antimatroid, an element \"x\" that can be removed from \"S\" to form another feasible set is called an \"endpoint\" of \"S\", and a feasible set that has only one endpoint is called a \"path\" of the antimatroid. The family of paths can be partially ordered by set inclusion, forming the \"path poset\" of the antimatroid.\n\nFor every feasible set \"S\" in the antimatroid, and every element \"x\" of \"S\", one may find a path subset of \"S\" for which \"x\" is an endpoint: to do so, remove one at a time elements other than \"x\" until no such removal leaves a feasible subset. Therefore, each feasible set in an antimatroid is the union of its path subsets. If \"S\" is not a path, each subset in this union is a proper subset of \"S\". But, if \"S\" is itself a path with endpoint \"x\", each proper subset of \"S\" that belongs to the antimatroid excludes \"x\". Therefore, the paths of an antimatroid are exactly the sets that do not equal the unions of their proper subsets in the antimatroid. Equivalently, a given family of sets \"P\" forms the set of paths of an antimatroid if and only if, for each \"S\" in \"P\", the union of subsets of \"S\" in \"P\" has one fewer element than \"S\" itself. If so, \"F\" itself is the family of unions of subsets of \"P\".\n\nIn the formal language formalization of an antimatroid we may also identify a subset of words that determine the whole language, the \"basic words\".\nThe longest strings in \"L\" are called \"basic words\"; each basic word forms a permutation of the whole alphabet. For instance, the basic words of a poset antimatroid are the linear extensions of the given partial order. If \"B\" is the set of basic words, \"L\" can be defined from \"B\" as the set of prefixes of words in \"B\". It is often convenient to define antimatroids from basic words in this way, but it is not straightforward to write an axiomatic definition of antimatroids in terms of their basic words.\n\nIf \"F\" is the set system defining an antimatroid, with \"U\" equal to the union of the sets in \"F\", then the family of sets\ncomplementary to the sets in \"F\" is sometimes called a convex geometry and the sets in \"G\" are called convex sets. For instance, in a shelling antimatroid, the convex sets are intersections of \"U\" with convex subsets of the Euclidean space into which \"U\" is embedded.\n\nComplementarily to the properties of set systems that define antimatroids, the set system defining a convex geometry should be closed under intersections, and for any set \"S\" in \"G\" that is not equal to \"U\" there must be an element \"x\" not in \"S\" that can be added to \"S\" to form another set in \"G\".\n\nA convex geometry can also be defined in terms of a closure operator τ that maps any subset of \"U\" to its minimal closed superset. To be a closure operator, τ should have the following properties:\nThe family of closed sets resulting from a closure operation of this type is necessarily closed under intersections. The closure operators that define convex geometries also satisfy an additional anti-exchange axiom:\nA closure operation satisfying this axiom is called an anti-exchange closure. If \"S\" is a closed set in an anti-exchange closure, then the anti-exchange axiom determines a partial order on the elements not belonging to \"S\", where \"x\" ≤ \"y\" in the partial order when \"x\" belongs to τ(\"S\" ∪ {\"y\"}). If \"x\" is a minimal element of this partial order, then \"S\" ∪ {\"x\"} is closed. That is, the family of closed sets of an anti-exchange closure has the property that for any set other than the universal set there is an element \"x\" that can be added to it to produce another closed set. This property is complementary to the accessibility property of antimatroids, and the fact that intersections of closed sets are closed is complementary to the property that unions of feasible sets in an antimatroid are feasible. Therefore, the complements of the closed sets of any anti-exchange closure form an antimatroid.\n\nThe undirected graphs in which the convex sets (subsets of vertices that contain all shortest paths between vertices in the subset) form a convex geometry are exactly the Ptolemaic graphs.\n\nAny two sets in an antimatroid have a unique least upper bound (their union) and a unique greatest lower bound (the union of the sets in the antimatroid that are contained in both of them). Therefore, the sets of an antimatroid, partially ordered by set inclusion, form a lattice. Various important features of an antimatroid can be interpreted in lattice-theoretic terms; for instance the paths of an antimatroid are the join-irreducible elements of the corresponding lattice, and the basic words of the antimatroid correspond to maximal chains in the lattice. The lattices that arise from antimatroids in this way generalize the finite distributive lattices, and can be characterized in several different ways.\n\n\nThese three characterizations are equivalent: any lattice with unique meet-irreducible decompositions has boolean atomistic intervals and is join-distributive, any lattice with boolean atomistic intervals has unique meet-irreducible decompositions and is join-distributive, and any join-distributive lattice has unique meet-irreducible decompositions and boolean atomistic intervals. Thus, we may refer to a lattice with any of these three properties as join-distributive. Any antimatroid gives rise to a finite join-distributive lattice, and any finite join-distributive lattice comes from an antimatroid in this way. Another equivalent characterization of finite join-distributive lattices is that they are graded (any two maximal chains have the same length), and the length of a maximal chain equals the number of meet-irreducible elements of the lattice. The antimatroid representing a finite join-distributive lattice can be recovered from the lattice: the elements of the antimatroid can be taken to be the meet-irreducible elements of the lattice, and the feasible set corresponding to any element \"x\" of the lattice consists of the set of meet-irreducible elements \"y\" such that \"y\" is not greater than or equal to \"x\" in the lattice.\n\nThis representation of any finite join-distributive lattice as an accessible family of sets closed under unions (that is, as an antimatroid) may be viewed as an analogue of Birkhoff's representation theorem under which any finite distributive lattice has a representation as a family of sets closed under unions and intersections.\n\nMotivated by a problem of defining partial orders on the elements of a Coxeter group, studied antimatroids which are also supersolvable lattices. A supersolvable antimatroid is defined by a totally ordered collection of elements, and a family of sets of these elements. The family must include the empty set. Additionally, it must have the property that if two sets \"A\" and \"B\" belong to the family, the set-theoretic difference \"B\" \\ \"A\" is nonempty, and \"x\" is the smallest element of \"B\" \\ \"A\", then \"A\" ∪ {\"x\"} also belongs to the family. As Armstrong observes, any family of sets of this type forms an antimatroid. Armstrong also provides a lattice-theoretic characterization of the antimatroids that this construction can form.\n\nIf \"A\" and \"B\" are two antimatroids, both described as a family of sets, and if the maximal sets in \"A\" and \"B\" are equal, we can form another antimatroid, the \"join\" of \"A\" and \"B\", as follows:\n\nThis is a different operation than the join considered in the lattice-theoretic characterizations of antimatroids: it combines two antimatroids to form another antimatroid, rather than combining two sets in an antimatroid to form another set.\nThe family of all antimatroids that have a given maximal set forms a semilattice with this join operation.\n\nJoins are closely related to a closure operation that maps formal languages to antimatroids, where the closure of a language \"L\" is the intersection of all antimatroids containing \"L\" as a sublanguage. This closure has as its feasible sets the unions of prefixes of strings in \"L\". In terms of this closure operation, the join is the closure of the union of the languages of \"A\" and \"B\".\n\nEvery antimatroid can be represented as a join of a family of chain antimatroids, or equivalently as the closure of a set of basic words; the \"convex dimension\" of an antimatroid \"A\" is the minimum number of chain antimatroids (or equivalently the minimum number of basic words) in such a representation. If \"F\" is a family of chain antimatroids whose basic words all belong to \"A\", then \"F\" generates \"A\" if and only if the feasible sets of \"F\" include all paths of \"A\". The paths of \"A\" belonging to a single chain antimatroid must form a chain in the path poset of \"A\", so the convex dimension of an antimatroid equals the minimum number of chains needed to cover the path poset, which by Dilworth's theorem equals the width of the path poset.\n\nIf one has a representation of an antimatroid as the closure of a set of \"d\" basic words, then this representation can be used to map the feasible sets of the antimatroid into \"d\"-dimensional Euclidean space: assign one coordinate per basic word \"w\", and make the coordinate value of a feasible set \"S\" be the length of the longest prefix of \"w\" that is a subset of \"S\". With this embedding, \"S\" is a subset of \"T\" if and only if the coordinates for \"S\" are all less than or equal to the corresponding coordinates of \"T\". Therefore, the order dimension of the inclusion ordering of the feasible sets is at most equal to the convex dimension of the antimatroid. However, in general these two dimensions may be very different: there exist antimatroids with order dimension three but with arbitrarily large convex dimension.\n\nThe number of possible antimatroids on a set of elements grows rapidly with the number of elements in the set. For sets of one, two, three, etc. elements, the number of distinct antimatroids is\n\nBoth the precedence and release time constraints in the standard notation for theoretic scheduling problems may be modeled by antimatroids. use antimatroids to generalize a greedy algorithm of Eugene Lawler for optimally solving single-processor scheduling problems with precedence constraints in which the goal is to minimize the maximum penalty incurred by the late scheduling of a task.\n\nIn Optimality Theory, grammars are logically equivalent to antimatroids ().\n\nIn mathematical psychology, antimatroids have been used to describe feasible states of knowledge of a human learner. Each element of the antimatroid represents a concept that is to be understood by the learner, or a class of problems that he or she might be able to solve correctly, and the sets of elements that form the antimatroid represent possible sets of concepts that could be understood by a single person. The axioms defining an antimatroid may be phrased informally as stating that learning one concept can never prevent the learner from learning another concept, and that any feasible state of knowledge can be reached by learning a single concept at a time. The task of a knowledge assessment system is to infer the set of concepts known by a given learner by analyzing his or her responses to a small and well-chosen set of problems. In this context antimatroids have also been called \"learning spaces\" and \"well-graded knowledge spaces\".\n\n\n"}
{"id": "24635433", "url": "https://en.wikipedia.org/wiki?curid=24635433", "title": "Balanced boolean function", "text": "Balanced boolean function\n\nIn mathematics and computer science, a balanced boolean function is a boolean function whose output yields as many 0s as 1s over its input set. This means that for a uniformly random input string of bits, the probability of getting a 1 is 1/2.\n\nAn example of a balanced boolean function is the function that assigns a 1 to every even number and 0 to all odd numbers (likewise the other way around). The same applies for functions assigning 1 to all positive numbers and 0 otherwise.\n\nA Boolean function of \"n\" bits is balanced if it takes the value 1 with probability 1⁄2.\n\nBalanced boolean functions are primarily used in cryptography. If a function is not balanced, it will have a statistical bias, making it subject to cryptanalysis such as the correlation attack.\n\n\n"}
{"id": "1446277", "url": "https://en.wikipedia.org/wiki?curid=1446277", "title": "Bessel's inequality", "text": "Bessel's inequality\n\nIn mathematics, especially functional analysis, Bessel's inequality is a statement about the coefficients of an element formula_1 in a Hilbert space with respect to an orthonormal sequence.\n\nLet formula_2 be a Hilbert space, and suppose that formula_3 is an orthonormal sequence in formula_2. Then, for any formula_1 in formula_2 one has\n\nwhere 〈•,•〉 denotes the inner product in the Hilbert space formula_2. If we define the infinite sum\nconsisting of \"infinite sum\" of vector resolute formula_1 in direction formula_11, Bessel's inequality tells us that this series converges. One can think of it that there exists formula_12 that can be described in terms of potential basis formula_13.\n\nFor a complete orthonormal sequence (that is, for an orthonormal sequence that is a basis), we have Parseval's identity, which replaces the inequality with an equality (and consequently formula_14 with formula_1).\n\nBessel's inequality follows from the identity\nwhich holds for any natural \"n\".\n\n\n"}
{"id": "28357925", "url": "https://en.wikipedia.org/wiki?curid=28357925", "title": "Calculus of moving surfaces", "text": "Calculus of moving surfaces\n\nThe calculus of moving surfaces (CMS) is an extension of the classical tensor calculus to deforming manifolds. Central to the CMS is the Tensorial Time Derivative formula_1 whose original definition was put forth by Jacques Hadamard. It plays the role analogous to that of the covariant derivative formula_2 on differential manifolds. in that it produces a tensor when applied to a tensor.\n\nSuppose that formula_3 is the evolution of the surface formula_4 indexed by a time-like parameter formula_5. The definitions of the surface velocity formula_6 and the operator formula_1 are the geometric foundations of the CMS. The velocity C is the rate of deformation of the surface formula_4 in the instantaneous normal direction. The value of formula_6 at a point formula_10 is defined as the limit\n\nwhere formula_12 is the point on formula_13 that lies on the straight line perpendicular to formula_14 at point P. This definition is illustrated in the first geometric figure below. The velocity formula_6 is a signed quantity: it is positive when formula_16 points in the direction of the chosen normal, and negative otherwise. The relationship between formula_14 and formula_6 is analogous to the relationship between location and velocity in elementary calculus: knowing either quantity allows one to construct the other by differentiation or integration.\n\nThe Tensorial Time Derivative formula_1 for a scalar field F defined on formula_14 is the rate of change in formula_21 in the instantaneously normal direction:\n\nThis definition is also illustrated in second geometric figure.\n\nThe above definitions are \"geometric\". In analytical settings, direct application of these definitions may not be possible. The CMS gives \"analytical\" definitions of C and formula_1 in terms of elementary operations from calculus and differential geometry.\n\nFor analytical definitions of formula_6 and formula_1, consider the evolution of formula_26 given by\n\nwhere formula_28 are general curvilinear space coordinates and formula_29 are the surface coordinates. By convention, tensor indices of function arguments are dropped. Thus the above equations contains formula_26 rather than formula_31.The velocity object formula_32 is defined as the partial derivative\n\nThe velocity formula_6 can be computed most directly by the formula\n\nwhere formula_36 are the covariant components of the normal vector formula_37.\n\nAlso, defining the shift tensor representation of the Surface's Tangent Space formula_38 and the Tangent Velocity as formula_39 , then the definition of the formula_1 derivative for an invariant \"F\" reads\n\nwhere formula_42 is the covariant derivative on S.\n\nFor \"tensors\", an appropriate generalization is needed. The proper definition for a representative tensor formula_43 reads\n\nwhere formula_45 are Christoffel symbols and formula_46 is the surface's appropriate temporal symbols (formula_47 is a matrix representation of the surface's curvature shape operator)\n\nThe formula_1-derivative commutes with contraction, satisfies the product rule for any collection of indices\n\nand obeys a chain rule for surface restrictions of spatial tensors:\n\nChain rule shows that the formula_51-derivative of spatial \"metrics\"\nvanishes\n\nwhere formula_53 and formula_54 are covariant and contravariant metric tensors, formula_55 is the Kronecker delta symbol, and formula_56 and formula_57 are the Levi-Civita symbols. The main article on Levi-Civita symbols describes them for Cartesian coordinate systems. The preceding rule is valid in general coordinates, where the definition of the Levi-Civita symbols must include the square root of the determinant of the covariant metric tensor formula_53.\n\nThe formula_1 derivative of the key surface objects leads to highly concise and attractive formulas. When applied to the covariant surface metric tensor formula_60 and the contravariant metric tensor formula_61, the following identities result\n\nwhere formula_63 and formula_64 are the doubly covariant and doubly contravariant curvature tensors. These curvature tensors, as well as for the mixed curvature tensor formula_47, satisfy\n\nThe shift tensor formula_67 and the normalformula_68 satisfy\n\nFinally, the surface Levi-Civita symbols formula_70 and formula_71 satisfy\n\nThe CMS provides rules for time differentiation of volume and surface integrals.\n"}
{"id": "2252922", "url": "https://en.wikipedia.org/wiki?curid=2252922", "title": "Charles Dupin", "text": "Charles Dupin\n\nBaron Pierre Charles François Dupin (6 October 1784, Varzy, Nièvre – 18 January 1873, Paris, France) was a French Catholic mathematician, engineer, economist and politician, particularly known for work in the field of mathematics, where the Dupin cyclide and Dupin indicatrix are named after him; and for his work in the field of statistical and thematic mapping, In 1826 he created the earliest known choropleth map.\n\nHe was born in Varzy in France, the son of Charles Andre Dupin, a lawyer, and Catherine Agnes Dupin.\n\nHe studied geometry with Monge at the École Polytechnique and then became a naval engineer (ENSTA). From 1807 he was responsible for the restoration of the damaged port and arsenal at Corfu. In 1813 he founded the Toulon Maritime Museum.\n\nIn 1819 he was appointed professor at the Conservatoire des Arts et Métiers; he kept this post until 1854. In 1822, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nIn 1808, he participated in the Greek science revival by teaching mathematics and mechanics lessons in Corfu. One of his students was Giovanni Carandino, who would go on to be the founder of the Greek Mathematics School in the 1820s.\n\nIn 1818, Dupin was elected to the body of the French Academy of Sciences, one of the Institut de France's five Academies. \n\nIn 1826 he published a thematic map showing the distribution of illiteracy in France, using shadings (from black to white), the first known instance of what is called a choropleth map today. Duplin had been inspired by the work of the German statisticians Georg Hassel and August Friedrich Wilhelm Crome.\nDupin was named rapporteur for the central jury of the Exposition des produits de l'industrie française en 1834.\nFor each branch of industry he noted the quantities and value of French exports and imports, with comparative figures for 1823, 1827 and 1834.\n\nIn addition, he had a political career and was appointed to the Senate in 1852. His mathematical work was in descriptive and differential geometry. He was the discoverer of conjugate tangents to a point on a surface and of the Dupin indicatrix.\n\n\n"}
{"id": "11598940", "url": "https://en.wikipedia.org/wiki?curid=11598940", "title": "Colinear map", "text": "Colinear map\n\nIn coalgebra theory, the notion of colinear map is dual to the notion for linear map of vector space, or more generally, for morphism between R-module. Specifically, let R be a ring, M,N,C be R-modules, and\n\nformula_1\n\nbe right C-comodules. Then an R-linear map formula_2 is called a (right) comodule morphism, or (right) C-colinear, if\n\nformula_3\n\n"}
{"id": "19472545", "url": "https://en.wikipedia.org/wiki?curid=19472545", "title": "Condensation point", "text": "Condensation point\n\nIn mathematics, a condensation point \"p\" of a subset \"S\" of a topological space, is any point \"p\", such that every open neighborhood of \"p\" contains uncountably many points of \"S\". Thus, \"condensation point\" is synonymous with \"formula_1-accumulation point\".\n\n\n"}
{"id": "3237776", "url": "https://en.wikipedia.org/wiki?curid=3237776", "title": "Conjunctive grammar", "text": "Conjunctive grammar\n\nConjunctive grammars are a class of formal grammars\nstudied in formal language theory.\nThey extend the basic type of grammars,\nthe context-free grammars,\nwith a conjunction operation.\nBesides explicit conjunction,\nconjunctive grammars allow implicit disjunction\nrepresented by multiple rules for a single nonterminal symbol,\nwhich is the only logical connective expressible in context-free grammars.\nConjunction can be used, in particular,\nto specify intersection of languages.\nA further extension of conjunctive grammars\nknown as Boolean grammars\nadditionally allows explicit negation.\n\nThe rules of a conjunctive grammar are of the form\n\nwhere formula_2 is a nonterminal and\nformula_3, ..., formula_4\nare strings formed of symbols in formula_5 and formula_6 (finite sets of terminal and nonterminal symbols respectively).\nInformally, such a rule asserts that \nevery string formula_7 over formula_5\nthat satisfies each of the syntactical conditions represented\nby formula_3, ..., formula_4\ntherefore satisfies the condition defined by formula_2.\n\nA conjunctive grammar formula_12 is defined by the 4-tuple formula_13 where\n\nIt is common to list all right-hand sides for the same left-hand side on the same line, using | (the pipe symbol) to separate them. Rules formula_15 and formula_16 can hence be written as formula_17.\n\nTwo equivalent formal definitions\nof the language specified by a conjunctive grammar exist.\nOne definition is based upon representing the grammar\nas a system of language equations with union, intersection and concatenation\nand considering its least solution.\nThe other definition generalizes\nChomsky's generative definition of the context-free grammars\nusing rewriting of terms over conjunction and concatenation.\n\nFor any strings formula_18, we say directly yields , written as formula_19, if \n\nFor any string formula_26 we say generates , written as formula_27 if formula_28 such that formula_29.\n\nThe language of a grammar formula_13 is the set of it generates.\n\nThe grammar formula_31, with productions\n\nis conjunctive. A typical derivation is :formula_37 This makes it clear that formula_38. The language is not context-free, proved by the pumping lemma.\n\nThough the expressive power of conjunctive grammars\nis greater than those of context-free grammars,\nconjunctive grammars retain some of the latter.\nMost importantly, there are generalizations of the main context-free parsing algorithms,\nincluding the linear-time recursive descent,\nthe cubic-time generalized LR,\nthe cubic-time Cocke-Kasami-Younger,\nas well as Valiant's algorithm running as fast as matrix multiplication.\n\nA number of theoretical properties of conjunctive grammars have been researched,\nincluding the expressive power of grammars over a one-letter alphabet\nand .\nThis work provided a basis\nfor the study language equations of a more general form.\n\nAizikowitz and Kaminski introduced a new class of pushdown automata (PDA) called synchronized alternating pushdown automata (SAPDA). They proved it to be equivalent to conjunctive grammars in the same way as nondeterministic PDAs are equivalent to context-free grammars.\n\n\n"}
{"id": "51513238", "url": "https://en.wikipedia.org/wiki?curid=51513238", "title": "Contact graph", "text": "Contact graph\n\nIn the mathematical area of graph theory, a contact graph or tangency graph is a graph whose vertices are represented by geometric objects (e.g. curves, line segments, or polygons), and whose edges correspond to two objects touching (but not crossing) according to some specified notion. It is similar to the notion of an intersection graph but differs from it in restricting the ways that the underlying objects are allowed to intersect each other.\n\nThe circle packing theorem states that every planar graph can be represented as a contact graph of circles. The contact graphs of unit circles are called penny graphs. Representations as contact graphs of triangles, rectangles, squares, line segments, or circular arcs have also been studied.\n"}
{"id": "443235", "url": "https://en.wikipedia.org/wiki?curid=443235", "title": "Covariant transformation", "text": "Covariant transformation\n\nIn physics, a covariant transformation is a rule that specifies how certain entities, such as vectors or tensors, change under a change of basis. The transformation that describes the new basis vectors as a linear combination of the old basis vectors is \"defined\" as a covariant transformation. Conventionally, indices identifying the basis vectors are placed as lower indices and so are all entities that transform in the same way. The inverse of a covariant transformation is a contravariant transformation. Whenever a vector should be \"invariant\" under a change of basis, that is to say it should represent the same geometrical or physical object having the same magnitude and direction as before, its \"components\" must transform according to the contravariant rule. Conventionally, indices identifying the components of a vector are placed as upper indices and so are all indices of entities that transform in the same way. The sum over pairwise matching indices of a product with the same lower and upper indices are invariant under a transformation.\n\nA vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector v is given, say, in components \"v\" on a chosen basis e. On another basis, say e′, the same vector v has different components \"v\"′ and\nAs a vector, v should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its \"real world\" direction and magnitude should appear the same regardless of the basis vectors. If we perform a change of basis by transforming the vectors e into the basis vectors e, we must also ensure that the components \"v\" transform into the new components \"v\" to compensate.\n\nThe needed transformation of v is called the contravariant transformation rule.\nIn the shown example, a vector formula_2 is described by two different coordinate systems: a rectangular coordinate system (the black grid), and a radial coordinate system (the red grid). Basis vectors have been chosen for both coordinate systems: e and e for the rectangular coordinate system, and e and e for the radial coordinate system. The radial basis vectors e and e appear rotated anticlockwise with respect to the rectangular basis vectors e and e. The covariant transformation, performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors.\n\nThe coordinates of v must be transformed into the new coordinate system, but the vector v itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates. The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view v from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors e and e. compared to how it appeared relative to the rectangular basis vectors e and e. Thus, the needed contravariant transformation to v in this example is a clockwise rotation.\n\nThe explicit form of a covariant transformation is best introduced with the transformation properties of the derivative of a function. Consider a scalar function \"f\" (like the temperature at a location in a space) defined on a set of points \"p\", identifiable in a given coordinate system formula_3 (such a collection is called a manifold). If we adopt a new coordinates system formula_4 then for each \"i\", the original coordinate formula_5 can be expressed as a function of the new coordinates, so formula_6 One can express the derivative of \"f\" in new coordinates in terms of the old coordinates, using the chain rule of the derivative, as\n\nThis is the explicit form of the covariant transformation rule. The notation of a normal derivative with respect to the coordinates sometimes uses a comma, as follows\nwhere the index \"i\" is placed as a lower index, because of the covariant transformation.\n\nA vector can be expressed in terms of basis vectors. For a certain coordinate system, we can choose the vectors tangent to the coordinate grid. This basis is called the coordinate basis.\n\nTo illustrate the transformation properties, consider again the set of points \"p\", identifiable in a given coordinate system formula_9 where formula_10 (manifold). A scalar function \"f\", that assigns a real number to every point \"p\" in this space, is a function of the coordinates formula_11. A curve is a one-parameter collection of points \"c\", say with curve parameter λ, \"c\"(λ). A tangent vector v to the curve is the derivative formula_12 along the curve with the derivative taken at the point \"p\" under consideration. Note that we can see the tangent vector v as an operator (the directional derivative) which can be applied to a function\n\nThe parallel between the tangent vector and the operator can also be worked out in coordinates\n\nor in terms of operators formula_15 \nwhere we have written formula_17, the tangent vectors to the curves which are simply the coordinate grid itself.\n\nIf we adopt a new coordinates system formula_18 then for each \"i\", the old coordinate formula_19 can be expressed as function of the new system, so formula_20\nLet formula_21 be the basis, tangent vectors in this new coordinates system. We can express formula_22 in the new system by applying the chain rule on \"x\". As a function of coordinates we find the following transformation\nwhich indeed is the same as the covariant transformation for the derivative of a function.\n\nThe \"components\" of a (tangent) vector transform in a different way, called contravariant transformation. Consider a tangent vector v and call its components formula_24 on a basis formula_22. On another basis formula_26 we call the components formula_27, so\nin which \n\nIf we express the new components in terms of the old ones, then\nThis is the explicit form of a transformation called the contravariant transformation and we note that it is different and just the inverse of the covariant rule. In order to distinguish them from the covariant (tangent) vectors, the index is placed on top.\n\nAn example of a contravariant transformation is given by a differential form \"df\". For \"f\" as a function of coordinates formula_9, \"df\" can be expressed in terms of formula_32. The differentials \"dx\" transform according to the contravariant rule since\n\nEntities that transform covariantly (like basis vectors) and the ones that transform contravariantly (like components of a vector and differential forms) are \"almost the same\" and yet they are different. They have \"dual\" properties.\nWhat is behind this, is mathematically known as the dual space that always goes together with a given linear vector space.\n\nTake any vector space T. A function \"f\" on T is called linear if, for any vectors v, w and scalar α:\n\nA simple example is the function which assigns a vector the value of one of its components (called a \"projection function\"). It has a vector as argument and assigns a real number, the value of a component.\n\nAll such \"scalar-valued\" linear functions together form a vector space, called the dual space of T. The sum \"f+g\" is again a linear function for linear \"f\" and \"g\", and the same holds for scalar multiplication α\"f\".\n\nGiven a basis formula_22 for T, we can define a basis, called the dual basis for the dual space in a natural way by taking the set of linear functions mentioned above: the projection functions. Each projection function (indexed by ω) produces the number 1 when applied to one of the basis vectors formula_22. For example, formula_37 gives a 1 on formula_38 and zero elsewhere. Applying this linear function formula_39 to a vector formula_40, gives (using its linearity)\nso just the value of the first coordinate. For this reason it is called the projection function.\n\nThere are as many dual basis vectors formula_42 as there are basis vectors formula_22, so the dual space has the same dimension as the linear space itself. It is \"almost the same space\", except that the elements of the dual space (called dual vectors) transform covariantly and the elements of the tangent vector space transform contravariantly.\n\nSometimes an extra notation is introduced where the real value of a linear function σ on a tangent vector u is given as\nwhere formula_45 is a real number. This notation emphasizes the bilinear character of the form. It is linear in σ since that is a linear function and it is linear in u since that is an element of a vector space.\n\nA tensor of type (\"r\", \"s\") may be defined as a real-valued multilinear function of \"r\" dual vectors and \"s\" vectors. Since vectors and dual vectors may be defined without dependence on a coordinate system, a tensor defined in this way is independent of the choice of a coordinate system.\n\nThe notation of a tensor is\nfor dual vectors (differential forms) \"ρ\", \"σ\" and tangent vectors formula_47. In the second notation the distinction between vectors and differential forms is more obvious.\n\nBecause a tensor depends linearly on its arguments, it is completely determined if one knows the values on a basis formula_48 and formula_49\nThe numbers formula_51 are called the components of the tensor on the chosen basis.\n\nIf we choose another basis (which are a linear combination of the original basis), we can use the linear properties of the tensor and we will find that the tensor components in the upper indices transform as dual vectors (so contravariant), whereas the lower indices will transform as the basis of tangent vectors and are thus covariant. For a tensor of rank 2, we can verify that\n\nFor a mixed co- and contravariant tensor of rank 2\n\n"}
{"id": "168609", "url": "https://en.wikipedia.org/wiki?curid=168609", "title": "Cycle (graph theory)", "text": "Cycle (graph theory)\n\nIn graph theory, a cycle is a path of edges and vertices wherein a vertex is reachable from itself.\nThere are several different types of cycles, principally a closed walk and a simple cycle; also, e.g., an element of the cycle space of the graph.\n\nA closed walk consists of a sequence of vertices starting and ending at the same vertex, with each two consecutive vertices in the sequence adjacent to each other in the graph. In a directed graph, each edge must be traversed by the walk consistently with its direction: the edge must be oriented from the earlier of two consecutive vertices to the later of the two vertices in the sequence. The choice of starting vertex is not important: traversing the same cyclic sequence of edges from different starting vertices produces the same closed walk.\n\nA simple cycle may be defined either as a closed walk with no repetitions of vertices and edges allowed, other than the repetition of the starting and ending vertex, or as the set of edges in such a walk. The two definitions are equivalent in directed graphs, where simple cycles are also called directed cycles: the cyclic sequence of vertices and edges in a walk is completely determined by the set of edges that it uses. In undirected graphs the set of edges of a cycle can be traversed by a walk in either of two directions, giving two possible directed cycles for every undirected cycle. (For closed walks more generally, in directed or undirected graphs, the multiset of edges does not unambiguously determine the vertex ordering.) A circuit can be a closed walk allowing repetitions of vertices but not edges; however, it can also be a simple cycle, so explicit definition is recommended when it is used.\n\nIn order to maintain a consistent terminology, for the rest of this article, \"cycle\" means a simple cycle, except where otherwise stated.\n\nA chordless cycle in a graph, also called a hole or an induced cycle, is a cycle such that no two vertices of the cycle are connected by an edge that does not itself belong to the cycle. An antihole is the complement of a graph hole. Chordless cycles may be used to characterize perfect graphs: by the strong perfect graph theorem, a graph is perfect if and only if none of its holes or antiholes have an odd number of vertices that is greater than three. A chordal graph, a special type of perfect graph, has no holes of any size greater than three.\n\nThe girth of a graph is the length of its shortest cycle; this cycle is necessarily chordless. Cages are defined as the smallest regular graphs with given combinations of degree and girth.\n\nA peripheral cycle is a cycle in a graph with the property that every two edges not on the cycle can be connected by a path whose interior vertices avoid the cycle. In a graph that is not formed by adding one edge to a cycle, a peripheral cycle must be an induced cycle.\n\nThe term \"cycle\" may also refer to an element of the cycle space of a graph. There are many cycle spaces, one for each coefficient field or ring. The most common is the \"binary cycle space\" (usually called simply the \"cycle space\"), which consists of the edge sets that have even degree at every vertex; it forms a vector space over the two-element field. By Veblen's theorem, every element of the cycle space may be formed as an edge-disjoint union of simple cycles. A cycle basis of the graph is a set of simple cycles that forms a basis of the cycle space.\n\nUsing ideas from algebraic topology, the binary cycle space generalizes to vector spaces or modules over other rings such as the integers, rational or real numbers, etc.\n\nThe existence of a cycle in directed and undirected graphs can be determined by whether depth-first search (DFS) finds an edge that points to an ancestor of the current vertex (it contains a back edge). In an undirected graph, finding any already visited vertex will indicate a back edge.\nAll the back edges which DFS skips over are part of cycles. In the case of undirected graphs, only \"O\"(\"n\") time is required to find a cycle in an \"n\"-vertex graph, since at most \"n\" − 1 edges can be tree edges.\n\nMany topological sorting algorithms will detect cycles too, since those are obstacles for topological order to exist. Also, if a directed graph has been divided into strongly connected components, cycles only exist within the components and not between them, since cycles are strongly connected.\n\nFor directed graphs, Rocha–Thatte Algorithm is a distributed cycle detection algorithm. Distributed cycle detection algorithms are useful for processing large-scale graphs using a distributed graph processing system on a computer cluster (or supercomputer).\n\nApplications of cycle detection include the use of wait-for graphs to detect deadlocks in concurrent systems.\n\nIn his 1736 paper on the Seven Bridges of Königsberg, widely considered to be the birth of graph theory, Leonhard Euler proved that, for a finite undirected graph to have a closed walk that visits each edge exactly once, it is necessary and sufficient that it be connected except for isolated vertices (that is, all edges are contained in one component) and have even degree at each vertex. The corresponding characterization for the existence of a closed walk visiting each edge exactly once in a directed graph is that the graph be strongly connected and have equal numbers of incoming and outgoing edges at each vertex. In either case, the resulting walk is known as an Euler cycle or Euler tour. If a finite undirected graph has even degree at each of its vertices, regardless of whether it is connected, then it is possible to find a set of simple cycles that together cover each edge exactly once: this is Veblen's theorem. When a connected graph does not meet the conditions of Euler's theorem, a closed walk of minimum length covering each edge at least once can nevertheless be found in polynomial time by solving the route inspection problem.\n\nThe problem of finding a single simple cycle that covers each vertex exactly once, rather than covering the edges, is much harder. Such a cycle is known as a Hamiltonian cycle, and determining whether it exists is NP-complete. Much research has been published concerning classes of graphs that can be guaranteed to contain Hamiltonian cycles; one example is Ore's theorem that a Hamiltonian cycle can always be found in a graph for which every non-adjacent pair of vertices have degrees summing to at least the total number of vertices in the graph.\n\nThe cycle double cover conjecture states that, for every bridgeless graph, there exists a multiset of simple cycles that covers each edge of the graph exactly twice. Proving that this is true (or finding a counterexample) remains an open problem.\n\nSeveral important classes of graphs can be defined by or characterized by their cycles. These include:\n\n"}
{"id": "5280990", "url": "https://en.wikipedia.org/wiki?curid=5280990", "title": "Domain coloring", "text": "Domain coloring\n\nIn mathematics, domain coloring or a color wheel graph is a technique for visualizing complex functions, which assigns a color to each point of the complex plane.\n\nA graph of a real function can be drawn in two dimensions, such as x and y. By contrast, a graph of a complex function (more precisely, a complex-valued function of one complex variable ) requires the visualization of four dimensions. One way to achieve that is with a Riemann surface, another by domain coloring.\n\nFor easy visibility, complex values are represented with colors. This assignment is called a \"color function\". There are many different color functions used. A common practice is to represent the complex argument (also known as \"phase\" or \"angle\") with a hue following the color wheel, and the magnitude by other means, such as brightness or saturation. \n\nThe following example colors the origin in black, in red, in cyan, and a point at infinity in white:\nwhere <math> 0.\n\nMore precisely, it uses the HLS (hue, lightness, saturation) color model. Saturation is always set at the maximum of 100%. Vivid colors of the rainbow are rotating in a continuous way on the complex unit circle, so the sixth roots of unity (starting with 1) are: red, yellow, green, cyan, blue, and magenta. Magnitude is coded by intensity via a strictly monotonic continuous function. \n\nSince the HSL color space is not perceptually uniform, one can see streaks of perceived brightness at yellow, cyan, and magenta (even though their absolute values are the same as red, green, and blue) and a halo around . Use of the Lab color space corrects this, making the images more accurate, but also makes them more drab/pastel.\nThe magnitude of a real number can range from to , a much wider range than that of the argument. Therefore, in a strictly monotonic continuous function that stretches the whole range compromises the resolution of smaller changes in magnitude. This can be achieved with a discontinuous color function, such as the following, which shows a repeating pattern for the magnitude. \n\nIn addition, this color function shows white rays for arguments , , , , , , , , , , , and a gray grid for equal real and imaginary values. A similar color function has been used for the graph on top of the article.\n\nThe method was probably first used in publication in the late 1980s by Larry Crone and Hans Lundmark.\n\nThe term \"domain coloring\" was coined by Frank Farris, possibly around 1998. There were many earlier uses of color to visualize complex functions, typically mapping argument (phase) to hue. The technique of using continuous color to map points from domain to codomain or image plane was used in 1999 by George Abdo and Paul Godfrey and colored grids were used in graphics by Doug Arnold that he dates to 1997.\n\nPeople who suffer color blindness may have trouble interpreting such graphs.\n\n"}
{"id": "24874263", "url": "https://en.wikipedia.org/wiki?curid=24874263", "title": "Donald A. Gillies", "text": "Donald A. Gillies\n\nDonald A. Gillies (; born 1944) is a British philosopher and historian of science and mathematics. He is an Emeritus Professor in the Department of Science and Technology Studies at University College, London.\n\nAfter undergraduate studies in mathematics and philosophy at Cambridge, Gillies became a graduate student of Karl Popper and Imre Lakatos at the London School of Economics, where he completed a PhD on the foundations of probability.\n\nGilles is a past President and a current Vice-President of British Society for the Philosophy of Science. From 1982 to 1985 he was an editor of the \"British Journal for the Philosophy of Science\".\n\nGillies is probably best known for his work on Bayesian confirmation theory, his attempt to simplify and extend Popper’s theory of corroboration. He proposes a novel \"principle of explanatory surplus\", likening a successful theoretician to a successful entrepreneur. The entrepreneur generates a surplus (of income) over and above his initial investment (of funds) to meet the necessary expenses of the enterprise. Similarly, the theoretician generates a surplus (of explanations) over and above his initial investment (of assumptions) to make the necessary explanations of known facts. The size of this surplus is held to be a measure of the confirmation of the theory — but only in qualitative, rather than quantitative, terms.\n\nGillies has researched the philosophy of science, most particularly the foundations of probability; the philosophy of logic and mathematics; and the interactions of artificial intelligence with some aspects of philosophy, including probability, logic, causality and scientific method.\n\n\n"}
{"id": "29902421", "url": "https://en.wikipedia.org/wiki?curid=29902421", "title": "Encyclopedic Dictionary of Mathematics", "text": "Encyclopedic Dictionary of Mathematics\n\nThe Encyclopedic Dictionary of Mathematics is a translation of the Japanese .\n\n\n"}
{"id": "56591592", "url": "https://en.wikipedia.org/wiki?curid=56591592", "title": "Engelbert–Schmidt zero–one law", "text": "Engelbert–Schmidt zero–one law\n\nThe Engelbert–Schmidt zero–one law is a theorem that gives a mathematical criterion for an event associated with a continuous, non-decreasing additive functional of Brownian motion to have probability either 0 or 1, without the possibility of an intermediate value. This zero-one law is used in the study of questions of finiteness and asymptotic behavior for stochastic differential equations. (A Wiener process is a mathematical formalization of Brownian motion used in the statement of the theorem.) This 0-1 law, published in 1981, is named after Hans-Jürgen Engelbert and the probabilist Wolfgang Schmidt (not to be confused with the number theorist Wolfgang M. Schmidt).\n\nLet formula_1 be a σ-algebra and let formula_2 be an increasing family of sub-\"σ\"-algebras of formula_1. Let formula_4 be a Wiener process on the probability space formula_5.\nSuppose that formula_6 is a Borel measurable function of the real line into [0,∞].\nThen the following three assertions are equivalent:\n\n(i) formula_7\n\n(ii) formula_8\n\n(iii) formula_9\n\nfor all compact subsets formula_10 of the real line.\n\n"}
{"id": "27646966", "url": "https://en.wikipedia.org/wiki?curid=27646966", "title": "Entanglement (graph measure)", "text": "Entanglement (graph measure)\n\nIn graph theory, entanglement of a directed graph is a number measuring how strongly\nthe cycles of the graph are intertwined. It is defined in terms of a mathematical game in which\n\"n\" cops try to capture a robber, who escapes along the edges of the graph. Similar to other\ngraph measures, such as cycle rank, some algorithmic problems, e.g. parity game, can be\nefficiently solved on graphs of bounded entanglement.\n\nThe entanglement game is played by \"n\" cops against a robber on\na directed graph \"G\". Initially, all cops are outside the graph and the robber selects an arbitrary starting vertex\n\"v\" of \"G\". Further on, the players move in turn. In each move the cops either stay where they are, or place one\nof them on the vertex currently occupied by the robber. The robber must move from her current vertex, along an edge,\nto a successor that is not occupied by a cop. The robber must move if no cop is following him. If there is\nno free successor to which the robber can move, she is caught, and the cops win. The robber wins if she\ncannot be caught, i.e. if the play can be made to last forever.\n\nA graph \"G\" has entanglement \"n\" if \"n\" cops win in the entanglement game on \"G\" but \"n\" − 1 cops lose the game.\n\nGraphs of entanglement zero and one can be characterized as follows:\n\nEntanglement has also been a key notion in proving that the variable hierarchy\nof the modal mu calculus is strict.\n\nYou can play the entanglement game online on tPlay.\n"}
{"id": "46900621", "url": "https://en.wikipedia.org/wiki?curid=46900621", "title": "Evolution of a random network", "text": "Evolution of a random network\n\nEvolution of a random network is a dynamical process, usually leading to emergence of giant component accompanied with striking consequences on the network topology. To quantify this process, there is a need of inspection on how the size of the largest connected cluster within the network, formula_1, varies with formula_2. Networks change their topology as they evolve, undergoing phase transitions. Phase transitions are generally known from physics, where it occurs as matter changes state according to its thermal energy level, or when ferromagnetic properties emerge in some materials as they are cooling down. Such phase transitions take place in matter because it is a network of particles, and as such, rules of network phase transition directly apply to it. Phase transitions in networks happen as links are added to a network, meaning that having N nodes, in each time increment, a link is placed between a randomly chosen pair of them. The transformation from a set of disconnected nodes to a fully connected network is called the evolution of a network.\n\nIf we begin with a network having N totally disconnected nodes (number of links is zero), and start adding links between randomly selected pairs of nodes, the evolution of the network begins. For some time we will just create pairs of nodes. After a while some of these pairs will connect, forming little trees. As we continue adding more links to the network, there comes a point when a giant component emerges in the network as some of these isolated trees connect to each other. This is called the critical point. In our natural example, this point corresponds to temperatures where materials change their state. Further adding nodes to the system, the giant component becomes even larger, as more and more nodes get a link to an other node which is already part of the giant component. The other special moment in this transition is when the network becomes fully connected, that is, when all nodes belong to the one giant component, which is effectively the network itself at that point.\n\nCondition for the emergence of the giant component was predicted by Erdős and Renyi in their paper:\n\nformula_3, where formula_2 is the average degree of a random network.\nThus, one link is sufficient for its emergence of the giant component.\nIf expressing the condition in terms of formula_5, one obtain:\nformula_6 (1)\n\nWhew formula_7 is the number of nodes, formula_8 is the probability of clustering. \nTherefore, the larger a network, the smaller formula_5 is sufficient for the giant component.\n\nThree topological regimes with its unique characteristics can be distinguished in network science: subcritical, supercritical and connected regimes.\n\nThe subcritical phase is characterised by small isolated clusters, as the number of links is\nmuch less than the number of nodes. A giant component can be designated any time to be the largest isolated small component, but the difference in cluster sizes is effectively negligible in this phase.\nformula_10, formula_11\n\nFor formula_12 the network consists of formula_7 isolated nodes. Increasing formula_2 means adding formula_15 links to the network. Yet, given that formula_16, there is only a small number of links in this regime, hence mainly tiny clusters could be observed. \nAt any moment the largest cluster can be designated to be the giant component. Yet in this regime the relative size of the largest cluster,formula_17, remains zero. The reason is that for formula_16 the largest cluster is a tree with size formula_19, hence its size increases much slower than the size of the network.\nTherefore, formula_20 in the formula_21 limit.\nIn summary, in the subcritical regime the network consists of numerous tiny components, whose size follows the exponential distribution. Hence, these components have comparable sizes, lacking a clear winner that we could designate as a giant.\n\nAs we keep connecting nodes, the pairs joining together will form small trees, and if we keep\nconnecting nodes, a distinguishable giant component emerges at critical point <k> = 1.\n\nThis means that at the moment that each component has on average 1 link, a giant component emerges. This point corresponds to probability p = 1/ (N-1), as the probability of having a link between two nodes is the\nratio of the one case when that one link connect the two randomly chosen nodes, divided by all the other possibilities how that one connection can connect one of the nodes to an other node, which is N-1, as a node can connect to all other nodes but itself (excluding the possibility of a self loop in this model).\n\nThis also has the implication, that the larger a network is, the smaller p\nit needs to have a giant component emerging.\nformula_22, formula_23.\n\nThe critical point separates the regime where there is not yet a giant component ( formula_10 ) from the regime where there is one ( formula_25 ). At this point, the relative size of the largest component is still zero. Indeed, the size of the largest component is formula_26. Consequently, formula_1 grows much slower than the network's size, so its relative size decreases as formula_28 in the formula_21 limit.\nNote, however, that in absolute terms there is a significant jump in the size of the largest component at formula_3.\nFor example, for a random network with formula_31 nodes, comparable to the globe's social network, for formula_3 the largest cluster is of the order of formula_33. In contrast at formula_34 we expect formula_35, a jump of about five orders of magnitude. Yet, both in the subcritical regime and at the critical point the largest component contains only a vanishing fraction of the total number of nodes in the network.\nIn summary, at the critical point most nodes are located in numerous small components, whose size distribution follows. The power law form indicates that components of rather different sizes coexist. These numerous small components are mainly trees, while the giant component may contain loops. Note that many properties of the network at the critical point resemble the properties of a physical system undergoing a phase.\n\nOnce the giant component had emerged upon passing the critical point, as we add more connections, the network will consist of a growing giant component, and less and less smaller isolated clusters and nodes.\nMost real networks belong to ths regime. The size of the giant component is described as follows Ng = (p – pc) N.\nformula_36, formula_37.\n\nThis regime has the most relevance to real systems, as for the first time we have a giant component that looks like a network. In the vicinity of the critical point the size of the giant component varies as:\n\nformula_38 \n\nor \n\nformula_39 (2)\nwhere pc is given by (1). In other words, the giant component contains a finite fraction of the nodes. The further we move from the critical point, a larger fraction of nodes will belong to it. Note that (2) is valid only in the vicinity of formula_40. For large formula_41 the dependence between formula_1 and formula_41 is nonlinear.\nIn summary in the supercritical regime numerous isolated components coexist with the giant component, their size distribution following exponential distribution. These small components are trees, while the giant component contains Loops and cycles. The supercritical regime lasts until all nodes are absorbed by the giant.\n\nAs connections are being added to a network there comes a point when <k> = lnN, and the giant\ncomponent absorbs all nodes making the network fully connected, having a complete graph.\nformula_44, formula_45.\n\nFor sufficiently large p the giant component absorbs all nodes and components, hence formula_46. In the absence of isolated nodes the network becomes connected. The average degree at which this happens depends on formula_7 as formula_48. Note that when we enter the connected regime the network is still relatively sparse, as formula_48 for large N. The network turns into a complete graph only at formula_50.\nIn summary, the random network model predicts that the emergence of a network is not a smooth, gradual process: The isolated nodes and tiny components observed for small <k> collapse into a giant component through a phase.\n\nPhase transitions take place in matter, as it can be considered as a network of particles. When water is frozen, upon reaching 0 degree (the critical point) the crystalline structure of ice emerges according to the phase transitions of random networks: As cooling continues, each water molecule binds strongly to four others, forming the ice lattice, which is the emerging network.\n\nSimilarly, magnetic phase transition in ferromagnetic materials also follow the pattern of network evolution: Above a certain temperature, spins of individual atoms can point in two different directions. However, upon cooling the material down, upon reaching a certain critical temperature, spins start to point in the same direction, creating the magnetic field. The emergence of magnetic properties in the structure of the material resemble to the evolution of a random network.\n\nAs we could see in the above examples, network theory applies to the structure of materials, therefore it is also applied in research related to materials and their properties in physics and chemistry.\n\nParticularly important areas are polymers, gels, and other material development such as cellulose with tunable properties.\n\nPhase transitions are used in research related to the functioning of proteins or emergence of diabetes on the cell-level. Neuroscience also extensively makes use of the model of the evolution of networks as phase-transition occur in neuron-networks.\n\nPhase transition of a network is naturally a building block of more advanced models within its own discipline too. It comes back in research examining clustering and percolation in networks, or prediction of node properties.\n"}
{"id": "409293", "url": "https://en.wikipedia.org/wiki?curid=409293", "title": "Factorial prime", "text": "Factorial prime\n\nA factorial prime is a prime number that is one less or one more than a factorial (all factorials > 1 are even). \n\nThe first 10 factorial primes (for n = 1, 2, 3, 4, 6, 7, 11, 12, 14) are :\n\n\"n\"! − 1 is prime for :\n\n\"n\"! + 1 is prime for :\n\nNo other factorial primes are known .\n\nAbsence of primes to both sides of a factorial \"n\"! implies a run of at least 2\"n\"+1 consecutive composite numbers, since \"n\"! ± \"k\" is divisible by \"k\" for 2 ≤ \"k\" ≤ \"n\". However, the necessary length of this run is asymptotically smaller than the average composite run for integers of similar size (see prime gap).\n\n\n"}
{"id": "17243191", "url": "https://en.wikipedia.org/wiki?curid=17243191", "title": "Geometric lattice", "text": "Geometric lattice\n\nIn the mathematics of matroids and lattices, a geometric lattice is a finite atomistic semimodular lattice, and a matroid lattice is an atomistic semimodular lattice without the assumptions of finiteness. Geometric lattices and matroid lattices, respectively, form the lattices of flats of finite and infinite matroids, and every geometric or matroid lattice comes from a matroid in this way.\n\nA lattice is a poset in which any two elements formula_1 and formula_2 have both a supremum, denoted by formula_3, and an infimum, denoted by formula_4.\n\nThe geometric lattices are cryptomorphic to (finite, simple) matroids, and the matroid lattices are cryptomorphic to simple matroids without the assumption of finiteness.\n\nLike geometric lattices, matroids are endowed with rank functions, but these functions map sets of elements to numbers rather than taking individual elements as arguments. The rank function of a matroid must be monotonic (adding an element to a set can never decrease its rank) and they must be submodular functions, meaning that they obey an inequality similar to the one for semimodular lattices:\n\nThe maximal sets of a given rank are called flats. The intersection of two flats is again a flat, defining a greatest lower bound operation on pairs of flats; one can also define a least upper bound of a pair of flats to be the (unique) maximal superset of their union that has the same rank as their union. In this way, the flats of a matroid form a matroid lattice, or (if the matroid is finite) a geometric lattice.\n\nConversely, if formula_26 is a matroid lattice, one may define a rank function on sets of its atoms, by defining the rank of a set of atoms to be the lattice rank of the greatest lower bound of the set. This rank function is necessarily monotonic and submodular, so it defines a matroid. This matroid is necessarily simple, meaning that every two-element set has rank two.\n\nThese two constructions, of a simple matroid from a lattice and of a lattice from a matroid, are inverse to each other: starting from a geometric lattice or a simple matroid, and performing both constructions one after the other, gives a lattice or matroid that is isomorphic to the original one.\n\nThere are two different natural notions of duality for a geometric lattice formula_26: the dual matroid, which has as its basis sets the complements of the bases of the matroid corresponding to formula_26, and the dual lattice, the lattice that has the same elements as formula_26 in the reverse order. They are not the same, and indeed the dual lattice is generally not itself a geometric lattice: the property of being atomistic is not preserved by order-reversal. defines the adjoint of a geometric lattice formula_26 (or of the matroid defined from it) to be a minimal geometric lattice into which the dual lattice of formula_26 is order-embedded. Some matroids do not have adjoints; an example is the Vámos matroid.\n\nEvery interval of a geometric lattice (the subset of the lattice between given lower and upper bound elements) is itself geometric; taking an interval of a geometric lattice corresponds to forming a minor of the associated matroid. Geometric lattices are complemented, and because of the interval property they are also relatively complemented.\n\nEvery finite lattice is a sublattice of a geometric lattice.\n"}
{"id": "2142850", "url": "https://en.wikipedia.org/wiki?curid=2142850", "title": "Glossary of probability and statistics", "text": "Glossary of probability and statistics\n\n\"Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself. However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together. You can help enhance this page by adding new terms or writing definitions for existing ones.\"\nThe following is a glossary of terms used in the mathematical sciences statistics and probability.\n\n\n"}
{"id": "11720315", "url": "https://en.wikipedia.org/wiki?curid=11720315", "title": "Hilbert's theorem (differential geometry)", "text": "Hilbert's theorem (differential geometry)\n\nIn differential geometry, Hilbert's theorem (1901) states that there exists no complete regular surface formula_1 of constant negative gaussian curvature formula_2 immersed in formula_3. This theorem answers the question for the negative case of which surfaces in formula_3 can be obtained by isometrically immersing complete manifolds with constant curvature.\n\n\nThe proof of Hilbert's theorem is elaborate and requires several lemmas. The idea is to show the nonexistence of an isometric immersion \n\nof a plane formula_6 to the real space formula_3. This proof is basically the same as in Hilbert's paper, although based in the books of Do Carmo and Spivak.\n\n\"Observations\": In order to have a more manageable treatment, but without loss of generality, the curvature may be considered equal to minus one, formula_8. There is no loss of generality, since it is being dealt with constant curvatures, and similarities of formula_3 multiply formula_2 by a constant. The exponential map formula_11 is a local diffeomorphism (in fact a covering map, by Cartan-Hadamard theorem), therefore, it induces an inner product in the tangent space of formula_1 at formula_13: formula_14. Furthermore, formula_6 denotes the geometric surface formula_14 with this inner product. If formula_17 is an isometric immersion, the same holds for \n\nThe first lemma is independent from the other ones, and will be used at the end as the counter statement to reject the results from the other lemmas.\n\nLemma 1: The area of formula_6 is infinite. \n\"Proof's Sketch:\" \nThe idea of the proof is to create a global isometry between formula_20 and formula_6. Then, since formula_20 has an infinite area, formula_6 will have it too. \nThe fact that the hyperbolic plane formula_20 has an infinite area comes by computing the surface integral with the corresponding coefficients of the First fundamental form. To obtain these ones, the hyperbolic plane can be defined as the plane with the following inner product around a point formula_25 with coordinates formula_26\n\nSince the hyperbolic plane is unbounded, the limits of the integral are infinite, and the area can be calculated through\n\nNext it is needed to create a map, which will show that the global information from the hyperbolic plane can be transfer to the surface formula_6, i.e. a global isometry. formula_30 will be the map, whose domain is the hyperbolic plane and image the 2-dimensional manifold formula_6, which carries the inner product from the surface formula_1 with negative curvature. formula_33 will be defined via the exponential map, its inverse, and a linear isometry between their tangent spaces, \n\nThat is \n\nwhere formula_36. That is to say, the starting point formula_37 goes to the tangent plane from formula_20 through the inverse of the exponential map. Then travels from one tangent plane to the other through the isometry formula_39, and then down to the surface formula_6 with another exponential map.\n\nThe following step involves the use of polar coordinates, formula_41 and formula_42, around formula_13 and formula_44 respectively. The requirement will be that the axis are mapped to each other, that is formula_45 goes to formula_46. Then formula_33 preserves the first fundamental form. \nIn a geodesic polar system, the Gaussian curvature formula_2 can be expressed as \n\nIn addition K is constant and fulfills the following differential equation \n\nSince formula_20 and formula_6 have the same constant Gaussian curvature, then they are locally isometric (Minding's Theorem). That means that formula_33 is a local isometry between formula_20 and formula_6. Furthermore, from the Hadamard's theorem it follows that formula_33 is also a covering map. \nSince formula_6 is simply connected, formula_33 is a homeomorphism, and hence, a (global) isometry. Therefore, formula_20 and formula_6 are globally isometric, and because formula_20 has an infinite area, then formula_62 has an infinite area, as well. formula_63\n\nLemma 2: For each formula_64 exists a parametrization formula_65, such that the coordinate curves of formula_66 are asymptotic curves of formula_67 and form a Tchebyshef net.\n\nLemma 3: Let formula_68 be a coordinate neighborhood of formula_6 such that the coordinate curves are asymptotic curves in formula_70. Then the area A of any quadrilateral formed by the coordinate curves is smaller than formula_71.\n\nThe next goal is to show that formula_66 is a parametrization of formula_6.\n\nLemma 4: For a fixed formula_74, the curve formula_75, is an asymptotic curve with formula_76 as arc length.\n\nThe following 2 lemmas together with lemma 8 will demonstrate the existence of a parametrization formula_77\n\nLemma 5: formula_66 is a local diffeomorphism.\n\nLemma 6: formula_66 is surjective.\n\nLemma 7: On formula_6 there are two differentiable linearly independent vector fields which are tangent to the asymptotic curves of formula_6.\n\nLemma 8: formula_66 is injective.\n\n\"Proof of Hilbert's Theorem:\" \nFirst, it will be assumed that an isometric immersion from a complete surface with negative curvatureformula_1 exists: formula_17\n\nAs stated in the observations, the tangent plane formula_14 is endowed with the metric induced by the exponential map formula_11 . Moreover, formula_87 is an isometric immersion and Lemmas 5,6, and 8 show the existence of a parametrization formula_77 of the whole formula_6, such that the coordinate curves of formula_66 are the asymptotic curves of formula_6. This result was provided by Lemma 4. Therefore, formula_6 can be covered by a union of \"coordinate\" quadrilaterals formula_93 with formula_94. By Lemma 3, the area of each quadrilateral is smaller than formula_95. On the other hand, by Lemma 1, the area of formula_6 is infinite, therefore has no bounds. This is a contradiction and the proof is concluded. formula_63\n\n\n"}
{"id": "42633078", "url": "https://en.wikipedia.org/wiki?curid=42633078", "title": "Horizontal translation", "text": "Horizontal translation\n\nIn function graphing, a horizontal translation is a transformation which results in a graph that is equivalent to shifting the base graph left or right in the direction of the \"x\"-axis. A graph is translated \"k\" units horizontally by moving each point on the graph \"k\" units horizontally.\n\nFor the base function \"f\"(\"x\") and a constant \"k\", the function given by \"g\"(\"x\") = \"f\"(\"x\" − \"k\"), can be sketched \"f\"(\"x\") shifted \"k\" units horizontally.\n\nIf function transformation was talked about in terms of geometric transformations it may be clearer why functions translate horizontally the way they do. When addressing translations on the Cartesian plane it is natural to introduce translations in this type of notation:\n\nformula_1\n\nor\n\nformula_2\n\nwhere formula_3 and formula_4 are horizontal and vertical changes respectively..\n\nTaking the parabola \"y\" = \"x\" , a horizontal translation 5 units to the right would be represented by \"T\"((\"x\",\"y\")) = (\"x\" + 5, \"y\"). Now we must connect this transformation notation to an algebraic notation. Consider the point (\"a\".\"b\") on the original parabola that moves to point (\"c\",\"d\") on the translated parabola. According to our translation, \"c\" = \"a\" + 5 and \"d\" = \"b\". The point on the original parabola was \"b\" = \"a\". Our new point can be described by relating \"d\" and \"c\" in the same equation. \"b\" = \"d\" and \"a\" = \"c\"  − 5.\nSo \"d\" = \"b\" = \"a\" = (\"c\" − 5) Since this is true for all the points on our new parabola the new equation is \"y\" = (\"x\" − 5).\n\n\n"}
{"id": "54262912", "url": "https://en.wikipedia.org/wiki?curid=54262912", "title": "Hypersequent", "text": "Hypersequent\n\nIn mathematical logic, the hypersequent framework is an extension of the proof-theoretical framework of sequent calculi used in structural proof theory to provide analytic calculi for logics which are not captured in the sequent framework. A hypersequent is usually taken to be a finite multiset of ordinary sequents, written\n\nThe sequents making up a hypersequent are called components. The added expressivity of the hypersequent framework is provided by rules manipulating different components, such as the communication rule for intermediate logic LC (below left) or the modal splitting rule for modal logic S5 (below right):\n\nHypersequent calculi have been used to treat modal logics, intermediate logics, and substructural logics. Hypersequents usually have a formula interpretation, i.e., are interpreted by a formula in the object language, nearly always as some kind of disjunction. The precise formula interpretation depends on the considered logic.\n\nFormally, a hypersequent is usually taken to be a finite multiset of ordinary sequents, written\n\nThe sequents making up a hypersequent consist of tuples of multisets of formulae, and are called the components of the hypersequent. Variants defining hypersequents and sequents in terms of sets or lists instead of multisets are also considered, and depending on the considered logic the sequents can be classical or intuitionistic. The rules for the propositional connectives usually are adaptions of the corresponding standard sequent rules with an additional side hypersequent, also called hypersequent context. E.g., a common set of rules for the functionally complete set of connectives formula_5 for classical propositional logic is given by the following four rules:\n\nformula_9\n\nDue to the additional structure in the hypersequent setting the structural rules are considered in their internal and external variants. The internal weakening and internal contraction rules are the adaptions of the corresponding sequent rules with an added hypersequent context:\n\nformula_11\n\nformula_12\n\nThe external weakening and external contraction rules are the corresponding rules on the level of hypersequent components instead of formulae:\n\nformula_13\n\nformula_14\n\nSoundness of these rules is closely connected to the formula interpretation of the hypersequent structure, nearly always as some form of disjunction. The precise formula interpretation depends on the considered logic, see below for some examples.\n\nHypersequents have been used to obtain analytic calculi for modal logics, for which analytic sequent calculi proved elusive. In the context of modal logics the standard formula interpretation of a hypersequent\n\nis the formula\n\nHere if formula_17 is the multiset formula_18 we write formula_19 for the result of prefixing every formula in formula_17 with formula_21, i.e., the multiset formula_22. Note that the single components are interpreted using the standard formula interpretation for sequents, and the hypersequent bar formula_23 is interpreted as a disjunction of boxes.The prime example of a modal logic for which hypersequents provide an analytic calculus is the logic S5. In a standard hypersequent calculus for this logic the formula interpretation is as above, and the propositional and structural rules are the ones from the previous section. Additionally, the calculus contains the modal rules\n\nformula_25\n\nformula_26\n\nAdmissibility of a suitably formulated version of the cut rule can be shown by a syntactic argument on the structure of derivations or by showing completeness of the calculus without the cut rule directly using the semantics of S5. In line with the importance of modal logic S5, a number of alternative calculi have been formulated. Hypersequent calculi have also been proposed for many other modal logics.\n\nHypersequent calculi based on intuitionistic or single-succedent sequents have been used successfully to capture a large class of intermediate logics, i.e., extensions of intuitionistic propositional logic. Since the hypersequents in this setting are based on single-succedent sequents, they have the following form:\n\nThe standard formula interpretation for such an hypersequent is\n\nMost hypersequent calculi for intermediate logics include the single-succedent versions of the propositional rules given above, a selection of the structural rules. The characteristics of a particular intermediate logic are mostly captured using a number of additional structural rules. E.g., the standard calculus for intermediate logic LC, sometimes also called Gödel–Dummett logic, contains additionally the so-called communication rule:\n\nHypersequent calculi for many other intermediate logics have been introduced, and there are very general results about cut elimination in such calculi.\n\nAs for intermediate logics, hypersequents have been used to obtain analytic calculi for many substructural logics and fuzzy logics.\n\nThe hypersequent structure seem to have appeared first in under the name of cortege to obtain a calculus for modal logic S5. It seems to have been developed independently in, also for treating modal logics, and in the influential, where calculi for modal, intermediate and substructural logics are considered, and the term hypersequent is introduced.\n"}
{"id": "251366", "url": "https://en.wikipedia.org/wiki?curid=251366", "title": "Hypotenuse", "text": "Hypotenuse\n\nIn geometry, a hypotenuse is the longest side of a right-angled triangle, the side opposite of the right angle. The length of the hypotenuse of a right triangle can be found using the Pythagorean theorem, which states that the square of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides.\nFor example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the square root of 25, that is, 5.\n\nThe word \"hypotenuse\" is derived from Greek (sc. or ), meaning \"[side] subtending the right angle\" (Apollodorus), \nThe Greek term was loaned into Late Latin, \nas \"hypotēnūsa\". \nAdoption as a learned Latinism used in modern languages dates to the 16th century. The spelling in \"-e\", as \"hypotenuse\", is French in origin (Estienne de La Roche 1520).\n\nThe length of the hypotenuse is calculated using the square root function implied by the Pythagorean theorem. Using the common notation that the length of the two legs of the triangle (the sides perpendicular to each other) are \"a\" and \"b\" and that of the hypotenuse is \"c\", we have\n\nThe Pythagorean theorem, and hence this length, can also be derived from the law of cosines by observing that the angle opposite the hypotenuse is 90° and noting that its cosine is 0:\n\nMany computer languages support the ISO C standard function hypot(\"x\",\"y\"), which returns the value above. The function is designed not to fail where the straightforward calculation might overflow or underflow and can be slightly more accurate.\n\nSome scientific calculators provide a function to convert from rectangular coordinates to polar coordinates. This gives both the length of the hypotenuse and the angle the hypotenuse makes with the base line (\"c\" above) at the same time when given \"x\" and \"y\". The angle returned is normally given by atan2(\"y\",\"x\").\n\nOrthographic projections:\n\n\n\nBy means of trigonometric ratios, one can obtain the value of two acute angles, formula_3 and formula_4, of the right triangle.\n\nGiven the length of the hypotenuse formula_5 and of a cathetus formula_6, the ratio is:\n\nThe trigonometric inverse function is:\n\nin which formula_9 is the angle opposite the cathetus formula_6.\n\nThe adjacent angle of the catheti formula_6 is formula_3 = 90° – formula_9\n\nOne may also obtain the value of the angle formula_9 by the equation:\n\nin which formula_16 is the other cathetus.\n\n\n"}
{"id": "1222310", "url": "https://en.wikipedia.org/wiki?curid=1222310", "title": "IBM 4758", "text": "IBM 4758\n\nThe IBM 4758 PCI Cryptographic Coprocessor is a secure cryptoprocessor implemented on a high-security, tamper resistant, programmable PCI board. Specialized cryptographic electronics, microprocessor, memory, and random number generator housed within a tamper-responding environment provide a highly secure subsystem in which data processing and cryptography can be performed. \n\nIBM supplies two cryptographic-system implementations, and toolkits for custom application development:\n\nAs of June 2005, the 4758 was discontinued and was replaced by an improved, faster model called the IBM 4764.\n\n"}
{"id": "26292477", "url": "https://en.wikipedia.org/wiki?curid=26292477", "title": "Krichevsky–Trofimov estimator", "text": "Krichevsky–Trofimov estimator\n\nIn information theory, given an unknown stationary source π with alphabet \"A\" and a sample \"w\" from π, the Krichevsky–Trofimov (KT) estimator produces an estimate p(\"w\") of the probability of each symbol \"i\" ∈ \"A\". This estimator is optimal in the sense that it minimizes the worst-case regret asymptotically.\n\nFor a binary alphabet and a string \"w\" with \"m\" zeroes and \"n\" ones, the KT estimator p(\"w\") is defined as:\n\n"}
{"id": "10399346", "url": "https://en.wikipedia.org/wiki?curid=10399346", "title": "Lagrange multipliers on Banach spaces", "text": "Lagrange multipliers on Banach spaces\n\nIn the field of calculus of variations in mathematics, the method of Lagrange multipliers on Banach spaces can be used to solve certain infinite-dimensional constrained optimization problems. The method is a generalization of the classical method of Lagrange multipliers as used to find extrema of a function of finitely many variables.\n\nLet \"X\" and \"Y\" be real Banach spaces. Let \"U\" be an open subset of \"X\" and let \"f\" : \"U\" → R be a continuously differentiable function. Let \"g\" : \"U\" → \"Y\" be another continuously differentiable function, the \"constraint\": the objective is to find the extremal points (maxima or minima) of \"f\" subject to the constraint that \"g\" is zero.\n\nSuppose that \"u\" is a \"constrained extremum\" of \"f\", i.e. an extremum of \"f\" on\n\nSuppose also that the Fréchet derivative D\"g\"(\"u\") : \"X\" → \"Y\" of \"g\" at \"u\" is a surjective linear map. Then there exists a Lagrange multiplier \"λ\" : \"Y\" → R in \"Y\", the dual space to \"Y\", such that\n\nSince D\"f\"(\"u\") is an element of the dual space \"X\", equation (L) can also be written as\n\nwhere (D\"g\"(\"u\"))(\"λ\") is the pullback of \"λ\" by D\"g\"(\"u\"), i.e. the action of the adjoint map (D\"g\"(\"u\")) on \"λ\", as defined by\n\nIn the case that \"X\" and \"Y\" are both finite-dimensional (i.e. linearly isomorphic to R and R for some natural numbers \"m\" and \"n\") then writing out equation (L) in matrix form shows that \"λ\" is the usual Lagrange multiplier vector; in the case \"n\" = 1, \"λ\" is the usual Lagrange multiplier, a real number.\n\nIn many optimization problems, one seeks to minimize a functional defined on an infinite-dimensional space such as a Banach space.\n\nConsider, for example, the Sobolev space \"X\" = \"H\"([−1, +1]; R) and the functional \"f\" : \"X\" → R given by\n\nWithout any constraint, the minimum value of \"f\" would be 0, attained by \"u\"(\"x\") = 0 for all \"x\" between −1 and +1. One could also consider the constrained optimization problem, to minimize \"f\" among all those \"u\" ∈ \"X\" such that the mean value of \"u\" is +1. In terms of the above theorem, the constraint \"g\" would be given by\n\nHowever this problem can be solved as in the finite dimensional case since the Lagrange multiplier formula_7 is only a scalar.\n\n\n"}
{"id": "27478537", "url": "https://en.wikipedia.org/wiki?curid=27478537", "title": "Leggett inequality", "text": "Leggett inequality\n\nThe Leggett inequalities, named for Anthony James Leggett, who derived them, are a related pair of mathematical expressions concerning the correlations of properties of entangled particles. (As published by Leggett, the inequalities were exemplified in terms of relative angles of elliptical and linear polarizations.) They are fulfilled by a large class of physical theories based on particular non-local and realistic assumptions, that may be considered to be plausible or intuitive according to common physical reasoning.\n\nThe Leggett inequalities are violated by quantum mechanical theory. The results of experimental tests in 2007 and 2010 have shown agreement with quantum mechanics rather than the Leggett inequalities. Given that experimental tests of Bell's inequalities have ruled out local realism in quantum mechanics, the violation of Leggett's inequalities is considered to have falsified realism in quantum mechanics. In quantum mechanics \"realism\" means \"notion that physical systems possess complete sets of definite values for various parameters prior to, and independent of, measurement\".\n\n\n"}
{"id": "5257795", "url": "https://en.wikipedia.org/wiki?curid=5257795", "title": "Martingale pricing", "text": "Martingale pricing\n\nMartingale pricing is a pricing approach based on the notions of martingale and risk neutrality. The martingale pricing approach is a cornerstone of modern quantitative finance and can be applied to a variety of derivatives contracts, e.g. options, futures, interest rate derivatives, credit derivatives, etc.\n\nIn contrast to the PDE approach to pricing, martingale pricing formulae are in the form of expectations which can be efficiently solved numerically using a Monte Carlo approach. As such, Martingale pricing is preferred when valuing high-dimensional contracts such as a basket of options. On the other hand, valuing American-style contracts is troublesome and requires discretizing the problem (making it like a Bermudan option) and only in 2001 F. A. Longstaff and E. S. Schwartz developed a practical Monte Carlo method for pricing American options.\n\nSuppose the state of the market can be represented by the filtered probability space,formula_1. Let formula_2 be a stochastic price process on this space. One may price a derivative security, formula_3 under the philosophy of no arbitrage as,\n\nformula_4\n\nwhere formula_5 is the risk-neutral measure.\n\nformula_6 is an formula_7-measurable (risk-free, possibly stochastic) interest rate process.\n\nThis is accomplished through almost sure replication of the derivative's time formula_8 payoff using only underlying securities, and the risk-free money market (MMA). These underlyings have prices that are observable and known.\nSpecifically, one constructs a portfolio process formula_9 in continuous time, where he holds formula_10 shares of the underlying stock at each time formula_11, and formula_12 cash earning the risk-free rate formula_13. The portfolio obeys the stochastic differential equation\n\nformula_14\n\nOne will then attempt to apply Girsanov theorem by first computing formula_15; that is, the Radon–Nikodym derivative with respect to the observed market probability distribution. This ensures that the discounted replicating portfolio process is a Martingale under risk neutral conditions.\n\nIf such a process formula_10 can be well-defined and constructed, then choosing formula_17 will result in formula_18, which immediately implies that this happens formula_19-almost surely as well, since the two measures are equivalent.\n\n"}
{"id": "398931", "url": "https://en.wikipedia.org/wiki?curid=398931", "title": "Measure-preserving dynamical system", "text": "Measure-preserving dynamical system\n\nIn mathematics, a measure-preserving dynamical system is an object of study in the abstract formulation of dynamical systems, and ergodic theory in particular.\n\nDespite the name, there are important \"static applications\", as in the Equal-area map projections. \n\nA measure-preserving dynamical system is defined as a probability space and a measure-preserving transformation on it. In more detail, it is a system\n\nwith the following structure:\n\n\nThis definition can be generalized to the case in which \"T\" is not a single transformation that is iterated to give the dynamics of the system, but instead is a monoid (or even a group) of transformations \"T\" : \"X\" → \"X\" parametrized by \"s\" ∈ Z (or R, or N ∪ {0}, or [0, +∞)), where each transformation \"T\" satisfies the same requirements as \"T\" above. In particular, the transformations obey the rules:\n\nThe earlier, simpler case fits into this framework by defining\"T\" = \"T\" for \"s\" ∈ N.\n\nThe existence of invariant measures for certain maps and Markov processes is established by the Krylov–Bogolyubov theorem.\n\n Examples include:\n\n\nThe concept of a homomorphism and an isomorphism may be defined.\n\nConsider two dynamical systems formula_12 and formula_13. Then a mapping\n\nis a homomorphism of dynamical systems if it satisfies the following three properties:\n\n\nThe system formula_13 is then called a factor of formula_12.\n\nThe map formula_22 is an isomorphism of dynamical systems if, in addition, there exists another mapping\n\nthat is also a homomorphism, which satisfies\n\n\nHence, one may form a category of dynamical systems and their homomorphisms.\n\nA point \"x\" ∈ \"X\" is called a generic point if the orbit of the point is distributed uniformly according to the measure.\n\nConsider a dynamical system formula_28, and let \"Q\" = {\"Q\", ..., \"Q\"} be a partition of \"X\" into \"k\" measurable pair-wise disjoint pieces. Given a point \"x\" ∈ \"X\", clearly \"x\" belongs to only one of the \"Q\". Similarly, the iterated point \"Tx\" can belong to only one of the parts as well. The symbolic name of \"x\", with regards to the partition \"Q\", is the sequence of integers {\"a\"} such that\n\nThe set of symbolic names with respect to a partition is called the symbolic dynamics of the dynamical system. A partition \"Q\" is called a generator or generating partition if μ-almost every point \"x\" has a unique symbolic name.\n\nGiven a partition Q = {\"Q\", ..., \"Q\"} and a dynamical system formula_28, we define \"T\"-pullback of \"Q\" as\n\nFurther, given two partitions \"Q\" = {\"Q\", ..., \"Q\"} and \"R\" = {\"R\", ..., \"R\"}, we define their refinement as\n\nWith these two constructs we may define \"refinement of an iterated pullback\"\n\nwhich plays crucial role in the construction of the measure-theoretic entropy of a dynamical system.\n\nThe entropy of a partition \"Q\" is defined as\n\nThe measure-theoretic entropy of a dynamical system formula_28 with respect to a partition \"Q\" = {\"Q\", ..., \"Q\"} is then defined as\n\nFinally, the Kolmogorov–Sinai or metric or measure-theoretic entropy of a dynamical system formula_37 is defined as\n\nwhere the supremum is taken over all finite measurable partitions. A theorem of Yakov Sinai in 1959 shows that the supremum is actually obtained on partitions that are generators. Thus, for example, the entropy of the Bernoulli process is log 2, since almost every real number has a unique binary expansion. That is, one may partition the unit interval into the intervals <nowiki>[</nowiki>0, 1/2<nowiki>)</nowiki> and [1/2, 1]. Every real number \"x\" is either less than 1/2 or not; and likewise so is the fractional part of 2\"x\".\n\nIf the space \"X\" is compact and endowed with a topology, or is a metric space, then the topological entropy may also be defined.\n\n\n\n"}
{"id": "27666612", "url": "https://en.wikipedia.org/wiki?curid=27666612", "title": "Meertens number", "text": "Meertens number\n\nIn mathematical logic, a Meertens number is an integer that is its own Gödel number.\n\nThe Gödel encoding of a decimal number with \"n\" digits is the product of the first \"n\" primes raised to the values of their corresponding digits in the sequence. \n\nThe only Meertens number that has been found is 81312000 = 235711131719. \n\nIt was \"given\" to Lambert Meertens by Richard S. Bird as a present during the celebration of his 25 years at the CWI, Amsterdam. \n\nIf other Meertens numbers exist, they must be greater than 1 × 10.\n"}
{"id": "17319790", "url": "https://en.wikipedia.org/wiki?curid=17319790", "title": "Modular neural network", "text": "Modular neural network\n\nA modular neural network is an artificial neural network characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform. The intermediary takes the outputs of each module and processes them to produce the output of the network as a whole. The intermediary only accepts the modules' outputs—it does not respond to, nor otherwise signal, the modules. As well, the modules do not interact with each other.\n\nAs artificial neural network research progresses, it is appropriate that artificial neural networks continue to draw on their biological inspiration and emulate the segmentation and modularization found in the brain. The brain, for example, divides the complex task of visual perception into many subtasks. Within a part of the brain, called the thalamus, lies the lateral geniculate nucleus (LGN), which is divided into layers that separately process color and contrast: both major components of vision. After the LGN processes each component in parallel, it passes the result to another region to compile the results.\n\nSome tasks that the brain handles, like vision, employ a hierarchy of sub-networks. However, it is not clear whether some intermediary ties these separate processes together. Rather, as the tasks grow more abstract, the modules communicate with each other, unlike the modular neural network model.\n\nUnlike a single large network that can be assigned to arbitrary tasks, each module in a modular network must be assigned a specific task and connected to other modules in specific ways by a designer. In the vision example, the brain evolved (rather than learned) to create the LGN. In some cases, the designer may choose to follow biological models. In other cases, other models may be superior. The quality of the result will be a function of the quality of the design.\n\nModular neural networks reduce a single large, unwieldy neural network to smaller, potentially more manageable components. Some tasks are intractably large for a single neural network. The benefits of modular neural networks include:\n\nThe possible neuron (node) connections increase exponentially as nodes are added to a network. Computation time depends on the number of nodes and their connections, any increase has drastic consequences for processing time. Assigning specific subtasks to individual modules reduce the number of necessary connections.\n\nA large neural network attempting to model multiple parameters can suffer from interference as new data can alter existing connections or just serve to confuse. Each module can be trained independently and more precisely master its simpler task. This means the training algorithm and the training data can be implemented more quickly.\n\nRegardless of whether a large neural network is biological or artificial, it remains largely susceptible to interference at and failure in any one of its nodes. By compartmentalizing subtasks, failure and interference are much more readily diagnosed and their effects on other sub-networks are eliminated as each one is independent of the other.\n\n"}
{"id": "33788375", "url": "https://en.wikipedia.org/wiki?curid=33788375", "title": "Mutual knowledge (logic)", "text": "Mutual knowledge (logic)\n\nMutual knowledge is a fundamental concept about information in game theory, (epistemic) logic, and epistemology. An event is mutual knowledge if all agents know that the event occurred. However, mutual knowledge by itself implies nothing about what agents know about other agents' knowledge: i.e. it is possible that an event is mutual knowledge but that each agent is unaware that the other agents know it has occurred. Common knowledge is a related but stronger notion; any event that is common knowledge is also mutual knowledge.\n\nThe philosopher Stephen Schiffer, in his book \"Meaning\", developed a notion he called \"mutual knowledge\" which functions quite similarly to David K. Lewis's \"common knowledge\".\n\n"}
{"id": "3758667", "url": "https://en.wikipedia.org/wiki?curid=3758667", "title": "Narcissistic number", "text": "Narcissistic number\n\nIn recreational number theory, a narcissistic number (also known as a pluperfect digital invariant (PPDI), an Armstrong number (after Michael F. Armstrong) or a plus perfect number) is a number that is the sum of its own digits each raised to the power of the number of digits. This definition depends on the base \"b\" of the number system used, e.g., \"b\" = 10 for the decimal system or \"b\" = 2 for the binary system.\n\nThe definition of a narcissistic number relies on the decimal representation \"n\" = \"d\"\"d\"...\"d\" of a natural number \"n\", i.e.,\nwith \"k\" digits \"d\" satisfying 0 ≤ \"d\" ≤ 9. Such a number \"n\" is called narcissistic if it satisfies the condition\nFor example, the 3-digit decimal number 153 is a narcissistic number because 153 = 1 + 5 + 3.\n\nNarcissistic numbers can also be defined with respect to numeral systems with a base \"b\" other than \"b\" = 10. The base-\"b\" representation of a natural number \"n\" is defined by\nwhere the base-\"b\" digits \"d\" satisfy the condition 0 ≤ \"d\" ≤ \"b\"-1.\nFor example, the (decimal) number 17 is a narcissistic number with respect to the numeral system with base \"b\" = 3. Its three base-3 digits are 122, because 17 = 1·3 + 2·3 + 2 , and it satisfies the equation 17 = 1 + 2 + 2.\n\nIf the constraint that the power must equal the number of digits is dropped, so that for some \"m\" possibly different from \"k\" it happens that\nthen \"n\" is called a perfect digital invariant or PDI. For example, the decimal number 4150 has four decimal digits and is the sum of the \"fifth\" powers of its decimal digits\nso it is a perfect digital invariant but \"not\" a narcissistic number.\n\nIn \"A Mathematician's Apology\", G. H. Hardy wrote:\n\nThe sequence of base 10 narcissistic numbers starts:\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 153, 370, 371, 407, 1634, 8208, 9474, ... \n\nThe sequence of base 8 narcissistic numbers starts:\n0, 1, 2, 3, 4, 5, 6, 7, 24, 64, 134, 205, 463, 660, 661, ... (sequence and in OEIS)\n\nThe sequence of base 12 narcissistic numbers starts:\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ᘔ, Ɛ, 25, ᘔ5, 577, 668, ᘔ83, ... \n\nThe sequence of base 16 narcissistic numbers starts:\n0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F, 156, 173, 208, 248, 285, 4A5, 5B0, 5B1, 60B, 64B, ... \n\nThe sequence of base 3 narcissistic numbers starts:\n0, 1, 2, 12, 22, 122\n\nThe sequence of base 4 narcissistic numbers starts:\n0, 1, 2, 3, 130, 131, 203, 223, 313, 332, 1103, 3303 (sequence and in OEIS)\n\nIn base 2, the only narcissistic numbers are 0 and 1.\n\nThe number of narcissistic numbers in a given base is finite, since the maximum possible sum of the \"k\"th powers of a \"k\" digit number in base \"b\" is\n\nand if \"k\" is large enough then\n\nEvery base \"b\" ≥ 3 that is not a multiple of nine has at least one three-digit narcissistic number. The bases that do not are\n\nUnlike narcissistic numbers, no upper bound can be determined for the size of PDIs in a given base, and it is not currently known whether or not the number of PDIs for an arbitrary base is finite or infinite.\n\nThe term \"narcissistic number\" is sometimes used in a wider sense to mean a number that is equal to any mathematical manipulation of its own digits. With this wider definition narcisstic numbers include:\n\nwhere \"d\" are the digits of \"n\" in some base.\n\n\n"}
{"id": "164501", "url": "https://en.wikipedia.org/wiki?curid=164501", "title": "Nimber", "text": "Nimber\n\nIn mathematics, the nimbers, also called Grundy numbers, are introduced in combinatorial game theory, where they are defined as the values of heaps in the game Nim. The nimbers are the ordinal numbers endowed with nimber addition and nimber multiplication, which are distinct from ordinal addition and ordinal multiplication.\n\nBecause of the Sprague–Grundy theorem which states that every impartial game is equivalent to a Nim heap of a certain size, nimbers arise in a much larger class of impartial games. They may also occur in partizan games like Domineering.\n\nNimbers have the characteristic that their Left and Right options are identical, following a certain schema, and that they are their own negatives, such that a positive ordinal may be added to another positive ordinal using nimber addition to find an ordinal of a lower value. The minimum excludant operation is applied to sets of nimbers.\n\nNim is a game in which two players take turns removing objects from distinct heaps. As moves depend only on the position and not on which of the two players is currently moving, and where the payoffs are symmetric, Nim is an impartial game. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap. The goal of the game is to avoid being the player who must remove the last object. Using nimber addition, each heap can be summed together to give a nim value for the heap. Furthermore, as all the heaps together can be summed using nim addition, one can calculate the nimber of the game as a whole. The winning strategy of this game is to force the cumulative nimber of the game to 0 for the opponents turn.\n\nCram is a game often played on a rectangular board in which players take turns placing dominoes either horizontally or vertically until no more dominoes can be placed. The first player that cannot make a move loses. As the possible moves for both players are the same, it is an impartial game and can have a nimber value. If each row and column is considered a heap, then the value of the game is the sum of all rows and columns using nimber addition. For example, any 2xn board will have a nimber of 0 for all even n and a nimber of 1 for all odd n.\n\nA game where pegs for each player are placed along a column with a finite number of spaces. Each turn each player must move the piece up or down the column, but may not move past the other player's piece. Several columns are stacked together to add complexity. The player that can no longer make any moves loses. Unlike many other nimber related games, the number of spaces between the two tokens on each row are the sizes of the Nim heaps. If your opponent increases the number of spaces between two tokens, just decrease it on your next move. Else, play the game of Nim and make the Nim-sum of the number of spaces between the tokens on each row be 0.\n\nHackenbush is a game invented by mathematician John Horton Conway. It may be played on any configuration of colored line segments connected to one another by their endpoints and to a \"ground\" line. players take turns removing line segments. An impartial game version, thereby a game able to be analyzed using nimbers can be found by removing distinction from the lines, allowing either player to cut any branch. Any segments reliant on the newly removed segment in order to connect to the ground line are removed as well. In this way, each connection to the ground can be considered a nim heap with a nimber value. Additionally, all the separate connections to the ground line can also be summed for a nimber of the game state.\n\nNimber addition (also known as nim-addition) can be used to calculate the size of a single nim heap equivalent to a collection of nim heaps. It is defined recursively by\nwhere the minimum excludant of a set of ordinals is defined to be the smallest ordinal that is \"not\" an element of .\n\nFor finite ordinals, the nim-sum is easily evaluated on a computer by taking the bitwise exclusive or (XOR, denoted by ) of the corresponding numbers. For example, the nim-sum of 7 and 14 can be found by writing 7 as 111 and 14 as 1110; the ones place adds to 1; the twos place adds to 2, which we replace with 0; the fours place adds to 2, which we replace with 0; the eights place adds to 1. So the nim-sum is written in binary as 1001, or in decimal as 9.\n\nThis property of addition follows from the fact that both mex and XOR yield a winning strategy for Nim and there can be only one such strategy; or it can be shown directly by induction: Let and be two finite ordinals, and assume that the nim-sum of all pairs with one of them reduced is already defined. The only number whose XOR with is is , and vice versa; thus is excluded. On the other hand, for any ordinal , XORing with all of , and must lead to a reduction for one of them (since the leading 1 in must be present in at least one of the three); since , we must have or ; thus is included as or as , and hence is the minimum excluded ordinal.\n\nNimber multiplication (nim-multiplication) is defined recursively by\n\nExcept for the fact that nimbers form a proper class and not a set, the class of nimbers determines an algebraically closed field of characteristic 2. The nimber additive identity is the ordinal 0, and the nimber multiplicative identity is the ordinal 1. In keeping with the characteristic being 2, the nimber additive inverse of the ordinal is itself. The nimber multiplicative inverse of the nonzero ordinal is given by , where is the smallest set of ordinals (nimbers) such that\n\n\nFor all natural numbers , the set of nimbers less than form the Galois field of order .\n\nIn particular, this implies that the set of finite nimbers is isomorphic to the direct limit as of the fields . This subfield is not algebraically closed, since no other field (so with not a power of 2) is contained in any of those fields, and therefore not in their direct limit; for instance the polynomial , which has a root in , does not have a root in the set of finite nimbers.\n\nJust as in the case of nimber addition, there is a means of computing the nimber product of finite ordinals. This is determined by the rules that\n\n\nThe smallest algebraically closed field of nimbers is the set of nimbers less than the ordinal , where is the smallest infinite ordinal. It follows that as a nimber, is transcendental over the field.\n\nThe following tables exhibit addition and multiplication among the first 16 nimbers. <br>\nThis subset is closed under both operations, since 16 is of the form .\n\n"}
{"id": "7791994", "url": "https://en.wikipedia.org/wiki?curid=7791994", "title": "Non-logical symbol", "text": "Non-logical symbol\n\nIn logic, the formal languages used to create expressions consist of symbols, which can be broadly divided into constants and variables. The constants of a language can further be divided into logical symbols and non-logical symbols (sometimes also called logical and non-logical constants).\n\nThe non-logical symbols of a language of first-order logic consist of predicates and individual constants. These include symbols that, in an interpretation, may stand for individual constants, variables, functions, or predicates. A language of first-order logic is a formal language over the alphabet consisting of its non-logical symbols and its logical symbols. The latter include logical connectives, quantifiers, and variables that stand for statements.\n\nA non-logical symbol only has meaning or semantic content when one is assigned to it by means of an interpretation. Consequently, a sentence containing a non-logical symbol lacks meaning except under an interpretation, so a sentence is said to be \"true or false under an interpretation\". Main article: first order logic; especially, \"Syntax of first-order logic\".\n\nThe logical constants, by contrast, have the same meaning in all interpretations. They include the symbols for truth-functional connectives (such as \"and\", \"or\", \"not\", \"implies\", and logical equivalence) and the symbols for the quantifiers \"for all\" and \"there exists\".\n\nThe equality symbol is sometimes treated as a non-logical symbol and sometimes treated as a symbol of logic. If it is treated as a logical symbol, then any interpretation will be required to interpret the equality sign using true equality; if interpreted as a non-logical symbol, it may be interpreted by an arbitrary equivalence relation.\n\nA \"signature\" is a set of non-logical constants together with additional information identifying each symbol as either a constant symbol, or a function symbol of a specific arity \"n\" (a natural number), or a relation symbol of a specific arity. The additional information controls how the non-logical symbols can be used to form terms and formulas. For instance if \"f\" is a binary function symbol and \"c\" is a constant symbol, then \"f\"(\"x\", \"c\") is a term, but \"c\"(\"x\", \"f\") is not a term. Relation symbols cannot be used in terms, but they can be used to combine one or more (depending on the arity) terms into an atomic formula.\n\nFor example a signature could consist of a binary function symbol +, a constant symbol 0, and a binary relation symbol <.\n\n\"Structures\" over a signature, also known as \"models\", provide formal semantics to a signature and the first-order language over it.\n\nA structure over a signature consists of a set \"D\", known as the domain of discourse, together with interpretations of the non-logical symbols: Every constant symbol is interpreted by an element of \"D\", and the interpretation of an \"n\"-ary function symbol is an \"n\"-ary function on \"D\", i.e. a function \"D\" → \"D\" from the \"n\"-fold cartesian product of the domain to the domain itself. Every \"n\"-ary relation symbol is interpreted by an \"n\"-ary relation on the domain, i.e. by a subset of \"D\".\n\nAn example of a structure over the signature mentioned above is the ordered group of integers. Its domain is the set formula_1 = {…, –2, –1, 0, 1, 2, …} of integers. The binary function symbol + is interpreted by addition, the constant symbol 0 by the additive identity, and the binary relation symbol < by the relation less than.\n\nOutside a mathematical context, it is often more appropriate to work with more informal interpretations.\n\nRudolf Carnap introduced a terminology distinguishing between logical and non-logical symbols (which he called \"descriptive signs\") of a formal system under a certain type of interpretation, defined by what they describe in the world.\n\nA descriptive sign is defined as any symbol of a formal language which designates things or processes in the world, or properties or relations of things. This is in contrast to \"logical signs\" which do not designate any thing in the world of objects. The use of logical signs is determined by the logical rules of the language, whereas meaning is arbitrarily attached to descriptive signs when they are applied to a given domain of individuals.\n\n\n\n"}
{"id": "22774540", "url": "https://en.wikipedia.org/wiki?curid=22774540", "title": "Option type", "text": "Option type\n\nIn programming languages (more so functional programming languages) and type theory, an option type or maybe type is a polymorphic type that represents encapsulation of an optional value; e.g., it is used as the return type of functions which may or may not return a meaningful value when they are applied. It consists of a constructor which either is empty (named \"None\" or \"Nothing\"), or which encapsulates the original data type A (written \"Just\" A or \"Some\" A). Outside of functional programming, these are termed nullable types.\n\nIn different programming languages, the option type has various names and definitions.\n\n\nIn type theory, it may be written as: formula_1.\n\nIn languages having tagged unions, as in most functional programming languages, option types can be expressed as the tagged union of a unit type plus the encapsulated type.\n\nIn the Curry-Howard correspondence, option types are related to the annihilation law for ∨: x∨1=1.\n\nAn option type can also be seen as a collection containing either one or zero elements.\n\nThe option type is a monad under these functions:\nWe may also describe the option monad in terms of functions \"return\", \"fmap\" and \"join\", where the latter two are given by:\n\nThe option monad is an additive monad: it has \"Nothing\" as a zero constructor and the following function as a monadic sum:\n\nThe resulting structure is an idempotent monoid.\n\nScala implements Option as a parameterized type, so a variable can be an Option, accessed as follows:\nTwo main ways to use an Option value exist. The first, not the best, is the pattern matching, as in the first example. The second, the best practice, is the monad method, as in the second example. In this way, a program is safe, as it can generate no exception or error (e.g., by trying to obtain the value of an codice_1 variable that is equal to codice_2). Thus, it essentially works as a type-safe alternative to the null value.\n\nOCaml implements Option as a parameterized variant type. Options are constructed and deconstructed as follows:\n\nRust allows using either pattern matching or optional binding to deconstruct the Option type:\n\nJulia requires explicit deconstruction to access a nullable value:\n\nThere are as many null values as there are types, that is because every type is its own null.\nSo all types are also their own option type.\n\nBasically when use type in a declaration, it can be a value of that type or a null of that type.\n\nTo designate that it must have a defined value assigned to it (not be in the null state), use the \"smiley\" designation.\n\nIt is also possible to designate that it must always be a null using the \"smiley\".\n\n<hr>\n\n\n\n\n\nType \"smileys\" are used more often for method and subroutine signatures than they are for variable declarations.\n\n\nThere are special variations of and called and that check for definedness rather than truthiness.\n\nThese set by default, unlike their boolean cousins.\n\nThere are also variations of , and that test for definedness.\n\n"}
{"id": "11735522", "url": "https://en.wikipedia.org/wiki?curid=11735522", "title": "Oracle Unified Method", "text": "Oracle Unified Method\n\nThe Oracle Unified Method (OUM), first released by Oracle Corporation in 2006, is a standards-based method with roots in the Unified Process (UP). OUM is business-process and use-case driven and includes support for the Unified Modeling Language (UML), though the use of UML is not required. OUM combines these standards with aspects of Oracle's legacy methods and Oracle implementation best-practices.\n\nOUM is applicable to any size or type of information technology project. While OUM is a plan-based method – that includes extensive overview material, task and artifact descriptions, and associated templates – the method is intended to be tailored to support the appropriate level of ceremony required for each project. Guidance is provided for identifying the minimum subset of tasks, tailoring the project approach, executing iterative and incremental project planning, and applying Agile techniques. Supplemental guidance provides specific support for Oracle products, tools, and technologies.\n\nOUM v6.4.0 provides support for:\n\nas well as the complete range of technology projects including:\n\nDetailed techniques and tool guidance are provided, including a supplemental guide related to Oracle Tutor and UPK.\n\nOUM is available for use by Oracle employees; for Oracle PartnerNetwork Diamond, Platinum, and Gold Partners; and for customers who participate in the OUM Customer Program.\n\nLegacy method retirement dates:\n\n"}
{"id": "15046429", "url": "https://en.wikipedia.org/wiki?curid=15046429", "title": "Philip Rabinowitz (mathematician)", "text": "Philip Rabinowitz (mathematician)\n\nPhilip Rabinowitz (August 14, 1926 – July 21, 2006) was an American and Israeli applied mathematician. He was best known for his work in numerical analysis, including his books \"A First Course in Numerical Analysis\" with Anthony Ralston and \"Methods of Numerical Integration\" with Philip J. Davis. He was the author of numerous articles on numerical computation.\n\nHe earned his Ph.D. in 1951 under Walter Gottschalk at the University of Pennsylvania. He worked for the American National Bureau of Standards and taught at the Weizmann Institute of Science in Israel.\n\n"}
{"id": "10113122", "url": "https://en.wikipedia.org/wiki?curid=10113122", "title": "Quantum radar", "text": "Quantum radar\n\nQuantum radar is an emerging remote-sensing technology based on input quantum correlations (in particular, quantum entanglement) and output quantum detections. If it is successfully developed, it will allow the radar system to pick out its own signal even when swamped by background noise. This allows it to detect stealth aircraft, filter out deliberate jamming attempts, and operate in areas of high background noise, e.g., due to ground clutter. The first feasible design of a quantum radar was proposed in 2015 by an international team and is based on the protocol of Gaussian quantum illumination.\n\nThe basic concept is to create a stream of entangled visible-frequency photons and split it in half. One half, the \"signal beam\", goes through a conversion to microwave frequencies in a way that preserves the original quantum state. The microwave signal is then sent and received as in a normal radar system. When the reflected signal is received it is converted back into photons and compared with the other half of the original entangled beam, the \"idler beam\".\n\nAlthough most of the original entanglement will be lost due to quantum decoherence as the microwaves travel to the target objects and back, enough quantum correlations will still remain between the reflected-signal and the idler beams. Using a suitable quantum detection scheme, the system can pick out just those photons that were originally sent by the radar, completely filtering out any other sources. If the system can be made to work in the field, it represents an enormous advance in detection capability.\n\nOne way to defeat conventional radar systems is to broadcast signals on the same frequencies used by the radar, making it impossible for the receiver to distinguish between their own broadcasts and the spoofing signal (or \"jamming\"). However, such systems cannot know, even in theory, what the original quantum state of the radar's internal signal was. Lacking such information, their broadcasts will not match the original signal and will be filtered out in the correlator. Environmental sources, like ground clutter and aurora, will similarly be filtered out.\n\nThere is considerable discussion of the use of quantum radar as an anti-stealth technology. Stealth aircraft are designed to reflect signals away from the radar, typically by using rounded surfaces and avoiding anything that might form a partial corner reflector. This so reduces the amount of signal returned to the radar's receiver that the target is (ideally) lost in the thermal background noise. Although stealth technologies will still be just as effective at reflecting the original signal away from the receiver of a quantum radar, it is the system's ability to separate out the remaining tiny signal, even when swamped by other sources, that allows it to pick out the return even from highly stealthy designs.\n\nThe most convincing model was proposed by an international team of researchers. The team designed a model of quantum radar for remote sensing of a low-reflectivity target that is embedded within a bright microwave background, with detection performance well beyond the capability of a classical microwave radar. By using a suitable wavelength \"electro-optomechanical converter\", this scheme generates excellent quantum entanglement between a microwave signal beam, sent to probe the target region, and an optical idler beam, retained for detection. The microwave return collected from the target region is subsequently converted into an optical beam and then measured jointly with the idler beam. Such a technique extends the powerful protocol of quantum illumination to its more natural spectral domain, namely microwave wavelengths. A prototype quantum radar could be realized with current technology, and is suited to applications from standoff sensing of stealth objects to environmental scanning of electrical circuits. Thanks to its quantum-enhanced sensitivity, this device could lead to low-flux non-invasive techniques for protein spectroscopy and biomedical imaging. Alternative methods were also considered by defense contractor Lockheed Martin whose aim was to create a radar system providing a better resolution and higher detail than classical radar could provide. According to Chinese state media, the first quantum radar was developed and tested by China in real-world environment in August 2016.. More recently, the generation of large numbers of entangled photons for radar detection has been studied by the University of Waterloo.\n"}
{"id": "1505379", "url": "https://en.wikipedia.org/wiki?curid=1505379", "title": "Robert Ammann", "text": "Robert Ammann\n\nRobert Ammann (October 1, 1946 – May, 1994) was an amateur mathematician who made several significant and groundbreaking contributions to the theory of quasicrystals and aperiodic tilings.\nAmmann attended Brandeis University, but generally did not go to classes, and left after three years. He worked as a programmer for Honeywell. After ten years, his position was eliminated as part of a routine cutback, and Ammann ended up working as a mail sorter for a post office.\n\nIn 1975, Ammann read an announcement by Martin Gardner of new work by Roger Penrose. Penrose had discovered two simple sets of aperiodic tiles, each consisting of just two quadrilaterals. Since Penrose was taking out a patent, he wasn't ready to publish them, and Gardner's description was rather vague. Ammann wrote a letter to Gardner, describing his own work, which duplicated one of Penrose's sets, plus a foursome of \"golden rhombohedra\" that formed aperiodic tilings in space.\n\nMore letters followed, and Ammann became a correspondent with many of the professional researchers. He discovered several new aperiodic tilings, each among the simplest known examples of aperiodic sets of tiles. He also showed how to generate tilings using lines in the plane as guides for lines marked on the tiles, now called \"Ammann bars\".\n\nThe discovery of quasicrystals in 1982 changed the status of aperiodic tilings and Ammann's work from mere recreational mathematics to respectable academic research.\n\nAfter more than ten years of coaxing, he agreed to meet various professionals in person, and eventually even went to two conferences and delivered a lecture at each. Afterwards, Ammann dropped out of sight, and died of a heart attack a few years later. News of his death did not reach the research community for a few more years.\n\nFive sets of tiles discovered by Ammann were described in \"Tilings and Patterns\" and later, in collaboration with the authors of the book, he published a paper proving the aperiodicity for four of them. Ammann's discoveries came to notice only after Penrose had published his own discovery and gained priority. In 1981 de Bruijn exposed the cut and project method and in 1984 came the sensational news about Shechtman quasicrystals which promoted the Penrose tiling to fame. But in 1982 Beenker published a similar mathematical explanation for the octagonal case which became known as the Ammann–Beenker tiling. In 1987 Wang, Chen and Kuo announced the discovery of a quasicrystal with octagonal symmetry. The decagonal covering of the Penrose tiling was proposed in 1996 and two years later F. Gahler proposed an octagonal variant for the Ammann–Beenker tiling Ammann's name became that of the perennial second. It is acknowledged however that Robert Ammann first proposed the construction of rhombic prisms which is the three-dimensional model of Shechtman's quasicrystals.\n\n"}
{"id": "1443423", "url": "https://en.wikipedia.org/wiki?curid=1443423", "title": "Scott Draves", "text": "Scott Draves\n\nScott Draves is the inventor of Fractal Flames and the leader of the distributed computing project Electric Sheep. He also invented patch-based texture synthesis and published the first implementation of this class of algorithms. He is also a video artist and accomplished VJ.\n\nIn summer 2010, Draves' work was exhibited at Google's New York City office, including his video piece \"Generation 243\" which was generated by the collaborative influences of 350,000 people and computers worldwide. Stephen Hawking's 2010 book The Grand Design used an image generated by Draves' \"flame\" algorithm on its cover. Known as \"Spot,\" Draves currently resides in New York City.\n\nIn July 2012 Draves won the ZKM App Art Award Special Prize for Cloud Art for the mobile Android version of Electric Sheep.\n\nDraves earned a Bachelor's in mathematics at Brown University, where he was a student of Andy van Dam before continuing on to earn a PhD in computer science at Carnegie Mellon University. At CMU he studied under Andy Witkin, Dana Scott, and Peter Lee.\n"}
{"id": "583637", "url": "https://en.wikipedia.org/wiki?curid=583637", "title": "Simplicial approximation theorem", "text": "Simplicial approximation theorem\n\nIn mathematics, the simplicial approximation theorem is a foundational result for algebraic topology, guaranteeing that continuous mappings can be (by a slight deformation) approximated by ones that are piecewise of the simplest kind. It applies to mappings between spaces that are built up from simplices—that is, finite simplicial complexes. The general continuous mapping between such spaces can be represented approximately by the type of mapping that is (\"affine\"-) linear on each simplex into another simplex, at the cost (i) of sufficient barycentric subdivision of the simplices of the domain, and (ii) replacement of the actual mapping by a homotopic one.\n\nThis theorem was first proved by L.E.J. Brouwer, by use of the Lebesgue covering theorem (a result based on compactness). It served to put the homology theory of the time—the first decade of the twentieth century—on a rigorous basis, since it showed that the topological effect (on homology groups) of continuous mappings could in a given case be expressed in a finitary way. This must be seen against the background of a realisation at the time that continuity was in general compatible with the pathological, in some other areas. This initiated, one could say, the era of combinatorial topology.\n\nThere is a further simplicial approximation theorem for homotopies, stating that a homotopy between continuous mappings can likewise be approximated by a combinatorial version.\n\nLet formula_1 and formula_2 be two simplicial complexes. A simplicial mapping formula_3 is called a simplicial approximation of a continuous function formula_4 if for every point formula_5, formula_6 belongs to the minimal closed simplex of formula_2 containing the point formula_8. If formula_9 is a simplicial approximation to a continuous map formula_10, then the geometric realization of formula_9, formula_12 is necessarily homotopic to formula_10.\n\nThe simplicial approximation theorem states that given any continuous map formula_4 there exists a natural number formula_15 such that for all formula_16 there exists a simplicial approximation formula_17 to formula_10 (where formula_19 denotes the barycentric subdivision of formula_1, and formula_21 denotes the result of applying barycentric subdivision formula_22 times.)\n"}
{"id": "37986691", "url": "https://en.wikipedia.org/wiki?curid=37986691", "title": "Spitzer's formula", "text": "Spitzer's formula\n\nIn probability theory, Spitzer's formula or Spitzer's identity gives the joint distribution of partial sums and maximal partial sums of a collection of random variables. The result was first published by Frank Spitzer in 1956. The formula is regarded as \"a stepping stone in the theory of sums of independent random variables\".\n\nLet \"X\", \"X\", ... be independent and identically distributed random variables and define the partial sums \"S\" = \"X\" + \"X\" + ... + \"X\". Define \"R\" = max(0,\"S\",\"S\"...,\"S\"). Then\n\nwhere\n\nand \"S\" denotes (|\"S\"| ± \"S\")/2.\n\nTwo proofs are known, due to Spitzer and Wendel.\n"}
{"id": "24226726", "url": "https://en.wikipedia.org/wiki?curid=24226726", "title": "Starlight Information Visualization System", "text": "Starlight Information Visualization System\n\nStarlight is a software product originally developed at Pacific Northwest National Laboratory and now by Future Point Systems. It is an advanced visual analysis environment. In addition to using information visualization to show the importance of individual pieces of data by showing how they relate to one another, it also contains a small suite of tools useful for collaboration and data sharing, as well as data conversion, processing, augmentation and loading.\n\nThe software, originally developed for the intelligence community, allows users to load data from XML files, databases, RSS feeds, web services, HTML files, Microsoft Word, PowerPoint, Excel, CSV, Adobe PDF, TXT files, etc. and analyze it with a variety of visualizations and tools. The system integrates structured, unstructured, geospatial, and multimedia data, offering comparisons of information at multiple levels of abstraction, simultaneously and in near real-time. In addition Starlight allows users to build their own named entity-extractors using a combination of algorithms, targeted normalization lists and regular expressions in the Starlight Data Engineer (SDE).\n\nAs an example, Starlight might be used to look for correlations in a database containing records about chemical spills. An analyst could begin by grouping records according to the cause of the spill to reveal general trends. Sorting the data a second time, they could apply different colors based on related details such as the company responsible, age of equipment or geographic location. Maps and photographs could be integrated into the display, making it even easier to recognize connections among multiple variables.\n\nStarlight has been deployed to both the Iraq and Afghanistan wars and used on a number of large-scale projects.\n\nPNNL began developing Starlight in the mid-90's, with funding from the Land Information Warfare Agency, a part of the Army Intelligence and Security Command and continued developed at the laboratory with funding from the NSA and the CIA. Starlight integrates visual representations of reports, radio transcripts, radar signals, maps and other information. The software system was recently honored with an R&D 100 Award for technical innovation.\n\nIn 2006 Future Point Systems, a Silicon Valley startup, acquired rights to jointly develop and distribute the Starlight product in cooperation with the Pacific Northwest National Laboratory.\n\nThe software is now also used outside of the military/intelligence communities in a number of commercial environments.\n\n\n"}
{"id": "58673886", "url": "https://en.wikipedia.org/wiki?curid=58673886", "title": "Steve Butler (mathematician)", "text": "Steve Butler (mathematician)\n\nSteven Kay Butler (born May 16, 1977) is an American mathematician specializing in graph theory and combinatorics. He is an associate professor of mathematics and Barbara J. Janson Professor in Mathematics at Iowa State University.\n\nButler earned his master's degree at Brigham Young University in 2003. His master's thesis was titled \"Bounding the Number of Graphs Containing Very Long Induced Paths\". He completed a doctorate at the University of California, San Diego in 2008, authoring the dissertation \"Eigenvalues and Structures of Graphs\", advised by Fan Chung. Upon completing his postdoctoral studies at the University of California, Los Angeles, Butler joined the Iowa State University faculty in 2011, and was named the Barbara J. Janson Professor in Mathematics in 2017. Butler's Erdős number is one. In 2015, Butler published the paper \"Egyptian Fractions With Each Denominator Having Three Distinct Prime Divisors\" with coauthors Paul Erdős and Ronald Graham.\n"}
{"id": "23696", "url": "https://en.wikipedia.org/wiki?curid=23696", "title": "Timeline of programming languages", "text": "Timeline of programming languages\n\nThis is a record of historically important programming languages, by decade.\n\n\n"}
{"id": "3350111", "url": "https://en.wikipedia.org/wiki?curid=3350111", "title": "Wiener–Khinchin theorem", "text": "Wiener–Khinchin theorem\n\nIn applied mathematics, the Wiener–Khinchin theorem, also known as the Wiener–Khintchine theorem and sometimes as the Wiener–Khinchin–Einstein theorem or the Khinchin–Kolmogorov theorem, states that the autocorrelation function of a wide-sense-stationary random process has a spectral decomposition given by the power spectrum of that process.\n\nNorbert Wiener proved this theorem for the case of a deterministic function in 1930; Aleksandr Khinchin later formulated an analogous result for stationary stochastic processes and published that probabilistic analogue in 1934. Albert Einstein explained, without proofs, the idea in a brief two-page memo in 1914.\n\nFor continuous time, the Wiener–Khinchin theorem says that if formula_1 is a wide-sense stationary process such that its autocorrelation function (sometimes called autocovariance) defined in terms of statistical expected value,\nformula_2 (the asterisk denotes complex conjugate, and of course it can be omitted if the random process is real-valued), exists and is finite at every lag formula_3, then there exists a monotone function formula_4 in the frequency domain formula_5 such that\n\nwhere the integral is a Riemann–Stieltjes integral. This is a kind of spectral decomposition of the auto-correlation function. \"F\" is called the power spectral distribution function and is a statistical distribution function. It is sometimes called the integrated spectrum.\n\nNote that the Fourier transform of formula_7 does not exist in general, because stationary random functions are not generally either square-integrable or absolutely integrable. Nor is formula_8 assumed to be absolutely integrable, so it need not have a Fourier transform either.\n\nBut if formula_4 is absolutely continuous, for example, if the process is purely indeterministic, then formula_10 is differentiable almost everywhere. In this case, one can define formula_11, the power spectral density of formula_7, by taking the averaged derivative of formula_10. Because the left and right derivatives of formula_10 exist everywhere, we can put formula_15 everywhere, (obtaining that \"F\" is the integral of its averaged derivative), and the theorem simplifies to\n\nIf now one assumes that \"r\" and \"S\" satisfy the necessary conditions for Fourier inversion to be valid, the Wiener–Khinchin theorem takes the simple form of saying that \"r\" and \"S\" are a Fourier-transform pair, and\n\nFor the discrete-time case, the power spectral density of the function with discrete values formula_18 is\n\nwhere\n\nis the discrete autocorrelation function of formula_18, provided this is absolutely integrable. Being a sampled and discrete-time sequence, the spectral density is periodic in the frequency domain. This is due to the problem of aliasing: the contribution of any frequency \nhigher than the Nyquist frequency seems to be equal to that of its alias between 0 and 1. For this reason, the domain of the function formula_22 is usually restricted to lie between 0 and 1.\n\nThe theorem is useful for analyzing linear time-invariant systems (LTI systems) when the inputs and outputs are not square-integrable, so their Fourier transforms do not exist. A corollary is that the Fourier transform of the autocorrelation function of the output of an LTI system is equal to the product of the Fourier transform of the autocorrelation function of the input of the system times the squared magnitude of the Fourier transform of the system impulse response.\nThis works even when the Fourier transforms of the input and output signals do not exist because these signals are not square-integrable, so the system inputs and outputs cannot be directly related by the Fourier transform of the impulse response.\n\nSince the Fourier transform of the autocorrelation function of a signal is the power spectrum of the signal, this corollary is equivalent to saying that the power spectrum of the output is equal to the power spectrum of the input times the energy transfer function.\n\nThis corollary is used in the parametric method for power spectrum estimation.\n\nIn many textbooks and in much of the technical literature it is tacitly assumed that Fourier inversion of the autocorrelation function and the power spectral density is valid, and the Wiener–Khinchin theorem is stated, very simply, as if it said that the Fourier transform of the autocorrelation function was equal to the power spectral density, ignoring all questions of convergence (Einstein is an example).\nBut the theorem (as stated here) was applied by Norbert Wiener and Aleksandr Khinchin to the sample functions (signals) of wide-sense-stationary random processes, signals whose Fourier transforms do not exist.\nThe whole point of Wiener's contribution was to make sense of the spectral decomposition of the autocorrelation function of a sample function of a wide-sense-stationary random process even when the integrals for the Fourier transform and Fourier inversion do not make sense.\n\nFurther complicating the issue is that the discrete Fourier transform always exists for digital, finite-length sequences, meaning that the theorem can be blindly applied to calculate auto-correlations of numerical sequences. As mentioned earlier, the relation of this discrete sampled data to a mathematical model is often misleading, and related errors can show up as a divergence when the sequence length is modified.\n\nSome authors refer to formula_23 as the autocovariance function. They then proceed to normalise it, by dividing by formula_24, to obtain what they refer to as the autocorrelation function.\n\n"}
{"id": "20134027", "url": "https://en.wikipedia.org/wiki?curid=20134027", "title": "William Shaw (mathematician)", "text": "William Shaw (mathematician)\n\nWilliam Shaw (born 14 May 1958) is a British mathematician, currently visiting professor of the mathematics and computation of risk at University College London. He is a consultant on financial derivatives, an author of a primary book on using Mathematica to model financial derivatives, co-Editor-in-Chief of the journal \"Applied Mathematical Finance\", and a member of the Mathematics and Computer Science Departments at University College London.\n\nWillian Shaw studied at King's College, Cambridge, where he studied mathematics; he was Wrangler and earned a B.A. in 1980. In 1981 he won the Mayhew Prize for his performance on the Cambridge Mathematical Tripos. In 1984 he received a Ph.D. in mathematical physics from Wolfson College, Oxford. From 1984 to 1987 he was a research fellow at Cambridge and C.L.E. Moore Instructor at the Massachusetts Institute of Technology. From 1991 to 2002 he was a lecturer in mathematics at Balliol College, Oxford, and in 2002 he moved to St Catherine's College, Oxford, where he was University lecturer in financial mathematics. In 2006 he moved to a Professorship at King's College London and in 2011 to UCL.\n\n\n"}
{"id": "471387", "url": "https://en.wikipedia.org/wiki?curid=471387", "title": "Éléments de géométrie algébrique", "text": "Éléments de géométrie algébrique\n\nThe Éléments de géométrie algébrique (\"Elements of Algebraic Geometry\") by Alexander Grothendieck (assisted by Jean Dieudonné), or EGA for short, is a rigorous treatise, in French, on algebraic geometry that was published (in eight parts or fascicles) from 1960 through 1967 by the Institut des Hautes Études Scientifiques. In it, Grothendieck established systematic foundations of algebraic geometry, building upon the concept of schemes, which he defined. The work is now considered the foundation stone and basic reference of modern algebraic geometry.\n\nInitially thirteen chapters were planned, but only the first four (making a total of approximately 1500 pages) were published. Much of the material which would have been found in the following chapters can be found, in a less polished form, in the Séminaire de géométrie algébrique (known as SGA). Indeed, as explained by Grothendieck in the preface of the published version of SGA, by 1970 it had become clear that incorporating all of the planned material in EGA would require significant changes in the earlier chapters already published, and that therefore the prospects of completing EGA in the near term were limited. An obvious example is provided by derived categories, which became an indispensable tool in the later SGA volumes, was not yet used in EGA III as the theory was not yet developed at the time. Considerable effort was therefore spent to bring the published SGA volumes to a high degree of completeness and rigour. Before work on the treatise was abandoned, there were plans in 1966-67 to expand the group of authors to include Grothendieck's students Pierre Deligne and Michel Raynaud, as evidenced by published correspondence between Grothendieck and David Mumford. Grothendieck's letter of 4 November 1966 to Mumford also indicates that the second-edition revised structure was in place by that time, with Chapter VIII already intended to cover the Picard scheme. In that letter he estimated that at the pace of writing up to that point, the following four chapters (V to VIII) would have taken eight years to complete, indicating an intended length comparable to the first four chapters, which had been in preparation for about eight years at the time.\n\nGrothendieck nevertheless wrote a revised version of EGA I which was published by Springer-Verlag. It updates the terminology, replacing \"prescheme\" by \"scheme\" and \"scheme\" by \"separated scheme\", and heavily emphasizes the use of representable functors. The new preface of the second edition also includes a slightly revised plan of the complete treatise, now divided into twelve chapters.\n\nGrothendieck's EGA 5 which deals with Bertini type theorems is to some extent available\nfrom the Grothendieck Circle website. Monografie Matematyczne in Poland has accepted this\nvolume for publication but the editing process is quite slow at this time 2010.\nJames Milne has preserved some of the original Grothendieck notes and a translation of them\ninto English. They may be available from his websites connected with the University of Michigan in Ann Arbor.\n\nThe following table lays out the original and revised plan of the treatise and indicates where (in SGA or elsewhere) the topics intended for the later, unpublished chapters were treated by Grothendieck and his collaborators.\n\nIn addition to the actual chapters, an extensive \"Chapter 0\" on various preliminaries was divided between the volumes in which the treatise appeared. Topics treated range from category theory, sheaf theory and general topology to commutative algebra and homological algebra. The longest part of Chapter 0, attached to Chapter IV, is more than 200 pages.\n\nGrothendieck never gave permission for the 2nd edition of EGA I to be republished, so copies are rare but found in many libraries. The work on EGA was finally disrupted by Grothendieck's departure first from IHÉS in 1970 and soon afterwards from the mathematical establishment altogether. Grothendieck's incomplete notes on EGA V can be found at .\n\nIn historical terms, the development of the \"EGA\" approach set the seal on the application of sheaf theory to algebraic geometry, set in motion by Serre's basic paper \"FAC\". It also contained the first complete exposition of the algebraic approach to differential calculus, via principal parts. The foundational unification it proposed (see for example unifying theories in mathematics) has stood the test of time.\n\n\"EGA\" has been scanned by NUMDAM and is available at under \"Publications mathématiques de l'IHÉS\", volumes 4, 8, 11, 17, 20, 24, 28 and 32.\n\n\n\n"}
