{"id": "391888", "url": "https://en.wikipedia.org/wiki?curid=391888", "title": "79 (number)", "text": "79 (number)\n\nSeventy-nine is the natural number following 78 and preceding 80.\n\n79 is:\n\n\n\n\n"}
{"id": "43405", "url": "https://en.wikipedia.org/wiki?curid=43405", "title": "Actuary", "text": "Actuary\n\nAn actuary is a business professional who deals with the measurement and management of risk and uncertainty . The name of the corresponding field is actuarial science. These risks can affect both sides of the balance sheet and require asset management, liability management, and valuation skills . Actuaries provide assessments of financial security systems, with a focus on their complexity, their mathematics, and their mechanisms .\n\nWhile the concept of insurance dates to antiquity (, , ), the concepts needed to scientifically measure and mitigate risks have their origins in the 17th century studies of probability and annuities . Actuaries of the 21st century require analytical skills, business knowledge, and an understanding of human behavior and information systems to design and manage programs that control risk . The actual steps needed to become an actuary are usually country-specific; however, almost all processes share a rigorous schooling or examination structure and take many years to complete (, ).\n\nThe profession has consistently been ranked as one of the most desirable . In various studies, being an actuary was ranked number one or two multiple times since 2010 (, , ).\n\nActuaries use skills primarily in mathematics, particularly calculus-based probability and mathematical statistics, but also economics, computer science, finance, and business. For this reason, actuaries are essential to the insurance and reinsurance industries, either as staff employees or as consultants; to other businesses, including sponsors of pension plans; and to government agencies such as the Government Actuary's Department in the United Kingdom or the Social Security Administration in the United States of America. Actuaries assemble and analyze data to estimate the probability and likely cost of the occurrence of an event such as death, sickness, injury, disability, or loss of property. Actuaries also address financial questions, including those involving the level of pension contributions required to produce a certain retirement income and the way in which a company should invest resources to maximize its return on investments in light of potential risk. Using their broad knowledge, actuaries help design and price insurance policies, pension plans, and other financial strategies in a manner that will help ensure that the plans are maintained on a sound financial basis (, ).\n\nMost traditional actuarial disciplines fall into two main categories: life and non-life.\n\nLife actuaries, which include health and pension actuaries, primarily deal with mortality risk, morbidity risk, and investment risk. Products prominent in their work include life insurance, annuities, pensions, short and long term disability insurance, health insurance, health savings accounts, and long-term care insurance . In addition to these risks, social insurance programs are influenced by public opinion, politics, budget constraints, changing demographics, and other factors such as medical technology, inflation, and cost of living considerations (, ).\n\nNon-life actuaries, also known as property and casualty or general insurance actuaries, deal with both physical and legal risks that affect people or their property. Products prominent in their work include auto insurance, homeowners insurance, commercial property insurance, workers' compensation, malpractice insurance, product liability insurance, marine insurance, terrorism insurance, and other types of liability insurance .\n\nActuaries are also called upon for their expertise in enterprise risk management . This can involve dynamic financial analysis, stress testing, the formulation of corporate risk policy, and the setting up and running of corporate risk departments . Actuaries are also involved in other areas of the financial services industry, such as analysing securities offerings or market research .\n\nOn both the life and casualty sides, the classical function of actuaries is to calculate premiums and reserves for insurance policies covering various risks . On the casualty side, this analysis often involves quantifying the probability of a loss event, called the frequency, and the size of that loss event, called the severity. The amount of time that occurs before the loss event is important, as the insurer will not have to pay anything until after the event has occurred. On the life side, the analysis often involves quantifying how much a potential sum of money or a financial liability will be worth at different points in the future. Since neither of these kinds of analysis are purely deterministic processes, stochastic models are often used to determine frequency and severity distributions and the parameters of these distributions. Forecasting interest yields and currency movements also plays a role in determining future costs, especially on the life side .\n\nActuaries do not always attempt to predict aggregate future events. Often, their work may relate to determining the cost of financial liabilities that have already occurred, called retrospective reinsurance, or the development or re-pricing of new products.\n\nActuaries also design and maintain products and systems. They are involved in financial reporting of companies' assets and liabilities. They must communicate complex concepts to clients who may not share their language or depth of knowledge. Actuaries work under a code of ethics that covers their communications and work products .\n\nAs an outgrowth of their more traditional roles, actuaries also work in the fields of risk management and enterprise risk management for both financial and non-financial corporations . Actuaries in traditional roles study and use the tools and data previously in the domain of finance . The Basel II accord for financial institutions (2004), and its analogue, the Solvency II accord for insurance companies (to come into effect in 2016), require institutions to account for operational risk separately, and in addition to, credit, reserve, asset, and insolvency risk. Actuarial skills are well suited to this environment because of their training in analyzing various forms of risk, and judging the potential for upside gain, as well as downside loss associated with these forms of risk .\n\nActuaries are also involved in investment advice and asset management, and can be general business managers and chief financial officers (, ). They analyze business prospects with their financial skills in valuing or discounting risky future cash flows, and apply their pricing expertise from insurance to other lines of business. For example, insurance securitization requires both actuarial and finance skills . Actuaries also act as expert witnesses by applying their analysis in court trials to estimate the economic value of losses such as lost profits or lost wages .\n\nThe basic requirements of communal interests gave rise to risk sharing since the dawn of civilization. For example, people who lived their entire lives in a camp had the risk of fire, which would leave their band or family without shelter. After barter came into existence, more complex risks emerged and new forms of risk manifested. Merchants embarking on trade journeys bore the risk of losing goods entrusted to them, their own possessions, or even their lives. Intermediaries developed to warehouse and trade goods, which exposed them to financial risk. The primary providers in extended families or households ran the risk of premature death, disability or infirmity, which could leave their dependents to starve. Credit procurement was difficult if the creditor worried about repayment in the event of the borrower's death or infirmity. Alternatively, people sometimes lived too long from a financial perspective, exhausting their savings, if any, or becoming a burden on others in the extended family or society .\n\nIn the ancient world there was not always room for the sick, suffering, disabled, aged, or the poor—these were often not part of the cultural consciousness of societies . Early methods of protection, aside from the normal support of the extended family, involved charity; religious organizations or neighbors would collect for the destitute and needy. By the middle of the 3rd century, 1,500 suffering people were being supported by charitable operations in Rome . Charitable protection remains an active form of support in the modern era , but receiving charity is uncertain and is often accompanied by social stigma. Elementary mutual aid agreements and pensions did arise in antiquity . Early in the Roman empire, associations were formed to meet the expenses of burial, cremation, and monuments—precursors to burial insurance and friendly societies. A small sum was paid into a communal fund on a weekly basis, and upon the death of a member, the fund would cover the expenses of rites and burial. These societies sometimes sold shares in the building of columbāria, or burial vaults, owned by the fund . Other early examples of mutual surety and assurance pacts can be traced back to various forms of fellowship within the Saxon clans of England and their Germanic forebears, and to Celtic society .\n\nNon-life insurance started as a hedge against loss of cargo during sea travel. Anecdotal reports of such guarantees occur in the writings of Demosthenes, who lived in the 4th century BCE . The earliest records of an official non-life insurance policy come from Sicily, where there is record of a 14th-century contract to insure a shipment of wheat . In 1350, Lenardo Cattaneo assumed \"all risks from act of God, or of man, and from perils of the sea\" that may occur to a shipment of wheat from Sicily to Tunis up to a maximum of 300 florins. For this he was paid a premium of 18% .\n\nDuring the 17th century, a more scientific basis for risk management was being developed. In 1662, a London draper named John Graunt showed that there were predictable patterns of longevity and death in a defined group, or cohort, of people, despite the uncertainty about the future longevity or mortality of any one individual. This study became the basis for the original life table. Combining this idea with that of compound interest and annuity valuation, it became possible to set up an insurance scheme to provide life insurance or pensions for a group of people, and to calculate with some degree of accuracy each member's necessary contributions to a common fund, assuming a fixed rate of interest. The first person to correctly calculate these values was Edmond Halley . In his work, Halley demonstrated a method of using his life table to calculate the premium someone of a given age should pay to purchase a life-annuity .\n\nJames Dodson's pioneering work on the level premium system led to the formation of the Society for Equitable Assurances on Lives and Survivorship (now commonly known as Equitable Life) in London in 1762. This was the first life insurance company to use premium rates that were calculated scientifically for long-term life policies, using Dodson's work. After Dodson's death in 1757, Edward Rowe Mores took over the leadership of the group that eventually became the Society for Equitable Assurances. It was he who specified that the chief official should be called an \"actuary\" . Previously, the use of the term had been restricted to an official who recorded the decisions, or \"acts\", of ecclesiastical courts, in ancient times originally the secretary of the Roman senate, responsible for compiling the \"Acta Senatus\" . Other companies that did not originally use such mathematical and scientific methods most often failed or were forced to adopt the methods pioneered by Equitable .\n\nIn the 18th and 19th centuries, computational complexity was limited to manual calculations. The actual calculations required to compute fair insurance premiums are complex. The actuaries of that time developed methods to construct easily used tables, using sophisticated approximations called commutation functions, to facilitate timely, accurate, manual calculations of premiums . Over time, actuarial organizations were founded to support and further both actuaries and actuarial science, and to protect the public interest by ensuring competency and ethical standards . Since calculations were cumbersome, actuarial shortcuts were commonplace.\n\nNon-life actuaries followed in the footsteps of their life compatriots in the early 20th century. In the United States, the 1920 revision to workers' compensation rates took over two months of around-the-clock work by day and night teams of actuaries . In the 1930s and 1940s, rigorous mathematical foundations for stochastic processes were developed . Actuaries began to forecast losses using models of random events instead of deterministic methods. Computers further revolutionized the actuarial profession. From pencil-and-paper to punchcards to microcomputers, the modeling and forecasting ability of the actuary has grown exponentially .\n\nAnother modern development is the convergence of modern financial theory with actuarial science . In the early 20th century, actuaries were developing techniques that can be found in modern financial theory, but for various historical reasons, these developments did not achieve much recognition . In the late 1980s and early 1990s, there was a distinct effort for actuaries to combine financial theory and stochastic methods into their established models . In the 21st century, the profession, both in practice and in the educational syllabi of many actuarial organizations, combines tables, loss models, stochastic methods, and financial theory , but is still not completely aligned with modern financial economics .\n\nAs there are relatively few actuaries in the world compared to other professions, actuaries are in high demand, and are highly paid for the services they render (, ). , in the United States, newly credentialed actuaries on average earn around $100,000 per year, while more experienced actuaries can earn over $150,000 per year . Similarly, survey in the United Kingdom indicated a starting salary for a newly credentialed actuary of about £50,000; actuaries with more experience can earn well in excess of £100,000 .\n\nThe actuarial profession has been consistently ranked for decades as one of the most desirable. Actuaries work comparatively reasonable hours, in comfortable conditions, without the need for physical exertion that may lead to injury, are well paid, and the profession consistently has a good hiring outlook . Not only has the overall profession ranked highly, but it also is considered one of the best professions for women , and one of the best recession-proof professions . In the United States, the profession was rated as the best profession by CareerCast, which uses five key criteria to rank jobs—environment, income, employment outlook, physical demands, and stress, in 2010 , 2013 , and 2015 . In other years, it remained in the top 10 (, , ). In the United Kingdom , and around the world , actuaries continue to be highly ranked as a profession.\n\nBecoming a fully credentialed actuary requires passing a rigorous series of professional examinations, usually taking several years. In some countries, such as Denmark, most study takes place in a university setting . In others, such as the US, most study takes place during employment through a series of examinations (, ). In the UK, and countries based on its process, there is a hybrid university-exam structure .\n\nAs these qualifying exams are extremely rigorous, support is usually available to people progressing through the exams. Often, employers provide paid on-the-job study time and paid attendance at seminars designed for the exams . Also, many companies that employ actuaries have automatic pay raises or promotions when exams are passed. As a result, actuarial students have strong incentives for devoting adequate study time during off-work hours. A common rule of thumb for exam students is that, for the Society of Actuaries examinations, roughly 400 hours of study time are necessary for each four-hour exam . Thus, thousands of hours of study time should be anticipated over several years, assuming no failures .\n\nHistorically, the actuarial profession has been reluctant to specify the pass marks for its examinations (, ). To address concerns that there are pre-existing pass/fail quotas, a former Chairman of the Board of Examiners of the Institute and Faculty of Actuaries stated, \"Although students find it hard to believe, the Board of Examiners does not have fail quotas to achieve. Accordingly pass rates are free to vary (and do). They are determined by the quality of the candidates sitting the examination and in particular how well prepared they are. Fitness to pass is the criterion, not whether you can achieve a mark in the top 40% of candidates sitting.\" . In 2000, the Casualty Actuarial Society (CAS) decided to start releasing pass marks for the exams it offers . The CAS's policy is also not to grade to specific pass ratios; the CAS board affirmed in 2001 that \"the CAS shall use no predetermined pass ratio as a guideline for setting the pass mark for any examination. If the CAS determines that 70% of all candidates have demonstrated sufficient grasp of the syllabus material, then those 70% should pass. Similarly, if the CAS determines that only 30% of all candidates have demonstrated sufficient grasp of the syllabus material, then only those 30% should pass.\".\n\n\n\n\n\n\n\n\n\n\n\n\n\nActuaries have appeared in works of fiction including literature, theater, television, and film. At times, they have been portrayed as \"math-obsessed, socially disconnected individuals with shockingly bad comb-overs\", which has resulted in a mixed response amongst actuaries themselves .\n\n\n\n\n\n\n"}
{"id": "4472066", "url": "https://en.wikipedia.org/wiki?curid=4472066", "title": "Atomic formula", "text": "Atomic formula\n\nIn mathematical logic, an atomic formula (also known simply as an atom) is a formula with no deeper propositional structure, that is, a formula that contains no logical connectives or equivalently a formula that has no strict subformulas. Atoms are thus the simplest well-formed formulas of the logic. Compound formulas are formed by combining the atomic formulas using the logical connectives.\n\nThe precise form of atomic formulas depends on the logic under consideration; for propositional logic, for example, the atomic formulas are the propositional variables. For predicate logic, the atoms are predicate symbols together with their arguments, each argument being a term. In model theory, atomic formula are merely strings of symbols with a given signature, which may or may not be satisfiable with respect to a given model.\n\nThe well-formed terms and propositions of ordinary first-order logic have the following syntax:\n\nTerms:\n\nthat is, a term is recursively defined to be a constant \"c\" (a named object from the domain of discourse), or a variable \"x\" (ranging over the objects in the domain of discourse), or an \"n\"-ary function \"f\" whose arguments are terms \"t\". Functions map tuples of objects to objects.\n\nPropositions:\n\nthat is, a proposition is recursively defined to be an \"n\"-ary predicate \"P\" whose arguments are terms \"t\", or an expression composed of logical connectives (and, or) and quantifiers (for-all, there-exists) used with other propositions.\n\nAn atomic formula or atom is simply a predicate applied to a tuple of terms; that is, an atomic formula is a formula of the form \"P\" (\"t\" ,…, \"t\") for \"P\" a predicate, and the \"t\" terms.\n\nAll other well-formed formulae are obtained by composing atoms with logical connectives and quantifiers.\n\nFor example, the formula ∀\"x. P\" (\"x\") ∧ ∃\"y. Q\" (\"y\", \"f\" (\"x\")) ∨ ∃\"z. R\" (\"z\") contains the atoms\n\nWhen all of the terms in an atom are ground terms, then the atom is called a ground atom or \"ground predicate\".\n\n"}
{"id": "545861", "url": "https://en.wikipedia.org/wiki?curid=545861", "title": "Bicategory", "text": "Bicategory\n\nIn mathematics, a bicategory (or a weak 2-category) is a concept in category theory used to extend the notion of category to handle the cases where the composition of morphisms is not (strictly) associative, but only associative \"up to\" an isomorphism. The notion was introduced in 1967 by Jean Bénabou.\n\nFormally, a bicategory B consists of:\nwith some more structure:\nThe horizontal composition is required to be associative up to a natural isomorphism α between morphisms formula_2 and formula_3. Some more coherence axioms, similar to those needed for monoidal categories, are moreover required to hold.\n\nBicategories may be considered as a weakening of the definition of 2-categories. A similar process for 3-categories leads to tricategories, and more generally to weak \"n\"-categories for \"n\"-categories.\n\n"}
{"id": "46926054", "url": "https://en.wikipedia.org/wiki?curid=46926054", "title": "Cosmos (category theory)", "text": "Cosmos (category theory)\n\nIn the area of mathematics known as category theory, a cosmos is a symmetric closed monoidal category that is complete and cocomplete. Enriched category theory is often considered over a cosmos.\n"}
{"id": "8984724", "url": "https://en.wikipedia.org/wiki?curid=8984724", "title": "Dirichlet algebra", "text": "Dirichlet algebra\n\nIn mathematics, a Dirichlet algebra is a particular type of algebra associated to a compact Hausdorff space \"X\". It is a closed subalgebra of \"C\"(\"X\"), the uniform algebra of bounded continuous functions on \"X\", whose real parts are dense in the algebra of bounded continuous real functions on \"X\". The concept was introduced by .\n\nLet formula_1 be the set of all rational functions that are continuous on formula_2; in other words functions that have no poles in formula_2. Then \n\nis a *-subalgebra of formula_5, and of formula_6. If formula_7 is dense in formula_6, we say formula_1 is a Dirichlet algebra.\n\nIt can be shown that if an operator formula_10 has formula_2 as a spectral set, and formula_1 is a Dirichlet algebra, then formula_10 has a normal boundary dilation. This generalises Sz.-Nagy's dilation theorem, which can be seen as a consequence of this by letting \n\n\n"}
{"id": "6594303", "url": "https://en.wikipedia.org/wiki?curid=6594303", "title": "Finite-dimensional distribution", "text": "Finite-dimensional distribution\n\nIn mathematics, finite-dimensional distributions are a tool in the study of measures and stochastic processes. A lot of information can be gained by studying the \"projection\" of a measure (or process) onto a finite-dimensional vector space (or finite collection of times).\n\nLet formula_1 be a measure space. The finite-dimensional distributions of formula_2 are the pushforward measures formula_3, where formula_4, formula_5, is any measurable function.\n\nLet formula_6 be a probability space and let formula_7 be a stochastic process. The finite-dimensional distributions of formula_8 are the push forward measures formula_9 on the product space formula_10 for formula_5 defined by\n\nVery often, this condition is stated in terms of measurable rectangles:\n\nThe definition of the finite-dimensional distributions of a process formula_8 is related to the definition for a measure formula_2 in the following way: recall that the law formula_16 of formula_8 is a measure on the collection formula_18 of all functions from formula_19 into formula_20. In general, this is an infinite-dimensional space. The finite dimensional distributions of formula_8 are the push forward measures formula_22 on the finite-dimensional product space formula_10, where\nis the natural \"evaluate at times formula_25\" function.\n\nIt can be shown that if a sequence of probability measures formula_26 is tight and all the finite-dimensional distributions of the formula_27 converge weakly to the corresponding finite-dimensional distributions of some probability measure formula_2, then formula_27 converges weakly to formula_2.\n\n"}
{"id": "58998821", "url": "https://en.wikipedia.org/wiki?curid=58998821", "title": "Fioralba Cakoni", "text": "Fioralba Cakoni\n\nFioralba Cakoni is an Albanian mathematician and an expert on inverse scattering theory. She is a professor of mathematics at Rutgers University.\n\nCakoni earned bachelor's and master's degrees from the University of Tirana in 1987 and 1990 respectively. She completed her Ph.D. in 1996, jointly between the University of Tirana and University of Patras, supervised by George Dassios. Her dissertation was \"Some Results on the Abstract Wave Equation. Problems of the Scattering Theory in Elasticity and Thermoelasticity in Low-Frequency\".\n\nShe became a lecturer at the University of Tirana and then, from 1998 to 2000, a Humboldt Research Fellow at the University of Stuttgart. She came to the US for additional postdoctoral research at the University of Delaware in 2000, and stayed at Delaware as an assistant professor beginning in 2002. She moved to Rutgers in 2015.\n\nCakoni is the author or coauthor of:\n\nCakoni was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to analysis of partial differential equations especially in inverse scattering theory\".\n\n"}
{"id": "31566167", "url": "https://en.wikipedia.org/wiki?curid=31566167", "title": "Gábor N. Sárközy", "text": "Gábor N. Sárközy\n\nGábor N. Sárközy (Gabor Sarkozy) is a Hungarian-American mathematician, the son of noted mathematician András Sárközy. He is currently on faculty of the Computer Science Department at Worcester Polytechnic Institute, MA, United States and is also a senior research fellow at the Alfréd Rényi Institute of Mathematics of the Hungarian Academy of Sciences.\n\nHe obtained a Diploma in Mathematics from Eötvös Loránd University and a PhD in Computer Science from Rutgers, under the advisement of Endre Szemerédi. \nPerhaps his best known result is the Blow-Up Lemma, in which, together with János Komlós and Endre Szemerédi he proved that the regular pairs in Szemerédi regularity lemma behave like complete bipartite graphs under the correct conditions. The lemma allowed for deeper exploration into the nature of embeddings of large sparse graphs into dense graphs. A hypergraph variant was developed later by Peter Keevash.\n\nHe is member of the editorial board of the \"European Journal of Combinatorics\".\n\nHe also has an Erdős number of 1.\n"}
{"id": "50664282", "url": "https://en.wikipedia.org/wiki?curid=50664282", "title": "Hesse's principle of transfer", "text": "Hesse's principle of transfer\n\nIn geometry, Hesse's principle of transfer () states that if the points of the projective line P are depicted by a rational normal curve in P, then the group of the projective transformations of P that preserve the curve is isomorphic to the group of the projective transformations of P (this is a generalization of the original Hesse's principle, in a form suggested by Wilhelm Franz Meyer). It was originally introduced by Otto Hesse in 1866, in a more restricted form. It influenced Felix Klein in the development of the Erlangen program. Since its original conception, it was generalized by many mathematicians, including Klein, Fano, and Cartan.\n\n\n\n"}
{"id": "4128810", "url": "https://en.wikipedia.org/wiki?curid=4128810", "title": "Homeotopy", "text": "Homeotopy\n\nIn algebraic topology, an area of mathematics, a homeotopy group of a topological space is a homotopy group of the group of self-homeomorphisms of that space.\n\nThe homotopy group functors formula_1 assign to each path-connected topological space formula_2 the group formula_3 of homotopy classes of continuous maps formula_4\n\nAnother construction on a space formula_2 is the group of all self-homeomorphisms formula_6, denoted formula_7 If \"X\" is a locally compact, locally connected Hausdorff space then a fundamental result of R. Arens says that formula_8 will in fact be a topological group under the compact-open topology.\n\nUnder the above assumptions, the homeotopy groups for formula_2 are defined to be:\n\nThus formula_11 is the extended mapping class group for formula_12 In other words, the extended mapping class group is the set of connected components of formula_8 as specified by the functor formula_14\n\nAccording to the Dehn-Nielsen theorem, if formula_2 is a closed surface then formula_16 the outer automorphism group of its fundamental group.\n\n"}
{"id": "1072915", "url": "https://en.wikipedia.org/wiki?curid=1072915", "title": "Kernel (linear algebra)", "text": "Kernel (linear algebra)\n\nIn mathematics, and more specifically in linear algebra and functional analysis, the kernel (also known as null space or nullspace) of a linear map between two vector spaces \"V\" and \"W\", is the set of all elements v of \"V\" for which , where 0 denotes the zero vector in \"W\". That is, in set-builder notation,\n\nThe kernel of \"L\" is a linear subspace of the domain \"V\".\nIn the linear map , two elements of \"V\" have the same image in \"W\" if and only if their difference lies in the kernel of \"L\":\nIt follows that the image of \"L\" is isomorphic to the quotient of \"V\" by the kernel:\nThis implies the rank–nullity theorem:\nwhere, by \"rank\" we mean the dimension of the image of \"L\", and by \"nullity\" that of the kernel of \"L\".\n\nWhen \"V\" is an inner product space, the quotient can be identified with the orthogonal complement in \"V\" of ker(\"L\"). This is the generalization to linear operators of the row space, or coimage, of a matrix.\n\nThe notion of kernel applies to the homomorphisms of modules, the latter being a generalization of the vector space over a field to that over a ring.\nThe domain of the mapping is a module, and the kernel constitutes a \"submodule\". Here, the concepts of rank and nullity do not necessarily apply.\n\nIf \"V\" and \"W\" are topological vector spaces (and \"W\" is finite-dimensional) then a linear operator \"L\": \"V\" → \"W\" is continuous if and only if the kernel of \"L\" is a closed subspace of \"V\".\n\nConsider a linear map represented as a \"m\" × \"n\" matrix \"A\" with coefficients in a field \"K\" (typically the field of the real numbers or of the complex numbers) and operating on column vectors \"x\" with \"n\" components over \"K\".\nThe kernel of this linear map is the set of solutions to the equation , where 0 is understood as the zero vector. The dimension of the kernel of \"A\" is called the nullity of \"A\". In set-builder notation,\nThe matrix equation is equivalent to a homogeneous system of linear equations:\nThus the kernel of \"A\" is the same as the solution set to the above homogeneous equations.\n\nThe kernel of an matrix \"A\" over a field \"K\" is a linear subspace of K. That is, the kernel of \"A\", the set Null(\"A\"), has the following three properties:\n\nThe product \"A\"x can be written in terms of the dot product of vectors as follows:\nHere a, ... , a denote the rows of the matrix \"A\". It follows that x is in the kernel of \"A\" if and only if x is orthogonal (or perpendicular) to each of the row vectors of \"A\" (because when the dot product of two vectors is equal to zero, they are by definition orthogonal).\n\nThe row space, or coimage, of a matrix \"A\" is the span of the row vectors of \"A\". By the above reasoning, the kernel of \"A\" is the orthogonal complement to the row space. That is, a vector x lies in the kernel of \"A\" if and only if it is perpendicular to every vector in the row space of \"A\".\n\nThe dimension of the row space of \"A\" is called the rank of \"A\", and the dimension of the kernel of \"A\" is called the nullity of \"A\". These quantities are related by the rank–nullity theorem\n\nThe left null space, or cokernel, of a matrix \"A\" consists of all vectors x such that x\"A\" = 0, where T denotes the transpose of a column vector. The left null space of \"A\" is the same as the kernel of \"A\". The left null space of \"A\" is the orthogonal complement to the column space of \"A\", and is dual to the cokernel of the associated linear transformation. The kernel, the row space, the column space, and the left null space of \"A\" are the four fundamental subspaces associated to the matrix \"A\".\n\nThe kernel also plays a role in the solution to a nonhomogeneous system of linear equations:\nIf u and v are two possible solutions to the above equation, then\nThus, the difference of any two solutions to the equation \"A\"x = b lies in the kernel of \"A\".\n\nIt follows that any solution to the equation \"A\"x = b can be expressed as the sum of a fixed solution v and an arbitrary element of the kernel. That is, the solution set to the equation \"Ax = b is\nGeometrically, this says that the solution set to \"A\"x = b is the translation of the kernel of \"A\" by the vector v. See also Fredholm alternative and flat (geometry).\n\nWe give here a simple illustration of computing the kernel of a matrix (see the section Basis below for methods better suited to more complex calculations.) We also touch on the row space and its relation to the kernel.\n\nConsider the matrix\nThe kernel of this matrix consists of all vectors (\"x\", \"y\", \"z\") ∈ R for which\nwhich can be expressed as a homogeneous system of linear equations involving \"x\", \"y\", and \"z\":\nwhich can be written in matrix form as:\nGauss–Jordan elimination reduces this to:\nRewriting yields:\nNow we can express an element of the kernel:\nfor \"c\" a scalar.\n\nSince \"c\" is a free variable, this can be expressed equally well as,\nThe kernel of \"A\" is precisely the solution set to these equations (in this case, a line through the origin in R); the vector (−1,−26,16) constitutes a basis of the kernel of \"A\".\nThus, the nullity of \"A\" is 1.\n\nNote also that the following dot products are zero:\nwhich illustrates that vectors in the kernel of A are orthogonal to each of the row vectors of A.\n\nThese two (linearly independent) row vectors span the row space of \"A\", a plane orthogonal to the vector (−1,−26,16).\n\nWith the rank of \"A\" 2, the nullity of \"A\" 1, and the dimension of \"A\" 3, we have an illustration of the rank-nullity theorem.\n\n\nA basis of the kernel of a matrix may be computed by Gaussian elimination.\n\nFor this purpose, given an \"m\" × \"n\" matrix \"A\", we construct first the row augmented matrix formula_26 where is the \"n\" × \"n\" identity matrix.\n\nComputing its column echelon form by Gaussian elimination (or any other suitable method), we get a matrix formula_27 A basis of the kernel of \"A\" consists in the non-zero columns of \"C\" such that the corresponding column of \"B\" is a zero column.\n\nIn fact, the computation may be stopped as soon as the upper matrix is in column echelon form: the remainder of the computation consists in changing the basis of the vector space generated by the columns whose upper part is zero.\n\nFor example, suppose that\nThen\n\nPutting the upper part in column echelon form by column operations on the whole matrix gives\n\nThe last three columns of \"B\" are zero columns. Therefore, the three last vectors of \"C\",\nare a basis of the kernel of \"A\".\n\nProof that the method computes the kernel: Since column operations correspond to post-multiplication by invertible matrices, the fact that formula_32 reduces to formula_33 means that there exists an invertible matrix formula_34 such that formula_35 with formula_36 in column echelon form. Thus formula_37 formula_38 and formula_39 A column vector formula_40 belongs to the kernel of formula_41 (that is formula_42) if and only formula_43 where formula_44 As formula_36 is in column echelon form, formula_43 if and only if the nonzero entries of formula_47 correspond to the zero columns of formula_48 By multiplying by formula_49, one may deduce that this is the case if and only if formula_50 is a linear combination of the corresponding columns of formula_51\n\nThe problem of computing the kernel on a computer depends on the nature of the coefficients.\n\nIf the coefficients of the matrix are exactly given numbers, the column echelon form of the matrix may be computed by Bareiss algorithm more efficiently than with Gaussian elimination. It is even more efficient to use modular arithmetic and Chinese remainder theorem, which reduces the problem to several similar ones over finite fields (this avoids the overhead induced by the non-linearity of the computational complexity of integer multiplication).\n\nFor coefficients in a finite field, Gaussian elimination works well, but for the large matrices that occur in cryptography and Gröbner basis computation, better algorithms are known, which have roughly the same computational complexity, but are faster and behave better with modern computer hardware.\n\nFor matrices whose entries are floating-point numbers, the problem of computing the kernel makes sense only for matrices such that the number of rows is equal to their rank: because of the rounding errors, a floating-point matrix has almost always a full rank, even when it is an approximation of a matrix of a much smaller rank. Even for a full-rank matrix, it is possible to compute its kernel only if it is well conditioned, i.e. it has a low condition number.\n\nEven for a well conditioned full rank matrix, Gaussian elimination does not behave correctly: it introduces rounding errors that are too large for getting a significant result. As the computation of the kernel of a matrix is a special instance of solving a homogeneous system of linear equations, the kernel may be computed by any of the various algorithms designed to solve homogeneous systems. A state of the art software for this purpose is the Lapack library.\n\n\n"}
{"id": "9750368", "url": "https://en.wikipedia.org/wiki?curid=9750368", "title": "Kuratowski's free set theorem", "text": "Kuratowski's free set theorem\n\nKuratowski's free set theorem, named after Kazimierz Kuratowski, is a result of set theory, an area of mathematics. It is a result which has been largely forgotten for almost 50 years, but has been applied recently in solving several lattice theory problems, such as the congruence lattice problem.\n\nDenote by formula_1 the set of all finite subsets of a set formula_2. Likewise, for a positive integer formula_3, denote by formula_4 the set of all formula_3-elements subsets of formula_2. For a mapping formula_7, we say that a subset formula_8 of formula_2 is \"free\" (with respect to formula_10), if for any formula_3-element subset formula_12 of formula_8 and any formula_14, formula_15. Kuratowski published in 1951 the following result, which characterizes the infinite cardinals of the form formula_16.\n\nThe theorem states the following. Let formula_3 be a positive integer and let formula_2 be a set. Then the cardinality of formula_2 is greater than or equal to formula_16 if and only if for every mapping formula_10 from formula_4 to formula_1,\nthere exists an formula_24-element free subset of formula_2 with respect to formula_10.\n\nFor formula_27, Kuratowski's free set theorem is superseded by Hajnal's set mapping theorem.\n\n"}
{"id": "7192559", "url": "https://en.wikipedia.org/wiki?curid=7192559", "title": "L/poly", "text": "L/poly\n\nIn computational complexity theory, L/poly is the complexity class of logarithmic space machines with a polynomial amount of advice. L/poly is a non-uniform logarithmic space class, analogous to the non-uniform polynomial time class P/poly. \n\nFormally, for a formal language to belong to L/poly, there must exist an advice function that maps an integer to a string of length polynomial in , and a Turing machine M with two read-only input tapes and one read-write tape of size logarithmic in the input size, such that an input of length belongs to if and only if machine M accepts the input . Alternatively and more simply, is in L/poly if and only if it can be recognized by branching programs of polynomial size. One direction of the proof that these two models of computation are equivalent in power is the observation that, if a branching program of polynomial size exists, it can be specified by the advice function and simulated by the Turing machine. In the other direction, a Turing machine with logarithmic writable space and a polynomial advice tape may be simulated by a branching program the states of which represent the combination of the configuration of the writable tape and the position of the Turing machine heads on the other two tapes.\n\nIn 1979, Aleliunas et al. showed that symmetric logspace is contained in L/poly. However, this result was superseded by Omer Reingold's result that SL collapses to uniform logspace.\n\nBPL is contained in L/poly, which is a variant of Adleman's theorem.\n"}
{"id": "1448472", "url": "https://en.wikipedia.org/wiki?curid=1448472", "title": "Laplacian matrix", "text": "Laplacian matrix\n\nIn the mathematical field of graph theory, the Laplacian matrix, sometimes called admittance matrix, Kirchhoff matrix or discrete Laplacian, is a matrix representation of a graph. The Laplacian matrix can be used to find many useful properties of a graph. Together with Kirchhoff's theorem, it can be used to calculate the number of spanning trees for a given graph. The sparsest cut of a graph can be approximated through the second smallest eigenvalue of its Laplacian by Cheeger's inequality. It can also be used to construct low dimensional embeddings, which can be useful for a variety of machine learning applications.\n\nGiven a simple graph \"G\" with \"n\" vertices, its Laplacian matrix formula_1 is \ndefined as:\n\nwhere \"D\" is the degree matrix and \"A\" is the adjacency matrix of the graph. Since formula_3 is a simple graph, formula_4 only contains 1s or 0s and its diagonal elements are all 0s.\n\nIn the case of directed graphs, either the indegree or outdegree might be used, depending on the application.\n\nThe elements of formula_5 are given by\n\nwhere deg(\"v\") is the degree of the vertex \"i\".\n\nThe symmetric normalized Laplacian matrix is defined as:\n\nThe elements of formula_8 are given by\n\nThe random-walk normalized Laplacian matrix is defined as:\n\nThe elements of formula_11 are given by\n\nThe generalized Laplacian, \"Q\", is defined as:\n\nNotice the ordinary Laplacian is a generalized Laplacian.\n\nHere is a simple example of a labeled, undirected graph and its Laplacian matrix.\n\nFor an (undirected) graph \"G\" and its Laplacian matrix \"L\" with eigenvalues formula_14:\n\n\nDefine an formula_26 oriented incidence matrix \"M\" with element \"M\" for edge \"e\" (connecting vertex \"i\" and \"j\", with \"i\" > \"j\") and vertex \"v\" given by\n\nThen the Laplacian matrix \"L\" satisfies\n\nwhere formula_29 is the matrix transpose of \"M\".\n\nNow consider an eigendecomposition of formula_5, with unit-norm eigenvectors formula_31 and corresponding eigenvalues formula_32:\n\nBecause formula_32 can be written as the inner product of the vector formula_35 with itself, this shows that formula_15 and so the eigenvalues of formula_5 are all non-negative.\n\nThe deformed Laplacian is commonly defined as\n\nwhere \"I\" is the unit matrix, \"A\" is the adjacency matrix, and \"D\" is the degree matrix, and \"s\" is a (complex-valued) number. Note that the standard Laplacian is just formula_39.\n\nThe (symmetric) normalized Laplacian is defined as\n\nwhere \"L\" is the (unnormalized) Laplacian, \"A\" is the adjacency matrix and \"D\" is the degree matrix. Since the degree matrix \"D\" is diagonal and positive, its reciprocal square root formula_41 is just the diagonal matrix whose diagonal entries are the reciprocals of the positive square roots of the diagonal entries of \"D\". The symmetric normalized Laplacian is a symmetric matrix.\n\nOne has: formula_42, where S is the matrix whose rows are indexed by the vertices and whose columns are indexed by the edges of G such that each column corresponding to an edge e = {u, v} has an entry formula_43 in the row corresponding to u, an entry formula_44 in the row corresponding to v, and has 0 entries elsewhere. (Note: formula_45 denotes the transpose of S).\n\nAll eigenvalues of the normalized Laplacian are real and non-negative. We can see this as follows. Since formula_8 is symmetric, its eigenvalues are real. They are also non-negative: consider an eigenvector formula_47 of formula_8 with eigenvalue λ and suppose formula_49. (We can consider g and f as real functions on the vertices v.) Then:\n\nwhere we use the inner product formula_51, a sum over all vertices v, and formula_52 denotes the sum over all unordered pairs of adjacent vertices {u,v}. The quantity formula_53 is called the \"Dirichlet sum\" of f, whereas the expression formula_54 is called the \"Rayleigh quotient\" of g.\n\nLet 1 be the function which assumes the value 1 on each vertex. Then formula_55 is an eigenfunction of formula_56 with eigenvalue 0.\n\nIn fact, the eigenvalues of the normalized symmetric Laplacian satisfy 0 = μ ≤ … ≤ μ ≤ 2. These eigenvalues (known as the spectrum of the normalized Laplacian) relate well to other graph invariants for general graphs.\n\nThe random walk normalized Laplacian is defined as\n\nwhere \"D\" is the degree matrix. Since the degree matrix \"D\" is diagonal, its inverse formula_58 is simply defined as a diagonal matrix, having diagonal entries which are the reciprocals of the corresponding positive diagonal entries of \"D\".\n\nFor the isolated vertices (those with degree 0), a common choice is to set the corresponding element formula_59 to 0.\n\nThis convention results in a nice property that the multiplicity of the eigenvalue 0 is equal to the number of connected components in the graph.\n\nThe matrix elements of formula_11 are given by\n\nThe name of the random-walk normalized Laplacian comes from the fact that this matrix is formula_62, where formula_63 is simply the transition matrix\nof a random walker on the graph. For example, let formula_64 denote the i-th standard basis vector. Then formula_65 is a probability vector representing the distribution of a random walker's locations after taking a single step from vertex formula_16; i.e., formula_67. More generally, if the vector formula_68 is a probability distribution of the location of a random walker on the vertices of the graph, then formula_69 is the probability distribution of the walker after formula_70 steps.\n\nOne can check that\n\ni.e., formula_11 is similar to the normalized Laplacian formula_8. For this reason, even if formula_11 is in general not hermitian, it has real eigenvalues. Indeed, its eigenvalues agree with those of formula_8 (which is hermitian). \n\nAs an aside about random walks on graphs, consider a simple undirected graph. Consider the probability that the walker is at the vertex \"i\" at time \"t\", given the probability distribution that he was at vertex \"j\" at time \"t − 1\" (assuming a uniform chance of taking a step along any of the edges attached to a given vertex):\n\nor in matrix-vector notation:\n\nWe can rewrite this relation as\n\nformula_81 is a symmetric matrix called the reduced adjacency matrix. So, taking steps on this random walk requires taking powers of formula_82, which is a simple operation because formula_82 is symmetric.\n\nThe Laplacian matrix can be interpreted as a matrix representation of a particular case of the discrete Laplace operator. Such an interpretation allows one, e.g., to generalise the Laplacian matrix to the case of graphs with an infinite number of vertices and edges, leading to a Laplacian matrix of an infinite size.\n\nSuppose formula_84 describes a heat distribution across a graph, where formula_85 is the heat at vertex formula_16. According to Newton's law of cooling, the heat transferred between nodes formula_16 and formula_88 is proportional to formula_89 if nodes formula_16 and formula_88 are connected (if they are not connected, no heat is transferred). Then, for heat capacity formula_92,\n\nIn matrix-vector notation,\n\nwhich gives\n\nNotice that this equation takes the same form as the heat equation, where the matrix −\"L\" is replacing the Laplacian operator formula_96; hence, the \"graph Laplacian\".\n\nTo find a solution to this differential equation, apply standard techniques for solving a first-order matrix differential equation. That is, write formula_84 as a linear combination of eigenvectors formula_31 of \"L\" (so that formula_99), with time-dependent formula_100\n\nPlugging into the original expression (note that we will use the fact that because \"L\" is a symmetric matrix, its unit-norm eigenvectors formula_31 are orthogonal):\n\nwhose solution is\n\nAs shown before, the eigenvalues formula_32 of \"L\" are non-negative, showing that the solution to the diffusion equation approaches an equilibrium, because it only exponentially decays or remains constant. This also shows that given formula_32 and the initial condition formula_106, the solution at any time \"t\" can be found.\n\nTo find formula_106 for each formula_16 in terms of the overall initial condition formula_109, simply project formula_109 onto the unit-norm eigenvectors formula_31;\n\nIn the case of undirected graphs, this works because formula_5 is symmetric, and by the spectral theorem, its eigenvectors are all orthogonal. So the projection onto the eigenvectors of formula_5 is simply an orthogonal coordinate transformation of the initial condition to a set of coordinates which decay exponentially and independently of each other.\n\nTo understand formula_115, note that the only terms formula_116 that remain are those where formula_117, since\n\nIn other words, the equilibrium state of the system is determined completely by the kernel of formula_5. \n\nSince by definition, formula_120, the vector formula_121 of all ones is in the kernel. Note also that if there are formula_92 disjoint connected components in the graph, then this vector of all ones can be split into the sum of formula_92 independent formula_124 eigenvectors of ones and zeros, where each connected component corresponds to an eigenvector with ones at the elements in the connected component and zeros elsewhere.\n\nThe consequence of this is that for a given initial condition formula_125 for a graph with formula_126 vertices\n\nwhere\n\nFor each element formula_129 of formula_84, i.e. for each vertex formula_88 in the graph, it can be rewritten as\n\nIn other words, at steady state, the value of formula_84 converges to the same value at each of the vertices of the graph, which is the average of the initial values at all of the vertices. Since this is the solution to the heat diffusion equation, this makes perfect sense intuitively. We expect that neighboring elements in the graph will exchange energy until that energy is spread out evenly throughout all of the elements that are connected to each other.\n\nThis section shows an example of a function formula_84 diffusing over time through a graph. The graph in this example is constructed on a 2D discrete grid, with points on the grid connected to their eight neighbors. Three initial points are specified to have a positive value, while the rest of the values in the grid are zero. Over time, the exponential decay acts to distribute the values at these points evenly throughout the entire grid.\n\nThe complete Matlab source code that was used to generate this animation is provided below. It shows the process of specifying initial conditions, projecting these initial conditions onto the eigenvalues of the Laplacian Matrix, and simulating the exponential decay of these projected initial conditions.\n\nThe graph Laplacian matrix can be further viewed as a matrix form of an approximation to the (positive semi-definite) Laplacian operator obtained by the finite difference method. In this interpretation, every graph vertex is treated as a grid point; the local connectivity of the vertex determines the finite difference approximation stencil at this grid point, the grid size is always one for every edge, and there are no constraints on any grid points, which corresponds to the case of the homogeneous Neumann boundary condition, i.e., free boundary.\nAn analogue of the Laplacian matrix can be defined for directed multigraphs. In this case the Laplacian matrix \"L\" is defined as\n\nwhere \"D\" is a diagonal matrix with \"D\" equal to the outdegree of vertex \"i\" and \"A\" is a matrix with \"A\" equal to the number of edges from \"i\" to \"j\" (including loops).\n\n\n"}
{"id": "167699", "url": "https://en.wikipedia.org/wiki?curid=167699", "title": "Liber Abaci", "text": "Liber Abaci\n\nLiber Abaci (also spelled as Liber Abbaci) (\"The Book of Calculation\") is a 1202 historic book on arithmetic by Leonardo of Pisa, posthumously known as Fibonacci.\n\n\"Liber Abaci\" was among the first Western books to describe the Hindu–Arabic numeral system and to use symbols traditionally described as \"Arabic numerals\". By addressing the applications of both commercial tradesmen and mathematicians, it contributed to convincing the public of the superiority of the system, and the use of these glyphs.\n\nAlthough the book's title has also been translated as \"The Book of the Abacus\", writes that this is an error: the intent of the book is to describe methods of doing calculations without aid of an abacus, and as confirms, for centuries after its publication the algorismists (followers of the style of calculation demonstrated in \"Liber Abaci\") remained in conflict with the abacists (traditionalists who continued to use the abacus in conjunction with Roman numerals).\n\nThe second version of \"Liber Abaci\" was dedicated to Michael Scot in 1227 CE. No versions of the original 1202 book have been found.\n\nThe first section introduces the Hindu–Arabic numeral system, including methods for converting between different representation systems. This section also includes the first known description of trial division for testing whether a number is composite and, if so, factoring it.\n\nThe second section presents examples from commerce, such as conversions of currency and measurements, and calculations of profit and interest.\n\nThe third section discusses a number of mathematical problems; for instance, it includes (ch. II.12) the Chinese remainder theorem, perfect numbers and Mersenne primes as well as formulas for arithmetic series and for square pyramidal numbers. Another example in this chapter, describing the growth of a population of rabbits, was the origin of the Fibonacci sequence for which the author is most famous today.\n\nThe fourth section derives approximations, both numerical and geometrical, of irrational numbers such as square roots.\n\nThe book also includes proofs in Euclidean geometry. Fibonacci's method of solving algebraic equations shows the influence of the early 10th-century Egyptian mathematician Abū Kāmil Shujāʿ ibn Aslam.\n\nIn reading \"Liber Abaci\", it is helpful to understand Fibonacci's notation for rational numbers, a notation that is intermediate in form between the Egyptian fractions commonly used until that time and the vulgar fractions still in use today. There are three key differences between Fibonacci's notation and modern fraction notation.\n\nThe complexity of this notation allows numbers to be written in many different ways, and Fibonacci described several methods for converting from one style of representation to another. In particular, chapter II.7 contains a list of methods for converting an improper fraction to an Egyptian fraction, including the greedy algorithm for Egyptian fractions, also known as the Fibonacci–Sylvester expansion.\n\nIn the \"Liber Abaci\", Fibonacci says the following introducing the \"Modus Indorum\" or the method of the Indians, today known as Hindu–Arabic numerals or traditionally, just Arabic numerals.\n\nIn other words, in his book he advocated the use of the digits 0–9, and of place value. Until this time Europe used Roman Numerals, making modern mathematics almost impossible. The book thus made an important contribution to the spread of decimal numerals. The spread of the Hindu-Arabic system, however, as Ore writes, was \"long-drawn-out\", taking many more centuries to spread widely, and did not become complete until the later part of the 16th century, accelerating dramatically only in the 1500s with the advent of printing.\n\n"}
{"id": "3686118", "url": "https://en.wikipedia.org/wiki?curid=3686118", "title": "Long double", "text": "Long double\n\nIn C and related programming languages, codice_1 refers to a floating-point data type that is often more precise than double-precision. As with C's other floating-point types, it may not necessarily map to an IEEE format.\n\nThe codice_1 type was present in the original 1989 C standard, but support was improved by the 1999 revision of the C standard, or C99, which extended the standard library to include functions operating on codice_1 such as codice_5 and codice_6.\n\nLong double constants are floating-point constants suffixed with \"L\" or \"l\" (lower-case L), e.g., 0.333333333333333333L. Without a suffix, the evaluation depends on FLT_EVAL_METHOD.\n\nOn the x86 architecture, most C compilers implement codice_1 as the 80-bit extended precision type supported by x86 hardware (sometimes stored as 12 or 16 bytes to maintain data structure alignment), as specified in the C99 / C11 standards (IEC 60559 floating-point arithmetic (Annex F)). An exception is Microsoft Visual C++ for x86, which makes codice_1 a synonym for codice_9. The Intel C++ compiler on Microsoft Windows supports extended precision, but requires the codice_10 switch for codice_1 to correspond to the hardware's extended precision format.\n\nCompilers may also use codice_1 for a 128-bit quadruple precision format. This is the case on HP-UX and on Solaris/SPARC machines. This format is currently implemented in software due to lack of hardware support.\n\nOn some PowerPC and SPARCv9 machines, codice_1 is implemented as a double-double arithmetic, where a codice_1 value is regarded as the exact sum of two double-precision values, giving at least a 106-bit precision; with such a format, the codice_1 type does not conform to the IEEE floating-point standard. Otherwise, codice_1 is simply a synonym for codice_9 (double precision).\n\nWith the GNU C Compiler, codice_1 is 80-bit extended precision on x86 processors regardless of the physical storage used for the type (which can be either 96 or 128 bits), On some other architectures, codice_1 can be double-double (e.g. on PowerPC) or 128-bit quadruple precision (e.g. on SPARC). As of gcc 4.3, a quadruple precision is also supported on x86, but as the nonstandard type codice_20 rather than codice_1.\n\nAlthough the x86 architecture, and specifically the x87 floating-point instructions on x86, supports 80-bit extended-precision operations, it is possible to configure the processor to automatically round operations to double (or even single) precision. Conversely, in extended-precision mode, extended precision may be used for intermediate compiler-generated calculations even when the final results are stored at a lower precision (i.e. FLT_EVAL_METHOD == 2). With gcc on Linux, 80-bit extended precision is the default; on several BSD operating systems (FreeBSD and OpenBSD), double-precision mode is the default, and codice_1 operations are effectively reduced to double precision. (NetBSD 7.0 and later, however, defaults to 80-bit extended precision ). However, it is possible to override this within an individual program via the FLDCW \"floating-point load control-word\" instruction. On x86_64 the BSDs default to 80-bit extended precision. Microsoft Windows with Visual C++ also sets the processor in double-precision mode by default, but this can again be overridden within an individual program (e.g. by the codice_23 function in Visual C++). The Intel C++ Compiler for x86, on the other hand, enables extended-precision mode by default. On OS X, long double is 80-bit extended precision \n\nIn CORBA (from specification of 3.0, which uses \"ANSI/IEEE Standard 754-1985\" as its reference), \"the long double data type represents an IEEE double-extended floating-point number, which has an exponent of at least 15 bits in length and a signed fraction of at least 64 bits\", with GIOP/IIOP CDR, whose floating-point types \"exactly follow the IEEE standard formats for floating point numbers\", marshalling this as what seems to be IEEE 754-2008 binary128 a.k.a. quadruple precision without using that name.\n\n"}
{"id": "1117315", "url": "https://en.wikipedia.org/wiki?curid=1117315", "title": "Lyapunov time", "text": "Lyapunov time\n\nIn mathematics, the Lyapunov time is the characteristic timescale on which a dynamical system is chaotic. It is named after the Russian mathematician Aleksandr Lyapunov. It is defined as the inverse of a system's largest Lyapunov exponent.\n\nThe Lyapunov time mirrors the limits of the predictability of the system. By convention, it is defined as the time for the distance between nearby trajectories of the system to increase by a factor of \"e\". However, measures in terms of 2-foldings and 10-foldings are sometimes found, since they correspond to the loss of one bit of information or one digit of precision respectively.\n\nWhile it is used in many applications of dynamical systems theory, it has been particularly used in celestial mechanics where it is important for the stability of the Solar System question. However, empirical estimation of the Lyapunov time is often associated with computational or inherent uncertainties.\n\nTypical values are:\n"}
{"id": "56689677", "url": "https://en.wikipedia.org/wiki?curid=56689677", "title": "Maria Hasse", "text": "Maria Hasse\n\nMaria-Viktoria Hasse (May 30, 1921 – January 10, 2014) was a German mathematician who became the first female professor in the faculty of mathematics and science at TU Dresden. She wrote books on set theory and category theory, and is known as one of the namesakes of the Gallai–Hasse–Roy–Vitaver theorem in graph coloring.\n\nHasse was born in Warnemünde. She went to the gymnasium in Rostock, and after a term in the Reich Labour Service from 1939 to 1940, studied mathematics, physics, and philosophy at the University of Rostock and University of Tübingen from 1940 to 1943, earning a diploma in 1943 from Rostock. She continued at Rostock as an assistant and lecturer, earning a doctorate (Dr. rer. nat.) in 1949 and a habilitation in 1954. Her doctoral dissertation, \"Über eine singuläre Intergralgleichung 1. Art mit logarithmischer Unstetigkeit\" [On a singular integral equation of the 1st kind with logarithmic discontinuity], was supervised by Hans Schubert; her habilitation thesis was \"Über eine Hillsche Differentialgleichung\" [On Hill's differential equation]. She worked as a professor of algebra at TU Dresden from 1954 until her 1981 retirement.\n\nWith Lothar Michler, Hasse wrote \"Theorie der Kategorien\" [Category Theory] (Deutscher Verlag, 1966). She also wrote \"Grundbegriffe der Mengenlehre und Logik\" [Basic Concepts of Set Theory and Logic] (Harri Deutsch, 1968).\n\nIn the theory of graph coloring, the Gallai–Hasse–Roy–Vitaver theorem provides a duality between colorings of the vertices of a graph and orientations of its edges. It states that the minimum number of colors needed in a coloring equals the number of vertices in a longest path, in an orientation chosen to minimize the length of this path. It was stated in 1958 in a graph theory textbook by Claude Berge, and independently published by Hasse, Tibor Gallai, B. Roy, and L. Vitaver. Hasse's publication of this result was the second chronologically, in 1965.\n"}
{"id": "51171385", "url": "https://en.wikipedia.org/wiki?curid=51171385", "title": "Michael D. Fried", "text": "Michael D. Fried\n\nMichael David Fried is an American mathematician working in the geometry and arithmetic of families of nonsingular projective curve covers. He uses group representation theory to avoid solving equations (the monodromy method).\n\nFried's mathematical articles can be roughly divided into four groups: Arithmetic of Covers and Regular Inverse Galois Problem, \nHilbert's Irreducibility Theorem,\n\nFried got his PhD from University of Michigan in Mathematics (1964–1967); from 1959–1961 he got his undergraduate degree from Michigan State University in electrical engineering. Between those degrees he worked for three years as an aerospace electrical engineer. This included work on the Lunar Excursion Module and the Nautilus submarine .\nHe chose the two years of postdoctoral at the \nInstitute for Advanced Study in Princeton (1967–1969). Before living in Montana in 2004, he was a Professor at Stony Brook University (8 years), University of California at Irvine (26 years), University of Florida (3 years) and Hebrew University (2 years). He has been a visiting professor at MIT, MSRI, University of Michigan, University of Florida, Hebrew University and Tel Aviv University. He has been an editor on several mathematics journals including the Research Announcements of the Bulletin of the American Mathematical Society, and the Journal of Finite Fields and its Applications. He was included in the inaugural (2013) class of Fellows of the AMS.\nFrieds fellowships include Alfred P. Sloan Foundation (1972–1974), Lady Davis Fellow at Hebrew University (1987–1988), Fulbright spent at Helsinki University (1982–1983), Alexander von Humboldt Research Fellowship (1994–1996), and the two periods at the Institute for Advanced Study (Fall, 1967 to Spring 1969 and Spring 1974).\n\n\n"}
{"id": "1508682", "url": "https://en.wikipedia.org/wiki?curid=1508682", "title": "Mixed-data sampling", "text": "Mixed-data sampling\n\nMixed-data sampling (MIDAS) is an econometric regression or filtering method developed by Ghysels \"et al.\" There is now a substantial literature on MIDAS regressions and their applications, including Andreou et al. (2010), and especially Andreou et al. (2013).\n\nA simple regression example has the independent variable appearing at a higher frequency than the dependent variable:\n\nwhere \"y\" is the dependent variable, \"x\" is the regressor, \"m\" denotes the frequency – for instance if \"y\" is yearly formula_2 is quarterly – formula_3 is the disturbance and formula_4 is a lag distribution, for instance the Beta function or the Almon Lag.\n\nThe regression models can be viewed in some cases as substitutes for the Kalman filter when applied in the context of mixed frequency data. Bai, Ghysels and Wright (2010) examine the relationship between MIDAS regressions and Kalman filter state space models applied to mixed frequency data. In general, the latter involve a system of equations, whereas in contrast MIDAS\nregressions involve a (reduced form) single equation. As a consequence, MIDAS regressions might be less efficient, but also less prone to specification errors. In cases where the MIDAS regression is only an approximation, the approximation errors tend to be small.\n\n\n\n"}
{"id": "34996922", "url": "https://en.wikipedia.org/wiki?curid=34996922", "title": "Nagata's conjecture", "text": "Nagata's conjecture\n\nIn algebra, Nagata's conjecture states that Nagata's automorphism of the polynomial ring \"k\"[\"x\",\"y\",\"z\"] is wild. The conjecture was proposed by and proved by .\n\nNagata's automorphism is given by\n\n"}
{"id": "48629", "url": "https://en.wikipedia.org/wiki?curid=48629", "title": "Normal space", "text": "Normal space\n\nIn topology and related branches of mathematics, a normal space is a topological space \"X\" that satisfies Axiom T: every two disjoint closed sets of \"X\" have disjoint open neighborhoods. A normal Hausdorff space is also called a T space. These conditions are examples of separation axioms and their further strengthenings define completely normal Hausdorff spaces, or T spaces, and perfectly normal Hausdorff spaces, or T spaces.\n\nA topological space \"X\" is a normal space if, given any disjoint closed sets \"E\" and \"F\", there are neighbourhoods \"U\" of \"E\" and \"V\" of \"F\" that are also disjoint. More intuitively, this condition says that \"E\" and \"F\" can be separated by neighbourhoods.\n\nA T space is a T space \"X\" that is normal; this is equivalent to \"X\" being normal and Hausdorff.\n\nA completely normal space or a is a topological space \"X\" such that every subspace of \"X\" with subspace topology is a normal space. It turns out that \"X\" is completely normal if and only if every two separated sets can be separated by neighbourhoods.\n\nA completely T space, or T space is a completely normal T space topological space \"X\", which implies that \"X\" is Hausdorff; equivalently, every subspace of \"X\" must be a T space.\n\nA perfectly normal space is a topological space \"X\" in which every two disjoint closed sets \"E\" and \"F\" can be precisely separated by a continuous function \"f\" from \"X\" to the real line R: the preimages of {0} and {1} under \"f\" are, respectively, \"E\" and \"F\". (In this definition, the real line can be replaced with the unit interval [0,1].)\n\nIt turns out that \"X\" is perfectly normal if and only if \"X\" is normal and every closed set is a \"G\" set. Equivalently, \"X\" is perfectly normal if and only if every closed set is a zero set. Every perfectly normal space is automatically completely normal.\n\nA Hausdorff perfectly normal space \"X\" is a T space, or perfectly T space.\n\nNote that the terms \"normal space\" and \"T\" and derived concepts occasionally have a different meaning. (Nonetheless, \"T\" always means the same as \"completely T\", whatever that may be.) The definitions given here are the ones usually used today. For more on this issue, see History of the separation axioms.\n\nTerms like \"normal regular space\" and \"normal Hausdorff space\" also turn up in the literature – they simply mean that the space both is normal and satisfies the other condition mentioned. In particular, a normal Hausdorff space is the same thing as a T space. Given the historical confusion of the meaning of the terms, verbal descriptions when applicable are helpful, that is, \"normal Hausdorff\" instead of \"T\", or \"completely normal Hausdorff\" instead of \"T\".\n\nFully normal spaces and fully T spaces are discussed elsewhere; they are related to paracompactness.\n\nA locally normal space is a topological space where every point has an open neighbourhood that is normal. Every normal space is locally normal, but the converse is not true. A classical example of a completely regular locally normal space that is not normal is the Nemytskii plane.\n\nMost spaces encountered in mathematical analysis are normal Hausdorff spaces, or at least normal regular spaces:\n\nAlso, all fully normal spaces are normal (even if not regular). Sierpinski space is an example of a normal space that is not regular.\n\nAn important example of a non-normal topology is given by the Zariski topology on an algebraic variety or on the spectrum of a ring, which is used in algebraic geometry.\n\nA non-normal space of some relevance to analysis is the topological vector space of all functions from the real line R to itself, with the topology of pointwise convergence.\nMore generally, a theorem of A. H. Stone states that the product of uncountably many non-compact metric spaces is never normal.\n\nEvery closed subset of a normal space is normal. The continuous and closed image of a normal space is normal.\n\nThe main significance of normal spaces lies in the fact that they admit \"enough\" continuous real-valued functions, as expressed by the following theorems valid for any normal space \"X\".\n\nUrysohn's lemma:\nIf \"A\" and \"B\" are two disjoint closed subsets of \"X\", then there exists a continuous function \"f\" from \"X\" to the real line R such that \"f\"(\"x\") = 0 for all \"x\" in \"A\" and \"f\"(\"x\") = 1 for all \"x\" in \"B\".\nIn fact, we can take the values of \"f\" to be entirely within the unit interval [0,1].\n\nMore generally, the Tietze extension theorem:\nIf \"A\" is a closed subset of \"X\" and \"f\" is a continuous function from \"A\" to R, then there exists a continuous function \"F\": \"X\" → R which extends \"f\" in the sense that \"F\"(\"x\") = \"f\"(\"x\") for all \"x\" in \"A\".\n\nIf U is a locally finite open cover of a normal space \"X\", then there is a partition of unity precisely subordinate to U.\n\nIn fact, any space that satisfies any one of these three conditions must be normal.\n\nA product of normal spaces is not necessarily normal. This fact was first proved by Robert Sorgenfrey. An example of this phenomenon is the Sorgenfrey plane. Also, a subset of a normal space need not be normal (i.e. not every normal Hausdorff space is a completely normal Hausdorff space), since every Tychonoff space is a subset of its Stone–Čech compactification (which is normal Hausdorff). A more explicit example is the Tychonoff plank. The only large class of product spaces of normal spaces known to be normal are the products of compact Hausdorff spaces, since both compactness (Tychonoff's theorem) and the axiom are preserved under arbitrary products.\n\nIf a normal space is R, then it is in fact completely regular.\nThus, anything from \"normal R\" to \"normal completely regular\" is the same as what we usually call \"normal regular\".\nTaking Kolmogorov quotients, we see that all normal T spaces are Tychonoff.\nThese are what we usually call \"normal Hausdorff\" spaces.\n\nA topological space is said to be pseudonormal if given two disjoint closed sets in it, one of which is countable, there are disjoint open sets containing them. Every normal space is pseudonormal, but not vice versa.\n\nCounterexamples to some variations on these statements can be found in the lists above.\nSpecifically, Sierpinski space is normal but not regular, while the space of functions from R to itself is Tychonoff but not normal.\n\n"}
{"id": "25092831", "url": "https://en.wikipedia.org/wiki?curid=25092831", "title": "Oligomorphic group", "text": "Oligomorphic group\n\nIn group theory, a branch of mathematics, an oligomorphic group is a particular kind of permutation group. If a group \"G\" acts on a set \"S\" (usually infinite), then \"G\" is said to be oligomorphic if this action has only finitely many orbits on every Cartesian product \"S\" of \"S\" (\"n\"-tuples of elements of \"S\" for every natural number \"n\"). The interest in oligomorphic groups is partly based on their application to model theory, e.g. automorphisms in countably categorical theories.\n\n"}
{"id": "49497", "url": "https://en.wikipedia.org/wiki?curid=49497", "title": "Pascal's triangle", "text": "Pascal's triangle\n\nIn mathematics, Pascal's triangle is a triangular array of the binomial coefficients. In much of the Western world, it is named after the French mathematician Blaise Pascal, although other mathematicians studied it centuries before him in India, Persia (Iran), China, Germany, and Italy.\n\nThe rows of Pascal's triangle are conventionally enumerated starting with row \"n\" = 0 at the top (the 0th row). The entries in each row are numbered from the left beginning with \"k\" = 0 and are usually staggered relative to the numbers in the adjacent rows. The triangle may be constructed in the following manner: In row 0 (the topmost row), there is a unique nonzero entry 1. Each entry of each subsequent row is constructed by adding the number above and to the left with the number above and to the right, treating blank entries as 0. For example, the initial number in the first (or any other) row is 1 (the sum of 0 and 1), whereas the numbers 1 and 3 in the third row are added to produce the number 4 in the fourth row.\n\nThe entry in the \"n\"th row and \"k\"th column of Pascal's triangle is denoted formula_1. For example, the unique nonzero entry in the topmost row is formula_2. With this notation, the construction of the previous paragraph may be written as follows:\n\nfor any non-negative integer \"n\" and any integer \"k\" between 0 and \"n\", inclusive. This recurrence for the binomial coefficients is known as Pascal's rule.\n\nPascal's triangle has higher dimensional generalizations. The three-dimensional version is called \"Pascal's pyramid\" or \"Pascal's tetrahedron\", while the general versions are called \"Pascal's simplices\".\n\nThe pattern of numbers that forms Pascal's triangle was known well before Pascal's time. Pascal innovated many previously unattested uses of the triangle's numbers, uses he described comprehensively in what is perhaps the earliest known mathematical treatise to be specially devoted to the triangle, his \"Traité du triangle arithmétique\" (1653). Centuries before, discussion of the numbers had arisen in the context of Indian studies of combinatorics and of binomial numbers and Greeks' study of figurate numbers.\n\nFrom later commentary, it appears that the binomial coefficients and the additive formula for generating them, formula_4, were known to Pingala in or before the 2nd century BC. While Pingala's work only survives in fragments, the commentator Varāhamihira, around 505, gave a clear description of the additive formula, and a more detailed explanation of the same rule was given by Halayudha, around 975. Halayudha also explained obscure references to \"Meru-prastaara\", the Staircase of Mount Meru, giving the first surviving description of the arrangement of these numbers into a triangle. In approximately 850, the Jain mathematician Mahāvīra gave a different formula for the binomial coefficients, using multiplication, equivalent to the modern formula formula_5. In 1068, four columns of the first sixteen rows were given by the mathematician Bhattotpala, who was the first recorded mathematician to equate the additive and multiplicative formulas for these numbers.\n\nAt around the same time, the Persian mathematician Al-Karaji (953–1029) wrote a now lost book which contained the first description of Pascal's triangle. It was later repeated by the Persian poet-astronomer-mathematician Omar Khayyám (1048–1131); thus the triangle is also referred to as the Khayyam triangle in Iran. Several theorems related to the triangle were known, including the binomial theorem. Khayyam used a method of finding \"n\"th roots based on the binomial expansion, and therefore on the binomial coefficients.\n\nPascal's triangle was known in China in the early 11th century through the work of the Chinese mathematician Jia Xian (1010–1070). In the 13th century, Yang Hui (1238–1298) presented the triangle and hence it is still called Yang Hui's triangle () in China.\n\nIn the west, the binomial coefficients were calculated by Gersonides in the early 14th century, using the multiplicative formula for them.\nPetrus Apianus (1495–1552) published the full triangle on the frontispiece of his book on business calculations in 1527. This is the first record of the triangle in Europe. Michael Stifel published a portion of the triangle (from the second to the middle column in each row) in 1544, describing it as a table of figurate numbers. In Italy, Pascal's triangle is referred to as Tartaglia's triangle, named for the Italian algebraist Niccolò Fontana Tartaglia (1500–1577), who published six rows of the triangle in 1556. \nGerolamo Cardano, also, published the triangle as well as the additive and multiplicative rules for constructing it in 1570.\n\nPascal's \"Traité du triangle arithmétique\" (\"Treatise on Arithmetical Triangle\") was published posthumously in 1665. In this, Pascal collected several results then known about the triangle, and employed them to solve problems in probability theory. The triangle was later named after Pascal by Pierre Raymond de Montmort (1708) who called it \"Table de M. Pascal pour les combinaisons\" (French: Table of Mr. Pascal for combinations) and Abraham de Moivre (1730) who called it \"Triangulum Arithmeticum PASCALIANUM\" (Latin: Pascal's Arithmetic Triangle), which became the modern Western name.\n\nPascal's triangle determines the coefficients which arise in binomial expansions. For an example, consider the expansion\n\nNotice the coefficients are the numbers in row two of Pascal's triangle: 1, 2, 1.\nIn general, when a binomial like \"x\" + \"y\" is raised to a positive integer power we have:\n\nwhere the coefficients \"a\" in this expansion are precisely the numbers on row \"n\" of Pascal's triangle. In other words,\n\nThis is the binomial theorem.\n\nNotice that the entire right diagonal of Pascal's triangle corresponds to the coefficient of \"y\" in these binomial expansions, while the next diagonal corresponds to the coefficient of \"xy\" and so on.\n\nTo see how the binomial theorem relates to the simple construction of Pascal's triangle, consider the problem of calculating the coefficients of the expansion of (\"x\" + 1) in terms of the corresponding coefficients of (\"x\" + 1) (setting \"y\" = 1 for simplicity). Suppose then that\n\nNow\n\nThe two summations can be reorganized as follows:\n\n(because of how raising a polynomial to a power works, \"a\" = \"a\" = 1).\n\nWe now have an expression for the polynomial (\"x\" + 1) in terms of the coefficients of (\"x\" + 1) (these are the \"a\"s), which is what we need if we want to express a line in terms of the line above it. Recall that all the terms in a diagonal going from the upper-left to the lower-right correspond to the same power of \"x\", and that the a-terms are the coefficients of the polynomial (\"x\" + 1), and we are determining the coefficients of (\"x\" + 1). Now, for any given \"i\" not 0 or \"n\" + 1, the coefficient of the \"x\" term in the polynomial (\"x\" + 1) is equal to \"a\"(the figure above and to the left of the figure to be determined, since it is on the same diagonal) + \"a\" (the figure to the immediate right of the first figure). This is indeed the simple rule for constructing Pascal's triangle row-by-row.\n\nIt is not difficult to turn this argument into a proof (by mathematical induction) of the binomial theorem. Since\n(\"a\" + \"b\") = \"b\"(\"a\"/\"b\" +  1), the coefficients are identical in the expansion of the general case.\n\nAn interesting consequence of the binomial theorem is obtained by setting both variables \"x\" and \"y\" equal to one. In this case, we know that (1 + 1) = 2, and so\n\nIn other words, the sum of the entries in the \"n\"th row of Pascal's triangle is the \"n\"th power of 2.\n\nA second useful application of Pascal's triangle is in the calculation of combinations. For example, the number of combinations of \"n\" things taken \"k\" at a time (called \"n choose k\") can be found by the equation\n\nBut this is also the formula for a cell of Pascal's triangle. Rather than performing the calculation, one can simply look up the appropriate entry in the triangle. Provided we have the first row and the first entry in a row numbered 0, the answer will be located at entry \"k\" in row \"n\". For example, suppose a basketball team has 10 players and wants to know how many ways there are of selecting 8. The answer is entry 8 in row 10, which is 45; that is, 10 choose 8 is 45.\n\nWhen divided by 2, the \"n\"th row of Pascal's triangle becomes the binomial distribution in the symmetric case where \"p\" = 1/2. By the central limit theorem, this distribution approaches the normal distribution as \"n\" increases. This can also be seen by applying Stirling's formula to the factorials involved in the formula for combinations.\n\nThis is related to the operation of discrete convolution in two ways. First, polynomial multiplication exactly corresponds to discrete convolution, so that repeatedly convolving the sequence {..., 0, 0, 1, 1, 0, 0, ...} with itself corresponds to taking powers of 1 + \"x\", and hence to generating the rows of the triangle. Second, repeatedly convolving the distribution function for a random variable with itself corresponds to calculating the distribution function for a sum of \"n\" independent copies of that variable; this is exactly the situation to which the central limit theorem applies, and hence leads to the normal distribution in the limit.\n\nPascal's triangle has many properties and contains many patterns of numbers.\n\n\n\nThe diagonals of Pascal's triangle contain the figurate numbers of simplices:\n\nThe symmetry of the triangle implies that the \"n\" d-dimensional number is equal to the \"d\" \"n\"-dimensional number.\n\nAn alternative formula that does not involve recursion is as follows:\n\nThe geometric meaning of a function P is: P(1) = 1 for all \"d\". Construct a \"d\"-dimensional triangle (a 3-dimensional triangle is a tetrahedron) by placing additional dots below an initial dot, corresponding to P(1) = 1. Place these dots in a manner analogous to the placement of numbers in Pascal's triangle. To find P(\"x\"), have a total of \"x\" dots composing the target shape. P(\"x\") then equals the total number of dots in the shape. A 0-dimensional triangle is a point and a 1-dimensional triangle is simply a line, and therefore P(\"x\") = \"1\" and P(\"x\") = \"x\", which is the sequence of natural numbers. The number of dots in each layer corresponds to P(\"x\").\n\nThere are simple algorithms to compute all the elements in a row or diagonal without computing other elements or factorials.\n\nTo compute row formula_24 with the elements formula_25, formula_26, ..., formula_27, begin with formula_28. For each subsequent element, the value is determined by multiplying the previous value by a fraction with slowly changing numerator and denominator:\n\nFor example, to calculate row 5, the fractions are  formula_30,  formula_31,  formula_32,  formula_33 and formula_34, and hence the elements are  formula_35,   formula_36,   formula_37, etc. (The remaining elements are most easily obtained by symmetry.)\n\nTo compute the diagonal containing the elements formula_25, formula_39, formula_40, ..., we again begin with formula_28 and obtain subsequent elements by multiplication by certain fractions:\n\nFor example, to calculate the diagonal beginning at formula_43, the fractions are  formula_44,  formula_45,  formula_46, ..., and the elements are formula_35,   formula_48,   formula_49, etc. By symmetry, these elements are equal to formula_50, formula_51, formula_52, etc.\n\n\n\n\nDue to its simple construction by factorials, a very basic representation of Pascal's triangle in terms of the matrix exponential can be given: Pascal's triangle is the exponential of the matrix which has the sequence 1, 2, 3, 4, … on its subdiagonal and zero everywhere else.\n\nPascal's triangle can be used as a lookup table for the number of elements (such as edges and corners) within a polytope (such as a triangle, a tetrahedron, a square and a cube).\n\nLet's begin by considering the 3rd line of Pascal's triangle, with values 1, 3, 3, 1. A 2-dimensional triangle has one 2-dimensional element (itself), three 1-dimensional elements (lines, or edges), and three 0-dimensional elements (vertices, or corners). The meaning of the final number (1) is more difficult to explain (but see below). Continuing with our example, a tetrahedron has one 3-dimensional element (itself), four 2-dimensional elements (faces), six 1-dimensional elements (edges), and four 0-dimensional elements (vertices). Adding the final 1 again, these values correspond to the 4th row of the triangle (1, 4, 6, 4, 1). Line 1 corresponds to a point, and Line 2 corresponds to a line segment (dyad). This pattern continues to arbitrarily high-dimensioned hyper-tetrahedrons (known as simplices).\n\nTo understand why this pattern exists, one must first understand that the process of building an \"n\"-simplex from an (\"n\" − 1)-simplex consists of simply adding a new vertex to the latter, positioned such that this new vertex lies outside of the space of the original simplex, and connecting it to all original vertices. As an example, consider the case of building a tetrahedron from a triangle, the latter of whose elements are enumerated by row 3 of Pascal's triangle: 1 face, 3 edges, and 3 vertices (the meaning of the final 1 will be explained shortly). To build a tetrahedron from a triangle, we position a new vertex above the plane of the triangle and connect this vertex to all three vertices of the original triangle.\n\nThe number of a given dimensional element in the tetrahedron is now the sum of two numbers: first the number of that element found in the original triangle, plus the number of new elements, \"each of which is built upon elements of one fewer dimension from the original triangle\". Thus, in the tetrahedron, the number of cells (polyhedral elements) is 0 (the original triangle possesses none) + 1 (built upon the single face of the original triangle) = 1; the number of faces is 1 (the original triangle itself) + 3 (the new faces, each built upon an edge of the original triangle) = 4; the number of edges is 3 (from the original triangle) + 3 (the new edges, each built upon a vertex of the original triangle) = 6; the number of new vertices is 3 (from the original triangle) + 1 (the new vertex that was added to create the tetrahedron from the triangle) = 4. This process of summing the number of elements of a given dimension to those of one fewer dimension to arrive at the number of the former found in the next higher simplex is equivalent to the process of summing two adjacent numbers in a row of Pascal's triangle to yield the number below. Thus, the meaning of the final number (1) in a row of Pascal's triangle becomes understood as representing the new vertex that is to be added to the simplex represented by that row to yield the next higher simplex represented by the next row. This new vertex is joined to every element in the original simplex to yield a new element of one higher dimension in the new simplex, and this is the origin of the pattern found to be identical to that seen in Pascal's triangle. The \"extra\" 1 in a row can be thought of as the -1 simplex, the unique center of the simplex, which ever gives rise to a new vertex and a new dimension, yielding a new simplex with a new center.\n\nA similar pattern is observed relating to squares, as opposed to triangles. To find the pattern, one must construct an analog to Pascal's triangle, whose entries are the coefficients of (\"x\" + 2), instead of (\"x\" + 1). There are a couple ways to do this. The simpler is to begin with Row 0 = 1 and Row 1 = 1, 2. Proceed to construct the analog triangles according to the following rule:\n\nThat is, choose a pair of numbers according to the rules of Pascal's triangle, but double the one on the left before adding. This results in:\n\nThe other way of manufacturing this triangle is to start with Pascal's triangle and multiply each entry by 2, where k is the position in the row of the given number. For example, the 2nd value in row 4 of Pascal's triangle is 6 (the slope of 1s corresponds to the zeroth entry in each row). To get the value that resides in the corresponding position in the analog triangle, multiply 6 by 2 = 6 × 2 = 6 × 4 = 24. Now that the analog triangle has been constructed, the number of elements of any dimension that compose an arbitrarily dimensioned cube (called a hypercube) can be read from the table in a way analogous to Pascal's triangle. For example, the number of 2-dimensional elements in a 2-dimensional cube (a square) is one, the number of 1-dimensional elements (sides, or lines) is 4, and the number of 0-dimensional elements (points, or vertices) is 4. This matches the 2nd row of the table (1, 4, 4). A cube has 1 cube, 6 faces, 12 edges, and 8 vertices, which corresponds to the next line of the analog triangle (1, 6, 12, 8). This pattern continues indefinitely.\n\nTo understand why this pattern exists, first recognize that the construction of an \"n\"-cube from an (\"n\" − 1)-cube is done by simply duplicating the original figure and displacing it some distance (for a regular \"n\"-cube, the edge length) orthogonal to the space of the original figure, then connecting each vertex of the new figure to its corresponding vertex of the original. This initial duplication process is the reason why, to enumerate the dimensional elements of an \"n\"-cube, one must double the first of a pair of numbers in a row of this analog of Pascal's triangle before summing to yield the number below. The initial doubling thus yields the number of \"original\" elements to be found in the next higher \"n\"-cube and, as before, new elements are built upon those of one fewer dimension (edges upon vertices, faces upon edges, etc.). Again, the last number of a row represents the number of new vertices to be added to generate the next higher \"n\"-cube.\n\nIn this triangle, the sum of the elements of row \"m\" is equal to 3. Again, to use the elements of row 4 as an example: formula_54, which is equal to formula_55.\n\nEach row of Pascal's triangle gives the number of vertices at each distance from a fixed vertex in an \"n\"-dimensional cube. For example, in three dimensions, the third row (1 3 3 1) corresponds to the usual three-dimensional cube: fixing a vertex \"V\", there is one vertex at distance 0 from \"V\" (that is, \"V\" itself), three vertices at distance 1, three vertices at distance and one vertex at distance (the vertex opposite \"V\"). The second row corresponds to a square, while larger-numbered rows correspond to hypercubes in each dimension.\n\nAs stated previously, the coefficients of (\"x\" + 1) are the nth row of the triangle. Now the coefficients of (\"x\" − 1) are the same, except that the sign alternates from +1 to −1 and back again. After suitable normalization, the same pattern of numbers occurs in the Fourier transform of sin(\"x\")/\"x\". More precisely: if \"n\" is even, take the real part of the transform, and if \"n\" is odd, take the imaginary part. Then the result is a step function, whose values (suitably normalized) are given by the \"n\"th row of the triangle with alternating signs. For example, the values of the step function that results from:\n\ncompose the 4th row of the triangle, with alternating signs. This is a generalization of the following basic result (often used in electrical engineering):\n\nis the boxcar function. The corresponding row of the triangle is row 0, which consists of just the number 1.\n\nIf n is congruent to 2 or to 3 mod 4, then the signs start with −1. In fact, the sequence of the (normalized) first terms corresponds to the powers of i, which cycle around the intersection of the axes with the unit circle in the complex plane:\n\nThe pattern produced by an elementary cellular automaton using rule 60 is exactly Pascal's triangle of binomial coefficients reduced modulo 2 (black cells correspond to odd binomial coefficients). Rule 102 also produces this pattern when trailing zeros are omitted. Rule 90 produces the same pattern but with an empty cell separating each entry in the rows.\n\nPascal's triangle can be extended to negative row numbers.\n\nFirst write the triangle in the following form:\n\nNext, extend the column of 1s upwards:\n\nNow the rule:\n\ncan be rearranged to:\n\nwhich allows calculation of the other entries for negative rows:\nThis extension preserves the property that the values in the \"m\"th column viewed as a function of \"n\" are fit by an order \"m\" polynomial, namely\n\nThis extension also preserves the property that the values in the \"n\"th row correspond to the coefficients of (1 + \"x\"):\nFor example:\n\nWhen viewed as a series, the rows of negative \"n\" diverge. However, they are still Abel summable, which summation gives the standard values of 2. (In fact, the \"n\" = -1 row results in Grandi's series which \"sums\" to 1/2, and the \"n\" = -2 row results in another well-known series which has an Abel sum of 1/4.)\n\nAnother option for extending Pascal's triangle to negative rows comes from extending the \"other\" line of 1s:\n\nApplying the same rule as before leads to\nNote that this extension also has the properties that just as\n\nwe have\n\nAlso, just as summing along the lower-left to upper-right diagonals of the Pascal matrix yields the Fibonacci numbers, this second type of extension still sums to the Fibonacci numbers for negative index.\n\nEither of these extensions can be reached if we define\n\nand take certain limits of the gamma function, formula_67.\n\n\n"}
{"id": "50316748", "url": "https://en.wikipedia.org/wiki?curid=50316748", "title": "Plug-in principle", "text": "Plug-in principle\n\nIn statistics, the plug-in principle is the method of estimation of functionals of a population distribution by evaluating the same functionals at the empirical distribution based on a sample.\n\nFor example, when estimating the population mean, this method uses the sample mean; to estimate the population median, it uses the sample median; to estimate the population regression line, it uses the sample regression line.\n\nIt is called a principle because it is too simple to be otherwise, it is just a guideline, not a theorem.\n\n\n\n"}
{"id": "20842828", "url": "https://en.wikipedia.org/wiki?curid=20842828", "title": "Poncelet Prize", "text": "Poncelet Prize\n\nThe Poncelet Prize () is awarded by the French Academy of Sciences. The prize was established in 1868 by the widow of General Jean-Victor Poncelet for the advancement of the sciences. It was in the amount of 2,000 francs (as of 1868), mostly for the work in applied mathematics. The precise wording of the announcement by the Academy varied from year to year and required the work be \"in mechanics\", or \"for work contributing to the progress of pure or applied mathematics\", or simply \"in applied mathematics\", and sometimes included condition that the work must be \"done during the ten years preceding the award.\"\n\n\n\n"}
{"id": "35793286", "url": "https://en.wikipedia.org/wiki?curid=35793286", "title": "Power diagram", "text": "Power diagram\n\nIn computational geometry, a power diagram, also called a Laguerre–Voronoi diagram, Dirichlet cell complex, radical Voronoi tesselation or a sectional Dirichlet tesselation, is a partition of the Euclidean plane into polygonal cells defined from a set of circles. The cell for a given circle \"C\" consists of all the points for which the power distance to \"C\" is smaller than the power distance to the other circles. The power diagram is a form of generalized Voronoi diagram, and coincides with the Voronoi diagram of the circle centers in the case that all the circles have equal radii.\n\nIf \"C\" is a circle and \"P\" is a point outside \"C\", then the power of \"P\" with respect to \"C\" is the square of the length of a line segment from \"P\" to a point \"T\" of tangency with \"C\". Equivalently, if \"P\" has distance \"d\" from the center of the circle, and the circle has radius \"r\", then (by the Pythagorean theorem) the power is \"d\" − \"r\". The same formula \"d\" − \"r\" may be extended to all points in the plane, regardless of whether they are inside or outside of \"C\": points on \"C\" have zero power, and points inside \"C\" have negative power.\n\nThe power diagram of a set of \"n\" circles \"C\" is a partition of the plane into \"n\" regions \"R\" (called cells), such that a point \"P\" belongs to \"R\" whenever circle \"C\" is the circle minimizing the power of \"P\".\nIn the case \"n\" = 2, the power diagram consists of two halfplanes, separated by a line called the radical axis or chordale of the two circles. Along the radical axis, both circles have equal power. More generally, in any power diagram, each cell \"R\" is a convex polygon, the intersection of the halfspaces bounded by the radical axes of circle \"C\" with each other circle. Triples of cells meet at vertices of the diagram, which are the radical centers of the three circles whose cells meet at the vertex.\n\nThe power diagram may be seen as a weighted form of the Voronoi diagram of a set of point sites, a partition of the plane into cells within which one of the sites is closer than all the other sites. Other forms of weighted Voronoi diagram include the additively weighted Voronoi diagram, in which each site has a weight that is added to its distance before comparing it to the distances to the other sites, and the multiplicatively weighted Voronoi diagram, in which the weight of a site is multiplied by its distance before comparing it to the distances to the other sites. In contrast, in the power diagram, we may view each circle center as a site, and each circle's squared radius as a weight that is subtracted from the squared distance before comparing it to other squared distances. In the case that all the circle radii are equal, this subtraction makes no difference to the comparison, and the power diagram coincides with the Voronoi diagram.\n\nA planar power diagram may also be interpreted as a planar cross-section of an unweighted three-dimensional Voronoi diagram. In this interpretation, the set of circle centers in the cross-section plane are the perpendicular projections of the three-dimensional Voronoi sites, and the squared radius of each circle is a constant \"K\" minus the squared distance of the corresponding site from the cross-section plane, where \"K\" is chosen large enough to make all these radii positive.\n\nLike the Voronoi diagram, the power diagram may be generalized to Euclidean spaces of any dimension. The power diagram of \"n\" spheres in \"d\" dimensions is combinatorially equivalent to the intersection of a set of \"n\" upward-facing halfspaces in \"d\" + 1 dimensions, and vice versa.\n\nTwo-dimensional power diagrams may be constructed by an algorithm that runs in time O(\"n\" log \"n\"). More generally, because of the equivalence with higher-dimensional halfspace intersections, \"d\"-dimensional power diagrams (for \"d\" > 2) may be constructed by an algorithm that runs in time formula_1.\n\nThe power diagram may be used as part of an efficient algorithm for computing the volume of a union of spheres. Intersecting each sphere with its power diagram cell gives its contribution to the total union, from which the volume may be computed in time proportional to the complexity of the power diagram.\n\nOther applications of power diagrams include data structures for testing whether a point belongs to a union of disks, algorithms for constructing the boundary of a union of disks, and algorithms for finding the closest two balls in a set of balls.\n\n traces the definition of the power distance to the work of 19th-century mathematicians Edmond Laguerre and Georgy Voronoy. defined power diagrams and used them to show that the boundary of a union of \"n\" circular disks can always be illuminated from at most 2\"n\" point light sources. Power diagrams have appeared in the literature under other names including the \"Laguerre–Voronoi diagram\", \"Dirichlet cell complex\", \"radical Voronoi tesselation\" and \"sectional Dirichlet tesselation\".\n"}
{"id": "15565331", "url": "https://en.wikipedia.org/wiki?curid=15565331", "title": "Projected tolerance zone", "text": "Projected tolerance zone\n\nIn geometric dimensioning and tolerancing, a projected tolerance zone is defined to predict the final dimensions and locations of features on a component or assembly subject to tolerance stack-up.\n\n"}
{"id": "698759", "url": "https://en.wikipedia.org/wiki?curid=698759", "title": "Propagator", "text": "Propagator\n\nIn quantum mechanics and quantum field theory, the propagator is a function that specifies the probability amplitude for a particle to travel from one place to another in a given time, or to travel with a certain energy and momentum. In Feynman diagrams, which serve to calculate the rate of collisions in quantum field theory, virtual particles contribute their propagator to the rate of the scattering event described by the respective diagram. These may also be viewed as the inverse of the wave operator appropriate to the particle, and are, therefore, often called \"(causal) Green's functions\" (called \"causal\" to distinguish it from the elliptic Laplacian Green's function).\n\nIn non-relativistic quantum mechanics, the propagator gives the probability amplitude for a particle to travel from one spatial point at one time to another spatial point at a later time. It is the Green's function (fundamental solution) for the Schrödinger equation. This means that, if a system has Hamiltonian , then the appropriate propagator is a function\nsatisfying\nwhere denotes the Hamiltonian written in terms of the coordinates, denotes the Dirac delta-function, is the Heaviside step function and is the kernel of the above Schrödinger differential operator in the big parenthesis, often referred to as the propagator instead of in this context, and henceforth in this article. (cf. Duhamel's principle.)\n\nThis propagator may also be written as the transition amplitude\nwhere is the unitary time-evolution operator for the system taking states at time to states at time . Note that \nformula_4.\n\nThe quantum mechanical propagator may also be found by using a path integral,\nwhere the boundary conditions of the path integral include . Here denotes the Lagrangian of the system. The paths that are summed over move only forwards in time, and are integrated with the differential formula_6 which follows the path in time.\n\nIn non-relativistic quantum mechanics, the propagator lets you find the wave function of a system given an initial wave function and a time interval. The new wave function is given by the equation\n\nIf only depends on the difference , this is a convolution of the initial wave function and the propagator. This kernel is the kernel of integral transform.\n\nFor a time-translationally invariant system, the propagator only depends on the time difference , so it may be rewritten as\n\nThe propagator of a one-dimensional free particle, with the far-right expression obtained via saddle-point methods, is then\n=\\left(\\frac{m}{2\\pi i\\hbar t}\\right)^{\\frac{1}{2}}e^{-\\frac{m(x-x')^2}{2i\\hbar t}}.</math>\n\nSimilarly, the propagator of a one-dimensional quantum harmonic oscillator is the Mehler kernel, \n\\exp\\left(-\\frac{m\\omega((x^2+x'^2)\\cos\\omega t-2xx')}{2i\\hbar \\sin\\omega t}\\right) ~.</math>\nThe latter may be obtained from the previous free particle result upon making use of van Kortryk's SU(2) Lie-group identity,\n\nvalid for operators formula_11 and formula_12 satisfying the Heisenberg relation formula_13.\n\nFor the -dimensional case, the propagator can be simply obtained by the product\n\nIn relativistic quantum mechanics and quantum field theory the propagators are Lorentz invariant. They give the amplitude for a particle to travel between two spacetime points.\n\nIn quantum field theory, the theory of a free (non-interacting) scalar field is a useful and simple example which serves to illustrate the concepts needed for more complicated theories. It describes spin zero particles. There are a number of possible propagators for free scalar field theory. We now describe the most common ones.\n\nThe position space propagators are Green's functions for the Klein–Gordon equation. This means they are functions which satisfy\n\nwhere:\n\nWe shall restrict attention to 4-dimensional Minkowski spacetime. We can perform a Fourier transform of the equation for the propagator, obtaining\n\nThis equation can be inverted in the sense of distributions noting that the equation has the solution, (see Sokhotski-Plemelj theorem)\nwith implying the limit to zero. Below, we discuss the right choice of the sign arising from causality requirements.\n\nThe solution is\nwhere \nis the 4-vector inner product.\n\nThe different choices for how to deform the integration contour in the above expression lead to various forms for the propagator. The choice of contour is usually phrased in terms of the formula_20 integral.\n\nThe integrand then has two poles at \nso different choices of how to avoid these lead to different propagators.\n\nA contour going clockwise over both poles gives the causal retarded propagator. This is zero if is spacelike or if (i.e. if is to the future of ).\n\nThis choice of contour is equivalent to calculating the limit,\n\nHere \nis the Heaviside step function and\nis the proper time from to and formula_25 is a Bessel function of the first kind. The expression formula_26 means causally precedes which, for Minkowski spacetime, means\n\nThis expression can be related to the vacuum expectation value of the commutator of the free scalar field operator,\nwhere \nis the commutator.\n\nA contour going anti-clockwise under both poles gives the causal advanced propagator. This is zero if is spacelike or if (i.e. if is to the past of ).\n\nThis choice of contour is equivalent to calculating the limit\n\nThis expression can also be expressed in terms of the vacuum expectation value of the commutator of the free scalar field.\nIn this case,\n\nA contour going under the left pole and over the right pole gives the Feynman propagator.\n\nThis choice of contour is equivalent to calculating the limit \n\nHere\n\nwhere and are two points in Minkowski spacetime, and the dot in the exponent is a four-vector inner product. is a and is a .\n\nThis expression can be derived directly from the field theory as the vacuum expectation value of the \"time-ordered product\" of the free scalar field, that is, the product always taken such that the time ordering of the spacetime points is the same,\n\nThis expression is Lorentz invariant, as long as the field operators commute with one another when the points and are separated by a spacelike interval.\n\nThe usual derivation is to insert a complete set of single-particle momentum states between the fields with Lorentz covariant normalization, and to then show that the functions providing the causal time ordering may be obtained by a contour integral along the energy axis, if the integrand is as above (hence the infinitesimal imaginary part), to move the pole off the real line.\n\nThe propagator may also be derived using the path integral formulation of quantum theory.\n\nThe Fourier transform of the position space propagators can be thought of as propagators in momentum space. These take a much simpler form than the position space propagators.\n\nThey are often written with an explicit term although this is understood to be a reminder about which integration contour is appropriate (see above). This term is included to incorporate boundary conditions and causality (see below).\n\nFor a 4-momentum the causal and Feynman propagators in momentum space are:\n\nFor purposes of Feynman diagram calculations, it is usually convenient to write these with an additional overall factor of (conventions vary).\n\nThe Feynman propagator has some properties that seem baffling at first. In particular, unlike the commutator, the propagator is \"nonzero\" outside of the light cone, though it falls off rapidly for spacelike intervals. Interpreted as an amplitude for particle motion, this translates to the virtual particle travelling faster than light. It is not immediately obvious how this can be reconciled with causality: can we use faster-than-light virtual particles to send faster-than-light messages?\n\nThe answer is no: while in classical mechanics the intervals along which particles and causal effects can travel are the same, this is no longer true in quantum field theory, where it is commutators that determine which operators can affect one another.\n\nSo what \"does\" the spacelike part of the propagator represent? In QFT the vacuum is an active participant, and particle numbers and field values are related by an uncertainty principle; field values are uncertain even for particle number \"zero\". There is a nonzero probability amplitude to find a significant fluctuation in the vacuum value of the field if one measures it locally (or, to be more precise, if one measures an operator obtained by averaging the field over a small region). Furthermore, the dynamics of the fields tend to favor spatially correlated fluctuations to some extent. The nonzero time-ordered product for spacelike-separated fields then just measures the amplitude for a nonlocal correlation in these vacuum fluctuations, analogous to an EPR correlation. Indeed, the propagator is often called a \"two-point correlation function\" for the free field.\n\nSince, by the postulates of quantum field theory, all observable operators commute with each other at spacelike separation, messages can no more be sent through these correlations than they can through any other EPR correlations; the correlations are in random variables.\n\nRegarding virtual particles, the propagator at spacelike separation can be thought of as a means of calculating the amplitude for creating a virtual particle-antiparticle pair that eventually disappears into the vacuum, or for detecting a virtual pair emerging from the vacuum. In Feynman's language, such creation and annihilation processes are equivalent to a virtual particle wandering backward and forward through time, which can take it outside of the light cone. However, no signaling back in time is allowed.\n\nThis can be made clearer by writing the propagator in the following form for a massless photon,\n\nThis is the usual definition but normalised by a factor of formula_40. Then the rule is that one only takes the limit formula_41 at the end of a calculation.\n\nOne sees that \nand\nHence this means a single photon will always stay on the light cone. It is also shown that the total probability for a photon at any time must be normalised by the reciprocal of the following factor:\nWe see that the parts outside the light cone usually are zero in the limit and only are important in Feynman diagrams.\n\nThe most common use of the propagator is in calculating probability amplitudes for particle interactions using Feynman diagrams. These calculations are usually carried out in momentum space. In general, the amplitude gets a factor of the propagator for every \"internal line\", that is, every line that does not represent an incoming or outgoing particle in the initial or final state. It will also get a factor proportional to, and similar in form to, an interaction term in the theory's Lagrangian for every internal vertex where lines meet. These prescriptions are known as \"Feynman rules\".\n\nInternal lines correspond to virtual particles. Since the propagator does not vanish for combinations of energy and momentum disallowed by the classical equations of motion, we say that the virtual particles are allowed to be off shell. In fact, since the propagator is obtained by inverting the wave equation, in general, it will have singularities on the shell.\n\nThe energy carried by the particle in the propagator can even be \"negative\". This can be interpreted simply as the case in which, instead of a particle going one way, its antiparticle is going the \"other\" way, and therefore carrying an opposing flow of positive energy. The propagator encompasses both possibilities. It does mean that one has to be careful about minus signs for the case of fermions, whose propagators are not even functions in the energy and momentum (see below).\n\nVirtual particles conserve energy and momentum. However, since they can be off the shell, wherever the diagram contains a closed \"loop\", the energies and momenta of the virtual particles participating in the loop will be partly unconstrained, since a change in a quantity for one particle in the loop can be balanced by an equal and opposite change in another. Therefore, every loop in a Feynman diagram requires an integral over a continuum of possible energies and momenta. In general, these integrals of products of propagators can diverge, a situation that must be handled by the process of renormalization.\n\nIf the particle possesses spin then its propagator is in general somewhat more complicated, as it will involve the particle's spin or polarization indices. The differential equation satisfied by the propagator for a spin particle is given by\n\nwhere is the unit matrix in four dimensions, and employing the Feynman slash notation. This is the Dirac equation for a delta function source in spacetime. Using the momentum representation,\n\nthe equation becomes\n\nwhere on the right-hand side an integral representation of the four-dimensional delta function is used. Thus\n\nBy multiplying from the left with\n\n(dropping unit matrices from the notation) and using properties of the gamma matrices,\n\nthe momentum-space propagator used in Feynman diagrams for a Dirac field representing the electron in quantum electrodynamics is found to have form\n\nThe downstairs is a prescription for how to handle the poles in the complex -plane. It automatically yields the Feynman contour of integration by shifting the poles appropriately. It is sometimes written\n\nfor short. It should be remembered that this expression is just shorthand notation for . \"One over matrix\" is otherwise nonsensical. In position space one has\n\nThis is related to the Feynman propagator by\n\nwhere formula_57.\n\nThe propagator for a gauge boson in a gauge theory depends on the choice of convention to fix the gauge. For the gauge used by Feynman and Stueckelberg, the propagator for a photon is\n\nThe propagator for a massive vector field can be derived from the Stueckelberg Lagrangian. The general form with gauge parameter reads\n\nWith this general form one obtains the propagator in unitary gauge for , the propagator in Feynman or 't Hooft gauge for and in Landau or Lorenz gauge for . There are also other notations where the gauge parameter is the inverse of . The name of the propagator, however, refers to its final form and not necessarily to the value of the gauge parameter.\n\nUnitary gauge:\n\nFeynman ('t Hooft) gauge:\n\nLandau (Lorenz) gauge:\n\nThe graviton propagator for Minkowski space in general relativity is \nwhere formula_64 is the transverse and traceless spin-2 projection operator and formula_65 is a spin-0 scalar multiplet. \nThe graviton propagator for (Anti) de Sitter space is \nwhere formula_67 is the Hubble constant. Note that upon taking the limit formula_68, the AdS propagator reduces to the Minkowski propagator.\n\nThe scalar propagators are Green's functions for the Klein–Gordon equation. There are related singular functions which are important in quantum field theory. We follow the notation in Bjorken and Drell. See also Bogolyubov and Shirkov (Appendix A). These functions are most simply defined in terms of the vacuum expectation value of products of field operators.\n\nThe commutator of two scalar field operators defines the Pauli–Jordan function formula_69 by\n\nwith\n\nThis satisfies \nand is zero if formula_73.\n\nWe can define the positive and negative frequency parts of formula_69, sometimes called cut propagators, in a relativistically invariant way.\n\nThis allows us to define the positive frequency part:\n\nand the negative frequency part:\n\nThese satisfy \n\nand\n\nThe anti-commutator of two scalar field operators defines formula_79 function by\nwith\n\nThis satisfies formula_82\n\nThe retarded, advanced and Feynman propagators defined above are all Green's functions for the Klein–Gordon equation.\n\nThey are related to the singular functions by\nwhere \n\n\n"}
{"id": "18421631", "url": "https://en.wikipedia.org/wiki?curid=18421631", "title": "Radius of curvature", "text": "Radius of curvature\n\nIn differential geometry, the radius of curvature, , is the reciprocal of the curvature. For a curve, it equals the radius of the circular arc which best approximates the curve at that point. For surfaces, the radius of curvature is the radius of a circle that best fits a normal section or combinations thereof.\n\nIn the case of a space curve, the radius of curvature is the length of the curvature vector.\n\nIn the case of a plane curve, then is the absolute value of\n\nwhere is the arc length from a fixed point on the curve, is the tangential angle and is the curvature.\n\nIf the curve is given in Cartesian coordinates as , then the radius of curvature is (assuming the curve is differentiable up to order 2):\n\nand denotes the absolute value of .\n\nIf the curve is given parametrically by functions and , then the radius of curvature is\n\nHeuristically, this result can be interpreted as\n\nIf is a parametrized curve in then the radius of curvature at each point of the curve, , is given by\n\nAs a special case, if is a function from to , then the radius of curvature of its graph, , is\n\nLet be as above, and fix . We want to find the radius of a parametrized circle which matches in its zeroth, first, and second derivatives at . Clearly the radius will not depend on the position , only on the velocity and acceleration . There are only three independent scalars that can be obtained from two vectors and , namely , , and . Thus the radius of curvature must be a function of the three scalars , and .\n\nThe general equation for a parametrized circle in is\nwhere is the center of the circle (irrelevant since it disappears in the derivatives), are perpendicular vectors of length (that is, and ), and is an arbitrary function which is twice differentiable at .\n\nThe relevant derivatives of work out to be\n\nIf we now equate these derivatives of to the corresponding derivatives of at we obtain\n\nThese three equations in three unknowns (, and ) can be solved for , giving the formula for the radius of curvature:\n\nor, omitting the parameter for readability,\n\nFor a semi-circle of radius in the upper half-plane\n\nFor a semi-circle of radius in the lower half-plane\n\nThe circle of radius has a radius of curvature equal to .\n\nIn an ellipse with major axis and minor axis , the vertices on the major axis have the smallest radius of curvature of any points, ; and the vertices on the minor axis have the largest radius of curvature of any points, .\n\n\nStress in the semiconductor structure involving evaporated thin films usually results from the thermal expansion (thermal stress) during the manufacturing process. Thermal stress occurs because film depositions are usually made above room temperature. Upon cooling from the deposition temperature to room temperature, the difference in the thermal expansion coefficients of the substrate and the film cause thermal stress.\n\nIntrinsic stress results from the microstructure created in the film as atoms are deposited on the substrate. Tensile stress results from microvoids in the thin film, because of the attractive interaction of atoms across the voids.\n\nThe stress in thin film semiconductor structures results in the buckling of the wafers. The radius of the curvature of the stressed structure is related to stress tensor in the structure, and can be described by modified Stoney formula. The topography of the stressed structure including radii of curvature can be measured using optical scanner methods. The modern scanner tools have capability to measure full topography of the substrate and to measure both principal radii of curvature, while providing the accuracy of the order of 0.1% for radii of curvature of 90 meters and more.\n\n"}
{"id": "23290990", "url": "https://en.wikipedia.org/wiki?curid=23290990", "title": "Recursive language", "text": "Recursive language\n\nIn mathematics, logic and computer science, a formal language (a set of finite sequences of symbols taken from a fixed alphabet) is called recursive if it is a recursive subset of the set of all possible finite sequences over the alphabet of the language. Equivalently, a formal language is recursive if there exists a total Turing machine (a Turing machine that halts for every given input) that, when given a finite sequence of symbols as input, accepts it if it belongs to the language and rejects it otherwise. Recursive languages are also called decidable.\n\nThe concept of decidability may be extended to other models of computation. For example one may speak of languages decidable on a non-deterministic Turing machine. Therefore, whenever an ambiguity is possible, the synonym for \"recursive language\" used is Turing-decidable language, rather than simply \"decidable\".\n\nThe class of all recursive languages is often called R, although this name is also used for the class RP.\n\nThis type of language was not defined in the Chomsky hierarchy of . All recursive languages are also recursively enumerable. All regular, context-free and context-sensitive languages are recursive.\n\nThere are two equivalent major definitions for the concept of a recursive language:\n\n\nBy the second definition, any decision problem can be shown to be decidable by exhibiting an algorithm for it that terminates on all inputs. An undecidable problem is a problem that is not decidable.\n\nAs noted above, every context-sensitive language is recursive. Thus, a simple example of a recursive language is the set \"L={abc, , , ...}\";\nmore formally, the set\nis context-sensitive and therefore recursive.\n\nExamples of decidable languages that are not context-sensitive are more difficult to describe. For one such example, some familiarity with mathematical logic is required: Presburger arithmetic is the first-order theory of the natural numbers with addition (but without multiplication). While the set of well-formed formulas in Presburger arithmetic is context-free, every deterministic Turing machine accepting the set of true statements in Presburger arithmetic has a worst-case runtime of at least formula_2, for some constant \"c\">0 . Here, \"n\" denotes the length of the given formula. Since every context-sensitive language can be accepted by a linear bounded automaton, and such an automaton can be simulated by a deterministic Turing machine with worst-case running time at most formula_3 for some constant \"c\" , the set of valid formulas in Presburger arithmetic is not context-sensitive. On positive side, it is known that there is a deterministic Turing machine running in time at most triply exponential in \"n\" that decides the set of true formulas in Presburger arithmetic . Thus, this is an example of a language that is decidable but not context-sensitive.\n\nRecursive languages are closed under the following operations. That is, if \"L\" and \"P\" are two recursive languages, then the following languages are recursive as well:\n\nThe last property follows from the fact that the set difference can be expressed in terms of intersection and complement.\n\n\n"}
{"id": "56509649", "url": "https://en.wikipedia.org/wiki?curid=56509649", "title": "Refinement (category theory)", "text": "Refinement (category theory)\n\nIn category theory and related fields of mathematics, a refinement is a construction that generalizes the operations of \"interior enrichment\", like bornologification or saturation of a locally convex space. A dual construction is called envelope. \n\nSuppose formula_1 is a category, formula_2 an object in formula_1, and formula_4 and formula_5 two classes of morphisms in formula_1. The definition of a refinement of formula_2 in the class formula_4 by means of the class formula_5 consists of two steps.\nNotations:\n\nIn a special case when formula_4 is a class of all morphisms whose ranges belong to a given class of objects formula_41 in formula_1 it is convenient to replace formula_4 with formula_41 in the notations (and in the terms):\n\nSimilarly, if formula_5 is a class of all morphisms whose ranges belong to a given class of objects formula_47 in formula_1 it is convenient to replace formula_5 with formula_47 in the notations (and in the terms):\n\nFor example, one can speak about a \"refinement of formula_2 in the class of objects formula_41 by means of the class of objects formula_47\":\n\n\n\n"}
{"id": "8304736", "url": "https://en.wikipedia.org/wiki?curid=8304736", "title": "RiskMetrics", "text": "RiskMetrics\n\nThe RiskMetrics variance model (also known as exponential smoother) was first established in 1989, when Sir Dennis Weatherstone, the new chairman of J.P. Morgan, asked for a daily report measuring and explaining the risks of his firm. Nearly four years later in 1992, J.P. Morgan launched the RiskMetrics methodology to the marketplace, making the substantive research and analysis that satisfied Sir Dennis Weatherstone's request freely available to all market participants.\n\nIn 1998, as client demand for the group's risk management expertise exceeded the firm's internal risk management resources, the Corporate Risk Management Department was spun off from J.P. Morgan as RiskMetrics Group with 23 founding employees. The RiskMetrics technical document was revised in 1996. In 2001, it was revised again in \"Return to RiskMetrics\". In 2006, a new method for modeling risk factor returns was introduced (RM2006). On 25 January 2008, RiskMetrics Group listed on the New York Stock Exchange (NYSE: RISK). In June 2010, RiskMetrics was acquired by MSCI for $1.55 billion.\n\nPortfolio risk measurement can be broken down into steps. The first is modeling the market that drives changes in the portfolio's value. The market model must be sufficiently specified so that the portfolio can be revalued using information from the market model. The risk measurements are then extracted from the probability distribution of the changes in portfolio value. The change in value of the portfolio is typically referred to by portfolio managers as profit and loss, or P&L\n\nRisk management systems are based on models that describe potential changes in the factors affecting portfolio value. These risk factors are the building blocks for all pricing functions. In general, the factors driving the prices of financial securities are equity prices, foreign exchange rates, commodity prices, interest rates, correlation and volatility. By generating future scenarios for each risk factor, we can infer changes in portfolio value and reprice the portfolio for different \"states of the world\".\n\nThe first widely used portfolio risk measure was the standard deviation of portfolio value, as described by Harry Markowitz. While comparatively easy to calculate, standard deviation is not an ideal risk measure since it penalizes profits as well as losses.\n\nThe 1994 tech doc popularized VaR as the risk measure of choice among investment banks looking to be able to measure their portfolio risk for the benefit of banking regulators. VaR is a downside risk measure, meaning that it typically focuses on losses.\n\nA third commonly used risk measure is expected shortfall, also known variously as expected tail loss, XLoss, conditional VaR, or CVaR.\n\nThe Marginal VaR of a position with respect to a portfolio can be thought of as the amount of risk that the position is adding to the portfolio. It can be formally defined as the difference between the VaR of the total portfolio and the VaR of the portfolio without the position.\n\nIncremental risk statistics provide information regarding the sensitivity of portfolio risk to changes in the position holding sizes in the portfolio.\n\nAn important property of incremental risk is subadditivity. That is, the sum of the incremental risks of the positions in a portfolio equals the total risk of the portfolio. This property has important applications in the allocation of risk to different units, where the goal is to keep the sum of the risks equal to the total risk.\n\nSince there are three risk measures covered by RiskMetrics, there are three incremental risk measures: Incremental VaR (IVaR), Incremental Expected Shortfall (IES), and Incremental Standard Deviation (ISD).\n\nIncremental statistics also have applications to portfolio optimization. A portfolio with minimum risk will have incremental risk equal to zero for all positions. Conversely, if the incremental risk is zero for all positions, the portfolio is guaranteed to have minimum risk only if the risk measure is subadditive.\n\nA coherent risk measure satisfies the following four properties:\n\n1. Subadditivity\n\nA risk measure is subadditive if for any portfolios A and B, the risk of A+B is never greater than the risk of A plus the risk of B. In other words, the risk of the sum of subportfolios is smaller than or equal to the sum of their individual risks.\n\nStandard deviation and expected shortfall are subadditive, while VaR is not.\n\nSubadditivity is required in connection with aggregation of risks across desks, business units, accounts, or subsidiary companies. This property is important when different business units calculate their risks independently and we want to get an idea of the total risk involved. Subadditivity could also be a matter of concern for regulators, where firms might be motivated to break up into affiliates to satisfy capital requirements.\n\n2. Translation invariance\n\nAdding cash to the portfolio decreases its risk by the same amount.\n\n3. Positive homogeneity of degree 1\n\nIf we double the size of every position in a portfolio, the risk of the portfolio will be twice as large.\n\n4. Monotonicity\n\nIf losses in portfolio A are larger than losses in portfolio B for all possible risk factor return scenarios, then the risk of portfolio A is higher than the risk of portfolio B.\n\nThe estimation process of any risk measure can be wrong by a considerable margin. If from the imprecise estimate we cannot get a good understanding what the true value could be, then the estimate is virtually worthless. A good risk measurement is to supplement any estimated risk measure with some indicator of their precision, or, of the size of its error.\n\nThere are various ways to quantify the error of some estimates. One approach is to estimate a confidence interval of the risk measurement.\n\nRiskMetrics describes three models for modeling the risk factors that define financial markets.\n\nThe first is very similar to the mean-covariance approach of Markowitz. Markowitz assumed that asset covariance matrix formula_1 can be observed. The covariance matrix can be used to compute portfolio variance. RiskMetrics assumes that the market is driven by risk factors with observable covariance. The risk factors are represented by time series of prices or levels of stocks, currencies, commodities, and interest rates. Instruments are evaluated from these risk factors via various pricing models. The portfolio itself is assumed to be some linear combination of these instruments.\n\nThe second market model assumes that the market only has finitely many possible changes, drawn from a risk factor return sample of a defined historical period. Typically one performs a historical simulation by sampling from past day-on-day risk factor changes, and applying them to the current level of the risk factors to obtain risk factor price scenarios. These perturbed risk factor price scenarios are used to generate a profit (loss) distribution for the portfolio.\n\nThis method has the advantage of simplicity, but as a model, it is slow to adapt to changing market conditions. It also suffers from simulation error, as the number of simulations is limited by the historical period (typically between 250 and 500 business days).\n\nThe third market model assumes that the logarithm of the return, or, log-return, of any risk factor typically follows a normal distribution. Collectively, the log-returns of the risk factors are multivariate normal. Monte Carlo algorithm simulation generates random market scenarios drawn from that multivariate normal distribution. For each scenario, the profit (loss) of the portfolio is computed. This collection of profit (loss) scenarios provides a sampling of the profit (loss) distribution from which one can compute the risk measures of choice.\n\nNassim Taleb in his book \"The Black Swan\" (2007) wrote:\nBanks are now more vulnerable to the Black Swan than ever before with “scientists” among their staff taking care of exposures. The giant firm J. P. Morgan put the entire world at risk by introducing in the nineties RiskMetrics, a phony method aiming at managing people’s risks. A related method called “Value-at-Risk,” which relies on the quantitative measurement of risk, has been spreading.\n\n"}
{"id": "22946501", "url": "https://en.wikipedia.org/wiki?curid=22946501", "title": "S. L. Hakimi", "text": "S. L. Hakimi\n\nSeifollah Louis Hakimi (1932 – June 23, 2005) was an Iranian-American mathematician born in Iran, a professor emeritus at Northwestern University, where he chaired the department of electrical engineering from 1973 to 1978. He was Chair of the Department of Electrical Engineering at University of California, Davis, from 1986 to 1996.\n\nHakimi received his Ph.D. from the University of Illinois at Urbana-Champaign in 1959, under the supervision of Mac Van Valkenburg. He has over 100 academic descendants, most of them via his student Narsingh Deo.\n\nHe is known for characterizing the degree sequences of undirected graphs, for formulating the Steiner tree problem on networks, and for his work on facility location problems on networks.\n\n"}
{"id": "29972293", "url": "https://en.wikipedia.org/wiki?curid=29972293", "title": "Schröder–Bernstein property", "text": "Schröder–Bernstein property\n\nA Schröder–Bernstein property is any mathematical property that matches the following pattern\nThe name Schröder–Bernstein (or Cantor–Schröder–Bernstein, or Cantor–Bernstein) property is in analogy to the theorem of the same name (from set theory).\n\nIn order to define a specific Schröder–Bernstein property one should decide\n\nIn the classical (Cantor–)Schröder–Bernstein theorem,\n\nNot all statements of this form are true. For example, assume that\nThen the statement fails badly: every triangle \"X\" evidently is similar to some triangle inside \"Y\", and the other way round; however, \"X\" and \"Y\" need not be similar.\n\nA Schröder–Bernstein property is a joint property of\nInstead of the relation \"be a part of\" one may use a binary relation \"be embeddable into\" (embeddability) interpreted as \"be similar to some part of\". Then a Schröder–Bernstein property takes the following form.\nThe same in the language of category theory:\nThe relation \"injects into\" is a preorder (that is, a reflexive and transitive relation), and \"be isomorphic\" is an equivalence relation. Also embeddability is usually a preorder, and similarity is usually an equivalence relation (which is natural, but not provable in the absence of formal definitions). Generally, a preorder leads to an equivalence relation and a partial order between the corresponding equivalence classes. The Schröder–Bernstein property claims that the embeddability preorder (assuming that it is a preorder) leads to the similarity equivalence relation, and a partial order (not just preorder) between classes of similar objects.\n\nThe problem of deciding whether a Schröder–Bernstein property (for a given class and two relations) holds or not, is called a Schröder–Bernstein problem. A theorem that states a Schröder–Bernstein property (for a given class and two relations), thus solving the Schröder–Bernstein problem in the affirmative, is called a Schröder–Bernstein theorem (for the given class and two relations), not to be confused with the classical (Cantor–)Schröder–Bernstein theorem mentioned above.\n\nThe Schröder–Bernstein theorem for measurable spaces states the Schröder–Bernstein property for the following case:\n\nIn the Schröder–Bernstein theorem for operator algebras,\nTaking into account that commutative von Neumann algebras are closely related to measurable spaces, one may say that the Schröder–Bernstein theorem for operator algebras is in some sense a noncommutative counterpart of the Schröder–Bernstein theorem for measurable spaces.\n\nThe Myhill isomorphism theorem can be viewed as a Schröder–Bernstein theorem in computability theory.\n\nBanach spaces violate the Schröder–Bernstein property; here\n\nMany other Schröder–Bernstein problems related to various spaces and algebraic structures (groups, rings, fields etc.) are discussed by informal groups of mathematicians (see External Links below).\n\n\n\n"}
{"id": "1432589", "url": "https://en.wikipedia.org/wiki?curid=1432589", "title": "Schwarz lemma", "text": "Schwarz lemma\n\nIn mathematics, the Schwarz lemma, named after Hermann Amandus Schwarz, is a result in complex analysis about holomorphic functions from the open unit disk to itself. The lemma is less celebrated than stronger theorems, such as the Riemann mapping theorem, which it helps to prove. It is, however, one of the simplest results capturing the rigidity of holomorphic functions.\n\nSchwarz Lemma. Let formula_1 be the open unit disk in the complex plane formula_2 centered at the origin and let formula_3 be a holomorphic map such that formula_4 and formula_5 on formula_6.\n\nThen, formula_7 and formula_8.\n\nMoreover, if formula_9 for some non-zero formula_10 or formula_11, then formula_12 for some formula_13 with formula_14.\nThe proof is a straightforward application of the maximum modulus principle on the function\n\nwhich is holomorphic on the whole of D, including at the origin (because \"f\" is differentiable at the origin and fixes zero). Now if D = {\"z\" : |\"z\"| ≤ \"r\"} denotes the closed disk of radius \"r\" centered at the origin, then the maximum modulus principle implies that, for \"r\" < 1, given any \"z\" in D, there exists \"z\" on the boundary of D such that\n\nAs formula_17 we get formula_18.\n\nMoreover, suppose that |\"f\"(\"z\")| = |\"z\"| for some non-zero \"z\" in D, or |\"f′\"(0)| = 1. Then, |\"g\"(\"z\")| = 1 at some point of D. So by the maximum modulus principle, \"g\"(\"z\") is equal to a constant \"a\" such that |\"a\"| = 1. Therefore, \"f\"(\"z\") = \"az\", as desired.\n\nA variant of the Schwarz lemma can be stated that is invariant under analytic automorphisms on the unit disk, i.e. bijective holomorphic mappings of the unit disc to itself. This variant is known as the Schwarz–Pick theorem (after Georg Pick):\n\nLet \"f\" : D → D be holomorphic. Then, for all \"z\", \"z\" ∈ D,\n\nand, for all \"z\" ∈ D,\n\nThe expression\n\nis the distance of the points \"z\", \"z\" in the Poincaré metric, i.e. the metric in the Poincaré disc model for hyperbolic geometry in dimension two. The Schwarz–Pick theorem then essentially states that a holomorphic map of the unit disk into itself \"decreases\" the distance of points in the Poincaré metric. If equality holds throughout in one of the two inequalities above (which is equivalent to saying that the holomorphic map preserves the distance in the Poincaré metric), then \"f\" must be an analytic automorphism of the unit disc, given by a Möbius transformation mapping the unit disc to itself.\n\nAn analogous statement on the upper half-plane H can be made as follows:\n\nLet \"f\" : H → H be holomorphic. Then, for all \"z\", \"z\" ∈ H,\n\nThis is an easy consequence of the Schwarz–Pick theorem mentioned above: One just needs to remember that the Cayley transform \"W\"(\"z\") = (\"z\" − \"i\")/(\"z\" + \"i\") maps the upper half-plane H conformally onto the unit disc D. Then, the map \"W\" o \"f\" o \"W\" is a holomorphic map from D onto D. Using the Schwarz–Pick theorem on this map, and finally simplifying the results by using the formula for \"W\", we get the desired result. Also, for all \"z\" ∈ H,\n\nIf equality holds for either the one or the other expressions, then \"f\" must be a Möbius transformation with real coefficients. That is, if equality holds, then\n\nwith \"a\", \"b\", \"c\", \"d\" ∈ R, and \"ad\" − \"bc\" > 0.\n\nThe proof of the Schwarz–Pick theorem follows from Schwarz's lemma and the fact that a Möbius transformation of the form\n\nmaps the unit circle to itself. Fix \"z\" and define the Möbius transformations\n\nSince \"M\"(\"z\") = 0 and the Möbius transformation is invertible, the composition φ(\"f\"(\"M\"(\"z\"))) maps 0 to 0 and the unit disk is mapped into itself. Thus we can apply Schwarz's lemma, which is to say\n\nNow calling \"z\" = \"M\"(\"z\") (which will still be in the unit disk) yields the desired conclusion\n\nTo prove the second part of the theorem, we rearrange the left-hand side into the difference quotient and let \"z\" tend to \"z\".\n\nThe Schwarz–Ahlfors–Pick theorem provides an analogous theorem for hyperbolic manifolds.\n\nDe Branges' theorem, formerly known as the Bieberbach Conjecture, is an important extension of the lemma, giving restrictions on the higher derivatives of \"f\" at 0 in case \"f\" is injective; that is, univalent.\n\nThe Koebe 1/4 theorem provides a related estimate in the case that \"f\" is univalent.\n\n"}
{"id": "44938612", "url": "https://en.wikipedia.org/wiki?curid=44938612", "title": "Shai Halevi", "text": "Shai Halevi\n\nShai Halevi (; born 1966) is a computer scientist who works in the cryptography research group at IBM's Thomas J. Watson Research Center.\n\nBorn in Israel in 1966, Halevi received a B.A. and M.Sc. in computer science from the Technion, Israel Institute of Technology in 1991 and 1993. He received his Ph.D. in Computer Science from MIT in 1997, and then joined IBM's Thomas J. Watson Research Center, where he is a Principal Research Staff Member.\n\nShai Halevi's research interests are in cryptography and security. He has published numerous original technical research papers, three of which were awarded the IBM Pat Goldberg memorial best-paper award (in 2004, 2012, and 2013).\nNotable contributions by Shai Halevi include:\n\n\n\n\n\nSince 2013 Halevi is the chair of the steering committee of the Theory of Cryptography Conference. He served on the board of directors of the International Association for Cryptologic Research. He chaired the CRYPTO conference in 2009 and co-chaired the TCC conference in 2006. Halevi also gave many invited talks, including in the USENIX Security Symposium in 2008 and the PKC conference in 2014.\n\nHalevi maintains two open-source software projects: The HElib homomorphic-encryption library, and a web-system for submission/review of articles to academic conferences\n\n"}
{"id": "87947", "url": "https://en.wikipedia.org/wiki?curid=87947", "title": "Sharkovskii's theorem", "text": "Sharkovskii's theorem\n\nIn mathematics, Sharkovskii's theorem, named after Oleksandr Mykolaiovych Sharkovskii, who published it in 1964, is a result about discrete dynamical systems. One of the implications of the theorem is that if a discrete dynamical system on the real line has a periodic point of period 3, then it must have periodic points of every other period.\n\nFor some interval formula_1, suppose\n\nis a continuous function. We say that the number \"x\" is a \"periodic point of period m\" if \"f\"(\"x\") = \"x\" (where \"f\" denotes the composition of \"m\" copies of \"f\") and having \"least period m\" if furthermore \"f\"(\"x\") ≠ \"x\" for all 0 < \"k\" < \"m\". We are interested in the possible periods of periodic points of \"f\". Consider the following ordering of the positive integers:\n\nIt consists of:\n\nEvery positive integer appears exactly once somewhere on this list. Note that this ordering is not a well-ordering.\n\nSharkovskii's theorem states that if \"f\" has a periodic point of least period \"m\", and \"m\" precedes \"n\" in the above ordering, then \"f\" has also a periodic point of least period \"n\".\n\nAs a consequence, we see that if \"f\" has only finitely many periodic points, then they must all have periods that are powers of two. Furthermore, if there is a periodic point of period three, then there are periodic points of all other periods.\n\nSharkovskii's theorem does not state that there are \"stable\" cycles of those periods, just that there are cycles of those periods. For systems such as the logistic map, the bifurcation diagram shows a range of parameter values for which apparently the only cycle has period 3. In fact, there must be cycles of all periods there, but they are not stable and therefore not visible on the computer-generated picture.\n\nThe assumption of continuity is important, as the discontinuous function formula_4, for which every non-zero value has period 3, would otherwise be a counterexample.\n\nSharkovskii also proved the converse theorem: every upper set of the above order is the set of periods for some continuous function from an interval to itself. In fact all such sets of periods are achieved by the family of functions formula_5, formula_6 for formula_7, except for the empty set of periods which is achieved by formula_8, formula_9.\n\nTien-Yien Li and James A. Yorke showed in 1975 that not only does the existence of a period-3 cycle imply the existence of cycles of all periods, but in addition it implies the existence of an uncountable infinitude of points that never map to any cycle (chaotic points)—a property known as period three implies chaos.\n\nSharkovskii's theorem does not immediately apply to dynamical systems on other topological spaces. It is easy to find a circle map with periodic points of period 3 only: take a rotation by 120 degrees, for example. But some generalizations are possible, typically involving the mapping class group of the space minus a periodic orbit. For example, Peter Kloeden showed that Sharkovskii's theorem holds for triangular mappings, i.e., mappings for which the component depends only on the first components .\n\n\n"}
{"id": "130280", "url": "https://en.wikipedia.org/wiki?curid=130280", "title": "Short five lemma", "text": "Short five lemma\n\nIn mathematics, especially homological algebra and other applications of abelian category theory, the short five lemma is a special case of the five lemma.\nIt states that for the following commutative diagram (in any abelian category, or in the category of groups), if the rows are short exact sequences, and if \"g\" and \"h\" are isomorphisms, then \"f\" is an isomorphism as well.\n\nIt follows immediately from the five lemma.\n\nThe essence of the lemma can be summarized as follows: if you have a homomorphism \"f\" from an object \"B\" to an object \"B′\", and this homomorphism induces an isomorphism from a subobject \"A\" of \"B\" to a subobject \"A′\" of \"B′\" and also an isomorphism from the factor object \"B\"/\"A\" to \"B′\"/\"A′\", then \"f\" itself is an isomorphism. Note however that the existence of \"f\" (such that the diagram commutes) has to be assumed from the start; two objects \"B\" and \"B′\" that simply have isomorphic sub- and factor objects need not themselves be isomorphic (for example, in the category of abelian groups, \"B\" could be the cyclic group of order four and \"B′\" the Klein four-group).\n\n"}
{"id": "20766753", "url": "https://en.wikipedia.org/wiki?curid=20766753", "title": "Sinuosity", "text": "Sinuosity\n\nSinuosity, sinuosity index, or sinuosity coefficient of a continuously differentiable curve having at least one inflection point is the ratio of the curvilinear length (along the curve) and the Euclidean distance (straight line) between the end points of the curve. This dimensionless quantity can also be rephrased as the \"actual path length\" divided by the \"shortest path length\" of a curve.\nThe value ranges from 1 (case of straight line) to infinity (case of a closed loop, where the shortest path length is zero) or for an infinitely-long actual path.\n\nThe curve must be continuous (no jump) between the two ends. The sinuosity value is really significant when the line is continuously differentiable (no angular point). The distance between both ends can also be evaluated by a plurality of segments according to a broken line passing through the successive inflection points (sinuosity of order 2).\n\nThe calculation of the sinuosity is valid in a 3-dimensional space (e.g. for the central axis of the small intestine), although it is often performed in a plane (with then a possible orthogonal projection of the curve in the selected plan; \"classic\" sinuosity on the horizontal plane, longitudinal profile sinuosity on the vertical plane).\n\nThe classification of a sinuosity (e.g. strong / weak) often depends on the cartographic scale of the curve (see the coastline paradox for further details) and of the object velocity which flowing therethrough (river, avalanche, car, bicycle, bobsleigh, skier, high speed train, etc.): the sinuosity of the same curved line could be considered very strong for a high speed train but low for a river. Nevertheless, it is possible to see a very strong sinuosity in the succession of few river bends, or of laces on some mountain roads.\n\nThe sinuosity \"S\" of:\nWith similar opposite arcs joints in the same plane, continuously differentiable:\nIn studies of rivers, the sinuosity index is similar but not identical to the general form given above, being given by:\n\nThe difference from the general form happens because the downvalley path is not perfectly straight. The sinuosity index can be explained, then, as the deviations from a path defined by the direction of maximum downslope. For this reason, bedrock streams that flow directly downslope have a sinuosity index of 1, and meandering streams have a sinuosity index that is greater than 1.\n\nIt is also possible to distinguish the case where the stream flowing on the line could not physically travel the distance between the ends: in some hydraulic studies, this leads to assign a sinuosity value of 1 for a torrent flowing over rocky bedrock along a horizontal rectilinear projection, even if the slope angle varies.\n\nFor rivers, the conventional classes of sinuosity, SI, are:\n\nIt has been claimed that river shapes are governed by a self-organizing system that causes their average sinuosity (measured in terms of the source-to-mouth distance, not channel length) to be , but this has not been borne out by later studies, which found an average value less than 2.\n\n"}
{"id": "202713", "url": "https://en.wikipedia.org/wiki?curid=202713", "title": "Standardized moment", "text": "Standardized moment\n\nIn probability theory and statistics, a standardized moment of a probability distribution is a moment (normally a higher degree central moment) that is normalized. The normalization is typically a division by an expression of the standard deviation which renders the moment scale invariant. This has the advantage that such normalized moments differ only in other properties than variability, facilitating e.g. comparison of shape of different probability distributions.\n\nLet \"X\" be a random variable with a probability distribution \"P\" and mean value formula_1 (i.e. the first raw moment or moment about zero), the operator E denoting the expected value of \"X\". Then the standardized moment of degree \"k\" is formula_2 that is, the ratio of the \"k\"th moment about the mean\n\nto the \"k\"th power of the standard deviation,\n\nThe power of \"k\" is because moments scale as formula_5 meaning that formula_6 they are homogeneous functions of degree \"k\", thus the standardized moment is scale invariant. This can also be understood as being because moments have dimension; in the above ratio defining standardized moments, the dimensions cancel, so they are dimensionless numbers.\n\nThe first four standardized moments can be written as:\n\nFor skewness and kurtosis, alternative definitions exist, which are based on the third and fourth cumulant respectively.\n\nAnother scale invariant, dimensionless measure for characteristics of a distribution is the coefficient of variation, formula_7. However, this is not a standardized moment, firstly because it is a reciprocal, and secondly because formula_8 is the first moment about zero (the mean), not the first moment about the mean (which is zero).\n\nSee Normalization (statistics) for further normalizing ratios.\n\n"}
{"id": "1464384", "url": "https://en.wikipedia.org/wiki?curid=1464384", "title": "Superadditivity", "text": "Superadditivity\n\nIn mathematics, a sequence { \"a\" }, \"n\" ≥ 1, is called superadditive if it satisfies the inequality\n\nfor all \"m\" and \"n\". The major reason for the use of superadditive sequences is the following lemma due to Michael Fekete.\n\nLemma: (Fekete) For every superadditive sequence { \"a\" }, \"n\" ≥ 1, the limit lim \"a\"/\"n\" exists and is equal to sup \"a\"/\"n\". (The limit may be positive infinity, for instance, for the sequence \"a\" = log \"n\"<nowiki>!</nowiki>.)\n\nSimilarly, a function \"f\" is \"superadditive\" if\n\nfor all \"x\" and \"y\" in the domain of \"f\".\n\nFor example, formula_3 is a superadditive function for nonnegative real numbers because the square of formula_4 is always greater than or equal to the square of formula_5 plus the square of formula_6, for nonnegative real numbers formula_5 and formula_6.\n\nThe analogue of Fekete's lemma holds for subadditive functions as well.\nThere are extensions of Fekete's lemma that do not require the definition of superadditivity above to hold for all \"m\" and \"n\". There are also results that allow one to deduce the rate of convergence to the limit whose existence is stated in Fekete's lemma if some kind of both superadditivity and subadditivity is present. A good exposition of this topic may be found in Steele (1997).\n\nIf \"f\" is a superadditive function, and if 0 is in its domain, then \"f\"(0) ≤ 0. To see this, take the inequality at the top. formula_9. Hence formula_10\n\nThe negative of a superadditive function is subadditive.\n\nThis follows from the Minkowski determinant theorem, which more generally states that formula_13 is superadditive (equivalently, concave) for nonnegative Hermitian matrices of size \"n\": If formula_11 are nonnegative Hermitian then formula_15.\n\n\n"}
{"id": "32668461", "url": "https://en.wikipedia.org/wiki?curid=32668461", "title": "Susanna S. Epp", "text": "Susanna S. Epp\n\nSusanna Samuels Epp (born 1943) is an author, mathematician, and professor. Her interests include discrete mathematics, mathematical logic, cognitive psychology, and mathematics education, and she has written numerous articles, publications, and textbooks. She is currently professor emerita at DePaul University, where she chaired the Department of Mathematical Sciences and was Vincent de Paul Professor in Mathematics.\n\nEpp holds degrees in mathematics from Northwestern University and the University of Chicago, where she completed her doctorate in 1968 under the supervision of Irving Kaplansky. She taught at Boston University and at the University of Illinois at Chicago before becoming a professor at DePaul University.\n\nInitially researching commutative algebra, Epp became interested by cognitive psychology, especially in education of Mathematics, Logic, Proof, and the Language of mathematics. She wrote several articles about teaching logic and proof in \"American Mathematical Monthly\", and the \"Mathematics Teacher\", a Journal by the National Council of Teachers of Mathematics.\n\nShe is the author of several books including \"Discrete Mathematics with Applications\" (4th ed., Brooks/Cole, 2011), the third edition of which earned a Textbook Excellence Award from the Textbook and Academic Authors Association.\n\n\"By combining discussion of theory and practice, I have tried to show that mathematics has engaging and important applications as well as being interesting and beautiful in its own right\" - Susanna S. Epp wrote in the Preface of the 4th Edition of \"Discrete Mathematics.\"\n\nIn 2005, she received the Louise Hay Award from the Association for Women in Mathematics in recognition for her contributions to mathematics education.\n\n\n\n[[Category:1943 births\n[[Category:21st-century American mathematicians]]\n[[Category:Women mathematicians]]\n[[Category:Living people]]\n[[Category:DePaul University faculty]]\n[[Category:Mathematical logicians]]"}
{"id": "28918145", "url": "https://en.wikipedia.org/wiki?curid=28918145", "title": "Table of simple cubic graphs", "text": "Table of simple cubic graphs\n\nThe connected 3-regular (cubic) simple graphs are listed for small vertex numbers.\n\nThe number of simple cubic graphs on 4, 6, 8, 10, ... vertices is 1, 2, 5, 19, ... . A classification according to edge connectivity is made as follows: the 1-connected and 2-connected graphs are defined as usual. This leaves the other graphs in the 3-connected class because each\n3-regular graph can be split by cutting all edges adjacent to any of the vertices. To refine this definition in the light of the algebra of coupling of angular momenta (see below), a subdivision of the 3-connected graphs is helpful. We shall call\nThis declares the numbers 3 and 4 in the fourth column of the tables below.\n\nBall-and-stick models of the graphs in another column of the\ntable show the vertices and edges in the style of\nimages of molecular bonds.\nComments on the individual pictures contain\ngirth, diameter, Wiener index,\nEstrada index and Kirchhoff index.\nA Hamiltonian circuit (where present) is indicated by enumerating vertices\nalong that path from 1 upwards.\n\nThe LCF notation is a notation by Joshua Lederberg, Coxeter and Frucht, for the representation of cubic graphs that are Hamiltonian.\n\nThe two edges along the cycle adjacent to any of the vertices are not written down.\n\nLet be the vertices of the graph and describe the Hamiltonian circle along the vertices by the edge sequence . Halting at a vertex , there is one unique vertex at a distance joined by a chord with ,\nThe vector of the integers is a suitable, although not unique, representation of the cubic Hamiltonian graph. This is augmented by two additional rules:\nSince the starting vertex of the path is of no importance, the numbers in the representation may be cyclically permuted. If a graph contains different Hamiltonian circuits, one may select one of these to accommodate the notation. The same graph may have different LCF notations, depending on precisely how the vertices are arranged.\n\nOften the anti-palindromic representations with\nare preferred (if they exist), and the redundant part is then replaced by a semicolon and a dash \"; –\". The LCF notation , for example, and would at that stage be condensed to .\n\nThe LCF entries are absent above if the graph has no Hamiltonian cycle, which is rare (see Tait's conjecture). In this case a list of edges between pairs of vertices labeled 0 to n−1 in the third column serves as an identifier.\n\nEach 4-connected (in the above sense) simple cubic graph on vertices defines a class of quantum mechanical j symbols. Roughly speaking, each vertex represents a 3-jm symbol, the graph is converted to a digraph by assigning signs to the angular momentum quantum numbers , the vertices are labelled with a handedness representing the order of the three (of the three edges) in the 3jm symbol, and the graph represents a sum over the product of all these numbers assigned to the vertices.\n\nThere are 1 (6j), 1 (9j), 2 (12j), 5 (15j), 18 (18j), 84 (21j), 607 (24j), 6100 (27j), 78824 (30j), 1195280 (33j), 20297600 (36j), 376940415 (39j) etc. of these .\n\nIf they are equivalent to certain vertex-induced binary trees (cutting one edge and finding a cut that splits the remaining graph into two trees), they are representations of recoupling coefficients, and are then also known as Yutsis graphs .\n\n\n"}
{"id": "4313610", "url": "https://en.wikipedia.org/wiki?curid=4313610", "title": "Texas Math and Science Coaches Association", "text": "Texas Math and Science Coaches Association\n\nThe Texas Math and Science Coaches Association or TMSCA is an organization for coaches of academic University Interscholastic League teams in Texas middle schools and high schools, specifically those that compete in mathematics and science-related tests.\n\nThere are four events in the TMSCA at both the middle and high school level: Number Sense, General Mathematics, Calculator Applications, and General Science.\n\nNumber Sense is an 80-question exam that students are given only 10 minutes to solve. Additionally, no scratch work or paper calculations are allowed. These questions range from simple calculations such as 99+98 to more complicated operations such as 1001×1938. Each calculation is able to be done with a certain trick or shortcut that makes the calculations easier.\nThe high school exam includes calculus and other difficult topics in the questions also with the same rules applied as to the middle school version.\nIt is well known that the grading for this event is particularly stringent as errors such as writing over a line or crossing out potential answers are considered as incorrect answers.\n\nGeneral Mathematics is a 50-question exam that students are given only 40 minutes to solve. These problems are usually more challenging than questions on the Number Sense test, and the General Mathematics word problems take more thinking to figure out. Every problem correct is worth 5 points, and for every problem incorrect, 2 points are deducted. Tiebreakers are determined by the person that misses the first problem and by percent accuracy.\n\nCalculator Applications is an 80-question exam that students are given only 30 minutes to solve. This test requires practice on the calculator, knowledge of a few crucial formulas, and much speed and intensity. Memorizing formulas, tips, and tricks will not be enough. In this event, plenty of practice is necessary in order to master the locations of the keys and develop the speed necessary. All correct questions are worth 5 points and all incorrect questions or skipped questions that are before the last answered questions will lose 4 points; answers are to be given with three significant figures.\n\nScience is a 50-question exam that is solved in 40 minutes at the middle school level or a 60-question exam that is solved in a 2-hour time limit at the high school level. Tiebreakers are determined by the person that misses the first problem and by percent accuracy. As the name suggests, the test focuses on the science subjects learned in the middle school or high school level depending on the student's grade and the version of the test being taken.\n\nIndividual schools that are members of TMSCA can host invitational competitions using TMSCA-released tests. Many schools use this as a fund-raising opportunity for their competitive math program.\n\nTMSCA also hosts two statewide competitions for member schools each year, one at the middle school level and one at the high school level, as well as a qualification competition at the middle school level prior to the state competition, also known as the Regional Qualifier. These statewide competitions are held at the University of Texas at San Antonio campus each spring. These competitions can often serve as practice for statewide UIL tournaments, which occur shortly after, and for middle school students are their only opportunity to compete at the state level (UIL competitions at the middle school level do not go beyond district).\n\nFor the General Mathematics and General Science contests in middle school, 5 points are awarded for each correct answer and 2 points are deducted for each incorrect answer. In the high school contest, 6 points are awarded for each correct answer and 2 points are deducted for each incorrect answer. The real way to calculate it is to multiply the number of questions you did times 5 and subtract 7 from each incorrect question. Unanswered questions do not affect the score. Thus, competitors are penalized for guessing incorrectly. (For both General Mathematics and General Science a perfect score is a 250.) \n\nOn the Number Sense test, scoring is 5 times the last question answered (so a student answering 32 questions would be awarded 160 points) and after that 9 points are deducted for incorrect answers, problems skipped up to the last attempted question and markovers/erasures, (so if the student above missed one and skipped three questions the student would end up with 124 points). Number sense tests are also checked for possible scratch work, overwrites, and erasures (bluntly called \"markovers\"), which if found could result in questions being counted as incorrect or tests being disqualified. (For both Number Sense and Calculator, a perfect score is a 400.)\n\nThe Calculator Applications test multiplies 5 times the last question answered and deducts 9 points for incorrect or skipped questions, similar to Number Sense, but scratch work, markovers/erasures, and the use of a calculator is allowed.\n\nAt almost all TMSCA competitions, students are ranked against each other in their specific grade level. For example, all eighth graders compete against each other, all seventh graders compete against each other, and so on and so forth. This ensures parity of competition since students in higher grades generally tend to score higher than students in the lower grades. Particularly at the high school level, there is a stark contrast between freshmen with little real math and science experience and seniors, who presumably have taken or are taking advanced placement science courses and calculus.\n"}
{"id": "54524279", "url": "https://en.wikipedia.org/wiki?curid=54524279", "title": "The First Moderns", "text": "The First Moderns\n\nThe First Moderns: Profiles in the Origins of Twentieth-Century Thought is a book on Modernism by historian William Everdell, published in 1997 by the University of Chicago Press. A \"New York Times\" Notable Book of 1997, \"The First Moderns\" suggests that \"the heart of Modernism is the postulate of ontological discontinuity.\"\n\nEverdell, Dean of Humanities at Saint Ann's School in Brooklyn Heights, posits that Modernism first emerged in the field of mathematics rather than the arts, specifically in the work of German mathematician Richard Dedekind, who, in 1872, demonstrated that mathematicians operate without a continuum; this represents the formalization of Everdell's axiom of \"ontological discontinuity,\" which he goes on to examine in a multiplicity of contexts. He examines this emerging framework of discreteness in science (Ludwig Boltzmann's mechanics, Cajal's neuroscience, Hugo de Vries's conception of the gene and Planck's quantum work, Einstein's physics); mathematics, logic, and philosophy (Georg Cantor, Gottlob Frege, Bertrand Russell and the linguistic turn, Husserl and the beginnings of phenomenology); in addition to the arts (James Joyce's novels, Picasso's \"Demoiselles D'Avignon\", Schoenberg's twelve-tone music).\n\nCritics largely reviewed \"The First Moderns\" favorably, appreciating Everdell's interdisciplinary approach, in publications including the \"New York Review of Books\", the \"New York Times,\" the \"Los Angeles Times,\" and the \"Washington Post\".\n"}
{"id": "491903", "url": "https://en.wikipedia.org/wiki?curid=491903", "title": "Thue–Morse sequence", "text": "Thue–Morse sequence\n\nIn mathematics, the Thue–Morse sequence, or Prouhet–Thue–Morse sequence, is the binary sequence (an infinite sequence of 0s and 1s) obtained by starting with 0 and successively appending the Boolean complement of the sequence obtained thus far. The first few steps of this procedure yield the strings 0 then 01, 0110, 01101001, 0110100110010110, and so on, which are prefixes of the Thue–Morse sequence. The full sequence begins:\n\nThere are several equivalent ways of defining the Thue–Morse sequence.\n\nTo compute the \"n\"th element \"t\", write the number \"n\" in binary. If the number of ones in this binary expansion is odd then \"t\" = 1, if even then \"t\" = 0. For this reason John H. Conway \"et al\". call numbers \"n\" satisfying \"t\" = 1 \"odious\" (for \"odd\") numbers and numbers for which \"t\" = 0 \"evil\" (for \"even\") numbers. In other words, t = 0 if \"n\" is an evil number and t = 1 if \"n\" is an odious number.\n\nThis method leads to a fast method for computing the Thue–Morse sequence: start with \"t\" = 0, and then, for each \"n\", find the highest-order bit in the binary representation of \"n\" that is different from the same bit in the representation of \"n\" − 1. (This bit can be isolated by letting \"x\" be the bitwise exclusive or of \"n\" and \"n\" − 1, shifting \"x\" right by one bit, and computing the exclusive or of this shifted value with \"x\".) If this bit is at an even index, \"t\" differs from \"t\", and otherwise it is the same as \"t\". The resulting algorithm takes constant time to generate each sequence element, using only a logarithmic number of bits (constant number of words) of memory.\n\nThe Thue–Morse sequence is the sequence \"t\" satisfying the recurrence relation\n\nfor all non-negative integers \"n\".\n\nThe Thue–Morse sequence is a morphic word: it is the output of the following Lindenmayer system:\nThe Thue–Morse sequence in the form given above, as a sequence of bits, can be defined recursively using the operation of bitwise negation.\nSo, the first element is 0.\nThen once the first 2 elements have been specified, forming a string \"s\", then the next 2 elements must form the bitwise negation of \"s\".\nNow we have defined the first 2 elements, and we recurse.\n\nSpelling out the first few steps in detail:\n\nSo\n\nThe sequence can also be defined by:\nwhere \"t\" is the \"j\"th element if we start at \"j\" = 0.\n\nBecause each new block in the Thue–Morse sequence is defined by forming the bitwise negation of the beginning, and this is repeated at the beginning of the next block, the Thue–Morse sequence is filled with \"squares\": consecutive strings that are repeated.\nThat is, there are many instances of \"XX\", where \"X\" is some string. Indeed, formula_3 is such a string if and only if formula_4 or formula_5 where formula_6 for some formula_7 and formula_8 denotes the bitwise negation of formula_9 (interchange 0s and 1s). For instance, with formula_10, we have formula_11, and the square formula_12 appears in formula_13 starting at the 16th bit. (Thus, squares in formula_14 have length either a power of 2 or 3 times a power of 2.)\nHowever, there are no \"cubes\": instances of \"XXX\".\nThere are also no \"overlapping squares\": instances of 0\"X\"0\"X\"0 or 1\"X\"1\"X\"1. The critical exponent is 2.\n\nNotice that \"T\" is palindrome for any \"n\" > 1. Further, let q be a word obtain from \"T\" by counting ones between consecutive zeros. For instance, \"q\" = 2 and \"q\" = 2102012 and so on. The words \"T\" do not contain \"overlapping squares\" in consequence, the words \"q\" are palindrome squarefree words.\n\nThe statement above that the Thue–Morse sequence is \"filled with squares\" can be made precise:\nIt is a \"uniformly recurrent word\", meaning that given any finite string \"X\" in the sequence, there is some length \"n\" (often much longer than the length of \"X\") such that \"X\" appears in \"every\" block of length \"n\".\nThe easiest way to make a recurrent sequence is to form a periodic sequence, one where the sequence repeats entirely after a given number \"m\" of steps.\nThen \"n\" can be set to any multiple of \"m\" that is larger than twice the length of \"X\".\nBut the Morse sequence is uniformly recurrent \"without\" being periodic, not even eventually periodic (meaning periodic after some nonperiodic initial segment).\n\nWe define the Thue–Morse morphism to be the function \"f\" from the set of binary sequences to itself by replacing every 0 in a sequence with 01 and every 1 with 10. Then if \"T\" is the Thue–Morse sequence, then \"f\"(\"T\") is \"T\" again; that is, \"T\" is a fixed point of \"f\". The function \"f\" is a prolongable morphism on the free monoid {0,1} with \"T\" as fixed point: indeed, \"T\" is essentially the \"only\" fixed point of \"f\"; the only other fixed point is the bitwise negation of \"T\", which is simply the Thue–Morse sequence on (1,0) instead of on (0,1). This property may be generalized to the concept of an automatic sequence.\n\nThe \"generating series\" of \"T\" over the binary field is the formal power series\n\nThis power series is algebraic over the field of formal power series, satisfying the equation\n\nThe set of \"evil numbers\" (numbers formula_17 with formula_18) forms a subspace of the nonnegative integers under nim-addition (bitwise exclusive or). For the game of Kayles, evil nim-values occur for few (finitely many) positions in the game, with all remaining positions having odious nim-values.\n\nThe Prouhet–Tarry–Escott problem can be defined as: given a positive integer \"N\" and a non-negative integer \"k\", partition the set \"S\" = { 0, 1, ..., \"N\"-1 } into two disjoint subsets \"S\" and \"S\" that have equal sums of powers up to k, that is:\n\nThis has a solution if \"N\" is a multiple of 2, given by:\n\nFor example, for \"N\" = 8 and \"k\" = 2,\n\nThe condition requiring that \"N\" be a multiple of 2 is not strictly necessary: there are some further cases for which a solution exists. However, it guarantees a stronger property: if the condition is satisfied, then the set of \"k\"th powers of any set of \"N\" numbers in arithmetic progression can be partitioned into two sets with equal sums. This follows directly from the expansion given by the binomial theorem applied to the binomial representing the \"n\"th element of an arithmetic progression.\n\nFor generalizations of the Thue–Morse sequence and the Prouhet–Tarry–Escott problem to partitions into more than two parts, see Bolker, Offner, Richman and Zara, \"The Prouhet–Tarry–Escott problem and generalized Thue–Morse sequences\".\n\nUsing turtle graphics, a curve can be generated if an automaton is programmed with a sequence.\nWhen Thue–Morse sequence members are used in order to select program states:\n\n\nThe resulting curve converges to the Koch curve, a fractal curve of\ninfinite length containing a finite area. This illustrates the fractal nature of the Thue–Morse Sequence.\n\nIt is also possible to draw the curve precisely using the following instructions:\n\n\nIn their book on the problem of fair division, Steven Brams and Alan Taylor invoked the Thue–Morse sequence but did not identify it as such. When allocating a contested pile of items between two parties who agree on the items' relative values, Brams and Taylor suggested a method they called \"balanced alternation\", or \"taking turns taking turns taking turns . . . \", as a way to circumvent the favoritism inherent when one party chooses before the other. An example showed how a divorcing couple might reach a fair settlement in the distribution of jointly-owned items. The parties would take turns to be the first chooser at different points in the selection process: Ann chooses one item, then Ben does, then Ben chooses one item, then Ann does.\n\nLionel Levine and Katherine Stange, in their discussion of how to fairly apportion a shared meal such as an Ethiopian dinner, proposed the Thue–Morse sequence as a way to reduce the advantage of moving first. They suggested that “it would be interesting to quantify the intuition that the Thue-Morse order tends to produce a fair outcome.”\n\nRobert Richman addressed this problem, but he too did not identify the Thue–Morse sequence as such at the time of publication. He presented the sequences \"T\" as step functions on the interval [0,1] and described their relationship to the Walsh and Rademacher functions. He showed that the \"n\"th derivative can be expressed in terms of \"T\". As a consequence, the step function arising from \"T\" is orthogonal to polynomials of order \"n\" − 1. A consequence of this result is that a resource whose value is expressed as a monotonically decreasing continuous function is most fairly allocated using a sequence that converges to Thue-Morse as the function becomes flatter. An example showed how to pour cups of coffee of equal strength from a carafe with a nonlinear concentration gradient, prompting a whimsical article in the popular press.\n\nJoshua Cooper and Aaron Dutle showed why the Thue-Morse order provides a fair outcome for discrete events. They considered the fairest way to stage a Galois duel, in which each of the shooters has equally poor shooting skills. Cooper and Dutle postulated that each dueler would demand a chance to fire as soon as the other’s \"a priori\" probability of winning exceeded their own. They proved that, as the duelers’ hitting probability approaches zero, the firing sequence converges to the Thue–Morse sequence. In so doing, they demonstrated that the Thue-Morse order produces a fair outcome not only for sequences \"T\" of length \"2\", but for sequences of any length.\n\nThus the mathematics supports using the Thue–Morse sequence instead of alternating turns when the goal is fairness but earlier turns differ monotonically from later turns in some meaningful quality, whether that quality varies continuously or discretely.\n\nSports competitions form an important class of equitable sequencing problems, because strict alternation often gives an unfair advantage to one team. Ignacio Palacios-Huerta proposed changing the sequential order to Thue-Morse to improve the \"ex post\" fairness of various tournament competitions, such as the kicking sequence of a penalty shoot-out in soccer, the rotation of color of pieces in a chess match, and the serving order in a tennis tie-break. In competitive rowing, \"T\" is the only arrangement of port- and starboard-rowing crew members that eliminates transverse forces (and hence sideways wiggle) on a four-membered coxless racing boat, while \"T\" is one of only four rigs to avoid wiggle on an eight-membered boat.\n\nFairness is especially important in player drafts. Many professional sports leagues attempt to achieve competitive parity by giving earlier selections in each round to weaker teams. By contrast, fantasy football leagues have no pre-existing imbalance to correct, so they often use a “snake” draft (forward, backward, etc.; or \"T\"). Ian Allan argued that a “third-round reversal” (forward, backward, backward, forward, etc.; or \"T\") would be even more fair. Richman suggested that the fairest way for “captain A” and “captain B” to choose sides for a pick-up game of basketball mirrors \"T\": captain A has the first, fourth, sixth, and seventh choices, while captain B has the second, third, fifth, and eighth choices.\n\nThe Thue–Morse sequence was first studied by Eugène Prouhet in 1851, who applied it to number theory.\nHowever, Prouhet did not mention the sequence explicitly; this was left to Axel Thue in 1906, who used it to found the study of combinatorics on words.\nThe sequence was only brought to worldwide attention with the work of Marston Morse in 1921, when he applied it to differential geometry.\nThe sequence has been discovered independently many times, not always by professional research mathematicians; for example, Max Euwe, a chess grandmaster, who held the World Championship title from 1935 to 1937, and mathematics teacher, discovered it in 1929 in an application to chess: by using its cube-free property (see above), he showed how to circumvent a rule aimed at preventing infinitely protracted games by declaring repetition of moves a draw.\n\n\n"}
{"id": "819251", "url": "https://en.wikipedia.org/wiki?curid=819251", "title": "Tombstone (typography)", "text": "Tombstone (typography)\n\nThe tombstone, Halmos, end of proof, or Q.E.D. mark \"∎\" is used in mathematics to denote the end of a proof, in place of the traditional abbreviation \"Q.E.D.\" for the Latin phrase \"quod erat demonstrandum\", \"which was to be shown\". In magazines, it is one of the various symbols used to indicate the end of an article.\n\nIn Unicode, it is represented as character . Its graphic form varies. It may be a hollow or filled rectangle or square.\n\nIn AMS-LaTeX, the symbol is automatically appended at the end of a proof environment \\begin{proof} ... \\end{proof}. It can also be obtained from the commands \\qedsymbol or \\qed (the latter causes the symbol to be right aligned).\n\nIt is sometimes called a halmos after the mathematician Paul Halmos, who first used it in mathematical context. He got the idea of using it from seeing it was being used to indicate the end of articles in magazines. In his memoir \"I Want to Be a Mathematician\", he wrote the following:\n"}
{"id": "35976444", "url": "https://en.wikipedia.org/wiki?curid=35976444", "title": "Wehrl entropy", "text": "Wehrl entropy\n\nIn quantum information theory, the Wehrl entropy, named after Alfred Wehrl, is a classical entropy of a quantum-mechanical density matrix. It is a type of quasi-entropy defined for the Husimi Q representation of the phase-space quasiprobability distribution. See for a comprehensive review of basic properties of classical, quantum and Wehrl entropies, and their implications in statistical mechanics.\n\nThe Husimi function is a \"classical phase-space\" function of position and momentum , and in one dimension is defined for any quantum-mechanical density matrix by \nwhere is a \"(Glauber) coherent state\", given by\n\nThe Wehrl entropy is then defined as\nThe definition can be easily generalized to any finite dimension.\n\nSuch a definition of the entropy relies on the fact that the Husimi Q representation remains non-negative definite, unlike other representations of quantum quasiprobability distributions in phase space. The Wehrl entropy has several important properties: \n\nIn his original paper Wehrl posted a conjecture that the smallest possible value of Wehrl entropy is 1, formula_5 and it occurs if and only if the density matrix formula_6 is a pure state projector onto any coherent state, i.e. for all choices of formula_16,\n\nSoon after the conjecture was posted, E. H. Lieb proved that the minimum of the Wehrl entropy is 1, and it occurs when the state is a projector onto any coherent state.\n\nIn 1991 E. Carlen proved the uniqueness of the minimizer, i.e. the minimum of the Wehrl entropy occurs only when the state is a projector onto any coherent state.\n\nHowever, it is not the fully quantum von Neumann entropy in the Husimi representation in phase space, : all the requisite star-products in that entropy have been dropped here. In the Husimi representation, the star products read\nand are isomorphic to the Moyal products of the Wigner–Weyl representation.\n\nThe Wehrl entropy, then, may be thought of as a type of heuristic semiclassical approximation to the full quantum von Neumann entropy, since it retains some dependence (through \"Q\") but \"not all of it\".\n\nLike all entropies, it reflects some measure of non-localization, as the Gauss transform involved in generating and the sacrifice of the star operators have effectively discarded information. In general, as indicated, for the same state, the Wehrl entropy exceeds the von Neumann entropy (which vanishes for pure states).\n\nWehrl entropy can be defined for other kinds of coherent states. For example, it can be defined for Bloch coherent states, that is, for angular momentum representations of the group formula_19 for quantum spin systems.\n\nConsider a space formula_20 with formula_21 . We consider a single quantum spin of fixed angular momentum , and shall denote by formula_22 the usual angular momentum operators that satisfy the following commutation relations: formula_23 and cyclic permutations.\n\nDefine formula_24, then formula_25 and formula_26.\n\nThe eigenstates of formula_27 are\n\nFor formula_29 the state formula_30 satisfies: formula_31 and formula_32.\n\nDenote the unit sphere in three dimensions by \nand by formula_34 the space of square integrable function on with the measure \n\nThe Bloch coherent state is defined by \n\nTaking into account the above properties of the state formula_37, the Bloch coherent state can also be expressed as\nwhere formula_39, and \nis a normalised eigenstate of formula_27 satisfying formula_42.\n\nThe Bloch coherent state is an eigenstate of the rotated angular momentum operator formula_27 with a maximum eigenvalue. In other words, for a rotation operator \nthe Bloch coherent state formula_45 satisfies \n\nGiven a density matrix , define the semi-classical density distribution \nThe Wehrl entropy of formula_6 for Bloch coherent states is defined as a classical entropy of the density distribution formula_49,\nwhere formula_51 is a classical differential entropy.\n\nThe analogue of the Wehrl's conjecture for Bloch coherent states was proposed in in 1978. It suggests the minimum value of the Werhl entropy for Bloch coherent states, \nand states that the minimum is reached if and only if the state is a pure Bloch coherent state.\n\nIn 2012 E. H. Lieb and J. P. Solovej proved a substantial part of this conjecture, confirming the minimum value of the Wehrl entropy for Bloch coherent states, and the fact that it is reached for any pure Bloch coherent state. The problem of the uniqueness of the minimizer remains unresolved.\n\nIn E. H. Lieb and J. P. Solovej proved Wehrl's conjecture for Bloch coherent states by generalizing it in the following manner.\n\nFor any concave function formula_53 (e.g. formula_54 as in the definition of the Wehrl entropy), and any density matrix , we have\nwhere is a pure coherent state defined in the section \"Wehrl conjecture\".\n\nGeneralized Wehrl's conjecture for Glauber coherent states was proved as a consequence of the similar statement for Bloch coherent states. For any concave function formula_53, and any density matrix we have\nwhere formula_58 is any point on a sphere.\n\nThe uniqueness of the minimizers for either statement remains an open problem.\n\n"}
