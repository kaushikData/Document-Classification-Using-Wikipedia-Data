{"id": "3893576", "url": "https://en.wikipedia.org/wiki?curid=3893576", "title": "Abstract type", "text": "Abstract type\n\nIn programming languages, an abstract type is a type in a nominative type system that cannot be instantiated directly; a type that is not abstract – which \"can\" be instantiated – is called a \"concrete type\". Every instance of an abstract type is an instance of some concrete subtype. Abstract types are also known as \"existential types\".\n\nAn abstract type may provide no implementation, or an incomplete implementation. In some languages, abstract types with no implementation (rather than an incomplete implementation) are known as \"protocols\", \"interfaces\", \"signatures\", or \"class types\". In class-based object-oriented programming, abstract types are implemented as \"abstract classes\" (also known as \"abstract base classes\"), and concrete types as \"concrete classes\". In generic programming, the analogous notion is a concept, which similarly specifies syntax and semantics, but does not require a subtype relationship: two unrelated types may satisfy the same concept.\n\nOften, abstract types will have one or more implementations provided separately, for example, in the form of concrete subtypes that \"can\" be instantiated. In object-oriented programming, an abstract class may include \"abstract methods\" or \"abstract properties\" that are shared by its subclasses. Other names for language features that are (or may be) used to implement abstract types include \"traits\", \"mixins\", \"flavors\", \"roles\", or \"type classes\".\n\nAbstract classes can be created, signified, or simulated in several ways:\n\nAbstract types are an important feature in statically typed OOP languages. Many dynamically typed languages have no equivalent feature (although the use of duck typing makes abstract types unnecessary); however \"traits\" are found in some modern dynamically-typed languages.\n\nSome authors argue that classes should be leaf classes (have no subtypes), or else be abstract.\n\nAbstract types are useful in that they can be used to define and enforce a \"protocol\"; a set of operations that all objects implementing the protocol must support.\n\n\n\n"}
{"id": "58475368", "url": "https://en.wikipedia.org/wiki?curid=58475368", "title": "Automatic Clustering Algorithms", "text": "Automatic Clustering Algorithms\n\nAutomatic Clustering Algorithm refers to algorithms that can perform clustering without prior knowledge of the data sets. In contrast with other Cluster Analysis techniques, automatic clustering algorithms can determine the optimal number of clusters even in the presence of noise and outlier points.\n\nGiven a set of \"n\" objects, centroid-based algorithms create \"k\" partitions based on a dissimilarity function, such that \"k≤n\". A major problem in applying this type of algorithm is determining the appropriate number of clusters for unlabeled data. Therefore, most research in clustering analysis has been focused on the automation of the process. \n\nAutomated selection of \"k\" in a \"K\"-means clustering algorithm, one of the most used centroid-based clustering algorithms, is still a big problem in machine learning. The most accepted solution to this problem is the elbow method. It consists of running \"k\"-means clustering to the data set with a range of values, calculating the sum of squared errors for each, and plotting them in a line chart. If the chart looks like an arm, the best value of \"k\" will be on the \"elbow\". \n\nAnother method that modifies the \"k\"-means algorithm for automatically choosing the optimal number of clusters is the \"G\"-means algorithm. It was developed from the hypothesis that a subset of the data follows a Gaussian distribution. Thus, \"k\" is increased until each \"k\"-means center's data is Gaussian. This algorithm only requires the standard statistical significance level as a parameter and it does not set limits for the covariance of the data.\n\nConnectivity-Based Clustering or Hierarchical Clustering is based on the idea that objects have more similarities to other nearby objects than to those further away. Therefore, the generated clusters from this type of algorithm will be the result of the distance between the analyzed objects. Hierarchical models can either be divisive—where partitions are built from the entire data set available—or agglomerating—where each partition begins with a single object and additional objects are added to the set. Although hierarchical clustering has the advantage of allowing any valid metric to be used as the defined distance, it comes with many problems. For instance, it is sensitive to noise, to fluctuations in the data set, and it is more difficult to automate. \n\nMethods have been developed to improve and automate existing hierarchical clustering algorithms such as an automated version of single linkage hierarchical cluster analysis (HCA). This computerized method bases its success on a self-consistent outlier reduction approach, followed by the building up of a descriptive function, which permits defining natural clusters. Discarded objects can also be assigned to these clusters. Essentially, one needs not to resort to external parameters to identify natural clusters. Information gathered from HCA, automated and reliable, can be resumed in a dendrogram with the number of natural clusters and the corresponding separation (an option not found in classical HCA). This method includes the two following steps: outliers are removed (this is applied in many filtering applications) and an optional classification allows expanding clusters with the whole set of objects. \n\nBIRCH (balanced iterative reducing and clustering using hierarchies) is an algorithm used to perform connectivity-based clustering for large data-sets. It is regarded as one of the fastest clustering algorithms, but it still has limitations because it requires the number of clusters as input. Therefore, new algorithms based on BIRCH have been developed in which there is no need to provide the cluster count from the beginning, but that preserve the quality and speed of the clusters. The main modification is to remove the final step of BIRCH, where the use had to input the cluster count, and improve the rest of the algorithm, referred to as tree-BIRCH, by optimizing a threshold parameter from the data. In this resulting algorithm, the threshold parameter is calculated from the maximum cluster radius and the minimum distance between clusters, which are often known. This method proved to be efficient for data sets of tens of thousands of clusters. If going beyond that amount, a supercluster splitting problem is introduced. For this, other algorithms have been developed, like MDB-BIRCH, which reduces supercluster splitting with relatively high speed.\n\nUnlike partitioning and hierarchical methods, density-based clustering algorithms are able to find clusters of any arbitrary shape, not only spheres. The Density-based clustering algorithm uses autonomous machine learning that identifies patterns regarding geographical location and distance to a particular number of neighbors. It is considered autonomous because a priori knowledge on what is a cluster is not required. This type of algorithm provides different methods to find clusters in the data. The fastest method is DBSCAN and uses a defined distance to differentiate between dense groups of information and sparser noise. Moreover, HDBSCAN can self-adjust by using a range of distances instead of a specified one. Lastly, the method OPTICS creates a reachability plot based on the distance from neighboring features to separate noise from clusters of varying density. \n\nThese methods still require the user to provide the cluster center and cannot be considered automatic. The Automatic Local Density Clustering Algorithm (ALDC) is an example of the new research focused on developing automatic density-based clustering. ALDC works out local density and distance deviation of every point, thus expanding the difference between the potential cluster center and other points. This expansion allows the machine to work automatically. The machine identifies cluster centers and assigns the points that are left by their closest neighbor of higher density. \"\" \n\nIn the automation of data density to identify clusters, research has also been focused on artificially generating the algorithms. For instance, the Estimation of Distribution Algorithms guarantees the generation of valid algorithms by the directed acyclic graph (DAG), in which nodes represent procedures (building block) and edges represent possible execution sequences between two nodes. Building Blocks determine the EDA’s alphabet or, in other words, any generated algorithm. Clustering algorithms artificially generated are compared to DBSCAN, a manual algorithm, in experimental results.\n"}
{"id": "19720831", "url": "https://en.wikipedia.org/wiki?curid=19720831", "title": "Bipartite dimension", "text": "Bipartite dimension\n\nIn the mathematical fields of graph theory and combinatorial optimization, the bipartite dimension or biclique cover number of a graph \"G\" = (\"V\", \"E\") is the minimum number of bicliques (that is complete bipartite subgraphs), needed to cover all edges in \"E\". A collection of bicliques covering all edges in \"G\" is called a biclique edge cover, or sometimes biclique cover. The bipartite dimension of \"G\" is often denoted by the symbol \"d\"(\"G\").\n\nAn example for a biclique edge cover is given in the following diagrams:\n\nThe biclique dimension of the \"n\"-vertex complete graph, formula_1 is formula_2.\n\nThe bipartite dimension of a \"2n\"-vertex\ncrown graph equals formula_3, where\nis the inverse function of the central binomial coefficient .\nhas formula_6 and the cycle formula_7 has formula_8.\n\nThe computational task of determining the bipartite dimension for a given graph \"G\" is an optimization problem. The decision problem for bipartite dimension can be phrased as:\n\nThis problem appears as problem GT18 in Garey and Johnson's classical book on NP-completeness, and is a rather straightforward reformulation of\nanother decision problem on families of finite sets.\n\nThe set basis problem appears as problem SP7 in Garey and Johnson's book.\nHere, for a family formula_12 of subsets of a finite set formula_13,\na set basis for formula_14 is another family of subsets formula_15 of formula_13, such that every set formula_17 can be described as the union of some basis elements from formula_18. The set basis problem is now given as follows:\n\nIn its former formulation, the problem was proved to be NP-complete by , even for bipartite graphs. The formulation as a set basis problem was proved to be NP-complete earlier by . The problem remains NP-hard even if we restrict our attention to bipartite graphs whose bipartite dimension is guaranteed to be at most formula_24, with \"n\" denoting the size of the given problem instance . On the positive side, the problem is solvable in polynomial time on bipartite domino-free graphs .\n\nRegarding the existence of approximation algorithms, proved that the problem cannot be approximated well (assuming P ≠ NP). Indeed, the bipartite dimension is NP-hard to approximate within formula_25 for every fixed formula_26, already for bipartite graphs .\n\nIn contrast, proving that the problem is fixed-parameter tractable is an exercise in designing kernelization algorithms, which appears as such in the textbook by . also provide a concrete bound on the size of the resulting kernel, which has meanwhile been improved by .\nIn fact, for a given bipartite graph on \"n\" vertices, it can be decided in time formula_27 with formula_28 whether its bipartite dimension is at most \"k\" \n\nThe problem of determining the bipartite dimension of a graph appears in various contexts of computing. For instance, in computer systems, different users of a system can be allowed or disallowed accessing various resources. In a role-based access control system, a role provides access rights to a set of resources. A user can own multiple roles, and he has permission to access all resources granted by some of his roles. Also, a role can be owned by multiple users. The \"role mining problem\" is to find a minimum set of roles, such that for each user, his roles taken together grant access to all specified resources. The set of users together with the set of resources in the system naturally induces a bipartite graph, whose edges are permissions. Each biclique in this graph is a potential role, and the optimum solutions to the role mining problem are precisely the minimum biclique edge covers .\n\nA similar scenario is known in computer security, more specifically in secure broadcasting. In that setup, several messages need to be sent each to a set of receivers, over an insecure channel. Each message has to be encrypted using some cryptographic key that is known only to the intended receivers. Each receiver may possess multiple encryption keys, and each key will be distributed to multiple receivers. The \"optimum key generation problem\" is to find a minimum set of encryption keys for ensuring secure transmission. As above, the problem can be modeled using a bipartite graph whose minimum biclique edge covers coincide with the solutions to the optimum key generation problem .\n\nA different application lies in biology, where minimum biclique edge covers are used in mathematical models of human leukocyte antigen (HLA) serology .\n\n\n\n"}
{"id": "39453637", "url": "https://en.wikipedia.org/wiki?curid=39453637", "title": "Carlyle circle", "text": "Carlyle circle\n\nIn mathematics, a Carlyle circle (named for Thomas Carlyle) is a certain circle in a coordinate plane associated with a quadratic equation. The circle has the property that the solutions of the quadratic equation are the horizontal coordinates of the intersections of the circle with the horizontal axis. Carlyle circles have been used to develop ruler-and-compass constructions of regular polygons.\n\nGiven the quadratic equation\nthe circle in the coordinate plane having the line segment joining the points \"A\"(0, 1) and \"B\"(\"s\", \"p\") as a diameter is called the Carlyle circle of the quadratic equation. \n\nThe defining property of the Carlyle circle can be established thus: the equation of the circle having the line segment AB as diameter is\nThe abscissas of the points where the circle intersects the \"x\"-axis are the roots of the equation (obtained by setting \"y\" = 0 in the equation of the circle)\n\nThe problem of constructing a regular pentagon is equivalent to the problem of constructing the roots of the equation\nOne root of this equation is \"z\" = 1 which corresponds to the point \"P\"(1, 0). Removing the factor corresponding to this root, the other roots turn out to be roots of the equation\nThese roots can be represented in the form ω, ω, ω, ω where ω = exp(2π\"i\"/5). Let these correspond to the points \"P\", \"P\", \"P\", \"P\". Letting \nwe have \nSo \"p\" and \"p\" are the roots of the quadratic equation \nThe Carlyle circle associated with this quadratic has a diameter with endpoints at (0, 1) and (-1, -1) and center at (-1/2, 0). Carlyle circles are used to construct \"p\" and \"p\". From the definitions of \"p\" and \"p\" it also follows that\nThese are then used to construct the points \"P\", \"P\", \"P\", \"P\".\n\nThis detailed procedure involving Carlyle circles for the construction of regular pentagons is given below.\n\nThere is a similar method involving Carlyle circles to construct regular heptadecagons. The attached figure illustrates the procedure.\n\nTo construct a regular 257-gon using Carlyle circles, as many as 24 Carlyle circles are to be constructed. One of these is the circle to solve the quadratic equation \"x\" + \"x\" − 64 = 0.\n\nThere is a procedure involving Carlyle circles for the construction of a regular 65537-gon. However there are practical problems for the implementation of the procedure, as, for example, it requires the construction of the Carlyle circle for the solution of the quadratic equation \"x\" + \"x\" − 2 = 0.\n\nAccording to Howard Eves (1911–2004) the mathematician John Leslie (1766-1832) described the geometric construction of roots of a quadratic equation with a circle in his book \"Elements of Geometry\" and noted that this idea was provided by his former student Thomas Carlyle (1795–1881). However while the description in Leslie's book contains an analogous circle construction, it was presented solely in elementary geometric terms without the notion of a Cartesian coordinate system or a quadratic function and its roots:\n\nIn 1867 the Austrian engineer Eduard Lill published a graphical method to determine the roots of a polynomial (Lill's method). If it is applied on a quadratic function, then it yields the trapezoid figure from Carlyle's solution to Leslie's problem (see graphic) with one of its sides being the diameter of the Carlyle circle. In an article from 1925 G. A. Miller pointed out that a slight modification of Lill's method applied to a normed quadratic function yields a circle that allows the geometric construction of the roots of that function and gave the explicit modern definition of what was later to be called Carlyle circle.\n\nEves used the circle in the modern sense in one of the exercises of his book \"Introduction to the History of Mathematics\" (1953) and pointed out the connection to Leslie and Carlyle. Later publications started to adopt the names \"Carly circle\" , \"Carlyle method\" or \"Carlyle algorithm\", though in German speaking countries the term \"Lill circle\" (\"Lill-Kreis\") is used as well. DeTemple used in 1989 and 1991 Carlyle circles to devise Compass-and-straightedge constructions for regular polygons, in particular the pentagon, the heptacontagon, the 257-gon and the 65537-gon. Ladislav Beran described in 1999, how the Carlyle circle can be used to construct the complex roots of a normed quadratic function.\n"}
{"id": "314204", "url": "https://en.wikipedia.org/wiki?curid=314204", "title": "Chemometrics", "text": "Chemometrics\n\nChemometrics is the science of extracting information from chemical systems by data-driven means. Chemometrics is inherently interdisciplinary, using methods frequently employed in core data-analytic disciplines such as multivariate statistics, applied mathematics, and computer science, in order to address problems in chemistry, biochemistry, medicine, biology and chemical engineering. In this way, it mirrors other interdisciplinary fields, such as psychometrics and econometrics.\n\nChemometrics is applied to solve both descriptive and predictive problems in experimental natural sciences, especially in chemistry. In descriptive applications, properties of chemical systems are modeled with the intent of learning the underlying relationships and structure of the system (i.e., model understanding and identification). In predictive applications, properties of chemical systems are modeled with the intent of predicting new properties or behavior of interest. In both cases, the datasets can be small but are often very large and highly complex, involving hundreds to thousands of variables, and hundreds to thousands of cases or observations.\n\nChemometric techniques are particularly heavily used in analytical chemistry and metabolomics, and the development of improved chemometric methods of analysis also continues to advance the state of the art in analytical instrumentation and methodology. It is an application-driven discipline, and thus while the standard chemometric methodologies are very widely used industrially, academic groups are dedicated to the continued development of chemometric theory, method and application development.\n\nAlthough one could argue that even the earliest analytical experiments in chemistry involved a form of chemometrics, the field is generally recognized to have emerged in the 1970s as computers became increasingly exploited for scientific investigation. The term ‘chemometrics’ was coined by Svante Wold in a grant application 1971, and the International Chemometrics Society was formed shortly thereafter by Svante Wold and Bruce Kowalski, two pioneers in the field. Wold was a professor of organic chemistry at Umeå University, Sweden, and Kowalski was a professor of analytical chemistry at University of Washington, Seattle.\n\nMany early applications involved multivariate classification, numerous quantitative predictive applications followed, and by the late 1970s and early 1980s a wide variety of data- and computer-driven chemical analyses were occurring.\n\nMultivariate analysis was a critical facet even in the earliest applications of chemometrics. The data resulting from infrared and UV/visible spectroscopy are often easily numbering in the thousands of measurements per sample. Mass spectrometry, nuclear magnetic resonance, atomic emission/absorption and chromatography experiments are also all by nature highly multivariate. The structure of these data was found to be conducive to using techniques such as principal components analysis (PCA), and partial least-squares (PLS). This is primarily because, while the datasets may be highly multivariate there is strong and often linear low-rank structure present. PCA and PLS have been shown over time very effective at empirically modeling the more chemically interesting low-rank structure, exploiting the interrelationships or ‘latent variables’ in the data, and providing alternative compact coordinate systems for further numerical analysis such as regression, clustering, and pattern recognition. Partial least squares in particular was heavily used in chemometric applications for many years before it began to find regular use in other fields.\n\nThrough the 1980s three dedicated journals appeared in the field: \"Journal of Chemometrics\", \"Chemometrics and Intelligent Laboratory Systems\", and \"Journal of Chemical Information and Modeling\". These journals continue to cover both fundamental and methodological research in chemometrics. At present, most routine applications of existing chemometric methods are commonly published in application-oriented journals (e.g., \"Applied Spectroscopy, \"Analytical Chemistry\", \"Anal. Chim. Acta.\", \"Talanta\"). Several important books/monographs on chemometrics were also first published in the 1980s, including the first edition of Malinowski’s \"Factor Analysis in Chemistry\", Sharaf, Illman and Kowalski’s \"Chemometrics\", Massart et al. \"Chemometrics: a textbook\", and \"Multivariate Calibration\" by Martens and Naes.\n\nSome large chemometric application areas have gone on to represent new domains, such as molecular modeling and QSAR, cheminformatics, the ‘-omics’ fields of genomics, proteomics, metabonomics and metabolomics, process modeling and process analytical technology.\n\nAn account of the early history of chemometrics was published as a series of interviews by Geladi and Esbensen.\n\nMany chemical problems and applications of chemometrics involve calibration. The objective is to develop models which can be used to predict properties of interest based on measured properties of the chemical system, such as pressure, flow, temperature, infrared, Raman, NMR spectra and mass spectra. Examples include the development of multivariate models relating 1) multi-wavelength spectral response to analyte concentration, 2) molecular descriptors to biological activity, 3) multivariate process conditions/states to final product attributes. The process requires a calibration or training data set, which includes reference values for the properties of interest for prediction, and the measured attributes believed to correspond to these properties. For case 1), for example, one can assemble data from a number of samples, including concentrations for an analyte of interest for each sample (the reference) and the corresponding infrared spectrum of that sample. Multivariate calibration techniques such as partial-least squares regression, or principal component regression (and near countless other methods) are then used to construct a mathematical model that relates the multivariate response (spectrum) to the concentration of the analyte of interest, and such a model can be used to efficiently predict the concentrations of new samples.\n\nTechniques in multivariate calibration are often broadly categorized as classical or inverse methods. The principal difference between these approaches is that in classical calibration the models are solved such that they are optimal in describing the measured analytical responses (e.g., spectra) and can therefore be considered optimal descriptors, whereas in inverse methods the models are solved to be optimal in predicting the properties of interest (e.g., concentrations, optimal predictors). Inverse methods usually require less physical knowledge of the chemical system, and at least in theory provide superior predictions in the mean-squared error sense, and hence inverse approaches tend to be more frequently applied in contemporary multivariate calibration.\n\nThe main advantages of the use of multivariate calibration techniques is that fast, cheap, or non-destructive analytical measurements (such as optical spectroscopy) can be used to estimate sample properties which would otherwise require time-consuming, expensive or destructive testing (such as LC-MS). Equally important is that multivariate calibration allows for accurate quantitative analysis in the presence of heavy interference by other analytes. The selectivity of the analytical method is provided as much by the mathematical calibration, as the analytical measurement modalities. For example, near-infrared spectra, which are extremely broad and non-selective compared to other analytical techniques (such as infrared or Raman spectra), can often be used successfully in conjunction with carefully developed multivariate calibration methods to predict concentrations of analytes in very complex matrices.\n\nSupervised multivariate classification techniques are closely related to multivariate calibration techniques in that a calibration or training set is used to develop a mathematical model capable of classifying future samples. The techniques employed in chemometrics are similar to those used in other fields – multivariate discriminant analysis, logistic regression, neural networks, regression/classification trees. The use of rank reduction techniques in conjunction with these conventional classification methods is routine in chemometrics, for example discriminant analysis on principal components or partial least squares scores.\n\nUnsupervised classification (also termed cluster analysis) is also commonly used to discover patterns in complex data sets, and again many of the core techniques used in chemometrics are common to other fields such as machine learning and statistical learning.\n\nIn chemometric parlance, multivariate curve resolution seeks to deconstruct data sets with limited or absent reference information and system knowledge. Some of the earliest work on these techniques was done by Lawton and Sylvestre in the early 1970s. These approaches are also called self-modeling mixture analysis, blind source/signal separation, and spectral unmixing. For example, from a data set comprising fluorescence spectra from a series of samples each containing multiple fluorophores, multivariate curve resolution methods can be used to extract the fluorescence spectra of the individual fluorophores, along with their relative concentrations in each of the samples, essentially unmixing the total fluorescence spectrum into the contributions from the individual components. The problem is usually ill-determined due to rotational ambiguity (many possible solutions can equivalently represent the measured data), so the application of additional constraints is common, such as non-negatively, unmodality, or known interrelationships between the individual components (e.g., kinetic or mass-balance constraints).\n\nExperimental design remains a core area of study in chemometrics and several monographs are specifically devoted to experimental design in chemical applications. Sound principles of experimental design have been widely adopted within the chemometrics community, although many complex experiments are purely observational, and there can be little control over the properties and interrelationships of the samples and sample properties.\n\nSignal processing is also a critical component of almost all chemometric applications, particularly the use of signal pretreatments to condition data prior to calibration or classification. The techniques employed commonly in chemometrics are often closely related to those used in related fields.\n\nPerformance characterization, and figures of merit Like most arenas in the physical sciences, chemometrics is quantitatively oriented, so considerable emphasis is placed on performance characterization, model selection, verification & validation, and figures of merit. The performance of quantitative models is usually specified by root mean squared error in predicting the attribute of interest, and the performance of classifiers as a true-positive rate/false-positive rate pairs (or a full ROC curve). A recent report by Olivieri et al. provides a comprehensive overview of figures of merit and uncertainty estimation in multivariate calibration, including multivariate definitions of selectivity, sensitivity, SNR and prediction interval estimation. Chemometric model selection usually involves the use of tools such as resampling (including bootstrap, permutation, cross-validation).\n\nMultivariate statistical process control (MSPC), modeling and optimization accounts for a substantial amount of historical chemometric development. Spectroscopy has been used successfully for online monitoring of manufacturing processes for 30–40 years, and this process data is highly amenable to chemometric modeling. Specifically in terms of MSPC, multiway modeling of batch and continuous processes is increasingly common in industry and remains an active area of research in chemometrics and chemical engineering. Process analytical chemistry as it was originally termed, or the newer term process analytical technology continues to draw heavily on chemometric methods and MSPC.\n\nMultiway methods are heavily used in chemometric applications. These are higher-order extensions of more widely used methods. For example, while the analysis of a table (matrix, or second-order array) of data is routine in several fields, multiway methods are applied to data sets that involve 3rd, 4th, or higher-orders. Data of this type is very common in chemistry, for example a liquid-chromatography / mass spectrometry (LC-MS) system generates a large matrix of data (elution time versus m/z) for each sample analyzed. The data across multiple samples thus comprises a data cube. Batch process modeling involves data sets that have time vs. process variables vs. batch number. The multiway mathematical methods applied to these sorts of problems include PARAFAC, trilinear decomposition, and multiway PLS and PCA.\n\n\n"}
{"id": "24215692", "url": "https://en.wikipedia.org/wiki?curid=24215692", "title": "Circular algebraic curve", "text": "Circular algebraic curve\n\nIn geometry, a circular algebraic curve is a type of plane algebraic curve determined by an equation \"F\"(\"x\", \"y\") = 0, where \"F\" is a polynomial with real coefficients and the highest-order terms of \"F\" form a polynomial divisible by \"x\" + \"y\". More precisely, if\n\"F\" = \"F\" + \"F\" + ... + \"F\" + \"F\", where each \"F\" is homogeneous of degree \"i\", then the curve \"F\"(\"x\", \"y\") = 0 is circular if and only if \"F\" is divisible by \"x\" + \"y\".\n\nEquivalently, if the curve is determined in homogeneous coordinates by \"G\"(\"x\", \"y\", \"z\") = 0, where \"G\" is a homogeneous polynomial, then the curve is circular if and only if \"G\"(1, \"i\",0) = \"G\"(1, −\"i\",0) = 0. In other words, the curve is circular if it contains the circular points at infinity, (1, \"i\" ,0) and (1, −\"i\", 0), when considered as a curve in the complex projective plane.\n\nAn algebraic curve is called \"p\"-circular if it contains the points (1, \"i\", 0) and (1, −\"i\", 0) when considered as a curve in the complex projective plane, and these points are singularities of order at least \"p\". The terms \"bicircular\", \"tricircular\", etc. apply when \"p\" = 2, 3, etc. In terms of the polynomial \"F\" given above, the curve \"F\"(\"x\", \"y\") = 0 is \"p\"-circular if \"F\" is divisible by (\"x\" + \"y\") when \"i\" < \"p\". When \"p\" = 1 this reduces to the definition of a circular curve. The set of \"p\"-circular curves is invariant under Euclidean transformations. Note that a \"p\"-circular curve must have degree at least 2\"p\".\n\nThe set of \"p\"-circular curves of degree \"p\" + \"k\", where \"p\" may vary but \"k\" is a fixed positive integer, is invariant under inversion. When \"k\" is 1 this says that the set of lines (0-circular curves of degree 1) together with the set of circles (1-circular curves of degree 2) form a set which is invariant under inversion.\n\n\n"}
{"id": "20488086", "url": "https://en.wikipedia.org/wiki?curid=20488086", "title": "Common integrals in quantum field theory", "text": "Common integrals in quantum field theory\n\nThere are common integrals in quantum field theory that appear repeatedly. These integrals are all variations and generalizations of gaussian integrals to the complex plane and to multiple dimensions. Other integrals can be approximated by versions of the gaussian integral. Fourier integrals are also considered.\n\nThe first integral, with broad application outside of quantum field theory, is the gaussian integral.\n\nIn physics the factor of 1/2 in the argument of the exponential is common.\n\nNote:\n\nThus we obtain\n\nwhere we have scaled\n\nand\n\nIn general\n\nNote that the integrals of exponents and odd powers of x are 0, due to odd symmetry.\n\nThis integral can be performed by completing the square:\n\nTherefore:\n\nThe integral\n\nis proportional to the Fourier transform of the gaussian where is the conjugate variable of .\n\nBy again completing the square we see that the Fourier transform of a gaussian is also a gaussian, but in the conjugate variable. The larger is, the narrower the gaussian in and the wider the gaussian in . This is a demonstration of the uncertainty principle.\n\nThis integral is also known as the Hubbard-Stratonovich transformation used in field theory.\n\nThe integral of interest is (for an example of an application see Relation between Schrödinger's equation and the path integral formulation of quantum mechanics)\n\nWe now assume that and may be complex.\n\nCompleting the square\n\nBy analogy with the previous integrals\n\nThis result is valid as an integration in the complex plane as long as has a positive imaginary part.\n\nThe one-dimensional integrals can be generalized to multiple dimensions.\n\nHere is a real symmetric matrix.\n\nThis integral is performed by diagonalization of with an orthogonal transformation\n\nwhere is a diagonal matrix and is an orthogonal matrix. This decouples the variables and allows the integration to be performed as one-dimensional integrations.\n\nThis is best illustrated with a two-dimensional example.\n\nThe Gaussian integral in two dimensions is\n\nwhere is a two-dimensional symmetric matrix with components specified as\n\nand we have used the Einstein summation convention.\n\nThe first step is to diagonalize the matrix. Note that\n\nwhere, since is a real symmetric matrix, we can choose to be orthogonal, and hence also a unitary matrix. can be obtained from the eigenvectors of . We choose such that: is diagonal.\n\nTo find the eigenvectors of one first finds the eigenvalues of given by\n\nThe eigenvalues are solutions of the characteristic polynomial\n\nwhich are found using the quadratic equation:\n\nSubstitution of the eigenvalues back into the eigenvector equation yields\n\nFrom the characteristic equation we know\n\nAlso note\n\nThe eigenvectors can be written as:\n\nfor the two eigenvectors. Here is a normalizing factor given by\n\nIt is easily verified that the two eigenvectors are orthogonal to each other.\n\nThe orthogonal matrix is constructed by assigning the normalized eigenvectors as columns in the orthogonal matrix\n\nNote that .\n\nIf we define\n\nthen the orthogonal matrix can be written\n\nwhich is simply a rotation of the eigenvectors with the inverse:\n\nThe diagonal matrix becomes\n\nwith eigenvectors\n\nThe eigenvalues are\n\nThe eigenvectors are\n\nwhere\n\nThen\n\nThe diagonal matrix becomes\n\nwith eigenvectors\n\nWith the diagonalization the integral can be written\n\nwhere\n\nSince the coordinate transformation is simply a rotation of coordinates the Jacobian determinant of the transformation is one yielding\n\nThe integrations can now be performed.\n\nwhich is the advertised solution.\n\nWith the two-dimensional example it is now easy to see the generalization to the complex plane and to multiple dimensions.\n\nAs an example consider the integral\n\nwhere formula_53 is a differential operator with formula_54 and functions of spacetime, and formula_55 indicates integration over all possible paths. In analogy with the matrix version of this integral the solution is\n\nwhere\n\nand , called the propagator, is the inverse of formula_58, and formula_59 is the Dirac delta function.\n\nSimilar arguments yield\n\nand\n\nSee Path-integral formulation of virtual-particle exchange for an application of this integral.\n\nIn quantum field theory n-dimensional integrals of the form\n\nappear often. Here formula_63 is the reduced Planck's constant and f is a function with a positive minimum at formula_64. These integrals can be approximated by the method of steepest descent.\n\nFor small values of Planck's constant, f can be expanded about its minimum\n\nHere formula_66 is the n by n matrix of second derivatives evaluated at the minimum of the function.\n\nIf we neglect higher order terms this integral can be integrated explicitly.\n\nA common integral is a path integral of the form\n\nwhere formula_69 is the classical action and the integral is over all possible paths that a particle may take. In the limit of small formula_70 the integral can be evaluated in the stationary phase approximation. In this approximation the integral is over the path in which the action is a minimum. Therefore, this approximation recovers the classical limit of mechanics.\n\nThe Dirac delta distribution in spacetime can be written as a Fourier transform\n\nIn general, for any dimension formula_72\n\nWhile not an integral, the identity in three-dimensional Euclidean space\n\nwhere\n\nis a consequence of Gauss's theorem and can be used to derive integral identities. For an example see Longitudinal and transverse vector fields.\n\nThis identity implies that the Fourier integral representation of 1/r is\n\nThe Yukawa potential in three dimensions can be represented as an integral over a Fourier transform\n\nwhere\n\nSee Static forces and virtual-particle exchange for an application of this integral.\n\nIn the small m limit the integral reduces to .\n\nTo derive this result note:\n\nwhere the hat indicates a unit vector in three dimensional space. The derivation of this result is as follows:\n\nNote that in the small limit the integral goes to the result for the Coulomb potential since the term in the brackets goes to .\n\nwhere the hat indicates a unit vector in three dimensional space. The derivation for this result is as follows:\n\nNote that in the small limit the integral reduces to\n\nIn the small mr limit the integral goes to\n\nFor large distance, the integral falls off as the inverse cube of r\n\nFor applications of this integral see Darwin Lagrangian and Darwin interaction in a vacuum.\n\nThere are two important integrals. The angular integration of an exponential in cylindrical coordinates can be written in terms of Bessel functions of the first kind\n\nand\n\nFor applications of these integrals see Magnetic interaction between current loops in a simple plasma or electron gas.\n\nSee Abramowitz and Stegun.\n\nFor formula_91, we have\n\nFor an application of this integral see Two line charges embedded in a plasma or electron gas.\n\nThe integration of the propagator in cylindrical coordinates is\n\nFor small mr the integral becomes\n\nFor large mr the integral becomes\n\nFor applications of this integral see Magnetic interaction between current loops in a simple plasma or electron gas.\n\nIn general\n\nThe two-dimensional integral over a magnetic wave function is\n\nHere, M is a confluent hypergeometric function. For an application of this integral see Charge density spread over a wave function.\n\n"}
{"id": "431848", "url": "https://en.wikipedia.org/wiki?curid=431848", "title": "Covariant derivative", "text": "Covariant derivative\n\nIn mathematics, the covariant derivative is a way of specifying a derivative along tangent vectors of a manifold. Alternatively, the covariant derivative is a way of introducing and working with a connection on a manifold by means of a differential operator, to be contrasted with the approach given by a principal connection on the frame bundle – see affine connection. In the special case of a manifold isometrically embedded into a higher-dimensional Euclidean space, the covariant derivative can be viewed as the orthogonal projection of the Euclidean derivative along a tangent vector onto the manifold's tangent space. In this case the Euclidean derivative is broken into two parts, the extrinsic normal component and the intrinsic covariant derivative component.\n\nIn physics, the covariant derivative is the derivative that under a general coordinate transformation transforms covariantly, that is, linearly via the Jacobian matrix of the coordinate transformation.\n\nThis article presents an introduction to the covariant derivative of a vector field with respect to a vector field, both in a coordinate free language and using a local coordinate system and the traditional index notation. The covariant derivative of a tensor field is presented as an extension of the same concept. The covariant derivative generalizes straightforwardly to a notion of differentiation associated to a connection on a vector bundle, also known as a Koszul connection.\n\nHistorically, at the turn of the 20th century, the covariant derivative was introduced by Gregorio Ricci-Curbastro and Tullio Levi-Civita in the theory of Riemannian and pseudo-Riemannian geometry. Ricci and Levi-Civita (following ideas of Elwin Bruno Christoffel) observed that the Christoffel symbols used to define the curvature could also provide a notion of differentiation which generalized the classical directional derivative of vector fields on a manifold. This new derivative – the Levi-Civita connection – was \"covariant\" in the sense that it satisfied Riemann's requirement that objects in geometry should be independent of their description in a particular coordinate system.\n\nIt was soon noted by other mathematicians, prominent among these being Hermann Weyl, Jan Arnoldus Schouten, and Élie Cartan, that a covariant derivative could be defined abstractly without the presence of a metric. The crucial feature was not a particular dependence on the metric, but that the Christoffel symbols satisfied a certain precise second order transformation law. This transformation law could serve as a starting point for defining the derivative in a covariant manner. Thus the theory of covariant differentiation forked off from the strictly Riemannian context to include a wider range of possible geometries.\n\nIn the 1940s, practitioners of differential geometry began introducing other notions of covariant differentiation in general vector bundles which were, in contrast to the classical bundles of interest to geometers, not part of the tensor analysis of the manifold. By and large, these generalized covariant derivatives had to be specified \"ad hoc\" by some version of the connection concept. In 1950, Jean-Louis Koszul unified these new ideas of covariant differentiation in a vector bundle by means of what is known today as a Koszul connection or a connection on a vector bundle. Using ideas from Lie algebra cohomology, Koszul successfully converted many of the analytic features of covariant differentiation into algebraic ones. In particular, Koszul connections eliminated the need for awkward manipulations of Christoffel symbols (and other analogous non-tensorial objects) in differential geometry. Thus they quickly supplanted the classical notion of covariant derivative in many post-1950 treatments of the subject.\n\nThe covariant derivative is a generalization of the directional derivative from vector calculus. As with the directional derivative, the covariant derivative is a rule, formula_1, which takes as its inputs: (1) a vector, u, defined at a point \"P\", and (2) a vector field, v, defined in a neighborhood of \"P\". The output is the vector formula_2, also at the point \"P\". The primary difference from the usual directional derivative is that formula_1 must, in a certain precise sense, be \"independent\" of the manner in which it is expressed in a coordinate system.\n\nA vector may be \"described\" as a list of numbers in terms of a basis, but as a geometrical object a vector retains its own identity regardless of how one chooses to describe it in a basis. This persistence of identity is reflected in the fact that when a vector is written in one basis, and then the basis is changed, the components of the vector transform according to a change of basis formula. Such a transformation law is known as a covariant transformation. The covariant derivative is required to transform, under a change in coordinates, in the same way as a basis does: the covariant derivative must change by a covariant transformation (hence the name).\n\nIn the case of Euclidean space, one tends to define the derivative of a vector field in terms of the difference between two vectors at two nearby points.\nIn such a system one translates one of the vectors to the origin of the other, keeping it parallel. With a Cartesian (fixed orthonormal) coordinate system \"keeping it parallel\" amounts to keeping the components constant. Thus is obtained the simplest example: a covariant derivative which is obtained by taking the ordinary directional derivative of the components in the direction of the displacement vector between the two nearby points.\n\nIn the general case, however, one must take into account the change of the coordinate system. For example, if the same covariant derivative is written in polar coordinates in a two dimensional Euclidean plane, then it contains extra terms that describe how the coordinate grid itself \"rotates\". In other cases the extra terms describe how the coordinate grid expands, contracts, twists, interweaves, etc. In this case \"keeping it parallel\" does \"not\" amount to keeping components constant under translation.\n\nConsider the example of moving along a curve \"γ\"(\"t\") in the Euclidean plane. In polar coordinates, γ may be written in terms of its radial and angular coordinates by \"γ\"(\"t\") = (\"r\"(\"t\"), \"θ\"(\"t\")). A vector at a particular time \"t\" (for instance, the acceleration of the curve) is expressed in terms of formula_4, where formula_5 and formula_6 are unit tangent vectors for the polar coordinates, serving as a basis to decompose a vector in terms of radial and tangential components. At a slightly later time, the new basis in polar coordinates appears slightly rotated with respect to the first set. The covariant derivative of the basis vectors (the Christoffel symbols) serve to express this change.\nIn a curved space, such as the surface of the Earth (regarded as a sphere), the translation is not well defined and its analog, parallel transport, depends on the path along which the vector is translated.\n\nA vector e on a globe on the equator at point Q is directed to the north. Suppose we parallel transport the vector first along the equator until at point P and then (keeping it parallel to itself) drag it along a meridian to the pole N and (keeping the direction there) subsequently transport it along another meridian back to Q. Then we notice that the parallel-transported vector along a closed circuit does not return as the same vector; instead, it has another orientation. This would not happen in Euclidean space and is caused by the \"curvature\" of the surface of the globe. The same effect can be noticed if we drag the vector along an infinitesimally small closed surface subsequently along two directions and then back. The infinitesimal change of the vector is a measure of the curvature.\n\nSuppose a (pseudo) Riemann manifold formula_8, is embedded into Euclidean space formula_9 via a twice continuously-differentiable (C) mapping formula_10 such that the tangent space at formula_11 is spanned by the vectors\n\nand the scalar product on formula_13 is compatible with the metric on \"M\":\n\nFor a tangent vector field, formula_15, one has\n\nThe last term is not tangential to \"M\", but can be expressed as a linear combination of the tangent space base vectors using the Christoffel symbols as linear factors plus a vector normal to the tangent space:\n\nThe covariant derivative formula_18, also written formula_19, is defined as just a tangential portion of the usual derivative:\n\nIn the case of the Levi-Civita connection, formula_21 is required to be orthogonal to tangent space, so\n\nOn the other hand,\n\nimplies (using the symmetry of the scalar product and swapping the order of partial differentiations)\n\nand yields the Christoffel symbols for the Levi-Civita connection in terms of the metric:\n\nFor a very simple example that captures the essence of the description above, draw a circle on a flat sheet of paper. Travel around the circle at a constant speed. The derivative of your velocity, your acceleration vector, always points radially inward. Roll this sheet of paper into a cylinder. Now the (Euclidean) derivative of your velocity has a component that sometimes points inward toward the axis of the cylinder depending on whether you're near a solstice or an equinox. (At the point of the circle when you are moving parallel to the axis, there is no inward acceleration. At the point, the velocity is along the cylinder's bend, the inward acceleration is maximum.) This is the (Euclidean) normal component. The covariant derivative component is the component parallel to the cylinder's surface, and is the same as that before you rolled the sheet into a cylinder.\n\nA covariant derivative is a (Koszul) connection on the tangent bundle and other tensor bundles. Thus it has a certain behavior on vector fields that extends that of the usual differential on functions. It also extends in a unique way to the duals of vector fields (i.e., covector fields), and to arbitrary tensor fields, that ensures compatibility with the tensor product and trace operations (tensor contraction).\n\nGiven a point \"p\" of the manifold, a real function \"f\" on the manifold, and a tangent vector v at \"p\", the covariant derivative of \"f\" at \"p\" along v is the scalar at \"p\", denoted formula_26, that represents the principal part of the change in the value of \"f\" when the argument of f is changed by the infinitesimal displacement vector v. (This is the differential of \"f\" evaluated against the vector v.) Formally, there is a differentiable curve formula_27 such that formula_28 and formula_29, and the covariant derivative of \"f\" at \"p\" is defined by\n\nWhen v is a vector field, the covariant derivative formula_31 is the function that associates with each point \"p\" in the common domain of \"f\" and v the scalar formula_32. This coincides with the usual Lie derivative of \"f\" along the vector field v.\n\nA covariant derivative formula_33 at a point \"p\" in a smooth manifold assigns a tangent vector formula_34 to each pair formula_35, consisting of a tangent vector v at \"p\" and vector field u defined in a neighborhood of \"p\", such that the following properties hold (for any vectors v, x and y at \"p\", vector fields u and w defined in a neighborhood of \"p\", scalar values \"g\" and \"h\" at \"p\", and scalar function \"f\" defined in a neighborhood of \"p\"):\n\nIf u and v are both vector fields defined over a common domain, then formula_45 denotes the vector field whose value at each point \"p\" of the domain is the tangent vector formula_46. Note that formula_36 depends not only on the value of v at \"p\" but also on values of u in an infinitesimal neighbourhood of \"p\" because of the last property, the product rule.\n\nGiven a field of covectors (or one-form) formula_48 defined in a neighborhood of \"p\", its covariant derivative formula_49 is defined in a way to make the resulting operation compatible with tensor contraction and the product rule. That is, formula_49 is defined as the unique one-form at \"p\" such that the following identity is satisfied for all vector fields u in a neighborhood of \"p\"\n\nThe covariant derivative of a covector field along a vector field v is again a covector field.\n\nOnce the covariant derivative is defined for fields of vectors and covectors it can be defined for arbitrary tensor fields by imposing the following identities for every pair of tensor fields formula_52 and formula_53 in a neighborhood of the point \"p\":\nand for formula_52 and formula_56 of the same valence\nThe covariant derivative of a tensor field along a vector field v is again a tensor field of the same type.\n\nExplicitly, let \"T\" be a tensor field of type . Consider \"T\" to be a differentiable multilinear map of smooth sections \"α\", \"α\", ..., \"α\" of the cotangent bundle \"T\"\"M\" and of sections \"X\", \"X\", ... \"X\" of the tangent bundle \"TM\", written \"T\"(α, α, ..., \"X\", \"X\", ...) into R. The covariant derivative of \"T\" along \"Y\" is given by the formula\n\nGiven coordinate functions\n\nany tangent vector can be described by its components in the basis\n\nThe covariant derivative of a basis vector along a basis vector is again a vector and so can be expressed as a linear combination formula_61.\nTo specify the covariant derivative it is enough to specify the covariant derivative of each basis vector field formula_62 along formula_63.\n\nthe coefficients formula_65 are the components of the connection with respect to a system of local coordinates. In the theory of Riemannian and pseudo-Riemannian manifolds, the components of the Levi-Civita connection with respect to a system of local coordinates are called Christoffel symbols.\n\nThen using the rules in the definition, we find that for general vector fields formula_66 and formula_67 we get\n\nso\n\nThe first term in this formula is responsible for \"twisting\" the coordinate system with respect to the covariant derivative and the second for changes of components of the vector field \"u\". In particular\n\nIn words: the covariant derivative is the usual derivative along the coordinates with correction terms which tell how the coordinates change.\n\nFor covectors similarly we have\n\nwhere formula_72.\n\nThe covariant derivative of a type tensor field along formula_73 is given by the expression:\n\nOr, in words: take the partial derivative of the tensor and add: formula_75 for every upper index formula_76, and formula_77 for every lower index formula_78.\n\nIf instead of a tensor, one is trying to differentiate a \"tensor density\" (of weight +1), then you also add a term\nIf it is a tensor density of weight \"W\", then multiply that term by \"W\".\nFor example, formula_80 is a scalar density (of weight +1), so we get:\n\nwhere semicolon \";\" indicates covariant differentiation and comma \",\" indicates partial differentiation. Incidentally, this particular expression is equal to zero, because the covariant derivative of a function solely of the metric is always zero.\n\nFor a scalar field formula_82, covariant differentiation is simply partial differentiation:\n\nFor a contravariant vector field formula_84, we have:\n\nFor a covariant vector field formula_86, we have:\n\nFor a type (2,0) tensor field formula_88, we have:\n\nFor a type (0,2) tensor field formula_90, we have:\n\nFor a type (1,1) tensor field formula_92, we have:\n\nThe notation above is meant in the sense\n\nCovariant derivatives do not commute; i.e. formula_95. It can be shown that:\n\nwhere formula_97 is the Riemann tensor. Similarly,\n\nand\n\nThe latter can be shown by taking (without loss of generality) that formula_100.\n\nIn textbooks on physics, the covariant derivative is sometimes simply stated in terms of its components in this equation.\n\nOften a notation is used in which the covariant derivative is given with a semicolon, while a normal partial derivative is indicated by a comma. In this notation we write the same as:\nOnce again this shows that the covariant derivative of a vector field is not just simply obtained by differentiating to the coordinates formula_102, but also depends on the vector v itself through formula_103.\n\nIn some older texts (notably Adler, Bazin & Schiffer, \"Introduction to General Relativity\"), the covariant derivative is denoted by a double pipe:\n\nSince the covariant derivative formula_105 of a tensor field formula_106 at a point formula_107 depends only on the value of the vector field formula_108 at formula_107 one can define the covariant derivative along a smooth curve formula_110 in a manifold:\nNote that the tensor field formula_106 only needs to be defined on the curve formula_110 for this definition to make sense.\n\nIn particular, formula_114 is a vector field along the curve formula_115 itself. If formula_116 vanishes then the curve is called a geodesic of the covariant derivative. If the covariant derivative is the Levi-Civita connection of a certain metric then the geodesics for the connection are precisely the geodesics of the metric that are parametrised by arc length.\n\nThe derivative along a curve is also used to define the parallel transport along the curve.\n\nSometimes the covariant derivative along a curve is called absolute or intrinsic derivative.\n\nA covariant derivative introduces an extra geometric structure on a manifold that allows vectors in neighboring tangent spaces to be compared. This extra structure is necessary because there is no canonical way to compare vectors from different vector spaces, as is necessary for this generalization of the directional derivative. There is however another generalization of directional derivatives which \"is\" canonical: the Lie derivative. The Lie derivative evaluates the change of one vector field along the flow of another vector field. Thus, one must know both vector fields in an open neighborhood. The covariant derivative on the other hand introduces its own change for vectors in a given direction, and it only depends on the vector direction at a single point, rather than a vector field in an open neighborhood of a point. In other words, the covariant derivative is linear (over \"C\"(\"M\")) in the direction argument, while the Lie derivative is linear in neither argument.\n\nNote that the antisymmetrized covariant derivative , and the Lie derivative \"L\"\"v\" differ by the torsion of the connection, so that if a connection is torsion free, then its antisymmetrization \"is\" the Lie derivative.\n\n"}
{"id": "15364270", "url": "https://en.wikipedia.org/wiki?curid=15364270", "title": "Cunningham number", "text": "Cunningham number\n\nIn mathematics, specifically in number theory, a Cunningham number is a certain kind of integer named after English mathematician A. J. C. Cunningham.\n\nCunningham numbers are a simple type of binomial number, they are of the form\n\nwhere \"b\" and \"n\" are integers and \"b\" is not a perfect power. They are denoted \"C\"(\"b\", \"n\").\n\nEstablishing whether or not a given Cunningham number is prime has been the main focus of research around this type of number. Two particularly famous families of Cunningham numbers in this respect are the Fermat numbers, which are those of the form \"C\"(2,2), and the Mersenne numbers, which are of the form \"C\"(2,\"n\").\n\nCunningham worked on gathering together all known data on which of these numbers were prime. In 1925 he published tables which summarised his findings with H. J. Woodall, and much computation has been done in the intervening time to fill these tables.\n\n\n"}
{"id": "162132", "url": "https://en.wikipedia.org/wiki?curid=162132", "title": "Derangement", "text": "Derangement\n\nIn combinatorial mathematics, a derangement is a permutation of the elements of a set, such that no element appears in its original position.\nIn other words, derangement is a permutation that has no fixed points.\n\nThe number of derangements of a set of size \"n\", usually written \"D\", \"d\", or !\"n\", is called the \"derangement number\" or \"de Montmort number\". (These numbers are generalized to rencontres numbers.) The subfactorial function (not to be confused with the factorial \"n\"!) maps \"n\" to !\"n\". No standard notation for subfactorials is agreed upon; \"n\"¡ is sometimes used instead of !\"n\".\n\nThe problem of counting derangements was first considered by Pierre Raymond de Montmort in 1708; he solved it in 1713, as did Nicholas Bernoulli at about the same time.\n\nSuppose that a professor gave a test to 4 students – A, B, C, and D – and wants to let them grade each other's tests. Of course, no student should grade his or her own test. How many ways could the professor hand the tests back to the students for grading, such that no student received his or her own test back? Out of (4!) for handing back the tests:\nthere are only 9 derangements (shown in blue italics above). In every other permutation of this 4-member set, at least one student gets his or her own test back (shown in bold red).\n\nAnother version of the problem arises when we ask for the number of ways \"n\" letters, each addressed to a different person, can be placed in \"n\" pre-addressed envelopes so that no letter appears in the correctly addressed envelope.\n\nSuppose that there are \"n\" people who are numbered 1, 2, ..., \"n\". Let there be \"n\" hats also numbered 1, 2, ..., \"n\". We have to find the number of ways in which no one gets the hat having the same number as their number. Let us assume that the first person takes hat \"i\". There are \"n\" − 1 ways for the first person to make such a choice. There are now two possibilities, depending on whether or not person \"i\" takes hat 1 in return:\nFrom this, the following relation is derived:\n\nwhere !n, known as the subfactorial, represents the number of derangements, with the starting values !0 = 1 and !1 = 0.\n\nAlso, the following formulae are known:\n\nwhere formula_4 is the nearest integer function and formula_5 is the floor function.\n\nThe following recurrence relationship also holds:\n\nStarting with \"n\" = 0, the numbers of derangements of \"n\" are:\n\nThese numbers are also called subfactorial or rencontres numbers.\n\nA well-known method of counting derangements uses the inclusion–exclusion principle: counting all arrangements, subtracting those that fix at least one element and permute the rest in any way, then adding back those that fix at least two elements, etc. \nFactoring out gives the formula above, formula_10\n\nUsing this recurrence, it can be shown that, in the limit,\n\nThis is the limit of the probability \"p\" = \"d\"/\"n\"! that a randomly selected permutation is a derangement. The probability converges to this limit extremely quickly as \"n\" increases, which is why \"d\" is the nearest integer to \"n\"!/\"e\". The above semi-log graph shows that the derangement graph lags the permutation graph by an almost constant value.\n\nMore information about this calculation and the above limit may be found in the article on the\nstatistics of random permutations.\n\nThe problème des rencontres asks how many permutations of a size-\"n\" set have exactly \"k\" fixed points.\n\nDerangements are an example of the wider field of constrained permutations. For example, the \"ménage problem\" asks if \"n\" opposite-sex couples are seated man-woman-man-woman-... around a table, how many ways can they be seated so that nobody is seated next to his or her partner?\n\nMore formally, given sets \"A\" and \"S\", and some sets \"U\" and \"V\" of surjections \"A\" → \"S\", we often wish to know the number of pairs of functions (\"f\", \"g\") such that \"f\" is in \"U\" and \"g\" is in \"V\", and for all \"a\" in \"A\", \"f\"(\"a\") ≠ \"g\"(\"a\"); in other words, where for each \"f\" and \"g\", there exists a derangement φ of \"S\" such that \"f\"(\"a\") = φ(\"g\"(\"a\")).\n\nAnother generalization is the following problem:\n\nFor instance, for a word made of only two different letters, say \"n\" letters A and \"m\" letters B, the answer is, of course, 1 or 0 according to whether \"n\" = \"m\" or not, for the only way to form an anagram without fixed letters is to exchange all the \"A\" with \"B\", which is possible if and only if \"n\" = \"m\". In the general case, for a word with \"n\" letters \"X\", \"n\" letters \"X\", ..., \"n\" letters \"X\" it turns out (after a proper use of the inclusion-exclusion formula) that the answer has the form:\n\nfor a certain sequence of polynomials \"P\", where \"P\" has degree \"n\". But the above answer for the case \"r\" = 2 gives an orthogonality relation, whence the \"P\"<nowiki>'</nowiki>s are the Laguerre polynomials (up to a sign that is easily decided).\nIn particular, for the classical derangements\n\nformula_13\n\nIt is NP-complete to determine whether a given permutation group (described by a given set of permutations that generate it) contains any derangements.\n\n"}
{"id": "15560774", "url": "https://en.wikipedia.org/wiki?curid=15560774", "title": "Ellis–Numakura lemma", "text": "Ellis–Numakura lemma\n\nIn mathematics, the Ellis–Numakura lemma states that if \"S\" is a non-empty semigroup with a topology such that \"S\" is compact and the product is semi-continuous, then \"S\" has an idempotent element \"p\", (that is, with \"pp\" = \"p\"). The lemma is named after Robert Ellis and Katsui Numakura.\n\nApplying this lemma to the Stone–Čech compactification \"βN\" of the natural numbers shows that there are idempotent elements in \"βN\". The product on \"βN\" is not continuous, but is only semi-continuous (right or left, depending on the preferred construction, but never both).\n\n\n\n"}
{"id": "17440918", "url": "https://en.wikipedia.org/wiki?curid=17440918", "title": "Elston–Stewart algorithm", "text": "Elston–Stewart algorithm\n\nThe Elston–Stewart algorithm is an algorithm for computing the likelihood of observed genotype data given a pedigree. It is due to Robert Elston and John Stewart. It can handle relatively large pedigrees providing they are (almost) outbred. Its computation time is exponential in the number of markers. It is used in the analysis of genetic linkage.\n\n"}
{"id": "31477835", "url": "https://en.wikipedia.org/wiki?curid=31477835", "title": "Entanglement-assisted classical capacity", "text": "Entanglement-assisted classical capacity\n\nIn the theory of quantum communication, the entanglement-assisted classical capacity of a quantum channel is the highest rate at which classical information can be transmitted from a sender to receiver when they share an unlimited amount of noiseless entanglement. It is given by the quantum mutual information of the channel, which is the input-output quantum mutual information maximized over all pure bipartite quantum states with one system transmitted through the channel. This formula is the natural generalization of Shannon's noisy channel coding theorem, in the sense that this formula is equal to the capacity, and there is no need to regularize it. An additional feature that it shares with Shannon's formula is that a noiseless classical or quantum feedback channel cannot increase the entanglement-assisted classical capacity. The entanglement-assisted classical capacity theorem is proved in two parts: the direct coding theorem and the converse theorem. The direct coding theorem demonstrates that the quantum mutual information of the channel is an achievable rate, by a random coding strategy that is effectively a noisy version of the super-dense coding protocol. The converse theorem demonstrates that this rate is optimal by making use of the strong subadditivity of quantum entropy.\n\n\n"}
{"id": "217472", "url": "https://en.wikipedia.org/wiki?curid=217472", "title": "Eternity", "text": "Eternity\n\nEternity in common parlance is an infinitely long period of time. In classical philosophy, however, eternity is defined as what exists outside time while sempiternity is the concept that corresponds to the colloquial definition of eternity. \n\nEternity is an important concept in many religions, where the god or gods are said to endure eternally. Some, such as Aristotle, would say the same about the natural cosmos in regard to both past and future eternal duration, and like the eternal Platonic forms, immutability was considered essential.\n\nAristotle argued that the cosmos has no beginning. In Aristotle's Metaphysics, eternity is the unmoved mover (God), understood as the gradient of total synergy (\"produces motion by being loved\"). Boethius defined eternity as \"simultaneously full and perfect possession of interminable life\".\n\nEternity is often symbolized by the image of a snake swallowing its own tail, known as the Ouroboros (or Uroboros). The circle is also commonly used as a symbol for eternity, as is the mathematical symbol of infinity, formula_1. Symbolically, it suggests that Eternity has no beginning or end. \n\n\n"}
{"id": "2461914", "url": "https://en.wikipedia.org/wiki?curid=2461914", "title": "Fundamental lemma of calculus of variations", "text": "Fundamental lemma of calculus of variations\n\nIn mathematics, specifically in the calculus of variations, a variation of a function can be concentrated on an arbitrarily small interval, but not a single point.\nAccordingly, the necessary condition of extremum (functional derivative equal zero) appears in a weak formulation (variational form) integrated with an arbitrary function . The fundamental lemma of the calculus of variations is typically used to transform this weak formulation into the strong formulation (differential equation), free of the integration with arbitrary function. The proof usually exploits the possibility to choose concentrated on an interval on which keeps sign (positive or negative). Several versions of the lemma are in use. Basic versions are easy to formulate and prove. More powerful versions are used when needed.\n\nHere \"smooth\" may be interpreted as \"infinitely differentiable\", but often is interpreted as \"twice continuously differentiable\" or \"continuously differentiable\" or even just \"continuous\", since these weaker statements may be strong enough for a given task. \"Compactly supported\" means \"vanishes outside formula_7 for some formula_8, formula_9 such that <math>a<c<d\"; but often a weaker statement suffices, assuming only that formula_4 (or formula_4 and a number of its derivatives) vanishes at the endpoints formula_12, formula_13; in this case the closed interval formula_14 is used.\n\nThe special case for \"g\" = 0 is just the basic version.\n\nHere is the special case for \"f\" = 0 (often sufficient).\n\nIf, in addition, continuous differentiability of \"g\" is assumed, then integration by parts reduces both statements to the basic version; this case is attributed to Joseph-Louis Lagrange, while the proof of differentiability of \"g\" is due to Paul du Bois-Reymond.\n\nThe given functions (\"f\", \"g\") may be discontinuous, provided that they are locally integrable (on the given interval). In this case, Lebesgue integration is meant, the conclusions hold almost everywhere (thus, in all continuity points), and differentiability of \"g\" is interpreted as local absolute continuity (rather than continuous differentiability). Sometimes the given functions are assumed to be piecewise continuous, in which case Riemann integration suffices, and the conclusions are stated everywhere except the finite set of discontinuity points.\n\nThis necessary condition is also sufficient, since the integrand becomes formula_21\n\nThe case \"n\" = 1 is just the version for two given functions, since formula_22 and formula_23 thus, formula_24\n\nIn contrast, the case \"n\"=2 does not lead to the relation formula_25 since the function formula_26 need not be differentiable twice. The sufficient condition formula_27 is not necessary. Rather, the necessary and sufficient condition may be written as formula_28 for \"n\"=2, formula_29 for \"n\"=3, and so on; in general, the brackets cannot be opened because of non-differentiability.\n\nGeneralization to vector-valued functions formula_30 is straightforward; one applies the results for scalar functions to each coordinate separately, or treats the vector-valued case from the beginning.\n\nSimilarly to the basic version, one may consider a continuous function \"f\" on the closure of Ω, assuming that \"h\" vanishes on the boundary of Ω (rather than compactly supported).\n\nHere is a version for discontinuous multivariable functions.\n\nThis lemma is used to prove that extrema of the functional\nare weak solutions formula_37 (for an appropriate vector space formula_38) of the Euler–Lagrange equation\nThe Euler–Lagrange equation plays a prominent role in classical mechanics and differential geometry.\n\n"}
{"id": "3851416", "url": "https://en.wikipedia.org/wiki?curid=3851416", "title": "Generalized Helmholtz theorem", "text": "Generalized Helmholtz theorem\n\nThe generalized Helmholtz theorem is the multi-dimensional generalization of the Helmholtz theorem which is valid only in one dimension. The generalized Helmholtz theorem reads as follows.\n\nLet\nbe the canonical coordinates of a \"s\"-dimensional Hamiltonian system, and let\n\nbe the Hamiltonian function, where\n\nis the kinetic energy and\n\nis the potential energy which depends on a parameter formula_6.\nLet the hyper-surfaces of constant energy in the 2\"s\"-dimensional phase space of the system be metrically indecomposable and let formula_7 denote time average. Define the quantities formula_8, formula_9, formula_10, formula_11, as follows:\n\nThen:\n\nThe thesis of this theorem of classical mechanics reads exactly as the heat theorem of thermodynamics. This fact shows that thermodynamic-like relations exist between certain mechanical quantities in multidimensional ergodic systems. This in turn allows to define the \"thermodynamic state\" of a multi-dimensional ergodic mechanical system, without the requirement that the system be composed of a large number of degrees of freedom. In particular the temperature formula_10 is given by twice the time average of the kinetic energy per degree of freedom, and the entropy formula_11 by the logarithm of the phase space volume enclosed by the constant energy surface (i.e. the so-called volume entropy).\n\n"}
{"id": "37506968", "url": "https://en.wikipedia.org/wiki?curid=37506968", "title": "Guidelines for Assessment and Instruction in Statistics Education", "text": "Guidelines for Assessment and Instruction in Statistics Education\n\nThe Guidelines for Assessment and Instruction in Statistics Education (GAISE) are a framework for statistics education in grades Pre-K–12 published by the American Statistical Association (ASA) in 2007. The foundations for this framework are the Principles and Standards for School Mathematics published by the National Council of Teachers of Mathematics (NCTM) in 2000. A second report focused on statistics education at the collegiate level, the GAISE College Report, was published in 2005. Both reports were endorsed by the ASA. Several grants awarded by the National Science Foundation explicitly reference the GAISE documents as influencing or guiding the projects, and several popular introductory statistics textbooks have cited the GAISE documents as informing their approach.\n\nThe GAISE document provides a two-dimensional framework, specifying four components used in statistical problem solving (formulating questions, collecting data, analyzing data, and interpreting results) and three levels of conceptual understanding through which a student should progress (Levels A, B, and C). A direct parallel between these conceptual levels and grade levels is not made because most students would begin at Level A when they are first exposed to statistics regardless of whether they are in primary, middle, or secondary school. A student's level of statistical maturity is based on experience rather than age.\n\nThe GAISE College Report begins by synthesizing the history and current understanding of introductory statistics courses and then lists goals for students based on statistical literacy. Six recommendations for introductory statistics courses are given, namely:\nExamples and suggestions for how these recommendations could be implemented are included in several appendices.\n\n"}
{"id": "7951427", "url": "https://en.wikipedia.org/wiki?curid=7951427", "title": "Higher spin alternating sign matrix", "text": "Higher spin alternating sign matrix\n\nIn mathematics, a higher spin alternating sign matrix is a generalisation of the alternating sign matrix (ASM), where the columns and rows sum to an integer \"r\" (the \"spin\") rather than simply summing to 1 as in the usual alternating sign matrix definition. HSASMs are square matrices whose elements may be integers in the range −\"r\" to +\"r\". When traversing any row or column of an ASM or HSASM, the partial sum of its entries must always be non-negative.\n\nHigh spin ASMs have found application in statistical mechanics and physics, where they have been found to represent symmetry groups in ice crystal formation.\n\nSome typical examples of HSASMs are shown below:\n\nThe set of HSASMs is a superset of the ASMs. The extreme points of the convex hull of the set of \"r\"-spin HSASMs are themselves integer multiples of the usual ASMs.\n"}
{"id": "4160992", "url": "https://en.wikipedia.org/wiki?curid=4160992", "title": "Invariant-based programming", "text": "Invariant-based programming\n\nInvariant-based programming is a programming methodology where specifications and invariants are written before the actual program statements. Writing down the invariants during the programming process has a number of advantages: it requires the programmer to make their intentions about the program behavior explicit before actually implementing it, and invariants can be evaluated dynamically during execution to catch common programming errors. Furthermore, if strong enough, invariants can be used to prove the correctness of the program based on the formal semantics of program statements. A combined programming and specification language, connected to a powerful formal proof system, will generally be required for full verification of non-trivial programs. In this case a high degree of automation of proofs is also possible.\n\nIn most existing programming languages the main organizing structures are control flow blocks such as for loops, while loops and if statements. Such languages may not be ideal for invariants-first programming, since they force the programmer to make decisions about control flow before writing the invariants. Furthermore, most programming languages do not have good support for writing specifications and invariants, since they lack quantifier operators and one can typically not express higher order properties.\n\nThe idea of developing the program together with its proof originated from E.W. Dijkstra. Actually writing invariants before program statements has been considered in a number of different forms by M.H. van Emden, J.C. Reynolds and R-J Back.\n\n"}
{"id": "54798502", "url": "https://en.wikipedia.org/wiki?curid=54798502", "title": "Jerry M. Chow", "text": "Jerry M. Chow\n\nJerry M. Chow is a physicist who conducts research in quantum information processing. He has worked as the manager of the Experimental Quantum Computing group at the IBM Thomas J. Watson Research Center in Yorktown Heights, New York since 2014 and is the primary investigator of the IBM team for the IARPA \"Multi-Qubit Coherent Operations\" and \"Logical Qubits\" programs. After graduating magna cum laude with a B.A. in physics and M.S. in applied mathematics from Harvard University, he went on to earn his Ph.D. in 2010 under Robert J. Schoelkopf at Yale University. While at Yale, he participated in experiments in which superconducting qubits were coupled via a cavity bus for the first time and two-qubit algorithms were executed on a superconducting quantum processor.\n\nHis work at IBM has led to the publication of findings related to the characterization of a universal set of all-microwave gates that can be executed on two transmon qubits, as well as the implementation of a subsection of a surface code fault-tolerant superconducting quantum computing architecture. His leadership at IBM has led to progress being made in quantum error correction and quantum machine learning, as well as the release of the cloud-based IBM Quantum Experience.\n\nJerry grew up in Sheepshead Bay neighborhood of Brooklyn.\n\n"}
{"id": "19900560", "url": "https://en.wikipedia.org/wiki?curid=19900560", "title": "János Pach", "text": "János Pach\n\nJános Pach (born May 3, 1954) is a mathematician and computer scientist working in the fields of combinatorics and discrete and computational geometry.\n\nPach was born and grew up in Hungary. He comes from a noted academic family: his father, was a well known historian, and his uncle Pál Turán was one of the best known Hungarian mathematicians.\n\nPach received his Candidate degree from the Hungarian Academy of Sciences, in 1983, where his advisor was Miklós Simonovits.\n\nSince 1977, he has been affiliated with the Alfréd Rényi Institute of Mathematics of the Hungarian Academy of Sciences.\n\nHe was Research Professor at the Courant Institute of Mathematical Sciences at NYU (since 1986), Distinguished Professor of Computer Science at City College, CUNY (1992-2011), and Neilson Professor at Smith College (2008-2009).\n\nIn 2008, he joined École Polytechnique Fédérale de Lausanne as Professor of Mathematics.\n\nHe was the program chair for the International Symposium on Graph Drawing in 2004 and\nSymposium on Computational Geometry in 2015. With Kenneth L. Clarkson and Günter Ziegler, he is co-editor-in-chief of the journal \"Discrete and Computational Geometry\", and he serves on the editorial boards of several other journals including \"Combinatorica\", \"SIAM Journal on Discrete Mathematics\", \"Computational Geometry\", \"Graphs and Combinatorics\", \"Central European Journal of Mathematics\", and \"Moscow Journal of Combinatorics and Number Theory\".\n\nHe was an invited speaker at the Combinatorics session of the International Congress of Mathematicians, in Seoul, 2014.\n\nPach has authored several books and over 200 research papers. He was one of the most frequent collaborators of Paul Erdős, authoring over 20 papers with him and thus has an Erdős number of one.\n\nPach's research is focused in the areas of combinatorics and discrete geometry.\nIn 1981, he solved Ulam's problem, showing that there exists\nno universal planar graph.\nIn the early 90s\ntogether with Micha Perles, he initiated the systematic study of extremal problems on topological and\ngeometric graphs.\n\nSome of Pach's most-cited research work concerns the combinatorial complexity of families of curves in the plane and their applications to motion planning problems the maximum number of k-sets and halving lines that a planar point set may have, crossing numbers of graphs, embedding of planar graphs onto fixed sets of points, and lower bounds for epsilon-nets.\n\nPach received the Grünwald Medal of the János Bolyai Mathematical Society (1982), the Ford Award from the Mathematical Association of America (1990), and the Alfréd Rényi Prize from the Hungarian Academy of Sciences (1992). He was an Erdős Lecturer at Hebrew University of Jerusalem in 2005. \nIn 2011 he was listed as a fellow of the Association for Computing Machinery for his research in computational geometry.\nIn 2014 he was elected as a member of Academia Europaea, and in 2015 as a fellow of the American Mathematical Society \"for contributions to discrete and combinatorial geometry and to convexity and combinatorics.\"\n\n\n\n"}
{"id": "31662097", "url": "https://en.wikipedia.org/wiki?curid=31662097", "title": "Kepert model", "text": "Kepert model\n\nThe Kepert model is a modification of VSEPR theory used to predict the 3-dimensional structures of transitional metal complexes. In the Kepert model, the ligands attached to the metal are considered to repel each other the same way that point charges repel each other in VSEPR theory. Unlike VSEPR theory, the Kepert model does not account for non-bonding electrons. Therefore, the geometry of the coordination complex is independent of the electronic configuration of the metal center. Thus [ML] has the same coordination geometry as [ML].\nThe Kepert model cannot explain the formation of square planar complexes or distorted structures.\n\nThe Kepert model predicts the following geometries for coordination numbers of 2 through 6:\n"}
{"id": "38017451", "url": "https://en.wikipedia.org/wiki?curid=38017451", "title": "Leone Burton", "text": "Leone Burton\n\nLeone Minna Burton (\"née\" Gold; 14 September 1936  - 1 December 2007) was a professor of education in mathematics and science at the University of Birmingham.\n\nShe attended the University of London studying education with a focus on teaching mathematics. She began teaching at the secondary level but disliked the educational structure and methods of the day and moved to teaching at the primary level to have a greater opportunity to change teaching structure. She received a postgraduate certificate in education in 1966 and an academic diploma in education in 1968, both from the University of London. In 1967 she obtained a post as a mathematical education specialist at Battersea College of Education, which is dedicated to teacher preparation. She received her PHD from the Institute of Education in 1980.\n\nHer research has included what is now termed the field of ethnomathematics which examines how mathematics is related to the culture in which it is developed,\n\nShe has been visiting professor at institutions in Asia. For 1984-1988 she was international convenor for the International Organization of Women and Mathematics Education. She founded the monograph series International Perspective on Mathematics Education with the Greenwood Publishing group in 2001 which published three monographs between 2002-2006. This monograph series was subsequently renamed in honor of her pioneering work on equity and gender issues in mathematics education, edited by Bharath Sriraman and Donna Ford, and published by Information Age Publishing.\n\nShe married John W. Burton in 1966 and they had a son in 1968.\n\n"}
{"id": "567667", "url": "https://en.wikipedia.org/wiki?curid=567667", "title": "Lexicographical order", "text": "Lexicographical order\n\nIn mathematics, the lexicographic or lexicographical order (also known as lexical order, dictionary order, alphabetical order or lexicographic(al) product) is a generalization of the way words are alphabetically ordered based on the alphabetical order of their component letters. This generalization consists primarily in defining a total order over the sequences (often called strings in computer science) of elements of a finite totally ordered set, often called an alphabet.\n\nThere are several variants and generalizations of the lexicographical ordering. One variant widely used in combinatorics orders subsets of a given finite set by assigning a total order to the finite set, and converting subsets into increasing sequences, to which the lexicographical order is applied. Another generalization defines an order on a Cartesian product of partially ordered sets; this order is a total order if and only if the factors of the Cartesian product are totally ordered.\n\nThe word \"lexicographic\" is derived from lexicon, the set of words that are used in some language and appear in dictionaries and encyclopedias. The lexicographic order has thus been introduced for sorting the entries of dictionaries and encyclopedias. This has been formalized in the following way.\n\nConsider a finite set , often called alphabet, which is totally ordered. In dictionaries, this is the common alphabet, ordered by the alphabetical order. In book indexes, the alphabet is generally extended to all alphanumeric characters; it is the object of a specific convention whether a digit is considered as smaller or larger than a letter. The lexicographic order is a total order on the sequences of elements of , often called words on , which is defined as follows.\n\nGiven two different sequences of the same length, and , the first one is smaller than the second one for the lexicographical order, if (for the order of ), for the first where and differ.\n\nTo compare sequences of different lengths, the shorter sequence is usually padded at the end with enough \"blanks\" (a special symbol that is treated as smaller than every element of ). This way of comparing sequences of different lengths is always used in dictionaries. However, in combinatorics, another convention is frequently used, whereby a shorter sequence is always smaller than a longer sequence. This variant of the lexicographical order is sometimes called \"shortlex order\".\n\nIn dictionary order, the word \"Thomas\" appears before \"Thompson\" because the letter 'a' comes before the letter 'p' in the alphabet. The 5th letter is the first that is different in the two words; the first four letters are \"Thom\" in both. Because it is the first difference, the 5th letter is the most significant difference for the alphabetical ordering.\n\nAn important property of the lexicographical order on words of a fixed length on a finite alphabet is that it is a well-order; that is, every decreasing sequence of words is finite.\n\nThe lexicographical order is used not only in dictionaries, but also commonly for numbers and dates.\n\nOne of the drawbacks of the Roman numeral system is that it is not always immediately obvious which of two numbers is the smaller. On the other hand, with the positional notation of the Hindu–Arabic numeral system, comparing numbers is easy, because the natural order on nonnegative integers is the same as the variant shortlex of the lexicographic order. In fact, with positional notation, a nonnegative integer is represented by a sequence of numerical digits, and an integer is larger than another one if either it has more digits (ignoring leading zeroes) or the number of digits is the same and the first digit which differs is larger.\n\nFor real numbers written in decimal notation, a slightly different variant of the lexicographical order is used: the parts on the left of the decimal point are compared as before; if they are equal, the parts at the right of the decimal point are compared with the lexicographical order.\n\nWhen negative numbers are also considered, one has to reverse the order for comparing negative numbers. This is not usually a problem for humans, but it may be for computers (testing the sign takes some time). This is one of the reasons for adopting two's complement representation for representing signed integers in computers.\n\nAnother example of a non-dictionary use of lexicographical ordering appears in the ISO 8601 standard for dates, which expresses a date as YYYY-MM-DD. This formatting scheme has the advantage that the lexicographical order on sequences of characters that represent dates coincides with the chronological order: an earlier date is smaller in the lexicographical order than a later date. This date ordering makes computerized sorting of dates easier by avoiding the need for a separate sorting algorithm.\n\nThe \"monoid of words\" over an alphabet is the free monoid over . That is, the elements of the monoid are the finite sequences (words) of elements of (including the empty sequence, of length 0), and the operation (multiplication) is the concatenation of words. A word is a prefix of another word if there exists a word such that .\n\nWith this terminology, the above definition of the lexicographical order becomes more concise: Given a partially or totally ordered set , and two words and over , then one has for the lexicographical order, if \n\n\nIf < is a total order on \"A\", then so is the lexicographic order on the words of . However, in general this is not a well-order, even if the alphabet is well-ordered. For instance, if , the language has no least element in the lexicographical order: .\n\nSince many applications require well orders, a variant of the lexicographical orders is often used. This well-order, sometimes called \"shortlex\" or \"quasi-lexicographical order\", consists in considering first the lengths of the words (if , then ), and, if the lengths are equal, using the lexicographical order. If the order on is a well-order, the same is true for the shortlex order.\n\nThe lexicographical order defines an order on a Cartesian product of ordered sets, which is a total order when all these sets are themselves totally ordered. In fact, an element of a Cartesian product is a sequence whose th element belongs to for every . As evaluating the lexicographical order of sequences compares only elements which have the same rank in the sequences, the lexicographical order extends to Cartesian products of ordered sets.\n\nSpecifically, given two partially ordered sets and , the lexicographical order on the Cartesian product is defined as\n\nThe result is a partial order. If and are each totally ordered, then the result is a total order as well. The lexicographical order of two totally ordered sets is thus a linear extension of their product order.\n\nOne can define similarly the lexicographic order on the Cartesian product of an infinite family of ordered sets, if the family is indexed by the nonnegative integers or, more generally by a well-ordered set. This generalized lexicographical order is a total order, if each factor set is totally ordered.\n\nUnlike the finite case, an infinite product of well-orders is not necessarily well-ordered by the lexicographical order. For instance, the set of countably infinite binary sequences (by definition, the set of functions from non-negative integers to , also known as the Cantor space ) is not well-ordered; the subset of sequences that have precisely one (i.e. ) does not have a least element under the lexicographical order induced by , because is an infinite descending chain. Similarly, the infinite lexicographic product is not Noetherian either because is an infinite ascending chain.\n\nThe functions from a well-ordered set to a totally ordered set may be identified with sequences indexed by of elements of . They can thus be ordered by the lexicographical order, and for two such functions and , the lexicographical order is thus determined by their values for the smallest such that .\n\nIf is also well-ordered and is finite, then the resulting order is a well-order. As shown above, if is infinite this is not the case.\n\nIn combinatorics, one has often to enumerate, and therefore to order the finite subsets of a given set . For this, one usually chooses an order on . Then, sorting a subset of is equivalent to convert it into an increasing sequence. The lexicographic order on the resulting sequences induces thus an order on the subsets, which is also called the \"lexicographical order\".\n\nIn this context, one generally prefer to sort first the subsets by cardinality, such as in the shortlex order. Therefore, in the following, we will consider only orders on subsets of fixed cardinal.\n\nFor example, using the natural order of the integers, the lexicographical ordering on the subsets of three elements of is\n\nFor ordering finite subsets of a given cardinality of the natural numbers, the \"colexicographical\" order (see below) is often more convenient, because all initial segments are finite, and thus the colexicographical order defines an order isomorphism between the natural numbers and the set of sets of natural numbers. This is not the case for the lexicographical order, as, with the lexicographical order, we have, for example, for every .\n\nLet formula_1 be the free Abelian group of rank , whose elements are sequences of integers, and operation is the addition. A group order on formula_1 is a total order, which is compatible with addition, that is \n\nThe lexicographical ordering may also be used to characterize all group orders on formula_5 In fact, linear forms with real coefficients, define a map from formula_1 into formula_7 which is injective if the forms are linearly independent (it may be also injective if the forms are dependent, see below). The lexicographic order on the image of this map induces a group order on formula_5 Robbiano's theorem is that every group order may be obtained in this way.\n\nMore precisely, given a group order on formula_9 there exist an integer and linear forms with real coefficients, such that the induced map formula_10 from formula_1 into formula_12 has the following properties;\n\n\nThe colexicographic or colex order is a variant of the lexicographical order, which is obtained by reading finite sequences from the right to the left instead of reading them from the left to the right. More precisely, while the lexicographical order between two sequences is defined by\n\nthe colexicographical order is defined by\n\nIn general, the difference between the colexicographical order and the lexicographical order is not very significant. However, when considering increasing sequences, typically for coding subsets, the two orders differ significantly.\n\nFor example, for ordering the increasing sequences (or the sets) of two natural integers, the lexicographical order begins by\n\nand the colexicographic order begins by\n\nThe main property of the colexicographical order for increasing sequences of a given length is that every initial segment is finite. In other words, the colexicographical order for increasing sequences of a given length induces an order isomorphism with the natural numbers, and allows enumerating these sequences. This is frequently used in combinatorics, for example in the proof of the Kruskal-Katona theorem.\n\nWhen considering polynomials, the order of the terms does not matter in general, as the addition is commutative. However, some algorithms, such as polynomial long division require a specific order of the terms. Many of the main algorithms for multivariate polynomials are related with Gröbner bases, concept that requires the choice of a monomial order, that is a total order, which is compatible with the monoid structure of the monomials. Here \"compatible\" means that formula_17) with their exponent vectors (here ). If is the number of variables, every monomial order is thus the restriction to formula_18 of a monomial order of formula_1 (see above formula_1, for a classification).\n\nOne of these admissible orders is the lexicographical order. It is, historically, the first to have been used for defining Gröbner bases, and is sometimes called \"pure lexicographical order\" for distinguishing it from other orders that are also related to a lexicographical order.\n\nAnother one consists in comparing first the total degrees, and then resolving the conflicts by using the lexicographical order. This order is not widely used, as either the lexicographical order or the degree reverse lexicographical order have generally better properties.\n\nThe \"degree reverse lexicographical order\" consists also in comparing first the total degrees, and, in case of equality of the total degrees, using the reverse of the colexicographical order. That is, given two exponent vectors, one has \n\nif either \n\nor\n\nFor this ordering, the monomials of degree one have the same order as the corresponding indeterminates (this would not be the case if the reverse lexicographical order would be used). For comparing monomials in two variables of the same total degree, this order is the same as the lexicographic order. This is not the case with more variables. For example, for exponent vectors of monomials of degree two in three variables, one has for the degree reverse lexicographic order:\n\nFor the lexicographical order, the same exponent vectors are ordered as\n\nA useful property of the degree reverse lexicographical order is that a homogeneous polynomial is a multiple of the least indeterminate if and only if its leading monomial (its greater monomial) is a multiple of this least indeterminate.\n\n"}
{"id": "31224296", "url": "https://en.wikipedia.org/wiki?curid=31224296", "title": "Liouville field theory", "text": "Liouville field theory\n\nIn physics, Liouville field theory (or simply Liouville theory) is a two-dimensional conformal field theory whose classical equation of motion is a generalization of Liouville's equation.\n\nLiouville theory is defined for all complex values of the central charge formula_1 of its Virasoro symmetry algebra, but it is unitary only if \n\nand its classical limit is \n\nAlthough it is an interacting theory with a continuous spectrum, Liouville theory has been solved. In particular, its three-point function on the sphere has been determined analytically. \n\nLiouville theory has a background charge formula_4 and coupling constant formula_5 that are related to the central charge formula_1 by\n\nStates and fields are characterized by a momentum formula_8 that is related to the conformal dimension formula_9 by\n\nThe coupling constant and the momentum are the natural parameters for writing correlation functions in Liouville theory. However, the duality\nleaves the central charge invariant, and therefore also leaves the correlation functions invariant. The conformal dimension is invariant under the reflection transformation\nand the correlation functions are covariant under reflection.\n\nThe spectrum formula_13 of Liouville theory is a diagonal combination of Verma modules of the Virasoro algebra,\nwhere formula_15 and formula_16 denote the same Verma module, viewed as a representation of the left- and right-moving Virasoro algebra respectively. In terms of momentums, \ncorresponds to \n\nLiouville theory is unitary if and only if formula_19. The spectrum of Liouville theory does not include a vacuum state. A vacuum state can be defined, but it does not contribute to operator product expansions.\n\nIn Liouville theory, primary fields are usually parametrized by their momentum rather than their conformal dimension, and denoted formula_20.\nBoth fields formula_20 and formula_22 correspond to the primary state of the representation formula_23, and are related by the reflection relation\nwhere the reflection coefficient is\n\nFor formula_30, the three-point structure constant is given by the DOZZ formula (for Dorn-Otto and Zamolodchikov-Zamolodchikov), \nwhere the special function formula_32 is a kind of multiple gamma function.\n\nFor formula_33, the three-point structure constant is\nwhere \nformula_36-point functions on the sphere can be expressed in terms of three-point structure constants, and conformal blocks. An formula_36-point function may have several different expressions: that they agree is equivalent to crossing symmetry of the four-point function, which has been checked numerically and proved analytically.\n\nLiouville theory exists not only on the sphere, but also on any Riemann surface of genus formula_38. Technically, this is equivalent to the modular invariance of the torus one-point function. Due to remarkable identities of conformal blocks and structure constants, this modular invariance property can be deduced from crossing symmetry of the sphere four-point function.\n\nUsing the conformal bootstrap approach, Liouville theory can be shown to be the unique conformal field theory such that\n\nLiouville theory is defined by the local action\nwhere formula_41 is the metric of the two-dimensional space on which the theory is formulated, formula_42 is the Ricci scalar of that space, and the field formula_43 is called the Liouville field. The parameter formula_44, which is sometimes called the cosmological constant, is related to the parameter formula_29 that appears in correlation functions by \n\nThe equation of motion associated to this action is\nwhere formula_48 is the Laplace–Beltrami operator. If formula_41 is the Euclidean metric, this equation reduces to\nwhich is equivalent to Liouville's equation.\n\nUsing a complex coordinate system formula_51 and a Euclidean metric \nthe energy-momentum tensor's components obey\nThe non-vanishing components are\nEach one of these two components generates a Virasoro algebra with the central charge \n\nFor both of these Virasoro algebras, a field formula_56 is a primary field with the conformal dimension \n\nFor the theory to have conformal invariance, the field formula_58 that appears in the action must be marginal, i.e. have the conformal dimension \n\nThis leads to the relation \n\nbetween the background charge and the coupling constant. If this relation is obeyed, then formula_58 is actually exactly marginal, and the theory is conformally invariant.\n\nThe path integral representation of an formula_36-point correlation function of primary fields is \nIt has been difficult to define and to compute this path integral. In the path integral representation, it is not obvious that Liouville theory has exact conformal invariance, and it is not manifest that correlation functions are invariant under formula_64 and obey the reflection relation. Nevertheless, the path integral representation can be used for computing the residues of correlation functions at some of their poles as Dotsenko-Fateev integrals (i.e. Coulomb gas integrals), and this is how the DOZZ formula was first guessed in the 1990s. It is only in the 2010s that a rigorous probabilistic construction of the path integral was found, which led to a proof of the DOZZ formula.\n\nWhen the central charge and conformal dimensions are sent to the relevant discrete values, correlation functions of Liouville theory reduce to correlation functions of diagonal (A-series) Virasoro minimal models.\n\nOn the other hand, when the central charge is sent to one while conformal dimensions stay continuous, Liouville theory tends to Runkel-Watts theory, a nontrivial conformal field theory (CFT) with a continuous spectrum whose three-point function is not analytic as a function of the momentums. Generalizations of Runkel-Watts theory are obtained from Liouville theory by taking limits of the type formula_65. So, for formula_66, two distinct CFTs with the same spectrum are known: Liouville theory, whose three-point function is analytic, and another CFT with a non-analytic three-point function.\n\nLiouville theory can be obtained from the formula_67 Wess–Zumino–Witten model by a quantum Drinfeld-Sokolov reduction. Moreover, correlation functions of the formula_68 model (the Euclidean version of the formula_67 WZW model) can be expressed in terms of correlation functions of Liouville theory. This is also true of correlation functions of the 2d black hole formula_70 coset model. Moreover, there exist theories that continuously interpolate between Liouville theory and the formula_68 model.\n\nLiouville theory is the simplest example of a Toda field theory, associated to the formula_72 Cartan matrix. More general conformal Toda theories can be viewed as generalizations of Liouville theory, whose Lagrangians involve several bosons rather than one boson formula_73, and whose symmetry algebras are W-algebras rather than the Virasoro algebra.\n\nLiouville theory admits two different supersymmetric extensions called formula_74 supersymmetric Liouville theory and formula_75 supersymmetric Liouville theory. \n\nIn two dimensions, the Einstein equations reduce to Liouville's equation, so Liouville theory provides a quantum theory of gravity that is called Liouville gravity. It should not be confused with the CGHS model or Jackiw–Teitelboim gravity.\n\nLiouville theory appears in the context of string theory when trying to formulate a non-critical version of the theory in the path integral formulation. Also in the string theory context, if coupled to a free bosonic field, Liouville field theory can be thought of as the theory describing string excitations in a two-dimensional space(time).\n\nLiouville theory is related to other subjects in physics and mathematics, such as three-dimensional general relativity in negatively curved spaces, the uniformization problem of Riemann surfaces, and other problems in conformal mapping. It is also related to instanton partition functions in a certain four-dimensional superconformal gauge theories by the AGT correspondence.\n\nLiouville theory with formula_76 first appeared as a model of time-dependent string theory under the name timelike Liouville theory.\nIt has also been called a generalized minimal model. It was first called Liouville theory when it was found to actually exist, and to be spacelike rather than timelike. As of 2018, not one of these three names is universally accepted.\n\n"}
{"id": "300568", "url": "https://en.wikipedia.org/wiki?curid=300568", "title": "Liskov substitution principle", "text": "Liskov substitution principle\n\nSubstitutability is a principle in object-oriented programming stating that, in a computer program, if S is a subtype of T, then objects of type T may be \"replaced\" with objects of type S (i.e. an object of type T may be \"substituted\" with any object of a subtype S) without altering any of the desirable properties of the program (correctness, task performed, etc.). More formally, the Liskov substitution principle (LSP) is a particular definition of a subtyping relation, called (strong) behavioral subtyping, that was initially introduced by Barbara Liskov in a 1987 conference keynote address titled \"Data abstraction and hierarchy\". It is a semantic rather than merely syntactic relation, because it intends to guarantee semantic interoperability of types in a hierarchy, object types in particular. Barbara Liskov and Jeannette Wing described the principle succinctly in a 1994 paper as follows:\n\"Subtype Requirement\": Let be a property provable about objects of type . Then should be true for objects of type where is a subtype of . \n\nIn the same paper, Liskov and Wing detailed their notion of behavioral subtyping in an extension of Hoare logic, which bears a certain resemblance to Bertrand Meyer's design by contract in that it considers the interaction of subtyping with preconditions, postconditions and invariants.\n\nLiskov's notion of a behavioural subtype defines a notion of substitutability for objects; that is, if \"S\" is a subtype of \"T\", then objects of type \"T\" in a program may be replaced with objects of type \"S\" without altering any of the desirable properties of that program (e.g. correctness).\n\nBehavioural subtyping is a stronger notion than typical subtyping of functions defined in type theory, which relies only on the contravariance of argument types and covariance of the return type. Behavioural subtyping is undecidable in general: if \"q\" is the property \"method for \"x\" always terminates\", then it is impossible for a program (e.g. a compiler) to verify that it holds true for some subtype \"S\" of \"T\", even if \"q\" does hold for \"T\". Nonetheless, the principle is useful in reasoning about the design of class hierarchies.\n\nLiskov's principle imposes some standard requirements on signatures that have been adopted in newer object-oriented programming languages (usually at the level of classes rather than types; see nominal vs. structural subtyping for the distinction):\n\n\nIn addition to the signature requirements, the subtype must meet a number of behavioural conditions. These are detailed in a terminology resembling that of design by contract methodology, leading to some restrictions on how contracts can interact with inheritance:\n\n\nThe rules on pre- and postconditions are identical to those introduced by Bertrand Meyer in his 1988 book \"Object-Oriented Software Construction\". Both Meyer, and later Pierre America, who was the first to use the term \"behavioral subtyping\", gave proof-theoretic definitions of some behavioral subtyping notions, but their definitions did not take into account aliasing that may occur in programming languages that support references or pointers. Taking aliasing into account was the major improvement made by Liskov and Wing (1994), and a key ingredient is the history constraint. Under the definitions of Meyer and America, a MutablePoint would be a behavioral subtype of ImmutablePoint, whereas LSP forbids this.\n\n\nGeneral references\n\n\nSpecific references\n\n"}
{"id": "11940501", "url": "https://en.wikipedia.org/wiki?curid=11940501", "title": "Macaulay brackets", "text": "Macaulay brackets\n\nMacaulay brackets are a notation used to describe the ramp function\n\nA popular alternative transcription uses angle brackets, \"viz.\" formula_2. \nAnother commonly used notation is formula_3 or formula_4 for the positive part of formula_3, which avoids conflicts with formula_6 for set notation.\n\nMacaulay's notation is commonly used in the static analysis of bending moments of a beam. This is useful because shear forces applied on a member render the shear and moment diagram discontinuous. Macaulay's notation also provides an easy way of integrating these discontinuous curves to give bending moments, angular deflection, and so on. For engineering purposes, angle brackets are often used to denote the use of Macaulay's method.\n\nThe above example simply states that the function takes the value formula_9 for all \"x\" values larger than \"a\". With this, all the forces acting on a beam can be added, with their respective points of action being the value of \"a\".\n\nA particular case is the unit step function,\n\n"}
{"id": "501758", "url": "https://en.wikipedia.org/wiki?curid=501758", "title": "Magnetocrystalline anisotropy", "text": "Magnetocrystalline anisotropy\n\nIn physics, a ferromagnetic material is said to have magnetocrystalline anisotropy if it takes more energy to magnetize it in certain directions than in others. These directions are usually related to the principal axes of its crystal lattice. It is a special case of magnetic anisotropy.\n\nThe spin-orbit interaction is the primary source of magnetocrystalline anisotropy. It is basically the orbital motion of the electrons which couples with crystal electric field giving rise to the first order contribution to magnetocrystalline anisotropy. The second order arises due to the mutual interaction of the magnetic dipoles.\n\nMagnetocrystalline anisotropy has a great influence on industrial uses of ferromagnetic materials. Materials with high magnetic anisotropy usually have high coercivity, that is, they are hard to demagnetize. These are called \"hard\" ferromagnetic materials and are used to make permanent magnets. For example, the high anisotropy of rare-earth metals is mainly responsible for the strength of rare-earth magnets. During manufacture of magnets, a powerful magnetic field aligns the microcrystalline grains of the metal such that their \"easy\" axes of magnetization all point in the same direction, freezing a strong magnetic field into the material.\n\nOn the other hand, materials with low magnetic anisotropy usually have low coercivity, their magnetization is easy to change. These are called \"soft\" ferromagnets and are used to make magnetic cores for transformers and inductors. The small energy required to turn the direction of magnetization minimizes core losses, energy dissipated in the transformer core when the alternating current changes direction.\n\nMagnetocrystalline anisotropy arises mostly from spin–orbit coupling. This effect is weak compared to the exchange interaction and is difficult to compute from first principles, although some successful computations have been made.\n\nThe magnetocrystalline anisotropy energy is generally represented as an expansion in powers of the direction cosines of the magnetization. The magnetization vector can be written , where is the saturation magnetization. Because of time reversal symmetry, only even powers of the cosines are allowed. The nonzero terms in the expansion depend on the crystal system (\"e.g.\", cubic or hexagonal). The \"order\" of a term in the expansion is the sum of all the exponents of magnetization components, \"i.e.\", is second order.\n\nMore than one kind of crystal system has a single axis of high symmetry (threefold, fourfold or sixfold). The anisotropy of such crystals is called \"uniaxial anisotropy\". If the axis is taken to be the main symmetry axis of the crystal, the lowest order term in the energy is\n\nThe ratio is an energy density (energy per unit volume). This can also be represented in spherical polar coordinates with , , and :\n\nThe parameter , often represented as , has units of energy density and depends on composition and temperature.\n\nThe minima in this energy with respect to satisfy\n\nIf ,\nthe directions of lowest energy are the directions. The axis is called the \"easy axis\". If , there is an \"easy plane\" perpendicular to the symmetry axis (the basal plane of the crystal).\n\nMany models of magnetization represent the anisotropy as uniaxial and ignore higher order terms. However, if , the lowest energy term does not determine the direction of the easy axes within the basal plane. For this, higher-order terms are needed, and these depend on the crystal system (hexagonal, tetragonal or rhombohedral).\n\nIn a hexagonal system the axis is an axis of sixfold rotation symmetry. The energy density is, to fourth \norder,\n\nThe uniaxial anisotropy is mainly determined by the first two terms. Depending on the values and , there are four different kinds of anisotropy (isotropic, easy axis, easy plane and easy cone):\n\n\nThe basal plane anisotropy is determined by the third term, which is sixth-order. The easy directions are projected onto three axes in the basal plane.\n\nBelow are some room-temperature anisotropy constants for hexagonal ferromagnets. Since all the values of and are positive, these materials have an easy axis.\nHigher order constants, in particular conditions, may lead to first order magnetization processes FOMP.\n\nThe energy density for a tetragonal crystal is\n\nNote that the term, the one that determines the basal plane anisotropy, is fourth order (same as the term). The definition of may vary by a constant multiple between publications.\n\nThe energy density for a rhombohedral crystal is\n\nIn a cubic crystal the lowest order terms in the energy are\n\nIf the second term can be neglected, the easy axes are the ⟨100⟩ axes (\"i.e.\", the , , and , directions) for and the ⟨111⟩ directions for (see images on right).\n\nIf is not assumed to be zero, the easy axes depend on both and . These are given in the table below, along with \"hard axes\" (directions of greatest energy) and \"intermediate axes\" (saddle points) in the energy). In energy surfaces like those on the right, the easy axes are analogous to valleys, the hard axes to peaks and the intermediate axes to mountain passes.\n\nBelow are some room-temperature anisotropy constants for cubic ferromagnets. The compounds involving are ferrites, an important class of ferromagnets. In general the anisotropy parameters for cubic ferromagnets are higher than those for uniaxial ferromagnets. This is consistent with the fact that the lowest order term in the expression for cubic anisotropy is fourth order, while that for uniaxial anisotropy is second order.\n\nThe magnetocrystalline anisotropy parameters have a strong dependence on temperature. They generally decrease rapidly as the temperature approaches the Curie temperature, so the crystal becomes effectively isotropic. Some materials also have an \"isotropic point\" at which . Magnetite (), a mineral of great importance to rock magnetism and paleomagnetism, has an isotropic point at 130 kelvin.\n\nMagnetite also has a phase transition at which the crystal symmetry changes from cubic (above) to monoclinic or possibly triclinic below. The temperature at which this occurs, called the Verwey temperature, is 120 Kelvin.\n\nThe magnetocrystalline anisotropy parameters are generally defined for ferromagnets that are constrained to remain undeformed as the direction of magnetization changes. However, coupling between the magnetization and the lattice does result in deformation, an effect called magnetostriction. To keep the lattice from deforming, a stress must be applied. If the crystal is not under stress, magnetostriction alters the effective magnetocrystalline anisotropy. If a ferromagnet is single domain (uniformly magnetized), the effect is to change the magnetocrystalline anisotropy parameters.\n\nIn practice, the correction is generally not large. In hexagonal crystals, there is no change in . In cubic crystals, there is a small change, as in the table below.\n\n"}
{"id": "401169", "url": "https://en.wikipedia.org/wiki?curid=401169", "title": "Model-driven architecture", "text": "Model-driven architecture\n\nModel-driven architecture (MDA) is a software design approach for the development of software systems. It provides a set of guidelines for the structuring of specifications, which are expressed as models. Model-driven architecture is a kind of domain engineering, and supports model-driven engineering of software systems. It was launched by the Object Management Group (OMG) in 2001.\n\nThen, given a platform model corresponding to CORBA, .NET, the Web, etc., the PIM is translated to one or more platform-specific models (PSMs) that computers can run. This requires mappings and transformations and should be modeled too.\n\nThe OMG organization provides rough specifications rather than implementations, often as answers to Requests for Proposals (RFPs). Implementations come from private companies or open source groups.\n\nThe MDA model is related to multiple standards, including the Unified Modeling Language (UML), the Meta-Object Facility (MOF), XML Metadata Interchange (XMI), Enterprise Distributed Object Computing (EDOC), the Software Process Engineering Metamodel (SPEM), and the Common Warehouse Metamodel (CWM). Note that the term “architecture” in Model-driven architecture does not refer to the architecture of the system being modeled, but rather to the architecture of the various standards and model forms that serve as the technology basis for MDA.\n\nExecutable UML was the UML profile used when MDA was born. Now, the OMG is promoting fUML, instead. (The action language for fUML is ALF.)\n\nThe Object Management Group holds registered trademarks on the term Model Driven Architecture and its acronym MDA, as well as trademarks for terms such as: Model Based Application Development, Model Driven Application Development, Model Based Application Development, Model Based Programming, Model Driven Systems, and others.\n\nOMG focuses Model-driven architecture on forward engineering, i.e. producing code from abstract, human-elaborated modelling diagrams (e.g. class diagrams). OMG's ADTF (Analysis and Design Task Force) group leads this effort. With some humour, the group chose ADM (MDA backwards) to name the study of reverse engineering. ADM decodes to Architecture-Driven Modernization. The objective of ADM is to produce standards for model-based reverse engineering of legacy systems. Knowledge Discovery Metamodel (KDM) is the furthest along of these efforts, and describes information systems in terms of various assets (programs, specifications, data, test files, database schemas, etc.).\n\nAs the concepts and technologies used to realize designs and the concepts and technologies used to realize architectures have changed at their own pace, decoupling them allows system developers to choose from the best and most fitting in both domains. The design addresses the functional (use case) requirements while architecture provides the infrastructure through which non-functional requirements like scalability, reliability and performance are realized. MDA envisages that the platform independent model (PIM), which represents a conceptual design realizing the functional requirements, will survive changes in realization technologies and software architectures.\n\nOf particular importance to model-driven architecture is the notion of model transformation. A specific standard language for model transformation has been defined by OMG called QVT.\n\nThe OMG organization provides rough specifications rather than implementations, often as answers to Requests for Proposals (RFPs). The OMG documents the overall process in a document called the MDA Guide.\n\nBasically, an MDA tool is a tool used to develop, interpret, compare, align, measure, verify, transform, etc. models or metamodels. In the following section \"model\" is interpreted as meaning any kind of model (e.g. a UML model) or metamodel (e.g. the CWM metamodel). In any MDA approach we have essentially two kinds of models: \"initial models\" are created manually by human agents while \"derived models\" are created automatically by programs. For example, an analyst may create a UML initial model from its observation of some loose business situation while a Java model may be automatically derived from this UML model by a Model transformation operation.\n\nAn MDA tool may be a tool used to check models for completeness, inconsistencies, or error and warning conditions. Also used to calculate metrics for the model.\n\nSome tools perform more than one of the functions listed above. For example, some creation tools may also have transformation and test capabilities. There are other tools that are solely for creation, solely for graphical presentation, solely for transformation, etc.\n\nImplementations of the OMG specifications come from private companies or open source groups. One important source of implementations for OMG specifications is the Eclipse Foundation (EF). Many implementations of OMG modeling standards may be found in the Eclipse Modeling Framework (EMF) or Graphical Modeling Framework (GMF), the Eclipse foundation is also developing other tools of various profiles as GMT. Eclipse's compliance to OMG specifications is often not strict. This is true for example for OMG's EMOF standard, which Eclipse approximates with its ECORE implementation. More examples may be found in the M2M project implementing the QVT standard or in the M2T project implementing the MOF2Text standard.\n\nOne should be careful not to confuse the \"List of MDA Tools\" and the List of UML tools, the former being much broader. This distinction can be made more general by distinguishing 'variable metamodel tools' and 'fixed metamodel tools'. A UML CASE tool is typically a 'fixed metamodel tool' since it has been hard-wired to work only with a given version of the UML metamodel (e.g. UML 2.1). On the contrary, other tools have internal generic capabilities allowing them to adapt to arbitrary metamodels or to a particular kind of metamodels.\n\nUsually MDA tools focus rudimentary architecture specification, although in some cases the tools are architecture-independent (or platform independent).\n\nSimple examples of architecture specifications include:\n\nSome key concepts that underpin the MDA approach (launched in 2001) were first elucidated by the Shlaer-Mellor method during the late 1980s. Indeed, a key absent technical standard of the MDA approach (that of an action language syntax for Executable UML) has been bridged by some vendors by adapting the original Shlaer-Mellor Action Language (modified for UML). However, during this period the MDA approach has not gained mainstream industry acceptance; with the Gartner Group still identifying MDA as an \"on the rise\" technology in its 2006 \"Hype Cycle\", and Forrester Research declaring MDA to be \"D.O.A.\" in 2006. Potential concerns that have been raised with the OMG MDA approach include:\n\nAmong the various conferences on this topic we may mention ECMDA, the European Conference on MDA and also MoDELS, former firmed as «UML» conference series (till 2004), the Italian Forum on MDA in collaboration with the OMG. There are also several conferences and workshops (at OOPSLA, ECOOP mainly) focusing on more specific aspects of MDA like model transformation, model composition, and generation.\n\nCode generation means that the user abstractly models solutions, which are connoted by some model data, and then an automated tool derives from the models parts or all of the source code for the software system. In some tools, the user can provide a skeleton of the program source code, in the form of a source code template where predefined tokens are then replaced with program source code parts during the code generation process.\n\nAn often cited criticism is that the UML diagrams just lack the detail which is needed to contain the same information as is covered with the program source. Some developers even claim that \"the Code \"is\" the design\".\n\n\n"}
{"id": "14650395", "url": "https://en.wikipedia.org/wiki?curid=14650395", "title": "Monadic second-order logic", "text": "Monadic second-order logic\n\nIn mathematical logic, monadic second order logic (MSO) is the fragment of second-order logic where the second-order quantification is limited to quantification over sets. It is particularly important in the logic of graphs, because of Courcelle's theorem, which provides algorithms for evaluating monadic second-order formulas over certain types of graphs.\n\nSecond-order logic allows quantification over predicates. However, MSO is the fragment in which second-order quantification is limited to monadic predicates (predicates having a single argument). This is often described as quantification over \"sets\" because monadic predicates are equivalent in expressive power to sets (the set of elements for which the predicate is true).\n\nExistential monadic second-order logic (EMSO) is the fragment of MSO in which all quantifiers over sets must be existential quantifiers, outside of any other part of the formula. The first-order quantifiers are not restricted. By analogy to Fagin's theorem, according to which existential (non-monadic) second-order logic captures precisely the descriptive complexity of the complexity class NP, the class of problems that may be expressed in existential monadic second-order logic has been called monadic NP. The restriction to monadic logic makes it possible to prove separations in this logic that remain unproven for non-monadic second-order logic. For instance, in the logic of graphs, testing whether a graph is disconnected belongs to monadic NP, as the test can be represented by a formula that describes the existence of a proper subset of vertices with no edges connecting them to the rest of the graph; however, the complementary problem, testing whether a graph is connected, does not belong to monadic NP. The existence of an analogous pair of complementary problems, only one of which has an existential second-order formula (without the restriction to monadic formulas) is equivalent to the inequality of NP and coNP, an open question in computational complexity.\n\nThe monadic second order theory of the infinite complete binary tree, called S2S, is decidable. As a consequence of this result, the following theories are decidable:\n\nFor each of these theories (S2S, S1S, wS2S, wS1S), the complexity of the decision problem is nonelementary.\n"}
{"id": "4737424", "url": "https://en.wikipedia.org/wiki?curid=4737424", "title": "NAR 2", "text": "NAR 2\n\nNAR 2 (Serbian Nastavni Računar 2, en. \"Educational Computer\" 2) is a theoretical model of a 32-bit word computer created by Faculty of Mathematics of University of Belgrade professor Nedeljko Parezanović as an enhancement to its predecessor, NAR 1. It was used for Assembly language and Computer architecture courses. The word \"nar\" means Pomegranate in Serbian. Many NAR 2 simulators have been created — for instance, one was named \"Šljiva\" (en. \"plum\") as that fruit grows in Serbia, while \"nar\" does not.\n\nThe NAR 2 processor uses 32-bit machine words. Each Machine instruction contains:\n\nNAR 2 has four registers:\n\n\nFollowing opcodes were available (actual codes were not specified, only mnemonics):\n\n\n\"Note: all mnemonincs in this group end with letter \"F\" indicating \"Fiksni zarez\" (en. Fixed point) arithmetic. However, this is only true for addition, subtraction and negation (sign change). Multiplication and division assume that the \"point\" is fixed to the right of least significant bit - that is that the numbers are integer.\"\n\n\n\n\nNote: above operations are all bitwise. Their names imply that they are purely logical operations but they can be explained as if they operate on vectors of bits and separately apply logical operations on each pair of bits.\n\n\n\nThe NAR 2 assembly language syntax was designed to be straightforward and easy to parse. Each program line may contain up to one instruction specified as follows:\n\n\nSample code:\n\nWith four address mode selection bits (P, R, I and N - indexed, relative, indirect and immediate), NAR 2 instructions can specify 16 different addressing modes but not all make sense in all instructions. In the following table:\n\n\nNote 1: \"N\" (immediate) flag has no effect on jump (flow control) instructions, as the processor can not jump into a specified value, but only to a memory address.\n\nNAR 2 supports multi-level memory indirect addressing mode. The location is first chosen by \"looking\" at P (indexed) and R (relative to program counter) flags. Then, if I (indirect) flag is detected, a 32-bit word is loaded from the memory location calculated so far and the calculation is restarted (including all addressing mode flags, index register selection and parameter value - only the \"opcode\" is omitted). Thus, the following program, if loaded at memory location 0 and executed:\n\nmua I, 0 ; Memory-Into-Accumulator, Indirect, from location 0\n\n... will freeze NAR 2 in an infinite address calculation loop:\n\n\nNote that:\n\nmua R, I, 0 ; Memory-Into-Accumulator, Relative, Indirect, from location BN+0\n\nseems more generic (could freeze NAR 2 from any location), but this depends on when BN register value is incremented/changed.\n\nThe question of treatment of \"N\" (immediate) flag in presence of I (indirect) flag is open as the situation is somewhat ambiguous—that is, whether or not to honour the flag value specified in the original instruction or the one in the indirectly specified (looked up) address leads to a conflict. The table above presents the first case to show different addressing modes achievable this way.\n\nNAR 2 has instructions to initialize the value of particular index register (\"PIR\" mnemonic). However, it does not have special instructions to read values index registers. This is achieved by using indexed and immediate (P, N) addressing mode flags, such as:\n\nmua Xi, P, N, n ; Memory-Into-Accumulator, Indexed, Immediate, 0\n\n... which essentially puts Xi+n into accumulator. For n=0, this turns into a \"load index register value into accumulator\" instruction.\n\n\n"}
{"id": "6042215", "url": "https://en.wikipedia.org/wiki?curid=6042215", "title": "Nathan Fine", "text": "Nathan Fine\n\nNathan Jacob Fine (22 October 1916 in Philadelphia – 18 November 1994 in Deerfield Beach, Florida) was a mathematician who worked on basic hypergeometric series. He is best known for his lecture notes on the subject which for four decades served as an inspiration to experts in the field until they were finally published as a book. He solved the Jeep problem in 1946.\n\nNathan Fine retired in 1978 as a professor at Pennsylvania State University. Prior to that he worked as a faculty at the University of Pennsylvania and Cornell University. For a brief period (1946–1947) he also worked at the Operations Evaluation Group, affiliated with the Massachusetts Institute of Technology. Beside the book he published about 40 papers in several fields of mathematics. \n\nNathan Fine received his Ph.D. in 1946 from University of Pennsylvania, where he was a student of Antoni Zygmund. Fine's doctoral students include J. J. Price.\n\nHe wrote the book \"Basic Hypergeometric Series and Applications\" .\n\n"}
{"id": "27641107", "url": "https://en.wikipedia.org/wiki?curid=27641107", "title": "Nine-dimensional space", "text": "Nine-dimensional space\n\nIn mathematics, a sequence of \"n\" real numbers can be understood as a point in \"n\"-dimensional space. When \"n\" = 9, the set of all such locations is called 9-dimensional space. Often such spaces are studied as vector spaces, without any notion of distance. Nine-dimensional Euclidean space is nine-dimensional space equipped with a Euclidean metric, which is defined by the dot product.\n\nMore generally, the term may refer to a nine-dimensional vector space over any field, such as a nine-dimensional complex vector space, which has 18 real dimensions. It may also refer to a nine-dimensional manifold such as a 9-sphere, or any of a variety of other geometric constructions.\n\nA polytope in nine dimensions is called a 9-polytope. The most studied are the regular polytopes, of which there are only three in nine dimensions: the 9-simplex, 9-cube, and 9-orthoplex. A broader family are the uniform 9-polytopes, constructed from fundamental symmetry domains of reflection, each domain defined by a Coxeter group. Each uniform polytope is defined by a ringed Coxeter-Dynkin diagram. The 9-demicube is a unique polytope from the D family.\n"}
{"id": "293504", "url": "https://en.wikipedia.org/wiki?curid=293504", "title": "Orbifold", "text": "Orbifold\n\nIn the mathematical disciplines of topology, geometry, and geometric group theory, an orbifold (for \"orbit-manifold\") is a generalization of a manifold. It is a topological space (called the \"underlying space\") with an orbifold structure (see below).\n\nThe underlying space locally looks like the quotient space of a Euclidean space under the linear action of a finite group. Definitions of orbifold have been given several times: by Satake in the context of automorphic forms in the 1950s under the name \"V-manifold\"; by Thurston in the context of the geometry of 3-manifolds in the 1970s when he coined the name \"orbifold\", after a vote by his students; and by Haefliger in the 1980s in the context of Gromov's programme on CAT(k) spaces under the name \"orbihedron\". The definition of Thurston will be described here: it is the most widely used and is applicable in all cases.\n\nMathematically, orbifolds arose first as surfaces with singular points long before they were formally defined. One of the first classical examples arose in the theory of modular forms with the action of the modular group \"SL\"(2,Z) on the upper half-plane: a version of the Riemann–Roch theorem holds after the quotient is compactified by the addition of two orbifold cusp points. In 3-manifold theory, the theory of Seifert fiber spaces, initiated by Seifert, can be phrased in terms of 2-dimensional orbifolds. In geometric group theory, post-Gromov, discrete groups have been studied in terms of the local curvature properties of orbihedra and their covering spaces.\n\nIn string theory, the word \"orbifold\" has a slightly different meaning, discussed in detail below. In two-dimensional conformal field theory, it refers to the theory attached to the fixed point subalgebra of a vertex algebra under the action of a finite group of automorphisms.\n\nThe main example of underlying space is a quotient space of a manifold under the properly discontinuous action of a possibly infinite group of diffeomorphisms with finite isotropy subgroups. In particular this applies to any action of a finite group; thus a manifold with boundary carries a natural orbifold structure, since it is the quotient of its double by an action of Z.\nSimilarly the quotient space of a manifold by a smooth proper action of \"S\" carries the structure of an orbifold.\n\nOrbifold structure gives a natural stratification by open manifolds on its underlying space, where one stratum corresponds to a set of singular points of the same type.\n\nOne topological space can carry many different orbifold structures. For example, consider the orbifold \"O\" associated with a factor space of the 2-sphere along a rotation by formula_1; it is homeomorphic to the 2-sphere, but the natural orbifold structure is different. It is possible to adopt most of the characteristics of manifolds to orbifolds and these characteristics are usually different from correspondent characteristics of underlying space.\nIn the above example, the \"orbifold fundamental group\" of \"O\" is Z and its \"orbifold Euler characteristic\" is 1.\n\nLike a manifold, an orbifold is specified by local conditions; however, instead of being locally modelled on open subsets of R, an orbifold is locally modelled on quotients of open subsets of R by finite group actions. The structure of an orbifold encodes not only that of the underlying quotient space, which need not be a manifold, but also that of the isotropy subgroups.\n\nAn \"n\"-dimensional orbifold is a Hausdorff topological space \"X\", called the underlying space, with a covering by a collection of open sets \"U\", closed under finite intersection. For each \"U\", there is\n\nThe collection of orbifold charts is called an orbifold atlas if the following properties are satisfied:\n\nThe orbifold atlas defines the orbifold structure completely:\ntwo orbifold atlases of \"X\" give the same orbifold structure if they can be consistently combined to give a larger orbifold atlas. Note that the orbifold structure determines the isotropy subgroup of any point of the orbifold up to isomorphism: it can be computed as the stabilizer of the point in any orbifold chart. If \"U\" formula_5 \"U\" formula_5 \"U\", then there is a unique \"transition element\" \"g\" in Γ such that\n\nThese transition elements satisfy\n\nas well as the \"cocycle relation\" (guaranteeing associativity)\n\nMore generally, attached to an open covering of an orbifold by orbifold charts, there is the combinatorial data of a so-called \"complex of groups\" (see below).\n\nExactly as in the case of manifolds, differentiability conditions can be imposed on the gluing maps to give a definition of a differentiable orbifold. It will be a \"Riemannian orbifold\" if in addition there are invariant Riemannian metrics on the orbifold charts and the gluing maps are isometries.\n\nFor applications in geometric group theory, it is often convenient to have a slightly more general notion of orbifold, due to Haefliger. An orbispace is to topological spaces what an orbifold is to manifolds. An orbispace is a topological generalization of the orbifold concept. It is defined by replacing the model for the orbifold charts by a locally compact space with a \"rigid\" action of a finite group, i.e. one for which points with trivial isotropy are dense. (This condition is automatically satisfied by faithful linear actions, because the points fixed by any non-trivial group element form a proper linear subspace.) It is also useful to consider metric space structures on an orbispace, given by invariant metrics on the orbispace charts for which the gluing maps preserve distance. In this case each orbispace chart is usually required to be a length space with unique geodesics connecting any two points.\n\n\nThere are several ways to define the orbifold fundamental group. More sophisticated approaches use orbifold covering spaces or classifying spaces of groupoids. The simplest approach (adopted by Haefliger and known also to Thurston) extends the usual notion of loop used in the standard definition of the fundamental group.\n\nAn orbifold path is a path in the underlying space provided with an explicit piecewise lift of path segments to orbifold charts and explicit group elements identifying paths in overlapping charts; if the underlying path is a loop, it is called an orbifold loop. Two orbifold paths are identified if they are related through multiplication by group elements in orbifold charts. The orbifold fundamental group is the group formed by homotopy classes of orbifold loops.\n\nIf the orbifold arises as the quotient of a simply connected manifold \"M\" by a proper rigid action of a discrete group Γ, the orbifold fundamental group can be identified with Γ. In general it is an extension of Γ by π \"M\".\n\nThe orbifold is said to be \"developable\" or \"good\" if it arises as the quotient by a finite group action; otherwise it is called \"bad\". A \"universal covering orbifold\" can be constructed for an orbifold by direct analogy with the construction of the universal covering space of a topological space, namely as the space of pairs consisting of points of the orbifold and homotopy classes of orbifold paths joining them to the basepoint. This space is naturally an orbifold.\n\nNote that if an orbifold chart on a contractible open subset corresponds to a group Γ, then there is a natural \"local homomorphism\" of Γ into the orbifold fundamental group.\n\nIn fact the following conditions are equivalent:\n\nAs explained above, an orbispace is basically a generalization of the orbifold concept applied to topological spaces. Let then \"X\" be an orbispace endowed with a metric space structure for which the charts are geodesic length spaces. The preceding definitions and results for orbifolds can be generalized to give definitions of \"orbispace fundamental group\" and \"universal covering orbispace\", with analogous criteria for developability. The distance functions on the orbispace charts can be used to define the length of an orbispace path in the universal covering orbispace. If the distance function in each chart is non-positively curved, then the Birkhoff curve shortening argument can be used to prove that any orbispace path with fixed endpoints is homotopic to a unique geodesic. Applying this to constant paths in an orbispace chart, it follows that each local homomorphism is injective and hence:\n\nEvery orbifold has associated with it an additional combinatorial structure given by a \"complex of groups\".\n\nA complex of groups (\"Y\",\"f\",\"g\") on an abstract simplicial complex \"Y\" is given by\n\nThe group elements must in addition satisfy the cocycle condition\n\nfor every chain of simplices π formula_2 ρformula_2 σformula_2 τ. (This condition is vacuous if \"Y\" has dimension 2 or less.)\n\nAny choice of elements \"h\" in Γ yields an \"equivalent\" complex of groups by defining\n\nA complex of groups is called simple whenever \"g\" = 1 everywhere.\n\nIt is often more convenient and conceptually appealing to pass to the barycentric subdivision of \"Y\". The vertices of this subdivision correspond to the simplices of \"Y\", so that each vertex has a group attached to it. The edges of the barycentric subdivision are naturally oriented (corresponding to inclusions of simplices) and each directed edge gives an inclusion of groups. Each triangle has a transition element attached to it belonging to the group of exactly one vertex; and the tetrahedra, if there are any, give cocycle relations for the transition elements. Thus a complex of groups involves only the 3-skeleton of the barycentric subdivision; and only the 2-skeleton if it is simple.\n\nIf \"X\" is an orbifold (or orbispace), choose a covering by open subsets from amongst the orbifold charts \"f\": \"V\" formula_3 \"U\". Let \"Y\" be the abstract simplicial complex given by the nerve of the covering: its vertices are the sets of the cover and its \"n\"-simplices correspond to \"non-empty\" intersections \"U\" = \"U\" formula_15 ··· formula_15 \"U\". For each such simplex there is an associated group Γ and the homomorphisms \"f\" become the homomorphisms \"f\". For every triple ρ formula_5 σ formula_5 τ corresponding to intersections\n\nthere are charts φ : \"V\" formula_3 \"U\", φ : \"V\" formula_3 \"U\" formula_15 \"U\" and φ : \"V\" formula_3 \"U\" formula_15 \"U\" formula_15 \"U\" and gluing maps ψ : \"V\" formula_30 \"V\", ψ' : \"V\" formula_30 \"V\" and ψ\" : \"V\" formula_30 \"V\".\n\nThere is a unique transition element \"g\" in Γ such that \"g\"·ψ\" = ψ·ψ'. The relations satisfied by the transition elements of an orbifold imply those required for a complex of groups. In this way a complex of groups can be canonically associated to the nerve of an open covering by orbifold (or orbispace) charts. In the language of non-commutative sheaf theory and gerbes, the complex of groups in this case arises as a sheaf of groups associated to the covering \"U\"; the data \"g\" is a 2-cocycle in non-commutative sheaf cohomology and the data \"h\" gives a 2-coboundary perturbation.\n\nThe edge-path group of a complex of groups can be defined as a natural generalisation of the edge path group of a simplicial complex. In the barycentric subdivision of \"Y\", take generators \"e\" corresponding to edges from \"i\" to \"j\" where \"i\" formula_30 \"j\", so that there is an injection ψ : Γ formula_34 Γ. Let Γ be the group generated by the \"e\" and Γ with relations\n\nfor \"g\" in Γ and\n\nif \"i\" formula_30 \"j\" formula_3 \"k\".\n\nFor a fixed vertex \"i\", the edge-path group Γ(\"i\") is defined to be the subgroup of Γ generated by all products\n\nwhere \"i\", \"i\", ..., \"i\", \"i\"\nis an edge-path, \"g\" lies in Γ and \"e\"=\"e\" if \"i\" formula_30 \"j\".\n\nA simplicial proper action of a discrete group Γ on a simplicial complex \"X\" with finite quotient is said to be regular if it\nsatisfies one of the following equivalent conditions (see Bredon 1972):\n\nThe fundamental domain and quotient \"Y\" = \"X\" / Γ can naturally be identified as simplicial complexes in this case, given by the stabilisers of the simplices in the fundamental domain. A complex of groups \"Y\" is said to be developable if it arises in this way.\n\nThe action of Γ on the barycentric subdivision \"X\" ' of \"X\" always satisfies the following condition, weaker than regularity:\n\nIndeed, simplices in \"X\" ' correspond to chains of simplices in \"X\", so that a subsimplices, given by subchains of simplices, is uniquely determined by the \"sizes\" of the simplices in the subchain. When an action satisfies this condition, then \"g\" necessarily fixes all the vertices of σ. A straightforward inductive argument shows that such an action becomes regular on the barycentric subdivision; in particular\n\nThere is in fact no need to pass to a \"third\" barycentric subdivision: as Haefliger observes using the language of category theory, in this case the 3-skeleton of the fundamental domain of \"X\"\" already carries all the necessary data – including transition elements for triangles – to define an edge-path group isomorphic to Γ.\n\nIn two dimensions this is particularly simple to describe. The fundamental domain of \"X\"\" has the same structure as the barycentric subdivision \"Y\" ' of a complex of groups \"Y\", namely:\n\nAn edge-path group can then be defined. A similar structure is inherited by the barycentric subdivision \"Z\" ' and its edge-path group is isomorphic to that of \"Z\".\n\nIf a countable discrete group acts by a \"regular\" \"simplicial\" proper action on a simplicial complex, the quotient can be given not only the structure of a complex of groups, but also that of an orbispace. This leads more generally to the definition of \"orbihedron\", the simplicial analogue of an orbifold.\n\nLet \"X\" be a finite simplicial complex with barycentric subdivision \"X\" '. An orbihedron structure consists of:\n\nThis action of Γ on \"L\"' extends to a simplicial action on the simplicial cone \"C\" over \"L\"' (the simplicial join of \"i\" and \"L\"'), fixing the centre \"i\" of the cone. The map φ extends to a simplicial map of\n\"C\" onto the star St(\"i\") of \"i\", carrying the centre onto \"i\"; thus φ identifies \"C\" / Γ, the quotient of the star of \"i\" in \"C\", with St(\"i\") and gives an \"orbihedron chart\" at \"i\".\n\nIf \"i\"formula_44 \"j\" formula_44 \"k\", then there is a unique \"transition element\" \"g\" in Γ such that\n\nThese transition elements satisfy\n\nas well as the cocycle relation\n\n\nHistorically one of the most important applications of orbifolds in geometric group theory has been to \"triangles of groups\". This is the simplest 2-dimensional example generalising the 1-dimensional \"interval of groups\" discussed in Serre's lectures on trees, where amalgamated free products are studied in terms of actions on trees. Such triangles of groups arise any time a discrete group acts simply transitively on the triangles in the affine Bruhat-Tits building for \"SL\"(Q); in 1979 Mumford discovered the first example for \"p\" = 2 (see below) as a step in producing an algebraic surface not isomorphic to projective space, but having the same Betti numbers. Triangles of groups were worked out in detail by Gersten and Stallings, while the more general case of complexes of groups, described above, was developed independently by Haefliger. The underlying geometric method of analysing finitely presented groups in terms of metric spaces of non-positive curvature is due to Gromov. In this context triangles of groups correspond to non-positively curved 2-dimensional simplicial complexes with the regular action of a group, transitive on triangles.\nA triangle of groups is a \"simple\" complex of groups consisting of a triangle with vertices A, B, C. There are groups\n\nThere is an injective homomorphisms of Γ into all the other groups and of an edge group Γ into Γ and Γ. The three ways of mapping Γ into a vertex group all agree. (Often Γ is the trivial group.) The Euclidean metric structure on the corresponding orbispace is non-positively curved if and only if the link of each of the vertices in the orbihedron chart has girth at least 6.\n\nThis girth at each vertex is always even and, as observed by Stallings, can be described at a vertex A, say, as the length of the smallest word in the kernel of the natural homomorphism into Γ of the amalgamated free product over Γ of the edge groups Γ and Γ:\n\nThe result using the Euclidean metric structure is not optimal. Angles α, β, γ at the vertices A, B and C were defined by Stallings as 2π divided by the girth. In the Euclidean case α, β, γ ≤ π/3. However, if it is only required that α + β + γ ≤ π, it is possible to identify the\ntriangle with the corresponding geodesic triangle in the hyperbolic plane with the Poincaré metric (or the Euclidean plane if equality holds). It is a classical result from hyperbolic geometry that the hyperbolic medians intersect in the hyperbolic barycentre, just as in the familiar Euclidean case. The barycentric subdivision and metric from this model yield a non-positively curved metric structure on the corresponding orbispace. Thus, if α+β+γ≤π,\n\nLet α = formula_47 be given by the binomial expansion of (1 − 8) in Q and set \"K\" = Q(α) formula_48 Q. Let\n\nLet \"E\" = Q(ζ), a 3-dimensional vector space over \"K\" with basis 1, ζ and ζ. Define \"K\"-linear operators on \"E\" as follows:\n\nThe elements ρ, σ and τ generate a discrete subgroup of \"GL\"(\"K\") which acts properly on the affine Bruhat–Tits building corresponding to \"SL\"(Q). This group acts \"transitively\" on all vertices, edges and triangles in the building. Let\n\nThen\n\nThe elements σ and τ generate the stabiliser of a vertex. The link of this vertex can be identified with the spherical building of \"SL\"(F) and the stabiliser can be identified with the collineation group of the Fano plane generated by a 3-fold symmetry σ fixing a point and a cyclic permutation τ of all 7 points, satisfying στ = τσ. Identifying F* with the Fano plane, σ can be taken to be the restriction of the Frobenius automorphism σ(\"x\") = \"x\" of F and τ to be multiplication by any element not in the prime field F, i.e. an order 7 generator of the cyclic multiplicative group of F. This Frobenius group acts simply transitively on the 21 flags in the Fano plane, i.e. lines with marked points. The formulas for σ and τ on \"E\" thus \"lift\" the formulas on F.\n\nMumford also obtains an action simply transitive on the vertices of the building by passing to a subgroup of Γ = <ρ, σ, τ, −\"I\">. The group Γ preserves the Q(α)-valued hermitian form\n\non Q(ζ) and can be identified with \"U\"(f) formula_15 \"GL\"(\"S\") where \"S\" = Z[α,½]. Since \"S\"/(α) = F, there is a homomorphism of the group Γ into \"GL\"(F). This action leaves invariant a\n2-dimensional subspace in F and hence gives rise to a homomorphism Ψ of Γ into \"SL\"(F), a group of order 16·3·7. On the other hand, the stabiliser of a vertex is a subgroup of order 21 and Ψ is injective on this subgroup. Thus if the congruence subgroup Γ is defined as the inverse image under Ψ of the 2-Sylow subgroup of \"SL\"(F), the action of\nΓ on vertices must be simply transitive.\n\nOther examples of triangles or 2-dimensional complexes of groups can be constructed by variations of the above example.\n\nCartwright et al. consider actions on buildings that are simply transitive on vertices. Each such action produces a bijection (or modified duality) between the points \"x\" and lines \"x\"* in the flag complex of a finite projective plane and a collection of oriented triangles of points (\"x\",\"y\",\"z\"), invariant under cyclic permutation, such that \"x\" lies on \"z\"*, \"y\" lies on \"x\"* and \"z\" lies on \"y\"* and any two points uniquely determine the third. The groups produced have generators \"x\", labelled by points, and relations \"xyz\" = 1 for each triangle. Generically this construction will not correspond to an action on a classical affine building.\n\nMore generally, as shown by Ballmann and Brin, similar algebraic data encodes all actions that are simply transitively on the vertices of a non-positively curved 2-dimensional simplicial complex, provided the link of each vertex has girth at least 6. This data consists of:\n\nThe elements \"g\" in \"S\" label the vertices \"g\"·\"v\" in the link of a fixed vertex \"v\"; and the relations correspond to edges (\"g\"·\"v\", \"h\"·\"v\") in that link. The graph with vertices \"S\" and edges (\"g\", \"h\"), for \"g\"\"h\" in \"S\", must have girth at least 6. The original simplicial complex can be reconstructed using complexes of groups and the second barycentric subdivision.\nFurther examples of non-positively curved 2-dimensional complexes of groups have been constructed by Swiatkowski based on actions simply transitive on oriented edges and inducing a 3-fold symmetry on each triangle; in this case too the complex of groups is obtained from the regular action on the second barycentric subdivision. The simplest example, discovered earlier with Ballmann, starts from a finite group \"H\" with a symmetric set of generators \"S\", not containing the identity, such that the corresponding Cayley graph has girth at least 6. The associated group is generated by \"H\" and an involution τ subject to (τg) = 1 for each \"g\" in \"S\".\n\nIn fact, if Γ acts in this way, fixing an edge (\"v\", \"w\"), there is an involution τ interchanging \"v\" and \"w\". The link of \"v\" is made up of vertices \"g\"·\"w\" for \"g\" in a symmetric subset \"S\" of \"H\" = Γ, generating \"H\" if the link is connected. The assumption on triangles implies that\n\nfor \"g\" in \"S\". Thus, if σ = τ\"g\" and \"u\" = \"g\"·\"w\", then\n\nBy simple transitivity on the triangle (\"v\", \"w\", \"u\"), it follows that σ = 1.\n\nThe second barycentric subdivision gives a complex of groups consisting of singletons or pairs of barycentrically subdivided triangles joined along their large sides: these pairs are indexed by the quotient space \"S\"/~ obtained by identifying inverses in \"S\". The single or \"coupled\" triangles are in turn joined along one common \"spine\". All stabilisers of simplices are trivial except for the two vertices at the ends of the spine, with stabilisers \"H\" and <τ>, and the remaining vertices of the large triangles, with stabiliser generated by an appropriate σ. Three of the smaller triangles in each large triangle contain transition elements.\n\nWhen all the elements of \"S\" are involutions, none of the triangles need to be doubled. If \"H\" is taken to be the dihedral group \"D\" of order 14, generated by an involution \"a\" and an element \"b\" of order 7 such that\n\nthen \"H\" is generated by the 3 involutions \"a\", \"ab\" and \"ab\". The link of each vertex is given by the corresponding Cayley graph, so is just the bipartite Heawood graph, i.e. exactly the same as in the affine building for \"SL\"(Q). This link structure implies that the corresponding simplicial complex is necessarily a Euclidean building. At present, however, it seems to be unknown whether any of these types of action can in fact be realised on a classical affine building: Mumford's group Γ (modulo scalars) is only simply transitive on edges, not on oriented edges.\n\nIn two dimensions, there are three singular point types of an orbifold:\n\nA compact 2-dimensional orbifold has an Euler characteristic Χ\ngiven by\nwhere Χ(\"X\") is the Euler characteristic of the underlying topological manifold \"X\", and \"n\" are the orders of the corner reflectors, and \"m\" are the orders of the elliptic points.\n\nA 2-dimensional compact connected orbifold has a hyperbolic structure if its Euler characteristic is less than 0, a Euclidean structure if it is 0, and if its Euler characteristic is positive it is either bad or has an elliptic structure (an orbifold is called bad if it does not have a manifold as a covering space). In other words, its universal covering space has a hyperbolic, Euclidean, or spherical structure.\n\nThe compact 2-dimensional connected orbifolds that are not hyperbolic are listed in the table below. The 17 parabolic orbifolds are the quotients of the plane by the 17 wallpaper groups.\n\nA 3-manifold is said to be \"small\" if it is closed, irreducible and does not contain any incompressible surfaces.\n\nOrbifold Theorem. Let \"M\" be a small 3-manifold. Let φ be a non-trivial periodic orientation-preserving diffeomorphism of \"M\". Then \"M\" admits a φ-invariant hyperbolic or Seifert fibered structure.\n\nThis theorem is a special case of Thurston's orbifold theorem, announced without proof in 1981; it forms part of his geometrization conjecture for 3-manifolds. In particular it implies that if \"X\" is a compact, connected, orientable, irreducible, atoroidal 3-orbifold with non-empty singular locus, then \"M\" has a geometric structure (in the sense of orbifolds). A complete proof of the theorem was published by Boileau, Leeb & Porti in 2005.\n\nIn string theory, the word \"orbifold\" has a slightly new meaning. For mathematicians, an orbifold is a generalization of the notion of manifold that allows the presence of the points whose neighborhood is diffeomorphic to a quotient of R by a finite group, i.e. R/\"Γ\". In physics, the notion of an orbifold usually describes an object that can be globally written as an orbit space \"M\"/\"G\" where \"M\" is a manifold (or a theory), and \"G\" is a group of its isometries (or symmetries) — not necessarily all of them. In string theory, these symmetries do not have to have a geometric interpretation.\n\nA quantum field theory defined on an orbifold becomes singular near the fixed points of \"G\". However string theory requires us to add new parts of the closed string Hilbert space — namely the twisted sectors where the fields defined on the closed strings are periodic up to an action from \"G\". Orbifolding is therefore a general procedure of string theory to derive a new string theory from an old string theory in which the elements of \"G\" have been identified with the identity. Such a procedure reduces the number of states because the states must be invariant under \"G\", but it also increases the number of states because of the extra twisted sectors. The result is usually a perfectly smooth, new string theory.\n\nD-branes propagating on the orbifolds are described, at low energies, by gauge theories defined by the quiver diagrams. Open strings attached to these D-branes have no twisted sector, and so the number of open string states is reduced by the orbifolding procedure.\n\nMore specifically, when the orbifold group \"G\" is a discrete subgroup of spacetime isometries, then if it has no fixed point, the result is usually a compact smooth space; the twisted sector consists of closed strings wound around the compact dimension, which are called \"winding states\".\n\nWhen the orbifold group G is a discrete subgroup of spacetime isometries, and it has fixed points, then these usually have conical singularities, because R/Z has such a singularity at the fixed point of \"Z\". In string theory, gravitational singularities are usually a sign of extra degrees of freedom which are located at a locus point in spacetime. In the case of the orbifold these degrees of freedom are the twisted states, which are strings \"stuck\" at the fixed points. When the fields related with these twisted states acquire a non-zero vacuum expectation value, the singularity is deformed, i.e. the metric is changed and becomes regular at this point and around it. An example for a resulting geometry is the Eguchi-Hanson spacetime.\n\nFrom the point of view of D-branes in the vicinity of the fixed points, the effective theory of the open strings attached to these D-branes is a supersymmetric field theory, whose space of vacua has a singular point, where additional massless degrees of freedom exist. The fields related with the closed string twisted sector couple to the open strings in such a way as to add a Fayet-Iliopoulos term to the supersymmetric field theory Lagrangian, so that when such a field acquires a non-zero vacuum expectation value, the Fayet-Iliopoulos term is non-zero, and thereby deforms the theory (i.e. changes it) so that the singularity no longer exists , .\n\nIn superstring theory,\nthe construction of realistic phenomenological models requires dimensional reduction because the strings naturally propagate in a 10-dimensional space whilst the observed dimension of space-time of the universe is 4. Formal constraints on the theories nevertheless place restrictions on the compactified space in which the extra \"hidden\" variables live: when looking for realistic 4-dimensional models with supersymmetry, the auxiliary compactified space must be a 6-dimensional Calabi–Yau manifold.\n\nThere are a large number of possible Calabi–Yau manifolds (tens of thousands), hence the use of the term \"landscape\" in the current theoretical physics literature to describe the baffling choice. The general study of Calabi–Yau manifolds is mathematically complex and for a long time examples have been hard to construct explicitly. Orbifolds have therefore proved very useful since they automatically satisfy the constraints imposed by supersymmetry. They provide degenerate examples of Calabi–Yau manifolds due to their singular points, but this is completely acceptable from the point of view of theoretical physics. Such orbifolds are called \"supersymmetric\": they are technically easier to study than general Calabi–Yau manifolds. It is very often possible to associate a continuous family of non-singular Calabi–Yau manifolds to a singular supersymmetric orbifold. In 4 dimensions this can be illustrated using complex K3 surfaces:\n\nThe study of Calabi–Yau manifolds in string theory and the duality between different models of string theory (type IIA and IIB) led to the idea of mirror symmetry in 1988. The role of orbifolds was first pointed out by Dixon, Harvey, Vafa and Witten around the same time.\n\nBeyond their manifold and various applications in mathematics and physics, orbifolds have been applied to music theory at least as early as 1985 in the work of Guerino Mazzola and later by Dmitri Tymoczko and collaborators and . One of the papers of Tymoczko was the first music theory paper published by the journal \"Science.\" Mazzola and Tymoczko have participated in debate regarding their theories documented in a series of commentaries available at their respective web sites.\n\nTymoczko models musical chords consisting of \"n\" notes, not necessarily distinct, as points in the orbifold formula_51 – the space of \"n\" unordered points (not necessarily distinct) in the circle, realized as the quotient of the \"n\"-torus formula_52 (the space of \"n\" \"ordered\" points on the circle) by the symmetric group formula_53 (corresponding from moving from an ordered set to an unordered set).\n\nMusically, this is explained as follows:\n\nFor dyads (two tones), this yields the closed Möbius strip; for triads (three tones), this yields an orbifold that can be described as a triangular prism with the top and bottom triangular faces identified with a 120° twist (a ⅓ twist) – equivalently, as a solid torus in 3 dimensions with a cross-section an equilateral triangle and such a twist.\n\nThe resulting orbifold is naturally stratified by repeated tones (properly, by integer partitions of \"t\") – the open set consists of distinct tones (the partition formula_58), while there is a 1-dimensional singular set consisting of all tones being the same (the partition formula_59), which topologically is a circle, and various intermediate partitions. There is also a notable circle which runs through the center of the open set consisting of equally spaced points. In the case of triads, the three side faces of the prism correspond to two tones being the same and the third different (the partition formula_60), while the three edges of the prism correspond to the 1-dimensional singular set. The top and bottom faces are part of the open set, and only appear because the orbifold has been cut – if viewed as a triangular torus with a twist, these artifacts disappear.\n\nTymoczko argues that chords close to the center (with tones equally or almost equally spaced) form the basis of much of traditional Western harmony, and that visualizing them in this way assists in analysis. There are 4 chords on the center (equally spaced under equal temperament – spacing of 4/4/4 between tones), corresponding to the augmented triads (thought of as musical sets) C♯FA, DF♯A♯, D♯GB, and EG♯C (then they cycle: FAC♯ = C♯FA), with the 12 major chords and 12 minor chords being the points next to but not on the center – almost evenly spaced but not quite. Major chords correspond to 4/3/5 (or equivalently, 5/4/3) spacing, while minor chords correspond to 3/4/5 spacing. Key changes then correspond to movement between these points in the orbifold, with smoother changes effected by movement between nearby points.\n\n\n"}
{"id": "20484815", "url": "https://en.wikipedia.org/wiki?curid=20484815", "title": "Petersson trace formula", "text": "Petersson trace formula\n\nIn analytic number theory, the Petersson trace formula is a kind of orthogonality relation between coefficients of a holomorphic modular form. It is a specialization of the more general Kuznetsov trace formula.\n\nIn its simplest form the Petersson trace formula is as follows. Let formula_1 be an orthonormal basis of formula_2, the space of cusp forms of weight formula_3 on formula_4. Then for any positive integers formula_5 we have\n\nwhere formula_7 is the Kronecker delta function, formula_8 is the Kloosterman sum and formula_9 is the Bessel function of the first kind.\n"}
{"id": "41345437", "url": "https://en.wikipedia.org/wiki?curid=41345437", "title": "Predictive intake modelling", "text": "Predictive intake modelling\n\nPredictive intake modelling uses mathematical modelling strategies to estimate intake of food, personal care products, and their formulations.\n\nPredictive intake modelling seeks to estimate intake of products and/or their constituents which may enter the body through various routes such as ingestion, inhalation and absorption.\n\nPredictive intake modelling can be applied to determine trends in food consumption and product use for the purpose of extrapolation.\n\nA predictive intake modelling approach is used to estimate voluntary food intake (VFI) by animals where their eating habits cannot be exactly measured. For humans, predictive intake modelling is used to make estimations of intake from foods, pesticides, cosmetics and inhalants as well as substances that can be contained in these like nutrients, functional ingredients, chemicals and contaminants.\n\nPredictive intake modelling has applications in public health, risk assessment and exposure assessment, where estimating intake or exposure to different substances can influence the decision making process.\n\nThe regression analysis approach is based on estimations through extrapolation or interpolation where there is a cause-and-effect relationship found by data fitting. These trends tend to be phenomenological.\n\nA mechanistic modelling approach is one where a model is derived from basic theory. Examples of these include compartmental models which can be used to describe the circulation and concentration of airborne particles in a room or household for estimating intake of inhalants.\n\nA population-based approach tracks consumer intake from individual members of a sample population over time. Mathematical models are used to combine these habits and practices databases with separate databases on product or food formulation to estimate intake or exposure for the sample population. Moreover, survey weights may be applied to each subject in the study based on their age, demographic and location allowing the sample of subjects to correctly represent an entire population, and thus estimate intake for that population.\n\nProbabilistic models are based on the Monte Carlo method where distributions of data from various sources are randomly sampled from to calculate percentile statistics. Such probabilistic techniques typically utilise product or consumption survey data from a sample population combined with distributions of substances that may be found within those foods or products. For example, The Food and Drug Administration (FDA) suggest that the estimation of intake of substances in food can be probabilistically conducted through food consumption surveys (NHANES/CSFII) from sample populations combined with distributions of substance concentration data to calculate the Estimated Daily Intake. The European Food Safety Authority (EFSA) funded the Monte Carlo Risk Assessment (MCRA) tool to estimate usual intake exposure distributions based on statistical models which utilise the EFSA Comprehensive Database, which contains detailed food consumption survey data. EFSA also funded Creme Global to develop a model and databases of European food consumption on which statistical models can be run to assess intake and exposure on a pan-European basis.\n\n"}
{"id": "12249052", "url": "https://en.wikipedia.org/wiki?curid=12249052", "title": "Princess and Monster game", "text": "Princess and Monster game\n\nIn game theory, a princess and monster game is a pursuit-evasion game played by two players in a region. The game was devised by Rufus Isaacs and published in his book \"Differential Games\" (1965) as follows:\n\nThis game remained a well-known open problem until it was solved by Shmuel Gal in the late 1970s. His optimal strategy for the princess is to move to a random location in the room and stay still for a time interval which is neither too short nor too long, before going to another (independent) random location and repeating the procedure. The proposed optimal search strategy, for the monster, is based on subdividing the room into many narrow rectangles, picking a rectangle at random and searching it in some specific way, after some time picking another rectangle randomly and independently, and so on.\n\nPrincess and monster games can be played on a pre-selected graph. It can be demonstrated that for any finite graph an optimal mixed search strategy exists that results in a finite payoff. This game has been solved by Steve Alpern and independently by Mikhail Zelikin only for the very simple graph consisting of a single loop (a circle). The value of the game on the unit interval (a graph with two nodes with a link in-between) has been estimated approximatively.\n\nThe game appears simple but is quite complicated. The obvious search strategy of starting at a random end and \"sweeping\" the whole interval as fast as possible guarantees a 0.75 expected capture time, and is not optimal. By utilising a more sophisticated mixed searcher and hider strategy, one can reduce the expected capture time by about 8.6%. This number would be quite close to the value of the game if someone was able to prove the optimality of the related strategy of the princess.\n\n"}
{"id": "1294354", "url": "https://en.wikipedia.org/wiki?curid=1294354", "title": "Property P conjecture", "text": "Property P conjecture\n\nIn mathematics, the Property P conjecture is a statement about 3-manifolds obtained by Dehn surgery on a knot in the 3-sphere. A knot in the 3-sphere is said to have Property P if every 3-manifold obtained by performing (non-trivial) Dehn surgery on the knot is not simply-connected. The conjecture states that all knots, except the unknot, have Property P.\n\nResearch on Property P was jump-started by R. H. Bing, who popularized the name and conjecture.\n\nThis conjecture can be thought of as a first step to resolving the Poincaré conjecture, since the Lickorish–Wallace theorem says any closed, orientable 3-manifold results from Dehn surgery on a link.\n\nA proof was announced in 2004, as the combined result of efforts of mathematicians working in several different fields.\n\n\n"}
{"id": "46732067", "url": "https://en.wikipedia.org/wiki?curid=46732067", "title": "Quasi-Newton inverse least squares method", "text": "Quasi-Newton inverse least squares method\n\nIn numerical analysis, the quasi-Newton inverse least squares method is a quasi-Newton method for finding roots of functions of several variables. It was originally described by Degroote et al. in 2009.\n\nNewton's method for solving uses the Jacobian matrix, , at every iteration. However, computing this Jacobian is a difficult (sometimes even impossible) and expensive operation. The idea behind the quasi-Newton inverse least squares method is to build up an approximate Jacobian based on known input–output pairs of the function .\n\nHaelterman et al. also showed that when the quasi-Newton inverse least squares method is applied to a linear system of size , it converges in at most steps, although like all quasi-Newton methods, it may not converge for nonlinear systems.\n\nThe method is closely related to the quasi-Newton least squares method.\n"}
{"id": "47651", "url": "https://en.wikipedia.org/wiki?curid=47651", "title": "Reproducibility", "text": "Reproducibility\n\nReproducibility is the closeness of the agreement between the results of measurements of the same measurand carried out with same methodology described in the corresponding scientific evidence (e.g. a publication in a peer-reviewed journal). Reproducibilty can also be applied under changed conditions of measurement for the same measurand to check, that the results are not an artefact of the measurment procedures. A related concept is replicability, meaning the ability to independently achieve non-identical conclusions that are at least similar, when differences in sampling, research procedures and data analysis methods may exist. Reproducibility and replicability together are among the main beliefs of 'the scientific method'—with the concrete expressions of the ideal of such a method varying considerably across research disciplines and fields of study. The reproduced measurement may be based on the raw data and computer programs provided by researchers.\n\nThe values obtained from distinct experimental trials are said to be \"commensurate\" if they are obtained according to the same reproducible experimental description and procedure. The basic idea can be seen in Aristotle's dictum that there is no scientific knowledge of the individual, where the word used for \"individual\" in Greek had the connotation of the \"idiosyncratic\", or wholly isolated occurrence. Thus all knowledge, all science, necessarily involves the formation of general concepts and the invocation of their corresponding symbols in language (cf. Turner). Aristotle′s conception about the knowledge of the individual being considered unscientific is due to lack of the field of statistics in his time, so he could not appeal to statistical averaging by the individual.\n\nA particular experimentally obtained value is said to be reproducible if there is a high degree of agreement between measurements or observations conducted on replicate specimens in different locations by different people—that is, if the experimental value is found to have a high precision. However, in science, a very well reproduced result is one that can be confirmed using as many \"different\" experimental setups as possible and as many lines of evidence as possible (consilience).\n\nThe first to stress the importance of reproducibility in science was the Irish chemist Robert Boyle, in England in the 17th century. Boyle's air pump was designed to generate and study vacuum, which at the time was a very controversial concept. Indeed, distinguished philosophers such as René Descartes and Thomas Hobbes denied the very possibility of vacuum existence. Historians of science e.g. Steven Shapin and Simon Schaffer, in their 1985 book \"Leviathan and the Air-Pump\", describe the debate between Boyle and Hobbes, ostensibly over the nature of vacuum, as fundamentally an argument about how useful knowledge should be gained. Boyle, a pioneer of the experimental method, maintained that the foundations of knowledge should be constituted by experimentally produced facts, which can be made believable to a scientific community by their reproducibility. By repeating the same experiment over and over again, Boyle argued, the certainty of fact will emerge.\n\nThe air pump, which in the 17th century was a complicated and expensive apparatus to build, also led to one of the first documented disputes over the reproducibility of a particular scientific phenomenon. In the 1660s, the Dutch scientist Christiaan Huygens built his own air pump in Amsterdam, the first one outside the direct management of Boyle and his assistant at the time Robert Hooke. Huygens reported an effect he termed \"anomalous suspension\", in which water appeared to levitate in a glass jar inside his air pump (in fact suspended over an air bubble), but Boyle and Hooke could not replicate this phenomenon in their own pumps. As Shapin and Schaffer describe, “it became clear that unless the phenomenon could be produced in England with one of the two pumps available, then no one in England would accept the claims Huygens had made, or his competence in working the pump”. Huygens was finally invited to England in 1663, and under his personal guidance Hooke was able to replicate anomalous suspension of water. Following this Huygens was elected a Foreign Member of the Royal Society. However, Shapin and Schaffer also note that “the accomplishment of replication was dependent on contingent acts of judgment. One cannot write down a formula saying when replication was or was not achieved”.\n\nThe philosopher of science Karl Popper noted briefly in his famous 1934 book \"The Logic of Scientific Discovery\" that “non-reproducible single occurrences are of no significance to science”. The Statistician Ronald Fisher wrote in his 1935 book \"The Design of Experiments\", which set the foundations for the modern scientific practice of hypothesis testing and statistical significance, that “we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us statistically significant results”. Such assertions express a common dogma in modern science that reproducibility is a necessary condition (although not necessarily sufficient) for establishing a scientific fact, and in practice for establishing scientific authority in any field of knowledge. However, as noted above by Shapin and Schaffer, this dogma is not well-formulated quantitatively, such as statistical significance for instance, and therefore it is not explicitly established how many times must a fact be replicated to be considered reproducible.\n\nReproducibility is one component of the precision of a measurement or test method. The other component is repeatability which is the degree of agreement of tests or measurements on replicate specimens by the same observer in the same laboratory. Both repeatability and reproducibility are usually reported as a standard deviation. A reproducibility limit is the value below which the difference between two test results obtained under reproducibility conditions may be expected to occur with a probability of approximately 0.95 (95%).\n\nReproducibility is determined from controlled interlaboratory test programs or a measurement systems analysis.\n\nAlthough they are often confused, there is an important distinction between replicates and an independent repetition of an experiment. Replicates are performed within an experiment. They are not and cannot provide independent evidence of reproducibility. Rather they serve as an internal \"check\" on an experiment and should not be shown as part of the experimental results within a scientific publication. It is the independent repetition of an experiment that serves to underpin its reproducibility.\n\nThe term \"reproducible research\" refers to the idea that the ultimate product of academic research is the paper along with the laboratory notebooks and full computational environment used to produce the results in the paper such as the code, data, etc. that can be used to reproduce the results and create new work based on the research. Typical examples of reproducible research comprise compendia of data, code and text files, often organised around an R Markdown source document or a Jupyter notebook.\n\nPsychology has seen a renewal of internal concerns about irreproducible results. Researchers showed in a 2006 study that, of 141 authors of a publication from the American Psychology Association (APA) empirical articles, 103 (73%) did not respond with their data over a 6-month period. In a follow up study published in 2015, it was found that 246 out of 394 contacted authors of papers in APA journals did not share their data upon request (62%). In a 2012 paper, it was suggested that researchers should publish data along with their works, and a dataset was released alongside as a demonstration, in 2017 it was suggested in an article published in \"Scientific Data\" that this may not be sufficient and that the whole analysis context should be disclosed. In 2015, Psychology became the first discipline to conduct and publish an open, registered empirical study of reproducibility called the Reproducibility Project. 270 researchers from around the world collaborated to replicate 100 empirical studies from three top Psychology journals. Fewer than half of the attempted replications were successful.\n\nThere have been initiatives to improve reporting and hence reproducibility in the medical literature for many years, which began with the CONSORT initiative, which is now part of a wider initiative, the EQUATOR Network. This group has recently turned its attention to how better reporting might reduce waste in research, especially biomedical research.\n\nReproducible research is key to new discoveries in pharmacology. A Phase I discovery will be followed by Phase II reproductions as a drug develops towards commercial production. In recent decades Phase II success has fallen from 28% to 18%. A 2011 study found that 65% of medical studies were inconsistent when re-tested, and only 6% were completely reproducible.\n\nIn 2012, a study by Begley and Ellis was published in \"Nature\" that reviewed a decade of research. That study found that 47 out of 53 medical research papers focused on cancer research were irreproducible. The irreproducible studies had a number of features in common, including that studies were not performed by investigators blinded to the experimental versus the control arms, there was a failure to repeat experiments, a lack of positive and negative controls, failure to show all the data, inappropriate use of statistical tests and use of reagents that were not appropriately validated. John P. A. Ioannidis writes, \"While currently there is unilateral emphasis on 'first' discoveries, there should be as much emphasis on replication of discoveries.\" The \"Nature\" study was itself reproduced in the journal \"PLOS ONE\", which confirmed that a majority of cancer researchers surveyed had been unable to reproduce a result.\n\nIn 2016, \"Nature\" conducted a survey of 1,576 researchers who took a brief online questionnaire on reproducibility in research. According to the survey, more than 70% of researchers have tried and failed to reproduce another scientist's experiments, and more than half have failed to reproduce their own experiments. \"Although 52% of those surveyed agree there is a significant 'crisis' of reproducibility, less than 31% think failure to reproduce published results means the result is probably wrong, and most say they still trust the published literature.\"\n\nHideyo Noguchi became famous for correctly identifying the bacterial agent of syphilis, but also claimed that he could culture this agent in his laboratory. Nobody else has been able to produce this latter result.\n\nIn March 1989, University of Utah chemists Stanley Pons and Martin Fleischmann reported the production of excess heat that could only be explained by a nuclear process (\"cold fusion\"). The report was astounding given the simplicity of the equipment: it was essentially an electrolysis cell containing heavy water and a palladium cathode which rapidly absorbed the deuterium produced during electrolysis. The news media reported on the experiments widely, and it was a front-page item on many newspapers around the world (see science by press conference). Over the next several months others tried to replicate the experiment, but were unsuccessful.\n\nNikola Tesla claimed as early as 1899 to have used a high frequency current to light gas-filled lamps from over away without using wires. In 1904 he built Wardenclyffe Tower on Long Island to demonstrate means to send and receive power without connecting wires. The facility was never fully operational and was not completed due to economic problems, so no attempt to reproduce his first result was ever carried out.\n\nOther examples which contrary evidence has refuted the original claim:\n\nThe reproducibility requirement cannot be applied to individual samples of phenomena which have a partially or totally non-deterministic nature. However, it still applies to the probabilistic description of such phenomena, with error tolerance given by probability theory.\n\n\n\n\n"}
{"id": "1055365", "url": "https://en.wikipedia.org/wiki?curid=1055365", "title": "Sangaku", "text": "Sangaku\n\nSangaku or San Gaku (算額; lit. translation: calculation tablet) are Japanese geometrical problems or theorems on wooden tablets which were placed as offerings at Shinto shrines or Buddhist temples during the Edo period by members of all social classes.\n\nThe Sangaku were painted in color on wooden tablets (ema) and hung in the precincts of Buddhist temples and Shinto shrines as offerings to the kami and buddhas, as challenges to the congregants, or as displays of the solutions to questions. Many of these tablets were lost during the period of modernization that followed the Edo period, but around nine hundred are known to remain.\n\nFujita Kagen (1765–1821), a Japanese mathematician of prominence, published the first collection of \"sangaku\" problems, his \"Shimpeki Sampo\" (Mathematical problems Suspended from the Temple) in 1790, and in 1806 a sequel, the \"Zoku Shimpeki Sampo\".\n\nDuring this period Japan applied strict regulations to commerce and foreign relations for western countries so the tablets were created using Japanese mathematics, (\"wasan\"), developed in parallel to western mathematics. For example, the connection between an integral and its derivative (the fundamental theorem of calculus) was unknown, so Sangaku problems on areas and volumes were solved by expansions in infinite series and term-by-term calculation.\n\nA typical problem, which is presented on an 1824 tablet in the Gunma Prefecture, covers the relationship of three touching circles with a common tangent. Given the size of the two outer large circles, what is the size of the small circle between them? \"The answer is:\" \n\nSoddy's hexlet, thought previously to have been discovered in the west in 1937, has been discovered on a Sangaku dating from 1822.\n\n\n\n"}
{"id": "693453", "url": "https://en.wikipedia.org/wiki?curid=693453", "title": "Skolem–Noether theorem", "text": "Skolem–Noether theorem\n\nIn ring theory, a branch of mathematics, the Skolem–Noether theorem characterizes the automorphisms of simple rings. It is a fundamental result in the theory of central simple algebras.\n\nThe theorem was first published by Thoralf Skolem in 1927 in his paper \"Zur Theorie der assoziativen Zahlensysteme\" (German: \"On the theory of associative number systems\") and later rediscovered by Emmy Noether.\n\nIn a general formulation, let \"A\" and \"B\" be simple unitary rings, and let \"k\" be the centre of \"B\". Notice that \"k\" is a field since given \"x\" nonzero in \"k\", the simplicity of \"B\" implies that the nonzero two-sided ideal \"BxB = (x)\" is the whole of \"B\", and hence that \"x\" is a unit. If the dimension of \"B\" over \"k\" is finite, i.e. if \"B\" is a central simple algebra of finite dimension, and \"A\" is also a \"k\"-algebra, then given \"k\"-algebra homomorphisms\n\nthere exists a unit \"b\" in \"B\" such that for all \"a\" in \"A\"\n\nIn particular, every automorphism of a central simple \"k\"-algebra is an inner automorphism.\n\nFirst suppose formula_1. Then \"f\" and \"g\" define the actions of \"A\" on formula_2; let formula_3 denote the \"A\"-modules thus obtained. Any two simple \"A\"-modules are isomorphic and formula_3 are finite direct sums of simple \"A\"-modules. Since they have the same dimension, it follows that there is an isomorphism of \"A\"-modules formula_5. But such \"b\" must be an element of formula_6. For the general case, note that formula_7 is a matrix algebra and thus by the first part this algebra has an element \"b\" such that\nfor all formula_9 and formula_10. Taking formula_11, we find\nfor all \"z\". That is to say, \"b\" is in formula_13 and so we can write formula_14. Taking formula_15 this time we find\nwhich is what was sought.\n\n"}
{"id": "47863111", "url": "https://en.wikipedia.org/wiki?curid=47863111", "title": "Susan Brown (mathematician)", "text": "Susan Brown (mathematician)\n\nSusan North Brown (22 December 1937 – 11 August 2017) was a professor of mathematics at University College London and a leading researcher in the field of fluid mechanics. \n\nAn exact timeline for Susan Brown's career has been difficult to pin down, but a newsletter published by Department of Mathematical and Physical Sciences at UCL shortly after her death offers a framework for her career achievements and highlights the esteem in which she was held by colleagues and students. Her undergraduate degree in mathematics was from St Hilda's College, Oxford. For about two years more years she continued studies at Oxford, in theoretical fluid mechanics -- and she then moved to the University of Durham to complete her DPhil in 1964. During this time she held temporary Lectureships at both Durham and Newcastle and in 1964 began a Lectureship that started her long association with UCL. From her Lectureship she advanced to a Readership in 1971 and was appointed to a Professorship in 1986. The afore-mentioned departmental newsletter that recapped her accomplishments after her death expresses the belief that Brown was the first female in the UK to be appointed to a professorship in Mathematics.\n\nBrown's department described her as an outstanding teacher and someone with an international reputation for her research. She had a productive partnership in fluid dynamics with UCL colleague Keith Stewartson -- who also arrived at UCL in 1964. Quoting from the afore-mentioned departmental newsletter, \" Together they published 29 papers and pioneered early developments of 'triple-deck' theory, which, in turn, enabled resolution of long-standing questions in steady and unsteady trailing-edge flows, and addressed associated important aerodynamic applications. Another area for which Professor Brown was especially renowned was a series of discussions of critical layers, especially effects of viscosity and nonlinearity and applications to geophysical flows such as atmospheric jets.\" Google-Scholar lists numerous papers for the pair \"SN Brown, K Stewartson\" and several of these are listed below.\n\nBrown died on 11 August 2017, aged 79, in London.\n\n\n"}
{"id": "11415193", "url": "https://en.wikipedia.org/wiki?curid=11415193", "title": "Swiss cheese (mathematics)", "text": "Swiss cheese (mathematics)\n\nIn mathematics, a Swiss cheese is a compact subset of the complex plane obtained by removing from a closed disc some countable union of open discs, usually with some restriction on the centres and radii of the removed discs. Traditionally the deleted discs should have pairwise disjoint closures which are subsets of the interior of the starting disc, the sum of the radii of the deleted discs should be finite, and the Swiss cheese should have empty interior. This is the type of Swiss cheese originally introduced by the Swiss mathematician Alice Roth.\n\nMore generally, a Swiss cheese may be all or part of Euclidean space R – or of an even more complicated manifold – with \"holes\" in it.\n"}
{"id": "38017293", "url": "https://en.wikipedia.org/wiki?curid=38017293", "title": "Sylvia Bozeman", "text": "Sylvia Bozeman\n\nSylvia D. Trimble Bozeman (\"née\" Sylvia Trimble, 1947) is an American mathematician and mathematics educator.\n\nBorn in rural Alabama, Bozeman attended segregated primary and secondary schools in Camp Hill, and was encouraged by her teachers and parents to continue her education.\n\nBozeman did her undergraduate studies in mathematics at Alabama A&M University, during which she also worked on summer projects at NASA and Harvard University. She graduated in 1968 as salutatorian and moved with her husband Robert, also a mathematician, to non-segregated Vanderbilt University, where they both began their graduate studies. She earned a master's degree in 1970, despite not having studied much of the prerequisite coursework that her white classmates had. The Bozemans had a son and a daughter while Sylvia taught part-time at Vanderbilt and Tennessee State University and Robert finished his doctoral studies in mathematics.\n\nIn 1974, Bozeman took a teaching position at Spelman College, a college for Black women in Atlanta, Georgia; Robert was then teaching at Morehouse College, another historically Black college. While there, she worked under Shirley Mathis McBay, Etta Zuber Falconer, and Gladys Glass, mathematicians and chemists who were pushing to improve Spelman's science and mathematics programs. In 1976, Bozeman took up graduate studies again at Emory University while continuing to hold a position at Spelman. She earned her doctorate in 1980 from Emory, under the supervision of Luis Kramarz and John Neuberger; her thesis was titled \"Representations of Generalized Inverses of Fredholm Operators\".\n\nBozeman continues to work as a professor at Spelman, and has also held the position of vice provost there. Her research there has focused on functional analysis and image processing, and has been funded by the Army Research Office, National Science Foundation, and NASA. In 1997 she became Section Governor in the Mathematical Association of America, the first African-American to reach that level. In 2012, she became a fellow of the American Mathematical Society. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
{"id": "15110024", "url": "https://en.wikipedia.org/wiki?curid=15110024", "title": "Transitive model", "text": "Transitive model\n\nIn mathematical set theory, a transitive model is a model of set theory that is standard and transitive. Standard means that the membership relation is the usual one, and transitive means that the model is a transitive set or class.\n\n\nIf \"M\" is a transitive model, then ω is the standard ω. This implies that the natural numbers, integers, and rational numbers of the model are also the same as their standard counterparts. Each real number in a transitive model is a standard real number, although not all standard reals need be included in a particular transitive model. \n"}
{"id": "44061729", "url": "https://en.wikipedia.org/wiki?curid=44061729", "title": "Van Lamoen circle", "text": "Van Lamoen circle\n\nIn Euclidean plane geometry, the van Lamoen circle is a special circle associated with any given triangle formula_1. It contains the circumcenters of the six triangles that are defined inside formula_1 by its three medians.\n\nSpecifically, let formula_3, formula_4, formula_5 be the vertices of formula_1, and let formula_7 be its centroid (the intersection of its three medians). Let formula_8, formula_9, and formula_10 be the midpoints of the sidelines formula_11, formula_12, and formula_13, respectively. It turns out that the circumcenters of the six triangles formula_14, formula_15, formula_16, formula_17, formula_18, and formula_19 lie on a common circle, which is the van Lamoen circle of formula_1.\n\nThe van Lamoen circle is named after the mathematician Floor van Lamoen who posed it as a problem in 2000. A proof was provided by Kin Y. Li in 2001, and the editors of the Amer. Math. Monthly in 2002.\n\nThe center of the van Lamoen circle is point formula_21 in Clark Kimberling's comprehensive list of triangle centers.\n\nIn 2003, Alexey Myakishev and Peter Y. Woo proved that the converse of the theorem is nearly true, in the following sense: let formula_22 be any point in the triangle's interior, and formula_23, formula_24, and formula_25 be its cevians, that is, the line segments that connect each vertex to formula_22 and are extended until each meets the opposite side. Then the circumcenters of the six triangles formula_27, formula_28, formula_29, formula_30, formula_31, and formula_32 lie on the same circle if and only if formula_22 is the centroid of formula_1 or its orthocenter (the intersection of its three altitudes). A simpler proof of this result was given by Nguyen Minh Ha in 2005.\n\n"}
{"id": "9399072", "url": "https://en.wikipedia.org/wiki?curid=9399072", "title": "Vector measure", "text": "Vector measure\n\nIn mathematics, a vector measure is a function defined on a family of sets and taking vector values satisfying certain properties. It is a generalization of the concept of finite measure, which takes nonnegative real values only.\n\nGiven a field of sets formula_1 and a Banach space formula_2, a finitely additive vector measure (or measure, for short) is a function formula_3 such that for any two disjoint sets formula_4 and formula_5 in formula_6 one has\n\nA vector measure formula_8 is called countably additive if for any sequence formula_9 of disjoint sets in formula_10 such that their union is in formula_10 it holds that\n\nwith the series on the right-hand side convergent in the norm of the Banach space formula_13\n\nIt can be proved that an additive vector measure formula_8 is countably additive if and only if for any sequence formula_9 as above one has\n\nwhere formula_17 is the norm on formula_13\n\nCountably additive vector measures defined on sigma-algebras are more general than finite measures, finite signed measures, and complex measures, which are countably additive functions taking values respectively on the real interval formula_19 the set of real numbers, and the set of complex numbers.\n\nConsider the field of sets made up of the interval formula_20 together with the family formula_10 of all Lebesgue measurable sets contained in this interval. For any such set formula_4, define\n\nwhere formula_24 is the indicator function of formula_25 Depending on where formula_8 is declared to take values, we get two different outcomes.\n\n\nBoth of these statements follow quite easily from the criterion (*) stated above.\n\nGiven a vector measure formula_33 the variation formula_34 of formula_8 is defined as\n\nwhere the supremum is taken over all the partitions\n\nof formula_4 into a finite number of disjoint sets, for all formula_4 in formula_6. Here, formula_17 is the norm on formula_13\n\nThe variation of formula_8 is a finitely additive function taking values in formula_44 It holds that\n\nfor any formula_4 in formula_47 If formula_48 is finite, the measure formula_8 is said to be of bounded variation. One can prove that if formula_8 is a vector measure of bounded variation, then formula_8 is countably additive if and only if formula_34 is countably additive.\n\nIn the theory of vector measures, \"Lyapunov<nowiki>'s theorem</nowiki>\" states that the range of a (non-atomic) vector measure is closed and convex. In fact, the range of a non-atomic vector measure is a \"zonoid\" (the closed and convex set that is the limit of a convergent sequence of zonotopes). It is used in economics, in (\"bang–bang\") control theory, and in statistical theory.\nLyapunov's theorem has been proved by using the Shapley–Folkman lemma, which has been viewed as a discrete analogue of Lyapunov's theorem.\n\n"}
{"id": "59095308", "url": "https://en.wikipedia.org/wiki?curid=59095308", "title": "William Loudon Mollison", "text": "William Loudon Mollison\n\nWilliam Loudon Mollison (19 September 1851 – 10 March 1929) was a Scottish mathematician and academic. From 1915 to 1929, he was Master of Clare College, Cambridge.\n\nMollison was born on 19 September 1851 in Aberdeen, Scotland. He was educated at Aberdeen Grammar School, then an all-boys grammar school. He studied mathematics and natural philosophy at the University of Aberdeen, graduating in 1872 with a first class degree. That year, he was awarded the Ferguson Scholarship by Aberdeen and matriculated into Clare College, Cambridge to continue his mathematical studies. He became a Foundation Scholar in 1873. His private tutor while at Cambridge was Edward Routh. He graduated from the University of Cambridge in 1876 as the Second Wrangler.\n\nOn 29 April 1876, Mollison was elected a Fellow of Clare College, Cambridge. He was an examiner for the University of St Andrews between 1876 and 1880. He was a mathematics lecturer at Jesus College, Cambridge from 1877 to 1882, and at Clare College from 1882. In addition to his college teaching, he was a private tutor or \"coach\" in mathematics.\n\nDue to ill health, he moved from teaching a large number of students, privately and through his college, into administration. He was appointed junior tutor of Clare College in 1880, and was made its senior tutor in May 1894. He was elected a member of the Council of the Senate of the University Of Cambridge in 1892, and appointed Secretary of the General Board of Studies of the University in 1904: he stepped down from both these posts in 1920. He served as \"locum tenens\" for the then Master (Edward Atkinson) from 1913 to 1915. Mollison was unanimously elected as Atkinson successor as the 38th Master of Clare College, Cambridge in March 1915.\n\nMollison was married to Ellen Mayhew. Together they had one son and one daughter. His wife died in 1917, and he provided the endowment for the Mayhew Prize, a mathematics prize awarded by the University of Cambridge, in her honour. His son, William Mayhew Mollison, was a distinguished ear, nose and throat surgeon, and his son Patrick Mollison, a noted haematologist.\n\nMollison died on 10 March 1929 in London, England; he was aged 77. His funeral was held at the chapel of Clare College, Cambridge, and he was buried in the Ascension Parish Burial Ground alongside his wife.\n"}
{"id": "3919417", "url": "https://en.wikipedia.org/wiki?curid=3919417", "title": "Zero suppression", "text": "Zero suppression\n\nZero suppression is the removal of redundant zeroes from a number. This can be done for storage, page or display space constraints or formatting reasons, such as making a letter more legible.\n\n\nOne must be careful; in physics and related disciplines, trailing zeros are used to indicate the precision of the number, as an error of ±1 in the last place is assumed. Examples:\n\nIt is also a way to store a large array of numbers, where many of the entries are zero. By omitting the zeroes, and instead storing the indices along with the values of the non-zero items, less space may be used in total. It only makes sense if the extra space used for storing the indices (on average) is smaller than the space saved by not storing the zeroes. This is sometimes used in a sparse array.\n\nExample:\n\n"}
