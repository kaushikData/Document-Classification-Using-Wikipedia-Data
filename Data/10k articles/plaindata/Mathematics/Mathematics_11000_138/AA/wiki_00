{"id": "356158", "url": "https://en.wikipedia.org/wiki?curid=356158", "title": "Annulus (mathematics)", "text": "Annulus (mathematics)\n\nIn mathematics, an annulus (the Latin word for \"little ring\" is \"anulus\"/\"annulus\", with plural \"anuli\"/\"annuli\") is a ring-shaped object, a region bounded by two concentric circles. The adjectival form is annular (as in annular eclipse).\n\nThe open annulus is topologically equivalent to both the open cylinder and the punctured plane. Informally, it has the shape of a hardware washer.\n\nThe area of an annulus is the difference in the areas of the larger circle of radius and the smaller one of radius :\n\nThe area of an annulus is determined by the length of the longest line segment within the annulus, which is the chord tangent to the inner circle, in the accompanying diagram. That can be shown using the Pythagorean theorem since this line is tangent to the smaller circle and perpendicular to its radius at that point, so and are sides of a right-angled triangle with hypotenuse , and the area of the annulus is given by\n\nThe area can also be obtained via calculus by dividing the annulus up into an infinite number of annuli of infinitesimal width and area and then integrating from to :\n\nThe area of an annulus sector of angle , with measured in radians, is given by\n\nIn complex analysis an annulus in the complex plane is an open region defined as\n\nIf is , the region is known as the punctured disk of radius around the point .\n\nAs a subset of the complex plane, an annulus can be considered as a Riemann surface. The complex structure of an annulus depends only on the ratio . Each annulus can be holomorphically mapped to a standard one centered at the origin and with outer radius 1 by the map\n\nThe inner radius is then .\n\nThe Hadamard three-circle theorem is a statement about the maximum value a holomorphic function may take inside an annulus.\n\n\n"}
{"id": "55728075", "url": "https://en.wikipedia.org/wiki?curid=55728075", "title": "Antonella Grassi", "text": "Antonella Grassi\n\nAntonella Grassi is a mathematician specializing in algebraic geometry and string theory. She is a Fellow of the American Mathematical Society.\n\nGrassi received her Ph.D. from Duke University under the supervision of David R. Morrison. Her dissertation was entitled \"Minimal Models of Elliptic Threefolds.\"\n\nGrassi is currently Professor of Mathematics at the University of Pennsylvania in Philadelphia, Pennsylvania. She has supervised two doctoral students, one at the University of Pennsylvania and the other at Università di Torino in Torino. She is an active participant in Women in Math at the University of Pennsylvania.\n\nGrassi has been a leader and mentor in the Institute for Advanced Study Program for Women in Mathematics; in particular, she organized the 2007 program on Algebraic Geometry and Group Actions.\n\nGrassi was elected to the 2018 class of fellows of the American Mathematical Society. Her citation read \"For contributions to algebraic geometry and mathematical physics, and for leadership in mentoring programs.\"\n"}
{"id": "2168889", "url": "https://en.wikipedia.org/wiki?curid=2168889", "title": "Asset allocation", "text": "Asset allocation\n\nAsset allocation is the rigorous implementation of an investment strategy that attempts to balance risk versus reward by adjusting the percentage of each asset in an investment portfolio according to the investor's risk tolerance, goals and investment time frame. The focus is on the characteristics of the overall portfolio. Such a strategy contrasts with an approach that focuses on individual assets.\n\nMany financial experts argue that asset allocation is an important factor in determining returns for an investment portfolio. Asset allocation is based on the principle that different assets perform differently in different market and economic conditions.\n\nA fundamental justification for asset allocation is the notion that different asset classes offer returns that are not perfectly correlated, hence diversification reduces the overall risk in terms of the variability of returns for a given level of expected return. Asset diversification has been described as \"the only free lunch you will find in the investment game\". Academic research has painstakingly explained the importance of asset allocation and the problems of active management (see academic studies section below).\n\nAlthough risk is reduced as long as correlations are not perfect, it is typically forecast (wholly or in part) based on statistical relationships (like correlation and variance) that existed over some past period. Expectations for return are often derived in the same way.\n\nWhen such backward-looking approaches are used to forecast future returns or risks using the traditional mean-variance optimization approach to asset allocation of modern portfolio theory (MPT), the strategy is, in fact, predicting future risks and returns based on history. As there is no guarantee that past relationships will continue in the future, this is one of the \"weak links\" in traditional asset allocation strategies as derived from MPT. Other, more subtle weaknesses include seemingly minor errors in forecasting leading to recommended allocations that are grossly skewed from investment mandates and/or impractical—often even violating an investment manager's \"common sense\" understanding of a tenable portfolio-allocation strategy.\n\nAn asset class is a group of economic resources sharing similar characteristics, such as riskiness and return. There are many types of assets that may or may not be included in an asset allocation strategy.\n\nThe \"traditional\" asset classes are \"stocks\", \"bonds\", and \"cash\":\nAllocation among these three provides a starting point. Usually included are hybrid instruments such as convertible bonds and preferred stocks, counting as a mixture of bonds and stocks.\n\nOther alternative assets that may be considered include:\n\nThere are several types of asset allocation strategies based on investment goals, risk tolerance, time frames and diversification. The most common forms of asset allocation are: strategic, dynamic, tactical, and core-satellite.\n\nThe primary goal of a strategic asset allocation is to create an asset mix that seeks to provide the optimal balance between expected risk and return for a long-term investment horizon. Generally speaking, strategic asset allocation strategies are agnostic to economic environments, i.e., they do not change their allocation postures relative to changing market or economic conditions. \n\nDynamic asset allocation is similar to strategic asset allocation in that portfolios are built by allocating to an asset mix that seeks to provide the optimal balance between expected risk and return for a long-term investment horizon. Like strategic allocation strategies, dynamic strategies largely retain exposure to their original asset classes; however, unlike strategic strategies, dynamic asset allocation portfolios will adjust their postures over time relative to changes in the economic environment. \n\nTactical asset allocation is a strategy in which an investor takes a more active approach that tries to position a portfolio into those assets, sectors, or individual stocks that show the most potential for perceived gains. While an original asset mix is formulated much like strategic and dynamic portfolio, tactical strategies are often traded more actively and are free to move entirely in and out of their core asset classes.\n\nCore-satellite allocation strategies generally contain a 'core' strategic element making up the most significant portion of the portfolio, while applying a dynamic or tactical 'satellite' strategy that makes up a smaller part of the portfolio. In this way, core-satellite allocation strategies are a hybrid of the strategic and dynamic/tactical allocation strategies mentioned above.\n\nA fund that holds more than one asset class is called an asset allocation fund. This includes many types such as \"balanced fund\" and so on.\n\nIn 1986, Gary P. Brinson, L. Randolph Hood, and SEI's Gilbert L. Beebower (BHB) published a study about asset allocation of 91 large pension funds measured from 1974 to 1983. They replaced the pension funds' stock, bond, and cash selections with corresponding market indexes. The indexed quarterly return were found to be higher than pension plan's actual quarterly return. The two quarterly return series' linear correlation was measured at 96.7%, with shared variance of 93.6%. A 1991 follow-up study by Brinson, Singer, and Beebower measured a variance of 91.5%. The conclusion of the study was that replacing active choices with simple asset classes worked just as well as, if not even better than, professional pension managers. Also, a small number of asset classes was sufficient for financial planning. Financial advisors often pointed to this study to support the idea that asset allocation is more important than all other concerns, which the BHB study lumped together as\n\"market timing\". One problem with the Brinson study was that the cost factor in the two return series was not clearly discussed. However, in response to a letter to the editor, Hood noted that the returns series were gross of management fees.\n\nIn 1997, William Jahnke initiated debate on this topic, attacking the BHB study in a paper titled “The Asset Allocation Hoax”. The Jahnke discussion appeared in the \"Journal of Financial Planning\" as an opinion piece, not a peer reviewed article. Jahnke's main criticism, still undisputed, was that BHB's use of quarterly data dampens the impact of compounding slight portfolio disparities over time, relative to the benchmark. One could compound 2% and 2.15% quarterly over 20 years and see the sizable difference in cumulative return. However, the difference is still 15 basis points (hundredths of a percent) per quarter; the difference is one of perception, not fact.\n\nIn 2000, Ibbotson and Kaplan used five asset classes in their study “Does Asset Allocation Policy Explain 40, 90, or 100 Percent of Performance?” The asset classes included were large-cap US stock, small-cap US stock, non-US stock, US bonds, and cash. Ibbotson and Kaplan examined the 10-year return of 94 US balanced mutual funds versus the corresponding indexed returns. This time, after properly adjusting for the cost of running index funds, the actual returns again failed to beat index returns. The linear correlation between monthly index return series and the actual monthly actual return series was measured at 90.2%, with shared variance of 81.4%. Ibbotson concluded 1) that asset allocation explained 40% of the variation of returns across funds, and 2) that it explained virtually 100% of the level of fund returns. Gary Brinson has expressed his general agreement with the Ibbotson-Kaplan conclusions.\n\nIn both studies, it is misleading to make statements such as \"asset allocation explains 93.6% of investment return\". Even \"asset allocation explains 93.6% of quarterly performance variance\" leaves much to be desired, because the shared variance could be from pension funds' operating structure. Hood, however, rejects this interpretation on the grounds that pension plans in particular cannot cross-share risks and that they are explicitly singular entities, rendering shared variance irrelevant. The statistics were most helpful when used to demonstrate the similarity of the index return series and the actual return series.\n\nA 2000 paper by Meir Statman found that using the same parameters that explained BHB's 93.6% variance result, a hypothetical financial advisor with perfect foresight in \"tactical\" asset allocation performed 8.1% better per year, yet the strategic asset allocation still explained 89.4% of the variance. Thus, explaining variance does not explain performance. Statman says that strategic asset allocation is movement \"along\" the efficient frontier, whereas tactical asset allocation involves movement \"of\" the efficient frontier. A more common sense explanation of the Brinson, Hood, and Beebower study is that asset allocation explains more than 90% of the volatility of returns of an overall portfolio, but will not explain the ending results of your portfolio over long periods of time. Hood notes in his review of the material over 20 years, however, that explaining performance over time is possible with the BHB approach but was not the focus of the original paper.\n\nBekkers, Doeswijk and Lam (2009) investigate the diversification benefits for a portfolio by distinguishing ten different investment categories simultaneously in a mean-variance analysis as well as a market portfolio approach. The results suggest that real estate, commodities, and high yield add most value to the traditional asset mix of stocks, bonds, and cash. A study with such a broad coverage of asset classes has not been conducted before, not in the context of determining capital market expectations and performing a mean-variance analysis, neither in assessing the global market portfolio.\n\nDoeswijk, Lam and Swinkels (2012) (2014) argue that the portfolio of the average investor contains important information for strategic asset allocation purposes. This portfolio shows the relative value of all assets according to the market crowd, which one could interpret as a benchmark or the optimal portfolio for the average investor. The authors determine the market values of equities, private equity, real estate, high yield bonds, emerging debt, non-government bonds, government bonds, inflation linked bonds, commodities, and hedge funds. For this range of assets, they estimate the invested global market portfolio for the period 1990-2012. For the main asset categories equities, real estate, non-government bonds and government bonds they extend the period to 1959-2012.\n\nDoeswijk, Lam and Swinkels (2017) show that the market portfolio realizes a compounded real return of 4.38% with a standard deviation of 11.6% from 1960 until 2015. In the inflationary period from 1960 to 1979, the compounded real return of the GMP is 2.27%, while this is 5.57% in the disinflationary period from 1980 to 2015. The reward for the average investor is a compounded return of 3.24%-points above the saver’s. \n\nMcGuigan described an examination of funds that were in the top quartile of performance during 1983 to 1993. During the second measurement period of 1993 to 2003, only 28.57% of the funds remained in the top quartile. 33.33% of the funds dropped to the second quartile. The rest of the funds dropped to the third or fourth quartile.\n\nIn fact, low cost was a more reliable indicator of performance. Bogle noted that an examination of five-year performance data of large-cap blend funds revealed that the lowest cost quartile funds had the best performance, and the highest cost quartile funds had the worst performance.\n\nIn asset allocation planning, the decision on the amount of stocks versus bonds in one's portfolio is a very important decision. Simply buying stocks without regard of a possible bear market can result in panic selling later. One's true risk tolerance can be hard to gauge until having experienced a real bear market with money invested in the market. Finding the proper balance is key.\n\nThe tables show why asset allocation is important. It determines an investor's future return, as well as the bear market burden that he or she will have to carry successfully to realize the returns.\n\nThere are various reasons why asset allocation fails to work.\n\n\n"}
{"id": "1241614", "url": "https://en.wikipedia.org/wiki?curid=1241614", "title": "Atoroidal", "text": "Atoroidal\n\nIn mathematics, an atoroidal 3-manifold is one that does not contain an essential torus.\nThere are two major variations in this terminology: a torus may be defined geometrically, as an embedded, non-boundary parallel, incompressible torus, or it may be defined algebraically, as a subgroup formula_1 of its fundamental group that is not conjugate to a peripheral subgroup (i.e. the image of the map on fundamental group induced by an inclusion of a boundary component). The terminology is not standardized, and different authors require atoroidal 3-manifolds to satisfy certain additional restrictions. For instance:\n\nA 3-manifold that is not atoroidal is called toroidal.\n"}
{"id": "1301302", "url": "https://en.wikipedia.org/wiki?curid=1301302", "title": "Augmented assignment", "text": "Augmented assignment\n\nAugmented assignment (or compound assignment) is the name given to certain \nassignment operators in certain programming languages (especially those derived from C). An augmented assignment is generally used to replace a statement where an operator takes a variable as one of its arguments and then assigns the result back to the same variable. A simple example is codice_1 which is expanded to codice_2. Similar constructions are often available for various binary operators.\n\nIn general, in languages offering this feature, most operators that can take a variable as one of their arguments and return a result of the same type have an augmented assignment equivalent that assigns the result back to the variable in place, including arithmetic operators, bitshift operators, and bitwise operators.\n\nFor example, the following statement or some variation of it can be found in many programs:\n\nThis means \"find the number stored in the variable \"x\", add 1 to it, and store the result of the addition in the variable \"x\".\" As simple as this seems, it may have an inefficiency, in that the location of variable \"x\" has to be looked up twice if the compiler does not recognize that two parts of the expression are identical: \"x\" might be a reference to some array element or other complexity. In comparison, here is the augmented assignment version:\n\nWith this version, there is no excuse for a compiler failing to generate code that looks up the location of variable \"x\" just once, and modifies it in place, if of course the machine code supports such a sequence. For instance, if x is a simple variable, the machine code sequence might be something like\nand the same code would be generated for both forms. But if there is a special op code, it might be\nmeaning \"Modify Memory\" by adding 1 to x, and a decent compiler would generate the same code for both forms. Some machine codes offer INC and DEC operations (to add or subtract one), others might allow constants other than one.\n\nMore generally, the form is\nwhere the ? stands for some operator (not always +), and there may be no special op codes to help. There is still the possibility that if \"x\" is a complicated entity the compiler will be encouraged to avoid duplication in accessing \"x\", and of course, if \"x\" is a lengthy name, there will be less typing required. This last was the basis of the similar feature in the ALGOL compilers offered via the Burroughs B6700 systems, using the tilde symbol to stand for the variable being assigned to, so that\nwould become\nand so forth. This is more general than just \"x:=~ + 1;\" Producing optimum code would remain the province of the compiler.\n\nIn expression-oriented programming languages such as C, assignment and augmented assignment are expressions, which have a value. This allows their use in complex expressions. However, this can produce sequences of symbols that are difficult to read or understand, and worse, a mistype can easily produce a different sequence of gibberish that although accepted by the compiler does not produce desired results. In other languages, such as Python, assignment and augmented assignment are statements, not expressions, and thus cannot be used in complex expressions. For example, the following is valid C, but not valid Python:\n\nAs with assignment, in these languages augmented assignment is a form of right-associative assignment.\n\nIn C, C++, and C#, the assignment operator is =, which is augmented as follows:\nEach of these is called a \"compound assignment\" operator in said languages.\n\nThe following list, though not complete or all-inclusive, lists some of the major programming languages that support augmented assignment operators.\n\n\n\n"}
{"id": "49460", "url": "https://en.wikipedia.org/wiki?curid=49460", "title": "Boole's syllogistic", "text": "Boole's syllogistic\n\nBoolean logic is a system of syllogistic logic invented by 19th-century British mathematician George Boole, which attempts to incorporate the \"empty set\", that is, a class of non-existent entities, such as round squares, without resorting to uncertain truth values.\n\nIn Boolean logic, the universal statements \"all S is P\" and \"no S is P\" (contraries in the traditional Aristotelian schema) are compossible provided that the set of \"S\" is the empty set. \"All S is P\" is construed to mean that \"there is nothing that is both S and not-P\"; \"no S is P\", that \"there is nothing that is both S and P\". For example, since there is nothing that is a round square, it is true both that nothing is a round square and purple, and that nothing is a round square and \"not\"-purple. Therefore, both universal statements, that \"all round squares are purple\" and \"no round squares are purple\" are true.\n\nSimilarly, the subcontrary relationship is dissolved between the existential statements \"some S is P\" and \"some S is not P\". The former is interpreted as \"there is some S such that S is P\" and the latter, \"there is some S such that S is not P\", both of which are clearly false where S is nonexistent.\n\nThus, the subaltern relationship between universal and existential also does not hold, since for a nonexistent S, \"All S is P\" is true but does not entail \"Some S is P\", which is false. Of the Aristotelian square of opposition, only the contradictory relationships remain intact.\n\n"}
{"id": "922464", "url": "https://en.wikipedia.org/wiki?curid=922464", "title": "Bunched logic", "text": "Bunched logic\n\nBunched logic is a variety of substructural logic proposed by Peter O'Hearn and David Pym. Bunched logic provides primitives for reasoning about \"resource composition\", which aid in the compositional analysis of computer and other systems. It has category-theoretic and truth-functional semantics which can be understood in terms of an abstract concept of resource, and a proof theory in which the contexts Γ in an entailment judgement Γ ⊢ A are tree-like structures (bunches) rather than lists or (multi)sets as in most proof calculi. Bunched logic has an associated type theory, and its first application was in providing a way to control the aliasing and other forms of interference in imperative programs.\nThe logic has seen further applications in program verification, where it is the basis of the assertion language of separation logic, and in systems modelling, where it provides a way to decompose the resources used by components of a system.\n\nThe deduction theorem of classical logic relates conjunction and implication:\nBunched logic has two versions of the deduction theorem:\nformula_3 and formula_4 are forms of conjunction and implication that take resources into account (explained below). In addition to these connectives\nbunched logic has a formula, sometimes written I or emp, which is the unit of *. In the original version of bunched logic formula_5 and formula_6 were the connectives from intuitionistic logic, while a boolean variant takes formula_5 and formula_6 (and formula_9) as from traditional boolean logic. Thus, bunched logic is compatible with constructive principles, but is in no way dependent on them.\n\nThe easiest way to understand these formulae is in terms of its truth-functional semantics. In this semantics a formula is true or false with respect to given resources. \nformula_10 asserts that the resource at hand can be decomposed into resources that satisfy formula_11 and formula_12. \nformula_13 says that if we compose the resource at hand with additional resource that satisfies formula_12, then the combined resource satisfies formula_15. formula_5 and formula_6 have their familiar meanings.\n\nThe foundation for this reading of formulae was provided by\na forcing semantics formula_18 advanced by Pym, where the forcing relation means `A holds of resource r`. The semantics is analogous to Kripke's semantics of intuitionistic or modal logic, but where the elements of the model are regarded as resources which can be composed and decomposed, rather than as possible worlds that are accessible from one another. For example, the forcing semantics for the conjunction is of the form\nwhere formula_20 is a way of combining resources and formula_21 is a relation of approximation.\n\nThis semantics of bunched logic draws on prior work in Relevant Logic (especially the operational semantics of Routley-Meyer), but differs from it by not requiring formula_22 and by accepting the semantics of standard intuitionistic or classical versions of formula_5 and formula_6. The property formula_22 is justified when thinking about relevance but denied by considerations of resource; having two copies of a resource is not the same as having one, and in some models (e.g. heap models) formula_26 might not even be defined. The standard semantics of formula_6 (or of negation) is often rejected by relevantists in their bid to escape the `paradoxes of material implication', which are not a problem from the perspective of modelling resources and so not rejected by bunched logic. The semantics is also related to the 'phase semantics' of linear logic, but again is differentiated by accepting the standard (even boolean) semantics of formula_5 and formula_6 which in linear logic is rejected in a bid to be constructive. These considerations are discussed in detail in an article on Resource semantics by Pym, O'Hearn and Yang.\n\nThe double version of the deduction theorem of bunched logic has a corresponding category-theoretic structure. Proofs in intuitionistic logic can be interpreted in \ncartesian closed categories, that is, categories with finite products satisfying the (natural in A and C) adjunction correspondence relating hom sets:\nBunched logic can be interpreted in categories possessing two such structures\nA host of categorial models can be given using Day's tensor product construction.\nAdditionally, implicational fragment of bunched logic has been given a games semantics.\n\nThe algebraic semantics of bunched logic is a special case of its categorical semantics, but is simple to state and can be more approachable.\nThe boolean version of bunched logic has models as follows.\n\nThe proof calculus of bunched logic\ndiffers from usual sequent calculi in having a tree-like context of hypotheses instead of a flat list-like structure. In its sequent-based proof theories, the \ncontext formula_32 in an entailment judgement formula_33\nis a tree whose leaves are propositions and whose internal nodes are labelled with modes of composition corresponding to the two conjunctions. \nThe two combining operators, comma and semicolon, are used (for instance) in the introduction rules for the two implications. \nThe difference between the two composition rules comes from additional rules that apply to them.\nThe structural rules and other operations on bunches are often applied deep within a tree-context, and not only at the top level: it is thus in a sense a calculus of deep inference.\n\nCorresponding to bunched logic is a type theory having two kinds of function type. Following the Curry–Howard correspondence, introduction rules for implications correspond to introduction rules for function types.\nHere, there are two distinct binders, formula_38 and formula_39, one for each kind of function type.\n\nThe proof theory of bunched logic has an historical debt to the use of bunches in Relevance logic. But the bunched structure can in a sense be derived from the categorical and algebraic semantics:\nto formulate an introduction rule for formula_40 we should mimick formula_41 on the left in sequents, and to introduce formula_42 we should mimick formula_5. This consideration leads to the use of two combining operators.\n\nJames Brotherston has done further significant work on a unified proof theory for bunched logic and variants, employing Belnap's notion of display logic.\n\nGalmiche, Méry, and Pym have provided a comprehensive treatment of bunched logic, including completeness and other meta-theory, based on labelled tableaux.\n\nIn perhaps the first use of substructural type theory to control resources, John C. Reynolds showed how to use an affine type theory to control aliasing and other forms of interference in Algol-like programming languages. O'Hearn used bunched type theory to extend Reynolds system by allowing interference and non-interference to be more flexibly mixed. This resolved open problems concerning recursion and jumps in Reynolds's system.\n\nSeparation logic is an extension of Hoare logic which facilitates reasoning about mutable data structures that use pointers. Following Hoare logic the formulae of separation logic are of the form\nformula_44, but the preconditions and postconditions are formulae interpreted in a model of bunched logic.\nThe original version of the logic was based on models as follows:\nIt is the undefinedness of the composition on overlapping heaps that models the separation idea. This is a model of the boolean variant of bunched logic.\n\nSeparation logic was used originally to prove sequential programs, but then was extended to concurrency using a proof rule\nthat divides the storage accessed by parallel threads.\n\nLater, the greater generality of the resource semantics was utilized: an abstract version of separation logic works for Hoare triples\nwhere the preconditions and postconditions are formulae interpreted over an arbitrary partial commutative monoid instead of a particular heap model. \nBy suitable choice of commutative monoid, it was surprisingly found that the proofs rules of abstract versions of concurrent separation logic could be used to reason about interfering concurrent processes, for example by encoding rely-guarantee and trace-based reasoning.\n\nSeparation logic is the basis of a number of tools for automatic and semi-automatic reasoning about programs, and is used in the Infer program analyzer currently deployed at Facebook.\n\nBunched logic has been used in connection with the (synchronous) resource-process calculus SCRP in order to give a (modal) logic which characterizes, in the sense of Hennessey-Milner, the compositional structure of concurrent systems.\n\nSCRP is notable for interpreting formula_48 in terms of \"both\" parallel composition of systems and composition of their associated resources.\nThe semantic clause of SCRP's process logic that corresponds to separation logic's rule for concurrency asserts that a formula formula_48 is true in resource-process state formula_50,formula_51 just in case there are decompositions of the resource formula_52 and process \nformula_53 ~ formula_54, where ~ denotes bisimulation, such that formula_11 is true in the resource-process state formula_56 , formula_57 and formula_12 is true in the resource-process state formula_59 , formula_60; that is formula_61 iff formula_62 and formula_63.\n\nThe system SCRP is based directly on bunched logic's resource semantics; that is, on ordered monoids of resource elements. While direct and intuitively appealing, this choice leads to a specific technical problem: the Hennessy-Milner completeness theorem holds only for fragments of the modal logic that exclude the multiplicative implication and multiplicative modalities. This problem is solved by basing resource-process calculus on a resource semantics in which resource elements are combined using two combinators, one corresponding to concurrent composition and one corresponding to choice.\n\nCardelli, Caires, Gordon and others have investigated a series of logics of process calculi, where a conjunction is interpreted in terms of parallel composition. [References, to add]\nUnlike the work of Pym et al. in SCRP, they do not distinguish between parallel composition of systems and composition of resources accessed by the systems.\n\nTheir logics are based on instances of the resource semantics which give rise to models of the boolean variant of bunched logic. \nAlthough these logics give rise to instances of boolean bunched logic, they appear to have been arrived at independently, and in any case have significant additional structure in the way of modalities and binders. Related logics have been proposed as well for modelling XML data.\n\n"}
{"id": "7393593", "url": "https://en.wikipedia.org/wiki?curid=7393593", "title": "Cash accumulation equation", "text": "Cash accumulation equation\n\nThe cash accumulation equation is an equation which calculates how much money will be in a bank account, at any point in time. The account pays interest, and is being fed a steady trickle of money.\n\nWe will approach the development of this equation by first considering the simpler case, that of just placing a lump sum in an account and then making no additions to the sum. With the usual notation, namely\n\nthe equation is\n\nand so the sum of money grows exponentially. Differentiating this we derive\n\nand applying the definition of from eqn (1) to eqn (2), yields\n\nNote that eqn. (1) is a particular solution to the ordinary differential equation in eqn. (3), with equal to at .\n\nHaving achieved this we are ready to start feeding money into the account, at a rate of formula_1 dollars/year. This is effected by making a small change to eqn (3) as follows\n\nand accordingly we need to solve the equation\n\nFrom a table of integrals, the solution is\n\nwhere formula_5 is the constant of integration. The initial sum deposited was formula_6 so we know one point on the curve :\n\nand making this substitution we find that\n\nUsing this expression for formula_5, and recalling that\n\ngives us the solution :\n\nThis is the neatest form of the cash accumulation equation, as we are calling it, but it not the most useful form. Using the exponential instead of the logarithmic function, the equation can be written out like this :\nFrom this new perspective, eqn (1) is just a special case of eqn (4) - namely with formula_12.\n\nFor completeness we will consider the case formula_13, and specifically the expression\nOne way of evaluating this is to write out the Maclaurin expansion\nAt a glance we can subtract formula_16 from this series and divide by formula_17, to find out that\nWith this result the cash accumulation equation now reads\nThus the cash sum just increases linearly, as expected, if no interest is being paid.\n\nThe only other special case to mention is formula_20. Upon making this substitution, eqn (4) becomes simply\nEvidently formula_1 is negative, and money is being withdrawn rather than deposited. Specifically, the interest is being withdrawn as fast as it is being earned.\n\nAn alternative interpretation of this special case is that formula_6 is negative - the account is overdrawn - and money is being fed in at a rate which just meets the interest charges. A force of interest value is always positive.\n\n"}
{"id": "162557", "url": "https://en.wikipedia.org/wiki?curid=162557", "title": "Collusion", "text": "Collusion\n\nCollusion is a secret agreement between two or more parties to limit open competition by deceiving, misleading, or defrauding others of their legal rights, or to obtain an objective forbidden by law typically by defrauding or gaining an unfair market advantage. It is an agreement among firms or individuals to divide a market, set prices, limit production or limit opportunities.\nIt can involve \"unions, wage fixing, kickbacks, or misrepresenting the independence of the relationship between the colluding parties\". In legal terms, all acts effected by collusion are considered void.\n\nIn the study of economics and market competition, collusion takes place within an industry when rival companies cooperate for their mutual benefit. Collusion most often takes place within the market structure of oligopoly, where the decision of a few firms to collude can significantly impact the market as a whole. Collusion which is covert, on the other hand, is known as tacit collusion, and is legal.\n\nAccording to neoclassical price-determination theory and game theory, the independence of suppliers forces prices to their minimum, increasing efficiency and decreasing the price determining ability of each individual firm. However, if firms collude to all increase prices, loss of sales is minimized, as consumers lack alternative choices at lower prices. This benefits the colluding firms at the cost of efficiency to society.\n\nOne variation of this traditional theory is the theory of kinked demand. Firms face a kinked demand curve if, when one firm decreases its price, other firms will follow suit in order to maintain sales, and when one firm increases its price, its rivals are unlikely to follow, as they would lose the sales' gains that they would otherwise get by holding prices at the previous level. Kinked demand potentially fosters supra-competitive prices because any one firm would receive a reduced benefit from cutting price, as opposed to the benefits accruing under neoclassical theory and certain game theoretic models such as Bertrand competition.\n\nPractices that suggest possible collusion include:\n\n\nCollusion is illegal in the United States, Canada and most of the EU due to antitrust laws, but implicit collusion in the form of price leadership and tacit understandings still takes place. Several examples of collusion in the United States include:\n\n\nThere are many ways that implicit collusion tends to develop:\n\nThere can be significant barriers to collusion. In any given industry, these may include:\n\n\n\n"}
{"id": "15910144", "url": "https://en.wikipedia.org/wiki?curid=15910144", "title": "Conway triangle notation", "text": "Conway triangle notation\n\nIn geometry, the Conway triangle notation, named after John Horton Conway, allows trigonometric functions of a triangle to be managed algebraically. Given a reference triangle whose sides are \"a\", \"b\" and \"c\" and whose corresponding internal angles are \"A\", \"B\", and \"C\" then the Conway triangle notation is simply represented as follows:\n\nwhere \"S\" = 2 × area of reference triangle and\n\nin particular\n\nFurthermore the convention uses a shorthand notation for formula_14 and formula_15\n\nHence:\n\nSome important identities:\n\nwhere \"R\" is the circumradius and \"abc\" = 2\"SR\" and where \"r\" is the incenter,   formula_22   and   formula_23\n\nSome useful trigonometric conversions:\n\nSome useful formulas:\n\nSome examples using Conway triangle notation:\n\nLet \"D\" be the distance between two points P and Q whose trilinear coordinates are \"p\" : \"p\" : \"p\" and \"q\" : \"q\" : \"q\". Let \"K\" = \"ap\" + \"bp\" + \"cp\" and let \"K\" = \"aq\" + \"bq\" + \"cq\". Then \"D\" is given by the formula:\n\nUsing this formula it is possible to determine OH, the distance between the circumcenter and the orthocenter as follows:\n\nFor the circumcenter \"p\" = \"aS\" and for the orthocenter \"q\" = \"S\"\"S\"/\"a\"\n\nHence:\n\nThis gives:\n"}
{"id": "4223273", "url": "https://en.wikipedia.org/wiki?curid=4223273", "title": "Economic capital", "text": "Economic capital\n\nIn finance, mainly for financial services firms, economic capital is the amount of risk capital, assessed on a realistic basis, which a firm requires to cover the risks that it is running or collecting as a going concern, such as market risk, credit risk, legal risk, and operational risk. It is the amount of money which is needed to secure survival in a worst-case scenario. Firms and financial services regulators should then aim to hold risk capital of an amount equal at least to economic capital.\n\nTypically, economic capital is calculated by determining the amount of capital that the firm needs to ensure that its realistic balance sheet stays solvent over a certain time period with a pre-specified probability. Therefore, economic capital is often calculated as value at risk. The balance sheet, in this case, would be prepared showing market value (rather than book value) of assets and liabilities.\n\nThe first accounts of economic capital date back to the ancient Phoenicians, who took rudimentary tallies of frequency and severity of illnesses among rural farmers to gain an intuition of expected losses in productivity. These calculations were advanced by correlations to climate change, political outbreak, and birth rate change.\n\nThe concept of economic capital differs from regulatory capital in the sense that regulatory capital is the mandatory capital the regulators require to be maintained while economic capital is the best estimate of required capital that financial institutions use internally to manage their own risk and to allocate the cost of maintaining regulatory capital among different units within the organization.\n\nIn social science, economic capital is distinguished in relation to other types of capital which may not necessarily reflect a monetary or exchange-value. These forms of capital include natural capital, cultural capital and social capital; the latter two represent a type of power or status that an individual can attain in a capitalist society via a formal education or through social ties. Non-economic forms of capital have been variously discussed most famously by sociologist Pierre Bourdieu.\n\n\n\n"}
{"id": "8364462", "url": "https://en.wikipedia.org/wiki?curid=8364462", "title": "Energetic space", "text": "Energetic space\n\nIn mathematics, more precisely in functional analysis, an energetic space is, intuitively, a subspace of a given real Hilbert space equipped with a new \"energetic\" inner product. The motivation for the name comes from physics, as in many physical problems the energy of a system can be expressed in terms of the energetic inner product. An example of this will be given later in the article. \n\nFormally, consider a real Hilbert space formula_1 with the inner product formula_2 and the norm formula_3. Let formula_4 be a linear subspace of formula_1 and formula_6 be a strongly monotone symmetric linear operator, that is, a linear operator satisfying \n\n\nThe energetic inner product is defined as \nand the energetic norm is\n\nThe set formula_4 together with the energetic inner product is a pre-Hilbert space. The energetic space formula_21 is defined as the completion of formula_4 in the energetic norm. formula_21 can be considered a subset of the original Hilbert space formula_24 since any Cauchy sequence in the energetic norm is also Cauchy in the norm of formula_1 (this follows from the strong monotonicity property of formula_26). \n\nThe energetic inner product is extended from formula_4 to formula_21 by\nwhere formula_30 and formula_31 are sequences in \"Y\" that converge to points in formula_21 in the energetic norm.\n\nThe operator formula_26 admits an energetic extension formula_34 \n\ndefined on formula_21 with values in the dual space formula_37 that is given by the formula \n\nHere, formula_41 denotes the duality bracket between formula_37 and formula_43 so formula_44 actually denotes formula_45\n\nIf formula_12 and formula_47 are elements in the original subspace formula_48 then\n\nby the definition of the energetic inner product. If one views formula_50 which is an element in formula_24 as an element in the dual formula_52 via the Riesz representation theorem, then formula_53 will also be in the dual formula_54 (by the strong monotonicity property of formula_26). Via these identifications, it follows from the above formula that formula_56 In different words, the original operator formula_6 can be viewed as an operator formula_58 and then formula_35 is simply the function extension of formula_26 from formula_4 to formula_40 \n\nConsider a string whose endpoints are fixed at two points formula_63 formula_64 on the string be formula_65, where formula_66 is a unit vector pointing vertically and formula_67 Let formula_68 be the deflection of the string at the point formula_69 under the influence of the force. Assuming that the deflection is small, the elastic energy of the string is \n\nand the total potential energy of the string is\n\nThe deflection formula_68 minimizing the potential energy will satisfy the differential equation\n\nwith boundary conditions\n\nTo study this equation, consider the space formula_75 that is, the Lp space of all square-integrable functions formula_76 in respect to the Lebesgue measure. This space is Hilbert in respect to the inner product\n\nwith the norm being given by \n\nLet formula_4 be the set of all twice continuously differentiable functions formula_76 with the boundary conditions formula_81 Then formula_4 is a linear subspace of formula_83\n\nConsider the operator formula_6 given by the formula\n\nso the deflection satisfies the equation formula_86 Using integration by parts and the boundary conditions, one can see that \n\nfor any formula_12 and formula_47 in formula_13 Therefore, formula_26 is a symmetric linear operator. \n\nformula_26 is also strongly monotone, since, by the Friedrichs's inequality \n\nfor some formula_94\n\nThe energetic space in respect to the operator formula_26 is then the Sobolev space formula_96 We see that the elastic energy of the string which motivated this study is \n\nso it is half of the energetic inner product of formula_12 with itself. \n\nTo calculate the deflection formula_12 minimizing the total potential energy formula_100 of the string, one writes this problem in the form \n\nNext, one usually approximates formula_12 by some formula_105, a function in a finite-dimensional subspace of the true solution space. For example, one might let formula_105 be a continuous piecewise linear function in the energetic space, which gives the finite element method. The approximation formula_105 can be computed by solving a system of linear equations.\n\nThe energetic norm turns out to be the natural norm in which to measure the error between formula_12 and formula_105, see Céa's lemma.\n\n"}
{"id": "3966889", "url": "https://en.wikipedia.org/wiki?curid=3966889", "title": "Frobenius theorem (real division algebras)", "text": "Frobenius theorem (real division algebras)\n\nIn mathematics, more specifically in abstract algebra, the Frobenius theorem, proved by Ferdinand Georg Frobenius in 1877, characterizes the finite-dimensional associative division algebras over the real numbers. According to the theorem, every such algebra is isomorphic to one of the following:\nThese algebras have real dimension , and , respectively. Of these three algebras, and are commutative, but is not.\n\nThe main ingredients for the following proof are the Cayley–Hamilton theorem and the fundamental theorem of algebra.\n\n\nThe key to the argument is the following\n\nProof of Claim: Let be the dimension of as an -vector space, and pick in with characteristic polynomial . By the fundamental theorem of algebra, we can write\n\nWe can rewrite in terms of the polynomials :\n\nSince , the polynomials are all irreducible over . By the Cayley–Hamilton theorem, and because is a division algebra, it follows that either for some or that for some . The first case implies that is real. In the second case, it follows that is the minimal polynomial of . Because has the same complex roots as the minimal polynomial and because it is real it follows that\n\nSince is the characteristic polynomial of the coefficient of in is up to a sign. Therefore, we read from the above equation we have: if and only if , in other words if and only if .\n\nSo is the subset of all with . In particular, it is a vector subspace. Moreover, has codimension since it is the kernel of a non-zero linear form, and note that is the direct sum of and as vector spaces.\n\nFor in define . Because of the identity , it follows that is real. Furthermore, since , we have: for . Thus is a positive definite symmetric bilinear form, in other words, an inner product on .\n\nLet be a subspace of that generates as an algebra and which is minimal with respect to this property. Let be an orthonormal basis of . With respect to the negative definite bilinear form these elements satisfy the following relations:\n\nIf , then is isomorphic to .\n\nIf , then is generated by and subject to the relation . Hence it is isomorphic to .\n\nIf , it has been shown above that is generated by subject to the relations \nThese are precisely the relations for .\n\nIf , then cannot be a division algebra. Assume that . Let . It is easy to see that (this only works if ). If were a division algebra, implies , which in turn means: and so generate . This contradicts the minimality of .\n\n\n"}
{"id": "25197704", "url": "https://en.wikipedia.org/wiki?curid=25197704", "title": "Galois geometry", "text": "Galois geometry\n\nGalois geometry (so named after the 19th century French Mathematician Évariste Galois) is the branch of finite geometry that is concerned with algebraic and analytic geometry over a finite field (or \"Galois field\"). More narrowly, \"a\" Galois geometry may be defined as a projective space over a finite field.\n\nObjects of study include vector spaces, affine and projective spaces over finite fields and various structures that are contained in them. In particular, arcs, ovals, hyperovals, unitals, blocking sets, ovoids, caps, spreads and all finite analogues of structures found in non-finite geometries.\n\nGeorge Conwell gave an early demonstration of Galois geometry in 1910 when he characterized a solution of Kirkman's schoolgirl problem as a partition of sets of skew lines in PG(3,2), the three-dimensional projective geometry over the Galois field GF(2).\nSimilar to methods of line geometry in space over a field of characteristic 0, Conwell used Plücker coordinates in PG(5,2) and identified the points representing lines in PG(3,2) as those on the Klein quadric.\n\nIn 1955 Beniamino Segre characterized the ovals for \"q\" odd. Segre's theorem states that in a Galois geometry of odd order (a projective plane defined over a finite field of odd characteristic) every oval is a conic. At the 1958 International Mathematical Congress Segre presented a survey of results in Galois geometry known up to then.\n\n\n\n"}
{"id": "32688684", "url": "https://en.wikipedia.org/wiki?curid=32688684", "title": "Graph energy", "text": "Graph energy\n\nIn mathematics, the energy of a graph is the sum of the absolute values of the eigenvalues of the adjacency matrix of the graph. This quantity is studied in the context of spectral graph theory.\n\nMore precisely, let \"G\" be a graph with \"n\" vertices. It is assumed that \"G\" is simple, that is, it does not contain loops or parallel edges. Let \"A\" be the adjacency matrix of \"G\" and let formula_1, formula_2, be the eigenvalues of \"A\". Then the energy of the graph is defined as:\n\n"}
{"id": "11973947", "url": "https://en.wikipedia.org/wiki?curid=11973947", "title": "Graph partition", "text": "Graph partition\n\nIn mathematics, the graph partition problem is defined on data represented in the form of a graph \"G\" = (\"V\",\"E\"), with \"V\" vertices and \"E\" edges, such that it is possible to partition \"G\" into smaller components with specific properties. For instance, a \"k\"-way partition divides the vertex set into \"k\" smaller components. A good partition is defined as one in which the number of edges running between separated components is small. Uniform graph partition is a type of graph partitioning problem that consists of dividing a graph into components, such that the components are of about the same size and there are few connections between the components. Important applications of graph partitioning include scientific computing, partitioning various stages of a VLSI design circuit and task scheduling in multi-processor systems. Recently, the graph partition problem has gained importance due to its application for clustering and detection of cliques in social, pathological and biological networks. For a survey on recent trends in computational methods and applications see .\n\nTypically, graph partition problems fall under the category of NP-hard problems. Solutions to these problems are generally derived using heuristics and approximation algorithms. However, uniform graph partitioning or a balanced graph partition problem can be shown to be NP-complete to approximate within any finite factor. Even for special graph classes such as trees and grids, no reasonable approximation algorithms exist, unless P=NP. Grids are a particularly interesting case since they model the graphs resulting from Finite Element Model (FEM) simulations. When not only the number of edges between the components is approximated, but also the sizes of the components, it can be shown that no reasonable fully polynomial algorithms exist for these graphs.\n\nConsider a graph \"G\" = (\"V\", \"E\"), where \"V\" denotes the set of \"n\" vertices and \"E\" the set of edges. For a (\"k\",\"v\") balanced partition problem, the objective is to partition \"G\" into \"k\" components of at most size \"v\" · (\"n\"/\"k\"), while minimizing the capacity of the edges between separate components. Also, given \"G\" and an integer \"k\" > 1, partition \"V\" into \"k\" parts (subsets) \"V\", \"V\", ..., \"V\" such that the parts are disjoint and have equal size, and the number of edges with endpoints in different parts is minimized. Such partition problems have been discussed in literature as bicriteria-approximation or resource augmentation approaches. A common extension is to hypergraphs, where an edge can connect more than two vertices. A hyperedge is not cut if all vertices are in one partition, and cut exactly once otherwise, no matter how many vertices are on each side. This usage is common in electronic design automation.\n\nFor a specific (\"k\", 1 + \"ε\") balanced partition problem, we seek to find a minimum cost partition of \"G\" into \"k\" components with each component containing maximum of (1 + \"ε\")·(\"n\"/\"k\") nodes. We compare the cost of this approximation algorithm to the cost of a (\"k\",1) cut, wherein each of the \"k\" components must have exactly the same size of (\"n\"/\"k\") nodes each, thus being a more restricted problem. Thus,\n\nWe already know that (2,1) cut is the minimum bisection problem and it is NP complete. Next we assess a 3-partition problem wherein \"n\" = 3\"k\", which is also bounded in polynomial time. Now, if we assume that we have an finite approximation algorithm for (\"k\", 1)-balanced partition, then, either the 3-partition instance can be solved using the balanced (\"k\",1) partition in \"G\" or it cannot be solved. If the 3-partition instance can be solved, then (\"k\", 1)-balanced partitioning problem in \"G\" can be solved without cutting any edge. Otherwise if the 3-partition instance cannot be solved, the optimum (\"k\", 1)-balanced partitioning in \"G\" will cut at least one edge. An approximation algorithm with finite approximation factor has to differentiate between these two cases. Hence, it can solve the 3-partition problem which is a contradiction under the assumption that \"P\" = \"NP\". Thus, it is evident that (\"k\",1)-balanced partitioning problem has no polynomial time approximation algorithm with finite approximation factor unless \"P\" = \"NP\".\n\nThe planar separator theorem states that any \"n\"-vertex planar graph can be partitioned into roughly equal parts by the removal of O() vertices. This is not a partition in the sense described above, because the partition set consists of vertices rather than edges. However, the same result also implies that every planar graph of bounded degree has a balanced cut with O() edges.\n\nSince graph partitioning is a hard problem, practical solutions are based on heuristics. There are two broad categories of methods, local and global. Well known local methods are the Kernighan–Lin algorithm, and Fiduccia-Mattheyses algorithms, which were the first effective 2-way cuts by local search strategies. Their major drawback is the arbitrary initial partitioning of the vertex set, which can affect the final solution quality. Global approaches rely on properties of the entire graph and do not rely on an arbitrary initial partition. The most common example is spectral partitioning, where a partition is derived from the spectrum of the adjacency matrix.\n\nA multi-level graph partitioning algorithm works by applying one or more stages. Each stage reduces the size of\nthe graph by collapsing vertices and edges, partitions the smaller graph, then maps back and refines this partition of the original graph. A wide variety of partitioning and refinement methods can be applied within the overall multi-level scheme. In many cases, this approach can give both fast execution times and very high quality results. \nOne widely used example of such an approach is METIS, a graph partitioner, and hMETIS, the corresponding partitioner for hypergraphs.\n\nGiven a graph formula_2 with adjacency matrix formula_3, where an entry formula_4 implies an edge between node formula_5 and formula_6, and degree matrix formula_7, which is a diagonal matrix, where each diagonal entry of a row formula_5, formula_9, represents the node degree of node formula_5. The Laplacian matrix formula_11 is defined as formula_12. Now, a ratio-cut partition for graph formula_13 is defined as a partition of formula_14 into disjoint formula_15, and formula_16, minimizing the ratio\nof the number of edges that actually cross this cut to the number of pairs of vertices that could support such edges. Spectral graph partitioning can be motivated by analogy with partitioning of a vibrating string or a mass-spring system.\n\nIn such a scenario, the second smallest eigenvalue (formula_18) of formula_11, yields a \"lower bound\" on the optimal cost (formula_20) of ratio-cut partition with formula_21. The eigenvector (formula_22) corresponding to formula_18, called the \"Fiedler vector\", bisects the graph into only two communities based on the \"sign of the corresponding vector entry\". Division into a larger number of communities can be achieved by repeated \"bisection\" or by using \"multiple eigenvectors\" corresponding to the smallest eigenvalues. The examples in Figures 1,2 illustrate the spectral bisection approach.\n\nMinimum cut partitioning however fails when the number of communities to be partitioned, or the partition sizes are unknown. For instance, optimizing the cut size for free group sizes puts all vertices in the same community. Additionally, cut size may be the wrong thing to minimize since a good division is not just one with small number of edges between communities. This motivated the use of Modularity (Q) as a metric to optimize a balanced graph partition. The example in Figure 3 illustrates 2 instances of the same graph such that in \"(a)\" modularity (Q) is the partitioning metric and in \"(b)\", ratio-cut is the partitioning metric.\n\nAnother objective function used for graph partitioning is Conductance which is the ratio between the number of cut edges and the volume of the smallest part. Conductance is related to electrical flows and random walks. The Cheeger bound guarantees that spectral bisection provides partitions with nearly optimal conductance. The quality of this approximation depends on the second smallest eigenvalue of the Laplacian λ.\n\nSpin models have been used for clustering of multivariate data wherein similarities are translated into coupling strengths. The properties of ground state spin configuration can be directly interpreted as communities. Thus, a graph is partitioned to minimize the Hamiltonian of the partitioned graph. The Hamiltonian (H) is derived by assigning the following partition rewards and penalties.\n\nAdditionally, Kernel PCA based Spectral clustering takes a form of least squares Support Vector Machine framework, and hence it becomes possible to project the data entries to a kernel induced feature space that has maximal variance, thus implying a high separation between the projected communities.\n\nSome methods express graph partitioning as a multi-criteria optimization problem which can be solved using local methods expressed in a game theoretic framework where each node makes a decision on the partition it chooses.\n\nChaco, due to Hendrickson and Leland, implements the multilevel approach outlined above and basic local search algorithms. \nMoreover, they implement spectral partitioning techniques.\n\nMETIS is a graph partitioning family by Karypis and Kumar. Among this family, kMetis aims at greater partitioning speed, hMetis, applies to hypergraphs and aims at partition quality, and ParMetis is a parallel implementation of the Metis graph partitioning algorithm.\n\nPaToH is another hypergraph partitioner.\n\nKaHyPar is a multilevel hypergraph partitioning framework providing direct k-way and recursive bisection based partitioning algorithms. It instantiates the multilevel approach in its most extreme version, removing only a single vertex in every level of the hierarchy. By using this very fine grained \"n\"-level approach combined with strong local search heuristics, it computes solutions of very high quality.\n\nScotch is graph partitioning framework by Pellegrini. It uses recursive multilevel bisection and includes sequential as well as parallel partitioning techniques.\n\nJostle is a sequential and parallel graph partitioning solver developed by Chris Walshaw. \nThe commercialized version of this partitioner is known as NetWorks.\n\nParty implements the Bubble/shape-optimized framework and the Helpful Sets algorithm.\n\nThe software packages DibaP and its MPI-parallel variant PDibaP by Meyerhenke implement the Bubble framework using diffusion; DibaP also uses AMG-based techniques for coarsening and solving linear systems arising in the diffusive approach.\n\nSanders and Schulz released a graph partitioning package KaHIP (Karlsruhe High Quality Partitioning) that implements for example flow-based methods, more-localized local searches and several parallel and sequential meta-heuristics.\n\nThe tools Parkway by Trifunovic and\nKnottenbelt as well as Zoltan by Devine et al. focus on hypergraph\npartitioning.\n\nList of free open-source frameworks:\n\n"}
{"id": "1139926", "url": "https://en.wikipedia.org/wiki?curid=1139926", "title": "Hyperbolic sector", "text": "Hyperbolic sector\n\nA hyperbolic sector is a region of the Cartesian plane {(\"x\",\"y\")} bounded by rays from the origin to two points (\"a\", 1/\"a\") and (\"b\", 1/\"b\") and by the rectangular hyperbola \"xy\" = 1 (or the corresponding region when this hyperbola is rescaled and its orientation is altered by a rotation leaving the center at the origin, as with the unit hyperbola).\n\nA hyperbolic sector in standard position has \"a\" = 1 and \"b\" > 1 .\n\nHyperbolic sectors are the basis for the hyperbolic functions.\n\nThe area of a hyperbolic sector in standard position is ln \"b\" .\n\nProof: Integrate under 1/\"x\" from 1 to \"b\", add triangle {(0, 0), (1, 0), (1, 1)}, and subtract triangle {(0, 0), (\"b\", 0), (\"b\", 1/\"b\")}.\nWhen in standard position, a hyperbolic sector corresponds to a positive hyperbolic angle at the origin, with the measure of the latter being defined as the area of the former.\n\nWhen in standard position, a hyperbolic sector determines a hyperbolic triangle, the right triangle with one vertex at the origin, base on the diagonal ray \"y\" = \"x\", and third vertex on the hyperbola \n\nwith the hypotenuse being the segment from the origin to the point (\"x, y\") on the hyperbola. The length of the base of this triangle is \nand the altitude is \nwhere \"u\" is the appropriate hyperbolic angle.\n\nThe analogy between circular and hyperbolic functions was described by Augustus De Morgan in his \"Trigonometry and Double Algebra\" (1849). William Burnside used such triangles, projecting from a point on the hyperbola \"xy\" = 1 onto the main diagonal, in his article \"Note on the addition theorem for hyperbolic functions\".\n\nStudents of integral calculus know that f(\"x\") = \"x\" has an algebraic antiderivative except in the case \"p\" = –1 corresponding to the quadrature of the hyperbola. The other cases are given by Cavalieri's quadrature formula. Whereas quadrature of the parabola had been accomplished by Archimedes in the third century BC (in \"The Quadrature of the Parabola\"), the hyperbolic quadrature required the invention in 1647 of a new function: Gregoire de Saint-Vincent addressed the problem of computing the areas bounded by a hyperbola. His findings led to the natural logarithm function, once called the hyperbolic logarithm since it is obtained by integrating, or finding the area, under the hyperbola.\n\nBefore 1748 and the publication of Introduction to the Analysis of the Infinite, the natural logarithm was known in terms of the area of a hyperbolic sector. Leonhard Euler changed that when he introduced transcendental functions such as 10. Euler identified e as the value of \"b\" producing a unit of area (under the hyperbola or in a hyperbolic sector in standard position). Then the natural logarithm could be recognized as the inverse function to the transcendental function e.\n\nWhen Felix Klein wrote his book on non-Euclidean geometry in 1928, he provided a foundation for the subject by reference to projective geometry. To establish hyperbolic measure on a line, he noted that the area of a hyperbolic sector provided visual illustration of the concept.\n\nHyperbolic sectors can also be drawn to the hyperbola formula_4. The area of such hyperbolic sectors has been used to define hyperbolic distance in a geometry textbook.\n\n\n"}
{"id": "2055223", "url": "https://en.wikipedia.org/wiki?curid=2055223", "title": "Hypergeometric function", "text": "Hypergeometric function\n\nIn mathematics, the Gaussian or ordinary hypergeometric function \"F\"(\"a\",\"b\";\"c\";\"z\") is a special function represented by the hypergeometric series, that includes many other special functions as specific or limiting cases. It is a solution of a second-order linear ordinary differential equation (ODE). Every second-order linear ODE with three regular singular points can be transformed into this equation.\n\nFor systematic lists of some of the many thousands of published identities involving the hypergeometric function, see the reference works by and . There is no known system for organizing all of the identities; indeed, there is no known algorithm that can generate all identities; a number of different algorithms are known that generate different series of identities. The theory of the algorithmic discovery of identities remains an active research topic.\n\nThe term \"hypergeometric series\" was first used by John Wallis in his 1655 book \"Arithmetica Infinitorum\".\n\nHypergeometric series were studied by Leonhard Euler, but \"the first full systematic treatment\" was given by .\n\nStudies in the nineteenth century included those of , and the fundamental characterisation by of the hypergeometric function by means of the differential equation it satisfies.\n\nRiemann showed that the second-order differential equation for \"F\"(\"z\"), examined in the complex plane, could be characterised (on the Riemann sphere) by its three regular singularities.\n\nThe cases where the solutions are algebraic functions were found by Hermann Schwarz (Schwarz's list).\n\nThe hypergeometric function is defined for by the power series\n\nIt is undefined (or infinite) if \"c\" equals a non-positive integer. Here (\"q\") is the (rising) Pochhammer symbol, which is defined by:\n\nThe series terminates if either \"a\" or \"b\" is a nonpositive integer, in which case the function reduces to a polynomial:\n\nFor complex arguments \"z\" with |\"z\"| ≥ 1 it can be analytically continued along any path in the complex plane that avoids the branch points 1 and infinity.\n\nAs , where is a non-negative integer, , but if we divide by , we have a limit:\n\nUsing the identity formula_5, it is easily shown that\n\nformula_6\n\nand more generally,\n\nformula_7\n\nIn the special case that formula_8, we have\n\nformula_9\n\nMany of the common mathematical functions can be expressed in terms of the hypergeometric function, or as limiting cases of it. Some typical examples are\n\nThe confluent hypergeometric function (or Kummer's function) can be given as a limit of the hypergeometric function\n\nso all functions that are essentially special cases of it, such as Bessel functions, can be expressed as limits of hypergeometric functions. These include most of the commonly used functions of mathematical physics.\n\nLegendre functions are solutions of a second order differential equation with 3 regular singular points so can be expressed in terms of the hypergeometric function in many ways, for example\n\nSeveral orthogonal polynomials, including Jacobi polynomials \"P\" and their special cases Legendre polynomials, Chebyshev polynomials, Gegenbauer polynomials can be written in terms of hypergeometric functions using\n\nOther polynomials that are special cases include Krawtchouk polynomials, Meixner polynomials, Meixner–Pollaczek polynomials.\n\nElliptic modular functions can sometimes be expressed as the inverse functions of ratios of hypergeometric functions whose arguments \"a\", \"b\", \"c\" are 1, 1/2, 1/3, ... or 0. For example, if\n\nwhich can be restated as\n\nwhenever −π < \"x\" < π and \"T\" is the (generalized) Chebyshev polynomial.\n\n\n\n"}
{"id": "24109545", "url": "https://en.wikipedia.org/wiki?curid=24109545", "title": "Implicit graph", "text": "Implicit graph\n\nIn the study of graph algorithms, an implicit graph representation (or more simply implicit graph) is a graph whose vertices or edges are not represented as explicit objects in a computer's memory, but rather are determined algorithmically from some more concise input.\n\nThe notion of an implicit graph is common in various search algorithms which are described in terms of graphs. In this context, an implicit graph may be defined as a set of rules to define all neighbors for any specified vertex. This type of implicit graph representation is analogous to an adjacency list, in that it provides easy access to the neighbors of each vertex. For instance, in searching for a solution to a puzzle such as Rubik's Cube, one may define an implicit graph in which each vertex represents one of the possible states of the cube, and each edge represents a move from one state to another. It is straightforward to generate the neighbors of any vertex by trying all possible moves in the puzzle and determining the states reached by each of these moves; however, an implicit representation is necessary, as the state space of Rubik's Cube is too large to allow an algorithm to list all of its states.\n\nIn computational complexity theory, several complexity classes have been defined in connection with implicit graphs, defined as above by a rule or algorithm for listing the neighbors of a vertex. For instance, PPA is the class of problems in which one is given as input an undirected implicit graph (in which vertices are -bit binary strings, with a polynomial time algorithm for listing the neighbors of any vertex) and a vertex of odd degree in the graph, and must find a second vertex of odd degree. By the handshaking lemma, such a vertex exists; finding one is a problem in NP, but the problems that can be defined in this way may not necessarily be NP-complete, as it is unknown whether PPA = NP. PPAD is an analogous class defined on implicit directed graphs that has attracted attention in algorithmic game theory because it contains the problem of computing a Nash equilibrium. The problem of testing reachability of one vertex to another in an implicit graph may also be used to characterize space-bounded nondeterministic complexity classes including NL (the class of problems that may be characterized by reachability in implicit directed graphs whose vertices are -bit bitstrings), SL (the analogous class for undirected graphs), and PSPACE (the class of problems that may be characterized by reachability in implicit graphs with polynomial-length bitstrings). In this complexity-theoretic context, the vertices of an implicit graph may represent the states of a nondeterministic Turing machine, and the edges may represent possible state transitions, but implicit graphs may also be used to represent many other types of combinatorial structure. PLS, another complexity class, captures the complexity of finding local optima in an implicit graph.\n\nImplicit graph models have also been used as a form of relativization in order to prove separations between complexity classes that are stronger than the known separations for non-relativized models. For instance, Childs et al. used neighborhood representations of implicit graphs to define a graph traversal problem that can be solved in polynomial time on a quantum computer but that requires exponential time to solve on any classical computer.\n\nIn the context of efficient representations of graphs, J. H. Muller defined a \"local structure\" or \"adjacency labeling scheme\" for a graph in a given family of graphs to be an assignment of an -bit identifier to each vertex of , together with an algorithm (that may depend on but is independent of the individual graph ) that takes as input two vertex identifiers and determines whether or not they are the endpoints of an edge in . That is, this type of implicit representation is analogous to an adjacency matrix: it is straightforward to check whether two vertices are adjacent but finding the neighbors of any vertex may involve looping through all vertices and testing which ones are neighbors.\n\nGraph families with adjacency labeling schemes include:\n\nNot all graph families have local structures. For some families, a simple counting argument proves that adjacency labeling schemes do not exist: only bits may be used to represent an entire graph, so a representation of this type can only exist when the number of -vertex graphs in the given family is at most . Graph families that have larger numbers of graphs than this, such as the bipartite graphs or the triangle-free graphs, do not have adjacency labeling schemes. However, even families of graphs in which the number of graphs in the family is small might not have an adjacency labeling scheme; for instance, the family of graphs with fewer edges than vertices has -vertex graphs but does not have an adjacency labeling scheme, because one could transform any given graph into a larger graph in this family by adding a new isolated vertex for each edge, without changing its labelability. Kannan et al. asked whether having a forbidden subgraph characterization and having at most -vertex graphs are together enough to guarantee the existence of an adjacency labeling scheme; this question, which Spinrad restated as a conjecture, remains open.\nAmong the families of graphs which satisfy the conditions of the conjecture and for which there is no known adjacency labeling scheme are the family of disk graphs and line segment intersection graphs.\n\nIf a graph family has an adjacency labeling scheme, then the -vertex graphs in may be represented as induced subgraphs of a common induced universal graph of polynomial size, the graph consisting of all possible vertex identifiers. Conversely, if an induced universal graph of this type can be constructed, then the identities of its vertices may be used as labels in an adjacency labeling scheme. For this application of implicit graph representations, it is important that the labels use as few bits as possible, because the number of bits in the labels translates directly into the number of vertices in the induced universal graph. Alstrup and Rauhe showed that any tree has an adjacency labeling scheme with bits per label, from which it follows that any graph with arboricity \"k\" has a scheme with bits per label and a universal graph with vertices. In particular, planar graphs have arboricity at most three, so they have universal graphs with a nearly-cubic number of vertices.\nThis bound was improved by Gavoille and Labourel who showed that planar graphs and minor-closed graph families have a labeling scheme with bits per label, and that graphs of bounded treewidth have a labeling scheme with bits per label.\n\nThe Aanderaa–Karp–Rosenberg conjecture concerns implicit graphs given as a set of labeled vertices with a black-box rule for determining whether any two vertices are adjacent. This definition differs from an adjacency labeling scheme in that the rule may be specific to a particular graph rather than being a generic rule that applies to all graphs in a family. Because of this difference, every graph has an implicit representation. For instance, the rule could be to look up the pair of vertices in a separate adjacency matrix. However, an algorithm that is given as input an implicit graph of this type must operate on it only through the implicit adjacency test, without reference to how the test is implemented.\n\nA \"graph property\" is the question of whether a graph belongs to a given family of graphs; the answer must remain invariant under any relabeling of the vertices. In this context, the question to be determined is how many pairs of vertices must be tested for adjacency, in the worst case, before the property of interest can be determined to be true or false for a given implicit graph. Rivest and Vuillemin proved that any deterministic algorithm for any nontrivial graph property must test a quadratic number of pairs of vertices. The full Aanderaa–Karp–Rosenberg conjecture is that any deterministic algorithm for a monotonic graph property (one that remains true if more edges are added to a graph with the property) must in some cases test every possible pair of vertices. Several cases of the conjecture have been proven to be true—for instance, it is known to be true for graphs with a prime number of vertices—but the full conjecture remains open. Variants of the problem for randomized algorithms and quantum algorithms have also been studied.\n\nBender and Ron have shown that, in the same model used for the evasiveness conjecture, it is possible in only constant time to distinguish directed acyclic graphs from graphs that are very far from being acyclic. In contrast, such a fast time is not possible in neighborhood-based implicit graph models,\n\n"}
{"id": "18006921", "url": "https://en.wikipedia.org/wiki?curid=18006921", "title": "Interfaces (journal)", "text": "Interfaces (journal)\n\nInterfaces is a bimonthly peer-reviewed academic journal about operations research that was established by The Institute of Management Sciences, now part of the Institute for Operations Research and the Management Sciences. The journal's distinguishing feature is its case-study style: It offers examples of how operations research theory has been applied in businesses and organizations.\n\nAn annual feature is an issue with papers by the previous year's Franz Edelman Award participants.\n"}
{"id": "7634908", "url": "https://en.wikipedia.org/wiki?curid=7634908", "title": "Intertemporal CAPM", "text": "Intertemporal CAPM\n\nThe Intertemporal Capital Asset Pricing Model, or ICAPM, is an alternative to the CAPM provided by Robert Merton. It is a linear factor model with wealth as state variable that forecast changes in the distribution of future returns or income.\n\nIn the ICAPM investors are solving lifetime consumption decisions when faced with more than one uncertainty. The main difference between ICAPM and standard CAPM is the additional state variables that acknowledge the fact that investors hedge against shortfalls in consumption or against changes in the future investment opportunity set.\n\nMerton considers a continuous time market in equilibrium.\nThe state variable (X) follows a brownian motion:\nThe investor maximizes his Von Neumann–Morgenstern utility:\nwhere T is the time horizon and B[W(T),T] the utility from wealth (W).\n\nThe investor has the following constraint on wealth (W). \nLet formula_3 be the weight invested in the asset i. Then:\nwhere formula_5 is the return on asset i.\nThe change in wealth is:\n\nWe can use dynamic programming to solve the problem. For instance, if we consider a series of discrete time problems:\nThen, a Taylor expansion gives:\nwhere formula_9 is a value between t and t+dt.\n\nAssuming that returns follow a brownian motion:\nwith:\nThen canceling out terms of second and higher order:\n\nUsing Bellman equation, we can restate the problem:\nsubject to the wealth constraint previously stated.\n\nUsing Ito's lemma we can rewrite:\nand the expected value:\nAfter some algebra\n, we have the following objective function:\nwhere formula_17 is the risk-free return.\nFirst order conditions are:\nIn matrix form, we have:\nwhere formula_20 is the vector of expected returns, formula_21 the covariance matrix of returns, formula_22 a unity vector formula_23 the covariance between returns and the state variable. The optimal weights are:\nNotice that the intertemporal model provides the same weights of the CAPM. Expected returns can be expressed as follows:\nwhere m is the market portfolio and h a portfolio to hedge the state variable.\n\n\n"}
{"id": "54943204", "url": "https://en.wikipedia.org/wiki?curid=54943204", "title": "Journal of Mathematical Analysis and Applications", "text": "Journal of Mathematical Analysis and Applications\n\nThe Journal of Mathematical Analysis and Applications () is an academic journal in mathematics, specializing in mathematical analysis and related topics in applied mathematics. It was founded in 1960, as part of a series of new journals on areas of mathematics published by Academic Press, and is now published by Elsevier. For most years since 1997 it has been ranked by SCImago Journal Rank as among the top 50% of journals in its topic areas.\n"}
{"id": "44340383", "url": "https://en.wikipedia.org/wiki?curid=44340383", "title": "Kawasaki's Riemann–Roch formula", "text": "Kawasaki's Riemann–Roch formula\n\nIn differential geometry, Kawasaki's Riemann–Roch formula, introduced by Tetsuro Kawasaki, is the Riemann–Roch formula for orbifolds. It can compute the Euler characteristic of an orbifold.\n\nKawasaki's original proof made a use of the equivariant index theorem. Today, the formula is known to follow from the Riemann–Roch formula for quotient stacks.\n\n"}
{"id": "184082", "url": "https://en.wikipedia.org/wiki?curid=184082", "title": "König's theorem (set theory)", "text": "König's theorem (set theory)\n\nIn set theory, König's theorem states that if the axiom of choice holds, \"I\" is a set, formula_1 and formula_2 are cardinal numbers for every \"i\" in \"I\", and formula_3 for every \"i\" in \"I\", then \n\nThe \"sum\" here is the cardinality of the disjoint union of the sets \"m\", and the product is the cardinality of the Cartesian product. However, without the use of the axiom of choice, the sum and the product cannot be defined as cardinal numbers, and the meaning of the inequality sign would need to be clarified.\n\nKönig's theorem was introduced by in the slightly weaker form that the sum of a strictly increasing sequence of nonzero cardinal numbers is less than their product.\n\nThe precise statement of the result: if \"I\" is a set, \"A\" and \"B\" are sets for every \"i\" in \"I\", and formula_5 for every \"i\" in \"I\", then \nwhere < means \"strictly less than in cardinality\", i.e. there is an injective function from \"A\" to \"B\", but not one going the other way. The union involved need not be disjoint (a non-disjoint union can't be any bigger than the disjoint version, also assuming the axiom of choice). In this formulation, König's theorem is equivalent to the axiom of choice.\n\n(Of course, König's theorem is trivial if the cardinal numbers \"m\" and \"n\" are finite and the index set \"I\" is finite. If \"I\" is empty, then the left sum is the empty sum and therefore 0, while the right product is the empty product and therefore 1).\n\nKönig's theorem is remarkable because of the strict inequality in the conclusion. There are many easy rules for the arithmetic of infinite sums and products of cardinals in which one can only conclude a weak inequality ≤, for example: if formula_7 for all \"i\" in \"I\", then one can only conclude\nsince, for example, setting formula_9 and formula_10, where the index set \"I\" is the natural numbers, yields the sum formula_11 for both sides, and we have a strict equality.\n\nIf we take \"m\" = 1, and \"n\" = 2 for each \"i\" in κ, then the left side of the above inequality is just κ, while the right side is 2, the cardinality of functions from κ to {0, 1}, that is, the cardinality of the power set of κ. Thus, König's theorem gives us an alternate proof of Cantor's theorem. (Historically of course Cantor's theorem was proved much earlier.)\n\nOne way of stating the axiom of choice is \"an arbitrary Cartesian product of non-empty sets is non-empty\". Let \"B\" be a non-empty set for each \"i\" in \"I\". Let \"A\" = {} for each \"i\" in \"I\". Thus by König's theorem, we have:\nThat is, the Cartesian product of the given non-empty sets \"B\" has a larger cardinality than the sum of empty sets. Thus it is non-empty, which is just what the axiom of choice states. Since the axiom of choice follows from König's theorem, we will use the axiom of choice freely and implicitly when discussing consequences of the theorem.\n\nKönig's theorem has also important consequences for cofinality of cardinal numbers.\n\n\nChoose a strictly increasing cf(κ)-sequence of ordinals approaching κ. Each of them is less than κ, so their sum, which is κ, is less than the product of cf(κ) copies of κ.\n\nAccording to Easton's theorem, the next consequence of König's theorem is the only nontrivial constraint on the continuum function for regular cardinals.\nLet formula_21. Suppose that, contrary to this corollary, formula_22. Then using the previous corollary, formula_23, a contradiction. Thus the supposition must be false, and this corollary must be true.\n\nAssuming Zermelo–Fraenkel set theory, including especially the axiom of choice, we can prove the theorem. Remember that we are given formula_24, and we want to show :formula_25\n\nThe axiom of choice implies that the condition \"A\" < \"B\" is equivalent to the condition that there is no function from \"A\" onto \"B\" and \"B\" is nonempty.\nSo we are given that there is no function from \"A\" onto \"B\"≠{}, and we have to show that any function \"f\" from the disjoint union of the \"A\"s to the product of the \"B\"s is not surjective and that the product is nonempty. That the product is nonempty follows immediately from the axiom of choice and the fact that the factors are nonempty. For each \"i\" choose a \"b\" in \"B\" not in the image of \"A\" under the composition of \"f\" with the projection to \"B\". Then the product of the elements \"b\" is not in the image of \"f\", so \"f\" does not map the disjoint union of the \"A\"s onto the product of the \"B\"s.\n\n"}
{"id": "7422265", "url": "https://en.wikipedia.org/wiki?curid=7422265", "title": "Lehmer mean", "text": "Lehmer mean\n\nIn mathematics, the Lehmer mean of a tuple formula_1 of positive real numbers, named after Derrick Henry Lehmer, is defined as:\n\nThe weighted Lehmer mean with respect to a tuple formula_3 of positive weights is defined as:\n\nThe Lehmer mean is an alternative to power means\nfor interpolating between minimum and maximum via arithmetic mean and harmonic mean.\n\nThe derivative of formula_5 is non-negative\n\nthus this function is monotonic and the inequality\n\nholds.\n\n\nLike a power mean,\na Lehmer mean serves a non-linear moving average which is shifted towards small signal values for small formula_20 and emphasizes big signal values for big formula_20. Given an efficient implementation of a moving arithmetic mean called smooth you can implement a moving Lehmer mean according to the following Haskell code.\n\n\nGonzalez and Woods call this a \"contraharmonic mean filter\" described for varying values of \"p\" (however, as above, the contraharmonic mean can refer to the specific case formula_24). Their convention is to substitute \"p\" with the order of the filter \"Q\":\n\n\"Q\"=0 is the arithmetic mean. Positive \"Q\" can reduce pepper noise and negative \"Q\" can reduce salt noise.\n\n\n"}
{"id": "8351939", "url": "https://en.wikipedia.org/wiki?curid=8351939", "title": "Liability-driven investment strategy", "text": "Liability-driven investment strategy\n\nLiability-driven investment policies and asset management decisions are those largely determined by the sum of current and future liabilities attached to the investor, be it a household or an institution. As it purports to associate constantly both sides of the balance sheet in the investment process, it has been called a \"holistic\" investment methodology. \n\nIn essence, the liability-driven investment strategy (LDI) is an investment strategy of a company or individual based on the cash flows needed to fund future liabilities. It is sometimes referred to as a \"dedicated portfolio\" strategy. It differs from a “benchmark-driven” strategy, which is based on achieving better returns than an external index such as the S&P 500 or a combination of indices that invest in the same types of asset classes. LDI is designed for situations where future liabilities can be predicted with some degree of accuracy. For individuals, the classic example would be the stream of withdrawals from a retirement portfolio that a retiree will make to pay living expenses from the date of retirement to the date of death. For companies, the classic example would be a pension fund that must make future payouts to pensioners over their expected lifetimes (see below).\n\nA retiree following an LDI strategy begins by estimating the income needed each year in the future. Social security payments and any other income is subtracted from the income needed to determine how much will have to be withdrawn each year from the money in the retirement portfolio to meet the income need. These withdrawals become the liabilities that the investment strategy targets. The portfolio must be invested so as to provide the cash flows that match the withdrawals each year, after factoring in adjustments for inflation, irregular spending (such as an ocean cruise every other year), and so on. Individual bonds provide the ability to match the cash flows needed, which is why the term \"cash flow matching\" is sometimes used to describe this strategy. Because the bonds are dedicated to providing the cash flows, the term \"dedicated portfolio\" or “asset dedication” is sometimes used to describe the strategy.\n\nA pension fund following an LDI strategy focuses on the pension-fund assets in the context of the promises made to employees and pensioners (liabilities). This is in contrast to an approach which focuses purely on the asset side of the pension fund balance sheet. There is no single accepted definition or approach to LDI and different managers apply different approaches. Typical LDI strategies involve hedging, in whole or in part, the fund's exposure to changes in interest rates and inflation. These risks can eat into a pension scheme's ability to keep their promises to members. Historically, bonds were used as a partial hedge for these interest rate risks but the recent growth in LDI has focused on using swaps and other derivatives. Various approaches will pursue a \"glide path\" which over time seeks to reduce interest rate and other risks while achieving a return that matches or exceeds the growth in projected pension plan liabilities. These various approaches offer significant additional flexibility and capital efficiency compared to bonds, but also raise issues of added complexity, especially when the rebalancing of an LDI portfolio following changes in interest rates is considered. \n\nLDI investment strategies have come to prominence in the UK as a result of changes in the regulatory and accounting framework. IAS 19 (one of the International Financial Reporting Standards) requires that UK companies post the funding position of a pension fund on the corporate sponsor's balance sheet. In the US the introduction of FAS 158 (Financial Accounting Standards Board) has created a similar requirement.\n\n\n"}
{"id": "12044399", "url": "https://en.wikipedia.org/wiki?curid=12044399", "title": "List decoding", "text": "List decoding\n\nIn computer science, particularly in coding theory, list decoding is an alternative to unique decoding of error-correcting codes for large error rates. The notion was proposed by Elias in the 1950s. The main idea behind list decoding is that the decoding algorithm instead of outputting a single possible message outputs a list of possibilities one of which is correct. This allows for handling a greater number of errors than that allowed by unique decoding.\n\nThe unique decoding model in coding theory, which is constrained to output a single valid codeword from the received word could not tolerate greater fraction of errors. This resulted in a gap between the error-correction performance for stochastic noise models (proposed by Shannon) and the adversarial noise model (considered by Richard Hamming). Since the mid 90s, significant algorithmic progress by the coding theory community has bridged this gap. Much of this progress is based on a relaxed error-correction model called list decoding, wherein the decoder outputs a list of codewords for worst-case pathological error patterns where the actual transmitted codeword is included in the output list. In case of typical error patterns though, the decoder outputs a unique single codeword, given a received word, which is almost always the case (However, this is not known to be true for all codes). The improvement here is significant in that the error-correction performance doubles. This is because now the decoder is not confined by the half-the-minimum distance barrier. This model is very appealing because having a list of codewords is certainly better than just giving up. The notion of list-decoding has many interesting applications in complexity theory.\n\nThe way the channel noise is modeled plays a crucial role in that it governs the rate at which reliable communication is possible. There are two main schools of thought in modeling the channel behavior:\n\nThe highlight of list-decoding is that even under adversarial noise conditions, it is possible to achieve the information-theoretic optimal trade-off between rate and fraction of errors that can be corrected. Hence, in a sense this is like improving the error-correction performance to that possible in case of a weaker, stochastic noise model.\n\nLet formula_1 be a formula_2 error-correcting code; in other words, formula_1 is a code of length formula_4, dimension formula_5 and minimum distance formula_6 over an alphabet formula_7 of size formula_8. The list-decoding problem can now be formulated as follows:\n\nInput: Received word formula_9, error bound formula_10\n\nOutput: A list of all codewords formula_11 whose hamming distance from formula_12 is at most formula_10.\n\nGiven a received word formula_14, which is a noisy version of some transmitted codeword formula_15, the decoder tries to output the transmitted codeword by placing its bet on a codeword that is “nearest” to the received word. The Hamming distance between two codewords is used as a metric in finding the nearest codeword, given the received word by the decoder. If formula_6 is the minimum Hamming distance of a code formula_1, then there exists two codewords formula_18 and formula_19 that differ in exactly formula_6 positions. Now, in the case where the received word formula_14 is equidistant from the codewords formula_18 and formula_19, unambiguous decoding becomes impossible as the decoder cannot decide which one of formula_18 and formula_19 to output as the original transmitted codeword. As a result, the half-the minimum distance acts as a combinatorial barrier beyond which unambiguous error-correction is impossible, if we only insist on unique decoding. However, received words such as formula_14 considered above occur only in the worst-case and if one looks at the way Hamming balls are packed in high-dimensional space, even for error patterns formula_10 beyond half-the minimum distance, there is only a single codeword formula_15 within Hamming distance formula_10 from the received word. This claim has been shown to hold with high probability for a random code picked from a natural ensemble and more so for the case of Reed–Solomon codes which is well studied and quite ubiquitous in the real world applications. In fact, Shannon’s proof of the capacity theorem for \"q\"-ary symmetric channels can be viewed in light of the above claim for random codes.\n\nUnder the mandate of list-decoding, for worst-case errors, the decoder is allowed to output a small list of codewords. With some context specific or side information, it may be possible to prune the list and recover the original transmitted codeword. Hence, in general, this seems to be a stronger error-recovery model than unique decoding.\n\nFor a polynomial-time list-decoding algorithm to exist, we need the combinatorial guarantee that any Hamming ball of radius formula_30 around a received word formula_31 (where formula_32 is the fraction of errors in terms of the block length formula_4) has a small number of codewords. This is because the list size itself is clearly a lower bound on the running time of the algorithm. Hence, we require the list size to be a polynomial in the block length formula_4 of the code. A combinatorial consequence of this requirement is that it imposes an upper bound on the rate of a code. List decoding promises to meet this upper bound. It has been shown non-constructively that codes of rate formula_35 exist that can be list decoded up to a fraction of errors approaching formula_36. The quantity formula_36 is referred to in the literature as the list-decoding capacity. This is a substantial gain compared to the unique decoding model as we now have the potential to correct twice as many errors. Naturally, we need to have at least a fraction formula_35 of the transmitted symbols to be correct in order to recover the message. This is an information-theoretic lower bound on the number of correct symbols required to perform decoding and with list decoding, we can potentially achieve this information-theoretic limit. However, to realize this potential, we need explicit codes (codes that can be constructed in polynomial time) and efficient algorithms to perform encoding and decoding.\n\nFor any error fraction formula_39 and an integer formula_40, a code formula_41 is said to be list decodable up to a fraction formula_32 of errors with list size at most formula_43 or formula_44-list-decodable if for every formula_45, the number of codewords formula_46 within Hamming distance formula_47 from formula_14 is at most formula_49\n\nThe relation between list decodability of a code and other fundamental parameters such as minimum distance and rate have been fairly well studied. It has been shown that every code can be list decoded using small lists beyond half the minimum distance up to a bound called the Johnson radius. This is quite significant because it proves the existence of formula_44-list-decodable codes of good rate with a list-decoding radius much larger than formula_51 In other words, the Johnson bound rules out the possibility of having a large number of codewords in a Hamming ball of radius slightly greater than formula_52 which means that it is possible to correct far more errors with list decoding.\n\nWhat this means is that for rates approaching the channel capacity, there exists list decodable codes with polynomial sized lists enabling efficient decoding algorithms whereas for rates exceeding the channel capacity, the list size becomes exponential which rules out the existence of efficient decoding algorithms.\n\nThe proof for list-decoding capacity is a significant one in that it exactly matches the capacity of a formula_8-ary symmetric channel formula_66. In fact, the term \"list-decoding capacity\" should actually be read as the capacity of an adversarial channel under list decoding. Also, the proof for list-decoding capacity is an important result that pin points the optimal trade-off between rate of a code and the fraction of errors that can be corrected under list decoding.\n\nThe idea behind the proof is similar to that of Shannon's proof for capacity of the binary symmetric channel formula_67 where a random code is picked and showing that it is formula_44-list-decodable with high probability as long as the rate formula_69 For rates exceeding the above quantity, it can be shown that the list size formula_43 becomes super-polynomially large.\n\nA \"bad\" event is defined as one in which, given a received word formula_71 and formula_72 messages formula_73 it so happens that formula_74, for every formula_75 where formula_32 is the fraction of errors that we wish to correct and formula_77 is the Hamming ball of radius formula_78 with the received word formula_79 as the center.\n\nNow, the probability that a codeword formula_80 associated with a fixed message formula_81 lies in a Hamming ball formula_82 is given by\n\nwhere the quantity formula_84 is the volume of a Hamming ball of radius formula_78 with the received word formula_79 as the center. The inequality in the above relation follows from the upper bound on the volume of a Hamming ball. The quantity formula_87 gives a very good estimate on the volume of a Hamming ball of radius formula_32 centered on any word in formula_89 Put another way, the volume of a Hamming ball is translation invariant. To continue with the proof sketch, we conjure the union bound in probability theory which tells us that the probability of a bad event happening for a given formula_90 is upper bounded by the quantity formula_91.\n\nWith the above in mind, the probability of \"any\" bad event happening can be shown to be less than formula_92. To show this, we work our way over all possible received words formula_93 and every possible subset of formula_43 messages in formula_95\n\nNow turning to the proof of part (ii), we need to show that there are super-polynomially many codewords around every formula_96 when the rate exceeds the list-decoding capacity. We need to show that formula_97 is super-polynomially large if the rate formula_58. Fix a codeword formula_99. Now, for every formula_96 picked at random, we have\n\nsince Hamming balls are translation invariant. From the definition of the volume of a Hamming ball and the fact that formula_79 is chosen uniformly at random from formula_103 we also have\n\nLet us now define an indicator variable formula_105 such that\n\nTaking the expectation of the volume of a Hamming ball we have\n\nTherefore, by the probabilistic method, we have shown that if the rate exceeds the list-decoding capacity, then the list size becomes super-polynomially large. This completes the proof sketch for the list-decoding capacity.\n\nIn the period from 1995 to 2007, the coding theory community developed progressively more efficient list-decoding algorithms. Algorithms for Reed–Solomon codes that can decode up to the Johnson radius which is formula_108 exist where formula_109 is the normalised distance or relative distance. However, for Reed-Solomon codes, formula_110 which means a fraction formula_111 of errors can be corrected. Some of the most prominent list-decoding algorithms are the following:\n\n\nBecause of their ubiquity and the nice algebraic properties they possess, list-decoding algorithms for Reed–Solomon codes were a main focus of researchers. The list-decoding problem for Reed–Solomon codes can be formulated as follows:\n\nInput: For an formula_120 Reed-Solomon code, we are given the pair formula_121 for formula_122, where formula_123 is the formula_124th bit of the received word and the formula_125's are distinct points in the finite field formula_126 and an error parameter formula_127.\n\nOutput: The goal is to find all the polynomials formula_128 of degree at most formula_129 which is the message length such that formula_130 for at least formula_131 values of formula_132. Here, we would like to have formula_131 as small as possible so that greater number of errors can be tolerated.\n\nWith the above formulation, the general structure of list-decoding algorithms for Reed-Solomon codes is as follows:\n\nStep 1: (Interpolation) Find a non-zero bivariate polynomial formula_134 such that formula_135 for formula_122.\n\nStep 2: (Root finding/Factorization) Output all degree formula_129 polynomials formula_138 such that formula_139 is a factor of formula_134 i.e. formula_141. For each of these polynomials, check if formula_142 for at least formula_131 values of formula_144. If so, include such a polynomial formula_138 in the output list.\n\nGiven the fact that bivariate polynomials can be factored efficiently, the above algorithm runs in polynomial time.\n\nAlgorithms developed for list decoding of several interesting code families have found interesting applications in computational complexity and the field of cryptography. Following is a sample list of applications outside of coding theory:\n\n\n"}
{"id": "56381043", "url": "https://en.wikipedia.org/wiki?curid=56381043", "title": "List of things named after Andrey Markov", "text": "List of things named after Andrey Markov\n\nThis article is a list of things named after Andrey Markov, an influential Russian mathematician.\n\n\n\n\n\n\n\n\n\n"}
{"id": "1247037", "url": "https://en.wikipedia.org/wiki?curid=1247037", "title": "Marcel Grossmann", "text": "Marcel Grossmann\n\nMarcel Grossmann (, April 9, 1878 – September 7, 1936) was a mathematician and a friend and classmate of Albert Einstein. Grossmann was a member of an old Swiss family from Zurich. His father managed a textile factory. He became a Professor of Mathematics at the Federal Polytechnic School in Zurich, today the ETH Zurich, specializing in descriptive geometry.\n\nIn 1900 Grossmann graduated from the Federal Polytechnic School (ETH) and became an assistant to the geometer Wilhelm Fiedler. He continued to do research on non-Euclidean geometry and taught in high schools for the next seven years. In 1902, he earned his doctorate from the ETH with the thesis \"On the Metrical Properties of Collinear Structures\" with Fiedler as advisor. In 1907, he was appointed full professor of descriptive geometry at the Federal Polytechnic School.\n\nAs a professor of geometry, Grossmann organized summer courses for high school teachers. In 1910, he became one of the founders of the Swiss Mathematical Society. He was an Invited Speaker of the ICM in 1912 at Cambridge UK and in 1920 at Strasbourg.\n\nAlbert Einstein's friendship with Grossmann began with their school days in Zurich. Grossmann's careful and complete lecture notes at the Federal Polytechnic School proved to be a salvation for Einstein, who missed many lectures. Grossmann's father helped Einstein get his job at the Swiss Patent Office in Bern, and it was Grossmann who helped to conduct the negotiations to bring Einstein back as a professor of physics at the Zurich Polytechnic. Grossmann was an expert in differential geometry and tensor calculus; just the mathematical tools Einstein discovered were needed for his work on gravity. Thus, it was natural that Einstein would enter into a scientific collaboration with Grossmann.\n\nIt was Grossmann who emphasized the importance of a non-Euclidean geometry called Riemannian geometry (also elliptic geometry) to Einstein, which was a necessary step in the development of Einstein's general theory of relativity. Abraham Pais's book on Einstein suggests that Grossmann mentored Einstein in tensor theory as well. Grossmann introduced Einstein to the absolute differential calculus, started by Christoffel and fully developed by Ricci-Curbastro and Levi-Civita. Grossmann facilitated Einstein's unique synthesis of mathematical and theoretical physics in what is still today considered the most elegant and powerful theory of gravity: the general theory of relativity. The collaboration of Einstein and Grossmann led to a ground-breaking paper: \"Outline of a Generalized Theory of Relativity and of a Theory of Gravitation\", which was published in 1913 and was one of the two fundamental papers which established Einstein's theory of gravity.\n\nGrossmann died of multiple sclerosis in 1936. The community of relativists celebrates Grossmann's contributions to physics by organizing Marcel Grossmann meetings every three years.\n\n\n\n"}
{"id": "47538197", "url": "https://en.wikipedia.org/wiki?curid=47538197", "title": "Mina Aganagic", "text": "Mina Aganagic\n\nMina Aganagic is a Bosnian-born mathematical physicist who works as a professor in the Department of Mathematics and the Department of Physics at the University of California, Berkeley.\n\nAganagic was raised in Sarajevo, Bosnia and Herzegovina.\nShe has a bachelor's degree and a doctorate from the California Institute of Technology, in 1995 and 1999 respectively; her PhD advisor was John Henry Schwarz. She was a Postdoctoral Fellow at the Harvard University physics department\nfrom 1999 to 2003. She then joined the physics faculty at the University of Washington, where she became a Sloan Fellow and a DOE Outstanding Junior Investigator. She moved to UC Berkeley in 2004. In 2016 the Simons Foundation gave her a Simons Investigator award.\n\nShe is known for applying string theory to various problems in mathematics, e.g. knot theory (refined Chern–Simons theory), enumerative geometry (topological vertex), mirror symmetry and Geometric Langlands Correspondence .\n\n"}
{"id": "22444842", "url": "https://en.wikipedia.org/wiki?curid=22444842", "title": "Monk's formula", "text": "Monk's formula\n\nIn mathematics, Monk's formula, found by , is an analogue of Pieri's formula that describes the product of a linear Schubert polynomial by a Schubert polynomial. Equivalently, it describes the product of a special Schubert cycle by a Schubert cycle in the cohomology of a flag manifold.\n\nWrite \"t\" for the transposition \"(i j)\", and \"s\" = \"t\". Then 𝔖 = \"x\" + ⋯ + \"x\", and Monk's formula states that for a permutation \"w\",\n\nformula_1\n\nwhere formula_2 is the length of \"w\". The pairs (\"i\", \"j\") appearing in the sum are exactly those such that \"i\" ≤ \"r\" < \"j\", \"w\" < \"w\", and there is no \"i\" < \"k\" < \"j\" with \"w\" < \"w\" < \"w\"; each \"wt\" is a cover of \"w\" in Bruhat order.\n"}
{"id": "287188", "url": "https://en.wikipedia.org/wiki?curid=287188", "title": "Number line", "text": "Number line\n\nIn basic mathematics, a number line is a picture of a graduated straight line that serves as abstraction for real numbers, denoted by formula_1. Every point of a number line is assumed to correspond to a real number, and every real number to a point.\n\nThe integers are often shown as specially-marked points evenly spaced on the line. Although this image only shows the integers from −9 to 9, the line includes all real numbers, continuing forever in each direction, and also numbers not marked that are between the integers. It is often used as an aid in teaching simple addition and subtraction, especially involving negative numbers.\n\nIn advanced mathematics, the expressions \"real number line\", or \"real line\" are typically used to indicate the above-mentioned concept that every point on a straight line corresponds to a single real number, and .\n\nA number line is usually represented as being horizontal, but in a Cartesian coordinate plane the vertical axis (y-axis) is also a number line. According to one convention, positive numbers always lie on the right side of zero, negative numbers always lie on the left side of zero, and arrowheads on both ends of the line are meant to suggest that the line continues indefinitely in the positive and negative directions. Another convention uses only one arrowhead which indicates the direction in which numbers grow. The line continues indefinitely in the positive and negative directions according to the rules of geometry which define a line without endpoints as an \"infinite line\", a line with one endpoint as a \"ray\", and a line with two endpoints as a \"line segment\".\n\nIf a particular number is farther to the right on the number line than is another number, then the first number is greater than the second (equivalently, the second is less than the first). The distance between them is the magnitude of their difference—that is, it measures the first number minus the second one, or equivalently the absolute value of the second number minus the first one. Taking this difference is the process of subtraction.\n\nThus, for example, the length of a line segment between 0 and some other number represents the magnitude of the latter number.\n\nTwo numbers can be added by \"picking up\" the length from 0 to one of the numbers, and putting it down again with the end that was 0 placed on top of the other number.\n\nTwo numbers can be multiplied as in this example: To multiply 5 × 3, note that this is the same as 5 + 5 + 5, so pick up the length from 0 to 5 and place it to the right of 5, and then pick up that length again and place it to the right of the previous result. This gives a result that is 3 combined lengths of 5 each; since the process ends at 15, we find that 5 × 3 = 15.\n\nDivision can be performed as in the following example: To divide 6 by 2—that is, to find out how many times 2 goes into 6—note that the length from 0 to 2 lies at the beginning of the length from 0 to 6; pick up the former length and put it down again to the right of its original position, with the end formerly at 0 now placed at 2, and then move the length to the right of its latest position again. This puts the right end of the length 2 at the right end of the length from 0 to 6. Since three lengths of 2 filled the length 6, 2 goes into 6 three times (that is, 6 ÷ 2 = 3).\n\nThe section of the number line between two numbers is called an interval. If the section includes both numbers it is said to be a closed interval, while if it excludes both numbers it is called an open interval. If it includes one of the numbers but not the other one, it is called a half-open interval.\n\nAll the points extending forever in one direction from a particular point are together known as a ray. If the ray includes the particular point, it is a closed ray; otherwise it is an open ray.\n\nOn the number line, the distance between two points is the unit length if and only if the difference of the represented numbers equals 1. Other choices are possible. \n\nOne of the most common choices is the \"logarithmic scale\", which is a representation of the \"positive\" numbers on a line, such that the distance of two points is the unit length, if the ratio of the represented numbers has a fixed value, typically 10. In such a logarithmic scale, the origin represents 1; one inch to the right, one has 10, one inch to the right of 10 one has , then , then , etc. Similarly, one inch to the left of 1, one has , then , etc.\n\nThis approach is useful, when one wants to represent, on the same figure, values with very different order of magnitude. For example, one requires a logarithmic scale for representing simultaneously the size of the different bodies that exist in the Universe, typically, a photon, an electron, an atom, a molecule, a human, the Earth, the Solar System, a galaxy, and the visible Universe.\n\nLogarithmic scales are used in slide rules for multiplying or dividing numbers by adding or subtracting lengths on logarithmic scales.\nA line drawn through the origin at right angles to the real number line can be used to represent the imaginary numbers. This line, called imaginary line, extends the number line to a complex number plane, with points representing complex numbers.\n\nAlternatively, one real number line can be drawn horizontally to denote possible values of one real number, commonly called \"x\", and another real number line can be drawn vertically to denote possible values of another real number, commonly called \"y\". Together these lines form what is known as a Cartesian coordinate system, and any point in the plane represents the value of a pair of real numbers. Further, the Cartesian coordinate system can itself be extended by visualizing a third number line \"coming out of the screen (or page)\", measuring a third variable called \"z\". Positive numbers are closer to the viewer's eyes than the screen is, while negative numbers are \"behind the screen\"; larger numbers are farther from the screen. Then any point in the three-dimensional space that we live in represents the values of a trio of real numbers.\n\n"}
{"id": "3810034", "url": "https://en.wikipedia.org/wiki?curid=3810034", "title": "Ordered probit", "text": "Ordered probit\n\nIn statistics, ordered probit is a whole of the widely used probit analysis to the case of more than two outcomes of an ordinal dependent variable (a dependent variable for which the potential values have a natural ordering, as in poor, fair, good, excellent). Similarly, the widely used logit method also has a counterpart ordered logit. Ordered probit, like ordered logit, is a particular method of ordinal regression.\n\nFor example, in clinical research, the effect a drug may have on a patient may be modeled with ordered probit regression. Independent variables may include the use or non-use of the drug as well as control variables such as age and details from medical history such as whether the patient suffers from high blood pressure, heart disease, etc. The dependent variable would be ranked from the following list: complete cure, relieve symptoms, no effect, deteriorate condition, death.\n\nAnother example application is the Likert scale commonly employed in survey research, where respondents rate their agreement on an ordered scale (e.g., \"Strongly disagree\" to \"Strongly agree\"). The ordered probit model provides an appropriate fit to these data, preserving the ordering of response options while making no assumptions of the interval distances between options. \n\nSuppose the underlying relationship to be characterized is\n\nwhere formula_2 is the exact but unobserved dependent variable (perhaps the exact level of improvement by the patient); formula_3 is the vector of independent variables, and formula_4 is the vector of regression coefficients which we wish to estimate. Further suppose that while we cannot observe formula_2, we instead can only observe the categories of response:\n\nThen the ordered probit technique will use the observations on formula_7, which are a form of censored data on formula_2, to fit the parameter vector formula_4.\n\nThe model cannot be consistently estimated using ordinary least squares; it is usually estimated using maximum likelihood. For details on how the equation is estimated, see the article Ordinal regression.\n"}
{"id": "7004401", "url": "https://en.wikipedia.org/wiki?curid=7004401", "title": "Pierre Rosenstiehl", "text": "Pierre Rosenstiehl\n\nPierre Rosenstiehl (born 1933) is a French mathematician recognized for his work in graph theory, planar graphs, and graph drawing.\n\nThe Fraysseix-Rosenstiehl's planarity criterion is at the origin of the left-right planarity algorithm implemented in Pigale software, which is considered as the fastest implemented planarity testing algorithm.\n\nRosenstiehl was directeur d’études at the École des Hautes Études en Sciences Sociales in Paris, before his retirement. He is co-editor in chief of the European Journal of Combinatorics. Rosenstiehl, Giuseppe Di Battista, Peter Eades and Roberto Tamassia organized in 1992 at Marino (Italy) a meeting devoted to graph drawing which initiated a long series of international conferences, the International Symposia on Graph Drawing.\n\nHe has been a member of the French literary group Oulipo since 1992. He married the French author and illustrator Agnès Rosenstiehl.\n"}
{"id": "9142932", "url": "https://en.wikipedia.org/wiki?curid=9142932", "title": "Proof sketch for Gödel's first incompleteness theorem", "text": "Proof sketch for Gödel's first incompleteness theorem\n\nThis article gives a sketch of a proof of Gödel's first incompleteness theorem. This theorem applies to any formal theory that satisfies certain technical hypotheses, which are discussed as needed during the sketch. We will assume for the remainder of the article that a fixed theory satisfying these hypotheses has been selected.\n\nThroughout this article the word \"number\" refers to a natural number. The key property these numbers possess is that any natural number can be obtained by starting with the number 0 and adding 1 a finite number of times.\n\nGödel's theorem applies to any formal theory that satisfies certain properties. Each formal theory has a signature that specifies the nonlogical symbols in the language of the theory. For simplicity, we will assume that the language of the theory is composed from the following collection of 15 (and only 15) symbols:\n\n\nThis is the language of Peano arithmetic. A well-formed formula is a sequence of these symbols that is formed so as to have a well-defined reading as a mathematical formula. Thus is well formed while is not well formed. A theory is a set of well-formed formulas with no free variables.\n\nA theory is consistent if there is no formula such that both and its negation are provable. ω-consistency is a stronger property than consistency. Suppose that is a formula with one free variable . In order to be ω-consistent, the theory cannot prove both while also proving for each natural number .\n\nThe theory is assumed to be effective, which means that the set of axioms must be recursively enumerable. This means that it is theoretically possible to write a finite-length computer program that, if allowed to run forever, would output the axioms of the theory (necessarily including every well-formed instance of the axiom schema of induction) one at a time and not output anything else. This requirement is necessary; there are theories that are complete, consistent, and include elementary arithmetic, but no such theory can be effective.\n\nThe sketch here is broken into three parts. In the first part, each formula of the theory is assigned a number, known as a Gödel number, in a manner that allows the formula to be effectively recovered from the number. This numbering is extended to cover finite sequences of formulas. In the second part, a specific formula is constructed such that for any two numbers and holds if and only if represents a sequence of formulas that constitutes a proof of the formula that represents. In the third part of the proof, we construct a self-referential formula that, informally, says \"I am not provable\", and prove that this sentence is neither provable nor disprovable within the theory.\n\nImportantly, all the formulas in the proof can be defined by primitive recursive functions, which themselves can be defined in first-order Peano arithmetic.\n\nThe first step of the proof is to represent (well-formed) formulas of the theory, and finite lists of these formulas, as natural numbers. These numbers are called the Gödel numbers of the formulas.\n\nBegin by assigning a natural number to each symbol of the language of arithmetic, similar to the manner in which the ASCII code assigns a unique binary number to each letter and certain other characters. This article will employ the following assignment, very similar to the one Douglas Hofstadter used in his \"Gödel, Escher, Bach\":\n\nThe Gödel number of a formula is obtained by concatenating the Gödel numbers of each symbol making up the formula. The Gödel numbers for each symbol are separated by a zero because by design, no Gödel number of a symbol includes a . Hence any formula may be correctly recovered from its Gödel number. Let denote the Gödel number of the formula .\n\nGiven the above Gödel numbering, the sentence asserting that addition commutes, translates as the number:\n\n(Spaces have been inserted on each side of every 0 only for readability; Gödel numbers are strict concatenations of decimal digits.) Not all natural numbers represent a formula. For example, the number\n\ntranslates to \"\", which is not well-formed.\n\nBecause each natural number can be obtained by applying the successor operation to a finite number of times, every natural number has its own Gödel number. For example, the Gödel number corresponding to , is:\n\nThe assignment of Gödel numbers can be extended to finite lists of formulas. To obtain the Gödel number of a list of formulas, write the Gödel numbers of the formulas in order, separating them by two consecutive zeros. Since the Gödel number of a formula never contains two consecutive zeros, each formula in a list of formulas can be effectively recovered from the Gödel number for the list.\n\nIt is crucial that the formal arithmetic be capable of proving a minimum set of facts. In particular, it must be able to prove that every number has a Gödel number. A second fact that the theory must prove is that given any Gödel number of a formula with one free variable and any number , there is a Gödel number of the formula obtained by replacing all occurrences of in with , and that this second Gödel number can be effectively obtained from the Gödel number of as a function of . To see that this is in fact possible, note that given the Gödel number for , one can recreate the original formula, make the substitution, and then find the Gödel number of the resulting formula. This is a uniform procedure.\n\nDeduction rules can then be represented by binary relations on Gödel numbers of lists of formulas. In other words, suppose that there is a deduction rule , by which one can move from the formulas to a new formula . Then the relation corresponding to this deduction rule says that is related to (in other words, holds) if is the Gödel number of a list of formulas containing and and is the Gödel number of the list of formulas consisting of those in the list coded by together with . Because each deduction rule is concrete, it is possible to effectively determine for any natural numbers and whether they are related by the relation.\n\nThe second stage in the proof is to use the Gödel numbering, described above, to show that the notion of provability can be expressed within the formal language of the theory. Suppose the theory has deduction rules: . Let be their corresponding relations, as described above.\n\nEvery provable statement is either an axiom itself, or it can be deduced from the axioms by a finite number of applications of the deduction rules. We wish to define a set of numbers that represents all these provable statements. We define as the minimal set consisting of all numbers in (representing axioms) and closed under all the relations . This means that whenever is in the set and for some numbers and , the number is also in the set . It is not hard to see that represents the set of provable statements. That is, the members of are the Gödel numbers of the provable statements.\nA proof of a formula is itself a string of mathematical statements related by particular relations (each is either an axiom or related to former statements by deduction rules), where the last statement is . Thus one can define the Gödel number of a proof. Moreover, one may define a statement form , which for every two numbers and is provable if and only if is the Gödel number of a proof of the statement and .\n\nThe detailed construction of the formula makes essential use of the assumption that the theory is effective; it would not be possible to construct this formula without such an assumption.\n\nFor every number and every formula , where is a free variable, we define , a relation between two numbers and , such that it corresponds to the statement \" is not the Gödel number of a proof of \". Here, can be understood as with its own Gödel number as its argument.\n\nNote that takes as an argument , the Gödel number of . In order to prove either , or , it is necessary to perform number-theoretic operations on that mirror the following steps: decode the number into the formula , replace all occurrences of in with the number , and then compute the Gödel number of the resulting formula .\n\nNote that for every specific number and formula is a straightforward (though complicated) arithmetical relation between two numbers and , building on the relation defined earlier. Further, is provable if the finite list of formulas encoded by is not a proof of , and is provable if the finite list of formulas encoded by is a proof of . Given any numbers and , either or (but not both) is provable.\n\nAny proof of can be encoded by a Gödel number , such that does not hold. If holds for all natural numbers , then there is no proof of . In other words, , a formula about natural numbers, corresponds to \"there is no proof of \".\n\nWe now define the formula , where is a free variable. The formula itself has a Gödel number as does every formula.\n\nThis formula has a free variable . Suppose we replace it with ,\nthe Gödel number of a formula , where is a free variable. Then, corresponds to \"there is no proof of \", as we have seen.\n\nConsider the formula . This formula concerning the number corresponds to \"there is no proof of \". We have here the self-referential feature that is crucial to the proof: A formula of the formal theory that somehow relates to its own provability within that formal theory. Very informally, says: \"I am not provable\".\n\nWe will now show that neither the formula , nor its negation , is provable.\n\nSuppose is provable. Let be the Gödel number of a proof of . Then, as seen earlier, the formula is provable. Proving both and violates the consistency of the formal theory. We therefore conclude that is not provable.\n\nConsider any number . Suppose is provable.\nThen, must be the Gödel number of a proof of . But we have just proved that is not provable. Since either or must be provable, we conclude that, for all natural numbers is provable.\n\nSuppose the negation of , , is provable. Proving both , and , for all natural numbers , violates ω-consistency of the formal theory. Thus if the theory is ω-consistent, is not provable.\n\nWe have sketched a proof showing that:\n\nFor any formal, recursively enumerable (i.e. effectively generated) theory of Peano Arithmetic,\n\nThe proof of Gödel's incompleteness theorem just sketched is proof-theoretic (also called syntactic) in that it shows that if certain proofs exist (a proof of or its negation) then they can be manipulated to produce a proof of a contradiction. This makes no appeal to whether is \"true\", only to whether it is provable. Truth is a model-theoretic, or semantic, concept, and is not equivalent to provability except in special cases.\n\nBy analyzing the situation of the above proof in more detail, it is possible to obtain a conclusion about the truth of in the standard model ℕ of natural numbers. As just seen, is provable for each natural number , and is thus true in the model ℕ. Therefore, within this model,\n\nholds. This is what the statement \" is true\" usually refers to—the sentence is true in the intended model. It is not true in every model, however: If it were, then by Gödel's completeness theorem it would be provable, which we have just seen is not the case.\n\nGeorge Boolos (1989) vastly simplified the proof of the First Theorem, if one agrees that the theorem is equivalent to:\n\n\"There is no algorithm whose output contains all true sentences of arithmetic and no false ones.\"\n\n\"Arithmetic\" refers to Peano or Robinson arithmetic, but the proof invokes no specifics of either, tacitly assuming that these systems allow '<' and '×' to have their usual meanings. Boolos proves the theorem in about two pages. His proof employs the language of first-order logic, but invokes no facts about the connectives or quantifiers. The domain of discourse is the natural numbers. The Gödel sentence builds on Berry's paradox.\n\nLet abbreviate successive applications of the successor function, starting from . Boolos then asserts (the details are only sketched) that there exists a defined predicate that comes out true iff an arithmetic formula containing symbols names the number . This proof sketch contains the only mention of Gödel numbering; Boolos merely assumes that every formula can be so numbered. Here, a formula \"names\" the number iff the following is provable:\n\nBoolos then defines the related predicates:\n\n\nThe above predicates contain the only existential quantifiers appearing in the entire proof. The '<' and '×' appearing in these predicates are the only defined arithmetical notions the proof requires. The proof nowhere mentions recursive functions or any facts from number theory, and Boolos claims that his proof dispenses with diagonalization. For more on this proof, see Berry's paradox.\n\n\n"}
{"id": "61003", "url": "https://en.wikipedia.org/wiki?curid=61003", "title": "Prototype-based programming", "text": "Prototype-based programming\n\nPrototype-based programming is a style of object-oriented programming in which behaviour reuse (known as inheritance) is performed via a process of reusing existing objects via delegation that serve as prototypes. This model can also be known as \"prototypal\", \"prototype-oriented,\" \"classless\", or \"instance-based\" programming. Delegation is the language feature that supports prototype-based programming.\n\nPrototype object oriented programming uses generalized objects, which can then be cloned and extended. Using fruit as an example, a \"fruit\" object would represent the properties and functionality of fruit in general. A \"banana\" object would be cloned from the \"fruit\" object, and would also be extended to include general properties specific to bananas. Each individual \"banana\" object would be cloned from the generic \"banana\" object. Compare to the class-based paradigm, where a \"fruit\" \"class\" would be extended by a \"banana\" \"class\".\n\nThe first prototype-oriented programming language was Self, developed by David Ungar and Randall Smith in the mid-1980s to research topics in object-oriented language design. Since the late 1990s, the classless paradigm has grown increasingly popular. Some current prototype-oriented languages are JavaScript (and other ECMAScript implementations such as JScript and Flash's ActionScript 1.0), Lua, Cecil, NewtonScript, Io, Ioke, MOO, REBOL, Lisaac and AHK.\n\nPrototypal inheritance in JavaScript is described by Douglas Crockford as: \"you make prototype objects, and then … make new instances. Objects are mutable in JavaScript, so we can augment the new instances, giving them new fields and methods. These can then act as prototypes for even newer objects. We don't need classes to make lots of similar objects… Objects inherit from objects. What could be more object oriented than that?\"\n\nAdvocates of prototype-based programming argue that it encourages the programmer to focus on the behavior of some set of examples and only later worry about classifying these objects into archetypal objects that are later used in a fashion similar to classes. Many prototype-based systems encourage the alteration of prototypes during run-time, whereas only very few class-based object-oriented systems (such as the dynamic object-oriented system, Common Lisp, Dylan, Objective-C, Perl, Python, Ruby, or Smalltalk) allow classes to be altered during the execution of a program.\n\nAlmost all prototype-based systems are based on interpreted and dynamically typed languages. Systems based on statically typed languages are technically feasible, however. The Omega language discussed in \"Prototype-Based Programming\" is an example of such a system, though according to Omega's website even Omega is not exclusively static, but rather its \"compiler may choose to use static binding where this is possible and may improve the efficiency of a program.\"\n\nIn prototype-based languages there are no explicit classes. Objects inherit directly from other objects through a prototype property. The prototype property is called prototype in Self and JavaScript, or proto in Io. There are two methods of constructing new objects: \"ex nihilo\" (\"from nothing\") object creation or through \"cloning\" an existing object. The former is supported through some form of object literal, declarations where objects can be defined at runtime through special syntax such as {...} and passed directly to a variable. While most systems support a variety of cloning, \"ex nihilo\" object creation is not as prominent.\n\nIn class-based languages, a new instance is constructed through a class's constructor function, a special function that reserves a block of memory for the object's members (properties and methods) and returns a reference to that block. An optional set of constructor arguments can be passed to the function and are usually held in properties. The resulting instance will inherit all the methods and properties that were defined in the class, which acts as a kind of template from which similar typed objects can be constructed.\n\nSystems that support \"ex nihilo\" object creation allow new objects to be created from scratch without cloning from an existing prototype. Such systems provide a special syntax for specifying the properties and behaviors of new objects without referencing existing objects. In many prototype languages there exists a root object, often called \"Object\", which is set as the default prototype for all other objects created in run-time and which carries commonly needed methods such as a toString() function to return a description of the object as a string. One useful aspect of \"ex nihilo\" object creation is to ensure that a new object's slot (properties and methods) names do not have namespace conflicts with the top-level \"Object\" object. (In the JavaScript language, one can do this by using a null prototype, i.e. Object.create(null).)\n\n\"Cloning\" refers to a process whereby a new object is constructed by copying the behavior of an existing object (its prototype). The new object then carries all the qualities of the original. From this point on, the new object can be modified. In some systems the resulting child object maintains an explicit link (via \"delegation\" or \"resemblance\") to its prototype, and changes in the prototype cause corresponding changes to be apparent in its clone. Other systems, such as the Forth-like programming language Kevo, do not propagate change from the prototype in this fashion, and instead follow a more \"concatenative\" model where changes in cloned objects do not automatically propagate across descendants.\n\nThis example in JS 1.8.5+ (see https://kangax.github.com/es5-compat-table/)\nIn prototype-based languages that use \"delegation\", the language runtime is capable of dispatching the correct method or finding the right piece of data simply by following a series of delegation pointers (from object to its prototype) until a match is found. All that is required to establish this behavior-sharing between objects is the delegation pointer. Unlike the relationship between class and instance in class-based object-oriented languages, the relationship between the prototype and its offshoots does not require that the child object have a memory or structural similarity to the prototype beyond this link. As such, the child object can continue to be modified and amended over time without rearranging the structure of its associated prototype as in class-based systems. It is also important to note that not only data, but also methods can be added or changed. For this reason, some prototype-based languages refer to both data and methods as \"slots\" or \"members\".\n\nIn \"concatenative\" prototyping - the approach implemented by the Kevo programming language - there are no visible pointers or links to the original prototype from which an object is cloned. The prototype (parent) object is copied rather than linked to and there is no delegation. As a result, changes to the prototype will not be reflected in cloned objects.\n\nThe main conceptual difference under this arrangement is that changes made to a prototype object are not automatically propagated to clones. This may be seen as an advantage or disadvantage. (However, Kevo does provide additional primitives for publishing changes across sets of objects based on their similarity — so-called \"family resemblances\" or \"clone family\" mechanism — rather than through taxonomic origin, as is typical in the delegation model.) It is also sometimes claimed that delegation-based prototyping has an additional disadvantage in that changes to a child object may affect the later operation of the parent. However, this problem is not inherent to the delegation-based model and does not exist in delegation-based languages such as JavaScript, which ensure that changes to a child object are always recorded in the child object itself and never in parents (i.e. the child's value shadows the parent's value rather than changing the parent's value).\n\nIn simplistic implementations, concatenative prototyping will have faster member lookup than delegation-based prototyping (because there is no need to follow the chain of parent objects), but will conversely use more memory (because all slots are copied, rather than there being a single slot pointing to the parent object). More sophisticated implementations can avoid these problems, however, although trade-offs between speed and memory are required. For example, systems with concatenative prototyping can use a copy-on-write implementation to allow for behind-the-scenes data sharing — and such an approach is indeed followed by Kevo. Conversely, systems with delegation-based prototyping can use caching to speed up data lookup.\n\nAdvocates of class-based object models who criticize prototype-based systems often have concerns similar to the concerns that proponents of static type systems for programming languages have of dynamic type systems (see datatype). Usually, such concerns involve: correctness, safety, predictability, efficiency and programmer unfamiliarity.\n\nOn the first three points, classes are often seen as analogous to types (in most statically typed object-oriented languages they serve that role) and are proposed to provide contractual guarantees to their instances, and to users of their instances, that they will behave in some given fashion.\n\nRegarding efficiency, declaring classes simplifies many compiler optimizations that allow developing efficient method and instance-variable lookup. For the Self language, much development time was spent on developing, compiling, and interpreting techniques to improve the performance of prototype-based systems versus class-based systems.\n\nA common criticism made against prototype-based languages is that the community of software developers is unfamiliar with them, despite the popularity and market permeation of JavaScript. This knowledge level of prototype-based systems seems to be increasing with the proliferation of JavaScript frameworks and the complex use of JavaScript as the Web matures. ECMAScript 6 introduced classes as syntactic sugar over JavaScript's existing prototype-based inheritance, providing an alternative way to create objects and deal with inheritance.\n\n\n\n"}
{"id": "8411212", "url": "https://en.wikipedia.org/wiki?curid=8411212", "title": "Quantum dot cellular automaton", "text": "Quantum dot cellular automaton\n\nQuantum dot cellular automata (sometimes referred to simply as quantum cellular automata, or QCA) are a proposed improvement on conventional computer design (CMOS), which have been devised in analogy to conventional models of cellular automata introduced by von Neumann.\n\nAny device designed to represent data and perform computation, regardless of the physics principles it exploits and materials used to build it, must have two fundamental properties: distinguishability and conditional change of state, the latter implying the former. This means that such a device must have barriers that make it possible to distinguish between states, and that it must have the ability to control these barriers to perform conditional change of state. For example, in a digital electronic system, transistors play the role of such controllable energy barriers, making it extremely practical to perform computing with them.\n\nA cellular automata (CA) is a discrete dynamical system consisting of a uniform (finite or infinite) grid of cells. Each cell can be in only one of a finite number of states at a discrete time. As time moves forward, the state of each cell in the grid is determined by a transformation rule that factors in its previous state and the states of the immediately adjacent cells (the cell's \"neighborhood\"). The most well-known example of a cellular automaton is John Horton Conway's \"Game of Life\", which he described in 1970.\n\nCellular automata are commonly implemented as software programs. However, in 1993, Lent et al. proposed a physical implementation of an automaton using quantum-dot cells. The automaton quickly gained popularity and it was first fabricated in 1997. Lent combined the discrete nature of both cellular automata and quantum mechanics, to create nano-scale devices capable of performing computation at very high switching speeds (order of Terahertz) and consuming extremely small amounts of electrical power.\n\nToday, standard solid state QCA cell design considers the distance between quantum dots to be about 20 nm, and a distance between cells of about 60 nm. Just like any CA, Quantum (-dot) Cellular Automata are based on the simple interaction rules between cells placed on a grid. A QCA cell is constructed from four quantum dots arranged in a square pattern. These quantum dots are sites electrons can occupy by tunneling to them.\n\nFigure 2 shows a simplified diagram of a quantum-dot cell. If the cell is charged with two electrons, each free to tunnel to any site in the cell, these electrons will try to occupy the furthest possible site with respect to each other due to mutual electrostatic repulsion. Therefore, two distinguishable cell states exist. Figure 3 shows the two possible minimum energy states of a quantum-dot cell. The state of a cell is called its polarization, denoted as P. Although arbitrarily chosen, using cell polarization P = -1 to represent logic “0” and P = +1 to represent logic “1” has become standard practice.\n\nGrid arrangements of quantum-dot cells behave in ways that allow for computation. The simplest practical cell arrangement is given by placing quantum-dot cells in series, to the side of each other. Figure 4 shows such an arrangement of four quantum-dot cells. The bounding boxes in the figure do not represent physical implementation, but are shown as means to identify individual cells.\n\nIf the polarization of any of the cells in the arrangement shown in figure 4 were to be changed (by a \"driver cell\"), the rest of the cells would immediately synchronize to the new polarization due to Coulombic interactions between them. In this way, a \"wire\" of quantum-dot cells can be made that transmits polarization state. Configurations of such wires can form a complete set of logic gates for computation.\n\nThere are two types of wires possible in QCA: A simple binary wire as shown in Figure 4 and an inverter chain, which is constituted by placing 45-degree inverted QCA cells side by side.\n\nMajority gate and inverter (NOT) gate are considered as the two most fundamental building blocks of QCA. Figure 5 shows a majority gate with three inputs and one output. In this structure, the electrical field effect of each input on the output is identical and additive, with the result that whichever input state (\"binary 0\" or \"binary 1\") is in the majority becomes the state of the output cell — hence the gate's name. For example, if inputs A and B exist in a “binary 0” state and input C exists in a “binary 1” state, the output will exist in a “binary 0” state since the combined electrical field effect of inputs A and B together is greater than that of input C alone.\n\nOther types of gates, namely AND gates and OR gates, can be constructed using a majority gate with fixed polarization on one of its inputs. A NOT gate, on the other hand, is fundamentally different from the majority gate, as shown in Figure 6. The key to this design is that the input is split and both resulting inputs impinge obliquely on the output. In contrast with an orthogonal placement, the electric field effect of this input structure forces a reversal of polarization in the output.\n\nThere is a connection between quantum-dot cells and cellular automata. Cells can only be in one of 2 states and the conditional change of state in a cell is dictated by the state of its adjacent neighbors. However, a method to control data flow is necessary to define the direction in which state transition occurs in QCA cells. The clocks of a QCA system serve two purposes: powering the automaton, and controlling data flow direction. QCA clocks are areas of conductive material under the automaton’s lattice, modulating the electron tunneling barriers in the QCA cells above it.\n\nA QCA clock induces four stages in the tunneling barriers of the cells above it. In the first stage, the tunneling barriers start to rise. The second stage is reached when the tunneling barriers are high enough to prevent electrons from tunneling. The third stage occurs when the high barrier starts to lower. And finally, in the fourth stage, the tunneling barriers allow electrons to freely tunnel again. In simple words, when the clock signal is high, electrons are free to tunnel. When the clock signal is low, the cell becomes latched.\n\nFigure 7 shows a clock signal with its four stages and the effects on a cell at each clock stage. A typical QCA design requires four clocks, each of which is cyclically 90 degrees out of phase with the prior clock. If a horizontal wire consisted of say, 8 cells and each consecutive pair, starting from the left were to be connected to each consecutive clock, data would naturally flow from left to right. The first pair of cells will stay latched until the second pair of cells gets latched and so forth. In this way, data flow direction is controllable through clock zones\n\nWire-crossing in QCA cells can be done by using two different quantum dot orientations (one at 45 degrees to the other) and allowing a wire composed of one type to pass perpendicularly \"through\" a wire of the other type, as shown schematically in figure 8. The distances between dots in both types of cells are exactly the same, producing the same Coulombic interactions between the electrons in each cell. Wires composed of these two cell types, however, are different: one type propagates polarization without change; the other reverses polarization from one adjacent cell to the next. The interaction between the different wire types at the point of crossing produces no net polarization change in either wire, thereby allowing the signals on both wires to be preserved.\n\nAlthough this technique is rather simple, it represents an enormous fabrication problem. A new kind of cell pattern potentially introduces as much as twice the amount of fabrication cost and infrastructure; the number of possible quantum dot locations on an interstitial grid is doubled and an overall increase in geometric design complexity is inevitable. Yet another problem this technique presents is that the additional space between cells of the same orientation decreases the energy barriers between a cell's ground state and a cell’s first excited state. This degrades the performance of the device in terms of maximum operating temperature, resistance to entropy, and switching speed.\n\nA different wire-crossing technique, which makes fabrication of QCA devices more practical, was presented by Christopher Graunke, David Wheeler, Douglas Tougaw, and Jeffrey D. Will, in their paper “Implementation of a crossbar network using quantum-dot cellular automata”. The paper not only presents a new method of implementing wire-crossings, but it also gives a new perspective on QCA clocking.\n\nTheir wire-crossing technique introduces the concept of implementing QCA devices capable of performing computation as a function of synchronization. This implies the ability to modify the device’s function through the clocking system without making any physical changes to the device. Thus, the fabrication problem stated earlier is fully addressed by: a) using only one type of quantum-dot pattern and, b) by the ability to make a universal QCA building block of adequate complexity, which function is determined only by its timing mechanism (i.e., its clocks).\n\nQuasi-adiabatic switching, however, requires that the tunneling barriers of a cell be switched relatively slowly compared to the intrinsic switching speed of a QCA. This prevents ringing and metastable states observed when cells are switched abruptly. Therefore, the switching speed of a QCA is limited not by the time it takes for a cell to change polarization, but by the appropriate quasi-adiabatic switching time of the clocks being used.\n\nWhen designing a device capable of computing, it is often necessary to convert parallel data lines into a serial data stream. This conversion allows different pieces of data to be reduced to a time-dependent series of values on a single wire. Figure 9 shows such a parallel-to-serial conversion QCA device. The numbers on the shaded areas represent different clocking zones at consecutive 90-degree phases. Notice how all the inputs are on the same clocking zone. If parallel data were to be driven at the inputs A, B, C and D, and then driven no more for at least the remaining 15 serial transmission phases, the output X would present the values of D, C, B and A –in that order, at phases three, seven, eleven and fifteen. If a new clocking region were to be added at the output, it could be clocked to latch a value corresponding to any of the inputs by correctly selecting an appropriate state-locking period.\n\nThe new latching clock region would be completely independent from the other four clocking zones illustrated in figure 9. For instance, if the value of interest to the new latching region were to be the value that D presents every 16th phase, the clocking mechanism of the new region would have to be configured to latch a value in the 4th phase and every 16th phase from then on, thus, ignoring all inputs but D.\n\nAdding a second serial line to the device, and adding another latching region would allow for the latching of two input values at the two different outputs. To perform computation, a gate that takes as inputs both serial lines at their respective outputs is added. The gate is placed over a new latching region configured to process data only when both latching regions at the end of the serial lines hold the values of interest at the same instant. Figure 10 shows such an arrangement. If correctly configured, latching regions 5 and 6 will each hold input values of interest to latching region 7. At this instant, latching region 7 will let the values latched on regions 5 and 6 through the AND gate, thus the output could be configured to be the AND result of any two inputs (i.e. R and Q) by merely configuring the latching regions 5, 6 and 7.\n\nThis represents the flexibility to implement 16 functions, leaving the physical design untouched. Additional serial lines and parallel inputs would obviously increase the number of realizable functions. However, a significant drawback of such devices is that, as the number of realizable functions increases, an increasing number of clocking regions is required. As a consequence, a device exploiting this method of function implementation may perform significantly slower than its traditional counterpart.\n\nGenerally speaking, there are four different classes of QCA implementations: metal-island, semiconductor, molecular, and magnetic.\n\nThe metal-island implementation was the first fabrication technology created to demonstrate the concept of QCA. It was not originally intended to compete with current technology in the sense of speed and practicality, as its structural properties are not suitable for scalable designs. The method consists of building quantum dots using aluminum islands. Earlier experiments were implemented with metal islands as big as 1 micrometer in dimension. Because of the relatively large-sized islands, metal-island devices had to be kept at extremely low temperatures for quantum effects (electron switching) to be observable.\n\nSemiconductor (or solid state) QCA implementations could potentially be used to implement QCA devices with the same highly advanced semiconductor fabrication processes used to implement CMOS devices. Cell polarization is encoded as charge position, and quantum-dot interactions rely on electrostatic coupling. However, current semiconductor processes have not yet reached a point where mass production of devices with such small features (~20 nanometers) is possible. Serial lithographic methods, however, make QCA solid state implementation achievable, but not necessarily practical. Serial lithography is slow, expensive and unsuitable for mass-production of solid-state QCA devices. Today, most QCA prototyping experiments are done using this implementation technology.\n\nA proposed but not yet implemented method consists of building QCA devices out of single molecules. The expected advantages of such a method include: highly symmetric QCA cell structure, very high switching speeds, extremely high device density, operation at room temperature, and even the possibility of mass-producing devices by means of self-assembly. A number of technical challenges, including choice of molecules, the design of proper interfacing mechanisms, and clocking technology remain to be solved before this method can be implemented.\n\nMagnetic QCA, commonly referred to as MQCA (or QCA: M), is based on the interaction between magnetic nanoparticles. The magnetization vector of these nanoparticles is analogous to the polarization vector in all other implementations. In MQCA, the term “Quantum” refers to the quantum-mechanical nature of magnetic exchange interactions and not to the electron-tunneling effects. Devices constructed this way could operate at room temperature.\n\nComplementary metal-oxide semiconductor (CMOS) technology has been the industry standard for implementing Very Large Scale Integrated (VLSI) devices for the last two decades, mainly due to the consequences of miniaturization of such devices (i.e. increasing switching speeds, increasing complexity and decreasing power consumption). Quantum Cellular Automata (QCA) is only one of the many alternative technologies proposed as a replacement solution to the fundamental limits CMOS technology will impose in the years to come.\n\nAlthough QCA solves most of the limitations of CMOS technology, it also brings its own. Research suggests that intrinsic switching time of a QCA cell is at best in the order of terahertz. However, the actual speed may be much lower, in the order of megahertz for solid state QCA and gigahertz for molecular QCA, due to the proper quasi-adiabatic clock switching frequency setting.\n\n\n"}
{"id": "33015741", "url": "https://en.wikipedia.org/wiki?curid=33015741", "title": "Rainville polynomials", "text": "Rainville polynomials\n\nIn mathematics, the Rainville polynomials \"p\"(\"z\") are polynomials introduced by given by the generating function\n\n"}
{"id": "3088231", "url": "https://en.wikipedia.org/wiki?curid=3088231", "title": "Ralph P. Boas Jr.", "text": "Ralph P. Boas Jr.\n\nRalph Philip Boas Jr. (August 8, 1912 – July 25, 1992) was a mathematician, teacher, and journal editor. He wrote over 200 papers, mainly in the fields of real and complex analysis.\n\nHe was born in Walla Walla, Washington, the son of an English professor at Whitman College, but moved frequently as a child; his younger sister, Marie Boas Hall, later to become a historian of science, was born in Springfield, Massachusetts, where his father had become a high school teacher. He was home-schooled until the age of eight, began his formal schooling in the sixth grade, and graduated from high school while still only 15. After a gap year auditing classes at Mount Holyoke College (where his father had become a professor) he entered Harvard, intending to major in chemistry and go into medicine, but ended up studying mathematics instead. His first mathematics publication was written as an undergraduate, after he discovered an incorrect proof in another paper. He got his A.B. degree in 1933, received a Sheldon Fellowship for a year of travel, and returned to Harvard for his doctoral studies in 1934. He earned his doctorate there in 1937, under the supervision of David Widder.\n\nAfter postdoctoral studies at Princeton University with Salomon Bochner, and then the University of Cambridge in England, he began a two-year instructorship at Duke University, where he met his future wife, Mary Layne, also a mathematics instructor at Duke. They were married in 1941, and when World War II started later that year, Boas moved to the Navy Pre-flight School in Chapel Hill, North Carolina. In 1942, he interviewed for a position in the Manhattan Project, at the Los Alamos National Laboratory, but ended up returning to Harvard to teach in a Navy instruction program there, while his wife taught at Tufts University.\n\nBeginning when he was an instructor at Duke University, Boas had become a prolific reviewer for \"Mathematical Reviews\", and at the end of the war he took a position as its full-time editor. In the academic year 1950–1951 he was a Guggenheim Fellow.\nIn 1950 he became Professor of Mathematics at Northwestern University, without ever previously having been an assistant or associate professor; his wife became a professor of physics at nearby DePaul University, due to the anti-nepotism rules then in place at Northwestern He stayed at Northwestern until his retirement in 1980, and was chair there from 1957 to 1972. He was president of the Mathematical Association of America from 1973 to 1974, and as president launched the \"Dolciani Mathematical Expositions\" series of books. He was also editor of the \"American Mathematical Monthly\" from 1976 to 1981. He continued mathematical work after retiring, for instance as co-editor (with George Leitmann) of the \"Journal of Mathematical Analysis and Applications\" from 1985 to 1991.\n\nAlong with his mathematical education, Boas was educated in many languages: Latin in junior high school, French and German in high school, Greek at Mount Holyoke, Sanskrit as a Harvard undergraduate, and later self-taught Russian while at Duke University.\n\nBoas' son Harold P. Boas is also a noted mathematician.\n\nBoas, Frank Smithies, and colleagues were behind the 1938 paper \"A Contribution to the Mathematical Theory of Big Game Hunting\" published in the \"American Mathematical Monthly\" under the pseudonym H. Pétard (referring to Hamlet's \"hoist by his own petard\"). The paper offers short spoofs of theorems and proofs from mathematics and physics, in the form of applications to the hunting of lions in the Sahara desert. One \"proof\" parodies the Bolzano–Weierstrass theorem,\n\n\nThe paper became a classic of mathematical humor and spawned various follow-ons over the years with theories or methods from other scientific areas adapted to hunting lions.\n\nThe paper and later work is published in \"Lion Hunting and Other Mathematical Pursuits : A Collection of Mathematics, Verse, and Stories by the Late Ralph P. Boas Jr.\", . Various online collections of the lion hunting methods exist too.\n\nE. S. Pondiczery was another pseudonym invented by Boas and Smithies as the fictional person behind the \"H. Pétard\" pseudonym, and later used again by Boas, this time for a serious paper on topology, \"Power problems in abstract spaces\", Duke Mathematical Journal, 11 (1944), 835–837. This paper and the name became part of the Hewitt-Marczewski-Pondiczery theorem.\n\nThe name, revealed in \"Lion Hunting and Other Mathematical Pursuits\" cited above, came from Pondicherry (a place in India disputed by the Dutch, English and French) and a slavic twist. The initials \"E.S.\" were a plan to write a spoof on extra-sensory perception (ESP).\n\nHis best-known books are the lion-hunting book previously mentioned and the monograph \"A Primer of Real Functions\". The current edition of the primer has been revised and edited by his son, mathematician Harold P. Boas.\n\nThe best-known of his 13 doctoral students is Philip J. Davis, who is also his only advisee who did not graduate from Northwestern. Boas advised Davis, who was at Harvard University, while Boas was visiting at Brown University.\n\n\n"}
{"id": "4781110", "url": "https://en.wikipedia.org/wiki?curid=4781110", "title": "Regina (program)", "text": "Regina (program)\n\nRegina is a suite of mathematical software for 3-manifold topologists. It focuses upon the study of 3-manifold triangulations and includes support for normal surfaces and angle structures.\n\n\n"}
{"id": "44330423", "url": "https://en.wikipedia.org/wiki?curid=44330423", "title": "Richard Zach", "text": "Richard Zach\n\nRichard Zach is a Canadian logician, philosopher of mathematics, and historian of logic and analytic philosophy. He is currently Professor of Philosophy at the University of Calgary. \n\nZach's research interests include the development of formal logic and historical figures (Hilbert, Gödel, and Carnap) associated with this development. In the philosophy of mathematics Zach has worked on Hilbert's program and the philosophical relevance of proof theory. In mathematical logic, he has made contributions to proof theory (epsilon calculus, proof complexity) and to modal and many-valued logic, especially Gödel logic.\n\nZach received his undergraduate education at the Vienna University of Technology and his Ph.D. at the Group in Logic and the Methodology of Science at the University of California, Berkeley. He has taught at the University of Calgary since 2001, and holds the rank of Professor. He has held visiting appointments at the University of California, Irvine and McGill University.Zach is a founding editor of the \"Review of Symbolic Logic\" and the \"Journal for the Study of the History of Analytic Philosophy\", and is also associate editor of \"Studia Logica\", and a subject editor for the \"Stanford Encyclopedia of Philosophy\" (History of Modern Logic). He serves on the editorial boards of the Bernays edition and the Carnap edition. He was elected to the Council of the Association for Symbolic Logic in 2008 and he has served on the ASL Committee on Logic Education and the executive committee of the Kurt Gödel Society.\n\n"}
{"id": "10478956", "url": "https://en.wikipedia.org/wiki?curid=10478956", "title": "Riemann–Hilbert correspondence", "text": "Riemann–Hilbert correspondence\n\nIn mathematics, the Riemann–Hilbert correspondence is a generalization of Hilbert's twenty-first problem to higher dimensions. The original setting was for the Riemann sphere, where it was about the existence of regular differential equations with prescribed monodromy groups. \nFirst the Riemann sphere may be replaced by an arbitrary Riemann surface and then, in higher dimensions, Riemann surfaces are replaced by complex manifolds of dimension > 1. \nThere is a correspondence between certain systems of partial differential equations (linear and having very special properties for their solutions) and possible monodromies of their solutions.\n\nSuch a result was proved for algebraic connections with regular singularities by Pierre Deligne (1970) and more generally for regular holonomic D-modules by Masaki Kashiwara (1980, 1984) and Zoghman Mebkhout (1980, 1984) independently.\n\nSuppose that \"X\" is a smooth complex algebraic variety.\n\nRiemann–Hilbert correspondence (for regular singular connections): \nthere is a functor \"Sol\" called the local solutions functor, that is an equivalence from the category of flat connections on algebraic vector bundles on \"X\" with regular singularities to the category of local systems of finite-dimensional complex vector spaces on \"X\". For \"X\" connected, the category of local systems is also equivalent to the category of complex representations of the fundamental group of \"X\".\n\nThe condition of regular singularities means that locally constant sections of the bundle (with respect to the flat connection) have moderate growth at points of \"Y − X\", where \"Y\" is an algebraic compactification of \"X\". In particular, when \"X\" is compact, the condition of regular singularities is vacuous.\n\nMore generally there is the\n\nRiemann–Hilbert correspondence (for regular holonomic D-modules): there is a functor \"DR\" called the de Rham functor, that is an equivalence from the category of holonomic D-modules on \"X\" with regular singularities to the category of perverse sheaves on \"X\".\n\nBy considering the irreducible elements of each category, this gives a 1:1 correspondence between isomorphism classes of\n\nand\n\nA D-module is something like a system of differential equations on \"X\", and a local system on a subvariety is something like a description of possible monodromies, so this correspondence can be thought of as describing certain systems of differential equations in terms of the monodromies of their solutions.\n\nIn the case \"X\" has dimension one (a complex algebraic curve) then there is a more general Riemann–Hilbert correspondence for algebraic connections with no regularity assumption (or for holonomic D-modules with no regularity assumption) described in Malgrange (1991), the Riemann–Hilbert–Birkhoff correspondence.\n\nAn example where the theorem applies is the differential equation\n\non the punctured affine line \"A\" − {0} (that is, on the nonzero complex numbers C − {0}). Here \"a\" is a fixed complex number. This equation has regular singularities at 0 and ∞ in the projective line P. The local solutions of the equation are of the form \"cz\" for constants \"c\". If \"a\" is not an integer, then the function \"z\" cannot be made well-defined on all of C − {0}. That means that the equation has nontrivial monodromy. Explicitly, the monodromy of this equation is the 1-dimensional representation of the fundamental group (\"A\" − {0}) = Z in which the generator (a loop around the origin) acts by multiplication by \"e\".\n\nTo see the need for the hypothesis of regular singularities, consider the differential equation\n\non the affine line \"A\" (that is, on the complex numbers C). This equation corresponds to a flat connection on the trivial algebraic line bundle over \"A\". The solutions of the equation are of the form \"ce\" for constants \"c\". Since these solutions do not have polynomial growth on some sectors around the point ∞ in the projective line P, the equation does not have regular singularities at ∞. (This can also be seen by rewriting the equation in terms of the variable \"w\" := 1/\"z\", where it becomes\n\nThe pole of order 2 in the coefficients means that the equation does not have regular singularities at \"w\" = 0, according to Fuchs's theorem.)\n\nSince the functions \"ce\" are defined on the whole affine line \"A\", the monodromy of this flat connection is trivial. But this flat connection is not isomorphic to the obvious flat connection on the trivial line bundle over \"A\" (as an algebraic vector bundle with flat connection), because its solutions do not have moderate growth at ∞. This shows the need to restrict to flat connections with regular singularities in the Riemann–Hilbert correspondence. On the other hand, if we work with holomorphic (rather than algebraic) vector bundles with flat connection on a noncompact complex manifold such as \"A\" = C, then the notion of regular singularities is not defined. A much more elementary theorem than the Riemann–Hilbert correspondence states that flat connections on holomorphic vector bundles are determined up to isomorphism by their monodromy.\n\n\n"}
{"id": "24641679", "url": "https://en.wikipedia.org/wiki?curid=24641679", "title": "Robin Thomas (mathematician)", "text": "Robin Thomas (mathematician)\n\nRobin Thomas is a mathematician working in graph theory at the Georgia Institute of Technology.\n\nThomas received his doctorate in 1985 from Charles University in Prague, Czechoslovakia (now the Czech Republic), under the supervision of Jaroslav Nešetřil. He joined the faculty at Georgia Tech in 1989, and is now a Regents' Professor there.\n\nThomas was awarded the Fulkerson Prize for outstanding papers in discrete mathematics twice, in 1994 as co-author of a paper on the Hadwiger conjecture, and in 2009 for the proof of the strong perfect graph theorem.\nIn 2011 he was awarded the Karel Janeček Foundation Neuron Prize for Lifetime Achievement in Mathematics. In 2012 he became a fellow of the American Mathematical Society.\nHe was named a SIAM Fellow in 2018.\n\n"}
{"id": "29237460", "url": "https://en.wikipedia.org/wiki?curid=29237460", "title": "Shapley–Folkman lemma", "text": "Shapley–Folkman lemma\n\nThe Shapley–Folkman lemma is a result in convex geometry with applications in mathematical economics that describes the Minkowski addition of sets in a vector space. \"Minkowski addition\" is defined as the addition of the sets' members: for example, adding the set consisting of the integers zero and one to itself yields the set consisting of zero, one, and two:\nThe Shapley–Folkman lemma and related results provide an affirmative answer to the question, \"Is the sum of many sets close to being convex?\" A set is defined to be \"convex\" if every line segment joining two of its points is a subset in the set: For example, the solid disk formula_1 is a convex set but the circle formula_2 is not, because the line segment joining two distinct points formula_3 is not a subset of the circle. The Shapley–Folkman lemma suggests that if the number of summed sets exceeds the dimension of the vector space, then their Minkowski sum is approximately convex.\n\nThe Shapley–Folkman lemma was introduced as a step in the proof of the Shapley–Folkman theorem, which states an upper bound on the distance between the Minkowski sum and its convex hull. The \"convex hull\" of a set \"Q\" is the smallest convex set that contains \"Q\". This distance is zero if and only if the sum is convex. \nThe theorem's bound on the distance depends on the dimension \"D\" and on the shapes of the summand-sets, but \"not\" on the number of summand-sets \"N\", \nThe shapes of a subcollection of only \"D\" summand-sets determine the bound on the distance between the Minkowski \"average\" of \"N\" sets\nand its convex hull. As \"N\" increases to infinity, the bound decreases to zero (for summand-sets of uniformly bounded size). The Shapley–Folkman theorem's upper bound was decreased by Starr's corollary (alternatively, the Shapley–Folkman–Starr theorem).\n\nThe lemma of Lloyd Shapley and Jon Folkman was first published by the economist Ross M. Starr, who was investigating the existence of economic equilibria while studying with Kenneth Arrow. In his paper, Starr studied a \"convexified\" economy, in which non-convex sets were replaced by their convex hulls; Starr proved that the convexified economy has equilibria that are closely approximated by \"quasi-equilibria\" of the original economy; moreover, he proved that every quasi-equilibrium has many of the optimal properties of true equilibria, which are proved to exist for convex economies. Following Starr's 1969 paper, the Shapley–Folkman–Starr results have been widely used to show that central results of (convex) economic theory are good approximations to large economies with non-convexities; for example, quasi-equilibria closely approximate equilibria of a convexified economy. \"The derivation of these results in general form has been one of the major achievements of postwar economic theory\", wrote Roger Guesnerie. The topic of non-convex sets in economics has been studied by many Nobel laureates, besides Lloyd Shapley who won the prize in 2012: Arrow (1972), Robert Aumann (2005), Gérard Debreu (1983), Tjalling Koopmans (1975), Paul Krugman (2008), and Paul Samuelson (1970); the complementary topic of convex sets in economics has been emphasized by these laureates, along with Leonid Hurwicz, Leonid Kantorovich (1975), and Robert Solow (1987).\n\nThe Shapley–Folkman lemma has applications also in optimization and probability theory. In optimization theory, the Shapley–Folkman lemma has been used to explain the successful solution of minimization problems that are sums of many functions. The Shapley–Folkman lemma has also been used in proofs of the \"law of averages\" for random sets, a theorem that had been proved for only convex sets.\nFor example, the subset of the integers {0, 1, 2} is contained in the interval of real numbers [0, 2], which is convex. The Shapley–Folkman lemma implies that every point in [0, 2] is the sum of an integer from {0, 1} and a real number from [0, 1].\n\nThe distance between the convex interval [0, 2] and the non-convex set {0, 1, 2} equals one-half\nHowever, the distance between the \"average\" Minkowski sum\nand its convex hull [0, 1] is only 1/4, which is half the distance (1/2) between its summand {0, 1} and [0, 1]. As more sets are added together, the average of their sum \"fills out\" its convex hull: The maximum distance between the average and its convex hull approaches zero as the average includes more summands.\n\nThe Shapley–Folkman lemma depends upon the following definitions and results from convex geometry.\n\nA real vector space of two dimensions can be given a Cartesian coordinate system in which every point is identified by an ordered pair of real numbers, called \"coordinates\", which are conventionally denoted by \"x\" and \"y\". Two points in the Cartesian plane can be \"added\" coordinate-wise\nfurther, a point can be \"multiplied\" by each real number \"λ\" coordinate-wise\n\nMore generally, any real vector space of (finite) dimension \"D\" can be viewed as the set of all \"D\"-tuples of \"D\" real numbers  } on which two operations are defined: vector addition and multiplication by a real number. For finite-dimensional vector spaces, the operations of vector addition and real-number multiplication can each be defined coordinate-wise, following the example of the Cartesian plane.\n\nIn a real vector space, a non-empty set \"Q\" is defined to be \"convex\" if, for each pair of its points, every point on the line segment that joins them is a subset of \"Q\". For example, a solid disk formula_1 is convex but a circle formula_2 is not, because it does not contain a line segment joining its points formula_3; the non-convex set of three integers {0, 1, 2} is contained in the interval [0, 2], which is convex. For example, a solid cube is convex; however, anything that is hollow or dented, for example, a crescent shape, is non-convex. The empty set is convex, either by definition or vacuously, depending on the author.\n\nMore formally, a set \"Q\" is convex if, for all points \"v\" and \"v\" in \"Q\" and for every real number \"λ\" in the unit interval [0,1], the point\nis a member of \"Q\".\n\nBy mathematical induction, a set \"Q\" is convex if and only if every convex combination of members of \"Q\" also belongs to \"Q\". By definition, a \"convex combination\" of an indexed subset {\"v\", \"v\", . . . , \"v\"} of a vector space is any weighted average  for some indexed set of non-negative real numbers {\"λ\"} satisfying the equation  = 1.\n\nThe definition of a convex set implies that the \"intersection\" of two convex sets is a convex set. More generally, the intersection of a family of convex sets is a convex set. In particular, the intersection of two disjoint sets is the empty set, which is convex.\n\nFor every subset \"Q\" of a real vector space, its is the minimal convex set that contains \"Q\". Thus Conv(\"Q\") is the intersection of all the convex sets that cover \"Q\". The convex hull of a set can be equivalently defined to be the set of all convex combinations of points in \"Q\". For example, the convex hull of the set of integers {0,1} is the closed interval of real numbers [0,1], which contains the integer end-points. The convex hull of the unit circle is the closed unit disk, which contains the unit circle.\n\nIn any vector space (or algebraic structure with addition), formula_7, the Minkowski sum of two non-empty sets formula_8 is defined to be the element-wise operation formula_9 (See also .)\nFor example\nThis operation is clearly commutative and associative on the collection of non-empty sets. All such operations extend in a well-defined manner to recursive forms formula_11 By the principle of induction it is easy to see that\n\nMinkowski addition behaves well with respect to taking convex hulls. Specifically, for all subsets formula_13 of a real vector space, formula_7, the convex hull of their Minkowski sum is the Minkowski sum of their convex hulls. That is,\nAnd by induction it follows that\nfor any formula_17 and non-empty subsets formula_18, formula_19.\n\nBy the preceding identity, for every point formula_20 there exist elements in the convex hulls, formula_21 for formula_19, dependent upon formula_23, and such that formula_24.\n\nWorking with the above setup, the Shapley–Folkman lemma states that in the above representation\n\n\"at most\" formula_26 of the summands formula_27 need to be taken strictly from the convex hulls. That is, there exists a representation of the above form, such that formula_28. Shuffling indexes if necessary, this means that the point has a representation\n\nwhere formula_30 for formula_31\nand formula_32 for formula_33. Note that the re-indexing depends on the point. More succinctly, the Shapley–Folkman lemma states that\n\nAs an example, every point in formula_35 is according to the lemma the sum of an element in formula_36 and an element in formula_37.\n\nConversely, the Shapley–Folkman lemma characterizes the dimension of finite-dimensional, real vector spaces. That is, if a vector space obeys the Shapley–Folkman lemma for a natural number \"D\", and for no number less than \"D\", then its dimension is exactly \"D\"; the Shapley–Folkman lemma holds for only \"finite-dimensional\" vector spaces.\n\nShapley and Folkman used their lemma to prove their theorem, which bounds the distance between a Minkowski sum and its convex hull, the \"convexified\" sum:\nThe Shapley–Folkman theorem states a bound on the distance between the Minkowski sum and its convex hull; this distance is zero if and only if the sum is convex. Their bound on the distance depends on the dimension \"D\" and on the shapes of the summand-sets, but \"not\" on the number of summand-sets \"N\", \n\nThe circumradius often exceeds (and cannot be less than) the \"inner radius\":\n\nStarr used the inner radius to reduce the upper bound stated in the Shapley–Folkman theorem:\nStarr's corollary states an upper bound on the Euclidean distance between the Minkowski sum of \"N\" sets and the convex hull of the Minkowski sum; this distance between the sum and its convex hull is a measurement of the non-convexity of the set. For simplicity, this distance is called the \"non-convexity\" of the set (with respect to Starr's measurement). Thus, Starr's bound on the non-convexity of the sum depends on only the \"D\" largest inner radii of the summand-sets; however, Starr's bound does not depend on the number of summand-sets \"N\", when .\nFor example, the distance between the convex interval [0, 2] and the non-convex set {0, 1, 2} equals one-half\nThus, Starr's bound on the non-convexity of the \"average\"\ndecreases as the number of summands \"N\" increases.\nFor example, the distance between the \"averaged\" set\nand its convex hull [0, 1] is only 1/4, which is half the distance (1/2) between its summand {0, 1} and [0, 1].\nThe shapes of a subcollection of only \"D\" summand-sets determine the bound on the distance between the \"average set\" and its convex hull; thus, as the number of summands increases to infinity, the bound decreases to zero (for summand-sets of uniformly bounded size). In fact, Starr's bound on the non-convexity of this average set decreases to zero as the number of summands \"N\" increases to infinity (when the inner radii of all the summands are bounded by the same number).\n\nThe original proof of the Shapley–Folkman lemma established only the existence of the representation, but did not provide an algorithm for computing the representation: Similar proofs have been given by Arrow and Hahn, Cassels, and Schneider, among others. An abstract and elegant proof by Ekeland has been extended by Artstein. Different proofs have appeared in unpublished papers, also. In 1981, Starr published an iterative method for computing a representation of a given sum-point; however, his computational proof provides a weaker bound than does the original result. An elementary proof of the Shapley–Folkman lemma in finite-dimensional space can be found in the book by Bertsekas\ntogether with applications in estimating the duality gap in separable optimization problems and zero-sum games.\n\nThe Shapley–Folkman lemma enables researchers to extend results for Minkowski sums of convex sets to sums of general sets, which need not be convex. Such sums of sets arise in economics, in mathematical optimization, and in probability theory; in each of these three mathematical sciences, non-convexity is an important feature of applications.\n\nIn economics, a consumer's preferences are defined over all \"baskets\" of goods. Each basket is represented as a non-negative vector, whose coordinates represent the quantities of the goods. On this set of baskets, an \"indifference curve\" is defined for each consumer; a consumer's indifference curve contains all the baskets of commodities that the consumer regards as equivalent: That is, for every pair of baskets on the same indifference curve, the consumer does not prefer one basket over another. Through each basket of commodities passes one indifference curve. A consumer's \"preference set\" (relative to an indifference curve) is the union of the indifference curve and all the commodity baskets that the consumer prefers over the indifference curve. A consumer's \"preferences\" are \"convex\" if all such preference sets are convex.\n\nAn optimal basket of goods occurs where the budget-line supports a consumer's preference set, as shown in the diagram. This means that an optimal basket is on the highest possible indifference curve given the budget-line, which is defined in terms of a price vector and the consumer's income (endowment vector). Thus, the set of optimal baskets is a function of the prices, and this function is called the consumer's \"demand\". If the preference set is convex, then at every price the consumer's demand is a convex set, for example, a unique optimal basket or a line-segment of baskets.\n\nHowever, if a preference set is \"non-convex\", then some prices determine a budget-line that supports two \"separate\" optimal-baskets. For example, we can imagine that, for zoos, a lion costs as much as an eagle, and further that a zoo's budget suffices for one eagle or one lion. We can suppose also that a zoo-keeper views either animal as equally valuable. In this case, the zoo would purchase either one lion or one eagle. Of course, a contemporary zoo-keeper does not want to purchase half of an eagle and half of a lion (or a griffin)! Thus, the zoo-keeper's preferences are non-convex: The zoo-keeper prefers having either animal to having any strictly convex combination of both.\n\nWhen the consumer's preference set is non-convex, then (for some prices) the consumer's demand is not connected; a disconnected demand implies some discontinuous behavior by the consumer, as discussed by Harold Hotelling:\n\nIf indifference curves for purchases be thought of as possessing a wavy character, convex to the origin in some regions and concave in others, we are forced to the conclusion that it is only the portions convex to the origin that can be regarded as possessing any importance, since the others are essentially unobservable. They can be detected only by the discontinuities that may occur in demand with variation in price-ratios, leading to an abrupt jumping of a point of tangency across a chasm when the straight line is rotated. But, while such discontinuities may reveal the existence of chasms, they can never measure their depth. The concave portions of the indifference curves and their many-dimensional generalizations, if they exist, must forever remain in\nunmeasurable obscurity.\nThe difficulties of studying non-convex preferences were emphasized by Herman Wold and again by Paul Samuelson, who wrote that non-convexities are \"shrouded in eternal according to Diewert.\n\nNonetheless, non-convex preferences were illuminated from 1959 to 1961 by a sequence of papers in \"The Journal of Political Economy\" (\"JPE\"). The main contributors were Farrell, Bator, Koopmans, and Rothenberg. In particular, Rothenberg's paper discussed the approximate convexity of sums of non-convex sets. These \"JPE\"-papers stimulated a paper by Lloyd Shapley and Martin Shubik, which considered convexified consumer-preferences and introduced the concept of an \"approximate equilibrium\". The \"JPE\"-papers and the Shapley–Shubik paper influenced another notion of \"quasi-equilibria\", due to Robert Aumann.\n\nPrevious publications on non-convexity and economics were collected in an annotated bibliography by Kenneth Arrow. He gave the bibliography to Starr, who was then an undergraduate enrolled in Arrow's (graduate) advanced mathematical-economics course. In his term-paper, Starr studied the general equilibria of an artificial economy in which non-convex preferences were replaced by their convex hulls. In the convexified economy, at each price, the aggregate demand was the sum of convex hulls of the consumers' demands. Starr's ideas interested the mathematicians Lloyd Shapley and Jon Folkman, who proved their eponymous lemma and theorem in \"private correspondence\", which was reported by Starr's published paper of 1969.\n\nIn his 1969 publication, Starr applied the Shapley–Folkman–Starr theorem. Starr proved that the \"convexified\" economy has general equilibria that can be closely approximated by \"quasi-equilibria\" of the original economy, when the number of agents exceeds the dimension of the goods: Concretely, Starr proved that there exists at least one quasi-equilibrium of prices \"p\" with the following properties:\n\n\nStarr established that\n\n\"in the aggregate, the discrepancy between an allocation in the fictitious economy generated by [taking the convex hulls of all of the consumption and production sets] and some allocation in the real economy is bounded in a way that is independent of the number of economic agents. Therefore, the average agent experiences a deviation from intended actions that vanishes in significance as the number of agents goes to infinity\".\n\nFollowing Starr's 1969 paper, the Shapley–Folkman–Starr results have been widely used in economic theory. Roger Guesnerie summarized their economic implications: \"Some key results obtained under the convexity assumption remain (approximately) relevant in circumstances where convexity fails. For example, in economies with a large consumption side, preference nonconvexities do not destroy the standard results\". \"The derivation of these results in general form has been one of the major achievements of postwar economic theory\", wrote Guesnerie. The topic of non-convex sets in economics has been studied by many Nobel laureates: Arrow (1972), Robert Aumann (2005), Gérard Debreu (1983), Tjalling Koopmans (1975), Paul Krugman (2008), and Paul Samuelson (1970); the complementary topic of convex sets in economics has been emphasized by these laureates, along with Leonid Hurwicz, Leonid Kantorovich (1975), and Robert Solow (1987). The Shapley–Folkman–Starr results have been featured in the economics literature: in microeconomics, in general-equilibrium theory, in public economics (including market failures), as well as in game theory, in mathematical economics, and in applied mathematics (for economists). The Shapley–Folkman–Starr results have also influenced economics research using measure and integration theory.\n\nThe Shapley–Folkman lemma has been used to explain why large minimization problems with non-convexities can be nearly solved (with iterative methods whose convergence proofs are stated for only convex problems). The Shapley–Folkman lemma has encouraged the use of methods of convex minimization on other applications with sums of many functions.\n\nNonlinear optimization relies on the following definitions for functions:\n\n\n\n\nFor example, the quadratic function \"f\"(\"x\") = \"x\" is convex, as is the absolute value function \"g\"(\"x\") = |\"x\"|. However, the sine function (pictured) is non-convex on the interval (0, π).\n\nIn many optimization problems, the objective function f is \"separable\": that is, \"f\" is the sum of \"many\" summand-functions, each of which has its own argument:\n\nFor example, problems of linear optimization are separable. Given a separable problem with an optimal solution, we fix an optimal solution\n\nwith the minimum value  For this separable problem, we also consider an optimal solution (\"x\", \"f\"(\"x\") )\nto the \"convexified problem\", where convex hulls are taken of the graphs of the summand functions. Such an optimal solution is the limit of a sequence of points in the convexified problem\nOf course, the given optimal-point is a sum of points in the graphs of the original summands and of a small number of convexified summands, by the Shapley–Folkman lemma.\n\nThis analysis was published by Ivar Ekeland in 1974 to explain the apparent convexity of separable problems with many summands, despite the non-convexity of the summand problems. In 1973, the young mathematician Claude Lemaréchal was surprised by his success with convex minimization methods on problems that were known to be non-convex; for minimizing nonlinear problems, a solution of the dual problem problem need not provide useful information for solving the primal problem, unless the primal problem be convex and satisfy a constraint qualification. Lemaréchal's problem was additively separable, and each summand function was non-convex; nonetheless, a solution to the dual problem provided a close approximation to the primal problem's optimal value. Ekeland's analysis explained the success of methods of convex minimization on \"large\" and \"separable\" problems, despite the non-convexities of the summand functions. Ekeland and later authors argued that additive separability produced an approximately convex aggregate problem, even though the summand functions were non-convex. The crucial step in these publications is the use of the Shapley–Folkman lemma. The Shapley–Folkman lemma has encouraged the use of methods of convex minimization on other applications with sums of many functions.\n\nConvex sets are often studied with probability theory. Each point in the convex hull of a (non-empty) subset \"Q\" of a finite-dimensional space is the expected value of a simple random vector that takes its values in \"Q\", as a consequence of Carathéodory's lemma. Thus, for a non-empty set \"Q\", the collection of the expected values of the simple, \"Q\"-valued random vectors equals \"Q\" convex hull; this equality implies that the Shapley–Folkman–Starr results are useful in probability theory. In the other direction, probability theory provides tools to examine convex sets generally and the Shapley–Folkman–Starr results specifically. The Shapley–Folkman–Starr results have been widely used in the probabilistic theory of random sets, for example, to prove a law of large numbers, a central limit theorem, and a large-deviations principle. These proofs of probabilistic limit theorems used the Shapley–Folkman–Starr results to avoid the assumption that all the random sets be convex.\n\nA probability measure is a finite measure, and the Shapley–Folkman lemma has applications in non-probabilistic measure theory, such as the theories of volume and of vector measures. The Shapley–Folkman lemma enables a refinement of the Brunn–Minkowski inequality, which bounds the volume of sums in terms of the volumes of their summand-sets. The volume of a set is defined in terms of the Lebesgue measure, which is defined on subsets of Euclidean space. In advanced measure-theory, the Shapley–Folkman lemma has been used to prove Lyapunov's theorem, which states that the range of a vector measure is convex. Here, the traditional term \"range\" (alternatively, \"image\") is the set of values produced by the function. \nA \"vector measure\" is a vector-valued generalization of a measure; \nfor example, \nif \"p\" and \"p\" are probability measures defined on the same measurable space, \nthen the product function  is a vector measure, \nwhere  \nis defined for every event \"ω\" \nby\nLyapunov's theorem has been used in economics, in (\"bang-bang\") control theory, and in statistical theory. Lyapunov's theorem has been called a continuous counterpart of the Shapley–Folkman lemma, which has itself been called a discrete analogue of Lyapunov's theorem.\n\n"}
{"id": "727811", "url": "https://en.wikipedia.org/wiki?curid=727811", "title": "Snark (graph theory)", "text": "Snark (graph theory)\n\nIn the mathematical field of graph theory, a snark is a simple, connected, bridgeless cubic graph with chromatic index equal to 4. In other words, it is a graph in which every vertex has three neighbors, the connectivity is redundant so that removing no one edge would split the graph, and the edges cannot be colored by only three colors without two edges of the same color meeting at a point. (By Vizing's theorem, the chromatic index of a cubic graph is 3 or 4.) In order to avoid trivial cases, snarks are often restricted to have girth at least 5.\n\nWriting in \"The Electronic Journal of Combinatorics\", Miroslav Chladný states that\nPeter Guthrie Tait initiated the study of snarks in 1880, when he proved that the four color theorem is equivalent to the statement that no snark is planar. The first known snark was the Petersen graph, discovered in 1898. In 1946, Croatian mathematician Danilo Blanuša discovered two more snarks, both on 18 vertices, now named the Blanuša snarks. The fourth known snark was found two years later by W. T. Tutte under the pseudonym Blanche Descartes; it has order 210. In 1973, George Szekeres found the fifth known snark — the Szekeres snark. In 1975, Rufus Isaacs generalized Blanuša's method to construct two infinite families of snarks: the flower snark and the BDS or Blanuša–Descartes–Szekeres snark, a family that includes the two Blanuša snarks, the Descartes snark and the Szekeres snark. Isaacs also discovered a 30-vertices snark that does not belong to the BDS family and that is not a flower snark: the double-star snark.\n\nSnarks were so named by the American mathematician Martin Gardner in 1976, after the mysterious and elusive object of the poem \"The Hunting of the Snark\" by Lewis Carroll.\n\nAll snarks are non-Hamiltonian, and many known snarks are hypohamiltonian: the removal of any single vertex leaves a Hamiltonian subgraph. A hypohamiltonian snark must be \"bicritical\": the removal of any two vertices leaves a 3-edge-colorable subgraph.\n\nIt has been shown that the number of snarks for a given even number of vertices is bounded below by an exponential function. (Being cubic graphs, all snarks must have an even number of vertices.) OEIS sequence contains the number of non-trivial snarks of \"2n\" vertices for small values of \"n\".\n\nThe cycle double cover conjecture posits that in every bridgeless graph one can find a collection of cycles covering each edge twice, or equivalently that the graph can be embedded onto a surface in such a way that all faces of the embedding are simple cycles. Snarks form the difficult case for this conjecture: if it is true for snarks, it is true for all graphs. In this connection, Branko Grünbaum conjectured that it was not possible to embed any snark onto a surface in such a way that all faces are simple cycles and such that every two faces either are disjoint or share only a single edge; however, a counterexample to Grünbaum's conjecture was found by Martin Kochol.\n\nW. T. Tutte conjectured that every snark has the Petersen graph as a minor. That is, he conjectured that the smallest snark, the Petersen graph, may be formed from any other snark by contracting some edges and deleting others. Equivalently (because the Petersen graph has maximum degree three) every snark has a subgraph that can be formed from the Petersen graph by subdividing some of its edges. This conjecture is a strengthened form of the four color theorem, because any graph containing the Petersen graph as a minor must be nonplanar. In 1999, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas announced a proof of this conjecture. , their proof remains largely unpublished. See the Hadwiger conjecture for other problems and results relating graph coloring to graph minors.\n\nTutte also conjectured a generalization of the snark theorem to arbitrary graphs: every bridgeless graph with no Petersen minor has a nowhere zero 4-flow. That is, the edges of the graph may be assigned a direction, and a number from the set {1, 2, 3}, such that the sum of the incoming numbers minus the sum of the outgoing numbers at each vertex is divisible by four. As Tutte showed, for cubic graphs such an assignment exists if and only if the edges can be colored by three colors, so the conjecture follows from the snark theorem in this case. However, this conjecture remains open for graphs that need not be cubic.\n\n\nA list of all of the snarks up to 36 vertices, except those with 36 vertices and girth 4, was generated by Gunnar Brinkmann, Jan Goedgebeur, Jonas Hägglund and Klas Markström in 2012.\n\n"}
{"id": "45239", "url": "https://en.wikipedia.org/wiki?curid=45239", "title": "Subalgebra", "text": "Subalgebra\n\nIn mathematics, a subalgebra is a subset of an algebra, closed under all its operations, and carrying the induced operations.\n\n\"Algebra\", when referring to a structure, often means a vector space or module equipped with an additional bilinear operation. Algebras in universal algebra are far more general: they are a common generalisation of \"all\" algebraic structures. Subalgebra can be a subset of both cases.\n\nA subalgebra of an algebra over a commutative ring or field is a vector subspace which is closed under the multiplication of vectors. The restriction of the algebra multiplication makes it an algebra over the same ring or field. This notion also applies to most specializations, where the multiplication must satisfy additional properties, e.g. to associative algebras or to Lie algebras. Only for unital algebras is there a stronger notion, of unital subalgebra, for which it is also required that the unit of the subalgebra be the unit of the bigger algebra.\n\nThe 2×2-matrices over the reals form a unital algebra in the obvious way. The 2×2-matrices for which all entries are zero, except for the first one on the diagonal, form a subalgebra. It is also unital, but it is not a unital subalgebra.\n\nIn universal algebra, a subalgebra of an algebra \"A\" is a subset \"S\" of \"A\" that also has the structure of an algebra of the same type when the algebraic operations are restricted to \"S\". If the axioms of a kind of algebraic structure is described by equational laws, as is typically the case in universal algebra, then the only thing that needs to be checked is that \"S\" is \"closed\" under the operations.\n\nSome authors consider algebras with partial functions. There are various ways of defining subalgebras for these. Another generalization of algebras is to allow relations. These more general algebras are usually called structures, and they are studied in model theory and in theoretical computer science. For structures with relations there are notions of weak and of induced substructures.\n\nFor example, the standard signature for groups in universal algebra is . (Inversion and unit are needed to get the right notions of homomorphism and so that the group laws can be expressed as equations.) Therefore, a subgroup of a group \"G\" is a subset \"S\" of \"G\" such that:\n\n"}
{"id": "50669027", "url": "https://en.wikipedia.org/wiki?curid=50669027", "title": "Tommy Bonnesen", "text": "Tommy Bonnesen\n\nTommy Bonnesen (27 March 1873 – 14 March 1935) was a Danish mathematician, known for Bonnesen's inequality.\n\nBonnesen studied at the University of Copenhagen, where in 1902 he received his Ph.D. (promotion) with thesis \"Analytiske studier over ikke-euklidisk geometri\" (Analytic studies of non-Euclidean geometry). He was the Professor for Descriptive Geometry at the Polytekniske Læreanstalt.\n\nHe did research on convex geometry and wrote a book on this subject with his student Werner Fenchel. Bonessen was an Invited Speaker at the ICM in 1924 in Toronto and in 1928 in Bologna.\n\nWith Harald Bohr he was for many years the co-editor-in-chief of the Matematisk Tidsskrift of the Danish Mathematical Society.\n\nHis younger daughter was the theatrical and cinematic star Beatrice Bonnesen (1906–1979). His elder daughter Merete Bonnesen (1901–1980) was a journalist employed by the newspaper Politiken.\n\n\n"}
{"id": "13619909", "url": "https://en.wikipedia.org/wiki?curid=13619909", "title": "Waring's prime number conjecture", "text": "Waring's prime number conjecture\n\nIn number theory, Waring's prime number conjecture is a conjecture related to Vinogradov's theorem, named after the English mathematician Edward Waring. It states that every odd number exceeding 3 is either a prime number or the sum of three prime numbers. It follows from the generalized Riemann hypothesis and (trivially) from Goldbach's weak conjecture.\n\n"}
{"id": "477578", "url": "https://en.wikipedia.org/wiki?curid=477578", "title": "Whitney embedding theorem", "text": "Whitney embedding theorem\n\nIn mathematics, particularly in differential topology, there are two Whitney embedding theorems, named after Hassler Whitney:\n\n\nThe general outline of the proof is to start with an immersion with transverse self-intersections. These are known to exist from Whitney's earlier work on the weak immersion theorem. Transversality of the double points follows from a general-position argument. The idea is to then somehow remove all the self-intersections. If has boundary, one can remove the self-intersections simply by isotoping into itself (the isotopy being in the domain of ), to a submanifold of that does not contain the double-points. Thus, we are quickly led to the case where has no boundary. Sometimes it is impossible to remove the double-points via an isotopy—consider for example the figure-8 immersion of the circle in the plane. In this case, one needs to introduce a local double point. Once one has two opposite double points, one constructs a closed loop connecting the two, giving a closed path in . Since is simply connected, one can assume this path bounds a disc, and provided one can further assume (by the weak Whitney embedding theorem) that the disc is embedded in such that it intersects the image of only in its boundary. Whitney then uses the disc to create a 1-parameter family of immersions, in effect pushing across the disc, removing the two double points in the process. In the case of the figure-8 immersion with its introduced double-point, the push across move is quite simple (pictured). This process of eliminating opposite sign double-points by pushing the manifold along a disc is called the Whitney Trick.\n\nTo introduce a local double point, Whitney created a family of immersions which are approximately linear outside of the unit ball, but containing a single double point. For such an immersion is given by\n\nNotice that if is considered as a map to like so:\n\nthen the double point can be resolved to an embedding:\n\nNotice and for then as a function of , is an embedding.\n\nFor higher dimensions \"m\", there are that can be similarly resolved in . For an embedding into , for example, define\n\nThis process ultimately leads one to the definition:\n\nwhere\n\nThe key properties of is that it is an embedding except for the double-point . Moreover, for large, it is approximately the linear embedding .\n\nThe Whitney trick was used by Steve Smale to prove the \"h\"-cobordism theorem; from which follows the Poincaré conjecture in dimensions , and the classification of smooth structures on discs (also in dimensions 5 and up). This provides the foundation for surgery theory, which classifies manifolds in dimension 5 and above.\n\nGiven two oriented submanifolds of complementary dimensions in a simply connected manifold of dimension ≥ 5, one can apply an isotopy to one of the submanifolds so that all the points of intersection have the same sign.\n\nThe occasion of the proof by Hassler Whitney of the embedding theorem for smooth manifolds is said (rather surprisingly) to have been the first complete exposition of the \"manifold concept\" precisely because it brought together and unified the differing concepts of manifolds at the time: no longer was there any confusion as to whether abstract manifolds, intrinsically defined via charts, were any more or less general than manifold extrinsically defined as submanifolds of Euclidean space. See also the history of manifolds and varieties for context.\n\nAlthough every -manifold embeds in , one can frequently do better. Let denote the smallest integer so that all compact connected -manifolds embed in . Whitney's strong embedding theorem states that . For we have , as the circle and the Klein bottle show. More generally, for we have , as the -dimensional real projective space show. Whitney's result can be improved to unless is a power of 2. This is a result of André Haefliger and Morris Hirsch (for ) and C. T. C. Wall (for ); these authors used important preliminary results and particular cases proved by Hirsch, William S. Massey, Sergey Novikov and Vladimir Rokhlin. At present the function is not known in closed-form for all integers (compare to the Whitney immersion theorem, where the analogous number is known).\n\nOne can strengthen the results by putting additional restrictions on the manifold. For example, the -sphere always embeds in  – which is the best possible (closed -manifolds cannot embed in ). Any compact \"orientable\" surface and any compact surface \"with non-empty boundary\" embeds in , though any \"closed non-orientable\" surface needs .\n\nIf is a compact orientable -dimensional manifold, then embeds in (for not a power of 2 the orientability condition is superfluous). For a power of 2 this is a result of André Haefliger and Morris Hirsch (for ), and Fuquan Fang (for ); these authors used important preliminary results proved by Jacques Boéchat and Haefliger, Simon Donaldson, Hirsch and William S. Massey. Haefliger proved that if is a compact -dimensional -connected manifold, then embeds in provided .\n\nA relatively 'easy' result is to prove that any two embeddings of a 1-manifold into R are isotopic. This is proved using general position, which also allows to show that any two embeddings of an -manifold into are isotopic. This result is an isotopy version of the weak Whitney embedding theorem.\n\nWu proved that for , any two embeddings of an -manifold into are isotopic. This result is an isotopy version of the strong Whitney embedding theorem.\n\nAs an isotopy version of his embedding result, Haefliger proved that if is a compact -dimensional -connected manifold, then any two embeddings of into are isotopic provided . The dimension restriction is sharp: Haefliger went on to give examples of non-trivially embedded 3-spheres in (and, more generally, -spheres in ). See further generalizations.\n\n\n\n"}
