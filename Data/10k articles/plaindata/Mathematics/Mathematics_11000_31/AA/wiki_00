{"id": "305606", "url": "https://en.wikipedia.org/wiki?curid=305606", "title": "33 (number)", "text": "33 (number)\n\n33 (thirty-three) is the natural number following 32 and preceding 34.\n\n33 is:\n\n\n\n\n\n\n\n\n\n\n\nThirty-three is:\n\n"}
{"id": "30744693", "url": "https://en.wikipedia.org/wiki?curid=30744693", "title": "Advances in Theoretical and Mathematical Physics", "text": "Advances in Theoretical and Mathematical Physics\n\nEstablished in 1997, the journal publishes articles on theoretical physics and mathematics.\n\nThe current managing editor is Charles Doran (University of Alberta).\n\nThis journal is indexed in the following databases:\n"}
{"id": "3942", "url": "https://en.wikipedia.org/wiki?curid=3942", "title": "Bijection", "text": "Bijection\n\nIn mathematics, a bijection, bijective function, or one-to-one correspondence is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. There are no unpaired elements. In mathematical terms, a bijective function \"f\": \"X\" → \"Y\" is a one-to-one (injective) and onto (surjective) mapping of a set \"X\" to a set \"Y\".\n\nA bijection from the set \"X\" to the set \"Y\" has an inverse function from \"Y\" to \"X\". If \"X\" and \"Y\" are finite sets, then the existence of a bijection means they have the same number of elements. For infinite sets the picture is more complicated, leading to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.\n\nA bijective function from a set to itself is also called a \"permutation\".\n\nBijective functions are essential to many areas of mathematics including the definitions of isomorphism, homeomorphism, diffeomorphism, permutation group, and projective map.\n\nFor a pairing between \"X\" and \"Y\" (where \"Y\" need not be different from \"X\") to be a bijection, four properties must hold:\n\nSatisfying properties (1) and (2) means that a bijection is a function with domain \"X\". It is more common to see properties (1) and (2) written as a single statement: Every element of \"X\" is paired with exactly one element of \"Y\". Functions which satisfy property (3) are said to be \"onto \"Y\" \" and are called surjections (or surjective functions). Functions which satisfy property (4) are said to be \"one-to-one functions\" and are called injections (or injective functions). With this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both \"one-to-one\" and \"onto\".\n\nBijections are sometimes denoted by a two-headed rightwards arrow with tail (), as in \"f\" : \"X\" ⤖ \"Y\". This symbol is a combination of the two-headed rightwards arrow () sometimes used to denote surjections and the rightwards arrow with a barbed tail () sometimes used to denote injections.\n\nConsider the batting line-up of a baseball or cricket team (or any list of all the players of any sports team where every player holds a specific spot in a line-up). The set \"X\" will be the players on the team (of size nine in the case of baseball) and the set \"Y\" will be the positions in the batting order (1st, 2nd, 3rd, etc.) The \"pairing\" is given by which player is in what position in this order. Property (1) is satisfied since each player is somewhere in the list. Property (2) is satisfied since no player bats in two (or more) positions in the order. Property (3) says that for each position in the order, there is some player batting in that position and property (4) states that two or more players are never batting in the same position in the list.\n\nIn a classroom there are a certain number of seats. A bunch of students enter the room and the instructor asks them all to be seated. After a quick look around the room, the instructor declares that there is a bijection between the set of students and the set of seats, where each student is paired with the seat they are sitting in. What the instructor observed in order to reach this conclusion was that:\nThe instructor was able to conclude that there were just as many seats as there were students, without having to count either set.\n\n\nA bijection \"f\" with domain \"X\" (indicated by \"f\": \"X → Y\" in functional notation) also defines a converse relation starting in \"Y\" and going to \"X\" (by turning the arrows around). The process of \"turning the arrows around\" for an arbitrary function does not, \"in general\", yield a function, but properties (3) and (4) of a bijection say that this inverse relation is a function with domain \"Y\". Moreover, properties (1) and (2) then say that this inverse \"function\" is a surjection and an injection, that is, the inverse function exists and is also a bijection. Functions that have inverse functions are said to be invertible. A function is invertible if and only if it is a bijection.\n\nStated in concise mathematical notation, a function \"f\": \"X → Y\" is bijective if and only if it satisfies the condition\n\nContinuing with the baseball batting line-up example, the function that is being defined takes as input the name of one of the players and outputs the position of that player in the batting order. Since this function is a bijection, it has an inverse function which takes as input a position in the batting order and outputs the player who will be batting in that position.\n\nThe composition formula_3 of two bijections \"f\": \"X → Y\" and \"g\": \"Y → Z\" is a bijection. The inverse of formula_3 is formula_5.\nConversely, if the composition formula_3 of two functions is bijective, we can only say that \"f\" is injective and \"g\" is surjective.\n\nIf \"X\" and \"Y\" are finite sets, then there exists a bijection between the two sets \"X\" and \"Y\" if and only if \"X\" and \"Y\" have the same number of elements. Indeed, in axiomatic set theory, this is taken as the definition of \"same number of elements\" (equinumerosity), and generalising this definition to infinite sets leads to the concept of cardinal number, a way to distinguish the various sizes of infinite sets.\n\n\nBijections are precisely the isomorphisms in the category Set of sets and set functions. However, the bijections are not always the isomorphisms for more complex categories. For example, in the category Grp of groups, the morphisms must be homomorphisms since they must preserve the group structure, so the isomorphisms are \"group isomorphisms\" which are bijective homomorphisms.\n\nThe notion of one-to-one correspondence generalizes to partial functions, where they are called partial bijections, although partial bijections are only required to be injective. The reason for this relaxation is that a (proper) partial function is already undefined for a portion of its domain; thus there is no compelling reason to constrain its inverse to be a total function, i.e. defined everywhere on its domain. The set of all partial bijections on a given base set is called the symmetric inverse semigroup.\n\nAnother way of defining the same notion is to say that a partial bijection from \"A\" to \"B\" is any relation \n\"R\" (which turns out to be a partial function) with the property that \"R\" is the graph of a bijection \"f\":\"A′\"→\"B′\", where \"A′\" is a subset of \"A\" and \"B′\" is a subset of \"B\".\n\nWhen the partial bijection is on the same set, it is sometimes called a one-to-one partial transformation. An example is the Möbius transformation simply defined on the complex plane, rather than its completion to the extended complex plane.\n\n\n\nThis topic is a basic concept in set theory and can be found in any text which includes an introduction to set theory. Almost all texts that deal with an introduction to writing proofs will include a section on set theory, so the topic may be found in any of these:\n\n"}
{"id": "26559592", "url": "https://en.wikipedia.org/wiki?curid=26559592", "title": "Computing with Memory", "text": "Computing with Memory\n\nComputing with Memory refers to computing platforms where function response is stored in memory array, either one or two-dimensional, in the form of lookup tables (LUTs) and functions are evaluated by retrieving the values from the LUTs. These computing platforms can follow either a purely spatial computing model, as in field-programmable gate array (FPGA), or a temporal computing model, where a function is evaluated across multiple clock cycles. The latter approach aims at reducing the overhead of programmable interconnect in FPGA by folding interconnect resources inside a computing element. It uses dense two-dimensional memory arrays to store large multiple-input multiple-output LUTs. Computing with Memory differs from Computing in Memory or processor-in-memory (PIM) concepts, widely investigated in the context of integrating a processor and memory on the same chip to reduce memory latency and increase bandwidth. These architectures seek to reduce the distance the data travels between the processor and the memory. The Berkeley IRAM project is one notable contribution in the area of PIM architectures.\n\nComputing with memory platforms are typically used to provide the benefit of hardware reconfigurability. Reconfigurable computing platforms offer advantages in terms of reduced design cost, early time-to-market, rapid prototyping and easily customizable hardware systems. FPGAs present a popular reconfigurable computing platform for implementing digital circuits. They follow a purely spatial computing model. Since their inception in 1985, the basic structure of the FPGAs has continued to consist of two-dimensional array of Configurable Logic blocks (CLBs) and a programmable interconnect matrix. FPGA performance and power dissipation is largely dominated by the elaborate programmable interconnect (PI) architecture. An effective way of reducing the impact of the PI architecture in FPGA is to place small LUTs in close proximity (referred as clusters) and to allow intra-cluster communication using local interconnects. Due to the benefits of a clustered FPGA architecture, major FPGA vendors have incorporated it in their commercial products. Investigations have also been made to reduce the overhead due to PI in fine-grained FPGAs by mapping larger multi-input multi-output LUTs to embedded memory blocks. Although it follows a similar spatial computing model, part of the logic functions are implemented using embedded memory blocks while the remaining part is realized using smaller LUTs. Such a heterogeneous mapping can improve the area and performance by reducing the contribution of programmable interconnects.\n\nContrary to the purely spatial computing model of FPGA, a reconfigurable computing platform that employs a temporal computing model (or a combination of both temporal and spatial) has also been investigated in the context of improving performance and energy over conventional FPGA. These platforms, referred as Memory Based Computing (MBC), use dense two-dimensional memory array to store the LUTs. Such frameworks rely on breaking a complex function (\"f\") into small sub-functions; representing the sub-functions as multi-input, multi-output LUTs in the memory array; and evaluating the function \"f\" over multiple cycles. MBC can leverage on the high density, low power and high performance advantages of nanoscale memory.\nEach computing element incorporates a two-dimensional memory array for storing LUTs, a small controller for sequencing evaluation of sub-functions and a set of temporary registers to hold the intermediate outputs from individual partitions. A fast, local routing framework inside each computing block generates the address for LUT access. Multiple such computing elements can be spatially connected using FPGA-like programmable interconnect architecture to enable mapping of large functions. The local time-multiplexed execution inside the computing elements can drastically reduce the requirement of programmable interconnects leading to large improvement in energy-delay product and better scalability of performance across technology generations. The memory array inside each computing element can be realized by content-addressable memory (CAM) to drastically reduce the memory requirement for certain applications.\n\n"}
{"id": "47548927", "url": "https://en.wikipedia.org/wiki?curid=47548927", "title": "David E. Barrett", "text": "David E. Barrett\n\nDavid Eugene Barrett is a professor of mathematics at the University of Michigan.\n\nBarrett received his Ph.D. from the University of Chicago in 1982 under the supervision of Raghavan Narasimhan.\n\nIn 2012, Barrett became a fellow of the American Mathematical Society.\n"}
{"id": "5302952", "url": "https://en.wikipedia.org/wiki?curid=5302952", "title": "Defective matrix", "text": "Defective matrix\n\nIn linear algebra, a defective matrix is a square matrix that does not have a complete basis of eigenvectors, and is therefore not diagonalizable. In particular, an \"n\" × \"n\" matrix is defective if and only if it does not have \"n\" linearly independent eigenvectors. A complete basis is formed by augmenting the eigenvectors with generalized eigenvectors, which are necessary for solving defective systems of ordinary differential equations and other problems.\n\nA defective matrix always has fewer than \"n\" distinct eigenvalues, since distinct eigenvalues always have linearly independent eigenvectors. In particular, a defective matrix has one or more eigenvalues \"λ\" with algebraic multiplicity \"m\" > 1 (that is, they are multiple roots of the characteristic polynomial), but fewer than \"m\" linearly independent eigenvectors associated with \"λ\". If the algebraic multiplicity of \"λ\" exceeds its geometric multiplicity (that is, the number of linearly independent eigenvectors associated with \"λ\"), then \"λ\" is said to be a defective eigenvalue. However, every eigenvalue with algebraic multiplicity \"m\" always has \"m\" linearly independent generalized eigenvectors.\n\nA Hermitian matrix (or the special case of a real symmetric matrix) or a unitary matrix is never defective; more generally, a normal matrix (which includes Hermitian and unitary as special cases) is never defective.\n\nAny nontrivial Jordan block of size 2×2 or larger (that is, not completely diagonal) is defective. (A diagonal matrix is a special case of the Jordan normal form and is not defective.) For example, the n × n Jordan block,\nhas an eigenvalue, λ, with algebraic multiplicity n, but only one distinct eigenvector,\n\nIn fact, any defective matrix has a nontrivial Jordan normal form, which is as close as one can come to diagonalization of such a matrix.\n\nA simple example of a defective matrix is:\nwhich has a double eigenvalue of 3 but only one distinct eigenvector\n(and constant multiples thereof).\n\n\n"}
{"id": "31905690", "url": "https://en.wikipedia.org/wiki?curid=31905690", "title": "Dehornoy order", "text": "Dehornoy order\n\nIn the mathematical area of braid theory, the Dehornoy order is a left-invariant total order on the braid group, found by .\n\nDehornoy's original discovery of the order on the braid group used huge cardinals, but there are now several more elementary constructions of it.\n\nSuppose that \"σ\", ..., \"σ\" are the usual generators of the braid group \"B\" on \"n\" strings. \nThe set \"P\" of positive elements in the Dehornoy order is defined to be the elements that can be written as word in the elements \"σ\", ..., \"σ\" and their inverses, such that for some \"i\" the word contains σ but does not contain \n\"σ\" for \"j\" < \"i\" \nnor \"σ\".\n\nThe set \"P\" has the properties \"PP\" ⊆ \"P\", and the braid group is a disjoint union of \"P\", 1, and \"P\". \nThese properties imply that if we define \"a\" < \"b\" to mean \"a\"\"b\" ∈ \"P\" then we get a left-invariant total order on the braid group.\n\nThe Dehornoy order is a well-ordering when restricted to the monoid generated by \"σ\", ..., \"σ\".\n\n\n"}
{"id": "1338726", "url": "https://en.wikipedia.org/wiki?curid=1338726", "title": "Delegated Path Discovery", "text": "Delegated Path Discovery\n\nDelegated Path Discovery (DPD) is a method for querying a trusted server for information about a public key certificate.\n\nDPD allows clients to obtain collated certificate information from a trusted DPD server. This information may then be used by the client to validate the subject certificate.\n\nThe requirements for DPD are described in RFC 3379.\n\n"}
{"id": "36618782", "url": "https://en.wikipedia.org/wiki?curid=36618782", "title": "Dudley triangle", "text": "Dudley triangle\n\nIn mathematics, the Dudley triangle is a triangular array of integers that was defined by . It consists of the numbers\n\nDudley exhibited several rows of this triangle, and challenged readers to find the next row; the challenge was met by J. G. Mauldon, who proposed two different solutions. In one of Mauldon's solutions, the number at the intersection of the \"m\"th and \"n\"th diagonals (counting the top of the triangle as having \"m\" = \"n\" = 1) is given by the formula\n\n"}
{"id": "1767634", "url": "https://en.wikipedia.org/wiki?curid=1767634", "title": "Egorov's theorem", "text": "Egorov's theorem\n\nIn measure theory, an area of mathematics, Egorov's theorem establishes a condition for the uniform convergence of a pointwise convergent sequence of measurable functions. It is also named Severini–Egoroff theorem or Severini–Egorov theorem, after Carlo Severini, an Italian mathematician, and Dmitri Egorov, a Russian physicist and geometer, who published independent proofs respectively in 1910 and 1911.\n\nEgorov's theorem can be used along with compactly supported continuous functions to prove Lusin's theorem for integrable functions.\n\nThe first proof of the theorem was given by Carlo Severini in 1910: he used the result as a tool in his research on series of orthogonal functions. His work remained apparently unnoticed outside Italy, probably due to the fact that it is written in Italian, appeared in a scientific journal with limited diffusion and was considered only as a means to obtain other theorems. A year later Dmitri Egorov published his independently proved results, and the theorem became widely known under his name: however, it is not uncommon to find references to this theorem as the Severini–Egoroff theorem or Severini–Egorov Theorem. The first mathematicians to prove independently the theorem in the nowadays common abstract measure space setting were , and in : an earlier generalization is due to Nikolai Luzin, who succeeded in slightly relaxing the requirement of finiteness of measure of the domain of convergence of the pointwise converging functions in the ample paper . Further generalizations were given much later by Pavel Korovkin, in the paper , and by Gabriel Mokobodzki in the paper .\n\nLet (\"f\") be a sequence of \"M\"-valued measurable functions, where \"M\" is a separable metric space, on some measure space (\"X\",Σ,μ), and suppose there is a measurable subset \"A⊆X\", with finite μ-measure, such that (\"f\") converges μ-almost everywhere on \"A\" to a limit function \"f\". The following result holds: for every ε > 0, there exists a measurable subset \"B\" of \"A\" such that μ(\"B\") < ε, and (\"f\") converges to \"f\" uniformly on the relative complement \"A\" \\ \"B\".\n\nHere, μ(\"B\") denotes the μ-measure of \"B\". In words, the theorem says that pointwise convergence almost everywhere on \"A\" implies the apparently much stronger uniform convergence everywhere except on some subset \"B\" of arbitrarily small measure. This type of convergence is also called \"almost uniform convergence\".\n\n\n\nFor natural numbers \"n\" and \"k\", define the set \"E\" by the union\n\nThese sets get smaller as \"n\" increases, meaning that \"E\" is always a subset of \"E\", because the first union involves fewer sets. A point \"x\", for which the sequence (\"f\"(\"x\")) converges to \"f\"(\"x\"), cannot be in every \"E\" for a fixed \"k\", because \"f\"(\"x\") has to stay closer to \"f\"(\"x\") than 1/\"k\" eventually. Hence by the assumption of μ-almost everywhere pointwise convergence on \"A\",\n\nfor every \"k\". Since \"A\" is of finite measure, we have continuity from above; hence there exists, for each \"k\", some natural number \"n\" such that\n\nFor \"x\" in this set we consider the speed of approach into the 1/\"k\"-neighbourhood of \"f\"(\"x\") as too slow. Define\n\nas the set of all those points \"x\" in \"A\", for which the speed of approach into at least one of these 1/\"k\"-neighbourhoods of \"f\"(\"x\") is too slow. On the set difference \"A\" \\ \"B\" we therefore have uniform convergence.\n\nAppealing to the sigma additivity of μ and using the geometric series, we get\n\nNikolai Luzin's generalization of the Severini–Egorov theorem is presented here according to .\n\nUnder the same hypothesis of the abstract Severini–Egorov theorem suppose that \"A\" is the union of a sequence of measurable sets of finite μ-measure, and (\"f\") is a given sequence of \"M\"-valued measurable functions on some measure space (\"X\",Σ,μ), such that (\"f\") converges μ-almost everywhere on \"A\" to a limit function \"f\", then \"A\" can be expressed as the union of a sequence of measurable sets \"H\", \"A\", \"A\"... such that μ(\"H\") = 0 and (\"f\") converges to \"f\" uniformly on each set \"A\".\n\nIt is sufficient to consider the case in which the set \"A\" is itself of finite μ-measure: using this hypothesis and the standard Severini–Egorov theorem, it is possible to define by mathematical induction a sequence of sets {\"A\"} such that\nand such that (\"f\") converges to \"f\" uniformly on each set \"A\" for each \"k\". Choosing\nthen obviously μ(\"H\") = 0 and the theorem is proved.\n\nThe proof of the Korovkin version follows closely the version on , which however generalizes it to some extent by considering admissible functionals instead of non-negative measures and inequalities formula_10 and formula_11 respectively in conditions 1 and 2.\n\nLet (\"M\",\"d\") denote a separable metric space and (\"X\",Σ) a measurable space: consider a measurable set \"A\" and a class formula_12 containing \"A\" and its measurable subsets such that their countable in unions and intersections belong to the same class. Suppose there exists a non-negative measure μ such that μ(\"A\") exists and\nIf (\"f\") is a sequence of M-valued measurable functions converging μ-almost everywhere on \"A\"formula_17 to a limit function \"f\", then there exists a subset \"A′\" of \"A\" such that 0 < μ(\"A\") − μ(\"A′\")<ε and where the convergence is also uniform.\n\nConsider the indexed family of sets whose index set is the set of natural numbers \"m\"formula_25, defined as follows:\nObiviously\nand\ntherefore there is a natural number \"m\" such that putting \"A\"=\"A\" the following relation holds true:\nUsing \"A\" it is possible to define the following indexed family\nsatisfying the following two relationships, analogous to the previously found ones, i.e.\nand\nThis fact enable us to define the set \"A\"=\"A\", where \"m\" is a surely existing natural number such that\nBy iterating the shown construction, another indexed family of set {\"A\"} is defined such that it has the following properties:\nand finally putting\nthe thesis is easily proved.\n\n\n"}
{"id": "1349499", "url": "https://en.wikipedia.org/wiki?curid=1349499", "title": "Evgenii Landis", "text": "Evgenii Landis\n\nEvgenii Mikhailovich Landis (, \"Yevgeny Mikhaylovich Landis\"; October 6, 1921 – December 12, 1997) was a Soviet mathematician who worked mainly on partial differential equations.\n\nLandis was born in Kharkiv, Ukrainian SSR, Soviet Union. He studied and worked at the Moscow State University, where his advisor was Alexander Kronrod, and later Ivan Petrovsky. In 1946, together with Kronrod, he rediscovered Sard's lemma unknown in Russia at the time. \n\nLater he worked on uniqueness theorems for elliptic and parabolic differential equations, Harnack inequalities, and Phragmén–Lindelöf type theorems. With Georgy Adelson-Velsky, he invented the AVL tree datastructure (where \"AVL\" stands for Adelson-Velsky Landis). \n\nHe died in Moscow. His students include Yulij Ilyashenko.\n\n"}
{"id": "25905119", "url": "https://en.wikipedia.org/wiki?curid=25905119", "title": "Fermat's Last Theorem (book)", "text": "Fermat's Last Theorem (book)\n\nFermat's Last Theorem is a popular science book (1997) by Simon Singh. It tells the story of the search for a proof of Fermat's last theorem, first conjectured by Pierre de Fermat in 1637, and explores how many mathematicians such as Évariste Galois had tried and failed to provide a proof for the theorem. Despite the efforts of many mathematicians, the proof would remain incomplete until as late as 1995, with the publication of Andrew Wiles' proof of the Theorem. The book is the first mathematics book to become a Number One seller in the United Kingdom, whilst Singh's documentary \"The Proof\", on which the book was based, won a BAFTA in 1997.\n\nIn the United States, the book was released as \"Fermat's Enigma: The Epic Quest to Solve the World's Greatest Mathematical Problem\". The book was released in the United States in October 1998 to coincide with the US release of Singh's documentary \"The Proof\" about Wiles's proof of Fermat's Last Theorem.\n"}
{"id": "10859", "url": "https://en.wikipedia.org/wiki?curid=10859", "title": "Fields Medal", "text": "Fields Medal\n\nThe Fields Medal is a prize awarded to two, three, or four mathematicians under 40 years of age at the International Congress of the International Mathematical Union (IMU), a meeting that takes place every four years.\n\nThe Fields Medal is regarded as one of the highest honors a mathematician can receive, and has been described as the mathematician's \"Nobel Prize\" , although there are several key differences, including frequency of award, number of awards, and age limits. According to the annual Academic Excellence Survey by ARWU, the Fields Medal is consistently regarded as the top award in the field of mathematics worldwide, and in another reputation survey conducted by IREG in 2013-14, the Fields Medal came closely after the Abel Prize as the second most prestigious international award in mathematics.\n\nThe prize comes with a monetary award which, since 2006, has been 15,000. The name of the award is in honour of Canadian mathematician John Charles Fields. Fields was instrumental in establishing the award, designing the medal itself, and funding the monetary component.\n\nThe medal was first awarded in 1936 to Finnish mathematician Lars Ahlfors and American mathematician Jesse Douglas, and it has been awarded every four years since 1950. Its purpose is to give recognition and support to younger mathematical researchers who have made major contributions. In 2014, the Iranian mathematician Maryam Mirzakhani became the first woman Fields Medalist. In all, sixty people have been awarded the Fields Medal.\n\nThe most recent group of Fields Medalists received their awards on 1 August 2018 at the opening ceremony of the IMU International Congress, held in Rio de Janeiro, Brazil. The medal belonging to one of the four joint winners, Caucher Birkar, was stolen shortly after the event. The ICM presented Birkar with a replacement medal a few days later.\n\nThe Fields Medal is often described as the \"Nobel Prize of Mathematics\" and for a long time has been regarded as the most prestigious award in the field of mathematics. Unlike the Nobel Prize, the Fields Medal is only awarded every four years. The Fields Medal also has an age limit: a recipient must be under age 40 on 1 January of the year in which the medal is awarded. This is similar to restrictions applicable to the Clark Medal in economics. The under-40 rule is based on Fields' desire that \"while it was in recognition of work already done, it was at the same time intended to be an encouragement for further achievement on the part of the recipients and a stimulus to renewed effort on the part of others.\" Moreover, an individual can only be awarded one Fields Medal; laureates are ineligible to be awarded future medals. This is in contrast with the Nobel Prize which can be, and has been awarded to an individual or an entity more than once, whether in the same category (John Bardeen and Frederick Sanger), or in different categories (Marie Curie and Linus Pauling).\n\nThe monetary award is much lower than the 8,000,000 Swedish kronor (roughly 1,400,000 Canadian dollars) given with each Nobel prize as of 2014. Other major awards in mathematics, such as the Abel Prize and the Chern Medal, have larger monetary prizes compared to the Fields Medal.\n\nIn 1954, Jean-Pierre Serre became the youngest winner of the Fields Medal, at 27. He retains this distinction.\n\nIn 1966, Alexander Grothendieck boycotted the ICM, held in Moscow, to protest Soviet military actions taking place in Eastern Europe. Léon Motchane, founder and director of the Institut des Hautes Études Scientifiques, attended and accepted Grothendieck's Fields Medal on his behalf.\n\nIn 1970, Sergei Novikov, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Nice to receive his medal.\n\nIn 1978, Grigory Margulis, because of restrictions placed on him by the Soviet government, was unable to travel to the congress in Helsinki to receive his medal. The award was accepted on his behalf by Jacques Tits, who said in his address: \"I cannot but express my deep disappointment—no doubt shared by many people here—in the absence of Margulis from this ceremony. In view of the symbolic meaning of this city of Helsinki, I had indeed grounds to hope that I would have a chance at last to meet a mathematician whom I know only through his work and for whom I have the greatest respect and admiration.\"\n\nIn 1982, the congress was due to be held in Warsaw but had to be rescheduled to the next year, because of martial law introduced in Poland on 13 December 1981. The awards were announced at the ninth General Assembly of the IMU earlier in the year and awarded at the 1983 Warsaw congress.\n\nIn 1990, Edward Witten became the first physicist to win the award.\n\nIn 1998, at the ICM, Andrew Wiles was presented by the chair of the Fields Medal Committee, Yuri I. Manin, with the first-ever IMU silver plaque in recognition of his proof of Fermat's Last Theorem. Don Zagier referred to the plaque as a \"quantized Fields Medal\". Accounts of this award frequently make reference that at the time of the award Wiles was over the age limit for the Fields medal. Although Wiles was slightly over the age limit in 1994, he was thought to be a favorite to win the medal; however, a gap (later resolved by Taylor and Wiles) in the proof was found in 1993.\n\nIn 2006, Grigori Perelman, who proved the Poincaré conjecture, refused his Fields Medal and did not attend the congress.\n\nIn 2014, Maryam Mirzakhani became the first woman as well as the first Iranian to win the Fields Medal, and Artur Avila became the first South American and Manjul Bhargava became the first person of Indian origins to do so.\n\nThe medal was designed by Canadian sculptor R. Tait McKenzie.\n\n\nTranslation: \"Mathematicians gathered from the entire world have awarded [understood but not written: 'this prize'] for outstanding writings.\"\n\nIn the background, there is the representation of Archimedes' tomb, with the carving illustrating his theorem On the Sphere and Cylinder, behind a branch. (This is the mathematical result of which Archimedes was reportedly most proud: Given a sphere and a circumscribed cylinder of the same height and diameter, the ratio between their volumes is equal to ⅔.)\n\nThe rim bears the name of the prizewinner.\n\nIn terms of the most prestigious awards in STEM fields, only a small proportion have been awarded to women. The Fields Medal has been obtained only one time by a woman, Maryam Mirzakhani, in 2014, out of a total of (currently) 60 medalists.\n\n\n"}
{"id": "8831251", "url": "https://en.wikipedia.org/wiki?curid=8831251", "title": "Georges Ifrah", "text": "Georges Ifrah\n\nGeorges Ifrah (born 1947, Marrakech, French Morocco) is a French author and historian of mathematics, especially numerals. He was formerly a teacher of mathematics.\n\nHis exhaustive work, \"From One to Zero: A Universal History of Numbers\" (1985, 1994) was translated into multiple languages, became an international bestseller, was included in \"American Scientist\"'s list of \"100 or so Books that shaped a Century of Science\", referring to the 20th century.\n\nDespite popular acclaim for his works on the history of numbers, they have been broadly criticized by scholars.\n\nSeveral books devoted to numbers and history of numbers and number related topics including:\n\n"}
{"id": "30861099", "url": "https://en.wikipedia.org/wiki?curid=30861099", "title": "Gilbert–Johnson–Keerthi distance algorithm", "text": "Gilbert–Johnson–Keerthi distance algorithm\n\nThe Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the Minkowski sum (CSO) of two convex shapes.\n\n\"Enhanced GJK\" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. This improves performance substantially for polytopes with large numbers of vertices.\n\nGJK algorithms are often used incrementally in simulation systems and video games. In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or \"frame\". If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. This yields collision detection systems which operate in near-constant time.\n\nThe algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games.\n\nGJK relies on two functions:\n\n\nThe simplices handled by formula_8 may each be any simplex sub-space of . For example in 3D, they may be a point, a line segment, a triangle, or a tetrahedron; each defined by 1, 2, 3, or 4 points respectively. \n\ncodice_1\n\n"}
{"id": "8132847", "url": "https://en.wikipedia.org/wiki?curid=8132847", "title": "Grand mean", "text": "Grand mean\n\nThe grand mean is the mean of the means of several subsamples, as long as the subsamples have the same number of data points. For example, consider several lots, each containing several items. The items from each lot are sampled for a measure of some variable and the means of the measurements from each lot are computed. The mean of the measures from each lot constitutes the subsample mean. The mean of these subsample means is then the grand mean.\n\nSuppose there are three groups of numbers: group A has 2, 6, 7, 11, 4; group B has 4, 6, 8, 14,8; group C has 8, 7, 4, 1, 5.\n\nThe mean of group A = (2+6+7+11+4)/5 = 6,\n\nThe mean of group B = (4+6+8+14+8)/5 = 8,\n\nThe mean of group C = (8+7+4+1+5)/5 = 5,\n\nTherefore, the grand mean of all numbers = (6+8+5)/3 = 6.333.\n\nSuppose one wishes to determine which states in America have the tallest men. To do so, one measures the height of a suitably sized sample of men in each state. Next, one calculates the means of height for each state, and then the grand mean (the mean of the state means) as well as the corresponding standard deviation of the state means. Now, one has the necessary information for a preliminary determination of which states have abnormally tall or short men by comparing the means of each state to the grand mean ± some multiple of the standard deviation.\n\nIn ANOVA, there is a similar usage of grand mean to calculate sum of squares (SSQ), a measurement of variation. The total variation is defined as the sum of squared differences between each score and the grand mean (designated as GM), given by the equation\n\nThe term \"grand mean\" is used for two different concepts that should not be confused, namely, the overall mean and the mean of means. The overall mean (in a grouped data set) is equal to the sample mean, namely, formula_2. The mean of means is literally the mean of the \"G (g=1...,G)\" group means formula_3, namely, formula_4. If the sample sizes across the \"G\" groups are equal, then the two statistics coincide.\n\n"}
{"id": "9982439", "url": "https://en.wikipedia.org/wiki?curid=9982439", "title": "Graph enumeration", "text": "Graph enumeration\n\nIn combinatorics, an area of mathematics, graph enumeration describes a class of combinatorial enumeration problems in which one must count undirected or directed graphs of certain types, typically as a function of the number of vertices of the graph. These problems may be solved either exactly (as an algebraic enumeration problem) or asymptotically.\nThe pioneers in this area of mathematics were George Pólya, Arthur Cayley and John Howard Redfield.\n\nIn some graphical enumeration problems, the vertices of the graph are considered to be \"labeled\" in such a way as to be distinguishable from each other, while in other problems any permutation of the vertices is considered to form the same graph, so the vertices are considered identical or \"unlabeled\". In general, labeled problems tend to be easier. As with combinatorial enumeration more generally, the Pólya enumeration theorem is an important tool for reducing unlabeled problems to labeled ones: each unlabeled class is considered as a symmetry class of labeled objects.\n\nSome important results in this area include the following.\n"}
{"id": "11492935", "url": "https://en.wikipedia.org/wiki?curid=11492935", "title": "Graph product", "text": "Graph product\n\nIn mathematics, a graph product is a binary operation on graphs. Specifically, it is an operation that takes two graphs \"G\" and \"G\" and produces a graph \"H\" with the following properties:\nThe terminology and notation for specific graph products in the literature varies quite a lot; even if the following may be considered standard, readers are advised to check what definition a particular author uses for a graph product, especially in older texts.\n\nThe following table shows the most common graph products, with formula_1 denoting “is connected by an edge to”, and formula_2 denoting non-connection. The operator symbols listed here are by no means standard, especially in older papers.\nIn general, a graph product is determined by any condition for (\"u\", \"u\") ∼ (\"v\", \"v\") that can be expressed in terms of the statements \"u\" ∼ \"v\", \"u\" ∼ \"v\", \"u\" = \"v\", and \"u\" = \"v\".\n\nLet formula_3 be the complete graph on two vertices (i.e. a single edge). The product graphs formula_4, formula_5, and formula_6 look exactly like the graph representing the operator. For example, formula_4 is a four cycle (a square) and formula_6 is the complete graph on four vertices. The formula_9 notation for lexicographic product serves as a reminder that this product is not commutative.\n\n\n"}
{"id": "23399762", "url": "https://en.wikipedia.org/wiki?curid=23399762", "title": "Hierarchical constraint satisfaction", "text": "Hierarchical constraint satisfaction\n\nIn artificial intelligence and operations research, hierarchical constraint satisfaction (HCS) is a method of handling constraint satisfaction problems where the variables have large domains by exploiting their internal structure.\n\nFor many real-world problems the domain elements cluster together into sets with common properties and relations. This structure can be represented as a hierarchy and is partially ordered on the subset of a relation. The expectation is that the domains are structured so that the elements of a set frequently share consistency properties permitting them to be retained or eliminated as a unit. Thus, if some elements of a set satisfy a constraint, but not all, the subsets of the set are considered. In this way, if no elements of a set can satisfy the constraint the whole set can be discarded. Thus, structuring the domain helps in considering sets of elements all at a time and hence helps in pruning the search space more quickly.\n"}
{"id": "1695231", "url": "https://en.wikipedia.org/wiki?curid=1695231", "title": "Hilbert's fourteenth problem", "text": "Hilbert's fourteenth problem\n\nIn mathematics, Hilbert's fourteenth problem, that is, number 14 of Hilbert's problems proposed in 1900, asks whether certain algebras are finitely generated. \n\nThe setting is as follows: Assume that \"k\" is a field and let \"K\" be a subfield of the field of rational functions in \"n\" variables, \n\nConsider now the \"k\"-algebra \"R\" defined as the intersection \n\nHilbert conjectured that all such algebras are finitely generated over \"k\".\n\nAfter some results were obtained confirming Hilbert's conjecture in special cases and for certain classes of rings (in particular the conjecture was proved unconditionally for \"n\" = 1 and \"n\" = 2 by Zariski in 1954) then in 1959 Masayoshi Nagata found a counterexample to Hilbert's conjecture. The counterexample of Nagata is a suitably constructed ring of invariants for the action of a linear algebraic group.\n\nThe problem originally arose in algebraic invariant theory. Here the ring \"R\" is given as a (suitably defined) ring of polynomial invariants of a linear algebraic group over a field \"k\" acting algebraically on a polynomial ring \"k\"[\"x\", ..., \"x\"] (or more generally, on a finitely generated algebra defined over a field). In this situation the field \"K\" is the field of \"rational\" functions (quotients of polynomials) in the variables \"x\" which are invariant under the given action of the algebraic group, the ring \"R\" is the ring of \"polynomials\" which are invariant under the action. A classical example in nineteenth century was the extensive study (in particular by Cayley, Sylvester, Clebsch, Paul Gordan and also Hilbert) of invariants of binary forms in two variables with the natural action of the special linear group \"SL\"(\"k\") on it. Hilbert himself proved the finite generation of invariant rings in the case of the field of complex numbers for some classical semi-simple Lie groups (in particular the general linear group over the complex numbers) and specific linear actions on polynomial rings, i.e. actions coming from finite-dimensional representations of the Lie-group. This finiteness result was later extended by Hermann Weyl to the class of all semi-simple Lie-groups. A major ingredient in Hilbert's proof is the Hilbert basis theorem applied to the ideal inside the polynomial ring generated by the invariants.\n\nZariski's formulation of Hilbert's fourteenth problem asks whether, for a quasi-affine algebraic variety \"X\" over a field \"k\", possibly assuming \"X\" normal or smooth, the ring of regular functions on \"X\" is finitely generated over \"k\".\n\nZariski's formulation was shown to be equivalent to the original problem, for \"X\" normal. (See also: Zariski's finiteness theorem.)\n\nÉfendiev F.F. (Fuad Efendi) provided symmetric algorithm generating basis of invariants of n-ary forms of degree r.\n\n gave the following counterexample to Hilbert's problem. The field \"k\" is a field containing 48 elements \"a\", ...,\"a\", for \"i\"=1, 2, 3 that are algebraically independent over the prime field. The ring \"R\" is the polynomial ring \"k\"[\"x\"...,\"x\", \"t\"...,\"t\"] in 32 variables. The vector space \"V\" is a 13-dimensional vector space over \"k\" consisting of all vectors (\"b\"...,\"b\") in \"k\" orthogonal to each of the three vectors (\"a\", ...,\"a\") for \"i\"=1, 2, 3. The vector space \"V\" is a 13-dimensional commutative unipotent algebraic group under addition, and its elements act on \"R\" by fixing all elements \"t\" and taking \"x\" to \"x\" + \"b\"\"t\". Then the ring of elements of \"R\" invariant under the action of the group \"V\" is not a finitely generated \"k\"-algebra.\n\nSeveral authors have reduced the sizes of the group and the vector space in Nagata's example. For example, showed that over any field there is an action of the sum \"G\" of three copies of the additive group on \"k\" whose ring of invariants is not finitely generated.\n\n\n"}
{"id": "6612596", "url": "https://en.wikipedia.org/wiki?curid=6612596", "title": "Hilbert series and Hilbert polynomial", "text": "Hilbert series and Hilbert polynomial\n\nIn commutative algebra, the Hilbert function, the Hilbert polynomial, and the Hilbert series of a graded commutative algebra finitely generated over a field are three strongly related notions which measure the growth of the dimension of the homogeneous components of the algebra.\n\nThese notions have been extended to filtered algebras, and graded or filtered modules over these algebras, as well as to coherent sheaves over projective schemes.\n\nThe typical situations where these notions are used are the following:\n\nThe Hilbert series of an algebra or a module is a special case of the Hilbert–Poincaré series of a graded vector space.\n\nHilbert polynomial and Hilbert series are important in computational algebraic geometry, as they are the easiest known way for computing the dimension and the degree of an algebraic variety defined by explicit polynomial equations.\n\nLet us consider a finitely generated graded commutative algebra over a field , which is finitely generated by elements of positive degree. This means that \nand that formula_2.\n\nThe Hilbert function\nmaps the integer onto the dimension of the -vector space . The Hilbert series, which is called Hilbert–Poincaré series in the more general setting of graded vector spaces, is the formal series\n\nIf is generated by homogeneous elements of positive degrees formula_5, then the sum of the Hilbert series is a rational fraction\n\nwhere is a polynomial with integer coefficients.\n\nIf is generated by elements of degree 1 then the sum of the Hilbert series may be rewritten as\n\nwhere is a polynomial with integer coefficients, and formula_8 is the Krull dimension of .\n\nIn this case the series expansion of this rational fraction is\n\nwhere the binomial coefficient formula_10 is formula_11 for formula_12 and 0 otherwise.\n\nIf formula_13\nthe coefficient of formula_14 in formula_15 is thus\nFor formula_17 the term of index in this sum is a polynomial in of degree formula_18 with leading coefficient formula_19\nThis shows that there exists a unique polynomial formula_20 with rational coefficients which is equal to formula_21 for large enough. This polynomial is the Hilbert polynomial, and has the form\n\nThe least such that formula_23 for is called the Hilbert regularity. It may be lower than formula_24.\n\nThe Hilbert polynomial is a numerical polynomial, since the dimensions are integers, but the polynomial almost never has integer coefficients .\n\nAll these definitions may be extended to finitely generated graded modules over , with the only difference that a factor appears in the Hilbert series, where is the minimal degree of the generators of the module, which may be negative.\n\nThe Hilbert function, the Hilbert series and the Hilbert polynomial of a filtered algebra are those of the associated graded algebra.\n\nThe Hilbert polynomial of a projective variety in is defined as the Hilbert polynomial of the homogeneous coordinate ring of .\n\nPolynomial rings and their quotients by homogeneous ideals are typical graded algebras. Conversely, if is a graded algebra generated over the field by homogeneous elements of degree 1, then the map which sends onto defines an homomorphism of graded rings from formula_25 onto . Its kernel is a homogeneous ideal and this defines an isomorphism of graded algebra between formula_26 and .\n\nThus, the graded algebras generated by elements of degree 1 are exactly, up to an isomorphism, the quotients of polynomial rings by homogeneous ideals. Therefore, the remainder of this article will be restricted to the quotients of polynomial rings by ideals.\n\nHilbert series and Hilbert polynomial are additive relatively to exact sequences. More precisely, if \nis an exact sequence of graded or filtered modules, then we have \nand \nThis follows immediately from the same property for the dimension of vector spaces.\n\nLet be a graded algebra and a homogeneous element of degree in which is not a zero divisor. Then we have \nIt follows from the additivity on the exact sequence \nwhere the arrow labeled is the multiplication by , and formula_32 is the graded module which is obtained from by shifting the degrees by , in order that the multiplication by has degree 0. This implies that formula_33\n\nThe Hilbert series of the polynomial ring formula_34 in formula_35 indeterminates is \nIt follows that the Hilbert polynomial is \n\nThe proof that the Hilbert series has this simple form is obtained by applying recursively the previous formula for the quotient by a non zero divisor (here formula_38) and remarking that formula_39\n\nA graded algebra generated by homogeneous elements of degree 1 has Krull dimension zero if the maximal homogeneous ideal, that is the ideal generated by the homogeneous elements of degree 1, is nilpotent. This implies that the dimension of as a -vector space is finite and the Hilbert series of is a polynomial such that is equal to the dimension of as a -vector space.\n\nIf the Krull dimension of is positive, there is a homogeneous element of degree one which is not a zero divisor (in fact almost all elements of degree one have this property). The Krull dimension of is the Krull dimension of minus one.\n\nThe additivity of Hilbert series shows that formula_40. Iterating this a number of times equal to the Krull dimension of , we get eventually an algebra of dimension 0 whose Hilbert series is a polynomial . This show that the Hilbert series of is\nwhere the polynomial is such that and is the Krull dimension of .\n\nThis formula for the Hilbert series implies that the degree of the Hilbert polynomial is , and that its leading coefficient is formula_42.\n\nThe Hilbert series allows us to compute the degree of an algebraic variety as the value at 1 of the numerator of the Hilbert series. This provides also a rather simple proof of Bézout's theorem.\n\nFor showing the relationship between the degree of a projective algebraic set and the Hilbert series, let us consider an projective algebraic set , defined as the set of the zeros of a homogeneous ideal formula_43, where is a field, and let formula_44 be the ring of the regular functions on the algebraic set.\n\nIn this section, one does not need irreducibility of algebraic sets nor primality of ideals. Also, as Hilbert series are not changed by extending the field of coefficients, the field is supposed, without loss of generality, to be algebraically closed.\n\nThe dimension of is equal to the Krull dimension minus one of , and the degree of is the number of points of intersection, counted with multiplicities, of with the intersection of formula_45 hyperplanes in general position. This implies the existence, in , of a regular sequence formula_46 of homogeneous polynomials of degree one. The definition of a regular sequence implies the existence of exact sequences\nfor formula_48 This implies that \nwhere formula_50 is the numerator of the Hilbert series of .\n\nThe ring formula_51 has Krull dimension one, and is the ring of regular functions of a projective algebraic set formula_52 of dimension 0 consisting of a finite number of points, which may be multiple points. As formula_53 belongs to a regular sequence, none of these points belong to the hyperplane of equation formula_54 The complement of this hyperplane is an affine space that contains formula_55 This makes formula_52 an affine algebraic set, which has formula_57 as its ring of regular functions. The linear polynomial formula_58 is not a zero divisor in formula_59 and one has thus an exact sequence\nwhich implies that \nHere we are using Hilbert series of filtered algebras, and the fact that the Hilbert series of a graded algebra is also its Hilbert series as filtered algebra.\n\nThus formula_62 is an Artinian ring, which is a -vector space of dimension , and Jordan–Hölder theorem may be used for proving that formula_63 is the degree of the algebraic set . In fact, the multiplicity of a point is the number of occurrences of the corresponding maximal ideal in a composition series.\n\nFor proving Bézout's theorem, one may proceed similarly. If is a homogeneous polynomial of degree formula_8, which is not a zero divisor in , the exact sequence \nshows that\nLooking on the numerators this proves the following generalization of Bézout's theorem:\n\n\"If\" \"is a homogeneous polynomial of degree\" formula_8, \"which is not a zero divisor in\" , \"then the degree of the intersection of\" \"with the hypersurface defined by\" \"is the product of the degree of\" \"by\" formula_8 \".\n\nIn a more geometrical form, this may restated as \"If a projective hypersurface of degree does not contain any irreducible component of an algebraic set of degree , then the degree of their intersection is .\n\nThe usual Bézout's theorem is easily deduced by starting from a hypersurface, and intersecting it with other hypersurfaces, one after the other.\n\nA projective algebraic set is a complete intersection is its defining ideal is generated by a regular sequence. In this case, there is a simple explicit formula for the Hilbert series.\n\nLet formula_69 be homogeneous polynomials in formula_70, of respective degrees formula_71 Setting formula_72 one has the following exact sequences \n\nThe addivity of Hilbert series implies thus \nA simple recursion gives \n\nThis shows that the complete intersection defined by a regular sequence of polynomials has a codimension of , and that its degree is the product of the degrees of the polynomials in the sequence.\n\nEvery graded module over a graded regular ring has a graded free resolution, meaning there exists an exact sequence\nwhere the formula_77 are graded free modules, and the arrows are graded linear maps of degree zero.\n\nThe additivity of Hilbert series implies that\n\nIf formula_79 is a polynomial ring, and if one knows the degrees of the basis elements of the formula_80 then the formulas of the preceding sections allow deducing formula_81 from formula_82 In fact, these formulas imply that, if a graded free module has a basis of homogeneous elements of degrees formula_83 then its Hilbert series is \n\nThese formulas may be viewed as a way for computing Hilbert series. This is rarely the case, as, with the known algorithms, the computation of the Hilbert series and the computation of a free resolution start from the same Gröbner basis, from which the Hilbert series may be directly computed with a computational complexity which is not higher than that the complexity of the computation of the free resolution.\n\nThe Hilbert polynomial is easily deducible from the Hilbert series (see above). This section describes how the Hilbert series may be computed in the case of a quotient of a polynomial ring, filtered or graded by the total degree.\n\nThus let \"K\" a field, formula_85 be a polynomial ring and \"I\" be an ideal in \"R\". Let \"H\" be the homogeneous ideal generated by the homogeneous parts of highest degree of the elements of \"I\". If \"I\" is homogeneous, then \"H\"=\"I\". Finally let \"B\" be a Gröbner basis of \"I\" for a monomial ordering refining the total degree partial ordering and \"G\" the (homogeneous) ideal generated by the leading monomials of the elements of \"B\".\n\nThe computation of the Hilbert series is based on the fact that \"the filtered algebra R/I and the graded algebras R/H and R/G have the same Hilbert series\".\n\nThus the computation of the Hilbert series is reduced, through the computation of a Gröbner basis, to the same problem for an ideal generated by monomials, which is usually much easier than the computation of the Gröbner basis. The computational complexity of the whole computation depends mainly on the regularity, which is the degree of the numerator of the Hilbert series. In fact the Gröbner basis may be computed by linear algebra over the polynomials of degree bounded by the regularity.\n\nThe computation of Hilbert series and Hilbert polynomials are available in most computer algebra systems. For example in both Maple and Magma these functions are named \"HilbertSeries\" and \"HilbertPolynomial\".\n\nIn algebraic geometry, graded rings generated by elements of degree 1 produce projective schemes by Proj construction while finitely generated graded modules correspond to coherent sheaves. If formula_86 is a coherent sheaf over a projective scheme \"X\", we define the Hilbert polynomial of formula_86 as a function formula_88, where \"χ\" is the Euler characteristic of coherent sheaf, and formula_89 a Serre twist. Note that the Euler characteristic in this case is a well-defined number by Grothendieck's finiteness theorem.\n\nThis function is indeed a polynomial. For large \"m\" it agrees with dim formula_90 by Serre's vanishing theorem. If \"M\" is a finitely generated graded module and formula_91 the associated coherent sheaf two definitions of Hilbert polynomial agree.\n\nSince the category of coherent sheaves on a projective variety formula_92 is equivalent to the category of graded-modules modulo a finite number of graded-pieces, we can use the results in the previous section to construct Hilbert polynomials of coherent sheaves. For example, a complete intersection formula_92 of multi-degree formula_94 has the resolution\n\n\n"}
{"id": "1202074", "url": "https://en.wikipedia.org/wiki?curid=1202074", "title": "Homotopy sphere", "text": "Homotopy sphere\n\nIn algebraic topology, a branch of mathematics, a homotopy sphere is an \"n\"-manifold that is homotopy equivalent to the \"n\"-sphere. It thus has the same homotopy groups and the same homology groups as the \"n\"-sphere, and so every homotopy sphere is necessarily a homology sphere.\n\nThe topological generalized Poincaré conjecture is that any \"n\"-dimensional homotopy sphere is homeomorphic to the \"n\"-sphere; it was solved by Stephen Smale in dimensions five and higher, by Michael Freedman in dimension 4, and for dimension 3 (the original Poincaré conjecture) by Grigori Perelman in 2005.\n\nThe resolution of the smooth Poincaré conjecture in dimensions 5 and larger implies that homotopy spheres in those dimensions are precisely exotic spheres. It is still an open question (as of 2014) whether or not there are non-trivial smooth homotopy spheres in dimension 4.\n\n\n"}
{"id": "26569158", "url": "https://en.wikipedia.org/wiki?curid=26569158", "title": "Hurwitz determinant", "text": "Hurwitz determinant\n\nIn mathematics, Hurwitz determinants were introduced by , who used them to give a criterion for all roots of a polynomial to have negative real part.\n\nLet us consider a characteristic polynomial \"P\" in the variable \"λ\" of the form:\n\nwhere formula_2, formula_3, are real.\n\nThe square Hurwitz matrix associated to \"P\" is given below:\n\nThe \"i\"th \"Hurwitz determinant\" is the determinant of the \"i\"th leading principal minor of the above Hurwitz matrix \"H\". There are \"n\" Hurwitz determinants for a characteristic polynomial of degree \"n\".\n\n\n"}
{"id": "44533982", "url": "https://en.wikipedia.org/wiki?curid=44533982", "title": "Legendre's formula", "text": "Legendre's formula\n\nIn mathematics, Legendre's formula gives an expression for the exponent of the largest power of a prime \"p\" that divides the factorial \"n\"<nowiki>!</nowiki>. It is named after Adrien-Marie Legendre. It is also sometimes known as de Polignac's formula, after Alphonse de Polignac.\n\nFor any prime number \"p\" and any positive integer \"n\", let formula_1 be the exponent of the largest power of \"p\" that divides \"n\" (that is, the \"p\"-adic valuation of \"n\"). Then\nwhere formula_3 is the floor function. While the formula on the right side is an infinite sum, for any particular values of \"n\" and \"p\" it has only finitely many nonzero terms: for every \"i\" large enough that formula_4, one has formula_5.\n\nFor \"n\" = 6, one has formula_6. The exponents formula_7 and formula_8 can be computed by Legendre's formula as follows:\n\nSince formula_10 is the product of the integers 1 through \"n\", we obtain at least one factor of \"p\" in formula_10 for each multiple of \"p\" in formula_12, of which there are formula_13. Each multiple of formula_14 contributes an additional factor of \"p\", each multiple of formula_15 contributes yet another factor of \"p\", etc. Adding up the number of these factors gives the infinite sum for formula_16.\n\nOne may also reformulate Legendre's formula in terms of the base-\"p\" expansion of \"n\". Let formula_17 denote the sum of the digits in the base-\"p\" expansion of \"n\"; then\n\nFor example, writing \"n\" = 6 in binary as 6 = 110, we have that formula_19 and so \nSimilarly, writing 6 in ternary as 6 = 20, we have that formula_21 and so \n\nWrite formula_23 in base \"p\". Then formula_24, and therefore\n\nLegendre's formula can be used to prove Kummer's theorem. As one special case, it can be used to prove that if \"n\" is a positive integer then 4 divides formula_26 if and only if \"n\" is not a power of 2.\n\nIt follows from Legendre's formula that the \"p\"-adic exponential function has radius of convergence formula_27.\n\n"}
{"id": "28157642", "url": "https://en.wikipedia.org/wiki?curid=28157642", "title": "List of aperiodic sets of tiles", "text": "List of aperiodic sets of tiles\n\nIn geometry, a tiling is a partition of the plane (or any other geometric setting) into closed sets (called \"tiles\"), without gaps or overlaps (other than the boundaries of the tiles). A tiling is considered periodic if there exist translations in two independent directions which map the tiling onto itself. Such a tiling is composed of a single fundamental unit or primitive cell which repeats endlessly and regularly in two independent directions. An example of such a tiling is shown in the adjacent diagram (see the image description for more information). A tiling that cannot be constructed from a single primitive cell is called nonperiodic. If a given set of tiles allows only nonperiodic tilings, then this set of tiles is called aperiodic. The tilings obtained from an aperiodic set of tiles are often called aperiodic tilings, though strictly speaking it is the tiles themselves that are aperiodic. (The tiling itself is said to be \"nonperiodic\".)\n\nThe first table explains the abbreviations used in the second table. The second table contains all known aperiodic sets of tiles and gives some additional basic information about each set. This list of tiles is still incomplete.\n\n"}
{"id": "2498388", "url": "https://en.wikipedia.org/wiki?curid=2498388", "title": "List of logic symbols", "text": "List of logic symbols\n\nIn logic, a set of symbols is commonly used to express logical representation. The following table lists many common symbols together with their name, pronunciation, and the related field of mathematics. Additionally, the third column contains an informal definition, the fourth column gives a short example, the fifth and sixth give the unicode location and name for use in HTML documents. The last column provides the LaTeX symbol.\n\nThese symbols are sorted by their Unicode value:\n\n\nNote that the following operators are rarely supported by natively installed fonts. If you wish to use these in a web page, you should always embed the necessary fonts so the page viewer can see the web page without having the necessary fonts installed in their computer.\n\n in Poland, the universal quantifier is sometimes written formula_4 and the existential quantifier as formula_5.\nThe same applies for Germany.\nThe ⇒ symbol is often used in text to mean \"result\" or \"conclusion\", as in \"We examined whether to sell the product ⇒ We will not sell it\". Also, the → symbol is often used to denote \"changed to\" as in the sentence \"The interest rate changed. March 20% → April 21%\".\n\n\n\n"}
{"id": "37723882", "url": "https://en.wikipedia.org/wiki?curid=37723882", "title": "Local language (formal language)", "text": "Local language (formal language)\n\nIn mathematics, a local language is a formal language for which membership of a word in the language can be determined by looking at a \"window\" of length two. Equivalently, it is a language recognised by a local automaton, a particular kind of deterministic finite automaton.\n\nFormally, a language \"L\" over an alphabet \"A\" is defined to be \"local\" if there are subsets \"R\" and \"S\" of \"A\" and a subset \"F\" of \"A\"×\"A\" such that a word \"w\" is in \"L\" if and only if the first letter of \"w\" is in \"R\", the last letter of \"w\" is in \"S\" and no factor of length 2 in \"w\" is in \"F\". This corresponds to the regular expression\n\nMore generally, a \"k\"-\"testable\" language \"L\" is one for which membership of a word \"w\" in \"L\" depends only on the prefix, suffix and the set of factors of \"w\" of length \"k\"; a language is \"locally testable\" if it is \"k\"-testable for some \"k\". A local language is 2-testable.\n\n\n\n"}
{"id": "236105", "url": "https://en.wikipedia.org/wiki?curid=236105", "title": "Longest common subsequence problem", "text": "Longest common subsequence problem\n\nThe longest common subsequence (LCS) problem is the problem of finding the longest subsequence common to all sequences in a set of sequences (often just two sequences). It differs from the longest common substring problem: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences. The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics. It is also widely used by revision control systems such as Git for reconciling multiple changes made to a revision-controlled collection of files. \nFor the general case of an arbitrary number of input sequences, the problem is NP-hard. When the number of sequences is constant, the problem is solvable in polynomial time by dynamic programming (see \"Solution\" below). Assume you have formula_1 sequences of lengths formula_2. A naive search would test each of the formula_3 subsequences of the first sequence to determine whether they are also subsequences of the remaining sequences; each subsequence may be tested in time linear in the lengths of the remaining sequences, so the time for this algorithm would be\n\nFor the case of two sequences of \"n\" and \"m\" elements, the running time of the dynamic programming approach is O(\"n\" × \"m\"). For an arbitrary number of input sequences, the dynamic programming approach gives a solution in\n\nThere exist methods with lower complexity,\nwhich often depend on the length of the LCS, the size of the alphabet, or both.\n\nNotice that the LCS is not necessarily unique; for example the LCS of \"ABC\" and \"ACB\" is both \"AB\" and \"AC\". Indeed, the LCS problem is often defined to be finding \"all\" common subsequences of a maximum length. This problem inherently has higher complexity, as the number of such subsequences is exponential in the worst case, even for only two input strings.\n\nThe LCS problem has an optimal substructure: the problem can be broken down into smaller, simple \"subproblems\", which can be broken down into yet simpler subproblems, and so on, until, finally, the solution becomes trivial. The LCS problem also has overlapping subproblems: the solution to high-level subproblems often reuse lower level subproblems. Problems with these two properties—optimal substructure and overlapping subproblems—can be approached by a problem-solving technique called dynamic programming, in which subproblem solutions are memoized rather than computed over and over. The procedure requires memoization—saving the solutions to one level of subproblem in a table (analogous to writing them to a \"memo\", hence the name) so that the solutions are available to the next level of subproblems.\nThis method is illustrated here.\n\nThe subproblems become simpler as the sequences become shorter. Shorter sequences are conveniently described using the term \"prefix\". A prefix of a sequence is the sequence with the end cut off. Let \"S\" be the sequence (AGCA). Then, the sequence (AG) is one of the prefixes of \"S\". Prefixes are denoted with the name of the sequence, followed by a subscript to indicate how many characters the prefix contains. The prefix (AG) is denoted \"S\", since it contains the first 2 elements of \"S\". The possible prefixes of \"S\" are\n\nThe solution to the LCS problem for two arbitrary sequences, \"X\" and \"Y\", amounts to constructing some function, \"LCS\"(\"X\", \"Y\"), that gives the longest subsequences common to \"X\" and \"Y\". That function relies on the following two properties.\n\nSuppose that two sequences both end in the same element. To find their LCS, shorten each sequence by removing the last element, find the LCS of the shortened sequences, and to that LCS append the removed element.\n\nIn general, for any sequences \"X\" and \"Y\" of length \"n\" and \"m\", if we denote their elements \"x\" to \"x\" and \"y\" to \"y\" and their prefixes \"X\" to \"X\" and \"Y\" to \"Y\", then we can say this:\nwhere the caret ^ indicates that the following element, \"x\", is appended to the sequence. Note that the LCS for \"X\" and \"Y\" involves determining the LCS of the shorter sequences, \"X\" and \"Y\".\n\nSuppose that the two sequences X and Y do not end in the same symbol.\nThen the LCS of X and Y is the longer of the two sequences LCS(X,Y) and LCS(X,Y).\n\nTo understand this property, consider the two following sequences :\n\nsequence X: ABCDEFG (n elements)\nsequence Y: BCDGK (m elements)\n\nThe LCS of these two sequences either ends with a G (the last element of sequence X) or does not.\n\nCase 1: the LCS ends with a G\nThen it cannot end with a K. Thus it does not hurt to remove the K from sequence Y: if K were in the LCS, it would be its last character; as a consequence K is not in the LCS. We can then write: LCS(X,Y) = LCS(X, Y).\n\nCase 2: the LCS does not end with a G\nThen it does not hurt to remove the G from the sequence X (for the same reason as above). And then we can write: LCS(X,Y) = LCS(X, Y).\n\nIn any case, the LCS we are looking for is one of LCS(X, Y) or LCS(X, Y). Those two last LCS are both common subsequences to X and Y. LCS(X,Y) is the longest. Thus its value is the longest sequence of LCS(X, Y) and LCS(X, Y).\n\nLet two sequences be defined as follows: formula_6 and formula_7. The prefixes of formula_8 are formula_9; the prefixes of formula_10 are formula_11. Let formula_12 represent the set of longest common subsequence of prefixes formula_13 and formula_14. This set of sequences is given by the following.\n\nTo find the longest subsequences common to formula_13 and formula_14, compare the elements formula_18 and formula_19. If they are equal, then the sequence formula_20 is extended by that element, formula_18. If they are not equal, then the longer of the two sequences, formula_22, and formula_23, is retained. (If they are both the same length, but not identical, then both are retained.) Notice that the subscripts are reduced by 1 in these formulas. That can result in a subscript of 0. Since the sequence elements are defined to start at 1, it was necessary to add the requirement that the LCS is empty when a subscript is zero.\n\nThe longest subsequence common to \"R\" = (GAC), and \"C\" = (AGCAT) will be found. Because the \"LCS\" function uses a \"zeroth\" element, it is convenient to define zero prefixes that are empty for these sequences: \"R\" = Ø; and \"C\" = Ø. All the prefixes are placed in a table with \"C\" in the first row (making it a column header) and \"R\" in the first column (making it a row header).\n\nThis table is used to store the LCS sequence for each step of the calculation. The second column and second row have been filled in with Ø, because when an empty sequence is compared with a non-empty sequence, the longest common subsequence is always an empty sequence.\n\n\"LCS\"(\"R\", \"C\") is determined by comparing the first elements in each sequence. G and A are not the same, so this LCS gets (using the \"second property\") the longest of the two sequences, \"LCS\"(\"R\", \"C\") and \"LCS\"(\"R\", \"C\"). According to the table, both of these are empty, so \"LCS\"(\"R\", \"C\") is also empty, as shown in the table below. The arrows indicate that the sequence comes from both the cell above, \"LCS\"(\"R\", \"C\") and the cell on the left, \"LCS\"(\"R\", \"C\").\n\n\"LCS\"(\"R\", \"C\") is determined by comparing G and G. They match, so G is appended to the upper left sequence, \"LCS\"(\"R\", \"C\"), which is (Ø), giving (ØG), which is (G).\n\nFor \"LCS\"(\"R\", \"C\"), G and C do not match. The sequence above is empty; the one to the left contains one element, G. Selecting the longest of these, \"LCS\"(\"R\", \"C\") is (G). The arrow points to the left, since that is the longest of the two sequences.\n\n\"LCS\"(\"R\", \"C\"), likewise, is (G).\n\n\"LCS\"(\"R\", \"C\"), likewise, is (G).\n\nFor \"LCS\"(\"R\", \"C\"), A is compared with A. The two elements match, so A is appended to Ø, giving (A).\n\nFor \"LCS\"(\"R\", \"C\"), A and G do not match, so the longest of \"LCS\"(\"R\", \"C\"), which is (G), and \"LCS\"(\"R\", \"C\"), which is (A), is used. In this case, they each contain one element, so this LCS is given two subsequences: (A) and (G).\n\nFor \"LCS\"(\"R\", \"C\"), A does not match C. \"LCS\"(\"R\", \"C\") contains sequences (A) and (G); LCS(\"R\", \"C\") is (G), which is already contained in \"LCS\"(\"R\", \"C\"). The result is that \"LCS\"(\"R\", \"C\") also contains the two subsequences, (A) and (G).\n\nFor \"LCS\"(\"R\", \"C\"), A matches A, which is appended to the upper left cell, giving (GA).\n\nFor \"LCS\"(\"R\", \"C\"), A does not match T. Comparing the two sequences, (GA) and (G), the longest is (GA), so \"LCS\"(\"R\", \"C\") is (GA).\n\nFor \"LCS\"(\"R\", \"C\"), C and A do not match, so \"LCS\"(\"R\", \"C\") gets the longest of the two sequences, (A).\n\nFor \"LCS\"(\"R\", \"C\"), C and G do not match. Both \"LCS\"(\"R\", \"C\") and \"LCS\"(\"R\", \"C\") have one element. The result is that \"LCS\"(\"R\", \"C\") contains the two subsequences, (A) and (G).\n\nFor \"LCS\"(\"R\", \"C\"), C and C match, so C is appended to \"LCS\"(\"R\", \"C\"), which contains the two subsequences, (A) and (G), giving (AC) and (GC).\n\nFor \"LCS\"(\"R\", \"C\"), C and A do not match. Combining \"LCS\"(\"R\", \"C\"), which contains (AC) and (GC), and \"LCS\"(\"R\", \"C\"), which contains (GA), gives a total of three sequences: (AC), (GC), and (GA).\n\nFinally, for \"LCS\"(\"R\", \"C\"), C and T do not match. The result is that \"LCS\"(\"R\", \"C\") also contains the three sequences, (AC), (GC), and (GA).\n\nThe final result is that the last cell contains all the longest subsequences common to (AGCAT) and (GAC); these are (AC), (GC), and (GA). The table also shows the longest common subsequences for every possible pair of prefixes. For example, for (AGC) and (GA), the longest common subsequence are (A) and (G).\n\nCalculating the LCS of a row of the LCS table requires only the solutions to the current row and the previous row. Still, for long sequences, these sequences can get numerous and long, requiring a lot of storage space. Storage space can be saved by saving not the actual subsequences, but the length of the subsequence and the direction of the arrows, as in the table below.\n\nThe actual subsequences are deduced in a \"traceback\" procedure that follows the arrows backwards, starting from the last cell in the table. When the length decreases, the sequences must have had a common element. Several paths are possible when two arrows are shown in a cell. Below is the table for such an analysis, with numbers colored in cells where the length is about to decrease. The bold numbers trace out the sequence, (GA).\n\nFor two strings formula_24 and formula_25, the length of the shortest common supersequence is related to the length of the LCS by\n\nThe edit distance when only insertion and deletion is allowed (no substitution), or when the cost of the substitution is the double of the cost of an insertion or deletion, is:\n\nThe function below takes as input sequences codice_1 and codice_2, computes the LCS between codice_3 and codice_4 for all codice_5 and codice_6, and stores it in codice_7. codice_8 will contain the length of the LCS of codice_9 and codice_10.\n\nAlternatively, memoization could be used.\n\nThe following function backtracks the choices taken when computing the codice_11 table. If the last characters in the prefixes are equal, they must be in an LCS. If not, check what gave the largest LCS of keeping formula_28 and formula_29, and make the same choice. Just choose one if they were equally long. Call the function with codice_12 and codice_13.\n\nIf choosing formula_28 and formula_29 would give an equally long result, read out both resulting subsequences. This is returned as a set by this function. Notice that this function is not polynomial, as it might branch in almost every step if the strings are similar.\n\nThis function will backtrack through the C matrix, and print the diff between the two sequences. Notice that you will get a different answer if you exchange codice_14 and codice_15, with codice_16 and codice_17 below.\n\nLet formula_8 be “codice_18” and formula_10 be “codice_19”. The longest common subsequence between formula_8 and formula_10 is “codice_20”. The table codice_11 shown below, which is generated by the function codice_22, shows the lengths of the longest common subsequences between prefixes of formula_8 and formula_10. The formula_38th row and formula_39th column shows the length of the LCS between formula_40 and formula_41.\n\nThe highlighted numbers show the path the function codice_23 would follow from the bottom right to the top left corner, when reading out an LCS. If the current symbols in formula_8 and formula_10 are equal, they are part of the LCS, and we go both up and left (shown in bold). If not, we go up or left, depending on which cell has a higher number. This corresponds to either taking the LCS between formula_44 and formula_41, or formula_40 and formula_47.\n\nSeveral optimizations can be made to the algorithm above to speed it up for real-world cases.\n\nThe C matrix in the naive algorithm grows quadratically with the lengths of the sequences. For two 100-item sequences, a 10,000-item matrix would be needed, and 10,000 comparisons would need to be done. In most real-world cases, especially source code diffs and patches, the beginnings and ends of files rarely change, and almost certainly not both at the same time. If only a few items have changed in the middle of the sequence, the beginning and end can be eliminated. This reduces not only the memory requirements for the matrix, but also the number of comparisons that must be done.\n\nIn the best-case scenario, a sequence with no changes, this optimization would completely eliminate the need for the C matrix. In the worst-case scenario, a change to the very first and last items in the sequence, only two additional comparisons are performed.\n\nMost of the time taken by the naive algorithm is spent performing comparisons between items in the sequences. For textual sequences such as source code, you want to view lines as the sequence elements instead of single characters. This can mean comparisons of relatively long strings for each step in the algorithm. Two optimizations can be made that can help to reduce the time these comparisons consume.\n\nA hash function or checksum can be used to reduce the size of the strings in the sequences. That is, for source code where the average line is 60 or more characters long, the hash or checksum for that line might be only 8 to 40 characters long. Additionally, the randomized nature of hashes and checksums would guarantee that comparisons would short-circuit faster, as lines of source code will rarely be changed at the beginning.\n\nThere are three primary drawbacks to this optimization. First, an amount of time needs to be spent beforehand to precompute the hashes for the two sequences. Second, additional memory needs to be allocated for the new hashed sequences. However, in comparison to the naive algorithm used here, both of these drawbacks are relatively minimal.\n\nThe third drawback is that of collisions. Since the checksum or hash is not guaranteed to be unique, there is a small chance that two different items could be reduced to the same hash. This is unlikely in source code, but it is possible. A cryptographic hash would therefore be far better suited for this optimization, as its entropy is going to be significantly greater than that of a simple checksum. However, the benefits may not be worth the setup and computational requirements of a cryptographic hash for small sequence lengths.\n\nIf only the length of the LCS is required, the matrix can be reduced to a formula_48 matrix with ease, or to a formula_49 vector (smarter) as the dynamic programming approach only needs the current and previous columns of the matrix. Hirschberg's algorithm allows the construction of the optimal sequence itself in the same quadratic time and linear space bounds.\n\nSeveral algorithms exist that are worst-case faster than the presented dynamic programming approach. For problems with a bounded alphabet size, the Method of Four Russians can be used to reduce the running time of the dynamic programming algorithm by a logarithmic factor. There is an algorithm that performs in formula_50 time (for formula_51), where formula_52 is the number of matches between the two sequences.\n\nBeginning with , a number of researchers have investigated the behavior of the longest common subsequence length when the two given strings are drawn randomly from the same alphabet. When the alphabet size is constant, the expected length of the LCS is proportional to the length of the two strings, and the constants of proportionality (depending on alphabet size) are known as the Chvátal–Sankoff constants. Their exact values are not known, but upper and lower bounds on their values have been proven, and it is known that they grow inversely proportionally to the square root of the alphabet size. Simplified mathematical models of the longest common subsequence problem have been shown to be controlled by the Tracy–Widom distribution.\n\n\n \n"}
{"id": "56880139", "url": "https://en.wikipedia.org/wiki?curid=56880139", "title": "Markov odometer", "text": "Markov odometer\n\nIn mathematics, a Markov odometer is a certain type of topological dynamical system. It plays a fundamental role in ergodic theory and especially in orbit theory of dynamical systems, since a theorem of H. Dye asserts that every ergodic nonsingular transformation is orbit-equivalent to a Markov odometer.\n\nThe basic example of such system is the \"nonsingular odometer\", based on the additive topological group of p-adic integers endowed with structure of a dynamical system for the transformation formula_1, where formula_2. The general form, which is called \"Markov odometer\", can be constructed through Bratteli–Vershik diagram to define \"Bratteli–Vershik compactum\" space together with a corresponding transformation.\n\nWe introduce first the basic example of dyadic integers. Let formula_3 be the dyadic integers, andowed with the addition which is defined for each coordinate by\nformula_4\nwhere formula_5 and\n\ninductively. Let formula_7 be the Borel sigma-algebra generated by the Cylinder set, with the measure formula_8 and formula_9, for some formula_10. Let formula_11 be the transformation formula_12, where formula_2. In other words, formula_14 is the transformation formula_15.\n\nOne can show that the formula_16 where formula_17. Hence formula_14 is formula_19-nonsingular. Moreover, one can show that formula_14 has the following property: For every formula_21 and natural number formula_22, the orbit of formula_23 under formula_14 along the steps formula_25 is exactly the set formula_26 (as presentation of dyadic integers). As a result, formula_14 is formula_19-ergodic. Finally, formula_14 is conservative since every invertible ergodic nonsingular transformation in nonatomic space is conservative.\n\nThe same construction enables to define such a system for every p-adic integers, and more generally for every product space of the form formula_30 for formula_31 where formula_32, endowed with the product Borel sigma-algebra and some nonatomic measure formula_33 where formula_34 is some measure on formula_35. The corresponding map is defined by formula_36 where formula_37 is the smallest index for which formula_38.\n\nLet formula_39 be an ordered Bratteli–Vershik diagram, consists on a set of vertices of the form formula_40 (disjoint union) where formula_41 is a singelton and on a set of edges formula_42 (disjoint union).\n\nThe diagram includes source surjection-mappings formula_43 and range surjection-mappings formula_44. We assume that formula_45 are comparable if and only if formula_46.\n\nFor such diagram we look at the product space formula_47 equipped with the product topology. Define \"Bratteli–Vershik compactum\" to be the subspace of infinite paths,\n\nAssume there exists only one infinite path formula_49 for which each formula_50 is maximal and similarly one infinite path formula_51. Define the \"Bratteli-Vershik map\" formula_52 by formula_53 and, for any formula_54 define formula_55, where formula_37 is the first index for which formula_57 is not maximal and accordingly let formula_58 be the unique path for which formula_59 are all maximal and formula_60 is the successor of formula_57. Then formula_62 is homeomorphism of formula_63.\n\nLet formula_64 be a sequence of stochastic matrices formula_65 such that formula_66 if and only if formula_67. Define \"Markov measure\" on the cylinders of formula_63 by formula_69. Then the system formula_70 is called a \"Markov odometer\".\n\nOne can show that the nonsingular odometer is a Markov odometer where all the formula_71 are singeltons.\n\n"}
{"id": "53725425", "url": "https://en.wikipedia.org/wiki?curid=53725425", "title": "Mei-Chu Chang", "text": "Mei-Chu Chang\n\nMei-Chu Chang is a mathematician who works in algebraic geometry and combinatorial number theory.\n\nChang did her undergraduate studies in Taiwan and received a BS from National Taiwan University. She did her doctoral work at University of California, Berkeley, under the supervision of Robin Hartshorne and was awarded her PhD in 1982. Her dissertation was on \"Some Results on Stable Rank 2 Vector Bundles and Reflexive Sheaves on P.\"\n\nAfter finishing her doctoral studies, Dr. Chang was appointed a Bateman Research Instructor at the California Institute of Technology. She held assistant professor positions at University of Michigan and University of South Carolina before accepting a position as an associate professor at the University of California, Riverside in 1987. She was promoted to professor at Riverside in 1991. Prof. Chang has held visiting positions in Sweden, Korea, and Italy, at the IHES in Paris, and the IAS in Princeton, as well at several institutions in the US.\n\nIn her most cited work, \"A polynomial bound in Freiman's theorem\", Professor Chang established new quantitative bounds for Freiman's inverse theorem.\n\nMei-Chu Chang was elected a Fellow of the American Mathematical Society in 2017. The citation reads \"For contributions to arithmetic combinatorics, analytic number theory, and algebraic geometry.\" In 2009 she was chosen to give a plenary address at the 9th International Conference on Finite Fields and Applications, which was held in Dublin, Ireland.\n\n"}
{"id": "28755276", "url": "https://en.wikipedia.org/wiki?curid=28755276", "title": "Nati Linial", "text": "Nati Linial\n\nNathan (Nati) Linial (born 1953 in Haifa, Israel) is an Israeli mathematician and computer scientist, a professor in the Rachel and Selim Benin School of Computer Science and Engineering at the Hebrew University of Jerusalem, and an ISI highly cited researcher.\n\nLinial did his undergraduate studies at the Technion, and received his PhD in 1978 from the Hebrew University under the supervision of Micha Perles. He was a postgraduate researcher at the University of California, Los Angeles before returning to the Hebrew University as a faculty member.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "265816", "url": "https://en.wikipedia.org/wiki?curid=265816", "title": "Open system (systems theory)", "text": "Open system (systems theory)\n\nAn open system is a system that has external interactions. Such interactions can take the form of information, energy, or material transfers into or out of the system boundary, depending on the discipline which defines the concept. An open system is contrasted with the concept of an isolated system which exchanges neither energy, matter, nor information with its environment. An open system is also known as a constant volume system or a flow system.\n\nThe concept of an open system was formalized within a framework that enabled one to interrelate the theory of the organism, thermodynamics, and evolutionary theory. This concept was expanded upon with the advent of information theory and subsequently systems theory. Today the concept has its applications in the natural and social sciences.\n\nIn the natural sciences an open system is one whose border is permeable to both energy and mass. In thermodynamics a closed system, by contrast, is permeable to energy but not to matter. \n\nThe definition of an open system assumes that there are supplies of energy that cannot be depleted; in practice, this energy is supplied from some source in the surrounding environment, which can be treated as infinite for the purposes of study. One type of open system is the radiant energy system, which receives its energy from solar radiation – an energy source that can be regarded as inexhaustible for all practical purposes. A closed system contains limited energies.\n\nIn the social sciences an open system is a process that exchanges material, energy, people, capital and information with its environment. French/Greek philosopher Kostas Axelos argued that seeing the \"world system\" as inherently open (though unified) would solve many of the problems in the social sciences, including that of praxis (the relation of knowledge to practice), so that various social scientific disciplines would work together rather than create a monopoly whereby the world appears only sociological, political, historical, or psychological. Axelos argues that theorizing a closed system contributes to \"making\" it closed, and is thus a conservative approach. The Althusserian concept of overdetermination (drawing on Sigmund Freud) posits that there are always multiple causes in every event.\nDavid Harvey uses this to argue that when systems such as capitalism enter a phase of crisis, it could happen through one of a number of elements, such as gender roles, the relation to nature/the environment, or crises in accumulation. Looking at the crisis in accumulation, Harvey argues that phenomena such as foreign direct investment, privatization of state-owned resources, and accumulation by dispossession act as necessary outlets when capital has overaccumulated too much in private hands and cannot circulate effectively in the marketplace. He cites the forcible displacement of Mexican and Indian peasants since the 1970s and the Asian and South-East Asian financial crisis of 1997-8, involving \"hedge fund raising\" of national currencies, as examples of this.\n\nStructural functionalists such as Talcott Parsons and neofunctionalists such as Niklas Luhmann have incorporated system theory to describe society and its components.\n\n\n\n"}
{"id": "7085075", "url": "https://en.wikipedia.org/wiki?curid=7085075", "title": "Oswald Veblen Prize in Geometry", "text": "Oswald Veblen Prize in Geometry\n\nThe Oswald Veblen Prize in Geometry is an award granted by the American Mathematical Society for notable research in geometry or topology. It was founded in 1961 in memory of Oswald Veblen. The Veblen Prize is now worth US$5000, and is awarded every three years.\n\nThe first seven prize winners were awarded for works in topology. James Harris Simons and William Thurston were the first ones to receive it for works in geometry (for some distinctions, see geometry and topology).\n\n\n"}
{"id": "45468", "url": "https://en.wikipedia.org/wiki?curid=45468", "title": "Pareto efficiency", "text": "Pareto efficiency\n\nPareto efficiency or Pareto optimality is a state of allocation of resources from which it is impossible to reallocate so as to make any one individual or preference criterion better off without making at least one individual or preference criterion worse off. The concept is named after Vilfredo Pareto (1848–1923), Italian engineer and economist, who used the concept in his studies of economic efficiency and income distribution. The concept has been applied in academic fields such as economics, engineering, and the life sciences.\n\nThe Pareto frontier is the set of all Pareto efficient allocations, conventionally shown graphically. It also is variously known as the Pareto front or Pareto set.\n\nA Pareto improvement is a change to a different allocation that makes at least one individual or preference criterion better off without making any other individual or preference criterion worse off, given a certain initial allocation of goods among a set of individuals. An allocation is defined as \"Pareto efficient\" or \"Pareto optimal\" when no further Pareto improvements can be made, in which case we are assumed to have reached Pareto optimality.\n\n\"Pareto efficiency\" is considered as a minimal notion of efficiency that does not necessarily result in a socially desirable distribution of resources: it makes no statement about equality, or the overall well-being of a society. It is simply a statement of impossibility of improving one variable without harming other variables in the subject of multi-objective optimization (also termed Pareto optimization). \n\nThe notion of Pareto efficiency has been applied to the selection of alternatives in engineering and similar fields. Each option is first assessed, under multiple criteria, and then a subset of options is ostensibly identified with the property that no other option can categorically outperform any of its members.\n\n\"Pareto optimality\" is a formally defined concept used to determine when an allocation is optimal. An allocation is \"not\" Pareto optimal if there is an alternative allocation where improvements can be made to at least one participant's well-being without reducing any other participant's well-being. If there is a transfer that satisfies this condition, the reallocation is called a \"Pareto improvement.\" When no further Pareto improvements are possible, the allocation is a \"Pareto optimum.\"\n\nThe formal presentation of the concept in an economy is as follows: Consider an economy with formula_1 agents and formula_2 goods. Then an allocation formula_3, where formula_4, is \"Pareto optimal\" if there is no other feasible allocation formula_5 such that, for utility function formula_6 for each agent formula_1, formula_8 for all formula_9 with formula_10 for some formula_11. Here, in this simple economy, \"feasibility\" refers to an allocation where the total amount of each good that is allocated sums to no more than the total amount of the good in the economy. In a more complex economy with production, an allocation would consist both of consumption vectors and production vectors, and feasibility would require that the total amount of each consumed good is no greater than the initial endowment plus the amount produced.\n\nIn principle, a change from a generally inefficient economic allocation to an efficient one is not necessarily considered to be a Pareto improvement. Even when there are overall gains in the economy, if a single agent is disadvantaged by the reallocation, the allocation is not Pareto optimal. For instance, if a change in economic policy eliminates a monopoly and that market subsequently becomes competitive, the gain to others may be large. However, since the monopolist is disadvantaged, this is not a Pareto improvement. In theory, if the gains to the economy are larger than the loss to the monopolist, the monopolist could be compensated for its loss while still leaving a net gain for others in the economy, allowing for a Pareto improvement. Thus, in practice, to ensure that nobody is disadvantaged by a change aimed at achieving Pareto efficiency, compensation of one or more parties may be required. It is acknowledged, in the real world, that such compensations may have unintended consequences leading to incentive distortions over time, as agents supposedly anticipate such compensations and change their actions accordingly.\n\nUnder the idealized conditions of the first welfare theorem, a system of free markets, also called a \"competitive equilibrium,\" leads to a Pareto-efficient outcome. It was first demonstrated mathematically by economists Kenneth Arrow and Gérard Debreu. \n\nHowever, the result only holds under the restrictive assumptions necessary for the proof: markets exist for all possible goods, so there are no externalities; all markets are in full equilibrium; markets are perfectly competitive; transaction costs are negligible; and market participants have perfect information. \n\nIn the absence of perfect information or complete markets, outcomes will generally be Pareto inefficient, per the Greenwald-Stiglitz theorem. \n\nThe second welfare theorem is essentially the reverse of the first welfare-theorem. It states that under similar, ideal assumptions, any Pareto optimum can be obtained by some competitive equilibrium, or free market system, although it may also require a lump-sum transfer of wealth.\n\nA \"weak Pareto optimum\" (WPO) is an allocation for which there are no possible alternative allocations whose realization would cause every individual to gain. Thus, an alternative allocation is considered to be a Pareto improvement \"if and only if\" the alternative allocation is strictly preferred by \"all\" individuals. When contrasted with weak Pareto efficiency, a standard Pareto optimum as described above may be referred to as a \"strong Pareto optimum\" (SPO).\n\nWeak Pareto-optimality is \"weaker\" than strong Pareto-optimality in the sense that any SPO also qualifies as a WPO, but a WPO allocation is not necessarily an SPO.\n\nA market doesn't require local nonsatiation to get to a weak Pareto-optimum.\n\nThe condition of constrained Pareto optimality is a weaker version of the standard condition of Pareto optimality employed in economics, which ostensibly accounts for the fact that a potential planner (e.g., the government) may not be able to improve upon a decentralized market outcome, even if that outcome is inefficient. This will occur if it is limited by the same informational or institutional constraints as are individual agents.\n\nThe most commonly proffered example is of a setting where individuals have private information (for example, a labor market where the worker's own productivity is known to the worker but not to a potential employer, or a used-car market where the quality of a car is known to the seller but not to the buyer) which results in moral hazard or an adverse selection and a sub-optimal outcome. In such a case, a planner who wishes to improve the situation is deemed unlikely to have access to any information that the participants in the markets do not have. Hence, the planner cannot implement allocation rules which are based on the idiosyncratic characteristics of individuals; for example, \"if a person is of type A, they pay price p1, but if of type B, they pay price p2\" (see Lindahl prices). Essentially, only anonymous rules are allowed (of the sort \"Everyone pays price p\") or rules based on observable behavior; \"if any person chooses x at price px, then they get a subsidy of ten dollars, and nothing otherwise\". If there exists no allowed rule that can successfully improve upon the market outcome, then that outcome is said to be \"constrained Pareto-optimal.\"\n\nNote that the concept of constrained Pareto optimality assumes benevolence on the part of the planner and hence it is distinct from the concept of government failure, which occurs when the policy making politicians fail to achieve an optimal outcome simply because they are not necessarily acting in the public's best interest.\n\n The notion of Pareto efficiency has been used in engineering. Given a set of choices and a way of valuing them, the Pareto frontier or Pareto set or Pareto front is the set of choices that are Pareto efficient. By restricting attention to the set of choices that are Pareto-efficient, a designer can make tradeoffs within this set, rather than considering the full range of every parameter.\n\nFor a given system, the Pareto frontier or Pareto set is the set of parameterizations (allocations) that are all Pareto efficient. Finding Pareto frontiers is particularly useful in engineering. By yielding all of the potentially optimal solutions, a designer can make focused tradeoffs within this constrained set of parameters, rather than needing to consider the full ranges of parameters.\n\nThe Pareto frontier, \"P\"(\"Y\"), may be more formally described as follows. Consider a system with function formula_12, where \"X\" is a compact set of feasible decisions in the metric space formula_13, and \"Y\" is the feasible set of criterion vectors in formula_14, such that formula_15.\n\nWe assume that the preferred directions of criteria values are known. A point formula_16 is preferred to (strictly dominates) another point formula_17, written as formula_18. The Pareto frontier is thus written as:\n\nA significant aspect about the Pareto frontier in economics is that, at a Pareto-efficient allocation, the marginal rate of substitution is the same for all consumers. A formal statement can be derived by considering a system with \"m\" consumers and \"n\" goods, and a utility function of each consumer as formula_20 where formula_21 is the vector of goods, both for all \"i\". The feasibility constraint is formula_22 for formula_23. To find the Pareto optimal allocation, we maximize the Lagrangian:\n\nwhere formula_25 and formula_26 are the vectors of multipliers. Taking the partial derivative of the Lagrangian with respect to each good formula_27 for formula_23 and formula_29 and gives the following system of first-order conditions:\n\nwhere formula_32 denotes the partial derivative of formula_33 with respect to formula_34. Now, fix any formula_35 and formula_36. The above first-order condition imply that\n\nThus, in a Pareto-optimal allocation, the marginal rate of substitution must be the same for all consumers.\n\nAlgorithms for computing the Pareto frontier of a finite set of alternatives have been studied in computer science and power engineering. They include:\n\n\nPareto optimisation has also been studied in biological processes. In bacteria, genes were shown to be either inexpensive to make (resource efficient) or easier to read (translation efficient). Natural selection acts to push highly expressed genes towards the Pareto frontier for resource use and translational efficiency. Genes near the Pareto frontier were also shown to evolve more slowly (indicating that they are providing a selective advantage).\n\nIt would be incorrect to treat Pareto efficiency as equivalent to societal optimization, as the latter is a normative concept that is a matter of interpretation that typically would account for the consequence of degrees of inequality of distribution. An example would be a school district with low property tax revenue versus one with much higher revenue. Generally, more equal distribution occurs with the help of government redistribution.\n\nPareto efficiency does not require a totally equitable distribution of wealth. An economy in which a wealthy few hold the vast majority of resources can be Pareto efficient. This possibility is inherent in the definition of Pareto efficiency; often the status quo is Pareto efficient regardless of the degree to which wealth is equitably distributed. A simple example is the distribution of a pie among three people. The most equitable distribution would assign one third to each person. However the assignment of, say, a half section to each of two individuals and none to the third is also Pareto optimal despite not being equitable, because none of the recipients could be made better off without decreasing someone else's share; and there are many other such distribution examples. An example of a Pareto inefficient distribution of the pie would be allocation of a quarter of the pie to each of the three, with the remainder discarded. The origin (and utility value) of the pie is conceived as immaterial in these examples. In such cases, whereby a \"windfall\" is gained that none of the potential distributees actually produced (e.g., land, inherited wealth, a portion of the broadcast spectrum, or some other resource), the criterion of Pareto efficiency does not determine a unique optimal allocation. Wealth consolidation may exclude others from wealth accumulation because of bars to market entry, etc.\n\nThe liberal paradox elaborated by Amartya Sen shows that when people have preferences about what other people do, the goal of Pareto efficiency can come into conflict with the goal of individual liberty.\n\n"}
{"id": "1362407", "url": "https://en.wikipedia.org/wiki?curid=1362407", "title": "Path of least resistance", "text": "Path of least resistance\n\nThe path of least resistance is the physical or metaphorical pathway that provides the least resistance to forward motion by a given object or entity, among a set of alternative paths. The concept is often used to describe why an object or entity takes a given path. The way in which water flows is often given as an example for the idea.\n\nIn physics, the \"path of least resistance\" is a heuristic from folk physics that can sometimes, in very simple situations, describe approximately what happens. It is an approximation of the tendency to the least energy state. Other examples are \"what goes up must come down\" (gravity) and \"heat goes from hot to cold\" (second law of thermodynamics). But these simple descriptions are not derived from laws of physics and in more complicated cases these heuristics will fail to give even approximately correct results. In electrical circuits, for example, current always follows all available paths, and in some simple cases the \"path of least resistance\" will take up most of the current, but this will not be generally true in even slightly more complicated circuits. It may seem for example, that if there are three paths of approximately equal resistance, the majority of the current will flow down one of the three paths. However, due to electrons repelling each other the total path of least resistance is in fact to have approximate equal current flowing through each path. The reason for this is that three paths made of equally conductive wire will have a total resistance that is one third of the single path.\n\nThe path of least resistance is also used to describe certain human behaviors, although with much less specificity than in the strict physical sense. In these cases, resistance is often used as a metaphor for personal effort or confrontation; a person taking the path of least resistance avoids these. In library science and technical writing, information is ideally arranged for users according to the principle of least effort, or the \"path of least resistance\". Recursive navigation systems are an example of this.\n\nThe path of least resistance applies on a local, not global, reference. For example, water always flows downhill, regardless of whether briefly flowing uphill will help it gain a lower final altitude (with certain exceptions such as superfluids and syphons). In physics, this phenomenon allows the formation of potential wells, where potential energy is stored because of a barrier restricting flow to a lower energy state.\n\n"}
{"id": "614984", "url": "https://en.wikipedia.org/wiki?curid=614984", "title": "Richard Rado", "text": "Richard Rado\n\nRichard Rado FRS (28 April 1906 – 23 December 1989) was a German-born British mathematician whose research concerned combinatorics and graph theory. He was Jewish and left Germany to escape Nazi persecution. He earned two Ph.D.s: in 1933 from the University of Berlin, and in 1935 from the University of Cambridge. He was interviewed in Berlin by Lord Cherwell for a scholarship given by the chemist Sir Robert Mond which provided financial support to study at Cambridge. After he was awarded the scholarship, Rado and his wife left for the UK in 1933. He was appointed Professor of Mathematics at the University of Reading in 1954 and remained there until he retired in 1971.\n\nRado made contributions in combinatorics and graph theory including 18 papers with Paul Erdős.\n\nIn graph theory, the Rado graph, a countably infinite graph containing all countably infinite graphs as induced subgraphs, is named after Rado. He rediscovered it in 1964 after previous works on the same graph by Wilhelm Ackermann, Paul Erdős, and Alfréd Rényi.\n\nIn combinatorial set theory, the Erdős–Rado theorem extends Ramsey's theorem to infinite sets. It was published by Erdős and Rado in 1956. Rado's theorem is another Ramsey-theoretic result concerning systems of linear equations, proved by Rado in his thesis. The Milner–Rado paradox, also in set theory, states the existence of a partition of an ordinal into subsets of small order-type; it was published by Rado and E. C. Milner in 1965.\n\nThe Erdős–Ko–Rado theorem can be described either in terms of set systems or hypergraphs. It gives an upper bound on the number of sets in a family of finite sets, all the same size, that all intersect each other. Rado published it with Erdős and Chao Ko in 1961, but according to Erdős it was originally formulated in 1938.\n\nThe Klarner–Rado Sequence is named after Rado and David A. Klarner.\n\nIn 1972, Rado was awarded the Senior Berwick Prize.\n\n"}
{"id": "26152632", "url": "https://en.wikipedia.org/wiki?curid=26152632", "title": "Rubinstein bargaining model", "text": "Rubinstein bargaining model\n\nA Rubinstein bargaining model refers to a class of bargaining games that feature alternating offers through an infinite time horizon. The original proof is due to Ariel Rubinstein in a 1982 paper. For a long time, the solution to this type of game was a mystery; thus, Rubinstein's solution is one of the most influential findings in game theory.\n\nA standard Rubinstein bargaining model has the following elements:\n\n\nConsider the typical Rubinstein bargaining game in which two players decide how to divide a pie of size 1. An offer by a player takes the form \"x\" = (\"x\", \"x\") with \"x\" + \"x\" = 1. Assume the players discount at the geometric rate of \"d\", which can be interpreted as cost of delay or \"pie spoiling\". That is, 1 step later, the pie is worth d times what it was, for some d with 0<d<1.\n\nAny \"x\" can be a Nash equilibrium outcome of this game, resulting from the following strategy profile: Player 1 always proposes \"x\" = (\"x\", \"x\") and only accepts offers \"x\" where \"x\"' ≥ \"x\". Player 2 always proposes \"x\" = (\"x\", \"x\") and only accepts offers \"x\" where \"x\"' ≥ \"x\".\n\nIn the above Nash equilibrium, player 2's threat to reject any offer less than \"x\" is not credible. In the subgame where player 1 did offer \"x\"' where \"x\" > \"x\"' > \"d\" \"x\", clearly player 2's best response is to accept.\n\nTo derive a sufficient condition for subgame perfect equilibrium, let \"x\" = (\"x\", \"x\") and \"y\" = (\"y\", \"y\") be two divisions of the pie with the following property:\n\n\nConsider the strategy profile where player 1 offers \"x\" and accepts no less than \"y\", and player \"2\" offers \"y\" and accepts no less than \"x\". Player 2 is now indifferent between accepting and rejecting, therefore the threat to reject lesser offers is now credible. Same applies to a subgame in which it is player 1's turn to decide whether to accept or reject. In this subgame perfect equilibrium, player 1 gets 1/(1+\"d\") while player 2 gets \"d\"/(1+\"d\"). This subgame perfect equilibrium is essentially unique.\n\nWhen the discount factor is different for the two players, formula_1 for the first one and formula_2 for the second, let us denote the value for the first player as formula_3.\nThen a reasoning similar to the above gives\n\nformula_4 <br>\nformula_5\n\nyielding formula_6. This expression reduces to the original one for formula_7.\n\nRubinstein bargaining has become pervasive in the literature because it has many desirable qualities:\n\n"}
{"id": "24918060", "url": "https://en.wikipedia.org/wiki?curid=24918060", "title": "Semilinear map", "text": "Semilinear map\n\nIn linear algebra, particularly projective geometry, a semilinear map between vector spaces \"V\" and \"W\" over a field \"K\" is a function that is a linear map \"up to a twist\", hence \"semi\"-linear, where \"twist\" means \"field automorphism of \"K\"\". Explicitly, it is a function that is:\n\nWhere the domain and codomain are the same space (i.e. ), it may be termed a semilinear transformation. The invertible semilinear transforms of a given vector space \"V\" (for all choices of field automorphism) form a group, called the general semilinear group and denoted formula_5 by analogy with and extending the general linear group.\n\nSimilar notation (replacing Latin characters with Greek) are used for semilinear analogs of more restricted linear transform; formally, the semidirect product of a linear group with the Galois group of field automorphism. For example, PΣU is used for the semilinear analogs of the projective special unitary group PSU. Note however, that it is only recently noticed that these generalized semilinear groups are not well-defined, as pointed out in – isomorphic classical groups \"G\" and \"H\" (subgroups of SL) may have non-isomorphic semilinear extensions. At the level of semidirect products, this corresponds to different actions of the Galois group on a given abstract group, a semidirect product depending on two groups and an action. If the extension is non-unique, there are exactly two semilinear extensions; for example, symplectic groups have a unique semilinear extension, while has two extensions if \"n\" is even and \"q\" is odd, and likewise for PSU.\n\nA map for vector spaces and over fields and respectively is -semilinear, or simply semilinear, if there exists a field homomorphism such that for all , in and in it holds that\n\nA given embedding of a field in allows us to identify with a subfield of , making a -semilinear map a \"K\"-linear map under this identification. However, a map that is -semilinear for a distinct embedding will not be \"K\"-linear with respect to the original identification , unless is identically zero.\n\nMore generally, a map between a right -module and a left -module is -semilinear if there exists a ring antihomomorphism such that for all , in and in it holds that\nThe term \"semilinear\" applies for any combination of left and right modules with suitable adjustment of the above expressions, with being a homomorphism as needed.\n\nThe pair is referred to as a dimorphism.\n\nLet be a ring isomorphism, a right -module and a right -module, and a -semilinear map. We define the transpose of as the mapping that satisfies\nThis is a -semilinear map.\n\nLet be a ring isomorphism, a right -module and a right -module, and a -semilinear map. The mapping\ndefines an -linear form.\n\n\nGiven a vector space \"V\", the set of all invertible semilinear transformations (over all field automorphisms) is the group ΓL(\"V\").\n\nGiven a vector space \"V\" over \"K\", ΓL(\"V\") decomposes as the semidirect product\nwhere Aut(\"K\") is the automorphisms of \"K\". Similarly, semilinear transforms of other linear groups can be \"defined\" as the semidirect product with the automorphism group, or more intrinsically as the group of semilinear maps of a vector space preserving some properties.\n\nWe identify Aut(\"K\") with a subgroup of ΓL(\"V\") by fixing a basis \"B\" for \"V\" and defining the semilinear maps:\nfor any formula_35. We shall denoted this subgroup by Aut(\"K\"). We also see these complements to GL(\"V\") in ΓL(\"V\") are acted on regularly by GL(\"V\") as they correspond to a change of basis.\n\nEvery linear map is semilinear, thus formula_36. Fix a basis \"B\" of \"V\". Now given any semilinear map \"f\" with respect to a field automorphism , then define by\nAs \"f\"(\"B\") is also a basis of \"V\", it follows that \"g\" is simply a basis exchange of \"V\" and so linear and invertible: .\n\nSet formula_38. For every formula_39 in \"V\",\nthus \"h\" is in the Aut(\"K\") subgroup relative to the fixed basis \"B.\" This factorization is unique to the fixed basis \"B\". Furthermore, GL(\"V\") is normalized by the action of Aut(\"K\"), so .\n\nThe formula_41 groups extend the typical classical groups in GL(\"V\"). The importance in considering such maps follows from the consideration of projective geometry. The induced action of formula_41 on the associated vector space P(\"V\") yields the , denoted formula_43, extending the projective linear group, PGL(\"V\").\n\nThe projective geometry of a vector space \"V\", denoted PG(\"V\"), is the lattice of all subspaces of \"V\". Although the typical semilinear map is not a linear map, it does follow that every semilinear map formula_19 induces an order-preserving map formula_45. That is, every semilinear map induces a projectivity. The converse of this observation (except for the projective line) is the fundamental theorem of projective geometry. Thus semilinear maps are useful because they define the automorphism group of the projective geometry of a vector space.\n\nThe group PΓL(3,4) can be used to construct the Mathieu group M, which is one of the sporadic simple groups; PΓL(3,4) is a maximal subgroup of M, and there are many ways to extend it to the full Mathieu group.\n\n"}
{"id": "39906398", "url": "https://en.wikipedia.org/wiki?curid=39906398", "title": "Slice theorem (differential geometry)", "text": "Slice theorem (differential geometry)\n\nIn differential geometry, the slice theorem states: given a manifold \"M\" on which a Lie group \"G\" acts as diffeomorphisms, for any \"x\" in \"M\", the map formula_1 extends to an invariant neighborhood of formula_2 (viewed as a zero section) in formula_3 so that it defines an equivariant diffeomorphism from the neighborhood to its image, which contains the orbit of \"x\".\n\nThe important application of the theorem is a proof of the fact that the quotient formula_4 admits a manifold structure when \"G\" is compact and the action is free.\n\nIn algebraic geometry, there is an analog of the slice theorem; it is called Luna's slice theorem.\n\nSince \"G\" is compact, there exists an invariant metric; i.e., \"G\" acts as isometries. One then adopts the usual proof of the existence of a tubular neighborhood using this metric.\n\n\n"}
{"id": "3108602", "url": "https://en.wikipedia.org/wiki?curid=3108602", "title": "Spanier–Whitehead duality", "text": "Spanier–Whitehead duality\n\nIn mathematics, Spanier–Whitehead duality is a duality theory in homotopy theory, based on a geometrical idea that a topological space \"X\" may be considered as dual to its complement in the \"n\"-sphere, where \"n\" is large enough. Its origins lie in the Alexander duality theory, in homology theory, concerning complements in manifolds. The theory is also referred to as S-duality, but this can now cause possible confusion with the S-duality of string theory. It is named for Edwin Spanier and J. H. C. Whitehead, who developed it in papers from 1955.\n\nThe basic point is that sphere complements determine the homology, but not the homotopy type, in general. What is determined, however, is the stable homotopy type, which was conceived as a first approximation to homotopy type. Thus Spanier–Whitehead duality fits into stable homotopy theory.\n\nLet \"X\" be a compact neighborhood retract in formula_1. Then formula_2 and formula_3 are dual objects in the category of pointed spectra with the smash product as a monoidal structure. Here formula_2 is the union of \"X\" and a point, formula_5 and formula_6 are reduced and unreduced suspensions respectively.\n\nTaking homology and cohomology with respect to an Eilenberg—MacLane spectrum recovers Alexander duality formally.\n"}
{"id": "28356", "url": "https://en.wikipedia.org/wiki?curid=28356", "title": "Symplectic manifold", "text": "Symplectic manifold\n\nIn mathematics, a symplectic manifold is a smooth manifold, \"M\", equipped with a closed nondegenerate differential 2-form, \"ω\", called the symplectic form. The study of symplectic manifolds is called symplectic geometry or symplectic topology. Symplectic manifolds arise naturally in abstract formulations of classical mechanics and analytical mechanics as the cotangent bundles of manifolds. For example, in the Hamiltonian formulation of classical mechanics, which provides one of the major motivations for the field, the set of all possible configurations of a system is modeled as a manifold, and this manifold's cotangent bundle describes the phase space of the system.\n\nAny real-valued differentiable function, \"H\", on a symplectic manifold can serve as an energy function or Hamiltonian. Associated to any Hamiltonian is a Hamiltonian vector field; the integral curves of the Hamiltonian vector field are solutions to Hamilton's equations. The Hamiltonian vector field defines a flow on the symplectic manifold, called a Hamiltonian flow or symplectomorphism. By Liouville's theorem, Hamiltonian flows preserve the volume form on the phase space.\n\nSymplectic manifolds arise from classical mechanics, in particular, they are a generalization of the phase space of a closed system. In the same way the Hamilton equations allow one to derive the time evolution of a system from a set of differential equations, the symplectic form should allow one to obtain a vector field describing the flow of the system from the differential \"dH\" of a Hamiltonian function \"H\". So we require a linear map , or equivalently, an element of . Letting \"ω\" denote a section of , the requirement that \"ω\" be non-degenerate ensures that for every differential \"dH\" there is a unique corresponding vector field \"V\" such that . Since one desires the Hamiltonian to be constant along flow lines, one should have , which implies that \"ω\" is alternating and hence a 2-form. Finally, one makes the requirement that \"ω\" should not change under flow lines, i.e. that the Lie derivative of \"ω\" along \"V\" vanishes. Applying Cartan's formula, this amounts to (here formula_1 is the interior product): \n\nso that, on repeating this argument for different smooth functions formula_3 such that the corresponding formula_4 span the tangent space at each point the argument is applied at, we see that the requirement for the vanishing Lie derivative along flows of formula_4 corresponding to arbitrary smooth formula_3 is equivalent to the requirement that \"ω\" should be closed.\n\nA symplectic form on a manifold \"M\" is a closed non-degenerate differential 2-form \"ω\".\nHere, non-degenerate means that for all , if there exists an such that for all , then . The skew-symmetric condition (inherent in the definition of differential 2-form) means that for all we have for all In odd dimensions, antisymmetric matrices are not invertible. Since \"ω\" is a differential two-form, the skew-symmetric condition implies that \"M\" has even dimension. The closed condition means that the exterior derivative of \"ω \"vanishes, . A symplectic manifold consists of a pair (\"M\",\"ω\"), of a manifold \"M\" and a symplectic form \"ω\". Assigning a symplectic form \"ω\" to a manifold \"M\" is referred to as giving \"M\" a symplectic structure.\n\nThere is a standard linear model, namely a symplectic vector space R. Let R have the basis Then we define our symplectic form \"ω\" so that for all we have , , and \"ω\" is zero for all other pairs of basis vectors. In this case the symplectic form reduces to a simple quadratic form. If \"I\" denotes the identity matrix then the matrix, \"Ω\", of this quadratic form is given by the () block matrix:\n\nThere are several natural geometric notions of submanifold of a symplectic manifold.\n\nThe most important case of the isotropic submanifolds is that of Lagrangian submanifolds. A Lagrangian submanifold is, by definition, an isotropic submanifold of maximal dimension, namely half the dimension of the ambient symplectic manifold. One major example is that the graph of a symplectomorphism in the product symplectic manifold is Lagrangian. Their intersections display rigidity properties not possessed by smooth manifolds; the Arnold conjecture gives the sum of the submanifold's Betti numbers as a lower bound for the number of self intersections of a smooth Lagrangian submanifold, rather than the Euler characteristic in the smooth case.\n\nLet formula_13 have global coordinates labelled formula_14. Then, we can equip formula_15 with the canonical symplectic form formula_16. There is a standard Lagrangian submanifold given by formula_17. The form formula_9 vanishes on formula_19 because given any pair of tangent vectors formula_20, we have that formula_21. To elucidate, consider the case formula_22. Then, formula_23, and formula_24. Notice that when we expand this out\nboth terms we have a formula_26 factor, which is 0, by definition.\n\nThe cotangent bundle of a manifold is locally modeled on a space similar to the first example. It can be shown that we can glue these affine symplectic forms hence this bundle forms a symplectic manifold. A more non-trivial example of a Lagrangian submanifold is the zero section of the cotangent bundle of a manifold. For example, let formula_27. Then, we can present formula_28 as\nwhere we are treating the symbols formula_30 as coordinates of formula_31. We can consider the subset where the coordinates formula_32 and formula_33, giving us the zero section. This example can be repeated for any manifold defined by the vanishing locus of smooth functions formula_34 and their differentials formula_35.\n\nAnother useful class of Lagrangian submanifolds can be found using Morse theory. Given a Morse function formula_36 and for a small enough formula_37 one can construct a Lagrangian submanifold given by the vanishing locus formula_38. For a generic morse function we have a Lagrangian intersection given by formula_39.\n\nIn the case of Kahler manifolds (or Calabi-Yau manifolds) we can make a choice formula_40 on formula_41 as a holomorphic n-form, where formula_42 is the real part and formula_43 imaginary. A Lagrangian submanifold formula_44 is called special if in addition to the above Lagrangian condition the restriction formula_43 to formula_44 is vanishing. In other words, the real part formula_42 restricted on formula_44 leads the volume form on formula_44. The following examples are known as special Lagrangian submanifolds,\nThe SYZ conjecture has been proved for special Lagrangian submanifolds but in general, it is open, and brings a lot of impacts to the study of mirror symmetry. see \n\nA Lagrangian fibration of a symplectic manifold \"M\" is a fibration where all of the fibres are Lagrangian submanifolds. Since \"M\" is even-dimensional we can take local coordinates and by Darboux's theorem the symplectic form \"ω\" can be, at least locally, written as , where d denotes the exterior derivative and ∧ denotes the exterior product. Using this set-up we can locally think of \"M\" as being the cotangent bundle TR, and the Lagrangian fibration as the trivial fibration This is the canonical picture.\n\nLet \"L\" be a Lagrangian submanifold of a symplectic manifold (\"K\",ω) given by an immersion (\"i\" is called a Lagrangian immersion). Let give a Lagrangian fibration of \"K\". The composite is a Lagrangian mapping. The critical value set of \"π\" ∘ \"i\" is called a caustic.\n\nTwo Lagrangian maps and are called Lagrangian equivalent if there exist diffeomorphisms \"σ\", \"τ\" and \"ν\" such that both sides of the diagram given on the right commute, and \"τ\" preserves the symplectic form. Symbolically:\nwhere \"τ\"\"ω\" denotes the pull back of \"ω\" by \"τ\".\n\n\n\n"}
{"id": "23006808", "url": "https://en.wikipedia.org/wiki?curid=23006808", "title": "System archetype", "text": "System archetype\n\nSystem archetypes are patterns of behavior of a system. Systems expressed by circles of causality have therefore similar structure. Identifying a system archetype and finding the leverage enables efficient changes in a system. The basic system archetypes and possible solutions of the problems are mentioned in the section \"Examples of system archetypes\". A fundamental property of nature is that no cause can affect the past. System archetypes do not imply that current causes affect past effects.\n\nThe basic idea of system thinking is that every action triggers a reaction. In system dynamics this reaction is called feedback. There are two types of feedback – reinforcing feedback and balancing feedback. Sometimes a feedback (or a reaction) does not occur immediately – the process contains delays. Any system can be drawn as a diagram set up with circles of causality – including actions, feedbacks and delays.\n\nReinforcing feedback (or amplifying feedback) accelerates the given trend of a process. If the trend is ascending, the reinforcing (positive) feedback will accelerate the growth. If the trend is descending, it will accelerate the decline. Falling of an avalanche is an example of the reinforcing feedback process.\n\nBalancing feedback (or stabilizing feedback) will work if any goal-state exists. Balancing process intends to reduce a gap between a current state and a desired state. The balancing (negative) feedback adjusts a present state to a desirable target regardless whether the trend is descending or ascending. An example of the balancing feedback process is staying upright on bicycle (when riding).\n\nDelays in systems cause people to perceive a response to an action incorrectly. This causes an under- or overestimation of the needed action and results in oscillation, instability or even breakdown.\n\nThe following System Archetyes describe the most common generic structures. Before effectively addressing a specific situation, the underlying pattern must be identified. The following Flow Diagram should help identifying these archetypes. The links between the different archetypes are an indicator of most common connections. Keep in mind that in every situation, there may be more possible ways to follow, though. Consider that everyone is located somewhere in the flow, and that every possible situation has its own advantages, down-sides, cave-ats, and options. Nevertheless, correctly identifying and understanding your situation is always the first step of solving your problem in a sustainable way.\n\nThis archetype explains the system in which the response to action is delayed. If the agents do not perceive the delayed feedback, they might overshoot or underestimate the requisite action in order to reach their goals. This could be avoided by being patient or by accelerating reactions of the system to realized measures.\nExample: supply chain (The Beer Game)\n\nThe unprecedented growth is produced by a reinforcing feedback process until the system reaches its peak. The halt of this growth is caused by limits inside or outside of the system. However, if the limits are not properly recognized; the former methods are continuously applied, but more and more aggressively. This results in the contrary of the desired state – a decrease of the system. The solution lies in the weakening or elimination of the cause of limitation. \nExample: dieting, learning foreign languages \n\nAttractiveness Principle is an archetype derived from Limits to Growth. The main difference is that Attractiveness Principle assumes growth is limited with two or more factors.\n\nThe problem is handled by a simple solution with immediate effect, thereby \"healing the symptoms\". The primary source of the problem is overlooked, because its remedy is demanding and has no immediate outcome. The origin of the problem should be identified and solved in the long-term run during which the addiction to the symptomatic remedy decreases. \nExample: drug addiction, paying debts by borrowing\n\nA special case of the “Shifting the Burden” systems archetype that occurs when an intervenor is brought in to help solve an ongoing problem.  Over time, as the intervenor successfully handles the problem, the people within the system become less capable of solving the problem themselves.  They become even more dependent on the intervenor. Examples: ongoing use of outside consultants.\n\nIn simple terms, this is an archetype whereby a system grows increasingly dependent on an outside intervenor to help it function. In the short-term this works, but in the long term the system is unable to function on its  own due to the dependence on the intervention and eventually fails to perform.\n\nA kind of shifting the burden archetype. As current problems need to be handled immediately, the long-term goals continuously decline. It can be avoided by sticking to the vision. \nExample: balancing the public debt, sliding limits of environmental pollution\n\nThis archetype could be seen as a non-cooperative game where both players suppose that just one of them can win. They are responding to actions of the other player in order to “defend themselves”. The aggression grows and can result in self-destructive behavior. The vicious circle can be broken by one agent stopping to react defensively and turn the game into cooperative one. \nExample: arms race\n\nTwo people or activities need the same limited resources. As one of them becomes more successful, more resources are assigned to him/it. The second one becomes less and less successful due to lacking resources, and “prove the right decision” to support the first one. Problems occur if the competition is unhealthy and interferes with the goals of the whole system. The two activities or agents might be decoupled or they should receive balanced amount of resources.\nExamples: two products of one company, work vs. family\n\nAgents use common limited resource to profit individually. As the use of the resource is not controlled, the agents would like to continuously raise their benefits. The resource is therefore used more and more and the revenues of the agents are decreasing. The agents are intensifying their exploitation until the resource is completely used up or seriously damaged. To protect common resources some form of regulation should be introduced.\nExample: fish stocks (The Fishing Game)\n\nIn the fixes that fail archetype, the problem is solved by some fix (a specific solution) with immediate positive effect. Nonetheless, the “side effects” of this solution turn out in the future. The best remedy seems to apply the same solution. \nExample: saving costs on maintenance, paying interest by other loans (with other interests)\n\nThe limit to growth is the current production capacity. It can be removed by sufficient investment in new capacities. If the investment is not aggressive enough (or it is too low), the capacities are overloaded, the quality of services declines and the demand decreases. This archetype is especially important in capacity planning. \nExample: small, but growing company\n\n\n"}
{"id": "43700825", "url": "https://en.wikipedia.org/wiki?curid=43700825", "title": "The Drunkard's Walk", "text": "The Drunkard's Walk\n\nThe Drunkard's Walk: How Randomness Rules Our Lives is a 2008 popular science book by American physicist and author Leonard Mlodinow, which became a \"New York Times\" bestseller and a \"New York Times\" notable book. \n\n\"The Drunkard's Walk\" discusses the role of randomness in everyday events, and the cognitive biases that lead people to misinterpret random events and stochastic processes. The title refers to a certain type of random walk, a mathematical process in which one or more variables change value under a series of random steps. Mlodinow discusses the contributions of mathematical heavyweights Jacob Bernoulli, Pierre-Simon Laplace, and Blaise Pascal, and introduces basic statistical concepts such as regression toward the mean and the law of large numbers, while discussing the role of probability in examples from wine ratings and school grades to political polls.\n\nIn 2008 the Committee for Skeptical Inquiry (CSICOP) awarded Mlodinow the Robert P. Balles Prize for Critical Thinking for the book.\n"}
{"id": "879681", "url": "https://en.wikipedia.org/wiki?curid=879681", "title": "Trigonometric interpolation", "text": "Trigonometric interpolation\n\nIn mathematics, trigonometric interpolation is interpolation with trigonometric polynomials. Interpolation is the process of finding a function which goes through some given data points. For trigonometric interpolation, this function has to be a trigonometric polynomial, that is, a sum of sines and cosines of given periods. This form is especially suited for interpolation of periodic functions.\n\nAn important special case is when the given data points are equally spaced, in which case the solution is given by the discrete Fourier transform.\n\nA trigonometric polynomial of degree \"K\" has the form\n\nThis expression contains 2\"K\" + 1 coefficients, \"a\", \"a\", … \"a\", \"b\", …, \"b\", and we wish to compute those coefficients so that the function passes through \"N\" points:\nSince the trigonometric polynomial is periodic with period 2π, the \"N\" points can be distributed and ordered in one period as \n(Note that we do \"not\" in general require these points to be equally spaced.) The interpolation problem is now to find coefficients such that the trigonometric polynomial \"p\" satisfies the interpolation conditions.\n\nThe problem becomes more natural if we formulate it in the complex plane. We can rewrite the formula for a trigonometric polynomial as\nformula_3\nwhere \"i\" is the imaginary unit. If we set \"z\" = \"e\", then this becomes\nwith\nThis reduces the problem of trigonometric interpolation to that of polynomial interpolation on the unit circle. Existence and uniqueness for trigonometric interpolation now follows immediately from the corresponding results for polynomial interpolation.\n\nFor more information on formulation of trigonometric interpolating polynomials in the complex plane see , p135 Interpolation using Fourier Polynomials.\n\nUnder the above conditions, there exists a solution to the problem for \"any\" given set of data points {\"x\", \"y\"} as long as \"N\", the number of data points, is not larger than the number of coefficients in the polynomial, i.e., \"N\" ≤ 2\"K\"+1 (a solution may or may not exist if \"N\">2\"K\"+1 depending upon the particular set of data points). Moreover, the interpolating polynomial is unique if and only if the number of adjustable coefficients is equal to the number of data points, i.e., \"N\" = 2\"K\" + 1. In the remainder of this article, we will assume this condition to hold true.\n\nIf the number of points \"N\" is odd, say \"N=2K+1\", applying the Lagrange formula for polynomial interpolation to the polynomial formulation in the complex plane yields that the solution can be written in the form\n\nwhere\nThe factor formula_7 in this formula compensates for the fact that the complex plane formulation contains also negative powers of formula_8 and is therefore not a polynomial expression in formula_8. The correctness of this expression can easily be verified by observing that formula_10 and that formula_11 is a linear combination of the right powers of formula_8.\nUpon using the identity\n\nthe coefficient formula_11 can be written in the form\nIf the number of points \"N\" is even, say \"N=2K\", applying the Lagrange formula for polynomial interpolation to the polynomial formulation in the complex plane yields that the solution can be written in the form\n\nwhere\nHere, the constants formula_14 can be chosen freely. This is caused by the fact that the interpolating function () contains an odd number of unknown constants. A common choice is to require that the highest frequency is of the form a constant times formula_15, i.e. the formula_16 term vanishes, but in general the phase of the highest frequency can be chosen to be formula_17. To get an expression for formula_14, we obtain by using () that () can we written in the form\nThis yields\nand\n\nNote that care must be taken in order to avoid infinities caused by zeros in the denominators.\n\nFurther simplification of the problem is possible if nodes formula_22 are equidistant, i.e.\nsee Zygmund for more details.\n\nFurther simplification by using () would be an obvious approach, but is obviously involved. A much simpler approach is to consider the Dirichlet kernel\nwhere formula_25 is odd. It can easily be seen that formula_26 is a linear combination of the right powers of formula_8 and satisfies\nSince these two properties uniquely define the coefficients formula_11 in (), it follows that\nHere, the sinc-function prevents any singularities and is defined by\n\nFor formula_32 even, we define the Dirichlet kernel as\nAgain, it can easily be seen that formula_26 is a linear combination of the right powers of formula_8, does not contain the term formula_36 and satisfies\nUsing these properties, it follows that the coefficients formula_11 in () are given by\nNote that formula_11 does not contain the formula_36 as well. Finally, note that the function formula_36 vanishes at all the points formula_22. Multiples of this term can, therefore, always be added, but it is commonly left out.\n\nA MATLAB implementation of the above can be found here and is given by:\n\nThe special case in which the points \"x\" are equally spaced is especially important. In this case, we have\n\nThe transformation that maps the data points \"y\" to the coefficients \"a\", \"b\" is obtained from the discrete Fourier transform (DFT) of order N.\n\nThe case of the cosine-only interpolation for equally spaced points, corresponding to a trigonometric interpolation when the points have even symmetry, was treated by Alexis Clairaut in 1754. In this case the solution is equivalent to a discrete cosine transform. The sine-only expansion for equally spaced points, corresponding to odd symmetry, was solved by Joseph Louis Lagrange in 1762, for which the solution is a discrete sine transform. The full cosine and sine interpolating polynomial, which gives rise to the DFT, was solved by Carl Friedrich Gauss in unpublished work around 1805, at which point he also derived a fast Fourier transform algorithm to evaluate it rapidly. Clairaut, Lagrange, and Gauss were all concerned with studying the problem of inferring the orbit of planets, asteroids, etc., from a finite set of observation points; since the orbits are periodic, a trigonometric interpolation was a natural choice. See also Heideman \"et al.\" (1984).\n\nChebfun, a fully integrated software system written in MATLAB for computing with functions, uses trigonometric interpolation and Fourier expansions for computing with periodic functions. Many algorithms related to trigonometric interpolation are readily available in Chebfun; several examples are available here.\n\n\n"}
{"id": "29753359", "url": "https://en.wikipedia.org/wiki?curid=29753359", "title": "Tversky index", "text": "Tversky index\n\nThe Tversky index, named after Amos Tversky, is an asymmetric similarity measure on sets that compares a variant to a prototype. The Tversky index can be seen as a generalization of Dice's coefficient and Tanimoto coefficient (aka Jaccard index).\n\nFor sets \"X\" and \"Y\" the Tversky index is a number between 0 and 1 given by\n\nformula_1,\n\nHere, formula_2 denotes the relative complement of Y in X.\n\nFurther, formula_3 are parameters of the Tversky index. Setting formula_4 produces the Tanimoto coefficient; setting formula_5 produces Dice's coefficient.\n\nIf we consider \"X\" to be the prototype and \"Y\" to be the variant, then formula_6 corresponds to the weight of the prototype and formula_7 corresponds to the weight of the variant. Tversky measures with formula_8 are of special interest.\n\nBecause of the inherent asymmetry, the Tversky index does not meet the criteria for a similarity metric. However, if symmetry is needed a variant of the original formulation has been proposed using max and min functions \n\nformula_9,\n\nformula_10,\n\nformula_11,\n\nThis formulation also re-arranges parameters formula_12 and formula_13. Thus, formula_14 controls the balance between formula_15 and formula_16 in the denominator. Similarly, formula_7 controls the effect of the symmetric difference formula_18 versus formula_19 in the denominator.\n"}
{"id": "19358368", "url": "https://en.wikipedia.org/wiki?curid=19358368", "title": "Waterloo Institute for Nanotechnology", "text": "Waterloo Institute for Nanotechnology\n\nThe Waterloo Institute for Nanotechnology (WIN) is located at the University of Waterloo and is co-located with the Institute for Quantum Computing in the Mike and Ophelia Lazaridis Quantum-Nano Centre (QNC). WIN is headed by Dr. Sushanta Mitra.\n\nThe Waterloo Institute for Nanotechnology comprises 84 faculty from nine different departments in the faculties of Science, Engineering, and Math.\n\nThe Quantum-Nano centre is the site of a community laboratory for nano-metrology and nano-fabrication. Construction began on 9 June 2008 and is expected to be completed 21 September 2012. The 160 million dollar, facility will be the home to a laboratory.\n\nCapital funding for construction of the QNC was made possible by major gifts and awards from multiple sources including a 101 million dollar donation from Ophelia and Mike Lazaridis (co-CEO of Research in Motion and Chancellor of the University of Waterloo). Government funding includes 17.9 million dollars from the Canada Foundation for Innovation (CFI) which has been matched by the Province of Ontario. In addition, an anonymous donor has provided an endowment of 29 million dollars for 3 endowed chairs and 42 Graduate Nanofellowships.\n\nG2N is a fabrication laboratory that integrates a range of thin-film manufacturing, assembly, testing, and characterization equipment to create electronic systems in the very large (a few billion pixels) and very small (a few nanometres) size range.\n\nWATlab is a nano-materials metrology research facility, equipped with surface and nano-materials research tools for exploring areas of nanotechnology and nano-scale sciences.\n\nThe Advanced Micro-Nano Lab will address the following device technologies.\n\nResearch in nano-engineered materials includes many departments and Faculties at Waterloo. Researchers in chemistry, chemical engineering, mechanical and mechatronics engineering, and electrical and computer engineering are collaborating on modeling, design, fabrication, processing, characterization and analysis of nano-scale properties of materials, structures, devices and systems. This development will be further driven by the need to address a critical issue also faced in the integration of nano-scale devices: the interface between nano-structured materials and the macroscopic world.\n\n\nDeveloping techniques to integrate NEMS/CMOS (nano electro mechanical systems/complementary metal oxide semiconductor) to develop manipulators with atomic precision in all three dimensions with on-chip control. Example applications include: scanning probe microscopy, atomic force microscopy, nano-materials characterization and atomic resolution imaging with the objective of developing technologies for precision nano-scale assembly and manufacturing.\n\n\nTechniques to fabricate new instrumentation to characterize critical parameters such as size, composition, stiffness, surface characteristics, dopant concentration, magnetic coercivity, and other properties of particular interest to the nano scale. Due to their small size, nano-systems are extremely challenging to assemble, and yet precise control of their parameters is often critical to their performance. A related goal is to cause a paradigm shift in classical chemical measurements (in which samples are brought to the lab for analysis) by developing wireless, energy-efficient mobile nano-instruments that allow users to bring the lab to the sample. Examples of \"the lab\" include nano-instruments of all types and two examples of \"the sample\" include the environment or a patient. The metrology arm of the new labs will be used to study measurements at the nano-scale, to develop new and unique nano-scale measuring instruments, and to calibrate such instruments. It will also address associated challenges involved in fabricating, integrating and packaging instruments at the nano-scale.\n\n\nNano–bio is a field that includes both the use of nanotechnology in biological and agrifood systems and use of biological or biomimetic techniques in nanotechnology. Nanobiotechnology shows a tremendous promise of improving the quality of life. For example, nanovehicles might deliver drugs directly to targeted cells, nanomembranes may be used for development of cheap, effective water purification systems, or nanochips that interface neurons with electronics may become common place. NEMS (nano-electromechanical systems) might use sensors and physical controls to stabilize individuals with heart, kidney or liver disease. As nanotechnology researchers strive to create self-assembling devices, they are beginning to exploit natural self-assemblers: proteins, DNA and viruses. Examples also include development of food quality monitoring sensors and microfluidic biosensor components.Nanoscale imaging of biological systems helps to understand the nanoscale structure–function relationship of materials and in evaluating the food quality–function information. Characterization of nanoscale fragments of biomaterials such as DNA, proteins, chromosomes, plant cells, bacteria, starch granules and anti-allergens are extremely important.\n\n\nNanotechnology research at Waterloo has national and international scope through collaboration and partnerships with:\n\n"}
{"id": "30357387", "url": "https://en.wikipedia.org/wiki?curid=30357387", "title": "William Beckner (mathematician)", "text": "William Beckner (mathematician)\n\nWilliam Beckner (born September 15, 1941) is an American mathematician, known for his work in harmonic analysis, especially geometric inequalities. He is the Paul V. Montgomery Centennial Memorial Professor in Mathematics at The University of Texas at Austin.\n\nBeckner earned his Bachelor of Science in physics from the University of Missouri in Columbia, Missouri in 1963, where he became a member of the Phi Beta Kappa Society. He later earned his Ph.D. in mathematics at Princeton University in Princeton, New Jersey, where his doctoral adviser was Elias Stein. He also completed some postgraduate work in mathematics under adviser A.P. Calderon at the University of Chicago.\n\n\n\n\n"}
{"id": "12918859", "url": "https://en.wikipedia.org/wiki?curid=12918859", "title": "Włodzimierz Kuperberg", "text": "Włodzimierz Kuperberg\n\nWłodzimierz Kuperberg (born January 19, 1941) is a professor of mathematics at Auburn University, with research interests in geometry and topology.\n\nAlthough Kuperberg is Polish-American, he was born in what is now Belarus, where his parents and older siblings had traveled east to escape World War II. In 1946, the family returned to Poland, resettling in Szczecin, where Kuperberg grew up. He began his studies at the University of Warsaw in 1959, and received his Ph.D. from the same institution in 1969, under the supervision of Karol Borsuk. During his time at Warsaw, he published three high school textbooks in Polish. Kuperberg left Poland due to the anti-semitic aspects of the 1967-1968 Polish political crisis, and worked at Stockholm University until 1972, when he assumed a visiting position at the University of Houston. In 1974, Kuperberg took a position at Auburn where he remains.\n\nKuperberg married mathematician Krystyna Kuperberg in 1964, and their son Greg Kuperberg is also a professional mathematician, while their daughter Anna Kuperberg is a photographer.\n\nAlthough much of Kuperberg's early mathematical work is in topology, he is best known today for his work in geometry, and in particular on packing and covering problems. His first paper in this area (1982) showed that the ratio of packing density to covering density of any convex body in the plane is at least 3/4. His 1990 paper on double lattices with his son Greg provides the best lower bound known at that time for packing densities of arbitrary two-dimensional convex bodies; with Bezdek (1990) he calculated the exact packing density of the infinite cylinder, which prior to Hales' 1998 solution of the Kepler conjecture was the first nontrivial calculation of the packing density of any three-dimensional convex body.\n\nAs a high school student, Kuperberg won first prize in the 10th Polish Mathematical Olympiad, leading him to enroll in mathematics when he began his college studies. While at the University of Warsaw he received both the university's Excellence in Teaching and Research Award and the Polish Mathematical Society Award for Young Mathematicians. He was honored again at Auburn by a five-year Alumni Professor chairship in 1996, and again by an Erdős Professorship in 1999, which he used to visit the Hungarian Academy of Sciences in Budapest. In 2003, his colleagues presented him with a festschrift edited by András Bezdek, consisting of 34 papers in discrete geometry.\n\n\n"}
