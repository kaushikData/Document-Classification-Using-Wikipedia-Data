{"id": "31460418", "url": "https://en.wikipedia.org/wiki?curid=31460418", "title": "AIMMS", "text": "AIMMS\n\nAIMMS is a prescriptive analytics software company with offices in the Netherlands, United States, China and Singapore. AIMMS has two main product offerings that provide modeling and optimization capabilities across a variety of industries. The AIMMS Prescriptive Analytics Platform is a tool for those with an Operations Research or Analytics background. It offers unlimited flexibility to develop optimization-based applications and deploy them to business users. AIMMS SC Navigator, launched in 2017, is built on the AIMMS Prescriptive Analytics Platform and provides configurable Apps for supply chain teams. SC Navigator provides supply chain analytics to individuals without a technical or analytics background so they can get the same benefits from sophisticated analytics without needing to code or model.\n\nAIMMS B.V. was founded in 1989 by noted mathematician Johannes Bisschop under the name of Paragon Decision Technology. His vision was to make optimization more approachable by building models rather than programming. In Bisschop’s view, modeling was able to build the bridge between the people who had problems and the people helping them solve those problems. \n\nAIMMS (an acronym for \"Advanced Interactive Multidimensional Modeling System\") began as a software system designed for modeling and solving large-scale optimization and scheduling-type problems. \n\nAIMMS is considered to be one of the five most important algebraic modeling languages and the creator (Johannes J. Bisschop) has been awarded with INFORMS Impact Prize for his work in this language.\n\nIn 2003, AIMMS was acquired by a small private equity firm who saw the potential value of mathematics to business. This led to the creation of a partnership program, further technical investment and the evolution of the platform. In 2011, the company launched AIMMS PRO, a way to deploy applications to end-users who don't have a technical background. This was quickly followed by the ability to publish and customize applications using a browser so that decision support applications are available on any device. These innovations led to rapid customer adoption and growth for the company. In 2017, AIMMS was recognized as a top B2B technology in the Netherlands. and was named one of the fastest growing companies in the Netherlands for the second consecutive year.\n\nIn 2017, the product management team did a listening tour with supply chain executives. This, along with the growing interest in embedded advanced analytics for supply chain management, generated the initial idea for the AIMMS SC Navigator Platform and the democratization of supply chain analytics. SC Navigator consists of ready-made applications that are easily configurable to put supply chain analytics in the hands of supply chain professionals. \n\nAIMMS SC Navigator launched in October, 2017 with three initial cloud-based Apps: Supply Chain Network Design, Sales & Operations Planning and Data Navigator. In 2018 two additional applications were rolled out - Center of Gravity and Product Lifecycle. Additional applications are rolling out every quarter.\n\nThe AIMMS Prescriptive Analytics Platform consists of an algebraic modeling language, an integrated development environment for both editing models and creating a graphical user interface around these models, and a graphical end-user environment.\nAIMMS is linked to multiple solvers through the AIMMS Open Solver Interface.\nSupported solvers include CPLEX, MOSEK, FICO Xpress, CBC, Conopt, MINOS, IPOPT, SNOPT, KNITRO and CP Optimizer.\n\nAIMMS features a mixture of declarative and imperative programming styles. Formulation of optimization models takes place through declarative language elements such as sets and indices, as well as scalar and multidimensional parameters, variables and constraints, which are common to all algebraic modeling languages, and allow for a concise description of most problems in the domain of mathematical optimization. Units of measurement are natively supported in the language, and compile- and runtime unit analysis may be employed to detect modeling errors.\n\nProcedures and control flow statements are available in AIMMS for \nTo support the re-use of common modeling components, AIMMS allows modelers to organize their model in user model libraries.\n\nAIMMS supports a wide range of mathematical optimization problem types:\nUncertainty can be taken into account in deterministic linear and mixed integer optimization models in AIMMS through the specification of additional attributes, such that stochastic or robust optimization techniques can be applied alongside the existing deterministic solution techniques.\n\nCustom hybrid and decomposition algorithms can be constructed using the GMP system library which makes available at the modeling level many of the basic building blocks used internally by the higher level solution methods present in AIMMS, matrix modification methods, as well as specialized steps for customizing solution algorithms for specific problem types.\n\nOptimization solutions created with AIMMS can be used either as a standalone desktop application or can be embedded as a software component in other applications.\n\nAIMMS Prescriptive Analytics Platform is used in a wide range of industries including retail, consumer products, healthcare, oil and chemicals, steel production and agribusiness.\n\nGE Grid uses AIMMS as the modeling and optimization engine of its energy market clearing software.\nTogether with GE Grid, AIMMS was part of the analytics team of Midwest ISO that won the Franz Edelman Award for Achievement in Operations Research and the Management Sciences of 2011 for successfully applying operations research in the Midwest ISO energy market. In 2012, TNT Express, an AIMMS customer won the Franz Edleman Award for modernizing its operations and reducing its carbon footprint. The AIMMS platform was also used by the Dutch Delta team to develop and implement a new method for calculating the most efficient levels of flood protection for the Netherlands and won the Edelman prize in 2013. \n\n\n"}
{"id": "30733508", "url": "https://en.wikipedia.org/wiki?curid=30733508", "title": "Aleksandrov–Rassias problem", "text": "Aleksandrov–Rassias problem\n\nThe theory of isometries in the framework of Banach spaces has its beginning in a paper by Stanisław Mazur and Stanisław M. Ulam in 1932. They proved that each isometry of a normed real linear space onto a normed real linear space is a linear mapping up to translation. In 1970, Aleksandr Danilovich Aleksandrov asked whether the existence of a single conservative distance for some mapping implies that it is an isometry. Themistocles M. Rassias posed the following problem:\n\nAleksandrov–Rassias Problem. If \"X\" and \"Y\" are normed linear spaces and if \"T\" : \"X\" → \"Y\" is a continuous and/or surjective mapping which satisfies the so-called distance one preserving property (DOPP), is then \"T\" necessarily an isometry?\n\nThere have been several attempts in the mathematical literature by a number of researchers for the solution to this problem.\n\n"}
{"id": "51389390", "url": "https://en.wikipedia.org/wiki?curid=51389390", "title": "Antonia Ferrín Moreiras", "text": "Antonia Ferrín Moreiras\n\nAntonia Ferrín Moreiras (Ourense, May 13, 1914 – Santiago de Compostela, August 6, 2009) was a mathematician, professor and the first female Galician astronomer. Her main contributions to astronomy were works on stellar occultations by the moon, measures of double stars and astrometric measurements, as well as the determination of the passage of stars through two verticals. She accomplished all of this while she was working at the Observatory of the University of Santiago de Compostela (USC).\n\nBefore the Spanish Civil War, she obtained degrees in chemistry and pharmacy from the USC, earned her teaching diploma and studied Exact Sciences, the name for mathematics, for two years. She obtained her degree in mathematics in the Complutense University of Madrid some years later.\n\nIn 1963 she became the first Spanish woman to defend a thesis that addressed the issue of astronomy: \"Observaciones de pasos por dos verticales\" (in English, \"Observations of passages of stars through two verticals\"). This thesis was also the first defended in the Faculty of Mathematics of the USC.\n\nEconomic hardship forced her to study and work at the same time, but she earned scholarships that helped her reach her academic goals.\n\nAntonia Ferrín Moreiras was born in Ourense on May 13, 1914, and was the third of four daughters. Contrary to the conventions of the time, and despite the limited economic resources of the family, her father, a mathematics teacher, wanted all his daughtersAntonia, Celsa, María and Pastora, to have access to higher-level studies, so the family moved to Santiago de Compostela in 1920.\n\nFerrín started school when she was seven years old and at only nine years old, she was ready to start high school. She did the Scientific Bachillerato, which is the stage of education for people over the age of 16 in Spain. After that, she studied the University Bachillerato, since admission to the university required the completion of this education stage, with twelve other girls.\n\nFerrin started her higher education studies in the Faculty of Sciences of the USC, at a time when chemistry was the only major offered. Even though her father worked in the Faculty of Pharmacy, his income was not enough, so she obtained free tuition and scholarships. With such financial support, she could earn a degree in chemistry and the title of Schoolteacher in 1935.\n\nShe started working immediately after she graduated, but her first jobs were not paid. From 1934 to 1936, Ferrín worked as training assistant teacher of physics and mathematics in the Faculty of Sciences of the USC and as temporary assistant teacher in the Department of Science of the high school Arcebispo Xelmírez in Santiago de Compostela. At the same time, she was pursuing a pharmacy degree and the only two courses of Exact Sciences that the USC offered. In 1937, she started working as mathematics teacher in the orphan girls’ school Nuestra Señora de los Remedios, a position she held until 1948.\n\nIn 1937, after the outbreak of the Spanish Civil War, she was sanctioned because of her political convictions, and an anonymous complaint lead to the opening of her file. As a result, she was removed from her duties as university teacher along with 45 other teachers. In 1940, however, she got her case reviewed, the sentence was revoked and she could teach again. After her removal, she restarted teaching at the university and sometimes she gave lessons in the girls' school until 1961.\n\nDuring the academic year 1939–1940, Ferrin completed the prerequisite courses for the pharmacy degree. It was during this time that she met Ramón María Aller, the founder and director of the Astronomical Observatory of the USC, and the person who made it possible for her to become the first female Galician astronomer. In that observatory, Ferrín started to use the passage instrument and the refracting telescope, which allowed her to visualize the concealment of stars by the moon, the passage of stars through two verticals, or micrometric measurements of double stars. The results of these investigations were published in the Spanish magazine of astronomy, \"Urania\". Ferrín recalled anecdotally that it was really cold while she was doing her research in the observatory. She never wore trousers because, according to her, it was a piece of clothing that “was not considered feminine and only the more daring movie actresses wore them on the big screen.”\n\nIn 1950, she received a scholarship from the Spanish National Research Council to conduct research in the Astronomical Observatory of Santiago. This scholarship became a contract of research assistantship two years later. In the same year, she obtained a degree in mathematics from the Complutense University of Madrid, even though she was a guest student for the last three courses of the degree.\n\nIn 1953, she passed the civil service examinations for professor of mathematics in the Escuela de Magisterio of Santander. However, two years later, she transferred to the girls’ school Isabel la Católica of Santiago.\n\nFrom 1954 until 1956 she studied to earn a doctorate in astronomy in Madrid. In the meantime, she helped professor Vidal Abascal to create the degree in mathematics in the Faculty of Sciences of the USC, a feat accomplished in 1957. Ferrín became the first female professor of this faculty. During this time, she continued her research under the direction of Ramón María Aller.\n\nThe thesis that she defended in 1963 with the help of Aller, even though he was 80 years old, was probably her greatest accomplishment not only because it was the first read in the Faculty of Mathematics of the USC, but also the first one defended by a woman addressing the issue of astronomy in Spain. In 1963, she was appointed as full-time professor of mathematics in the Escuela de Magisterio Santa María in Madrid and she also worked as professor of mathematics in the degree of medicine for two years. In addition, she taught astronomy and celestial mechanics, she served in management, and sat on the selection boards. She also participated in the first international meetings of mathematics that were carried out in Spain.\n\nHowever, in spite of her great professional career, being a woman sometimes stood in the way of her aspirations. For example, when Aller got sick in 1964, a substitute was needed so that Aller’s chair of astronomy in the observatory could continue. Even though she applied for the public examination, she was excluded. She was considered as an applicant following a complaint, but the chair was declared void and the observatory deteriorated.\n\nFerrín retired in 1984.\n\nOn May 24, 2008, she was proclaimed as the patroness of the fiftieth anniversary of the Faculty of Mathematics of the USC.\n\n"}
{"id": "28326876", "url": "https://en.wikipedia.org/wiki?curid=28326876", "title": "Asplund space", "text": "Asplund space\n\nIn mathematics — specifically, in functional analysis — an Asplund space or strong differentiability space is a type of well-behaved Banach space. Asplund spaces were introduced in 1968 by the mathematician Edgar Asplund, who was interested in the Fréchet differentiability properties of Lipschitz functions on Banach spaces.\n\nThere are many equivalent definitions of what it means for a Banach space \"X\" to be an Asplund space:\n\n\n"}
{"id": "8529968", "url": "https://en.wikipedia.org/wiki?curid=8529968", "title": "Bayesian regret", "text": "Bayesian regret\n\nIn game theory, Bayesian regret is the average difference between the utility of a strategy and an ideal utility where desired outcomes are maximized. \n\nThe term \"Bayesian\" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.\n\nIn social choice theory, Bayesian regret is the average difference in social utility between the chosen candidate and the best candidate. It is only measurable if it is possible to know the voters' true numerical utility for each candidate – that is, in Monte Carlo simulations of virtual elections. The term Bayesian is somewhat a misnomer, really meaning only \"average probabilistic\"; there is no standard or objective way to create distributions of voters and candidates.\n\nThe Bayesian regret concept was recognized as useful (and used) for comparing single-winner voting systems by Bordley and Merrill, and it also was invented independently by R. J. Weber. Bordley attributed it (and the whole idea of the usefulness of \"social\" utility, that is, summed over all people in the population) to John Harsanyi in 1955.\n\nThis term has been used to compare a random buy-and-hold strategy to professional traders' records. This same concept has received numerous different names, as the New York Times notes: \n\n\"In 1957, for example, a statistician named James Hanna called his theorem Bayesian Regret. He had been preceded by David Blackwell, also a statistician, who called his theorem Controlled Random Walks. Other, later papers had titles like 'On Pseudo Games', 'How to Play an Unknown Game', 'Universal Coding' and 'Universal Portfolios'\".\n\n\n"}
{"id": "34458555", "url": "https://en.wikipedia.org/wiki?curid=34458555", "title": "Böttcher's equation", "text": "Böttcher's equation\n\nBöttcher's equation is the functional equation \nwhere \nThe logarithm of this functional equation amounts to Schröder's equation.\nThe equation is named after Lucjan Böttcher. \n\nSolution of functional equation is a function in implicit form. \n\nLucian Emil Böttcher sketched a proof in 1904 on the existence of solution: an analytic function \"F\" in a neighborhood of the fixed point \"a\", such that: \n\nThis solution is sometimes called:\nThe complete proof was published by Joseph Ritt in 1920, who was unaware of the original formulation.\n\nBöttcher's coordinate (the logarithm of the Schröder function) conjugates in a neighbourhood of the fixed point to the function . An especially important case is when is a polynomial of degree , and = ∞ .\n\nFor the function h and n=2\n\nthe Böttcher function F is: \n\nBöttcher's equation plays a fundamental role in the part of holomorphic dynamics which studies iteration of polynomials of one complex variable. \nGlobal properties of the Böttcher coordinate were studied by Fatou\n\n"}
{"id": "31917729", "url": "https://en.wikipedia.org/wiki?curid=31917729", "title": "Circular permutation in proteins", "text": "Circular permutation in proteins\n\nA circular permutation is a relationship between proteins whereby the proteins have a changed order of amino acids in their peptide sequence. The result is a protein structure with different connectivity, but overall similar three-dimensional (3D) shape. In 1979, the first pair of circularly permuted proteins – concanavalin A and lectin – were discovered; over 2000 such proteins are now known.\n\nCircular permutation can occur as the result of evolutionary events, posttranslational modifications, or artificially engineered mutations. The two main models proposed to explain the evolution of circularly permuted proteins are \"permutation by duplication\" and \"fission and fusion\". Permutation by duplication occurs when a gene undergoes duplication to form a tandem repeat, before redundant sections of the protein are removed; this relationship is found between saposin and swaposin. Fission and fusion occurs when partial proteins fuse to form a single polypeptide, such as in nicotinamide nucleotide transhydrogenases.\n\nCircular permutations are routinely engineered in the laboratory to improve their catalytic activity or thermostability, or to investigate properties of the original protein.\n\nTraditional algorithms for sequence alignment and structure alignment are not able to detect circular permutations between proteins. New non-linear approaches have been developed that overcome this and are able to detect topology-independent similarities.\n\nIn 1979, Bruce Cunningham and his colleagues discovered the first instance of a circularly permuted protein in nature. After determining the peptide sequence of the lectin protein favin, they noticed its similarity to a known protein – concanavalin A – except that the ends were circularly permuted. Later work confirmed the circular permutation between the pair and showed that concanavalin A is permuted post-translationally through cleavage and an unusual protein ligation.\n\nAfter the discovery of a natural circularly permuted protein, researchers looked for a way to emulate this process. In 1983, David Goldenberg and Thomas Creighton were able to create a circularly permuted version of a protein by chemically ligating the termini to create a cyclic protein, then introducing new termini elsewhere using trypsin. In 1989, Karolin Luger and her colleagues introduced a genetic method for making circular permutations by carefully fragmenting and ligating DNA. This method allowed for permutations to be introduced at arbitrary sites.\n\nDespite the early discovery of post-translational circular permutations and the suggestion of a possible genetic mechanism for evolving circular permutants, it was not until 1995 that the first circularly permuted pair of genes were discovered. Saposins are a class of proteins involved in sphingolipid catabolism and antigen presentation of lipids in humans. Chris Ponting and Robert Russell identified a circularly permuted version of a saposin inserted into plant aspartic proteinase, which they nicknamed swaposin. Saposin and swaposin were the first known case of two natural genes related by a circular permutation.\n\nHundreds of examples of protein pairs related by a circular permutation were subsequently discovered in nature or produced in the laboratory. As of February 2012, the Circular Permutation Database contains 2,238 circularly permuted protein pairs with known structures, and many more are known without structures. The CyBase database collects proteins that are cyclic, some of which are permuted variants of cyclic wild-type proteins. SISYPHUS is a database that contains a collection of hand-curated manual alignments of proteins with non-trivial relationships, several of which have circular permutations.\n\nThere are two main models that are currently being used to explain the evolution of circularly permuted proteins: \"permutation by duplication\" and \"fission and fusion\". The two models have compelling examples supporting them, but the relative contribution of each model in evolution is still under debate. Other, less common, mechanisms have been proposed, such as \"cut and paste\" or \"exon shuffling\".\n\nThe earliest model proposed for the evolution of circular permutations is the permutation by duplication mechanism. In this model, a precursor gene first undergoes a duplication and fusion to form a large tandem repeat. Next, start and stop codons are introduced at corresponding locations in the duplicated gene, removing redundant sections of the protein.\n\nOne surprising prediction of the permutation by duplication mechanism is that intermediate permutations can occur. For instance, the duplicated version of the protein should still be functional, since otherwise evolution would quickly select against such proteins. Likewise, partially duplicated intermediates where only one terminus was truncated should be functional. Such intermediates have been extensively documented in protein families such as DNA methyltransferases.\n\nAn example for permutation by duplication is the relationship between saposin and swaposin. Saposins are highly conserved glycoproteins, approximately 80 amino acid residues long and forming a four alpha helical structure. They have a nearly identical placement of cysteine residues and glycosylation sites. The cDNA sequence that codes for saposin is called prosaposin. It is a precursor for four cleavage products, the saposins A, B, C, and D. The four saposin domains most likely arose from two tandem duplications of an ancestral gene. This repeat suggests a mechanism for the evolution of the relationship with the plant-specific insert (PSI). The PSI is a domain exclusively found in plants, consisting of approximately 100 residues and found in plant aspartic proteases. It belongs to the saposin-like protein family (SAPLIP) and has the N- and C- termini \"swapped\", such that the order of helices is 3-4-1-2 compared with saposin, thus leading to the name \"swaposin\".\n\nAnother model for the evolution of circular permutations is the fission and fusion model. The process starts with two partial proteins. These may represent two independent polypeptides (such as two parts of a heterodimer), or may have originally been halves of a single protein that underwent a fission event to become two polypeptides.\n\nThe two proteins can later fuse together to form a single polypeptide. Regardless of which protein comes first, this fusion protein may show similar function. Thus, if a fusion between two proteins occurs twice in evolution (either between paralogues within the same species or between orthologues in different species) but in a different order, the resulting fusion proteins will be related by a circular permutation.\n\nEvidence for a particular protein having evolved by a fission and fusion mechanism can be provided by observing the halves of the permutation as independent polypeptides in related species, or by demonstrating experimentally that the two halves can function as separate polypeptides.\n\nAn example for the fission and fusion mechanism can be found in nicotinamide nucleotide transhydrogenases. These are membrane-bound enzymes that catalyze the transfer of a hydride ion between NAD(H) and NADP(H) in a reaction that is coupled to transmembrane proton translocation. They consist of three major functional units (I, II, and III) that can be found in different arrangement in bacteria, protozoa, and higher eukaryotes. Phylogenetic analysis suggests that the three groups of domain arrangements were acquired and fused independently.\n\nThe two evolutionary models mentioned above describe ways in which genes may be circularly permuted, resulting in a circularly permuted mRNA after transcription. Proteins can also be circularly permuted via post-translational modification, without permuting the underlying gene. Circular permutations can happen spontaneously through autocatalysis, as in the case of concanavalin A. Alternately, permutation may require restriction enzymes and ligases.\n\nMany proteins have their termini located close together in 3D space. Because of this, it is often possible to design circular permutations of proteins. Today, circular permutations are generated routinely in the lab using standard genetics techniques. Although some permutation sites prevent the protein from folding correctly, many permutants have been created with nearly identical structure and function to the original protein.\n\nThe motivation for creating a circular permutant of a protein can vary. Scientists may want to improve some property of the protein, such as:\n\nAlternately, scientists may be interested in properties of the original protein, such as:\n\nMany sequence alignment and protein structure alignment algorithms have been developed assuming linear data representations and as such are not able to detect circular permutations between proteins. Two examples of frequently used methods that have problems correctly aligning proteins related by circular permutation are dynamic programming and many hidden Markov models. As an alternative to these, a number of algorithms are built on top of non-linear approaches and are able to detect topology-independent similarities, or employ modifications allowing them to circumvent the limitations of dynamic programming. The table below is a collection of such methods.\n\nThe algorithms are classified according to the type of input they require. \"Sequence\"-based algorithms require only the sequence of two proteins in order to create an alignment. Sequence methods are generally fast and suitable for searching whole genomes for circularly permuted pairs of proteins. \"Structure\"-based methods require 3D structures of both proteins being considered. They are often slower than sequence-based methods, but are able to detect circular permutations between distantly related proteins with low sequence similarity. Some structural methods are \"topology independent\", meaning that they are also able to detect more complex rearrangements than circular permutation.\n\n"}
{"id": "22174129", "url": "https://en.wikipedia.org/wiki?curid=22174129", "title": "Community matrix", "text": "Community matrix\n\nIn mathematical biology, the community matrix is the linearization of the Lotka–Volterra equation at an equilibrium point. The eigenvalues of the community matrix determine the stability of the equilibrium point.\n\nThe Lotka–Volterra predator–prey model is\nwhere \"x\"(\"t\") denotes the number of prey, \"y\"(\"t\") the number of predators, and \"α\", \"β\", \"γ\" and \"δ\" are constants. By the Hartman–Grobman theorem the non-linear system is topologically equivalent to a linearization of the system about an equilibrium point (\"x\"*, \"y\"*), which has the form\nwhere \"u\" = \"x\" − \"x\"* and \"v\" = \"y\" − \"y\"*. In mathematical biology, the Jacobian matrix formula_3 evaluated at the equilibrium point (\"x\"*, \"y\"*) is called the community matrix. By the stable manifold theorem, if one or both eigenvalues of formula_3 have positive real part then the equilibrium is unstable, but if all eigenvalues have negative real part then it is stable.\n\n"}
{"id": "7620568", "url": "https://en.wikipedia.org/wiki?curid=7620568", "title": "Computation history", "text": "Computation history\n\nIn computer science, a computation history is a sequence of steps taken by an abstract machine in the process of computing its result. Computation histories are frequently used in proofs about the capabilities of certain machines, and particularly about the undecidability of various formal languages.\n\nFormally, a computation history is a (normally finite) sequence of configurations of a formal automaton. Each configuration fully describes the status of the machine at a particular point. To be valid, certain conditions must hold:\nIn addition, to be complete, a computation history must be finite and\nThe definitions of \"valid initial configuration\", \"valid transition\", and \"valid terminal configuration\" vary for different kinds of formal machines.\n\nA deterministic automaton has exactly one computation history for a given initial configuration, though the history may be infinite and therefore incomplete.\n\nFor a finite state machine formula_1, a configuration is simply\nthe current state of the machine, together with the remaining input. The first configuration must be the initial state of formula_1 and the complete input. A transition from a configuration formula_3 to\na configuration formula_4 is allowed if formula_5 for\nsome input symbol formula_6 and if formula_1 has a transition from\nformula_8 to formula_9 on input formula_6. The final\nconfiguration must have the empty string formula_11 as its remaining\ninput; whether formula_1 has accepted or rejected the input depends\non whether the final state is an accepting state. \n\nComputation histories are more commonly used in reference to Turing machines. The configuration of a single-tape Turing machine consists of the contents of the tape, the position of the read/write head on the tape, and the current state of the associated state machine; this is usually written\n\nformula_13\n\nwhere formula_14 is the current state of the machine, represented in some\nway that's distinguishable from the tape language, and where formula_14 is\npositioned immediately before the position of the read/write head.\n\nConsider a Turing machine formula_1 on input formula_17. The first\nconfiguration must be formula_18, where formula_19\nis the initial state of the Turing machine. The machine's state in the final\nconfiguration must be either formula_20 (the accept state) or formula_21\n(the reject state). A configuration formula_22 is a valid successor\nto configuration formula_23 if there's a transition from the state in\nformula_23 to the state in formula_22 which manipulates the\ntape and moves the read/write head in a way that produces the result in\nformula_22.\n\nComputation histories can be used to show that certain problems for\npushdown automata are undecidable. This is because the language of\nnon-accepting computation histories of a Turing machine formula_1\non input formula_17 is a context-free language recognizable by a\nnon-deterministic pushdown automaton.\n\nWe encode a Turing computation history formula_29 as the\nstring formula_30, where formula_31\nis the encoding of configuration formula_23, as discussed above, and where\nevery other configuration is written in reverse. Before reading a particular\nconfiguration, the pushdown automaton makes a non-deterministic choice\nto either ignore the configuration or read it completely onto the stack.\n\n\nIn addition, the automaton verifies that the first configuration is the correct\ninitial configuration (if not, it accepts) and that the state of the final\nconfiguration of the history is the accept state (if not, it accepts). Since\na non-deterministic automaton accepts if there's any valid way for it to accept,\nthe automaton described here will discover if the history is not a valid\naccepting history and will accept if so, and reject if not. \n\nThis same trick cannot be used to recognize \"accepting\" computation histories\nwith an NPDA, since non-determinism could be used to skip past a test that would\notherwise fail. A linear-bounded Turing machine is sufficient to recognize\naccepting computation histories.\n\nThis result allows us to prove that formula_35, the language\nof pushdown automata which accept all input, is undecidable. Suppose\nwe have a decider for it, formula_36. For any Turing machine\nformula_1 and input formula_17, we can form the pushdown automaton\nformula_39 which accepts non-accepting computation histories for that\nmachine. formula_40 will accept if and only if there are no\naccepting computation histories for formula_1 on formula_17; this\nwould allow us to decide formula_43, which we know to be undecidable.\n"}
{"id": "19999420", "url": "https://en.wikipedia.org/wiki?curid=19999420", "title": "Container (type theory)", "text": "Container (type theory)\n\nIn type theory, containers are abstractions which permit various \"collection types\", such as lists and trees, to be represented in a uniform way. A (unary) container is defined by a type of \"shapes\" S and a type family of \"positions\" P, indexed by S. The \"extension\" of a container is a family of dependent pairs consisting of a shape (of type S) and a function from positions of that shape to the element type. Containers can be seen as canonical forms for collection types.\n\nFor lists, the shape type is the natural numbers (including zero). The corresponding position types are the types of natural numbers less than the shape, for each shape.\n\nFor trees, the shape type is the type of trees of units (that is, trees with no information in them, just structure). The corresponding position types are isomorphic to the types of valid paths from the root to particular nodes on the shape, for each shape.\n\nNote that the natural numbers are isomorphic to lists of units. In general the shape type will always be isomorphic to the original non-generic container type family (list, tree etc.) applied to unit.\n\nOne of the main motivations for introducing the notion of containers is to support generic programming in a dependently typed setting.\n\nThe extension of a container is an endofunctor. It takes a function codice_1 to\n\nformula_1\n\nThis is equivalent to the familiar codice_2 in the case of lists, and does something similar for other containers.\n\nIndexed containers (also known as dependent polynomial functors) are a generalisation of containers, which can represent a wider class of types, such as vectors (sized lists).\n\nThe element type (called the \"input type\") is indexed by shape and position, so it can vary by shape and position, and the extension (called the \"output type\") is also indexed by shape.\n\n"}
{"id": "2979145", "url": "https://en.wikipedia.org/wiki?curid=2979145", "title": "Delta neutral", "text": "Delta neutral\n\nIn finance, delta neutral describes a portfolio of related financial securities, in which the portfolio value remains unchanged when small changes occur in the value of the underlying security. Such a portfolio typically contains options and their corresponding underlying securities such that positive and negative delta components offset, resulting in the portfolio's value being relatively insensitive to changes in the value of the underlying security.\n\nA related term, delta hedging is the process of setting or keeping the delta of a portfolio as close to zero as possible. In practice, maintaining a zero delta is very complex because there are risks associated with re-hedging on large movements in the underlying stock's price, and research indicates portfolios tend to have lower cash flows if re-hedged too frequently.\n\nformula_1 The sensitivity of an option's value to a change in the underlying stock's price.\n\nformula_2 The initial value of the option.\n\nformula_3 The current value of the option.\n\nformula_4 The initial value of the underlying stock.\n\nDelta measures the sensitivity of the value of an option to changes in the price of the underlying stock assuming all other variables remain unchanged.\n\nMathematically, delta is represented as partial derivative formula_5 \nof the option's fair value with respect to the price of the underlying security.\n\nDelta is clearly a function of S, however Delta is also a function of strike price and time to expiry.\nTherefore, if a position is delta neutral (or, instantaneously delta-hedged) its instantaneous change in value, for an infinitesimal change in the value of the underlying security, will be zero; see Hedge (finance). Since \"delta\" measures the exposure of a derivative to changes in the value of the underlying, a portfolio that is delta neutral is effectively hedged. That is, its overall value will not change for small changes in the price of its underlying instrument.\n\nDelta hedging - i.e. establishing the required hedge - may be accomplished by buying or selling an amount of the underlier that corresponds to the delta of the portfolio. By adjusting the amount bought or sold on new positions, the portfolio delta can be made to sum to zero, and the portfolio is then delta neutral. See Rational pricing delta hedging.\n\nOptions market makers, or others, may form a delta neutral portfolio using related options instead of the underlying. The portfolio's delta (assuming the same underlier) is then the sum of all the individual options' deltas. This method can also be used when the underlier is difficult to trade, for instance when an underlying stock is hard to borrow and therefore cannot be sold short.\n\nThe existence of a delta neutral portfolio was shown as part of the original proof of the Black–Scholes model, the first comprehensive model to produce correct prices for some classes of options. See Black-Scholes: Derivation.\n\nFrom the Taylor expansion of the value of an option, we get the change in the value of an option, formula_6, for a change in the value of the underlier formula_7:\n\nFor any small change in the underlier, we can ignore the second-order term and use the quantity formula_11 to determine how much of the underlier to buy or sell to create a hedged portfolio. However, when the change in the value of the underlier is not small, the second-order term, formula_12, cannot be ignored: see Convexity (finance).\n\nIn practice, maintaining a delta neutral portfolio requires continuous recalculation of the position's Greeks and rebalancing of the underlier's position. Typically, this rebalancing is performed daily or weekly.\n\n"}
{"id": "571109", "url": "https://en.wikipedia.org/wiki?curid=571109", "title": "Dirichlet problem", "text": "Dirichlet problem\n\nIn mathematics, a Dirichlet problem is the problem of finding a function which solves a specified partial differential equation (PDE) in the interior of a given region that takes prescribed values on the boundary of the region.\n\nThe Dirichlet problem can be solved for many PDEs, although originally it was posed for Laplace's equation. In that case the problem can be stated as follows:\n\nThis requirement is called the Dirichlet boundary condition. The main issue is to prove the existence of a solution; uniqueness can be proved using the maximum principle.\n\nThe Dirichlet problem goes back to George Green who studied the problem on general domains with general boundary conditions in his \"Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\", published in 1828. He reduced the problem into a problem of constructing what we now call Green's functions, and argued that Green's\nfunction exists for any domain. His methods were not rigorous by today's standards, but the\nideas were highly influential in the subsequent developments.\nThe next steps in the study of the Dirichlet's problem were taken by Karl Friedrich Gauss, William Thomson (Lord Kelvin) and Peter Gustav Lejeune Dirichlet after whom the problem was named and the solution to the problem (at least for the ball) using the Poisson kernel was known to Dirichlet (judging by his 1850 paper submitted to the Prussian academy). Lord Kelvin and Dirichlet suggested a solution to the problem by a variational method based on the minimization of \"Dirichlet's energy\". According to Hans Freudenthal (in the \"Dictionary of Scientific Biography\", vol 11), Bernhard Riemann was the first mathematician who solved this variational problem based on a method which he called Dirichlet's principle. The existence of a unique solution is very plausible by the 'physical argument': any charge distribution on the boundary should, by the laws of electrostatics, determine an electrical potential as solution. However, Karl Weierstrass found a flaw in Riemann's argument, and a rigorous proof of existence was found only in 1900 by David Hilbert, using his direct method in the calculus of variations. It turns out that the existence of a solution depends delicately on the smoothness of the boundary and the prescribed data.\n\nFor a domain formula_1 having a sufficiently smooth boundary formula_2, the general solution to the Dirichlet problem is given by\n\nwhere formula_4 is the Green's function for the partial differential equation, and\n\nis the derivative of the Green's function along the inward-pointing unit normal vector formula_6. The integration is performed on the boundary, with measure formula_7. The function formula_8 is given by the unique solution to the Fredholm integral equation of the second kind,\n\nThe Green's function to be used in the above integral is one which vanishes on the boundary:\n\nfor formula_11 and formula_12. Such a Green's function is usually a sum of the free-field Green's function and a harmonic solution to the differential equation.\n\nThe Dirichlet problem for harmonic functions always has a solution, and that solution is unique, when the boundary is sufficiently smooth and formula_13 is continuous. More precisely, it has a solution when\n\nfor some formula_15, where formula_16 denotes the Hölder condition.\n\nIn some simple cases the Dirichlet problem can be solved explicitly. For example, the solution to the Dirichlet problem for the unit disk in R is given by the Poisson integral formula.\n\nIf formula_17 is a continuous function on the boundary formula_2 of the open unit disk formula_1, then the solution to the Dirichlet problem is formula_20 given by\n\nThe solution formula_22 is continuous on the closed unit disk formula_23 and harmonic on formula_24\n\nThe integrand is known as the Poisson kernel; this solution follows from the Green's function in two dimensions:\n\nwhere formula_26 is harmonic\n\nand chosen such that formula_28 for formula_29.\n\nFor bounded domains, the Dirichlet problem can be solved using the Perron method, which relies on the maximum principle for subharmonic functions. This approach is described in many text books. It is not well-suited to describing smoothness of solutions when the boundary is smooth. Another classical Hilbert space approach through Sobolev spaces does yield such information. The solution of the Dirichlet problem using Sobolev spaces for planar domains can be used to prove the smooth version of the Riemann mapping theorem. has outlined a different approach for establishing the smooth Riemann mapping theorem, based on the reproducing kernels of Szegő and Bergman, and in turn used it to solve the Dirichlet problem. The classical methods of potential theory allow the Dirichlet problem to be solved directly in terms of integral operators, for which the standard theory of compact and Fredholm operators is applicable. The same methods work equally for the Neumann problem.\nDirichlet problems are typical of elliptic partial differential equations, and potential theory, and the Laplace equation in particular. Other examples include the biharmonic equation and related equations in elasticity theory.\n\nThey are one of several types of classes of PDE problems defined by the information given at the boundary, including Neumann problems and Cauchy problems.\n\nLet us consider the Dirichlet problem for the wave equation which describes a string attached between walls with one end\nattached permanently and with the other moving with the constant velocity i.e. the d'Alembert equation\non the triangular region of the Cartesian product of the space and the time:\n\nAs one can easily check by substitution that the solution fulfilling the first condition is\n\nAdditionally we want\n\nSubstituting\n\nwe get the condition of self-similarity\n\nformula_36\n\nwhere\n\nformula_37\n\nIt is fulfilled for example by the composite function\nformula_38\n\nwith\n\nformula_39\n\nthus in general\n\nformula_40\n\nwhere formula_41 is a periodic function with a period formula_42\n\nformula_43\n\nand we get the general solution\n\n\n"}
{"id": "39378", "url": "https://en.wikipedia.org/wiki?curid=39378", "title": "Distance", "text": "Distance\n\nDistance is a numerical measurement of how far apart objects are. In physics or everyday usage, distance may refer to a physical length or an estimation based on other criteria (e.g. \"two counties over\"). In most cases, \"distance from A to B\" is interchangeable with \"distance from B to A\". In mathematics, a distance function or metric is a generalization of the concept of physical distance. A metric is a function that behaves according to a specific set of rules, and is a way of describing what it means for elements of some space to be \"close to\" or \"far away from\" each other.\n\nA physical distance can mean several different things:\n\n\n\"Circular distance\" is the distance traveled by a wheel, which can be useful when designing vehicles or mechanical gears. The circumference of the wheel is 2\"π\" × radius, and assuming the radius to be 1, then each revolution of the wheel is equivalent of the distance 2\"π\" radians. In engineering \"ω\" = 2\"πƒ\" is often used, where \"ƒ\" is the frequency.\n\nUnusual definitions of distance can be helpful to model certain physical situations, but are also used in theoretical mathematics:\n\nDistance measures in cosmology are complicated by the expansion of the universe, and by effects described by the theory of relativity such as length contraction of moving objects.\n\nThe term \"distance\" is also used by analogy to measure non-physical entities in certain ways.\n\nIn computer science, there is the notion of the \"edit distance\" between two strings. For example, the words \"dog\" and \"dot\", which vary by only one letter, are closer than \"dog\" and \"cat\", which differ by three letters. This idea is used in spell checkers and in coding theory, and is mathematically formalized in several different ways, such as:\n\nIn mathematics, a metric space is a set for which distances between all members of the set are defined. In this way, many different types of \"distances\" can be calculated, such as for traversal of graphs, comparison of distributions and curves, and using unusual definitions of \"space\" (for example using a manifold or reflections). The notion of distance in graph theory has been used to describe social networks, for example with the Erdős number or the Bacon number, the number of collaborative relationships away a person is from prolific mathematician Paul Erdős or actor Kevin Bacon, respectively.\n\nIn psychology, human geography, and the social sciences, distance is often theorized not as an objective metric, but as a subjective experience.\n\nBoth distance and displacement measure the movement of an object. Distance cannot be negative, and never decreases. Distance is a scalar quantity, or a magnitude. Whereas displacement is a vector quantity with both magnitude and direction. It can be negative, zero, or positive. Directed distance does not measure movement, it measures the separation of two points, and can be a positive, zero, or negative vector.\n\nThe distance covered by a vehicle (for example as recorded by an odometer), person, animal, or object along a curved path from a point \"A\" to a point \"B\" should be distinguished from the straight-line distance from \"A\" to \"B\". For example, whatever the distance covered during a round trip from \"A\" to \"B\" and back to \"A\", the displacement is zero as start and end points coincide. In general the straight-line distance does not equal distance travelled, except for journeys in a straight line.\n\nDirected distances can be determined along straight lines and along curved lines.\n\nDirected distances along straight lines are vectors that give the distance and direction between a starting point and an ending point. A directed distance of a point \"C\" from point \"A\" in the direction of \"B\" on a line \"AB\" in a Euclidean vector space is the distance from \"A\" to \"C\" if \"C\" falls on the ray \"AB\", but is the negative of that distance if \"C\" falls on the ray \"BA\" (I.e., if \"C\" is not on the same side of \"A\" as \"B\" is). I.e. the directed distance from the New York City Main Library flag pole to the Statue of Liberty flag pole has: \n\nAnother kind of directed distance is that between two different particles or point masses at a given time. For instance, the distance from the center of gravity of the Earth \"A\" and the center of gravity of the Moon \"B\" (which does not strictly imply motion from \"A\" to \"B\") falls into this category.\n\nA directed distance along a curved line is not a vector and is represented by a segment of that curved line defined by endpoints \"A\" and \"B\", with some specific information indicating the sense (or direction) of an ideal or real motion from one endpoint of the segment to the other (see figure). For instance, just labelling the two endpoints as \"A\" and \"B\" can indicate the sense, if the ordered sequence (\"A\", \"B\") is assumed, which implies that \"A\" is the starting point.\n\nA displacement (see above) is a special kind of directed distance defined in mechanics. A directed distance is called displacement when it is the distance along a straight line (minimum distance) from \"A\" and \"B\", and when \"A\" and \"B\" are positions occupied by the \"same particle\" at two \"different instants\" of time. This implies motion of the particle. The distance traveled by a particle must always be greater than or equal to its displacement, with equality occurring only when the particle moves along a straight path.\n\nIn analytic geometry, the distance between two points of the xy-plane can be found using the distance formula. The distance between (\"x\", \"y\") and (\"x\", \"y\") is given by:\nSimilarly, given points (\"x\", \"y\", \"z\") and (\"x\", \"y\", \"z\") in three-space, the distance between them is:\nThese formula are easily derived by constructing a right triangle with a leg on the hypotenuse of another (with the other leg orthogonal to the plane that contains the 1st triangle) and applying the Pythagorean theorem.\nIn the study of complicated geometries, we call this (most common) type of distance Euclidean distance, as it is derived from the Pythagorean theorem, which does not hold in non-Euclidean geometries. This distance formula can also be expanded into the arc-length formula.\n\nIn the Euclidean space R, the distance between two points is usually given by the Euclidean distance (2-norm distance). Other distances, based on other norms, are sometimes used instead.\n\nFor a point (\"x\", \"x\", ...,\"x\") and a point (\"y\", \"y\", ...,\"y\"), the Minkowski distance of order \"p\" (\"p\"-norm distance) is defined as:\n\"p\" need not be an integer, but it cannot be less than 1, because otherwise the triangle inequality does not hold.\n\nThe 2-norm distance is the Euclidean distance, a generalization of the Pythagorean theorem to more than two coordinates. It is what would be obtained if the distance between two points were measured with a ruler: the \"intuitive\" idea of distance.\n\nThe 1-norm distance is more colourfully called the \"taxicab norm\" or \"Manhattan distance\", because it is the distance a car would drive in a city laid out in square blocks (if there are no one-way streets).\n\nThe infinity norm distance is also called Chebyshev distance. In 2D, it is the minimum number of moves kings require to travel between two squares on a chessboard.\n\nThe \"p\"-norm is rarely used for values of \"p\" other than 1, 2, and infinity, but see super ellipse.\n\nIn physical space the Euclidean distance is in a way the most natural one, because in this case the length of a rigid body does not change with rotation.\n\nThe Euclidean distance between two points in space (formula_3 and formula_4) may be written in a variational form where the distance is the minimum value of an integral:\n\nHere formula_6 is the trajectory (path) between the two points. The value of the integral (D) represents the length of this trajectory. The distance is the minimal value of this integral and is obtained when formula_7 where formula_8 is the optimal trajectory. In the familiar Euclidean case (the above integral) this optimal trajectory is simply a straight line. It is well known that the shortest path between two points is a straight line. Straight lines can formally be obtained by solving the Euler–Lagrange equations for the above functional. In non-Euclidean manifolds (curved spaces) where the nature of the space is represented by a metric tensor formula_9 the integrand has to be modified to formula_10, where Einstein summation convention has been used.\n\nThe Euclidean distance between two objects may also be generalized to the case where the objects are no longer points but are higher-dimensional manifolds, such as space curves, so in addition to talking about distance between two points one can discuss concepts of distance between two strings. Since the new objects that are dealt with are extended objects (not points anymore) additional concepts such as non-extensibility, curvature constraints, and non-local interactions that enforce non-crossing become central to the notion of distance. The distance between the two manifolds is the scalar quantity that results from minimizing the generalized distance functional, which represents a transformation between the two manifolds:\n\nThe above double integral is the generalized distance functional between two polymer conformation. formula_12 is a spatial parameter and formula_13 is pseudo-time. This means that formula_14 is the polymer/string conformation at time formula_15 and is parameterized along the string length by formula_16. Similarly formula_17 is the trajectory of an infinitesimal segment of the string during transformation of the entire string from conformation formula_18 to conformation formula_19. The term with cofactor formula_20 is a Lagrange multiplier and its role is to ensure that the length of the polymer remains the same during the transformation. If two discrete polymers are inextensible, then the minimal-distance transformation between them no longer involves purely straight-line motion, even on a Euclidean metric. There is a potential application of such generalized distance to the problem of protein folding\nThis generalized distance is analogous to the Nambu–Goto action in string theory, however there is no exact correspondence because the Euclidean distance in 3-space is inequivalent to the spacetime distance minimized for the classical relativistic string.\n\nThis is a metric often used in computer vision that can be minimized by least squares estimation. For curves or surfaces given by the equation formula_21 (such as a conic in homogeneous coordinates), the algebraic distance from the point formula_22 to the curve is simply formula_23.\nIt may serve as an \"initial guess\" for geometric distance to refine estimations of the curve by more accurate methods, such as non-linear least squares.\n\nIn mathematics, in particular geometry, a distance function on a given set \"M\" is a function , where R denotes the set of real numbers, that satisfies the following conditions:\n\nFor example, the usual definition of distance between two real numbers \"x\" and \"y\" is: . This definition satisfies the three conditions above, and corresponds to the standard topology of the real line. But distance on a given set is a definitional choice. Another possible choice is to define: if , and 1 otherwise. This also defines a metric, but gives a completely different topology, the \"discrete topology\"; with this definition numbers cannot be arbitrarily close.\n\nVarious distance definitions are possible between objects. For example, between celestial bodies one should not confuse the surface-to-surface distance and the center-to-center distance. If the former is much less than the latter, as for a low earth orbit, the first tends to be quoted (altitude), otherwise, e.g. for the Earth–Moon distance, the latter.\n\nThere are two common definitions for the distance between two non-empty subsets of a given metric space:\n\nThe distance between a point and a set is the infimum of the distances between the point and those in the set. This corresponds to the distance, according to the first-mentioned definition above of the distance between sets, from the set containing only this point to the other set.\n\nIn terms of this, the definition of the Hausdorff distance can be simplified: it is the larger of two values, one being the supremum, for a point ranging over one set, of the distance between the point and the set, and the other value being likewise defined but with the roles of the two sets swapped.\n\nIn graph theory the distance between two vertices is the length of the shortest path between those vertices.\n\n\n\n"}
{"id": "34534043", "url": "https://en.wikipedia.org/wiki?curid=34534043", "title": "Eckhard Meinrenken", "text": "Eckhard Meinrenken\n\nEckhard Meinrenken is a Canadian mathematician specializing in Symplectic Geometry, Lie Theory, Mathematical Physics. He is a professor at the University of Toronto Department of Mathematics.\n\nHe was an invited speaker at International Congress of Mathematicians, Beijing—2002.\n\n"}
{"id": "1290005", "url": "https://en.wikipedia.org/wiki?curid=1290005", "title": "FISH (cipher)", "text": "FISH (cipher)\n\nThe FISH (FIbonacci SHrinking) stream cipher is a fast software based stream cipher using Lagged Fibonacci generators, plus a concept from the shrinking generator cipher. It was published by Siemens in 1993. FISH is quite fast in software and has a huge key length. However, in the same paper where he proposed Pike, Ross Anderson showed that FISH can be broken with just a few thousand bits of known plaintext.\n\n"}
{"id": "230193", "url": "https://en.wikipedia.org/wiki?curid=230193", "title": "Falling and rising factorials", "text": "Falling and rising factorials\n\nIn mathematics, the falling factorial (sometimes called the descending factorial, falling sequential product, or lower factorial) is defined as\n\nThe rising factorial (sometimes called the Pochhammer function, Pochhammer polynomial, ascending factorial, rising sequential product, or upper factorial) is defined as\n\nThe value of each is taken to be 1 (an empty product) when \"n\" = 0. These symbols are collectively called\nfactorial powers.\n\nThe Pochhammer symbol introduced by Leo August Pochhammer is the notation , where is a non-negative integer. Depending on the context the Pochhammer symbol may represent either the rising factorial or the falling factorial as defined above. Care needs to be taken to check which interpretation is being used in any particular article. Pochhammer himself actually used with yet another meaning, namely to denote the binomial coefficient formula_3.\n\nIn this article the symbol is used to represent the falling factorial and the symbol is used for the rising factorial. These conventions are used in combinatorics, although Knuth's underline/overline notations formula_4 are increasingly popular. In the theory of special functions (in particular the hypergeometric function) the Pochhammer symbol is used to represent the rising factorial.\nA useful list of formulas for manipulating the rising factorial in this last notation is given in .\nWhen is a positive integer, gives the number of -permutations of an -element set, or equivalently the number of injective functions from a set of size to a set of size . However, for these meanings other notations like and \"P\"(\"x,n\") are commonly used. The Pochhammer symbol serves mostly for more algebraic uses, for instance when is an indeterminate, in which case designates a particular polynomial of degree in . Feller describes as \"the number of ways to arrange \"n\" flags on \"x\" flagpoles\".\n\nThe first few rising factorials are as follows:\nThe first few falling factorials are as follows:\nThe coefficients that appear in the expansions are Stirling numbers of the first kind.\n\nThe rising and falling factorials can be used to express a binomial coefficient:\n\nThus many identities on binomial coefficients carry over to the falling and rising factorials.\n\nA rising factorial can be expressed as a falling factorial that starts from the other end,\n\nor as a falling factorial with opposite argument,\n\nThe rising and falling factorials are well defined in any unital ring, and therefore \"x\" can be taken to be, for example, a complex number, including negative integers, or a polynomial with complex coefficients, or any complex-valued function.\n\nThe rising factorial can be extended to real values of using the Gamma function provided and are real numbers that are not negative integers:\n\nand so can the falling factorial:\n\nIf denotes differentiation with respect to , one has\n\nThe Pochhammer symbol is also integral to the definition of the hypergeometric function: The hypergeometric function is defined for |\"z\"| < 1 by the power series\n\nprovided that \"c\" does not equal 0, −1, −2, ... . Note, however, that the hypergeometric function literature uses the notation formula_22 for rising factorials.\n\nThe falling factorial occurs in a formula which represents polynomials using the forward difference operator and which is formally similar to Taylor's theorem:\nIn this formula and in many other places, the falling factorial in the calculus of finite differences plays the role of in differential calculus. Note for instance the similarity of\nformula_24 to formula_25. \n\nA similar result holds for the rising factorial.\n\nThe study of analogies of this type is known as umbral calculus. A general theory covering such relations, including the falling and rising factorial functions, is given by the theory of polynomial sequences of binomial type and Sheffer sequences. Rising and falling factorials are Sheffer sequences of binomial type, as shown by the relations:\n\nwhere the coefficients are the same as the ones in the expansion of a power of a binomial (Chu–Vandermonde identity).\n\nSimilarly, the generating function of Pochhammer polynomials then amounts to the umbral exponential,\n\nas Δ(1 + \"t\") = \"t\"(1 + \"t\" ).\n\nThe falling and rising factorials are related to one another through the Lah numbers and through sums for integral powers of a variable formula_29 involving the Stirling numbers of the second kind in the following forms where formula_30:\n\nSince the falling factorials are a basis for the polynomial ring, we can re-express the product of two of them as a linear combination of falling factorials:\n\nThe coefficients of the (\"x\"), called connection coefficients, have a combinatorial interpretation as the number of ways to identify (or glue together) elements each from a set of size and a set of size .\nWe also have a connection formula for the ratio of two Pochhammer symbols given by\n\nAdditionally, we can expand generalized exponent laws and negative rising and falling powers through the following identities:\n\nFinally, duplication and multiplication formulas for the rising factorials provide the next relations:\n\nAn alternate notation for the rising factorial\n\nand for the falling factorial\n\ngoes back to A. Capelli (1893) and L. Toscano (1939), respectively. Graham, Knuth and Patashnik propose to pronounce these expressions as \" to the rising\" and \" to the falling\", respectively.\n\nOther notations for the falling factorial include , , , or . (See permutation and combination.)\n\nAn alternate notation for the rising factorial is the less common . When the notation is used for the rising factorial, the notation is typically used for the ordinary falling factorial to avoid confusion.\n\nThe Pochhammer symbol has a generalized version called the generalized Pochhammer symbol, used in multivariate analysis. There is also a \"q\"-analogue, the \"q\"-Pochhammer symbol.\n\nA generalization of the falling factorial in which a function is evaluated on a descending arithmetic sequence of integers and the values are multiplied is:\n\nwhere is the decrement and is the number of factors. The corresponding generalization of the rising factorial is\n\nThis notation unifies the rising and falling factorials, which are [\"x\"] and [\"x\"], respectively.\n\nFor any fixed arithmetic function formula_42 and symbolic parameters formula_43, related generalized factorial products of the form\n\nmay be studied from the point of view of the classes of generalized Stirling numbers of the first kind defined by the following coefficients of the powers of formula_29 in the expansions of formula_46 and then by the next corresponding triangular recurrence relation:\n\nThese coefficients satisfy a number of analogous properties to those for the Stirling numbers of the first kind as well as recurrence relations and functional equations related to the \"f-harmonic numbers\", formula_48.\n\n\n\n"}
{"id": "48171871", "url": "https://en.wikipedia.org/wiki?curid=48171871", "title": "Federico Amodeo", "text": "Federico Amodeo\n\nFederico Amodeo (8 October 1859, Avellino – 3 November 1946, Naples) was an Italian mathematician, specializing in projective geometry, and a historian of mathematics.\n\nHe received in 1883 his Ph.D. (\"laurea\") in mathematics from the University of Naples, where he became an instructor (\"libero docente\") and from 1885 to 1923 taught projective geometry. He also taught as a professor in Naples at the Istituto Tecnico \"Gianbattista Della Porta\" from 1890 to 1923, when he retired. In 1890–1891 he visited the geometers at the University of Turin.\nAs a historian, he specialized in the history of mathematics in Naples before 1860, which he explicated in a two-volume work entitled \"Vita matematica napoletana\"; volume I (1905), volume II (1924). At the University of Naples from 1905 to 1922 he taught a course on the history of mathematics.\n\nAmodeo was an invited speaker at the International Congress of Mathematicians in 1900 at Paris and again in 1908 in Rome. He was elected a member of the Accademia Pontaniana.\n\n\n"}
{"id": "11015555", "url": "https://en.wikipedia.org/wiki?curid=11015555", "title": "Finite element exterior calculus", "text": "Finite element exterior calculus\n\nFinite element exterior calculus (FEEC) is a mathematical framework that formulates finite element methods in the calculus of differential forms. Its main application has been a comprehensive theory for finite element methods in computational electromagnetism. FEEC has been developed in the early 2000s by Douglas N. Arnold, Richard S. Falk and Ragnar Winther, \namong others.\n\nFinite element exterior calculus is sometimes called as an example of a compatible discretization technique, and bears similarities with discrete exterior calculus, although they are distinct theories.\n"}
{"id": "36620043", "url": "https://en.wikipedia.org/wiki?curid=36620043", "title": "Gallai–Hasse–Roy–Vitaver theorem", "text": "Gallai–Hasse–Roy–Vitaver theorem\n\nIn graph theory, the Gallai–Hasse–Roy–Vitaver theorem is a form of duality between the colorings of the vertices of a given undirected graph and the orientations of its edges. It states that the minimum number of colors needed to properly color any graph \"G\" equals one plus the length of a longest path in an orientation of \"G\" chosen to minimize this path's length. The orientations for which the longest path has minimum length always include at least one acyclic orientation.\n\nAn alternative statement of the same theorem is that every orientation of a graph with chromatic number \"k\" contains a simple directed path with \"k\" vertices; this path can be constrained to begin at any vertex that can reach all other vertices of the oriented graph.\n\nA bipartite graph may be oriented from one side of the bipartition to the other; the longest path in this orientation has only two vertices. Conversely, if a graph is oriented without any three-vertex paths, then every vertex must either be a source (with no incoming edges) or a sink (with no outgoing edges) and the partition of the vertices into sources and sinks shows that it is bipartite.\n\nIn any orientation of a cycle graph of odd length, it is not possible for the edges to alternate in orientation all around the cycle, so some two consecutive edges must form a path with three vertices. Correspondingly, the chromatic number of an odd cycle is three.\n\nTo prove that the chromatic number is greater than or equal to the minimum number of vertices in a longest path, suppose that a given graph has a coloring with \"k\" colors, for some number \"k\". Then it may be acyclically oriented by numbering colors and by directing each edge from its lower-numbered endpoint to the higher-numbered endpoint. With this orientation, the numbers are strictly increasing along each directed path, so each path can include at most one vertex of each color, for a total of at most \"k\" vertices per path.\n\nTo prove that the chromatic number is less than or equal to the minimum number of vertices in a longest path, suppose that a given graph has an orientation with at most \"k\" vertices per simple directed path, for some number \"k\". Then the vertices of the graph may be colored with \"k\" colors by choosing a maximal acyclic subgraph of the orientation, and then coloring each vertex by the length of the longest path in the chosen subgraph that ends at that vertex. Each edge within the subgraph is oriented from a vertex with a lower number to a vertex with a higher number, and is therefore properly colored. For each edge that is not in the subgraph, there must exist a directed path within the subgraph connecting the same two vertices in the opposite direction, for otherwise the edge could have been included in the chosen subgraph; therefore, the edge is oriented from a higher number to a lower number and is again properly colored.\n\nThe proof of this theorem was used as a test case for a formalization of mathematical induction by Yuri Matiyasevich.\n\nThe theorem also has a natural interpretation in the category of directed graphs and graph homomorphisms. A homomorphism is a map from the vertices of one graph to the vertices of another that always maps edges to edges. Thus, a \"k\"-coloring of an undirected graph \"G\" may be described by a homomorphism from \"G\" to the complete graph \"K\". If the complete graph is given an orientation, it becomes a tournament, and the orientation can be lifted back across the homomorphism to give an orientation of \"G\". In particular, the coloring given by the length of the longest incoming path corresponds in this way to a homomorphism to a transitive tournament (an acyclically oriented complete graph), and every coloring can be described by a homomorphism to a transitive tournament in this way.\n\nConsidering homomorphisms in the other direction, to \"G\" instead of from \"G\", a directed graph \"G\" is acyclic and has at most \"k\" vertices in its longest path if and only if there is no homomorphism from the path graph \"P\" to \"G\".\n\nThus, the Gallai–Hasse–Roy–Vitaver theorem is equivalent to the theorem that, for every directed graph \"G\", there is a homomorphism to the \"k\"-vertex transitive tournament if and only if there is not a homomorphism from the (\"k\" + 1)-vertex path. In the case that \"G\" is acyclic this can also be seen as a form of Mirsky's theorem that the longest chain in a partially ordered set equals the minimum number of antichains into which the set may be partitioned. This statement can be generalized from paths to other directed graphs: for every polytree \"P\" there is a dual directed graph \"D\" such that, for every directed graph \"G\", there is a homomorphism from \"G\" to \"D\" if and only if there is not a homomorphism from \"P\" to \"G\".\n\nThe Gallai–Hasse–Roy–Vitaver theorem has been repeatedly rediscovered. It is named after separate publications by Tibor Gallai, Maria Hasse, B. Roy, and L. M. Vitaver. Roy credits the statement of the theorem to a conjecture in a 1958 graph theory textbook by Claude Berge.\n"}
{"id": "12169694", "url": "https://en.wikipedia.org/wiki?curid=12169694", "title": "Geometric measure theory", "text": "Geometric measure theory\n\nIn mathematics, geometric measure theory (GMT) is the study of geometric properties of sets (typically in Euclidean space) through measure theory. It allows mathematicians to extend tools from differential geometry to a much larger class of surfaces that are not necessarily smooth.\n\nGeometric measure theory was born out of the desire to solve the Plateau problem which asks if for every smooth closed curve in formula_1 there exists a surface of least area among all surfaces whose boundary equals the given curve. Such surfaces mimic soap films.\n\nThe problem had remained open since it was posed in 1760 by Lagrange. It was solved independently in the 1930s by Jesse Douglas and Tibor Radó under certain topological restrictions. In 1960 Herbert Federer and Wendell Fleming used the theory of currents with which they were able to solve the orientable Plateau's problem analytically without topological restrictions, thus sparking geometric measure theory. Later Jean Taylor after Fred Almgren proved Plateau's laws for the kind of singularities that can occur in these more general soap films and soap bubbles clusters.\n\nThe following objects are central in geometric measure theory:\n\n\nThe following theorems and concepts are also central:\n\n\nThe Brunn–Minkowski inequality for the \"n\"-dimensional volumes of convex bodies \"K\" and \"L\",\n\ncan be proved on a single page and quickly yields the classical isoperimetric inequality. The Brunn–Minkowski inequality also leads to Anderson's theorem in statistics. The proof of the Brunn–Minkowski inequality predates modern measure theory; the development of measure theory and Lebesgue integration allowed connections to be made between geometry and analysis, to the extent that in an integral form of the Brunn–Minkowski inequality known as the Prékopa–Leindler inequality the geometry seems almost entirely absent.\n\n\n\n"}
{"id": "628183", "url": "https://en.wikipedia.org/wiki?curid=628183", "title": "Goldstone boson", "text": "Goldstone boson\n\nIn particle and condensed matter physics, Goldstone bosons or Nambu–Goldstone bosons (NGBs) are bosons that appear necessarily in models exhibiting spontaneous breakdown of continuous symmetries. They were discovered by Yoichiro Nambu in the context of the BCS superconductivity mechanism, and subsequently elucidated by Jeffrey Goldstone, and systematically generalized in the context of quantum field theory.\n\nThese spinless bosons correspond to the spontaneously broken internal symmetry generators, and are characterized by the quantum numbers of these.\nThey transform \"nonlinearly\" (shift) under the action of these generators, and can thus be excited out of the asymmetric vacuum by these generators. Thus, they can be thought of as the excitations of the field in the broken symmetry directions in group space—and are massless if the spontaneously broken symmetry is \"not also broken explicitly\".\n\nIf, instead, the symmetry is not exact, i.e. if it is \"explicitly broken as well as spontaneously broken\", then the Nambu–Goldstone bosons are not massless, though they typically remain relatively light; they are then called pseudo-Goldstone bosons or pseudo-Nambu–Goldstone bosons (abbreviated PNGBs).\n\nGoldstone's theorem examines a generic continuous symmetry which is spontaneously broken; i.e., its currents are conserved, but the ground state is not invariant under the action of the corresponding charges. Then, necessarily, new massless (or light, if the symmetry is not exact) scalar particles appear in the spectrum of possible excitations. There is one scalar particle—called a Nambu–Goldstone boson—for each generator of the symmetry that is broken, i.e., that does not preserve the ground state. The Nambu–Goldstone mode is a long-wavelength fluctuation of the corresponding order parameter.\n\nBy virtue of their special properties in coupling to the vacuum of the respective symmetry-broken theory, vanishing momentum (\"soft\") Goldstone bosons involved in field-theoretic amplitudes make such amplitudes vanish (\"Adler zeros\").\n\nIn theories with gauge symmetry, the Goldstone bosons are \"eaten\" by the gauge bosons. The latter become massive and their new, longitudinal polarization is provided by the Goldstone boson.\n\n\nConsider a complex scalar field , with the constraint that , a constant. One way to impose a constraint of this sort is by including a potential interaction term in its Lagrangian density, \nand taking the limit as (this is called the \"Abelian nonlinear σ-model\". It corresponds to the where the tip and the sides shoot to infinity, preserving the location of the minimum at its base).\n\nThe constraint, and the action, below, are invariant under a \"U\"(1) phase transformation, . The field can be redefined to give a real scalar field (i.e., a spin-zero particle) without any constraint by\nwhere is the Nambu–Goldstone boson (actually is), and the \"U\"(1) symmetry transformation effects a shift on , namely \nbut does not preserve the ground state (i.e. the above infinitesimal transformation \"does not annihilate it\"—the hallmark of invariance), as evident in the charge of the current below.\n\nThus, the vacuum is degenerate and noninvariant under the action of the spontaneously broken symmetry.\n\nThe corresponding Lagrangian density is given by\nand thus \nNote that the constant term in the Lagrangian density has no physical significance, and the other term in it is simply the kinetic term for a massless scalar.\n\nThe symmetry-induced conserved \"U\"(1) current is \nThe charge, \"Q\", resulting from this current shifts and the ground state to a new, degenerate, ground state. Thus, a vacuum with will shift to a \"different vacuum\" with . The current connects the original vacuum with the Nambu–Goldstone boson state, .\n\nIn general, in a theory with several scalar fields, , the Nambu–Goldstone mode is massless, and parameterises the curve of possible (degenerate) vacuum states. Its hallmark under the broken symmetry transformation is nonvanishing vacuum expectation , an order parameter, for vanishing , at some ground state |0〉 chosen at the minimum of the potential, . Symmetry dictates that all variations of the potential with respect to the fields in all symmetry directions vanish. The vacuum value of the first order variation in any direction vanishes as just seen; while the vacuum value of the second order variation must also vanish, as follows. Vanishing vacuum values of field symmetry transformation increments add no new information.\n\nBy contrast, however, \"nonvanishing vacuum expectations of transformation increments\", , specify the relevant (Goldstone) \"null eigenvectors of the mass matrix\",\n\nand hence the corresponding zero-mass eigenvalues.\n\nThe principle behind Goldstone's argument is that the ground state is not unique. Normally, by current conservation, the charge operator for any symmetry current is time-independent,\n\nActing with the charge operator on the vacuum either \"annihilates the vacuum\", if that is symmetric; else, if \"not\", as is the case in spontaneous symmetry breaking, it produces a zero-frequency state out of it, through its shift transformation feature illustrated above. Actually, here, the charge itself is ill-defined, cf. the Fabri–Picasso argument below. \n\nBut its better behaved commutators with fields, that is, the nonvanishing transformation shifts , are, nevertheless, \"time-invariant\", \nthus generating a in its Fourier transform. (This ensures that, inserting a complete set of intermediate states in a nonvanishing current commutator can lead to vanishing time-evolution only when one or more of these states is massless.)\n\nThus, if the vacuum is not invariant under the symmetry, action of the charge operator produces a state which is different from the vacuum chosen, but which has zero frequency. This is a long-wavelength oscillation of a field which is nearly stationary: there are physical states with zero frequency, , so that the theory cannot have a mass gap.\n\nThis argument is further clarified by taking the limit carefully. If an approximate charge operator acting in a huge but finite region is applied to the vacuum,\n\na state with approximately vanishing time derivative is produced,\n\nAssuming a nonvanishing mass gap , the frequency of any state like the above, which is orthogonal to the vacuum, is at least ,\n\nLetting become large leads to a contradiction. Consequently  = 0. However this argument fails when the symmetry is gauged, because then the symmetry generator is only performing a gauge transformation. A gauge transformed state is the same exact state, so that acting with a symmetry generator does not get one out of the vacuum.\n\nThe argument requires both the vacuum and the charge to be translationally invariant, , .\n\nConsider the correlation function of the charge with itself,\nso the integrand in the right hand side does not depend on the position.\n\nThus, its value is proportional to the total space volume, formula_13 — unless the symmetry is unbroken, . Consequently, does not properly exist in the Hilbert space.\n\nThere is an arguable loophole in the theorem. If one reads the theorem carefully, it only states that there exist non-vacuum states with arbitrarily small energies. Take for example a chiral N = 1 super QCD model with a nonzero squark VEV which is conformal in the IR. The chiral symmetry is a global symmetry which is (partially) spontaneously broken. Some of the \"Goldstone bosons\" associated with this spontaneous symmetry breaking are charged under the unbroken gauge group and hence, these composite bosons have a continuous mass spectrum with arbitrarily small masses but yet there is no Goldstone boson with exactly zero mass. In other words, the Goldstone bosons are infraparticles.\n\nA version of Goldstone's theorem also applies to nonrelativistic theories (and also relativistic theories with spontaneously broken spacetime symmetries, such as Lorentz symmetry or conformal symmetry, rotational, or translational invariance).\n\nIt essentially states that, for each spontaneously broken symmetry, there corresponds some quasiparticle with no energy gap—the nonrelativistic version of the mass gap. (Note that the energy here is really and not .) However, two \"different\" spontaneously broken generators may now give rise to the \"same\" Nambu–Goldstone boson. For example, in a superfluid, both the \"U(1)\" particle number symmetry and Galilean symmetry are spontaneously broken. However, the phonon is the Goldstone boson for both.\n\nIn general, the phonon is effectively the Nambu–Goldstone boson for spontaneously broken Galilean/Lorentz symmetry. However, in contrast to the case of internal symmetry breaking, when spacetime symmetries are broken, the order parameter \"need not\" be a scalar field, but may be a tensor field, and the corresponding independent massless modes may now be \"fewer\" than the number of spontaneously broken generators, because the \nGoldstone modes may now be linearly dependent among themselves: e.g., the Goldstone modes for some generators might be expressed as gradients of Goldstone modes for other broken generators.\n\nSpontaneously broken global fermionic symmetries, which occur in some supersymmetric models, lead to Nambu–Goldstone fermions, or \"goldstinos\". These have spin ½, instead of 0, and carry all quantum numbers of the respective supersymmetry generators broken spontaneously.\n\nSpontaneous supersymmetry breaking smashes up (\"reduces\") supermultiplet structures into the characteristic nonlinear realizations of broken supersymmetry, so that goldstinos are superpartners of \"all\" particles in the theory, of \"any spin\", and the only superpartners, at that. That is, to say, two non-goldstino particles \nare connected to only goldstinos through supersymmetry transformations, and not to each other, even if they were so connected before the breaking of supersymmetry. As a result, the masses and spin multiplicities of such particles are then arbitrary.\n\n"}
{"id": "44541887", "url": "https://en.wikipedia.org/wiki?curid=44541887", "title": "Gopakumar–Vafa invariant", "text": "Gopakumar–Vafa invariant\n\nIn theoretical physics Rajesh Gopakumar and Cumrun Vafa introduced new topological invariants, which named Gopakumar–Vafa invariant, that represent the number of BPS states on Calabi–Yau 3-fold, in a series of papers. (see , and also see , .)　They lead the following formula generating function for the Gromov–Witten invariant on Calabi–Yau 3-fold \"M\".\n\nwhere formula_2 is Gromov–Witten invariant, formula_3 the number of pseudoholomorphic curves with genus \"g\" and formula_4 the number of the BPS states.\n\nGopakumar–Vafa invariants can be viewed as a partition function in topological quantum field theory.　They are proposed to be the partition function in Gopakumar–Vafa form:\n\n"}
{"id": "30556829", "url": "https://en.wikipedia.org/wiki?curid=30556829", "title": "Hadamard's maximal determinant problem", "text": "Hadamard's maximal determinant problem\n\nHadamard's maximal determinant problem, named after Jacques Hadamard, asks for the largest determinant of a matrix with elements equal to 1 or −1. The analogous question for matrices with elements equal to 0 or 1 is equivalent since, as will be shown below, the maximal determinant of a {1,−1} matrix of size \"n\" is 2 times the maximal determinant of a {0,1} matrix of size \"n\"−1. The problem was posed by Hadamard in the 1893 paper in which he presented his famous determinant bound and remains unsolved for matrices of general size. Hadamard's bound implies that {1, −1}-matrices of size \"n\" have determinant at most \"n\". Hadamard observed that a construction of Sylvester\nproduces examples of matrices that attain the bound when \"n\" is a power of 2, and produced examples of his own of sizes 12 and 20. He also showed that the bound is only attainable when \"n\" is equal to 1, 2, or a multiple of 4. Additional examples were later constructed by Scarpis and Paley and subsequently by many other authors. Such matrices are now known as Hadamard matrices. They have received intensive study.\n\nMatrix sizes \"n\" for which \"n\" ≡ 1, 2, or 3 (mod 4) have received less attention. The earliest results are due to Barba, who tightened Hadamard's bound for \"n\" odd, and Williamson, who found the largest determinants for \"n\"=3, 5, 6, and 7. Some important results include\n\nThe design of experiments in statistics makes use of {1, −1} matrices \"X\" (not necessarily square) for which the information matrix \"X\"\"X\" has maximal determinant. (The notation \"X\" denotes the transpose of \"X\".) Such matrices are known as D-optimal designs. If \"X\" is a square matrix, it is known as a saturated D-optimal design.\n\nAny two rows of an \"n\"×\"n\" Hadamard matrix are orthogonal. For a {1, −1} matrix, it means any two rows differ in exactly half of the entries, which is impossible when \"n\" is an odd number. When \"n\" ≡ 2 (mod 4), two rows that are both orthogonal to a third row cannot be orthogonal to each other. Together, these statements imply that an \"n\"×\"n\" Hadamard matrix can exist only if \"n\" = 1, 2, or a multiple of 4. Hadamard matrices have been well studied, but it is not known whether an \"n\"×\"n\" Hadamard matrix exists for every \"n\" that is a positive multiple of 4. The smallest \"n\" for which an \"n\"×\"n\" Hadamard matrix is not known to exist is 668.\n\nAny of the following operations, when performed on a {1, −1} matrix \"R\", changes the determinant of \"R\" only by a minus sign: \nTwo {1,−1} matrices, \"R\" and \"R\", are considered equivalent if \"R\" can be converted to \"R\" by some sequence of the above operations. The determinants of equivalent matrices are equal, except possibly for a sign change, and it is often convenient to standardize \"R\" by means of negations and permutations of rows and columns. A {1, −1} matrix is normalized if all elements in its first row and column equal 1. When the size of a matrix is odd, it is sometimes useful to use a different normalization in which every row and column contains an even number of elements 1 and an odd number of elements −1. Either of these normalizations can be accomplished using the first two operations.\n\nThere is a one-to-one map from the set of normalized \"n\"×\"n\" {1, −1} matrices to the set of (\"n\"−1)×(\"n\"-1) {0, 1} matrices under which the magnitude of the determinant is reduced by a factor of 2. This map consists of the following steps.\nExample:\nIn this example, the original matrix has determinant −16 and its image has determinant 2 = −16·(−2).\n\nSince the determinant of a {0, 1} matrix is an integer, the determinant of an \"n\"×\"n\" {1, −1} matrix is an integer multiple of 2.\n\nLet \"R\" be an \"n\" by \"n\" {1, −1} matrix. The Gram matrix of \"R\" is defined to be the matrix \"G\" = \"RR\". From this definition it follows that \"G\"\nNegating rows of \"R\" or applying a permutation to them results in the same negations and permutation being applied both to the rows, and to the corresponding columns, of \"G\". We may also define the matrix \"G\"′=\"R\"\"R\". The matrix \"G\" is the usual Gram matrix of a set of vectors, derived from the set of rows of \"R\", while \"G\"′ is the Gram matrix derived from the set of columns of \"R\". A matrix \"R\" for which \"G\" = \"G\"′ is a normal matrix. Every known maximal-determinant matrix is equivalent to a normal matrix, but it is not known whether this is always the case.\n\nHadamard's bound can be derived by noting that |det \"R\"| = (det \"G\") ≤ (det \"nI\") = \"n\", which is a consequence of the observation that \"nI\", where \"I\" is the \"n\" by \"n\" identity matrix, is the unique matrix of maximal determinant among matrices satisfying properties 1–4. That det \"R\" must be an integer multiple of 2 can be used to provide another demonstration that Hadamard's bound is not always attainable. When \"n\" is odd, the bound \"n\" is either non-integer or odd, and is therefore unattainable except when \"n\" = 1. When \"n\" = 2\"k\" with \"k\" odd, the highest power of 2 dividing Hadamard's bound is 2 which is less than 2 unless \"n\" = 2. Therefore, Hadamard's bound is unattainable unless \"n\" = 1, 2, or a multiple of 4.\n\nWhen \"n\" is odd, property 1 for Gram matrices can be strengthened to\nThis allows a sharper upper bound to be derived: |det \"R\"| = (det \"G\") ≤ (det (\"n\"-1)\"I\"+\"J\") = (2\"n\"−1)(\"n\"−1), where \"J\" is the all-one matrix. Here (\"n\"-1)\"I\"+\"J\" is the maximal-determinant matrix satisfying the modified property 1 and properties 2–4. It is unique up to multiplication of any set of rows and the corresponding set of columns by −1. The bound is not attainable unless 2\"n\"−1 is a perfect square, and is therefore never attainable when \"n\" ≡ 3 (mod 4).\n\nWhen \"n\" is even, the set of rows of \"R\" can be partitioned into two subsets.\nThe dot product of two rows of the same type is congruent to \"n\" (mod 4); the dot product of two rows of opposite type is congruent to \"n\"+2 (mod 4). When \"n\" ≡ 2 (mod 4), this implies that, by permuting rows of \"R\", we may assume the standard form,\nwhere \"A\" and \"D\" are symmetric integer matrices whose elements are congruent to 2 (mod 4) and \"B\" is a matrix whose elements are congruent to 0 (mod 4). In 1964, Ehlich and Wojtas independently showed that in the maximal determinant matrix of this form, \"A\" and \"D\" are both of size \"n\"/2 and equal to (\"n\"−2)\"I\"+2\"J\" while \"B\" is the zero matrix. This optimal form is unique up to multiplication of any set of rows and the corresponding set of columns by −1 and to simultaneous application of a permutation to rows and columns. This implies the bound det \"R\" ≤ (2\"n\"−2)(\"n\"−2). Ehlich showed that if \"R\" attains the bound, and if the rows and columns of \"R\" are permuted so that both \"G\" = \"RR\" and \"G\"′ = \"R\"\"R\" have the standard form and are suitably normalized, then we may write\nwhere \"W\", \"X\", \"Y\", and \"Z\" are (\"n\"/2)×(\"n\"/2) matrices with constant row and column sums \"w\", \"x\", \"y\", and \"z\" that satisfy \"z\" = −\"w\", \"y\" = \"x\", and \"w\"+\"x\" = 2\"n\"−2. Hence the Ehlich–Wojtas bound is not attainable unless 2\"n\"−2 is expressible as the sum of two squares.\n\nWhen \"n\" is odd, then by using the freedom to multiply rows by −1, one may impose the condition that each row of \"R\" contain an even number of elements 1 and an odd number of elements −1. It can be shown that, if this normalization is assumed, then property 1 of \"G\" may be strengthened to\nWhen \"n\" ≡ 1 (mod 4), the optimal form of Barba satisfies this stronger property, but when \"n\" ≡ 3 (mod 4), it does not. This means that the bound can be sharpened in the latter case. Ehlich showed that when \"n\" ≡ 3 (mod 4), the strengthened property 1 implies that the maximal-determinant form of \"G\" can be written as \"B\"−\"J\" where \"J\" is the all-one matrix and \"B\" is a block-diagonal matrix whose diagonal blocks are of the form (\"n\"-3)\"I\"+4\"J\". Moreover, he showed that in the optimal form, the number of blocks, \"s\", depends on \"n\" as shown in the table below, and that each block either has size \"r\" or size \"r+1\" where formula_4\n\nExcept for \"n\"=11 where there are two possibilities, the optimal form is unique up to multiplication of any set of rows and the corresponding set of columns by −1 and to simultaneous application of a permutation to rows and columns. This optimal form leads to the bound\nwhere \"v\" = \"n\"−\"rs\" is the number of blocks of size \"r\"+1 and \"u\" =\"s\"−\"v\" is the number of blocks of size \"r\". \nCohn analyzed the bound and determined that, apart from \"n\" = 3, it is an integer only for \"n\" = 112\"t\"±28\"t\"+7 for some positive integer \"t\". Tamura derived additional restrictions on the attainability of the bound using the Hasse-Minkowski theorem on the rational equivalence of quadratic forms, and showed that the smallest \"n\" > 3 for which Ehlich's bound is conceivably attainable is 511.\n\nThe maximal determinants of {1, −1} matrices up to size \"n\" = 21 are given in the following table. Size 22 is the smallest open case. In the table, \"D\"(\"n\") represents the maximal determinant divided by 2. Equivalently, \"D\"(\"n\") represents the maximal determinant of a {0, 1} matrix of size \"n\"−1.\n"}
{"id": "43136239", "url": "https://en.wikipedia.org/wiki?curid=43136239", "title": "Hausdorff gap", "text": "Hausdorff gap\n\nIn mathematics, a Hausdorff gap consists roughly of two collections of sequences of integers, such that there is no sequence lying between the two collections. The first example was found by . The existence of Hausdorff gaps shows that the partially ordered set of possible growth rates of sequences is not complete.\n\nLet ω be the set of all sequences of non-negative integers, and define \"f\" < \"g\" to mean lim \"g\"(\"n\") – \"f\"(\"n\") = +∞.\n\nIf \"X\" is a poset and κ and λ are cardinals, then a (κ,λ)-pregap in \"X\" is a set of elements \"f\" for α in κ and a set of elements \"g\" for β in λ such that\n\nA pregap is called a gap if it satisfies the additional condition:\n\n\nA Hausdorff gap is a (ω,ω)-gap in ω such that for every countable ordinal α and every natural number \"n\" there are only a finite number of β less than α such that for all \"k\" > \"n\" we have \"f\"(\"k\") < \"g\"(\"k\").\n\nThere are some variations of these definitions, with the ordered set ω replaced by a similar set. For example, one can redefine \"f\" < \"g\" to mean \"f\"(\"n\") < \"g\"(\"n\") for all but finitely many \"n\". Another variation introduced by is to replace ω by the set of all subsets of ω, with the order given by \"A\" < \"B\" if \"A\" has only finitely many elements not in \"B\" but \"B\" has infinitely many elements not in \"A\".\n\n"}
{"id": "8816788", "url": "https://en.wikipedia.org/wiki?curid=8816788", "title": "History of Grandi's series", "text": "History of Grandi's series\n\nGuido Grandi (1671–1742) reportedly provided a simplistic account of the series in 1703. He noticed that inserting parentheses into produced varying results: either\nor\n\nGrandi's explanation of this phenomenon became well known for its religious overtones:\n\nIn fact, the series was not an idle subject for Grandi, and he didn't think it summed to either 0 or 1. Rather, like many mathematicians to follow, he thought the true value of the series was ⁄ for a variety of reasons.\nGrandi's mathematical treatment of occurs in his 1703 book \"Quadratura circula et hyperbolae per infinitas hyperbolas geometrice exhibita\". Broadly interpreting Grandi's work, he derived through geometric reasoning connected with his investigation of the witch of Agnesi. Eighteenth-century mathematicians immediately translated and summarized his argument in analytical terms: for a generating circle with diameter \"a\", the equation of the witch \"y\" = \"a\"/(\"a\" + \"x\") has the series expansion\n\nGrandi offered a new explanation that in 1710, both in the second edition of the \"Quadratura circula\" and in a new work, \"De Infinitis infinitorum, et infinite parvorum ordinibus disquisitio geometrica\". Two brothers inherit a priceless gem from their father, whose will forbids them to sell it, so they agree that it will reside in each other's museums on alternating years. If this agreement lasts for all eternity between the brother's descendants, then the two families will each have half possession of the gem, even though it changes hands infinitely often. This argument was later criticized by Leibniz.\n\nThe parable of the gem is the first of two additions to the discussion of the corollary that Grandi added to the second edition. The second repeats the link between the series and the creation of the universe by God:\nAfter Grandi published the second edition of the \"Quadratura\", his fellow countryman Alessandro Marchetti became one of his first critics. One historian charges that Marchetti was motivated more by jealousy than any other reason. Marchetti found the claim that an infinite number of zeros could add up to a finite quantity absurd, and he inferred from Grandi's treatment the danger posed by theological reasoning. The two mathematicians began attacking each other in a series of open letters; their debate was ended only by Marchetti's death in 1714.\n\nWith the help and encouragement of Antonio Magliabechi, Grandi sent a copy of the 1703 \"Quadratura\" to Leibniz, along with a letter expressing compliments and admiration for the master's work. Leibniz received and read this first edition in 1705, and he called it an unoriginal and less-advanced \"attempt\" at his calculus. Grandi's treatment of 1 − 1 + 1 − 1 + · · · would not catch Leibniz's attention until 1711, near the end of his life, when Christian Wolff sent him a letter on Marchetti's behalf describing the problem and asking for Leibniz's opinion.\n\nAs early as 1674, in a minor, lesser-known writing \"De Triangulo Harmonico\" on the harmonic triangle, Leibniz mentioned very briefly in an example:\n\nPresumably he arrived at this series by repeated substitution:\n\nThe series also appears indirectly in a discussion with Tschirnhaus in 1676.\n\nLeibniz had already considered the divergent alternating series as early as 1673. In that case he argued that by subtracting either on the left or on the right, one could produce either positive or negative infinity, and therefore both answers are wrong and the whole should be finite. Two years after that, Leibniz formulated the first convergence test in the history of mathematics, the alternating series test, in which he implicitly applied the modern definition of convergence.\n\nIn the 1710s, Leibniz described Grandi's series in his correspondence with several other mathematicians. The letter with the most lasting impact was his first reply to Wolff, which he published in the \"Acta Eruditorum\". In this letter, Leibniz attacked the problem from several angles.\n\nIn general, Leibniz believed that the algorithms of calculus were a form of \"blind reasoning\" that ultimately had to be founded upon geometrical interpretations. Therefore, he agreed with Grandi that claiming that the relation was well-founded because there existed a geometric demonstration.\n\nOn the other hand, Leibniz sharply criticized Grandi's example of the shared gem, claiming that the series has no relation to the story. He pointed out that for any finite, even number of years, the brothers have equal possession, yet the sum of the corresponding terms of the series is zero.\n\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all \"x\" less than 1, it should hold for \"x\" equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as \"f\"(1) of some function's power series was thought to be the most natural strategy.\n\nLeibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0:\nTaking an odd number of terms, the last term is +1 and the sum is 1:\n\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is \n\nEli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"\n\nCharles Moore muses that Leibniz would hardly have had such confidence in his metaphysical strategy if it did not give the same result (namely ⁄) as other approaches. Mathematically, this was no accident: Leibniz's treatment would be partially justified when the compatibility of averaging techniques and power series was finally proven in 1880.\n\nWhen he had first raised the question of Grandi's series to Leibniz, Wolff was inclined toward skepticism along with Marchetti. Upon reading Leibniz's reply in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. For one, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series.\n\nLeibniz described Grandi's series along with the general problem of convergence and divergence in letters to Nicolaus I Bernoulli in 1712 and early 1713. J. Dutka suggests that this correspondence, along with Nicolaus I Bernoulli's interest in probability, motivated him to formulate the St. Petersburg paradox, another situation involving a divergent series, in September 1713.\n\nAccording to Pierre-Simon Laplace in his \"Essai Philosophique sur les Probabilités\", Grandi's series was connected with Leibniz seeing \"an image of the Creation in his binary arithmetic\", and thus Leibniz wrote a letter to Jesuit missionary Claudio Filippo Grimaldi, court mathematician in China, in the hope that Claudio Filippo Grimaldi's interest in science and the mathematical \"emblem of creation\" might combine to convert the nation to Christianity. Laplace remarks, \"I record this anecdote only to show how far the prejudices of infancy may mislead the greatest men.\"\n\nJacob Bernoulli (1654–1705) dealt with a similar series in 1696 in the third part of his \"Positiones arithmeticae de seriebus infinitis\". Applying Nicholas Mercator's method for polynomial long division to the ratio , he noticed that one always had a remainder. If then this remainder decreases and \"finally is less than any given quantity\", and one has\nIf \"m\" = \"n\", then this equation becomes\nBernoulli called this equation a \"not inelegant paradox\".\n\nPierre Varignon (1654–1722) treated Grandi's series in his report \"Précautions à prendre dans l'usage des Suites ou Series infinies résultantes…\". The first of his purposes for this paper was to point out the divergence of Grandi's series and expand on Jacob Bernoulli's 1696 treatment.\n\nThe final version of Varignon's paper is dated February 16, 1715, and it appeared in a volume of the \"Mémories\" of the French Academy of Sciences that was itself not published until 1718. For such a relatively late treatment of Grandi's series, it is surprising that Varignon's report does not even mention Leibniz's earlier work. But most of the \"Précautions\" was written in October 1712, while Varignon was away from Paris. The Abbé Poignard's 1704 book on magic squares, \"Traité des Quarrés sublimes\", had become a popular subject around the Academy, and the second revised and expanded edition weighed in at 336 pages. To make the time to read the \"Traité\", Varignon had to escape to the countryside for nearly two months, where he wrote on the topic of Grandi's series in relative isolation. Upon returning to Paris and checking in at the Academy, Varignon soon discovered that the great Leibniz had ruled in favor of Grandi. Having been separated from his sources, Varignon still had to revise his paper by looking up and including the citation to Jacob Bernoulli. Rather than also take Leibniz's work into account, Varignon explains in a postscript to his report that the citation was the only revision he had made in Paris, and that \"if\" other research on the topic arose, his thoughts on it would have to wait for a future report.\n\nIn the 1751 \"Encyclopédie\", Jean le Rond d'Alembert echoes the view that Grandi's reasoning based on division had been refuted by Varignon in 1715. (Actually, d'Alembert attributes the problem to \"Guido Ubaldus\", an error that is still occasionally propagated today.)\n\nIn a 1715 letter to Jacopo Riccati, Leibniz mentioned the question of Grandi's series and advertised his own solution in the \"Acta Eruditorum\". Later, Riccati would criticize Grandi's argument in his 1754 \"Saggio intorno al sistema dell'universo\", saying that it causes contradictions. He argues that one could just as well write but that this series has \"the same quantity of zeroes\" as Grandi's series. These zeroes lack any evanescent character of \"n\", as Riccati points out that the equality is guaranteed by He concludes that the fundamental mistake is in using a divergent series to begin with:\nAnother 1754 publication also criticized Grandi's series on the basis of its collapse to 0. Louis Antoine de Bougainville briefly treats the series in his acclaimed 1754 textbook \"Traité du calcul intégral\". He explains that a series is \"true\" if its sum is equal to the expression from which is expanded; otherwise it is \"false\". Thus Grandi's series is false because and yet .\n\nLeonhard Euler treats along with other divergent series in his \"De seriebus divergentibus\", a 1746 paper that was read to the Academy in 1754 and published in 1760. He identifies the series as being first considered by Leibniz, and he reviews Leibniz's 1713 argument based on the series , calling it \"fairly sound reasoning\", and he also mentions the even/odd median argument. Euler writes that the usual objection to the use of is that it does not equal unless \"a\" is less than 1; otherwise all one can say is that\nwhere the last remainder term does not vanish and cannot be disregarded as \"n\" is taken to infinity. Still writing in the third person, Euler mentions a possible rebuttal to the objection: essentially, since an infinite series has no last term, there is no place for the remainder and it should be neglected. After reviewing more badly divergent series like , where he judges his opponents to have firmer support, Euler seeks to define away the issue:\n\nEuler also used finite differences to attack . In modern terminology, he took the Euler transform of the sequence and found that it equalled ⁄. As late as 1864, De Morgan claims that \"this transformation has always appeared one of the strongest presumptions in favour of being ⁄.\"\n\nDespite the confident tone of his papers, Euler expressed doubt over divergent series in his correspondence with Nicolaus I Bernoulli. Euler claimed that his attempted definition had never failed him, but Bernoulli pointed out a clear weakness: it does not specify how one should determine \"the\" finite expression that generates a given infinite series. Not only is this a practical difficulty, it would be theoretically fatal if a series were generated by expanding two expressions with different values. Euler's treatment of rests upon his firm belief that ⁄ is the only possible value of the series; what if there were another?\n\nIn a 1745 letter to Christian Goldbach, Euler claimed that he was not aware of any such counterexample, and in any case Bernoulli had not provided one. Several decades later, when Jean-Charles Callet finally asserted a counterexample, it was aimed at . The background of the new idea begins with Daniel Bernoulli in 1771.\n\nDaniel Bernoulli, who accepted the probabilistic argument that , noticed that by inserting 0s into the series in the right places, it could achieve any value between 0 and 1. In particular, the argument suggested that\nIn a memorandum sent to Joseph Louis Lagrange toward the end of the century, Callet pointed out that could also be obtained from the series\nsubstituting \"x\" = 1 now suggests a value of ⁄, not ⁄.\nLagrange approved Callet's submission for publication in the \"Mémoires\" of the French Academy of Sciences, but it was never directly published. Instead, Lagrange (along with Charles Bossut) summarized Callet's work and responded to it in the \"Mémoires\" of 1799. He defended Euler by suggesting that Callet's series actually should be written with the 0 terms left in:\nwhich reduces to\ninstead.\nThe 19th century is remembered as the approximate period of Cauchy's and Abel's largely successful ban on the use of divergent series, but Grandi's series continued to make occasional appearances. Some mathematicians did not follow Abel's lead, mostly outside France, and British mathematicians especially took \"a long time\" to understand the analysis coming from the continent.\n\nIn 1803, Robert Woodhouse proposed that summed to something called\nwhich could be distinguished from ⁄. Ivor Grattan-Guinness remarks on this proposal, \"… R. Woodhouse … wrote with admirable honesty on the problems which he failed to understand. … Of course, there is no harm in defining new symbols such as ⁄; but the idea is 'formalist' in the unflattering sense, and it does not bear on the problem of the convergence of series.\"\n\nIn 1830, a mathematician identified only as \"M. R. S.\" wrote in the \"Annales de Gergonne\" on a technique to numerically find fixed points of functions of one variable. If one can transform a problem into the form of an equation \"x = A + f(x)\", where \"A\" can be chosen at will, then \nshould be a solution, and truncating this infinite expression results in a sequence of approximations. Conversely, given the series , the author recovers the equation\nto which the solution is (⁄)\"a\".\n\nM. R. S. notes that the approximations in this case are \"a\", 0, \"a\", 0, …, but there is no need for Leibniz's \"subtle reasoning\". Moreover, the argument for averaging the approximations is problematic in a wider context. For equations not of the form \"x = A + f(x)\", M. R. S.'s solutions are continued fractions, continued radicals, and other infinite expressions. In particular, the expression should be a solution of the equation . Here, M. R. S. writes that based on Leibniz's reasoning, one is tempted to conclude that \"x\" is the average of the truncations \"a\", 1, \"a\", 1, …. This average is , but the solution to the equation is the square root of \"a\".\n\nBernard Bolzano criticized M. R. S.' algebraic solution of the series. In reference to the step\nBolzano charged,\n\nThis comment exemplifies Bolzano's intuitively appealing but deeply problematic views on infinity. In his defense, Cantor himself pointed out that Bolzano worked in a time when the concept of the cardinality of a set was absent.\n\nAs late as 1844, Augustus De Morgan commented that if a single instance where did not equal ⁄ could be given, he would be willing to reject the entire theory of trigonometric series.\n\nThe same volume contains papers by Samuel Earnshaw and J R Young dealing in part with . G. H. Hardy dismisses both of these as \"little more than nonsense\", in contrast to De Morgan's \"remarkable mixture of acuteness and confusion\"; in any case, Earnshaw got De Morgan's attention with the following remarks:\n\nDe Morgan fired back in 1864 in the same journal:\nThe last scholarly article to be motivated by 1 − 1 + 1 − 1 + · · · might be identified as the first article in the modern history of divergent series. Georg Frobenius published an article titled \"Ueber die Leibnitzsche Reihe\" (\"On Leibniz's series\") in 1880. He had found Leibniz's old letter to Wolff, citing it along with an 1836 article by Joseph Ludwig Raabe, who in turn drew on ideas by Leibniz and Daniel Bernoulli.\n\nFrobenius' short paper, barely two pages, begins by quoting from Leibniz's treatment of 1 − 1 + 1 − 1 + · · ·. He infers that Leibniz was actually stating a generalization of Abel's Theorem. The result, now known as Frobenius' theorem, has a simple statement in modern terms: any series that is Cesàro summable is also Abel summable to the same sum. Historian Giovanni Ferraro emphasizes that Frobenius did not actually state the theorem in such terms, and Leibniz did not state it at all. Leibniz was defending the association of the divergent series with the value ⁄, while Frobenius' theorem is stated in terms of convergent sequences and the epsilon-delta formulation of the limit of a function.\n\nFrobenius' theorem was soon followed with further generalizations by Otto Hölder and Thomas Joannes Stieltjes in 1882. Again, to a modern reader their work strongly suggests new definitions of the sum of a divergent series, but those authors did not yet make that step. Ernesto Cesàro proposed a systematic definition for the first time in 1890. Since then, mathematicians have explored many different summability methods for divergent series. Most of these, especially the simpler ones with historical parallels, sum Grandi's series to ⁄. Others, motivated by Daniel Bernoulli's work, sum the series to another value, and a few do not sum it at all.\n\nThe full texts of many of the following references are publicly available on the Internet from Google Books; the Euler archive at Dartmouth College; DigiZeitschriften, a service of Deutsche Forschungsgemeinschaft; or Gallica, a service of the Bibliothèque nationale de France.\n\n\n"}
{"id": "1531409", "url": "https://en.wikipedia.org/wiki?curid=1531409", "title": "Homeomorphism group", "text": "Homeomorphism group\n\nIn mathematics, particularly topology, the homeomorphism group of a topological space is the group consisting of all homeomorphisms from the space to itself with function composition as the group operation. Homeomorphism groups are very important in the theory of topological spaces and in general are examples of automorphism groups. Homeomorphism groups are topological invariants in the sense that the homeomorphism groups of homeomorphic topological spaces are isomorphic as groups.\n\nThere is a natural group action of the homeomorphism group of a space on that space. Let formula_1 be a topological space and denote the homeomorphism group of formula_1 by formula_3. The action is defined as follows:\n\nformula_4\n\nThis is a group action since for all formula_5, \n\nformula_6\n\nwhere formula_7 denotes the group action, and the identity element of formula_3 (which is the identity function on formula_1) sends points to themselves. If this action is transitive, then the space is said to be homogeneous.\n\nAs with other sets of maps between topological spaces, the homeomorphism group can be given a topology, such as the compact-open topology.\nIn the case of regular, locally compact spaces the group multiplication is then continuous.\n\nIf the space is compact and Hausdorff, the inversion is continuous as well and formula_10 becomes a topological group as one can easily show. \nIf formula_1 is Hausdorff, locally compact and locally connected this holds as well. \nHowever there are locally compact separable metric spaces for which the inversion map is not continuous and formula_10 therefore not a topological group.\n\nIn the category of topological spaces with homeomorphisms, object groups are exactly homeomorphism groups.\n\nIn geometric topology especially, one considers the quotient group obtained by quotienting out by isotopy, called the mapping class group:\nThe MCG can also be interpreted as the 0th homotopy group, formula_14.\nThis yields the short exact sequence:\n\nIn some applications, particularly surfaces, the homeomorphism group is studied via this short exact sequence, and by first studying the mapping class group and group of isotopically trivial homeomorphisms, and then (at times) the extension.\n\n"}
{"id": "41444886", "url": "https://en.wikipedia.org/wiki?curid=41444886", "title": "Homotopy group with coefficients", "text": "Homotopy group with coefficients\n\nIn topology, a branch of mathematics, for formula_1, the \"i\"-th homotopy group with coefficients in an abelian group \"G\" of a based space \"X\" is the pointed set of homotopy classes of based maps from the Moore space of type formula_2 to \"X\", and is denoted by formula_3. For formula_4, formula_3 is a group. Note formula_6 are the usual homotopy groups of \"X\".\n\n"}
{"id": "8177257", "url": "https://en.wikipedia.org/wiki?curid=8177257", "title": "Interaction nets", "text": "Interaction nets\n\nInteraction nets are a graphical model of computation devised by Yves Lafont in 1990 as a generalisation of the proof structures of linear logic. An interaction net system is specified by a set of agent types and a set of interaction rules. Interaction nets are an inherently distributed model of computation in the sense that computations can take place simultaneously in many parts of an interaction net, and no synchronisation is needed. The latter is guaranteed by the strong confluence property of reduction in this model of computation. Thus interaction nets provide a natural language for massive parallelism. Interaction nets are at the heart of many implementations of the lambda calculus, such as efficient closed reduction and optimal, in Lévy's sense, Lambdascope.\n\nInteractions nets are graph-like structures consisting of \"agents\" and \"edges\".\n\nAn agent of type formula_1 and with \"arity\" formula_2 has one \"principal port\" and formula_3 \"auxiliary ports\". Any port can be connected to at most one edge. Ports that are not connected to any edge are called \"free ports\". Free ports together form the \"interface\" of an interaction net. All agent types belong to a set formula_4 called \"signature\".\n\nAn interaction net that consists solely of edges is called a \"wiring\" and usually denoted as formula_5. A \"tree\" formula_6 with its \"root\" formula_7 is inductively defined either as an edge formula_7, or as an agent formula_1 with its free principal port formula_7 and its auxiliary ports formula_11 connected to the roots of other trees formula_12.\n\nGraphically, the primitive structures of interaction nets can be represented as follows:\n\nWhen two agents are connected to each other with their principal ports, they form an \"active pair\". For\nactive pairs one can introduce \"interaction rules\" which describe how the active pair rewrites to another interaction\nnet. An interaction net with no active pairs is said to be in \"normal form\". A signature formula_4 (with formula_14 defined on it) along with a set of interaction rules defined for agents formula_15 together constitute an \"interaction system\".\n\nTextual representation of interaction nets is called the \"interaction calculus\" and can be seen as a programming language.\n\nInductively defined trees correspond to \"terms\" formula_16 in the interaction calculus, where formula_7 is called a \"name\".\n\nAny interaction net formula_18 can be redrawn using the previously defined wiring and tree primitives as follows:\n\nwhich in the interaction calculus corresponds to a \"configuration\"\n\nformula_19,\n\nwhere formula_12, formula_21, and formula_22 are arbitrary terms. The ordered sequence formula_23 in the left-hand side is called an \"interface\", while the right-hand side contains an unordered multiset of \"equations\" formula_24. Wiring formula_5 translates to names, and each name has to occur exactly twice in a configuration.\n\nJust like in the formula_26-calculus, the interaction calculus has the notions of \"formula_1-conversion\" and \"substitution\" naturally defined on configurations. Specifically, both occurrences of any name can be replaced with a\nnew name if the latter does not occur in a given configuration. Configurations are considered equivalent up to formula_1-conversion. In turn, substitution formula_29 is the result of replacing the name formula_7 in a term formula_6 with another term formula_32 if formula_7 has exactly one occurrence in the term formula_6.\n\nAny interaction rule can be graphically represented as follows:\n\nwhere formula_35, and the interaction net formula_18 on the right-hand side is redrawn using the wiring and tree primitives in order to translate into the interaction calculus as formula_37 using Lafont's notation.\n\nThe interaction calculus defines reduction on configurations in more details than seen from graph\nrewriting defined on interaction nets. Namely, if formula_38, the following reduction:\n\nformula_39\n\nis called \"interaction\". When one of equations has the form of formula_40, \"indirection\" can be applied resulting\nin substitution of the other occurrence of the name formula_7 in some term formula_6:\n\nformula_43\nor\nformula_44.\n\nAn equation formula_45 is called a \"deadlock\" if formula_7 has occurrence in term formula_6. Generally only deadlock-free interaction nets are considered. Together, interaction and indirection define the reduction relation on configurations. The fact that configuration formula_48 reduces to its \"normal form\" formula_49 with no equations left is denoted as formula_50.\n\nInteraction nets benefit from the following properties:\n\n\nThese properties together allow massive parallelism.\n\nOne of the simplest interaction systems that can simulate any other interaction system is that of \"interaction combinators\". Its signature is formula_56 with formula_57 and formula_58. Interaction rules for these agents are:\n\n\nGraphically, the erasing and duplication rules can be represented as follows:\n\nwith an example of a non-terminating interaction net that reduces to itself. Its infinite reduction sequence starting from the corresponding configuration in the interaction calculus is as follows:\n\nformula_63\n\nInteraction nets are essentially deterministic and cannot model non-deterministic computations directly. In order to express non-deterministic choice, interaction nets need to be extended. In fact, it is sufficient to introduce just one agent formula_64 with two principal ports and the following interaction rules:\n\nThis distinguished agent represents ambiguous choice and can be used to simulate any other agent with arbitrary number of principal ports. For instance, it allows to define a formula_65 boolean operation that returns true if any of its arguments is true, independently of the computation taking place in the other arguments.\n\n\n\n"}
{"id": "51935121", "url": "https://en.wikipedia.org/wiki?curid=51935121", "title": "International Conference on Applications and Theory of Petri Nets and Concurrency", "text": "International Conference on Applications and Theory of Petri Nets and Concurrency\n\nPetri Nets, the International Conference on Applications and Theory of Petri Nets and Concurrency is an academic conference organized annually by the Petri net community. The conference was first organized in 1980 Strasbourg, France\n\nSince then the conference has been organized annually. The Petri Nets Steering Committee is responsible for the conference, including selection of organisers, PC members, invited speakers, tutorials and workshops, etc.\n\n\n\n"}
{"id": "17131740", "url": "https://en.wikipedia.org/wiki?curid=17131740", "title": "International Society of Dynamic Games", "text": "International Society of Dynamic Games\n\nThe International Society of Dynamic Games (ISDG) is an international non-profit, professional organization for the advancement of the theory of dynamic games.\n\nThe ISDG was founded on August 9, 1990 in Helsinki, Finland, at the site of the 4th International Symposium on Dynamic games and Applications in the Helsinki University of Technology. ISDG is governed by an executive board chaired by a president. The first president of the society was professor Tamer Başar. In past years the presidents of ISDG were\n\n\n\nThe executive board of the International Society of Dynamic Games decided in 2003 to establish a prize to recognize the \"outstanding contribution to the theory and applications of dynamic games\" of two scholars at each of its symposium, starting in 2004. The prize was named after Rufus Isaacs, the acknowledged founding father of differential games. The recipients of this prize are:\n\n"}
{"id": "55756466", "url": "https://en.wikipedia.org/wiki?curid=55756466", "title": "Judy Green (mathematician)", "text": "Judy Green (mathematician)\n\nJudith (Judy) Green is an American logician and historian of mathematics who studies women in mathematics. She is a founding member of the Association for Women in Mathematics; she has also served as its vice president, and as the vice president of the American Association of University Professors.\n\nGreen earned her bachelor's degree at Cornell University.\nThe completed a master's degree at Yale University,\nand a Ph.D. at the University of Maryland, College Park.\nHer dissertation, supervised by Carol Karp and finished in 1972, was\n\"Consistency Properties for Uncountable Finite-Quantifier Languages\".\nShe belonged to the faculty of Rutgers University before moving to Marymount University in 1989. After retiring from Marymount in 2007, she became a volunteer at the National Museum of American History.\n\nWith Jeanne LaDuke, she wrote \"Pioneering Women in American Mathematics: The Pre-1940 PhD’s\" (American Mathematical Society and London Mathematical Society, 2009). a biographical study of the first women in the U.S. to earn doctorates in mathematics.\n\nShe is part of the 2019 class of fellows of the Association for Women in Mathematics.\n"}
{"id": "2186050", "url": "https://en.wikipedia.org/wiki?curid=2186050", "title": "Karl Georg Christian von Staudt", "text": "Karl Georg Christian von Staudt\n\nKarl Georg Christian von Staudt (24 January 1798 – 1 June 1867) was a German mathematician who used synthetic geometry to provide a foundation for arithmetic.\n\nKarl was born in the Free Imperial City of Rothenburg, which is now called Rothenburg ob der Tauber in Germany. From 1814 he studied in Gymnasium in Ausbach. He attended the University of Göttingen from 1818 to 1822 where he studied with Gauss who was director of the observatory. Staudt provided an ephemeris for the orbits of Mars and the asteroid Pallas. When in 1821 Comet Nicollet-Pons was observed, he provided the elements of its orbit. These accomplishments in astronomy earned him his doctorate from University of Erlangen in 1822.\n\nStaudt's professional career began as a secondary school instructor in Würzburg until 1827 and then Nuremberg until 1835. He married Jeanette Dreschler in 1832. They had a son Eduard and daughter Mathilda, but Jeanette died in 1848.\n\nThe book \"Geometrie der Lage\" (1847) was a landmark in projective geometry. As Burau (1976) wrote:\nFurthermore, this book (page 43) uses the complete quadrangle to \"construct the fourth harmonic associated with three points on a straight line\", the projective harmonic conjugate.\n\nIndeed, in 1889 Mario Pieri translated von Staudt, before writing his \"I Principii della Geometrie di Posizione Composti in un Systema Logico-deduttivo\" (1898). In 1900 Charlotte Scott of Bryn Mawr College paraphased much of von Staudt's work in English for \"The Mathematical Gazette\".\nWhen Wilhelm Blaschke published his textbook \"Projective Geometry\" in 1948, a portrait of the young Karl was placed opposite the \"Vorwort\".\n\nStaudt went beyond real projective geometry and into complex projective space in his three volumes of \"Beiträge zur Geometrie der Lage\" published from 1856 to 1860.\n\nIn 1922 H. F. Baker wrote of von Staudt's work:\n\nVon Staudt is also remembered for his view of conic sections and the relation of pole and polar:\n\nIn 1857, in the second \"Beiträge\", von Staudt contributed a route to number through geometry called the algebra of throws (). It is based on projective range and the relation of projective harmonic conjugates. Through operations of addition of points and multiplication of points, one obtains an \"algebra of points\", as in chapter 6 of Veblen & Young's textbook on projective geometry. The usual presentation relies on cross ratio (\"CA,BD\") of four collinear points. For instance, Coolidge wrote:\n\nA summary statement is given by Veblen & Young as Theorem 10: \"The set of points on a line, with formula_3 removed, forms a field with respect to the operations previously defined\". As Freudenthal notes\n\nAnother affirmation of von Staudt's work with the harmonic conjugates comes in the form of a theorem:\n\nThe algebra of throws was described as \"projective arithmetic\" in \"The Four Pillars of Geometry\" (2005).\nIn a section called \"Projective arithmetic\", he says\n\nIf one interprets von Staudt’s work as a construction of the real numbers, then it is incomplete. One of the required properties is that a bounded sequence has a cluster point. As Hans Freudenthal observed:\n\nOne of the Italian mathematicians was Giovanni Vailati who studied the circular order property of the real projective line. The science of this order requires a quaternary relation called the separation relation. Using this relation, the concepts of monotone sequence and limit can be addressed, in a cyclic \"line\". Assuming that every monotone sequence has a limit, the line becomes a complete space. These developments were inspired by von Staudt’s deductions of field axioms as an initiative in the derivation of properties of ℝ from axioms in projective geometry.\n\nThe following links are to Cornell University Historical Mathematical Monographs:\n\n\n"}
{"id": "51856036", "url": "https://en.wikipedia.org/wiki?curid=51856036", "title": "Khinchin's theorem on the factorization of distributions", "text": "Khinchin's theorem on the factorization of distributions\n\nKhinchin's theorem on the factorization of distributions says that every probability distribution \"P\" admits (in the convolution semi-group of probability distributions) a factorization\n\nwhere \"P\" is a probability distribution without any indecomposable factor and \"P\" is a distribution that is either degenerate or is representable as the convolution of a finite or countable set of indecomposable distributions. The factorization is not unique, in general.\n\nThe theorem was proved by A. Ya. Khinchin for distributions on the line, and later it became clear that it is valid for distributions on considerably more general groups. A broad class (see) of topological semi-groups is known, including the convolution semi-group of distributions on the line, in which factorization theorems analogous to Khinchin's theorem are valid.\n"}
{"id": "24186720", "url": "https://en.wikipedia.org/wiki?curid=24186720", "title": "LCF notation", "text": "LCF notation\n\nIn combinatorial mathematics, LCF notation or LCF code is a notation devised by Joshua Lederberg, and extended by H. S. M. Coxeter and Robert Frucht, for the representation of cubic graphs that contain a Hamiltonian cycle. The cycle itself includes two out of the three adjacencies for each vertex, and the LCF notation specifies how far along the cycle each vertex's third neighbor is. A single graph may have multiple different representations in LCF notation.\n\nIn a Hamiltonian graph, the vertices can be arranged in a cycle, which accounts for two edges per vertex. The third edge from each vertex can then be described by how many positions clockwise (positive) or counter-clockwise (negative) it leads. The basic form of the LCF notation is just the sequence of these numbers of positions, starting from an arbitrarily chosen vertex and written in square brackets.\nThe numbers between the brackets are interpreted modulo \"N\", where \"N\" is the number of vertices. Entries congruent modulo \"N\" to 0, 1, or \"N\" − 1 do not appear in this sequence of numbers, because they would correspond either to a loop or multiple adjacency, neither of which are permitted in simple graphs.\n\nOften the pattern repeats, and the number of repetitions can be indicated by a superscript in the notation. For example, the Nauru graph, shown on the right, has four repetitions of the same six offsets, and can be represented by the LCF notation [5, −9, 7, −7, 9, −5]. A single graph may have multiple different LCF notations, depending on the choices of Hamiltonian cycle and starting vertex.\n\nLCF notation is useful in publishing concise descriptions of Hamiltonian cubic graphs, such as the examples below. In addition, some software packages for manipulating graphs include utilities for creating a graph from its LCF notation.\n\nIf a graph is represented by LCF notation, it is straightforward to test whether the graph is bipartite: this is true if and only if all of the offsets in the LCF notation are odd.\n\nA more complex extended version of LCF notation was provided by Coxeter, Frucht, and Powers in later work. In particular, they introduced an \"anti-palindromic\" notation: if the second half of the numbers between the square brackets was the reverse of the first half, but with all the signs changed, then it was replaced by a semicolon and a dash. The Nauru graph satisfies this condition with [5, −9, 7, −7, 9, −5], and so can be written [5, −9, 7; −] in the extended notation.\n\n"}
{"id": "3980340", "url": "https://en.wikipedia.org/wiki?curid=3980340", "title": "Longest repeated substring problem", "text": "Longest repeated substring problem\n\nIn computer science, the longest repeated substring problem is the problem of finding the longest substring of a string that occurs at least twice. \n\nThis problem can be solved in linear time and space formula_1 by building a suffix tree for the string (with a special end-of-string symbol like '$' appended), and finding the deepest internal node in the tree. Depth is measured by the number of characters traversed from the root. The string spelled by the edges from the root to such a node is a longest repeated substring. The problem of finding the longest substring with at least formula_2 occurrences can be solved by first preprocessing the tree to count the number of leaf descendants for each internal node, and then finding the deepest node with at least formula_2 leaf descendants that have no children. To avoid overlapping repeats, you can check that the list of suffix lengths has no consecutive elements with less than prefix-length difference.\n\nIn the figure with the string \"ATCGATCGA$\", the longest substring that repeats at least twice is \"ATCGA\".\n\n"}
{"id": "23722890", "url": "https://en.wikipedia.org/wiki?curid=23722890", "title": "Loopless algorithm", "text": "Loopless algorithm\n\nIn computational combinatorics, a loopless algorithm or loopless imperative algorithm is an imperative algorithm that generates successive combinatorial objects, such as partitions, permutations, and combinations, in constant time and the first object in linear time. The objects must be immediately available in simple form without requiring any additional steps. \n\nA loopless functional algorithm is a functional algorithm that takes the form \"unfoldr step • prolog\" where \"step\" takes constant time and \"prolog\" takes linear time in the size of the input. The standard function \"unfoldr\" is a right-associative Bird unfold.\n"}
{"id": "10790676", "url": "https://en.wikipedia.org/wiki?curid=10790676", "title": "Modulated complex lapped transform", "text": "Modulated complex lapped transform\n\nThe modulated complex lapped transform (MCLT) is a lapped transform, similar to the modified discrete cosine transform, that explicitly represents the phase (complex values) of the signal.\n\n\n"}
{"id": "4866818", "url": "https://en.wikipedia.org/wiki?curid=4866818", "title": "NSMB (mathematics)", "text": "NSMB (mathematics)\n\nNSMB is a computer system for solving Navier–Stokes equations using the finite volume method. It supports meshes built of several blocks (multi-blocks) and supports parallelisation. The name stands for \"Navier–Stokes multi-block\". It was developed by a consortium of European scientific institutions and companies, between 1992 and 2003.\n"}
{"id": "21693427", "url": "https://en.wikipedia.org/wiki?curid=21693427", "title": "Napkin ring problem", "text": "Napkin ring problem\n\nIn geometry, the napkin-ring problem involves finding the volume of a \"band\" of specified height around a sphere, i.e. the part that remains after a hole in the shape of a circular cylinder is drilled through the center of the sphere. It is a counterintuitive fact that this volume does not depend on the original sphere's radius but only on the resulting band's height.\n\nThe problem is so called because after removing a cylinder from the sphere, the remaining band resembles the shape of a napkin ring.\n\nSuppose that the axis of a right circular cylinder passes through the center of a sphere of radius \"R\" and that \"h\" represents the height (defined as the distance in a direction parallel to the axis) of the part of the cylinder that is inside the sphere. The \"band\" is the part of the sphere that is outside the cylinder. The volume of the band depends on \"h\" but not on \"R\":\n\nAs the radius \"R\" of the sphere shrinks, the diameter of the cylinder must also shrink in order that \"h\" can remain the same. The band gets thicker, and this would increase its volume. But it also gets shorter in circumference, and this would decrease its volume. The two effects exactly cancel each other out. In the extreme case of the smallest possible sphere, the cylinder vanishes (its radius becomes zero) and the height \"h\" equals the diameter of the sphere. In this case the volume of the band is the volume of the whole sphere, which matches the formula given above.\n\nAn early study of this problem was written by 17th-century Japanese mathematician Seki Kōwa. According to , Seki called this solid an arc-ring, or in Japanese \"kokan\" or \"kokwan\".\n\nSuppose the radius of the sphere is formula_2 and the length of the cylinder (or the tunnel) is formula_3.\n\nBy the Pythagorean theorem, the radius of the cylinder is\n\nand the radius of the horizontal cross-section of the sphere at height \"y\" above the \"equator\" is\n\nThe cross-section of the band with the plane at height \"y\" is the region inside the larger circle of radius given by (2) and outside the smaller circle of radius given by (1). The cross-section's area is therefore the area of the larger circle minus the area of the smaller circle:\n\nThe radius \"R\" does not appear in the last quantity. Therefore, the area of the horizontal cross-section at height \"y\" does not depend on \"R\", as long as \"y\" ≤ ≤ \"R\". The volume of the band is\n\nand that does not depend on \"R\".\n\nThis is an application of Cavalieri's principle: volumes with equal-sized corresponding cross-sections are equal. Indeed, the area of the cross-section is the same as that of the corresponding cross-section of a sphere of radius \"h\"/2, which has volume\n\n\n"}
{"id": "9164216", "url": "https://en.wikipedia.org/wiki?curid=9164216", "title": "Neyman construction", "text": "Neyman construction\n\nNeyman construction is a frequentist method to construct an interval at a confidence level formula_1 such that if we repeat the experiment many times the interval will contain the true value of some parameter a fraction formula_2 of the time. It is named after Jerzy Neyman.\n\nThe probability that the interval contains the true value is called the coverage probability.\n\nA Neyman construction can be carried out by performing pseudo-experiments, i.e. constructing data sets corresponding to a given value of the parameter. The pseudo-experiments are fitted with conventional methods, and the space of fitted parameter values constitutes the band which the confidence interval can be selected from.\n\n\n"}
{"id": "23551012", "url": "https://en.wikipedia.org/wiki?curid=23551012", "title": "Non-uniform discrete Fourier transform", "text": "Non-uniform discrete Fourier transform\n\nIn applied mathematics, the nonuniform discrete Fourier transform (NUDFT or NDFT) of a signal is a type of Fourier transform, related to a discrete Fourier transform or discrete-time Fourier transform, but in which the input signal is not sampled at equally spaced points or frequencies (or both). It is a generalization of the shifted DFT. It has important applications in signal processing, magnetic resonance imaging, and the numerical solution of partial differential equations.\n\nAs a generalized approach for nonuniform sampling, the NUDFT allows one to obtain frequency domain information of a finite length signal at any frequency. One of the reasons to adopt the NUDFT is that many signals have their energy distributed nonuniformly in the frequency domain. Therefore, a nonuniform sampling scheme could be more convenient and useful in many digital signal processing applications. For example, the NUDFT provides a variable spectral resolution controlled by the user.\n\nThe \"nonuniform discrete Fourier transform\" transforms a sequence of formula_1 complex numbers formula_2 into another sequence of complex numbers formula_3 defined by\n\nwhere formula_4 are sample points and formula_5 are frequencies. Note that if formula_6 and formula_7, then equation () reduces to the discrete Fourier transform. There are three types of NUDFTs.\n\n\nInverse NUDFTs are defined by substituting formula_14 for formula_15 in equation ().\n\nThe multidimensional NUDFT converts a formula_16-dimensional array of complex numbers formula_17 into another formula_16-dimensional array of complex numbers formula_19 defined by\nwhere formula_21 are sample points, formula_22 are frequencies, and formula_23 and formula_24 are formula_16-dimensional vectors of indices from 0 to formula_26. The multidimensional NUDFTs of types I, II, and III are defined analogously to the 1D case.\n\nThe NUDFT can be expressed as a Z-transform. The NUDFT of a sequence formula_27 of length formula_1 is\n\nwhere formula_30 is the Z-transform of formula_27, and formula_32 are arbitrarily distinct points in the z-plane. Note that the NUDFT reduces to the DFT when the sampling points are located on the unit circle at equally spaced angles.\n\nExpressing the above as a matrix, we get\n\nwhere\n\nAs we can see, the NUDFT is characterized by formula_35 and hence the formula_1 formula_37 points. If we further factorize formula_38, we can see that formula_35 is nonsingular provided the formula_1 formula_37 points are distinct. If formula_35 is nonsingular, we can get a unique inverse NUDFT as follows:\n\nGiven formula_44, we can use Gaussian elimination to solve for formula_45. However, the complexity of this method is formula_46. To solve this problem more efficiently, we first determine formula_30 directly by polynomial interpolation:\n\nThen formula_27 are the coefficients of the above interpolating polynomial.\n\nExpressing formula_30 as the Lagrange polynomial of order formula_51, we get\n\nwhere formula_53 are the fundamental polynomials:\n\nExpressing formula_30 by the Newton interpolation method, we get\n\nwhere formula_57 is the divided difference of the formula_58th order of formula_59 with respect to formula_60:\n\nThe disadvantage of the Lagrange representation is that any additional point included will increase the order of the interpolating polynomial, leading to the need to recompute all the fundamental polynomials. However, any additional point included in the Newton representation only requires the addition of one more term.\n\nWe can use a lower triangular system to solve formula_65:\n\nwhere\n\nBy the above equation, formula_65 can be computed within formula_69 operations. In this way Newton interpolation is more efficient than Lagrange Interpolation unless the latter is modified by\n\nWhile a naive application of equation () results in an formula_69 algorithm for computing the NUDFT, formula_72 algorithms based on the fast Fourier transform (FFT) do exist. Such algorithms are referred to as NUFFTs or NFFTs and have been developed based on oversampling and interpolation, min-max interpolation, and low-rank approximation. In general, NUFFTs leverage the FFT by converting the nonuniform problem into a uniform problem (or a sequence of uniform problems) to which the FFT can be applied. Software libraries for performing NUFFTs are available in 1D, 2D, and 3D.\n\nThe applications of the NUDFT include:\n\n\n"}
{"id": "1270068", "url": "https://en.wikipedia.org/wiki?curid=1270068", "title": "Pierre-Louis Lions", "text": "Pierre-Louis Lions\n\nPierre-Louis Lions (; born 11 August 1956) is a French mathematician.\n\nHis parents were Jacques-Louis Lions, a mathematician and at that time professor at the University of Nancy, who became President of the International Mathematical Union, and Andrée Olivier, his wife. He graduated from the École normale supérieure in 1977 (same year as Jean-Christophe Yoccoz, another Fields Medalist). Refusing to take the \"agrégation\" in Mathematics, he chose to carry out research in applied mathematics and received his doctorate from the University of Pierre and Marie Curie in 1979.\n\nLions received the Fields Medal, for his work on theory of nonlinear partial differential equations, in 1994 while working at the University of Paris-Dauphine. He was the first to give a complete solution to the Boltzmann equation with proof. Other awards Lions received include the IBM Prize in 1987 and the Philip Morris Prize in 1991. He was an invited professor at the Conservatoire national des arts et métiers (2000). He is a doctor honoris causa of Heriot-Watt University (Edinburgh), Narvik University College (2014), and of the City University of Hong-Kong and is listed as an ISI highly cited researcher. He holds the position of Professor of \"Partial differential equations and their applications\" at the Collège de France in Paris as well as a position at École Polytechnique.\n\nIn the paper \"Viscosity solutions of Hamilton-Jacobi equations\" (1983), written with Michael G. Crandall, he introduced the notion of viscosity solutions. This has had an effect on the theory of partial differential equations.\n\n\n"}
{"id": "41143378", "url": "https://en.wikipedia.org/wiki?curid=41143378", "title": "Proof-of-stake", "text": "Proof-of-stake\n\nProof of stake (PoS) is a type of algorithm by which a cryptocurrency blockchain network aims to achieve distributed consensus. In PoS-based cryptocurrencies the creator of the next block is chosen via various combinations of random selection and wealth or age (\"i.e.,\" the stake). In contrast, the algorithm of proof-of-work-based cryptocurrencies such as bitcoin uses mining; that is, the solving of computationally intensive puzzles to validate transactions and create new blocks.\n\nProof of stake must have a way of defining the next valid block in any blockchain. Selection by account balance would result in (undesirable) centralization, as the single richest member would have a permanent advantage. Instead, several different methods of selection have been devised.\n\nNxt and BlackCoin use randomization to predict the following generator by using a formula that looks for the lowest hash value in combination with the size of the stake. Since the stakes are public, each node can predict—with reasonable accuracy—which account will next win the right to forge a block.\n\nPeercoin's proof-of-stake system combines randomization with the concept of \"coin age\", a number derived from the product of the number of coins multiplied by the number of days the coins have been held.\n\nCoins that have been unspent for at least 30 days begin competing for the next block. Older and larger sets of coins have a greater probability of signing the next block. However, once a stake of coins has been used to sign a block, it must start over with zero \"coin age\" and thus wait at least 30 more days before signing another block. Also, the probability of finding the next block reaches a maximum after 90 days in order to prevent very old or very large collections of stakes from dominating the blockchain.\n\nThis process secures the network and gradually produces new coins over time without consuming significant computational power.\n\nVarious projects such as EOS, Bitcoin-sCrypt, Steem, Ark and Bitshares are using delegated proof-of-stake, or DPoS. The system uses a limited number of nodes to propose and validate blocks to the blockchain. This is meant to keep transaction processing fast, rather than using several hundred or several thousand nodes. EOS uses a limited number of block validators, 21, whose reputation may or may not drop, allowing back-up validators to replace former nodes.\n\nOrbs uses a similar process, dubbed randomized proof-of-stake (or RPoS) but selects an entire committee rather than a single block leader. Each node is selected randomly using a verifiable random beacon to propose the current block of transactions. Then, the block is verified through that committee containing a pre-set number of nodes from within the total network of nodes available.\n\nIncentives differ between the two systems of block generation. Under proof of work, miners may potentially own none of the currency they are mining and thus seek only to maximize their own profits. It is unclear whether this disparity lowers or raises security risks. Under proof of stake, however, those \"guarding\" the coins always own the coins, although several cryptocurrencies do allow or enforce the lending of staking power to other nodes.\n\nSome authors argue that proof of stake is not an ideal option for a distributed consensus protocol. One issue that can arise is the \"nothing-at-stake\" problem, wherein block generators have nothing to lose by voting for multiple blockchain histories, thereby preventing consensus from being achieved. Because unlike in proof-of-work systems, there is little cost to working on several chains.\n\nMany have attempted to solve these problems:\n\n"}
{"id": "400010", "url": "https://en.wikipedia.org/wiki?curid=400010", "title": "Rao–Blackwell theorem", "text": "Rao–Blackwell theorem\n\nIn statistics, the Rao–Blackwell theorem, sometimes referred to as the Rao–Blackwell–Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude estimator into an estimator that is optimal by the mean-squared-error criterion or any of a variety of similar criteria.\n\nThe Rao–Blackwell theorem states that if \"g\"(\"X\") is any kind of estimator of a parameter θ, then the conditional expectation of \"g\"(\"X\") given \"T\"(\"X\"), where \"T\" is a sufficient statistic, is typically a better estimator of θ, and is never worse. Sometimes one can very easily construct a very crude estimator \"g\"(\"X\"), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.\n\nThe theorem is named after Calyampudi Radhakrishna Rao and David Blackwell. The process of transforming an estimator using the Rao–Blackwell theorem is sometimes called Rao–Blackwellization. The transformed estimator is called the Rao–Blackwell estimator.\n\n\n\nOne case of Rao–Blackwell theorem states:\n\nIn other words,\n\nThe essential tools of the proof besides the definition above are the law of total expectation and the fact that for any random variable \"Y\", E(\"Y\") cannot be less than [E(\"Y\")]. That inequality is a case of Jensen's inequality, although it may also be shown to follow instantly from the frequently mentioned fact that\n\nMore precisely, the mean square error of the Rao-Blackwell estimator has the following decomposition\n\nSince formula_4, the Rao-Blackwell theorem immediately follows.\n\nThe more general version of the Rao–Blackwell theorem speaks of the \"expected loss\" or risk function:\n\nwhere the \"loss function\" \"L\" may be any convex function. If the loss function is twice-differentiable, as in the case for mean-squared-error, then we have the sharper inequality\n\nThe improved estimator is unbiased if and only if the original estimator is unbiased, as may be seen at once by using the law of total expectation. The theorem holds regardless of whether biased or unbiased estimators are used.\n\nThe theorem seems very weak: it says only that the Rao–Blackwell estimator is no worse than the original estimator. In practice, however, the improvement is often enormous.\n\nPhone calls arrive at a switchboard according to a Poisson process at an average rate of λ per minute. This rate is not observable, but the numbers \"X\", ..., \"X\" of phone calls that arrived during \"n\" successive one-minute periods are observed. It is desired to estimate the probability \"e\" that the next one-minute period passes with no phone calls.\n\nAn \"extremely\" crude estimator of the desired probability is\n\ni.e., it estimates this probability to be 1 if no phone calls arrived in the first minute and zero otherwise. Despite the apparent limitations of this estimator, the result given by its Rao–Blackwellization is a very good estimator.\n\nThe sum\n\ncan be readily shown to be a sufficient statistic for λ, i.e., the \"conditional\" distribution of the data \"X\", ..., \"X\", depends on λ only through this sum. Therefore, we find the Rao–Blackwell estimator\n\nAfter doing some algebra we have\n\nSince the average number of calls arriving during the first \"n\" minutes is \"n\"λ, one might not be surprised if this estimator has a fairly high probability (if \"n\" is big) of being close to\n\nSo δ is clearly a very much improved estimator of that last quantity. In fact, since \"S\" is complete and δ is unbiased, δ is the unique minimum variance unbiased estimator by the Lehmann–Scheffé theorem.\n\nRao–Blackwellization is an idempotent operation. Using it to improve the already improved estimator does not obtain a further improvement, but merely returns as its output the same improved estimator.\n\nIf the conditioning statistic is both complete and sufficient, and the starting estimator is unbiased, then the Rao–Blackwell estimator is the unique \"best unbiased estimator\": see Lehmann–Scheffé theorem.\n\nAn example of an improvable Rao–Blackwell improvement, when using a minimal sufficient statistic that is not complete, was provided by Galili and Meilijson in 2016. Let formula_12 be a random sample from a scale-uniform distribution formula_13 with unknown mean formula_14 and known design parameter formula_15. In the search for \"best\" possible unbiased estimators for formula_16 it is natural to consider formula_17 as an initial (crude) unbiased estimator for formula_18 and then try to improve it. Since formula_17 is not a function of formula_20, the minimal sufficient statistic for formula_18 (where formula_22 and formula_23), it may be improved using the Rao–Blackwell theorem as follows:\n\nHowever, the following unbiased estimator can be shown to have lower variance:\n\nAnd in fact, it could be even further improved when using the following estimator:\n\n"}
{"id": "1278389", "url": "https://en.wikipedia.org/wiki?curid=1278389", "title": "Related rates", "text": "Related rates\n\nIn differential calculus, related rates problems involve finding a rate at which a quantity changes by relating that quantity to other quantities whose rates of change are known. The rate of change is usually with respect to time. Because science and engineering often relate quantities to each other, the methods of related rates have broad applications in these fields. Differentiation with respect to time or one of the other variables requires application of the chain rule, since most problems involve several variables.\n\nFundamentally, if a function formula_1 is defined such that formula_2, then the derivative of the function formula_1 can be taken with respect to another variable. (The Variable formula_4 is frequently used as many Related Rates problems apply to finding changes with respect to time.) We assume formula_5 is a function of formula_4, i.e. formula_7. Then formula_8, so \n\nformula_9 \n\nWritten in Leibniz notation, this is:\n\nformula_10\n\nThe value of this is: if it is known how formula_5 changes with respect to formula_4, then we can determine how formula_1 changes with respect to formula_4 and vice versa. We can extend this application of the chain rule with the sum, difference, product and quotient rules of calculus, etc.\n\ne.g. \n\nIf formula_15 \n\nthen formula_16.\n\nThe most common way to approach related rates problems is the following:\n\nErrors in this procedure are often caused by plugging in the known values for the variables \"before\" (rather than after) finding the derivative with respect to time. Doing so will yield an incorrect result, since if those values are substituted for the variables before differentiation, those variables will become constants; and when the equation is differentiated, zeroes appear in places of all variables for which the values were plugged in.\n\nA 10-meter ladder is leaning against the wall of a building, and the base of the ladder is sliding away from the building at a rate of 3 meters per second. How fast is the top of the ladder sliding down the wall when the base of the ladder is 6 meters from the wall?\n\nThe distance between the base of the ladder and the wall, \"x\", and the height of the ladder on the wall, \"y\", represent the sides of a right triangle with the ladder as the hypotenuse, \"h\". The objective is to find \"dy\"/\"dt\", the rate of change of \"y\" with respect to time, \"t\", when \"h\", \"x\" and \"dx\"/\"dt\", the rate of change of \"x\", are known.\n\nStep 1:\n<br>\nStep 2:\nFrom the Pythagorean theorem, the equation\ndescribes the relationship between \"x\", \"y\" and \"h\", for a right triangle. Differentiating both sides of this equation with respect to time, \"t\", yields\n<br>\nStep 3:\nWhen solved for the wanted rate of change, \"dy\"/\"dt\", gives us\n<br>\nStep 4 & 5:\nUsing the variables from step 1 gives us:\nSolving for y using the Pythagorean Theorem gives:\n<br>\nPlugging in 8 for the equation:\n\nIt is generally assumed that negative values represent the downward direction. In doing such, the top of the ladder is sliding down the wall at a rate of meters per second.\n\nBecause one physical quantity often depends on another, which, in turn depends on others, such as time, related rate methods have broad applications in Physics. This section presents an example of related rates kinematics and electromagnetic induction.\n\nFor example, one can consider the kinematics problem where one vehicle is heading West toward an intersection at 80 miles per hour while another is heading North away from the intersection at 60 miles per hour. One can ask whether the vehicles are getting closer or further apart and at what rate at the moment when the North bound vehicle is 3 miles North of the intersection and the West bound vehicle is 4 miles East of the intersection.\n\nBig idea: use chain rule to compute rate of change of distance between two vehicles.\n\nPlan:\n\nChoose coordinate system:\nLet the y-axis point North and the x-axis point East.\n\nIdentify variables:\nDefine y(t) to be the distance of the vehicle heading North from the origin and x(t) to be the distance of the vehicle heading West from the origin.\n\nExpress c in terms of x and y via Pythagorean theorem:\n\nformula_34\n\nExpress dc/dt using chain rule in terms of dx/dt and dy/dt:\n\nSubstitute in x = 4 mi, y = 3 mi, dx/dt = -80 mi/hr, dy/dt = 60 mi/hr and Simplify\n\nformula_35\n\nConsequently, the two vehicles are getting closer together at a rate of 28 mi/hr.\n\nThe magnetic flux through a loop of area A whose normal is at an angle θ to a magnetic field of strength B is\n\nformula_36\n\nFaraday's law of electromagnetic induction states that the induced electromotive force formula_37 is the negative rate of change of magnetic flux formula_38 through a conducting loop.\n\n<math> \\mathcal{E} = -\n"}
{"id": "19389837", "url": "https://en.wikipedia.org/wiki?curid=19389837", "title": "Relativistic quantum mechanics", "text": "Relativistic quantum mechanics\n\nIn physics, relativistic quantum mechanics (RQM) is any Poincaré covariant formulation of quantum mechanics (QM). This theory is applicable to massive particles propagating at all velocities up to those comparable to the speed of light \"c\", and can accommodate massless particles. The theory has application in high energy physics, particle physics and accelerator physics, as well as atomic physics, chemistry and condensed matter physics. \"Non-relativistic quantum mechanics\" refers to the mathematical formulation of quantum mechanics applied in the context of Galilean relativity, more specifically quantizing the equations of classical mechanics by replacing dynamical variables by operators. \"Relativistic quantum mechanics\" (RQM) is quantum mechanics applied with special relativity, but not general relativity. Although the earlier formulations, like the Schrödinger picture and Heisenberg picture were originally formulated in a non-relativistic background, these pictures of quantum mechanics also apply with special relativity.\n\nThe relativistic formulation is more successful than the original quantum mechanics in some contexts, in particular: the prediction of antimatter, electron spin, spin magnetic moments of elementary spin  fermions, fine structure, and quantum dynamics of charged particles in electromagnetic fields. The key result is the Dirac equation, from which these predictions emerge automatically. By contrast, in quantum mechanics, terms have to be introduced artificially into the Hamiltonian operator to achieve agreement with experimental observations.\n\nNevertheless, RQM is only an approximation to a fully self-consistent relativistic theory of known particle interactions because it does not describe cases where the number of (elementary) particles changes; for example in matter creation and annihilation. By yet another theoretical advance, a more accurate theory that allows for these occurrences and other predictions is \"relativistic quantum field theory\" in which particles are interpreted as \"field quanta\" (see article for details).\n\nIn this article, the equations are written in familiar 3d vector calculus notation and use hats for operators (not necessarily in the literature), and where space and time components can be collected, tensor index notation is shown also (frequently used in the literature), in addition the Einstein summation convention is used. SI units are used here; Gaussian units and natural units are common alternatives. All equations are in the position representation; for the momentum representation the equations have to be Fourier transformed – see position and momentum space.\n\nOne approach is to modify the Schrödinger picture to be consistent with special relativity.\n\nA postulate of quantum mechanics is that the time evolution of any quantum system is given by the Schrödinger equation:\n\nusing a suitable Hamiltonian operator corresponding to the system. The solution is a complex-valued wavefunction , a function of the 3d position vector of the particle at time , describing the behavior of the system.\n\nEvery particle has a non-negative spin quantum number . The number is an integer, odd for fermions and even for bosons. Each has \"z\"-projection quantum numbers; . This is an additional discrete variable the wavefunction requires; .\n\nHistorically, in the early 1920s Pauli, Kronig, Uhlenbeck and Goudsmit were the first to propose the concept of spin. The inclusion of spin in the wavefunction incorporates the Pauli exclusion principle (1925) and the more general spin-statistics theorem (1939) due to Fierz, rederived by Pauli a year later. This is the explanation for a diverse range of subatomic particle behavior and phenomena: from the electronic configurations of atoms, nuclei (and therefore all elements on the periodic table and their chemistry), to the quark configurations and colour charge (hence the properties of baryons and mesons).\n\nA fundamental prediction of special relativity is the relativistic energy–momentum relation; for a particle of rest mass , and in a particular frame of reference with energy and 3-momentum with magnitude in terms of the dot product , it is:\n\nThese equations are used together with the energy and momentum operators, which are respectively:\n\nto construct a relativistic wave equation (RWE): a partial differential equation consistent with the energy–momentum relation, and is solved for to predict the quantum dynamics of the particle. For space and time to be placed on equal footing, as in relativity, the orders of space and time partial derivatives should be equal, and ideally as low as possible, so that no initial values of the derivatives need to be specified. This is important for probability interpretations, exemplified below. The lowest possible order of any differential equation is the first (zeroth order derivatives would not form a differential equation).\n\nThe Heisenberg picture is another formulation of QM, in which case the wavefunction is \"time-independent\", and the operators contain the time dependence, governed by the equation of motion:\n\nThis equation is also true in RQM, provided the Heisenberg operators are modified to be consistent with SR.\n\nHistorically, around 1926, Schrödinger and Heisenberg show that wave mechanics and matrix mechanics are equivalent, later furthered by Dirac using transformation theory.\n\nA more modern approach to RWEs, first introduced during the time RWEs were developing for particles of any spin, is to apply representations of the Lorentz group.\n\nIn classical mechanics and non-relativistic QM, time is an absolute quantity all observers and particles can always agree on, \"ticking away\" in the background independent of space. Thus in non-relativistic QM one has for a many particle system .\n\nIn relativistic mechanics, the spatial coordinates and coordinate time are \"not\" absolute; any two observers moving relative to each other can measure different locations and times of events. The position and time coordinates combine naturally into a four-dimensional spacetime position corresponding to events, and the energy and 3-momentum combine naturally into the four momentum of a dynamic particle, as measured in \"some\" reference frame, change according to a Lorentz transformation as one measures in a different frame boosted and/or rotated relative the original frame in consideration. The derivative operators, and hence the energy and 3-momentum operators, are also non-invariant and change under Lorentz transformations.\n\nUnder a proper orthochronous Lorentz transformation in Minkowski space, all one-particle quantum states locally transform under some representation of the Lorentz group:\n\nwhere is a finite-dimensional representation, in other words a square matrix . Again, is thought of as a column vector containing components with the allowed values of . The quantum numbers and as well as other labels, continuous or discrete, representing other quantum numbers are suppressed. One value of may occur more than once depending on the representation.\n\nThe classical Hamiltonian for a particle in a potential is the kinetic energy plus the potential energy , with the corresponding quantum operator in the Schrödinger picture:\n\nand substituting this into the above Schrödinger equation gives a non-relativistic QM equation for the wavefunction: the procedure is a straightforward substitution of a simple expression. By contrast this is not as easy in RQM; the energy–momentum equation is quadratic in energy \"and\" momentum leading to difficulties. Naively setting:\n\nis not helpful for several reasons. The square root of the operators cannot be used as it stands; it would have to be expanded in a power series before the momentum operator, raised to a power in each term, could act on . As a result of the power series, the space and time derivatives are \"completely asymmetric\": infinite-order in space derivatives but only first order in the time derivative, which is inelegant and unwieldy. Again, there is the problem of the non-invariance of the energy operator, equated to the square root which is also not invariant. Another problem, less obvious and more severe, is that it can be shown to be nonlocal and can even \"violate causality\": if the particle is initially localized at a point so that is finite and zero elsewhere, then at any later time the equation predicts delocalization everywhere, even for which means the particle could arrive at a point before a pulse of light could. This would have to be remedied by the additional constraint .\n\nThere is also the problem of incorporating spin in the Hamiltonian, which isn't a prediction of the non-relativistic Schrödinger theory. Particles with spin have a corresponding spin magnetic moment quantized in units of , the Bohr magneton:\n\nwhere is the (spin) g-factor for the particle, and the spin operator, so they interact with electromagnetic fields. For a particle in an externally applied magnetic field , the interaction term\n\nhas to be added to the above non-relativistic Hamiltonian. On the contrary; a relativistic Hamiltonian introduces spin \"automatically\" as a requirement of enforcing the relativistic energy-momentum relation.\n\nRelativistic Hamiltonians are analogous to those of non-relativistic QM in the following respect; there are terms including rest mass and interaction terms with externally applied fields, similar to the classical potential energy term, as well as momentum terms like the classical kinetic energy term. A key difference is that relativistic Hamiltonians contain spin operators in the form of matrices, in which the matrix multiplication runs over the spin index , so in general a relativistic Hamiltonian:\n\nis a function of space, time, and the momentum and spin operators.\n\nSubstituting the energy and momentum operators directly into the energy–momentum relation may at first sight seem appealing, to obtain the Klein–Gordon equation:\n\nand was discovered by many people because of the straightforward way of obtaining it, notably by Schrödinger in 1925 before he found the non-relativistic equation named after him, and by Klein and Gordon in 1927, who included electromagnetic interactions in the equation. This \"is\" relativistically invariant, yet this equation alone isn't a sufficient foundation for RQM for a few reasons; one is that negative-energy states are solutions, another is the density (given below), and this equation as it stands is only applicable to spinless particles. This equation can be factored into the form:\n\nwhere and are not simply numbers or vectors, but 4 × 4 Hermitian matrices that are required to anticommute for :\n\nand square to the identity matrix:\n\nso that terms with mixed second-order derivatives cancel while the second-order derivatives purely in space and time remain. The first factor:\n\nis the Dirac equation. The other factor is also the Dirac equation, but for a particle of negative mass. Each factor is relativistically invariant. The reasoning can be done the other way round: propose the Hamiltonian in the above form, as Dirac did in 1928, then pre-multiply the equation by the other factor of operators , and comparison with the KG equation determines the constraints on and . The positive mass equation can continue to be used without loss of continuity. The matrices multiplying suggest it isn't a scalar wavefunction as permitted in the KG equation, but must instead be a four-component entity. The Dirac equation still predicts negative energy solutions, so Dirac postulated that negative energy states are always occupied, because according to the Pauli principle, electronic transitions from positive to negative energy levels in atoms would be forbidden. See Dirac sea for details.\n\nIn non-relativistic quantum mechanics, the square-modulus of the wavefunction gives the probability density function . This is the Copenhagen interpretation, circa 1927. In RQM, while is a wavefunction, the probability interpretation is not the same as in non-relativistic QM. Some RWEs do not predict a probability density or probability current (really meaning \"probability current density\") because they are \"not\" positive definite functions of space and time. The Dirac equation does:\n\nwhere the dagger denotes the Hermitian adjoint (authors usually write for the Dirac adjoint) and is the probability four-current, while the Klein–Gordon equation does not:\n\nwhere is the four gradient. Since the initial values of both and may be freely chosen, the density can be negative.\n\nInstead, what appears look at first sight a \"probability density\" and \"probability current\" has to be reinterpreted as charge density and current density when multiplied by electric charge. Then, the wavefunction is not a wavefunction at all, but reinterpreted as a \"field\". The density and current of electric charge always satisfy a continuity equation:\n\nas charge is a conserved quantity. Probability density and current also satisfy a continuity equation because probability is conserved, however this is only possible in the absence of interactions.\n\nIncluding interactions in RWEs is generally difficult. Minimal coupling is a simple way to include the electromagnetic interaction. For one charged particle of electric charge in an electromagnetic field, given by the magnetic vector potential defined by the magnetic field , and electric scalar potential , this is:\n\nwhere is the four-momentum that has a corresponding 4-momentum operator, and the four-potential. In the following, the non-relativistic limit refers to the limiting cases:\n\nthat is, the total energy of the particle is approximately the rest energy for small electric potentials, and the momentum is approximately the classical momentum.\n\nIn RQM, the KG equation admits the minimal coupling prescription;\n\nIn the case where the charge is zero, the equation reduces trivially to the free KG equation so nonzero charge is assumed below. This is a scalar equation that is invariant under the \"irreducible\" one-dimensional scalar representation of the Lorentz group. This means that all of its solutions will belong to a direct sum of representations. Solutions that do not belong to the irreducible representation will have two or more \"independent\" components. Such solutions cannot in general describe particles with nonzero spin since spin components are not independent. Other constraint will have to be imposed for that, e.g. the Dirac equation for spin , see below. Thus if a system satisfies the KG equation \"only\", it can only be interpreted as a system with zero spin.\n\nThe electromagnetic field is treated classically according to Maxwell's equations and the particle is described by a wavefunction, the solution to the KG equation. The equation is, as it stands, not always very useful, because massive spinless particles, such as the \"π\"-mesons, experience the much stronger strong interaction in addition to the electromagnetic interaction. It does, however, correctly describe charged spinless bosons in the absence of other interactions.\n\nThe KG equation is applicable to spinless charged bosons in an external electromagnetic potential. As such, the equation cannot be applied to the description of atoms, since the electron is a spin  particle. In the non-relativistic limit the equation reduces to the Schrödinger equation for a spinless charged particle in an electromagnetic field:\n\nNon relativistically, spin was \"phenomenologically\" introduced in the Pauli equation by Pauli in 1927 for particles in an electromagnetic field:\n\nby means of the 2 × 2 Pauli matrices, and is not just a scalar wavefunction as in the non-relativistic Schrödinger equation, but a two-component spinor field:\n\nwhere the subscripts ↑ and ↓ refer to the \"spin up\" () and \"spin down\" () states.\n\nIn RQM, the Dirac equation can also incorporate minimal coupling, rewritten from above;\n\nand was the first equation to accurately \"predict\" spin, a consequence of the 4 × 4 gamma matrices . There is a 4 × 4 identity matrix pre-multiplying the energy operator (including the potential energy term), conventionally not written for simplicity and clarity (i.e. treated like the number 1). Here is a four-component spinor field, which is conventionally split into two two-component spinors in the form:\n\nThe 2-spinor corresponds to a particle with 4-momentum and charge and two spin states (, as before). The other 2-spinor corresponds to a similar particle with the same mass and spin states, but \"negative\" 4-momentum and \"negative\" charge , that is, negative energy states, time-reversed momentum, and negated charge. This was the first interpretation and prediction of a particle and \"corresponding antiparticle\". See Dirac spinor and bispinor for further description of these spinors. In the non-relativistic limit the Dirac equation reduces to the Pauli equation (see Dirac equation for how). When applied a one-electron atom or ion, setting and to the appropriate electrostatic potential, additional relativistic terms include the spin-orbit interaction, electron gyromagnetic ratio, and Darwin term. In ordinary QM these terms have to be put in by hand and treated using perturbation theory. The positive energies do account accurately for the fine structure.\n\nWithin RQM, for massless particles the Dirac equation reduces to:\n\nthe first of which is the Weyl equation, a considerable simplification applicable for massless neutrinos. This time there is a 2 × 2 identity matrix pre-multiplying the energy operator conventionally not written. In RQM it is useful to take this as the zeroth Pauli matrix which couples to the energy operator (time derivative), just as the other three matrices couple to the momentum operator (spatial derivatives).\n\nThe Pauli and gamma matrices were introduced here, in theoretical physics, rather than pure mathematics itself. They have applications to quaternions and to the SO(2) and SO(3) Lie groups, because they satisfy the important commutator [ , ] and anticommutator [ , ] relations respectively:\n\nwhere is the three-dimensional Levi-Civita symbol. The gamma matrices form bases in Clifford algebra, and have a connection to the components of the flat spacetime Minkowski metric in the anticommutation relation:\n\n(This can be extended to curved spacetime by introducing vierbeins, but is not the subject of special relativity).\n\nIn 1929, the Breit equation was found to describe two or more electromagnetically interacting massive spin  fermions to first-order relativistic corrections; one of the first attempts to describe such a relativistic quantum many-particle system. This is, however, still only an approximation, and the Hamiltonian includes numerous long and complicated sums.\n\nThe helicity operator is defined by;\n\nwhere p is the momentum operator, S the spin operator for a particle of spin \"s\", \"E\" is the total energy of the particle, and \"m\" its rest mass. Helicity indicates the orientations of the spin and translational momentum vectors. Helicity is frame-dependent because of the 3-momentum in the definition, and is quantized due to spin quantization, which has discrete positive values for parallel alignment, and negative values for antiparallel alignment.\n\nA automatic occurrence in the Dirac equation (and the Weyl equation) is the projection of the spin  operator on the 3-momentum (times \"c\"), , which is the helicity (for the spin  case) times formula_31.\n\nFor massless particles the helicity simplifies to:\n\nThe Dirac equation can only describe particles of spin . Beyond the Dirac equation, RWEs have been applied to free particles of various spins. In 1936, Dirac extended his equation to all fermions, three years later Fierz and Pauli rederived the same equation. The Bargmann–Wigner equations were found in 1948 using Lorentz group theory, applicable for all free particles with any spin. Considering the factorization of the KG equation above, and more rigorously by Lorentz group theory, it becomes apparent to introduce spin in the form of matrices.\n\nThe wavefunctions are multicomponent spinor fields, which can be represented as column vectors of functions of space and time:\n\nwhere the expression on the right is the Hermitian conjugate. For a \"massive\" particle of spin , there are components for the particle, and another for the corresponding antiparticle (there are possible values in each case), altogether forming a -component spinor field:\n\nwith the + subscript indicating the particle and − subscript for the antiparticle. However, for \"massless\" particles of spin \"s\", there are only ever two-component spinor fields; one is for the particle in one helicity state corresponding to +\"s\" and the other for the antiparticle in the opposite helicity state corresponding to −\"s\":\n\nAccording to the relativistic energy-momentum relation, all massless particles travel at the speed of light, so particles traveling at the speed of light are also described by two-component spinors. Historically, Élie Cartan found the most general form of spinors in 1913, prior to the spinors revealed in the RWEs following the year 1927.\n\nFor equations describing higher-spin particles, the inclusion of interactions is nowhere near as simple minimal coupling, they lead to incorrect predictions and self-inconsistencies. For spin greater than , the RWE is not fixed by the particle's mass, spin, and electric charge; the electromagnetic moments (electric dipole moments and magnetic dipole moments) allowed by the spin quantum number are arbitrary. (Theoretically, magnetic charge would contribute also). For example, the spin  case only allows a magnetic dipole, but for spin 1 particles magnetic quadrupoles and electric dipoles are also possible. For more on this topic, see multipole expansion and (for example) Cédric Lorcé (2009).\n\nThe Schrödinger/Pauli velocity operator can be defined for a massive particle using the classical definition , and substituting quantum operators in the usual way:\n\nwhich has eigenvalues that take \"any\" value. In RQM, the Dirac theory, it is:\n\nwhich must have eigenvalues between ±\"c\". See Foldy–Wouthuysen transformation for more theoretical background.\n\nThe Hamiltonian operators in the Schrödinger picture are one approach to forming the differential equations for . An equivalent alternative is to determine a Lagrangian (really meaning \"Lagrangian density\"), then generate the differential equation by the field-theoretic Euler–Lagrange equation:\n\nFor some RWEs, a Lagrangian can be found by inspection. For example, the Dirac Lagrangian is:\n\nand Klein–Gordon Lagrangian is:\n\nThis is not possible for all RWEs; and is one reason the Lorentz group theoretic approach is important and appealing: fundamental invariance and symmetries in space and time can be used to derive RWEs using appropriate group representations. The Lagrangian approach with field interpretation of is the subject of QFT rather than RQM: Feynman's path integral formulation uses invariant Lagrangians rather than Hamiltonian operators, since the latter can become extremely complicated, see (for example) S. Weinberg (1995).\n\nIn non-relativistic QM, the angular momentum operator is formed from the classical pseudovector definition . In RQM, the position and momentum operators are inserted directly where they appear in the orbital relativistic angular momentum tensor defined from the four-dimensional position and momentum of the particle, equivalently a bivector in the exterior algebra formalism:\n\nwhich are six components altogether: three are the non-relativistic 3-orbital angular momenta; , , , and the other three , , are boosts of the centre of mass of the rotating object. An additional relativistic-quantum term has to be added for particles with spin. For a particle of rest mass , the \"total\" angular momentum tensor is:\n\nwhere the star denotes the Hodge dual, and\n\nis the Pauli–Lubanski pseudovector. For more on relativistic spin, see (for example) S.M. Troshin and N.E. Tyurin (1994).\n\nIn 1926 the Thomas precession is discovered: relativistic corrections to the spin of elementary particles with application in the spin–orbit interaction of atoms and rotation of macroscopic objects. In 1939 Wigner derived the Thomas precession.\n\nIn classical electromagnetism and special relativity, an electron moving with a velocity through an electric field but not a magnetic field , will in its own frame of reference experience a Lorentz-transformed magnetic field :\n\nIn the non-relativistic limit :\n\nso the non-relativistic spin interaction Hamiltonian becomes:\n\nwhere the first term is already the non-relativistic magnetic moment interaction, and the second term the relativistic correction of order , but this disagrees with experimental atomic spectra by a factor of . It was pointed out by L. Thomas that there is a second relativistic effect: An electric field component perpendicular to the electron velocity causes an additional acceleration of the electron perpendicular to its instantaneous velocity, so the electron moves in a curved path. The electron moves in a rotating frame of reference, and this additional precession of the electron is called the \"Thomas precession\". It can be shown that the net result of this effect is that the spin–orbit interaction is reduced by half, as if the magnetic field experienced by the electron has only one-half the value, and the relativistic correction in the Hamiltonian is:\n\nIn the case of RQM, the factor of is predicted by the Dirac equation.\n\nThe events which led to and established RQM, and the continuation beyond into quantum electrodynamics (QED), are summarized below [see, for example, R. Resnick and R. Eisberg (1985), and P.W Atkins (1974)]. More than half a century of experimental and theoretical research from the 1890s through to the 1950s in the new and mysterious quantum theory as it was up and coming revealed that a number of phenomena cannot be explained by QM alone. SR, found at the turn of the 20th century, was found to be a \"necessary\" component, leading to unification: RQM. Theoretical predictions and experiments mainly focused on the newly found atomic physics, nuclear physics, and particle physics; by considering spectroscopy, diffraction and scattering of particles, and the electrons and nuclei within atoms and molecules. Numerous results are attributed to the effects of spin.\n\nEinstein in 1905 explained of the photoelectric effect; a particle description of light as photons. In 1916, Sommerfeld explains fine structure; the splitting of the spectral lines of atoms due to first order relativistic corrections. The Compton effect of 1923 provided more evidence that special relativity does apply; in this case to a particle description of photon–electron scattering. de Broglie extends wave–particle duality to matter: the de Broglie relations, which are consistent with special relativity and quantum mechanics. By 1927, Davisson and Germer and separately G. Thomson successfully diffract electrons, providing experimental evidence of wave-particle duality.\n\n\nIn 1935; Einstein, Rosen, Podolsky published a paper concerning quantum entanglement of particles, questioning quantum nonlocality and the apparent violation of causality upheld in SR: particles can appear to interact instantaneously at arbitrary distances. This was a misconception since information is not and cannot be transferred in the entangled states; rather the information transmission is in the process of measurement by two observers (one observer has to send a signal to the other, which cannot exceed \"c\"). QM does \"not\" violate SR. In 1959, Bohm and Aharonov publish a paper on the Aharonov–Bohm effect, questioning the status of electromagnetic potentials in QM. The EM field tensor and EM 4-potential formulations are both applicable in SR, but in QM the potentials enter the Hamiltonian (see above) and influence the motion of charged particles even in regions where the fields are zero. In 1964, Bell's theorem was published in a paper on the EPR paradox, showing that QM cannot be derived from local hidden variable theories if locality is to be maintained.\n\nIn 1947 the Lamb shift was discovered: a small difference in the \"S\" and \"P\" levels of hydrogen, due to the interaction between the electron and vacuum. Lamb and Retherford experimentally measure stimulated radio-frequency transitions the \"S\" and \"P\" hydrogen levels by microwave radiation. An explanation of the Lamb shift is presented by Bethe. Papers on the effect were published in the early 1950s.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48211279", "url": "https://en.wikipedia.org/wiki?curid=48211279", "title": "Sunčica Čanić", "text": "Sunčica Čanić\n\nSunčica Čanić is a Croatian-American mathematician, the Hugh Roy and Lillie Cranz Cullen Distinguished Professor of Mathematics and Director of the Center for Mathematical Biosciences at the University of Houston, and Professor of Mathematics at the University of California, Berkeley. She is known for her work in mathematically modeling the human cardiovascular system and medical devices for it.\n\nČanić earned bachelor's and master's degrees in mathematics in 1984 and 1986 from the University of Zagreb. She completed her Ph.D. in 1992 in applied mathematics from Stony Brook University, under the joint supervision of Bradley J. Plohr and James Glimm. She became an assistant professor at Iowa State University in 1992, and moved to the University of Houston in 1998. She became the Cullen Distinguished Professor in 2008, and Professor of Mathematics at U.C. Berkeley in 2018. She is also a member of the board of governors of the Institute for Mathematics and its Applications.\n\nČanić's research has involved the computational simulation of the stents used to treat arterial clogging. By finding ways of simplifying computer models of stents from hundreds of thousands of nodes to only 400 nodes, she was able to make these simulations much more efficient, and used them to design improved stents that reduce clotting and scar formation. She has also led the development of a procedure for heart valve replacement surgery that is less traumatic than open-heart surgery.\n\nIn 2014 she was elected as a fellow of the Society for Industrial and Applied Mathematics \"for contributions to the modeling and analysis of partial differential equations motivated by applications in the life sciences.\"\n"}
{"id": "360601", "url": "https://en.wikipedia.org/wiki?curid=360601", "title": "Turán's theorem", "text": "Turán's theorem\n\nIn graph theory, Turán's theorem is a result on the number of edges in a \"K\"-free graph.\n\nAn -vertex graph that does not contain any -vertex clique may be formed by partitioning the set of vertices into parts of equal or nearly equal size, and connecting two vertices by an edge whenever they belong to two different parts. We call the resulting graph the Turán graph . Turán's theorem states that the Turán graph has the largest number of edges among all -free -vertex graphs.\n\nTurán graphs were first described and studied by Hungarian mathematician Pál Turán in 1941, though a special case of the theorem was stated earlier by Mantel in 1907.\n\nAn equivalent formulation is the following:\n\nLet be an -vertex simple graph with no -clique and with the maximum number of edges.\n\nAssume the claim is false. Construct a new -vertex simple graph that contains no -clique but has more edges than , as follows:\n\nCase 1: formula_11\n\nAssume that formula_12. Delete vertex and create a copy of vertex (with all of the same neighbors as ); call it . Any clique in the new graph contains at most one vertex among formula_13. So this new graph does not contain any -clique. However, it contains more edges: \n\nCase 2: formula_15 and formula_16\n\nDelete vertices and and create two new copies of vertex . Again, the new graph does not contain any -clique. However it contains more edges: \n\nThis proves Claim 1.\n\nThe claim proves that one can partition the vertices of into equivalence classes based on their non-neighbors; i.e. two vertices are in the same equivalence class if they are nonadjacent. This implies that is a complete multipartite graph (where the parts are the equivalence classes).\n\nIf is a complete -partite graph with parts \"A\" and \"B\" and formula_18, then we can increase the number of edges in by moving a vertex from part \"A\" to part . By moving a vertex from part \"A\" to part \"B\", the graph loses formula_19 edges, but gains formula_20 edges. Thus, it gains at least formula_21 edge. This proves Claim 2.\n\nThis proof shows that the Turan graph has the maximum number of edges. Additionally, the proof shows that the Turan graph is the \"only\" graph that has the maximum number of edges.\n\nAs a special case of Turán's theorem, for \"r\" = 2, one obtains:\n\nIn other words, one must delete nearly half of the edges in to obtain a triangle-free graph.\n\nA strengthened form of Mantel's theorem states that any hamiltonian graph with at least \"n\"/4 edges must either be the complete bipartite graph \"K\" or it must be pancyclic: not only does it contain a triangle, it must also contain cycles of all other possible lengths up to the number of vertices in the graph .\n\nAnother strengthening of Mantel's theorem states that the edges of every -vertex graph may be covered by at most formula_23 cliques which are either edges or triangles. As a corollary, the intersection number is at most formula_23 .\n\n\n"}
{"id": "46062204", "url": "https://en.wikipedia.org/wiki?curid=46062204", "title": "Wetzel's problem", "text": "Wetzel's problem\n\nIn mathematics, Wetzel's problem concerns bounds on the cardinality of a set of analytic functions that, for each of their arguments, take on few distinct values. It is named after John Wetzel, a mathematician at the University of Illinois at Urbana–Champaign.\n\nLet \"F\" be a family of distinct analytic functions on a given domain with the property that, for each \"x\" in the domain, the functions in \"F\" map \"x\" to a countable set of values. In his doctoral dissertation, Wetzel asked whether this assumption implies that \"F\" is necessarily itself countable. Paul Erdős in turn learned about the problem at the University of Michigan, likely via Lee Albert Rubel. In his paper on the problem, Erdős credited an anonymous mathematician with the observation that, when each \"x\" is mapped to a finite set of values, \"F\" is necessarily finite.\n\nHowever, as Erdős showed, the situation for countable sets is more complicated: the answer to Wetzel's question is yes if and only if the continuum hypothesis is false. That is, the existence of an uncountable set of functions that maps any argument \"x\" to a countable set of values is equivalent to the nonexistence of an uncountable set of real numbers whose cardinality is less than the cardinality of the set of all real numbers. One direction of this equivalence was also proven independently, but not published, by another UIUC mathematician, Robert Dan Dixon. It follows from the independence of the continuum hypothesis, proved in 1963 by Paul Cohen, that the answer to Wetzel's problem is independent of ZFC set theory.\nErdös' proof is so short and elegant that it is considered to be one of the Proofs from THE BOOK.\n"}
{"id": "37487414", "url": "https://en.wikipedia.org/wiki?curid=37487414", "title": "Wu–Sprung potential", "text": "Wu–Sprung potential\n\nIn mathematical physics, the Wu–Sprung potential, named after Hua Wu and Donald Sprung, is a potential function in one dimension inside a Hamiltonian formula_1 with the potential defined by solving a non-linear integral equation defined by the Bohr–Sommerfeld quantization conditions involving the spectral staircase, the energies formula_2 and the potential formula_3.\n\nhere a is a classical turning point so formula_5, the quantum energies of the model are the roots of the Riemann Xi function formula_6\n\nand formula_7. In general, although Wu and Sprung considered only the smooth part, the potential is defined implicitly by formula_8; with \"N\"(\"x\") being the eigenvalue staircase formula_9 and \"H\"(\"x\") is the Heaviside step function.\n\nFor the case of the Riemann zeros Wu and Sprung and others have shown that the potential can be written implicitly in terms of the Gamma function and zeroth-order Bessel function.\n\nand that the density of states of this Hamiltonian is just the Delsarte's formula for the Riemann zeta function and defined semiclassically as formula_11\n\nhere they have taken the derivative of the Euler product on the critical line formula_13; also they use the Dirichlet generating function formula_14. formula_15 is the Mangoldt function.\n\nThe main idea by Wu and Sprung and others is to interpret the density of states as the distributional Delsarte's formula and then use the WKB method to evaluate the imaginary part of the zeros by using quantum mechanics.\n\nWu and Sprung also showed that the zeta-regularized functional determinant is the Riemann Xi-function formula_16\n\nThe main idea inside this problem is to recover the potential from spectral data as in some inverse spectral problems in this case the spectral data is the Eigenvalue staircase, which is a quantum property of the system, the inverse of the potential then, satisfies an Abel integral equation (fractional calculus) which can be immediately solved to obtain the potential.\n\nFor big \"x\" if we take only the smooth part of the eigenvalue staircase formula_17, then the potential as formula_18 is positive and it is given by the asymptotic expression formula_19 with formula_20 and formula_21 in the limit formula_22. This potential is approximately a Morse Potential with formula_23\n\nThe asymptotic of the energies depend on the quantum number \"n\" as formula_24 here \"W\" is the Lambert function\n\n"}
