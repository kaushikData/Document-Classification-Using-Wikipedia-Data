{"id": "43756445", "url": "https://en.wikipedia.org/wiki?curid=43756445", "title": "Al-Isfizari", "text": "Al-Isfizari\n\nAbū Ḥātim al-Muẓaffar al-Isfazārī (fl. late 11th – early 12th century CE) was a Persian Muslim mathematician from Khurasan. According to Ibn al-Athir and Qutb al-Din al-Shirazi, he worked in the Seljuq observatory of Isfahan. Nezami Aruzi met him in Balkh in (in present‐day Afghanistan) in 1112 or 1113.\n\nHe was a contemporary of Umar al-Khayyam and Abd al-Raḥmān al-Khāzinī. He main work is entitled \"Irshād dhawī al-cirfān ilā ṣinācat al-qaffān\" (Guiding the possessors of learning in the art of the steelyard), a relatively long text on the theory of the steelyard balance with unequal arms. His other surviving works include a summary of Euclid's Elements, a text on geometrical measurements, and a treatise on meteorology in Persian language.\n\nAl-Isfazārī's corpus of mechanics is composed of two sets of texts, which have been published in \"Matn al-Muẓaffar al-Isfazārī fī cilmay al-aṯqāl wa’l-ḥiyal\" by the Al-Furqan Islamic Heritage Foundation.\n\n"}
{"id": "36378090", "url": "https://en.wikipedia.org/wiki?curid=36378090", "title": "Algorithmic cooling", "text": "Algorithmic cooling\n\nAlgorithmic cooling is an algorithmic method for transferring heat (or entropy) from some qubits to others or outside the system and into the environment, which results in a cooling effect. This method uses regular quantum operations on ensembles of qubits, and it can be shown that it can succeed beyond Shannon's bound on data compression. The phenomenon is a result of the connection between thermodynamics and information theory.\n\nThe cooling itself is done in an algorithmic manner using ordinary quantum operations. The input is a set of qubits, and the output is a subset of qubits cooled to a desired threshold determined by the user. This cooling effect may have usages in initializing cold (highly pure) qubits for quantum computation and in increasing polarization of certain spins in nuclear magnetic resonance. Therefore, it can be used in the initializing process taking place before a regular quantum computation.\n\nQuantum computers need qubits (quantum bits) on which they operate. Generally, in order to make the computation more reliable, the qubits must be as pure as possible, minimizing possible fluctuations. Since the purity of a qubit is related to von Neumann entropy and to temperature, making the qubits as pure as possible is equivalent to making them as cold as possible (or having as little entropy as possible). One method of cooling qubits is extracting entropy from them, thus purifying them. This can be done in two general ways: reversibly (namely, using unitary operations) or irreversibly (for example, using a heat bath). Algorithmic cooling is the name of a family of algorithms that are given a set of qubits and purify (cool) a subset of them to a desirable level.\n\nThis can also be viewed in a probabilistic manner. Since qubits are two-level systems, they can be regarded as coins, unfair ones in general. Purifying a qubit means (in this context) making the coin as unfair as possible: increasing the difference between the probabilities for tossing different results as much as possible. Moreover, the entropy previously mentioned can be viewed using the prism of information theory, which assigns entropy to any random variable. The purification can therefore be considered as using probabilistic operations (such as classical logical gates and conditional probability) for minimizing the entropy of the coins, making them more unfair.\n\nThe case in which the algorithmic method is reversible, such that the total entropy of the system is not changed, was first named \"molecular scale heat engine\", and is also named \"reversible algorithmic cooling\". This process cools some qubits while heating the others. It is limited by a variant of Shannon's bound on data compression and it can asymptotically reach quite close to the bound.\n\nA more general method, \"irreversible algorithmic cooling\", makes use of irreversible transfer of heat outside of the system and into the environment (and therefore may bypass the Shannon bound). Such an environment can be a heat bath, and the family of algorithms which use it is named \"heat-bath algorithmic cooling\". In this algorithmic process entropy is transferred reversibly to specific qubits (named reset spins) that are coupled with the environment much more strongly than others. After a sequence of reversible steps that let the entropy of these reset qubits increase they become hotter than the environment. Then the strong coupling results in a heat transfer (irreversibly) from these reset spins to the environment. The entire process may be repeated and may be applied recursively to reach low temperatures for some qubits.\n\nAlgorithmic cooling can be discussed using classical and quantum thermodynamics points of view.\n\nThe classical interpretation of \"cooling\" is transferring heat from one object to the other. However, the same process can be viewed as entropy transfer. For example, if two gas containers that are both in thermal equilibrium with two different temperatures are put in contact, entropy will be transferred from the \"hotter\" object (with higher entropy) to the \"colder\" one. This approach can be used when discussing the cooling of an object whose temperature is not always intuitively defined, e.g. a single particle. Therefore, the process of cooling spins can be thought of as a process of transferring entropy between spins, or outside of the system.\n\nThe concept of heat reservoir is discussed extensively in classical thermodynamics (for instance in carnot cycle). For the purposes of algorithmic cooling, it is sufficient to consider heat reservoirs, or \"heat baths\", as large objects whose temperature remains unchanged even when in contact with other (\"normal\" sized) objects. Intuitively, this can be pictured as a bath filled with room-temperature water that practically retains its temperature even when a small piece of hot metal is put in it.\n\nUsing the entropy form of thinking from the previous subsection, an object which is considered hot (whose entropy is large) can transfer heat (and entropy) to a colder heat bath, thus lowering its own entropy. This process results in cooling.\n\nUnlike entropy transfer between two \"regular\" objects which preserves the entropy of the system, entropy transfer to a heat bath is normally regarded as non-preserving. This is because the bath is normally not considered as a part of the relevant system, due to its size. Therefore, when transferring entropy to a heat bath, one can essentially lower the entropy of their system, or equivalently, cool it. Continuing this approach, the goal of algorithmic cooling is to reduce as much as possible the entropy of the system of qubits, thus cooling it.\n\nAlgorithmic cooling applies to quantum systems. Therefore, it is important to be familiar with both the core principles and the relevant notations.\n\nA qubit (or quantum bit) is a unit of information that can be in a superposition of two states, denoted as formula_1 and formula_2. The general superposition can be written as formula_3where formula_4 and formula_5. If one measures the state of the qubit in the orthonormal basis composed of formula_1 and formula_2, one gets the result formula_1 with probability formula_9 and the result formula_2 with probability formula_11.\n\nThe above description is known as a quantum pure state. A general mixed quantum state can be prepared as a probability distribution over pure states, and is represented by a density matrix of the general form formula_12, where each formula_13 is a pure state (see ket-bra notations) and each formula_14 is the probability of formula_15 in the distribution. The quantum states that play a major role in algorithmic cooling are mixed states in the diagonal form formula_16 for formula_17. Essentially, this means that the state is the pure formula_1 state with probability formula_19 and is pure formula_2 with probability formula_21. In the ket-bra notations, the density matrix is formula_22. For formula_23 the state is called pure, and for formula_24 the state is called completely mixed (represented by the normalized identity matrix). The completely mixed state represents a uniform probability distribution over the states formula_1 and formula_2.\n\nThe state formula_27 above is called formula_28-polarized, or formula_28-biased, since it deviates by formula_28 in the diagonal entries from the completely mixed state.\nAnother approach for the definition of bias or polarization is using Bloch sphere (or generally Bloch ball). Restricted to a diagonal density matrix, a state can be on the straight line connecting the antipodal points representing the states formula_1 and formula_2 (\"north and south poles\" of the sphere). In this approach, the formula_28 parameter (formula_34) is exactly the distance (up to a sign) of the state from the center of the ball, which represents the completely mixed state. For formula_35 the state is exactly on the poles and for formula_24 the state is exactly in the center. A bias can be negative (for example formula_37), and in this case the state is in the middle between the center and the south pole.\n\nIn the Pauli matrices representation form, an formula_28-biased quantum state is formula_39.\n\nSince quantum systems are involved, the entropy used here is von Neumann entropy. For a single qubit represented by the (diagonal) density matrix above, its entropy is formula_40 (where the logarithm is to base formula_41). This expression coincides with the entropy of an unfair coin with \"bias\" formula_28, meaning probability formula_19 for tossing heads. A coin with bias formula_35 is deterministic with zero entropy, and a coin with bias formula_24 is fair with maximal entropy (formula_46.\n\nThe relation between the coins approach and von Neumann entropy is an example of the relation between entropy in thermodynamics and in information theory.\n\nAn intuition for this family of algorithms can come from various fields and mindsets, which are not necessarily quantum. This is due to the fact that these algorithms do not explicitly use quantum phenomena in their operations or analysis, and mainly rely on information theory. Therefore, the problem can be inspected from a classical (physical, computational, etc.) point of view.\n\nThe physical intuition for this family of algorithms comes from classical thermodynamics.\n\nThe basic scenario is an array of qubits with equal initial biases. This means that the array contains small thermodynamic systems, each with the same entropy. The goal is to transfer entropy from some qubits to others, eventually resulting in a sub-array of \"cold\" qubits and another sub-array of \"hot\" qubits (the sub-arrays being distinguished by their qubits' entropies, as in the background section). The entropy transfers are restricted to be reversible, which means that the total entropy is conserved. Therefore, reversible algorithmic cooling can be seen as an act of redistributing the entropy of all the qubits to obtain a set of colder ones while the others are hotter.\n\nTo see the analogy from classical thermodynamics, two qubits can be considered as a gas container with two compartments, separated by a movable and heat-insulating partition. If external work is applied in order to move the partition in a reversible manner, the gas in one compartment is compressed, resulting in higher temperature (and entropy), while the gas in the other is expanding, similarly resulting in lower temperature (and entropy). Since it is reversible, the opposite action can be done, returning the container and the gases to the initial state. The entropy transfer here is analogous to the entropy transfer in algorithmic cooling, in the sense that by applying external work entropy can be transferred reversibly between qubits.\n\nThe basic scenario remains the same, however an additional object is present - a heat bath. This means that entropy can be transferred from the qubits to an external reservoir and some operations can be irreversible, which can be used for cooling some qubits without heating the others. In particular, hot qubits (hotter than the bath) that were on the receiving side of reversible entropy transfer can be cooled by letting them interact with the heat bath. The classical analogy for this situation is the Carnot refrigerator, specifically the stage in which the engine is in contact with the cold reservoir and heat (and entropy) flows from the engine to the reservoir.\n\nThe intuition for this family of algorithms can come from an extension of Von-Neumann's solution for the problem of obtaining fair results from a biased coin. In this approach to algorithmic cooling, the bias of the qubits is merely a probability bias, or the \"unfairness\" of a coin.\n\nTwo typical applications that require a large number of pure qubits are quantum error correction (QEC) and ensemble computing. In realizations of quantum computing (implementing and applying the algorithms on actual qubits), algorithmic cooling was involved in realizations in optical lattices. In addition, algorithmic cooling can be applied to \"in vivo\" magnetic resonance spectroscopy.\n\nQuantum error correction is a quantum algorithm for protection from errors. The algorithm operates on the relevant qubits (which operate within the computation) and needs a supply of new pure qubits for each round. This requirement can be weakened to purity above a certain thershold instead of requiring fully pure qubits. For this, algorithmic cooling can be used to produce qubits with the desired purity for quantum error correction.\n\nEnsemble computing is a computational model that uses a macroscopic number of identical computers. Each computer contains a certain number of qubits, and the computational operations are performed simultaneously on all the computers. The output of the computation can be obtained by measuring the state of the entire ensemble, which would be the average output of each computer in it. Since the number of computers is macroscopic, the output signal is easier to detect and measure than the output signal of each single computer.\n\nThis model is widely used in NMR quantum computing: each computer is represented by a single (identical) molecule, and the qubits of each computer are the nuclear spins of its atoms. The obtained (averaged) output is a detectable magnetic signal.\n\nNuclear magnetic resonance spectroscopy (sometimes called MRS - magnetic resonance spectroscopy) is a non-invasive technique related to MRI (magnetic resonance imaging) for analyzing metabolic changes \"in vivo\" (from Latin: \"within the living organism\"), which can potentially be used for diagnosing brain tumors, Parkinson's disease, depression, etc. It uses some magnetic properties of the relevant metabolites to measure their concentrations in the body, which are correlated with certain diseases. For example, the difference between the concentrations of the metabolites glutamate and glutamine can be linked to some stages of neurodegenerative diseases, such as Alzheimer's disease.\n\nSome uses of MRS focus on the carbon atoms of the metabolites (see carbon-13 nuclear magnetic resonance). One major reason for this is the presence of carbon in a large portion of all tested metabolites. Another reason is the ability to mark certain metabolites by the C isotope, which is more easy to measure than the usually used hydrogen atoms mainly because of its magnetic properties (such as its gyromagnetic ratio).\n\nIn MRS, the nuclear spins of the atoms of the metabolites are required to be with a certain degree of polarization, so the spectroscopy can succeed. Algorithmic cooling can be applied \"in vivo\", increasing the resolution and precision of the MRS. Realizations (not in vivo) of algorithmic cooling on metabolites with C isotope have been shown to increase the polarization of C in amino acids and other metabolites.\n\nMRS can be used to obtain biochemical information about certain body tissues in a non-invasive manner. This means that the operation must be carried out at room temperature. Some methods of increasing polarization of spins (such as hyperpolarization, and in particular dynamic nuclear polarization) are not able to operate under this condition since they require a cold environment (a typical value is 1K, about -272 degrees Celsius). On the other hand, algorithmic cooling can be operated in room temperature and be used in MRS \"in vivo,\" while methods that required lower temperature can be used in biopsy, outside of the living body.\n\nThe algorithm operates on an array of equally (and independently) biased qubits. After the algorithm transfers heat (and entropy) from some qubits to the others, the resulting qubits are rearranged in increasing order of bias. Then this array is divided into two sub-arrays: \"cold\" qubits (with bias exceeding a certain threshold chosen by the user) and \"hot\" qubits (with bias lower than that threshold). Only the \"cold\" qubits are used for further quantum computation. The basic procedure is called \"Basic Compression Subroutine\" or \"3 Bit Compression\".\n\nThe reversible case can be demonstrated on 3 qubits, using the probabilistic approach. Each qubit is represented by a \"coin\" (two-level system) whose sides are labeled 0 and 1, and with a certain bias: each coin is independently with bias formula_28, meaning probability formula_19 for tossing 0. The coins are formula_49 and the goal is to use coins formula_50 to cool coin (qubit) formula_51. The procedure:\nAfter this procedure, the average (expected value) of the bias of coin formula_51 is, to leading order, formula_57.\n\nCoins formula_50 are used for C-NOT operation, also known as XOR (exclusive or). The operation is applied in the following manner: formula_59, which means that formula_60 is computed and replaces the old value of formula_54, and formula_55 remain unchanged. More specifically, the following operation is applied:\nNow, the result of coin formula_67 is checked (without looking at formula_68). Classically, this means that the result of coin formula_63 must be \"forgotten\" (cannot be used anymore). This is somewhat problematic classically, because the result of coin formula_63 is no longer probabilistic; however, the equivalent quantum operators (which are the ones that are actually used in realizations and implementations of the algorithm) are capable of doing so.\n\nAfter the C-NOT operation is over, the bias of coin formula_71 is computed using conditional probability:\n\nCoins formula_82 are used for C-SWAP operation. The operation is applied in the following manner: formula_83, which means that formula_68 are swapped if formula_72.\n\nAfter the C-SWAP operation is over:\nThe average bias of coin formula_89 can be calculated by looking at those two cases, using the final bias in each case and the probability of each case:\nUsing the approximation formula_101, the new average bias of coin formula_89 is formula_57. Therefore, these two steps increase the polarization of coin formula_51 on average.\n\nThe algorithm can be written using quantum operations on qubits, as opposed to the classical treatment. In particular, the C-NOT and C-SWAP steps can be replaced by a single unitary quantum operator that operates on the 3 qubits. Although this operation changes qubits formula_50 in a different manner than the two classical steps, it yields the same final bias for qubit formula_51. The operator formula_107 can be uniquely defined by its action on the computational basis of the Hilbert space of 3 qubits:\nIn matrix form, this operator is the identity matrix of size 8, except that the 4th and 5th rows are swapped. The result of this operation can be obtained by writing the product state of the 3 qubits, formula_116, and applying formula_107 on it. Afterwards, the bias of qubit formula_51 can be calculated by projecting its state on the state formula_1 (without projecting qubits formula_50) and taking the trace of the result (see density matrix measurement):\n\nformula_121, where formula_122 is the projection on the state formula_1.\n\nAgain, using the approximation formula_101, the new average bias of coin formula_51 is formula_57.\n\nThe irreversible case is an extension of the reversible case: it uses the reversible algorithm as a subroutine. The irreversible algorithm contains another procedure called \"Refresh\" and extends the reversible one by using a heat bath. This allows for cooling certain qubits (called \"reset qubits\") without affecting the others, which results in an overall cooling of all the qubits as a system. The cooled reset qubits are used for cooling the rest (called \"computational qubits\") by applying a compression on them which is similar to the basic compression subroutine from the reversible case. The \"insulation\" of the computational qubits from the heat bath is a theoretical idealization that does not always hold when implementing the algorithm. However, with a proper choice of the physical implementation of each type of qubit, this assumption fairly holds.\n\nThere are many different versions of this algorithm, with different uses of the reset qubits and different achievable biases. The common idea behind them can be demonstrated using three qubits: two computational qubits formula_127 and one reset qubit formula_63.\n\nEach of the three qubits is initially in a completely mixed state with bias formula_129 (see the background section). The following steps are then applied:\nEach round of the algorithm consists of three iterations, and each iteration consists of these two steps (refresh, and then compression). The compression step in each iteration is slightly different, but its goal is to sort the qubits in descending order of bias, so that the reset qubit would have the smallest bias (namely, the highest temperature) of all qubits. This serves two goals:\nWhen writing the density matrices after each iteration, the compression step in the 1st round can be effectively treated as follows:\nThe description of the compression step in the following rounds depends on the state of the system before the round has begun and may be more complicated than the above description. In this illustrative description of the algorithm, the boosted bias of qubit formula_51 (obtained after the end of the first round) is formula_137, where formula_138 is the bias of the qubits within the heat bath. This result is obtained after the last compression step; just before this step, the qubits were each formula_138-biased, which is exactly the state of the qubits before the reversible algorithm is applied.\n\nThe contact that is established between the reset qubit and the heat bath can be modeled in a several possible ways:\nIn both ways, the result is a reset qubit whose bias is identical to the bias of the qubits in the bath. In addition, the resulted reset qubit is uncorrelated with the other ones, independently of the correlations between them before the refresh step was held. Therefore, the refresh step can be viewed as discarding the information about the current reset qubit and gaining information about a fresh new one from the bath.\n\nThe goal of this step is to reversibly redistribute the entropy of all qubits, such that the biases of the qubits are in descending (or non-ascending) order. The operation is done reversibly in order to prevent the entropy of the entire system from increasing (as it cannot decrease in a closed system, see entropy). In terms of temperature, this step rearranges the qubits in ascending order of temperature, so that the reset qubits are the hottest. In the example of the three qubits formula_49, this means that after the compression is done, the bias of qubit formula_51 is the highest and the bias of formula_63 is the lowest. In addition, the compression is used for the cooling of the computational qubits.\n\nThe state of the system will be denoted by formula_150 if the qubits formula_151 are uncorrelated with each other (namely, if the system is in a product state) and their corresponding biases are formula_152.\n\nThe compression can be described as a sort operation on the diagonal entries of the density matrix which describes the system. For instance, if the state of the system after a certain reset step is formula_153, then the compression operates on the state as follows:\n\nformula_154\n\nThis notation denotes a diagonal matrix whose diagonal entries are listed within the parentheses. The density matrices formula_155 represent the state of the system (including possible correlations between the qubits) before and after the compression step, respectively. In the above notations, the state after compression is formula_156.\n\nThis sort operation is used for the rearrangement of the qubits in descending order of bias. As in the example, for some cases the sort operation can be described by a simpler operation, such as swap. However, the general form of the compression operation is a sort operation on the diagonal entries of the density matrix.\n\nFor an intuitive demonstration of the compression step, the flow of the algorithm in the 1st round is presented below:\n\nAfter the 1st round is over, the bias of the reset qubit (formula_167) is smaller than the bias of the heat bath (formula_168). This means that in the next refresh step (in the 2nd round of the algorithm), the reset qubit will be replaced by a fresh qubit with bias formula_168: this cools the entire system, similarly to the previous refresh steps. Afterwards, the algorithm continues in a similar way.\n\nThe number of rounds is not bounded: since the biases of the reset qubits asymptotically reach the bias of the bath after each round, the bias of the target computational qubit asymptotically reaches its limit as the algorithm proceeds. The target qubit is the computational qubit that the algorithm aims to cool the most. The \"cooling limit\" (the maximum bias the target qubit can reach) depends on the bias of the bath and the number of qubits of each kind in the system. If the number of the computational qubits (excluding the target one) is formula_170 and the number of reset qubits is formula_171, then the cooling limit is formula_172. In the case where formula_173, the maximal polarization that can be obtained is proportional to formula_174. Otherwise, the maximal bias reaches arbitrarily close to formula_175. The number of rounds required in order to reach a certain bias depends on the desired bias, the bias of the bath and the number of qubits, and moreover varies between different versions of the algorithm.\n\nThere are other theoretical results which give bounds on the number of iterations required to reach a certain bias. For example, if the bias of the bath is formula_176, then the number of iterations required to cool a certain qubit to bias formula_177 is at least formula_178.\n"}
{"id": "21663267", "url": "https://en.wikipedia.org/wiki?curid=21663267", "title": "Analytic polyhedron", "text": "Analytic polyhedron\n\nIn mathematics, especially several complex variables, an analytic polyhedron is a subset of the complex space of the form\n\nwhere is a bounded connected open subset of and formula_2 are holomorphic on . If formula_2 above are polynomials, then the set is called a polynomial polyhedron. Every analytic polyhedron is a domain of holomorphy and it is thus pseudo-convex.\n\nThe boundary of an analytic polyhedron is the union of the set of hypersurfaces\n\nAn analytic polyhedron is a \"Weil polyhedron\", or Weil domain if the intersection of hypersurfaces has dimension no greater than .\n\n\n"}
{"id": "19783560", "url": "https://en.wikipedia.org/wiki?curid=19783560", "title": "Appell series", "text": "Appell series\n\nIn mathematics, Appell series are a set of four hypergeometric series \"F\", \"F\", \"F\", \"F\" of two variables that were introduced by and that generalize Gauss's hypergeometric series \"F\" of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable.\n\nThe Appell series \"F\" is defined for |\"x\"| < 1, |\"y\"| < 1 by the double series:\n\nwhere the Pochhammer symbol (\"q\") represents the rising factorial:\n\nwhere the second equality is true for all complex formula_4 except formula_5.\n\nFor other values of \"x\" and \"y\" the function \"F\" can be defined by analytic continuation.\n\nSimilarly, the function \"F\" is defined for |\"x\"| + |\"y\"| < 1 by the series:\n\nthe function \"F\" for |\"x\"| < 1, |\"y\"| < 1 by the series:\n\nand the function \"F\" for |\"x\"| + |\"y\"| < 1 by the series:\n\nLike the Gauss hypergeometric series \"F\", the Appell double series entail recurrence relations among contiguous functions. For example, a basic set of such relations for Appell's \"F\" is given by:\n\nAny other relation valid for \"F\" can be derived from these four.\n\nSimilarly, all recurrence relations for Appell's \"F\" follow from this set of five:\n\nFor Appell's \"F\", the following derivatives result from the definition by a double series:\n\nFrom its definition, Appell's \"F\" is further found to satisfy the following system of second-order differential equations:\n\nA system partial differential equations for \"F\" is\n\nThe system have solution\n\nSimilarly, for \"F\" the following derivatives result from the definition:\n\nAnd for \"F\" the following system of differential equations is obtained:\n\nA system partial differential equations for \"F\" is\n\nThe system have solution\n\nThe four functions defined by Appell's double series can be represented in terms of double integrals involving elementary functions only . However, discovered that Appell's \"F\" can also be written as a one-dimensional Euler-type integral:\n\nThis representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration.\n\nPicard's integral representation implies that the incomplete elliptic integrals \"F\" and \"E\" as well as the complete elliptic integral Π are special cases of Appell's \"F\":\n\n\n"}
{"id": "14559354", "url": "https://en.wikipedia.org/wiki?curid=14559354", "title": "Artstein's theorem", "text": "Artstein's theorem\n\nArtstein's theorem states that a dynamical system has a differentiable control-Lyapunov function if and only if there exists a regular stabilizing feedback: formula_1.\n\n"}
{"id": "1725134", "url": "https://en.wikipedia.org/wiki?curid=1725134", "title": "BK-space", "text": "BK-space\n\nIn functional analysis and related areas of mathematics, a BK-space or Banach coordinate space is a sequence space endowed with a suitable norm to turn it into a Banach space. All BK-spaces are normable FK-spaces.\n\n"}
{"id": "6895284", "url": "https://en.wikipedia.org/wiki?curid=6895284", "title": "Baltic Way (mathematical contest)", "text": "Baltic Way (mathematical contest)\n\nOriginally, the three Baltic states participated, but the list of invitees has since grown to include all countries around the Baltic Sea; Germany sends a team representing only its northernmost parts, and Russia a team from St. Petersburg. Iceland is invited on grounds of being the first state to recognize the newfound independence of the Baltic states. Extra \"guest\" teams are occasionally invited at the discretion of the organizers: Israel was invited in 2001, Belarus in 2004 and 2014, Belgium in 2005, South Africa in 2011 and the Netherlands in 2015. Responsibility for organizing the contest circulates among the regular participants.\n"}
{"id": "49569", "url": "https://en.wikipedia.org/wiki?curid=49569", "title": "Bayes' theorem", "text": "Bayes' theorem\n\nIn probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.\n\nOne of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in Bayes' theorem may have different probability interpretations. With the Bayesian probability interpretation the theorem expresses how a subjective degree of belief should rationally change to account for availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.\n\nBayes' theorem is named after Reverend Thomas Bayes (; 1701–1761), who first provided an equation that allows new evidence to update beliefs in his \"An Essay towards solving a Problem in the Doctrine of Chances\" (1763). It was further developed by Pierre-Simon Laplace, who first published the modern formulation in his 1812 \"Théorie analytique des probabilités\". Sir Harold Jeffreys put Bayes' algorithm and Laplace's formulation on an axiomatic basis. Jeffreys wrote that Bayes' theorem \"is to the theory of probability what the Pythagorean theorem is to geometry\".\n\nBayes' theorem is stated mathematically as the following equation:\n\nwhere formula_2 and formula_3 are events and formula_4.\n\n\nSuppose that a test for using a particular drug is 99% sensitive and 99% specific. That is, the test will produce 99% true positive results for drug users and 99% true negative results for non-drug users. Suppose that 0.5% of people are users of the drug. What is the probability that a randomly selected individual with a positive test is a drug user?\n\nEven if an individual tests positive, it is more likely that they do not use the drug than that they do. This is because the number of non-users is large compared to the number of users. The number of false positives outweighs the number of true positives. For example, if 1000 individuals are tested, there are expected to be 995 non-users and 5 users. From the 995 non-users, 0.01 × 995 ≃ 10 false positives are expected. From the 5 users, 0.99 × 5 ≈ 5 true positives are expected. Out of 15 positive results, only 5 are genuine.\n\nThe importance of specificity in this example can be seen by calculating that even if sensitivity is raised to 100% and specificity remains at 99% then the probability of the person being a drug user only rises from 33.2% to 33.4%, but if the sensitivity is held at 99% and the specificity is increased to 99.5% then the probability of the person being a drug user rises to about 49.9%.\n\nThe entire output of a factory is produced on three machines. The three machines account for 20%, 30%, and 50% of the factory output. The fraction of defective items produced is 5% for the first machine; 3% for the second machine; and 1% for the third machine. If an item is chosen at random from the total output and is found to be defective, what is the probability that it was produced by the third machine?\n\nOnce again, the answer can be reached without recourse to the formula by applying the conditions to any hypothetical number of cases. For example, if 100,000 items are produced by the factory, 20,000 will be produced by Machine A, 30,000 by Machine B, and 50,000 by Machine C. Machine A will produce 1000 defective items, Machine B 900, and Machine C 500. Of the total 2400 defective items, only 500, or 5/24 were produced by Machine C.\n\nA solution is as follows. Let \"X\" denote the event that a randomly chosen item was made by the \"i\" machine (for \"i\" = A,B,C). Let \"Y\" denote the event that a randomly chosen item is defective. Then, we are given the following information:\nIf the item was made by the first machine, then the probability that it is defective is 0.05; that is, \"P\"(\"Y\" | \"X\") = 0.05. Overall, we have\nTo answer the original question, we first find \"P\"(Y). That can be done in the following way:\n\nHence 2.4% of the total output of the factory is defective.\n\nWe are given that \"Y\" has occurred, and we want to calculate the conditional\nprobability of \"X\". By Bayes' theorem,\n\nGiven that the item is defective, the probability that it was made by the third\nmachine is only 5/24. Although machine C produces half of the total output, it\nproduces a much smaller fraction of the defective items. Hence the knowledge\nthat the item selected was defective enables us to replace the prior probability\n\"P\"(\"X\") = 1/2 by the smaller posterior probability \"P\"(X | \"Y\") = 5/24.\n\nThe interpretation of Bayes' theorem depends on the interpretation of probability ascribed to the terms. The two main interpretations are described below.\n\nIn the Bayesian (or epistemological) interpretation, probability measures a \"degree of belief.\" Bayes' theorem then links the degree of belief in a proposition before and after accounting for evidence. For example, suppose it is believed with 50% certainty that a coin is twice as likely to land heads than tails. If the coin is flipped a number of times and the outcomes observed, that degree of belief may rise, fall or remain the same depending on the results.\n\nFor proposition \"A\" and evidence \"B\",\n\nFor more on the application of Bayes' theorem under the Bayesian interpretation of probability, see Bayesian inference.\n\nIn the frequentist interpretation, probability measures a \"proportion of outcomes.\" For example, suppose an experiment is performed many times. \"P\"(\"A\") is the proportion of outcomes with property \"A\", and \"P\"(\"B\") that with property \"B\". \"P\"(\"B\" | \"A\" ) is the proportion of outcomes with property \"B\" \"out of\" outcomes with property \"A\", and \"P\"(\"A\" | \"B\" ) the proportion of those with \"A\" \"out of\" those with \"B\".\n\nThe role of Bayes' theorem is best visualized with tree diagrams, as shown to the right. The two diagrams partition the same outcomes by \"A\" and \"B\" in opposite orders, to obtain the inverse probabilities. Bayes' theorem serves as the link between these different partitionings.\n\nAn entomologist spots what might be a rare subspecies of beetle, due to the pattern on its back. In the rare subspecies, 98% have the pattern, or \"P\"(Pattern | Rare) = 98%. In the common subspecies, 5% have the pattern. The rare subspecies accounts for only 0.1% of the population. How likely is the beetle having the pattern to be rare, or what is \"P\"(Rare | Pattern)?\n\nFrom the extended form of Bayes' theorem (since any beetle can be only rare or common),\n\nFor events \"A\" and \"B\", provided that \"P\"(\"B\") ≠ 0,\n\nIn many applications, for instance in Bayesian inference, the event \"B\" is fixed in the discussion, and we wish to consider the impact of its having been observed on our belief in various possible events \"A\". In such a situation the denominator of the last expression, the probability of the given evidence \"B\", is fixed; what we want to vary is \"A\". Bayes' theorem then shows that the posterior probabilities are proportional to the numerator:\n\nThe posterior is proportional to the prior times the likelihood.\n\nIf events \"A\", \"A\", ..., are mutually exclusive and exhaustive, i.e., one of them is certain to occur but no two can occur together, and we know their probabilities up to proportionality, then we can determine the proportionality constant by using the fact that their probabilities must add up to one. For instance, for a given event \"A\", the event \"A\" itself and its complement ¬\"A\" are exclusive and exhaustive. Denoting the constant of proportionality by \"c\" we have\n\nAdding these two formulas we deduce that\n\nor\n\nAnother form of Bayes' Theorem that is generally encountered when looking at two competing statements or hypotheses is:\n\nFor an epistemological interpretation:\n\nFor proposition \"A\" and evidence or background \"B\",\n\n\nOften, for some partition {\"A\"} of the sample space, the event space is given or conceptualized in terms of \"P\"(\"A\") and \"P\"(\"B\" | \"A\"). It is then useful to compute \"P\"(\"B\") using the law of total probability:\n\nIn the special case where \"A\" is a binary variable:\n\nConsider a sample space Ω generated by two random variables \"X\" and \"Y\". In principle, Bayes' theorem applies to the events \"A\" = {\"X\" = \"x\"} and \"B\" = {\"Y\" = \"y\"}. However, terms become 0 at points where either variable has finite probability density. To remain useful, Bayes' theorem may be formulated in terms of the relevant densities (see Derivation).\n\nIf \"X\" is continuous and \"Y\" is discrete,\n\nwhere each formula_34 is a density function.\n\nIf \"X\" is discrete and \"Y\" is continuous,\n\nIf both \"X\" and \"Y\" are continuous,\n\nA continuous event space is often conceptualized in terms of the numerator terms. It is then useful to eliminate the denominator using the law of total probability. For \"f\"(\"y\"), this becomes an integral:\n\nBayes' theorem in odds form is:\n\nwhere\n\nis called the Bayes factor or likelihood ratio and the odds between two events is simply the ratio of the probabilities of the two events. Thus\n\nSo the rule says that the posterior odds are the prior odds times the Bayes factor, or in other words, posterior is proportional to prior times likelihood.\n\nBayes' theorem may be derived from the definition of conditional probability:\n\nwhere formula_44 is the joint probability of both A and B being true, because\n\nFor two continuous random variables \"X\" and \"Y\", Bayes' theorem may be analogously derived from the definition of conditional density:\n\nTherefore\n\n\"Bayes' theorem\" represents a generalisation of contraposition which in propositional logic can be expressed as:\n\nThe corresponding formula in terms of probability calculus is Bayes' theorem which in its expanded form is expressed as: \n\nIn the equation above the conditional probability formula_53 generalizes the logical statement formula_54, i.e. in addition to assigning TRUE or FALSE we can also assign any probability to the statement. The term formula_55 denotes the prior probability (aka. the base rate) of formula_2. Assume that formula_57 is equivalent to formula_58 being TRUE, and that formula_59 is equivalent to formula_58 being FALSE. It is then easy to see that formula_57 when formula_62 i.e. when formula_63 is TRUE. This is because formula_64 so that the fraction on the right-hand side of the equation above is equal to 1, and hence formula_57 which is equivalent to formula_58 being TRUE. Hence, Bayes' theorem represents a generalization of contraposition.\n\n\"Bayes' theorem\" represents a special case of conditional inversion in subjective logic expressed as:\n\nHence, the subjective Bayes' theorem represents a generalization of Bayes' theorem.\n\nBayes' theorem was named after Thomas Bayes (1701–1761), who studied how to compute a distribution for the probability parameter of a binomial distribution (in modern terminology). Bayes' unpublished manuscript was significantly edited by Richard Price before it was posthumously read at the Royal Society. Price edited Bayes' major work \"An Essay towards solving a Problem in the Doctrine of Chances\" (1763), which appeared in \"Philosophical Transactions\", and contains Bayes' Theorem. Price wrote an introduction to the paper which provides some of the philosophical basis of Bayesian statistics. In 1765, he was elected a Fellow of the Royal Society in recognition of his work on the legacy of Bayes.\n\nThe French mathematician Pierre-Simon Laplace reproduced and extended Bayes' results in 1774, apparently unaware of Bayes' work. The Bayesian interpretation of probability was developed mainly by Laplace.\n\nStephen Stigler suggested in 1983 that Bayes' theorem was discovered by Nicholas Saunderson, a blind English mathematician, some time before Bayes; that interpretation, however, has been disputed.\nMartyn Hooper and Sharon McGrayne have argued that Richard Price's contribution was substantial:\n\n\n\n"}
{"id": "1134659", "url": "https://en.wikipedia.org/wiki?curid=1134659", "title": "Block code", "text": "Block code\n\nIn coding theory, block codes are a large and important family of error-correcting codes that encode data in blocks.\nThere is a vast number of examples for block codes, many of which have a wide range of practical applications. The abstract definition of block codes is conceptually useful because it allows coding theorists, mathematicians, and computer scientists to study the limitations of \"all\" block codes in a unified way.\nSuch limitations often take the form of \"bounds\" that relate different parameters of the block code to each other, such as its rate and its ability to detect and correct errors.\n\nExamples of block codes are Reed–Solomon codes, Hamming codes, Hadamard codes, Expander codes, Golay codes, and Reed–Muller codes. These examples also belong to the class of linear codes, and hence they are called linear block codes. More particularly, these codes are known as algebraic block codes, or cyclic block codes, because they can be generated using boolean polynomials.\n\nAlgebraic block codes are typically hard-decoded using algebraic decoders.\n\nThe term \"block code\" may also refer to any error-correcting code that acts on a block of formula_1 bits of input data to produce formula_2 bits of output data formula_3. Consequently, the block coder is a \"memoryless\" device. Under this definition codes such as turbo codes, terminated convolutional codes and other iteratively decodable codes (turbo-like codes) would also be considered block codes. A non-terminated convolutional encoder would be an example of a non-block (unframed) code, which has \"memory\" and is instead classified as a \"tree code\".\n\nThis article deals with \"algebraic block codes\".\n\nError-correcting codes are used to reliably transmit digital data over unreliable communication channels subject to channel noise.\nWhen a sender wants to transmit a possibly very long data stream using a block code, the sender breaks the stream up into pieces of some fixed size. Each such piece is called \"message\" and the procedure given by the block code encodes each message individually into a codeword, also called a \"block\" in the context of block codes. The sender then transmits all blocks to the receiver, who can in turn use some decoding mechanism to (hopefully) recover the original messages from the possibly corrupted received blocks.\nThe performance and success of the overall transmission depends on the parameters of the channel and the block code.\n\nFormally, a block code is an injective mapping\nHere, formula_5 is a finite and nonempty set and formula_1 and formula_2 are integers. The meaning and significance of these three parameters and other parameters related to the code are described below.\n\nThe data stream to be encoded is modeled as a string over some alphabet formula_5. The size formula_9 of the alphabet is often written as formula_10. If formula_11, then the block code is called a \"binary\" block code. In many applications it is useful to consider formula_10 to be a prime power, and to identify formula_5 with the finite field formula_14.\n\nMessages are elements formula_15 of formula_16, that is, strings of length formula_1.\nHence the number formula_1 is called the message length or dimension of a block code.\n\nThe block length formula_2 of a block code is the number of symbols in a block. Hence, the elements formula_20 of formula_21 are strings of length formula_2 and correspond to blocks that may be received by the receiver. Hence they are also called received words.\nIf formula_23 for some message formula_15, then formula_20 is called the codeword of formula_15.\n\nThe rate of a block code is defined as the ratio between its message length and its block length:\nA large rate means that the amount of actual message per transmitted block is high. In this sense, the rate measures the transmission speed and the quantity formula_28 measures the overhead that occurs due to the encoding with the block code.\nIt is a simple information theoretical fact that the rate cannot exceed formula_29 since data cannot in general be losslessly compressed. Formally, this follows from the fact that the code formula_30 is an injective map.\n\nThe distance or minimum distance of a block code is the minimum number of positions in which any two distinct codewords differ, and the relative distance formula_31 is the fraction formula_32.\nFormally, for received words formula_33, let formula_34 denote the Hamming distance between formula_35 and formula_36, that is, the number of positions in which formula_35 and formula_36 differ.\nThen the minimum distance formula_39 of the code formula_30 is defined as\nSince any code has to be injective, any two codewords will disagree in at least one position, so the distance of any code is at least formula_29. Besides, the distance equals the minimum weight for linear block codes because:\n\nA larger distance allows for more error correction and detection.\nFor example, if we only consider errors that may change symbols of the sent codeword but never erase or add them, then the number of errors is the number of positions in which the sent codeword and the received word differ.\nA code with distance allows the receiver to detect up to formula_44 transmission errors since changing formula_44 positions of a codeword can never accidentally yield another codeword. Furthermore, if no more than formula_46 transmission errors occur, the receiver can uniquely decode the received word to a codeword. This is because every received word has at most one codeword at distance formula_46. If more than formula_46 transmission errors occur, the receiver cannot uniquely decode the received word in general as there might be several possible codewords. One way for the receiver to cope with this situation is to use list decoding, in which the decoder outputs a list of all codewords in a certain radius.\n\nThe notation formula_49 describes a block code over an alphabet formula_5 of size formula_10, with a block length formula_2, message length formula_1, and distance formula_39.\nIf the block code is a linear block code, then the square brackets in the notation formula_55 are used to represent that fact.\nFor binary codes with formula_11, the index is sometimes dropped.\nFor maximum distance separable codes, the distance is always formula_57, but sometimes the precise distance is not known, non-trivial to prove or state, or not needed. In such cases, the formula_39-component may be missing.\n\nSometimes, especially for non-block codes, the notation formula_59 is used for codes that contain formula_60 codewords of length formula_2. For block codes with messages of length formula_1 over an alphabet of size formula_10, this number would be formula_64.\n\nAs mentioned above, there are a vast number of error-correcting codes that are actually block codes.\nThe first error-correcting code was the Hamming(7,4) code, developed by Richard W. Hamming in 1950. This code transforms a message consisting of 4 bits into a codeword of 7 bits by adding 3 parity bits. Hence this code is a block code. It turns out that it is also a linear code and that it has distance 3. In the shorthand notation above, this means that the Hamming(7,4) code is a formula_65 code.\n\nReed–Solomon codes are a family of formula_55 codes with formula_57 and formula_10 being a prime power. Rank codes are family of formula_55 codes with formula_70. Hadamard codes are a family of formula_71 codes with formula_72 and formula_73.\n\nA codeword formula_74could be considered as a point in the formula_2-dimension space formula_21 and the code formula_77 is the subset of formula_21. A code formula_77 has distance formula_39 means that formula_81, there is no other codeword in the \"Hamming ball\" centered at formula_20 with radius formula_44, which is defined as the collection of formula_2-dimension words whose \"Hamming distance\" to formula_20 is no more than formula_44. Similarly, formula_87 with (minimum) distance formula_39 has the following properties:\nLet formula_97 be the maximum number of codewords in a Hamming ball of radius for any code formula_98 of distance .\n\nThen we have the \"Johnson Bound\" : formula_99, if formula_100\n\nBlock codes are tied to the sphere packing problem which has received some attention over the years. In two dimensions, it is easy to visualize. Take a bunch of pennies flat on the table and push them together. The result is a hexagon pattern like a bee's nest. But block codes rely on more dimensions which cannot easily be visualized. The powerful Golay code used in deep space communications uses 24 dimensions. If used as a binary code (which it usually is), the dimensions refer to the length of the codeword as defined above.\n\nThe theory of coding uses the \"N\"-dimensional sphere model. For example, how many pennies can be packed into a circle on a tabletop or in 3 dimensions, how many marbles can be packed into a globe. Other considerations enter the choice of a code. For example, hexagon packing into the constraint of a rectangular box will leave empty space at the corners. As the dimensions get larger, the percentage of empty space grows smaller. But at certain dimensions, the packing uses all the space and these codes are the so-called perfect codes. There are very few of these codes.\n\nAnother property is the number of neighbors a single codeword may have. \nAgain, consider pennies as an example. First we pack the pennies in a rectangular grid. Each penny will have 4 near neighbors (and 4 at the corners which are farther away). In a hexagon, each penny will have 6 near neighbors. Respectively, in three and four dimensions, the maximum packing is given by the 12-face and 24-cell with 12 and 24 neighbors, respectively. When we increase the dimensions, the number of near neighbors increases very rapidly. In general, the value is given by the kissing numbers.\n\nThe result is that the number of ways for noise to make the receiver choose\na neighbor (hence an error) grows as well. This is a fundamental limitation\nof block codes, and indeed all codes. It may be harder to cause an error to\na single neighbor, but the number of neighbors can be large enough so the\ntotal error probability actually suffers.\n\n\n\n"}
{"id": "3827112", "url": "https://en.wikipedia.org/wiki?curid=3827112", "title": "Block walking", "text": "Block walking\n\nIn combinatorial mathematics, block walking is a method useful in thinking about sums of combinations graphically as \"walks\" on Pascal's triangle. As the name suggests, block walking problems involve counting the number of ways an individual can walk from one corner A of a city block to another corner B of another city block given restrictions on the number of blocks the person may walk, the directions the person may travel, the distance from A to B, et cetera.\n\nSuppose such an individual, say \"Fred\", must walk exactly \"k\" blocks to get to a point B that is exactly \"k\" blocks from A. It is convenient to regard Fred's starting point A as the origin, formula_1, of a rectangular array of lattice points and B as some lattice point formula_2, e units \"East\" and \"n\" units \"North\" of A, where formula_3 and both formula_4 and formula_5 are nonnegative.\n\nA \"brute force\" solution to this problem may be obtained by systematically counting the number of ways Fred can reach each point formula_6 where\n\nwithout backtracking (i.e. only traveling North or East from one point to another) until a pattern is observed. For example, the number of ways Fred could go from formula_1 to formula_10 or (0,1) is exactly one; to (1,1) is two; to (2,0) or (0,2) is one; to (1,2) or (2,1) is three; and so on. Actually, you could receive the number of ways to get to a particular point by adding up the number of ways you can get to the point south of it and the number of ways you can get to the point west of it.(With the starting point being zero and all the points directly north and south of it one.) In general, one soon discovers that the number of paths from A to any such X corresponds to an entry of Pascal's Triangle.\n\nSince the problem involves counting a finite, discrete number of paths between lattice points, it is reasonable to assume a combinatorial solution exists to the problem. Towards this end, we note that for Fred to still be on a path that will take him from A to B over formula_11 blocks, at any point X he must either travel along one of the unit vectors <1,0> and <0,1>. For the sake of clarity, let formula_12 and formula_13. Given the coordinates of B, regardless of the path Fred travels he must walk along the vectors E and N exactly formula_4 and formula_5 times, respectively. As such, the problem reduces to finding the number of distinct rearrangements of the word\nwhich is equivalent to finding the number of ways to choose formula_4 indistinct objects from a group of formula_11. Thus the total number of paths Fred could take from A to B traveling only formula_11 blocks is\n\n\n"}
{"id": "33294058", "url": "https://en.wikipedia.org/wiki?curid=33294058", "title": "Busemann–Petty problem", "text": "Busemann–Petty problem\n\nIn the mathematical field of convex geometry, the Busemann–Petty problem, introduced by , asks whether it is true that a symmetric convex body with larger central hyperplane sections has larger volume. More precisely, if \"K\", \"T\" are symmetric convex bodies in R such that\n\nfor every hyperplane \"A\" passing through the origin, is it true that Vol \"K\" ≤ Vol \"T\"?\n\nBusemann and Petty showed that the answer is positive if \"K\" is a ball. In general, the answer is positive in dimensions at most 4, and negative in dimensions at least 5.\n\n showed that the Busemann–Petty problem has a negative solution in dimensions at least 12, and this bound was reduced to dimensions at least 5 by several other authors. pointed out a particularly simple counterexample: all sections of the unit volume cube have measure at most , while in dimensions at least 10 all central sections of the unit volume ball have measure at least . introduced intersection bodies, and showed that the Busemann–Petty problem has a positive solution in a given dimension if and only if every symmetric convex body is an intersection body. An intersection body is a star body whose radial function in a given direction \"u\" is the volume of the hyperplane section \"u\" ∩ \"K\" for some fixed star body \"K\". \n\n\n"}
{"id": "23659805", "url": "https://en.wikipedia.org/wiki?curid=23659805", "title": "Computer algebra", "text": "Computer algebra\n\nIn computational mathematics, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes \"exact\" computation with expressions containing variables that have no given value and are manipulated as symbols.\n\nSoftware applications that perform symbolic calculations are called \"computer algebra systems\", with the term \"system\" alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.\n\nComputer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.\n\nSome authors distinguish \"computer algebra\" from \"symbolic computation\" using the latter name to refer to kinds of symbolic computation other than the computation with mathematical formulas. Some authors use \"symbolic computation\" for the computer science aspect of the subject and \"computer algebra\" for the mathematical aspect. In some languages the name of the field is not a direct translation of its English name. Typically, it is called \"calcul formel\" in French, which means \"formal computation\". This name reflects the ties this field has with formal methods.\n\nSymbolic computation has also been referred to, in the past, as \"symbolic manipulation\", \"algebraic manipulation\", \"symbolic processing\", \"symbolic mathematics\", or \"symbolic algebra\", but these terms, which also refer to non-computational manipulation, are no longer used in reference to computer algebra.\n\nThere is no learned society that is specific to computer algebra, but this function is assumed by the special interest group of the Association for Computing Machinery named SIGSAM (Special Interest Group\non Symbolic and Algebraic Manipulation).\n\nThere are several annual conferences on computer algebra, the premier being ISSAC (International Symposium on Symbolic and Algebraic Computation), which is regularly sponsored by SIGSAM.\n\nThere are several journals specializing in computer algebra, the top one being Journal of Symbolic Computation founded in 1985 by Bruno Buchberger. There are also several other journals that regularly publish articles in computer algebra.\n\nAs numerical software is highly efficient for approximate numerical computation, it is common, in computer algebra, to emphasize \"exact\" computation with exactly represented data. Such an exact representation implies that, even when the size of the output is small, the intermediate data generated during a computation may grow in an unpredictable way. This behavior is called \"expression swell\". To obviate this problem, various methods are used in the representation of the data, as well as in the algorithms that manipulate them.\n\nThe usual numbers systems used in numerical computation are floating point numbers and integers of a fixed bounded size. None of these is convenient for computer algebra, due to expression swell.\n\nTherefore, the basic numbers used in computer algebra are the integers of the mathematicians, commonly represented by an unbounded signed sequence of digits in some base of numeration, usually the largest base allowed by the machine word. These integers allow to define the rational numbers, which are irreducible fractions of two integers.\n\nProgramming an efficient implementation of the arithmetic operations is a hard task. Therefore, most free computer algebra systems and some commercial ones, like Maple (software), use the GMP library, which is thus a \"de facto\" standard.\n\nExcept for numbers and variables, every mathematical expression may be viewed as the symbol of an operator followed by a sequence of operands. In computer algebra software, the expressions are usually represented in this way. This representation is very flexible, and many things, that seem not to be mathematical expressions at first glance, may be represented and manipulated as such. For example, an equation is an expression with “=” as an operator, a matrix may be represented as an expression with “matrix” as an operator and its rows as operands.\n\nEven programs may be considered and represented as expressions with operator “procedure” and, at least, two operands, the list of parameters and the body, which is itself an expression with “body” as an operator and a sequence of instructions as operands. Conversely, any mathematical expression may be viewed as a program. For example, the expression may be viewed as a program for the addition, with and as parameters. Executing this program consists in \"evaluating\" the expression for given values of and ; if they do not have any value—that is they are indeterminates—, the result of the evaluation is simply its input.\n\nThis process of delayed evaluation is fundamental in computer algebra. For example, the operator “=” of the equations is also, in most computer algebra systems, the name of the program of the equality test: normally, the evaluation of an equation results in an equation, but, when an equality test is needed,—either explicitly asked by the user through an “evaluation to a Boolean” command, or automatically started by the system in the case of a test inside a program—then the evaluation to a boolean 0 or 1 is executed.\n\nAs the size of the operands of an expression is unpredictable and may change during a working session, the sequence of the operands is usually represented as a sequence of either pointers (like in Macsyma) or entries in a hash table (like in Maple).\n\nThe raw application of the basic rules of differentiation with respect to on the expression formula_1 gives the result \nSuch a complicated expression is clearly not acceptable, and a procedure of simplification is needed as soon as one works with general expressions.\n\nThis simplification is normally done through rewriting rules. There are several classes of rewriting rules that have to be considered. The simplest consists in the rewriting rules that always reduce the size of the expression, like or . They are systematically applied in the computer algebra systems.\n\nThe first difficulty occurs with associative operations like addition and multiplication. The standard way to deal with associativity is to consider that addition and multiplication have an arbitrary number of operands, that is that is represented as . Thus and are both simplified to , which is displayed . What about ? To deal with this problem, the simplest way is to rewrite systematically , , as, respectively, , , . In other words, in the internal representation of the expressions, there is no subtraction nor division nor unary minus, outside the representation of the numbers.\n\nA second difficulty occurs with the commutativity of addition and multiplication. The problem is to recognize quickly the like terms in order to combine or canceling them. In fact, the method for finding like terms, consisting of testing every pair of terms, is too costly for being practicable with very long sums and products. For solving this problem, Macsyma sorts the operands of sums and products with a function of comparison that is designed in order that like terms are in consecutive places, and thus easily detected. In Maple, the hash function is designed for generating collisions when like terms are entered, allowing to combine them as soon as they are introduced. This design of the hash function allows also to recognize immediately the expressions or subexpressions that appear several times in a computation and to store them only once. This allows not only to save some memory space, but also to speed up computation, by avoiding repetition of the same operations on several identical expressions.\n\nSome rewriting rules sometimes increase and sometimes decrease the size of the expressions to which they are applied. This is the case of distributivity or trigonometric identities. For example, the distributivity law allows rewriting formula_3 and formula_4 As there is no way to make a good general choice of applying or not such a rewriting rule, such rewritings are done only when explicitly asked for by the user. For the distributivity, the computer function that apply this rewriting rule is generally called \"expand\". The reverse rewriting rule, called \"factor\", requires a non-trivial algorithm, which is thus a key function in computer algebra systems (see Polynomial factorization).\n\nIn this section we consider some fundamental mathematical questions that arise as soon as one wants to manipulate mathematical expressions in a computer. We consider mainly the case of the multivariate rational fractions. This is not a real restriction, because, as soon as the irrational functions appearing in an expression are simplified, they are usually considered as new indeterminates. For example, \nis viewed as a polynomial in formula_6 and formula_7\n\nThere are two notions of equality for mathematical expressions. The \"syntactic equality\" is the equality of the expressions which means that they are written (or represented in a computer) in the same way. Being trivial, the syntactic equality is rarely considered by mathematicians, although it is the only equality that is easy to test with a program. The \"semantic equality\" is when two expressions represent the same mathematical object, like in\n\nIt is known from Richardson's theorem that there may not exist an algorithm that decides if two expressions representing numbers are semantically equal, if exponentials and logarithms are allowed in the expressions. Therefore, (semantical) equality may be tested only on some classes of expressions such as the polynomials and rational fractions.\n\nTo test the equality of two expressions, instead of designing specific algorithms, it is usual to put expressions in some \"canonical form\" or to put their difference in a \"normal form\", and to test the syntactic equality of the result.\n\nUnlike in usual mathematics, \"canonical form\" and \"normal form\" are not synonymous in computer algebra. A \"canonical form\" is such that two expressions in canonical form are semantically equal if and only if they are syntactically equal, while a \"normal form\" is such that an expression in normal form is semantically zero only if it is syntactically zero. In other words, zero has a unique representation by expressions in normal form.\n\nNormal forms are usually preferred in computer algebra for several reasons. Firstly, canonical forms may be more costly to compute than normal forms. For example, to put a polynomial in canonical form, one has to expand by distributivity every product, while it is not necessary with a normal form (see below). Secondly, It may be the case, like for expressions involving radicals, that a canonical form, if it exists, depends on some arbitrary choices and that these choices may be different for two expressions that have been computed independently. This may make impracticable the use of a canonical form.\n\nAt the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid's algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.\n\n\nFor a detailed definition of the subject: \nFor textbooks devoted to the subject:\n"}
{"id": "2749223", "url": "https://en.wikipedia.org/wiki?curid=2749223", "title": "Conchospiral", "text": "Conchospiral\n\nIn mathematics, a conchospiral is a three-dimensional spiral. In cylindrical coordinates, it is described by the parametric equations:\nThe projection of a conchospiral on the (\"r\", θ)-plane is a logarithmic spiral.\n"}
{"id": "2034219", "url": "https://en.wikipedia.org/wiki?curid=2034219", "title": "Covering set", "text": "Covering set\n\nIn mathematics, a covering set for a sequence of integers refers to a set of prime numbers such that \"every\" term in the sequence is divisible by \"at least one\" member of the set. The term \"covering set\" is used only in conjunction with sequences possessing exponential growth.\n\nThe use of the term \"covering set\" is related to Sierpinski and Riesel numbers. These are odd natural numbers for which the formula (Sierpinski number) or (Riesel number) produces no prime numbers. Since 1960 it has been known that there exists an infinite number of both Sierpinski and Riesel numbers (as solutions to families of congruences based upon the set ) but, because there are an infinitude of numbers of the form or for any , one can only prove to be a Sierpinski or Riesel number through showing that \"every\" term in the sequence or is divisible by one of the prime numbers of a covering set.\n\nThese covering sets form from prime numbers that in base 2 have short periods. To achieve a complete covering set, Wacław Sierpiński showed that a sequence can repeat no more frequently than every 24 numbers. A repeat every 24 numbers give the covering set , while a repeat every 36 terms can give several covering sets: {3, 5, 7, 13, 19, 37, 73}; {3, 5, 7, 13, 19, 37, 109}; {3, 5, 7, 13, 19, 73, 109} and {3, 5, 7, 13, 37, 73, 109}.\n\nRiesel numbers have the same covering sets as Sierpinski numbers.\n\nCovering sets are also used to prove the existence of composite Fibonacci sequences (primefree sequence).\n\nThe concept of a covering set can easily be generalised to other sequences which turn out to be much simpler.\n\nIn the following examples + is used as it is in regular expressions to mean 1 or more. For example, 913 means the set \n\nAn example are the following eight sequences:\n\nIn each case, every term is divisible by one of the primes . These primes can be said to form a covering set exactly analogous to Sierpinski and Riesel numbers. The covering set is found for several similar sequences, including:\n\nAn even simpler case can be found in the sequence:\n\n\nHere, it can be shown that if:\n\nThus we have a covering set with only three primes {3, 7, 13}. This is only possible because the sequence gives integer terms \"only for odd n\".\n\nA covering set also occurs in the sequence:\n\n\nHere, it can be shown that:\nSince can be written as 23, for the sequence 381, we have a covering set of {3, 37, 23} – a covering set with \"infinitely many\" terms.\n\n\nThese are of course the only known Fermat primes and the two prime factors of F.\n\n"}
{"id": "5299894", "url": "https://en.wikipedia.org/wiki?curid=5299894", "title": "Daniel Kleitman", "text": "Daniel Kleitman\n\nDaniel J. Kleitman (born October 4, 1934) is a professor of applied mathematics at MIT. His research interests include combinatorics, graph theory, genomics, and operations research.\n\nKleitman was born in New York, New York, in 1934. received his PhD in Physics from Harvard University in 1958 under Nobel Laureates Julian Schwinger and Roy Glauber. He is the \"k\" in G. W. Peck, a pseudonym for a group of six mathematicians that includes Kleitman. Formerly a physics professor at Brandeis University, Kleitman was encouraged by Paul Erdős to change his field of study to mathematics. Perhaps humorously, Erdős once asked him, \"Why are you \"only\" a physicist?\"\n\nKleitman joined the applied mathematics faculty at MIT in 1966, and was promoted to professor in 1969.\n\nKleitman coauthored at least six papers with Erdős, giving him an Erdős number of 1.\n\nHe was a math advisor and extra for the film \"Good Will Hunting.\" Since Minnie Driver, who appeared in \"Good Will Hunting\", also appeared in \"Sleepers\" with Kevin Bacon, Kleitman has a Bacon number of 2. Adding the two numbers results in an Erdős–Bacon number of 3, tied for lowest currently.\n\n\n"}
{"id": "4009928", "url": "https://en.wikipedia.org/wiki?curid=4009928", "title": "EPCC", "text": "EPCC\n\nEPCC, formerly the Edinburgh Parallel Computing Centre, is a supercomputing centre based at the University of Edinburgh. Since its foundation in 1990, its stated mission has been to \"accelerate the effective exploitation of novel computing throughout industry, academia and commerce\".\n\nThe University has supported high-performance computing (HPC) services since 1982. , through EPCC, it supports the UK’s national high-end computing system, ARCHER (Advanced Research Computing High End Resource), and the UK Research Data Facility (UK-RDF).\n\nEPCC's activities include: consultation and software development for industry and academia; research into high-performance computing; hosting advanced computing facilities and supporting their users; training and education .\n\nThe Centre offers two Masters programmes: MSc in High-Performance Computing and MSc in High-Performance Computing with Data Science .\n\nIt is a member of the Globus Alliance and, through its involvement with the OGSA-DAI project, it works with the Open Grid Forum DAIS-WG.\n\nAround half of EPCC’s annual turnover comes from collaborative projects with industry and commerce. In addition to privately funded projects with businesses, EPCC receives funding from Scottish Enterprise, the Engineering and Physical Sciences Research Council and the European Commission.\n\nEPCC was established in 1990, following on from the earlier Edinburgh Concurrent Supercomputer Project and chaired by Jeffery Collins from 1991. From 2002 to 2016 EPCC was part of the University's School of Physics & Astronomy, becoming an independent Centre of Excellence within the University's College of Science and Engineering in August 2016.\n\nIt was extensively involved in all aspects of Grid computing including: developing Grid middleware and architecture tools to facilitate the uptake of e-Science; developing business applications and collaborating in scientific applications and demonstration projects.\n\nThe Centre was a founder member of the UK’s National e-Science Centre (NeSC), the hub of Grid and e-Science activity in the UK. EPCC and NeSC were both partners in OMII-UK, which offers consultancy and products to the UK e-Science community. EPCC was also a founder partner of the Numerical Algorithms and Intelligent Software Centre (NAIS).\n\nEPCC has hosted a variety of supercomputers over the years, including several Meiko Computing Surfaces, a Thinking Machines CM-200 Connection Machine, and a number of Cray systems including a Cray T3D and T3E.\n\nEPCC manages a collection of HPC systems including ARCHER (the UK’s national high-end computing system) and a variety of smaller HPC systems. These systems are all available for industry use on a pay-per-use basis.\n\nCurrent systems hosted by EPCC include:\n\nRecent systems hosted by EPCC include:\n\nProjects that EPCC has been involved with that also have entries on Wikipedia.\n\n"}
{"id": "663997", "url": "https://en.wikipedia.org/wiki?curid=663997", "title": "Eduard Study", "text": "Eduard Study\n\nEduard Study, more properly Christian Hugo Eduard Study (March 23, 1862 – January 6, 1930), was a German mathematician known for work on invariant theory of ternary forms (1889) and for the study of spherical trigonometry. He is also known for contributions to space geometry, hypercomplex numbers, and criticism of early physical chemistry.\n\nStudy was born in Coburg in the Duchy of Saxe-Coburg-Gotha. His family was of Jewish descent. He died in Bonn.\n\nEduard Study began his university career in Jena, Strasbourg, Leipzig, and Munich. He loved to study biology, especially entomology. He was awarded the doctorate in mathematics at the University of Munich in 1884. Paul Gordan, an expert in invariant theory was at Leipzig, and Study returned there as Privatdozent. In 1888 he moved to Marburg and in 1893 embarked on a speaking tour in the U.S.A. He appeared at the first International Congress of Mathematicians (ICM) in Chicago as part of the World's Columbian Exposition and took part in mathematics at Johns Hopkins University. Back in Germany, in 1894, he was appointed extraordinary professor at Göttingen. Then he gained the rank of full professor in 1897 at Greifswald. In 1904 he was called to the University of Bonn as the position held by Rudolf Lipschitz was vacant. There he settled until retirement in 1927.\n\nHe was an Invited Speaker of the ICM in 1904 at Heidelberg and in 1912 at Cambridge, UK.\n\nIn 1891 Eduard Study published \"Of Motions and Translations, in two parts\". It treats the Euclidean group E(3). The second part of his article\nintroduces the associative algebra of dual quaternions, that is numbers\n\nwhere \"a\", \"b\", \"c\", and \"d\" are dual numbers and {1, \"i\", \"j\", \"k\"} multiply as in the quaternion group. Actually Study uses notation such that\n\nThe multiplication table is found on page 520 of volume 39 (1891) in Mathematische Annalen under the title \"Von Bewegungen und Umlegungen, I. und II. Abhandlungen\".\nEduard Study cites William Kingdon Clifford as an earlier source on these biquaternions. In 1901 Study published \"Geometrie der Dynamen\" also using dual quaternions. In 1913 he wrote a review article treating both E(3) and elliptic geometry. This article, \"Foundations and goals of analytical kinematics\" develops the field of kinematics, in particular exhibiting an element of E(3) as a homography of dual quaternions.\n\nStudy's use of abstract algebra was noted in \"A History of Algebra\" (1985) by B. L. van der Waerden. On the other hand, Joe Rooney recounts these developments in relation to kinematics.\n\nStudy showed an early interest in systems of complex numbers and their application to transformation groups with his article in 1890. He addressed this popular subject again in 1898 in \"Klein's encyclopedia\". The essay explored quaternions and other hypercomplex number systems. This 34 page article was expanded to 138 pages in 1908 by Élie Cartan, who surveyed the hypercomplex systems in \"Encyclopédie des sciences mathématiques pures et appliqueés\". Cartan acknowledged Eduard Study's guidance, in his title, with the words \"after Eduard Study\".\n\nIn the 1993 biography of Cartan by Akivis and Rosenfeld, one reads:\n\nIn 1985 Helmut Karzel and Günter Kist developed \"Study's quaternions\" as the kinematic algebra corresponding to the group of motions of the Euclidean plane. These quaternions arise in \"Kinematic algebras and their geometries\" alongside ordinary quaternions and the ring of 2 × 2 real matrices which Karzel and Kist cast as the kinematic algebras of the elliptic plane and hyperbolic plane respectively. See the \"Motivation and Historical Review\" at page 437 of \"Rings and Geometry\", R. Kaya editor.\n\nSome of the other hypercomplex systems that Study worked with are dual numbers, dual quaternions, and split-biquaternions, all being\nassociative algebras over R.\n\nStudy's work with dual numbers and line coordinates was noted by Heinrich Guggenheimer in 1963 in his book \"Differential Geometry\" (see pages 162–5). He cites and proves the following theorem of Study: The oriented lines in R are in one-to-one correspondence with the points of the dual unit sphere in D. Later he says \"A differentiable curve A(\"u\") on the dual unit sphere, depending on a \"real\" parameter \"u\", represents a differentiable family of straight lines in R: a ruled surface. The lines A(\"u\") are the \"generators\" or \"rulings\" of the surface.\" Guggenheimer also shows the representation of the Euclidean motions in R by orthogonal dual matrices.\n\nIn 1905 Study wrote \"Kürzeste Wege im komplexen Gebiet\" (Shortest paths in the complex domain) for Mathematische Annalen (60:321–378). Some of its contents were anticipated by Guido Fubini a year before. The distance Study refers to is a Hermitian form on complex projective space. Since then this metric has been called the Fubini–Study metric. Study was careful in 1905 to distinguish the hyperbolic and elliptic cases in Hermitian geometry.\n\nSomewhat surprisingly Eduard Study is known by practitioners of quantum chemistry. Like James Joseph Sylvester, Paul Gordan believed that invariant theory could contribute to the understanding of chemical valence. In 1900 Gordan and his student G. Alexejeff contributed an article on an analogy between the coupling problem for angular momenta and their work on invariant theory to the \"Zeitschrift für Physikalische Chemie\" (v. 35, p. 610). In 2006 Wormer and Paldus summarized Study's role as follows:\n\n\n\n"}
{"id": "42069990", "url": "https://en.wikipedia.org/wiki?curid=42069990", "title": "Estevez–Mansfield–Clarkson equation", "text": "Estevez–Mansfield–Clarkson equation\n\nThe Estevez–Mansfield–Clarkson equation is a nonlinear partial differential equation introduced by Pilar Estevez, Elizabeth Mansfield, and Peter Clarkson.\n\nIf \"U\" is a function of some other variables \"x\", \"y\", \"t\", then we denote formula_1 by \"U\", and so on. With that notation, the equation is\n\nin which formula_3\n\n"}
{"id": "172199", "url": "https://en.wikipedia.org/wiki?curid=172199", "title": "Faltings's theorem", "text": "Faltings's theorem\n\nIn number theory, the Mordell conjecture is the conjecture made by that a curve of genus greater than 1 over the field Q of rational numbers has only finitely many rational points. In 1983 it was proved by , and is now known as Faltings's theorem. The conjecture was later generalized by replacing Q by any number field.\n\nLet \"C\" be a non-singular algebraic curve of genus \"g\" over Q. Then the set of rational points on \"C\" may be determined as follows:\n\n\nFaltings's original proof used the known reduction to a case of the Tate conjecture, and a number of tools from algebraic geometry, including the theory of Néron models. A very different proof, based on diophantine approximation, was found by . A more elementary variant of Vojta's proof was given by .\n\nFaltings's 1983 paper had as consequences a number of statements which had previously been conjectured:\n\n\nThe reduction of the Mordell conjecture to the Shafarevich conjecture was due to . A sample application of Faltings's theorem is to a weak form of Fermat's Last Theorem: for any fixed \"n\" > 4 there are at most finitely many primitive integer solutions to \"a\" + \"b\" = \"c\", since for such \"n\" the curve \"x\" + \"y\" = \"1\" has genus greater than 1.\n\nBecause of the Mordell–Weil theorem, Faltings's theorem can be reformulated as a statement about the intersection of a curve \"C\" with a finitely generated subgroup Γ of an abelian variety \"A\". Generalizing by replacing \"C\" by an arbitrary subvariety of \"A\" and \"Γ\" by an arbitrary finite-rank subgroup of \"A\" leads to the Mordell–Lang conjecture, which was proved by .\n\nAnother higher-dimensional generalization of Faltings's theorem is the Bombieri–Lang conjecture that if \"X\" is a pseudo-canonical variety (i.e., a variety of general type) over a number field \"k\", then \"X\"(\"k\") is not Zariski dense in \"X\". Even more general conjectures have been put forth by Paul Vojta.\n\nThe Mordell conjecture for function fields was proved by and by . In 1990, found and fixed a gap in Manin's proof.\n\n"}
{"id": "579390", "url": "https://en.wikipedia.org/wiki?curid=579390", "title": "Gene prediction", "text": "Gene prediction\n\nIn computational biology, gene prediction or gene finding refers to the process of identifying the regions of genomic DNA that encode genes. This includes protein-coding genes as well as RNA genes, but may also include prediction of other functional elements such as regulatory regions. Gene finding is one of the first and most important steps in understanding the genome of a species once it has been sequenced.\n\nIn its earliest days, \"gene finding\" was based on painstaking experimentation on living cells and organisms. Statistical analysis of the rates of homologous recombination of several different genes could determine their order on a certain chromosome, and information from many such experiments could be combined to create a genetic map specifying the rough location of known genes relative to each other. Today, with comprehensive genome sequence and powerful computational resources at the disposal of the research community, gene finding has been redefined as a largely computational problem.\n\nDetermining that a sequence is functional should be distinguished from determining the function of the gene or its product. Predicting the function of a gene and confirming that the gene prediction is accurate still demands \"in vivo\" experimentation through gene knockout and other assays, although frontiers of bioinformatics research are making it increasingly possible to predict the function of a gene based on its sequence alone.\n\nGene prediction is one of the key steps in genome annotation, following sequence assembly, the filtering of non-coding regions and repeat masking.\n\nGene prediction is closely related to the so-called 'target search problem' investigating how DNA-binding proteins (transcription factors) locate specific binding sites within the genome. Many aspects of structural gene prediction are based on current understanding of underlying biochemical processes in the cell such as gene transcription, translation, protein–protein interactions and regulation processes, which are subject of active research in the various omics fields such as transcriptomics, proteomics, metabolomics, and more generally structural and functional genomics.\n\nIn empirical (similarity, homology or evidence-based) gene finding systems, the target genome is searched for sequences that are similar to extrinsic evidence in the form of the known expressed sequence tags, messenger RNA (mRNA), protein products, and homologous or orthologous sequences. Given an mRNA sequence, it is trivial to derive a unique genomic DNA sequence from which it had to have been transcribed. Given a protein sequence, a family of possible coding DNA sequences can be derived by reverse translation of the genetic code. Once candidate DNA sequences have been determined, it is a relatively straightforward algorithmic problem to efficiently search a target genome for matches, complete or partial, and exact or inexact. Given a sequence, local alignment algorithms such as BLAST, FASTA and Smith-Waterman look for regions of similarity between the target sequence and possible candidate matches. Matches can be complete or partial, and exact or inexact. The success of this approach is limited by the contents and accuracy of the sequence database.\n\nA high degree of similarity to a known messenger RNA or protein product is strong evidence that a region of a target genome is a protein-coding gene. However, to apply this approach systemically requires extensive sequencing of mRNA and protein products. Not only is this expensive, but in complex organisms, only a subset of all genes in the organism's genome are expressed at any given time, meaning that extrinsic evidence for many genes is not readily accessible in any single cell culture. Thus, to collect extrinsic evidence for most or all of the genes in a complex organism requires the study of many hundreds or thousands of cell types, which presents further difficulties. For example, some human genes may be expressed only during development as an embryo or fetus, which might be difficult to study for ethical reasons.\n\nDespite these difficulties, extensive transcript and protein sequence databases have been generated for human as well as other important model organisms in biology, such as mice and yeast. For example, the RefSeq database contains transcript and protein sequence from many different species, and the Ensembl system comprehensively maps this evidence to human and several other genomes. It is, however, likely that these databases are both incomplete and contain small but significant amounts of erroneous data.\n\nNew high-throughput transcriptome sequencing technologies such as RNA-Seq and ChIP-sequencing open opportunities for incorporating additional extrinsic evidence into gene prediction and validation, and allow structurally rich and more accurate alternative to previous methods of measuring gene expression such as expressed sequence tag or DNA microarray.\n\nMajor challenges involved in gene prediction involve dealing with sequencing errors in raw DNA data, dependence on the quality of the sequence assembly, handling short reads, frameshift mutations, overlapping genes and incomplete genes.\n\nIn prokaryotes it's essential to consider horizontal gene transfer when searching for gene sequence homology. An additional important factor underused in current gene detection tools is existence of gene clusters—operons in both prokaryotes and eukaryotes. Most popular gene detectors treat each gene in isolation, independent of others, which is not biologically accurate.\n\nAb Initio gene prediction is an intrinsic method based on gene content and signal detection. Because of the inherent expense and difficulty in obtaining extrinsic evidence for many genes, it is also necessary to resort to \"ab initio\" gene finding, in which the genomic DNA sequence alone is systematically searched for certain tell-tale signs of protein-coding genes. These signs can be broadly categorized as either \"signals\", specific sequences that indicate the presence of a gene nearby, or \"content\", statistical properties of the protein-coding sequence itself. \"Ab initio\" gene finding might be more accurately characterized as gene \"prediction\", since extrinsic evidence is generally required to conclusively establish that a putative gene is functional.\n\nIn the genomes of prokaryotes, genes have specific and relatively well-understood promoter sequences (signals), such as the Pribnow box and transcription factor binding sites, which are easy to systematically identify. Also, the sequence coding for a protein occurs as one contiguous open reading frame (ORF), which is typically many hundred or thousands of base pairs long. The statistics of stop codons are such that even finding an open reading frame of this length is a fairly informative sign. (Since 3 of the 64 possible codons in the genetic code are stop codons, one would expect a stop codon approximately every 20–25 codons, or 60–75 base pairs, in a random sequence.) Furthermore, protein-coding DNA has certain periodicities and other statistical properties that are easy to detect in sequence of this length. These characteristics make prokaryotic gene finding relatively straightforward, and well-designed systems are able to achieve high levels of accuracy.\n\n\"Ab initio\" gene finding in eukaryotes, especially complex organisms like humans, is considerably more challenging for several reasons. First, the promoter and other regulatory signals in these genomes are more complex and less well-understood than in prokaryotes, making them more difficult to reliably recognize. Two classic examples of signals identified by eukaryotic gene finders are CpG islands and binding sites for a poly(A) tail.\n\nSecond, splicing mechanisms employed by eukaryotic cells mean that a particular protein-coding sequence in the genome is divided into several parts (exons), separated by non-coding sequences (introns). (Splice sites are themselves another signal that eukaryotic gene finders are often designed to identify.) A typical protein-coding gene in humans might be divided into a dozen exons, each less than two hundred base pairs in length, and some as short as twenty to thirty. It is therefore much more difficult to detect periodicities and other known content properties of protein-coding DNA in eukaryotes.\n\nAdvanced gene finders for both prokaryotic and eukaryotic genomes typically use complex probabilistic models, such as hidden Markov models (HMMs) to combine information from a variety of different signal and content measurements. The GLIMMER system is a widely used and highly accurate gene finder for prokaryotes. GeneMark is another popular approach. Eukaryotic \"ab initio\" gene finders, by comparison, have achieved only limited success; notable examples are the GENSCAN and geneid programs. The SNAP gene finder is HMM-based like Genscan, and attempts to be more adaptable to different organisms, addressing problems related to using a gene finder on a genome sequence that it was not trained against. A few recent approaches like mSplicer, CONTRAST, or mGene also use machine learning techniques like support vector machines for successful gene prediction. They build a discriminative model using hidden Markov support vector machines or conditional random fields to learn an accurate gene prediction scoring function.\n\n\"Ab Initio\" methods have been benchmarked, with some approaching 100% sensitivity, however as the sensitivity increases, accuracy suffers as a result of increased false positives.\n\nAmong the derived signals used for prediction are statistics resulting from the sub-sequence statistics like k-mer statistics, Isochore (genetics) or Compositional domain GC composition/uniformity/entropy, sequence and frame length, Intron/Exon/Donor/Acceptor/Promoter and Ribosomal binding site vocabulary, Fractal dimension, Fourier transform of a pseudo-number-coded DNA, Z-curve parameters and certain run features.\n\nIt has been suggested that signals other than those directly detectable in sequences may improve gene prediction. For example, the role of secondary structure in the identification of regulatory motifs has been reported. In addition, it has been suggested that RNA secondary structure prediction helps splice site prediction.\n\nArtificial neural networks are computational models that excel at machine learning and pattern recognition. Neural networks must be trained with example data before being able to generalise for experimental data, and tested against benchmark data. Neural networks are able to come up with approximate solutions to problems that are hard to solve algorithmically, provided there is sufficient training data. When applied to gene prediction, neural networks can be used alongside other \"ab initio\" methods to predict or identify biological features such as splice sites. One approach involves using a sliding window, which traverses the sequence data in an overlapping manner. The output at each position is a score based on whether the network thinks the window contains a donor splice site or an acceptor splice site. Larger windows offer more accuracy but also require more computational power. A neural network is an example of a signal sensor as its goal is to identify a functional site in the genome.\n\nPrograms such as Maker combine extrinsic and \"ab initio\" approaches by mapping protein and EST data to the genome to validate \"ab initio\" predictions. Augustus, which may be used as part of the Maker pipeline, can also incorporate hints in the form of EST alignments or protein profiles to increase the accuracy of the gene prediction.\n\nAs the entire genomes of many different species are sequenced, a promising direction in current research on gene finding is a comparative genomics approach.\n\nThis is based on the principle that the forces of natural selection cause genes and other functional elements to undergo mutation at a slower rate than the rest of the genome, since mutations in functional elements are more likely to negatively impact the organism than mutations elsewhere. Genes can thus be detected by comparing the genomes of related species to detect this evolutionary pressure for conservation. This approach was first applied to the mouse and human genomes, using programs such as SLAM, SGP and TWINSCAN/N-SCAN and CONTRAST.\n\nTWINSCAN examined only human-mouse synteny to look for orthologous genes. Programs such as N-SCAN and CONTRAST allowed the incorporation of alignments from multiple organisms, or in the case of N-SCAN, a single alternate organism from the target. The use of multiple informants can lead to significant improvements in accuracy.\n\nCONTRAST is composed of two elements. The first is a smaller classifier, identifying donor splice sites and acceptor splice sites as well as start and stop codons. The second element involves constructing a full model using machine learning. Breaking the problem into two means that smaller targeted data sets can be used to train the classifiers,\nand that classifier can operate independently and be trained with smaller windows. The full model can use the independent classifier, and not have to waste computational time or model complexity re-classifying intron-exon boundaries. The paper in which CONTRAST is introduced proposes that their method (and those of TWINSCAN, etc.) be classified as \"de novo\" gene assembly, using alternate genomes, and identifying it as distinct from \"ab initio\", which uses a target 'informant' genomes.\n\nComparative gene finding can also be used to project high quality annotations from one genome to another. Notable examples include Projector, GeneWise, GeneMapper and GeMoMa. Such techniques now play a central role in the annotation of all genomes.\n\nPseudogenes are close relatives of genes, sharing very high sequence homology, but being unable to code for the same protein product. Whilst once relegated as byproducts of gene sequencing, increasingly, as regulatory roles are being uncovered, they are becoming predictive targets in their own right. Pseudogene prediction utilises existing sequence similarity and ab initio methods, whilst adding additional filtering and methods of identifying pseudogene characteristics.\n\nSequence similarity methods can be customised for pseudogene prediction using additional filtering to find candidate pseudogenes. This could use disablement detection, which looks for nonsense or frameshift mutations that would truncate or collapse an otherwise functional coding sequence. Additionally, translating DNA into proteins sequences can be more effective than just straight DNA homology.\n\nContent sensors can be filtered according to the differences in statistical properties between pseudogenes and genes, such as a reduced count of CpG islands in pseudogenes, or the differences in G-C content between pseudogenes and their neighbours. Signal sensors also can be honed to pseudogenes, looking for the absence of introns or polyadenine tails. \n\nMetagenomics is the study of genetic material recovered from the environment, resulting in sequence information from a pool of organisms. Predicting genes is useful for comparative metagenomics.\n\nMetagenomics tools also fall into the basic categories of using either sequence similarity approaches (MEGAN4) and ab initio techniques (GLIMMER-MG).\n\nGlimmer-MG is an extension to GLIMMER that relies mostly on an ab initio approach for gene finding and by using training sets from related organisms. The prediction strategy is augmented by classification and clustering gene data sets prior to applying ab initio gene prediction methods. The data is clustered by species. This classification method leverages techniques from metagenomic phylogenetic classification. An example of software for this purpose is, Phymm, which uses interpolated markov models—and PhymmBL, which integrates BLAST into the classification routines.\n\nMEGAN4 uses a sequence similarity approach, using local alignment against databases of known sequences, but also attempts to classify using additional information on functional roles, biological pathways and enzymes. As in single organism gene prediction, sequence similarity approaches are limited by the size of the database.\n\nFragGeneScan and MetaGeneAnnotator are popular gene prediction programs based on Hidden Markov model. These predictors account for sequencing errors, partial genes and work for short reads.\n\nAnother fast and accurate tool for gene prediction in metagenomes is MetaGeneMark. This tool is used by the DOE Joint Genome Institute to annotate IMG/M, the largest metagenome collection to date.\n\n\n"}
{"id": "38300392", "url": "https://en.wikipedia.org/wiki?curid=38300392", "title": "Geometrothermodynamics", "text": "Geometrothermodynamics\n\nIn physics, geometrothermodynamics (GTD) is a formalism developed in 2007 by Hernando Quevedo to describe the properties of thermodynamic systems in terms of concepts of differential geometry. \n\nConsider a thermodynamic system in the framework of classical equilibrium thermodynamics. The states of thermodynamic equilibrium are considered as points of an abstract equilibrium space in which a Riemannian metric can be introduced in several ways. In particular, one can introduce Hessian metrics like the Fisher information metric, the Weinhold metric, the Ruppeiner metric and others, whose components are calculated as the Hessian of a particular thermodynamic potential. \n\nAnother possibility is to introduce metrics which are independent of the thermodynamic potential, a property which is shared by all thermodynamic systems in classical thermodynamics. Since a change of thermodynamic potential is equivalent to a Legendre transformation, and Legendre transformations do not act in the equilibrium space, it is necessary to introduce an auxiliary space to correctly handle the Legendre transformations. This is the so-called thermodynamic phase space. If the phase space is equipped with a Legendre invariant Riemannian metric, a smooth map can be introduced that induces a thermodynamic metric in the equilibrium manifold. The thermodynamic metric can then be used with different thermodynamic potentials without changing the geometric properties of the equilibrium manifold. One expects the geometric properties of the equilibrium manifold to be related to the macroscopic physical properties. \n\nThe details of this relation can be summarized in three main points:\n\nThe main ingredient of GTD is a (2\"n\" + 1)-dimensional manifold formula_1 \nwith coordinates formula_2, where formula_3 is an arbitrary thermodynamic potential, formula_4, formula_5, are the\nextensive variables, and formula_6 the intensive variables. It is also\npossible to introduce in a canonical manner the fundamental\none-form formula_7 (summation over repeated indices) with formula_8, which satisfies the condition formula_9, where formula_10 is the number of thermodynamic\ndegrees of freedom of the system, and is invariant with respect to\nLegendre transformations\n\nwhere formula_12 is any disjoint decomposition of the set of indices formula_13,\nand formula_14. In particular, for formula_15 and formula_16 we obtain\nthe total Legendre transformation and the identity, respectively.\nIt is also assumed that in formula_17 \nthere exists a metric formula_18 which is also\ninvariant with respect to Legendre transformations. The triad\nformula_19 defines a Riemannian contact manifold which is\ncalled the thermodynamic phase space (phase manifold). The space of\nthermodynamic equilibrium states (equilibrium manifold) is an\nn-dimensional Riemannian submanifold formula_20\ninduced by a smooth map formula_21,\ni.e. formula_22, with formula_23\nand formula_24, such that formula_25 holds, where formula_26 is the\npullback of formula_27. The manifold formula_28 is naturally equipped\nwith the Riemannian metric formula_29. The purpose of GTD is\nto demonstrate that the geometric properties of formula_28 are\nrelated to the thermodynamic properties of a system with fundamental\nthermodynamic equation formula_23.\nThe condition of invariance with respect total Legendre transformations leads to the metrics\n\nwhere formula_34 is a constant diagonal matrix that can be expressed in terms of formula_35 and\nformula_36, and formula_37 is an arbitrary Legendre invariant function of formula_38. The metrics formula_39 and formula_40 have been used to describe thermodynamic systems with first and second order phase transitions, respectively. The most general metric which is invariant with respect to partial Legendre transformations is\n\nThe components of the corresponding metric for the equilibrium manifold formula_42 can be computed as\n\nGTD has been applied to describe laboratory systems like the ideal gas, van der Waals gas, the Ising model, etc., more exotic systems like black holes in different gravity theories, in the context of relativistic cosmology, and to describe chemical reactions\n"}
{"id": "14245076", "url": "https://en.wikipedia.org/wiki?curid=14245076", "title": "Hundred-dollar, Hundred-digit Challenge problems", "text": "Hundred-dollar, Hundred-digit Challenge problems\n\nThe Hundred-dollar, Hundred-digit Challenge problems are 10 problems in numerical mathematics published in 2002 by . A $100 prize was offered to whoever produced the most accurate solutions, measured up to 10 significant digits. The deadline for the contest was May 20, 2002. In the end, 20 teams solved all of the problems perfectly within the required precision, and an anonymous donor aided in producing the required prize monies. The challenge and its solutions were described in detail in the book .\n\nFrom :\n\n\nThese answers have been assigned the identifiers , , , , , , , , , and in the On-Line Encyclopedia of Integer Sequences.\n\n"}
{"id": "1576323", "url": "https://en.wikipedia.org/wiki?curid=1576323", "title": "Incidence geometry", "text": "Incidence geometry\n\nIn mathematics, incidence geometry is the study of incidence structures. A geometric structure such as the Euclidean plane is a complicated object that involves concepts such as length, angles, continuity, betweenness, and incidence. An \"incidence structure\" is what is obtained when all other concepts are removed and all that remains is the data about which points lie on which lines. Even with this severe limitation, theorems can be proved and interesting facts emerge concerning this structure. Such fundamental results remain valid when additional concepts are added to form a richer geometry. It sometimes happens that authors blur the distinction between a study and the objects of that study, so it is not surprising to find that some authors refer to incidence structures as incidence geometries.\n\nIncidence structures arise naturally and have been studied in various areas of mathematics. Consequently there are different terminologies to describe these objects. In graph theory they are called hypergraphs, and in combinatorial design theory they are called block designs. Besides the difference in terminology, each area approaches the subject differently and is interested in questions about these objects relevant to that discipline. Using geometric language, as is done in incidence geometry, shapes the topics and examples that are normally presented. It is, however, possible to translate the results from one discipline into the terminology of another, but this often leads to awkward and convoluted statements that do not appear to be natural outgrowths of the topics. In the examples selected for this article we use only those with a natural geometric flavor.\n\nA special case that has generated much interest deals with finite sets of points in the Euclidean plane and what can be said about the number and types of (straight) lines they determine. Some results of this situation can extend to more general settings since only incidence properties are considered.\n\nAn \"incidence structure\" consists of a set whose elements are called \"points\", a disjoint set whose elements are called \"lines\" and an \"incidence relation\" between them, that is, a subset of whose elements are called \"flags\". If is a flag, we say that is \"incident with\" or that is incident with (the relation is symmetric), and write . Intuitively, a point and line are in this relation if and only if the point is \"on\" the line. Given a point and a line which do not form a flag, that is, the point is not on the line, the pair is called an \"anti-flag\".\n\nThere is no natural concept of distance (a metric) in an incidence structure. However, a combinatorial metric does exist in the corresponding incidence graph (Levi graph), namely the length of the shortest path between two vertices in this bipartite graph. The distance between two objects of an incidence structure – two points, two lines or a point and a line – can be defined to be the distance between the corresponding vertices in the incidence graph of the incidence structure.\n\nAnother way to define a distance again uses a graph-theoretic notion in a related structure, this time the \"collinearity graph\" of the incidence structure. The vertices of the collinearity graph are the points of the incidence structure and two points are joined if there exists a line incident with both points. The distance between two points of the incidence structure can then be defined as their distance in the collinearity graph.\n\nWhen distance is considered in an incidence structure, it is necessary to mention how it is being defined.\n\nIncidence structures that are most studied are those that satisfy some additional properties (axioms), such as projective planes, affine planes, generalized polygons, partial geometries and near polygons. Very general incidence structures can be obtained by imposing \"mild\" conditions, such as:\n\nA partial linear space is an incidence structure for which the following axioms are true:\n\nIn a partial linear space it is also true that every pair of distinct lines meet in at most one point. This statement does not have to be assumed as it is readily proved from axiom one above.\n\nFurther constraints are provided by the regularity conditions:\n\nRLk: Each line is incident with the same number of points. If finite this number is often denoted by .\n\nRPr: Each point is incident with the same number of lines. If finite this number is often denoted by .\n\nThe second axiom of a partial linear space implies that . Neither regularity condition implies the other, so it has to be assumed that .\n\nA finite partial linear space satisfying both regularity conditions with is called a \"tactical configuration\". Some authors refer to these simply as \"configurations\", or \"projective configurations\". If a tactical configuration has points and lines, then, by double counting the flags, the relationship is established. A common notation refers to -\"configurations\". In the special case where (and hence, ) the notation is often simply written as .\n\nA \"linear space\" is a partial linear space such that:\n\nSome authors add a \"non-degeneracy\" (or \"non-triviality\") axiom to the definition of a (partial) linear space, such as:\n\n\nThis is used to rule out some very small examples (mainly when the sets or have fewer than two elements) that would normally be exceptions to general statements made about the incidence structures. An alternative to adding the axiom is to refer to incidence structures that do not satisfy the axiom as being \"trivial\" and those that do as \"non-trivial\".\n\nEach non-trivial linear space contains at least three points and three lines, so the simplest non-trivial linear space that can exist is a triangle.\n\nA linear space having at least three points on every line is a Sylvester–Gallai design.\n\nSome of the basic concepts and terminology arises from geometric examples, particularly projective planes and affine planes.\n\nA \"projective plane\" is a linear space in which:\nand that satisfies the non-degeneracy condition:\n\nThere is a bijection between and in a projective plane. If is a finite set, the projective plane is referred to as a \"finite\" projective plane. The order of a finite projective plane is , that is, one less than the number of points on a line. All known projective planes have orders that are prime powers. A projective plane of order is an configuration.\n\nThe smallest projective plane has order two and is known as the \"Fano plane\".\nThis famous incidence geometry was developed by the Italian mathematician Gino Fano. In his work on proving the independence of the set of axioms for projective \"n\"-space that he developed, he produced a finite three-dimensional space with 15 points, 35 lines and 15 planes, in which each line had only three points on it. The planes in this space consisted of seven points and seven lines and are now known as Fano planes.\n\nThe Fano plane cannot be represented in the Euclidean plane using only points and straight line segments (i.e., it is not realizable). This is a consequence of the Sylvester–Gallai theorem, according to which every realizable incidence geometry must include an \"ordinary line\", a line containing only two points. The Fano plane has no such line (that is, it is a Sylvester–Gallai configuration), so it is not realizable.\n\nA complete quadrangle consists of four points, no three of which are collinear. In the Fano plane, the three points not on a complete quadrangle are the diagonal points of that quadrangle and are collinear. This contradicts the \"Fano axiom\", often used as an axiom for the Euclidean plane, which states that the three diagonal points of a complete quadrangle are never collinear.\n\nAn \"affine plane\" is a linear space satisfying:\nand satisfying the non-degeneracy condition:\n\nThe lines and in the statement of Playfair's axiom are said to be \"parallel\". Every affine plane can be uniquely extended to a projective plane. The \"order\" of a finite affine plane is , the number of points on a line. An affine plane of order is an configuration.\nThe affine plane of order three is a configuration. When embedded in some ambient space it is called the Hesse configuration. It is not realizable in the Euclidean plane but is realizable in the complex projective plane as the nine inflection points of an elliptic curve with the 12 lines incident with triples of these.\n\nThe 12 lines can be partitioned into four classes of three lines apiece where, in each class the lines are mutually disjoint. These classes are called \"parallel classes\" of lines. Adding four new points, each being added to all the lines of a single parallel class (so all of these lines now intersect), and one new line containing just these four new points produces the projective plane of order three, a configuration. Conversely, starting with the projective plane of order three (it is unique) and removing any single line and all the points on that line produces this affine plane of order three (it is also unique).\n\nRemoving one point and the four lines that pass through that point (but not the other points on them) produces the Möbius–Kantor configuration.\n\nGiven an integer , a tactical configuration satisfying:\nis called a \"partial geometry\". If there are points on a line and lines through a point, the notation for a partial geometry is .\n\nIf these partial geometries are generalized quadrangles.\n\nIf these are called Steiner systems.\n\nFor , a generalized -gon is a partial linear space whose incidence graph has the property:\n\nA \"generalized 2-gon\" is an incidence structure, which is not a partial linear space, consisting of at least two points and two lines with every point being incident with every line. The incidence graph of a generalized 2-gon is a complete bipartite graph.\n\nA generalized -gon contains no ordinary -gon for and for every pair of objects (two points, two lines or a point and a line) there is an ordinary -gon that contains them both.\n\nGeneralized 3-gons are projective planes. Generalized 4-gons are called generalized quadrangles. By the Feit-Higman theorem the only finite generalized -gons with at least three points per line and three lines per point have = 2, 3, 4, 6 or 8.\n\nFor a non-negative integer a near -gon is an incidence structure such that:\n\nA near 0-gon is a point, while a near 2-gon is a line. The collinearity graph of a near 2-gon is a complete graph. A near 4-gon is a generalized quadrangle (possibly degenerate). Every finite generalized polygon except the projective planes is a near polygon. Any connected bipartite graph is a near polygon and any near polygon with precisely two points per line is a connected bipartite graph. Also, all dual polar spaces are near polygons.\n\nMany near polygons are related to finite simple groups like the Mathieu groups and the Janko group J2. Moreover, the generalized 2\"d\"-gons, which are related to Groups of Lie type, are special cases of near 2\"d\"-gons.\n\nAn abstract Mōbius plane (or inversive plane) is an incidence structure where, to avoid possible confusion with the terminology of the classical case, the lines are referred to as \"cycles\" or \"blocks\".\n\nSpecifically, a Möbius plane is an incidence structure of points and cycles such that:\n\nThe incidence structure obtained at any point of a Möbius plane by taking as points all the points other than and as lines only those cycles that contain (with removed), is an affine plane. This structure is called the \"residual\" at in design theory.\n\nA finite Möbius plane of \"order\" is a tactical configuration with points per cycle that is a 3-design, specifically a block design.\n\nA question raised by J.J. Sylvester in 1893 and finally settled by Tibor Gallai concerned incidences of a finite set of points in the Euclidean plane.\n\nTheorem (Sylvester-Gallai): A finite set of points in the Euclidean plane is either collinear or there exists a line incident with exactly two of the points.\n\nA line containing exactly two of the points is called an \"ordinary line\" in this context. Sylvester was probably led to the question while pondering about the embeddability of the Hesse configuration.\n\nA related result is the de Bruijn–Erdős theorem. Nicolaas Govert de Bruijn and Paul Erdős proved the result in the more general setting of projective planes, but it still holds in the Euclidean plane. The theorem is:\n\nAs the authors pointed out, since their proof was combinatorial, the result holds in a larger setting, in fact in any incidence geometry in which there is a unique line through every pair of distinct points. They also mention that the Euclidean plane version can be proved from the Sylvester-Gallai theorem using induction.\n\nA bound on the number of flags determined by a finite set of points and the lines they determine is given by:\n\nTheorem (Szemerédi–Trotter): given points and lines in the plane, the number of flags (incident point-line pairs) is:\n\nand this bound cannot be improved, except in terms of the implicit constants.\n\nThis result can be used to prove Beck's theorem.\n\nBeck's theorem says that finite collections of points in the plane fall into one of two extremes; one where a large fraction of points lie on a single line, and one where a large number of lines are needed to connect all the points.\n\nThe theorem asserts the existence of positive constants such that given any points in the plane, at least one of the following statements is true:\n\n\nIn Beck's original argument, is 100 and is an unspecified constant; it is not known what the optimal values of and are.\n\n\n\n\n"}
{"id": "3418590", "url": "https://en.wikipedia.org/wiki?curid=3418590", "title": "Induced topology", "text": "Induced topology\n\nIn topology and related areas of mathematics, an induced topology on a topological space is a topology which makes the inducing function continuous from/to this topological space.\n\nLet formula_1 be sets, formula_2.\n\nIf formula_3 is a topology on formula_4, then a topology coinduced on formula_5 by formula_6 is formula_7.\n\nIf formula_8 is a topology on formula_5, then a topology induced on formula_4 by formula_6 is formula_12.\n\nThe easy way to remember the definitions above is to notice that finding an inverse image is used in both. This is because inverse image preserves union and intersection. Finding a direct image does not preserve intersection in general. Here is an example where this becomes a hurdle. Consider a set formula_13 with a topology formula_14, a set formula_15 and a function formula_2 such that formula_17. A set of subsets formula_18 is not a topology, because formula_19 but formula_20.\n\nThere are equivalent definitions below.\n\nA topology formula_8 coinduced on formula_5 by formula_6 is the finest topology such that formula_6 is continuous formula_25. This is a particular case of the final topology on formula_5.\n\nA topology formula_3 induced on formula_4 by formula_6 is the coarsest topology such that formula_6 is continuous formula_25. This is a particular case of the initial topology on formula_4.\n\n\n"}
{"id": "47771159", "url": "https://en.wikipedia.org/wiki?curid=47771159", "title": "International Workshop on Operator Theory and its Applications", "text": "International Workshop on Operator Theory and its Applications\n\nInternational Workshop on Operator Theory and its Applications (IWOTA) was started in 1981 to bring together mathematicians and engineers working in operator theoretic side of functional analysis and its applications to related fields. These include:\n\n\nThe other major branch of operator theory, Operator algebras (C* and von Neumann Algebras), is not heavily represented at IWOTA and has its own conferences.\n\nIWOTA gathers leading experts from all over the world for an intense exchange of new results, information and opinion, and for tracing the future developments in the field. The IWOTA meetings provide opportunities for participants (including young researchers) to present their own work in invited and contributed talks, to interact with other researchers from around the globe, and to broaden their knowledge of the field. \nIn addition, IWOTA emphasizes cross-disciplinary interaction among mathematicians, electrical engineers and mathematical physicists. In the even years, the IWOTA workshop is a satellite meeting to the biennial International Symposium on the Mathematical Theory of Networks and Systems (MTNS). From the humble beginnings in the early 80's, the IWOTA workshops grew to become one of the largest continuing conferences attended by the community of researchers in operator theory.\n\nThe International Workshop on Operator Theory and its Applications was started on August 1, 1981, adjacent to the International Symposium on Mathematical Theory of Networks and Systems (MTNS) with goal of exposing operator theorists even pure ones to recent developments in engineering (especially \"H-infinity methods in control theory\") which had a great intersection with operator theory. Israel Gohberg was the visionary and driving force of IWOTA and president of the IWOTA Steering Committee. From the beginning, J. W. Helton and M. A. Kaashoek served as vice presidents of the steering committee.\n\nBesides the excitement of mathematical discovery over the decades at IWOTA, there was great excitement when the curtain between Soviet bloc and Western operator theorists fell. Until 1990, these two collections of extremely strong mathematicians seldom met due to the tight restrictions on travel from and in the communist countries. When the curtain dropped, the western mathematicians knew the classic Soviet papers but had a spotty knowledge of much of what else their counterparts were doing. Gohberg was one of the operator theorists who knew both sides and he guided IWOTA, a western institution, in bringing (and funding) many prominent FSU bloc operator theorists to speak at the meetings. As the IWOTA programs demonstrate, this significantly accelerated the cultures' mutual assimilation.\n\nProceedings of the IWOTA workshops appear in the \"Springer\" / \"Birkhäuser Verlag\" book series \"Operator Theory: Advances and Applications (OTAA)\" (founder: Israel Gohberg). While engineering conference proceedings often are handed to participants as they arrive and contain short papers on each conference talk, the IWOTA proceedings follow mathematics conference tradition and contain a modest number of papers and are published several years after the conference.\n\nIWOTA has received support from many sources, including the National Science Foundation\n\n, the London Mathematical Society, the Engineering and Physical Sciences Research Council, Deutsche Forschungsgemeinschaft, Stichting Advancement of Mathematics, Secretaría de Estado de Investigación, Desarrollo e Innovación (Spain), Australian Mathematical Sciences Institute, National Board for Higher Mathematics, International Centre for Theoretical Physics, Indian Statistical Institute, Korea Research Foundation, United States-India Science & Technology Endowment Fund, Nederlandse Organisatie voor Wetenschappelijk Onderzoek, the Commission for Developing Countries of the International Mathematical Union, Stichting Advancement of Mathematics (Netherlands), the National Research Foundation of South Africa, and Birkhäuser Publishing Ltd.\n\nIWOTA is directed by a steering committee which chooses the site for the next meeting, elects the chief local organizer(s) and insures the appearance of the enduring themes of IWOTA. The sub-themes of an IWOTA workshop and the lecturers are chosen by the local organizing committee after hearing the steering committee's board. The board consists of its vice presidents: Joseph A. Ball, J. William Helton (Chair), M. A. Kaashoek, Igor Klep, Christiane Tretter, Victor Vinnikov and Hugo J. Woerdeman. In addition, past chief organizers who remain active in IWOTA are members of the steering committee. The board governs IWOTA with consultation and the consent of the full steering committee. Honorary members of the steering committee, elected in 2016, are: Israel Gohberg (deceased), Leiba Rodman (deceased), Tsuyoshi Ando, Harry Dym, Ciprian Foiaş, Heinz Langer, Nikolai Nikolski.\n\n\nThe Israel Gohberg ILAS-IWOTA Lecture was introduced in August 2016 and honors the legacy of Israel Gohberg, whose research crossed borders between operator theory, linear algebra, and related fields. This lecture is in collaboration with the International Linear Algebra Society (ILAS). This series of lectures will be delivered at IWOTA and ILAS Conferences, in different years, in the approximate ratio two-thirds at IWOTA and one-third at ILAS. The first three lectures will take place at IWOTA 2020, ILAS 2022, and IWOTA 2024. Donations for the Israel Gohberg ILAS-IWOTA Lecture Fund are most welcome and can be submitted via the ILAS donation form. Donations are tax deductible in the United States.\n\n"}
{"id": "12019625", "url": "https://en.wikipedia.org/wiki?curid=12019625", "title": "J. H. van Lint", "text": "J. H. van Lint\n\nJacobus Hendricus (\"Jack\") van Lint (1 September 1932 – 28 September 2004) was a Dutch mathematician, professor at the Eindhoven University of Technology, of which he was rector magnificus from 1991 till 1996.\n\nHe gained his Ph.D. from Utrecht University in 1957 under the supervision of Fred van der Blij. He was professor of mathematics at Eindhoven University of Technology from 1959 to 1997.\n\nHe was appointed a full professor at Eindhoven University of Technology at the age of 26 years. His field of research was initially number theory, but he worked mainly in combinatorics and coding theory.\n\nVan Lint was honored with a great number of awards. He became a member of Royal Netherlands Academy of Arts and Sciences in 1972, received four honorary doctorates, was an honorary member of the Royal Netherlands Mathematics Society (Koninklijk Wiskundig Genootschap), and received a Knighthood.\n\n\n"}
{"id": "4320975", "url": "https://en.wikipedia.org/wiki?curid=4320975", "title": "KOI-18", "text": "KOI-18\n\nThe KOI-18 is a hand-held paper tape reader developed by the U.S. National Security Agency as a fill device for loading cryptographic keys, or \"crypto variables,\" into security devices, such as encryption systems. It can read 8-level paper or PET tape, which is manually pulled through the reader slot by the operator. It is battery powered and has no internal storage, so it can load keys of different lengths, including the 128-bit keys used by more modern systems. The KOI-18 can also be used to load keys into other fill devices that do have internal storage, such as the KYK-13 and AN/CYZ-10. The KOI-18 only supports the DS-102 interface.\n\nA similar device was developed by Prof. Jean-Daniel Nicoud for the Smaky 4 in 1975.\n\n"}
{"id": "42109111", "url": "https://en.wikipedia.org/wiki?curid=42109111", "title": "Kazimierz Cwojdziński", "text": "Kazimierz Cwojdziński\n\nKazimierz Cwojdziński (born January 8, 1878 in Plewiska near Poznań, died August 12, 1948 in Poznań) was a Polish mathematician and professor of the School of Engineering in Poznań.\n"}
{"id": "25103155", "url": "https://en.wikipedia.org/wiki?curid=25103155", "title": "Kolmogorov's three-series theorem", "text": "Kolmogorov's three-series theorem\n\nIn probability theory, Kolmogorov's Three-Series Theorem, named after Andrey Kolmogorov, gives a criterion for the almost sure convergence of an infinite series of random variables in terms of the convergence of three different series involving properties of their probability distributions. Kolmogorov's three-series theorem, combined with Kronecker's lemma, can be used to give a relatively easy proof of the Strong Law of Large Numbers.\n\nLet (\"X\") be independent random variables. The random series ∑\"X\" converges almost surely in ℝ if and only if the following conditions hold for some A > 0:\n\ni. ∑(|X| ≥ A) converges\n\nii. Let Y:= X1, then ∑E(Y), the series of expected values of Y , converges\n\niii. ∑var(Y) converges\n\nCondition (i) and Borel–Cantelli give that almost surely X = Y for n large, hence ∑X converges if and only if ∑Y converges. Conditions (ii)-(iii) and Kolmogorov's Two-Series Theorem give the almost sure convergence of ∑Y.\n\nSuppose that ∑X converges almost surely.\n\nWithout condition (i), by Borel–Cantelli there would exist some A > 0 such that almost surely {|X| ≥ A} for infinitely many values n, but then the series would diverge. Therefore, we must have condition (i).\n\nWe see that condition (iii) implies condition (ii): Kolmogorov's two-series theorem along with condition (i) applied to the case A = 1 gives the convergence of ∑(Y − 𝔼(Y)). So given the convergence of ∑Y, we have ∑𝔼(Y) converges, so condition (ii) is implied.\n\nThus, it only remains to demonstrate the necessity of condition (iii), and we will have obtained the full result. It is equivalent to check condition (iii) for the series ∑Z = ∑(Y − Y') where for each n, Y and Y' are IID—that is, to employ the assumption that 𝔼(Y) = 0, since Z is a sequence of random variables bounded by 2, converging almost surely, and with 𝕍ar(Z) = 2𝕍ar(Y). So we wish to check that if ∑Z converges, ∑𝕍ar(Z) converges as well. This is a special case of a more general result from martingale theory with summands equal to the increments of a martingale sequence and the same conditions (𝔼(Z) = 0, series of the variances converging, summands bounded).\n\nAs an illustration of the theorem, consider the example of the \"harmonic series with random signs\":\nHere, \"formula_2\" means that each term formula_3 is taken with a random sign that is either formula_4 or formula_5 with respective probabilities formula_6, and all random signs are chosen independently. Letting formula_7 in the theorem denote a random variable that takes the values formula_3 and formula_9 with equal probabilities, one can check easily that the conditions of the theorem are satisfied, so it follows that the harmonic series with random signs converges almost surely. On the other hand, the analogous series of (for example) square root reciprocals with random signs, namely\n\n\"diverges\" almost surely, since condition (3) in the theorem is not satisfied. Note that this is different from the behavior of the analogous series with \"alternating\" signs, formula_11, which does converge.\n"}
{"id": "5971837", "url": "https://en.wikipedia.org/wiki?curid=5971837", "title": "List of mathematicians (V)", "text": "List of mathematicians (V)\n\n\n\n\n\n\n"}
{"id": "56083331", "url": "https://en.wikipedia.org/wiki?curid=56083331", "title": "List of tessellations", "text": "List of tessellations\n\n"}
{"id": "10986798", "url": "https://en.wikipedia.org/wiki?curid=10986798", "title": "Molecular symmetry", "text": "Molecular symmetry\n\nMolecular symmetry in chemistry describes the symmetry present in molecules and the classification of molecules according to their symmetry. Molecular symmetry is a fundamental concept in chemistry, as it can be used to predict or explain many of a molecule's chemical properties, such as its dipole moment and its allowed spectroscopic transitions. Many university level textbooks on physical chemistry, quantum chemistry, and inorganic chemistry devote a chapter to symmetry.\n\nThe predominant framework for the study of molecular symmetry is group theory. Symmetry is useful in the study of molecular orbitals, with applications such as the Hückel method, ligand field theory, and the Woodward-Hoffmann rules. Another framework on a larger scale is the use of crystal systems to describe crystallographic symmetry in bulk materials. \nMany techniques for the practical assessment of molecular symmetry exist, including X-ray crystallography and various forms of spectroscopy. Spectroscopic notation is based on symmetry considerations.\n\nThe study of symmetry in molecules makes use of group theory.\n\nThe point group symmetry of a molecule can be described by 5 types of symmetry element. \n\n\nThe five symmetry elements have associated with them five types of symmetry operation, which leave the molecule in a state indistinguishable from the starting state. They are sometimes distinguished from symmetry elements by a caret or circumflex. Thus, Ĉ is the rotation of a molecule around an axis and Ê is the identity operation. A symmetry element can have more than one symmetry operation associated with it. For example, the C axis of the square xenon tetrafluoride (XeF) molecule is associated with two Ĉ rotations (90°) in opposite directions and a Ĉ rotation (180°). Since Ĉ is equivalent to Ê, Ŝ to σ and Ŝ to \"î\", all symmetry operations can be classified as either proper or improper rotations.\n\nThe symmetry operations of a molecule (or other object) form a \"group\", which is a mathematical structure usually denoted in the form (\"G\",*) consisting of a set \"G\" and a binary combination operation say '*' satisfying certain properties listed below.\n\nIn a symmetry group, the group elements are the symmetry operations (\"not\" the symmetry elements), and the binary combination consists of applying first one symmetry operation and then the other. An example is the sequence of a C rotation about the z-axis and a reflection in the xy-plane, denoted σ(xy)C. By convention the order of operations is from right to left.\n\nA symmetry group obeys the defining properties of any group.\n\n(1) \"closure\" property: <br>           \nFor every pair of elements \"x\" and \"y\" in \"G\", the \"product\" \"x\"*\"y\" is also in \"G\". <br>           ( in symbols, for every two elements \"x\", \"y\"∈\"G\", \"x\"*\"y\" is also in \"G\" ). <br>This means that the group is \"closed\" so that combining two elements produces no new elements. Symmetry operations have this property because a sequence of two operations will produce a third state indistinguishable from the second and therefore from the first, so that the net effect on the molecule is still a symmetry operation.<br>\n(2) \"associative\" property: <br>           \nFor every \"x\" and \"y\" and \"z\" in \"G\", both (\"x\"*\"y\")*\"z\" and \"x\"*(\"y\"*\"z\") result with the same element in \"G\". <br>           \n( in symbols, (\"x\"*\"y\")*\"z\" = \"x\"*(\"y\"*\"z\" ) for every \"x\", \"y\", and \"z\" ∈ \"G\")<br>\n(3) \"existence of identity\" property: <br>           \nThere must be an element ( say \"e\" ) in \"G\" such that product any element of \"G\" with \"e\" make no change to the element. <br>           \n( in symbols, \"x\"*\"e\"=\"e\"*\"x\"= \"x\" for every \"x\"∈ \"G\" )<br>\n(4) \"existence of inverse\" property: <br>           \nFor each element ( \"x\" ) in \"G\", there must be an element \"y\" in \"G\" such that product of \"x\" and \"y\" is the identity element \"e\". <br>           \n\nThe \"order\" of a group is the number of elements in the group. For groups of small orders, the group properties can be easily verified by considering its composition table, a table whose rows and columns correspond to elements of the group and whose entries correspond to their products.\n\nThe successive application (or \"composition\") of one or more symmetry operations of a molecule has an effect equivalent to that of some single symmetry operation of the molecule. For example, a C rotation followed by a σ reflection is seen to be a σ' symmetry operation: σ*C = σ'. (Note that \"Operation A followed by B to form C\" is written BA = C). Moreover, the set of all symmetry operations (including this composition operation) obeys all the properties of a group, given above. So (\"S\",\"*\") is a group, where \"S\" is the set of all symmetry operations of some molecule, and * denotes the composition (repeated application) of symmetry operations.\n\nThis group is called the point group of that molecule, because the set of symmetry operations leave at least one point fixed (though for some symmetries an entire axis or an entire plane remains fixed). In other words, a point group is a group that summarizes all symmetry operations that all molecules in that category have. The symmetry of a crystal, by contrast, is described by a space group of symmetry operations, which includes translations in space.\n\nOne can determine the symmetry operations of the point group for a particular molecule by considering the geometrical symmetry of its molecular model. However, when one USES a point group, the operations in it are not to be interpreted in the same way. Instead the operations are interpreted as rotating and/or reflecting the vibronic (vibration-electronic) coordinates and these operations commute with the vibronic Hamiltonian. They are \"symmetry operations\" for that vibronic Hamiltonian. The point group is used to classify by symmetry the vibronic eigenstates. The symmetry classification of the rotational levels, the eigenstates of the full (rovibronic nuclear spin) Hamiltonian, requires the use of the appropriate permutation-inversion group as introduced by Longuet-Higgins. The relation between point groups and permutation-inversion groups is explained in this pdf file Link .\n\nAssigning each molecule a point group classifies molecules into categories with similar symmetry properties. For example, PCl, POF, XeO, and NH all share identical symmetry operations. They all can undergo the identity operation E, two different C rotation operations, and three different σ plane reflections without altering their identities, so they are placed in one point group, C, with order 6. Similarly, water (HO) and hydrogen sulfide (HS) also share identical symmetry operations. They both undergo the identity operation E, one C rotation, and two σ reflections without altering their identities, so they are both placed in one point group, C, with order 4. This classification system helps scientists to study molecules more efficiently, since chemically related molecules in the same point group tend to exhibit similar bonding schemes, molecular bonding diagrams, and spectroscopic properties.\n\nThe following table contains a list of point groups labelled using the Schoenflies notation, which is common in chemistry and molecular spectroscopy. The description of structure includes common shapes of molecules, which can be explained by the VSEPR model.\n\nThe symmetry operations can be represented in many ways. A convenient representation is by matrices. For any vector representing a point in Cartesian coordinates, left-multiplying it gives the new location of the point transformed by the symmetry operation. Composition of operations corresponds to matrix multiplication. Within a point group, a multiplication of the matrices of two symmetry operations leads to a matrix of another symmetry operation in the same point group. For example, in the C example this is:\nAlthough an infinite number of such representations exist, the irreducible representations (or \"irreps\") of the group are commonly used, as all other representations of the group can be described as a linear combination of the irreducible representations.\n\nFor each point group, a character table summarizes information on its symmetry operations and on its irreducible representations. As there are always equal numbers of irreducible representations and classes of symmetry operations, the tables are square.\n\nThe table itself consists of characters that represent how a particular irreducible representation transforms when a particular symmetry operation is applied. Any symmetry operation in a molecule's point group acting on the molecule itself will leave it unchanged. But, for acting on a general entity, such as a vector or an orbital, this need not be the case. The vector could change sign or direction, and the orbital could change type. For simple point groups, the values are either 1 or −1: 1 means that the sign or phase (of the vector or orbital) is unchanged by the symmetry operation (\"symmetric\") and −1 denotes a sign change (\"asymmetric\").\n\nThe representations are labeled according to a set of conventions:\n\n\nThe tables also capture information about how the Cartesian basis vectors, rotations about them, and quadratic functions of them transform by the symmetry operations of the group, by noting which irreducible representation transforms in the same way. These indications are conventionally on the righthand side of the tables. This information is useful because chemically important orbitals (in particular \"p\" and \"d\" orbitals) have the same symmetries as these entities.\n\nThe character table for the C symmetry point group is given below:\nConsider the example of water (HO), which has the C symmetry described above. The 2\"p\" orbital of oxygen has B symmetry as in the fourth row of the character table above, with x in the sixth column). It is oriented perpendicular to the plane of the molecule and switches sign with a C and a σ'(yz) operation, but remains unchanged with the other two operations (obviously, the character for the identity operation is always +1). This orbital's character set is thus {1, −1, 1, −1}, corresponding to the B irreducible representation. Likewise, the 2\"p\" orbital is seen to have the symmetry of the A irreducible representation (\"i.e\".: none of the symmetry operations change it), 2\"p\" B, and the 3\"d\" orbital A. These assignments and others are noted in the rightmost two columns of the table.\n\nHans Bethe used characters of point group operations in his study of ligand field theory in 1929, and Eugene Wigner used group theory to explain the selection rules of atomic spectroscopy. The first character tables were compiled by László Tisza (1933), in connection to vibrational spectra. Robert Mulliken was the first to publish character tables in English (1933), and E. Bright Wilson used them in 1934 to predict the symmetry of vibrational normal modes. The complete set of 32 crystallographic point groups was published in 1936 by Rosenthal and Murphy.\n\nPoint groups are useful for describing \"rigid\" molecules which undergo only small oscillations about a single equilibrium geometry, and for which the distorting effects of molecular rotation can be ignored, so that the symmetry operations all correspond to simple geometrical operations. However Longuet-Higgins has introduced a more general type of symmetry group suitable not only for rigid molecules but also for \"non-rigid\" molecules that tunnel between equivalent geometries (called \"versions\") and which can also allow for the distorting effects of molecular rotation. These groups are known as \"permutation-inversion\" groups, because the symmetry operations in them are energetically feasible permutations of identical nuclei, or inversion with respect to the center of mass, or a combination of the two.\n\nFor example, ethane (CH) has three equivalent staggered conformations. Tunneling between the conformations occurs at ordinary temperatures by \"internal rotation\" of one methyl group relative to the other. This is not a rotation of the entire molecule about the C axis. Although each conformation has D symmetry, as in the table above, description of the internal rotation and associated quantum states and energy levels requires the more complete permutation-inversion group G.\n\nSimilarly, ammonia (NH) has two equivalent pyramidal (C) conformations which are interconverted by the process known as nitrogen inversion. This is not an inversion in the sense used for point group symmetry operations of rigid molecules (i.e., the inversion of vibrational displacements and electronic coordinates in the center of mass) since NH has no inversion center. Rather it the inversion of all nuclei and electrons in the center of mass (close to the nitrogen atom), which happens to be energetically feasible for this molecule. The appropriate permutation-inversion group to be used in this situation is D(M) which is isomorphic with the point group D.\n\nAdditionally, as examples, the methane (CH) and H molecules have highly symmetric equilibrium structures with T and D point group symmetries respectively; they lack permanent electric dipole moments but they do have very weak pure rotation spectra because of rotational\ncentrifugal distortion. The permutation-inversion groups required for the complete study of CH and H are T(M) and D(M), respectively.\n\nA second and less general approach to the symmetry of nonrigid molecules is due to Altmann. In this approach the symmetry groups are known as \"Schrödinger supergroups\" and consist of two types of operations (and their combinations): (1) the geometric symmetry operations (rotations, reflections, inversions) of rigid molecules, and (2) \"isodynamic operations\", which take a nonrigid molecule into an energetically equivalent form by a physically reasonable process such as rotation about a single bond (as in ethane) or a molecular inversion (as in ammonia).\n\n\n"}
{"id": "47297286", "url": "https://en.wikipedia.org/wiki?curid=47297286", "title": "N conjecture", "text": "N conjecture\n\nIn number theory the \"n\" conjecture is a conjecture stated by as a generalization of the \"abc\" conjecture to more than three integers.\n\nGiven formula_1, let formula_2 satisfy three conditions:\n\nFirst formulation\n\nThe \"n\" conjecture states that for every formula_7, there is a constant formula_8, depending on formula_9 and formula_10, such that:\nwhere formula_12 denotes the radical of the integer formula_13, defined as the product of the distinct prime factors of formula_13.\n\nSecond formulation\n\nDefine the \"quality\" of formula_5 as\nThe \"n\" conjecture states that formula_17.\n\n proposed a stronger variant of the \"n\" conjecture, where setwise coprimeness of formula_5 is replaced by pairwise coprimeness of formula_5.\n\nThere are two different formulations of this \"strong n\" conjecture.\n\nGiven formula_1, let formula_2 satisfy three conditions:\n\nFirst formulation\n\nThe \"strong n\" conjecture states that for every formula_7, there is a constant formula_8, depending on formula_9 and formula_10, such that:\n\nSecond formulation\n\nDefine the \"quality\" of formula_5 as\nThe \"strong n\" conjecture states that formula_33.\n\n"}
{"id": "746751", "url": "https://en.wikipedia.org/wiki?curid=746751", "title": "OpenMath", "text": "OpenMath\n\nOpenMath is the name of a markup language for specifying the meaning of mathematical formulae. Among other things, it can be used to complement MathML, a standard which mainly focuses on the presentation of formulae, with information about their semantic meaning. OpenMath can be encoded in XML or in a binary format.\n\nOpenMath consists of the definition of \"OpenMath Objects\", which is an abstract datatype for describing the logical structure of a mathematical formula and the definition of \"OpenMath Content Dictionaries\", or collections of names for mathematical concepts. The names available from the latter type of collections are specifically intended for use in extending MathML, and conversely, a basic set of such \"Content Dictionaries\" has been designed to be compatible with the small set of mathematical concepts defined in Content MathML, the non-presentational subset of MathML.\n\nOpenMath has been developed in a long series of workshops and (mostly European) research projects that began in 1993 and continues through today. The OpenMath 1.0 Standard was released in February 2000, and revised as OpenMath 1.1 in October 2002. Two years later, the OpenMath 2.0 Standard was released in June 2004. OpenMath 1 fixed the basic language architecture, while OpenMath2 brought better XML integration, structure sharing and liberalized the notion of OpenMath Content dictionaries.\n\nThe OpenMath Effort is governed by the OpenMath Society, based in Helsinki, Finland. The Society brings together tool builders, software suppliers, publishers and authors. Membership is by invitation of the Societies Executive Committee, which welcomes self-nominations of individuals who have worked on OpenMath-related issues in research or application. As of 2007, Michael Kohlhase is president of the OpenMath society. He succeeded Arjeh M. Cohen, who was the first president.\n\nThe well-known quadratic formula:\n\nwould be marked up like this in OpenMath (the representation is an expression tree made up from functional elements like OMA for function application or OMV for variables):\n\nIn the expression tree above symbols—i.e. elements like <nowiki><OMS cd=\"arith1\" name=\"times\"/></nowiki>—stand for mathematical functions that are applied to sibling expressions in an OMA which are interpreted as arguments. The OMS element is a generic extension element that means whatever is specified in the content dictionary referred to in the cd attribute (this document can be found at the URI specified in the innermost cdbase attribute dominating the respective OMS element. In the example above, all symbols come from the content dictionary for arithmetics (arith1, see below), except for the plusminus, which comes from a non-standard place, hence the cdbase attribute here.\n\nContent Dictionaries are structured XML documents that define mathematical symbols that can be referred to by <nowiki>OMS</nowiki> elements in OpenMath Objects. The OpenMath 2 standard does not prescribe a canonical encoding for content dictionaries, but only requires an infrastructure sufficient for unique referencing in <nowiki>OMS</nowiki> elements. OpenMath provides a very basic XML encoding that meets these requirements, and a set of specific content dictionaries for some areas of mathematics, in particular covering the K-14 fragment covered by content MathML.\n\nFor more richly structured content dictionaries (and generally for arbitrary mathematical documents) the OMDoc format extends OpenMath by a “statement level” (including structures like definitions, theorems, proofs and examples, as well as means for interrelating them) and a “theory level”, where a theory is a collection of several contextually related statements. OMDoc's theories are designed to be compatible to OpenMath content dictionaries, but they can also be set into inheritance and import relations.\n\nOpenMath is criticised for being inadequate for general mathematics, exposing not enough formal precision to capture the intricacies of numerics, lacking a proof-of-concept and as an inferior technology to already established approaches of encoding mathematical semantics, amongst other presumed shortcomings.\n\n\n"}
{"id": "36974047", "url": "https://en.wikipedia.org/wiki?curid=36974047", "title": "P-adic Teichmüller theory", "text": "P-adic Teichmüller theory\n\nIn mathematics, \"p\"-adic Teichmüller theory describes the \"uniformization\" of \"p\"-adic curves and their moduli, generalizing the usual Teichmüller theory that describes the uniformization of Riemann surfaces and their moduli. It was introduced and developed by .\n\nThe first problem is to reformulate the Fuchsian uniformization of a complex Riemann surface (an isomorphism from the upper half plane to a universal covering space of the surface) in a way that makes sense for \"p\"-adic curves. The existence of a Fuchsian uniformization is equivalent to the existence of a canonical indigenous bundle over the Riemann surface: the unique indigenous bundle that is invariant under complex conjugation and whose monodromy representation is quasi-Fuchsian. For \"p\"-adic curves the analogue of complex conjugation is the Frobenius endomorphism, and the analogue of the quasi-Fuchsian condition is an integrality condition on the indigenous line bundle. So \"p\"-adic Teichmüller theory, the \"p\"-adic analogue the Fuchsian uniformization of Teichmüller theory, is the study of integral Frobenius invariant indigenous bundles.\n\n\n"}
{"id": "299184", "url": "https://en.wikipedia.org/wiki?curid=299184", "title": "Percolation", "text": "Percolation\n\nIn physics, chemistry and materials science, percolation (from Latin \"percōlāre\", \"to filter\" or \"trickle through\") refers to the movement and filtering of fluids through porous materials.\n\nDuring the last decades, percolation theory, the mathematical study of percolation, has brought new understanding and techniques to a broad range of topics in physics, materials science, complex networks, epidemiology, and other fields. For example, in geology, percolation refers to filtration of water through soil and permeable rocks. The water flows to recharge the groundwater in the water table and aquifers. In places where infiltration basins or septic drain fields are planned to dispose of substantial amounts of water, a percolation test is needed beforehand to determine whether the intended structure is likely to succeed or fail.\n\nPercolation typically exhibits universality. Statistical physics concepts such as scaling theory, renormalization, phase transition, critical phenomena and fractals are used to characterize percolation properties. Combinatorics is commonly employed to study percolation thresholds.\n\nDue to the complexity involved in obtaining exact results from analytical models of percolation, computer simulations are typically used. The current fastest algorithm for percolation was published in 2000 by Mark Newman and Robert Ziff.\n\n\n\n"}
{"id": "47787685", "url": "https://en.wikipedia.org/wiki?curid=47787685", "title": "Physical biochemistry", "text": "Physical biochemistry\n\nPhysical biochemistry is a branch of biochemistry that deals with the theory, techniques and methodology used to study the physical chemistry of biomolecules. \nIt also deals with the mathematical approaches for the analysis of biochemical reaction and the modelling of biological systems. It provides insight to the structure of macromolecules, and how chemical structure influences physical properties of a biological substance.\n\n"}
{"id": "3891878", "url": "https://en.wikipedia.org/wiki?curid=3891878", "title": "Probabilistic metric space", "text": "Probabilistic metric space\n\nA probabilistic metric space is a generalization of metric spaces where the distance has no longer values in non-negative real numbers, but in distribution functions. \n\nLet \"D+\" be the set of all probability distribution functions F such that F(0) = 0 (F is a nondecreasing, left\ncontinuous mapping from formula_1 into [0, 1] such that max(F) = 1).\n\nThe ordered pair (S,F) is said to be a probabilistic metric space if S is a nonempty set and F: S×S →\nD+ (F(p, q) is denoted by F for every (p, q) ∈ S × S) satisfies the following conditions:\n\nA probability metric \"D\" between two random variables \"X\" and \"Y\" may be defined e.g. as: \n\nwhere \"F\"(\"x\", \"y\") denotes the joint probability density function of random variables \"X\" and \"Y\". Obviously if \"X\" and \"Y\" are independent from each other the equation above transforms into:\n\nwhere \"f\"(\"x\") and \"g\"(\"y\") are probability density functions of \"X\" and \"Y\" respectively.\n\nOne may easily show that such probability metrics do not satisfy the first metric axiom or satisfies is only if, and only if, both of its arguments \"X\", \"Y\" are certain events described by Dirac delta density probability distribution functions. In this case:\n\nthe probability metric simply transforms into the metric between expected values formula_5, formula_6 of the variables \"X\" and \"Y\". \n\nFor all other random variables \"X\", \"Y\" the probability metric does not satisfy the identity of indiscernibles condition required to be satisfied by the metric of the metric space, that is:\n\nFor example if both probability distribution functions of random variables \"X\" and \"Y\" are normal distributions (N) having the same standard deviation formula_8, integrating formula_9 yields to:\n\nwhere:\n\nand formula_12 is the complementary error function.\n\nIn this case:\n\nThe probability metric of random variables may be extended into metric \"D\"(X, Y) of random vectors X, Y by substituting formula_14 with any metric operator \"d\"(x,y):\n\nwhere \"F\"(X, Y) is the joint probability density function of random vectors X and Y.\nFor example substituting \"d\"(x,y) with Euclidean metric and providing the vectors X and Y are mutually independent would yield to:\n"}
{"id": "25418", "url": "https://en.wikipedia.org/wiki?curid=25418", "title": "Proof by contradiction", "text": "Proof by contradiction\n\nIn logic, proof by contradiction is a form of proof, and more specifically a form of indirect proof, that establishes the truth or validity of a proposition. It starts by assuming that the opposite proposition is true, and then shows that such an assumption leads to a contradiction. Proof by contradiction is also known as indirect proof, apagogical argument, proof by assuming the opposite, and reductio ad impossibilem. It is a particular kind of the more general form of argument known as \"reductio ad absurdum\".\n\nG. H. Hardy described proof by contradiction as \"one of a mathematician's finest weapons\", saying \"It is a far finer gambit than any chess gambit: a chess player may offer the sacrifice of a pawn or even a piece, but a mathematician offers the game.\"\n\nProof by contradiction is based on the law of noncontradiction as first formalized as a metaphysical principle by Aristotle. Noncontradiction is also a theorem in propositional logic. This states that an assertion or mathematical statement cannot be both true and false. That is, a proposition \"Q\" and its negation formula_1\"Q\" (\"not-\"Q\"\") cannot both be true. In a proof by contradiction, it is shown that the denial of the statement being proved results in such a contradiction. It has the form of a \"reductio ad absurdum\" argument. If \"P\" is the proposition to be proved:\n\nAn alternate form derives a contradiction with the statement to be proved itself:\n\nAn existence proof by contradiction assumes that some object doesn't exist, and then proves that this would lead to a contradiction; thus, such an object must exist. Although it is quite freely used in mathematical proofs, not every school of mathematical thought accepts this kind of nonconstructive proof as universally valid.\n\nProof by contradiction also depends on the law of the excluded middle, also first formulated by Aristotle. This states that either an assertion or its negation must be true\nThat is, there is no other truth value besides \"true\" and \"false\" that a proposition can take. Combined with the principle of noncontradiction, this means that exactly one of formula_9 and formula_10 is true. In proof by contradiction, this permits the conclusion that since the possibility of formula_10 has been excluded, formula_9 must be true.\n\nThe law of the excluded middle is accepted in virtually all formal logics; however, some intuitionist mathematicians do not accept it, and thus reject proof by contradiction as a proof technique.\n\nProof by contradiction is closely related to proof by contrapositive, and the two are sometimes confused, though they are distinct methods. The main distinction is that a proof by contrapositive applies only to statements formula_9 that can be written in the form formula_14 (i.e., implications), whereas the technique of proof by contradiction applies to statements formula_9 of any form:\n\nIn the case where the statement to be proven \"is\" an implication formula_19, let us look at the differences between direct proof, proof by contrapositive, and proof by contradiction:\n\nA classic proof by contradiction from mathematics is the proof that the square root of 2 is irrational. If it were rational, it could be expressed as a fraction \"a\"/\"b\" in lowest terms, where \"a\" and \"b\" are integers, at least one of which is odd. But if \"a\"/\"b\" = , then \"a\" = 2\"b\". Therefore, \"a\" must be even. Because the square of an odd number is odd, that in turn implies that \"a\" is even. This means that \"b\" must be odd because a/b is in lowest terms.\n\nOn the other hand, if \"a\" is even, then \"a\" is a multiple of 4. If \"a\" is a multiple of 4 and \"a\" = 2\"b\", then 2\"b\" is a multiple of 4, and therefore \"b\" is even, and so is \"b\".\n\nSo \"b\" is odd and even, a contradiction. Therefore, the initial assumption—that can be expressed as a fraction—must be false.\n\nThe method of proof by contradiction has also been used to show that for any non-degenerate right triangle, the length of the hypotenuse is less than the sum of the lengths of the two remaining sides. The proof relies on the Pythagorean theorem. Letting \"c\" be the length of the hypotenuse and \"a\" and \"b\" the lengths of the legs, the claim is that \"a\" + \"b\" > \"c\".\n\nThe claim is negated to assume that \"a\" + \"b\" ≤ \"c\". Squaring both sides results in (\"a\" + \"b\") ≤ \"c\" or, equivalently, \"a\" + 2\"ab\" + \"b\" ≤ \"c\". A triangle is non-degenerate if each edge has positive length, so it may be assumed that \"a\" and \"b\" are greater than 0. Therefore, \"a\" + \"b\" < \"a\" + 2\"ab\" + \"b\" ≤ \"c\". The transitive relation may be reduced to \"a\" + \"b\" < \"c\". It is known from the Pythagorean theorem that \"a\" + \"b\" = \"c\". This results in a contradiction since strict inequality and equality are mutually exclusive. The latter was a result of the Pythagorean theorem and the former the assumption that \"a\" + \"b\" ≤ \"c\". The contradiction means that it is impossible for both to be true and it is known that the Pythagorean theorem holds. It follows that the assumption that \"a\" + \"b\" ≤ \"c\" must be false and hence \"a\" + \"b\" > \"c\", proving the claim.\n\nConsider the proposition, \"P\": \"there is no smallest rational number greater than 0\". In a proof by contradiction, we start by assuming the opposite, ¬\"P\": that there \"is\" a smallest rational number, say, \"r\".\n\nNow \"r\"/2 is a rational number greater than 0 and smaller than \"r\". But that contradicts our initial assumption, ¬\"P\", that \"r\" was the \"smallest\" rational number. (In the above symbolic argument, \"\"r\" is the smallest rational number\" would be \"Q\" and \"\"r\"/2 is a rational number smaller than \"r\"\" would be ¬\"Q\".)\nSo we can conclude that the original proposition, \"P\", must be true — \"there is no smallest rational number greater than 0\".\n\nFor other examples, see proof that the square root of 2 is not rational (where indirect proofs different from the above one can be found) and Cantor's diagonal argument.\n\nProofs by contradiction sometimes end with the word \"Contradiction!\". Isaac Barrow and Baermann used the notation Q.E.A., for \"quod est absurdum\" (\"which is absurd\"), along the lines of Q.E.D., but this notation is rarely used today. A graphical symbol sometimes used for contradictions is a downwards zigzag arrow \"lightning\" symbol (U+21AF: ↯), for example in Davey and Priestley. Others sometimes used include a pair of opposing arrows (as formula_28 or formula_29), struck-out arrows (formula_30), a stylized form of hash (such as U+2A33: ⨳), or the \"reference mark\" (U+203B: ※). The \"up tack\" symbol (U+22A5: ⊥) used by philosophers and logicians (see contradiction) also appears, but is often avoided due to its usage for orthogonality.\n\nA curious logical consequence of the principle of non-contradiction is that a contradiction implies any statement; if a contradiction is accepted as true, any proposition (or its negation) can be proved from it. This is known as the principle of explosion (, \"from a falsehood, anything [follows]\", or \"\", \"from a contradiction, anything follows\"), or the principle of pseudo-scotus.\nThus a contradiction in a formal axiomatic system is disastrous; since any theorem can be proven true it destroys the conventional meaning of truth and falsity.\n\nThe discovery of contradictions at the foundations of mathematics at the beginning of the 20th century, such as Russell's paradox, threatened the entire structure of mathematics due to the principle of explosion. This motivated a great deal of work during the 20th century to create consistent axiomatic systems to provide a logical underpinning for mathematics. This has also led a few philosophers such as Newton da Costa, Walter Carnielli and Graham Priest to reject the principle of non-contradiction, giving rise to theories such as paraconsistent logic and dialethism, which accepts that there exist statements that are both true and false.\n\n\n"}
{"id": "5075270", "url": "https://en.wikipedia.org/wiki?curid=5075270", "title": "Pseudonormal space", "text": "Pseudonormal space\n\nIn mathematics, in the field of topology, a topological space is said to be pseudonormal if given two disjoint closed sets in it, one of which is countable, there are disjoint open sets containing them. Note the following:\n\nAn example of a pseudonormal Moore space that is not metrizable was given by , in connection with the conjecture that all normal Moore spaces are metrizable.\n"}
{"id": "3857398", "url": "https://en.wikipedia.org/wiki?curid=3857398", "title": "Pursuit-evasion", "text": "Pursuit-evasion\n\nPursuit-evasion (variants of which are referred to as cops and robbers and graph searching) is a family of problems in mathematics and computer science in which one group attempts to track down members of another group in an environment. Early work on problems of this type modeled the environment geometrically. In 1976, Torrence Parsons introduced a formulation whereby movement is constrained by a graph. The geometric formulation is sometimes called continuous pursuit-evasion, and the graph formulation discrete pursuit-evasion (also called graph searching). Current research is typically limited to one of these two formulations.\n\nIn the discrete formulation of the pursuit-evasion problem, the environment is modeled as a graph.\n\nThere are innumerable possible variants of pursuit-evasion, though they tend to share many elements. A typical, basic example is as follows (cops and robber games): Pursuers and evaders occupy nodes of a graph. The two sides take alternate turns, which consist of each member either staying put or moving along an edge to an adjacent node. If a pursuer occupies the same node as an evader the evader is captured and removed from the graph. The question usually posed is how many pursuers are necessary to ensure the eventual capture of all the evaders. If one pursuer suffices, the graph is called a cop-win graph. In this case, a single evader can always be captured in time linear to the number of \"n\" nodes of the graph. Capturing \"r\" evaders with \"k\" pursuers can take in the order of \"rn\" time as well, but the exact bounds for more than one pursuer are still unknown.\n\nOften the movement rules are altered by changing the velocity of the evaders. This velocity is the maximum number of edges that an evader can move along in a single turn. In the example above, the evaders have a velocity of one. At the other extreme is the concept of infinite velocity, which allows an evader to move to any node in the graph so long as there is a path between its original and final positions that contains no nodes occupied by a pursuer. Similarly some variants arm the pursuers with \"helicopters\" which allow them to move to any vertex on their turn.\n\nOther variants ignore the restriction that pursuers and evaders must always occupy a node and allow for the possibility that they are positioned somewhere along an edge. These variants are often referred to as sweeping problems, whilst the previous variants would fall under the category of searching problems.\n\nSeveral variants are equivalent to important graph parameters. Specifically, finding the number of pursuers necessary to capture a single evader with infinite velocity in a graph \"G\" (when pursuers and evader are not constrained to move turn by turn, but move simultaneously) is equivalent to finding the treewidth of \"G\", and a winning strategy for the evader may be described in terms of a haven in \"G\". If this evader is invisible to the pursuers then the problem is equivalent to finding the pathwidth or vertex separation. Finding the number of pursuers necessary to capture a single invisible evader in a graph \"G\" in a single turn (that is, one movement by the pursuers from their initial deployment) is equivalent to finding the size of the minimum dominating set of \"G\", assuming the pursuers can initially deploy wherever they like (this later assumption holds when pursuers and evader are assumed to move turn by turn).\n\nThe board game Scotland Yard is a variant of the pursuit-evasion problem.\n\nThe complexity of several pursuit-evasion variants, namely how many pursuers are needed to clear a given graph and how a given number of pursuers should move on the graph to clear it with either a minimum sum of their travel distances or minimum task-completion time, has been studied by Nimrod Megiddo, S. L. Hakimi, Michael R. Garey, David S. Johnson, and Christos H. Papadimitriou (J. ACM 1988), and R. Borie, C. Tovey and S. Koenig.\n\nSolving multi-player pursuit-evasion games has also received increased attention. See R Vidal et al., Chung and Furukawa , Hespanha et al. and the references therein. Marcos A. M. Vieira and Ramesh Govindan and Gaurav S. Sukhatme provided an algorithm that computes the minimal completion time strategy for pursuers to capture all evaders when all players make optimal decisions based on complete knowledge. This algorithm can also be applied to when evader are significantly faster than pursuers. Unfortunately, these algorithms do not scale beyond a small number of robots. To overcome this problem, Marcos A. M. Vieira and Ramesh Govindan and Gaurav S. Sukhatme design and implement a partition algorithm where pursuers capture evaders by decomposing the game into multiple multi-pursuer single-evader games.\n\nIn the continuous formulation of pursuit-evasion games, the environment is modeled geometrically, typically taking the form of the Euclidean plane or another manifold. Variants of the game may impose maneuverability constraints on the players, such as a limited range of speed or acceleration. Obstacles may also be used.\n\nOne of the initial applications of the pursuit-evasion problem was missile guidance systems formulated by Rufus Isaacs at the RAND Corporation.\n\n\n"}
{"id": "52423915", "url": "https://en.wikipedia.org/wiki?curid=52423915", "title": "Rick Jardine", "text": "Rick Jardine\n\nJohn Frederick \"Rick\" Jardine (born December 6, 1951 in Belleville, Canada) is a Canadian mathematician working in the fields of homotopy theory, category theory, and number theory. \n\nJardine obtained his Ph.D. from the University of British Columbia in 1981, under the direction of Roy Douglas. Following a research fellowship at the University of Toronto and a Dickson instructorship at the University of Chicago, he joined the Department of Mathematics at the University of Western Ontario in 1984, where he is currently a professor.\n\nFrom 2002 to 2016, Jardine held a Canada Research Chair in applied homotopy theory. Since 2008, he is fellow of the Fields Institute, and has been recognized with the Coxeter–James Prize in 1992 by the Canadian Mathematical Society.\n\nJardine is known for his work on model category structures on simplicial presheaves.\n\n"}
{"id": "23756785", "url": "https://en.wikipedia.org/wiki?curid=23756785", "title": "Right conoid", "text": "Right conoid\n\nIn geometry, a right conoid is a ruled surface generated by a family of straight lines that all intersect perpendicularly to a fixed straight line, called the \"axis\" of the right conoid.\n\nUsing a Cartesian coordinate system in three-dimensional space, if we take the z-axis to be the axis of a right conoid, then the right conoid can be represented by the parametric equations:\n\nwhere \"h\"(\"u\") is some function for representing the \"height\" of the moving line.\n\nA typical example of right conoids is given by the parametric equations\n\nThe image on the right shows how the coplanar lines generate the right conoid.\n\nOther right conoids include:\n\n\n"}
{"id": "364460", "url": "https://en.wikipedia.org/wiki?curid=364460", "title": "Ring of symmetric functions", "text": "Ring of symmetric functions\n\nIn algebra and in particular in algebraic combinatorics, the ring of symmetric functions is a specific limit of the rings of symmetric polynomials in \"n\" indeterminates, as \"n\" goes to infinity. This ring serves as universal structure in which relations between symmetric polynomials can be expressed in a way independent of the number \"n\" of indeterminates (but its elements are neither polynomials nor functions). Among other things, this ring plays an important role in the representation theory of the symmetric group.\n\nThe ring of symmetric functions can be given a coproduct and a bilinear form making it into a positive selfadjoint graded Hopf algebra that is both commutative and cocommutative.\n\nThe study of symmetric functions is based on that of symmetric polynomials. In a polynomial ring in some finite set of indeterminates, a polynomial is called \"symmetric\" if it stays the same whenever the indeterminates are permuted in any way. More formally, there is an action by ring automorphisms of the symmetric group \"S\" on the polynomial ring in \"n\" indeterminates, where a permutation acts on a polynomial by simultaneously substituting each of the indeterminates for another according to the permutation used. The invariants for this action form the subring of symmetric polynomials. If the indeterminates are \"X\"...,\"X\", then examples of such symmetric polynomials are\n\nand\n\nA somewhat more complicated example is\n\"X\"\"X\"\"X\" +\"X\"\"X\"\"X\" +\"X\"\"X\"\"X\" +\"X\"\"X\"\"X\" +\"X\"\"X\"\"X\" +\"X\"\"X\"\"X\" +...\nwhere the summation goes on to include all products of the third power of some variable and two other variables. There are many specific kinds of symmetric polynomials, such as elementary symmetric polynomials, power sum symmetric polynomials, monomial symmetric polynomials, complete homogeneous symmetric polynomials, and Schur polynomials.\n\nMost relations between symmetric polynomials do not depend on the number \"n\" of indeterminates, other than that some polynomials in the relation might require \"n\" to be large enough in order to be defined. For instance the Newton's identity for the third power sum polynomial \"p\" leads to\nwhere the formula_5 denote elementary symmetric polynomials; this formula is valid for all natural numbers \"n\", and the only notable dependency on it is that \"e\"(\"X\"...,\"X\") = 0 whenever \"n\" < \"k\". One would like to write this as an identity \nthat does not depend on \"n\" at all, and this can be done in the ring of symmetric functions. In that ring there are elements \"e\" for all integers \"k\" ≥ 1, and any element of the ring can be given by a polynomial expression in the elements \"e\".\n\nA ring of symmetric functions can be defined over any commutative ring \"R\", and will be denoted Λ; the basic case is for \"R\" = Z. The ring Λ is in fact a graded \"R\"-algebra. There are two main constructions for it; the first one given below can be found in (Stanley, 1999), and the second is essentially the one given in (Macdonald, 1979).\n\nThe easiest (though somewhat heavy) construction starts with the ring of formal power series formula_7 over \"R\" in infinitely (countably) many indeterminates; the elements of this power series ring are formal infinite sums of terms, each of which consists of a coefficient from \"R\" multiplied by a monomial, where each monomial is a product of finitely many finite powers of indeterminates. One defines Λ as its subring consisting of those power series \"S\" that satisfy \nNote that because of the second condition, power series are used here only to allow infinitely many terms of a fixed degree, rather than to sum terms of all possible degrees. Allowing this is necessary because an element that contains for instance a term \"X\" should also contain a term \"X\" for every \"i\" > 1 in order to be symmetric. Unlike the whole power series ring, the subring Λ is graded by the total degree of monomials: due to condition 2, every element of Λ is a finite sum of homogeneous elements of Λ (which are themselves infinite sums of terms of equal degree). For every \"k\" ≥ 0, the element \"e\" ∈ Λ is defined as the formal sum of all products of \"k\" distinct indeterminates, which is clearly homogeneous of degree \"k\".\n\nAnother construction of Λ takes somewhat longer to describe, but better indicates the relationship with the rings \"R\"[\"X\"...,\"X\"] of symmetric polynomials in \"n\" indeterminates. For every \"n\" there is a surjective ring homomorphism ρ from the analogous ring \"R\"[\"X\"...,\"X\"] with one more indeterminate onto \"R\"[\"X\"...,\"X\"], defined by setting the last indeterminate \"X\" to 0. Although ρ has a non-trivial kernel, the nonzero elements of that kernel have degree at least formula_8 (they are multiples of \"X\"\"X\"...\"X\"). This means that the restriction of ρ to elements of degree at most \"n\" is a bijective linear map, and ρ(\"e\"(\"X\"...,\"X\")) = \"e\"(\"X\"...,\"X\") for all \"k\" ≤ \"n\". The inverse of this restriction can be extended uniquely to a ring homomorphism φ from \"R\"[\"X\"...,\"X\"] to \"R\"[\"X\"...,\"X\"], as follows for instance from the fundamental theorem of symmetric polynomials. Since the images φ(\"e\"(\"X\"...,\"X\")) = \"e\"(\"X\"...,\"X\") for \"k\" = 1...,\"n\" are still algebraically independent over \"R\", the homomorphism φ is injective and can be viewed as a (somewhat unusual) inclusion of rings; applying φ to a polynomial amounts to adding all monomials containing the new indeterminate obtained by symmetry from monomials already present. The ring Λ is then the \"union\" (direct limit) of all these rings subject to these inclusions. Since all φ are compatible with the grading by total degree of the rings involved, Λ obtains the structure of a graded ring.\n\nThis construction differs slightly from the one in (Macdonald, 1979). That construction only uses the surjective morphisms ρ without mentioning the injective morphisms φ: it constructs the homogeneous components of Λ separately, and equips their direct sum with a ring structure using the ρ. It is also observed that the result can be described as an inverse limit in the category of \"graded\" rings. That description however somewhat obscures an important property typical for a \"direct\" limit of injective morphisms, namely that every individual element (symmetric function) is already faithfully represented in some object used in the limit construction, here a ring \"R\"[\"X\"...,\"X\"]. It suffices to take for \"d\" the degree of the symmetric function, since the part in degree \"d\" of that ring is mapped isomorphically to rings with more indeterminates by φ for all \"n\" ≥ \"d\". This implies that for studying relations between individual elements, there is no fundamental difference between symmetric polynomials and symmetric functions.\n\nIt should be noted that the name \"symmetric function\" for elements of Λ is a misnomer: in neither construction the elements are functions, and in fact, unlike symmetric polynomials, no function of independent variables can be associated to such elements (for instance \"e\" would be the sum of all infinitely many variables, which is not defined unless restrictions are imposed on the variables). However the name is traditional and well established; it can be found both in (Macdonald, 1979), which says (footnote on p. 12)\nThe elements of Λ (unlike those of Λ) are no longer polynomials: they are formal infinite sums of monomials. We have therefore reverted to the older terminology of symmetric functions.\n(here Λ denotes the ring of symmetric polynomials in \"n\" indeterminates), and also in (Stanley, 1999).\n\nTo define a symmetric function one must either indicate directly a power series as in the first construction, or give a symmetric polynomial in \"n\" indeterminates for every natural number \"n\" in a way compatible with the second construction. An expression in an unspecified number of indeterminates may do both, for instance\ncan be taken as the definition of an elementary symmetric function if the number of indeterminates is infinite, or as the definition of an elementary symmetric polynomial in any finite number of indeterminates. Symmetric polynomials for the same symmetric function should be compatible with the morphisms ρ (decreasing the number of indeterminates is obtained by setting some of them to zero, so that the coefficients of any monomial in the remaining indeterminates is unchanged), and their degree should remain bounded. (An example of a family of symmetric polynomials that fails both conditions is formula_10; the family formula_11 fails only the second condition.) Any symmetric polynomial in \"n\" indeterminates can be used to construct a compatible family of symmetric polynomials, using the morphisms ρ for \"i\" < \"n\" to decrease the number of indeterminates, and φ for \"i\" ≥ \"n\" to increase the number of indeterminates (which amounts to adding all monomials in new indeterminates obtained by symmetry from monomials already present).\n\nThe following are fundamental examples of symmetric functions.\n\nThere is no power sum symmetric function \"p\": although it is possible (and in some contexts natural) to define formula_14 as a symmetric \"polynomial\" in \"n\" variables, these values are not compatible with the morphisms ρ. The \"discriminant\" formula_15 is another example of an expression giving a symmetric polynomial for all \"n\", but not defining any symmetric function. The expressions defining Schur polynomials as a quotient of alternating polynomials are somewhat similar to that for the discriminant, but the polynomials \"s\"(\"X\"...,\"X\") turn out to be compatible for varying \"n\", and therefore do define a symmetric function.\n\nFor any symmetric function \"P\", the corresponding symmetric polynomials in \"n\" indeterminates for any natural number \"n\" may be designated by \"P\"(\"X\"...,\"X\"). The second definition of the ring of symmetric functions implies the following fundamental principle:\n\nThis is because one can always reduce the number of variables by substituting zero for some variables, and one can increase the number of variables by applying the homomorphisms φ; the definition of those homomorphisms assures that φ(\"P\"(\"X\"...,\"X\")) = \"P\"(\"X\"...,\"X\") (and similarly for \"Q\") whenever \"n\" ≥ \"d\". See a proof of Newton's identities for an effective application of this principle.\n\nThe ring of symmetric functions is a convenient tool for writing identities between symmetric polynomials that are independent of the number of indeterminates: in Λ there is no such number, yet by the above principle any identity in Λ automatically gives identities the rings of symmetric polynomials over \"R\" in any number of indeterminates. Some fundamental identities are\nwhich shows a symmetry between elementary and complete homogeneous symmetric functions; these relations are explained under complete homogeneous symmetric polynomial.\nthe Newton identities, which also have a variant for complete homogeneous symmetric functions:\n\nImportant properties of Λ include the following.\n\n\nProperty 2 is the essence of the fundamental theorem of symmetric polynomials. It immediately implies some other properties:\n\nThis final point applies in particular to the family (\"h\") of complete homogeneous symmetric functions. \nIf \"R\" contains the field formula_21 of rational numbers, it applies also to the family (\"p\") of power sum symmetric functions. This explains why the first \"n\" elements of each of these families define sets of symmetric polynomials in \"n\" variables that are free polynomial generators of that ring of symmetric polynomials.\n\nThe fact that the complete homogeneous symmetric functions form a set of free polynomial generators of Λ already shows the existence of an automorphism ω sending the elementary symmetric functions to the complete homogeneous ones, as mentioned in property 3. The fact that ω is an involution of Λ follows from the symmetry between elementary and complete homogeneous symmetric functions expressed by the first set of relations given above.\n\nThe ring of symmetric functions Λ is the Exp ring of the integers Z. It is also a lambda-ring in a natural fashion; in fact it is the universal lambda-ring in one generator.\n\nThe first definition of Λ as a subring of formula_22 allows the generating functions of several sequences of symmetric functions to be elegantly expressed. Contrary to the relations mentioned earlier, which are internal to Λ, these expressions involve operations taking place in \"R\"<nowiki></nowiki>\"X\",\"X\"...;\"t\"<nowiki></nowiki> but outside its subring Λ, so they are meaningful only if symmetric functions are viewed as formal power series in indeterminates \"X\". We shall write \"(\"X\")\" after the symmetric functions to stress this interpretation.\n\nThe generating function for the elementary symmetric functions is\nSimilarly one has for complete homogeneous symmetric functions\nThe obvious fact that formula_25 explains the symmetry between elementary and complete homogeneous symmetric functions.\nThe generating function for the power sum symmetric functions can be expressed as\n((Macdonald, 1979) defines \"P\"(\"t\") as Σ \"p\"(\"X\")\"t\", and its expressions therefore lack a factor \"t\" with respect to those given here). The two final expressions, involving the formal derivatives of the generating functions \"E\"(\"t\") and \"H\"(\"t\"), imply Newton's identities and their variants for the complete homogeneous symmetric functions. These expressions are sometimes written as\nwhich amounts to the same, but requires that \"R\" contain the rational numbers, so that the logarithm of power series with constant term 1 is defined (by formula_28).\n\n\n"}
{"id": "15294970", "url": "https://en.wikipedia.org/wiki?curid=15294970", "title": "Seesaw molecular geometry", "text": "Seesaw molecular geometry\n\nDisphenoidal or Seesaw is a type of molecular geometry where there are four bonds to a central atom with overall C symmetry. The name \"seesaw\" comes from the observation that it looks like a playground seesaw. Most commonly, four bonds to a central atom result in tetrahedral or, less commonly, square planar geometry. The seesaw geometry, just like its name, is unusual.\n\nIt occurs when a molecule has a steric number of 5, with the central atom being bonded to 4 other atoms and 1 lone pair (AXE in AXE notation). An atom bonded to 5 other atoms (and no lone pairs) forms a trigonal bipyramid; but in this case one of the atoms is replaced by a lone pair. The atom replaced is always an equatorial atom, because the lone pairs repel other electrons more strongly than atoms do.\n\nCompounds with disphenoidal geometry (See-Saw Geometry) have two types of ligands: axial and equatorial. The axial pair lie along a common bond axis so that are related by a bond angle of 180°. The equatorial pair of ligands is situated in a plane orthogonal to the axis of the axial pair. Typically the bond distance to the axial ligands is longer than to the equatorial ligands. The ideal angle between the axial ligands and the equatorial ligands is 90°; whereas the ideal angle between the two equatorial ligands themselves is 120°.\n\nDisphenoidal molecules, like trigonal bipyramidal ones, are subject to Berry pseudorotation. Thus, the F NMR spectrum of SF (like that of PF) consists of single resonance near room temperature. The four atoms in motion act as a lever about the central atom; for example, the four fluorine atoms of sulfur tetrafluoride rotate around the sulfur atom.\n\nSulfur tetrafluoride is the premier example of a molecule with the disphenoidal molecular geometry (see image at upper right). The following compounds and ions have disphenoidal geometry:\n\n\n\n"}
{"id": "13651683", "url": "https://en.wikipedia.org/wiki?curid=13651683", "title": "Spectral clustering", "text": "Spectral clustering\n\nIn multivariate statistics and the clustering of data, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\n\nIn application to image segmentation, spectral clustering is known as segmentation-based object categorization.\n\nGiven an enumerated set of data points, the similarity matrix may be defined as a symmetric matrix formula_1, where formula_2 represents a measure of the similarity between data points with indices formula_3 and formula_4. The general approach to spectral clustering is to use a standard clustering method (there are many such methods, \"k\"-means is discussed below) on relevant eigenvectors of a Laplacian matrix of formula_1. There are many different ways to define a Laplacian which have different mathematical interpretations, and so the clustering will also have different interpretations. The eigenvectors that are relevant are the ones that correspond to smallest several eigenvalues of the Laplacian except for the smallest eigenvalue which will have a value of 0. For computational efficiency, these eigenvectors are often computed as the eigenvectors corresponding to the largest several eigenvalues of a function of the Laplacian.\n\nSpectral clustering is well known to relate to partitioning of a mass-spring system, where each mass is associated with a data point and each spring stiffness corresponds to a weight of an edge describing a similarity of the two related data points. Specifically, the classical reference explains that the eigenvalue problem describing transversal vibration modes of a mass-spring system is exactly the same as the eigenvalue problem for the graph Laplacian matrix defined as \nwhere formula_7 is the diagonal matrix\nThe masses that are tightly connected by the springs in the mass-spring system evidently move together from the equilibrium position in low-frequency vibration modes, so that the components of the eigenvectors corresponding to the smallest eigenvalues of the graph Laplacian can be used for meaningful clustering of the masses. \n\nA popular related spectral clustering technique is the normalized cuts algorithm or \"Shi–Malik algorithm\" introduced by Jianbo Shi and Jitendra Malik, commonly used for image segmentation. It partitions points into two sets formula_9 based on the eigenvector formula_10 corresponding to the second-smallest eigenvalue of the \nsymmetric normalized Laplacian defined as\n\nA mathematically equivalent algorithm takes the eigenvector corresponding to the largest eigenvalue of the random walk normalized adjacency matrix formula_12. \n\nKnowing the eigenvectors, partitioning may be done in various ways, such as by computing the median formula_13 of the components of the second smallest eigenvector formula_10, and placing all points whose component in formula_10 is greater than formula_13 in formula_17, and the rest in formula_18. The algorithm can be used for hierarchical clustering by repeatedly partitioning the subsets in this fashion.\n\nIf the similarity matrix formula_1 has not already been explicitly constructed, the efficiency of spectral clustering may be improved if the solution to the corresponding eigenvalue problem is performed in a matrix-free fashion (without explicitly manipulating or even computing the similarity matrix), as in the Lanczos algorithm.\n\nFor large-sized graphs, the second eigenvalue of the (normalized) graph Laplacian matrix is often ill-conditioned, leading to slow convergence of iterative eigenvalue solvers. Preconditioning is a key technology accelerating the convergence, e.g., in the matrix-free LOBPCG method. Spectral clustering has been successfully applied on large graphs by first identifying their community structure, and then clustering communities.\n\nSpectral clustering is closely related to nonlinear dimensionality reduction, and dimension reduction techniques such as locally-linear embedding can be used to reduce errors from noise or outliers.\n\nFree software to implement spectral clustering is available in large open source projects like Scikit-learn using LOBPCG with multigrid preconditioning or ARPACK, MLlib for pseudo-eigenvector clustering using the power iteration method, and R.\n\nThe kernel \"k\"-means problem is an extension of the \"k\"-means problem where the input data points are mapped non-linearly into a higher-dimensional feature space via a kernel function formula_20. The weighted kernel \"k\"-means problem further extends this problem by defining a weight formula_21 for each cluster as the reciprocal of the number of elements in the cluster,\nSuppose formula_23 is a matrix of the normalizing coefficients for each point for each cluster formula_24 if formula_25 and zero otherwise. Suppose formula_26 is the kernel matrix for all points. The weighted kernel \"k\"-means problem with n points and k clusters is given as,\nsuch that\nsuch that formula_30. In addition, there are identity constrains on formula_23 given by,\nwhere formula_33 represents a vector of ones.\nThis problem can be recast as,\nThis problem is equivalent to the spectral clustering problem when the identity constraints on formula_23 are relaxed. In particular, the weighted kernel \"k\"-means problem can be reformulated as a spectral clustering (graph partitioning) problem and vice versa. The output of the algorithms are eigenvectors which do not satisfy the identity requirements for indicator variables defined by formula_23. Hence, post-processing of the eigenvectors is required for the equivalence between the problems.\nTransforming the spectral clustering problem into a weighted kernel \"k\"-means problem greatly reduces the computational burden.\n\nRavi Kannan, Santosh Vempala and Adrian Vetta proposed a bicriteria measure to define the quality of a given clustering. They said that a clustering was an (α, ε)-clustering if the conductance of each cluster (in the clustering) was at least α and the weight of the inter-cluster edges was at most ε fraction of the total weight of all the edges in the graph. They also look at two approximation algorithms in the same paper.\n\n"}
{"id": "25310346", "url": "https://en.wikipedia.org/wiki?curid=25310346", "title": "Standard translation", "text": "Standard translation\n\nIn modal logic, standard translation is a way of transforming formulas of modal logic into formulas of first-order logic which capture the meaning of the modal formulas. Standard translation is defined inductively on the structure of the formula. In short, atomic formulas are mapped onto unary predicates and the objects in the first-order language are the accessible worlds. The logical connectives from propositional logic remain untouched and the modal operators are transformed into first-order formulas according to their semantics.\n\nStandard translation is defined as follows:\n\n\nIn the above, formula_4 is the world from which the formula is evaluated. Initially, a free variable formula_4 is used and whenever a modal operator needs to be translated, a fresh variable is introduced to indicate that the remainder of the formula needs to be evaluated from that world. Here, the subscript formula_15 refers to the accessibility relation that should be used: normally, formula_16 and formula_17 refer to a relation formula_18 of the Kripke model but more than one accessibility relation can exist (a multimodal logic) in which case subscripts are used. For example, formula_19 and formula_20 refer to an accessibility relation formula_21 and formula_22 and formula_23 to formula_24 in the model. Alternatively, it can also be placed inside the modal symbol.\n\nAs an example, when standard translation is applied to formula_25, we expand the outer box to get\n\nmeaning that we have now moved from formula_4 to an accessible world formula_28 and we now evaluate the remainder of the formula, formula_29, in each of those accessible worlds.\n\nThe whole standard translation of this example becomes\n\nwhich precisely captures the semantics of two boxes in modal logic. The formula formula_25 holds in formula_4 when for all accessible worlds formula_28 from formula_4 and for all accessible worlds formula_35 from formula_28, the predicate formula_37 is true for formula_35. Note that the formula is also true when no such accessible worlds exist. In case formula_4 has no accessible worlds then formula_40 is false but the whole formula is vacuously true: an implication is also true when the antecedent is false.\n\nThe modal depth of a formula also becomes apparent in the translation to first-order logic. When the modal depth of a formula is \"k\", then the first-order logic formula contains a 'chain' of \"k\" transitions from the starting world formula_4. The worlds are 'chained' in the sense that these worlds are visited by going from accessible to accessible world. Informally, the number of transitions in the 'longest chain' of transitions in the first-order formula is the modal depth of the formula.\n\nThe modal depth of the formula used in the example above is two. The first-order formula indicates that the transitions from formula_4 to formula_28 and from formula_28 to formula_35 are needed to verify the validity of the formula. This is also the modal depth of the formula as each modal operator increases the modal depth by one.\n\n"}
{"id": "54104355", "url": "https://en.wikipedia.org/wiki?curid=54104355", "title": "Tarski Lectures", "text": "Tarski Lectures\n\nThe Alfred Tarski Lectures are an annual distinction in mathematical logic and series of lectures held at the University of California, Berkeley. Established in tribute to Alfred Tarski, the award has been given every year since 1989.\n\n"}
{"id": "28807941", "url": "https://en.wikipedia.org/wiki?curid=28807941", "title": "Ε-net (computational geometry)", "text": "Ε-net (computational geometry)\n\nAn \"ε\"-net (pronounced epsilon-net) in computational geometry is the approximation of a general set by a collection of simpler subsets. In probability theory it is the approximation of one probability distribution by another.\n\nLet \"X\" be a set and R be a set of subsets of \"X\"; such a pair is called a \"range space\" or hypergraph, and the elements of \"R\" are called \"ranges\" or \"hyperedges\". An ε-net of a subset \"P\" of \"X\" is a subset \"N\" of \"P\" such that any range \"r\" ∈ R with |\"r\" ∩ \"P\"| ≥ \"ε\"|\"P\"| intersects \"N\". In other words, any range that intersects at least a proportion ε of the elements of \"P\" must also intersect the \"ε\"-net \"N\".\n\nFor example, suppose \"X\" is the set of points in the two-dimensional plane, \"R\" is the set of closed filled rectangles (products of closed intervals), and \"P\" is the unit square [0, 1] × [0, 1]. Then the set N consisting of the 8 points shown in the adjacent diagram is a 1/4-net of P, because any closed filled rectangle intersecting at least 1/4 of the unit square must intersect one of these points. In fact, any (axis-parallel) square, regardless of size, will have a similar 8-point 1/4-net.\n\nFor any range space with finite VC dimension \"d\", regardless of the choice of P, there exists an ε-net of \"P\" of size\n\nbecause the size of this set is independent of \"P\", any set \"P\" can be described using a set of fixed size.\n\nThis facilitates the development of efficient approximation algorithms. For example, suppose we wish to estimate an upper bound on the area of a given region, that falls inside a particular rectangle \"P\". One can estimate this to within an additive factor of \"ε\" times the area of \"P\" by first finding an \"ε\"-net of \"P\", counting the proportion of elements in the ε-net falling inside the region with respect to the rectangle \"P\", and then multiplying by the area of \"P\". The runtime of the algorithm depends only on \"ε\" and not \"P\". One straightforward way to compute an ε-net with high probability is to take a sufficient number of random points, where the number of random points also depends only on \"ε\". For example, in the diagram shown, any rectangle in the unit square containing at most three points in the 1/4-net has an area of at most 3/8 + 1/4 = 5/8.\n\nε-nets also provide approximation algorithms for the NP-complete hitting set and set cover problems.\n\nLet formula_2 be a probability distribution over some set formula_3. An formula_4-net for a class formula_5 of subsets of formula_3 is any subset formula_7 such that\nfor any formula_8\n\nIntuitively formula_10 approximates the probability distribution.\n\nA stronger notion is formula_4-approximation. An formula_4-approximation for class formula_13 is a subset formula_7 such that for any formula_8 it holds\n"}
