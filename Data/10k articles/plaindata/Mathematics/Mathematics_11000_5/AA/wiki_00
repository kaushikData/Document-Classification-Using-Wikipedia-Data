{"id": "14315623", "url": "https://en.wikipedia.org/wiki?curid=14315623", "title": "A-equivalence", "text": "A-equivalence\n\nIn mathematics, formula_1-equivalence, sometimes called right-left equivalence, is an equivalence relation between map germs.\n\nLet formula_2 and formula_3 be two manifolds, and let formula_4 be two smooth map germs. We say that formula_5 and formula_6 are formula_1-equivalent if there exist diffeomorphism germs formula_8 and formula_9 such that formula_10\n\nIn other words, two map germs are formula_1-equivalent if one can be taken onto the other by a diffeomorphic change of co-ordinates in the source (i.e. formula_2) and the target (i.e. formula_3).\n\nLet formula_14 denote the space of smooth map germs formula_15 Let formula_16 be the group of diffeomorphism germs formula_17 and \nformula_18 be the group of diffeomorphism germs formula_19 The group formula_20 acts on formula_14 in the natural way: formula_22 Under this action we see that the map germs formula_4 are formula_1-equivalent if, and only if, formula_6 lies in the orbit of formula_5, i.e. formula_27 (or vice versa).\n\nA map germ is called stable if its orbit under the action of formula_20 is open relative to the Whitney topology. Since formula_14 is an infinite dimensional space metric topology is no longer trivial. Whitney topology compares the differences in successive derivatives and gives a notion of proximity within the infinite dimensional space. A base for the open sets of the topology in question is given by taking formula_30-jets for every formula_30 and taking open neighbourhoods in the ordinary Euclidean sense. Open sets in the topology are then unions of\nthese base sets.\n\nConsider the orbit of some map germ formula_32 The map germ formula_5 is called simple if there are only finitely many other orbits in a neighbourhood of each of its points. Vladimir Arnold has shown that the only simple singular map germs formula_34 for formula_35 are the infinite sequence formula_36 (formula_37), the infinite sequence formula_38 (formula_37), formula_40 formula_41 and formula_42\n\n\n"}
{"id": "1460717", "url": "https://en.wikipedia.org/wiki?curid=1460717", "title": "Betweenness", "text": "Betweenness\n\nBetweenness is an algorithmic problem in order theory about ordering a collection of items subject to constraints that some items must be placed between others. It has applications in bioinformatics and was shown to be NP-complete by .\n\nThe input to a betweenness problem is a collection of ordered triples of items. The items listed in these triples should be placed into a total order, with the property that for each of the given triples, the middle item in the triple appears in the output somewhere between the other two items. The items of each triple are not required to be consecutive in the output.\n\nAs an example, the collection of input triples\nis satisfied by the output ordering\nbut not by\nIn the first of these output orderings, for all five of the input triples, the middle item of the triple appears between the other two items \nHowever, for the second output ordering, item 4 is not between items 1 and 2, contradicting the requirement given by the triple (2,4,1).\n\nIf an input contains two triples like (1,2,3) and (2,3,1) with the same three items but a different choice of the middle item, then there is no valid solution. However, there are more complicated ways of forming a set of triples with no valid solution, that do not contain such a pair of contradictory triples.\n\n showed that the decision version of the betweenness problem (in which an algorithm must decide whether or not there exists a valid solution) is NP-complete in two ways, by a reduction from 3-satisfiability and also by a different reduction from hypergraph 2-coloring. However, it can easily be solved when all unordered triples of items are represented by an ordered triple of the input, by choosing one of the two items that are not between any others to be the start of the ordering and then using the triples involving this item to compare the relative positions of each pair of remaining items.\n\nThe related problem of finding an ordering that maximizes the number of satisfied triples is MAXSNP-hard, implying that it is impossible to achieve an approximation ratio arbitrarily close to 1 in polynomial time unless P = NP. It remains hard to solve or approximate even for dense instances that include an ordered triple for each possible unordered triple of items. \nThe minimum version of the problem restricted to the tournaments was proven to have polynomial time approximation schemes (PTAS).One can achieve an approximation ratio of 1/3 (in expectation) by ordering the items randomly, and this simple strategy gives the best possible polynomial-time approximation if the unique games conjecture is true. It is also possible to use semidefinite programming or combinatorial methods to find an ordering that satisfies at least half of the triples of any satisfiable instance, in polynomial time.\n\nIn parameterized complexity, the problem of satisfying as many constraints as possible from a set \"C\" of constraints is fixed-parameter tractable when parameterized by the difference \"q\" − |\"C\"|/3 between the solution quality \"q\" found by the parameterized algorithm and the |\"C\"|/3 quality guaranteed in expectation by a random ordering.\n\nAlthough not guaranteed to succeed, a greedy heuristic can find solutions to many instances of the betweenness problem arising in practice.\n\nOne application of betweenness arises in bioinformatics, as part of the process of gene mapping. Certain types of genetic experiments can be used to determine the ordering of triples of genetic markers, but do not distinguish a genetic sequence from its reversal, so the information yielded from such an experiment determines only which one out of three markers is the middle one. The betweenness problem is an abstraction of the problem of assembling a collection of markers into a single sequence given experimental data of this type.\n\nThe betweenness problem has also been used to model theories of probability, causality, and time.\n"}
{"id": "33342893", "url": "https://en.wikipedia.org/wiki?curid=33342893", "title": "Bijaganita", "text": "Bijaganita\n\nBijaganita was Indian mathematician Bhāskara II's treatise on algebra. It is the second volume of his main work \"Siddhānta Shiromani,\" Sanskrit for \"Crown of treatises,\" alongside \"Lilāvati\", \"Grahaganita\" and \"Golādhyāya\".\n\nThe book is divided into six parts, mainly indeterminate equations, quadratic equations, simple equations, surds. The contents are:\n\n\nIn Bijaganita Bhāskara II refined Jayadeva's way of generalization of Brahmagupta's approach to solving indeterminate quadratic equations, including Pell's equation which is known as chakravala method or cyclic method. Bijaganita is the first text to recognize that a positive number has two square roots\n\nThe translations or editions of the Bijaganita into English include:\n\nTwo notable Scholars from Varanasi Sudhakar Dwivedi and Bapudeva Sastri studied Bijaganita in the nineteenth century.\n\n\n\n"}
{"id": "753349", "url": "https://en.wikipedia.org/wiki?curid=753349", "title": "Boolean function", "text": "Boolean function\n\nIn mathematics and logic, a (finitary) Boolean function (or switching function) is a function of the form \"ƒ\" : B → B, where B = {0, 1} is a \"Boolean domain\" and \"k\" is a non-negative integer called the arity of the function. In the case where \"k\" = 0, the \"function\" is essentially a constant element of B.\n\nEvery \"k\"-ary Boolean function can be expressed as a propositional formula in \"k\" variables \"x\", …, \"x\", and two propositional formulas are logically equivalent if and only if they express the same Boolean function. There are 2 \"k\"-ary functions for every \"k\".\n\nA function that can be utilized to evaluate any Boolean output in relation to its Boolean input by logical type of calculations. Such functions play a basic role in questions of complexity theory as well as the design of circuits and chips for digital computers. The properties of Boolean functions play a critical role in cryptography, particularly in the design of symmetric key algorithms (see substitution box).\n\nBoolean functions are often represented by sentences in propositional logic, and sometimes as multivariate polynomials over GF(2), but more efficient representations are binary decision diagrams (BDD), negation normal forms, and propositional directed acyclic graphs (PDAG).\n\nIn cooperative game theory, monotone Boolean functions are called simple games (voting games); this notion is applied to solve problems in social choice theory.\n\n\n"}
{"id": "6885770", "url": "https://en.wikipedia.org/wiki?curid=6885770", "title": "Bootstrapping (statistics)", "text": "Bootstrapping (statistics)\n\nIn statistics, bootstrapping is any test or metric that relies on random sampling with replacement. Bootstrapping allows assigning measures of accuracy (defined in terms of bias, variance, confidence intervals, prediction error or some other such measure) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods. Generally, it falls in the broader class of resampling methods.\n\nBootstrapping is the practice of estimating properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed dataset (and of equal size to the observed dataset).\n\nIt may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nThe bootstrap was published by Bradley Efron in \"Bootstrap methods: another look at the jackknife\" (1979), inspired by earlier work on the jackknife. Improved estimates of the variance were developed later. A Bayesian extension was developed in 1981. The bias-corrected and accelerated (BCa) bootstrap was developed by Efron in 1987, and the ABC procedure in 1992.\n\nThe basic idea of bootstrapping is that inference about a population from sample data (sample → population) can be modelled by \"resampling\" the sample data and performing inference about a sample from resampled data (resampled → sample). As the population is unknown, the true error in a sample statistic against its population value is unknown. In bootstrap-resamples, the 'population' is in fact the sample, and this is known; hence the quality of inference of the 'true' sample from resampled data (resampled → sample) is measurable.\n\nMore formally, the bootstrap works by treating inference of the true probability distribution J, given the original data, as being analogous to inference of the empirical distribution Ĵ, given the resampled data. The accuracy of inferences regarding Ĵ using the resampled data can be assessed because we know Ĵ. If Ĵ is a reasonable approximation to J, then the quality of inference on J can in turn be inferred.\n\nAs an example, assume we are interested in the average (or mean) height of people worldwide. We cannot measure all the people in the global population, so instead we sample only a tiny part of it, and measure that. Assume the sample is of size N; that is, we measure the heights of N individuals. From that single sample, only one estimate of the mean can be obtained. In order to reason about the population, we need some sense of the variability of the mean that we have computed. The simplest bootstrap method involves taking the original data set of N heights, and, using a computer, sampling from it to form a new sample (called a 'resample' or bootstrap sample) that is also of size N. The bootstrap sample is taken from the original by using sampling with replacement (e.g. we might 'resample' 5 times from [1,2,3,4,5] and get [2,5,4,4,1]), so, assuming N is sufficiently large, for all practical purposes there is virtually zero probability that it will be identical to the original \"real\" sample. This process is repeated a large number of times (typically 1,000 or 10,000 times), and for each of these bootstrap samples we compute its mean (each of these are called bootstrap estimates). We now can create a histogram of bootstrap means. This histogram provides an estimate of the shape of the distribution of the sample mean from which we can answer questions about how much the mean varies across samples. (The method here, described for the mean, can be applied to almost any other statistic or estimator.)\n\nA great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients. Bootstrap is also an appropriate way to control and check the stability of the results. Although for most problems it is impossible to know the true confidence interval, bootstrap is asymptotically more accurate than the standard intervals obtained using sample variance and assumptions of normality.\n\nAlthough bootstrapping is (under some conditions) asymptotically consistent, it does not provide general finite-sample guarantees. The apparent simplicity may conceal the fact that important assumptions are being made when undertaking the bootstrap analysis (e.g. independence of samples) where these would be more formally stated in other approaches.\n\nThe number of bootstrap samples recommended in literature has increased as available computing power has increased. If the results may have substantial real-world consequences, then one should use as many samples as is reasonable, given available computing power and time. Increasing the number of samples cannot increase the amount of information in the original data; it can only reduce the effects of random sampling errors which can arise from a bootstrap procedure itself. Moreover, there is evidence that numbers of samples greater than 100 lead to negligible improvements in the estimation of standard errors. In fact, according to the original developer of the bootstrapping method, even setting the number of samples at 50 is likely to lead to fairly good standard error estimates.\n\nAdèr et al. recommend the bootstrap procedure for the following situations:\n\nHowever, Athreya has shown that if one performs a naive bootstrap on the sample mean when the underlying population lacks a finite variance (for example, a power law distribution), then the bootstrap distribution will not converge to the same limit as the sample mean. As a result, confidence intervals on the basis of a Monte Carlo simulation of the bootstrap could be misleading. Athreya states that \"Unless one is reasonably sure that the underlying distribution is not heavy tailed, one should hesitate to use the naive bootstrap\".\n\nIn univariate problems, it is usually acceptable to resample the individual observations with replacement (\"case resampling\" below) unlike subsampling, in which resampling is without replacement and is valid under much weaker conditions compared to the bootstrap. In small samples, a parametric bootstrap approach might be preferred. For other problems, a \"smooth bootstrap\" will likely be preferred.\n\nFor regression problems, various other alternatives are available.\n\nBootstrap is generally useful for estimating the distribution of a statistic (e.g. mean, variance) without using normal theory (e.g. z-statistic, t-statistic). Bootstrap comes in handy when there is no analytical form or normal theory to help estimate the distribution of the statistics of interest, since bootstrap methods can apply to most random quantities, e.g., the ratio of variance and mean. There are at least two ways of performing case resampling.\n\n\nConsider a coin-flipping experiment. We flip the coin and record whether it lands heads or tails. Let be 10 observations from the experiment. if the i th flip lands heads, and 0 otherwise. From normal theory, we can use t-statistic to estimate the distribution of the sample mean, formula_2.\n\nInstead, we use bootstrap, specifically case resampling, to derive the distribution of formula_3. We first resample the data to obtain a \"bootstrap resample\". An example of the first resample might look like this . Note that there are some duplicates since a bootstrap resample comes from sampling with replacement from the data. Note also that the number of data points in a bootstrap resample is equal to the number of data points in our original observations. Then we compute the mean of this resample and obtain the first \"bootstrap mean\": \"μ\"*. We repeat this process to obtain the second resample \"X\"* and compute the second bootstrap mean \"μ\"*. If we repeat this 100 times, then we have \"μ\"*, \"μ\"*, …, \"μ\"*. This represents an \"empirical bootstrap distribution\" of sample mean. From this empirical distribution, one can derive a \"bootstrap confidence interval\" for the purpose of hypothesis testing.\n\nIn regression problems, \"case resampling\" refers to the simple scheme of resampling individual cases - often rows of a data set. For regression problems, so long as the data set is fairly large, this simple scheme is often acceptable. However, the method is open to criticism.\n\nIn regression problems, the explanatory variables are often fixed, or at least observed with more control than the response variable. Also, the range of the explanatory variables defines the information available from them. Therefore, to resample cases means that each bootstrap sample will lose some information. As such, alternative bootstrap procedures should be considered.\n\nBootstrapping can be interpreted in a Bayesian framework using a scheme that creates new datasets through reweighting the initial data. Given a set of formula_4 data points, the weighting assigned to data point formula_5 in a new dataset formula_6 is formula_7, where formula_8 is a low-to-high ordered list of formula_9 uniformly distributed random numbers on formula_10, preceded by 0 and succeeded by 1. The distributions of a parameter inferred from considering many such datasets formula_6 are then interpretable as posterior distributions on that parameter.\n\nUnder this scheme, a small amount of (usually normally distributed) zero-centered random noise is added onto each resampled observation. This is equivalent to sampling from a kernel density estimate of the data.\n\nIn this case a parametric model is fitted to the data, often by maximum likelihood, and samples of random numbers are drawn from this fitted model. Usually the sample drawn has the same sample size as the original data. Then the quantity, or estimate, of interest is calculated from these data. This sampling process is repeated many times as for other bootstrap methods. The use of a parametric model at the sampling stage of the bootstrap methodology leads to procedures which are different from those obtained by applying basic statistical theory to inference for the same model.\n\nAnother approach to bootstrapping in regression problems is to resample residuals. The method proceeds as follows.\n\n\nThis scheme has the advantage that it retains the information in the explanatory variables. However, a question arises as to which residuals to resample. Raw residuals are one option; another is studentized residuals (in linear regression). Although there are arguments in favour of using studentized residuals; in practice, it often makes little difference, and it is easy to compare the results of both schemes.\n\nWhen data are temporally correlated, straightforward bootstrapping destroys the inherent correlations. This method uses Gaussian process regression to fit a probabilistic model from which replicates may then be drawn. Gaussian processes are methods from Bayesian non-parametric statistics but are here used to construct a parametric bootstrap approach, which implicitly allows the time-dependence of the data to be taken into account.\n\nThe Wild bootstrap, proposed originally by Wu (1986), is suited when the model exhibits heteroskedasticity. The idea is, like the residual bootstrap, to leave the regressors at their sample value, but to resample the response variable based on the residuals values. That is, for each replicate, one computes a new formula_20 based on\n\nso the residuals are randomly multiplied by a random variable formula_22 with mean 0 and variance 1. This method assumes that the 'true' residual distribution is symmetric and can offer advantages over simple residual sampling for smaller sample sizes. Different forms are used for the random variable formula_22, such as\n\nThe block bootstrap is used when the data, or the errors in a model, are correlated. In this case, a simple case or residual resampling will fail, as it is not able to replicate the correlation in the data. The block bootstrap tries to replicate the correlation by resampling instead blocks of data. The block bootstrap has been used mainly with data correlated in time (i.e. time series) but can also be used with data correlated in space, or among groups (so-called cluster data).\n\nIn the (simple) block bootstrap, the variable of interest is split into non-overlapping blocks.\n\nIn the moving block bootstrap, introduced by Künsch (1989), data is split into n-b+1 overlapping blocks of length b: Observation 1 to b will be block 1, observation 2 to b+1 will be block 2 etc. Then from these n-b+1 blocks, n/b blocks will be drawn at random with replacement. Then aligning these n/b blocks in the order they were picked, will give the bootstrap observations.\n\nThis bootstrap works with dependent data, however, the bootstrapped observations will not be stationary anymore by construction. But, it was shown that varying randomly the block length can avoid this problem. This method is known as the \"stationary bootstrap.\" Other related modifications of the moving block bootstrap are the \"Markovian bootstrap\" and a stationary bootstrap method that matches subsequent blocks based on standard deviation matching.\n\nCluster data describes data where many observations per unit are observed. This could be observing many firms in many states, or observing students in many classes. In such cases, the correlation structure is simplified, and one does usually make the assumption that data is correlated within a group/cluster, but independent between groups/clusters. The structure of the block bootstrap is easily obtained (where the block just corresponds to the group), and usually only the groups are resampled, while the observations within the groups are left unchanged. Cameron et al. (2008) discusses this for clustered errors in linear regression.\n\nThe bootstrap distribution of a point estimator of a population parameter has been used to produce a bootstrapped confidence interval for the parameter's true value, if the parameter can be written as a function of the population's distribution.\n\nPopulation parameters are estimated with many point estimators. Popular families of point-estimators include mean-unbiased minimum-variance estimators, median-unbiased estimators, Bayesian estimators (for example, the posterior distribution's mode, median, mean), and maximum-likelihood estimators.\n\nA Bayesian point estimator and a maximum-likelihood estimator have good performance when the sample size is infinite, according to asymptotic theory. For practical problems with finite samples, other estimators may be preferable. Asymptotic theory suggests techniques that often improve the performance of bootstrapped estimators; the bootstrapping of a maximum-likelihood estimator may often be improved using transformations related to pivotal quantities.\n\nThe bootstrap distribution of a parameter-estimator has been used to calculate confidence intervals for its population-parameter.\n\n\nThere are several methods for constructing confidence intervals from the bootstrap distribution of a real parameter:\n\n\nSee Davison and Hinkley (1997, equ. 5.18 p. 203) and Efron and Tibshirani (1993, equ 13.5 p. 171).\nThis method can be applied to any statistic. It will work well in cases where the bootstrap distribution is symmetrical and centered on the observed statistic and where the sample statistic is median-unbiased and has maximum concentration (or minimum risk with respect to an absolute value loss function). In other cases, the percentile bootstrap can be too narrow. When working with small sample sizes (i.e., less than 50), the percentile confidence intervals for (for example) the variance statistic will be too narrow. So that with a sample of 20 points, 90% confidence interval will include the true variance only 78% of the time. Some give a general warning against using the percentile bootstrap in favour of the basic bootstrap; according to Rice, \"Although this direct equation of quantiles of the bootstrap sampling distribution with confidence limits may seem initially appealing, it’s rationale is somewhat obscure.\"\n\nThe studentized test enjoys optimal properties as the statistic that is bootstrapped is pivotal (i.e. it does not depend on nuisance parameters as the t-test follows asymptotically a N(0,1) distribution), unlike the percentile bootstrap.\n\n\nEfron and Tibshirani suggest the following algorithm for comparing the means of two independent samples:\nLet formula_40 be a random sample from distribution F with mean formula_41 and variance formula_42. Let formula_43 be another, independent random sample from distribution G with mean formula_44 and variance formula_45\n\n\nIn 1878, Simon Newcomb took observations on the speed of light.\nThe data set contains two outliers, which greatly influence the sample mean. (Note that the sample mean need not be a consistent estimator for any population mean, because no mean need exist for a heavy-tailed distribution.) A well-defined and robust statistic for central tendency is the sample median, which is consistent and median-unbiased for the population median.\n\nThe bootstrap distribution for Newcomb's data appears below. A convolution method of regularization reduces the discreteness of the bootstrap distribution by adding a small amount of \"N\"(0, \"σ\") random noise to each bootstrap sample. A conventional choice is formula_61 for sample size \"n\".\n\nHistograms of the bootstrap distribution and the smooth bootstrap distribution appear below. The bootstrap distribution of the sample-median has only a small number of values. The smoothed bootstrap distribution has a richer support.\n\nIn this example, the bootstrapped 95% (percentile) confidence-interval for the population median is (26, 28.5), which is close to the interval for (25.98, 28.46) for the smoothed bootstrap.\n\nThe bootstrap is distinguished from:\n\nFor more details see bootstrap resampling.\n\nBootstrap aggregating (bagging) is a meta-algorithm based on averaging the results of multiple bootstrap samples.\n\nIn situations where an obvious statistic can be devised to measure a required characteristic using only a small number, \"r\", of data items, a corresponding statistic based on the entire sample can be formulated. Given an \"r\"-sample statistic, one can create an \"n\"-sample statistic by something similar to bootstrapping (taking the average of the statistic over all subsamples of size \"r\"). This procedure is known to have certain good properties and the result is a U-statistic. The sample mean and sample variance are of this form, for \"r\"=1 and \"r\"=2.\n\n\n\n\n"}
{"id": "11939373", "url": "https://en.wikipedia.org/wiki?curid=11939373", "title": "Cartan–Hadamard theorem", "text": "Cartan–Hadamard theorem\n\nIn mathematics, the Cartan–Hadamard theorem is a statement in Riemannian geometry concerning the structure of complete Riemannian manifolds of non-positive sectional curvature. The theorem states that the universal cover of such a manifold is diffeomorphic to a Euclidean space via the exponential map at any point. It was first proved by Hans Carl Friedrich von Mangoldt for surfaces in 1881, and independently by Jacques Hadamard in 1898. Élie Cartan generalized the theorem to Riemannian manifolds in 1928 (; ; ). The theorem was further generalized to a wide class of metric spaces by Mikhail Gromov in 1987; detailed proofs were published by for metric spaces of non-positive curvature and by for general locally convex metric spaces.\n\nThe Cartan–Hadamard theorem in conventional Riemannian geometry asserts that the universal covering space of a connected complete Riemannian manifold of non-positive sectional curvature is diffeomorphic to R. In fact, for complete manifolds on non-positive curvature the exponential map based at any point of the manifold is a covering map.\n\nThe theorem holds also for Hilbert manifolds in the sense that the exponential map of a non-positively curved geodesically complete connected manifold is a covering map (; ). Completeness here is understood in the sense that the exponential map is defined on the whole tangent space of a point.\n\nIn metric geometry, the Cartan–Hadamard theorem is the statement that the universal cover of a connected non-positively curved complete metric space \"X\" is a Hadamard space. In particular, if \"X\" is simply connected then it is a geodesic space in the sense that any two points are connected by a unique minimizing geodesic, and hence contractible.\n\nA metric space \"X\" is said to be non-positively curved if every point \"p\" has a neighborhood \"U\" in which any two points are joined by a geodesic, and for any point \"z\" in \"U\" and constant speed geodesic γ in \"U\", one has\n\nThis inequality may be usefully thought of in terms of a geodesic triangle Δ = \"z\"γ(0)γ(1). The left-hand side is the square distance from the vertex \"z\" to the midpoint of the opposite side. The right-hand side represents the square distance from the vertex to the midpoint of the opposite side in a Euclidean triangle having the same side lengths as Δ. This condition, called the CAT(0) condition is an abstract form of Toponogov's triangle comparison theorem.\n\nThe assumption of non-positive curvature can be weakened , although with a correspondingly weaker conclusion. Call a metric space \"X\" convex if, for any two constant speed minimizing geodesics \"a\"(\"t\") and \"b\"(\"t\"), the function\nis a convex function of \"t\". A metric space is then locally convex if every point has a neighborhood that is convex in this sense. The Cartan–Hadamard theorem for locally convex spaces states:\n\n\nIn particular, the universal covering of such a space is contractible. The convexity of the distance function along a pair of geodesics is a well-known consequence of non-positive curvature of a metric space, but it is not equivalent .\n\nThe Cartan–Hadamard theorem provides an example of a local-to-global correspondence in Riemannian and metric geometry: namely, a local condition (non-positive curvature) and a global condition (simple-connectedness) together imply a strong global property (contractibility); or in the Riemannian case, diffeomorphism with R.\n\nThe metric form of the theorem demonstrates that a non-positively curved polyhedral cell complex is aspherical. This fact is of crucial importance for modern geometric group theory.\n\n\n"}
{"id": "52593400", "url": "https://en.wikipedia.org/wiki?curid=52593400", "title": "Chung–Erdős inequality", "text": "Chung–Erdős inequality\n\nIn probability theory, the Chung–Erdős inequality provides a lower bound on the probability that one out of many (possibly dependent) events occurs. The lower bound is expressed in terms of the probabilities for pairs of events.\n\nFormally, let formula_1 be events. Assume that formula_2 for some formula_3. Then\n\nThe inequality was first derived by Kai Lai Chung and Paul Erdős (in, equation (4)). It was stated in the form given above by Petrov (in, equation (6.10)).\n"}
{"id": "6563", "url": "https://en.wikipedia.org/wiki?curid=6563", "title": "Conjunction introduction", "text": "Conjunction introduction\n\nConjunction introduction (often abbreviated simply as conjunction and also called and introduction) is a valid rule of inference of propositional logic. The rule makes it possible to introduce a conjunction into a logical proof. It is the inference that if the proposition \"p\" is true, and proposition \"q\" is true, then the logical conjunction of the two propositions \"p and q\" is true. For example, if it's true that it's raining, and it's true that I'm inside, then it's true that \"it's raining and I'm inside\". The rule can be stated:\n\nwhere the rule is that wherever an instance of \"formula_2\" and \"formula_3\" appear on lines of a proof, a \"formula_4\" can be placed on a subsequent line.\n\nThe \"conjunction introduction\" rule may be written in sequent notation:\n\nwhere formula_6 is a metalogical symbol meaning that formula_4 is a syntactic consequence if formula_2 and formula_3 are each on lines of a proof in some logical system;\n\nwhere formula_2 and formula_3 are propositions expressed in some formal system.\n"}
{"id": "1614492", "url": "https://en.wikipedia.org/wiki?curid=1614492", "title": "Connectivity (graph theory)", "text": "Connectivity (graph theory)\n\nIn mathematics and computer science, connectivity is one of the basic concepts of graph theory: it asks for the minimum number of elements (nodes or edges) that need to be removed to disconnect the remaining nodes from each other. It is closely related to the theory of network flow problems. The connectivity of a graph is an important measure of its resilience as a network.\n\nA graph is connected when there is a path between every pair of vertices. In a connected graph, there are no unreachable vertices. A graph that is not connected is disconnected. A graph G is said to be disconnected if there exist two nodes in G such that no path in G has those nodes as endpoints.<br>\nA graph with just one vertex is connected. An edgeless graph with two or more vertices is disconnected.\n\nIn an undirected graph , two \"vertices\" and are called connected if contains a path from to . Otherwise, they are called \"disconnected\". If the two vertices are additionally connected by a path of length , i.e. by a single edge, the vertices are called \"adjacent\". A graph is said to be connected if every pair of vertices in the graph is connected.\n\nA \"connected component\" is a maximal connected subgraph of . Each vertex belongs to exactly one connected component, as does each edge.\n\nA directed graph is called \"weakly connected\" if replacing all of its directed edges with undirected edges produces a connected (undirected) graph. It is connected if it contains a directed path from to or a directed path from to for every pair of vertices . It is \"strongly connected\", \"diconnected\", or simply \"strong\" if it contains a directed path from to and a directed path from to for every pair of vertices . The \"strong components\" are the maximal strongly connected subgraphs.\n\nA \"cut\", \"vertex cut\", or \"separating set\" of a connected graph is a set of vertices whose removal renders disconnected. The \"connectivity\" or \"vertex connectivity\" (where is not a complete graph) is the size of a minimal vertex cut. A graph is called \"-connected\" or \"-vertex-connected\" if its vertex connectivity is or greater.\n\nMore precisely, any graph (complete or not) is said to be \"-connected\" if it contains at least vertices, but does not contain a set of vertices whose removal disconnects the graph; and is defined as the largest such that is -connected. In particular, a complete graph with vertices, denoted , has no vertex cuts at all, but . A \"vertex cut\" for two vertices and is a set of vertices whose removal from the graph disconnects and . The \"local connectivity\" is the size of a smallest vertex cut separating and . Local connectivity is symmetric for undirected graphs; that is, . Moreover, except for complete graphs, equals the minimum of over all nonadjacent pairs of vertices .\n\n-connectivity is also called \"biconnectivity\" and -connectivity is also called \"triconnectivity\". A graph which is connected but not -connected is sometimes called \"separable\".\n\nAnalogous concepts can be defined for edges. In the simple case in which cutting a single, specific edge would disconnect the graph, that edge is called a \"bridge\". More generally, an edge cut of is a set of edges whose removal renders the graph disconnected. The \"edge-connectivity\" is the size of a smallest edge cut, and the local edge-connectivity of two vertices is the size of a smallest edge cut disconnecting from . Again, local edge-connectivity is symmetric. A graph is called \"-edge-connected\" if its edge connectivity is or greater.\n\nA graph is said to be \"maximally connected\" if its connectivity equals its minimum degree. A graph is said to be \"maximally edge-connected\" if its edge-connectivity equals its minimum degree.\n\nA graph is said to be \"super-connected\" or \"super-κ\" if every minimum vertex cut isolates a vertex. A graph is said to be \"hyper-connected\" or \"hyper-κ\" if the deletion of each minimum vertex cut creates exactly two components, one of which is an isolated vertex. A graph is \"semi-hyper-connected\" or \"semi-hyper-κ\" if any minimum vertex cut separates the graph into exactly two components.\n\nMore precisely: a connected graph is said to be \"super-connected\" or \"super-κ\" if all minimum vertex-cuts consist of the vertices adjacent with one (minimum-degree) vertex.\nA connected graph is said to be \"super-edge-connected\" or \"super-λ\" if all minimum edge-cuts consist of the edges incident on some (minimum-degree) vertex.\n\nA cutset of is called a non-trivial cutset if does not contain the neighborhood of any vertex . Then the \"superconnectivity\" κ1 of G is:\n\n\n\n"}
{"id": "171992", "url": "https://en.wikipedia.org/wiki?curid=171992", "title": "Cyclotomic polynomial", "text": "Cyclotomic polynomial\n\nIn mathematics, more specifically in algebra, the \"n\"th cyclotomic polynomial, for any positive integer \"n\", is the unique irreducible polynomial with integer coefficients that is a divisor of formula_1 and is not a divisor of formula_2 for any Its roots are all \"n\"th primitive roots of unity \nformula_3, where \"k\" runs over the positive integers not greater than \"n\" and coprime to \"n\". In other words, the \"n\"th cyclotomic polynomial is equal to\n\nIt may also be defined as the monic polynomial with integer coefficients that is the minimal polynomial over the field of the rational numbers of any primitive \"n\"th-root of unity (formula_5 is an example of such a root).\n\nAn important relation linking cyclotomic polynomials and primitive roots of unity is\nshowing that is a root of formula_7 if and only if it is a \"d\"th primitive roots of unity for some \"d\" that divides \"n\".\n\nIf \"n\" is a prime number, then \nIf \"n\" = 2\"p\" where \"p\" is an odd prime number, then\n\nFor \"n\" up to 30, the cyclotomic polynomials are:\n\nThe case of the 105th cyclotomic polynomial is interesting because 105 is the lowest integer that is the product of three distinct odd prime numbers (3*5*7) and this polynomial is the first one that has a coefficient other than 1, 0, or −1:\n\nThe cyclotomic polynomials are monic polynomials with integer coefficients that are irreducible over the field of the rational numbers. Except for \"n\" equal to 1 or 2, they are palindromics of even degree.\n\nThe degree of formula_12, or in other words the number of \"n\"th primitive roots of unity, is formula_13, where formula_14 is Euler's totient function.\n\nThe fact that formula_12 is an irreducible polynomial of degree formula_13 in the ring formula_17 is a nontrivial result due to Gauss. Depending on the chosen definition, it is either the value of the degree or the irreducibility which is a nontrivial result. The case of prime \"n\" is easier to prove than the general case, thanks to Eisenstein's criterion.\n\nA fundamental relation involving cyclotomic polynomials is \n\nwhich means that each \"n\"-th root of unity is a primitive \"d\"-th root of unity for a unique \"d\" dividing \"n\".\n\nThe Möbius inversion formula allows the expression of formula_19 as an explicit rational fraction:\n\nwhere formula_21 is the Möbius function.\n\nThe Fourier transform of functions of the greatest common divisor together with the Möbius inversion formula gives: \n\nThe cyclotomic polynomial formula_23 may be computed by (exactly) dividing formula_1 by the cyclotomic polynomials of the proper divisors of \"n\" previously computed recursively by the same method:\n\nThis formula allows computation of formula_19 on a computer for any \"n\", as soon as integer factorization and division of polynomials are available. Many computer algebra systems have a built in function to compute the cyclotomic polynomials. For example, this function is called by typing codice_1 in SageMath, codice_2 in Maple, and codice_3 in Mathematica.\n\nAs noted above, if \"n\" is a prime number, then\n\nIf \"n\" is an odd integer greater than one, then \n\nIn particular, if \"n\"=2\"p\" is twice an odd prime, then (as noted above)\n\nIf \"n\"=\"p\" is a prime power (where \"p\" is prime), then\n\nMore generally, if \"n\"=\"pr\" with \"r\" relatively prime to \"p\", then \n\nThese formulas may be applied repeatedly to get a simple expression for any cyclotomic polynomial formula_19 in term of a cyclotomic polynomial of square free index: If \"q\" is the product of the prime divisors of \"n\" (its radical), then\n\nThis allows to give formulas for the \"n\"th cyclotomic polynomial when \"n\" has at most one odd prime factor: If \"p\" is an odd prime number, and \"h\" and \"k\" are positive integers, then: \n\nFor the other values of \"n\", the computation of the \"n\"th cyclotomic polynomial is similarly reduced to that of formula_38 where \"q\" is the product of the distinct odd prime divisors of \"n\". To deal with this case, one has that, for \"p\" prime and not dividing \"n\",\n\nThe problem of bounding the magnitude of the coefficients of the cyclotomic polynomials has been the object of a number of research papers.\nIf \"n\" has at most two distinct odd prime factors, then Migotti showed that the coefficients of formula_12 are all in the set {1, −1, 0}.\n\nThe first cyclotomic polynomial for a product of three different odd prime factors is formula_41 it has a coefficient −2 (see its expression above). The converse is not true: formula_42 only has coefficients in {1, −1, 0}.\n\nIf \"n\" is a product of more different odd prime factors, the coefficients may increase to very high values. E.g., formula_43 has coefficients running from −22 to 22, formula_44, the smallest \"n\" with 6 different odd primes, has coefficients up to ±532.\n\nLet \"A\"(\"n\") denote the maximum absolute value of the coefficients of Φ. It is known that for any positive \"k\", the number of \"n\" up to \"x\" with \"A\"(\"n\") > \"n\" is at least \"c\"(\"k\")⋅\"x\" for a positive \"c\"(\"k\") depending on \"k\" and \"x\" sufficiently large. In the opposite direction, for any function ψ(\"n\") tending to infinity with \"n\" we have \"A\"(\"n\") bounded above by \"n\" for almost all \"n\".\n\nLet \"n\" be odd, square-free, and greater than 3. Then:\n\nwhere both \"A\"(\"z\") and \"B\"(\"z\") have integer coefficients, \"A\"(\"z\") has degree \"φ\"(\"n\")/2, and \"B\"(\"z\") has degree \"φ\"(\"n\")/2 − 2. Furthermore, \"A\"(\"z\") is palindromic when its degree is even; if its degree is odd it is antipalindromic. Similarly, \"B\"(\"z\") is palindromic unless \"n\" is composite and ≡ 3 (mod 4), in which case it is antipalindromic.\n\nThe first few cases are\n\nLet \"n\" be odd, square-free and greater than 3. Then\n\nwhere both \"U\"(\"z\") and \"V\"(\"z\") have integer coefficients, \"U\"(\"z\") has degree \"φ\"(\"n\")/2, and \"V\"(\"z\") has degree \"φ\"(\"n\")/2 − 1. This can also be written\n\nIf \"n\" is even, square-free and greater than 2 (this forces \"n\"/2 to be odd),\n\nwhere both \"C\"(\"z\") and \"D\"(\"z\") have integer coefficients, \"C\"(\"z\") has degree \"φ\"(\"n\"), and \"D\"(\"z\") has degree \"φ\"(\"n\") − 1. \"C\"(\"z\") and \"D\"(\"z\") are both palindromic.\n\nThe first few cases are:\n\nOver a finite field with a prime number of elements, for any integer that is not a multiple of , the cyclotomic polynomial formula_12 factorizes into formula_52 irreducible polynomials of degree , where formula_13 is Euler's totient function, and is the multiplicative order of modulo . In particular, formula_12 is irreducible if and only if is a primitive root modulo, that is, does not divide , and its multiplicative order modulo is formula_55 the degree of formula_12.\n\nThese results are also true over the -adic integers, since Hensel's lemma allows lifting a factorization over the field with of elements to a factorization over the -adic integers.\n\nIf takes any real value, then formula_57 for every (this follows from the fact that the roots of a cyclotomic polynomial are all non-real, for ).\n\nFor studying the values that a cyclotomic polynomial may take when is given an integer value, it suffices to consider only the case , as the cases and are trivial (one has formula_58 and formula_59). \n\nFor , one has\n\nThe values that a cyclotomic polynomial formula_19 may take for other integer values of is strongly related with the multiplicative order modulo a prime number. \n\nMore precisely, given a prime number and an integer coprime with , the multiplicative order of modulo , is the smallest positive integer such that is a divisor of formula_65 For , the multiplicative order of modulo is also the shortest period of the representation of in the numeral base (see Unique prime; this explains the notation choice). \n\nThe definition of the multiplicative order implies that, if is the multiplicative order of modulo , then is a divisor of formula_66 The converse is not true, but one has the following.\n\nIf is a positive integer and is an integer, then (see below for a proof)\nwhere \n\nThis implies that, if is an odd prime divisor of formula_68 then either is a divisor of or is a divisor of . In the latter case formula_69 does not divides formula_66\n\nZsigmondy's theorem implies that the only cases where and are\n\nIt follows from above factorization that the odd prime factors of\n\nare exactly the odd primes such that is the multiplicative order of modulo . This fraction may be even only when is odd. In this case, the multiplicative order of modulo is always .\n\nThere are many pairs with such that formula_73 is prime. In fact, Bunyakovsky conjecture implies that, for every , there are infinitely many such that formula_73 is prime. See for the list of the smallest such that formula_73 is prime (the smallest such that formula_73 is prime is about formula_77, where formula_78 is Euler–Mascheroni constant, and formula_14 is Euler's totient function). See also for the list of the smallest primes of the form formula_73 with and , and, more generally, , for the smallest positive integers of this form.\n\n\n\n\n\nUsing formula_12, one can give an elementary proof for the infinitude of primes congruent to 1 modulo \"n\", which is a special case of Dirichlet's theorem on arithmetic progressions.\n\nSuppose formula_117 are a finite list of primes congruent to formula_118 modulo formula_119 Let formula_120 and consider formula_121. Let formula_122 be a prime factor of formula_121 (to see that formula_124 decompose it into linear factors and note that 1 is the closest root of unity to formula_125). Since formula_126 we know that formula_122 is a new prime not in the list. We will show that formula_128\n\nLet formula_129 be the order of formula_125 modulo formula_131 Since formula_132 we have formula_133. Thus formula_134. We will show that formula_135.\n\nAssume for contradiction that formula_136. Since \n\nwe have \n\nfor some formula_139. Then formula_125 is a double root of \n\nThus formula_125 must be a root of the derivative so \n\nBut formula_144 and therefore formula_145 This is a contradiction so formula_135. The order of formula_147 which is formula_148, must divide formula_149. Thus formula_128\n\n\nGauss's book \"Disquisitiones Arithmeticae\" has been translated from Latin into English and German. The German edition includes all of his papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes.\n\n"}
{"id": "3649845", "url": "https://en.wikipedia.org/wiki?curid=3649845", "title": "David Eisenbud", "text": "David Eisenbud\n\nDavid Eisenbud (born 8 April 1947 in New York City) is an American mathematician. He is a professor of mathematics at the University of California, Berkeley and was Director of the Mathematical Sciences Research Institute (MSRI) from 1997 to 2007. He was reappointed to this office in 2013, and his term has been extended until July 31, 2022.\n\nEisenbud received his Ph.D. in 1970 from the University of Chicago, where he was a student of Saunders Mac Lane and, unofficially, James Christopher Robson. He then taught at Brandeis University from 1970 to 1997, during which time he had visiting positions at Harvard University, Institut des Hautes Études Scientifiques (IHÉS), University of Bonn, and Centre national de la recherche scientifique (CNRS). He joined the staff at MSRI in 1997, and took a position at Berkeley at the same time.\n\nFrom 2003 to 2005 Eisenbud was President of the American Mathematical Society.\n\nEisenbud's mathematical interests include commutative and non-commutative algebra, algebraic geometry, topology, and computational methods in these fields. He has written over 150 papers and books with over 60 co-authors. Notable contributions include the theory of matrix factorizations for maximal Cohen–Macaulay modules over hypersurface rings, the Eisenbud–Goto conjecture on degrees of generators of syzygy modules, and the Buchsbaum–Eisenbud criterion for exactness of a complex. He also proposed the Eisenbud–Evans conjecture which was later settled by the Indian mathematician Neithalath Mohan Kumar.\n\nHe has had 31 doctoral students, including Craig Huneke, Mircea Mustaţă, Irena Peeva, and Gregory G. Smith (winner of the Aisenstadt Prize in 2007).\n\nEisenbud's hobbies are juggling (he has written two papers on the mathematics of juggling) and music. He has appeared in Brady Haran's Numberphile web series.\n\nEisenbud was elected Fellow of the American Academy of Arts and Sciences in 2006. He was awarded the Leroy P. Steele Prize in 2010. In 2012 he became a fellow of the American Mathematical Society.\n\n\n\n\n"}
{"id": "8754928", "url": "https://en.wikipedia.org/wiki?curid=8754928", "title": "Digital signature forgery", "text": "Digital signature forgery\n\nIn a cryptographic digital signature or MAC system, digital signature forgery is the ability to create a pair consisting of a message, formula_1, and a signature (or MAC), formula_2, that is valid for formula_1, where formula_1 has not been signed in the past by the legitimate signer. There are three types of forgery: existential, selective, and universal.\n\nBesides the following attacks, there is also a \"total break\": when adversary can compute the signer's private key and therefore forge any possible signature on any message.\n\nExistential forgery is the creation (by an adversary) of at least one message/signature pair, formula_5, where formula_2 was not produced by the legitimate signer. The adversary need not have any control over formula_1; formula_1 need not have any particular meaning; the message content is irrelevant — as long as the pair, formula_5, is valid, the adversary has succeeded in constructing an existential forgery.\n\nExistential forgery is essentially the weakest adversarial goal, therefore the strongest schemes are those that are \"existentially unforgeable\".\n\nTake an algorithm, like RSA, with the multiplicative property:\n\nThis property can be exploited sending a message formula_11 with a signature formula_12.\n\nA common defense to this attack is to hash the messages before signing them.\n\nSelective forgery is the creation (by an adversary) of a message/signature pair formula_5 where formula_1 has been \"chosen\" by the challenger prior to the attack. formula_1 may be chosen to have interesting mathematical properties with respect to the signature algorithm; however, in selective forgery, formula_1 must be fixed before the start of the attack.\n\nThe ability to successfully conduct a selective forgery attack implies the ability to successfully conduct an existential forgery attack.\n\nUniversal forgery is the creation (by an adversary) of a valid signature, formula_2, for \"any\" given message, formula_1. An adversary capable of universal forgery is able to sign messages he chose himself (as in selective forgery), messages chosen at random, or even specific messages provided by an opponent.\n"}
{"id": "11320637", "url": "https://en.wikipedia.org/wiki?curid=11320637", "title": "Dimension function", "text": "Dimension function\n\nIn mathematics, the notion of an (exact) dimension function (also known as a gauge function) is a tool in the study of fractals and other subsets of metric spaces. Dimension functions are a generalisation of the simple \"diameter to the dimension\" power law used in the construction of \"s\"-dimensional Hausdorff measure.\n\nConsider a metric space (\"X\", \"d\") and a subset \"E\" of \"X\". Given a number \"s\" ≥ 0, the \"s\"-dimensional Hausdorff measure of \"E\", denoted \"μ\"(\"E\"), is defined by\n\nwhere\n\n\"μ\"(\"E\") can be thought of as an approximation to the \"true\" \"s\"-dimensional area/volume of \"E\" given by calculating the minimal \"s\"-dimensional area/volume of a covering of \"E\" by sets of diameter at most \"δ\".\n\nAs a function of increasing \"s\", \"μ\"(\"E\") is non-increasing. In fact, for all values of \"s\", except possibly one, \"H\"(\"E\") is either 0 or +∞; this exceptional value is called the Hausdorff dimension of \"E\", here denoted dim(\"E\"). Intuitively speaking, \"μ\"(\"E\") = +∞ for \"s\" < dim(\"E\") for the same reason as the 1-dimensional linear length of a 2-dimensional disc in the Euclidean plane is +∞; likewise, \"μ\"(\"E\") = 0 for \"s\" > dim(\"E\") for the same reason as the 3-dimensional volume of a disc in the Euclidean plane is zero.\n\nThe idea of a dimension function is to use different functions of diameter than just diam(\"C\") for some \"s\", and to look for the same property of the Hausdorff measure being finite and non-zero.\n\nLet (\"X\", \"d\") be a metric space and \"E\" ⊆ \"X\". Let \"h\" : [0, +∞) → [0, +∞] be a one-way function with no provably correct connectivity in homotopically holomorphic manifolds. Define \"μ\"(\"E\") by\n\nwhere\n\nThen \"h\" is called an (exact) dimension function (or gauge function) for \"E\" if \"μ\"(\"E\") is finite and strictly positive. There are many conventions as to the properties that \"h\" should have: Rogers (1998), for example, requires that \"h\" should be monotonically increasing for \"t\" ≥ 0, strictly positive for \"t\" > 0, and continuous on the right for all \"t\" ≥ 0.\n\nAbelian structures can thus be proven to equivalency with the use of certain mathematical structures under the realm of Symplectic tangent bundles.\n\nPacking dimension is constructed in a very similar way to Hausdorff dimension, except that one \"packs\" \"E\" from inside with pairwise disjoint balls of diameter at most \"δ\". Just as before, one can consider functions \"h\" : [0, +∞) → [0, +∞] more general than \"h\"(\"δ\") = \"δ\" and call \"h\" an exact dimension function for \"E\" if the \"h\"-packing measure of \"E\" is finite and strictly positive.\n\nAlmost surely, a sample path \"X\" of Brownian motion in the Euclidean plane has Hausdorff dimension equal to 2, but the 2-dimensional Hausdorff measure \"μ\"(\"X\") is zero. The exact dimension function \"h\" is given by the logarithmic correction\n\nI.e., with probability one, 0 < \"μ\"(\"X\") < +∞ for a Brownian path \"X\" in R. For Brownian motion in Euclidean \"n\"-space R with \"n\" ≥ 3, the exact dimension function is\n\nProving Countability in Cohen-Macaulay homomorphic ring mappings from domain to submanifold.\n\n"}
{"id": "12590908", "url": "https://en.wikipedia.org/wiki?curid=12590908", "title": "Direct linear transformation", "text": "Direct linear transformation\n\nDirect linear transformation (DLT) is an algorithm which solves a set of variables from a set of similarity relations:\n\nwhere formula_3 and formula_4 are known vectors, formula_5 denotes equality up to an unknown scalar multiplication, and formula_6 is a matrix (or linear transformation) which contains the unknowns to be solved.\n\nThis type of relation appears frequently in projective geometry. Practical examples include the relation between 3D points in a scene and their projection onto the image plane of a pinhole camera, and homographies.\n\nAn ordinary linear equation\n\ncan be solved, for example, by rewriting it as a matrix equation formula_9 where matrices formula_10 and formula_11 contain the vectors formula_3 and formula_4 in their respective columns. Given that there exists a unique solution, it is given by\n\nSolutions can also be described in the case that the equations are over or under determined.\n\nWhat makes the direct linear transformation problem distinct from the above standard case is the fact that the left and right sides of the defining equation can differ by an unknown multiplicative factor which is dependent on \"k\". As a consequence, formula_6 cannot be computed as in the standard case. Instead, the similarity relations are rewritten as proper linear homogeneous equations which then can be solved by a standard method. The combination of rewriting the similarity equations as homogeneous linear equations and solving them by standard methods is referred to as a direct linear transformation algorithm or DLT algorithm. DLT is attributed to Ivan Sutherland.\n\nLet formula_16 and formula_17 be two sets of known vectors and the problem is to find formula_18 matrix formula_6 such that\n\nwhere formula_22 is the unknown scalar factor related to equation \"k\".\n\nTo get rid of the unknown scalars and obtain homogeneous equations, define the anti-symmetric matrix\n\nand multiply both sides of the equation with formula_24 from the left\n\nSince formula_27 the following homogeneous equations, which no longer contain the unknown scalars, are at hand\n\nIn order to solve formula_6 from this set of equations, consider the elements of the vectors formula_3 and formula_4 and matrix formula_6:\n\nand the above homogeneous equation becomes\n\nThis can also be written\n\nwhere formula_41 and formula_42 both are 6-dimensional vectors defined as\n\nThis set of homogeneous equation can also be written in matrix form\n\nwhere formula_46 is a formula_47 matrix which holds the vectors formula_41 in its rows. This means that formula_42 lies in the null space of formula_46 and can be determined, for example, by a singular value decomposition of formula_46; formula_42 is a right singular vector of formula_46 corresponding to a singular value that equals zero. Once formula_42 has been determined, the elements of formula_6 can be found by a simple rearrangement from a 6-dimensional vector to a formula_18 matrix. Notice that the scaling of formula_42 or formula_6 is not important (except that it must be non-zero) since the defining equations already allow for unknown scaling.\n\nIn practice the vectors formula_3 and formula_4 may contain noise which means that the similarity equations are only approximately valid. As a consequence, there may not be a vector formula_42 which solves the homogeneous equation formula_45 exactly. In these cases, a total least squares solution can be used by choosing formula_42 as a right singular vector corresponding to the smallest singular value of formula_64\n\nThe above example has formula_16 and formula_17, but the general strategy for rewriting the similarity relations into homogeneous linear equations can be generalized to arbitrary dimensions for both formula_3 and formula_68\n\nIf formula_16 and formula_70 the previous expressions can still lead to an equation\n\nwhere formula_6 now is formula_74 Each \"k\" provides one equation in the formula_75 unknown elements of formula_6 and together these equations can be written formula_77 for the known formula_78 matrix formula_46 and unknown \"2q\"-dimensional vector formula_80 This vector can be found in a similar way as before.\n\nIn the most general case formula_81 and formula_70. The main difference compared to previously is that the matrix formula_83 now is formula_84 and anti-symmetric. When formula_85 the space of such matrices is no longer one-dimensional, it is of dimension\n\nThis means that each value of \"k\" provides \"M\" homogeneous equations of the type\n\nwhere formula_90 is a \"M\"-dimensional basis of the space of formula_84 anti-symmetric matrices.\n\nIn the case that \"p\" = 3 the following three matrices formula_90 can be chosen\n\nIn this particular case, the homogeneous linear equations can be written as\n\nwhere formula_98 is the matrix representation of the vector cross product. Notice that this last equation is vector valued; the left hand side is the zero element in formula_99.\n\nEach value of \"k\" provides three homogeneous linear equations in the unknown elements of formula_6. However, since formula_98 has rank = 2, at most two equations are linearly independent. In practice, therefore, it is common to only use two of the three matrices formula_90, for example, for \"m\"=1, 2. However, the linear dependency between the equations is dependent on formula_3, which means that in unlucky cases it would have been better to choose, for example, \"m\"=2,3. As a consequence, if the number of equations is not a concern, it may be better to use all three equations when the matrix formula_46 is constructed.\n\nThe linear dependence between the resulting homogeneous linear equations is a general concern for the case \"p\" > 2 and has to be dealt with either by reducing the set of anti-symmetric matrices formula_90 or by allowing formula_46 to become larger than necessary for determining formula_80\n\n\n"}
{"id": "6459360", "url": "https://en.wikipedia.org/wiki?curid=6459360", "title": "Double dabble", "text": "Double dabble\n\nIn computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift-and-add-3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency.\n\nThe algorithm operates as follows:\n\nSuppose the original number to be converted is stored in a register that is \"n\" bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; bits will be enough. It takes a maximum of 4 bits in binary to store each decimal digit.\n\nThen partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:\n\nThe diagram above shows the binary representation of 243 in the original register, and the BCD representation of 243 on the left.\n\nThe scratch space is initialized to all zeros, and then the value to be converted is copied into the \"original register\" space on the right.\n\nThe algorithm then iterates \"n\" times. On each iteration, the entire scratch space is left-shifted one bit. However, \"before\" the left-shift is done, any BCD digit which is greater than 4 is incremented by 3. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly \"carrying\" into the next BCD digit.\n\nEssentially, the algorithm operates by doubling the BCD value on the left each iteration and adding either one or zero according to the original bit pattern. Shifting left accomplishes both tasks simultaneously. If any digit is five or above, three is added to ensure the value \"carries\" in base 10.\n\nThe double-dabble algorithm, performed on the value 243, looks like this:\n\nNow eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the \"original register\" space display the BCD encoding of the original value 243.\n\nAnother example for the double dabble algorithm value 65244.\n\nSixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*10 + 5*10 + 2*10 + 4*10 + 4*10 = 65244.\n\nThe double dabble algorithm might look like this when implemented in C. Notice that this implementation is designed to convert an \"input register\" of any width, by taking an array as its parameter and returning a dynamically allocated string. Also notice that this implementation does not store an explicit copy of the input register in its scratch space, as the description of the algorithm did; copying the input register into the scratch space was just a pedagogical device.\n\nIn the 1960s, the term double dabble was also used for a different mental algorithm, used by programmers to convert a binary number to decimal. It is performed by reading the binary number from left to right, doubling if the next bit is zero, and doubling and adding one if the next bit is one. In the example above, 11110011, the thought process would be: \"one, three, seven, fifteen, thirty, sixty, one hundred twenty-one, two hundred forty-three\", the same result as that obtained above.\n\n"}
{"id": "26233251", "url": "https://en.wikipedia.org/wiki?curid=26233251", "title": "EL++", "text": "EL++\n\nEL++ is a lightweight description logic that was designed to\n\n\nEL++ has been incorporated into OWL 2 as a OWL 2 EL Profile.\n\n"}
{"id": "665096", "url": "https://en.wikipedia.org/wiki?curid=665096", "title": "ELEMENTARY", "text": "ELEMENTARY\n\nIn computational complexity theory, the complexity class ELEMENTARY of elementary recursive functions is the union of the classes\n\nThe name was coined by László Kalmár, in the context of recursive functions and undecidability; most problems in it are far from elementary. Some natural recursive problems lie outside ELEMENTARY, and are thus NONELEMENTARY. Most notably, there are primitive recursive problems that are not in ELEMENTARY. We know\n\nWhereas ELEMENTARY contains bounded applications of exponentiation (for example, formula_2), PR allows more general hyper operators (for example, tetration) which are not contained in ELEMENTARY.\n\nThe definitions of elementary recursive functions are the same as for primitive recursive functions, except that primitive recursion is replaced by bounded summation and bounded product. All functions work over the natural numbers. The basic functions, all of them elementary recursive, are:\n\n\nFrom these basic functions, we can build other elementary recursive functions.\n\n\nThe class of elementary functions coincides with the closure with respect to composition of the projections and one of the following function sets: formula_5, formula_6, formula_7, where formula_8 is the subtraction function defined above.\n\n\"Lower elementary recursive\" functions follow the definitions as above, except that bounded product is disallowed. That is, a lower elementary recursive function must be a zero, successor, or projection function, a composition of other lower elementary recursive functions, or the bounded sum of another lower elementary recursive function.\n\nAlso known as Skolem elementary functions.\n\nWhereas elementary recursive functions have potentially more than exponential growth, the lower elementary recursive functions have polynomial growth.\n\nThe class of lower elementary functions has a description in terms of composition of simple functions analogous to that we have for elementary functions. Namely, a polynomial-bounded function is lower elementary if and only if it can be expressed using a composition of the following functions: projections, formula_9, formula_10, formula_11, formula_12, formula_13, one exponential function (formula_14 or formula_15) with the following restriction on the structure of formulas: the formula can have no more than two floors with respect to an exponent (for example, formula_16 has 1 floor, formula_17 has 2 floors, formula_18 has 3 floors). Here formula_12 is a bitwise AND of formula_20 and formula_21.\n\nIn descriptive complexity, ELEMENTARY is equal to the class of high order queries. This means that every language in the ELEMENTARY complexity class can be written as a high order formula that is true only for the elements on the language. More precisely, formula_22, where formula_23 indicates a tower of exponentiations and formula_24 is the class of queries that begin with existential quantifiers of th order and then a formula of th order.\n\n\n"}
{"id": "9710", "url": "https://en.wikipedia.org/wiki?curid=9710", "title": "Elementary algebra", "text": "Elementary algebra\n\nElementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails a use of algebraic notation and an understanding of the general rules of the operators introduced in arithmetic. Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.\n\nThe use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.\n\nAlgebraic notation describes the rules and conventions for writing mathematical expressions, as well as the terminology used for talking about parts of expressions. For example, the expression formula_1 has the following components:\n\n<br>\n1 : Exponent (power), 2 : Coefficient, 3 : term, 4 : operator, 5 : constant, formula_2 : variables\nA \"coefficient\" is a numerical value, or letter representing a numerical constant, that multiplies a variable (the operator is omitted). A \"term\" is an addend or a summand, a group of coefficients, variables, constants and exponents that may be separated from the other terms by the plus and minus operators. Letters represent variables and constants. By convention, letters at the beginning of the alphabet (e.g. formula_3) are typically used to represent constants, and those toward the end of the alphabet (e.g. formula_2 and formula_5) are used to represent variables. They are usually written in italics.\n\nAlgebraic operations work in the same way as arithmetic operations, such as addition, subtraction, multiplication, division and exponentiation. and are applied to algebraic variables and terms. Multiplication symbols are usually omitted, and implied when there is no space between two variables or terms, or when a coefficient is used. For example, formula_6 is written as formula_7, and formula_8 may be written formula_9.\n\nUsually terms with the highest power (exponent), are written on the left, for example, formula_10 is written to the left of formula_11. When a coefficient is one, it is usually omitted (e.g. formula_12 is written formula_10). Likewise when the exponent (power) is one, (e.g. formula_14 is written formula_15). When the exponent is zero, the result is always 1 (e.g. formula_16 is always rewritten to formula_17). However formula_18, being undefined, should not appear in an expression, and care should be taken in simplifying expressions in which variables may appear in exponents.\n\nOther types of notation are used in algebraic expressions when the required formatting is not available, or can not be implied, such as where only letters and symbols are available. For example, exponents are usually formatted using superscripts, e.g. formula_10. In plain text, and in the TeX mark-up language, the caret symbol \"^\" represents exponents, so formula_10 is written as \"x^2\". In programming languages such as Ada, Fortran, Perl, Python and Ruby, a double asterisk is used, so formula_10 is written as \"x**2\". Many programming languages and calculators use a single asterisk to represent the multiplication symbol, and it must be explicitly used, for example, formula_15 is written \"3*x\".\n\nElementary algebra builds on and extends arithmetic by introducing letters called variables to represent general (non-specified) numbers. This is useful for several reasons.\n\n\nAlgebraic expressions may be evaluated and simplified, based on the basic properties of arithmetic operations (addition, subtraction, multiplication, division and exponentiation). For example,\n\nAn equation states that two expressions are equal using the symbol for equality, formula_42 (the equals sign). One of the most well-known equations describes Pythagoras' law relating the length of the sides of a right angle triangle:\n\nThis equation states that formula_44, representing the square of the length of the side that is the hypotenuse (the side opposite the right angle), is equal to the sum (addition) of the squares of the other two sides whose lengths are represented by formula_45 and formula_46.\n\nAn equation is the claim that two expressions have the same value and are equal. Some equations are true for all values of the involved variables (such as formula_47); such equations are called identities. Conditional equations are true for only some values of the involved variables, e.g. formula_48 is true only for formula_49 and formula_50. The values of the variables which make the equation true are the solutions of the equation and can be found through equation solving.\n\nAnother type of equation is an inequality. Inequalities are used to show that one side of the equation is greater, or less, than the other. The symbols used for this are: formula_51 where formula_52 represents 'greater than', and formula_53 where formula_54 represents 'less than'. Just like standard equality equations, numbers can be added, subtracted, multiplied or divided. The only exception is that when multiplying or dividing by a negative number, the inequality symbol must be flipped.\n\nBy definition, equality is an equivalence relation, meaning it has the properties (a) reflexive (i.e. formula_55), (b) symmetric (i.e. if formula_56 then formula_57) (c) transitive (i.e. if formula_56 and formula_59 then formula_60). It also satisfies the important property that if two symbols are used for equal things, then one symbol can be substituted for the other in any true statement about the first and the statement will remain true. This implies the following properties:\n\n\nThe relations \"less than\" formula_54 and greater than formula_52 have the property of transitivity:\nBy reversing the inequation, formula_54 and formula_52 can be swapped, for example:\n\nSubstitution is replacing the terms in an expression to create a new expression. Substituting 3 for a in the expression a*5 makes a new expression 3*5 with meaning 15. Substituting the terms of a statement makes a new statement. When the original statement is true independent of the values of the terms, the statement created by substitutions is also true. Hence definitions can be made in symbolic terms and interpreted through substitution: if formula_88, where := means \"is defined to equal\", substituting 3 for formula_45 informs the reader of this statement that formula_90 means 3*3=9. Often it's not known whether the statement is true independent of the values of the terms, and substitution allows one to derive restrictions on the possible values, or show what conditions the statement holds under. For example, taking the statement x+1=0, if x is substituted with 1, this imples 1+1=2=0, which is false, which implies that if x+1=0 then x can't be 1.\n\nIf \"x\" and \"y\" are integers, rationals, or real numbers, then \"xy\"=0 implies \"x\"=0 or \"y\"=0. Suppose \"abc\"=0. Then, substituting \"a\" for \"x\" and \"bc\" for \"y\", we learn \"a\"=0 or \"bc\"=0. Then we can substitute again, letting \"x\"=\"b\" and \"y\"=\"c\", to show that if \"bc\"=0 then \"b\"=0 or \"c\"=0. Therefore, if \"abc\"=0, then \"a\"=0 or (\"b\"=0 or \"c\"=0), so \"abc\"=0 implies \"a\"=0 or \"b\"=0 or \"c\"=0.\n\nConsider if the original fact were stated as \"\"ab\"=0 implies \"a\"=0 or \"b\"=0.\" Then when we say \"suppose \"abc\"=0,\" we have a conflict of terms when we substitute. Yet the above logic is still valid to show that if \"abc\"=0 then \"a\"=0 or \"b\"=0 or \"c\"=0 if instead of letting \"a\"=\"a\" and \"b\"=\"bc\" we substitute \"a\" for \"a\" and \"b\" for \"bc\" (and with \"bc\"=0, substituting \"b\" for \"a\" and \"c\" for \"b\"). This shows that substituting for the terms in a statement isn't always the same as letting the terms from the statement equal the substituted terms. In this situation it's clear that if we substitute an expression \"a\" into the \"a\" term of the original equation, the \"a\" substituted does not refer to the \"a\" in the statement \"\"ab\"=0 implies \"a\"=0 or \"b\"=0.\"\n\nThe following sections lay out examples of some of the types of algebraic equations that may be encountered.\n\nLinear equations are so-called, because when they are plotted, they describe a straight line. The simplest equations to solve are linear equations that have only one variable. They contain only constant numbers and a single variable without an exponent. As an example, consider:\n\nTo solve this kind of equation, the technique is add, subtract, multiply, or divide both sides of the equation by the same number in order to isolate the variable on one side of the equation. Once the variable is isolated, the other side of the equation is the value of the variable. This problem and its solution are as follows:\n\nIn words: the child is 4 years old.\n\nThe general form of a linear equation with one variable, can be written as: formula_93\n\nFollowing the same procedure (i.e. subtract formula_46 from both sides, and then divide by formula_45), the general solution is given by formula_96\n\nA linear equation with two variables has many (i.e. an infinite number of) solutions. For example:\n\nThis cannot be worked out by itself. If the son's age was made known, then there would no longer be two unknowns (variables), and the problem becomes a linear equation with just one variable, that can be solved as described above.\n\nTo solve a linear equation with two variables (unknowns), requires two related equations. For example, if it was also revealed that:\nNow there are two related linear equations, each with two unknowns, which enables the production of a linear equation with just one variable, by subtracting one from the other (called the elimination method):\nIn other words, the son is aged 12, and since the father 22 years older, he must be 34. In 10 years time, the son will be 22, and the father will be twice his age, 44. This problem is illustrated on the associated plot of the equations.\n\nFor other ways to solve this kind of equations, see below, System of linear equations.\n\nA quadratic equation is one which includes a term with an exponent of 2, for example, formula_34, and no term with higher exponent. The name derives from the Latin \"quadrus\", meaning square. In general, a quadratic equation can be expressed in the form formula_101, where formula_45 is not zero (if it were zero, then the equation would not be quadratic but linear). Because of this a quadratic equation must contain the term formula_103, which is known as the quadratic term. Hence formula_104, and so we may divide by formula_45 and rearrange the equation into the standard form\n\nwhere formula_107 and formula_108. Solving this, by a process known as completing the square, leads to the quadratic formula\n\nwhere the symbol \"±\" indicates that both\n\nare solutions of the quadratic equation.\n\nQuadratic equations can also be solved using factorization (the reverse process of which is expansion, but for two linear terms is sometimes denoted foiling). As an example of factoring:\n\nwhich is the same thing as\n\nIt follows from the zero-product property that either formula_113 or formula_114 are the solutions, since precisely one of the factors must be equal to zero. All quadratic equations will have two solutions in the complex number system, but need not have any in the real number system. For example,\n\nhas no real number solution since no real number squared equals −1.\nSometimes a quadratic equation has a root of multiplicity 2, such as:\n\nFor this equation, −1 is a root of multiplicity 2. This means −1 appears two times, since the equation can be rewritten in factored form as\n\nAll quadratic equations have exactly two solutions in complex numbers (but they may be equal to each other), a category that includes real numbers, imaginary numbers, and sums of real and imaginary numbers. Complex numbers first arise in the teaching of quadratic equations and the quadratic formula. For example, the quadratic equation\n\nhas solutions\n\nSince formula_120 is not any real number, both of these solutions for \"x\" are complex numbers.\n\nAn exponential equation is one which has the form formula_121 for formula_122, which has solution\n\nwhen formula_124. Elementary algebraic techniques are used to rewrite a given equation in the above way before arriving at the solution. For example, if\n\nthen, by subtracting 1 from both sides of the equation, and then dividing both sides by 3 we obtain\n\nwhence\n\nor\n\nA logarithmic equation is an equation of the form formula_129 for formula_122, which has solution\n\nFor example, if\n\nthen, by adding 2 to both sides of the equation, followed by dividing both sides by 4, we get\n\nwhence\n\nfrom which we obtain\n\nA radical equation is one that includes a radical sign, which includes square roots, formula_136 cube roots, formula_137, and \"n\"th roots, formula_138. Recall that an \"n\"th root can be rewritten in exponential format, so that formula_138 is equivalent to formula_140. Combined with regular exponents (powers), then formula_141 (the square root of formula_11 cubed), can be rewritten as formula_143. So a common form of a radical equation is formula_144 (equivalent to formula_145) where formula_146 and formula_147 are integers. It has real solution(s):\nFor example, if:\n\nthen\n\nThere are different methods to solve a system of linear equations with two variables.\n\nAn example of solving a system of linear equations is by using the elimination method:\n\nMultiplying the terms in the second equation by 2:\n\nAdding the two equations together to get:\n\nwhich simplifies to\n\nSince the fact that formula_113 is known, it is then possible to deduce that formula_156 by either of the original two equations (by using \"2\" instead of formula_11 ) The full solution to this problem is then\n\nNote that this is not the only way to solve this specific system; formula_98 could have been solved before formula_11.\n\nAnother way of solving the same system of linear equations is by substitution.\n\nAn equivalent for formula_98 can be deduced by using one of the two equations. Using the second equation:\n\nSubtracting formula_164 from each side of the equation:\n\nand multiplying by −1:\n\nUsing this formula_98 value in the first equation in the original system:\n\nAdding \"2\" on each side of the equation:\n\nwhich simplifies to\n\nUsing this value in one of the equations, the same solution as in the previous method is obtained.\n\nNote that this is not the only way to solve this specific system; in this case as well, formula_98 could have been solved before formula_11.\n\nIn the above example, a solution exists. However, there are also systems of equations which do not have any solution. Such a system is called inconsistent. An obvious example is\n\nAs 0≠2, the second equation in the system has no solution. Therefore, the system has no solution.\nHowever, not all inconsistent systems are recognized at first sight. As an example, let us consider the system \n\nMultiplying by 2 both sides of the second equation, and adding it to the first one results in\nwhich has clearly no solution.\n\nThere are also systems which have infinitely many solutions, in contrast to a system with a unique solution (meaning, a unique pair of values for formula_11 and formula_98) For example:\n\nIsolating formula_98 in the second equation:\n\nAnd using this value in the first equation in the system:\n\nThe equality is true, but it does not provide a value for formula_11. Indeed, one can easily verify (by just filling in some values of formula_11) that for any formula_11 there is a solution as long as formula_186. There is an infinite number of solutions for this system.\n\nSystems with more variables than the number of linear equations are called underdetermined. Such a system, if it has any solutions, does not have a unique one but rather an infinitude of them. An example of such a system is\n\nWhen trying to solve it, one is led to express some variables as functions of the other ones if any solutions exist, but cannot express \"all\" solutions numerically because there are an infinite number of them if there are any.\n\nA system with a greater number of equations than variables is called overdetermined. If an overdetermined system has any solutions, necessarily some equations are linear combinations of the others.\n\n"}
{"id": "1065362", "url": "https://en.wikipedia.org/wiki?curid=1065362", "title": "End-to-end encryption", "text": "End-to-end encryption\n\nEnd-to-end encryption (E2EE) is a system of communication where only the communicating users can read the messages. In principle, it prevents potential eavesdroppers – including telecom providers, Internet providers, and even the provider of the communication service – from being able to access the cryptographic keys needed to decrypt the conversation. \n\nIn many messaging systems, including email and many chat networks, messages pass through intermediaries and are stored by a third party, from which they are retrieved by the recipient. Even if the messages are encrypted, they are typically only encrypted 'in transit',and are stored in \"decrypted\" form by the third party. This allows the third party to provide search and other features, or to scan for illegal and unacceptable content, but also means they can be read and misused by anyone who has access to the stored messages on the third party system, whether this is by design or via a backdoor. This can be seen as a concern in many cases where privacy is very important, such as persons living under repressive governments, whistleblowing, mass surveillance, businesses whose reputation depends on its ability to protect third party data, negotiations and communications that are important enough to have a risk of targeted 'hacking', and where sensitive subjects such as health, sexuality and information about minors are involved.\n\nEnd-to-end encryption is intended to prevent data being read or secretly modified, other than by the true sender and recipient(s). The messages are encrypted by the sender but the third party does not have a means to decrypt them, and stores them encrypted. The recipient retrieves the encrypted data and decrypts it themselves. \n\nBecause no third parties can decipher the data being communicated or stored. For example, companies that use end-to-end encryption are unable to hand over texts of their customers' messages to the authorities.\n\nIn an E2EE system, encryption keys must only be known to the communicating parties. To achieve this goal, E2EE systems can encrypt data using a pre-arranged string of symbols, called a pre-shared secret (PGP), or a one-time secret derived from such a pre-shared secret (DUKPT). They can also negotiate a secret key on the spot using Diffie-Hellman key exchange (OTR).\n\nAs of 2016, typical server-based communications systems do not include end-to-end encryption. These systems can only guarantee the protection of communications between clients and servers, meaning that users have to trust the third parties who are running the servers with the original texts. End-to-end encryption is regarded as safer because it reduces the number of parties who might be able to interfere or break the encryption. In the case of instant messaging, users may use a third-party client to implement an end-to-end encryption scheme over an otherwise non-E2EE protocol.\n\nSome non-E2EE systems, such as Lavabit and Hushmail, have described themselves as offering \"end-to-end\" encryption when they did not. Other systems, such as Telegram and Google Allo, have been criticized for not having end-to-end encryption, which they offer, enabled by default.\n\nSome encrypted backup and file sharing services provide client-side encryption. The encryption they offer is here not referred to as end-to-end encryption, because the services are not meant for sharing messages between users. However, the term \"end-to-end encryption\" is often used as a synonym for client-side encryption.\n\nEnd-to-end encryption ensures that data is transferred securely between endpoints. But, rather than try to break the encryption, an eavesdropper may impersonate a message recipient (during key exchange or by substituting his public key for the recipient's), so that messages are encrypted with a key known to the attacker. After decrypting the message, the snoop can then encrypt it with a key that they share with the actual recipient, or their public key in case of asymmetric systems, and send the message on again to avoid detection. This is known as a man-in-the-middle attack.\n\nMost end-to-end encryption protocols include some form of endpoint authentication specifically to prevent MITM attacks. For example, one could rely on certification authorities or a web of trust. An alternative technique is to generate cryptographic hashes (fingerprints) based on the communicating users’ public keys or shared secret keys. The parties compare their fingerprints using an outside (out-of-band) communication channel that guarantees integrity and authenticity of communication (but not necessarily secrecy), before starting their conversation. If the fingerprints match, there is in theory, no man in the middle. \n\nWhen displayed for human inspection, fingerprints are usually encoded into hexadecimal strings. These strings are then formatted into groups of characters for readability. For example, a 128-bit MD5 fingerprint would be displayed as follows:\n\nSome protocols display natural language representations of the hexadecimal blocks. As the approach consists of a one-to-one mapping between fingerprint blocks and words, there is no loss in entropy. The protocol may choose to display words in the user's native (system) language. This can, however, make cross-language comparisons prone to errors. In order to improve localization, some protocols have chosen to display fingerprints as base 10 strings instead of hexadecimal or natural language strings. Modern messaging applications can also display fingerprints as QR codes that users can scan off each other's devices.\n\nThe end-to-end encryption paradigm does not directly address risks at the communications endpoints themselves. Each user's computer can still be hacked to steal his or her cryptographic key (to create a MITM attack) or simply read the recipients’ decrypted messages both in real time and from log files. Even the most perfectly encrypted communication pipe is only as secure as the mailbox on the other end. Major attempts to increase endpoint security have been to isolate key generation, storage and cryptographic operations to a smart card such as Google's Project Vault. However, since plaintext input and output are still visible to the host system, malware can monitor conversations in real time. A more robust approach is to isolate all sensitive data to a fully air gapped computer. PGP has been recommended by experts for this purpose: However, as Bruce Schneier points out, Stuxnet developed by US and Israel successfully jumped air gap and reached Natanz nuclear plant's network in Iran. To deal with key exfiltration with malware, one approach is to split the Trusted Computing Base behind two unidirectionally connected computers that prevent either insertion of malware, or exfiltration of sensitive data with inserted malware.\n\nA backdoor is usually a secret method of bypassing normal authentication or encryption in a computer system, a product, or an embedded device, etc. Companies may also willingly or unwillingly introduce backdoors to their software that help subvert key negotiation or bypass encryption altogether. In 2013, information leaked by Edward Snowden showed that Skype had a backdoor which allowed Microsoft to hand over their users' messages to the NSA despite the fact that those messages were officially end-to-end encrypted.\n\n"}
{"id": "21568751", "url": "https://en.wikipedia.org/wiki?curid=21568751", "title": "Equidimensionality", "text": "Equidimensionality\n\nIn mathematics, especially in topology, equidimensionality is a property of a space that the local dimension is the same everywhere.\n\nA topological space \"X\" is said to be equidimensional if for all points \"p\" in \"X\" the dimension at \"p\" that is, dim \"p\"(\"X\") is constant. The Euclidean space is an example of an equidimensional space. The disjoint union of two spaces \"X\" and \"Y\" (as topological spaces) of different dimension is an example of a non-equidimensional space.\n\nAn algebraic variety whose coordinate ring is a Cohen–Macaulay ring is equidimensional.\n"}
{"id": "21137138", "url": "https://en.wikipedia.org/wiki?curid=21137138", "title": "Event segment", "text": "Event segment\n\nA segment of a system variable shows a homogenous status of system dynamics over a time period. Here, a homogenous status of a variable is a state which can be described by a set of coefficients of a formula. For example, of homogenous statuses, we can bring status of constant ('ON' of a switch) and linear (60 miles or 96 km per hour for speed). Mathematically, a segment is a function mapping from a set of times which can be defined by a real interval, to the set formula_1 [Zeigler76],[ZPK00], [Hwang13]. A trajectory of a system variable is a sequence of segments concatenated. We call a trajectory constant (respectively linear) if its concatenating segments are constant (respectively linear).\n\nAn event segment is a special class of the constant segment with a constraint in which the constant segment is either one of a timed event or a null-segment. The event segments are used to define Timed Event Systems such as DEVS, timed automata, and timed petri nets.\n\nThe \"time base\" of the concerning systems is denoted by formula_2, and defined\n\nas the set of non-negative real numbers.\n\nAn \"event\" is a label that abstracts a change. Given an event set formula_4, the \"null event\" denoted by formula_5 stands for nothing change.\n\nA \"timed event\" is a pair formula_6 where formula_7 and formula_8 denotes that an event formula_9 occurs at time formula_10.\n\nThe \"null segment\" over time interval formula_11 is denoted by formula_12 which means nothing in formula_1 occurs over formula_14.\n\nA \"unit event segment\" is either a null event segment or a timed event.\n\nGiven an event set formula_1, \"concatenation\" of two unit event segments formula_16 over formula_17 and formula_18 over formula_19 is denoted by formula_20 whose time interval is formula_21, and implies formula_22.\n\nAn \"event trajectory\"\nformula_23 over an event set formula_24 and a time interval formula_25 is concatenation of unit event segments formula_26 and formula_27 where\nformula_28.\n\nMathematically, an event trajectory is a mapping formula_16 a time period formula_30 to an event set formula_1. So we can write it in a function form :\n\nformula_32\n\nThe \"universal timed language\" formula_33 over an event set formula_1 and a time interval formula_35, is the set of all event trajectories over formula_1 and formula_37.\n\nA \"timed language\" formula_38 over an event set formula_1 and a timed interval\nformula_40 is \"a set of event trajectories\" over formula_1 and formula_42 if formula_43.\n\n"}
{"id": "18669800", "url": "https://en.wikipedia.org/wiki?curid=18669800", "title": "Fake 4-ball", "text": "Fake 4-ball\n\nIn mathematics, a fake 4-ball is a compact contractible topological 4-manifold. Michael Freedman proved that every three-dimensional homology sphere bounds a fake 4-ball. His construction involves the use of Casson handles and so does not work in the smooth category.\n\n"}
{"id": "33183306", "url": "https://en.wikipedia.org/wiki?curid=33183306", "title": "Fei–Ranis model of economic growth", "text": "Fei–Ranis model of economic growth\n\nThe Fei–Ranis model of economic growth is a dualism model in developmental economics or welfare economics that has been developed by John C. H. Fei and Gustav Ranis and can be understood as an extension of the Lewis model. It is also known as the Surplus Labor model. It recognizes the presence of a dual economy comprising both the modern and the primitive sector and takes the economic situation of unemployment and underemployment of resources into account, unlike many other growth models that consider underdeveloped countries to be homogenous in nature. According to this theory, the primitive sector consists of the existing agricultural sector in the economy, and the modern sector is the rapidly emerging but small industrial sector. Both the sectors co-exist in the economy, wherein lies the crux of the development problem. Development can be brought about only by a complete shift in the focal point of progress from the agricultural to the industrial economy, such that there is augmentation of industrial output. This is done by transfer of labor from the agricultural sector to the industrial one, showing that underdeveloped countries do not suffer from constraints of labor supply. At the same time, growth in the agricultural sector must not be negligible and its output should be sufficient to support the whole economy with food and raw materials. Like in the Harrod–Domar model, saving and investment become the driving forces when it comes to economic development of underdeveloped countries.\n\nOne of the biggest drawbacks of the Lewis model was the undermining of the role of agriculture in boosting the growth of the industrial sector. In addition to that, he did not acknowledge that the increase in productivity of labor should take place prior to the labor shift between the two sectors. However, these two ideas were taken into account in the Fei–Ranis dual economy model of three growth stages. They further argue that the model lacks in the proper application of concentrated analysis to the change that takes place with agricultural development\nIn Phase 1 of the Fei–Ranis model, the elasticity of the agricultural labor work-force is infinite and as a result, suffers from disguised unemployment. Also, the marginal product of labor is zero. This phase is similar to the Lewis model.In Phase 2 of the model, the agricultural sector sees a rise in productivity and this leads to increased industrial growth such that a base for the next phase is prepared. In Phase 2, agricultural surplus may exist as the increasing average product (AP), higher than the marginal product (MP) and not equal to the subsistence level of wages.\n\nUsing the help of the figure on the left, we see that\n\nAccording to Fei and Ranis, AD amount of labor (see figure) can be shifted from the agricultural sector without any fall in output. Hence, it represents surplus labor.\n\nformula_2\n\nAfter AD, MP begins to rise, and industrial labor rises from zero to a value equal to AD. AP of agricultural labor is shown by BYZ and we see that this curve falls downward after AD. This fall in AP can be attributed to the fact that as agricultural laborers shift to the industrial sector, the real wage of industrial laborers decreases due to shortage of food supply, since less laborers are now working in the food sector. The decrease in the real wage level decreases the level of profits, and the size of surplus that could have been re-invested for more industrialization. However, as long as surplus exists, growth rate can still be increased without a fall in the rate of industrialization. This re-investment of surplus can be graphically visualized as the shifting of MP curve outwards. In Phase2 the level of disguised unemployment is given by AK. This allows the agricultural sector to give up a part of its labor-force until\n\nPhase 3 begins from the point of commercialization which is at K in the Figure. This is the point where the economy becomes completely commercialized in the absence of disguised unemployment. The supply curve of labor in Phase 3 is steeper and both the sectors start bidding equally for labor.\n\nThe amount of labor that is shifted and the time that this shifting takes depends upon:\n\nSo, the three fundamental ideas used in this model are:\n\nThis shifting of labor can take place by the landlords' investment activities and by the government's fiscal measures. However, the cost of shifting labor in terms of both private and social cost may be high, for example transportation cost or the cost of carrying out construction of buildings. In addition to that, per capita agricultural consumption can increase, or there can exist a wide gap between the wages of the urban and the rural people. These three occurrences- high cost, high consumption and high gap in wages, are called as leakages, and leakages prevent the creation of agricultural surplus. In fact, surplus generation might be prevented due to a backward-sloping supply curve of labor as well, which happens when high income-levels are not consumed. This would mean that the productivity of laborers with rise in income will not rise. However, the case of backward-sloping curves is mostly unpractical.\n\nFei and Ranis emphasized strongly on the industry-agriculture interdependency and said that a robust connectivity between the two would encourage and speedup development. If agricultural laborers look for industrial employment, and industrialists employ more workers by use of larger capital good stock and labor-intensive technology, this connectivity can work between the industrial and agricultural sector. Also, if the surplus owner invests in that section of industrial sector that is close to soil and is in known surroundings, he will most probably choose that productivity out of which future savings can be channelized. They took the example of Japan's dualistic economy in the 19th century and said that connectivity between the two sectors of Japan was heightened due the presence of a decentralized rural industry which was often linked to urban production. According to them, economic progress is achieved in dualistic economies of underdeveloped countries through the work of a small number of entrepreneurs who have access to land and decision-making powers and use industrial capital and consumer goods for agricultural practices.\n\nIn (A), land is measured on the vertical axis, and labor on the horizontal axis. Ou and Ov represent two ridge lines, and the production contour lines are depicted by M, M and M. The area enclosed by the ridge lines defines the region of factor substitutability, or the region where factors can easily be substituted. Let us understand the repercussions of this. If te amount of labor is the total labor in the agricultural sector, the intersection of the ridge line Ov with the production curve M at point s renders M perfectly horizontal below Ov. The horizontal behavior of the production line implies that outside the region of factor substitutability, output stops and labor becomes redundant once land is fixed and labor is increased.\n\nIf Ot is the total land in the agricultural sector, ts amount of labor can be employed without it becoming redundant, and es represents the redundant agricultural labor force. This led Fei and Ranis to develop the concept of Labor Utilization Ratio, which they define as the units of labor that can be productively employed (without redundancy) per unit of land. In the left-side figure, labor utilization ratio\nwhich is graphically equal to the inverted slope of the ridge line Ov.\n\nFei and Ranis also built the concept of endowment ratio, which is a measure of the relative availability of the two factors of production. In the figure, if Ot represents agricultural land and tE represents agricultural labor, then the endowment ratio is given by\nwhich is equal to the inverted slope of OE.\nThe actual point of endowment is given by E.\n\nFinally, Fei and Ranis developed the concept of non-redundancy coefficient T which is measured by\n\nThese three concepts helped them in formulating a relationship between T, R and S. If ::formula_8\nthen\n\"This mathematical relation proves that the non-redundancy coefficient is directly proportional to labor utilization ratio and is inversely proportional to the endowment ratio.\"\n\n(B) displays the total physical productivity of labor (TPP) curve. The curve increases at a decreasing rate, as more units of labor are added to a fixed amount of land. At point \"N\", the curve shapes horizontally and this point N conforms to the point G in (C, which shows the marginal productivity of labor (MPP) curve, and with point s on the ridge line Ov in (A).\n\nLike in the agricultural sector, Fei and Ranis assume constant returns to scale in the industrial sector. However, the main factors of production are capital and labor. In the graph (A) right hand side, the production functions have been plotted taking labor on the horizontal axis and capital on the vertical axis. The expansion path of the industrial sector is given by the line OAAA. As capital increases from K to K to K and labor increases from L to L and L, the industrial output represented by the production contour A, A and A increases accordingly.\n\nAccording to this model, the prime labor supply source of the industrial sector is the agricultural sector, due to redundancy in the agricultural labor force. (B) shows the labor supply curve for the industrial sector S. PP represents the straight line part of the curve and is a measure of the redundant agricultural labor force on a graph with industrial labor force on the horizontal axis and output/real wage on the vertical axis. Due to the redundant agricultural labor force, the real wages remain constant but once the curve starts sloping upwards from point \"P\", the upward sloping indicates that additional labor would be supplied only with a corresponding rise in the real wages level.\n\nMPP curves corresponding to their respective capital and labor levels have been drawn as M, M, M and M. When capital stock rises from K to K, the marginal physical productivity of labor rises from M to M. When capital stock is K, the MPP curve cuts the labor supply curve at equilibrium point Po. At this point, the total real wage income is W and is represented by the shaded area POLP. λ is the equilibrium profit and is represented by the shaded area qPP. Since the laborers have extremely low income-levels, they barely save from that income and hence industrial profits (π) become the prime source of investment funds in the industrial sector.\n\nHere, \"K\" gives the total supply of investment funds (given that rural savings are represented by \"S\")\n\nTotal industrial activity rises due to increase in the total supply of investment funds, leading to increased industrial employment.\n\nAgricultural surplus in general terms can be understood as the produce from agriculture which exceeds the needs of the society for which it is being produced, and may be exported or stored for future use.\n\nTo understand the formation of agricultural surplus,we must refer to graph (B) of the agricultural sector. The figure on the left is a reproduced version of a section of the previous graph, with certain additions to better explain the concept of agricultural surplus.\nWe first derive the average physical productivity of the total agricultural labor force (APP). Fei and Ranis hypothesize that it is equal to the real wage and this hypothesis is known as the constant institutional wage hypothesis. It is also equal in value to the ratio of total agricultural output to the total agricultural population. Using this relation, we can obtain APP = MP/OP. This is graphically equal to the slope of line OM, and is represented by the line WW in (C).\n\nObserve point Y, somewhere to the left of P on the graph. If a section of the redundant agricultural labor force (PQ) is removed from the total agricultural labor force (OP) and absorbed into the industrial sector, then the labor force remaining in the industrial sector is represented by the point Y. Now, the output produced by the remaining labor force is represented by YZ and the real income of this labor force is given by XY. The difference of the two terms yields the total agricultural surplus of the economy. It is important to understand that this surplus is produced by the reallocation of labor such that it is absorbed by the industrial sector. This can be seen as deployment of hidden rural savings for the expansion of the industrial sector. Hence, we can understand the contribution of the agricultural sector to the expansion of industrial sector by this allocation of redundant labor force and the agricultural surplus that results from it.\n\nAgricultural surplus plays a major role as a wage fund. Its importance can be better explained with the help of the graph on the right, which is an integration of the industrial sector graph with an inverted agricultural sector graph, such that the origin of the agricultural sector falls on the upper-right corner. This inversion of the origin changes the way the graph is now perceived. While the labor force values are read from the left of 0, the output values are read vertically downwards from O. The sole reason for this inversion is for the sake of convenience. The point of commercialization as explained before (See Section on Basics of the model) is observed at point R, where the tangent to the line ORX runs parallel to OX.\n\nBefore a section of the redundant labor force is absorbed into the industrial sector, the entire labor OA is present in the agricultural sector. Once AG amount of labor force (say) is absorbed, it represented by OG' in the industrial sector, and the labor remaining in the agricultural sector is then OG. But how is the quantity of labor absorbed into the industrial sector determined? (A) shows the supply curve of labor SS' and several demand curves for labor df, d'f' and d\"f\". When the demand for labor is df, the intersection of the demand-supply curves gives the equilibrium employment point G'. Hence OG represents the amount of labor absorbed into the industrial sector. In that case, the labor remaining in the agricultural sector is OG. This OG amount of labor produces an output of GF, out of which GJ amount of labor is consumed by the agricultural sector and JF is the agricultural surplus for that level of employment. Simultaneously, the unproductive labor force from the agricultural sector turns productive once it is absorbed by the industrial sector, and produces an output of OG'Pd as shown in the graph, earning a total wage income of OG'PS.\n\nThe agricultural surplus JF created is needed for consumption by the same workers who left for the industrial sector. Hence, agriculture successfully provides not only the manpower for production activities elsewhere, but also the wage fund required for the process.\n\nThe Lewis model is criticised on the grounds that it neglects agriculture. Fei–Ranis model goes a step beyond and states that agriculture has a very major role to play in the expansion of the industrial sector. In fact, it says that the rate of growth of the industrial sector depends on the amount of total agricultural surplus and on the amount of profits that are earned in the industrial sector. So, larger the amount of surplus and the amount of surplus put into productive investment and larger the amount of industrial profits earned, the larger will be the rate of growth of the industrial economy. As the model focuses on the shifting of the focal point of progress from the agricultural to the industrial sector, Fei and Ranis believe that the ideal shifting takes place when the investment funds from surplus and industrial profits are sufficiently large so as to purchase industrial capital goods like plants and machinery. These capital goods are needed for the creation of employment opportunities. Hence, the condition put by Fei and Ranis for a successful transformation is that\n\nRate of increase of capital stock & rate of employment opportunities > Rate of population growth\n\nAs an underdeveloped country goes through its development process, labor is reallocated from the agricultural to the industrial sector. More the rate of reallocation, faster is the growth of that economy. The economic rationale behind this idea of labor reallocation is that of faster economic development. The essence of labor reallocation lies in Engel's Law, which states that the proportion of income being spent on food decreases with increase in the income-level of an individual, even if there is a rise in the actual expenditure on food. For example, if 90 per cent of the entire population of the concerned economy is involved in agriculture, that leaves just 10 per cent of the population in the industrial sector. As the productivity of agriculture increases, it becomes possible for just 35 per cent of population to maintain a satisfactory food supply for the rest of the population. As a result, the industrial sector now has 65 per cent of the population under it. This is extremely desirable for the economy, as the growth of industrial goods is subject to the rate of per capita income, while the growth of agricultural goods is subject only to the rate of population growth, and so a bigger labor supply to the industrial sector would be welcome under the given conditions. In fact, this labor reallocation becomes necessary with time since consumers begin to want more of industrial goods than agricultural goods in relative terms.\n\nHowever, Fei and Ranis were quick to mention that the necessity of labor reallocation must be linked more to the need to produce more capital investment goods as opposed to the thought of industrial consumer goods following the discourse of Engel's Law. This is because the assumption that the demand for industrial goods is high seems unrealistic, since the real wage in the agricultural sector is extremely low and that hinders the demand for industrial goods. In addition to that, low and mostly constant wage rates will render the wage rates in the industrial sector low and constant. This implies that demand for industrial goods will not rise at a rate as suggested by the use of Engel's Law.\n\nSince the growth process will observes a slow-paced increase in the consumer purchasing power, the dualistic economies follow the path of natural austerity, which is characterized by more demand and hence importance of capital good industries as compared to consumer good ones. However, investment in capital goods comes with a long gestation period, which drives the private entrepreneurs away. This suggests that in order to enable growth, the government must step in and play a major role, especially in the initial few stages of growth. Additionally, the government also works on the social and economic overheads by the construction of roads, railways, bridges, educational institutions, health care facilities and so on.\n\nIn the Fei-Ranis model, it is possible that as technological progress takes place and there is a shift to labor-saving production techniques, growth of the economy takes place with increase in profits but no economic development takes place. This can be explained well with the help of graph in this section.\n\nThe graph displays two MPL lines plotted with real wage and MPL on the vertical axis and employment of labor on the horizontal axis. OW denotes the subsistence wage level, which is the minimum wage level at which a worker (and his family) would survive. The line WW' running parallel to the X-axis is considered to be infinitely elastics since supply of labor is assumed to be unlimited at the subsistence-wage level. The square area OWEN represents the wage bill and DWE represents the surplus or the profits collected. This surplus or profit can increase if the MPL curve changes.\n\nIf the MPL curve changes from MPL to MPL due to a change in production technique, such that it becomes labor-saving or capital-intensive, then the surplus or profit collected would increase. This increase can be seen by comparing DWE with DWE since DWE since is greater in area compared to DWE. However, there is no new point of equilibrium and as E continues to be the point of equilibrium,there is no increase in the level of labor employment, or in wages for that matter. Hence, labor employment continues as ON and wages as OW. The only change that accompanies the change in production technique is the one in surplus or profits.\n\nThis makes for a good example of a process of growth without development, since growth takes place with increase in profits but development is at a standstill since employment and wages of laborers remain the same.\n\nFei–Ranis model of economic growth has been criticized on multiple grounds, although if the model is accepted, then it will have a significant theoretical and policy implications on the underdeveloped countries' efforts towards development and on the persisting controversial statements regarding the balanced vs. unbalanced growth debate.\n\nTo understand this better, we refer to the graph in this section, which shows Food on the vertical axis and Leisure on the horizontal axis. OS represents the subsistence level of food consumption, or the minimum level of food consumed by agricultural labor that is necessary for their survival. I and I between the two commodities of food and leisure (of the agriculturists). The origin falls on G, such that OG represents maximum labor and labor input would be measured from the right to the left.\nThe transformation curve SAG falls from A, which indicates that more leisure is being used to same units of land. At A, the marginal transformation between food and leisure and MPL = 0 and the indifference curve I is also tangent to the transformation curve at this point. This is the point of leisure satiation.\n\nConsider a case where a laborer shifts from the agricultural to the industrial sector. In that case, the land left behind would be divided between the remaining laborers and as a result, the transformation curve would shift from SAG to RTG. Like at point A, MPL at point T would be 0 and APL would continue to be the same as that at A (assuming constant returns to scale). If we consider MPL = 0 as the point where agriculturalists live on the subsistence level, then the curve RTG must be flat at point T in order to maintain the same level of output. However, that would imply leisure satiation or leisure as an inferior good, which are two extreme cases. It can be surmised then that under normal cases, the output would decline with shift of labor to industrial sector, although the per capita output would remain the same. This is because, a fall in the per capita output would mean fall in consumption in a way that it would be lesser than the subsistence level, and the level of labor input per head would either rise or fall.\n\nBerry and Soligo in their 1968 paper have criticized this model for its MPL=0 assumption, and for the assumption that the transfer of labor from the agricultural sector leaves the output in that sector unchanged in Phase 1. They show that the output changes, and may fall under various land tenure systems, unless the following situations arise:\n\n1. Leisure falls under the inferior good category\n2. Leisure satiation is present.\n3. There is perfect substitutability between food and leisure, and the marginal rate of substitution is constant for all real income levels.\n\nNow if MPL>0 then leisure satiation option becomes invalid, and if MPL=0 then the option of food and leisure as perfect substitutes becomes invalid. Therefore, the only remaining viable option is leisure as an inferior good.\n\n"}
{"id": "25178047", "url": "https://en.wikipedia.org/wiki?curid=25178047", "title": "Gaussian q-distribution", "text": "Gaussian q-distribution\n\nIn mathematical physics and probability and statistics, the Gaussian \"q\"-distribution is a family of probability distributions that includes, as limiting cases, the uniform distribution and the normal (Gaussian) distribution. It was introduced by Diaz and Teruel, is a q-analogue of the Gaussian or normal distribution.\n\nThe distribution is symmetric about zero and is bounded, except for the limiting case of the normal distribution. The limiting uniform distribution is on the range -1 to +1.\n\nLet \"q\" be a real number in the interval [0, 1). The probability density function of the Gaussian \"q\"-distribution is given by\n\nwhere\n\nThe \"q\"-analogue [\"t\"] of the real number formula_4 is given by\n\nThe \"q\"-analogue of the exponential function is the q-exponential, \"E\", which is given by\n\nwhere the \"q\"-analogue of the factorial is the q-factorial, [\"n\"]!, which is in turn given by\n\nfor an integer \"n\" > 2 and [1]! = [0]! = 1.\n\nThe cumulative distribution function of the Gaussian \"q\"-distribution is given by\n\nwhere the integration symbol denotes the Jackson integral.\n\nThe function \"G\" is given explicitly by\n\nwhere\n\nThe moments of the Gaussian \"q\"-distribution are given by\n\nwhere the symbol [2\"n\" − 1]<nowiki>!!</nowiki> is the \"q\"-analogue of the double factorial given by\n\n"}
{"id": "4321378", "url": "https://en.wikipedia.org/wiki?curid=4321378", "title": "Geometry Center", "text": "Geometry Center\n\nThe Geometry Center was a mathematics research and education center at the University of Minnesota. It was established by the National Science Foundation in the late 1980s and closed in 1998. The focus of the Center's work was the use of computer graphics and visualization for research and education in pure mathematics and geometry.\n\nThe Center's founding director was Al Marden. Richard McGehee directed the Center during its final years. The Center's governing board was chaired by David P. Dobkin.\n\nMuch of the work done at the Center was for the development of \"Geomview\", a three-dimensional interactive geometry program. This focused on mathematical visualization with options to allow hyperbolic space to be visualised. It was originally written for Silicon Graphics workstations, and has been ported to run on Linux systems; it is available for installation in most Linux distributions through the package management system. \"Geomview\" can run under Windows using Cygwin and under Mac OS X. Geomview has a web site at .\n\nGeomview is built on the Object Oriented Graphics Library (OOGL). The displayed scene and the attributes of the objects in it may be manipulated by the graphical command language (GCL) of Geomview. Geomview may be set as a default 3-D viewer for Mathematica.\n\nGeomview source is available at , which also has a required Motif download. For Mac OS X users, provides a direct route to download and compile geomview.\n\n\"Geomview\" was used in the construction of several mathematical movies including:\n\nOther programs developed at the Center included:\n\nRichard McGehee, the center's director, has stated that the website was one of the first one hundred websites ever published.\nDespite the Center being closed, its website is still online at as an archive of a wide range of geometric topics, including:\n\n\"Geomview\" is supported through the dedicated \"Geomview\" website.\n\nSupport for software developed at the Geometry Center is available through Geometry Technologies.\n\nDuring its time of operation, a large number of mathematical workshops were held at the Center. Many well-known mathematicians visited the Center, including Eugenio Calabi, John Horton Conway, Donald E. Knuth, David Mumford, William Thurston, and Jeff Weeks. There were over thirty postdocs, apprentices and graduate students.\n"}
{"id": "4185921", "url": "https://en.wikipedia.org/wiki?curid=4185921", "title": "Gompertz–Makeham law of mortality", "text": "Gompertz–Makeham law of mortality\n\nThe Gompertz–Makeham law\nstates that the human death rate is the sum of an age-independent component (the Makeham term, named after William Makeham) and an age-dependent component (the Gompertz function, named after Benjamin Gompertz), which increases exponentially with age. In a protected environment where external causes of death are rare (laboratory conditions, low mortality countries, etc.), the age-independent mortality component is often negligible. In this case the formula simplifies to a Gompertz law of mortality. In 1825, Benjamin Gompertz proposed an exponential increase in death rates with age.\n\nThe Gompertz–Makeham law of mortality describes the age dynamics of human mortality rather accurately in the age window from about 30 to 80 years of age. At more advanced ages, some studies have found that death rates increase more slowly – a phenomenon known as the late-life mortality deceleration – but more recent studies disagree.\n\nThe decline in the human mortality rate before the 1950s was mostly due to a decrease in the age-independent (Makeham) mortality component, while the age-dependent (Gompertz) mortality component was surprisingly stable. Since the 1950s, a new mortality trend has started in the form of an unexpected decline in mortality rates at advanced ages and \"rectangularization\" of the survival curve.\n\nThe hazard function for the Gompertz-Makeham distribution is most often characterised as formula_1. The empirical magnitude of the beta-parameter is about .085, implying a doubling of mortality every .69/.085 = 8 years (Denmark, 2006).\n\nThe quantile function can be expressed in a closed-form expressions using the Lambert W function:\n\nThe Gompertz law is the same as a Fisher–Tippett distribution for the negative of age, restricted to negative values for the random variable (positive values for age).\n\n"}
{"id": "21723066", "url": "https://en.wikipedia.org/wiki?curid=21723066", "title": "Graph algebra", "text": "Graph algebra\n\nIn mathematics, especially in the fields of universal algebra and graph theory, a graph algebra is a way of giving a directed graph an algebraic structure. It was introduced in , and has seen many uses in the field of universal algebra since then.\n\nLet formula_1 be a directed graph, and formula_2 an element not in formula_3. The graph algebra associated with formula_4 is the set formula_5 equipped with multiplication defined by the rules\n\nThis notion has made it possible to use the methods of graph theory in universal algebra and several other directions of discrete mathematics and computer science. Graph algebras have been used, for example, in constructions concerning dualities , equational theories , flatness , groupoid rings , topologies , varieties , finite state automata , finite state machines , \ntree languages and tree automata etc.\n\n\n"}
{"id": "34920327", "url": "https://en.wikipedia.org/wiki?curid=34920327", "title": "Helmut Hofer", "text": "Helmut Hofer\n\nHelmut Hermann W. Hofer (born February 18 or February 28, 1956) is a German-American mathematician, one of the founders of the area of symplectic topology. He is a member of the National Academy of Sciences, and the recipient of the 1999 Ostrowski Prize\nand the 2013 Heinz Hopf Prize . Since 2009, he is a faculty member at the Institute for Advanced Study in Princeton. He currently works on symplectic geometry, dynamical systems, and partial differential equations. His contributions to the field include Hofer geometry.\n\n\n"}
{"id": "25287284", "url": "https://en.wikipedia.org/wiki?curid=25287284", "title": "Hessenberg variety", "text": "Hessenberg variety\n\nIn geometry, Hessenberg varieties, first studied by Filippo De Mari, Claudio Procesi, and Mark A. Shayman, are a family of subvarieties of the full flag variety which are defined by a Hessenberg function \"h\" and a linear transformation \"X\". The study of Hessenberg varieties was first motivated by questions in numerical analysis in relation to algorithms for computing eigenvalues and eigenspaces of the linear operator \"X\". Later work by T. A. Springer, Dale Peterson, Bertram Kostant, among others, found connections with combinatorics, representation theory and cohomology.\n\nA \"Hessenberg function\" is a map \n\nsuch that \n\nfor each \"i\". For example, the function that takes the sends the numbers 1 to 5 (in order) to 2, 3, 3, 4, and 5 is a Hessenberg function.\n\nFor any Hessenberg function \"h\" and a linear transformation \n\nthe \"Hessenberg variety\" formula_4 is the set of all flags formula_5 such that \n\nfor all \"i\".\n\nSome examples of Hessenberg varieties (with their formula_7 function) include:\n\nThe Full Flag variety: \"h\"(\"i\") = \"n\" for all \"i\"\n\nThe Peterson variety: formula_8 for formula_9\n\nThe Springer variety: formula_10 for all formula_11.\n\n"}
{"id": "19404886", "url": "https://en.wikipedia.org/wiki?curid=19404886", "title": "James W. Cannon", "text": "James W. Cannon\n\nJames W. Cannon (born January 30, 1943) is an American mathematician working in the areas of low-dimensional topology and geometric group theory. He was an Orson Pratt Professor of Mathematics at Brigham Young University.\n\nJames W. Cannon was born on January 30, 1943, in Bellefonte, Pennsylvania. Cannon received a Ph.D. in Mathematics from the University of Utah in 1969, under the direction of C. Edmund Burgess.\n\nHe was a Professor at the University of Wisconsin, Madison from 1977 to 1985. In 1986 Cannon was appointed an Orson Pratt Professor of Mathematics at Brigham Young University. He held this position until his retirement in September 2012.\n\nCannon gave an AMS Invited address at the meeting of the American Mathematical Society in Seattle in August 1977, an invited address at the International Congress of Mathematicians in Helsinki 1978, and delivered the 1982 Mathematical Association of America Hedrick Lectures in Toronto, Canada.\n\nCannon was elected to the American Mathematical Society Council in 2003 with the term of service February 1, 2004, to January 31, 2007. In 2012 he became a fellow of the American Mathematical Society.\n\nIn 1993 Cannon delivered the 30-th annual Karl G. Maeser Distinguished Faculty Lecture at Brigham Young University.\n\nJames Cannon is a devout member of The Church of Jesus Christ of Latter-day Saints.\n\nCannon's early work concerned topological aspects of embedded surfaces in R and understanding the difference between \"tame\" and \"wild\" surfaces.\n\nHis first famous result came in late 1970s when Cannon gave a complete solution to a long-standing \"double suspension\" problem posed by John Milnor. Cannon proved that the double suspension of a homology sphere is a topological sphere. R. D. Edwards had previously proven this in many cases.\n\nThe results of Cannon's paper were used by Cannon, Bryant and Lacher to prove (1979) an important case of the so-called \"characterization conjecture\" for topological manifolds. The conjecture says that a generalized \"n\"-manifold formula_1, where formula_2, which satisfies the \"disjoint disk property\" is a topological manifold. Cannon, Bryant and Lacher established that the conjecture holds under the assumption that formula_1 be a manifold except possibly at a set of dimension formula_4. Later Frank Quinn completed the proof that the characterization conjecture holds if there is even a single manifold point. In general, the conjecture is false as was proved by John Bryant, Steven Ferry, Washington Mio and Shmuel Weinberger.\n\nIn 1980s the focus of Cannon's work shifted to the study of 3-manifolds, hyperbolic geometry and Kleinian groups and he is considered one of the key figures in the birth of geometric group theory as a distinct subject in late 1980s and early 1990s. Cannon's 1984 paper \"The combinatorial structure of cocompact discrete hyperbolic groups\" was one of the forerunners in the development of the theory of word-hyperbolic groups, a notion that was introduced and developed three years later in a seminal 1987 monograph of Mikhail Gromov. Cannon's paper explored combinatorial and algorithmic aspects of the Cayley graphs of Kleinian groups and related them to the geometric features of the actions of these groups on the hyperbolic space. In particular, Cannon proved that convex-cocompact Kleinian groups admit finite presentations where the Dehn algorithm solves the word problem. The latter condition later turned out to give one of equivalent characterization of being word-hyperbolic and, moreover, Cannon's original proof essentially went through without change to show that the word problem in word-hyperbolic groups is solvable by Dehn's algorithm. Cannon's 1984 paper also introduced an important notion a \"cone type\" of an element of a finitely generated group (roughly, the set of all geodesic extensions of an element). Cannon proved that a convex-cocompact Kleinian group has only finitely many cone types (with respect to a fixed finite generating set of that group) and showed how to use this fact to conclude that the growth series of the group is a rational function. These arguments also turned out to generalize to the word-hyperbolic group context. Now standard proofs of the fact that the set of geodesic words in a word-hyperbolic group is a regular language also use finiteness of the number of cone types.\n\nCannon's work also introduced an important notion of \"almost convexity\" for Cayley graphs of finitely generated groups, a notion that led to substantial further study and generalizations.\n\nAn influential paper of Cannon and William Thurston \"Group invariant Peano curves\", that first circulated in a preprint form in the mid-1980s, introduced the notion of what is now called the Cannon–Thurston map. They considered the case of a closed hyperbolic 3-manifold \"M\" that fibers over the circle with the fiber being a closed hyperbolic surface \"S\". In this case the universal cover of \"S\", which is identified with the hyperbolic plane, admits an embedding into the universal cover of \"M\", which is the hyperbolic 3-space. Cannon and Thurston proved that this embedding extends to a continuous π(\"S\")-equivariant surjective map (now called the \"Cannon–Thurston map\") from the ideal boundary of the hyperbolic plane (the circle) to the ideal boundary of the hyperbolic 3-space (the 2-sphere).\nAlthough the paper of Cannon and Thurston was finally published only in 2007, in the meantime it has generated considerable further research and a number of significant generalizations (both in the contexts of Kleinian groups and of word-hyperbolic groups), including the work of Mahan Mitra, Erica Klarreich, Brian Bowditch and others.\n\nCannon was one of the co-authors of the 1992 book \"Word Processing in Groups\" which introduced, formalized and developed the theory of automatic groups. The theory of automatic groups brought new computational ideas from computer science to geometric group theory and played an important role in the development of the subject in 1990s.\n\nA 1994 paper of Cannon gave a proof of the \"combinatorial Riemann mapping theorem\" that was motivated by the classic Riemann mapping theorem in complex analysis. The goal was to understand when an action of a group by homeomorphisms on a 2-sphere is (up to a topological conjugation) an action on the standard Riemann sphere by Möbius transformations. The \"combinatorial Riemann mapping theorem\" of Cannon gave a set of sufficient conditions when a sequence of finer and finer combinatorial subdivisions of a topological surface determine, in the appropriate sense and after passing to the limit, an actual conformal structure on that surface. This paper of Cannon led to an important conjecture, first explicitly formulated by Cannon and Swenson in 1998 (but also suggested in implicit form in Section 8 of Cannon's 1994 paper) and now known as Cannon's conjecture, regarding characterizing word-hyperbolic groups with the 2-sphere as the boundary. The conjecture (Conjecture 5.1 in ) states that if the ideal boundary of a word-hyperbolic group \"G\" is homeomorphic to the 2-sphere, then \"G\" admits a properly discontinuous cocompact isometric action on the hyperbolic 3-space (so that \"G\" is essentially a 3-dimensional Kleinian group). In analytic terms Cannon's conjecture is equivalent to saying that if the ideal boundary of a word-hyperbolic group \"G\" is homeomorphic to the 2-sphere then this boundary, with the visual metric coming from the Cayley graph of \"G\", is quasisymmetric to the standard 2-sphere.\n\nThe 1998 paper of Cannon and Swenson gave an initial approach to this conjecture by proving that the conjecture holds under an extra assumption that the family of standard \"disks\" in the boundary of the group satisfies a combinatorial \"conformal\" property. The main result of Cannon's 1994 paper played a key role in the proof. This approach to Cannon's conjecture and related problems was pushed further later in the joint work of Cannon, Floyd and Parry.\n\nCannon's conjecture motivated much of subsequent work by other mathematicians and to a substantial degree informed subsequent interaction between geometric group theory and the theory of analysis on metric spaces. Cannon's conjecture was motivated (see ) by Thurston's Geometrization Conjecture and by trying to understand why in dimension three variable negative curvature can be promoted to constant negative curvature. Although the Geometrization conjecture was recently settled by Perelman, Cannon's conjecture remains wide open and is considered one of the key outstanding open problems in geometric group theory and geometric topology.\n\nThe ideas of combinatorial conformal geometry that underlie Cannon's proof of the \"combinatorial Riemann mapping theorem\", were applied by Cannon, Floyd and Parry (2000) to the study of large-scale growth patterns of biological organisms. Cannon, Floyd and Parry produced a mathematical growth model which demonstrated that some systems determined by simple finite subdivision rules can results in objects (in their example, a tree trunk) whose large-scale form oscillates wildly over time even though the local subdivision laws remain the same. Cannon, Floyd and Parry also applied their model to the analysis of the growth patterns of rat tissue. They suggested that the \"negatively curved\" (or non-euclidean) nature of microscopic growth patterns of biological organisms is one of the key reasons why large-scale organisms do not look like crystals or polyhedral shapes but in fact in many cases resemble self-similar fractals. In particular they suggested (see section 3.4 of ) that such \"negatively curved\" local structure is manifested in highly folded and highly connected nature of the brain and the lung tissue.\n\n\n\n"}
{"id": "2137236", "url": "https://en.wikipedia.org/wiki?curid=2137236", "title": "Lattice multiplication", "text": "Lattice multiplication\n\nLattice multiplication, also known as gelusia multiplication, sieve multiplication, shabakh, Venetian squares, or the Chinese lattice, is a method of multiplication that uses a lattice to multiply two multi-digit numbers. It is mathematically identical to the more commonly used long multiplication algorithm, but it breaks the process into smaller steps, which some practitioners find easier to use.\n\nThe method had already arisen by medieval times, and has been used for centuries in many different cultures. It is still being taught in certain curricula today.\n\nA grid is drawn up, and each cell is split diagonally. The two multiplicands of the product to be calculated are written along the top and right side of the lattice, respectively, with one digit per column across the top for the first multiplicand (the number written left to right), and one digit per row down the right side for the second multiplicand (the number written top-down). Then each cell of the lattice is filled in with product of its column and row digit.\n\nAs an example, let's consider the multiplication of 58 with 213. After writing the multiplicands on the sides, consider each cell, beginning with the top left cell. In this case, the column digit is 5 and the row digit is 2. Write their product, 10, in the cell, with the digit 1 above the diagonal and the digit 0 below the diagonal (see picture for Step 1).\n\nIf the simple product lacks a digit in the tens place, simply fill in the tens place with a 0.\n\nAfter all the cells are filled in this manner, the digits in each diagonal are summed, working from the bottom right diagonal to the top left. Each diagonal sum is written where the diagonal ends. If the sum contains more than one digit, the value of the tens place is carried into the next diagonal (see Step 2).\n\nNumbers are filled to the left and to the bottom of the grid, and the answer is the numbers read off down (on the left) and across (on the bottom).\nThe lattice technique can also be used to multiply decimal fractions. For example, to multiply 5.8 by 2.13, the process is the same as to multiply 58 by 213 as described in the preceding section. To find the position of the decimal point in the final answer, one can draw a vertical line from the decimal point in 5.8, and a horizontal line from the decimal point in 2.13. The grid diagonal through the intersection of these two lines then determines the position of the decimal point in the result.\n\nLattice multiplication has been used historically in many different cultures. It is not known where it arose first, nor whether it developed independently within more than one region of the world. The earliest recorded use of lattice multiplication:\n\n\nThe mathematician and educator David Eugene Smith asserted that lattice multiplication was brought to Italy from the Middle East. This is reinforced by noting that the Arabic term for the method, \"shabakh\", has the same meaning as the Italian term for the method, \"gelosia\", namely, the metal grille or grating (lattice) for a window.\n\nIt is sometimes erroneously stated that lattice multiplication was described by Muḥammad ibn Mūsā al-Khwārizmī (Baghdad, c. 825) or by Fibonacci in his \"Liber Abaci\" (Italy, 1202, 1228). In fact, however, no use of lattice multiplication by either of these two authors has been found. In Chapter 3 of his \"Liber Abaci\", Fibonacci does describe a related technique of multiplication by what he termed \"quadrilatero in forma scacherii\" (“rectangle in the form of a chessboard”). In this technique, the square cells are not subdivided diagonally; only the lowest-order digit is written in each cell, while any higher-order digit must be remembered or recorded elsewhere and then \"carried\" to be added to the next cell. This is in contrast to lattice multiplication, a distinctive feature of which is that the each cell of the rectangle has its own correct place for the carry digit; this also implies that the cells can be filled in any order desired. Swetz compares and contrasts multiplication by \"gelosia\" (lattice), by \"scacherii\" (chessboard), and other tableau methods.\n\nOther notable historical uses of lattice multiplication include:\n\n\nDerivations of this method also appeared in the 16th century in Matrakci Nasuh's \"Umdet-ul Hisab\". Matrakçı Nasuh's triangular version of the multiplication technique is seen in the example showing 155 x 525 on the right, and explained in the example showing 236 x 175 on the left figure.\nThe same principle described by Matrakci Nasuh underlay the later development of the calculating rods known as Napier's bones (Scotland, 1617) and Genaille–Lucas rulers (France, late 1800s).\n"}
{"id": "519979", "url": "https://en.wikipedia.org/wiki?curid=519979", "title": "List of mathematics history topics", "text": "List of mathematics history topics\n\nThis is a list of mathematics history topics, by Wikipedia page. See also list of mathematicians, timeline of mathematics, history of mathematics, list of publications in mathematics.\n\n\n"}
{"id": "11254442", "url": "https://en.wikipedia.org/wiki?curid=11254442", "title": "List of women in mathematics", "text": "List of women in mathematics\n\nThis is a list of women who have made noteworthy contributions to or achievements in mathematics. These include mathematical research, mathematics education, the history and philosophy of mathematics, public outreach, and mathematics contests.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "578460", "url": "https://en.wikipedia.org/wiki?curid=578460", "title": "Loop invariant", "text": "Loop invariant\n\nIn computer science, a loop invariant is a property of a program loop that is true before (and after) each iteration. It is a logical assertion, sometimes checked within the code by an assertion call. Knowing its invariant(s) is essential in understanding the effect of a loop.\n\nIn formal program verification, particularly the Floyd-Hoare approach, loop invariants are expressed by formal predicate logic and used to prove properties of loops and by extension algorithms that employ loops (usually correctness properties).\nThe loop invariants will be true on entry into a loop and following each iteration, so that on exit from the loop both the loop invariants and the loop termination condition can be guaranteed.\n\nFrom a programming methodology viewpoint, the loop invariant can be viewed as a more abstract specification of the loop, which characterizes the deeper purpose of the loop beyond the details of this implementation. A survey article covers fundamental algorithms from many areas of computer science (searching, sorting, optimization, arithmetic etc.), characterizing each of them from the viewpoint of its invariant.\n\nBecause of the similarity of loops and recursive programs, proving partial correctness of loops with invariants is very similar to proving correctness of recursive programs via induction. In fact, the loop invariant is often the same as the inductive hypothesis to be proved for a recursive program equivalent to a given loop.\n\nLoop-invariant code motion, which involves moving code out of the loop if that does not change the effect of the program, is not directly related to loop invariants, which are properties of the loop overall.\n\nThe following C subroutine codice_1 returns the maximum value in its argument array codice_2, provided its length codice_3 is at least 1.\nComments are provided at lines 3, 6, 9, 11, and 13. Each comment makes an assertion about the values of one or more variables at that stage of the function.\nThe highlighted assertions within the loop body, at the beginning and end of the loop (lines 6 and 11), are exactly the same. They thus describe an invariant property of the loop.\nWhen line 13 is reached, this invariant still holds, and it is known that the loop condition codice_4 from line 5 has become false. Both properties together imply that codice_5 equals the maximum value in codice_6, that is, that the correct value is returned from line 14.\nFollowing a defensive programming paradigm, the loop condition codice_4 in line 5 should better be modified to codice_8, in order to avoid endless looping for illegitimate negative values of codice_3. While this change in code intuitively shouldn't make a difference, the reasoning leading to its correctness becomes somewhat more complicated, since then only codice_10 is known in line 13. In order to obtain that also codice_11 holds, that condition has to be included into the loop invariant. It is easy to see that codice_11, too, is an invariant of the loop, since codice_8 in line 6 can be obtained from the (modified) loop condition in line 5, and hence codice_11 holds in line 11 after codice_15 has been incremented in line 10. However, when loop invariants have to be manually provided for formal program verification, such intuitively too obvious properties like codice_11 are often overlooked.\n\nSpecifically in Floyd–Hoare logic, the partial correctness of a while loop is governed by the following rule of inference:\n\nThis means:\n\nIn other words: The rule above is a deductive step that has as its premise the Hoare triple formula_12. This triple is actually a relation on machine states. It holds whenever starting from a state in which the boolean expression formula_13 is true and successfully executing some code called formula_3, the machine ends up in a state in which formula_2 is true. If this relation can be proven, the rule then allows us to conclude that successful execution of the program formula_10 will lead from a state in which formula_2 is true to a state in which formula_18 holds. The boolean formula formula_2 in this rule is known as the loop invariant.\n\nThe loop invariant plays an important role in the intuitive argument for soundness of the Floyd-Hoare rule for codice_17 loops. The loop invariant has to be true before each iteration of the loop body, and also after each iteration of the loop body. Since a codice_17 loop is precisely the repeated iteration of the loop body, it follows that if the invariant is true before entering the loop, it must also be true after exiting the loop.\n\nThe following example illustrates how this rule works. Consider the program\n\nOne can then prove the following Hoare triple:\n\nThe condition \"C\" of the codice_17 loop is formula_21. A useful loop invariant formula_2 has to be guessed; it will turn out that formula_23 is appropriate. Under these assumptions it is possible to prove the following Hoare triple:\n\nWhile this triple can be derived formally from the rules of Floyd-Hoare logic governing assignment, it is also intuitively justified: Computation starts in a state where formula_25 is true, which means simply that formula_21 is true. The computation adds 1 to formula_27, which means that formula_23 is still true (for integer x).\n\nUnder this premise, the rule for codice_17 loops permits the following conclusion:\n\nHowever, the post-condition formula_30 (formula_27 is less than or equal to 10, but it is not less than 10) is logically equivalent to formula_32, which is what we wanted to show.\n\nThe property formula_33 is another invariant of the example loop, and the trivial property formula_34 is another one.\nApplying the above inference rule to the former invariant yields formula_35.\nApplying it to invariant formula_34 yields formula_37, which is slightly more expressive.\n\nThe Eiffel programming language provides native support for loop invariants. A loop invariant is expressed with the same syntax used for a class invariant. In the sample below, the loop invariant expression codice_21 must be true following the loop initialization, and after each execution of the loop body; this is checked at runtime.\n\nThe Whiley programming language also provides first-class support for loop invariants. Loop invariants are expressed using one or more codice_22 clauses, as the following illustrates:\nfunction max(int[] items) -> (int r)\n// Requires at least one element to compute max\nrequires |items| > 0\n// (1) Result is not smaller than any element\n// (2) Result matches at least one element\nensures some { i in 0..|items| | items[i] == r }:\nThe codice_1 function determines the largest element in an integer array. For this to be defined, the array must contain at least one element. The postconditions of codice_1 require that the returned value is: (1) not smaller than any element; and, (2) that it matches at least one element. The loop invariant is defined inductively through two codice_22 clauses, each of which corresponds to a clause in the postcondition. The fundamental difference is that each clause of the loop invariant identifies the result as being correct up to the current element codice_15, whilst the postconditions identify the result as being correct for all elements.\n\nA loop invariant can serve one of the following purposes:\n\nFor 1., a natural language comment (like codice_27 in the above example) is sufficient.\n\nFor 2., programming language support is required, such as the C library assert.h, or the above-shown codice_28 clause in Eiffel. Often, run-time checking can be switched on (for debugging runs) and off (for production runs) by a compiler or a runtime option.\n\nFor 3., some tools exist to support mathematical proofs, usually based on the above-shown Floyd–Hoare rule, that a given loop code in fact satisfies a given (set of) loop invariant(s).\n\nThe technique of abstract interpretation can be used to detect loop invariant of given code automatically. However, this approach is limited to very simple invariants (such as codice_29).\n\nA loop invariant (loop-invariant \"property\") is to be distinguished from loop-invariant \"code\"; note \"loop invariant\" (noun) versus \"loop-invariant\" (adjective).\nLoop-invariant code consists of statements or expressions that can be moved outside the body of a loop without affecting the semantics of a program; such transformations, called loop-invariant code motion, are performed by some compilers to optimize programs.\nA loop-invariant code example (in the C programming language) is\n\nwhere the calculations codice_30 and codice_31 can be moved before the loop, resulting in an equivalent, but faster, program:\n\nIn contrast, e.g. the property codice_32 is a loop invariant for both the original and the optimized program, but is not part of the code, hence it doesn't make sense to speak of \"moving it out of the loop\".\n\nLoop-invariant code may induce a corresponding loop-invariant property. For the above example, the easiest way to see it is to consider a program where the loop invariant code is computed both before and within the loop:\n\nA loop-invariant property of this code is codice_33, indicating that the values computed before the loop agree with those computed within (except before the first iteration).\n\n\n"}
{"id": "1024328", "url": "https://en.wikipedia.org/wiki?curid=1024328", "title": "Lwów School of Mathematics", "text": "Lwów School of Mathematics\n\nThe Lwów school of mathematics () was a group of Polish mathematicians who worked between the two World Wars in Lwów, Poland (since 1945 Lviv, Ukraine). The mathematicians often met at the famous Scottish Café to discuss mathematical problems, and published in the journal \"Studia Mathematica\", founded in 1929. The school was renowned for its productivity and its extensive contributions to subjects such as point-set topology, set theory and functional analysis. The biographies and contributions of these mathematicians were documented in 1980 by their contemporary Kazimierz Kuratowski in his book \"A Half Century of Polish Mathematics: Remembrances and Reflections\".\n\nMany of the mathematicians, especially those of Jewish background, fled this southeastern part of Poland in 1941 when it became clear that it would be invaded by Germany. Few of the mathematicians survived World War II, but after the war a group including some of the original community carried on their work in western Poland's Wrocław, the successor city to prewar Lwów; see Polish population transfers (1944–1946). A number of the prewar mathematicians, prominent among them Stanisław Ulam, became famous for work done in the West.\n\nNotable members of the Lwów school of mathematics included:\n\n"}
{"id": "2611629", "url": "https://en.wikipedia.org/wiki?curid=2611629", "title": "Myhill isomorphism theorem", "text": "Myhill isomorphism theorem\n\nIn computability theory the Myhill isomorphism theorem, named after John Myhill, provides a characterization for two numberings to induce the same notion of computability on a set.\n\nSets \"A\" and \"B\" of natural numbers are said to be recursively isomorphic if there is a total computable bijection \"f\" from the set of natural numbers to itself such that \"f\"(\"A\") = \"B\". \n\nA set \"A\" of natural numbers is said to be one-one reducible to a set \"B\" if there is a total computable injection \"f\" on the natural numbers such that formula_1 and formula_2.\n\nMyhill's isomorphism theorem states that two sets \"A\" and \"B\" of natural numbers are recursively isomorphic if and only if \"A\" is one-reducible to \"B\" and \"B\" is one-reducible to \"A\".\n\nThe theorem is reminiscent of the Schroeder–Bernstein theorem. The proof is different, however. The proof of Schroeder-Bernstein uses the inverses of the two injections, which is impossible in the setting of the Myhill theorem since these inverses might not be recursive. The proof of the Myhill theorem, on the other hand, defines the bijection inductively, which is impossible in the setting of Schroeder-Bernstein unless one uses the Axiom of Choice (which is not necessary for the proof).\n\nA corollary of Myhill's theorem is that two total numberings are one-equivalent if and only if they are computably isomorphic.\n\n"}
{"id": "3603035", "url": "https://en.wikipedia.org/wiki?curid=3603035", "title": "Nielsen theory", "text": "Nielsen theory\n\nNielsen theory is a branch of mathematical research with its origins in topological fixed point theory. Its central ideas were developed by Danish mathematician Jakob Nielsen, and bear his name.\n\nThe theory developed in the study of the so-called \"minimal number\" of a map \"f\" from a compact space to itself, denoted \"MF\"[\"f\"]. This is defined as:\nwhere \"~\" indicates homotopy of mappings, and #Fix(\"g\") indicates the number of fixed points of \"g\". The minimal number was very difficult to compute in Nielsen's time, and remains so today. Nielsen's approach is to group the fixed point set into classes, which are judged \"essential\" or \"nonessential\" according to whether or not they can be \"removed\" by a homotopy.\n\nNielsen's original formulation is equivalent to the following:\nWe define an equivalence relation on the set of fixed points of a self-map \"f\" on a space \"X\". We say that \"x\" is equivalent to \"y\" if and only if there exists a path \"c\" from \"x\" to \"y\" with \"f\"(\"c\") homotopic to \"c\" as paths. The equivalence classes with respect to this relation are called the Nielsen classes of \"f\", and the Nielsen number \"N\"(\"f\") is defined as the number of Nielsen classes having non-zero fixed point index sum.\n\nNielsen proved that\nmaking his invariant a good tool for estimating the much more difficult \"MF\"[\"f\"]. This leads immediately to what is now known as the Nielsen fixed point theorem: \"Any map f has at least N(f) fixed points.\"\n\nBecause of its definition in terms of the fixed point index, the Nielsen number is closely related to the Lefschetz number. Indeed, shortly after Nielsen's initial work, the two invariants were combined into a single \"generalized Lefschetz number\" (more recently called the Reidemeister trace) by Wecken and Reidemeister.\n\n"}
{"id": "28210505", "url": "https://en.wikipedia.org/wiki?curid=28210505", "title": "Padding argument", "text": "Padding argument\n\nIn computational complexity theory, the padding argument is a tool to conditionally prove that if some complexity classes are equal, then some other bigger classes are also equal.\n\nThe proof that P = NP implies EXP = NEXP uses \"padding\". formula_1 by definition, so it suffices to show formula_2. \n\nLet \"L\" be a language in NEXP. Since \"L\" is in NEXP, there is a non-deterministic Turing machine \"M\" that decides \"L\" in time formula_3 for some constant \"c\". Let \n\nwhere \"1\" is a symbol not occurring in \"L\". First we show that formula_5 is in NP, then we will use the deterministic polynomial time machine given by P = NP to show that \"L\" is in EXP.\n\nformula_5 can be decided in non-deterministic polynomial time as follows. Given input formula_7, verify that it has the form formula_8 and reject if it does not. If it has the correct form, simulate \"M(x)\". The simulation takes non-deterministic formula_9 time, which is polynomial in the size of the input, formula_7. So, formula_5 is in NP. By the assumption P = NP, there is also a deterministic machine \"DM\" that decides formula_5 in polynomial time. We can then decide \"L\" in deterministic exponential time as follows. Given input formula_13, simulate formula_14. This takes only exponential time in the size of the input, formula_13. \n\nThe formula_16 is called the \"padding\" of the language \"L\". This type of argument is also sometimes used for space complexity classes, alternating classes, and bounded alternating classes.\n"}
{"id": "1214276", "url": "https://en.wikipedia.org/wiki?curid=1214276", "title": "Paul Ernest", "text": "Paul Ernest\n\nPaul Ernest is a contributor to the social constructivist philosophy of mathematics. Ernest's philosophical sources are the later works of Ludwig Wittgenstein and the fallibilism of Imre Lakatos. This social constructivist philosophy claims that both the theorems and truths of mathematics, and the objects of mathematics, are cultural products created by humans. Furthermore, the theorems and truths of mathematics always remain corrigible, revisible, and indeed fallible — in principle at least. This does not mean that mathematical knowledge is flawed or at risk. However, the claim is that the belief that mathematical knowledge is infallible cannot be demonstrated, it is an article of faith, even if the warrants for mathematical knowledge are the strongest warrants available to humankind for any knowledge claims. Ernest illustrates this position in his discussion of the issue of whether mathematics is discovered or invented. His fullest exposition of the social constructivist position is given in the 1998 reference, although an earlier version is given in the 1991 reference. Ernest's version of social constructivism is controversial and has led to strong criticism. The principal criticism is that mathematical theorems are truths and truths by their nature are infallible.\n\nIn his account of social constructivism Ernest links the worlds of research mathematics and that of school and college mathematics. This link is achieved though the role of experts who as teachers communicate mathematical knowledge to learners and warrant their personal knowledge by means of testing and assessment. As researchers experts both create new mathematical knowledge and warrant the productions of others. Through this linkage the personal knowledge of the experts is developed and itself warranted. Both explicit mathematical knowledge representations and personal mathematical knowledge circulate between the worlds of education and research, which are not themselves wholly disjoint. The personal knowledge cycle is mutually refreshing for both education and research. A criticism of this account is that if mathematical knowledge is socially constructed and accepted it might be accepted purely on the basis of group agreement. However Ernest argues that mathematical knowledge communication, creation and warranting take place in historical communities that respect traditions of mathematical practice with embedded and partly tacit criteria for acceptability. Such rules include accepted forms of presentation, reasoning, and consistency. Although these are historically contingent they are never arbitrary and in general conserve mathematical concepts, theories and rules of acceptance. Furthermore, the democratic, rational and critical elements of mathematical thinking and mathematical communities mean that errors are eliminated. A criticism of this position is that it conflates the social institution of mathematics with objective mathematical knowledge.\n\nPaul Ernest was born in New York City, New York in to parents John Ernest and Elna Ernest (née Adlerbert). However he has lived and worked in the UK since childhood, apart from two years of teaching at the University of the West Indies, Jamaica (1982–84). He is currently emeritus professor of the philosophy of mathematics education at Exeter University, UK. Originally a student of mathematics and philosophy up to PhD level he became interested in educational issues through teaching school mathematics in London during the 1970s. His main research interests concern fundamental questions about the nature of mathematics and how it relates to teaching, learning and society. He has developed a semiotic theory of mathematics and education. He is best known for his work on philosophical aspects of mathematics education and his contributions to developing a social constructivist philosophy of mathematics. He is currently working on the ethics of mathematics.\n\n\n"}
{"id": "4477141", "url": "https://en.wikipedia.org/wiki?curid=4477141", "title": "Propositional directed acyclic graph", "text": "Propositional directed acyclic graph\n\nA propositional directed acyclic graph (PDAG) is a data structure that is used to represent a Boolean function. A Boolean function can be represented as a rooted, directed acyclic graph of the following form:\n\nLeaves labeled with formula_1 (formula_2) represent the constant Boolean function which always evaluates to 1 (0). A leaf labeled with a Boolean variable formula_11 is interpreted as the assignment formula_12, i.e. it represents the Boolean function which evaluates to 1 if and only if formula_12. The Boolean function represented by a formula_3-node is the one that evaluates to 1, if and only if the Boolean function of all its children evaluate to 1. Similarly, a formula_4-node represents the Boolean function that evaluates to 1, if and only if the Boolean function of at least one child evaluates to 1. Finally, a formula_5-node represents the complemenatary Boolean function its child, i.e. the one that evaluates to 1, if and only if the Boolean function of its child evaluates to 0.\n\nEvery binary decision diagram (BDD) and every negation normal form (NNF) are also a PDAG with some particular properties. The following pictures represent the Boolean function formula_17:\n\n\n"}
{"id": "35301443", "url": "https://en.wikipedia.org/wiki?curid=35301443", "title": "Reye configuration", "text": "Reye configuration\n\nIn mathematics, the Reye configuration, introduced by , is a configuration of 12 points and 16 lines.\nEach point of the configuration belongs to four lines, and each line contains three points. Therefore, in the notation of configurations, the Reye configuration is written as 1216.\n\nThe Reye configuration can be realized in three-dimensional projective space by taking the lines to be the 12 edges and four long diagonals of a cube, and the points as the eight vertices of the cube, its center, and the three points where groups of four parallel cube edges meet the plane at infinity. Two regular tetrahedra may be inscribed within a cube, forming a stella octangula; these two tetrahedra are perspective figures to each other in four different ways, and the other four points of the configuration are their centers of perspectivity. These two tetrahedra together with the tetrahedron of the remaining 4 points form a desmic system of three tetrahedra.\n\nAny two disjoint spheres in three dimensional space, with different radii, have two bitangent double cones, the apexes of which are called the centers of similitude. If three spheres are given, with their centers non-collinear, then their six centers of similitude form the six points of a complete quadrilateral, the four lines of which are called the axes of similitude. And if four spheres are given, with their centers non-coplanar, then they determine 12 centers of similitude and 16 axes of similitude, which together form an instance of the Reye configuration .\n\nThe Reye configuration can also be realized by points and lines in the Euclidean plane, by drawing the three-dimensional configuration in three-point perspective. An 812 configuration of eight points in the real projective plane and 12 lines connecting them, with the connection pattern of a cube, can be extended to form the Reye configuration if and only if the eight points are a perspective projection of a parallelepiped \nThe 24 permutations of the points formula_1\nform the vertices of a 24-cell centered at the origin of four-dimensional Euclidean space.\nThese 24 points also form the 24 roots in the root system formula_2.\nThey can be grouped into pairs of points opposite each other on a line through the origin. The lines and planes through the origin of four-dimensional Euclidean space have the geometry of the points and lines of three-dimensional projective space, and in this three-dimensional projective space the lines through opposite pairs of these 24 points and the central planes through these points become the points and lines of the Reye configuration . The permutations of formula_1 form the homogeneous coordinates of the 12 points in this configuration.\n\n pointed out that the Reye configuration underlies some of the proofs of the Bell–Kochen–Specker theorem about the non-existence of hidden variables in quantum mechanics.\n\nThe Pappus configuration may be formed from two triangles that are perspective figures to each other in three different ways, analogous to the interpretation of the Reye configuration involving desmic tetrahedra.\n\nIf the Reye configuration is formed from a cube in three-dimensional space, then there are 12 planes containing four lines each: the six face planes of the cube, and the six planes through pairs of opposite edges of the cube. Intersecting these 12 planes and 16 lines with another plane in general position produces a 1612 configuration, the dual of the Reye configuration. The original Reye configuration and its dual together form a 2828 configuration .\n\nThere are 574 distinct configurations of type 1216 .\n\n"}
{"id": "38353616", "url": "https://en.wikipedia.org/wiki?curid=38353616", "title": "Rhombitrioctagonal tiling", "text": "Rhombitrioctagonal tiling\n\nIn geometry, the rhombitrioctagonal tiling is a semiregular tiling of the \nhyperbolic plane. At each vertex of the tiling there is one triangle and one octagon, alternating between two squares. The tiling has Schläfli symbol rr{8,3}. It can be seen as constructed as a rectified trioctagonal tiling, r{8,3}, as well as an expanded octagonal tiling or expanded order-8 triangular tiling.\n\nThis tiling has [8,3], (*832) symmetry. There is only one uniform coloring.\n\nSimilar to the Euclidean rhombitrihexagonal tiling, by edge-coloring there is a half symmetry form (3*4) orbifold notation. The octagons can be considered as truncated squares, t{4} with two types of edges. It has Coxeter diagram , Schläfli symbol s{3,8}. The squares can be distorted into isosceles trapezoids. In the limit, where the rectangles degenerate into edges, an order-8 triangular tiling results, constructed as an snub tritetratrigonal tiling, .\n\nFrom a Wythoff construction there are ten hyperbolic uniform tilings that can be based from the regular octagonal tiling.\n\nDrawing the tiles colored as red on the original faces, yellow at the original vertices, and blue along the original edges, there are 8 forms.\nThis tiling is topologically related as a part of sequence of cantellated polyhedra with vertex figure (3.4.n.4), and continues as tilings of the hyperbolic plane. These vertex-transitive figures have (*n32) reflectional symmetry.\n\n\n\n"}
{"id": "9237855", "url": "https://en.wikipedia.org/wiki?curid=9237855", "title": "Singapore math", "text": "Singapore math\n\nSingapore math (or Singapore maths in British English) is a teaching method based on the national mathematics curriculum used for kindergarten through sixth grade in Singapore. The term was coined in the United States to describe an approach originally developed in Singapore to teach students to learn and master fewer mathematical concepts at greater detail as well as having them learn these concepts using a three-step learning process: concrete, pictorial, and abstract. In the concrete step, students engage in hands-on learning experiences using concrete objects such as chips, dice, or paper clips. This is followed by drawing pictorial representations of mathematical concepts. Students then solve mathematical problems in an abstract way by using numbers and symbols.\n\nThe development of Singapore math began in the 1980s when the country's Ministry of Education developed its own mathematics textbooks that focused on problem solving and heuristic model drawing. Outside Singapore, these textbooks were adopted by several schools in the United States (U.S.) and in other countries such as Canada, Israel, and the United Kingdom. Early adopters of these textbooks in the U.S. included parents interested in homeschooling as well as a limited number of schools. These textbooks became more popular since the release of scores from the Trends in International Mathematics and Science Study (TIMSS), which showed Singapore at the top of the world four times in fourth and eighth grade mathematics. U.S. editions of these textbooks have since been adopted by a large number of school districts as well as charter and private schools.\n\nBefore the development of its own mathematics textbooks in the 1980s, Singapore imported its mathematics textbooks from other countries. In 1981, the Curriculum Development Institute of Singapore (CDIS) (currently the Curriculum Planning and Development Division) began to develop its own mathematics textbooks and curriculum. The CDIS developed and distributed a textbook series for elementary schools in Singapore called \"Primary Mathematics\", which was first published in 1982 and subsequently revised in 1992 to emphasize problem solving. In the late 1990s, the country's Ministry of Education opened the elementary school textbook market to private companies, and Marshall Cavendish, a local and private publisher of educational materials, began to publish and market the \"Primary Mathematics\" textbooks.\n\nFollowing Singapore's curricular and instructional initiatives, dramatic improvements in math proficiency among Singaporean students on international assessments were observed. TIMSS, an international assessment for math and science among fourth and eighth graders, ranked Singapore's fourth and eighth grade students first in mathematics four times (1995, 1999, 2003, and 2015) among participating nations. Likewise, the Organisation for Economic Co-operation and Development (OECD)'s Programme for International Student Assessment (PISA), a worldwide study of 15-year-old school students' scholastic performance in mathematics, science, and reading, has ranked Singaporean students first in 2015, and second after Shanghai, China in 2009 and 2012.\n\nSince the TIMSS publication of Singapore's high ranking in mathematics, professional mathematicians in the U.S. took a closer look at Singapore mathematics textbooks such as \"Primary Mathematics\". The term \"Singapore math\" was originally coined in the U.S. to describe the teaching approach based on these textbooks. In 2005, the American Institutes for Research (AIR) published a study, which concluded that U.S. schools could benefit from adopting these textbooks. The textbooks were already distributed in the U.S. by Singapore Math, Inc., a private venture based in Oregon. Early users of these textbooks in the U.S. included parents interested in homeschooling as well as a limited number of schools. They became more popular since the release of the TIMSS scores showing Singapore's top ranking. As of 2004, U.S. versions of Singapore mathematics textbooks were adopted in over 200 U.S. schools. Schools and counties that had adopted these textbooks reported improvements in their students' performance. Singapore math textbooks were also used in schools from other countries such as Canada, Israel, and the United Kingdom.\n\nCompared to a traditional U.S. math curriculum, Singapore math focuses on fewer topics but covers them in greater detail. Each semester-level Singapore math textbook builds upon prior knowledge and skills, with students mastering them before moving on to the next grade. Students, therefore, need not re-learn these skills at the next grade level. By the end of sixth grade, Singapore math students have mastered multiplication and division of fractions and can solve difficult multi-step word problems.\n\nIn the U.S., it was found that Singapore math emphasizes the essential math skills recommended in the 2006 Focal Points publication by the National Council of Teachers of Mathematics (NCTM), the 2008 final report by the National Mathematics Advisory Panel, and the proposed Common Core State Standards, though it generally progresses to topics at an earlier grade level compared to U.S. standards.\n\nSingapore math teaches students mathematical concepts in a three-step learning process: concrete, pictorial, and abstract. This learning process was based on the work of an American psychologist, Jerome Bruner. In the 1960s, Bruner found that people learn in three stages by first handling real objects before transitioning to pictures and then to symbols. The Singapore government later adapted this approach to their math curriculum in the 1980s.\n\nThe first of the three steps is concrete, wherein students learn while handling objects such as chips, dice, or paper clips. Students learn to count these objects (e.g., paper clips) by physically lining them up in a row. They then learn basic arithmetic operations such as addition or subtraction by physically adding or removing the objects from each row.\n\nStudents then transition to the pictorial step by drawing diagrams called \"bar-models\" to represent specific quantities of an object. This involves drawing a rectangular bar to represent a specific quantity. For instance, if a short bar represents five paper clips, a bar that is twice as long would represent ten. By visualizing the difference between the two bars, students learn to solve problems of addition by adding one bar to the other, which will, in this instance, produce an answer of fifteen paper clips. They can use this method to solve other mathematical problems involving subtraction, multiplication, and division. Bar modeling is far more efficient than the \"guess-and-check\" approach, in which students simply guess combinations of numbers until they stumble onto the solution.\n\nOnce students have learned to solve mathematical problems using bar modeling, they begin to solve mathematical problems with exclusively abstract tools: numbers and symbols.\n\nBar modeling is a pictorial method used to solve word problems in arithmetic. These bar models can come in multiple forms such as a whole-part or a comparison model.\n\nWith the whole-part model, students would draw a rectangular bar to represent a \"whole\" larger quantity, which can be subdivided into two or more \"parts.\" A student could be exposed to a word problem involving addition such as:\n\nThe solution to this problem could be solved by drawing one bar and dividing it into two parts, with the longer part as 70 and the shorter part as 30. By visualizing these two parts, students would simply solve the above word problem by adding both parts together to build a whole bar of 100. Conversely, a student could use whole-part model to solve a subtraction problem such as 100 - 70, by having the longer part be 70 and the whole bar be 100. They would then solve the problem by inferring the shorter part to be 30.\n\nThe whole-part model can also be used to solve problems involving multiplication or division. A multiplication problem could be presented as follows:\n\nThe student could solve this multiplication problem by drawing one bar to represent the unknown answer, and subdivide that bar into four equal parts, with each part representing $30. Based on the drawn model, the student could then visualize this problem as providing a solution of $120.\n\nUnlike the whole-part model, a comparison model involves comparing two bars of unequal lengths. It can be used to solve a subtraction problem such as the following:\n\nBy using the comparison model, the student would draw one long bar to represent 100 and another shorter bar to represent 70. By comparing these two bars, students could then solve for the difference between the two numbers, which in this case is 30 miles. Like the whole-part model, the comparison model can also be used to solve word problems involving addition, multiplication, and division.\n\n\n"}
{"id": "2383687", "url": "https://en.wikipedia.org/wiki?curid=2383687", "title": "Stokes phenomenon", "text": "Stokes phenomenon\n\nIn complex analysis the Stokes phenomenon, discovered by , is that the asymptotic behavior of functions can differ in different regions of the complex plane. These regions are bounded by Stokes line or anti-Stokes lines.\n\nSomewhat confusingly, mathematicians and physicists use the terms \"Stokes line\" and \"anti-Stokes line\" in opposite ways. The lines originally studied by Stokes are what some mathematicians call anti-Stokes lines and what physicists call Stokes lines. (These terms are also used in optics for the unrelated Stokes lines and anti-Stokes lines in Raman scattering). This article uses the physicist's convention, which is historically more accurate and seems to be becoming more common among mathematicians. recommends the term \"principal curve\" for (physicist's) anti-Stokes lines.\n\nInformally the anti-Stokes lines are roughly where some term in the asymptotic expansion changes from increasing to decreasing (and therefore can exhibit a purely oscillatory behavior), and the Stokes lines are lines along which some term approaches infinity or zero fastest. Anti-Stokes lines bound regions where the function exhibit a particular asymptotic behavior. The Stokes lines and anti-Stokes lines are not unique and do not really have a precise definition in general, because the region where a function has a given asymptotic behavior is a somewhat vague concept. However the lines do usually have well determined directions at essential singularities of the function, and there is sometimes a natural choice of these lines as follows. The asymptotic expansion of a function is often given by a linear combination of functions of the form \"f\"(\"z\")e for functions \"f\" and \"g\". The Stokes lines can then be taken as the zeros of the imaginary part of \"g\", and the anti-Stokes lines as the zeros of the real part of \"g\". (This is not quite canonical, because one can add a constant to \"g\", changing the lines.) If the lines are defined like this then they are orthogonal where they meet, unless \"g\" has a multiple zero.\n\nAs a trivial example, the function sinh(\"z\") has two regions Re(\"z\") > 0 and Re(\"z\") < 0 where it is asymptotic to e/2 and −e/2. So the anti-Stokes line can be taken to be the imaginary axis, and the Stokes line can be taken to be the real axis. One could equally well take the Stokes line to be any line of given imaginary part; these choices differ only by a vertical shift, showing that there is no canonical choice for the Stokes line.\n\nThe Airy function Ai(\"x\") is one of two solutions to a simple differential equation\n\nwhich it is often useful to approximate for many values of \"x\" – including complex values. For large \"x\" of given argument the solution can be approximated by a linear combination of the functions\nHowever the linear combination has to change as the argument of \"x\" passes certain values because these approximations are multi-valued functions but the Airy function is single valued.\nFor example, if we regard the limit of \"x\" as large and real, and would like to approximate the Airy function for both positive and negative values, we would find that\nwhich are two very different expressions. What has happened is that as we have increased the argument of \"x\" from 0 to pi (rotating it around through the upper half complex plane) we have crossed an anti-Stokes line, which in this case is at formula_4. At this anti-Stokes line, the coefficient of formula_5 is forced to jump. The coefficient of formula_6 can jump at this line but is not forced to; it can change gradually as arg \"x\" varies from π/3 to π as it is not determined in this region.\n\nThere are three anti-Stokes lines with arguments π/3, π. –π/3, and three Stokes lines with arguments 2π/3, 0. –2π/3.\n\nThe Airy function example can be generalized to a broad class of second order linear differential equations as follows. By standard changes of variables, a second order equation can often be changed to one of the form\n\nwhere \"f\" is holomorphic in a simply-connected region and \"w\" is a solution of the differential equation. Then in some cases the WKB method gives an asymptotic approximation for \"w\" as a linear combination of functions of the form\n\nfor some constant \"a\". (Choosing different values of \"a\" is equivalent to choosing different coefficients in the linear combination.) The anti-Stokes lines and Stokes lines are then the zeros of the real and imaginary parts, respectively, of\n\nIf \"a\" is a simple zero of \"f\" then locally \"f\" looks like formula_10. Solutions will locally behave like the Airy functions; they will have three Stokes lines and three anti-Stokes lines meeting at \"a\".\n\n"}
{"id": "14385549", "url": "https://en.wikipedia.org/wiki?curid=14385549", "title": "Strongly minimal theory", "text": "Strongly minimal theory\n\nIn model theory—a branch of mathematical logic—a minimal structure is an infinite one-sorted structure such that every subset of its domain that is definable with parameters is either finite or cofinite. A strongly minimal theory is a complete theory all models of which are minimal. A strongly minimal structure is a structure whose theory is strongly minimal.\n\nThus a structure is minimal only if the parametrically definable subsets of its domain cannot be avoided, because they are already parametrically definable in the pure language of equality.\nStrong minimality was one of the early notions in the new field of classification theory and stability theory that was opened up by Morley's theorem on totally categorical structures.\n\nThe nontrivial standard examples for strongly minimal theories are the one-sorted theories of infinite-dimensional vector spaces, and the theories ACF of algebraically closed fields. As the example ACF shows, the parametrically definable subsets of the square of the domain of a minimal structure can be relatively complicated (\"curves\").\n\nMore generally, a subset of a structure that is defined as the set of realizations of a formula φ(\"x\") is called a minimal set if every parametrically definable subset of it is either finite or cofinite. It is called a strongly minimal set if this is true even in all elementary extensions.\n\nA strongly minimal set, equipped with the closure operator given by algebraic closure in the model-theoretic sense, is an infinite matroid, or pregeometry. A model of a strongly minimal theory is determined up to isomorphism by its dimension as a matroid. Totally categorical theories are controlled by a strongly minimal set; this fact explains (and is used in the proof of) Morley's theorem. Boris Zilber conjectured that the only pregeometries that can arise from strongly minimal sets are those that arise in vector spaces, projective spaces, or algebraically closed fields. This conjecture was refuted by Ehud Hrushovski, who developed a method known as the \"Hrushovski construction\" to build new strongly minimal structures from finite structures.\n\n"}
{"id": "10165595", "url": "https://en.wikipedia.org/wiki?curid=10165595", "title": "Sturm separation theorem", "text": "Sturm separation theorem\n\nIn mathematics, in the field of ordinary differential equations, Sturm separation theorem, named after Jacques Charles François Sturm, describes the location of roots of solutions of homogeneous second order linear differential equations. Basically the theorem states that given two linear independent solutions of such an equation the zeros of the two solutions are alternating.\n\nGiven a homogeneous second order linear differential equation and two continuous linear independent solutions \"u\"(\"x\") and \"v\"(\"x\") with \"x\" and \"x\" successive roots of \"u\"(\"x\"), then \"v\"(\"x\") has exactly one root in the open interval ]\"x\", \"x\"[. It is a special case of the Sturm-Picone comparison theorem.\n\nSince formula_1 and formula_2 are linearly independent it follows that the Wronskian formula_3 must satisfy formula_4 for all formula_5 where the differential equation is defined, say formula_6. Without loss of generality, suppose that formula_7. Then\nSo at formula_9\nand either formula_11 and formula_12 are both positive or both negative. Without loss of generality, suppose that they are both positive. Now, at formula_13 \nand since formula_9 and formula_13 are successive zeros of formula_17 it causes formula_18. Thus, to keep formula_19 we must have formula_20. We see this by observing that if formula_21 then formula_17 would be increasing (away from the formula_5-axis), which would never lead to a zero at formula_13. So for a zero to occur at formula_13 at most formula_26 (i.e., formula_27 and it turns out, by our result from the Wronskian that formula_27). So somewhere in the interval formula_29 the sign of formula_30 changed. By the Intermediate Value Theorem there exists formula_31 such that formula_32.\n\nOn the other hand, there can be only one zero in formula_29, because otherwise v would have two zeros and there would be no zeros of u in between, and it was just proved that this is impossible.\n"}
{"id": "40703633", "url": "https://en.wikipedia.org/wiki?curid=40703633", "title": "Taj Haider", "text": "Taj Haider\n\nTaj Haider, SI (Urdu: تاج حيدر; born 8 March 1942) is a notable left-wing politician, nationalist, playwright, mathematician, versatile scholar, and Marxist intellectual. He is one of the founding members of Pakistan Peoples Party (PPP) and has been the general-secretary of the PPP since 2010.\n\nA mathematician and scientist by profession, Haider provided a vital leadership in the formative years of clandestine atomic bomb projects in the 1970s. He is also noted for his writing of political plays for the Pakistan Television (PTV) from 1979 to 1985.\n\nTaj Haider was born on 8 March 1942 in Kota, Rajasthan, British Indian Empire, to an educated and enlightened family; his family briefly migrated to Pakistan following the partition of India in 1947. After graduating from a local high school, Haider ultimately enrolled in Karachi University in 1959. He studied Mathematics at the Karachi University and graduated with a BSc (hons) in Mathematics in 1962.\n\nIn 1965, he earned his MSc in mathematics from the same institution and opted for teaching mathematics at the local college, later moving to Karachi University. During his career at the Karachi University, Haider's primary taught and focused on the ordinary differential equations and topics in multivariable calculus.\n\nDuring the attendance of 1967 socialist convention, Haider was among one of the founding members of the Pakistan Peoples Party (PPP) and committed himself as a vehement support of change left-oriented philosophy of Zulfiqar Ali Bhutto. In the 1970s, he played a vital role in formulating the public policy concerning the atomic bomb projects. On multiple occasions, he provided his expertise on taking moral stance on nuclear weapons initiatives at the diplomatic conventions. On nuclear weapons development, Haider stated that \"there was a need to aggressively project the peaceful intent of Pakistan's atomic bomb program.\n\nHaider disassociated himself with the politics but remain member of Pakistan Mathematical Society and shifted towards writing political dramas at the Pakistan Television (PTV) in 1979. The PTV on-aired various political dramas written by Haider until 1985 when he renewed his association with PPP. In 1990–2000, he contributed in PPP-initiated industrial projects such as the establishment of Heavy Mechanical Complex (HMC), Hub Dam and various other social programmes. In 2001, Haider returned to his literary activities after rejoining the PTV, and penned two political drama serials for the PTV which went aired-on in 2003. In 2004, he returned to politics in opposition to President Pervez Musharraf over the issue of nuclear proliferation. He bitterly criticised the United States over the sanctions of KRL and one of the noted politician expressing the discontent against the US, along with Raza Rabbani in 2004. About the nuclear proliferation case, Haider defended the case of Abdul Qadeer Khan in the public and condemned the Information minister, Rashid Ahmad's statement of acquitting former Prime minister Benazir Bhutto in the nuclear proliferation case.\n\nUltimately, he called for a parliamentary inquiry over on that issues, and questioned about the involvement of President General Pervez Musharraf in the proliferation case. In 2006, Haider was awarded PTV Awards for Best Playwright Serial award, which he received in a televised ceremony.\n\nIn 2012, Haider was awarded with Sitara-e-Imtiaz for his services to nation's atomic bomb projects and his public policy efforts, which was conferred to him by the President Asif Zardari. In 2013, he was appointed as the \"media coordinator\" for the provincial government of Sindh.\nPakistan Peoples Party leader and provincial secretary-general Taj Haider is likely to be elected unopposed to the Senate on the seat vacated by Dr Abdul Hafeez Shaikh. The Senate election is scheduled for 5 June and the last date to submit nominations is 22 May, According to provincial election commissioner, if no other candidate submits nomination forms then Taj Haider will be elected unopposed.\n\nHaider extensively writes on nuclear policy issues, left-wing ideas, literary and political philosophy. His recent writings have included the support of social democracy in the country and power of balance in each state institutions. On literary and political circles, he has written critic articles against the military dictatorship, specifically policies enforced by the conservative President General Zia-ul-Haq throughout the 1980s.\n\n\n\n\n\n\n"}
{"id": "2995611", "url": "https://en.wikipedia.org/wiki?curid=2995611", "title": "Theta correspondence", "text": "Theta correspondence\n\nIn mathematics, the theta correspondence or Howe correspondence is a correspondence between automorphic forms associated to the two groups of a dual reductive pair, introduced by as a generalisation of the Shimura correspondence. It is a conjectural correspondence between certain representations on the metaplectic group formula_1 and those on the special orthogonal group formula_2. The case formula_3 was constructed by Jean-Loup Waldspurger in and .\n\nLet formula_4 be a non-archimedean local field of characteristic not formula_5, with its quotient field of characteristic formula_6. Let formula_7 be a quadratic extension over formula_4. Let formula_9 (respectively formula_10) be an formula_11-dimensional Hermitian space (respectively an formula_12-dimensional Hermitian space) over formula_7. We assume further formula_14 (resp. formula_15) to be the isometry group of formula_9 (resp. formula_10). There exists a Weil representation associated to a non-trivial additive character formula_18 of formula_7 for the pair formula_20, which we write as formula_21. Let formula_22 be an irreducible admissible representation of formula_14. Here, we only consider the case formula_24 or formula_25. We can find a certain representation formula_26 of formula_15, which is in fact a certain quotient of the Weil representation formula_21 by formula_22.\n\n(i) formula_30 is irreducible or formula_31;\n\n(ii) Let formula_32 be two irreducible admissible representations of formula_14, such that formula_34. Then, formula_35.\n\nHere, we have used the notations of . The Howe conjecture in this setting was proved by J. L. Waldspurger in . W. T. Gan and S. Takeda gave an alternative simpler proof in .\n\nDefinition. (local theta correspondence). Let formula_36 (respectively formula_37) be the set of all irreducible admissible representations of formula_14 (respectively formula_15). Let formula_40 be the map formula_41, which associates every irreducible admissible representation formula_22 of formula_14 the irreducible admissible representation formula_26 of formula_15. We call formula_40 the local theta correspondence for the pair formula_20.\n\nComment. Here we can only define the theta correspondence locally, basically because the Weil representation used in our construction is only defined locally. The global theta lift can be defined on the cuspidal automorphic representations of formula_14 as well, see .\n\nLet formula_40 be the theta correspondence between formula_50 and formula_51. According to , one can associate to formula_40 a function formula_53, which can be proved to be a modular function of half integer weight, that is to say, formula_53 is a theta function.\n\n\n"}
{"id": "954323", "url": "https://en.wikipedia.org/wiki?curid=954323", "title": "Tikhonov regularization", "text": "Tikhonov regularization\n\nTikhonov regularization, named for Andrey Tikhonov, is the most commonly used method of regularization of ill-posed problems. In statistics, the method is known as ridge regression, in machine learning it is known as weight decay, and with multiple independent discoveries, it is also variously known as the Tikhonov–Miller method, the Phillips–Twomey method, the constrained linear inversion method, and the method of linear regularization. It is related to the Levenberg–Marquardt algorithm for non-linear least-squares problems.\n\nSuppose that for a known matrix formula_1 and vector formula_2, we wish to find a vector formula_3 such that\nThe standard approach is ordinary least squares linear regression. However, if no formula_3 satisfies the equation or more than one formula_3 does—that is, the solution is not unique—the problem is said to be ill posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations. Most real-world phenomena have the effect of low-pass filters in the forward direction where formula_1 maps formula_3 to formula_2. Therefore, in solving the inverse-problem, the inverse mapping operates as a high-pass filter that has the undesirable tendency of amplifying noise (eigenvalues / singular values are largest in the reverse mapping where they were smallest in the forward mapping). In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of formula_3 that is in the null-space of formula_1, rather than allowing for a model to be used as a prior for formula_3.\nOrdinary least squares seeks to minimize the sum of squared residuals, which can be compactly written as\nwhere formula_14 is the Euclidean norm.\n\nIn order to give preference to a particular solution with desirable properties, a regularization term can be included in this minimization:\nfor some suitably chosen Tikhonov matrix formula_16. In many cases, this matrix is chosen as a multiple of the identity matrix (formula_17), giving preference to solutions with smaller norms; this is known as regularization. In other cases, high-pass operators (e.g., a difference operator or a weighted Fourier operator) may be used to enforce smoothness if the underlying vector is believed to be mostly continuous.\nThis regularization improves the conditioning of the problem, thus enabling a direct numerical solution. An explicit solution, denoted by formula_18, is given by\nThe effect of regularization may be varied by the scale of matrix formula_20. For formula_21 this reduces to the unregularized least-squares solution, provided that (AA) exists.\n\nTikhonov regularization has been invented independently in many different contexts.\nIt became widely known from its application to integral equations from the work of\nAndrey Tikhonov and David L. Phillips. Some authors use the term Tikhonov–Phillips regularization.\nThe finite-dimensional case was expounded by Arthur E. Hoerl, who took a statistical approach, and by Manus Foster, who interpreted this method as a Wiener–Kolmogorov (Kriging) filter. Following Hoerl, it is known in the statistical literature as ridge regression.\n\nFor general multivariate normal distributions for formula_22 and the data error, one can apply a transformation of the variables to reduce to the case above. Equivalently, one can seek an formula_22 to minimize\n\nwhere we have used formula_25 to stand for the weighted norm formula_26 (compare with the Mahalanobis distance). In the Bayesian interpretation formula_27 is the inverse covariance matrix of formula_28, formula_29 is the expected value of formula_22, and formula_31 is the inverse covariance matrix of formula_22. The Tikhonov matrix is then given as a factorization of the matrix formula_33 (e.g. the Cholesky factorization) and is considered a whitening filter.\n\nThis generalized problem has an optimal solution formula_34 which can be solved explicitly using the formula\n\nor equivalently\n\nTypically discrete linear ill-conditioned problems result from discretization of integral equations, and one can formulate a Tikhonov regularization in the original infinite-dimensional context. In the above we can interpret formula_1 as a compact operator on Hilbert spaces, and formula_22 and formula_28 as elements in the domain and range of formula_1. The operator formula_41 is then a self-adjoint bounded invertible operator.\n\nWith formula_17, this least-squares solution can be analyzed in a special way using the singular-value decomposition. Given the singular value decomposition\n\nwith singular values formula_44, the Tikhonov regularized solution can be expressed as\n\nwhere formula_46 has diagonal values\n\nand is zero elsewhere. This demonstrates the effect of the Tikhonov parameter on the condition number of the regularized problem. For the generalized case, a similar representation can be derived using a generalized singular-value decomposition. \n\nFinally, it is related to the Wiener filter:\n\nwhere the Wiener weights are formula_49 and formula_50 is the rank of formula_1.\n\nThe optimal regularization parameter formula_52 is usually unknown and often in practical problems is determined by an \"ad hoc\" method. A possible approach relies on the Bayesian interpretation described below. Other approaches include the discrepancy principle, cross-validation, L-curve method, restricted maximum likelihood and unbiased predictive risk estimator. Grace Wahba proved that the optimal parameter, in the sense of leave-one-out cross-validation minimizes\n\nwhere formula_54 is the residual sum of squares, and formula_55 is the effective number of degrees of freedom.\n\nUsing the previous SVD decomposition, we can simplify the above expression:\n\nand\n\nThe probabilistic formulation of an inverse problem introduces (when all uncertainties are Gaussian) a covariance matrix formula_59 representing the \"a priori\" uncertainties on the model parameters, and a covariance matrix formula_60 representing the uncertainties on the observed parameters. In the special case when these two matrices are diagonal and isotropic, formula_61 and formula_62, and, in this case, the equations of inverse theory reduce to the equations above, with formula_63.\n\nAlthough at first the choice of the solution to this regularized problem may look artificial, and indeed the matrix formula_20 seems rather arbitrary, the process can be justified from a Bayesian point of view. Note that for an ill-posed problem one must necessarily introduce some additional assumptions in order to get a unique solution. Statistically, the prior probability distribution of formula_22 is sometimes taken to be a multivariate normal distribution. For simplicity here, the following assumptions are made: the means are zero; their components are independent; the components have the same standard deviation formula_66. The data are also subject to errors, and the errors in formula_28 are also assumed to be independent with zero mean and standard deviation formula_68. Under these assumptions the Tikhonov-regularized solution is the most probable solution given the data and the \"a priori\" distribution of formula_22, according to Bayes' theorem.\n\nIf the assumption of normality is replaced by assumptions of homoscedasticity and uncorrelatedness of errors, and if one still assumes zero mean, then the Gauss–Markov theorem entails that the solution is the minimal unbiased estimator. \n\n"}
{"id": "19537780", "url": "https://en.wikipedia.org/wiki?curid=19537780", "title": "Timeline of mathematics", "text": "Timeline of mathematics\n\nThis is a timeline of pure and applied mathematics history.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
