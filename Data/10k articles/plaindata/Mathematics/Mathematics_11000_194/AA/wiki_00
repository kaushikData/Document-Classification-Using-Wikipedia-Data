{"id": "825933", "url": "https://en.wikipedia.org/wiki?curid=825933", "title": "Abraham Gotthelf Kästner", "text": "Abraham Gotthelf Kästner\n\nAbraham Gotthelf Kästner was a German mathematician and epigrammatist.\n\nHe was known in his professional life for writing textbooks and compiling encyclopedias rather than for original research. Georg Christoph Lichtenberg was one of his doctoral students, and admired the man greatly. He became most well known for his epigrammatic poems. The crater Kästner on the Moon is named after him.\n\nKästner was the son of law professor Abraham Kästner. He married Anna Rosina Baumann in 1757 after a 12-year engagement. She died on 4 March 1758, less than a year later, of a lung disease. Later Kästner had a daughter Catharine with his cleaning lady.\n\nKästner studied law, philosophy, physics, mathematics and metaphysics in Leipzig from 1731, and was appointed a Notary in 1733. He gained his Habilitation from the University of Leipzig in 1739, and lectured there in mathematics, philosophy, logic and law, becoming an associate professor in 1746. In 1751 he was elected a member of the Royal Swedish Academy of Sciences. In 1756 he took up a position as full professor of natural philosophy and geometry at the University of Göttingen. In 1763, succeeding Tobias Mayer, he became director of the observatory as well. One of his doctoral students was the physicist and aphorist Georg Christoph Lichtenberg, who became a colleague of his at Göttingen. Other notable doctoral students were Johann Christian Polycarp Erxleben, Johann Pfaff (doctoral adviser of Carl Friedrich Gauss), Johann Tobias Mayer, Heinrich Wilhelm Brandes, Farkas Bolyai (father of János Bolyai), and Georg Klügel. Kästner died in 1800 in Göttingen.\n\nKaestner became most well known for his poems, which appeared first in print without his consent in 1781 and were notable for their biting humour and sharp irony on different contemporary personalities. They were published in \"Vermischten Schriften 1 und 2\" (Altenburg 1783, 2 volumes), and further poems were published in \"Gesammelten poetischen und prosaischen schönwissenschaftlichen Werken\" (Berlin 1841, 4 volumes) and later in Joseph Kürschner's \"Deutscher Nationalliteratur\", volume 73 (hrsg. von Minor; Stuttgart 1883).\n\nHis numerous mathematical writings include \"Anfangsgründe der Mathematik\" (\"Foundations of Mathematics\") (Göttingen 1758-69, 4 volumes; 6th edition 1800) and \"Geschichte der Mathematik\" (\"History of Mathematics\") (Göttingen 1796-1800, 4 volumes). \"Geschichte der Mathematik\" is considered an astute work, but lacks a comprehensive overview of all subsections of mathematics.\n\nHe also translated many volumes of the Proceedings of Royal Swedish Academy of Sciences into German, including all volumes of the Proceedings (\"Handlingar\") between 1749 and 1781 and some volumes of New Proceedings (\"Nya handlingar\") from 1784 to 1792.\n\nHe was elected a Fellow of the Royal Society in April 1789.\n\n"}
{"id": "43620657", "url": "https://en.wikipedia.org/wiki?curid=43620657", "title": "Antonio Monteiro (mathematician)", "text": "Antonio Monteiro (mathematician)\n\nAntónio Aniceto Monteiro (1907–1980) was a mathematician born in Portuguese Angola who later emigrated to Brazil in 1945 and finally to Argentina in 1950. Monteiro is best known for establishing a school of algebraic logic at Universidad Nacional del Sur, Bahía Blanca, Argentina. His efforts to promote theoretical computer science research in Argentina were less successful.\n\nAfter his undergraduate studies at the University of Lisbon (completed in 1930), Monteiro obtained a PhD at Sorbonne in 1936 under the advisement of Maurice Fréchet with a thesis in topology. In Portugal Monteiro was the main founder of the journal \"Portugaliae Mathematica\" in 1937.\n\nIn 1945 Monteiro moved to Brazil. There are two versions of why Monteiro left Portugal. The first version is that Monteiro and other Portuguese mathematicians like Ruy Luís Gomes fell foul of Salazar's regime for their political beliefs; some, like Gomes, were imprisoned; others, like Monteiro, were simply denied employment and practically forced to emigrate. The second version, supported by Monteiro's written documents, is that he was tired of the problems created by his fellow scholars that were blocking his attempts to modernize Mathematics in Portuguese universities.\n\nLeopoldo Nachbin was one of Monteiro's Brazilian students. Monteiro's impact on Argentinean mathematics has been compared to that of Julio Rey Pastor.\n\n\n"}
{"id": "1898401", "url": "https://en.wikipedia.org/wiki?curid=1898401", "title": "Arc length", "text": "Arc length\n\nDetermining the length of an irregular arc segment is also called rectification of a curve. Historically, many methods were used for specific curves. The advent of infinitesimal calculus led to a general formula that provides closed-form solutions in some cases.\n\nA curve in the plane can be approximated by connecting a finite number of points on the curve using line segments to create a polygonal path. Since it is straightforward to calculate the length of each linear segment (using the Pythagorean theorem in Euclidean space, for example), the total length of the approximation can be found by summing the lengths of each linear segment; that approximation is known as the \"(cumulative) chordal distance\".\n\nIf the curve is not already a polygonal path, using a progressively larger number of segments of smaller lengths will result in better approximations. The lengths of the successive approximations will not decrease and may keep increasing indefinitely, but for smooth curves they will tend to a finite limit as the lengths of the segments get arbitrarily small.\n\nFor some curves there is a smallest number formula_1 that is an upper bound on the length of any polygonal approximation. These curves are called rectifiable and the number formula_1 is defined as the arc length.\n\nLet formula_3 be a continuously differentiable function. The length of the curve defined by formula_4 can be defined as the limit of the sum of line segment lengths for a regular partition of formula_5 as the number of segments approaches infinity. This means\n\nwhere formula_7 for formula_8 This definition is equivalent to the standard definition of arc length as an integral:\n\nThe last equality above is true because the definition of the derivative as a limit implies that there is a positive real function formula_10 of positive real formula_11 such that formula_12 implies formula_13 This means\n\nhas absolute value less than formula_15 for formula_16 This means that in the limit formula_17 the left term above equals the right term, which is just the Riemann integral of formula_18 on formula_19 This definition of arc length shows that the length of a curve formula_20 continuously differentiable on formula_5 is always finite. In other words, the curve is always rectifiable.\n\nThe definition of arc length of a smooth curve as the integral of the norm of the derivative is equivalent to the definition\n\nwhere the supremum is taken over all possible partitions formula_23 of formula_19 This definition is also valid if formula_4 is merely continuous, not differentiable.\n\nA curve can be parameterized in infinitely many ways. Let formula_26 be any continuously differentiable bijection. Then formula_27 is another continuously differentiable parameterization of the curve originally defined by formula_28 The arc length of the curve is the same regardless of the parameterization used to define the curve:\n\nIf a planar curve in formula_30 is defined by the equation formula_31 where formula_4 is continuously differentiable, then it is simply a special case of a parametric equation where formula_33 and formula_34 The arc length is then given by:\n\nCurves with closed-form solutions for arc length include the catenary, circle, cycloid, logarithmic spiral, parabola, semicubical parabola and straight line. The lack of a closed form solution for the arc length of an elliptic arc led to the development of the elliptic integrals.\n\nIn most cases, including even simple curves, there are no closed-form solutions for arc length and numerical integration is necessary. Numerical integration of the arc length integral is usually very efficient. For example, consider the problem of finding the length of a quarter of the unit circle by numerically integrating the arc length integral. The upper half of the unit circle can be parameterized as formula_36 The interval formula_37 corresponds to a quarter of the circle. Since formula_38 and formula_39 the length of a quarter of the unit circle is\n\nThe 15-point Gauss-Kronrod rule estimate for this integral of differs from the true length of formula_41 by and the 16-point Gaussian quadrature rule estimate of differs from the true length by only . This means it is possible to evaluate this integral to almost machine precision with only 16 integrand evaluations.\n\nLet formula_42 be a surface mapping and let formula_43 be a curve on this surface. The integrand of the arc length integral is formula_44 Evaluating the derivative requires the chain rule for vector fields:\n\nThe squared norm of this vector is formula_46 (where formula_47 is the first fundamental form coefficient), so the integrand of the arc length integral can be written as formula_48 (where formula_49 and formula_50).\n\nLet formula_51 be a curve expressed in polar coordinates. The mapping that transforms from polar coordinates to rectangular coordinates is\n\nThe integrand of the arc length integral is formula_44 The chain rule for vector fields shows that formula_54 So the squared integrand of the arc length integral is\n\nSo for a curve expressed in polar coordinates, the arc length is\n\nNow let formula_57 be a curve expressed in spherical coordinates where formula_58 is the polar angle measured from the positive formula_59-axis and formula_60 is the azimuthal angle. The mapping that transforms from spherical coordinates to rectangular coordinates is\n\nUsing the chain rule again shows that formula_62 All dot products formula_63 where formula_64 and formula_65 differ are zero, so the squared norm of this vector is\n\nSo for a curve expressed in spherical coordinates, the arc length is\n\nA very similar calculation shows that the arc length of a curve expressed in cylindrical coordinates is\n\nArc lengths are denoted by \"s\", since the Latin word for length (or size) is \"spatium\".\n\nIn the following lines, formula_69 represents the radius of a circle, formula_70 is its diameter, formula_71 is its circumference, formula_72 is the length of an arc of the circle, and formula_58 is the angle which the arc subtends at the centre of the circle. The distances formula_74 and formula_72 are expressed in the same units.\n\n\nTwo units of length, the nautical mile and the metre (or kilometre), were originally defined so the lengths of arcs of great circles on the Earth's surface would be simply numerically related to the angles they subtend at its centre. The simple equation formula_91 applies in the following circumstances:\n\nThe lengths of the distance units were chosen to make the circumference of the Earth equal kilometres, or nautical miles. Those are the numbers of the corresponding angle units in one complete turn.\n\nThose definitions of the metre and the nautical mile have been superseded by more precise ones, but the original definitions are still accurate enough for conceptual purposes and some calculations. For example, they imply that one kilometre is exactly 0.54 nautical miles. Using official modern definitions, one nautical mile is exactly 1.852 kilometres, which implies that 1 kilometre is about nautical miles. This modern ratio differs from the one calculated from the original definitions by less than one part in 10,000.\n\nFor much of the history of mathematics, even the greatest thinkers considered it impossible to compute the length of an irregular arc. Although Archimedes had pioneered a way of finding the area beneath a curve with his \"method of exhaustion\", few believed it was even possible for curves to have definite lengths, as do straight lines. The first ground was broken in this field, as it often has been in calculus, by approximation. People began to inscribe polygons within the curves and compute the length of the sides for a somewhat accurate measurement of the length. By using more segments, and by decreasing the length of each segment, they were able to obtain a more and more accurate approximation. In particular, by inscribing a polygon of many sides in a circle, they were able to find approximate values of π.\n\nIn the 17th century, the method of exhaustion led to the rectification by geometrical methods of several transcendental curves: the logarithmic spiral by Evangelista Torricelli in 1645 (some sources say John Wallis in the 1650s), the cycloid by Christopher Wren in 1658, and the catenary by Gottfried Leibniz in 1691.\n\nIn 1659, Wallis credited William Neile's discovery of the first rectification of a nontrivial algebraic curve, the semicubical parabola. The accompanying figures appear on page 145. On page 91, William Neile is mentioned as \"Gulielmus Nelius\".\n\nBefore the full formal development of calculus, the basis for the modern integral form for arc length was independently discovered by Hendrik van Heuraet and Pierre de Fermat.\n\nIn 1659 van Heuraet published a construction showing that the problem of determining arc length could be transformed into the problem of determining the area under a curve (i.e., an integral). As an example of his method, he determined the arc length of a semicubical parabola, which required finding the area under a parabola. In 1660, Fermat published a more general theory containing the same result in his \"De linearum curvarum cum lineis rectis comparatione dissertatio geometrica\" (Geometric dissertation on curved lines in comparison with straight lines).\n\nBuilding on his previous work with tangents, Fermat used the curve\n\nwhose tangent at \"x\" = \"a\" had a slope of\n\nso the tangent line would have the equation\n\nNext, he increased \"a\" by a small amount to \"a\" + \"ε\", making segment \"AC\" a relatively good approximation for the length of the curve from \"A\" to \"D\". To find the length of the segment \"AC\", he used the Pythagorean theorem:\n\nwhich, when solved, yields\n\nIn order to approximate the length, Fermat would sum up a sequence of short segments.\n\nAs mentioned above, some curves are non-rectifiable. That is, there is no upper bound on the lengths of polygonal approximations; the length can be made arbitrarily large. Informally, such curves are said to have infinite length. There are continuous curves on which every arc (other than a single-point arc) has infinite length. An example of such a curve is the Koch curve. Another example of a curve with infinite length is the graph of the function defined by \"f\"(\"x\") = \"x\" sin(1/\"x\") for any open set with 0 as one of its delimiters and \"f\"(0) = 0. Sometimes the Hausdorff dimension and Hausdorff measure are used to quantify the size of such curves.\n\nLet formula_101 be a (pseudo-)Riemannian manifold, formula_102 a curve in formula_101 and formula_104 the (pseudo-) metric tensor.\n\nThe length of formula_105 is defined to be\n\nwhere formula_107 is the tangent vector of formula_105 at formula_109 The sign in the square root is chosen once for a given curve, to ensure that the square root is a real number. The positive sign is chosen for spacelike curves; in a pseudo-Riemannian manifold, the negative sign may be chosen for timelike curves . Thus the length of a curve in a non-negative real number. Usually no curves are considered which are partly spacelike and partly timelike.\n\nIn theory of relativity, arc length of timelike curves (world lines) is the proper time elapsed along the world line, and arc length of a spacelike curve the proper distance along the curve.\n\n\n"}
{"id": "13653678", "url": "https://en.wikipedia.org/wiki?curid=13653678", "title": "BEAR and LION ciphers", "text": "BEAR and LION ciphers\n\nThe BEAR and LION block ciphers were invented by Ross Anderson and Eli Biham by combining a stream cipher and a cryptographic hash function. The algorithms use a very large variable block size, on the order of 2 to 2 bits . Both are 3-round generalized (alternating) Feistel ciphers, using the hash function and the stream cipher as round functions. BEAR uses the hash function twice with independent keys, and the stream cipher once. LION uses the stream cipher twice and the hash function once. The inventors proved that an attack on either BEAR or LION that recovers the key would break both the stream cipher and the hash.\n\n"}
{"id": "50529465", "url": "https://en.wikipedia.org/wiki?curid=50529465", "title": "Behrend function", "text": "Behrend function\n\nIn algebraic geometry, the Behrend function of a scheme \"X\", introduced by Kai Behrend, is a constructible function\nsuch that if \"X\" is a quasi-projective proper moduli scheme carrying a symmetric obstruction theory, then the weighted Euler characteristic\nis the degree of the virtual fundamental class\nof \"X\", which is an element of the zeroth Chow group of \"X\". Modulo some solvable technical difficulties (e.g., what is the Chow group of a stack?), the definition extends to moduli stacks such as the moduli stack of stable sheaves (the Donaldson–Thomas theory) or that of stable maps (the Gromov–Witten theory).\n"}
{"id": "490067", "url": "https://en.wikipedia.org/wiki?curid=490067", "title": "Blinding (cryptography)", "text": "Blinding (cryptography)\n\nIn cryptography, blinding is a technique by which an agent can provide a service to (i.e., compute a function for) a client in an encoded form without knowing either the real input or the real output. Blinding techniques also have applications to preventing side-channel attacks on encryption devices.\n\nMore precisely, Alice has an input \"x\" and Oscar has a function \"f\". Alice would like Oscar to compute for her without revealing either \"x\" or \"y\" to him. The reason for her wanting this might be that she doesn't know the function \"f\" or that she does not have the resources to compute it.\nAlice \"blinds\" the message by encoding it into some other input \"E\"(\"x\"); the encoding \"E\" must be a bijection on the input space of \"f\", ideally a random permutation. Oscar gives her \"f\"(\"E\"(\"x\")), to which she applies a decoding \"D\" to obtain .\n\nNot all functions allow for blind computation. At other times, blinding must be applied with care. An example of the latter is Rabin–Williams signatures. If blinding is applied to the formatted message but the random value does not honor Jacobi requirements on \"p\" and \"q\", then it could lead to private key recovery. A demonstration of the recovery can be seen in CVE 2015-2141 discovered by Evgeny Sidorov.\n\nThe most common application of blinding is the blind signature. In a blind signature protocol, the signer digitally signs a message without being able to learn its content.\n\nThe one-time pad (OTP) is an application of blinding to the secure communication problem, by its very nature. Alice would like to send a message to Bob secretly, however all of their communication can be read by Oscar. Therefore, Alice sends the message after blinding it with a secret key or OTP that she shares with Bob. Bob reverses the blinding after receiving the message. In this example, the function \n\"f\" is the identity and \"E\" and \"D\" are both typically the XOR operation.\n\nBlinding can also be used to prevent certain side-channel attacks on asymmetric encryption schemes. Side-channel attacks allow an adversary to recover information about the input to a cryptographic operation, by measuring something other than the algorithm's result, e.g., power consumption, computation time, or radio-frequency emanations by a device. Typically these attacks depend on the attacker knowing the characteristics of the algorithm, as well as (some) inputs. In this setting, blinding serves to alter the algorithm's input into some unpredictable state. Depending on the characteristics of the blinding function, this can prevent some or all leakage of useful information. Note that security depends also on the resistance of the blinding functions themselves to side-channel attacks.\n\nFor example, in RSA blinding involves computing the blinding operation , where \"r\" is a random integer between 1 and \"N\" and relatively prime to \"N\" (i.e. , \"x\" is the plaintext, \"e\" is the public RSA exponent and \"N\" is the RSA modulus. As usual, the decryption function is applied thus giving . Finally it is unblinded using the function . Multiplying by yields , as desired. When decrypting in this manner, an adversary who is able to measure time taken by this operation would not be able to make use of this information (by applying timing attacks RSA is known to be vulnerable to) as she does not know the constant \"r\" and hence has no knowledge of the real input fed to the RSA primitives.\n\n\n"}
{"id": "26275483", "url": "https://en.wikipedia.org/wiki?curid=26275483", "title": "Braikenridge–Maclaurin theorem", "text": "Braikenridge–Maclaurin theorem\n\nIn geometry, the , named for 18th century British mathematicians William Braikenridge and Colin Maclaurin , is the converse to Pascal's theorem. It states that if the three intersection points of the three pairs of lines through opposite sides of a hexagon lie on a line \"L\", then the six vertices of the hexagon lie on a conic \"C\"; the conic may be degenerate, as in Pappus's theorem. . The Braikenridge–Maclaurin theorem may be applied in the Braikenridge–Maclaurin construction, which is a synthetic construction of the conic defined by five points, by varying the sixth point. Namely, Pascal's theorem states that given six points on a conic (the vertices of a hexagon), the lines defined by opposite sides intersect in three collinear points. This can be reversed to construct the possible locations for a sixth point, given five existing ones.\n\n"}
{"id": "15893520", "url": "https://en.wikipedia.org/wiki?curid=15893520", "title": "Chudnovsky algorithm", "text": "Chudnovsky algorithm\n\nThe Chudnovsky algorithm is a fast method for calculating the digits of . It was published by the Chudnovsky brothers in 1989, and was used in the world record calculations of 2.7 trillion digits of in December 2009, 5 trillion digits in August 2010, 10 trillion digits in October 2011, 12.1 trillion digits in December 2013 and 22.4 trillion digits of in November 2016.\n\nThe algorithm is based on the negated Heegner number formula_1, the \"j\"-function formula_2, and on the following rapidly convergent generalized hypergeometric series:\n\nFor a high performance iterative implementation, this can be simplified to\n\nThere are 3 big integer terms (the multinomial term \"M\", the linear term \"L\", and the exponential term \"X\") that make up the series and equals the constant \"C\" divided by the sum of the series, as below:\n\nThe terms \"M\", \"L\", and \"X\" satisfy the following recurrences and can be computed as such:\n\nThe computation of \"M\" can be further optimized by introducing an additional term \"K\" as follows:\n\nNote that \n\nThis identity is similar to some of Ramanujan's formulas involving , and is an example of a Ramanujan–Sato series.\n\nThe time complexity of the algorithm is formula_13.\n\n can be computed to any precision using the above algorithm in any environment which supports arbitrary-precision arithmetic. As an example, here is a Python implementation:\nfrom decimal import Decimal as Dec, getcontext as gc\n\ndef PI(maxK=70, prec=1008, disp=1007): # parameter defaults chosen to gain 1000+ digits within a few seconds\n\nPi = PI()\nprint(\"\\nFor greater precision and more digits (takes a few extra seconds) - Try\")\nprint(\"Pi = PI(317,4501,4500)\") \nprint(\"Pi = PI(353,5022,5020)\")\n"}
{"id": "14776839", "url": "https://en.wikipedia.org/wiki?curid=14776839", "title": "Communications on Pure and Applied Mathematics", "text": "Communications on Pure and Applied Mathematics\n\nCommunications on Pure and Applied Mathematics is a monthly peer-reviewed scientific journal which is published by John Wiley & Sons on behalf of the Courant Institute of Mathematical Sciences. It covers research originating from or solicited by the institute, typically in the fields of applied mathematics, mathematical analysis, or mathematical physics. The journal was established in 1948 as the \"Communications on Applied Mathematics\", obtaining its current title the next year. According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 3.080.\n"}
{"id": "217548", "url": "https://en.wikipedia.org/wiki?curid=217548", "title": "Conjugate transpose", "text": "Conjugate transpose\n\nIn mathematics, the conjugate transpose or Hermitian transpose of an \"m\"-by-\"n\" matrix A with complex entries is the \"n\"-by-\"m\" matrix A obtained from A by taking the transpose and then taking the complex conjugate of each entry. (The complex conjugate of \"a\" + \"bi\", where \"a\" and \"b\" are reals, is \"a\" − \"bi\".) The conjugate transpose is formally defined by\n\nwhere the subscripts denote the (\"i\", \"j\")-th entry, for and , and the overbar denotes a scalar complex conjugate.\n\nThis definition can also be written as\n\nwhere formula_3 denotes the transpose and formula_4 denotes the matrix with complex conjugated entries.\n\nOther names for the conjugate transpose of a matrix are Hermitian conjugate, bedaggered matrix, adjoint matrix or transjugate. The conjugate transpose of a matrix \"A\" can be denoted by any of these symbols:\n\nIn some contexts, formula_5 denotes the matrix with complex conjugated entries, and the conjugate transpose is then denoted by formula_10 or formula_11.\n\nIf\nthen\n\nA square matrix \"A\" with entries formula_14 is called\n\nEven if \"A\" is not square, the two matrices \"A\"\"A\" and \"AA\" are both Hermitian and in fact positive semi-definite matrices.\n\nThe conjugate transpose \"adjoint\" matrix \"A\" should not be confused with the adjugate, adj(\"A\"), which is also sometimes called \"adjoint\".\n\nThe conjugate transpose of a matrix \"A\" with real entries reduces to the transpose of \"A\", as the conjugate of a real number is the number itself.\n\nThe conjugate transpose can be motivated by noting that complex numbers can be usefully represented by 2×2 real matrices, obeying matrix addition and multiplication:\n\nThat is, denoting each \"complex\" number \"z\" by the \"real\" 2×2 matrix of the linear transformation on the Argand diagram (viewed as the \"real\" vector space formula_18) affected by complex \"z\"-multiplication on formula_19.\n\nAn \"m\"-by-\"n\" matrix of complex numbers could therefore equally well be represented by a 2\"m\"-by-2\"n\" matrix of real numbers. The conjugate transpose therefore arises very naturally as the result of simply transposing such a matrix, when viewed back again as \"n\"-by-\"m\" matrix made up of complex numbers.\n\n\nThe last property given above shows that if one views \"A\" as a linear transformation from Euclidean Hilbert space formula_20 to formula_21, then the matrix \"A\" corresponds to the adjoint operator of \"A\". The concept of adjoint operators between Hilbert spaces can thus be seen as a generalization of the conjugate transpose of matrices with respect to an orthonormal basis.\n\nAnother generalization is available: suppose \"A\" is a linear map from a complex vector space \"V\" to another, \"W\", then the complex conjugate linear map as well as the transposed linear map are defined, and we may thus take the conjugate transpose of \"A\" to be the complex conjugate of the transpose of \"A\". It maps the conjugate dual of \"W\" to the conjugate dual of \"V\".\n\n\n"}
{"id": "10364657", "url": "https://en.wikipedia.org/wiki?curid=10364657", "title": "Coreset", "text": "Coreset\n\nIn computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers. Many natural geometric optimization problems have coresets that approximate an optimal solution to within a factor of , that can be found quickly (in linear time or near-linear time), and that have size bounded by a function of independent of the input size, where is an arbitrary positive number. When this is the case, one obtains a linear-time or near-linear time approximation scheme, based on the idea of finding a coreset and then applying an exact optimization algorithm to the coreset. Regardless of how slow the exact optimization algorithm is, for any fixed choice of , the running time of this approximation scheme will be plus the time to find the coreset.\n"}
{"id": "37782737", "url": "https://en.wikipedia.org/wiki?curid=37782737", "title": "Credal network", "text": "Credal network\n\nCredal networks are probabilistic graphical models based on imprecise probability. Credal networks can be regarded as an extension of Bayesian networks, where credal sets replace probability mass functions in the specification of the local models for the network variables given their parents. As a Bayesian network defines a joint probability mass function over its variables, a credal network defines a joint credal set. The way this credal set is defined depends on the particular notion of independence for imprecise probability adopted. Most of the research on credal networks focused on the case of strong independence. Given strong independence the joint credal set associated to a credal network is called its strong extension. Let formula_1 denote a collection of categorical variables and formula_2. If formula_3 is, for each formula_4, a conditional credal set over formula_5, then the strong extension of a credal network is defined as follows:\n\nwhere formula_7 denote the convex hull.\n\nInference on a credal network is intended as the computation of the bounds of an expectation with respect to its strong extensions. When computing the bounds of a conditional event, inference is called updating. Say that the queried variable formula_8 and the observed variables are formula_9, the lower bound to be evaluated is:\n\nBeing a generalization of the same problem for Bayesian networks, updating with credal networks is a NP-hard task. Yet a number of algorithm have been specified.\n\n"}
{"id": "20902340", "url": "https://en.wikipedia.org/wiki?curid=20902340", "title": "Cycles per byte", "text": "Cycles per byte\n\nCycles per byte (sometimes abbreviated cpb) is a unit of measurement which indicates the number of clock cycles a microprocessor will perform per byte (usually of octet size) of data processed in an algorithm. It is commonly used as a partial indicator of real-world performance in cryptographic functions.\n\n"}
{"id": "3338671", "url": "https://en.wikipedia.org/wiki?curid=3338671", "title": "Definite clause grammar", "text": "Definite clause grammar\n\nA definite clause grammar (DCG) is a way of expressing grammar, either for natural or formal languages, in a logic programming language such as Prolog. It is closely related to the concept of attribute grammars / affix grammars from which Prolog was originally developed.\nDCGs are usually associated with Prolog, but similar languages such as Mercury also include DCGs. They are called definite clause grammars because they represent a grammar as a set of definite clauses in first-order logic.\n\nThe term DCG refers to the specific type of expression in Prolog and other similar languages; not all ways of expressing grammars using definite clauses are considered DCGs. However, all of the capabilities or properties of DCGs will be the same for any grammar that is represented with definite clauses in essentially the same way as in Prolog.\n\nThe definite clauses of a DCG can be considered a set of axioms where the validity of a sentence, and the fact that it has a certain parse tree can be considered theorems that follow from these axioms. This has the advantage of making it so that recognition and parsing of expressions in a language becomes a general matter of proving statements, such as statements in a logic programming language.\n\nThe history of DCGs is closely tied to the history of Prolog, and the history of Prolog revolves around several researchers in both Marseilles, France, and Edinburgh, Scotland. According to Robert Kowalski, an early developer of Prolog, the first Prolog system was developed in 1972 by Alain Colmerauer and Phillipe Roussel. The first program written in the language was a large natural-language processing system. Fernando Pereira and David Warren at the University of Edinburgh were also involved in the early development of Prolog.\n\nColmerauer had previously worked on a language processing system called Q-systems that was used to translate between English and French. In 1978, Colmerauer wrote a paper about a way of representing grammars called metamorphosis grammars which were part of the early version of Prolog called Marseille Prolog. In this paper, he gave a formal description of metamorphosis grammars and some examples of programs that use them.\n\nFernando Pereira and David Warren, two other early architects of Prolog, coined the term \"definite clause grammar\" and created the notation for DCGs that is used in Prolog today. They gave credit for the idea to Colmerauer and Kowalski, and they note that DCGs are a special case of Colmerauer's metamorphosis grammars. They introduced the idea in an article called \"Definite Clause Grammars for Language Analysis\", where they describe DCGs as a \"formalism ... in which grammars are expressed clauses of first-order predicate logic\" that \"constitute effective programs of the programming language Prolog\".\n\nPereira, Warren, and other pioneers of Prolog later wrote about several other aspects of DCGs. Pereira and Warren wrote an article called \"Parsing as Deduction\", describing things such as how the Earley Deduction proof procedure is used for parsing. Pereira also collaborated with Stuart M. Shieber on a book called \"Prolog and Natural Language Analysis\", that was intended as a general introduction to computational linguistics using logic programming.\n\nA basic example of DCGs helps to illustrate what they are and what they look like.\n\nThis generates sentences such as \"the cat eats the bat\", \"a bat eats the cat\". One can generate all of the valid expressions in the language generated by this grammar at a Prolog interpreter by typing codice_1. Similarly, one can test whether a sentence is valid in the language by typing something like codice_2.\n\nDCG notation is just syntactic sugar for normal definite clauses in Prolog. For example, the previous example could be translated into the following:\n\nThe arguments to each functor, such as codice_3 and codice_4 are difference lists; difference lists are a way of representing a prefix of a list as the difference between its two suffixes (the bigger including the smaller one). Using Prolog's notation for lists, a singleton list prefix codice_5 can be seen as the difference between codice_6 and codice_7, and thus represented with the pair codice_8, for instance.\n\nSaying that codice_9 is the difference between codice_10 and codice_11 is the same as saying that codice_12 holds. Or in the case of the previous example, codice_13.\n\nDifference lists are used to represent lists with DCGs for reasons of efficiency. It is much more efficient to concatenate list differences (prefixes), in the circumstances that they can be used, because the concatenation of codice_3 and codice_4 is just codice_16.\n\nIndeed, codice_17 entails codice_18. This is the same as saying that list concatenation is \"associative\":\n\nIn pure Prolog, normal DCG rules with no extra arguments on the functors, such as the previous example, can only express context-free grammars; there is only one argument on the left side of the production. However, context-sensitive grammars can also be expressed with DCGs, by providing extra arguments, such as in the following example:\n\nThis set of DCG rules describes the grammar which generates the language that consists of strings of the form formula_1.\n\nThis set of DCG rules describes the grammar which generates the language that consists of strings of the form formula_1, by structurally representing \n\nVarious linguistic features can also be represented fairly concisely with DCGs by providing extra arguments to the functors. For example, consider the following set of DCG rules:\n\nThis grammar allows sentences like \"he likes her\" and \"he likes him\", but \"not\" \"her likes he\" and \"him likes him\".\n\nThe main practical use of a DCG is to parse sentences of the given grammar, i.e. to construct a parse tree. This can be done by providing \"extra arguments\" to the functors in the DCG, like in the following rules:\n\nOne can now query the interpreter to yield a parse tree of any given sentence:\n\nDCGs can serve as a convenient syntactic sugar to hide certain parameters in code in other places besides parsing applications.\nIn the declaratively pure programming language Mercury I/O must be represented by a pair of codice_19 arguments.\nDCG notation can be used to make using I/O more covenient, although state variable notation is usually preferred.\n\nDCG notation is also used for parsing and similar things in Mercury as it is in Prolog.\n\nSince DCGs were introduced by Pereira and Warren, several extensions have been proposed. Pereira himself proposed an extension called extraposition grammars (XGs). This formalism was intended in part to make it easier to express certain grammatical phenomena, such as left-extraposition. Pereira states, \"The difference between XG rules and DCG rules is then that the left-hand side of an XG rule may contain several symbols.\" This makes it easier to express rules for context-sensitive grammars.\n\nPeter Van Roy extended DCGs to allow multiple accumulators.\n\nAnother, more recent, extension was made by researchers at NEC Corporation called Multi-Modal Definite Clause Grammars (MM-DCGs) in 1995. Their extensions were intended to allow the recognizing and parsing expressions that include non-textual parts such as pictures.\n\nAnother extension, called definite clause translation grammars (DCTGs) was described by in 1984. DCTG notation looks very similar to DCG notation; the major difference is that one uses codice_20 instead of codice_21 in the rules. It was devised to handle grammatical attributes conveniently. The translation of DCTGs into normal Prolog clauses is like that of DCGs, but 3 arguments are added instead of 2.\n\n\n"}
{"id": "16974776", "url": "https://en.wikipedia.org/wiki?curid=16974776", "title": "Dialectica interpretation", "text": "Dialectica interpretation\n\nIn proof theory, the Dialectica interpretation is a proof interpretation of intuitionistic arithmetic (Heyting arithmetic) into a finite type extension of primitive recursive arithmetic, the so-called System T. It was developed by Kurt Gödel to provide a consistency proof of arithmetic. The name of the interpretation comes from the journal \"Dialectica\", where Gödel's paper was published in a 1958 special issue dedicated to Paul Bernays on his 70th birthday.\n\nVia the Gödel–Gentzen negative translation, the consistency of classical Peano arithmetic had already been reduced to the consistency of intuitionistic Heyting arithmetic. Gödel's motivation for developing the dialectica interpretation was to obtain a relative consistency proof for Heyting arithmetic (and hence for Peano arithmetic).\n\nThe interpretation has two components: a formula translation and a proof translation. The formula translation describes how each formula formula_1 of Heyting arithmetic is mapped to a quantifier-free formula formula_2 of the system T, where formula_3 and formula_4 are tuples of fresh variables (not appearing free in formula_1). Intuitively, formula_1 is interpreted as formula_7. The proof translation shows how a proof of formula_1 has enough information to witness the interpretation of formula_1, i.e. the proof of formula_1 can be converted into a closed term formula_11 and a proof of formula_12 in the system T.\n\nThe quantifier-free formula formula_2 is defined inductively on the logical structure of formula_1 as follows, where formula_15 is an atomic formula:\n\nThe formula interpretation is such that whenever formula_1 is provable in Heyting arithmetic then there exists a sequence of closed terms formula_11 such that formula_12 is provable in the system T. The sequence of terms formula_11 and the proof of formula_12 are constructed from the given proof of formula_1 in Heyting arithmetic. The construction of formula_11 is quite straightforward, except for the contraction axiom formula_24 which requires the assumption that quantifier-free formulas are decidable.\n\nIt has also been shown that Heyting arithmetic extended with the following principles\n\n\nis necessary and sufficient for characterising the formulas of HA which are interpretable by the Dialectica interpretation.\n\nThe basic dialectica interpretation of intuitionistic logic has been extended to various stronger systems. Intuitively, the dialectica interpretation can be applied to a stronger system, as long as the dialectica interpretation of the extra principle can be witnessed by terms in the system T (or an extension of system T).\n\nGiven Gödel's incompleteness theorem (which implies that the consistency of PA cannot be proven by finitistic means) it is reasonable to expect that system T must contain non-finitistic constructions. Indeed this is the case. The non-finitistic constructions show up in the interpretation of mathematical induction. To give a Dialectica interpretation of induction, Gödel makes use of what is nowadays called Gödel's primitive recursive functionals, which are higher order functions with primitive recursive descriptions.\n\nFormulas and proofs in classical arithmetic can also be given a dialectica interpretation via an initial embedding into Heyting arithmetic followed the dialectica interpretation of Heyting arithmetic. Shoenfield, in his book, combines the negative translation and the dialectica interpretation into a single interpretation of classical arithmetic.\n\nIn 1962 Spector\n\nThe Dialectica interpretation has been used to build a model of Girard's refinement of intuitionistic logic known as linear logic, via the so-called Dialectica spaces. Since linear logic is a refinement of intuitionistic logic, the dialectica interpretation of linear logic can also be viewed as a refinement of the dialectica interpretation of intuitionistic logic.\n\nAlthough the linear interpretation in Shirahata's work validates the weakening rule (it is actually an interpretation of affine logic), de Paiva's dialectica spaces interpretation does not validate weakening for arbitrary formulas.\n\nSeveral variants of the Dialectica interpretation have been proposed since. Most notably the Diller-Nahm variant (to avoid the contraction problem) and Kohlenbach's monotone and Ferreira-Oliva bounded interpretations (to interpret weak König's lemma). \nComprehensive treatments of the interpretation can be found at \n"}
{"id": "43177114", "url": "https://en.wikipedia.org/wiki?curid=43177114", "title": "Elliptic algebra", "text": "Elliptic algebra\n\nIn algebra, an elliptic algebra is a certain regular algebra of a Gelfand–Kirillov dimension three (quantum polynomial ring in three variables) that corresponds to a cubic divisor in the projective space P. If the cubic divisor happens to be an elliptic curve, then the algebra is called a Sklyanin algebra. The notion is studied in the context of noncommutative projective geometry.\n"}
{"id": "148420", "url": "https://en.wikipedia.org/wiki?curid=148420", "title": "Euler characteristic", "text": "Euler characteristic\n\nIn mathematics, and more specifically in algebraic topology and polyhedral combinatorics, the Euler characteristic (or Euler number, or Euler–Poincaré characteristic) is a topological invariant, a number that describes a topological space's shape or structure regardless of the way it is bent. It is commonly denoted by formula_1 (Greek lower-case letter chi).\n\nThe Euler characteristic was originally defined for polyhedra and used to prove various theorems about them, including the classification of the Platonic solids. Leonhard Euler, for whom the concept is named, introduced it for polyhedra but failed to rigorously prove that it is an invariant. In modern mathematics, the Euler characteristic arises from homology and, more abstractly, homological algebra.\n\nThe Euler characteristic formula_2 was classically defined for the surfaces of polyhedra, according to the formula\n\nwhere \"V\", \"E\", and \"F\" are respectively the numbers of vertices (corners), edges and faces in the given polyhedron. Any convex polyhedron's surface has Euler characteristic\n\nThis equation is known as Euler's polyhedron formula. It corresponds to the Euler characteristic of the sphere (i.e. χ = 2), and applies identically to spherical polyhedra. An illustration of the formula on some polyhedra is given below.\n\nThe surfaces of nonconvex polyhedra can have various Euler characteristics:\n\nFor regular polyhedra, Arthur Cayley derived a modified form of Euler's formula using the density \"D\", vertex figure density \"d\", and face density formula_5:\nThis version holds both for convex polyhedra (where the densities are all 1) and the non-convex Kepler-Poinsot polyhedra.\n\nProjective polyhedra all have Euler characteristic 1, like the real projective plane, while the surfaces of toroidal polyhedra all have Euler characteristic 0, like the torus.\n\nThe Euler characteristic can be defined for connected plane graphs by the same formula_7 formula as for polyhedral surfaces, where \"F\" is the number of faces in the graph, including the exterior face.\n\nThe Euler characteristic of any plane connected graph G is 2. This is easily proved by induction on the number of faces determined by G, starting with a tree as the base case. For trees, formula_8 and formula_9. If G has C components (disconnected graphs), the same argument by induction on F shows that formula_10. One of the few graph theory papers of Cauchy also proves this result.\n\nVia stereographic projection the plane maps to the two-dimensional sphere, such that a connected graph maps to a polygonal decomposition of the sphere, which has Euler characteristic 2. This viewpoint is implicit in Cauchy's proof of Euler's formula given below.\n\nThere are many proofs of Euler's formula. One was given by Cauchy in 1811, as follows. It applies to any convex polyhedron, and more generally to any polyhedron whose boundary is topologically equivalent to a sphere and whose faces are topologically equivalent to disks.\n\nRemove one face of the polyhedral surface. By pulling the edges of the missing face away from each other, deform all the rest into a planar graph of points and curves, in such a way that the perimeter of the missing face is placed externally, surrounding the graph obtained, as illustrated by the first of the three graphs for the special case of the cube. (The assumption that the polyhedral surface is homeomorphic to the sphere at the beginning is what makes this possible.) After this deformation, the regular faces are generally not regular anymore. The number of vertices and edges has remained the same, but the number of faces has been reduced by 1. Therefore, proving Euler's formula for the polyhedron reduces to proving \"V\" − \"E\" + \"F\" =1 for this deformed, planar object.\n\nIf there is a face with more than three sides, draw a diagonal—that is, a curve through the face connecting two vertices that aren't connected yet. This adds one edge and one face and does not change the number of vertices, so it does not change the quantity \"V\" − \"E\" + \"F\". (The assumption that all faces are disks is needed here, to show via the Jordan curve theorem that this operation increases the number of faces by one.) Continue adding edges in this manner until all of the faces are triangular.\n\nApply repeatedly either of the following two transformations, maintaining the invariant that the exterior boundary is always a simple cycle:\nThese transformations eventually reduce the planar graph to a single triangle. (Without the simple-cycle invariant, removing a triangle might disconnect the remaining triangles, invalidating the rest of the argument. A valid removal order is an elementary example of a shelling.)\n\nAt this point the lone triangle has \"V\" = 3, \"E\" = 3, and \"F\" = 1, so that \"V\" − \"E\" + \"F\" = 1. Since each of the two above transformation steps preserved this quantity, we have shown \"V\" − \"E\" + \"F\" = 1 for the deformed, planar object thus demonstrating \"V\" − \"E\" + \"F\" = 2 for the polyhedron. This proves the theorem.\n\nFor additional proofs, see \"Twenty Proofs of Euler's Formula\" by David Eppstein. Multiple proofs, including their flaws and limitations, are used as examples in \"Proofs and Refutations\" by Imre Lakatos.\n\nThe polyhedral surfaces discussed above are, in modern language, two-dimensional finite CW-complexes. (When only triangular faces are used, they are two-dimensional finite simplicial complexes.) In general, for any finite CW-complex, the Euler characteristic can be defined as the alternating sum\n\nwhere \"k\" denotes the number of cells of dimension \"n\" in the complex.\n\nSimilarly, for a simplicial complex, the Euler characteristic equals the alternating sum\n\nwhere \"k\" denotes the number of \"n\"-simplexes in the complex.\n\nMore generally still, for any topological space, we can define the \"n\"th Betti number \"b\" as the rank of the \"n\"-th singular homology group. The Euler characteristic can then be defined as the alternating sum\n\nThis quantity is well-defined if the Betti numbers are all finite and if they are zero beyond a certain index \"n\". For simplicial complexes, this is not the same definition as in the previous paragraph but a homology computation shows that the two definitions will give the same value for formula_2.\n\nThe Euler characteristic behaves well with respect to many basic operations on topological spaces, as follows.\n\nHomology is a topological invariant, and moreover a homotopy invariant: Two topological spaces that are homotopy equivalent have isomorphic homology groups. It follows that the Euler characteristic is also a homotopy invariant.\n\nFor example, any contractible space (that is, one homotopy equivalent to a point) has trivial homology, meaning that the 0th Betti number is 1 and the others 0. Therefore, its Euler characteristic is 1. This case includes Euclidean space formula_15 of any dimension, as well as the solid unit ball in any Euclidean space — the one-dimensional interval, the two-dimensional disk, the three-dimensional ball, etc.\n\nFor another example, any convex polyhedron is homeomorphic to the three-dimensional ball, so its surface is homeomorphic (hence homotopy equivalent) to the two-dimensional sphere, which has Euler characteristic 2. This explains why convex polyhedra have Euler characteristic 2.\n\nIf \"M\" and \"N\" are any two topological spaces, then the Euler characteristic of their disjoint union is the sum of their Euler characteristics, since homology is additive under disjoint union:\n\nMore generally, if \"M\" and \"N\" are subspaces of a larger space \"X\", then so are their union and intersection. In some cases, the Euler characteristic obeys a version of the inclusion–exclusion principle:\n\nThis is true in the following cases:\n\n\nIn general, the inclusion–exclusion principle is false. A counterexample is given by taking \"X\" to be the real line, \"M\" a subset consisting of one point and \"N\" the complement of \"M\".\n\nFor two connected closed n-manifolds formula_18 one can obtain a new connected manifold formula_19\nvia the connected sum operation.\nThe Euler characteristic is related by the formula \n\nAlso, the Euler characteristic of any product space \"M\" × \"N\" is\n\nThese addition and multiplication properties are also enjoyed by cardinality of sets. In this way, the Euler characteristic can be viewed as a generalisation of cardinality; see .\n\nSimilarly, for an \"k\"-sheeted covering space formula_22 one has\nMore generally, for a ramified covering space, the Euler characteristic of the cover can be computed from the above, with a correction factor for the ramification points, which yields the Riemann–Hurwitz formula.\n\nThe product property holds much more generally, for fibrations with certain conditions.\n\nIf formula_24 is a fibration with fiber \"F,\" with the base \"B\" path-connected, and the fibration is orientable over a field \"K,\" then the Euler characteristic with coefficients in the field \"K\" satisfies the product property:\nThis includes product spaces and covering spaces as special cases,\nand can be proven by the Serre spectral sequence on homology of a fibration.\n\nFor fiber bundles, this can also be understood in terms of a transfer map formula_26 – note that this is a lifting and goes \"the wrong way\" – whose composition with the projection map formula_27 is multiplication by the Euler class of the fiber:\n\nThe Euler characteristic can be calculated easily for general surfaces by finding a polygonization of the surface (that is, a description as a CW-complex) and using the above definitions.\n\nIt is common to construct soccer balls by stitching together pentagonal and hexagonal pieces, with three pieces meeting at each vertex (see for example the Adidas Telstar). If \"P\" pentagons and \"H\" hexagons are used, then there are \"F\" = \"P\" + \"H\" faces, \"V\" = (5 \"P\" + 6 \"H\") / 3 vertices, and \"E\" = (5 \"P\" + 6 \"H\") / 2 edges. The Euler characteristic is thus\n\nBecause the sphere has Euler characteristic 2, it follows that \"P\" = 12. That is, a soccer ball constructed in this way always has 12 pentagons. In principle, the number of hexagons is unconstrained. This result is also applicable to fullerenes.\n\nThe \"n\"-dimensional sphere has singular homology groups equal to\n\nhence has Betti number 1 in dimensions 0 and \"n\", and all other Betti numbers are 0. Its Euler characteristic is then 1 + (−1) — that is, either 0 or 2.\n\nThe \"n\"-dimensional real projective space is the quotient of the \"n\"-sphere by the antipodal map. It follows that its Euler characteristic is exactly half that of the corresponding sphere — either 0 or 1.\n\nThe \"n\"-dimensional torus is the product space of \"n\" circles. Its Euler characteristic is 0, by the product property. More generally, any compact parallelizable manifold, including any compact Lie group, has Euler characteristic 0.\n\nThe Euler characteristic of any closed odd-dimensional manifold is also 0. The case for orientable examples is a corollary of Poincaré duality. This property applies more generally to any compact stratified space all of whose strata have odd dimension. It also applies to closed odd-dimensional non-orientable manifolds, via the two-to-one orientable double cover.\n\nThe Euler characteristic of a closed orientable surface can be calculated from its genus \"g\" (the number of tori in a connected sum decomposition of the surface; intuitively, the number of \"handles\") as\n\nThe Euler characteristic of a closed non-orientable surface can be calculated from its non-orientable genus \"k\" (the number of real projective planes in a connected sum decomposition of the surface) as\n\nFor closed smooth manifolds, the Euler characteristic coincides with the Euler number, i.e., the Euler class of its tangent bundle evaluated on the fundamental class of a manifold. The Euler class, in turn, relates to all other characteristic classes of vector bundles.\n\nFor closed Riemannian manifolds, the Euler characteristic can also be found by integrating the curvature; see the Gauss–Bonnet theorem for the two-dimensional case and the generalized Gauss–Bonnet theorem for the general case.\n\nA discrete analog of the Gauss–Bonnet theorem is Descartes' theorem that the \"total defect\" of a polyhedron, measured in full circles, is the Euler characteristic of the polyhedron; see defect (geometry).\n\nHadwiger's theorem characterizes the Euler characteristic as the \"unique\" (up to scalar multiplication) translation-invariant, finitely additive, not-necessarily-nonnegative set function defined on finite unions of compact convex sets in R that is \"homogeneous of degree 0\".\n\nFor every combinatorial cell complex, one defines the Euler characteristic as the number of 0-cells, minus the number of 1-cells, plus the number of 2-cells, etc., if this alternating sum is finite. In particular, the Euler characteristic of a finite set is simply its cardinality, and the Euler characteristic of a graph is the number of vertices minus the number of edges.\n\nMore generally, one can define the Euler characteristic of any chain complex to be the alternating sum of the ranks of the homology groups of the chain complex, assuming that all these ranks are finite.\n\nA version of Euler characteristic used in algebraic geometry is as follows. For any coherent sheaf formula_33 on a proper scheme \"X\", one defines its Euler characteristic to be\nwhere formula_35 is the dimension of the \"i\"-th sheaf cohomology group of formula_33. In this case, the dimensions are all finite by Grothendieck's finiteness theorem. This is an instance of the Euler characteristic of a chain complex, where the chain complex is a finite resolution of formula_33 by acyclic sheaves.\n\nAnother generalization of the concept of Euler characteristic on manifolds comes from orbifolds (see Euler characteristic of an orbifold). While every manifold has an integer Euler characteristic, an orbifold can have a fractional Euler characteristic. For example, the teardrop orbifold has Euler characteristic 1 + 1/\"p\", where \"p\" is a prime number corresponding to the cone angle 2\"\" / \"p\".\n\nThe concept of Euler characteristic of a bounded finite poset is another generalization, important in combinatorics. A poset is \"bounded\" if it has smallest and largest elements; call them 0 and 1. The Euler characteristic of such a poset is defined as the integer \"μ\"(0,1), where \"μ\" is the Möbius function in that poset's incidence algebra.\n\nThis can be further generalized by defining a Q-valued Euler characteristic for certain finite categories, a notion compatible with the Euler characteristics of graphs, orbifolds and posets mentioned above. In this setting, the Euler characteristic of a finite group or monoid \"G\" is 1/|\"G\"|, and the Euler characteristic of a finite groupoid is the sum of 1/|\"G\"|, where we picked one representative group \"G\" for each connected component of the groupoid.\n\n\n\n\n"}
{"id": "1410595", "url": "https://en.wikipedia.org/wiki?curid=1410595", "title": "Field equation", "text": "Field equation\n\nIn theoretical physics and applied mathematics, a field equation is a partial differential equation which determines the dynamics of a physical field, specifically the time evolution and spatial distribution of the field. The solutions to the equation are mathematical functions which correspond directly to the field, as functions of time and space. Since the field equation is a partial differential equation, there are families of solutions which represent a variety of physical possibilities. Usually, there is not just a single equation, but a set of coupled equations which must be solved simultaneously. Field equations are not ordinary differential equations since a field depends on space and time, which requires at least two variables.\n\nWhereas the \"wave equation\", the \"diffusion equation\", and the \"continuity equation\" all have standard forms (and various special cases or generalizations), there is no single, special equation referred to as \"the field equation\".\n\nThe topic broadly splits into equations of classical field theory and quantum field theory. Classical field equations describe many physical properties like temperature of a substance, velocity of a fluid, stresses in an elastic material, electric and magnetic fields from a current, etc. They also describe the fundamental forces of nature, like electromagnetism and gravity. In quantum field theory, particles or systems of \"particles\" like electrons and photons are associated with fields, allowing for infinite degrees of freedom (unlike finite degrees of freedom in particle mechanics) and variable particle numbers which can be created or annihilated.\n\nUsually, field equations are postulated (like the Einstein field equations and the Schrödinger equation, which underlies all quantum field equations) or obtained from the results of experiments (like Maxwell's equations). The extent of their validity is their extent to correctly predict and agree with experimental results.\n\nFrom a theoretical viewpoint, field equations can be formulated in the frameworks of Lagrangian field theory, Hamiltonian field theory, and field theoretic formulations of the principle of stationary action. Given a suitable Lagrangian or Hamiltonian density, a function of the fields in a given system, as well as their derivatives, the principle of stationary action will obtain the field equation.\n\nIn both classical and quantum theories, field equations will satisfy the symmetry of the background physical theory. Most of the time Galilean symmetry is enough, for speeds (of propagating fields) much less than light. When particles and fields propagate at speeds close to light, Lorentz symmetry is one of the most common settings because the equation and its solutions are then consistent with special relativity.\n\nAnother symmetry arises from gauge freedom, which is intrinsic to the field equations. Fields which correspond to interactions may be gauge fields, which means they can be derived from a potential, and certain values of potentials correspond to the same value of the field.\n\nField equations can be classified in many ways: classical or quantum, nonrelativistsic or relativistic, according to the spin or mass of the field, and the number of components the field has and how they change under coordinate transformations (e.g. scalar fields, vector fields, tensor fields, spinor fields, twistor fields etc.). They can also inherit the classification of differential equations, as linear or nonlinear, the order of the highest derivative, or even as fractional differential equations. Gauge fields may be classified as in group theory, as abelian or nonabelian.\n\nField equations underlie wave equations, because periodically changing fields generate waves. Wave equations can be thought of as field equations, in the sense they can often be derived from field equations. Alternatively, given suitable Lagrangian or Hamiltonian densities and using the principle of stationary action, the wave equations can be obtained also.\n\nFor example, Maxwell's equations can be used to derive inhomogeneous electromagnetic wave equations, and from the Einstein field equations one can derive equations for gravitational waves.\n\nNot every partial differential equation (PDE) in physics is automatically called a \"field equation\", even if fields are involved. They are extra equations to provide additional constraints for a given physical system.\n\n\"Continuity equations\" and \"diffusion equations\" describe transport phenomena, even though they may involve fields which influence the transport processes.\n\nIf a \"constitutive equation\" takes the form of a PDE and involves fields, it is not usually called a field equation because it does not govern the dynamical behaviour of the fields. They relate one field to another, in a given material. Constitutive equations are used along with field equations when the effects of matter need to be taken into account.\n\nClassical field equations arise in continuum mechanics (including elastodynamics and fluid mechanics), heat transfer, electromagnetism, and gravitation.\n\nFundamental classical field equations include\n\n\nImportant equations derived from fundamental laws include:\n\n\nAs part of real-life mathematical modelling processes, classical field equations are accompanied by other equations of motion, equations of state, constitutive equations, and continuity equations.\n\nIn quantum field theory, particles are described by quantum fields which satisfy the Schrödinger equation. They are also creation and annihilation operators which satisfy commutation relations and are subject to the spin–statistics theorem.\n\nParticular cases of relativistic quantum field equations include\n\n\nIn quantum field equations, it is common to use momentum components of the particle instead of position coordinates of the particle's location, the fields are in momentum space and Fourier transforms relate them to the position representation.\n\n\n\n\n"}
{"id": "164600", "url": "https://en.wikipedia.org/wiki?curid=164600", "title": "General circulation model", "text": "General circulation model\n\nA general circulation model (GCM) is a type of climate model. It employs a mathematical model of the general circulation of a planetary atmosphere or ocean. It uses the Navier–Stokes equations on a rotating sphere with thermodynamic terms for various energy sources (radiation, latent heat). These equations are the basis for computer programs used to simulate the Earth's atmosphere or oceans. Atmospheric and oceanic GCMs (AGCM and OGCM) are key components along with sea ice and land-surface components.\n\nGCMs and global climate models are used for weather forecasting, understanding the climate and forecasting climate change.\n\nVersions designed for decade to century time scale climate applications were originally created by Syukuro Manabe and Kirk Bryan at the Geophysical Fluid Dynamics Laboratory (GFDL) in Princeton, New Jersey. These models are based on the integration of a variety of fluid dynamical, chemical and sometimes biological equations.\n\nThe acronym \"GCM\" originally stood for \"General Circulation Model\". Recently, a second meaning came into use, namely \"Global Climate Model\". While these do not refer to the same thing, General Circulation Models are typically the tools used for modelling climate, and hence the two terms are sometimes used interchangeably. However, the term \"global climate model\" is ambiguous and may refer to an integrated framework that incorporates multiple components including a general circulation model, or may refer to the general class of climate models that use a variety of means to represent the climate mathematically.\n\nIn 1956, Norman Phillips developed a mathematical model that could realistically depict monthly and seasonal patterns in the troposphere. It became the first successful climate model. Following Phillips's work, several groups began working to create GCMs. The first to combine both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory. By the early 1980s, the United States' National Center for Atmospheric Research had developed the Community Atmosphere Model; this model has been continuously refined. In 1996, efforts began to model soil and vegetation types. Later the Hadley Centre for Climate Prediction and Research's HadCM3 model coupled ocean-atmosphere elements. The role of gravity waves was added in the mid-1980s. Gravity waves are required to simulate regional and global scale circulations accurately.\n\nAtmospheric (AGCMs) and oceanic GCMs (OGCMs) can be coupled to form an atmosphere-ocean coupled general circulation model (CGCM or AOGCM). With the addition of submodels such as a sea ice model or a model for evapotranspiration over land, AOGCMs become the basis for a full climate model.\n\nA recent trend in GCMs is to apply them as components of Earth system models, e.g. by coupling ice sheet models for the dynamics of the Greenland and Antarctic ice sheets, and one or more chemical transport models (CTMs) for species important to climate. Thus a carbon CTM may allow a GCM to better predict anthropogenic changes in carbon dioxide concentrations. In addition, this approach allows accounting for inter-system feedback: e.g. chemistry-climate models allow the possible effects of climate change on ozone hole to be studied.\n\nClimate prediction uncertainties depend on uncertainties in chemical, physical and social models (see IPCC scenarios below). Significant uncertainties and unknowns remain, especially regarding the future course of human population, industry and technology.\n\nThree-dimensional (more properly four-dimensional) GCMs apply discrete equations for fluid motion and integrate these forward in time. They contain parameterisations for processes such as convection that occur on scales too small to be resolved directly.\n\nA simple general circulation model (SGCM) consists of a dynamic core that relates properties such as temperature to others such as pressure and velocity. Examples are programs that solve the primitive equations, given energy input and energy dissipation in the form of scale-dependent friction, so that atmospheric waves with the highest wavenumbers are most attenuated. Such models may be used to study atmospheric processes, but are not suitable for climate projections.\n\nAtmospheric GCMs (AGCMs) model the atmosphere (and typically contain a land-surface model as well) using imposed sea surface temperatures (SSTs). They may include atmospheric chemistry.\n\nAGCMs consist of a dynamical core which integrates the equations of fluid motion, typically for:\n\n\nA GCM contains prognostic equations that are a function of time (typically winds, temperature, moisture, and surface pressure) together with diagnostic equations that are evaluated from them for a specific time period. As an example, pressure at any height can be diagnosed by applying the hydrostatic equation to the predicted surface pressure and the predicted values of temperature between the surface and the height of interest. Pressure is used to compute the pressure gradient force in the time-dependent equation for the winds.\n\nOGCMs model the ocean (with fluxes from the atmosphere imposed) and may contain a sea ice model. For example, the standard resolution of HadOM3 is 1.25 degrees in latitude and longitude, with 20 vertical levels, leading to approximately 1,500,000 variables.\n\nAOGCMs (e.g. HadCM3, GFDL CM2.X) combine the two submodels. They remove the need to specify fluxes across the interface of the ocean surface. These models are the basis for model predictions of future climate, such as are discussed by the IPCC. AOGCMs internalise as many processes as possible. They have been used to provide predictions at a regional scale. While the simpler models are generally susceptible to analysis and their results are easier to understand, AOGCMs may be nearly as hard to analyse as the climate itself.\n\nThe fluid equations for AGCMs are made discrete using either the finite difference method or the spectral method. For finite differences, a grid is imposed on the atmosphere. The simplest grid uses constant angular grid spacing (i.e., a latitude / longitude grid). However, non-rectangular grids (e.g., icosahedral) and grids of variable resolution are more often used. The LMDz model can be arranged to give high resolution over any given section of the planet. HadGEM1 (and other ocean models) use an ocean grid with higher resolution in the tropics to help resolve processes believed to be important for the El Niño Southern Oscillation (ENSO). Spectral models generally use a gaussian grid, because of the mathematics of transformation between spectral and grid-point space. Typical AGCM resolutions are between 1 and 5 degrees in latitude or longitude: HadCM3, for example, uses 3.75 in longitude and 2.5 degrees in latitude, giving a grid of 96 by 73 points (96 x 72 for some variables); and has 19 vertical levels. This results in approximately 500,000 \"basic\" variables, since each grid point has four variables (\"u\",\"v\", \"T\", \"Q\"), though a full count would give more (clouds; soil levels). HadGEM1 uses a grid of 1.875 degrees in longitude and 1.25 in latitude in the atmosphere; HiGEM, a high-resolution variant, uses 1.25 x 0.83 degrees respectively. These resolutions are lower than is typically used for weather forecasting. Ocean resolutions tend to be higher, for example HadCM3 has 6 ocean grid points per atmospheric grid point in the horizontal.\n\nFor a standard finite difference model, uniform gridlines converge towards the poles. This would lead to computational instabilities (see CFL condition) and so the model variables must be filtered along lines of latitude close to the poles. Ocean models suffer from this problem too, unless a rotated grid is used in which the North Pole is shifted onto a nearby landmass. Spectral models do not suffer from this problem. Some experiments use geodesic grids and icosahedral grids, which (being more uniform) do not have pole-problems. Another approach to solving the grid spacing problem is to deform a Cartesian cube such that it covers the surface of a sphere.\n\nSome early versions of AOGCMs required an \"ad hoc\" process of \"flux correction\" to achieve a stable climate. This resulted from separately prepared ocean and atmospheric models that each used an implicit flux from the other component different than that component could produce. Such a model failed to match observations. However, if the fluxes were 'corrected', the factors that led to these unrealistic fluxes might be unrecognised, which could affect model sensitivity. As a result, the vast majority of models used in the current round of IPCC reports do not use them. The model improvements that now make flux corrections unnecessary include improved ocean physics, improved resolution in both atmosphere and ocean, and more physically consistent coupling between atmosphere and ocean submodels. Improved models now maintain stable, multi-century simulations of surface climate that are considered to be of sufficient quality to allow their use for climate projections.\n\nMoist convection releases latent heat and is important to the Earth's energy budget. Convection occurs on too small a scale to be resolved by climate models, and hence it must be handled via parameters. This has been done since the 1950s. Akio Arakawa did much of the early work, and variants of his scheme are still used, although a variety of different schemes are now in use. Clouds are also typically handled with a parameter, for a similar lack of scale. Limited understanding of clouds has limited the success of this strategy, but not due to some inherent shortcoming of the method.\n\nMost models include software to diagnose a wide range of variables for comparison with observations or study of atmospheric processes. An example is the 2-metre temperature, which is the standard height for near-surface observations of air temperature. This temperature is not directly predicted from the model but is deduced from surface and lowest-model-layer temperatures. Other software is used for creating plots and animations.\n\nCoupled AOGCMs use transient climate simulations to project/predict climate changes under various scenarios. These can be idealised scenarios (most commonly, CO emissions increasing at 1%/yr) or based on recent history (usually the \"IS92a\" or more recently the SRES scenarios). Which scenarios are most realistic remains uncertain.\n\nThe 2001 IPCC Third Assessment Report F igure 9.3 shows the global mean response of 19 different coupled models to an idealised experiment in which emissions increased at 1% per year. Figure 9.5 shows the response of a smaller number of models to more recent trends. For the 7 climate models shown there, the temperature change to 2100 varies from 2 to 4.5 °C with a median of about 3 °C.\n\nFuture scenarios do not include unknown events for example, volcanic eruptions or changes in solar forcing. These effects are believed to be small in comparison to greenhouse gas (GHG) forcing in the long term, but large volcanic eruptions, for example, can exert a substantial temporary cooling effect.\n\nHuman GHG emissions are a model input, although it is possible to include an economic/technological submodel to provide these as well. Atmospheric GHG levels are usually supplied as an input, though it is possible to include a carbon cycle model that reflects vegetation and oceanic processes to calculate such levels.\n\nFor the six SRES marker scenarios, IPCC (2007:7–8) gave a \"best estimate\" of global mean temperature increase (2090–2099 relative to the period 1980–1999) of 1.8 °C to 4.0 °C. Over the same time period, the \"likely\" range (greater than 66% probability, based on expert judgement) for these scenarios was for a global mean temperature increase of 1.1 to 6.4 °C.\n\nIn 2008 a study made climate projections using several emission scenarios. In a scenario where global emissions start to decrease by 2010 and then declined at a sustained rate of 3% per year, the likely global average temperature increase was predicted to be 1.7 °C above pre-industrial levels by 2050, rising to around 2 °C by 2100. In a projection designed to simulate a future where no efforts are made to reduce global emissions, the likely rise in global average temperature was predicted to be 5.5 °C by 2100. A rise as high as 7 °C was thought possible, although less likely.\n\nAnother no-reduction scenario resulted in a median warming over land (2090–99 relative to the period 1980–99) of 5.1 °C. Under the same emissions scenario but with a different model, the predicted median warming was 4.1 °C.\n\nAOGCMs internalise as many processes as are sufficiently understood. However, they are still under development and significant uncertainties remain. They may be coupled to models of other processes, such as the carbon cycle, so as to better model feedbacks. Most recent simulations show \"plausible\" agreement with the measured temperature anomalies over the past 150 years, when driven by observed changes in greenhouse gases and aerosols. Agreement improves by including both natural and anthropogenic forcings.\n\nImperfect models may nevertheless produce useful results. GCMs are capable of reproducing the general features of the observed global temperature over the past century.\n\nA debate over how to reconcile climate model predictions that upper air (tropospheric) warming should be greater than observed surface warming, some of which appeared to show otherwise, was resolved in favour of the models, following data revisions.\n\nCloud effects are a significant area of uncertainty in climate models. Clouds have competing effects on climate. They cool the surface by reflecting sunlight into space; they warm it by increasing the amount of infrared radiation transmitted from the atmosphere to the surface. In the 2001 IPCC report possible changes in cloud cover were highlighted as a major uncertainty in predicting climate.\n\nClimate researchers around the world use climate models to understand the climate system. Thousands of papers have been published about model-based studies. Part of this research is to improve the models.\n\nIn 2000, a comparison between measurements and dozens of GCM simulations of ENSO-driven tropical precipitation, water vapor, temperature, and outgoing longwave radiation found similarity between measurements and simulation of most factors. However the simulated change in precipitation was about one-fourth less than what was observed. Errors in simulated precipitation imply errors in other processes, such as errors in the evaporation rate that provides moisture to create precipitation. The other possibility is that the satellite-based measurements are in error. Either indicates progress is required in order to monitor and predict such changes.\n\nA more complete discussion of climate models is provided in the IPCC's Third Assessment Report.\n\n\nThe precise magnitude of future changes in climate is still uncertain; for the end of the 21st century (2071 to 2100), for SRES scenario A2, the change of global average SAT change from AOGCMs compared with 1961 to 1990 is +3.0 °C (5.4 °F) and the range is +1.3 to +4.5 °C (+2.3 to 8.1 °F).\n\nThe IPCC's Fifth Assessment Report asserted \"...very high confidence that models reproduce the general features of the global-scale annual mean surface temperature increase over the historical period.\" However, the report also observed that the rate of warming over the period 1998-2012 was lower than that predicted by 111 out of 114 Coupled Model Intercomparison Project climate models.\n\nThe global climate models used for climate projections are similar in structure to (and often share computer code with) numerical models for weather prediction, but are nonetheless logically distinct.\n\nMost weather forecasting is done on the basis of interpreting numerical model results. Since forecasts are short typically a few days or a week such models do not usually contain an ocean model but rely on imposed SSTs. They also require accurate initial conditions to begin the forecast typically these are taken from the output of a previous forecast, blended with observations. Predictions must require only a few hours; but because they only cover a one-week the models can be run at higher resolution than in climate mode. Currently the ECMWF runs at resolution as opposed to the scale used by typical climate model runs. Often local models are run using global model results for boundary conditions, to achieve higher local resolution: for example, the Met Office runs a mesoscale model with an resolution covering the UK, and various agencies in the US employ models such as the NGM and NAM models. Like most global numerical weather prediction models such as the GFS, global climate models are often spectral models instead of grid models. Spectral models are often used for global models because some computations in modeling can be performed faster, thus reducing run times.\n\nClimate models use quantitative methods to simulate the interactions of the atmosphere, oceans, land surface and ice.\n\nAll climate models take account of incoming energy as short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared, as well as outgoing energy as long wave (far) infrared electromagnetic radiation from the earth. Any imbalance results in a change in temperature.\n\nThe most talked-about models of recent years relate temperature to emissions of greenhouse gases. These models project an upward trend in the surface temperature record, as well as a more rapid increase in temperature at higher altitudes.\n\nThree (or more properly, four since time is also considered) dimensional GCM's discretise the equations for fluid motion and energy transfer and integrate these over time. They also contain parametrisations for processes such as convection that occur on scales too small to be resolved directly.\n\nAtmospheric GCMs (AGCMs) model the atmosphere and impose sea surface temperatures as boundary conditions. Coupled atmosphere-ocean GCMs (AOGCMs, e.g. HadCM3, EdGCM, GFDL CM2.X, ARPEGE-Climat) combine the two models.\n\nModels range in complexity:\n\n\nOther submodels can be interlinked, such as land use, allowing researchers to predict the interaction between climate and ecosystems.\n\nThe Climber-3 model uses a 2.5-dimensional statistical-dynamical model with 7.5° × 22.5° resolution and time step of 1/2 a day. An oceanic submodel is MOM-3 (Modular Ocean Model) with a 3.75° × 3.75° grid and 24 vertical levels.\n\nOne-dimensional, radiative-convective models were used to verify basic climate assumptions in the 1980s and 1990s.\n\n\n"}
{"id": "34918164", "url": "https://en.wikipedia.org/wiki?curid=34918164", "title": "Generalized Clifford algebra", "text": "Generalized Clifford algebra\n\nIn mathematics, a Generalized Clifford algebra (GCA) is an associative algebra that generalizes the Clifford algebra, and goes back to the work of Hermann Weyl, who utilized and formalized these clock-and-shift operators introduced by J. J. Sylvester (1882), and organized by Cartan (1898) and Schwinger.\n\nClock and shift matrices find routine applications in numerous areas of mathematical physics, providing the cornerstone of quantum mechanical dynamics in finite-dimensional vector spaces. The concept of a spinor can further be linked to these algebras.\n\nThe term Generalized Clifford Algebras can also refer to associative algebras that are constructed using forms of higher degree instead of quadratic forms.\n\nThe -dimensional generalized Clifford algebra is defined as an associative algebra over a field , generated by\nand\n\nMoreover, in any irreducible matrix representation, relevant for physical applications, it is required that\n,   and formula_6gcdformula_7. The field is usually taken to be the complex numbers C.\n\nIn the more common cases of GCA, the -dimensional generalized Clifford algebra of order has the property , formula_8   for all \"j\",\"k\", and formula_9. It follows that \nand\nfor all \"j\",\"k\",l = 1...,\"n\", and \nis the th root of 1.\n\nThere exist several definitions of a Generalized Clifford Algebra in the literature.\n\nIn the (orthogonal) Clifford algebra, the elements follow an anticommutation rule, with .\n\nThe Clock and Shift matrices can be represented by matrices in Schwinger's canonical notation as\n\nNotably, , (the Weyl braiding relations), and (the Discrete Fourier transform). \nWith , one has three basis elements which, together with , fulfil the above conditions of the Generalized Clifford Algebra (GCA).\n\nThese matrices, and , normally referred to as \"shift and clock matrices\", were introduced by J. J. Sylvester in the 1880s. (Note that the matrices are cyclic permutation matrices that perform a circular shift; \"they are not to be confused\" with upper and lower shift matrices which have ones only either above or below the diagonal, respectively).\n\nIn this case, we have = −1, and\nthus\nwhich constitute the Pauli matrices.\n\nIn this case we have = , and\nand may be determined accordingly.\n\n\n"}
{"id": "33791047", "url": "https://en.wikipedia.org/wiki?curid=33791047", "title": "Geometrically and materially nonlinear analysis with imperfections included", "text": "Geometrically and materially nonlinear analysis with imperfections included\n\nGeometrically and materially nonlinear analysis with imperfections included (GMNIA), is a structural analysis method designed to verify the strength capacity of a structure, which accounts for both plasticity and buckling failure modes.\nGMNIA is currently considered the most sophisticated and perspectively the most accurate method of a numerical buckling strength verification.\n"}
{"id": "3216387", "url": "https://en.wikipedia.org/wiki?curid=3216387", "title": "Hausdorff moment problem", "text": "Hausdorff moment problem\n\nbe the sequence of moments\n\nof some Borel measure \"μ\" supported on the closed unit interval [0, 1]. In the case \"m\" = 1, this is equivalent to the existence of a random variable \"X\" supported on [0, 1], such that E \"X\" = \"m\".\n\nThe essential difference between this and other well-known moment problems is that this is on a bounded interval, whereas in the Stieltjes moment problem one considers a half-line [0, ∞), and in the Hamburger moment problem one considers the whole line (−∞, ∞). The Stieltjes moment problems and the Hamburger moment problems, if they are solvable, may have infinitely many solutions (indeterminate moment problem) whereas a Hausdorff moment problem always has a unique solution if it is solvable (determinate moment problem). In the indeterminate moment problem case, there are infinite measures corresponding to the same prescribed moments and they consist of a convex set. The set of polynomials may or may not be dense in the associated Hilbert spaces if the moment problem is indeterminate, and it depends on whether measure is extremal or not. But in the determinate moment problem case, the set of polynomials is dense in the associated Hilbert space.\n\nIn 1921, Hausdorff showed that { \"m\" : \"n\" = 0, 1, 2, ... } is such a moment sequence if and only if the sequence is completely monotonic, i.e., its difference sequences satisfy the equation\n\nfor all \"n\",\"k\" ≥ 0. Here, Δ is the difference operator given by\n\nThe necessity of this condition is easily seen by the identity\nwhich is \"≥ 0\", being the integral of an almost sure non-negative function.\nFor example, it is necessary to have\n\n\n\n"}
{"id": "32197567", "url": "https://en.wikipedia.org/wiki?curid=32197567", "title": "Hecke algebra of a pair", "text": "Hecke algebra of a pair\n\nIn mathematical representation theory, the HT of a pair (\"g\",\"K\") is an algebra with an approximate identity, whose approximately unital modules are the same as \"K\"-finite representations of the pairs (\"g\",\"K\"). Here \"K\" is a compact subgroup of a Lie group with Lie algebra \"g\".\n\nThe Hecke algebra of a pair (\"g\",\"K\") is the algebra of \"K\"-finite distributions on \"G\" with support in \"K\", with the product given by convolution.\n"}
{"id": "13416497", "url": "https://en.wikipedia.org/wiki?curid=13416497", "title": "Hicks optimality", "text": "Hicks optimality\n\nIn game theory, a Hicks-optimal outcome, named after John Hicks, is an outcome in which the total payoff for all of the players of a game is the most it could possibly be. A Hicks-optimal outcome is always Pareto efficient.\n"}
{"id": "9588430", "url": "https://en.wikipedia.org/wiki?curid=9588430", "title": "Higher-order statistics", "text": "Higher-order statistics\n\nIn statistics, the term higher-order statistics (HOS) refers to functions which use the third or higher power of a sample, as opposed to more conventional techniques of lower-order statistics, which use constant, linear, and quadratic terms (zeroth, first, and second powers). The third and higher moments, as used in the skewness and kurtosis, are examples of HOS, whereas the first and second moments, as used in the arithmetic mean (first), and variance (second) are examples of low-order statistics. HOS are particularly used in estimation of shape parameters, such as skewness and kurtosis, as when measuring the deviation of a distribution from the normal distribution. On the other hand, due to the higher powers, HOS are significantly less robust than lower-order statistics.\n\nIn statistical theory, one long-established approach to higher-order statistics, for univariate and multivariate distributions is through the use of cumulants and joint cumulants. In time series analysis, the extension of these is to higher order spectra, for example the bispectrum and trispectrum.\n\nAn alternative to the use of HOS and higher moments is to instead use L-moments, which are linear statistics (linear combinations of order statistics), and thus more robust than HOS.\n\n"}
{"id": "43027438", "url": "https://en.wikipedia.org/wiki?curid=43027438", "title": "Hilary Priestley", "text": "Hilary Priestley\n\nHilary Ann Priestley is a British mathematician. She is a professor at the University of Oxford and a Fellow of St Anne's College, Oxford, where she has been Tutor in Mathematics since 1972.\n\nHilary Priestley introduced ordered separable topological spaces that are important in the study of distributive lattices; such topological spaces are now usually called Priestley spaces in her honour. The term \"Priestley duality\" is also used. In addition, Priestly has contributed to the representation theory of distributive lattices.\n\nFollowing Priestley's academic advisors back by 15 generations on the Mathematics Genealogy Project database, one arrives at Isaac Newton.\n\n\n"}
{"id": "56590405", "url": "https://en.wikipedia.org/wiki?curid=56590405", "title": "Ineke De Moortel", "text": "Ineke De Moortel\n\nIneke De Moortel is a Belgian applied mathematician in Scotland, where she is a professor of applied mathematics at the University of St Andrews, director of research in the School of Mathematics and Statistics at St Andrews, and president of the Edinburgh Mathematical Society. Her research concerns the computational and mathematical modelling of solar physics, and particularly of the sun's corona.\n\nDe Moortel earned a master's degree in mathematics in 1997 at KU Leuven. She completed a Ph.D. in solar physics in 2001 at the University of St Andrews; her dissertation, \"Theoretical & Observational Aspects of Wave Propagation in the Solar Corona\", was supervised by Alan Hood. She remained at St Andrews as a postdoctoral researcher and research fellow, becoming a reader there in 2008 and a professor in 2013.\n\nIn 2005, De Moortel became a Fellow of the Royal Astronomical Society.\nIn 2009 she won the Philip Leverhulme Prize in Astronomy and Astrophysics.\nShe was elected to the Royal Society of Edinburgh in 2015, and previously co-chaired its affiliate society, the Young Academy of Scotland.\n\n"}
{"id": "58527587", "url": "https://en.wikipedia.org/wiki?curid=58527587", "title": "JanusGraph", "text": "JanusGraph\n\nJanusGraph is an open source, distributed graph database under The Linux Foundation. JanusGraph is available under Apache Software License 2.0. The project is supported by IBM, Google, Hortonworks.\n\nJanusGraph supports various storage backends (Apache Cassandra, Apache HBase, Google Cloud Bigtable, Oracle BerkeleyDB). Scalability of JanusGraph depends on the underlying technologies, which are used with JanusGraph. For example, by using Apache Cassandra as a storage backend scaling to multiple datacenters is provided out of the box.\n\nJanusGraph supports global graph data analytics, reporting, and ETL through integration with big data platforms (Apache Spark, Apache Giraph, Apache Hadoop).\n\nJanusGraph supports geo, numeric range, and full-text search via external index storages (ElasticSearch, Apache Solr, Apache Lucene).\n\nJanusGraph has native integration with the Apache TinkerPop graph stack (Gremlin graph query language, Gremlin graph server, Gremlin applications).\n\nJanusGraph is the fork of TitanDB graph database which is being developed since 2012.\n\n\nJanusGraph is available under Apache Software License 2.0. \n\nFor contributions an individual or an organisation must sign a CLA paper.\n\n\n\n"}
{"id": "1187928", "url": "https://en.wikipedia.org/wiki?curid=1187928", "title": "Jensen's alpha", "text": "Jensen's alpha\n\nIn finance, Jensen's alpha (or Jensen's Performance Index, ex-post alpha) is used to determine the abnormal return of a security or portfolio of securities over the theoretical expected return. It is a version of the standard alpha based on a theoretical performance index instead of a market index.\n\nThe security could be any asset, such as stocks, bonds, or derivatives. The theoretical return is predicted by a market model, most commonly the capital asset pricing model (CAPM). The market model uses statistical methods to predict the appropriate risk-adjusted return of an asset. The CAPM for instance uses beta as a multiplier.\n\nJensen's alpha was first used as a measure in the evaluation of mutual fund managers by Michael Jensen in 1968. The CAPM return is supposed to be 'risk adjusted', which means it takes account of the relative riskiness of the asset.\n\nThis is based on the concept that riskier assets should have higher expected returns than less risky assets. If an asset's return is even higher than the risk adjusted return, that asset is said to have \"positive alpha\" or \"abnormal returns\". Investors are constantly seeking investments that have higher alpha.\n\nSince Eugene Fama, many academics believe financial markets are too efficient to allow for repeatedly earning positive Alpha, unless by chance. Nevertheless, Alpha is still widely used to evaluate mutual fund and portfolio manager performance, often in conjunction with the Sharpe ratio and the Treynor ratio.\n\nIn the context of CAPM, calculating alpha requires the following inputs:\n\nJensen's alpha = Portfolio Return − [Risk Free Rate + Portfolio Beta * (Market Return − Risk Free Rate)]\n\nAn additional way of understanding the definition can be obtained by rewriting it as:\n\nIf we define the excess return of the fund (market) over the risk free return as formula_7 and formula_8 then Jensen's alpha can be expressed as:\n\nJensen's alpha is a statistic that is commonly used in empirical finance to assess the marginal return associated with unit exposure to a given strategy. Generalizing the above definition to the multifactor setting, Jensen's alpha is a measure of the marginal return associated with an additional strategy that is not explained by existing factors.\n\nWe obtain the CAPM alpha if we consider excess market returns as the only factor. If we add in the Fama-French factors, we obtain the 3-factor alpha, and so on. If Jensen's alpha is significant and positive, then the strategy being considered has a history of generating returns on top of what would be expected based on other factors alone. For example, in the 3-factor case, we may regress momentum factor returns on 3-factor returns to find that momentum generates a significant premium on top of size, value, and market returns.\n"}
{"id": "11167326", "url": "https://en.wikipedia.org/wiki?curid=11167326", "title": "Littelmann path model", "text": "Littelmann path model\n\nIn mathematics, the Littelmann path model is a combinatorial device due to Peter Littelmann for computing multiplicities \"without overcounting\" in the representation theory of symmetrisable Kac–Moody algebras. Its most important application is to complex semisimple Lie algebras or equivalently compact semisimple Lie groups, the case described in this article. Multiplicities in irreducible representations, tensor products and branching rules can be calculated using a coloured directed graph, with labels given by the simple roots of the Lie algebra.\n\nDeveloped as a bridge between the theory of crystal bases arising from the work of Kashiwara and Lusztig on quantum groups and the standard monomial theory of C. S. Seshadri and Lakshmibai, Littelmann's path model associates to each irreducible representation a rational vector space with basis given by paths from the origin to a weight as well as a pair of root operators acting on paths for each simple root. This gives a direct way of recovering the algebraic and combinatorial structures previously discovered by Kashiwara and Lusztig using quantum groups.\n\nSome of the basic questions in the representation theory of complex semisimple Lie algebras or compact semisimple Lie groups going back to Hermann Weyl include:\n\n\nAnswers to these questions were first provided by Hermann Weyl and Richard Brauer as consequences of explicit character formulas, followed by later combinatorial formulas of Hans Freudenthal, Robert Steinberg and Bertram Kostant; see . An unsatisfactory feature of these formulas is that they involved alternating sums for quantities that were known a priori to be non-negative. Littelmann's method expresses these multiplicities as sums of non-negative integers \"without overcounting\". His work generalizes classical results based on Young tableaux for the general linear Lie algebra formula_5 or the special linear Lie algebra formula_6:\n\n\nAttempts at finding similar algorithms without overcounting for the other classical Lie algebras had only been partially successful.\n\nLittelmann's contribution was to give a unified combinatorial model that applied to all symmetrizable Kac–Moody algebras and provided explicit subtraction-free combinatorial formulas for weight multiplicities, tensor product rules and branching rules. He accomplished this by introducing the vector space \"V\" over Q generated by the weight lattice of a Cartan subalgebra; on the vector space of piecewise-linear paths in \"V\" connecting the origin to a weight, he defined a pair of \"root operators\" for each simple root of formula_3.\nThe combinatorial data could be encoded in a coloured directed graph, with labels given by the simple roots.\n\nLittelmann's main motivation was to reconcile two different aspects of representation theory:\n\n\nAlthough differently defined, the crystal basis, its root operators and crystal graph were later shown to be equivalent to Littelmann's path model and graph; see . In the case of complex semisimple Lie algebras, there is a simplified self-contained account in relying only on the properties of root systems; this approach is followed here.\n\nLet \"P\" be the weight lattice in the dual of a Cartan subalgebra of the semisimple Lie algebra formula_3.\n\nA Littelmann path is a piecewise-linear mapping\n\nsuch that π(0) = 0 and π(1) is a weight.\n\nLet (\"H\") be the basis of formula_15 consisting of \"coroot\" vectors, dual to basis of formula_15* formed by simple roots (α). For fixed α and a path π, the function formula_17 has a minimum value \"M\".\n\nDefine non-decreasing self-mappings \"l\" and \"r\" of [0,1] formula_18 Q by\n\nThus \"l\"(\"t\") = 0 until the last time that \"h\"(\"s\") = \"M\" and \"r\"(\"t\") = 1 after the first time that \"h\"(\"s\") = \"M\".\n\nDefine new paths π and π by\n\nThe root operators \"e\" and \"f\" are defined on a basis vector [π] by\n\n\nThe key feature here is that the paths form a basis for the root operators like that of a monomial representation: when a root operator is applied to the basis element for a path, the result is either 0 or the basis element for another path.\n\nLet formula_23 be the algebra generated by the root operators. Let π(\"t\") be a path lying wholly within the positive Weyl chamber defined by the simple roots. Using results on the path model of C. S. Seshadri and Lakshmibai, Littelmann showed that\n\n\nThere is also an action of the Weyl group on paths [π]. If α is a simple root and \"k\" = \"h\"(1), with \"h\" as above, then the corresponding reflection \"s\" acts as follows:\n\n\nIf π is a path lying wholly inside the positive Weyl chamber, the Littelmann graph formula_25 is defined to be the coloured, directed graph having as vertices the non-zero paths obtained by successively applying the operators \"f\" to π. There is a directed arrow from one path to another labelled by the simple root α, if the target path is obtained from the source path by applying \"f\".\n\n\nThe Littelmann graph therefore only depends on λ. Kashiwara and Joseph proved that it coincides with the \"crystal graph\" defined by Kashiwara in the theory of crystal bases.\n\nIf π(1) = λ, the multiplicity of the weight μ in \"L\"(λ) is the number of vertices σ in the Littelmann graph formula_26 with σ(1) = μ.\n\nLet π and σ be paths in the positive Weyl chamber with π(1) = λ and σ(1) = μ. Then\n\nwhere τ ranges over paths in formula_28 such that π formula_29 τ lies entirely in the positive Weyl chamber and\nthe \"concatenation\" π formula_29 τ (t) is defined as π(2\"t\") for \"t\" ≤ 1/2 and π(1) + τ( 2\"t\" – 1) for \"t\" ≥ 1/2.\n\nIf formula_2 is the Levi component of a parabolic subalgebra of formula_3 with weight lattice \"P\" formula_33 \"P\" then\n\nwhere the sum ranges over all paths σ in formula_25 which lie wholly in the positive Weyl chamber for formula_2.\n\n\n"}
{"id": "23211888", "url": "https://en.wikipedia.org/wiki?curid=23211888", "title": "Neo-Riemannian theory", "text": "Neo-Riemannian theory\n\nNeo-Riemannian theory is a loose collection of ideas present in the writings of music theorists such as David Lewin, Brian Hyer, Richard Cohn, and Henry Klumpenhouwer. What binds these ideas is a central commitment to relating harmonies directly to each other, without necessary reference to a tonic. Initially, those harmonies were major and minor triads; subsequently, neo-Riemannian theory was extended to standard dissonant sonorities as well. Harmonic proximity is characteristically gauged by efficiency of voice leading. Thus, C major and E minor triads are close by virtue of requiring only a single semitonal shift to move from one to the other. Motion between proximate harmonies is described by simple transformations. For example, motion between a C major and E minor triad, in either direction, is executed by an \"L\" transformation. Extended progressions of harmonies are characteristically displayed on a geometric plane, or map, which portrays the entire system of harmonic relations. Where consensus is lacking is on the question of what is most central to the theory: smooth voice leading, transformations, or the system of relations that is mapped by the geometries. The theory is often invoked when analyzing harmonic practices within the Late Romantic period characterized by a high degree of chromaticism, including work of Schubert, Liszt, Wagner and Bruckner.\n\nNeo-Riemannian theory is named after Hugo Riemann (1849–1919), whose \"dualist\" system for relating triads was adapted from earlier 19th-century harmonic theorists. (The term \"dualism\" refers to the emphasis on the inversional relationship between major and minor, with minor triads being considered \"upside down\" versions of major triads; this \"dualism\" is what produces the change-in-direction described above. See also: Utonality) In the 1880s, Riemann proposed a system of transformations that related triads directly to each other The revival of this aspect of Riemann's writings, independently of the dualist premises under which they were initially conceived, originated with David Lewin (1933–2003), particularly in his article \"Amfortas's Prayer to Titurel and the Role of D in Parsifal\" (1984) and his influential book, \"Generalized Musical Intervals and Transformations\" (1987). Subsequent development in the 1990s and 2000s has expanded the scope of neo-Riemannian theory considerably, with further mathematical systematization to its basic tenets, as well as inroads into 20th century repertoires and music psychology.\n\nThe principal transformations of neo-Riemannian triadic theory connect triads of different species (major and minor), and are their own inverses (a second application undoes the first). These transformations are purely harmonic, and do not need any particular voice leading between chords: all instances of motion from a C major to a C minor triad represent the same neo-Riemannian transformation, no matter how the voices are distributed in register.\n\nThe three transformations move one of the three notes of the triad to produce a different triad:\n\nObserve that P preserves the \"perfect fifth\" interval (so given say C and G there are only two candidates for the third note: E and E), L preserves the \"minor third\" interval (given E and G our candidates are C and B) and R preserves the \"major third\" interval (given C and E our candidates are G and A).\n\nSecondary operations can be constructed by combining these basic operations:\n\n\nAny combination of the L, P, and R transformations will act inversely on major and minor triads: for instance, R-then-P transposes C major down a minor third, to A major via A minor, whilst transposing C minor to E minor up a minor 3rd via E major.\n\nInitial work in neo-Riemannian theory treated these transformations in a largely harmonic manner, without explicit attention to voice leading. Later, Cohn pointed out that neo-Riemannian concepts arise naturally when thinking about certain problems in voice leading. For example, two triads (major or minor) share two common tones and can be connected by stepwise voice leading the third voice if and only if they are linked by one of the L, P, R transformations described above. (This property of stepwise voice leading in a single voice is called voice-leading parsimony.) Note that here the emphasis on inversional relationships arises naturally, as a byproduct of interest in \"parsimonious\" voice leading, rather than being a fundamental theoretical postulate, as it was in Riemann's work.\n\nMore recently, Dmitri Tymoczko has argued that the connection between neo-Riemannian operations and voice leading is only approximate (see below). Furthermore, the formalism of neo-Riemannian theory treats voice leading in a somewhat oblique manner: \"neo-Riemannian transformations,\" as defined above, are purely harmonic relationships that do not necessarily involve any particular mapping between the chords' notes.\n\nNeo-Riemannian transformations can be modeled with several interrelated geometric structures. The Riemannian Tonnetz (\"tonal grid,\" shown on the right) is a planar array of pitches along three simplicial axes, corresponding to the three consonant intervals. Major and minor triads are represented by triangles which tile the plane of the Tonnetz. Edge-adjacent triads share two common pitches, and so the principal transformations are expressed as minimal motion of the Tonnetz. Unlike the historical theorist for which it is named, neo-Riemannian theory typically assumes enharmonic equivalence (G = A), which wraps the planar graph into a torus.\n\nAlternate tonal geometries have been described in neo-Riemannian theory that isolate or expand upon certain features of the classical Tonnetz. Richard Cohn developed the Hyper Hexatonic system to describe motion within and between separate major third cycles, all of which exhibit what he formulates as \"maximal smoothness.\" (Cohn, 1996). Another geometric figure, Cube Dance, was invented by Jack Douthett; it features the geometric dual of the Tonnetz, where triads are vertices instead of triangles (Douthett and Steinbach, 1998) and are interspersed with augmented triads, allowing smoother voice-leadings.\n\nMany of the geometrical representations associated with neo-Riemannian theory are unified into a more general framework by the continuous voice-leading spaces explored by Clifton Callender, Ian Quinn, and Dmitri Tymoczko. This work originates in 2004, when Callender described a continuous space in which points represented three-note \"chord types\" (such as \"major triad\"), using the space to model \"continuous transformations\" in which voices slid continuously from one note to another. Later, Tymoczko showed that paths in Callender's space were isomorphic to certain classes of voice leadings (the \"individually T related\" voice leadings discussed in Tymoczko 2008) and developed a family of spaces more closely analogous to those of neo-Riemannian theory. In Tymoczko's spaces, points represent particular chords of any size (such as \"C major\") rather than more general chord types (such as \"major triad\"). Finally, Callender, Quinn, and Tymoczko together proposed a unified framework connecting these and many other geometrical spaces representing diverse range of music-theoretical properties.\n\nThe Harmonic table note layout is a modern day realisation of this graphical representation to create a musical interface.\n\nIn 2011, Gilles Baroin presented the Planet-4D model, a new vizualisation system based on graph theory that embeds the traditional Tonnetz on a 4D Hypersphere. Another recent continuous version of the Tonnetz — simultaneously in original and dual form — is the Torus of phases which enables even finer analyses, for instance in early romantic music.\n\nNeo-Riemannian theorists often analyze chord progressions as combinations of the three basic LPR transformations, the only ones that preserve two common tones. \nThus the progression from C major to E major might be analyzed as L-then-P, which is a 2-unit motion since it involves two transformations. (This same transformation sends C minor to A minor, since L of C minor is A major, while P of A major is A minor.) These distances reflect voice-leading only imperfectly. For example, according to strains of neo-Riemannian theory that prioritize common-tone preservation, the C major triad is closer to F major than to F minor, since C major can be transformed into F major by R-then-L, while it takes three moves to get from C major to F minor (R-then-L-then-P). However, from a chromatic voice-leading perspective F minor is closer to C major than F major is, since it takes just two semitones of motion to transform F minor into C major (A->G and F->E) whereas it takes three semitones to transform F major into C major. Thus LPR transformations are unable to account for the voice-leading efficiency of the IV-iv-I progression, one of the basic routines of nineteenth-century harmony. Note that similar points can be made about common tones: on the Tonnetz, F minor and E minor are both three steps from C major, even though F minor and C major have one common tone, while E minor and C major have none.\n\nUnderlying these discrepancies are different ideas about whether harmonic proximity is maximized when two common tones are shared, or when the total voice-leading distance is minimized. For example, in the R transformation, a single voice moves by whole step; in the N or S transformation, two voices move by semitone. When common-tone maximization is prioritized, R is more efficient; when voice-leading efficiency is measured by summing the motions of the individual voices, the transformations are equivalently efficient. Early neo-Riemannian theory conflated these two conceptions. More recent work has disentangled them, and measures distance unilaterally by voice-leading proximity independently of common-tone preservation. Accordingly, the distinction between \"primary\" and \"secondary\" transformations becomes problematized. As early as 1992, Jack Douthett created an exact geometric model of inter-triadic voice-leading by interpolating augmented triads between R-related triads, which he called \"Cube Dance\". Though Douthett's figure was published in 1998, its superiority as a model of voice leading was not fully appreciated until much later, in the wake of the geometrical work of Callender, Quinn, and Tymoczko; indeed, the first detailed comparison of \"Cube Dance\" to the neo-Riemannian \"Tonnetz\" appeared in 2009, more than fifteen years after Douthett's initial discovery of his figure. In this line of research, the triadic transformations lose the foundational status that they held in the early phases of neo-Riemannian theory. The geometries to which voice-leading proximity give rise attain central status, and the transformations become heuristic labels for certain kinds of standard routines, rather than their defining property.\n\nNonetheless, among all possible sets of the twenty-four Riemannian triadic transformations, the length of combinations of members from the set of L, P, and R transformations better correlates with chromatic voice-leading distance than nearly every other set of transformations. For example, if only L and R transformations were used to measure transformational distance between triads, the number of contradictions between transformational distance and voice-leading distance like those examples above is much greater than when using L, P, and R. This partially restores some distinction between \"primary\" and \"secondary\" transformations.\n\nBeyond its application to triadic chord progressions, neo-Riemannian theory has inspired numerous subsequent investigations. These include\n\n\nSome of these extensions share neo-Riemannian theory's concern with non-traditional relations among familiar tonal chords; others apply voice-leading proximity or harmonic transformation to characteristically atonal chords.\n\n\nTouchTonnetz - an interactive mobile app to explore Neo-Riemannian Theory - Android or iPhone\n\n"}
{"id": "228361", "url": "https://en.wikipedia.org/wiki?curid=228361", "title": "Open formula", "text": "Open formula\n\nAn open formula is a formula that contains at least one free variable. Some educational resources use the term \"open sentence\", but this use conflicts with the definition of \"sentence\" as a formula that does not contain any free variables.\n\n"}
{"id": "48404101", "url": "https://en.wikipedia.org/wiki?curid=48404101", "title": "Peter Comrie", "text": "Peter Comrie\n\nPeter Comrie FRSE LLD EIS (1868–1944) was a Scottish mathematician and educator. He served as Rector of Leith Academy 1922 to 1933 and President of the Edinburgh Mathematical Society 1916–17.\n\nHe was born on 17 July 1868 in Muthill the son of Peter Comrie, master blacksmith and Elizabeth Ritchie.\nHe was educated at Muthill School and then Morrison's Academy in Crieff. He then won a place at St Andrews University studying Mathematics, graduating BSc, MA.\nFrom 1895 he began working as a mathematics teacher, firstly in Greenock Academy, then Hutcheson's Grammar School, and Robert Gordon's College. In 1904 he moved to Edinburgh and remained there for the rest of his life, but teaching in various schools. Firstly in Boroughmuir High School then in 1917 received his first role as headmaster: at Castlehill School at the head of the Royal Mile. In 1922 he moved to be headmaster of Leith Academy where he remained until retiral in 1933.\n\nHe was elected a Fellow of the Royal Society of Edinburgh in 1909 his main proposer being Sir James Donaldson.\nSt Andrews University awarded him an honorary doctorate (LLD) in 1928.\n\nHe died at home 19 Craighouse Terrace in Edinburgh on 20 December 1944.\n\n\n\nComrie was married to Charlotte Aikman from St Andrews.\n"}
{"id": "34790475", "url": "https://en.wikipedia.org/wiki?curid=34790475", "title": "Phragmen–Brouwer theorem", "text": "Phragmen–Brouwer theorem\n\nIn topology, the Phragmén–Brouwer theorem, introduced by Lars Edvard Phragmén and Luitzen Egbertus Jan Brouwer, states that if \"X\" is a normal connected locally connected topological space, then the following two properties are equivalent:\n\nThe theorem remains true with the weaker condition that \"A\" and \"B\" be separated.\n\n"}
{"id": "39720474", "url": "https://en.wikipedia.org/wiki?curid=39720474", "title": "Poincaré separation theorem", "text": "Poincaré separation theorem\n\nIn mathematics, the Poincaré separation theorem gives the upper and lower bounds of eigenvalues of a real symmetric matrix \"B<nowiki>'</nowiki>AB\" that can be considered as the orthogonal projection of a larger real symmetric matrix \"A\" onto a linear subspace spanned by the columns of \"B\". The theorem is named after Henri Poincaré.\n\nMore specifically, let \"A\" be an \"n\" × \"n\" real symmetric matrix and \"B\" an \"n\" × \"r\" semi-orthogonal matrix such that \"B<nowiki>'</nowiki>B\" = \"I\". Denote by formula_1, \"i\" = 1, 2, ..., \"n\" and formula_2, \"i\" = 1, 2, ..., \"r\" the eigenvalues of \"A\" and \"B<nowiki>'</nowiki>AB\", respectively (in descending order). We have\n\nAn algebraic proof, based on the variational interpretation of eigenvalues, has been published in Magnus' \"Matrix Differential Calculus with Applications in Statistics and Econometrics\". From the geometric point of view, \"B'AB\" can be considered as the orthogonal projection of \"A\" onto the linear subspace spanned by \"B\", so the above results follow immediately.\n"}
{"id": "3492608", "url": "https://en.wikipedia.org/wiki?curid=3492608", "title": "Poly-Bernoulli number", "text": "Poly-Bernoulli number\n\nIn mathematics, poly-Bernoulli numbers, denoted as formula_1, were defined by M. Kaneko as\n\nwhere \"Li\" is the polylogarithm. The formula_3 are the usual Bernoulli numbers.\n\nMoreover, the Generalization of Poly-Bernoulli numbers with a,b,c parameters defined as follows\n\nwhere \"Li\" is the polylogarithm. \n\nKaneko also gave two combinatorial formulas:\n\nwhere formula_7 is the number of ways to partition a size formula_8 set into formula_9 non-empty subsets (the Stirling number of the second kind).\n\nA combinatorial interpretation is that the poly-Bernoulli numbers of negative index enumerate the set of formula_8 by formula_9 (0,1)-matrices uniquely reconstructible from their row and column sums.\n\nFor a positive integer \"n\" and a prime number \"p\", the poly-Bernoulli numbers satisfy\n\nwhich can be seen as an analog of Fermat's little theorem. Further, the equation\n\nhas no solution for integers \"x\", \"y\", \"z\", \"n\" > 2; an analog of Fermat's last theorem.\nMoreover, there is an analogue of Poly-Bernoulli numbers (like Bernoulli numbers and Euler numbers) which is known as Poly-Euler numbers\n\n\n"}
{"id": "54112321", "url": "https://en.wikipedia.org/wiki?curid=54112321", "title": "Polynomial matrix spectral factorization", "text": "Polynomial matrix spectral factorization\n\nPolynomial matrices are widely studied in the fields of systems theory and control theory and have seen other uses relating to stable polynomials. In stability theory, Spectral Factorization has been used to find determinental matrix representations for bivariate stable polynomials and real zero polynomials. A key tool used to study these is a matrix factorization known as either the Polynomial Matrix Spectral Factorization or the Matrix Fejer–Riesz Theorem.\n\nGiven a univariate positive polynomial formula_1, a polynomial which takes on non-negative values for any real input formula_2, the Fejer–Riesz Theorem yields the polynomial spectral factorization formula_3. Results of this form are generically referred to as Positivstellensatz. Considering positive definiteness as the matrix analogue of positivity, Polynomial Matrix Spectral Factorization provides a similar factorization for polynomial matrices which have positive definite range. This decomposition also relates to the Cholesky decomposition for scalar matrices formula_4. This result was originally proven by Wiener in a more general context which was concerned with integrable matrix-valued functions that also had integrable log determinant. Because applications are often concerned with the polynomial restriction, simpler proofs and individual analysis exist focusing on this case. Weaker positivstellensatz conditions have been studied, specifically considering when the polynomial matrix has positive definite image on semi-algebraic subsets of the reals. Many publications recently have focused on streamlining proofs for these related results. This article roughly follows the recent proof method of Lasha Ephremidze which relies only on elementary linear algebra and complex analysis.\n\nSpectral Factorization is used extensively in linear–quadratic–Gaussian control. Because of this application there have been many algorithms to calculate spectral factors. Some modern algorithms focus on the more general setting originally studied by Wiener. In the formula_5 case the problem is known as polynomial spectral factorization, or Fejer-Riesz Theorem, and has many classical algorithms. Some modern algorithms have used Toeplitz matrix advances to speed up factor calculations.\n\nLet formula_6be a polynomial matrix where each entry formula_7 is a complex coefficient polynomial of degree at most formula_8. Suppose that for almost all formula_9 we have formula_10 is a positive definite hermitian matrix. Then there exists a polynomial matrix formula_11 such that formula_12 for all formula_9. We can furthermore find formula_11 which is nonsingular on the lower half plane.\n\nNote that if formula_15then formula_16. When formula_17 is a complex coefficient polynomial or complex coefficient rational function we have formula_18is also a polynomial or rational function respectively. For formula_9 we haveformula_20Since the entries of formula_21 and formula_10 are complex polynomials which agree on the real line, they are in fact the same polynomials. We can conclude they in fact agree for all complex inputs.\n\nLet formula_1 be a rational polynomial function where formula_24 for almost all formula_9. Then there exists rational formula_26 with formula_3 where formula_26 has no poles or zeroes in the lower half plane. This decomposition is unique up to multiplication by complex scalars of norm formula_29. This is related to the statement of the Polynomial Matrix Spectral Factorization theorem restricted to the formula_5 case.\n\nTo prove existence write formula_31where formula_32. Letting formula_33we can conclude that formula_34 is real and positive. Dividing out by formula_35 we reduce to the monic scenario. The numerator and denominator have distinct sets of roots, so all real roots which show up in either must have even multiplicity (to prevent a sign change locally). We can divide out these real roots to reduce to the case where formula_1 has only complex roots and poles. By hypothesis we have formula_37. Since all of the formula_38are complex (and hence not fixed points of conjugation) they both come in conjugate pairs. For each conjugate pair, pick the zero or pole in the upper half plane and accumulate these to obtain formula_26. The uniqueness result follows in a standard fashion.\n\nThe inspiration for this result is a factorization which characterizes positive definite matrices.\n\nGiven any positive definite scalar matrix formula_40, we are guaranteed by Cholesky decomposition formula_41 where formula_42 is a lower triangular matrix. If we don't restrict to lower triangular matrices we can consider all factorizations of the form formula_43. It is not hard to check that all factorizations are achieved by looking at the orbit of formula_42 under right multiplication by a unitary matrix, formula_45.\n\nTo obtain the lower triangular decomposition we induct by splitting off the first row and first column:formula_46Solving these in terms of formula_47 we getformula_48formula_49formula_50\n\nSince formula_40 is positive definite we have formula_52is a positive real number, so it has a square root. The last condition from induction since the right hand side is the Schur complement of formula_40, which is itself positive definite.\n\nNow consider formula_6where the formula_7are complex rational functions and formula_10 is positive definite hermitian for almost all real formula_2. Then by the symmetric Gaussian elimination we performed above, all we need to show is there exists a rational formula_58 such that formula_59for real formula_2, which follows from our rational spectral factorization. Once we have that then we can solve for formula_61. Since the Schur complement is positive definite for the real formula_2 away from the poles and the Schur complement is a rational polynomial matrix we can induct to find formula_63.\n\nIt is not hard to check that we in fact get formula_64where formula_65 is a rational polynomial matrix with no poles in the lower half plane.\n\nTo prove the existence of polynomial matrix spectral factorization, we begin with the rational polynomial matrix Cholesky Decomposition and modify it to remove lower half plane singularities. Namely given formula_6with each entry formula_7 a complex coefficient polynomial we have rational polynomial matrix formula_65 with formula_64for real formula_2, where formula_65 has no lower half plane poles. Given a rational polynomial matrix formula_72 which is unitary valued for real formula_2, there exists another decomposition,formula_74.\n\nIf formula_75 then there exists a scalar unitary matrix formula_76such that formula_77. This implies formula_78has first column vanish at formula_79. To remove the singularity at formula_79 we multiply byformula_81formula_82has determinant with one less zero (by multiplicity) at a, without introducing any poles in the lower half plane of any of the entries.\n\nAfter modifications, the decomposition formula_83satisfies formula_11 is analytic and invertible on the lower half plane. To extend analyticity to the upper half plane we need this key observation: Given a rational matrix formula_11 who is analytic in the lower half plane and nonsingular in the lower half plane, we have formula_86 is analytic and nonsingular in the lower half plane. The analyticity follows from the adjugate matrix formula (since both the entries of formula_11 and formula_88are analytic on the lower half plane). The nonsingularity follows from formula_89which can only have zeroes at places where formula_90 had poles. The determinant of a rational polynomial matrix can only have poles where its entries have poles, so formula_90 has no poles in the lower half plane.\n\nFrom our observation in Extension to Complex Inputs, we have formula_92for all complex numbers. This implies formula_93. Since formula_86 is analytic on the lower half plane, formula_11 is analytic on the upper half plane. Finally if formula_11 has a pole on the real line then formula_97has the same pole on the real line which contradicts the fact formula_10 has no poles on the real line (it is analytic everywhere by hypothesis).\n\nThe above shows that if formula_11 is analytic and invertible on the lower half plane indeed formula_11 is analytic everywhere and hence a polynomial matrix.\n\nGiven two polynomial matrix decompositions which are invertible on the lower half planeformula_101 we rearrange to formula_102. Since formula_103 is analytic on the lower half plane and nonsingular, formula_104is a rational polynomial matrix which is analytic and invertible on the lower half plane. Then by the same argument as above we have formula_104 is in fact a polynomial matrix which is unitary for all real formula_2. This means that if formula_107 is the formula_108th row of formula_11then formula_110. For real formula_2 this is a sum of non-negative polynomials which sums to a constant, implying each of the summands are in fact constant polynomials. Then formula_112where formula_76 is a scalar unitary matrix.\n\nConsider formula_114. Then through symmetric Gaussian elimination we get the rational decomposition formula_115. This decomposition has no poles in the upper half plane. However the determinant is formula_116, so we need to modify our decomposition to get rid of the singularity at formula_117. First we multiply by a scalar unitary matrix to make a column vanish at formula_108. Consider formula_119. Then we have a new candidate for our decompositionformula_120. Now the first column vanishes at\n\nformula_108, so we multiply through (on the right) by formula_122 to obtain formula_123 Notice formula_124. This is our desired decomposition formula_125 with no singularities in the lower half plane.\n"}
{"id": "18411966", "url": "https://en.wikipedia.org/wiki?curid=18411966", "title": "Power graph analysis", "text": "Power graph analysis\n\nIn computational biology, power graph analysis is a method for the analysis and\nrepresentation of complex networks. Power graph analysis is the computation, analysis and visual representation of a power graph from a graph (networks).\n\nPower graph analysis can be thought of as a lossless compression algorithm for graphs. It extends graph syntax with representations of cliques, bicliques and stars. Compression levels of up to 95% have been obtained for complex biological networks.\n\nHypergraphs are a generalization of graphs in which edges are not just couples of nodes but arbitrary n-tuples. Power graphs are not another generalization of graphs, but instead a novel representation of graphs that proposes a shift from the \"node and edge\" language to one using cliques, bicliques and stars as primitives.\n\nGraphs are drawn with circles or points that represent nodes and lines connecting pairs of nodes that represent edges. Power graphs extend the syntax of graphs with power nodes, which are drawn as a circle enclosing nodes or \"other power nodes\", and power edges, which are lines between power nodes.\n\nBicliques are two sets of nodes with an edge between every member of one set and every member of the other set. In a power graph, a biclique is represented as an edge between two power nodes.\n\nCliques are a set of nodes with an edge between every pair of nodes. In a power graph, a clique is represented by a power node with a loop.\n\nStars are a set of nodes with an edge between every member of that set and a single node outside the set. In a power graph, a star is represented by a power edge between a regular node and a power node.\n\nGiven a graph formula_1 where formula_2 is the set of nodes and formula_3 is the set of edges, a power graph formula_4 is a graph defined on the power set formula_5 of power nodes connected to each other by power edges: formula_6. Hence power graphs are defined on the power set of nodes as well as on the power set of edges of the graph formula_7.\n\nThe semantics of power graphs are as follows: if two power nodes are connected by a power edge, this means that all nodes of the first power node are connected to all nodes of the second power node. Similarly, if a power node is connected to itself by a power edge, this signifies that all nodes in the power node are connected to each other by edges.\n\nThe following two conditions are required:\n\nThe Fourier analysis of a function\ncan be seen as a rewriting of the function in terms of harmonic functions instead of\nformula_8 pairs. This transformation changes the point of view from time domain\nto frequency domain and enables many interesting applications in signal analysis, data compression, \nand filtering.\nSimilarly, Power Graph Analysis is a rewriting or decomposition of a network using bicliques, cliques and stars\nas primitive elements (just as harmonic functions for Fourier analysis). \nIt can be used to analyze, compress and filter networks.\nThere are, however, several key differences. First, in Fourier analysis the two spaces (time and frequency domains)\nare the same function space - but stricto sensu, power graphs are not graphs.\nSecond, there is not a unique power graph representing a given graph. Yet a very interesting class of power graphs \nare minimal power graphs which have the least number of power edges and power nodes necessary to represent a given graph.\n\nIn general, there is no unique minimal power graph for a given graph.\nIn this example (right) a graph of four nodes and five edges admits two minimal power graphs of two power edges each.\nThe main difference between these two minimal power graphs is the higher nesting level of the second power graph as well as a loss of symmetry with respect to the underlying graph.\nLoss of symmetry is only a problem in small toy examples since complex networks rarely exhibit such symmetries in the first place.\nAdditionally, one can minimize the nesting level but even then, there is in general not a unique minimal power graph of minimal nesting level.\n\nThe power graph greedy algorithm relies on two simple steps to perform the decomposition:\n\nThe first step identifies candidate power nodes through a hierarchical clustering of the nodes in the network \nbased on the similarity of their neighboring nodes. The similarity of two sets of neighbors is taken as the Jaccard index \nof the two sets.\n\nThe second step performs a greedy search for possible power edges between candidate power nodes.\nPower edges abstracting the most edges in the original network are added first to the power graph. \nThus bicliques, cliques and stars are incrementally replaced with power edges, until all remaining single edges are also added.\nCandidate power nodes that are not the end point of any power edge are ignored.\n\nModular decomposition can be used to compute a power graph by using \nthe strong modules of the modular decomposition.\nModules in modular decomposition are groups of nodes in a graph that\nhave identical neighbors. A Strong Module is a module that does not overlap \nwith another module. \nHowever, in complex networks strong modules are more the exception than the \nrule. Therefore, the power graphs obtained through modular decomposition are far \nfrom minimality.\nThe main difference between modular decomposition and power graph analysis is the \nemphasis of power graph analysis in decomposing graphs not only using modules of nodes \nbut also modules of edges (cliques, bicliques). Indeed, power graph analysis can be seen as a loss-less \nsimultaneous clustering of both nodes and edges.\n\nPower Graph Analysis has been shown to be useful for the analysis of several types of biological networks such as Protein-protein interaction networks, domain-peptide binding motifs, Gene regulatory networks and Homology/Paralogy networks. \nAlso a network of significant disease-trait pairs have been recently visualized and analyzed with Power Graphs.\n\nNetwork compression, a new measure derived from Power Graphs, has been proposed as a quality measure for protein interaction networks.\n\nPower Graphs have been also applied to the analysis of drug-target-disease networks for Drug repositioning.\n\nPower Graphs have been applied to large-scale data in social networks, for community mining or for modeling author types.\n\n\n"}
{"id": "25280", "url": "https://en.wikipedia.org/wiki?curid=25280", "title": "Quantum teleportation", "text": "Quantum teleportation\n\nQuantum teleportation is a process by which quantum information (e.g. the exact state of an atom or photon) can be transmitted (exactly, in principle) from one location to another, with the help of classical communication and previously shared quantum entanglement between the sending and receiving location. Because it depends on classical communication, which can proceed no faster than the speed of light, it cannot be used for faster-than-light transport or communication of classical bits. While it has proven possible to teleport one or more qubits of information between two (entangled) atoms, this has not yet been achieved between anything larger than molecules.\n\nAlthough the name is inspired by the teleportation commonly used in fiction, quantum teleportation is limited to the transfer of information rather than matter itself. Quantum teleportation is not a form of transportation, but of communication: it provides a way of transporting a qubit from one location to another without having to move a physical particle along with it.\n\nThe term was coined by physicist Charles Bennett. The seminal paper first expounding the idea of quantum teleportation was published by C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres, and W. K. Wootters in 1993. Quantum teleportation was first realized in single photons, later being demonstrated in various material systems such as atoms, ions, electrons and superconducting circuits. The latest reported record distance for quantum teleportation is by the group of Jian-Wei Pan using the Micius satellite for space-based quantum teleportation.\n\nIn matters relating to quantum or classical information theory, it is convenient to work with the simplest possible unit of information, the two-state system. In classical information, this is a bit, commonly represented using one or zero (or true or false). The quantum analog of a bit is a quantum bit, or qubit. Qubits encode a type of information, called quantum information, which differs sharply from \"classical\" information. For example, quantum information can be neither copied (the no-cloning theorem) nor destroyed (the no-deleting theorem).\n\nQuantum teleportation provides a mechanism of moving a qubit from one location to another, without having to physically transport the underlying particle to which that qubit is normally attached. Much like the invention of the telegraph allowed classical bits to be transported at high speed across continents, quantum teleportation holds the promise that one day, qubits could be moved likewise. As of 2015, the quantum states of single photons, photon modes, single atoms, atomic ensembles, defect centers in solids, single electrons, and superconducting circuits have been employed as information bearers.\n\nThe movement of qubits does not require the movement of \"things\" any more than communication over the internet does: no quantum object needs to be transported, but it is necessary to communicate two classical bits per teleported qubit from the sender to the receiver. The actual teleportation protocol requires that an entangled quantum state or Bell state be created, and its two parts shared between two locations (the source and destination, or Alice and Bob). In essence, a certain kind of quantum channel between two sites must be established first, before a qubit can be moved. Teleportation also requires a classical information channel to be established, as two classical bits must be transmitted to accompany each qubit. The reason for this is that the results of the measurements must be communicated, and this must be done over ordinary classical communication channels. The need for such classical channels may, at first, seem disappointing; however, this is not unlike ordinary communications, which requires wires, radios or lasers. What's more, Bell states are most easily shared using photons from lasers, and so teleportation could be done, in principle, through open space, i.e., without the need to send the light through cables or optical fibers.\n\nThe quantum states of single atoms have been teleported. An atom consists of several parts: the qubits in the electronic state or electron shells surrounding the atomic nucleus, the qubits in the nucleus itself, and, finally, the electrons, protons and neutrons making up the atom. Physicists have teleported the qubits encoded in the electronic state of atoms; they have not teleported the nuclear state, nor the nucleus itself. It is therefore inaccurate to say \"an atom has been teleported\". The quantum state of an atom has. Thus, performing this kind of teleportation requires a stock of atoms at the receiving site, available for having qubits imprinted on them. The importance of teleporting nuclear state is unclear: nuclear state does affect the atom, e.g. in hyperfine splitting, but whether such state would need to be teleported in some futuristic \"practical\" application is debatable.\n\nAn important aspect of quantum information theory is entanglement, which imposes statistical correlations between otherwise distinct physical systems. These correlations hold even when measurements are chosen and performed independently, out of causal contact from one another, as verified in Bell test experiments. Thus, an observation resulting from a measurement choice made at one point in spacetime seems to instantaneously affect outcomes in another region, even though light hasn't yet had time to travel the distance; a conclusion seemingly at odds with special relativity (EPR paradox). However such correlations can never be used to transmit any information faster than the speed of light, a statement encapsulated in the no-communication theorem. Thus, teleportation, as a whole, can never be superluminal, as a qubit cannot be reconstructed until the accompanying classical information arrives.\n\nUnderstanding quantum teleportation requires a good grounding in finite-dimensional linear algebra, Hilbert spaces and projection matrixes. A qubit is described using a two-dimensional complex number-valued vector space (a Hilbert space), which are the primary basis for the formal manipulations given below. A working knowledge of quantum mechanics is not absolutely required to understand the mathematics of quantum teleportation, although without such acquaintance, the deeper meaning of the equations may remain quite mysterious.\n\nThe prerequisites for quantum teleportation are a qubit that is to be teleported, a conventional communication channel capable of transmitting two classical bits (i.e., one of four states), and means of generating an entangled EPR pair of qubits, transporting each of these to two different locations, A and B, performing a Bell measurement on one of the EPR pair qubits, and manipulating the quantum state of the other pair. The protocol is then as follows:\n\n\nWork in 1998 verified the initial predictions, and the distance of teleportation was increased in August 2004 to 600 meters, using optical fiber. Subsequently, the record distance for quantum teleportation has been gradually increased to 16 km, then to 97 km, and is now , set in open air experiments done between two of the Canary Islands. There has been a recent record set (as of September 2015) using superconducting nanowire detectors that reached the distance of over optical fiber. For material systems, the record distance is 21m.\n\nA variant of teleportation called \"open-destination\" teleportation, with receivers located at multiple locations, was demonstrated in 2004 using five-photon entanglement. Teleportation of a composite state of two single photons has also been realized. In April 2011, experimenters reported that they had demonstrated teleportation of wave packets of light up to a bandwidth of 10 MHz while preserving strongly nonclassical superposition states. In August 2013, the achievement of \"fully deterministic\" quantum teleportation, using a hybrid technique, was reported. On 29 May 2014, scientists announced a reliable way of transferring data by quantum teleportation. Quantum teleportation of data had been done before but with highly unreliable methods. On 26 February 2015, scientists at the University of Science and Technology of China in Hefei, led by Chao-yang Lu and Jian-Wei Pan carried out the first experiment teleporting multiple degrees of freedom of a quantum particle. They managed to teleport the quantum information from ensemble of rubidium atoms to another ensemble of rubidium atoms over a distance of 150 metres using entangled photons. In September 2016, researchers at the University of Calgary demonstrated quantum teleportation over the Calgary metropolitan fiber network over a distance of 6.2 km.\n\nResearchers have also successfully used quantum teleportation to transmit information between clouds of gas atoms, notable because the clouds of gas are macroscopic atomic ensembles.\n\nIn 2018, physicists at Yale demonstrated a deterministic teleported CNOT operation between logically encoded qubits.\n\nThere are a variety of ways in which the teleportation protocol can be written mathematically. Some are very compact but abstract, and some are verbose but straightforward and concrete. The presentation below is of the latter form: verbose, but has the benefit of showing each quantum state simply and directly. Later sections review more compact notations.\n\nThe teleportation protocol begins with a quantum state or qubit formula_4, in Alice's possession, that she wants to convey to Bob. This qubit can be written generally, in bra–ket notation, as:\n\nThe subscript \"C\" above is used only to distinguish this state from \"A\" and \"B\", below.\n\nNext, the protocol requires that Alice and Bob share a maximally entangled state. This state is fixed in advance, by mutual agreement between Alice and Bob, and can be any one of the four Bell states shown. It does not matter which one.\n\nIn the following, assume that Alice and Bob share the state formula_10\nAlice obtains one of the particles in the pair, with the other going to Bob. (This is implemented by preparing the particles together and shooting them to Alice and Bob from a common source.) The subscripts \"A\" and \"B\" in the entangled state refer to Alice's or Bob's particle.\n\nAt this point, Alice has two particles (\"C\", the one she wants to teleport, and \"A\", one of the entangled pair), and Bob has one particle, \"B\". In the total system, the state of these three particles is given by\n\nAlice will then make a local measurement in the Bell basis (i.e. the four Bell states) on the two particles in her possession. To make the result of her measurement clear, it is best to write the state of Alice's two qubits as superpositions of the Bell basis. This is done by using the following general identities, which are easily verified:\n\nand\n\nOne applies these identities with \"A\" and \"C\" subscripts. The total three particle state, of \"A\", \"B\" and \"C\" together, thus becomes the following four-term superposition:\n\nThe above is just a change of basis on Alice's part of the system. No operation has been performed and the three particles are still in the same total state. The actual teleportation occurs when Alice measures her two qubits A,C, in the Bell basis\n\nExperimentally, this measurement may be achieved via a series of laser pulses directed at the two particles. Given the above expression, evidently the result of Alice's (local) measurement is that the three-particle state would collapse to one of the following four states (with equal probability of obtaining each):\n\n\nAlice's two particles are now entangled to each other, in one of the four Bell states, and the entanglement originally shared between Alice's and Bob's particles is now broken. Bob's particle takes on one of the four superposition states shown above. Note how Bob's qubit is now in a state that resembles the state to be teleported. The four possible states for Bob's qubit are unitary images of the state to be teleported.\n\nThe result of Alice's Bell measurement tells her which of the above four states the system is in. She can now send her result to Bob through a classical channel. Two classical bits can communicate which of the four results she obtained.\n\nAfter Bob receives the message from Alice, he will know which of the four states his particle is in. Using this information, he performs a unitary operation on his particle to transform it to the desired state formula_22:\n\n\nto recover the state.\n\n\nto his qubit.\n\n\nTeleportation is thus achieved. The above-mentioned three gates correspond to rotations of π radians (180°) about appropriate axes (X, Y and Z).\nSome remarks:\n\nQuantum circuit for a single qubit quantum teleportation is demonstrated using Q-Kit, a quantum circuit simulator with graphical interface. Alice's state in qubit 2 is transferred to Bob's qubit 0 using a priorly entangled pair of qubits between Alice and Bob, qubits 1 and 0.\n\nThere are a variety of different notations in use that describe the teleportation protocol. One common one is by using the notation of quantum gates. In the above derivation, the unitary transformation that is the change of basis (from the standard product basis into the Bell basis) can be written using quantum gates. Direct calculation shows that this gate is given by\n\nwhere \"H\" is the one qubit Walsh-Hadamard gate and formula_31 is the Controlled NOT gate.\n\nTeleportation can be applied not just to pure states, but also mixed states, that can be regarded as the state of a single subsystem of an entangled pair. The so-called entanglement swapping is a simple and illustrative example.\n\nIf Alice has a particle which is entangled with a particle owned by Bob, and Bob teleports it to Carol, then afterwards, Alice's particle is entangled with Carol's.\n\nA more symmetric way to describe the situation is the following: Alice has one particle, Bob two, and Carol one. Alice's particle and Bob's first particle are entangled, and so are Bob's second and Carol's particle:\n\nNow, if Bob does a projective measurement on his two particles in the Bell state basis and communicates the results to Carol, as per the teleportation scheme described above, the state of Bob's first particle can be teleported to Carol's. Although Alice and Carol never interacted with each other, their particles are now entangled.\n\nA detailed diagrammatic derivation of entanglement swapping has been given by Bob Coecke, presented in terms of categorical quantum mechanics.\n\nThe basic teleportation protocol for a qubit described above has been generalized in several directions, in particular regarding the dimension of the system teleported and the number of parties involved (either as sender, controller, or receiver).\n\nA generalization to formula_32-level systems (so-called qudits) is straight forward and was already discussed in the original paper by Bennett \"et al.\": the maximally entangled state of two qubits has to be replaced by a maximally entangled state of two qudits and the Bell measurement by a measurement defined by a maximally entangled orthonormal basis. All possible such generalizations were discussed by Werner in 2001. \nThe generalization to infinite-dimensional so-called continuous-variable systems was proposed in and lead to the first teleportation experiment that worked unconditionally.\n\nThe use of multipartite entangled states instead of a bipartite maximally entangled state allows for several new features: either the sender can teleport information to several receivers either sending the same state to all of them (which allows to reduce the amount of entanglement needed for the process) or teleporting multipartite states or sending a single state in such a way that the receiving parties need to cooperate to extract the information. A different way of viewing the latter setting is that some of the parties can control whether the others can teleport.\n\nIn general, mixed states ρ may be transported, and a linear transformation ω applied during teleportation, thus allowing data processing of quantum information. This is one of the foundational building blocks of quantum information processing. This is demonstrated below.\n\nA general teleportation scheme can be described as follows. Three quantum systems are involved. System 1 is the (unknown) state \"ρ\" to be teleported by Alice. Systems 2 and 3 are in a maximally entangled state \"ω\" that are distributed to Alice and Bob, respectively. The total system is then in the state\n\nA successful teleportation process is a LOCC quantum channel Φ that satisfies\n\nwhere Tr is the partial trace operation with respect systems 1 and 2, and formula_35 denotes the composition of maps. This describes the channel in the Schrödinger picture.\n\nTaking adjoint maps in the Heisenberg picture, the success condition becomes\n\nfor all observable \"O\" on Bob's system. The tensor factor in formula_37 is formula_38 while that of formula_39 is formula_40.\n\nThe proposed channel Φ can be described more explicitly. To begin teleportation, Alice performs a local measurement on the two subsystems (1 and 2) in her possession. Assume the local measurement have \"effects\"\n\nIf the measurement registers the \"i\"-th outcome, the overall state collapses to\n\nThe tensor factor in formula_43 is formula_38 while that of formula_39 is formula_40. Bob then applies a corresponding local operation Ψ\"\" on system 3. On the combined system, this is described by\n\nwhere \"Id\" is the identity map on the composite system formula_48.\n\nTherefore, the channel Φ is defined by\n\nNotice Φ satisfies the definition of LOCC. As stated above, the teleportation is said to be successful if, for all observable \"O\" on Bob's system, the equality\n\nholds. The left hand side of the equation is:\n\nwhere Ψ\"*\" is the adjoint of Ψ\"\" in the Heisenberg picture. Assuming all objects are finite dimensional, this becomes\n\nThe success criterion for teleportation has the expression\n\nA local explanation of quantum teleportation is put forward by David Deutsch and Patrick Hayden, with respect to the many-worlds interpretation of quantum mechanics. Their paper asserts that the two bits that Alice sends Bob contain \"locally inaccessible information\" resulting in the teleportation of the quantum state. \"The ability of quantum information to flow through a classical channel ..., surviving decoherence, is ... the\nbasis of quantum teleportation.\"\n\n\n\n"}
{"id": "50568483", "url": "https://en.wikipedia.org/wiki?curid=50568483", "title": "Read-once function", "text": "Read-once function\n\nIn mathematics, a read-once function is a special type of Boolean function that can be described by a Boolean expression in which each variable appears only once.\n\nMore precisely, the expression is required to use only the operations of logical conjunction, logical disjunction, and negation. By applying De Morgan's laws, such an expression can be transformed into one in which negation is used only on individual variables (still with each variable appearing only once). By replacing each negated variable with a new positive variable representing its negation, such a function can be transformed into an equivalent positive read-once Boolean function, represented by a read-once expression without negations.\n\nFor example, for three variables , , and , the expressions\nare all read-once (as are the other functions obtained by permuting the variables in these expressions). However, the Boolean median operation, given by the expression\nis not read-once: this formula has more than one copy of each variable, and there is no equivalent formula that uses each variable only once.\n\nThe disjunctive normal form of a (positive) read-once function is not generally itself read-once. Nevertheless, it carries important information about the function. In particular, if one forms a \"co-occurrence graph\" in which the vertices represent variables, and edges connect pairs of variables that both occur in the same clause of the conjunctive normal form, then the co-occurrence graph of a read-once function is necessarily a cograph. More precisely, a positive Boolean function is read-once if and only if its co-occurrence graph is a cograph, and in addition every maximal clique of the co-occurrence graph forms one of the conjunctions (prime implicants) of the disjunctive normal form. That is, when interpreted as a function on sets of vertices of its co-occurrence graph, a read-once function is true for sets of vertices that contain a maximal clique, and false otherwise. \nFor instance the median function has the same co-occurrence graph as the conjunction of three variables, a triangle graph, but the three-vertex complete subgraph of this graph (the whole graph) forms a subset of a clause only for the conjunction and not for the median.\nTwo variables of a positive read-once expression are adjacent in the co-occurrence graph if and only if their lowest common ancestor in the expression is a conjunction, so the expression tree can be interpreted as a cotree for the corresponding cograph.\n\nAnother alternative characterization of positive read-once functions combines their disjunctive and conjunctive normal form. A positive function of a given system of variables, that uses all of its variables, is read-once if and only if every prime implicant of the disjunctive normal form and every clause of the conjunctive normal form have exactly one variable in common.\n\nIt is possible to recognize read-once functions from their disjunctive normal form expressions in polynomial time.\nIt is also possible to find a read-once expression for a positive read-once function, given access to the function only through a \"black box\" that allows its evaluation at any truth assignment, using only a quadratic number of function evaluations.\n\n"}
{"id": "56348140", "url": "https://en.wikipedia.org/wiki?curid=56348140", "title": "Richard von Mises Prize", "text": "Richard von Mises Prize\n\nThe Richard von Mises Prize is awarded annually by the International Association of Applied Mathematics and Mechanics (GAMM). Since its inception in 1989, the award is given to a young scientist (not older than 36) for outstanding scientific achievements in the field of Applied Mathematics and Mechanics. The prize is presented during the opening ceremony of the GAMM Annual Meeting where the winner will present his research in a plenary talk. The prize aims to reward and encourage young scientists whose research represents a major advancement in the field of Applied Mathematics and Mechanics.\n\nRichard von Mises was an Austrian-American mathematician who worked among others on numerical mathematics, solid mechanics, fluid mechanics, statistics and probability theory.\n"}
{"id": "7290730", "url": "https://en.wikipedia.org/wiki?curid=7290730", "title": "Rotation formalisms in three dimensions", "text": "Rotation formalisms in three dimensions\n\nIn geometry, various formalisms exist to express a rotation in three dimensions as a mathematical transformation. In physics, this concept is applied to classical mechanics where rotational (or angular) kinematics is the science of quantitative description of a purely rotational motion. The orientation of an object at a given instant is described with the same tools, as it is defined as an imaginary rotation from a reference placement in space, rather than an actually observed rotation from a previous placement in space.\n\nAccording to Euler's rotation theorem the rotation of a rigid body (or three-dimensional coordinate system with the fixed origin) is described by a single rotation about some axis. Such a rotation may be uniquely described by a minimum of three real parameters. However, for various reasons, there are several ways to represent it. Many of these representations use more than the necessary minimum of three parameters, although each of them still has only three degrees of freedom.\n\nAn example where rotation representation is used is in computer vision, where an automated observer needs to track a target. Let's consider a rigid body, with three orthogonal unit vectors fixed to its body (representing the three axes of the object's local coordinate system). The basic problem is to specify the orientation of these three unit vectors, and hence the rigid body, with respect to the observer's coordinate system, regarded as a reference placement in space.\n\nRotation formalisms are focused on proper (orientation-preserving) motions of the Euclidean space with one fixed point, that a \"rotation\" refers to. Although physical motions with a fixed point are an important case (such as ones described in the center-of-mass frame, or motions of a joint), this approach creates a knowledge about all motions. Any proper motion of the Euclidean space decomposes to a rotation around the origin and a translation. Whichever the order of their composition will be, the \"pure\" rotation component wouldn't change, uniquely determined by the complete motion.\n\nOne can also understand \"pure\" rotations as linear maps in a vector space equipped with Euclidean structure, not as maps of points of a corresponding affine space. In other words, a rotation formalism captures only the rotational part of a motion, that contains three degrees of freedom, and ignores the translational part, that contains another three.\n\nThe above-mentioned triad of unit vectors is also called a basis. Specifying the coordinates (\"components\") of vectors of this basis in its current (rotated) position, in terms of the reference (non-rotated) coordinate axes, will completely describe the rotation. The three unit vectors, , and , that form the rotated basis each consist of 3 coordinates, yielding a total of 9 parameters.\n\nThese parameters can be written as the elements of a matrix , called a rotation matrix. Typically, the coordinates of each of these vectors are arranged along a column of the matrix (however, beware that an alternative definition of rotation matrix exists and is widely used, where the vectors coordinates defined above are arranged by rows)\n\nThe elements of the rotation matrix are not all independent—as Euler's rotation theorem dictates, the rotation matrix has only three degrees of freedom.\n\nThe rotation matrix has the following properties:\n\nThe angle which appears in the eigenvalue expression corresponds to the angle of the Euler axis and angle representation. The eigenvector corresponding to the eigenvalue of 1 is the accompanying Euler axis, since the axis is the only (nonzero) vector which remains unchanged by left-multiplying (rotating) it with the rotation matrix.\n\nThe above properties are equivalent to:\nwhich is another way of stating that form a 3D orthonormal basis. These statements comprise a total of 6 conditions (the cross product contains 3), leaving the rotation matrix with just 3 degrees of freedom, as required.\n\nTwo successive rotations represented by matrices and are easily combined as elements of a group,\n(Note the order, since the vector being rotated is multiplied from the right).\n\nThe ease by which vectors can be rotated using a rotation matrix, as well as the ease of combining successive rotations, make the rotation matrix a useful and popular way to represent rotations, even though it is less concise than other representations.\n\nFrom Euler's rotation theorem we know that any rotation can be expressed as a single rotation about some axis. The axis is the unit vector (unique except for sign) which remains unchanged by the rotation. The magnitude of the angle is also unique, with its sign being determined by the sign of the rotation axis.\n\nThe axis can be represented as a three-dimensional unit vector\nand the angle by a scalar .\n\nSince the axis is normalized, it has only two degrees of freedom. The angle adds the third degree of freedom to this rotation representation.\n\nOne may wish to express rotation as a rotation vector, or Euler vector, an un-normalized three-dimensional vector the direction of which specifies the axis, and the length of which is ,\n\nThe rotation vector is in some contexts useful, as it represents a three-dimensional rotation with only three scalar values (its components), representing the three degrees of freedom. This is also true for representations based on sequences of three Euler angles (see below).\n\nIf the rotation angle is zero, the axis is not uniquely defined. Combining two successive rotations, each represented by an Euler axis and angle, is not straightforward, and in fact does not satisfy the law of vector addition, which shows that finite rotations are not really vectors at all. It is best to employ the rotation matrix or quaternion notation, calculate the product, and then convert back to Euler axis and angle.\n\nThe idea behind Euler rotations is to split the complete rotation of the coordinate system into three simpler constitutive rotations, called precession, nutation, and intrinsic rotation, being each one of them an increment on one of the Euler angles. Notice that the outer matrix will represent a rotation around one of the axes of the reference frame, and the inner matrix represents a rotation around one of the moving frame axes. The middle matrix represents a rotation around an intermediate axis called line of nodes.\n\nHowever, the definition of Euler angles is not unique and in the literature many different conventions are used. These conventions depend on the axes about which the rotations are carried out, and their sequence (since rotations are not commutative).\n\nThe convention being used is usually indicated by specifying the axes about which the consecutive rotations (before being composed) take place, referring to them by index or letter . The engineering and robotics communities typically use 3-1-3 Euler angles. Notice that after composing the independent rotations, they do not rotate about their axis anymore. The most external matrix rotates the other two, leaving the second rotation matrix over the line of nodes, and the third one in a frame comoving with the body. There are possible combinations of three basic rotations but only of them can be used for representing arbitrary 3D rotations as Euler angles. These 12 combinations avoid consecutive rotations around the same axis (such as XXY) which would reduce the degrees of freedom that can be represented.\n\nTherefore, Euler angles are never expressed in terms of the external frame, or in terms of the co-moving rotated body frame, but in a mixture. Other conventions (e.g., rotation matrix or quaternions) are used to avoid this problem.\n\nIn aviation orientation of the aircraft is usually expressed as intrinsic Tait-Bryan angles following the convention, which are called heading, elevation, and bank (or synonymously, yaw, pitch, and roll).\n\nQuaternions, that form a four-dimensional vector space, have proven very useful in representing rotations due to several advantages above the other representations mentioned in this article.\n\nA quaternion representation of rotation is written as a versor (normalized quaternion)\n\nThe above definition stores quaternion as an array following the convention used in (Wertz 1980) and (Markley 2003). An alternative definition used for example in (Coutsias 1999) and (Schmidt 2001) defines the \"scalar\" term as the first quaternion element, with the other elements shifted down one position.\n\nIn terms of the Euler axis \n\nand angle this versor's components are expressed as follows: \n\nInspection shows that the quaternion parametrization obeys the following constraint:\n\nThe last term (in our definition) is often called the scalar term, which has its origin in quaternions when understood as the mathematical extension of the complex numbers, written as \n\nand where are the hypercomplex numbers satisfying\n\nQuaternion multiplication, that is used to specify a composite rotation, is performed in the same manner as multiplication of complex numbers, except that the order of elements must be taken into account, since multiplication is not commutative. In matrix notation we can write quaternion multiplication as\n\nCombining two consecutive quaternion rotations is therefore just as simple as using the rotation matrix. Remember that two successive rotation matrices, followed by , are combined as follows:\n\nWe can represent this with quaternion parameters in a similarly concise way:\n\nQuaternions are a very popular parametrization due to the following properties:\n\nLike rotation matrices, quaternions must sometimes be renormalized due to rounding errors, to make sure that they correspond to valid rotations. The computational cost of renormalizing a quaternion, however, is much less than for normalizing a matrix.\n\nQuaternions also capture the spinorial character of rotations in three dimensions. For a three-dimensional object connected to its (fixed) surroundings by slack strings or bands, the strings or bands can be untangled after \"two\" complete turns about some fixed axis from an initial untangled state. Algebraically, the quaternion describing such a rotation changes from a scalar +1 (initially), through (scalar + pseudovector) values to scalar −1 (at one full turn), through (scalar + pseudovector) values back to scalar +1 (at two full turns). This cycle repeats every 2 turns. After turns (integer ), without any intermediate untangling attempts, the strings/bands can be partially untangled back to the turns state with each application of the same procedure used in untangling from 2 turns to 0 turns. Applying the same procedure times will take a -tangled object back to the untangled or 0 turn state. The untangling process also removes any rotation-generated twisting about the strings/bands themselves. Simple 3D mechanical models can be used to demonstrate these facts.\n\nRodrigues parameters can be expressed in terms of Euler axis and angle as follows, \n\nThis has a discontinuity at 180° ( radians): each vector, , with a norm of radians represent the same rotation as .\n\nSimilarly, the Gibbs representation can be expressed as\nA rotation followed by a rotation in the Gibbs representation has the simple form\n\nToday, the most straightforward way to prove this formula is in the (faithful) doublet representation, where , etc. \n\nAnother way to prove this equation is to use quaternions. Construct a quaternion associated with a spatial rotation R as,\nThen the composition of the rotation R with R is the rotation R=RR with rotation axis and angle defined by the product of the quaternions\nthat is\n\nExpand this quaternion product to obtain\n\nDivide both sides of this equation by the identity,\nand compute\n\nThis is Rodrigues formula for the axis of a composite rotation defined in terms of the axes of the two rotations. He derived this formula in 1840 (see page 408).\n\nThe three rotation axes A, B, and C form a spherical triangle and the dihedral angles between the planes formed by the sides of this triangle are defined by the rotation angles.\nThe Gibbs vector has the advantage (or disadvantage, depending on context) that 180° rotations cannot be represented in it. (Even using floating point numbers that include infinity, rotation direction cannot be well-defined; for example, naively, a 180° rotation about the axis (1, 1, 0) would be (∞, ∞, 0), which is the same representation as 180° rotation about (1, 0.0001, 0).)\n\nModified Rodrigues parameters (MRPs) can be expressed in terms of Euler axis and angle by\n\nThe modified Rodrigues parameterization shares many characteristics with the rotation vector parametrization, including the occurrence of discontinuous jumps in the parameter space when incrementing the rotation.\n\nSee definition at Wolfram Mathworld.\n\nThe Euler angles can be extracted from the rotation matrix formula_26 by inspecting the rotation matrix in analytical form.\n\nUsing the -convention, the 3-1-3 extrinsic Euler angles , and (around the -axis, -axis and again the formula_27-axis) can be obtained as follows:\n\nNote that is equivalent to where it also takes into account the quadrant that the point is in; see atan2.\n\nWhen implementing the conversion, one has to take into account several situations:\n\nThe rotation matrix is generated from the 3-2-1 intrinsic Euler angles by multiplying the three matrices generated by rotations about the axes. \n\nThe axes of the rotation depend on the specific convention being used. For the -convention the rotations are about the -, - and -axes with angles , and , the individual matrices are as follows:\n\nThis yields\nNote: This is valid for a right-hand system, which is the convention used in almost all engineering and physics disciplines.\n\nThe interpretation of these right-handed rotation matrices is that they express coordinate transformations (passive) as opposed to point transformations (active). Because expresses a rotation from the local frame to the global frame (i.e., encodes the axes of frame w.r.t frame ), the elementary rotation matrices are composed as above. Because the inverse rotation is just the rotation transposed, if we wanted the global-to-local rotation from frame to frame , we would write formula_32.\n\nIf the Euler angle is not a multiple of , the Euler axis and angle can be computed from the elements of the rotation matrix as follows:\n\nAlternatively, the following method can be used:\n\nEigendecomposition of the rotation matrix yields the eigenvalues 1 and . The Euler axis is the eigenvector corresponding to the eigenvalue of 1, and can be computed from the remaining eigenvalues.\n\nThe Euler axis can be also found using singular value decomposition since it is the normalized vector spanning the null-space of the matrix .\n\nTo convert the other way the rotation matrix corresponding to an Euler axis and angle can be computed according to Rodrigues' rotation formula (with appropriate modification) as follows:\n\nwith the identity matrix, and\n\nis the cross-product matrix.\n\nThis expands to:\n\nWhen computing a quaternion from the rotation matrix there is a sign ambiguity, since and represent the same rotation.\n\nOne way of computing the quaternion\nfrom the rotation matrix is as follows:\n\nThere are three other mathematically equivalent ways to compute . Numerical inaccuracy can be reduced by avoiding situations in which the denominator is close to zero. One of the other three methods looks as follows:\n\nThe rotation matrix corresponding to the quaternion can be computed as follows:\n\nwhere\n\nwhich gives\n\nor equivalently\n\nWe will consider the -convention 3-1-3 extrinsic Euler angles for the following algorithm. The terms of the algorithm depend on the convention used.\n\nWe can compute the quaternion\nfrom the Euler angles as follows:\n\nA quaternion equivalent to yaw (), pitch () and roll () angles. or intrinsic Tait–Bryan angles following the convention, can be computed by\n\nGiven the rotation quaternion\nthe -convention 3-1-3 extrinsic Euler Angles can be computed by\n\nGiven the rotation quaternion\nyaw, pitch and roll angles, or intrinsic Tait–Bryan angles following the convention, can be computed by\n\nGiven the Euler axis and angle , the quaternion\n\ncan be computed by\n\nGiven the rotation quaternion , define\nThen the Euler axis and angle can be computed by\n\nThe angular velocity vector\ncan be extracted from the time derivative of the rotation matrix by the following relation:\n\nThe derivation is adapted from Ioffe as follows:\n\nFor any vector , consider and differentiate it:\n\nThe derivative of a vector is the linear velocity of its tip. Since is a rotation matrix, by definition the length of is always equal to the length of , and hence it does not change with time. Thus, when rotates, its tip moves along a circle, and the linear velocity of its tip is tangential to the circle; i.e., always perpendicular to . In this specific case, the relationship between the linear velocity vector and the angular velocity vector is \n(see circular motion and cross product).\n\nBy the transitivity of the abovementioned equations, \n\nwhich implies\n\nThe angular velocity vector\ncan be obtained from the derivative of the quaternion as follows:\nwhere is the inverse of .\n\nConversely, the derivative of the quaternion is\n\nThe formalism of geometric algebra (GA) provides an extension and interpretation of the quaternion method. Central to GA is the geometric product of vectors, an extension of the traditional inner and cross products, given by\n\nwhere the symbol denotes the exterior product or wedge product. This product of vectors , and produces two terms: a scalar part from the inner product and a bivector part from the wedge product. This bivector describes the plane perpendicular to what the cross product of the vectors would return.\n\nBivectors in GA have some unusual properties compared to vectors. Under the geometric product, bivectors have a negative square: the bivector describes the -plane. Its square is . Because the unit basis vectors are orthogonal to each other, the geometric product reduces to the antisymmetric outer product – and can be swapped freely at the cost of a factor of −1. The square reduces to since the basis vectors themselves square to +1.\n\nThis result holds generally for all bivectors, and as a result the bivector plays a role similar to the imaginary unit. Geometric algebra uses bivectors in its analogue to the quaternion, the \"rotor\", given by\nwhere is a unit bivector that describes the plane of rotation. Because squares to −1, the power series expansion of generates the trigonometric functions. The rotation formula that maps a vector to a rotated vector is then\n\nwhere\nis the \"reverse\" of formula_68 (reversing the order of the vectors in formula_69 is equivalent to changing its sign).\n\nExample. A rotation about the axis\ncan be accomplished by converting to its dual bivector,\nwhere is the unit volume element, the only trivector (pseudoscalar) in three-dimensional space. The result is\n\nIn three-dimensional space, however, it is often simpler to leave the expression for , using the fact that commutes with all objects in 3D and also squares to −1. A rotation of the vector in this plane by an angle is then\n\nRecognizing that\nand that is the reflection of about the plane perpendicular to gives a geometric interpretation to the rotation operation: the rotation preserves the components that are parallel to and changes only those that are perpendicular. The terms are then computed:\n\nThe result of the rotation is then\n\nA simple check on this result is the angle . Such a rotation should map to . Indeed, the rotation reduces to\n\nexactly as expected. This rotation formula is valid not only for vectors but for any multivector. In addition, when Euler angles are used, the complexity of the operation is much reduced. Compounded rotations come from multiplying the rotors, so the total rotor from Euler angles is\n\nbut\nThese rotors come back out of the exponentials like so:\n\nwhere refers to rotation in the original coordinates. Similarly for the rotation,\nNoting that and commute (rotations in the same plane must commute), and the total rotor becomes\n\nThus, the compounded rotations of Euler angles become a series of equivalent rotations in the original fixed frame.\n\nWhile rotors in geometric algebra work almost identically to quaternions in three dimensions, the power of this formalism is its generality: this method is appropriate and valid in spaces with any number of dimensions. In 3D, rotations have three degrees of freedom, a degree for each linearly independent plane (bivector) the rotation can take place in. It has been known that pairs of quaternions can be used to generate rotations in 4D, yielding six degrees of freedom, and the geometric algebra approach verifies this result: in 4D, there are six linearly independent bivectors that can be used as the generators of rotations.\n\n\n\n"}
{"id": "7031031", "url": "https://en.wikipedia.org/wiki?curid=7031031", "title": "Samuel Laing (science writer)", "text": "Samuel Laing (science writer)\n\nSamuel Laing, (12 December 1812 – 6 August 1897), was a British railway administrator, politician, and writer on science and religion during the Victorian era.\n\nSamuel Laing was born on 12 December 1810 in Edinburgh, Scotland. He was the nephew of Malcolm Laing, the historian of Scotland; and his father, also called Samuel Laing (1780–1868), was a well-known author, whose books on Norway and Sweden attracted much attention. Samuel Laing the younger entered St John's College, Cambridge in 1827, and after graduating as Second Wrangler and Smith's Prizeman, was elected a fellow. He remained at Cambridge temporarily as a coach, before being called to the bar in 1837.\n\nHe became private secretary to Henry Labouchere, later 1st Baron Taunton, who was then the President of the Board of Trade. In 1842, he was made secretary to the railway department, and retained this post until 1847. He had by then become an authority on railways, and had been a member of the Dalhousie Railway Commission; it was at his suggestion that the \"parliamentary\" rate of a penny a mile was instituted. In 1848, he was appointed chairman and managing director of the London, Brighton and South Coast Railway (LB&SCR), and his business acumen showed itself in the largely increased prosperity of the line. He also became chairman (1852) of The Crystal Palace Company, but retired from both posts in 1855.\n\nIn 1852, he was elected to Parliament as a Liberal Party candidate in Wick Burghs. After losing his seat in 1857, he was re-elected in 1859, and appointed Financial Secretary to the Treasury; in 1860 he was made finance minister in India. On returning from India, he was re-elected to parliament for Wick in 1865. He was defeated in 1868, but in 1873 he was returned for Orkney and Shetland, and retained his seat till 1885. Early in 1867 he was elected to the board of the Great Eastern Railway who by that point were sliding towards receivership. On July 1, the day before the GER went into receivership, he was reappointed chairman of the Brighton line, which was now on the point of bankruptcy following the over-ambitious expansion plans of the previous chairman. He continued in that post until 1896, and gradually restored the company to financial health. He was also chairman of the Railway Debenture Trust and the Railway Share Trust\n\nAlthough often claimed to have been the father of the novelist Mary Eliza Kennard (1850–1936), this is in dispute. However birth entries at the General Register Office for her sons Lionel Edward Kennard and Malcolm Alfred Kennard both have Laing as the mother's maiden name. Furthermore the transcribed parish record entry for her marriage to Edward Kennard on 19 April 1870 at Saint Nicholas church, Brighton gives her name as Mary Eliza Laing, daughter of Samuel Laing.\n\nIn later life, he became well known as an author, his \"Modern Science and Modern Thought\" (1885), \"Problems of the Future\" (1889) and \"Human Origins\" (1892). Laing's attitude was generally positive towards new developments in science, and he offered an optimistic vision of progressive modernity. He also wrote on religion. His book \"A Modern Zoroastrian\" argued that the ancient religion of Zoroastrianism was more consistent with modern scientific thought than was traditional Christianity. He argued that the \"all pervading principle of polarity\" that was central Zoroastrian thought has been confirmed by science, and that modern Christianity should abandon its traditional theology to centre on the figure of Jesus as an ideal of humanity.\n\nHe died on 6 August 1897 in Sydenham, England and was buried in the Brighton Extra Mural Cemetery. \n\n"}
{"id": "57634027", "url": "https://en.wikipedia.org/wiki?curid=57634027", "title": "Sequential decision making", "text": "Sequential decision making\n\nIn artificial intelligence, Sequential decision making refers to algorithms that takes the dynamics of the world into consideration, thus delay parts of the problem until it must be solved. It can be described as a procedural approach to decision-making, or as a step by step decision theory. Sequential decision making has as a consequence the intertemporal choice problem, where earlier decisions influences the later available choices.\n"}
{"id": "12277357", "url": "https://en.wikipedia.org/wiki?curid=12277357", "title": "Swift–Hohenberg equation", "text": "Swift–Hohenberg equation\n\nThe Swift–Hohenberg equation (named after Jack B. Swift and Pierre Hohenberg) is a partial differential equation noted for its pattern-forming behaviour. It takes the form\n\nwhere \"u\" = \"u\"(\"x\", \"t\") or \"u\" = \"u\"(\"x\", \"y\", \"t\") is a scalar function defined on the line or the plane, \"r\" is a real bifurcation parameter, and \"N\"(\"u\") is some smooth nonlinearity.\n\nThe equation is named after the authors of the paper, where it was derived from the equations for thermal convection.\n\nThe webpage of Michael Cross contains some numerical integrators which demonstrate the behaviour of several Swift–Hohenberg-like systems.\n\nIn 2009 Ruggero Gabbrielli published a way to use the Swift-Hohenberg equation to find candidate solutions to the Kelvin Problem on minimal surfaces.\n"}
{"id": "780445", "url": "https://en.wikipedia.org/wiki?curid=780445", "title": "Thomas Harriot", "text": "Thomas Harriot\n\nThomas Harriot (Oxford, c. 1560 – London, 2 July 1621), also spelled Harriott, Hariot or Heriot, was an English astronomer, mathematician, ethnographer and translator who made advances within the scientific field. \n\nHe is sometimes credited with the introduction of the potato to the British Isles. Harriot was the first person to make a drawing of the Moon through a telescope, on 26 July 1609, over four months before Galileo.\n\nAfter graduating from St Mary Hall, Oxford, Harriot travelled to the Americas, accompanying the 1585 expedition to Roanoke island funded by Sir Walter Raleigh and led by Sir Ralph Lane. Harriot was a vital member of the venture, having learned and translating the Carolina Algonquian language from two Native Americans: Wanchese and Manteo. On his return to England, he worked for the 9th Earl of Northumberland. At the Earl's house, he became a prolific mathematician and astronomer to whom the theory of refraction is attributed.\n\nBorn in 1560 in Oxford, England, Thomas Harriot attended St Mary Hall, Oxford. His name appears in the hall's registry dating from 1577.\n\nAfter his graduation from Oxford in 1580, Harriot was first hired by Sir Walter Raleigh as a mathematics tutor; he used his knowledge of astronomy/astrology to provide navigational expertise, help design Raleigh's ships, and serve as his accountant. Prior to his expedition with Raleigh, Harriot wrote a treatise on navigation. He made efforts to communicate with Manteo and Wanchese, two Native Americans who had been brought to England. Harriot devised a phonetic alphabet to transcribe their Carolina Algonquian language.\n\nHarriot and Manteo spent many days in one another's company; Harriot interrogated Manteo closely about life in the New World and learned much that was to the advantage of the English settlers. In addition, he recorded the sense of awe with which the Native Americans viewed European technology:\n\nHe made only one expedition, around 1585-86, and spent some time in the New World visiting Roanoke Island off the coast of North Carolina, expanding his knowledge by improving his understanding of the Carolina Algonquian language. As the only Englishman who had learned Algonquin prior to the voyage, Harriot was vital to the success of the expedition. Hariot smoked tobacco before Raleigh, and may have taught him to do so.\n\nHis account of the voyage, named \"A Briefe and True Report of the New Found Land of Virginia\", was published in 1588 (probably written a year before). The \"True Report\" contains an early account of the Native American population encountered by the expedition; it proved very influential upon later English explorers and colonists. He wrote: \"Whereby it may be hoped, if means of good government be used, that they may in short time be brought to civility and the embracing of true religion.\" At the same time, his views of Native Americans' industry and capacity to learn were later largely ignored in favour of the parts of the \"True Report\" about extractable minerals and resources.\n\nAs a scientific adviser during the voyage, Harriot was asked by Raleigh to find the most efficient way to stack cannonballs on the deck of the ship. His ensuing theory about the close-packing of spheres shows a striking resemblance to atomism and modern atomic theory, which he was later accused of believing. His correspondence about optics with Johannes Kepler, in which he described some of his ideas, later influenced Kepler's conjecture.\n\nHarriot was employed for many years by Henry Percy, 9th Earl of Northumberland, with whom he resided at Syon House, which was run by Henry Percy's cousin Thomas Percy.\n\nHarriot's sponsors began to fall from favour: Raleigh was the first, and Harriot's other patron Henry Percy, the Earl of Northumberland, was imprisoned in 1605 in connection with the Gunpowder Plot as he was closely connected to one of the conspirators, Thomas Percy.\n\nHarriot himself was interrogated and briefly imprisoned but was soon released. Walter Warner, Robert Hues, William Lower, and other scientists were present around the Earl of Northumberland's mansion as they worked for him and assisted in the teaching of the family's children.\n\nHalley's Comet in 1607 turned Harriot's attention towards astronomy. In early 1609 he bought a \"Dutch trunke\" (telescope), invented in 1608, and his observations were among the first uses of a telescope for astronomy. Harriot is now credited as the first astronomer to draw an astronomical object after viewing it through a telescope: he drew a map of the Moon on 26 July 1609, preceding Galileo by several months. He also observed sunspots in December 1610.\n\nIn 1615 or 1616, Harriot wrote to an unknown friend with medical expertise, describing what would have been the reason for the eruption of a cancerous ulcer on his lip. This progressed until 1621, when he was living with a friend named Thomas Buckner on Threadneedle Street, where he died. Sources cited below are among several that describe his condition as a cancer of the nose. In either case, Harriot apparently died from skin cancer.\n\nHe died on 2 July 1621, three days after writing his will (discovered by Henry Stevens). His executors posthumously published his \"Artis Analyticae Praxis\" on algebra in 1631; Nathaniel Torporley was the intended executor of Harriot's wishes, but Walter Warner in the end pulled the book into shape. It may be a compendium of some of his works but does not represent all that he left unpublished (more than 400 sheets of annotated writing). It is not directed in a way that follows the manuscripts and it fails to give the full significance of Harriot's writings.\n\nThomas Harriot was buried in the church of St Christopher le Stocks in Threadneedle Street, near where he died. The church was subsequently damaged in the Great Fire of London, and demolished in 1781 to enable expansion of the Bank of England.\n\nHe also studied optics and refraction, and apparently discovered Snell's law 20 years before Snellius did, although it was previously discovered by Ibn Sahl; like so many of his works, this remained unpublished. In Virginia he learned the local Algonquian language, which may have had some effect on his mathematical thinking. He founded the \"English school\" of algebra. Around 1600, he introduced an algebraic symbolism close to modern notation, therefore, computation with unknowns became as easy as with numbers. He is also credited with discovering Girard's theorem, although the formula bears Girard's name as he was the first to publish it.\nHis algebra book \"Artis Analyticae Praxis\" (1631) was published posthumously in Latin. Unfortunately the editors did not understand much of his reasoning and removed the parts they did not comprehend such as the negative and complex roots of equations. Because of the dispersion of Harriot's writings the full annotated English translation of the \"Praxis\" was not completed until 2007.\n\nThe first biography of Harriot was written in 1876 by Henry Stevens of Vermont but not published until 1900 fourteen years after his death. The publication was limited to 167 copies and so the work was not widely known until 1972 when a reprint edition appeared. Prominent American poet, novelist and biographer Muriel Rukeyser wrote an extended literary inquiry into the life and significance of Hariot (her preferred spelling), \"The Traces of Thomas Hariot\" (1970, 1971). Interest in Harriot continued to revive with the convening of a symposium at the University of Delaware in April, 1971 with the proceedings published by the Oxford University Press in 1974. John W. Shirley the editor (1908-1988) went on to publish \"A Sourcebook for the Study of Thomas Harriot\" (1981) and his Harriot biography (1983). The papers of John Shirley have been deposited in the University of Delaware Library.\n\nHarriot's accomplishments remain relatively obscure because he did not publish any of his results and also because many of his manuscripts have been lost; those that survive are in the British Museum and in the archives of the Percy family at Petworth House (Sussex) and Alnwick Castle (Northumberland).\nAn event was held at Syon House, West London, to celebrate the 400th anniversary of Harriot's first observations of the moon on 26 July 2009. This event, Telescope400, included the unveiling of a plaque to commemorate Harriot by Lord Egremont. The plaque can now be seen by visitors to Syon House, the location of Harriot's historic observations. His drawing made 400 years earlier is believed to be based on the first ever observations of the moon through a telescope. The event (sponsored by the Royal Astronomical Society) was run as part of the International Year of Astronomy (IYA).\n\nThe original documents showing Harriot's moon map of c. 1611, observations of Jupiter's satellites, and first observations of sunspots were on display at the Science Museum, London, from 23 July 2009 until the end of the IYA.\n\nThe observatory in the campus of the College of William & Mary is named in Harriot's honour. A crater on the Moon was belatedly named after him in 1970; it is on the Moon's far side and hence unobservable from Earth.\n\nIn July 2014 the International Astronomical Union launched a process for giving proper names to certain exoplanets and their host stars. The process involved public nomination and voting for the new names. In December 2015, the IAU announced the winning name was Harriot for this planet. (55 Cancri in the constellation Cancer). The winning name was submitted by the Royal Netherlands Association for Meteorology and Astronomy of the Netherlands. It honors the astronomer.\n\nThe Thomas Harriot College of Arts and Sciences at East Carolina University in Greenville, NC is named in recognition of this Harriot's scientific contributions to the New World such as his work \"A Briefe and True Report of the New Found Land of Virginia\".\n\n\n\n\n\n"}
{"id": "43946143", "url": "https://en.wikipedia.org/wiki?curid=43946143", "title": "Tobias Nipkow", "text": "Tobias Nipkow\n\nTobias Nipkow (born 1958) is a German computer scientist. He received his Diplom (MSc) in computer science from the Technische Hochschule Darmstadt in 1982, and his Ph.D. from the University of Manchester in 1987. He worked at MIT from 1987, changed to Cambridge University in 1989, and to Technical University Munich in 1992, where he was appointed professor for programming theory. He is chair of the Logic and Verification group since 2011.\n\nHe is known for his work in interactive and automatic theorem proving, in particular for the Isabelle proof assistant; he is the editor of the Journal of Automated Reasoning. Moreover, he focuses on programming language semantics, type systems and functional programming.\n\n\n"}
{"id": "701142", "url": "https://en.wikipedia.org/wiki?curid=701142", "title": "Toy model", "text": "Toy model\n\nIn the modeling of physics, a toy model is a deliberately simplistic model with many details removed so that it can be used to explain a mechanism concisely. It is also useful in a description of the fuller model. \n\n\nThe phrase \"tinker-toy model\" is also used, in reference to the popular Tinkertoys used for children's constructivist learning.\n\nExamples of toy models in physics include:\n\n\n"}
{"id": "59182989", "url": "https://en.wikipedia.org/wiki?curid=59182989", "title": "Translation surface (differential geometry)", "text": "Translation surface (differential geometry)\n\nIn differential geometry a translation surface is a surface, that is generated by translations: \n\nIf both curves are contained in a common plane the translation surface is planar (part of a plane). This case is of no interest and will be omitted here.\n\nSimple \"examples\":\n\nTranslation surfaces are popular in descriptive geometry and architecture , because they can be modelled easily. <br>\nIn differential geometry minimal surfaces are represented by translation surfaces or as \"midchord surfaces\" (s. below) .\n\nThe translation surfaces as defined here should not be confused with the translation surfaces in complex geometry.\n\nFor two space curves formula_15 and formula_16 with formula_17 the translation surface formula_18 can be represented by :\nand contains the origin. Obviously this definition is symmetric regarding the curves formula_3 and formula_5. Therefore both curves are called generatrices (one: generatrix). Any point formula_22 of the surface is contained in a shifted copy of formula_3 and formula_5 resp.. The tangent plane at formula_22 is generated by the tangentvectors of the generatrices at this point, if these vectors are linearly independent.\n\nIf the precondition formula_17 is not fulfilled, the surface defined by (TS) may not contain the origin and the curves formula_27. But in any case the surface contains shifted copies of any of the curves formula_27 as parametric curves formula_29 and formula_30 respectively.\n\nThe two curves formula_27 can be used to generate the so called corresponding midchord surface. Its parametric representaion is\n\nA helicoid is a special case of a generalized helicoid and a ruled surface. It is an example of a minimal surface and can be represented as a translation surface.\nThe helicoid with the parametric representation \nhas a \"turn around shift\" (German: Ganghöhe) formula_34. \nIntroducing new parameters formula_35 such that\nand formula_37 a positive real number, one gets a new parametric representation \nwhich is the parametric representation of a translation surface with the two \"identical\" (!) generatrices\nThe common point used for the diagram is formula_42.\nThe (identical) generatrices are helices with the turn around shift formula_43 which lie on the cylinder with the equation formula_44. Any parametric curve is a shifted copy of the generatrix formula_3 (in diagram: purple) and is contained in the right circular cylinder with radius formula_37, which contains the \"z\"-axis.\nThe new parametric representation represents only such points of the helicoid that are within the cylinder with the equation formula_47.\nFrom the new parametric representation one recognizes, that the helicoid is a midchord surface, too:\n\nwhere \nare two identical generatrices.\n\nIn diagram: formula_51 lies on the helix formula_52 and formula_53 on the (identical) helix formula_54. The midpoint of the chord is formula_55.\n\nA surface (for example a roof) can be manufactured using a jig for curve \nformula_5 and several identical jigs of curve formula_3. The jigs can be designed without any knowledge of mathematics. By positioning the jigs the rules of a translation surface have to be respected only. \n\nEstablishing a parallel projection of a translation surface one 1) has to produce projections of the two generatrices, 2) make a jig of curve formula_3 and 3) draw with help of this jig copies of the curve respecting the rules of a translation surface. The contour of the surface is the envelope of the curves drawn with the jig. This procedure works for orthogonal and oblique projections, but not for central projections.\n\nFor a translation surface with parametric representation\nformula_59\nthe partial derivatives of formula_60 are simple derivatives of the curves. Hence the mixed derivatives are always formula_61 and the coefficient formula_62 of the second fundamental form is formula_61, too. This is an essential facilitation for showing that (for example) a helicoid is a minimal surface.\n\n\n"}
{"id": "8367611", "url": "https://en.wikipedia.org/wiki?curid=8367611", "title": "Two-center bipolar coordinates", "text": "Two-center bipolar coordinates\n\nIn mathematics, two-center bipolar coordinates is a coordinate system, based on two coordinates which give distances from two fixed centers, formula_1 and formula_2. This system is very useful in some scientific applications (e.g. calculating the electric field of a dipole on a plane).\n\nThe transformation to Cartesian coordinates formula_3 from two-center bipolar coordinates formula_4 is\n\nwhere the centers of this coordinate system are at formula_7 and formula_8.\n\nWhen \"x\">0 the transformation to polar coordinates from two-center bipolar coordinates is\n\nwhere formula_11 is the distance between the poles (coordinate system centers).\n\n"}
