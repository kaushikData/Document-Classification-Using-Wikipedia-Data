{"id": "6317445", "url": "https://en.wikipedia.org/wiki?curid=6317445", "title": "252 (number)", "text": "252 (number)\n\n252 (two hundred [and] fifty-two) is the natural number following 251 and preceding 253.\n\n252 is:\n\nThere are 252 points on the surface of a cuboctahedron of radius five in the fcc lattice, 252 ways of writing the number 4 as a sum of six squares of integers, 252 ways of choosing four squares from a 4×4 chessboard up to reflections and rotations, and 252 ways of placing three pieces on a Connect Four board.\n"}
{"id": "27344508", "url": "https://en.wikipedia.org/wiki?curid=27344508", "title": "Abstract elementary class", "text": "Abstract elementary class\n\nIn model theory, a discipline within mathematical logic, an abstract elementary class, or AEC for short, is a class of models with a partial order similar to the relation of an elementary substructure of an elementary class in first-order model theory. They were introduced by Saharon Shelah.\n\nformula_1, for formula_2 a class of structures in some language formula_3, is an AEC if it has the following properties:\n\nNote that we usually do not care about the models of size less than the Löwenheim–Skolem number and often assume that there are none (we will adopt this convention in this article). This is justified since we can always remove all such models from an AEC without influencing its structure above the Löwenheim–Skolem number.\n\nA formula_2-embedding is a map formula_39 for formula_40 such that formula_41 and formula_42 is an isomorphism from formula_7 onto formula_44. If formula_2 is clear from context, we omit it.\n\nThe following are examples of abstract elementary classes:\n\n\nAECs are very general objects and one usually make some of the assumptions below when studying them:\n\n\nNote that in elementary classes, joint embedding holds whenever the theory is complete, while amalgamation and no maximal models are well-known consequences of the compactness theorem. These three assumptions allow us to build a universal model-homogeneous monster model formula_67, exactly as in the elementary case.\n\nAnother assumption that one can make is tameness.\n\nShelah introduced AECs to provide a uniform framework in which to generalize first-order classification theory. Classification theory started with Morley's categoricity theorem, so it is natural to ask whether a similar result holds in AECs. This is Shelah's eventual categoricity conjecture. It states that there should be a Hanf number for categoricity:\n\nFor every AEC \"K\" there should be a cardinal formula_36 depending only on formula_35 such that if \"K\" is categorical in \"some\" formula_70 (i.e. \"K\" has exactly one (up to isomorphism) model of size formula_71), then \"K\" is categorical in formula_72 for \"all\" formula_73.\n\nShelah also has several stronger conjectures: The threshold cardinal for categoricity is the Hanf number of psedo elemtary classes in a language of cardinality LS(K). More specifically when the class is in a countable language and axiomaziable by an formula_74 sentence the threshold number for categoricity is formula_75. This conjecture dates back to 1976.\n\nSeveral approximations have been published (see for example the results section below), assuming set-theoretic assumptions (such as the existence of large cardinals or variations of the generalized continuum hypothesis), or model-theoretic assumptions (such as amalgamation or tameness). As of 2014, the original conjecture remains open.\n\nThe following are some important results about AECs. Except for the last, all results are due to Shelah.\n\n\n\n"}
{"id": "20946870", "url": "https://en.wikipedia.org/wiki?curid=20946870", "title": "Alistair Sinclair", "text": "Alistair Sinclair\n\nAlistair Sinclair (born 1960) is a British computer scientist and computational theorist.\n\nSinclair received his B.A. in Mathematics from St. John’s College, Cambridge in 1979, and his Ph.D. in Computer Science from the University of Edinburgh in 1988 under the supervision of Mark Jerrum. He is professor at the Computer Science division at UC Berkeley and has held faculty positions at University of Edinburgh and visiting positions at DIMACS and the International Computer Science Institute in Berkeley.\n\nSinclair’s research interests include the design and analysis of randomized algorithms, computational applications of stochastic processes and nonlinear dynamical systems, Monte Carlo methods in Statistical Physics,\nand combinatorial optimization. With his advisor Mark Jerrum, Sinclair investigated the mixing behaviour of Markov chains to construct approximation algorithms for counting problems such as the computing the permanent, with applications in diverse fields such as matching algorithms, geometric algorithms, mathematical programming, statistics, physics-inspired applications, and dynamical systems. This work has been highly influential in theoretical computer science and was recognised with the Gödel Prize in 1996. A refinement of these methods led to a fully polynomial time randomised approximation algorithm for computing the permanent, for which Sinclair and his co-authors received the Fulkerson Prize in 2006.\n"}
{"id": "31104610", "url": "https://en.wikipedia.org/wiki?curid=31104610", "title": "Apollonian network", "text": "Apollonian network\n\nIn combinatorial mathematics, an Apollonian network is an undirected graph formed by a process of recursively subdividing a triangle into three smaller triangles. Apollonian networks may equivalently be defined as the planar 3-trees, the maximal planar chordal graphs, the uniquely 4-colorable planar graphs, and the graphs of stacked polytopes. They are named after Apollonius of Perga, who studied a related circle-packing construction.\n\nAn Apollonian network may be formed, starting from a single triangle embedded in the Euclidean plane, by repeatedly selecting a triangular face of the embedding, adding a new vertex inside the face, and connecting the new vertex to each vertex of the face containing it. In this way, the triangle containing the new vertex is subdivided into three smaller triangles, which may in turn be subdivided in the same way.\n\nThe complete graphs on three and four vertices, and , are both Apollonian networks. is formed by starting with a triangle and not performing any subdivisions, while is formed by making a single subdivision before stopping.\n\nThe Goldner–Harary graph is an Apollonian network that forms the smallest non-Hamiltonian maximal planar graph. Another more complicated Apollonian network was used by to provide an example of a 1-tough non-Hamiltonian maximal planar graph.\n\nAs well as being defined by recursive subdivision of triangles, the Apollonian networks have several other equivalent mathematical characterizations. They are the chordal maximal planar graphs, the chordal polyhedral graphs, and the planar 3-trees. They are the uniquely 4-colorable planar graphs, and the planar graphs with a unique Schnyder wood decomposition into three trees. They are the maximal planar graphs with treewidth three, a class of graphs that can be characterized by their forbidden minors or by their reducability under Y-Δ transforms. They are the maximal planar graphs with degeneracy three. They are also the planar graphs on a given number of vertices that have the largest possible number of triangles, the largest possible number of tetrahedral subgraphs, the largest possible number of cliques, and the largest possible number of pieces after decomposing by separating triangles.\n\nApollonian networks are examples of maximal planar graphs, graphs to which no additional edges can be added without destroying planarity, or equivalently graphs that can be drawn in the plane so that every face (including the outer face) is a triangle. They are also chordal graphs, graphs in which every cycle of four or more vertices has a diagonal edge connecting two non-consecutive cycle vertices, and the order in which vertices are added in the subdivision process that forms an Apollonian network is an elimination ordering as a chordal graph. This forms an alternative characterization of the Apollonian networks: they are exactly the chordal maximal planar graphs or equivalently the chordal polyhedral graphs.\n\nIn an Apollonian network, every maximal clique is a complete graph on four vertices, formed by choosing any vertex and its three earlier neighbors. Every minimal clique separator (a clique that partitions the graph into two disconnected subgraphs) is one of the subdivided triangles. A chordal graph in which all maximal cliques and all minimal clique separators have the same size is a -tree, and Apollonian networks are examples of 3-trees. Not every 3-tree is planar, but the planar 3-trees are exactly the Apollonian networks.\n\nEvery Apollonian network is also a uniquely 4-colorable graph. Because it is a planar graph, the four color theorem implies that it has a graph coloring with only four colors, but once the three colors of the initial triangle are selected, there is only one possible choice for the color of each successive vertex, so up to permutation of the set of colors it has exactly one 4-coloring. It is more difficult to prove, but also true, that every uniquely 4-colorable planar graph is an Apollonian network. Therefore, Apollonian networks may also be characterized as the uniquely 4-colorable planar graphs. Apollonian networks also provide examples of planar graphs having as few -colorings as possible for .\n\nThe Apollonian networks are also exactly the maximal planar graphs that (once an exterior face is fixed) have a unique Schnyder wood, a partition of the edges of the graph into three interleaved trees rooted at the three vertices of the exterior face.\n\nThe Apollonian networks do not form a family of graphs that is closed under the operation of taking graph minors, as removing edges but not vertices from an Apollonian network produces a graph that is not an Apollonian network. However, the planar partial 3-trees, subgraphs of Apollonian networks, are minor-closed. Therefore, according to the Robertson–Seymour theorem, they can be characterized by a finite number of forbidden minors. The minimal forbidden minors for the planar partial 3-trees are the four minimal graphs among the forbidden minors for the planar graphs and the partial 3-trees: the complete graph , the complete bipartite graph , the graph of the octahedron, and the graph of the pentagonal prism. The Apollonian graphs are the maximal graphs that do not have any of these four graphs as a minor.\n\nA Y-Δ transform, an operation that replaces a degree-three vertex in a graph by a triangle connecting its neighbors, is sufficient (together with the removal of parallel edges) to reduce any Apollonian network to a single triangle, and more generally the planar graphs that can be reduced to a single edge by Y-Δ transforms, removal of parallel edges, removal of degree-one vertices, and compression of degree-two vertices are exactly the planar partial 3-trees. The dual graphs of the planar partial 3-trees form another minor-closed graph family and are exactly the planar graphs that can be reduced to a single edge by Δ-Y transforms, removal of parallel edges, removal of degree-one vertices, and compression of degree-two vertices.\n\nIn every subgraph of an Apollonian network, the most recently added vertex has degree at most three, so Apollonian networks have degeneracy three. The order in which the vertices are added to create the network is therefore a degeneracy ordering, and the Apollonian networks coincide with the 3-degenerate maximal planar graphs.\n\nAnother characterization of the Apollonian networks involves their connectivity. Any maximal planar graph may be decomposed into 4-vertex-connected maximal planar subgraphs by splitting it along its separating triangles (triangles that are not faces of the graph): given any non-facial triangle: one can form two smaller maximal planar graphs, one consisting of the part inside the triangle and the other consisting of the part outside the triangle. The maximal planar graphs without separating triangles that may be formed by repeated splits of this type are sometimes called blocks, although that name has also been used for the biconnected components of a graph that is not itself biconnected. An Apollonian network is a maximal planar graph in which all of the blocks are isomorphic to the complete graph .\n\nIn extremal graph theory, Apollonian networks are also exactly the -vertex planar graphs in which the number of blocks achieves its maximum, , and the planar graphs in which the number of triangles achieves its maximum, . Since each subgraph of a planar graph must be a block, these are also the planar graphs in which the number of subgraphs achieves its maximum, , and the graphs in which the number of cliques of any type achieves its maximum, .\n\nApollonian networks are named after Apollonius of Perga, who studied the Problem of Apollonius of constructing a circle tangent to three other circles. One method of constructing Apollonian networks is to start with three mutually-tangent circles and then repeatedly inscribe another circle within the gap formed by three previously-drawn circles. The fractal collection of circles produced in this way is called an Apollonian gasket.\n\nIf the process of producing an Apollonian gasket is stopped early, with only a finite set of circles, then the graph that has one vertex for each circle and one edge for each pair of tangent circles is an Apollonian network. The existence of a set of tangent circles whose tangencies represent a given Apollonian network forms a simple instance of the Koebe–Andreev–Thurston circle-packing theorem, which states that any planar graph can be represented by tangent circles in the same way.\n\nApollonian networks are planar 3-connected graphs and therefore, by Steinitz's theorem, can always be represented as the graphs of convex polyhedra. The convex polyhedron representing an Apollonian network is a 3-dimensional stacked polytope. Such a polytope can be obtained from a tetrahedron by repeatedly gluing additional tetrahedra one at a time onto its triangular faces. Therefore, Apollonian networks may also be defined as the graphs of stacked 3d polytopes. It is possible to find a representation of any Apollonian network as convex 3d polyhedron in which all of the coordinates are integers of polynomial size, better than what is known for other planar graphs.\n\nThe recursive subdivision of triangles into three smaller triangles was investigated as an image segmentation technique in computer vision by ; in this context, they called it the ternary scalene triangle decomposition. They observed that, by placing each new vertex at the centroid of its enclosing triangle, the triangulation could be chosen in such a way that all triangles have equal areas, although they do not all have the same shape. More generally,\nApollonian networks may be drawn in the plane with any prescribed area in each face; if the areas are rational numbers, so are all of the vertex coordinates.\n\nIt is also possible to carry out the process of subdividing a triangle to form an Apollonian network in such a way that, at every step, the edge lengths are rational numbers; it is an open problem whether every planar graph has a drawing with this property. It is possible in polynomial time to find a drawing of a planar 3-tree with integer coordinates minimizing the area of the bounding box of the drawing, and to test whether a given planar 3-tree may be drawn with its vertices on a given set of points.\n\n used Apollonian networks to construct an infinite family of maximal planar graphs with an even number of vertices but with no perfect matching. Plummer's graphs are formed in two stages. In the first stage, starting from a triangle , one repeatedly subdivides the triangular face of the subdivision that contains edge : the result is a graph consisting of a path from to the final subdivision vertex together with an edge from each path vertex to each of and . In the second stage, each of the triangular faces of the resulting planar graph is subdivided one more time. If the path from to the final subdivision vertex of the first stage has even length, then the number of vertices in the overall graph is also even. However, approximately 2/3 of the vertices are the ones inserted in the second stage; these form an independent set, and cannot be matched to each other, nor are there enough vertices outside the independent set to find matches for all of them.\n\nAlthough Apollonian networks themselves may not have perfect matchings, the planar dual graphs of Apollonian networks are 3-regular graphs with no cut edges, so by a theorem of they are guaranteed to have at least one perfect matching. However, in this case more is known: the duals of Apollonian networks always have an exponential number of perfect matchings. László Lovász and Michael D. Plummer conjectured that a similar exponential lower bound holds more generally for every 3-regular graph without cut edges, a result that was later proven.\n\n studied power laws in the degree sequences of a special case of networks of this type, formed by subdividing all triangles the same number of times. They used these networks to model packings of space by particles of varying sizes. Based on their work, other authors introduced random Apollonian networks, formed by repeatedly choosing a random face to subdivide, and they showed that these also obey power laws in their degree distribution and have small average distances. Alan M. Frieze and Charalampos E. Tsourakakis analyzed the highest degrees and the eigenvalues of random Apollonian networks . Andrade et al. also observed that their networks satisfy the small world effect, that all vertices are within a small distance of each other. Based on numerical evidence they estimated the average distance between randomly selected pairs of vertices in an -vertex network of this type to be proportional to , but later researchers showed that the average distance is actually proportional to .\n\n observed that if each new vertex is placed at the incenter of its triangle, so that the edges to the new vertex bisect the angles of the triangle, then the set of triples of angles of triangles in the subdivision, when reinterpreted as triples of barycentric coordinates of points in an equilateral triangle, converges in shape to the Sierpinski triangle as the number of levels of subdivision grows.\n\n claimed erroneously that all Apollonian networks have Hamiltonian cycles; however, the Goldner–Harary graph provides a counterexample. If an Apollonian network has toughness greater than one (meaning that removing any set of vertices from the graph leaves a smaller number of connected components than the number of removed vertices) then it necessarily has a Hamiltonian cycle, but there exist non-Hamiltonian Apollonian networks whose toughness is equal to one.\n\nThe combinatorial enumeration problem of counting Apollonian triangulations was studied by , who showed that they have the simple generating function described by the equation .\nIn this generating function, the term of degree counts the number of Apollonian networks with a fixed outer triangle and vertices.\nThus, the numbers of Apollonian networks (with a fixed outer triangle) on 3, 4, 5, ... vertices are:\na sequence that also counts ternary trees and dissections of convex polygons into odd-sided polygons.\nFor instance, there are 12 6-vertex Apollonian networks: three formed by subdividing the outer triangle once and then subdividing two of the resulting triangles,\nand nine formed by subdividing the outer triangle once, subdividing one of its triangles, and then subdividing one of the resulting smaller triangles.\n\n is an early paper that uses a dual form of Apollonian networks, the planar maps formed by repeatedly placing new regions at the vertices of simpler maps, as a class of examples of planar maps with few colorings.\n\nGeometric structures closely related to Apollonian networks have been studied in polyhedral combinatorics since at least the early 1960s, when they were used by to describe graphs that can be realized as the graph of a polytope in only one way, without dimensional or combinatorial ambiguities, and by to find simplicial polytopes with no long paths. In graph theory, the close connection between planarity and treewidth goes back to , who showed that every minor-closed family of graphs either has bounded treewidth or contains all of the planar graphs. Planar 3-trees, as a class of graphs, were explicitly considered by , , , and many authors since them.\n\nThe name \"Apollonian network\" was given by to the networks they studied in which the level of subdivision of triangles is uniform across the network; these networks correspond geometrically to a type of stacked polyhedron called a Kleetope. Other authors applied the same name more broadly to planar 3-trees in their work generalizing the model of Andrade et al. to random Apollonian networks. The triangulations generated in this way have also been named \"stacked triangulations\" or \"stack-triangulations\".\n\n\n\n"}
{"id": "30543702", "url": "https://en.wikipedia.org/wiki?curid=30543702", "title": "Arieh Iserles", "text": "Arieh Iserles\n\nArieh Iserles (born 2 September 1947) is a computational mathematician, currently Professor of the Numerical Analysis of Differential Equations at the University of Cambridge and a member of the Department of Applied Mathematics and Theoretical Physics.\n\nHe studied at the Hebrew University of Jerusalem and Ben-Gurion University of the Negev and wrote his PhD dissertation on numerical methods for stiff ordinary differential equations. His research comprises many themes in computational and applied mathematics: ordinary and partial differential equations, approximation theory, geometric numerical integration, orthogonal polynomials, functional equations, computational dynamics and the computation of highly oscillatory phenomena.\n\nHe has written a textbook, \"A First Course in the Numerical Analysis of Differential Equations\" (Cambridge University Press, 2nd ed. 2009).\n\nArieh Iserles is the Managing Editor of Acta Numerica, Editor-in-Chief of IMA Journal of Numerical Analysis and an editor of several other mathematical journals. From 1997 to 2000 he was the chair of the Society for the Foundations of Computational Mathematics.\n\nFrom 2010 to 2015 he was a Director of the Cambridge Centre for Analysis (CCA), an EPSRC-funded Centre for Doctoral Training in mathematical analysis.\n\nIn 1999, he was awarded the Onsager Medal, by the Norwegian University of Science and Technology, in 2012 he received the David Crighton medal, presented by the Institute of Mathematics and its Applications and London Mathematical Society \"for services to mathematics and the mathematics community\" and in 2014 he was awarded by Society for Industrial and Applied Mathematics the SIAM Prize for Distinguished Service to the Profession. In 2012, Professor Iserles was an invited speaker at the 6th European Congress of Mathematics in Kraków, 2–7 July 2012.\n\n"}
{"id": "1965869", "url": "https://en.wikipedia.org/wiki?curid=1965869", "title": "Banks–Zaks fixed point", "text": "Banks–Zaks fixed point\n\nIn quantum chromodynamics (and also \"N\" = 1 superquantum chromodynamics) with massless flavors, if the number of flavors, \"N\", is sufficiently small (i.e. small enough to guarantee asymptotic freedom, depending on the number of colors), the theory can flow to an interacting conformal fixed point of the renormalization group. If the value of the coupling at that point is less than one (\"i.e.\" one can perform perturbation theory in weak coupling), then the fixed point is called a Banks–Zaks fixed point. The existence of the fixed point was first reported by William E. Caswell in 1974, and later used by Banks and Zaks in their analysis of the phase structure of vector-like gauge theories with massless fermions. For this reason one also justifiably finds references to a Caswell-Banks–Zaks fixed point.\n\nMore specifically, suppose that we find that the beta function of a theory up to two loops has the form\n\nwhere formula_2 and formula_3 are positive constants. Then there exists a value formula_4 such that formula_5:\n\nIf we can arrange formula_2 to be smaller than formula_3, then we have formula_9. It follows that when the theory flows to the IR it is a conformal, weakly coupled theory with coupling formula_10.\n\nFor the case of a non-Abelian gauge theory with gauge group formula_11 and Dirac fermions in the fundamental representation of the gauge group for the flavored particles we have \n\nwhere formula_13 is the number of colors and formula_14 the number of flavors. Then formula_14 should lie just below formula_16 in order for the Banks–Zaks fixed point to appear. Note that this fixed point only occurs if, in addition to the previous requirement on formula_14 (which guarantees asymptotic freedom), \nwhere the lower bound comes from requiring formula_19. This way formula_3 remains positive while formula_21 is still negative (see first equation in article) and one can solve formula_22 with real solutions for formula_23.\n\n\n"}
{"id": "6093954", "url": "https://en.wikipedia.org/wiki?curid=6093954", "title": "Bernays–Schönfinkel class", "text": "Bernays–Schönfinkel class\n\nThe Bernays–Schönfinkel class (also known as Bernays–Schönfinkel-Ramsey class) of formulas, named after Paul Bernays and Moses Schönfinkel (and Frank P. Ramsey), is a fragment of first-order logic formulas where satisfiability is decidable.\n\nIt is the set of sentences that, when written in prenex normal form, have an formula_1 quantifier prefix and do not contain any function symbols.\n\nThis class of logic formulas is also sometimes referred as effectively propositional (EPR) since it can be effectively translated into propositional logic formulas by a process of grounding or instantiation.\n\nThe satisfiability problem for this class is NEXPTIME-complete.\n\n\n"}
{"id": "672190", "url": "https://en.wikipedia.org/wiki?curid=672190", "title": "Bifundamental representation", "text": "Bifundamental representation\n\nIn mathematics and theoretical physics, a bifundamental representation is a representation obtained as a tensor product of two fundamental or antifundamental representations.\n\nFor example, the \"MN\"-dimensional representation (\"M\",\"N\") of the group \n\nis a bifundamental representation. \n\nThese representations occur in quiver diagrams.\n"}
{"id": "40147217", "url": "https://en.wikipedia.org/wiki?curid=40147217", "title": "Bipolar orientation", "text": "Bipolar orientation\n\nIn graph theory, a bipolar orientation or st\"-orientation of an undirected graph is an assignment of a direction to each edge (an orientation) that causes the graph to become a directed acyclic graph with a single source \"s\" and a single sink \"t\", and an st\"-numbering of the graph is a topological ordering of the resulting directed acyclic graph.\n\nLet \"G\" = (\"V\",\"E\") be an undirected graph with \"n\" = |\"V\"| vertices. An orientation of \"G\" is an assignment of a direction to each edge of \"G\", making it into a directed graph. It is an acyclic orientation if the resulting directed graph has no directed cycles. Every acyclically oriented graph has at least one \"source\" (a vertex with no incoming edges) and at least one \"sink\" (a vertex with no outgoing edges); it is a bipolar orientation if it has exactly one source and exactly one sink. In some situations, \"G\" may be given together with two designated vertices \"s\" and \"t\"; in this case, a bipolar orientation for \"s\" and \"t\" must have \"s\" as its unique source and \"t\" as its unique sink.\n\nAn \"st\"-numbering of \"G\" (again, with two designated vertices \"s\" and \"t\") is an assignment of the integers from 1 to \"n\" to the vertices of \"G\", such that\n\nA graph has a bipolar orientation if and only if it has an \"st\"-numbering. For, if it has a bipolar orientation, then an \"st\"-numbering may be constructed by finding a topological ordering of the directed acyclic graph given by the orientation, and numbering each vertex by its position in the ordering. In the other direction, every \"st\"-numbering gives rise to a topological ordering, in which each edge of \"G\" is oriented from its lower-numbered endpoint to its higher-numbered endpoint. In a graph containing edge \"st\", an orientation is bipolar if and only if it is acyclic and the orientation formed by reversing edge \"st\" is totally cyclic.\n\nA connected graph \"G\", with designated vertices \"s\" and \"t\", has a bipolar orientation and an \"st\"-numbering if and only if the graph formed from \"G\" by adding an edge from \"s\" to \"t\" is 2-vertex-connected. In one direction, if this graph is 2-vertex-connected, then a bipolar orientation may be obtained by consistently orienting each ear in an ear decomposition of the graph. In the other direction, if the graph is not 2-vertex-connected, then it has an articulation vertex \"v\" separating some biconnected component of \"G\" from \"s\" and \"t\". If this component contains a vertex with a lower number than \"v\", then the lowest-numbered vertex in the component cannot have a lower-numbered neighbor, and symmetrically if it contains a vertex with a higher number than \"v\" then the highest-numbered vertex in the component cannot have a higher-numbered neighbor.\n\n formulated \"st\"-numberings as part of a planarity testing algorithm, and formulated bipolar orientations as part of an algorithm for constructing tessellation representations of planar graphs.\n\nA bipolar orientation of a planar graph results in an \"st\"-planar graph, a directed acyclic planar graph with one source and one sink. These graphs are of some importance in lattice theory as well as in graph drawing: the Hasse diagram of a two-dimensional lattice is necessarily \"st\"-planar, and every transitively reduced \"st\"-planar graph represents a two-dimensional lattice in this way. A directed acyclic graph \"G\" has an upward planar drawing if and only if \"G\" is a subgraph of an \"st\"-planar graph.\n\nIt is possible to find an \"st\"-numbering, and a bipolar orientation, of a given graph with designated vertices \"s\" and \"t\", in linear time using depth-first search. The algorithm of uses a depth-first search that starts at vertex \"s\" and first traverses edge \"st\". As in the depth-first-search based algorithm for testing whether a graph is biconnected, this algorithm defines pre(\"v\"), for a vertex \"v\", to be the preorder number of \"v\" in the depth-first traversal, and low(\"v\") to be the smallest preorder number that can be reached by following a single edge from a descendant of \"v\" in the depth-first search tree. Both of these numbers may be computed in linear time as part of the depth-first search. The given graph will be biconnected (and will have a bipolar orientation) if and only if \"t\" is the only child of \"s\" in the depth-first search tree and low(\"v\") < pre(\"v\") for all vertices \"v\" other than \"s\". Once these numbers have been computed, Tarjan's algorithm performs a second traversal of the depth-first search tree, maintaining a number sign(\"v\") for each vertex \"v\" and a linked list of vertices that will eventually list all vertices of the graph in the order given by an \"st\"-numbering. Initially, the list contains \"s\" and \"t\", and sign(\"s\") = −1. When each vertex \"v\" is first encountered by this second traversal, \"v\" is inserted into the list, either before or after its parent p(\"v\") in the depth-first search tree according to whether sign(low(\"v\")) is negative or positive respectively; then sign(p(\"v\")) is set to −sign(low(\"v\")). As Tarjan shows, the vertex ordering resulting from this procedure gives an \"st\"-numbering of the given graph.\n\nAlternatively, efficient sequential and parallel algorithms may be based on ear decomposition. An open ear decomposition of a given graph, with designated vertices \"s\" and \"t\", may be defined as a partition of the edges of the graph into a sequence of paths called \"ears\", in which the endpoints of the first ear are \"s\" and \"t\", the endpoints of each subsequent ear appear in previous ears in the sequence, and each vertex in the interior of one of the ears appears for the first time in that ear. An open ear decomposition exists if and only if the graph formed from the given graph by adding an edge \"st\" is biconnected (the same condition as the existence of a bipolar orientation), and it can be found in linear time. An \"st\"-orientation may be obtained by directing each ear in a consistent direction, taking care that if there already exists a directed path connecting the same two endpoints among the edges of previous ears then the new ear must be oriented in the same direction. However, testing this directly by a reachability computation would be slow. provide a complicated but localized search procedure for determining an appropriate orientation for each ear that (unlike the approach using depth-first search) is suitable for parallel computation.\n\nFor 3-vertex-connected graphs, with designated vertices \"s\" and \"t\", any two bipolar orientations may be connected to each other by a sequence of operations that reverse one edge at a time, at each step maintaining a bipolar orientation. More strongly, for planar 3-connected graphs, the set of bipolar orientations can be given the structure of a finite distributive lattice, with the edge-reversal operation corresponding to the covering relation of the lattice. For any graph with designated source and sink, the set of all bipolar orientations may be listed in polynomial time per orientation.\n"}
{"id": "34552620", "url": "https://en.wikipedia.org/wiki?curid=34552620", "title": "Box counting", "text": "Box counting\n\nBox counting is a method of gathering data for analyzing complex patterns by breaking a dataset, object, image, etc. into smaller and smaller pieces, typically \"box\"-shaped, and analyzing the pieces at each smaller scale. The essence of the process has been compared to zooming in or out using optical or computer based methods to examine how observations of detail change with scale. In box counting, however, rather than changing the magnification or resolution of a lens, the investigator changes the size of the element used to inspect the object or pattern (see Figure 1). Computer based box counting algorithms have been applied to patterns in 1-, 2-, and 3-dimensional spaces. The technique is usually implemented in software for use on patterns extracted from digital media, although the fundamental method can be used to investigate some patterns physically. The technique arose out of and is used in fractal analysis. It also has application in related fields such as lacunarity and multifractal analysis.\n\nTheoretically, the intent of box counting is to quantify fractal scaling, but from a practical perspective this would require that the scaling be known ahead of time. This can be seen in Figure 1 where choosing boxes of the right relative sizes readily shows how the pattern repeats itself at smaller scales. In fractal analysis, however, the scaling factor is not always known ahead of time, so box counting algorithms attempt to find an optimized way of cutting a pattern up that will reveal the scaling factor. The fundamental method for doing this starts with a set of measuring elements—\"boxes\"—consisting of an arbitrary number, called formula_1 here for convenience, of sizes or calibres, which we will call the set of formula_2s. Then these formula_2-sized boxes are applied to the pattern and counted. To do this, for each formula_2 in formula_1, a measuring element that is typically a 2-dimensional square or 3-dimensional box with side length corresponding to formula_2 is used to scan a pattern or data set (e.g., an image or object) according to a predetermined scanning plan to cover the relevant part of the data set, recording, i.e.,\"counting\", for each step in the scan relevant features captured within the measuring element.\n\nThe relevant features gathered during box counting depend on the subject being investigated and the type of analysis being done. Two well-studied subjects of box counting, for instance, are binary (meaning having only two colours, usually black and white) and gray-scale digital images (i.e., jpgs, tiffs, etc.). Box counting is generally done on patterns extracted from such still images in which case the raw information recorded is typically based on features of pixels such as a predetermined colour value or range of colours or intensities. When box counting is done to determine a fractal dimension known as the box counting dimension, the information recorded is usually either yes or no as to whether or not the box contained any pixels of the predetermined colour or range (i.e., the number of boxes containing relevant pixels at each formula_2 is counted). For other types of analysis, the data sought may be the number of pixels that fall within the measuring box, the range or average values of colours or intensities, the spatial arrangement amongst pixels within each box, or properties such as average speed (e.g., from particle flow).\n\nEvery box counting algorithm has a scanning plan that describes how the data will be gathered, in essence, how the box will be moved over the space containing the pattern. A variety of scanning strategies has been used in box counting algorithms, where a few basic approaches have been modified in order to address issues such as sampling, analysis methods, etc.\n\nThe traditional approach is to scan in a non-overlapping regular grid or lattice pattern. To illustrate, Figure 2a shows the typical pattern used in software that calculates box counting dimensions from patterns extracted into binary digital images of contours such as the fractal contour illustrated in Figure 1 or the classic example of the coastline of Britain often used to explain the method of finding a box counting dimension. The strategy simulates repeatedly laying a square box as though it were part of a grid overlaid on the image, such that the box for each formula_2 never overlaps where it has previously been (see Figure 4). This is done until the entire area of interest has been scanned using each formula_2 and the relevant information has been recorded.\n\nAnother approach that has been used is a sliding box algorithm, in which each box is slid over the image overlapping the previous placement. Figure 2b illustrates the basic pattern of scanning using a sliding box. The fixed grid approach can be seen as a sliding box algorithm with the increments horizontally and vertically equal to formula_2. Sliding box algorithms are often used for analyzing textures in lacunarity analysis and have also been applied to multifractal analysis\n\nBox counting may also be used to determine local variation as opposed to global measures describing an entire pattern. Local variation can be assessed after the data have been gathered and analyzed (e.g., some software colour codes areas according to the fractal dimension for each subsample), but a third approach to box counting is to move the box according to some feature related to the pixels of interest. In local connected dimension box counting algorithms, for instance, the box for each formula_2 is centred on each pixel of interest, as illustrated in Figure 2c.\n\nThe implementation of any box counting algorithm has to specify certain details such as how to determine the actual values in formula_1, including the minimum and maximum sizes to use and the method of incrementing between sizes. Many such details reflect practical matters such as the size of a digital image but also technical issues related to the specific analysis that will be performed on the data. Another issue that has received considerable attention is how to approximate the so-called \"optimal covering\" for determining box counting dimensions and assessing multifractal scaling.\n\nOne known issue in this respect is deciding what constitutes the edge of the useful information in a digital image, as the limits employed in the box counting strategy can affect the data gathered.\n\nThe algorithm has to specify the type of increment to use between box sizes (e.g., linear vs exponential), which can have a profound effect on the results of a scan.\n\nAs Figure 4 illustrates, the overall positioning of the boxes also influences the results of a box count. One approach in this respect is to scan from multiple orientations and use averaged or optimized data.\n\nTo address various methodological considerations, some software is written so users can specify many such details, and some includes methods such as smoothing the data after the fact to be more amenable to the type of analysis being done.\n\n"}
{"id": "273329", "url": "https://en.wikipedia.org/wiki?curid=273329", "title": "Catenoid", "text": "Catenoid\n\nA catenoid is a type of surface, arising by rotating a catenary curve about an axis. It is a minimal surface, meaning that it occupies the least area when bounded by a closed space. It was formally described in 1744 by the mathematician Leonhard Euler.\n\nSoap film attached to twin circular rings will take the shape of a catenoid. Because they are members of the same associate family of surfaces, a catenoid can be bent into a portion of a helicoid, and vice versa.\n\nThe catenoid was the first non-trivial minimal surface in 3-dimensional Euclidean space to be discovered apart from the plane. The catenoid is obtained by rotating a catenary about its directrix. It was found and proved to be minimal by Leonhard Euler in 1744.\n\nEarly work on the subject was published also by Jean Baptiste Meusnier. There are only two minimal surfaces of revolution (surfaces of revolution which are also minimal surfaces): the plane and the catenoid.\n\nThe catenoid may be defined by the following parametric equations:\n\nIn cylindrical coordinates:\n\nA physical model of a catenoid can be formed by dipping two circular rings into a soap solution and slowly drawing the circles apart.\n\nThe catenoid may be also defined approximately by the Stretched grid method as a facet 3D model.\n\nBecause they are members of the same associate family of surfaces, one can bend a catenoid into a portion of a helicoid without stretching. In other words, one can make a (mostly) continuous and isometric deformation of a catenoid to a portion of the helicoid such that every member of the deformation family is minimal (having a mean curvature of zero). A parametrization of such a deformation is given by the system\n\nwhere \nformula_14 corresponds to a right-handed helicoid, \nformula_15 corresponds to a catenoid, and\nformula_16 corresponds to a left-handed helicoid.\n\n"}
{"id": "23388928", "url": "https://en.wikipedia.org/wiki?curid=23388928", "title": "Cayley's ruled cubic surface", "text": "Cayley's ruled cubic surface\n\nIn differential geometry, Cayley's ruled cubic surface is the surface\n\n"}
{"id": "735067", "url": "https://en.wikipedia.org/wiki?curid=735067", "title": "Conull set", "text": "Conull set\n\nIn measure theory, a conull set is a set whose complement is null, i.e., the measure of the complement is zero. For example, the set of irrational numbers is a conull subset of the real line with Lebesgue measure.\n\nA property that is true of the elements of a conull set is said to be true almost everywhere.\n"}
{"id": "254299", "url": "https://en.wikipedia.org/wiki?curid=254299", "title": "Curry–Howard correspondence", "text": "Curry–Howard correspondence\n\nIn programming language theory and proof theory, the Curry–Howard correspondence (also known as the Curry–Howard isomorphism or equivalence, or the proofs-as-programs and propositions- or formulae-as-types interpretation) is the direct relationship between computer programs and mathematical proofs. \n\nIt is a generalization of a syntactic analogy between systems of formal logic and computational calculi that was first discovered by the American mathematician Haskell Curry and logician William Alvin Howard. It is the link between logic and computation that is usually attributed to Curry and Howard, although the idea is related to the operational interpretation of intuitionistic logic given in various formulations by L. E. J. Brouwer, Arend Heyting and Andrey Kolmogorov (see Brouwer–Heyting–Kolmogorov interpretation) and Stephen Kleene (see Realizability). The relationship has been extended to include category theory as the three-way Curry–Howard–Lambek correspondence.\n\nThe beginnings of the Curry–Howard correspondence lie in several observations:\n\nIn other words, the Curry–Howard correspondence is the observation that two families of seemingly unrelated formalisms—namely, the proof systems on one hand, and the models of computation on the other—are in fact the same kind of mathematical objects.\n\nIf one abstracts on the peculiarities of either formalism, the following generalization arises: \"a proof is a program, and the formula it proves is the type for the program\". More informally, this can be seen as an analogy that states that the return type of a function (i.e., the type of values returned by a function) is analogous to a logical theorem, subject to hypotheses corresponding to the types of the argument values passed to the function; and that the program to compute that function is analogous to a proof of that theorem. This sets a form of logic programming on a rigorous foundation: \"proofs can be represented as programs, and especially as lambda terms\", or \"proofs can be run\".\n\nThe correspondence has been the starting point of a large spectrum of new research after its discovery, leading in particular to a new class of formal systems designed to act both as a proof system and as a typed functional programming language. This includes Martin-Löf's intuitionistic type theory and Coquand's Calculus of Constructions, two calculi in which proofs are regular objects of the discourse and in which one can state properties of proofs the same way as of any program. This field of research is usually referred to as modern type theory.\n\nSuch typed lambda calculi derived from the Curry–Howard paradigm led to software like Coq in which proofs seen as programs can be formalized, checked, and run.\n\nA converse direction is to \"use a program to extract a proof\", given its correctness—an area of research closely related to proof-carrying code. This is only feasible if the programming language the program is written for is very richly typed: the development of such type systems has been partly motivated by the wish to make the Curry–Howard correspondence practically relevant.\n\nThe Curry–Howard correspondence also raised new questions regarding the computational content of proof concepts that were not covered by the original works of Curry and Howard. In particular, classical logic has been shown to correspond to the ability to manipulate the continuation of programs and the symmetry of sequent calculus to express the duality between the two evaluation strategies known as call-by-name and call-by-value.\n\nSpeculatively, the Curry–Howard correspondence might be expected to lead to a substantial unification between mathematical logic and foundational computer science:\n\nHilbert-style logic and natural deduction are but two kinds of proof systems among a large family of formalisms. Alternative syntaxes include sequent calculus, proof nets, calculus of structures, etc. If one admits the Curry–Howard correspondence as the general principle that any proof system hides a model of computation, a theory of the underlying untyped computational structure of these kinds of proof system should be possible. Then, a natural question is whether something mathematically interesting can be said about these underlying computational calculi.\n\nConversely, combinatory logic and simply typed lambda calculus are not the only models of computation, either. Girard's linear logic was developed from the fine analysis of the use of resources in some models of lambda calculus; is there typed version of Turing's machine that would behave as a proof system? Typed assembly languages are such an instance of \"low-level\" models of computation that carry types.\n\nBecause of the possibility of writing non-terminating programs, Turing-complete models of computation (such as languages with arbitrary recursive functions) must be interpreted with care, as naive application of the correspondence leads to an inconsistent logic. The best way of dealing with arbitrary computation from a logical point of view is still an actively debated research question, but one popular approach is based on using monads to segregate provably terminating from potentially non-terminating code (an approach that also generalizes to much richer models of computation, and is itself related to modal logic by a natural extension of the Curry–Howard isomorphism). A more radical approach, advocated by total functional programming, is to eliminate unrestricted recursion (and forgo Turing completeness, although still retaining high computational complexity), using more controlled corecursion wherever non-terminating behavior is actually desired.\n\nIn its more general formulation, the Curry–Howard correspondence is a correspondence between formal proof calculi and type systems for models of computation. In particular, it splits into two correspondences. One at the level of formulas and types that is independent of which particular proof system or model of computation is considered, and one at the level of proofs and programs which, this time, is specific to the particular choice of proof system and model of computation considered.\n\nAt the level of formulas and types, the correspondence says that implication behaves the same as a function type, conjunction as a \"product\" type (this may be called a tuple, a struct, a list, or some other term depending on the language), disjunction as a sum type (this type may be called a union), the false formula as the empty type and the true formula as the singleton type (whose sole member is the null object). Quantifiers correspond to dependent function space or products (as appropriate). \nThis is summarized in the following table:\n\nAt the level of proof systems and models of computations, the correspondence mainly shows the identity of structure, first, between some particular formulations of systems known as Hilbert-style deduction system and combinatory logic, and, secondly, between some particular formulations of systems known as natural deduction and lambda calculus.\n\nBetween the natural deduction system and the lambda calculus there are the following correspondences:\n\nIt was at the beginning a simple remark in Curry and Feys's 1958 book on combinatory logic: the simplest types for the basic combinators K and S of combinatory logic surprisingly corresponded to the respective axiom schemes α → (β → α) and (α → (β → γ)) → ((α → β) → (α → γ)) used in Hilbert-style deduction systems. For this reason, these schemes are now often called axioms K and S. Examples of programs seen as proofs in a Hilbert-style logic are given below.\n\nIf one restricts to the implicational intuitionistic fragment, a simple way to formalize logic in Hilbert's style is as follows. Let Γ be a finite collection of formulas, considered as hypotheses. Then δ is \"derivable\" from Γ, denoted Γ ⊢ δ, in the following cases:\n\n\nThis can be formalized using inference rules, as in the left column of the following table.\n\nTyped combinatory logic can be formulated using a similar syntax: let Γ be a finite collection of variables, annotated with their types. A term T (also annotated with its type) will depend on these variables [Γ ⊢ T:δ] when:\n\n\nThe generation rules defined here are given in the right-column below. Curry's remark simply states that both columns are in one-to-one correspondence. The restriction of the correspondence to intuitionistic logic means that some classical tautologies, such as Peirce's law ((α → β) → α) → α, are excluded from the correspondence.\n\nSeen at a more abstract level, the correspondence can be restated as shown in the following table. Especially, the deduction theorem specific to Hilbert-style logic matches the process of abstraction elimination of combinatory logic.\n\nThanks to the correspondence, results from combinatory logic can be transferred to Hilbert-style logic and vice versa. For instance, the notion of reduction of terms in combinatory logic can be transferred to Hilbert-style logic and it provides a way to canonically transform proofs into other proofs of the same statement. One can also transfer the notion of normal terms to a notion of normal proofs, expressing that the hypotheses of the axioms never need to be all detached (since otherwise a simplification can happen).\n\nConversely, the non provability in intuitionistic logic of Peirce's law can be transferred back to combinatory logic: there is no typed term of combinatory logic that is typable with type ((α → β) → α) → α.\n\nResults on the completeness of some sets of combinators or axioms can also be transferred. For instance, the fact that the combinator X constitutes a one-point basis of (extensional) combinatory logic implies that the single axiom scheme\nwhich is the principal type of X, is an adequate replacement to the combination of the axiom schemes \n\nAfter Curry emphasized the syntactic correspondence between Hilbert-style deduction and combinatory logic, Howard made explicit in 1969 a syntactic analogy between the programs of simply typed lambda calculus and the proofs of natural deduction. Below, the left-hand side formalizes intuitionistic implicational natural deduction as a calculus of sequents (the use of sequents is standard in discussions of the Curry–Howard isomorphism as it allows the deduction rules to be stated more cleanly) with implicit weakening and the right-hand side shows the typing rules of lambda calculus. In the left-hand side, Γ, Γ and Γ denote ordered sequences of formulas while in the right-hand side, they denote sequences of named (i.e., typed) formulas with all names different.\n\nTo paraphrase the correspondence, proving Γ ⊢ α means having a program that, given values with the types listed in Γ, manufactures an object of type α. An axiom corresponds to the introduction of a new variable with a new, unconstrained type, the → I rule corresponds to function abstraction and the → E rule corresponds to function application. Observe that the correspondence is not exact if the context Γ is taken to be a set of formulas as, e.g., the λ-terms λx.λy.x and λx.λy.y of type α → α → α would not be distinguished in the correspondence. Examples are given below.\n\nHoward showed that the correspondence extends to other connectives of the logic and other constructions of simply typed lambda calculus. Seen at an abstract level, the correspondence can then be summarized as shown in the following table. Especially, it also shows that the notion of normal forms in lambda calculus matches Prawitz's notion of normal deduction in natural deduction, from which it follows that the algorithms for the type inhabitation problem can be turned into algorithms for deciding intuitionistic provability.\n\nHoward's correspondence naturally extends to other extensions of natural deduction and simply typed lambda calculus. Here is a non-exhaustive list:\n\n\nAt the time of Curry, and also at the time of Howard, the proofs-as-programs correspondence concerned only intuitionistic logic, i.e. a logic in which, in particular, Peirce's law was \"not\" deducible. The extension of the correspondence to Peirce's law and hence to classical logic became clear from the work of Griffin on typing operators that capture the evaluation context of a given program execution so that this evaluation context can be later on reinstalled. The basic Curry–Howard-style correspondence for classical logic is given below. Note the correspondence between the double-negation translation used to map classical proofs to intuitionistic logic and the continuation-passing-style translation used to map lambda terms involving control to pure lambda terms. More particularly, call-by-name continuation-passing-style translations relates to Kolmogorov's double negation translation and call-by-value continuation-passing-style translations relates to a kind of double-negation translation due to Kuroda.\n\nA finer Curry–Howard correspondence exists for classical logic if one defines classical logic not by adding an axiom such as Peirce's law, but by allowing several conclusions in sequents. In the case of classical natural deduction, there exists a proofs-as-programs correspondence with the typed programs of Parigot's λμ-calculus.\n\nA proofs-as-programs correspondence can be settled for the formalism known as Gentzen's sequent calculus but it is not a correspondence with a well-defined pre-existing model of computation as it was for Hilbert-style and natural deductions.\n\nSequent calculus is characterized by the presence of left introduction rules, right introduction rule and a cut rule that can be eliminated. The structure of sequent calculus relates to a calculus whose structure is close to the one of some abstract machines. The informal correspondence is as follows:\n\nN. G. de Bruijn used the lambda notation for representing proofs of the theorem checker Automath, and represented propositions as \"categories\" of their proofs. It was in the late 1960s at the same period of time Howard wrote his manuscript; de Bruijn was likely unaware of Howard's work, and stated the correspondence independently (Sørensen & Urzyczyn [1998] 2006, pp 98–99). Some researchers tend to use the term Curry–Howard–de Bruijn correspondence in place of Curry–Howard correspondence.\n\nThe BHK interpretation interprets intuitionistic proofs as functions but it does not specify the class of functions relevant for the interpretation. If one takes lambda calculus for this class of function, then the BHK interpretation tells the same as Howard's correspondence between natural deduction and lambda calculus.\n\nKleene's recursive realizability splits proofs of intuitionistic arithmetic into the pair of a recursive function and of\na proof of a formula expressing that the recursive function \"realizes\", i.e. correctly instantiates the disjunctions and existential quantifiers of the initial formula so that the formula gets true.\n\nKreisel's modified realizability applies to intuitionistic higher-order predicate logic and shows that the simply typed lambda term inductively extracted from the proof realizes the initial formula. In the case of propositional logic, it coincides with Howard's statement: the extracted lambda term is the proof itself (seen as an untyped lambda term) and the realizability statement is a paraphrase of the fact that the extracted lambda term has the type that the formula means (seen as a type).\n\nGödel's dialectica interpretation realizes (an extension of) intuitionistic arithmetic with computable functions. The connection with lambda calculus is unclear, even in the case of natural deduction.\n\nJoachim Lambek showed in the early 1970s that the proofs of intuitionistic propositional logic and the combinators of typed combinatory logic share a common equational theory which is the one of cartesian closed categories. The expression Curry–Howard–Lambek correspondence is now used by some people to refer to the three way isomorphism between intuitionistic logic, typed lambda calculus and cartesian closed categories, with objects being interpreted as types or propositions and morphisms as terms or proofs. The correspondence works at the equational level and is not the expression of a syntactic identity of structures as it is the case for each of Curry's and Howard's correspondences: i.e. the structure of a well-defined morphism in a cartesian-closed category is not comparable to the structure of a proof of the corresponding judgment in either Hilbert-style logic or natural deduction. To clarify this distinction, the underlying syntactic structure of cartesian closed categories is rephrased below.\n\nObjects (types) are defined by\n\nMorphisms (terms) are defined by\n\nWell-defined morphisms (typed terms) are defined by the following typing rules (in which the usual categorical morphism notation formula_19 is replaced with sequent calculus notation formula_20).\n\nIdentity:\n\nComposition:\n\nUnit type (terminal object):\n\nCartesian product:\n\nLeft and right projection:\n\nCurrying:\n\nApplication:\n\nFinally, the equations of the category are\n\nThese equations imply the following formula_40-laws:\n\nNow, there exists formula_13 such that formula_44 iff formula_45 is provable in implicational intuitionistic logic.\n\nThanks to the Curry–Howard correspondence, a typed expression whose type corresponds to a logical formula is analogous to a proof of that formula. Here are examples.\n\nAs an example, consider a proof of the theorem α → α. In lambda calculus, this is the type of the identity function I = λ\"x\".\"x\" and in combinatory logic, the identity function is obtained by applying S = λ\"fgx\".\"fx(gx)\" twice to K = λ\"xy\".\"x\". That is, I = ((S K) K). As a description of a proof, this says that the following steps can be used to prove α → α:\n\n\nIn general, the procedure is that whenever the program contains an application of the form (\"P\" \"Q\"), these steps should be followed:\n\nAs a more complicated example, let's look at the theorem that corresponds to the B function. The type of B is (β → α) → (γ → β) → γ → α. B is equivalent to (S (K S) K). This is our roadmap for the proof of the theorem (β → α) → (γ → β) → γ → α.\n\nThe first step is to construct (K S). To make the antecedent of the K axiom look like the S axiom, set α equal to (α → β → γ) → (α → β) → α → γ, and β equal to δ (to avoid variable collisions):\n\n<br>K[α = (α → β → γ) → (α → β) → α → γ, β=δ] : ((α → β → γ) → (α → β) → α → γ) → δ → (α → β → γ) → (α → β) → α → γ\n\nSince the antecedent here is just S, the consequent can be detached using Modus Ponens:\n\nThis is the theorem that corresponds to the type of (K S). Now apply S to this expression. Taking S as follows\n\nput α = δ, β = α → β → γ, and γ = (α → β) → α → γ, yielding\n\nand then detach the consequent:\n\nThis is the formula for the type of (S (K S)). A special case of this theorem has δ = (β → γ):\n\nThis last formula must be applied to K. Specialize K again, this time by replacing α with (β → γ) and β with α:\n\nThis is the same as the antecedent of the prior formula so, detaching the consequent:\n\nSwitching the names of the variables α and γ gives us\n\nwhich was what remained to prove.\n\nThe diagram below gives proof of (β → α) → (γ → β) → γ → α in natural deduction and shows how it can be interpreted as the λ-expression λ \"a\". λ\"b\". λ \"g\".(\"a\" (\"b\" \"g\")) of type (β → α) → (γ → β) → γ → α.\n\nRecently, the isomorphism has been proposed as a way to define search space partition in genetic programming. The method indexes sets of genotypes (the program trees evolved by the GP system) by their Curry–Howard isomorphic proof (referred to as a species).\n\nThe correspondences listed here go much farther and deeper. For example, cartesian closed categories are generalized by closed monoidal categories. The internal language of these categories is the linear type system (corresponding to linear logic), which generalizes simply-typed lambda calculus as the internal language of cartesian closed categories. Moreover, these can be shown to correspond to cobordisms, which play a vital role in string theory.\n\nAn extended set of equivalences is also explored in homotopy type theory, which became a very active area of research around 2013 and still is. Here, type theory is extended by the univalence axiom (\"equivalence is equivalent to equality\") which permits homotopy type theory to be used as a foundation for all of mathematics (including set theory and classical logic, providing new ways to discuss the axiom of choice and many other things). That is, the Curry–Howard correspondence that proofs are elements of inhabited types is generalized to the notion homotopic equivalence of proofs (as paths in space, the identity type or equality type of type theory being interpreted as a path).\n\n\n\n\n\n\n\n"}
{"id": "654387", "url": "https://en.wikipedia.org/wiki?curid=654387", "title": "Distance geometry problem", "text": "Distance geometry problem\n\nThe distance geometry problem is that of characterization and study of sets of points based \"only\" on given values of the distances between member pairs. Therefore distance geometry has immediate relevance where distance values are determined or considered, such as biology, sensor network, surveying, cartography, and physics.\n\nThe distance geometry problem (DGP) is that of finding the coordinates of a set of points by using the distances between some pairs of such points. There exists nowadays a large community that is actively working on this problem, because there are several real-life applications that can lead to the formulation of a DGP. As an example, an interesting application is that of locating sensors in telecommunication networks. In such a case, the positions of some sensors are known (which are called anchors) and some of the distances between sensors (which can be anchors or not) are also known: the problem is to identify the positions in space for all sensors.\n\nAn interesting application arises in biology. Experimental techniques are able to estimate distances between pairs of atoms of a given molecule, and the problem becomes the one of identifying the three-dimensional conformation of the molecule, i.e. the positions of all its atoms. In this field, the main interest is on proteins, because discovering their three-dimensional conformation allows us to get clues about the function they are able to perform. The implications in related fields, such as biomedicine and drug design, are evident. When dealing with biological molecules, the DGP is generally referred to as molecular DGP (MDGP).\n\nIn the following, even if the article considers in general the DGP, the MDGP will be used as an example.\n\nA straight line is the shortest path between two points. Therefore the distance from \"A\" to \"B\" is no bigger than the length of the straight-line path from \"A\" to \"C\" plus the length of the straight-line path from \"C\" to \"B\". This fact is called the triangle inequality. If that sum happens to be \"equal\" to the distance from \"A\" to \"B\", then the three points \"A\", \"B\", and \"C\" lie on a straight line, with \"C\" between \"A\" and \"B\".\n\nSimilarly, suppose one knows\n\n\nKnowing only these six numbers, one would like to figure out\n\n\nDistance geometry includes the solution of such problems.\n\nOf particular utility and importance are classifications by means of Cayley–Menger determinants, named after Arthur Cayley and Karl Menger:\n\n\n\n\nThe DGP is, by definition, a constraint satisfaction problem. It is however generally reformulated as an optimization problem in a continuous space, and its solution is then attempted by using techniques for global optimization (see for example ).\n\nUnder certain assumptions, however, the problem can be discretized, in the sense that the search domain of the optimization problem can be reduced to a discrete domain. When all distances are supposed to be exact (no experimental errors), the search domain becomes a binary tree, where the candidate positions for the same atom of the molecule are given on a common layer of the tree. The discretization allows us to enumerate the entire solution set (this is not possible in general when using global optimization methods).\n\nThe discretization assumptions are strongly based on the order in which the atoms of the molecule are considered. When considering the atoms of the molecule in their natural ordering, such assumptions are generally not satisfied. An interesting and fundamental pre-processing step for the discretization of DGPs is, therefore, the problem of identifying an order for the atoms that allows for the discretization. This problem can be solved in polynomial time, when all distances are supposed to be exact, as well as when some available distance is represented by a suitable interval.\n\n\nCrippen and Havel are two pioneers of DGP, and they co-authored the book \"Distance Geometry and Molecular Conformation\", 1988. Much more recently, an edited book, collecting the most recent efforts from the scientific community for solving the DGP, was published by Springer. See this web page for the list of contributions.\n\nVarious conferences and workshops are held every year, where the focus is on DGP-related topics. However, the very first workshop completely devoted to DGP and its applications was held in 2013 in Manaus, Brazil: DGA13.\n\n"}
{"id": "22145766", "url": "https://en.wikipedia.org/wiki?curid=22145766", "title": "Early Algebra", "text": "Early Algebra\n\nEarly Algebra is an approach to early mathematics teaching and learning. It is about teaching traditional topics in more profound ways. It is also an area of research in mathematics education.\n\nTraditionally, algebra instruction has been postponed until adolescence. However, data of early algebra researchers shows ways to teach algebraic thinking much earlier. The National Council of Teachers of Mathematics (NCTM) integrates algebra into its Principles and Standards starting from Kindergarten. \n\nOne of the major goals of early algebra is generalizing number and set ideas. It moves from particular numbers to patterns in numbers. This includes generalizing arithmetic operations as functions, as well as engaging children in noticing and beginning to formalize properties of numbers and operations such as the commutative property, identities, and inverses.\n\nStudents historically have had a very difficult time adjusting to algebra for a number of reasons. Researchers have found that by working with students on such ideas as developing rules for the use of letters to stand in for numbers and the true meaning of the equals symbol (it is a balance point, and does not mean \"put the answer next\"), children are much better prepared for formal algebra instruction.\n\nTeacher professional development in this area consists of presenting common student misconceptions and then developing lessons to move students out of faulty ways of thinking and into correct generalizations. The use of true, false, and open number sentences can go a long way toward getting students thinking about the properties of number and operations and the meaning of the equals sign.\n\nResearch areas in early algebra include use of representations, such as symbols, graphs and tables; cognitive development of students; viewing arithmetic as a part of algebraic conceptual fields \n\nTufts/TERC Early Algebra Project\n\n"}
{"id": "6440284", "url": "https://en.wikipedia.org/wiki?curid=6440284", "title": "Earnings response coefficient", "text": "Earnings response coefficient\n\nIn financial economics and accounting, the earnings response coefficient, or ERC, is the estimated relationship between equity returns and the unexpected portion of (i.e., new information in) companies' earnings announcements.\n\nArbitrage pricing theory describes the theoretical relationship between information that is known to market participants about a particular equity (e.g., a common stock share of a particular company) and the price of that equity. Under the efficient market hypothesis, equity prices are expected in the aggregate to reflect all relevant information at a given time. Market participants with superior information are expected to exploit that information until share prices have effectively impounded the information. Therefore, in the aggregate, a portion of changes in a company's share price is expected to result from changes in the relevant information available to the market. The ERC is an estimate of the change in a company's stock price due to the information provided in a company's earnings announcement.\n\nThe ERC is expressed mathematically as follows:\n\nformula_1\n\nEarnings response coefficient research attempts to identify and explain the differential market response to earnings information of different firms. An Earnings response coefficient measures the extent of security’s abnormal market return in response to the unexpected component of reported earnings of the firm issuing that security.\n\nThe relationship between stock returns to profit to determine the extent of the response that occurs to as the Earnings Response Coefficient (ERC). Some studies reveal there are four factors that affect Earnings Response Coefficient (ERC), namely : beta, capital structure, persistence and growth. \n\nReasons for differential market response:\n\nERCs are used primarily in research in accounting and finance. In particular, ERCs have been used in research in positive accounting, a branch of financial accounting research, as they theoretically describe how markets react to different information events. Research in Finance has used ERCs to study, among other things, how different investors react to information events.\n\nThere is some debate concerning the true nature and strength of the ERC relationship. As demonstrated in the above model, the ERC is generally considered to be the slope coefficient of a linear equation between unexpected earnings and equity return. However, certain research results suggest that the relationship is nonlinear.\n\n\n"}
{"id": "2233179", "url": "https://en.wikipedia.org/wiki?curid=2233179", "title": "Edward Routh", "text": "Edward Routh\n\nEdward John Routh FRS (; 20 January 1831 – 7 June 1907), was an English mathematician, noted as the outstanding coach of students preparing for the Mathematical Tripos examination of the University of Cambridge in its heyday in the middle of the nineteenth century. He also did much to systematise the mathematical theory of mechanics and created several ideas critical to the development of modern control systems theory.\n\nRouth was born of an English father and a French-Canadian mother in Quebec, at that time the British colony of Lower Canada. His father's family could trace its history back to the Norman conquest when it acquired land at Routh near Beverley, Yorkshire. His mother's family, the Taschereau family, was well-established in Quebec, tracing their ancestry back to the early days of the French colony. His parents were Sir Randolph Isham Routh (1782–1858) and his second wife, Marie Louise Taschereau (1810–1891). Sir Randolph was Commissary General of the British Army 1826, Chairman of the Irish Famine Relief Commission (1845–48) and Deputy Commissary General at the Battle of Waterloo, and Marie Louise was the daughter of Judge Jean-Thomas Taschereau and the sister of Judge Jean-Thomas and Cardinal Elzéar-Alexandre Taschereau.\n\nRouth came to England aged eleven and attended University College School and then entered University College, London in 1847, having won a scholarship. There he studied under Augustus De Morgan, whose influence led to Routh to decide on a career in mathematics.\n\nRouth obtained his BA (1849) and MA (1853) in London. He attended Peterhouse, Cambridge, where he was taught by Isaac Todhunter and coached by \"senior wrangler maker\" William Hopkins. In 1854, Routh graduated just above James Clerk Maxwell, as Senior Wrangler, sharing the Smith's prize with him. Routh was elected fellow of Peterhouse in 1856.\n\nOn graduation, Routh took up work as a private mathematics tutor in Cambridge and took on the pupils of William John Steele during the latter's fatal illness, though insisting that Steele take the fees. Routh inherited Steele's pupils, going on to establish an unbeaten record as a coach. He coached over 600 pupils between 1855 and 1888, 28 of them making Senior Wrangler, as to Hopkins' 17 with 43 of his pupils winning the Smith Prize.\n\nRouth worked conscientiously and systematically, taking rigidly timetabled classes of ten pupils during the day and spending the evenings preparing extra material for the ablest men. \"His lectures were enlivened by mathematical jokes of a rather heavy kind.\"\n\nRouth was a staunch defender of the Cambridge competitive system and despaired when the university started to publish examination results in alphabetical order, observing \"They will want to run the Derby alphabetically next\".\n\nAstronomer Royal George Biddell Airy sought to entice Routh to work at the Royal Observatory, Greenwich. Though Airy did not succeed, at Greenwich Routh met Airy's eldest daughter Hilda (1840–1916) whom he married in 1864. At the time, the university had a celibacy requirement, forcing Routh to vacate his fellowship and move out of Peterhouse. On the reformation of the college statutes, removing the celibacy requirement, Routh was the first person elected to an honorary fellowship by Peterhouse. The couple had five sons and a daughter. Routh was a \"kindly man and a good conversationalist with friends, but with strangers he was shy and reserved.\"\n\n\nRouth collaborated with Henry Brougham on the \"Analytical View of Sir Isaac Newton's Principia\" (1855). He published a textbook, \"Dynamics of a System of Rigid Bodies\" (1860, 6th ed. 1897) in which he did much to define and systematise the modern mathematical approach to mechanics. This influenced Felix Klein and Arnold Sommerfeld. In fact, Klein arranged the German translation. It also did much to influence William Thomson and Peter Guthrie Tait's \"Treatise on Natural Philosophy\" (1867). The Routhian, which can be obtained from the Lagrangian by the Legendre transform, in classical mechanics is named in his honour.\n\nIn addition to his intensive work in teaching and writing, which had a persistent effect on the presentation of mathematical physics, he also contributed original research such as the Routh-Hurwitz theorem.\n\nCentral tenets of modern control systems theory relied upon the Routh stability criterion (though nowadays due to modern computers it is not as important), an application of Sturm's Theorem to evaluate Cauchy indices through the use of the Euclidean algorithm.\n\n\n\n"}
{"id": "46732608", "url": "https://en.wikipedia.org/wiki?curid=46732608", "title": "Finsler's lemma", "text": "Finsler's lemma\n\nFinsler's lemma is a mathematical result named after Paul Finsler. It states equivalent ways to express the positive definiteness of a quadratic form \"Q\" constrained by a linear form \"L\". \nSince it is equivalent to another lemmas used in optimization and control theory, such as Yakubovich's S-lemma, Finsler's lemma has been given many proofs and has been widely used, particularly in results related to robust optimization and linear matrix inequalities. \n\nLet , and . The following statements are equivalent:\n\n\nIn the particular case that \"L\" is positive semi-definite, it is possible to decompose it as . The following statements, which are also referred as Finsler's lemma in the literature, are equivalent:\n\n\nThe following statement, known as Projection Lemma (or also as Elimination Lemma), is common on the literature of linear matrix inequalities:\nThis can be seen as a generalization of one of Finsler's lemma variants with the inclusion of an extra matrix and an extra constraint.\n\nFinsler's lemma also generalizes for matrices \"Q\" and \"B\" depending on a parameter \"s\" within a set \"S\". In this case, it is natural to ask if the same variable μ (respectively \"X\") can satisfy formula_9 for all formula_10 (respectively, formula_11). If \"Q\" and \"B\" depends continuously on the parameter \"s\", and \"S\" is compact, then this is true. If \"S\" is not compact, but \"Q\" and \"B\" are still continuous matrix-valued functions, then μ and \"X\" can be guaranteed to be at least continuous functions.\n\nFinsler's lemma can be used to give novel linear matrix inequality (LMI) characterizations to stability and control problems. The set of LMIs stemmed from this procedure yields less conservative results when applied to control problems where the system matrices has dependence on a parameter, such robust control problems and control of linear-parameter varying systems. This approach has recently been called as S-variable approach and the LMIs stemming from this approach are known as SV-LMIs (also known as dilated LMIs).\n\nA non-linear system has the universal stabilizability property if every forward-complete solution of a system can be globally stabilized. By the use of Finsler's lemma, it is possible to derive a sufficient condition for universal stabilizability in terms of a differential linear matrix inequality.\n\n"}
{"id": "11449648", "url": "https://en.wikipedia.org/wiki?curid=11449648", "title": "Gilbert Baumslag", "text": "Gilbert Baumslag\n\nGilbert Baumslag (April 30, 1933 – October 20, 2014) was a Distinguished Professor at the City College of New York, with joint appointments in mathematics, computer science, and electrical engineering. He was director of the Center for Algorithms and Interactive Scientific Software, which grew out of the MAGNUS computational group theory project he also headed. Baumslag was also the organizer of the New York Group Theory Seminar, perhaps the foremost research seminar in group theory in the world.\n\nBaumslag graduated from the University of the Witwatersrand in South Africa with a B.Sc. Honours (Masters) and D.Sc. He earned his Ph.D. from the University of Manchester in 1958; his thesis, written under the direction of Bernhard Neumann, was titled \"Some aspects of groups with unique roots\". His contributions include the Baumslag-Solitar groups and parafree groups.\n\nBaumslag was a visiting scholar at the Institute for Advanced Study in 1968-69. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "37174589", "url": "https://en.wikipedia.org/wiki?curid=37174589", "title": "History of the Theory of Numbers", "text": "History of the Theory of Numbers\n\nHistory of the Theory of Numbers is a three-volume work by L. E. Dickson summarizing work in number theory up to about 1920. The style is unusual in that Dickson mostly just lists results by various authors, with little further discussion. The central topic of quadratic reciprocity and higher reciprocity laws is barely mentioned; this was apparently going to be the topic of a fourth volume that was never written .\n\n\n\n"}
{"id": "33974223", "url": "https://en.wikipedia.org/wiki?curid=33974223", "title": "Homotopy type theory", "text": "Homotopy type theory\n\nIn mathematical logic and computer science, homotopy type theory (HoTT ) is the reification turning the judgement that two terms of a given type are equal in intensional type theory into an actual identity type. More broadly, homotopy type theory refers to various lines of development of intensional type theory, based on the interpretation of types as objects to which the intuition of (abstract) homotopy theory applies.\nThis includes, among other lines of work, the construction of homotopical and higher-categorical models for such type theories; the use of type theory as a logic (or internal language) for abstract homotopy theory and higher category theory; the development of mathematics within a type-theoretic foundation (including both previously existing mathematics and new mathematics that homotopical types make possible); and the formalization of each of these in computer proof assistants.\n\nThere is a large overlap between the work referred to as homotopy type theory, and as the univalent foundations project. Although neither is precisely delineated, and the terms are sometimes used interchangeably, the choice of usage also sometimes corresponds to differences in viewpoint and emphasis. As such, this article may not represent the views of all researchers in the fields equally. This kind of variability is unavoidable when a field is in rapid flux.\n\nAt one time the idea that types in intensional type theory with their identity types could be regarded as groupoids was mathematical folklore. It was first made precise semantically in the 1998 paper of Martin Hofmann and Thomas Streicher called \"The groupoid interpretation of type theory\", in which they showed that intensional type theory had a model in the category of groupoids. This was the first truly \"homotopical\" model of type theory, albeit only \"1-dimensional\" (the traditional models in the category of sets being homotopically 0-dimensional).\n\nTheir paper also foreshadowed several later developments in homotopy type theory. For instance, they noted that the groupoid model satisfies a rule they called \"universe extensionality\", which is none other than the restriction to 1-types of the \"univalence axiom\" that Vladimir Voevodsky proposed ten years later. (The axiom for 1-types is notably simpler to formulate, however, since a coherence notion of \"equivalence\" is not required.) They also defined \"categories with isomorphism as equality\" and conjectured that in a model using higher-dimensional groupoids, for such categories one would have \"equivalence is equality\"; this was later proven by Benedikt Ahrens, Krzysztof Kapulkin, and Michael Shulman.\n\nThe first higher-dimensional models of intensional type theory were constructed by Steve Awodey and his student Michael Warren in 2005 using Quillen model categories. These results were first presented in public at the conference FMCS 2006 at which Warren gave a talk titled \"Homotopy models of intensional type theory\", which also served as his thesis prospectus (the dissertation committee present were Awodey, Nicola Gambino and Alex Simpson). A summary is contained in Warren's thesis prospectus abstract.\n\nAt a subsequent workshop about identity types at Uppsala University in 2006 there were two talks about the relation between intensional type theory and factorization systems: one by Richard Garner, \"Factorisation systems for type theory\", and one by Michael Warren, \"Model categories and intensional identity types\". Related ideas were discussed in the talks by Steve Awodey, \"Type theory of higher-dimensional categories\", and Thomas Streicher, \"Identity types vs. weak omega-groupoids: some ideas, some problems\". At the same conference Benno van den Berg gave a talk titled \"Types as weak omega-categories\" where he outlined the ideas that later became the subject of a joint paper with Richard Garner.\n\nAll early constructions of higher dimensional models had to deal with the problem of coherence typical of models of dependent type theory, and various solutions were developed. One such was given in 2009 by Voevodsky, another in 2010 by van den Berg and Garner. A general solution, building on Voevodsky's construction, was eventually given by Lumsdaine and Warren in 2014.\n\nAt the PSSL86 in 2007 Awodey gave a talk titled \"Homotopy type theory\" (this was the first public usage of that term, which was coined by Awodey). Awodey and Warren summarized their results in the paper \"Homotopy theoretic models of identity types\", which was posted on the ArXiv preprint server in 2007 and published in 2009; a more detailed version appeared in Warren's thesis \"Homotopy theoretic aspects of constructive type theory\" in 2008.\n\nAt about the same time, Vladimir Voevodsky was independently investigating type theory in the context of the search of a language for practical formalization of mathematics. In September 2006 he posted to the Types mailing list \"A very short note on homotopy lambda calculus\", which sketched the outlines of a type theory with dependent products, sums and universes and of a model of this type theory in Kan simplicial sets. It began by saying \"The homotopy λ-calculus is a hypothetical (at the moment) type system\" and ended with \"At the moment much of what I said above is at the level of conjectures. Even the definition of the model of TS in the homotopy category is non-trivial\" referring to the complex coherence issues that were not resolved until 2009. This note included a syntactic definition of \"equality types\" that were claimed to be interpreted in the model by path-spaces, but did not consider Per Martin-Löf's rules for identity types. It also stratified the universes by homotopy dimension in addition to size, an idea that later was mostly discarded.\n\nOn the syntactic side, Benno van den Berg conjectured in 2006 that the tower of identity types of a type in intensional type theory should have the structure of an ω-category, and indeed a ω-groupoid, in the \"globular, algebraic\" sense of Michael Batanin. This was later proven independently by van den Berg and Garner in the paper \"Types are weak omega-groupoids\" (published 2008), and by Peter Lumsdaine in the paper \"Weak ω-Categories from Intensional Type Theory\" (published 2009) and as part of his 2010 Ph.D. thesis \"Higher Categories from Type Theories\".\n\nThe concept of a univalent fibration was introduced by Voevodsky in early 2006\nHowever, because of the insistence of all presentations of the Martin-Löf type theory on the property that the identity types, in the empty context, may contain only reflexivity, Voevodsky did not recognize until 2009 that these identity types can be used in combination with the univalent universes. In particular, the idea that univalence can be introduced simply by adding an axiom to the existing Martin-Löf type theory appeared only in 2009.\n\nAlso in 2009, Voevodsky worked out more of the details of a model of type theory in Kan complexes, and observed that the existence of a universal Kan fibration could be used to resolve the coherence problems for categorical models of type theory. He also proved, using an idea of A. K. Bousfield, that this universal fibration was univalent: the associated fibration of pairwise homotopy equivalences between the fibers is equivalent to the paths-space fibration of the base.\n\nTo formulate univalence as an axiom Voevodsky found a way to define \"equivalences\" syntactically that had the important property that the type representing the statement \"f is an equivalence\" was (under the assumption of function extensionality) (-1)-truncated (i.e. contractible if inhabited). This enabled him to give a \"syntactic\" statement of univalence, generalizing Hofmann and Streicher's \"universe extensionality\" to higher dimensions. He was also able to use these definitions of equivalences and contractibility to start developing significant amounts of \"synthetic homotopy theory\" in the proof assistant Coq; this formed the basis of the library later called \"Foundations\" and eventually \"UniMath\".\n\nUnification of the various threads began in February 2010 with an informal meeting at Carnegie Mellon University, where Voevodsky presented his model in Kan complexes and his Coq code to a group including Awodey, Warren, Lumsdaine, and Robert Harper, Dan Licata, Michael Shulman, and others. This meeting produced the outlines of a proof (by Warren, Lumsdaine, Licata, and Shulman) that every homotopy equivalence is an equivalence (in Voevodsky's good coherent sense), based on the idea from category theory of improving equivalences to adjoint equivalences. Soon afterwards, Voevodsky proved that the univalence axiom implies function extensionality.\n\nThe next pivotal event was a mini-workshop at the Mathematical Research Institute of Oberwolfach in March 2011 organized by Steve Awodey, Richard Garner, Per Martin-Löf, and Vladimir Voevodsky, titled \"The homotopy interpretation of constructive type theory\". As part of a Coq tutorial for this workshop, Andrej Bauer wrote a small Coq library. based on Voevodsky's ideas (but not actually using any of his code); this eventually became the kernel of the first version of the \"HoTT\" Coq library (the first commit of the latter by Michael Shulman notes \"Development based on Andrej Bauer's files, with many ideas taken from Vladimir Voevodsky's files\"). One of the most important things to come out of the Oberwolfach meeting was the basic idea of higher inductive types, due to Lumsdaine, Shulman, Bauer, and Warren. The participants also formulated a list of important open questions, such as whether the univalence axiom satisfies canonicity (still open, although some special cases have been resolved positively), whether the univalence axiom has nonstandard models (since answered positively by Shulman), and how to define (semi)simplicial types (still open in MLTT, although it can be done in Voevodsky's Homotopy Type System (HTS), a type theory with two equality types).\n\nSoon after the Oberwolfach workshop, the \"Homotopy Type Theory website and blog\" was established, and the subject began to be popularized under that name. An idea of some of the important progress during this period can be obtained from the blog history.\n\nThe phrase \"univalent foundations\" is agreed by all to be closely related to homotopy type theory, but not everyone uses it in the same way. It was originally used by Vladimir Voevodsky to refer to his vision of a foundational system for mathematics in which the basic objects are homotopy types, based on a type theory satisfying the univalence axiom, and formalized in a computer proof assistant.\n\nAs Voevodsky's work became integrated with the community of other researchers working on homotopy type theory, \"univalent foundations\" was sometimes used interchangeably with \"homotopy type theory\", and other times to refer only to its use as a foundational system (excluding, for example, the study of model-categorical semantics or computational metatheory). For instance, the subject of the IAS special year was officially given as \"univalent foundations\", although a lot of the work done there focused on semantics and metatheory in addition to foundations. The book produced by participants in the IAS program was titled \"Homotopy type theory: Univalent foundations of mathematics\"; although this could refer to either usage, since the book only discusses HoTT as a mathematical foundation.\n\nIn 2012–13 researchers at the Institute for Advanced Study held \"A Special Year on Univalent Foundations of Mathematics\". The special year brought together researchers in topology, computer science, category theory, and mathematical logic. The program was organized by Steve Awodey, Thierry Coquand and Vladimir Voevodsky.\n\nDuring the program Peter Aczel, who was one of the participants, initiated a working group which investigated how to do type theory informally but rigorously, in a style that is analogous to ordinary mathematicians doing set theory. After initial experiments it became clear that this was not only possible but highly beneficial, and that a book (the so-called HoTT Book) could and should be written. Many other participants of the project then joined the effort with technical support, writing, proof reading, and offering ideas. Unusually for a mathematics text, it was developed collaboratively and in the open on GitHub, is released under a Creative Commons license that allows people to fork their own version of the book, and is both purchasable in print and downloadable free of charge.\n\nMore generally, the special year was a catalyst for the development of the entire subject; the HoTT Book was only one, albeit the most visible, result.\n\n\n\"ACM Computing Reviews\" listed the book as a notable 2013 publication in the category \"mathematics of computing\".\n\nHoTT uses a modified version of the \"propositions as types\" interpretation of type theory, according to which types can also represent propositions and terms can then represent proofs. In HoTT, however, unlike in standard \"propositions as types\", a special role is played by 'mere propositions' which, roughly speaking, are those types having at most one term, up to propositional equality. These are more like conventional logical propositions than are general types, in that they are proof-irrelevant.\n\nThe fundamental concept of homotopy type theory is the path. In HoTT, the type formula_1 is the type of all paths from the point formula_2 to the point formula_3. (Therefore, a proof that a point formula_2 equals a point formula_3 is the same thing as a path from the point formula_2 to the point formula_3.) For any point formula_2, there exists a path of type formula_9, corresponding to the reflexive property of equality. A path of type formula_1 can be inverted, forming a path of type formula_11, corresponding to the symmetric property of equality. Two paths of type formula_1 resp. formula_13 can be concatenated, forming a path of type formula_14; this corresponds to the transitive property of equality.\n\nMost importantly, given a path formula_15, and a proof of some property formula_16, the proof can be \"transported\" along the path formula_17 to yield a proof of the property formula_18. (Equivalently stated, an object of type formula_16 can be turned into an object of type formula_18.) This corresponds to the substitution property of equality. Here, an important difference between HoTT and classical mathematics comes in. In classical mathematics, once the equality of two values formula_2 and formula_3 has been established, formula_2 and formula_3 may be used interchangeably thereafter, with no regard to any distinction between them. In homotopy type theory, however, there may be multiple different paths formula_1, and transporting an object along two different paths will yield two different results. Therefore, in homotopy type theory, when applying the substitution property, it is necessary to state which path is being used.\n\nIn general, a \"proposition\" can have multiple different proofs. (For example, the type of all natural numbers, when considered as a proposition, has every natural number as a proof.) Even if a proposition has only one proof formula_2, the space of paths formula_9 may be non-trivial in some way. A \"mere proposition\" is any type which either is empty, or contains only one point with a trivial path space.\n\nNote that people write formula_1 for formula_29, \nthereby leaving the type formula_30 of formula_31 implicit. \nDo not confuse it with formula_32, denoting the identity function on formula_30.\n\nTwo types formula_30 and formula_35 belonging to some universe formula_36 are defined as being \"equivalent\" if there exists an \"equivalence\" between them. An equivalence is a function \nwhich has both a left inverse and a right inverse, in the sense that for suitably chosen formula_38 and formula_39, the following types are both inhabited:\ni.e.\nThis expresses a general notion of \"f has both a left inverse and right inverse\", using equality types. Note that the invertibility conditions above are equality types in the function types formula_44 and formula_45. One generally assumes the function extensionality axiom, which ensures that these are equivalent to the following types that express invertibility using the equality on the domain and codomain formula_30 and formula_35:\ni.e. for all formula_50 and formula_51,\n\nThe functions of type\ntogether with a proof that they are equivalences are denoted by\n\nHaving defined functions that are equivalences as above, one can show that there is a canonical way to turn paths to equivalences.\nIn other words, there is a function of the type\nwhich expresses that types formula_57 that are equal are, in particular, also equivalent.\n\nThe univalence axiom states that this function is itself an equivalence. Therefore, we have\n\n\"In other words, identity is equivalent to equivalence. In particular, one may say that 'equivalent\ntypes are identical'.\"\n\nHoTT allows mathematical proofs to be translated into a computer programming language for computer proof assistants much more easily than before. This approach offers the potential for computers to check difficult proofs.\n\nOne goal of mathematics is to formulate axioms from which virtually all mathematical theorems can be derived and proven unambiguously. Correct proofs in mathematics must follow the rules of logic. They must be derivable without error from axioms and already-proven statements.\n\nHoTT adds the univalence axiom, which relates the equality of logical-mathematical propositions to homotopy theory. An equation such as “a=b” is a mathematical proposition in which two different symbols have the same value. In homotopy type theory, this is taken to mean that the two shapes which represent the values of the symbols are topologically equivalent.\n\nThese topological equivalence relationships, ETH Zürich Institute for Theoretical Studies director Giovanni Felder argues, can be better formulated in homotopy theory because it is more comprehensive: Homotopy theory explains not only why “a equals b” but also how to derive this. In set theory, this information would have to be defined additionally, which makes the translation of mathematical propositions into programming languages more difficult.\n\nAs of 2015, intense research work was underway to model and formally analyse the computational behavior of the univalence axiom in homotopy type theory.\n\n\"Cubical type theory\" is one attempt to give computational content to homotopy type theory.\n\nHowever, it is believed that certain objects, such as semi-simplicial types, cannot be constructed without reference to some notion of exact equality. Therefore, various \"two-level type theories\" have been developed which partition their types into fibrant types, which respect paths, and non-fibrant types, which do not. Cartesian Cubical Computational Type Theory is the first two-level type theory which gives a full computational interpretation to homotopy type theory.\n\n\n\n"}
{"id": "15095049", "url": "https://en.wikipedia.org/wiki?curid=15095049", "title": "Journal of Graph Theory", "text": "Journal of Graph Theory\n\nThe Journal of Graph Theory is a peer-reviewed mathematics journal specializing in graph theory and related areas, such as structural results about graphs, graph algorithms with theoretical emphasis, and discrete optimization on graphs. \n\nThe scope of the journal also includes related areas in combinatorics and the interaction of graph theory with other mathematical sciences. It is published by John Wiley & Sons. The journal was established in 1977 by Frank Harary. The editors-in-chief are Paul Seymour (Princeton University) and Carsten Thomassen (Technical University of Denmark).\n\nThe journal is abstracted and indexed in the Science Citation Index Expanded, Scopus, and \"Zentralblatt MATH\". According to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 0.626.\n"}
{"id": "22305825", "url": "https://en.wikipedia.org/wiki?curid=22305825", "title": "Lawvere theory", "text": "Lawvere theory\n\nIn category theory, a Lawvere theory (named after American mathematician William Lawvere) is a category which can be considered a categorical counterpart of the notion of an equational theory.\n\nLet formula_1 be a skeleton of the category FinSet of finite sets and functions. Formally, a Lawvere theory consists of a small category \"L\" with (strictly associative) finite products and a strict identity-on-objects functor formula_2 preserving finite products.\n\nA model of a Lawvere theory in a category \"C\" with finite products is a finite-product preserving functor . A morphism of models where \"M\" and \"N\" are models of \"L\" is a natural transformation of functors.\n\nA map between Lawvere theories (\"L\",\"I\") and (\"L\"′,\"I\"′) is a finite-product preserving functor which commutes with \"I\" and \"I\"′. Such a map is commonly seen as an interpretation of (\"L\",\"I\") in (\"L\"′,\"I\"′).\n\nLawvere theories together with maps between them form the category Law.\n\nVariations include multisorted (or multityped) Lawvere theory, infinitary Lawvere theory, Fermat theory (named Fermat's difference quotient), and finite-product theory.\n\n\n"}
{"id": "18409176", "url": "https://en.wikipedia.org/wiki?curid=18409176", "title": "Legendre–Clebsch condition", "text": "Legendre–Clebsch condition\n\nIn the calculus of variations the Legendre–Clebsch condition is a second-order condition which a solution of the Euler–Lagrange equation must satisfy in order to be a maximum (and not a minimum or another kind of extremal).\n\nFor the problem of maximizing\n\nthe condition is\n\nIn optimal control, the situation is more complicated because of the possibility of a singular solution. The generalized Legendre–Clebsch condition, also known as convexity, is a sufficient condition for local optimality such that when the linear sensitivity of the Hamiltonian to changes in u is zero, i.e.,\n\nThe Hessian of the Hamiltonian is positive definite along the trajectory of the solution:\n\nIn words, the generalized LC condition guarantees that over a singular arc, the Hamiltonian is minimized.\n\n"}
{"id": "4165631", "url": "https://en.wikipedia.org/wiki?curid=4165631", "title": "List of films about mathematicians", "text": "List of films about mathematicians\n\nThis is a list of feature films that include mathematicians, scientists who use math or references to mathematicians.\n\nFilms where mathematics are central to the plot:\n\nBiographical films based on real-life mathematicians:\n\nFilms where one or more mathematicians play the main role, but that are not otherwise about mathematics:\n\nFilms where one or more of the members of the main cast is a mathematician:\n\n"}
{"id": "1978133", "url": "https://en.wikipedia.org/wiki?curid=1978133", "title": "Logic in Islamic philosophy", "text": "Logic in Islamic philosophy\n\nEarly Islamic law placed importance on formulating standards of argument, which gave rise to a \"novel approach to logic\" ( \"manṭiq\" \"speech, eloquence\") in Kalam (Islamic scholasticism)\nHowever, with the rise of the Mu'tazili philosophers, who highly valued Aristotle's \"Organon\", this approach was displaced by the older ideas from Hellenistic philosophy, \nThe works of al-Farabi, Avicenna, al-Ghazali and other Persian Muslim logicians who often criticized and corrected Aristotelian logic and introduced their own forms of logic, also played a central role in the subsequent development of European logic during the Renaissance.\nThe use of Aristotelian logic in Islamic theology again began to decline from the 10th century, with the rise of Ashʿari theology to the intellectual mainstream, which rejects causal reasoning in favour of clerical authority.\n\nAccording to the Routledge Encyclopedia of Philosophy:\nImportant developments made by Muslim logicians included the development of \"Avicennian logic\" as a replacement of Aristotelian logic. Avicenna's system of logic was responsible for the introduction of hypothetical syllogism, temporal modal logic and inductive logic. Other important developments in early Islamic philosophy include the development of a strict science of citation, the isnad or \"backing\", and the development of a scientific method of open inquiry to disprove claims, the ijtihad, which could be generally applied to many types of questions.\n\nThe works of Hellenistic-influenced Islamic philosophers were crucial in the reception of Aristotelianism in medieval Europe, \n\nEarly forms of analogical reasoning, inductive reasoning and categorical syllogism were introduced in Fiqh (Islamic jurisprudence), Sharia (Islamic law) and Kalam (Islamic theology) from the 7th century with the process of \"Qiyas\", before the Arabic translations of Aristotle's works. Later during the Islamic Golden Age, there was a logical debate among Islamic philosophers, logicians and theologians over whether the term \"Qiyas\" refers to analogical reasoning, inductive reasoning or categorical syllogism. Some Islamic scholars argued that \"Qiyas\" refers to inductive reasoning, which Ibn Hazm (994-1064) disagreed with, arguing that \"Qiyas\" does not refer to inductive reasoning, but refers to categorical syllogism in a real sense and analogical reasoning in a metaphorical sense. On the other hand, al-Ghazali (1058–1111) (and in modern times, Abu Muhammad Asem al-Maqdisi) argued that \"Qiyas\" refers to analogical reasoning in a real sense and categorical syllogism in a metaphorical sense. Other Islamic scholars at the time, however, argued that the term \"Qiyas\" refers to both analogical reasoning and categorical syllogism in a real sense.\n\nThe first original Arabic writings on logic were produced by al-Kindi (Alkindus) (805–873), who produced a summary on earlier logic up to his time. The first writings on logic with non-Aristotelian elements was produced by al-Farabi (Alfarabi) (873–950), who discussed the topics of future contingents, the number and relation of the categories, the relation between logic and grammar, and non-Aristotelian forms of inference. He is also credited for categorizing logic into two separate groups, the first being \"idea\" and the second being \"proof\".\n\nAverroes (1126–98) was the last major logician from al-Andalus, who wrote the most elaborate commentaries on Aristotelian logic.\n\nAvicenna (980-1037) developed his own system of logic known as \"Avicennian logic\" as an alternative to Aristotelian logic. By the 12th century, Avicennian logic had replaced Aristotelian logic as the dominant system of logic in the Islamic world.\n\nThe first criticisms of Aristotelian logic were written by Avicenna, who produced independent treatises on logic rather than commentaries. He criticized the logical school of Baghdad for their devotion to Aristotle at the time. He investigated the theory of definition and classification and the quantification of the predicates of categorical propositions, and developed an original theory on \"temporal modal\" syllogism. Its premises included modifiers such as \"at all times\", \"at most times\", and \"at some time\".\n\nWhile Avicenna often relied on deductive reasoning in philosophy, he used a different approach in medicine. Ibn Sina contributed inventively to the development of inductive logic, which he used to pioneer the idea of a syndrome. In his medical writings, Avicenna was the first to describe the methods of agreement, difference and concomitant variation which are critical to inductive logic and the scientific method.\n\nIbn Hazm (994-1064) wrote the \"Scope of Logic\", in which he stressed on the importance of sense perception as a source of knowledge. Al-Ghazali (Algazel) (1058–1111) had an important influence on the use of logic in theology, making use of Avicennian logic in Kalam. Despite the logical sophistication of al-Ghazali, the rise of the Ash'ari school from the 12th century slowly suffocated original work on logic in much of the Islamic world, though logic continued to be studied in some Islamic regions such as Persia and the Levant.\n\nFakhr al-Din al-Razi (b. 1149) criticised Aristotle's \"first figure\" and developed a form of inductive logic, foreshadowing the system of inductive logic developed by John Stuart Mill (1806–1873). Systematic refutations of Greek logic were written by the Illuminationist school, founded by Shahab al-Din Suhrawardi (1155–1191), who developed the idea of \"decisive necessity\", an important innovation in the history of logical philosophical speculation. Another systematic refutation of Greek logic was written by Ibn Taymiyyah (1263–1328), the \"Ar-Radd 'ala al-Mantiqiyyin\" (\"Refutation of Greek Logicians\"), where he argued against the usefulness, though not the validity, of the syllogism and in favour of inductive reasoning.\n\n\n\n"}
{"id": "5223628", "url": "https://en.wikipedia.org/wiki?curid=5223628", "title": "MathChallengers", "text": "MathChallengers\n\nMathChallengers is the former Mathcounts in British Columbia. It is open to all grade 8 and 9 students from British Columbia. The major sponsors are the Association of Professional Engineers and Geoscientists of B.C. (APEGBC), the B.C. Association of Mathematics Teachers (BCAMT), BC Hydro, and IBM Canada.\n\nThe Competition consists of 4 stages. Stages 1 and 2 are individual competitions. Stage 3 is a Team competition. Stage 4 is a one-on-one competition between the top 10 individuals who participated in stages 1 and 2. Math Challengers competitions may consist of the following rounds:\n\nStage 1 consists of one session on a variety of mathematical subjects. Participants will be allowed to work for 40 minutes on 26 questions written on four pages (each correct answer will count as one point). Thus, the maximum number of points available in this stage is: 26.\n\nStage 2 consists of three sessions on a certain mathematical subject. For each of the sessions, participants will be given 12 minutes to work on the 4 questions on that subject. The total number of questions in Stage 1 is 12 and each correct answer will count as two points. Thus, the maximum number of points available in this stage is: 24.\n\nStage 3 is a Team competition and it consists of three sessions on a variety of mathematical subjects. Participants will be allowed to work for 36 minutes on 15 questions written on one page (each correct answer will count as two points). Thus, the maximum number of points available in this stage is 30. Scientific calculators are allowed for this stage of the competition. Graphing calculators and programmable calculators are not allowed at all. Devices with wireless communication capabilities are absolutely not allowed.\n\nStage 4 is a one-to-one buzz-in verbal competition for the top scoring 10 individuals.\n\nThere will be a total of 9 match up rounds.\n\nParticipants should be provided with ample amounts of scratch paper and pencil for calculating answers.\n\nFor each questions the participants will be allowed to work for 45 seconds from the time it appears on the screen.\n\nA participant who wishes to provide an answer must buzz. But, only the first person who buzzes will be called to provide an answer. Any answer without pressing the buzzer will be disqualified. Only ONE answer per participant per question is allowed and he/she must provide the answer in an acceptable format (i.e. simplified to lowest terms) and within 3 seconds after being called upon. The opponent may continue working while an answer is provided and if the answer of the first participant who buzzes is incorrect the opponent may use the remainder of the 45 seconds to buzz and be called to provide an answer. The participant who was called on and provides the correct answer will score one point in the round.\n\nThe maximum possible individual score is 50.\n\nThe maximum possible team score is 80. The team score is calculated in the following way: The average of the top 4 entries per team + the score of the team from the Co-op stage of the competition.\n\nThe final Team marks of each of the teams will be calculated by taking into account the marks of the best four individual participants out of the five marks of the individuals of that team.\n\nIf a team consists of less than 4 individuals, scores of 0 will be used for any individual missing and will be included in the calculation of the team's average (for example if a team has only 3 participants who scored 50, 40, and 30, and if they scored 18 in the Co-op stage, then their final team score will be (50+40+30+0) /4+18=48.\n\nThe top 3 schools in each of the competitions (only if more than 3 schools participate) will be awarded trophies. The top 3 individuals in each of the competitions (Based on their score before the Face-Off stage) will be awarded trophies. The top 10 individuals (but no more than 5 individuals of the same grade from any one school), or the top 25% of participants, (whichever is less), in each of the competitions will advance to the Face-Off stage and will be awarded medals. All individual final standings are based on the performance in the Blitz and Bull's Eye stages. All schools that were awarded trophies in any of the Regional pools will be invited to participate in the Provincial finals.\n\nAdditional awards or prizes will be awarded to the best performers of the Face-Off stage (Provincial only). The top 3 schools participating in any of the Regional pools may be invited to participate in the Provincial finals subject to the rules. At the Provincial finals, the top schools and top individuals will be awarded trophies and/or medals. Some of the top individuals in the grade 8 (and grade 9) competition may be invited to participate in a post season competition or event.\n\n2006:\n\n2007:\n\n2006:\n\n2007:\n\n\n"}
{"id": "432897", "url": "https://en.wikipedia.org/wiki?curid=432897", "title": "Mechanical calculator", "text": "Mechanical calculator\n\nA mechanical calculator, or calculating machine, is a mechanical device used to perform automatically the basic operations of arithmetic. Most mechanical calculators were comparable in size to small desktop computers and have been rendered obsolete by the advent of the electronic calculator.\n\nSurviving notes from Wilhelm Schickard in 1623 reveal that he designed and had built the earliest of the modern attempts at mechanizing calculation. His machine was composed of two sets of technologies: first an abacus made of Napier's bones, to simplify multiplications and divisions first described six years earlier in 1617, and for the mechanical part, it had a dialed pedometer to perform additions and subtractions. A study of the surviving notes shows a machine that would have jammed after a few entries on the same dial, and that it could be damaged if a carry had to be propagated over a few digits (like adding 1 to 999). Schickard abandoned his project in 1624 and never mentioned it again until his death 11 years later in 1635.\n\nTwo decades after Schickard's supposedly failed attempt, in 1642, Blaise Pascal decisively solved these particular problems with his invention of the mechanical calculator. Co-opted into his father's labour as tax collector in Rouen, Pascal designed the calculator to help in the large amount of tedious arithmetic required; it was called Pascal's Calculator or Pascaline.\n\nThomas' arithmometer, the first commercially successful machine, was manufactured two hundred years later in 1851; it was the first mechanical calculator strong enough and reliable enough to be used daily in an office environment. For forty years the arithmometer was the only type of mechanical calculator available for sale.\n\nThe comptometer, introduced in 1887, was the first machine to use a keyboard which consisted of columns of nine keys (from 1 to 9) for each digit. The Dalton adding machine, manufactured from 1902, was the first to have a 10 key keyboard. Electric motors were used on some mechanical calculators from 1901. In 1961, a comptometer type machine, the Anita mk7 from Sumlock comptometer Ltd., became the first desktop mechanical calculator to receive an all electronic calculator engine, creating the link in between these two industries and marking the beginning of its decline. The production of mechanical calculators came to a stop in the middle of the 1970s closing an industry that had lasted for 120 years.\n\nCharles Babbage designed two new kinds of mechanical calculators, which were so big that they required the power of a steam engine to operate, and that were too sophisticated to be built in his lifetime. The first one was an \"automatic\" mechanical calculator, his difference engine, which could automatically compute and print mathematical tables. In 1855, Georg Scheutz became the first of a handful of designers to succeed at building a smaller and simpler model of his difference engine. The second one was a \"programmable\" mechanical calculator, his analytical engine, which Babbage started to design in 1834; \"in less than two years he had sketched out many of the salient features of the modern computer. A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1937, Howard Aiken convinced IBM to design and build the ASCC/Mark I, the first machine of its kind, based on the architecture of the analytical engine; when the machine was finished some hailed it as \"Babbage's dream come true\".\n\nA short list of other precursors to the mechanical calculator must include a group of mechanical analog computers which, once set, are only modified by the continuous and repeated action of their actuators (crank handle, weight, wheel, water...). Before common era, there are odometers and the Antikythera mechanism, an out of place, unique, geared astronomical clock, followed more than a millennium later by early mechanical clocks, geared astrolabes and followed in the 15th century by pedometers. These machines were all made of toothed gears linked by some sort of carry mechanisms. These machines always produce identical results for identical initial settings unlike a mechanical calculator where all the wheels are independent but are also linked together by the rules of arithmetic.\n\nThe 17th century marked the beginning of the history of mechanical calculators, as it saw the invention of its first machines, including Pascal's calculator, in 1642. Blaise Pascal had invented a machine which he presented as being able to perform computations that were previously thought to be only humanly possible, but he wasn't successful in creating an industry.\n\nThe 17th century also saw the invention of some very powerful tools to aid arithmetic calculations like Napier's bones, logarithmic tables and the slide rule which, for their ease of use by scientists in multiplying and dividing, ruled over and impeded the use and development of mechanical calculators until the production release of the arithmometer in the mid 19th century.\nBlaise Pascal invented a mechanical calculator with a sophisticated carry mechanism in 1642. After three years of effort and 50 prototypes he introduced his calculator to the public. He built twenty of these machines in the following ten years. This machine could add and subtract two numbers directly and multiply and divide by repetition. Since, unlike Schickard's machine, the Pascaline dials could only rotate in one direction zeroing it after each calculation required the operator to dial in all 9s and then (method of) propagate a carry right through the machine. This suggests that the carry mechanism would have proved itself in practice many times over. This is a testament to the quality of the Pascaline because none of the 17th and 18th century criticisms of the machine mentioned a problem with the carry mechanism and yet it was fully tested on all the machines, by their resets, all the time.\n\nIn 1672, Gottfried Leibniz started working on adding direct multiplication to what he understood was the working of Pascal's calculator. However, it is doubtful that he had ever fully seen the mechanism and the method could not have worked because of the lack of reversible rotation in the mechanism. Accordingly, he eventually designed an entirely new machine called the Stepped Reckoner; it used his Leibniz wheels, was the first two-motion calculator, the first to use cursors (creating a memory of the first operand) and the first to have a movable carriage. Leibniz built two Stepped Reckoners, one in 1694 and one in 1706. Only the machine built in 1694 is known to exist, it was rediscovered at the end of the 19th century having been forgotten in an attic in the University of Göttingen.\n\nLeibniz had invented his namesake wheel and the principle of a two motion calculator, but after forty years of development he wasn't able to produce a machine that was fully operational; this makes Pascal's calculator the only working mechanical calculator in the 17th century. Leibniz was also the first person to describe a pinwheel calculator. He once said \"It is unworthy of excellent men to lose hours like slaves in the labour of calculation which could safely be relegated to anyone else if machines were used.\"\n\nSchickard, Pascal and Leibniz were inevitably inspired by the role of clockwork which was highly celebrated in the seventeenth century. However, simple minded application of interlinked gears was insufficient for any of their purposes. Schickard introduced the use of a single toothed \"mutilated gear\" to enable the carry to take place. Pascal improved on that with his famous weighted sautoir. Leibniz went even further in relation to the ability to use a moveable carriage to perform multiplication more efficiently, albeit at the expense of a fully working carry mechanism.\n\nThe principle of the clock (input wheels and display wheels added to a clock like mechanism) for a direct entry calculating machine couldn't be implemented to create a fully effective calculating machine without additional innovation with the technological capabilities of the 17th century. because their gears would jam when a carry had to be moved several places along the accumulator. The only 17th century calculating clocks that have survived to this day do not have a machine wide carry mechanism and therefore cannot be called fully effective mechanical calculators. A much more successful calculating clock was built by the Italian Giovanni Poleni in the 18th century and was a two-motion calculating clock (the numbers are inscribed first and then they are processed).\n\n\nThe 18th century saw the first mechanical calculator that could perform a multiplication automatically; designed and built by Giovanni Poleni in 1709 and made of wood, it was the first successful calculating clock. For all the machines built in this century, division still required the operator to decide when to stop a repeated subtraction at each index, and therefore these machines were only providing a help in dividing, like an abacus. Both pinwheel calculators and Leibniz wheel calculators were built with a few unsuccessful attempts at their commercialization.\n\n\nLuigi Torchi invented the first direct multiplication machine in 1834. This was also the second key-driven machine in the world, following that of James White (1822).\n\nThe mechanical calculator industry started in 1851 Thomas de Colmar released his simplified Arithmomètre which was the first machine that could be used daily in an office environment.\n\nFor 40 years, the arithmometer was the only mechanical calculator available for sale and was sold all over the world. By then, in 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two licensed arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883). Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers in three years.\n\nThe 19th century also saw the designs of Charles Babbage calculating machines, first with his difference engine, started in 1822, which was the first automatic calculator since it continuously used the results of the previous operation for the next one, and second with his analytical engine, which was the first programmable calculator, using Jacquard's cards to read program and data, that he started in 1834, and which gave the blueprint of the mainframe computers built in the middle of the 20th century.\n\n\n\n\nThe cash register, invented by the American saloonkeeper James Ritty in 1879, addressed the old problems of disorganization and dishonesty in business transactions. It was a pure adding machine coupled with a printer, a bell and a two-sided display that showed the paying party and the store owner, if he wanted to, the amount of money exchanged for the current transaction.\n\nThe cash register was easy to use and, unlike genuine mechanical calculators, was needed and quickly adopted by a great number of businesses. \"Eighty four companies sold cash registers between 1888 and 1895, only three survived for any length of time\".\n\nIn 1890, 6 years after John Patterson started NCR Corporation, 20,000 machines had been sold by his company alone against a total of roughly 3,500 for all genuine calculators combined.\n\nBy 1900, NCR had built 200,000 cash registers and there were more companies manufacturing them, compared to the \"Thomas/Payen\" arithmometer company that had just sold around 3,300 and Burroughs had only sold 1,400 machines.\n\n\nTwo different classes of mechanisms had become established by this time, reciprocating and rotary. The former type of mechanism was operated typically by a limited-travel hand crank; some internal detailed operations took place on the pull, and others on the release part of a complete cycle. The illustrated 1914 machine is this type; the crank is vertical, on its right side. Later on, some of these mechanisms were operated by electric motors and reduction gearing that operated a crank and connecting rod to convert rotary motion to reciprocating.\n\nThe latter, type, rotary, had at least one main shaft that made one [or more] continuous revolution[s], one addition or subtraction per turn. Numerous designs, notably European calculators, had handcranks, and locks to ensure that the cranks were returned to exact positions once a turn was complete.\nThe first half of the 20th century saw the gradual development of the mechanical calculator mechanism.\n\nThe Dalton adding-listing machine introduced in 1902 was the first of its type to use only ten keys, and became the first of many different models of \"10-key add-listers\" manufactured by many companies.\n\nIn 1948 the cylindrical Curta calculator, which was compact enough to be held in one hand, was introduced after being developed by Curt Herzstark in 1938. This was an extreme development of the stepped-gear calculating mechanism. It subtracted by adding complements; between the teeth for addition were teeth for subtraction.\n\nFrom the early 1900s through the 1960s, mechanical calculators dominated the desktop computing market. Major suppliers in the USA included Friden, Monroe, and SCM/Marchant. These devices were motor-driven, and had movable carriages where results of calculations were displayed by dials. Nearly all keyboards were \"full\" — each digit that could be entered had its own column of nine keys, 1..9, plus a column-clear key, permitting entry of several digits at once. (See the illustration below of a Marchant Figurematic.) One could call this parallel entry, by way of contrast with ten-key serial entry that was commonplace in mechanical adding machines, and is now universal in electronic calculators. (Nearly all Friden calculators, as well as some rotary (German) Diehls had a ten-key auxiliary keyboard for entering the multiplier when doing multiplication.) Full keyboards generally had ten columns, although some lower-cost machines had eight. Most machines made by the three companies mentioned did not print their results, although other companies, such as Olivetti, did make printing calculators.\n\nIn these machines, addition and subtraction were performed in a single operation, as on a conventional adding machine, but multiplication and division were accomplished by repeated mechanical additions and subtractions. Friden made a calculator that also provided square roots, basically by doing division, but with added mechanism that automatically incremented the number in the keyboard in a systematic fashion. The last of the mechanical calculators were likely to have short-cut multiplication, and some ten-key, serial-entry types had decimal-point keys. However, decimal-point keys required significant internal added complexity, and were offered only in the last designs to be made. Handheld mechanical calculators such as the 1948 Curta continued to be used until they were displaced by electronic calculators in the 1970s.\n\nTypical European four-operation machines use the Odhner mechanism, or variations of it. This kind of machine included the \"Original Odhner\", Brunsviga and several following imitators, starting from Triumphator, Thales, Walther, Facit up to Toshiba. Although most of these were operated by handcranks, there were motor-driven versions. Hamann calculators externally resembled pinwheel machines, but the setting lever positioned a cam that disengaged a drive pawl when the dial had moved far enough.\n\nAlthough Dalton introduced in 1902 first ten-key printing \"adding\" (two operations, the other being subtraction) machine, these feature were not present in \"computing\" (four operations) machines for many decades. Facit-T (1932) was the first 10-key computing machine sold in large numbers. Olivetti Divisumma-14 (1948) was the first computing machine with both printer and a 10-key keyboard.\n\nFull-keyboard machines, including motor-driven ones, were also built until the 1960s. Among the major manufacturers were Mercedes-Euklid, Archimedes, and MADAS in Europe; in the USA, Friden, Marchant, and Monroe were the principal makers of rotary calculators with carriages. Reciprocating calculators (most of which were adding machines, many with integral printers) were made by Remington Rand and Burroughs, among others. All of these were key-set. Felt & Tarrant made Comptometers, as well as Victor, which were key-driven.\n\nThe basic mechanism of the Friden and Monroe was a modified Leibniz wheel (better known, perhaps informally, in the USA as a \"stepped drum\" or \"stepped reckoner\"). The Friden had an elementary reversing drive between the body of the machine and the accumulator dials, so its main shaft always rotated in the same direction. The Swiss MADAS was similar. The Monroe, however, reversed direction of its main shaft to subtract.\n\nThe earliest Marchants were pinwheel machines, but most of them were remarkably-sophisticated rotary types. They ran at 1,300 addition cycles per minute if the [+] bar is held down. Others were limited to 600 cycles per minute, because their accumulator dials started and stopped for every cycle; Marchant dials moved at a steady and proportional speed for continuing cycles. Most Marchants had a row of nine keys on the extreme right, as shown in the photo of the Figurematic. These simply made the machine add for the number of cycles corresponding to the number on the key, and then shifted the carriage one place. Even nine add cycles took only a short time.\n\nIn a Marchant, near the beginning of a cycle, the accumulator dials moved downward \"into the dip\", away from the openings in the cover. They engaged drive gears in the body of the machine, which rotated them at speeds proportional to the digit being fed to them, with added movement (reduced 10:1) from carries created by dials to their right. At the completion of the cycle, the dials would be misaligned like the pointers in a traditional watt-hour meter. However, as they came up out of the dip, a constant-lead disc cam realigned them by way of a (limited-travel) spur-gear differential. As well, carries for lower orders were added in by another, planetary differential. (The machine shown has 39 differentials in its (20-digit) accumulator!)\n\nIn any mechanical calculator, in effect, a gear, sector, or some similar device moves the accumulator by the number of gear teeth that corresponds to the digit being added or subtracted – three teeth changes the position by a count of three. The great majority of basic calculator mechanisms move the accumulator by starting, then moving at a constant speed, and stopping. In particular, stopping is critical, because to obtain fast operation, the accumulator needs to move quickly. Variants of Geneva drives typically block overshoot (which, of course, would create wrong results).\n\nHowever, two different basic mechanisms, the Mercedes-Euklid and the Marchant, move the dials at speeds corresponding to the digit being added or subtracted; a [1] moves the accumulator the slowest, and a [9], the fastest. In the Mercedes-Euklid, a long slotted lever, pivoted at one end, moves nine racks (\"straight gears\") endwise by distances proportional to their distance from the lever's pivot. Each rack has a drive pin that is moved by the slot. The rack for [1] is closest to the pivot, of course.\nFor each keyboard digit, a sliding selector gear, much like that in the Leibniz wheel, engages the rack that corresponds to the digit entered. Of course, the accumulator changes either on the forward or reverse stroke, but not both. This mechanism is notably simple and relatively easy to manufacture.\n\nThe Marchant, however, has, for every one of its ten columns of keys, a nine-ratio \"preselector transmission\" with its output spur gear at the top of the machine's body; that gear engages the accumulator gearing. When one tries to work out the numbers of teeth in such a transmission, a straightforward approach leads one to consider a mechanism like that in mechanical gasoline pump registers, used to indicate the total price. However, this mechanism is seriously bulky, and utterly impractical for a calculator; 90-tooth gears are likely to be found in the gas pump. Practical gears in the computing parts of a calculator cannot have 90 teeth. They would be either too big, or too delicate.\n\nGiven that nine ratios per column implies significant complexity, a Marchant contains a few hundred individual gears in all, many in its accumulator. Basically, the accumulator dial has to rotate 36 degrees (1/10 of a turn) for a [1], and 324 degrees (9/10 of a turn) for a [9], not allowing for incoming carries. At some point in the gearing, one tooth needs to pass for a [1], and nine teeth for a [9]. There is no way to develop the needed movement from a driveshaft that rotates one revolution per cycle with few gears having practical (relatively small) numbers of teeth.\n\nThe Marchant, therefore, has three driveshafts to feed the little transmissions. For one cycle, they rotate 1/2, 1/4, and 1/12 of a revolution. . The 1/2-turn shaft carries (for each column) gears with 12, 14, 16, and 18 teeth, corresponding to digits 6, 7, 8, and 9. The 1/4-turn shaft carries (also, each column) gears with 12, 16, and 20 teeth, for 3, 4, and 5. Digits [1] and [2] are handled by 12 and 24-tooth gears on the 1/12-revolution shaft. Practical design places the 12th-rev. shaft more distant, so the 1/4-turn shaft carries freely-rotating 24 and 12-tooth idler gears. For subtraction, the driveshafts reversed direction.\n\nIn the early part of the cycle, one of five pendants moves off-center to engage the appropriate drive gear for the selected digit.\n\nSome machines had as many as 20 columns in their full keyboards. The monster in this field was the \"Duodecillion\" made by Burroughs for exhibit purposes.\n\nFor sterling currency, £/s/d (and even farthings), there were variations of the basic mechanisms, in particular with different numbers of gear teeth and accumulator dial positions. To accommodate shillings and pence, extra columns were added for the tens digit[s], 10 and 20 for shillings, and 10 for pence. Of course, these functioned as radix-20 and radix-12 mechanisms.\n\nA variant of the Marchant, called the Binary-Octal Marchant, was a radix-8 (octal) machine. It was sold to check very early vacuum-tube (valve) binary computers for accuracy. (Back then, the mechanical calculator was much more reliable than a tube/valve computer.)\n\nAs well, there was a twin Marchant, comprising two pinwheel Marchants with a common drive crank and reversing gearbox. Twin machines were relatively rare, and apparently were used for surveying calculations. At least one triple machine was made.\n\nThe Facit calculator, and one similar to it, are basically pinwheel machines, but the array of pinwheels moves sidewise, instead of the carriage. The pinwheels are biquinary; digits 1 through 4 cause the corresponding number of sliding pins to extend from the surface; digits 5 through 9 also extend a five-tooth sector as well as the same pins for 6 through 9.\n\nThe keys operate cams that operate a swinging lever to first unlock the pin-positioning cam that is part of the pinwheel mechanism; further movement of the lever (by an amount determined by the key's cam) rotates the pin-positioning cam to extend the necessary number of pins.\n\nStylus-operated adders with circular slots for the stylus, and side-by -side wheels, as made by Sterling Plastics (USA), had an ingenious anti-overshoot mechanism to ensure accurate carries.\nMechanical calculators continued to be sold, though in rapidly decreasing numbers, into the early 1970s, with many of the manufacturers closing down or being taken over. Comptometer type calculators were often retained for much longer to be used for adding and listing duties, especially in accounting, since a trained and skilled operator could enter all the digits of a number in one movement of the hands on a Comptometer quicker than was possible serially with a 10-key electronic calculator. In fact, it was quicker to enter larger digits in two strokes using only the lower-numbered keys; for instance, a 9 would be entered as 4 followed by 5. Some key-driven calculators had keys for every column, but only 1 through 5; they were correspondingly compact. The spread of the computer rather than the simple electronic calculator put an end to the Comptometer. Also, by the end of the 1970s, the slide rule had become obsolete.\n\n\n\n"}
{"id": "2301290", "url": "https://en.wikipedia.org/wiki?curid=2301290", "title": "Murderous Maths", "text": "Murderous Maths\n\nMurderous Maths is a series of British educational books by author Kjartan Poskitt. Most of the books in the series are illustrated by illustrator and author Philip Reeve, with the exception of \"The Secret Life of Codes\", which is illustrated by Ian Baker, \"The Essential Arithmetricks: How to plus, minus, times and divide.\" illustrated by Daniel Postgate and Rob Davis, and \"The Murderous Maths of Everything\", also illustrated by Rob Davis.\n\nThe \"Murderous Maths\" books have been published in over 25 countries. The books, which are aimed at children aged 8 and above, teach maths, spanning from basic arithmetic to relatively complex concepts such as the quadratic formula and trigonometry. The books are written in an informal similar style to the \"Horrible Histories\", \"Horrible Science\" and \"Horrible Geography\" series, involving evil geniuses, gangsters, and a generally comedic tone.\n\nThe first two books of the series were originally part of \"The Knowledge\" (now \"Totally\") series (), itself a spin-off of Horrible Histories. However, these books were eventually redesigned and they, as well as the rest of the titles in the series, now use the Murderous Maths banner. According to Poskitt, \"these books have even found their way into schools and proved to be a boost to GCSE studies\". The books are also available in foreign editions, including: German, Spanish, Polish, Czech, Greek, Dutch, Norwegian, Turkish, Croatian, Italian, Lithuanian, Korean, Danish, Portuguese, Hungarian, Finnish, Thai and Portuguese (Latin America) . In 2009, the books were redesigned again, changing the cover art style and the titles of most of the books in the series.\n\nPoskitt's goal, according to the Murderous Maths website, is to write books that are \"something funny to read\", have \"good amusing illustrations\", include \"tricks\", and \"explaining the maths involved as clearly as possible\". He adds that although he doesn't \"work to any government imposed curriculum or any stage achievement levels\", he has \"been delighted to receive many messages of support and thanks from parents and teachers in the UK, the United States and elsewhere\".\n\nThe following are the thirteen books that are available in the series.\nRelated puzzle books have been published also:\nOne title that covers many different areas of mathematics has also been released:\n\nKjartan has also written a book entitled \"Everyday Maths for Grown-Ups\" (2011).\n\nA recommendation of the series by \"Scientific American\" includes a quote from a Stanford engineer named Stacy F. Bennet, who described the series as \"very humorous and engaging introductions to such topics as algebra, geometry and probability\". On 22 November 1997, that same publication said of the series, \"Have a look at \"Murderous Maths\" by Kjartan Poskitt. It is a truly addictive reading book, and was leapt on and devoured by my children. The book is full of awful jokes, fascinating facts, real murders and yes, the maths is good too. This is a brilliant book.\"\n\nThe \"Primary Times\" released a review of \"Professor Fiendish's Book of Diabolical Brain-benders\" on November 25, 2002, describing the title as \"intriguing, fun to do, and not at all dry\", and adding \"I warn you, once you start, you'll be 'hooked'!\". The \"Times Educational Supplement\" also published a review on the book on December 6, 2002, describing the title as being \"action-packed\" and reasoning that \"behind the non-stop fun, serious mathematical principles are being investigated\".\n\nKjartan did a presentation for 350 kids and 10 teachers at Wolfreton School, Hull in June 2004. Reporter Linda Blackbourne described it as a \"stand-up maths routine [that] has children - and teachers - in fits of laughter\". \"Carousel\" issue 16 (the guide to children's books) commented on the event: \"...he possesses a prodigious gift of the (Yorkshire) gab, appears to be incapable of not enjoying himself, and plays his audience with the finesse of a maestro. Maths will never seem the same again\".\n\nThe \"Times Educational Supplement\" described Murderous Maths as \"A stand-up maths routine has children and teachers in fits of laughter... maths has never been so much fun\". The \"Western Gazette\" said: \"It is not often that you see a grown maths teacher cry with laughter...\", while \"The\" \"Worthing\" \"Gazette\" said: \"The kids went wild, they absolutely loved it...\". The \"Stockton Evening Gazette\" said: \"Headteacher Barry Winter said it was a stroke of genius inviting the quick-witted author to open the resource centre\". The GCSE book in the \"Guardian\" said: \"Those who have experienced Poskitt \"live\" will recognise his commitment to getting readers involved with the learning process\" (Nov 6th 2001), and \"The Press (York)\" described it as \"...charismatic...\"\n\nA review by science writer Brian Clegg described his views on \"Murderous Maths: Desperate Measures\": \n\nThere are a number of recurring characters in the MM books. These include:\n\n\n\n"}
{"id": "2387489", "url": "https://en.wikipedia.org/wiki?curid=2387489", "title": "Nancy Lynch", "text": "Nancy Lynch\n\nNancy Ann Lynch (born January 19, 1948) is a mathematician, a theorist, and a professor at the Massachusetts Institute of Technology. She is the NEC Professor of Software Science and Engineering in the EECS department and heads the \"Theory of Distributed Systems\" research group at MIT's Computer Science and Artificial Intelligence Laboratory.\n\nLynch was born in Brooklyn, and her academic training was in mathematics. She attended Brooklyn College and MIT, where she received her Ph.D. in 1972 under the supervision of Albert R. Meyer. \n\nShe served on the math and computer science faculty at several other universities, including Tufts University, the University of Southern California and Georgia Tech, prior to joining the MIT faculty in 1982. Since then, she has been working on applying mathematics to the tasks of understanding and constructing complex distributed systems.\n\nHer 1985 work with Michael J. Fischer and Mike Paterson on consensus problems received the PODC Influential-Paper Award in 2001. Their work showed that in an asynchronous distributed system, consensus is impossible if there is one processor that crashes. On their contribution, Jennifer Welch wrote that “this result has had a monumental impact in distributed computing, both theory and practice. Systems designers were motivated to clarify their claims concerning under what circumstances the systems work.”\n\nShe is the author of numerous research articles about distributed algorithms and impossibility results, and about formal modeling and validation of distributed systems (see, e.g., input/output automaton). She is the author of the graduate textbook \"Distributed Algorithms\". She is a member of the National Academy of Sciences, the National Academy of Engineering, and an ACM Fellow.\n\n\n"}
{"id": "33350956", "url": "https://en.wikipedia.org/wiki?curid=33350956", "title": "Projection body", "text": "Projection body\n\nIn convex geometry, the projection body formula_1 of a convex body formula_2 in \"n\"-dimensional Euclidean space is the convex body such that for any vector formula_3, the support function of formula_1 in the direction \"u\" is the (\"n\" – 1)-dimensional volume of the projection of \"K\" onto the hyperplane orthogonal to \"u\". \n\nMinkowski showed that the projection body of a convex body is convex. and used projection bodies in their solution to Shephard's problem.\n\nFor formula_2 a convex body, let formula_6 denote the polar body of its projection body. There are two remarkable affine isoperimetric inequality for this body. proved that for all convex bodies formula_2,\nwhere formula_9 denotes the \"n\"-dimensional unit ball and formula_10 is \"n\"-dimensional volume, and there is equality precisely for ellipsoids. proved that for all convex bodies formula_2,\nwhere formula_13 denotes any formula_14-dimensional simplex, and there is equality precisely for such simplices.\n\nThe intersection body \"IK\" of \"K\" is defined similarly, as the star body such that for any vector \"u\" the radial function of \"IK\" from the origin in direction \"u\" is the (\"n\" – 1)-dimensional volume of the intersection of \"K\" with the hyperplane \"u\".\nEquivalently, the radial function of the intersection body \"IK\" is the Funk transform of the radial function of \"K\".\nIntersection bodies were introduced by .\n\n\n"}
{"id": "30220726", "url": "https://en.wikipedia.org/wiki?curid=30220726", "title": "Quasisymmetric map", "text": "Quasisymmetric map\n\nIn mathematics, a quasisymmetric homeomorphism between metric spaces is a map that generalizes bi-Lipschitz maps. While bi-Lipschitz maps shrink or expand the diameter of a set by no more than a multiplicative factor, quasisymmetric maps satisfy the weaker geometric property that they preserve the relative sizes of sets: if two sets \"A\" and \"B\" have diameters \"t\" and are no more than distance \"t\" apart, then the ratio of their sizes changes by no more than a multiplicative constant. These maps are also related to quasiconformal maps, since in many circumstances they are in fact equivalent.\n\nLet (\"X\", \"d\") and (\"Y\", \"d\") be two metric spaces. A homeomorphism \"f\":\"X\" → \"Y\" is said to be η-quasisymmetric if there is an increasing function \"η\" : [0, ∞) → [0, ∞) such that for any triple \"x\", \"y\", \"z\" of distinct points in \"X\", we have\n\n\nA map \"f:X→Y\" is said to be H-weakly-quasisymmetric for some \"H\" > 0 if for all triples of distinct points \"x,y,z\" in \"X\", we have\n\nNot all weakly quasisymmetric maps are quasisymmetric. However, if \"X\" is connected and \"X\" and \"Y\" are doubling, then all weakly quasisymmetric maps are quasisymmetric. The appeal of this result is that proving weak-quasisymmetry is much easier than proving quasisymmetry directly, and in many natural settings the two notions are equivalent.\n\nA monotone map \"f\":\"H\" → \"H\" on a Hilbert space \"H\" is δ-monotone if for all \"x\" and \"y\" in \"H\",\n\nTo grasp what this condition means geometrically, suppose \"f\"(0) = 0 and consider the above estimate when \"y\" = 0. Then it implies that the angle between the vector \"x\" and its image \"f\"(\"x\") stays between 0 and arccos \"δ\" < \"π\"/2.\n\nThese maps are quasisymmetric, although they are a much narrower subclass of quasisymmetric maps. For example, while a general quasisymmetric map in the complex plane could map the real line to a set of Hausdorff dimension strictly greater than one, a \"δ\"-monotone will always map the real line to a rotated graph of a Lipschitz function \"L\":ℝ → ℝ.\n\nQuasisymmetric homeomorphisms of the real line to itself can be characterized in terms of their derivatives. An increasing homeomorphism \"f\":ℝ → ℝ is quasisymmetric if and only if there is a constant \"C\" > 0 and a doubling measure \"μ\" on the real line such that\n\nAn analogous result holds in Euclidean space. Suppose \"C\" = 0 and we rewrite the above equation for \"f\" as \n\nWriting it this way, we can attempt to define a map using this same integral, but instead integrate (what is now a vector valued integrand) over ℝ: if \"μ\" is a doubling measure on ℝ and\nis quasisymmetric (in fact, it is \"δ\"-monotone for some \"δ\" depending on the measure \"μ\").\n\nLet \"Ω\" and \"Ω´\" be open subsets of ℝ. If \"f\" : Ω → Ω´ is \"η\"-quasisymmetric, then it is also \"K\"-quasiconformal, where \"K\" > 0 is a constant depending on \"η\".\n\nConversely, if \"f\" : Ω → Ω´ is \"K\"-quasiconformal and \"B\"(\"x\", 2\"r\") is contained in \"Ω\", then \"f\" is \"η\"-quasisymmetric on \"B\"(\"x\", \"r\"), where \"η\" depends only on \"K\".\n\n"}
{"id": "2736939", "url": "https://en.wikipedia.org/wiki?curid=2736939", "title": "Random number generation", "text": "Random number generation\n\nA random number generator (RNG) is a device that generates a sequence of numbers or symbols that cannot be reasonably predicted better than by a random chance. Random number generators can be true hardware random-number generators (HRNG), which generate genuinely random numbers, or pseudo-random number generators (PRNG) which generate numbers which look random, but are actually deterministic, and can be reproduced if the state of the PRNG is known.\n\nVarious applications of randomness have led to the development of several different methods for generating random data, of which some have existed since ancient times, among whose ranks are well-known \"classic\" examples, including the rolling of dice, coin flipping, the shuffling of playing cards, the use of yarrow stalks (for divination) in the I Ching, as well as countless other techniques. Because of the mechanical nature of these techniques, generating large numbers of sufficiently random numbers (important in statistics) required a lot of work and/or time. Thus, results would sometimes be collected and distributed as random number tables.\n\nSeveral computational methods for pseudo-random number generation exist. All fall short of the goal of true randomness, although they may meet, with varying success, some of the statistical tests for randomness intended to measure how unpredictable their results are (that is, to what degree their patterns are discernible). This generally makes them unusable for applications such as cryptography. However, carefully designed cryptographically secure pseudo-random number generators (CSPRNG) also exist, with special features specifically designed for use in cryptography.\n\nRandom number generators have applications in gambling, statistical sampling, computer simulation, cryptography, completely randomized design, and other areas where producing an unpredictable result is desirable. Generally, in applications having unpredictability as the paramount, such as in security applications, hardware generators are generally preferred over pseudo-random algorithms, where feasible.\n\nRandom number generators are very useful in developing Monte Carlo-method simulations, as debugging is facilitated by the ability to run the same sequence of random numbers again by starting from the same \"random seed\". They are also used in cryptography – so long as the \"seed\" is secret. Sender and receiver can generate the same set of numbers automatically to use as keys.\n\nThe generation of pseudo-random numbers is an important and common task in computer programming. While cryptography and certain numerical algorithms require a very high degree of \"apparent\" randomness, many other operations only need a modest amount of unpredictability. Some simple examples might be presenting a user with a \"Random Quote of the Day\", or determining which way a computer-controlled adversary might move in a computer game. Weaker forms of \"randomness\" are used in hash algorithms and in creating amortized searching and sorting algorithms.\n\nSome applications which appear at first sight to be suitable for randomization are in fact not quite so simple. For instance, a system that \"randomly\" selects music tracks for a background music system must only \"appear\" random, and may even have ways to control the selection of music: a true random system would have no restriction on the same item appearing two or three times in succession.\n\nThere are two principal methods used to generate random numbers. The first method measures some physical phenomenon that is expected to be random and then compensates for possible biases in the measurement process. Example sources include measuring atmospheric noise, thermal noise, and other external electromagnetic and quantum phenomena. For example, cosmic background radiation or radioactive decay as measured over short timescales represent sources of natural entropy.\n\nThe speed at which entropy can be harvested from natural sources is dependent on the underlying physical phenomena being measured. Thus, sources of naturally occurring \"true\" entropy are said to be blocking they are rate-limited until enough entropy is harvested to meet the demand. On some Unix-like systems, including most Linux distributions, the pseudo device file will block until sufficient entropy is harvested from the environment. Due to this blocking behavior, large bulk reads from , such as filling a hard disk drive with random bits, can often be slow on systems that use this type of entropy source.\n\nThe second method uses computational algorithms that can produce long sequences of apparently random results, which are in fact completely determined by a shorter initial value, known as a seed value or key. As a result, the entire seemingly random sequence can be reproduced if the seed value is known. This type of random number generator is often called a pseudorandom number generator. This type of generator typically does not rely on sources of naturally occurring entropy, though it may be periodically seeded by natural sources. This generator type is non-blocking, so they are not rate-limited by an external event, making large bulk reads a possibility.\n\nSome systems take a hybrid approach, providing randomness harvested from natural sources when available, and falling back to periodically re-seeded software-based cryptographically secure pseudorandom number generators (CSPRNGs). The fallback occurs when the desired read rate of randomness exceeds the ability of the natural harvesting approach to keep up with the demand. This approach avoids the rate-limited blocking behavior of random number generators based on slower and purely environmental methods.\n\nWhile a pseudorandom number generator based solely on deterministic logic can never be regarded as a \"true\" random number source in the purest sense of the word, in practice they are generally sufficient even for demanding security-critical applications. Indeed, carefully designed and implemented pseudo-random number generators can be certified for security-critical cryptographic purposes, as is the case with the yarrow algorithm and fortuna. The former is the basis of the source of entropy on FreeBSD, AIX, OS X, NetBSD, and others. OpenBSD also uses a pseudo-random number algorithm based on ChaCha20 known as arc4random.\n\nThe earliest methods for generating random numbers, such as dice, coin flipping and roulette wheels, are still used today, mainly in games and gambling as they tend to be too slow for most applications in statistics and cryptography.\n\nA physical random number generator can be based on an essentially random atomic or subatomic physical phenomenon whose unpredictability can be traced to the laws of quantum mechanics. Sources of entropy include radioactive decay, thermal noise, shot noise, avalanche noise in Zener diodes, clock drift, the timing of actual movements of a hard disk read/write head, and radio noise. However, physical phenomena and tools used to measure them generally feature asymmetries and systematic biases that make their outcomes not uniformly random. A randomness extractor, such as a cryptographic hash function, can be used to approach a uniform distribution of bits from a non-uniformly random source, though at a lower bit rate.\n\nThe appearance of wideband photonic entropy sources such as chaotic laser and amplified spontaneous emission noise greatly promote the rapid development of the physical random number generator. Among them, laser chaos gets great concerns to produce high-speed physical random numbers due to its merits of the high bandwidth and large amplitude. Wang report a prototype of high-speed real-time physical random bit generator based on a chaotic laser.\n\nVarious imaginative ways of collecting this entropic information have been devised. One technique is to run a hash function against a frame of a video stream from an unpredictable source. Lavarand used this technique with images of a number of lava lamps. HotBits measures radioactive decay with Geiger–Muller tubes, while Random.org uses variations in the amplitude of atmospheric noise recorded with a normal radio.\n\nAnother common entropy source is the behavior of human users of the system. While people are not considered good randomness generators upon request, they generate random behavior quite well in the context of playing mixed strategy games. Some security-related computer software requires the user to make a lengthy series of mouse movements or keyboard inputs to create sufficient entropy needed to generate random keys or to initialize pseudorandom number generators.\n\nMost computer generated random numbers use pseudorandom number generators (PRNGs) which are algorithms that can automatically create long runs of numbers with good random properties but eventually the sequence repeats (or the memory usage grows without bound). These random numbers are fine in many situations but are not as random as numbers generated from electromagnetic atmospheric noise used as a source of entropy. The series of values generated by such algorithms is generally determined by a fixed number called a seed. One of the most common PRNG is the linear congruential generator, which uses the recurrence\n\nto generate numbers, where , and are large integers, and formula_2 is the next in as a series of pseudo-random numbers. The maximum number of numbers the formula can produce is one less than the modulus, -1. The recurrence relation can be extended to matrices to have much longer periods and better statistical properties\nTo avoid certain non-random properties of a single linear congruential generator, several such random number generators with slightly different values of the multiplier coefficient, , can be used in parallel, with a \"master\" random number generator that selects from among the several different generators.\n\nA simple pen-and-paper method for generating random numbers is the so-called middle square method suggested by John von Neumann. While simple to implement, its output is of poor quality. It has a very short period and severe weaknesses, such as the output sequence almost always converging to zero. A recent innovation is to combine the middle square with a Weyl sequence. This method produces high quality output through a long period. See Middle Square Weyl Sequence PRNG.\n\nMost computer programming languages include functions or library routines that provide random number generators. They are often designed to provide a random byte or word, or a floating point number uniformly distributed between 0 and 1.\n\nThe quality i.e. randomness of such library functions varies widely from completely predictable output, to cryptographically secure. The default random number generator in many languages, including Python, Ruby, R, IDL and PHP is based on the Mersenne Twister algorithm and is \"not\" sufficient for cryptography purposes, as is explicitly stated in the language documentation. Such library functions often have poor statistical properties and some will repeat patterns after only tens of thousands of trials. They are often initialized using a computer's real time clock as the seed, since such a clock generally measures in milliseconds, far beyond the person's precision. These functions may provide enough randomness for certain tasks (for example video games) but are unsuitable where high-quality randomness is required, such as in cryptography applications, statistics or numerical analysis.\n\nMuch higher quality random number sources are available on most operating systems; for example /dev/random on various BSD flavors, Linux, Mac OS X, IRIX, and Solaris, or CryptGenRandom for Microsoft Windows. Most programming languages, including those mentioned above, provide a means to access these higher quality sources.\n\nThere are a couple of methods to generate a random number based on a probability density function. These methods involve transforming a uniform random number in some way. Because of this, these methods work equally well in generating both pseudo-random and true random numbers. One method, called the inversion method, involves integrating up to an area greater than or equal to the random number (which should be generated between 0 and 1 for proper distributions). A second method, called the acceptance-rejection method, involves choosing an x and y value and testing whether the function of x is greater than the y value. If it is, the x value is accepted. Otherwise, the x value is rejected and the algorithm tries again.\n\nRandom number generation may also be performed by humans, in the form of collecting various inputs from end users and using them as a randomization source. However, most studies find that human subjects have some degree of non-randomness when attempting to produce a random sequence of e.g. digits or letters. They may alternate too much between choices when compared to a good random generator; thus, this approach is not widely used.\n\nEven given a source of plausible random numbers (perhaps from a quantum mechanically based hardware generator), obtaining numbers which are completely unbiased takes care. In addition, behavior of these generators often changes with temperature, power supply voltage, the age of the device, or other outside interference. And a software bug in a pseudo-random number routine, or a hardware bug in the hardware it runs on, may be similarly difficult to detect.\n\nGenerated random numbers are sometimes subjected to statistical tests before use to ensure that the underlying source is still working, and then post-processed to improve their statistical properties. An example would be the TRNG9803 hardware random number generator, which uses an entropy measurement as a hardware test, and then post-processes the random sequence with a shift register stream cipher. It is generally hard to use statistical tests to validate the generated random numbers. Wang and Nicol proposed a distance-based statistical testing technique that is used to identify the weaknesses of several random generators. Li and Wang proposed a method of testing random numbers based on laser chaotic entropy sources using Brownian motion properties.\n\nRandom numbers uniformly distributed between 0 and 1 can be used to generate random numbers of any desired distribution by passing them through the inverse cumulative distribution function (CDF) of the desired distribution (see Inverse transform sampling). Inverse CDFs are also called quantile functions. To generate a pair of statistically independent standard normally distributed random numbers (\"x\", \"y\"), one may first generate the polar coordinates (\"r\", \"θ\"), where \"r\"~χ and \"θ\"~UNIFORM(0,2π) (see Box–Muller transform).\n\nSome 0 to 1 RNGs include 0 but exclude 1, while others include or exclude both.\n\nThe outputs of multiple independent RNGs can be combined (for example, using a bit-wise XOR operation) to provide a combined RNG at least as good as the best RNG used. This is referred to as software whitening.\n\nComputational and hardware random number generators are sometimes combined to reflect the benefits of both kinds. Computational random number generators can typically generate pseudo-random numbers much faster than physical generators, while physical generators can generate \"true randomness.\"\n\nSome computations making use of a random number generator can be summarized as the computation of a total or average value, such as the computation of integrals by the Monte Carlo method. For such problems, it may be possible to find a more accurate solution by the use of so-called low-discrepancy sequences, also called quasirandom numbers. Such sequences have a definite pattern that fills in gaps evenly, qualitatively speaking; a truly random sequence may, and usually does, leave larger gaps.\n\nThe following sites make available Random Number samples:\n\nSince much cryptography depends on a cryptographically secure random number generator for key and cryptographic nonce generation, if a random number generator can be made predictable, it can be used as backdoor by an attacker to break the encryption.\n\nThe NSA is reported to have inserted a backdoor into the NIST certified cryptographically secure pseudorandom number generator Dual_EC_DRBG. If for example an SSL connection is created using this random number generator, then according to Matthew Green it would allow NSA to determine the state of the random number generator, and thereby eventually be able to read all data sent over the SSL connection. Even though it was apparent that Dual_EC_DRBG was a very poor and possibly backdoored pseudorandom number generator long before the NSA backdoor was confirmed in 2013, it had seen significant usage in practice until 2013, for example by the prominent security company RSA Security. There have subsequently been accusations that RSA Security knowingly inserted a NSA backdoor into its products, possibly as part of the Bullrun program. RSA has denied knowingly inserting a backdoor into its products.\n\nIt has also been theorized that hardware RNGs could be secretly modified to have less entropy than stated, which would make encryption using the hardware RNG susceptible to attack. One such method which has been published works by modifying the dopant mask of the chip, which would be undetectable to optical reverse-engineering. For example, for random number generation in Linux, it is seen as unacceptable to use Intel's RdRand hardware RNG without mixing in the RdRand output with other sources of entropy to counteract any backdoors in the hardware RNG, especially after the revelation of the NSA Bullrun program.\n\nIn 2010, a U.S. lottery draw was rigged by the information security director of the Multi-State Lottery Association (MUSL), who surreptitiously installed backdoor malware on the MUSL's secure RNG computer during routine maintenance. During the hacks the man won a total amount of $16,500,000 by predicting the numbers correct a few times in year.\n\nASLR or Address Space Layout Randomization, a mitigation against rowhammer and related attacks on the physical hardware of memory chips has been found to be inadequate as of early 2017 by VUSec. The random number algorithm if based on a shift register implemented in hardware is predictable at sufficiently large values of p and can be reverse engineered with enough processing power.\nThis also indirectly means that malware using this method can run on both GPUs and CPUs if coded to do so, even using GPU to break ASLR on the CPU itself.\n\n\n"}
{"id": "30850331", "url": "https://en.wikipedia.org/wiki?curid=30850331", "title": "Robert Romer", "text": "Robert Romer\n\nSir Robert Romer, GCB, FRS (23 December 1840 – 19 March 1918) was a British judge. He was a High Court judge 1890-1899, and a Lord Justice of Appeal 1899-1906 when he was known as Lord Justice Romer.\n\nRomer was born in Kilburn, Middlesex. He attended St John's School, Leatherhead (in Surrey) and excelled at mathematics at Trinity Hall, Cambridge where he was Senior Wrangler in 1863 (the first from Trinity Hall) and was joint winner of Smith's Prize in that year. Following the premature late 1864 death of George Boole, the first professor of maths at Queen's College, Cork, Romer beat out Robert Ball for the vacant position, however he only stayed in Cork for 1865-1866. He became QC in 1881, and a bencher of Lincoln's Inn in 1884. The same year he unsuccessfully stood as the Liberal candidate in Brighton.\n\nIn 1890 he was appointed High Court judge and assigned to the Chancery Division, receiving the customary knighthood. He served as such until 1899, when he was appointed a Lord Justice of Appeal, in succession to Chitty. He resigned in 1906.\n\nRomer was sworn a Privy Councillor in 1899, and elected a Fellow of the Royal Society in the same year. He was appointed Knight Grand Cross of the Order of the Bath (GCB) in the New Year Honours list 1 January 1901, and was invested by King Edward VII in February the same year.\n\nHe was a member of the Royal Commission on South African Hospitals in 1901, during the Boer War. He was a member of the Royal Commission on University Education in London in 1909.\n\nIn 1864 he married his first cousin Betty Lemon, daughter of his aunt Helen (Nelly) Romer and Mark Lemon, editor of \"Punch\". Their son was Mark Romer, Baron Romer and their grandson was Sir Charles Robert Romer; both were also Lords Justice of Appeal. Their daughter Helen Mary married future Lord Chancellor Frederic Maugham.\n\n"}
{"id": "57351611", "url": "https://en.wikipedia.org/wiki?curid=57351611", "title": "Ruth F. Curtain", "text": "Ruth F. Curtain\n\nRuth F. Curtain (born 1941) is an Australian mathematician who worked for many years in the Netherlands as a professor of mathematics at the University of Groningen. Her research concerns infinite-dimensional linear systems.\n\nCurtain was born in Melbourne. She was the daughter of a house painter, who wanted her to leave school at age 14, but with the support of her mother she persisted. She studied mathematics at the University of Melbourne, earning a bachelor's degree in 1962, diploma in education in 1963, and master's degree in 1965. She moved to Brown University, in the United States, for graduate study in applied mathematics, and completed her Ph.D. there in 1969. Her dissertation, \"Stochastic Differential Equations In A Hilbert Space\", was supervised by Peter Falb.\n\nShe then joined the faculty at Purdue University, but in 1971 moved to the University of Warwick. In 1977 she moved again, to the University of Groningen, where she remained until her 2006 retirement.\n\nCurtain is the author of:\n\nIn 1991 Curtain was elected as a Fellow of the IEEE, associated with the IEEE Control Systems Society, \"for contributions to the control theory of stochastic and infinite-dimensional systems\".\n\nIn 2012 the Society for Industrial and Applied Mathematics gave Curtain their W. T. and Idalia Reid Prize for outstanding research in differential equations and control theory. The award citation recognized Curtain for her \"fundamental contributions to the theory of infinite dimensional systems and the control of systems governed by partial and delay differential equation\".\n"}
{"id": "13037119", "url": "https://en.wikipedia.org/wiki?curid=13037119", "title": "Scattering-matrix method", "text": "Scattering-matrix method\n\nIn computational electromagnetics, the scattering-matrix method (SMM) is a numerical method used to solve Maxwell's equations.\n\nSMM can, for example, use cylinders to model dielectric/metal objects in the domain.\nThe total-field/scattered-field (TF/SF) formalism where the total field is written as sum of incident and scattered at each point in the domain:\n\nBy assuming series solutions for the total field, the SMM method transforms the domain into a cylindrical problem. In this domain total field is written in terms of Bessel and Hankel function solutions to the cylindrical Helmholtz equation. SMM method formulation, finally helps compute these coefficients of the cylindrical harmonic functions within the cylinder and outside it, at the same time satisfying EM boundary conditions.\n\nFinally, SMM accuracy can be increased by adding (removing) cylindrical harmonic terms used to model the scattered fields.\n\nSMM, eventually leads to a matrix formalism, and the coefficients are calculated through matrix inversion. For N-cylinders, each scattered field modeled using 2M+1 harmonic terms, SMM requires to solve a N(2M + 1) system of equations. \n\nSMM, is a rigorous and accurate method deriving from first principles. Hence, it is guaranteed to be accurate within limits of model, and not show suprious effects of numerical dispersion arising in other techniques like FDTD.\n\n"}
{"id": "1874173", "url": "https://en.wikipedia.org/wiki?curid=1874173", "title": "Singleton bound", "text": "Singleton bound\n\nIn coding theory, the Singleton bound, named after Richard Collom Singleton, is a relatively crude upper bound on the size of an arbitrary block code formula_1 with block length formula_2, size formula_3 and minimum distance formula_4.\n\nThe minimum distance of a set formula_1 of codewords of length formula_2 is defined as \nwhere formula_8 is the Hamming distance between formula_9 and formula_10. The expression formula_11 represents the maximum number of possible codewords in a formula_12-ary block code of length formula_2 and minimum distance formula_4.\n\nThen the Singleton bound states that\n\nFirst observe that the number of formula_12-ary words of length formula_2 is formula_18, since each letter in such a word may take one of formula_12 different values, independently of the remaining letters.\n\nNow let formula_1 be an arbitrary formula_12-ary block code of minimum distance formula_4. Clearly, all codewords formula_23 are distinct. If we puncture the code by deleting the first formula_24 letters of each codeword, then all resulting codewords must still be pairwise different, since all of the original codewords in formula_1 have Hamming distance at least formula_4 from each other. Thus the size of the altered code is the same as the original code.\n\nThe newly obtained codewords each have length\n\nand thus, there can be at most formula_28 of them. Since formula_1 was arbitrary, this bound must hold for the largest possible code with these parameters, thus:\n\nIf formula_1 is a linear code with block length formula_2, dimension formula_33 and minimum distance formula_4 over the finite field with formula_12 elements, then the maximum number of codewords is formula_36 and the Singleton bound implies:\nso that\nwhich is usually written as\n\nIn the linear code case a different proof of the Singleton bound can be obtained by observing that rank of the parity check matrix is formula_40. Another simple proof follows from observing that the rows of any generator matrix in standard form have weight at most formula_41.\n\nThe usual citation given for this result is , but according to the result can be found in a 1953 paper of Komamiya.\n\nLinear block codes that achieve equality in the Singleton bound are called MDS (maximum distance separable) codes. Examples of such codes include codes that have only two codewords (the all-zero word and the all-one word, having thus minimum distance formula_2), codes that use the whole of formula_43 (minimum distance 1), codes with a single parity symbol (minimum distance 2) and their dual codes. These are often called \"trivial\" MDS codes.\n\nIn the case of binary alphabets, only trivial MDS codes exist.\n\nExamples of non-trivial MDS codes include Reed-Solomon codes and their extended versions.\n\nMDS codes are an important class of block codes since, for a fixed formula_2 and formula_33, they have the greatest error correcting and detecting capabilities. There are several ways to characterize MDS codes:\n\nThe last of these characterizations permits, by using the MacWilliams identities, an explicit formula for the complete weight distribution of an MDS code.\n\nThe linear independence of the columns of a generator matrix of an MDS code permits a construction of MDS codes from objects in finite projective geometry. Let formula_66 be the finite projective space of (geometric) dimension formula_67 over the finite field formula_48. Let formula_69 be a set of points in this projective space represented with homogeneous coordinates. Form the formula_70 matrix formula_71 whose columns are the homogeneous coordinates of these points. Then,\n\n\n\n"}
{"id": "3869419", "url": "https://en.wikipedia.org/wiki?curid=3869419", "title": "Smooth infinitesimal analysis", "text": "Smooth infinitesimal analysis\n\nSmooth infinitesimal analysis is a modern reformulation of the calculus in terms of infinitesimals. Based on the ideas of F. W. Lawvere and employing the methods of category theory, it views all functions as being continuous and incapable of being expressed in terms of discrete entities. As a theory, it is a subset of synthetic differential geometry.\n\nThe \"nilsquare\" or \"nilpotent\" infinitesimals are numbers \"ε\" where \"ε\"² = 0 is true, but \"ε\" = 0 need not be true at the same time.\n\nThis approach departs from the classical logic used in conventional mathematics by denying the law of the excluded middle, e.g., \"NOT\" (\"a\" ≠ \"b\") does not imply \"a\" = \"b\". In particular, in a theory of smooth infinitesimal analysis one can prove for all infinitesimals \"ε\", \"NOT\" (\"ε\" ≠ 0); yet it is provably false that all infinitesimals are equal to zero. One can see that the law of excluded middle cannot hold from the following basic theorem (again, understood in the context of a theory of smooth infinitesimal analysis):\n\nDespite this fact, one could attempt to define a discontinuous function \"f\"(\"x\") by specifying that \"f\"(\"x\") = 1 for \"x\" = 0, and \"f\"(\"x\") = 0 for \"x\" ≠ 0. If the law of the excluded middle held, then this would be a fully defined, discontinuous function. However, there are plenty of \"x\", namely the infinitesimals, such that neither \"x\" = 0 nor \"x\" ≠ 0 holds, so the function is not defined on the real numbers.\n\nIn typical models of smooth infinitesimal analysis, the infinitesimals are not invertible, and therefore the theory does not contain infinite numbers. However, there are also models that include invertible infinitesimals.\n\nOther mathematical systems exist which include infinitesimals, including non-standard analysis and the surreal numbers. Smooth infinitesimal analysis is like non-standard analysis in that (1) it is meant to serve as a foundation for analysis, and (2) the infinitesimal quantities do not have concrete sizes (as opposed to the surreals, in which a typical infinitesimal is 1/ω, where ω is a von Neumann ordinal). However, smooth infinitesimal analysis differs from non-standard analysis in its use of nonclassical logic, and in lacking the transfer principle. Some theorems of standard and non-standard analysis are false in smooth infinitesimal analysis, including the intermediate value theorem and the Banach–Tarski paradox. Statements in non-standard analysis can be translated into statements about limits, but the same is not always true in smooth infinitesimal analysis.\n\nIntuitively, smooth infinitesimal analysis can be interpreted as describing a world in which lines are made out of infinitesimally small segments, not out of points. These segments can be thought of as being long enough to have a definite direction, but not long enough to be curved. The construction of discontinuous functions fails because a function is identified with a curve, and the curve cannot be constructed pointwise. We can imagine the intermediate value theorem's failure as resulting from the ability of an infinitesimal segment to straddle a line. Similarly, the Banach–Tarski paradox fails because a volume cannot be taken apart into points.\n\n\n\n"}
{"id": "1850040", "url": "https://en.wikipedia.org/wiki?curid=1850040", "title": "Snake-in-the-box", "text": "Snake-in-the-box\n\nThe snake-in-the-box problem in graph theory and computer science deals with finding a certain kind of path along the edges of a hypercube. This path starts at one corner and travels along the edges to as many corners as it can reach. After it gets to a new corner, the previous corner and all of its neighbors must be marked as unusable. The path should never travel to a corner after it has been marked unusable.\n\nIn other words, a \"snake\" is a connected open path in the hypercube where each node in the path, with the exception of the head (start) and the tail (finish), has exactly two neighbors that are also in the snake. The head and the tail each have only one neighbor in the snake. The rule for generating a snake is that a node in the hypercube may be visited if it is connected to the current node and it is not a neighbor of any previously visited node in the snake, other than the current node.\n\nIn graph theory terminology, this is called finding the longest possible induced path in a hypercube; it can be viewed as a special case of the induced subgraph isomorphism problem. There is a similar problem of finding long induced cycles in hypercubes, called the coil-in-the-box problem.\n\nThe snake-in-the-box problem was first described by , motivated by the theory of error-correcting codes. The vertices of a solution to the snake or coil in the box problems can be used as a Gray code that can detect single-bit errors. Such codes have applications in electrical engineering, coding theory, and computer network topologies. In these applications, it is important to devise as long a code as is possible for a given dimension of hypercube. The longer the code, the more effective are its capabilities.\n\nFinding the longest snake or coil becomes notoriously difficult as the dimension number increases and the search space suffers a serious combinatorial explosion. Some techniques for determining the upper and lower bounds for the snake-in-the-box problem include proofs using discrete mathematics and graph theory, exhaustive search of the search space, and heuristic search utilizing evolutionary techniques.\n\nThe maximum length for the snake-in-the-box problem is known for dimensions one through eight; it is\nBeyond that length, the exact length of the longest snake is not known; the best lengths found so far for dimensions nine through thirteen are\n\nFor cycles (the coil-in-the-box problem), a cycle cannot exist in a hypercube of dimension less than two. Starting at that dimension, the lengths of the longest possible cycles are\nBeyond that length, the exact length of the longest cycle is not known; the best lengths found so far for dimensions nine through thirteen are\n\n\"Doubled coils\" are a special case: cycles whose second half repeats the structure of their first half, also known as \"symmetric coils\". For dimensions two through seven the lengths of the longest possible doubled coils are\nBeyond that, the best lengths found so far for dimensions eight through thirteen are\n\nFor both the snake and the coil in the box problems, it is known that the maximum length is proportional to 2 for an \"n\"-dimensional box, asymptotically as \"n\" grows large, and bounded above by 2. However the constant of proportionality is not known, but is observed to be in the range 0.3 - 0.4 for current best known values.\n\n"}
{"id": "23856672", "url": "https://en.wikipedia.org/wiki?curid=23856672", "title": "Spectral triple", "text": "Spectral triple\n\nIn noncommutative geometry and related branches of mathematics and mathematical physics, a spectral triple is a set of data which encodes a geometric phenomenon in an analytic way. The definition typically involves a Hilbert space, an algebra of operators on it and an unbounded self-adjoint operator, endowed with supplemental structures. It was conceived by Alain Connes who was motivated by the Atiyah-Singer index theorem and sought its extension to 'noncommutative' spaces. Some authors refer to this notion as unbounded K-cycles or as unbounded Fredholm modules.\n\nA motivating example of spectral triple is given by the algebra of smooth functions on a compact spin manifold, acting on the Hilbert space of L-spinors, accompanied by the Dirac operator associated to the spin structure. From the knowledge of these objects one is able to recover the original manifold as a metric space: the manifold as a topological space is recovered as the spectrum of the algebra, while the (absolute value of) Dirac operator retains the metric. On the other hand, the phase part of the Dirac operator, in conjunction with the algebra of functions, gives a K-cycle which encodes index-theoretic information. The local index formula expresses the pairing of the K-group of the manifold with this K-cycle in two ways: the 'analytic/global' side involves the usual trace on the Hilbert space and commutators of functions with the phase operator (which corresponds to the 'index' part of the index theorem), while the 'geometric/local' side involves the Dixmier trace and commutators with the Dirac operator (which corresponds to the 'characteristic class integration' part of the index theorem).\n\nExtensions of the index theorem can be considered in cases, typically when one has an action of a group on the manifold, or when the manifold is endowed with a foliation structure, among others. In those cases the algebraic system of the 'functions' which expresses the underlying geometric object is no longer commutative, but one may able to find the space of square integrable spinors (or, sections of a Clifford module) on which the algebra acts, and the corresponding 'Dirac' operator on it satisfying certain boundedness of commutators implied by the pseudo-differential calculus.\n\nAn odd spectral triple is a triple (A, H, D) consisting of a Hilbert space H, an algebra A of operators on H (usually closed under taking adjoints) and a densely defined self adjoint operator D satisfying <nowiki>‖[a, D]‖ < ∞</nowiki> for any a ∈ A. An even spectral triple is an odd spectral triple with a Z/2Z-grading on H, such that the elements in A are even while D is odd with respect to this grading. One could also say that an even spectral triple is given by a quartet (A, H, D, γ) such that γ is a self adjoint unitary on H satisfying a γ = γ a for any a in A and D γ = - γ D.\n\nA finitely summable spectral triple is a spectral triple (A, H, D) such that a.D for any a in A has a compact resolvent which belongs to the class of L-operators for a fixed p (when A contains the identity operator on H, it is enough to require D in L(H)). When this condition is satisfied, the triple (A, H, D) is said to be p-summable. A spectral triple is said to be θ-summable when e is of trace class for any t > 0.\n\nLet δ(T) denote the commutator of |D| with an operator T on H. A spectral triple is said to be regular when the elements in A and the operators of the form <nowiki>[a, D]</nowiki> for a in A are in the domain of the iterates δ of δ.\n\nWhen a spectral triple (A, H, D) is p-summable, one may define its zeta function ζ(s) = Tr(|D|); more generally there are zeta functions ζ(s) = Tr(b|D|) for each element b in the algebra B generated by δ(A) and δ(<nowiki>[a, D]</nowiki>) for positive integers n. They are related to the heat kernel exp(-t|D|) by a Mellin transform. The collection of the poles of the analytic continuation of ζ for b in B is called the dimension spectrum of (A, H, D).\n\nA real spectral triple is a spectral triple (A, H, D) accompanied with an anti-linear involution J on H, satisfying <nowiki>[a, JbJ] = 0</nowiki> for a, b in A. In the even case it is usually assumed that J is even with respect to the grading on H.\n\nGiven a spectral triple (A, H, D), one can apply several important operations to it. The most fundamental one is the polar decomposition D = F|D| of D into a self adjoint unitary operator F (the 'phase' of D) and a densely defined positive operator |D| (the 'metric' part).\nThe positive |D| operator defines a metric on the set of pure states on the norm closure of A.\n\nThe self adjoint unitary \"F\" gives a map of the K-theory of \"A\" into integers by taking Fredholm index as follows. In the even case, each projection \"e\" in \"A\" decomposes as \"e\" ⊕ \"e\" under the grading and \"e\"\"Fe\" becomes a Fredholm operator from \"e\"\"H\" to \"e\"\"H\". Thus \"e\" → Ind \"e\"\"Fe\" defines an additive mapping of \"K\"(\"A\") to Z. In the odd case the eigenspace decomposition of \"F\" gives a grading on \"H\", and each invertible element in \"A\" gives a Fredholm operator (\"F\" + 1) u (\"F\" − 1)/4 from (\"F\" − 1)\"H\" to (\"F\" + 1)\"H\". Thus \"u\" → Ind (\"F\" + 1) u (\"F\" − 1)/4 gives an additive mapping from \"K\"(\"A\") to Z.\n\nWhen the spectral triple is finitely summable, one may write the above indexes using the (super) trace, and a product of \"F\", \"e\" (resp. \"u\") and commutator of \"F\" with \"e\" (resp. \"u\"). This can be encoded as a (\"p\" + 1)-functional on \"A\" satisfying some algebraic conditions and give Hochschild / cyclic cohomology cocycles, which describe the above maps from K-theory to the integers.\n\n\n"}
{"id": "19726159", "url": "https://en.wikipedia.org/wiki?curid=19726159", "title": "Table of the largest known graphs of a given diameter and maximal degree", "text": "Table of the largest known graphs of a given diameter and maximal degree\n\nIn graph theory, the degree diameter problem is the problem of finding the largest possible graph for a given maximum degree and diameter. The Moore bound sets limits on this, but for many years mathematicians in the field have been interested in a more precise answer. The table below gives current progress on this problem (excluding the case of degree 2, where the largest graphs are cycles with an odd number of vertices).\n\nBelow is the table of the vertex numbers for the best-known graphs (as of October 2008) in the undirected degree diameter problem for graphs of degree at most 3 ≤ \"d\" ≤ 16 and diameter 2 ≤ \"k\" ≤ 10. Only a few of the graphs in this table (marked in bold) are known to be optimal (that is, largest possible). The remainder are merely the largest so far discovered, and thus finding a larger graph that is closer in order (in terms of the size of the vertex set) to the Moore bound is considered an open problem. Some general constructions are known for values of \"d\" and \"k\" outside the range shown in the table.\nThe following table is the key to the colors in the table presented above:\n\n"}
{"id": "937739", "url": "https://en.wikipedia.org/wiki?curid=937739", "title": "Time evolution", "text": "Time evolution\n\nTime evolution is the change of state brought about by the passage of time, applicable to systems with internal state (also called \"stateful systems\"). In this formulation, \"time\" is not required to be a continuous parameter, but may be discrete or even finite. In classical physics, time evolution of a collection of rigid bodies is governed by the principles of classical mechanics. In their most rudimentary form, these principles express the relationship between forces acting on the bodies and their acceleration given by Newton's laws of motion. These principles can also be equivalently expressed more abstractly by Hamiltonian mechanics or Lagrangian mechanics.\n\nThe concept of time evolution may be applicable to other stateful systems as well. For instance, the operation of a Turing machine can be regarded as the time evolution of the machine's control state together with the state of the tape (or possibly multiple tapes) including the position of the machine's read-write head (or heads). In this case, time is discrete.\n\nStateful systems often have dual descriptions in terms of states or in terms of observable values. In such systems, time evolution can also refer to the change in observable values. This is particularly relevant in quantum mechanics where the Schrödinger picture and Heisenberg picture are (mostly) equivalent descriptions of time evolution.\n\nConsider a system with state space \"X\" for which evolution is deterministic and reversible. For concreteness let us also suppose time is a parameter that ranges over the set of real numbers R. Then time evolution is given by a family of bijective state transformations \n\nF(\"x\") is the state of the system at time \"t\", whose state at time \"s\" is \"x\". The following identity holds\n\nTo see why this is true, suppose \"x\" ∈ \"X\" is the state at time \"s\". Then by the definition of F, F(\"x\") is the state of the system at time \"t\" and consequently applying the definition once more, F(F(\"x\")) is the state at time \"u\". But this is also F(\"x\").\n\nIn some contexts in mathematical physics, the mappings F are called propagation operators or simply propagators. In classical mechanics, the propagators are functions that operate on the phase space of a physical system. In quantum mechanics, the propagators are usually unitary operators on a Hilbert space. The propagators can be expressed as time-ordered exponentials of the integrated Hamiltonian. The asymptotic properties of time evolution are given by the scattering matrix.\n\nA state space with a distinguished propagator is also called a dynamical system.\n\nTo say time evolution is homogeneous means that\n\nIn the case of a homogeneous system, the mappings G = F form a one-parameter group of transformations of \"X\", that is\n\nNon-reversibility. For non-reversible systems, the propagation operators F are defined whenever \"t\" ≥ \"s\" and satisfy the propagation identity \n\nIn the homogeneous case the propagators are exponentials of the Hamiltonian.\n\n\n"}
{"id": "28537741", "url": "https://en.wikipedia.org/wiki?curid=28537741", "title": "Tree (automata theory)", "text": "Tree (automata theory)\n\nIn automata theory, a tree is a particular way of representing a tree structure as sequences of natural numbers. \n\nFor example, each node of the tree is a word over set of natural numbers (ℕ), which helps this definition to be used in automata theory.\n\nA tree is a set \"T\" ⊆ ℕ such that if \"t\".\"c\" ∈ \"T\", with \"t\" ∈ ℕ and \"c\" ∈ ℕ, then \"t\" ∈ \"T\" and \"t\".\"c\" ∈ \"T\" for all 0 ≤ \"c\" < \"c\". The elements of \"T\" are known as nodes, and the empty word ε is the (single) root of \"T\". For every \"t\" ∈ \"T\", the element \"t\".\"c\" ∈ \"T\" is a successor of \"t\" in direction \"c\". The number of successors of \"t\" is called its degree or arity, and represented as \"d\"(\"t\"). A node is a leaf if it has no successors. If every node of a tree has finitely many successors, then it is called a finitely, otherwise an infinitely branching tree. A path π is a subset of \"T\" such that ε ∈ π and for every \"t\" ∈ \"T\", either \"t\" is a leaf or there exists a unique \"c\" ∈ ℕ such that \"t\".\"c\" ∈ π. A path may be a finite or infinite set. If all paths of a tree are finite then the tree is called finite, otherwise infinite. A tree is called fully infinite if all its paths are infinite. Given an alphabet Σ, a Σ-labeled tree is a pair (\"T\",\"V\"), where \"T\" is a tree and \"V\": \"T\" → Σ maps each node of \"T\" to a symbol in Σ. A labeled tree formally defines a commonly used term tree structure. A set of labeled trees is called a tree language.\n\nA tree is called ordered if there is an order among the successors of each of its nodes. The above definition of tree naturally suggests an order among the successors, which can be used to make the tree ranked. \n\nIn the case of ranked alphabets, an extra function \"Ar\": Σ → ℕ is defined. This function associates a fixed arity to each symbol of the alphabet. In this case, each \"t\" ∈ \"T\" has to satisfy \"Ar\"(\"V\"(\"t\")) = \"d\"(\"t\"). The trees that satisfy this property are called ranked trees. The trees that do not (necessarily) satisfy that property are called unranked.\n\nFor example, the above definition is used in the definition of an infinite tree automaton.\n\nLet \"T\" = {0,1} and Σ = {\"a\",\"b\"}. We define a labeling function \"V\" as follows: the labeling for the root node is \"V\"(ε) = \"a\" and, for every other node \"t\" ∈ {0,1}, the labellings for its successor nodes are \"V\"(\"t\".0) = \"a\" and \"V\"(\"t\".1) = \"b\". It is clear from the picture that \"T\" forms a (fully) infinite binary tree.\n"}
{"id": "491095", "url": "https://en.wikipedia.org/wiki?curid=491095", "title": "Weyl algebra", "text": "Weyl algebra\n\nIn abstract algebra, the Weyl algebra is the ring of differential operators with polynomial coefficients (in one variable), namely expressions of the form\n\nMore precisely, let \"F\" be the underlying field, and let \"F\"[\"X\"] be the ring of polynomials in one variable, \"X\", with coefficients in \"F\". Then each \"f\" lies in \"F\"[\"X\"].\n\n\"∂\" is the derivative with respect to \"X\". The algebra is generated by \"X\" and \"∂\" .\n\nThe Weyl algebra is an example of a simple ring that is not a matrix ring over a division ring. It is also a noncommutative example of a domain, and an example of an Ore extension.\n\nThe Weyl algebra is isomorphic to the quotient of the free algebra on two generators, \"X\" and \"Y\", by the ideal generated by the element\n\nThe Weyl algebra is the first in an infinite family of algebras, also known as Weyl algebras. The \"n\"-th Weyl algebra, \"A\", is the ring of differential operators with polynomial coefficients in \"n\" variables. It is generated by \"X\" and \"∂\", .\n\nWeyl algebras are named after Hermann Weyl, who introduced them to study the Heisenberg uncertainty principle in quantum mechanics. It is a quotient of the universal enveloping algebra of the Heisenberg algebra, the Lie algebra of the Heisenberg group, by setting the central element of\nthe Heisenberg algebra (namely [\"X\",\"Y\"]) equal to the unit of the universal enveloping algebra (called 1 above).\n\nThe Weyl algebra is also referred to as the symplectic Clifford algebra. Weyl algebras represent the same structure for symplectic bilinear forms that Clifford algebras represent for non-degenerate symmetric bilinear forms.\n\nOne may give an abstract construction of the algebras \"A\" in terms of generators and relations. Start with an abstract vector space \"V\" (of dimension 2\"n\") equipped with a symplectic form \"ω\". Define the Weyl algebra \"W\"(\"V\") to be\n\nwhere \"T\"(\"V\") is the tensor algebra on \"V\", and the notation formula_4 means \"the ideal generated by\".\n\nIn other words, \"W\"(\"V\") is the algebra generated by \"V\" subject only to the relation . Then, \"W\"(\"V\") is isomorphic to \"A\" via the choice of a Darboux basis for .\n\nThe algebra \"W\"(\"V\") is a quantization of the symmetric algebra Sym(\"V\"). If \"V\" is over a field of characteristic zero, then \"W\"(\"V\") is naturally isomorphic to the underlying vector space of the symmetric algebra Sym(\"V\") equipped with a deformed product – called the Groenewold–Moyal product (considering the symmetric algebra to be polynomial functions on \"V\", where the variables span the vector space \"V\", and replacing \"iħ\" in the Moyal product formula with 1).\n\nThe isomorphism is given by the symmetrization map from Sym(\"V\") to \"W\"(\"V\")\n\nIf one prefers to have the \"iħ\" and work over the complex numbers, one could have instead defined the Weyl algebra above as generated by \"X\" and \"iħ∂\" (as per quantum mechanics usage).\n\nThus, the Weyl algebra is a quantization of the symmetric algebra, which is essentially the same as the Moyal quantization (if for the latter one restricts to polynomial functions), but the former is in terms of generators and relations (considered to be differential operators) and the latter is in terms of a deformed multiplication.\n\nIn the case of exterior algebras, the analogous quantization to the Weyl one is the Clifford algebra, which is also referred to as the \"orthogonal Clifford algebra\".\n\nIn the case that the ground field has characteristic zero, the \"n\"th Weyl algebra is a simple Noetherian domain. It has global dimension \"n\", in contrast to the ring it deforms, Sym(\"V\"), which has global dimension 2\"n\".\n\nIt has no finite-dimensional representations. Although this follows from simplicity, it can be more directly shown by taking the trace \"σ\"(\"X\") and \"σ\"(\"Y\") for some finite-dimensional representation \"σ\" (where ).\nSince the trace of a commutator is zero, and the trace of the identity is the dimension of the matrix, the representation must be zero dimensional.\n\nIn fact, there are stronger statements than the absence of finite-dimensional representations. To any finitely generated \"A\"-module \"M\", there is a corresponding subvariety Char(\"M\") of called the 'characteristic variety' whose size roughly corresponds to the size of \"M\" (a finite-dimensional module would have zero-dimensional characteristic variety). Then Bernstein's inequality states that for \"M\" non-zero,\nAn even stronger statement is Gabber's theorem, which states that Char(\"M\") is a co-isotropic subvariety of for the natural symplectic form.\n\nThe situation is considerably different in the case of a Weyl algebra over a field of characteristic .\n\nIn this case, for any element \"D\" of the Weyl algebra, the element \"D\" is central, and so the Weyl algebra has a very large center. In fact, it is a finitely generated module over its center; even more so, it is an Azumaya algebra over its center. As a consequence, there are many finite-dimensional representations which are all built out of simple representations of dimension \"p\".\n\nFor more details about this quantization in the case \"n\" = 1 (and an extension using the Fourier transform to a class of integrable functions larger than the polynomial functions), see Wigner–Weyl transform.\n\nWeyl algebras and Clifford algebras admit a further structure of a *-algebra, and can be unified as even and odd terms of a superalgebra, as discussed in CCR and CAR algebras.\n\nWeyl algebras also generalize in the case of algebraic varieties. Consider a polynomial ring\nthen a differential operator is defined as a composition formula_9-linear derivations of formula_10. This can be described explicitly as the quotient ring\n"}
{"id": "51426434", "url": "https://en.wikipedia.org/wiki?curid=51426434", "title": "Winifred Margaret Deans", "text": "Winifred Margaret Deans\n\nWinifred Margaret Deans (9 October 1901 – 7 June 1990) was a prolific translator of German scientific texts into English, who also taught mathematics and physics to secondary schoolchildren and worked at the Commonwealth Bureau of Animal Nutrition.\n\nDeans was one of two siblings, born to Duncan Deans and Mary Ann Sharp, in New Milton, Hampshire, United Kingdom. She graduated with an M.A. with First Class Honours in Mathematics from the University of Aberdeen in 1922. She also obtained a B. Sc. from the same university in 1923. She later studied at Newnham College, Cambridge, obtaining a First Class B.A. after she took Part I of the Mathematical Tripos in 1925. She earned another M.A, from Cambridge, in 1929. \n\nDeans won several awards in the course of her education, the University of Aberdeen awarding her the Simpson mathematical prize and the Neil Arnott prize for experimental physics in 1921; she also stood first in the examination for the Greig prize in natural philosophy. \n\nDeans taught mathematics and physics at the Harrow County Secondary School for Girls for two years. She then returned to Aberdeen and received a Diploma in Education in 1927. She joined Blackie and Son, a publishing house in Glasgow as an Assistant Science Editor. She began translating German publications, primarily related to Physics and Mathematics, for them. Important translations included those of texts by Max Born, Léon Brillouin, Louis de Broglie, Peter Debye, Richard Gans, Robert Pohl and Erwin Schrödinger. She also translated Else Wegener and Fritz Loewe's chronicle of Alfred Wegener’s fourth expedition to Greenland, undertaken in 1930–31.\n\nDeans joined the Commonwealth Bureau of Animal Nutrition which was part of the Rowett Research Institute, Aberdeen, in 1945. She retired from the bureau in 1966.\n\nDeans’ library and personal papers were given to the University of Aberdeen, and can now be found in their Special Collections, Library and Archives.\n\n"}
{"id": "28363938", "url": "https://en.wikipedia.org/wiki?curid=28363938", "title": "Witting polytope", "text": "Witting polytope\n\nIn 4-dimensional complex geometry, the Witting polytope is a regular complex polytope, named as: {3}{3}{3}, and Coxeter diagram . It has 240 vertices, 2160 {} edges, 2160 {3} faces, and 240 {3}{3} cells. It is self-dual. Each vertex belongs to 27 edges, 72 faces, and 27 cells, corresponding to the Hessian polyhedron vertex figure.\n\nIts symmetry by [3][3][3] or , order 155,520. It has 240 copies of , order 648 at each cell.\n\nThe configuation matrix is:\nformula_1\n\nThe number of vertices, edges, faces, and cells are seen in the diagonal of the matrix. These are computed by the order of the group divided by the order of the subgroup, by removing certain complex reflections, shown with X below. The number of elements of the k-faces are seen in rows below the diagonal. The number of elements in the vertex figure, etc, are given in rows above the digonal.\nIts 240 vertices are given coordinates in formula_2: \nwhere formula_3.\n\nThe last 6 points form hexagonal \"holes\" on one of its 40 diameters. There are 40 hyperplanes contain central {3}{4}, figures, with 72 vertices.\n\nCoxeter named it after Alexander Witting for being a \"Witting configuration\" in complex projective 3-space:\n\nThe Witting configuration is related to the finite space PG(3,2), consisting of 85 points, 357 lines, and 85 planes.\n\nIts 240 vertices are shared with the real 8-dimensional polytope 4, . Its 2160 3-edges are sometimes drawn as 6480 simple edges, slightly less than the 6720 edges of 4. The 240 difference is accounted by 40 central hexagons in 4 whose edges are not included in {3}{3}{3}.\n\nThe regular Witting polytope has one further stage as a 4-dimensional honeycomb, . It has the Witting polytope as both its facets, and vertex figure. It is self-dual, and its dual coincides with itself.\n\nHyperplane sections of this honeycomb include 3-dimensional honeycombs .\n\nThe honeycomb of Witting polytopes has a real representation as the 8-dimensional polytope 5, .\n\nIts f-vector element counts are in proportion: 1, 80, 270, 80, 1. The configuration matrix for the honeycomb is:\n"}
{"id": "58946268", "url": "https://en.wikipedia.org/wiki?curid=58946268", "title": "Zimmer's conjecture", "text": "Zimmer's conjecture\n\nZimmer's conjecture is a statement in mathematics \"which has to do with the circumstances under which geometric spaces exhibit certain kinds of symmetries.\" It was named after the mathematician Robert Zimmer. The conjecture states that there can exist symmetries (specifically higher-rank lattices) in a higher dimension that cannot exist in lower dimensions.\n\nIn 2017, the conjecture was proven by Aaron Brown and Sebastian Hurtado-Salazar of the University of Chicago and David Fisher of Indiana University.\n"}
