{"id": "7044429", "url": "https://en.wikipedia.org/wiki?curid=7044429", "title": "Absolutely irreducible", "text": "Absolutely irreducible\n\nIn mathematics, a multivariate polynomial defined over the rational numbers is absolutely irreducible if it is irreducible over the complex field. For example, formula_1 is absolutely irreducible, but while formula_2 is irreducible over the integers and the reals, it is reducible over the complex numbers as formula_3 and thus not absolutely irreducible.\n\nMore generally, a polynomial defined over a field \"K\" is absolutely irreducible if it is irreducible over every algebraic extension of \"K\", and an affine algebraic set defined by equations with coefficients in a field \"K\" is absolutely irreducible if it is not the union of two algebraic sets defined by equations in an algebraically closed extension of \"K\". In other words, an absolutely irreducible algebraic set is a synonym of an algebraic variety, which emphasizes that the coefficients of the defining equations may not belong to an algebraically closed field.\n\nAbsolutely irreducible is also applied, with the same meaning to linear representations of algebraic groups.\n\nIn all cases, being absolutely irreducible is the same as being irreducible over the algebraic closure of the ground field.\n\n\n"}
{"id": "44399285", "url": "https://en.wikipedia.org/wiki?curid=44399285", "title": "Basel Computational Biology Conference", "text": "Basel Computational Biology Conference\n\nThe Basel Computational Biology Conference (stylized as [BC]) is a scientific meeting on the subjects of bioinformatics and computational biology. It covers a wide spectrum of disciplines, including bioinformatics, computational biology, genomics, computational structural biology, and systems biology. The conference is organized biannually by the SIB Swiss Institute of Bioinformatics in Basel, Switzerland. The next edition will take place 12–15 September 2017 at the Congress Center Basel. \n\n"}
{"id": "74361", "url": "https://en.wikipedia.org/wiki?curid=74361", "title": "Binary symmetric channel", "text": "Binary symmetric channel\n\nA binary symmetric channel (or BSC) is a common communications channel model used in coding theory and information theory. In this model, a transmitter wishes to send a bit (a zero or a one), and the receiver receives a bit. It is assumed that the bit is \"usually\" transmitted correctly, but that it will be \"flipped\" with a small probability (the \"crossover probability\"). This channel is used frequently in information theory because it is one of the simplest channels to analyze.\n\nThe BSC is a \"binary channel\"; that is, it can transmit only one of two symbols (usually called 0 and 1). (A non-binary channel would be capable of transmitting more than 2 symbols, possibly even an infinite number of choices.) The transmission is not perfect, and occasionally the receiver gets the wrong bit.\n\nThis channel is often used by theorists because it is one of the simplest noisy channels to analyze. Many problems in communication theory can be reduced to a BSC. Conversely, being able to transmit effectively over the BSC can give rise to solutions for more complicated channels.\n\nA binary symmetric channel with crossover probability formula_1 denoted by BSC, is a channel with binary input and binary output and probability of error formula_1; that is, if formula_3 is the transmitted random variable and formula_4 the received variable, then the channel is characterized by the conditional probabilities\n\nIt is assumed that formula_6. If formula_7, then the receiver can swap the output (interpret 1 when it sees 0, and vice versa) and obtain an equivalent channel with crossover probability formula_8.\n\nThe channel capacity of the binary symmetric channel is \nwhere formula_10 is the binary entropy function.\n\nProof: The capacity is defined as the maximum mutual entropy between input and output for all possible input distributions formula_11:\n\nThe mutual information can be reformulated as\n\nwhere the first and second step follows from the definition of mutual information and conditional entropy respectively. The entropy at the output for a given and fixed input symbol (formula_14) equals the binary entropy function, which leads to the third line and this can be further simplified.\n\nIn the last line, only the first term formula_15 depends on the input distribution formula_11. And one knows, that the entropy of a binary variable is at maximum one, and reaches this only if its probability distribution is uniform. This case (uniform distribution of formula_4) can only be reached by a uniform distribution at the input formula_3, which is because of the symmetry of the channel. So one finally gets formula_19.\nShannon's noisy coding theorem is general for all kinds of channels. We consider a special case of this theorem for a binary symmetric channel with an error probability p.\n\nThe noise formula_20 that characterizes formula_21 is a random variable consisting of n independent random bits (n is defined below) where each random bit is a formula_22 with probability formula_1 and a formula_24 with probability formula_25. We indicate this by writing \"formula_26\".\n\nWhat this theorem actually implies is, a message when picked from formula_37, encoded with a random encoding function formula_38, and sent across a noisy formula_21, there is a very high probability of recovering the original message by decoding, if formula_40 or in effect the rate of the channel is bounded by the quantity stated in the theorem. The decoding error probability is exponentially small.\n\nProof of Theorem 1. First we describe the encoding function and decoding functions used in the theorem. We will use the probabilistic method to prove this theorem. Shannon's theorem was one of the earliest applications of this method.\n\nEncoding function: Consider an encoding function formula_33 that is selected at random. This means that for each message formula_42, the value formula_43 is selected at random (with equal probabilities).\n\nDecoding function: For a given encoding function formula_38, the decoding function formula_45 is specified as follows: given any received codeword formula_46, we find the message formula_35 such that the Hamming distance formula_48 is as small as possible (with ties broken arbitrarily). This kind of a decoding function is called a \"maximum likelihood decoding (MLD)\" function.\n\nUltimately, we will show (by integrating the probabilities) that at least one such choice formula_49 satisfies the conclusion of theorem; that is what is meant by the probabilistic method.\n\nThe proof runs as follows. Suppose formula_1 and formula_31 are fixed. First we show, for a fixed formula_52 and formula_38 chosen randomly, the probability of failure over formula_54 noise is exponentially small in \"n\". At this point, the proof works for a fixed message formula_55. Next we extend this result to work for \"all\" formula_55. We achieve this by eliminating half of the codewords from the code with the argument that the proof for the decoding error probability holds for at least half of the codewords. The latter method is called expurgation. This gives the total process the name \"random coding with expurgation\".\n\nA high level proof: Fix formula_1 and formula_31. Given a fixed message formula_52, we need to estimate the expected value of the probability of the received codeword along with the noise does not give back formula_55 on decoding. That is to say, we need to estimate:\n\nLet formula_62 be the received codeword. In order for the decoded codeword formula_63 not to be equal to the message formula_55, one of the following events must occur:\n\n\nWe can apply Chernoff bound to ensure the non occurrence of the first event. By applying Chernoff bound we have,\n\nThis is exponentially small for large formula_29 (recall that formula_31 is fixed).\n\nAs for the second event, we note that the probability that formula_73 is formula_74 where formula_75 is the Hamming ball of radius formula_76 centered at vector formula_77 and formula_78 is its volume. Using approximation to estimate the number of codewords in the Hamming ball, we have formula_79. Hence the above probability amounts to formula_80. Now using union bound, we can upper bound the existence of such an formula_68 by formula_82 which is formula_83, as desired by the choice of formula_40.\n\nA detailed proof: From the above analysis, we calculate the probability of the event that the decoded codeword plus the channel noise is not the same as the original message sent. We shall introduce some symbols here. Let formula_85 denote the probability of receiving codeword formula_62 given that codeword formula_67 was sent. Let formula_88 denote formula_89\n\nWe get the last inequality by our analysis using the Chernoff bound above. Now taking expectation on both sides we have,\n\nby appropriately choosing the value of formula_92. Since the above bound holds for each message, we have\n\nNow we can change the order of summation in the expectation with respect to the message and the choice of the encoding function formula_38. Hence:\n\nHence in conclusion, by probabilistic method, we have some encoding function formula_96 and a corresponding decoding function formula_97 such that\n\nAt this point, the proof works for a fixed message formula_55. But we need to make sure that the above bound holds for all the messages formula_55 simultaneously. For that, let us sort the formula_101 messages by their decoding error probabilities. Now by applying Markov's inequality, we can show the decoding error probability for the first formula_102 messages to be at most formula_103. Thus in order to confirm that the above bound to hold for every message formula_55, we could just trim off the last formula_102 messages from the sorted order. This essentially gives us another encoding function formula_106 with a corresponding decoding function formula_107 with a decoding error probability of at most formula_108 with the same rate. Taking formula_109 to be equal to formula_110 we bound the decoding error probability to formula_111. This expurgation process completes the proof of Theorem 1.\n\nThe converse of the capacity theorem essentially states that formula_112 is the best rate one can achieve over a binary symmetric channel. Formally the theorem states:\n\nTheorem 2\nIf formula_40 formula_114 formula_115 formula_116 formula_117 then the following is true for every encoding and decoding function formula_38: formula_37 formula_120 formula_121 and formula_122: formula_123 formula_120 formula_125 respectively: formula_126[formula_127 formula_128 formula_129 formula_114 formula_131.\n\nFor a detailed proof of this theorem, the reader is asked to refer to the bibliography. The intuition behind the proof is however showing the number of errors to grow rapidly as the rate grows beyond the channel capacity. The idea is the sender generates messages of dimension formula_40, while the channel formula_54 introduces transmission errors. When the capacity of the channel is formula_134, the number of errors is typically formula_135 for a code of block length formula_29. The maximum number of messages is formula_137. The output of the channel on the other hand has formula_138 possible values. If there is any confusion between any two messages, it is likely that formula_139. Hence we would have formula_140, a case we would like to avoid to keep the decoding error probability exponentially small.\n\nVery recently, a lot of work has been done and is also being done to design explicit error-correcting codes to achieve the capacities of several standard communication channels. The motivation behind designing such codes is to relate the rate of the code with the fraction of errors which it can correct.\n\nThe approach behind the design of codes which meet the channel capacities of formula_141, formula_142 have been to correct a lesser number of errors with a high probability, and to achieve the highest possible rate. Shannon's theorem gives us the best rate which could be achieved over a formula_21, but it does not give us an idea of any explicit codes which achieve that rate. In fact such codes are typically constructed to correct only a small fraction of errors with a high probability, but achieve a very good rate. The first such code was due to George D. Forney in 1966. The code is a concatenated code by concatenating two different kinds of codes. We shall discuss the construction Forney's code for the Binary Symmetric Channel and analyze its rate and decoding error probability briefly here. Various explicit codes for achieving the capacity of the binary erasure channel have also come up recently.\n\nForney constructed a concatenated code formula_144 to achieve the capacity of Theorem 1 for formula_54. In his code,\n\nFor the outer code formula_146, a Reed-Solomon code would have been the first code to have come in mind. However, we would see that the construction of such a code cannot be done in polynomial time. This is why a binary linear code is used for formula_146.\n\nFor the inner code formula_155 we find a linear code by exhaustively searching from the linear code of block length formula_29 and dimension formula_40, whose rate meets the capacity of formula_54, by Theorem 1.\n\nThe rate formula_170 which almost meets the formula_54 capacity. We further note that the encoding and decoding of formula_172 can be done in polynomial time with respect to formula_147. As a matter of fact, encoding formula_172 takes time formula_175. Further, the decoding algorithm described takes time formula_176 as long as formula_177; and formula_178.\n\nA natural decoding algorithm for formula_172 is to:\n\n\nNote that each block of code for formula_155 is considered a symbol for formula_146. Now since the probability of error at any index formula_185 for formula_159 is at most formula_187 and the errors in formula_54 are independent, the expected number of errors for formula_159 is at most formula_190 by linearity of expectation. Now applying Chernoff bound, we have bound error probability of more than formula_191 errors occurring to be formula_192. Since the outer code formula_146 can correct at most formula_191 errors, this is the decoding error probability of formula_172. This when expressed in asymptotic terms, gives us an error probability of formula_196. Thus the achieved decoding error probability of formula_172 is exponentially small as Theorem 1.\n\nWe have given a general technique to construct formula_172. For more detailed descriptions on formula_155 and formula_200 please read the following references. Recently a few other codes have also been constructed for achieving the capacities. LDPC codes have been considered for this purpose for their faster decoding time.\n\n\n\n//http://oscar.iitb.ac.in/availableProposalsAction1.do?type=av&id=534&language=english A Java applet implementing Binary Symmetric Channel\nbroken\n"}
{"id": "37994961", "url": "https://en.wikipedia.org/wiki?curid=37994961", "title": "C-Thru Ruler", "text": "C-Thru Ruler\n\nThe C-Thru Ruler Company is an American maker of measuring devices and specialized products for drafting, designing and drawing. The company was formed in 1939 in Bloomfield, Connecticut, by Jennie R. Zachs, a schoolteacher, who saw the need for transparent measuring tools such as rulers, triangles, curves and protractors. \n\nDuring the 1990s, the company expanded into the paper crafting and scrapbooking fields under the Little Yellow Bicycle and Déjà Views brands.\n\nIn June 2012, Acme United Corporation bought the ruler, lettering and drafting portions of C-Thru Ruler. The scrap booking part of the business, continues to be managed by the Zachs family under the Little Yellow Bicycle Inc. name.\n\nJennie R. Zachs, born in 1898, was the daughter of Benjamin and Julia Zachs who emigrated from Russia to the United States in the early 1900s. She graduated from high school in Hartford, CT. A few years later, she graduated from college and became a schoolteacher.\n\nWhile teaching, she developed the idea that when students would be able to see through their rulers, it would make the tool much more useful in the classroom. As a result, Ms. Zachs started the development of two transparent rulers made out of plastic.\n\nIn 1939, she founded C-Thru Ruler Company in Bloomfield, Connecticut and designed a whole family of transparent measuring tools like rulers, triangles, curves and protractors. Shortly after, she engaged a supplier to mill the tools out of plastic sheet and began to attend different trade shows and conventions for blue printers and art materials dealers to sell the products. She noticed that the transparent measuring tools could effectively replace wood and metal measuring devices for many applications in drafting, designing and drawing.\n\nOnly one year after founding the company, Jennie Zachs took in two partners to handle the expansion of C-Thru. Edward Zachs, her brother, joined C-Thru and Anna Zachs, her sister, became an investor. On October 14, 1946, C-Thru Ruler Company was formally incorporated in Connecticut.\n\nIn 1957, Edward’s son, Ted also started working at the Company. The following years, sales representatives were hired and the products line was significantly expanded through a distribution arrangement with Letraset, a manufacturer of dry transfer, rub on lettering and lettering tapes.\n\nIn 1970, Letraset decided to start selling its own line of transparent tools. In response thereto, C-Thru acquired a small manufacturer of dry transfer lettering products and started offering its own line of vinyl lettering. One year later, Ted Zachs bought out the other family members and became the sole shareholder of C-Thru.\n\nIn 1983, Jed Zachs, the son of Ted, joined the business and in 1985 another small acquisition was completed, which expanded the line of adhesive backed vinyl letters and stencils for signage purposes.\n\nIn 1993, Ross Zachs, Ted Zachs’ younger son also became part of the company. Jed oversaw manufacturing and related operations, while Ross played a key role in the creative and marketing sides of the business.\n\nThe following year, C-Thru’s owners saw that scrapbooking was growing rapidly and decided to enter that market with a line of stencils, templates and rulers. In the same year, it also created the Deja Views brand and began to sell to hobby and craft dealers, primarily small independent stores and a few chains, including Michaels, which only had a handful of stores at that time.\n\nTo expand the design and paper crafting business, a new brand called Little Yellow Bicycle was launched in 2008 with a differentiated line for independent retailers.\n\nIn June 2012, the non-scrapbooking part of the business was sold to Acme United Corporation. Acme United purchased the inventory, tooling, brands, and other intellectual property for approximately $1.47 million. In 2011, revenues for the C-Thru Ruler part of the business reached about $2.7 million.\n\nThe Zachs family continued to focus on the design and paper crafting business under the Little Yellow Bicycle Inc. name with Ted Zachs as president.\n\nAfter the acquisition, the C-Thru products were gradually integrated into Acme United’s Westcott product family, which mainly consists of rulers, scissors and other school and office products. So, the identity of C-Thru in most cases has become Westcott.\n\n"}
{"id": "10066673", "url": "https://en.wikipedia.org/wiki?curid=10066673", "title": "Causal sets", "text": "Causal sets\n\nThe causal sets program is an approach to quantum gravity. Its founding principles are that spacetime is fundamentally discrete (a collection of discrete spacetime points, called the elements of the causal set) and that spacetime events are related by a partial order. This partial order has the physical meaning of the causality relations between spacetime events.\n\nThe program is based on a theorem by David Malament that states that if there is a bijective map between two past and future distinguishing space times that preserves their causal structure then the map is a conformal isomorphism. The conformal factor that is left undetermined is related to the volume of regions in the spacetime. This volume factor can be recovered by specifying a volume element for each space time point. The volume of a space time region could then be found by counting the number of points in that region.\n\nCausal sets was initiated by Rafael Sorkin who continues to be the main proponent of the program. He has coined the slogan \"Order + Number = Geometry\" to characterize the above argument. The program provides a theory in which space time is fundamentally discrete while retaining local Lorentz invariance.\n\nA causal set (or causet) is a set formula_1 with a partial order relation formula_2 that is\n\nWe'll write formula_15 if formula_16 and formula_17.\n\nThe set formula_1 represents the set of spacetime events and the order relation formula_2 represents the causal relationship between events (see causal structure for the analogous idea in a Lorentzian manifold).\n\nAlthough this definition uses the reflexive convention we could have chosen the irreflexive convention in which the order relation is irreflexive. The causal relation of a Lorentzian manifold (without closed causal curves) satisfies the first three conditions. It is the local finiteness condition that introduces spacetime discreteness.\n\nGiven a causal set we may ask whether it can be embedded into a Lorentzian manifold. An embedding would be a map taking elements of the causal set into points in the manifold such that the order relation of the causal set matches the causal ordering of the manifold. A further criterion is needed however before the embedding is suitable. If, on average, the number of causal set elements mapped into a region of the manifold is proportional to the volume of the region then the embedding is said to be \"faithful\". In this case we can consider the causal set to be 'manifold-like'\n\nA central conjecture to the causal set program is that the same causal set cannot be faithfully embedded into two spacetimes that are not similar on large scales. This is called the \"Hauptvermutung\", meaning 'fundamental conjecture'. It is difficult to define this conjecture precisely because it is difficult to decide when two spacetimes are 'similar on large scales'.\n\nModelling spacetime as a causal set would require us to restrict attention to those causal sets that are 'manifold-like'. Given a causal set this is a difficult property to determine.\n\nThe difficulty of determining whether a causal set can be embedded into a manifold can be approached from the other direction. We can create a causal set by sprinkling points into a Lorentzian manifold. By sprinkling points in proportion to the volume of the spacetime regions and using the causal order relations in the manifold to induce order relations between the sprinkled points, we can produce a causal set that (by construction) can be faithfully embedded into the manifold.\n\nTo maintain Lorentz invariance this sprinkling of points must be done randomly using a Poisson process. Thus the probability of sprinkling formula_20 points into a region of volume formula_21 is\n\nformula_22\n\nwhere formula_23 is the density of the sprinkling.\n\nSprinkling points as a regular lattice would not keep the number of points proportional to the region volume.\n\nSome geometrical constructions in manifolds carry over to causal sets. When defining these we must remember to rely only on the causal set itself, not on any background spacetime into which it might be embedded. For an overview of these constructions, see.\n\nA \"link\" in a causal set is a pair of elements formula_5 such that formula_15 but with no formula_26 such that formula_27.\n\nA \"chain\" is a sequence of elements formula_28 such that formula_29 for formula_30. The length of a chain is formula_20.\nIf every formula_32 in the chain form a link, then the chain is called a \"path\".\n\nWe can use this to define the notion of a geodesic between two causal set elements, provided they are order comparable, that is, causally connected (physically, this means they are time-like). A geodesic between two elements formula_33 is a chain consisting only of links such that\nIn general there can be more than one geodesic between two comparable elements.\n\nMyrheim first suggested that the length of such a geodesic should be directly proportional to the proper time along a timelike geodesic joining the two spacetime points. Tests of this conjecture have been made using causal sets generated from sprinklings into flat spacetimes. The proportionality has been shown to hold and is conjectured to hold for sprinklings in curved spacetimes too.\n\nMuch work has been done in estimating the manifold dimension of a causal set. This involves algorithms using the causal set aiming to give the dimension of the manifold into which it can be faithfully embedded. The algorithms developed so far are based on finding the dimension of a Minkowski spacetime into which the causal set can be faithfully embedded.\n\nThis approach relies on estimating the number of formula_39-length chains present in a sprinkling into formula_40-dimensional Minkowski spacetime. Counting the number of formula_39-length chains in the causal set then allows an estimate for formula_40 to be made.\n\nThis approach relies on the relationship between the proper time between two points in Minkowski spacetime and the volume of the spacetime interval between them. By computing the maximal chain length (to estimate the proper time) between two points formula_37 and formula_38 and counting the number of elements formula_45 such that formula_27 (to estimate the volume of the spacetime interval) the dimension of the spacetime can be calculated.\n\nThese estimators should give the correct dimension for causal sets generated by high-density sprinklings into formula_40-dimensional Minkowski spacetime. Tests in conformally-flat spacetimes have shown these two methods to be accurate.\n\nAn ongoing task is to develop the correct dynamics for causal sets. These would provide a set of rules that determine which causal sets correspond to physically realistic spacetimes. The most popular approach to developing causal set dynamics is based on the \"sum-over-histories\" version of quantum mechanics. This approach would perform a \"sum-over-causal sets\" by \"growing\" a causal set one element at a time. Elements would be added according to quantum mechanical rules and interference would ensure a large manifold-like spacetime would dominate the contributions. The best model for dynamics at the moment is a classical model in which elements are added according to probabilities. This model, due to David Rideout and Rafael Sorkin, is known as \"classical sequential growth\" (CSG) dynamics. The classical sequential growth model is a way to generate causal sets by adding new elements one after another. Rules for how new elements are added are specified and, depending on the parameters in the model, different causal sets result.\n\nIn analogy to the path integral formulation of quantum mechanics, one approach to developing a quantum dynamics for causal sets has been to apply an action principle in the sum-over-causal sets approach. Sorkin has proposed a discrete analogue for the d'Alembertian, which can in turn be used to define the Ricci curvature scalar and thereby the \"Benincasa-Dowker action\" on a causal set. Monte-Carlo simulations have provided evidence for a continuum phase in 2D using the Benincasa-Dowker Action.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7079248", "url": "https://en.wikipedia.org/wiki?curid=7079248", "title": "Centrosymmetric matrix", "text": "Centrosymmetric matrix\n\nIn mathematics, especially in linear algebra and matrix theory, a centrosymmetric matrix is a matrix which is symmetric about its center. More precisely, an \"n\" × \"n\" matrix \"A\" = [ \"A\" ] is centrosymmetric when its entries satisfy\n\nIf \"J\" denotes the \"n\" × \"n\" matrix with 1 on the counterdiagonal and 0 elsewhere (that is, \"J\" = 1; \"J\" = 0 if j ≠ n+1-i), then a matrix \"A\" is centrosymmetric if and only if \"AJ = JA\". The matrix \"J\" is sometimes referred to as the exchange matrix.\n\n\n\n\nIf \"A\" and \"B\" are centrosymmetric matrices over a given field \"K\", then so are \"A+B\" and \"cA\" for any \"c\" in \"K\". In addition, the matrix product \"AB\" is centrosymmetric, since \"JAB = AJB = ABJ\". Since the identity matrix is also centrosymmetric, it follows that the set of \"n × n\" centrosymmetric matrices over \"K\" is a subalgebra of the associative algebra of all \"n × n\" matrices.\n\nAn \"n × n\" matrix \"A\" is said to be \"skew-centrosymmetric\" if its entries satisfy \"A\" = -\"A\" for 1 ≤ i,j ≤ n. Equivalently, \"A\" is skew-centrosymmetric if \"AJ = -JA\", where \"J\" is the exchange matrix defined above.\n\nThe centrosymmetric relation \"AJ = JA\" lends itself to a natural generalization, where \"J\" is replaced with an involutory matrix \"K\" (i.e., \"K = I\")\n\nSymmetric centrosymmetric matrices are sometimes called bisymmetric matrices. When the ground field is the field of real numbers, it has been shown that bisymmetric matrices are precisely those symmetric matrices whose eigenvalues are the same up to sign after pre or post multiplication by the exchange matrix. A similar result holds for Hermitian centrosymmetric and skew-centrosymmetric matrices.\n\n\n"}
{"id": "7738360", "url": "https://en.wikipedia.org/wiki?curid=7738360", "title": "Conditional event algebra", "text": "Conditional event algebra\n\nA conditional event algebra (CEA) is an algebraic structure whose domain consists of logical objects described by statements of forms such as \"If \"A\", then \"B\", \"B\", given \"A\", and \"B\", in case \"A\"\". Unlike the standard Boolean algebra of events, a CEA allows the defining of a probability function, \"P\", which satisfies the equation \"P\"(If \"A\" then \"B\") = \"P\"(\"A\" and \"B\") / \"P\"(\"A\") over a usefully broad range of conditions.\n\nIn standard probability theory, one begins with a set, Ω, of outcomes (or, in alternate terminology, a set of possible worlds) and a set, \"F\", of some (not necessarily all) subsets of Ω, such that \"F\" is closed under the countably infinite versions of the operations of basic set theory: union (∪), intersection (∩), and complementation ( ′). A member of \"F\" is called an event (or, alternatively, a proposition), and \"F\", the set of events, is the domain of the algebra. Ω is, necessarily, a member of \"F\", namely the trivial event \"Some outcome occurs.\"\n\nA probability function \"P\" assigns to each member of \"F\" a real number, in such a way as to satisfy the following axioms:\n\nIt follows that \"P\"(\"E\") is always less than or equal to 1. The probability function is the basis for statements like \"P\"(\"A\" ∩ \"B\"′) = 0.73, which means, \"The probability that \"A\" but not \"B\" is 73%.\"\n\nThe statement \"The probability that if \"A\", then \"B\", is 24%.\" means (put intuitively) that event \"B\" occurs in 24% of the outcomes where event \"A\" occurs. The standard formal expression of this is \"P\"(\"B\"|\"A\") = 0.24, where the conditional probability \"P\"(\"B\"|\"A\") equals, by definition, \"P\"(\"A\" ∩ \"B\") / \"P\"(\"A\").\n\nIt is tempting to write, instead, \"P\"(\"A\" → \"B\") = 0.24, where \"A\" → \"B\" is the conditional event \"If \"A\", then \"B\"\". That is, given events \"A\" and \"B\", one might posit an event, \"A\" → \"B\", such that \"P\"(\"A\" → \"B\") could be counted on to equal \"P\"(\"B\"|\"A\"). One benefit of being able to refer to conditional events would be the opportunity to nest conditional event descriptions within larger constructions. Then, for instance, one could write \"P\"(\"A\" ∪ (\"B\" → \"C\")) = 0.51, meaning, \"The probability that either \"A\", or else if \"B\", then \"C\", is 51%\".\n\nUnfortunately, philosopher David Lewis showed that in orthodox probability theory, only certain trivial Boolean algebras with very few elements contain, for any given \"A\" and \"B\", an event \"X\" such that \"P\"(\"X\") = \"P\"(\"B\"|\"A\") is true for any probability function \"P\". Later extended by others, this result stands as a major obstacle to any talk about logical objects that can be the bearers of conditional probabilities.\n\nThe classification of an algebra makes no reference to the nature of the objects in the domain, being entirely a matter of the formal behavior of the operations on the domain. However, investigation of the properties of an algebra often proceeds by assuming the objects to have a particular character. Thus, the canonical Boolean algebra is, as described above, an algebra of subsets of a universe set. What Lewis in effect showed is what can and cannot be done with an algebra whose members behave like members of such a set of subsets.\n\nConditional event algebras circumvent the obstacle identified by Lewis by using a nonstandard domain of objects. Instead of being members of a set \"F\" of subsets of some universe set Ω, the canonical objects are normally higher-level constructions of members of \"F.\" The most natural construction, and historically the first, uses ordered pairs of members of \"F.\" Other constructions use sets of members of \"F\" or infinite sequences of members of \"F\".\n\nSpecific types of CEA include the following (listed in order of discovery):\n\nCEAs differ in their formal properties, so that they cannot be considered a single, axiomatically characterized class of algebra. Goodman-Nguyen-van Frassen algebras, for example, are Boolean while Calabrese algebras are non-distributive. The latter, however, support the intuitively appealing identity \"A\" → (\"B\" → \"C\") = (\"A\" ∩ \"B\") → \"C\", while the former do not.\n\nGoodman, I. R., R. P. S. Mahler, and H. T. Nguyen. 1999. \"What is conditional event algebra and why should you care?\" \"SPIE Proceedings\", Vol 3720.\n\nLewis, David K. 1976. \"Probabilities of conditionals and conditional probabilities\". \"Philosophical Review\" 85: 297-315.\n"}
{"id": "13673454", "url": "https://en.wikipedia.org/wiki?curid=13673454", "title": "Conference Board of the Mathematical Sciences", "text": "Conference Board of the Mathematical Sciences\n\nThe Conference Board of the Mathematical Sciences (CBMS) is an umbrella organization of seventeen professional societies in the mathematical sciences in the United States. \nIt and its member societies are recognized by the International Mathematical Union as the national mathematical societies for their country.\n\nThe CBMS was founded in 1960 as the successor organization to the six-organization Policy Committee for Mathematics (founded by the American Mathematical Society and the Mathematical Association of America as the War Policy Committee in 1942) and the 1958 Conference Organization of the Mathematical Sciences. As well as representing US mathematics at the IMU, it acts as a communication channel between its member societies and the US Government, and coordinates joint projects of its member societies.\n\n\n"}
{"id": "2618686", "url": "https://en.wikipedia.org/wiki?curid=2618686", "title": "Conformal Killing equation", "text": "Conformal Killing equation\n\nIn conformal geometry, the conformal Killing equation on a manifold of space-dimension \"n\" with metric formula_1 describes those vector fields formula_2 which preserve formula_1 up to scale, i.e.\nfor some function formula_5 (where formula_6 is the Lie derivative). Vector fields that satisfy the conformal Killing equation are exactly those vector fields whose flow preserves the conformal structure of the manifold. The name Killing refers to Wilhelm Killing, who first investigated the Killing equation for vector fields that preserve a Riemannian metric.\n\nBy taking the trace we find that necessarily formula_7. Therefore we can write the conformal Killing equation as\nIn abstract indices,\nwhere the round brackets denote symmetrization.\n\nFor any but 2, there is a finite number of solutions, specifying the conformal symmetry of that space, but in two dimensions, there is an infinity of solutions.\n\n"}
{"id": "8556497", "url": "https://en.wikipedia.org/wiki?curid=8556497", "title": "Contraposition", "text": "Contraposition\n\nIn logic, contraposition is an inference that says that a conditional statement is logically equivalent to its contrapositive. The contrapositive of the statement has its antecedent and consequent inverted and flipped: the contrapositive of formula_1 is thus formula_2. For instance, the proposition \"\"All cats are mammals\" can be restated as the conditional \"If something is a cat, then it is a mammal\". The law of contraposition says that statement is identical to the contrapositive \"If something is not a mammal, then it is not a cat\".\"\n\nThe contrapositive can be compared with three other relationships between conditional statements:\n\n\nNote that if formula_1 is true and we are given that \"Q\" is false, formula_7, it can logically be concluded that \"P\" must be false, formula_8. This is often called the \"law of contrapositive\", or the \"modus tollens\" rule of inference.\n\nConsider the Euler diagram shown. According to this diagram, if something is in A, it must be in B as well. So we can interpret \"all of A is in B\" as:\n\nIt is also clear that anything that is not within B (the blue region) cannot be within A, either. This statement,\n\nis the contrapositive. Therefore, we can say that\n\nPractically speaking, this makes trying to prove something easier. For example, if we want to prove that every girl in the United States (A) has brown hair (B), we can try to directly prove formula_9 by checking all girls in the United States to see if they all have brown hair. Alternatively, we can try to prove formula_10 by checking all girls without brown hair to see if they are all outside the US. This means that if we find at least one girl without brown hair within the US, we will have disproved formula_10, and equivalently formula_9.\n\nTo conclude, for any statement where A implies B, then \"not B\" always implies \"not A\". Proving or disproving either one of these statements automatically proves or disproves the other. They are fully equivalent.\n\nA proposition \"Q\" is implicated by a proposition \"P\" when the following relationship holds:\n\nThis states that, \"if \"P\", then \"Q\"\", or, \"if \"Socrates is a man\", then \"Socrates is human\".\" In a conditional such as this, \"P\" is the antecedent, and \"Q\" is the consequent. One statement is the contrapositive of the other only when its antecedent is the negated consequent of the other, and vice versa. The contrapositive of the example is\n\nThat is, \"If not-\"Q\", then not-\"P\"\", or, more clearly, \"If \"Q\" is not the case, then \"P\" is not the case.\" Using our example, this is rendered \"If \"Socrates is not human\", then \"Socrates is not a man\".\" This statement is said to be \"contraposed\" to the original and is logically equivalent to it. Due to their logical equivalence, stating one effectively states the other; when one is true, the other is also true. Likewise with falsity.\n\nStrictly speaking, a contraposition can only exist in two simple conditionals. However, a contraposition may also exist in two complex conditionals, if they are similar. Thus, formula_18, or \"All \"P\"s are \"Q\"s,\" is contraposed to formula_19, or \"All non-\"Q\"s are non-\"P\"s.\"\n\nIn first-order logic, the conditional is defined as: \n\nWe have:\n\nLet:\n\nIt is given that, if A is true, then B is true, and it is also given that B is not true. We can then show that A must not be true by contradiction. For, if A were true, then B would have to also be true (given). However, it is given that B is not true, so we have a contradiction. Therefore, A is not true (assuming that we are dealing with concrete statements that are either true or not true):\n\nWe can apply the same process the other way round:\n\nWe also know that B is either true or not true. If B is not true, then A is also not true. However, it is given that A is true; so, the assumption that B is not true leads to contradiction and must be false. Therefore, B must be true:\n\nCombining the two proved statements makes them logically equivalent:\n\nLogical equivalence between two propositions means that they are true together or false together. To prove that contrapositives are logically equivalent, we need to understand when material implication is true or false.\n\nThis is only false when \"P\" is true and \"Q\" is false. Therefore, we can reduce this proposition to the statement \"False when \"P\" and not-\"Q\"\" (i.e. \"True when it is not the case that \"P\" and not-\"Q\"\"):\n\nThe elements of a conjunction can be reversed with no effect (by commutativity):\n\nWe define formula_30 as equal to \"formula_7\", and formula_32 as equal to formula_8 (from this, formula_34 is equal to formula_35, which is equal to just formula_36):\n\nThis reads \"It is not the case that (\"R\" is true and \"S\" is false)\", which is the definition of a material conditional. We can then make this substitution:\n\nWhen we swap our definitions of \"R\" and \"S\", we arrive at the following:\n\nTake the statement \"\"All red objects have color.\" This can be equivalently expressed as \"If an object is red, then it has color.\"\n\n\nIn other words, the contrapositive is logically equivalent to a given conditional statement, though not sufficient for a biconditional.\n\nSimilarly, take the statement \"All quadrilaterals have four sides,\" or equivalently expressed \"If a polygon is a quadrilateral, then it has four sides.\"\n\nSince the statement and the converse are both true, it is called a biconditional, and can be expressed as \"A polygon is a quadrilateral \"if, and only if,\" it has four sides.\" (The phrase \"if and only if\" is sometimes abbreviated \"iff\".) That is, having four sides is both necessary to be a quadrilateral, and alone sufficient to deem it a quadrilateral.\n\n\nBecause the contrapositive of a statement always has the same truth value (truth or falsity) as the statement itself, it can be a powerful tool for proving mathematical theorems. A proof by contraposition (contrapositive) is a direct proof of the contrapositive of a statement. However, indirect methods such as proof by contradiction can also be used with contraposition, as, for example, in the proof of the irrationality of the square root of 2. By the definition of a rational number, the statement can be made that \"\"If formula_40 is rational, then it can be expressed as an irreducible fraction\". This statement is true because it is a restatement of a definition. The contrapositive of this statement is \"If formula_40 cannot be expressed as an irreducible fraction, then it is not rational\"\". This contrapositive, like the original statement, is also true. Therefore, if it can be proven that formula_40 cannot be expressed as an irreducible fraction, then it must be the case that formula_40 is not a rational number. The latter can be proved by contradiction.\n\nThe previous example employed the contrapositive of a definition to prove a theorem. One can also prove a theorem by proving the contrapositive of the theorem's statement. To prove that \"if a positive integer \"N\" is a non-square number, its square root is irrational\", we can equivalently prove its contrapositive, that \"if a positive integer \"N\" has a square root that is rational, then \"N\" is a square number.\" This can be shown by setting equal to the rational expression \"a/b\" with \"a\" and \"b\" being positive integers with no common prime factor, and squaring to obtain \"N\" = \"a\"/\"b\" and noting that since \"N\" is a positive integer \"b\"=1 so that \"N\" = \"a\", a square number.\n\n\"Contraposition\" represents an instance of Bayes' theorem which in a specific form can be expressed as:\n\nformula_44.\n\nIn the equation above the conditional probability formula_45 generalizes the logical statement formula_46, i.e. in addition to assigning TRUE or FALSE we can also assign any probability to the statement. The term formula_47 denotes the base rate (aka. the prior probability) of formula_36. Assume that formula_49 is equivalent to formula_50 being TRUE, and that formula_51 is equivalent to formula_46 being FALSE. It is then easy to see that formula_53 when formula_54 i.e. when formula_55 is TRUE. This is because formula_56 so that the fraction on the right-hand side of the equation above is equal to 1, and hence formula_57 which is equivalent to formula_58 being TRUE. Hence, Bayes' theorem represents a generalization of \"contraposition\" .\n\n\"Contraposition\" represents an instance of the subjective Bayes' theorem in subjective logic expressed as:\n\nformula_59 which is equivalent to formula_58 being TRUE. Hence, the subjective Bayes' theorem represents a generalization of both \"contraposition\" and Bayes' theorem .\n\n\n"}
{"id": "1286849", "url": "https://en.wikipedia.org/wiki?curid=1286849", "title": "Correspondence theorem (group theory)", "text": "Correspondence theorem (group theory)\n\nIn the area of mathematics known as group theory, the correspondence theorem, sometimes referred to as the fourth isomorphism theorem or the lattice theorem, states that if formula_1 is a normal subgroup of a group formula_2, then there exists a bijection from the set of all subgroups formula_3 of formula_2 containing formula_1, onto the set of all subgroups of the quotient group formula_6. The structure of the subgroups of formula_6 is exactly the same as the structure of the subgroups of formula_2 containing formula_1, with formula_1 collapsed to the identity element.\n\nSpecifically, if \nthen there is a bijective map formula_14 such that\n\nOne further has that if \"A\" and \"B\" are in formula_11, and \"A' = A/N\" and \"B' = B/N\", then\n\nThis list is far from exhaustive. In fact, most properties of subgroups are preserved in their images under the bijection onto subgroups of a quotient group.\n\nMore generally, there is a monotone Galois connection formula_32 between the lattice of subgroups of formula_2 (not necessarily containing formula_1) and the lattice of subgroups of formula_6: the lower adjoint of a subgroup formula_36 of formula_2 is given by formula_38 and the upper adjoint of a subgroup formula_39 of formula_6 is a given by formula_41. The associated closure operator on subgroups of formula_2 is formula_43; the associated kernel operator on subgroups of formula_6 is the identity.\nSimilar results hold for rings, modules, vector spaces, and algebras.\n\n"}
{"id": "663184", "url": "https://en.wikipedia.org/wiki?curid=663184", "title": "DEAL", "text": "DEAL\n\nIn cryptography, DEAL (Data Encryption Algorithm with Larger blocks) is a symmetric block cipher derived from the Data Encryption Standard (DES). The design was proposed in a report by Lars Knudsen in 1998, and was submitted to the AES contest by Richard Outerbridge (who notes that Knudsen had presented the design at the SAC conference in 1997).\n\nDEAL is a Feistel network which uses DES as the round function. It has a 128-bit block size and a variable key size of either 128, 192, or 256 bits. For key sizes of 128 and 192 bits, the cipher uses 6 rounds, increasing to 8 for the 256-bits size. The scheme has a comparable performance to Triple DES, and was relatively slow compared to many other AES candidates.\n\n\n\n"}
{"id": "30781131", "url": "https://en.wikipedia.org/wiki?curid=30781131", "title": "David Buchsbaum", "text": "David Buchsbaum\n\nDavid Alvin Buchsbaum (born November 6, 1929) is a mathematician at Brandeis University who works on commutative algebra, homological algebra, and representation theory. He proved the Auslander–Buchsbaum formula and the Auslander–Buchsbaum theorem.\n\nBuchsbaum earned his Ph.D. under Samuel Eilenberg in 1954 from Columbia University with thesis \"Exact Categories and Duality\". Among his doctoral students are Peter J. Freyd and Hema Srinivasan. In 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "45462233", "url": "https://en.wikipedia.org/wiki?curid=45462233", "title": "Device-independent quantum cryptography", "text": "Device-independent quantum cryptography\n\nA quantum cryptographic protocol is device-independent if its security does not rely on trusting that the quantum devices used are truthful.\nThus the security analysis of such a protocol needs to consider scenarios of imperfect or even malicious devices. Several important problems have been shown to admit unconditional secure and device-independent protocols. A closely related topic (that is not discussed in this article) is measurement-device independent quantum key distribution.\n\nMayers and Yao proposed the idea of designing quantum protocols using \"self-testing\" quantum apparatus, the internal operations of which can be uniquely determined by their input-output statistics. Subsequently, Roger Colbeck in his Thesis proposed the use of Bell tests for checking the honesty of the devices. Since then, several problems have been shown to admit unconditional secure and device-independent protocols, even when the actual devices performing the Bell test are substantially \"noisy,\" i.e., far from being ideal. These problems include\nquantum key distribution, randomness expansion, and randomness amplification.\n\nThe goal of quantum key distribution is for two parties, Alice and Bob, to share a common secret string through communications over public channels. This was a problem of central interest in quantum cryptography. It was also the motivating problem in Mayers and Yao's paper. A long sequence of works aim to prove unconditional security with robustness. Vazirani and Vidick were the first to reach this goal. Subsequently, Miller and Shi proved a similar result using a different approach.\n\nThe goal of randomness expansion is to generate a longer private random string starting from a uniform input string and using untrusted quantum devices. The idea of using Bell test to achieve this goal was first proposed by Roger Colbeck in his Ph.D. Thesis. Subsequent works have aimed to prove unconditional security with robustness and the increase the rate of expansion. Vazrani and Vidick were the first to prove full quantum security for an exponentially expanding protocol.\nMiller and Shi achieved several additional features, including cryptographic level security, robustness, and a single-qubit requirement on the quantum memory. \nThe approach was subsequently extended by the same authors to show that the noise level can approach the obvious upper bound, when the output may become deterministic.\n\nThe goal of randomness amplification is to generate near-perfect randomness (approximating a fair coin toss) starting from a single source of weak randomness (a coin each of whose tosses is somewhat unpredictable, though it may be biased and correlated with previous tosses). This is known to be impossible classically. However, by using quantum devices, it becomes possible even if the devices are untrusted. Roger Colbeck and Renato Renner were motivated by physics considerations to ask the question first. Their construction and the subsequent improvement by Gallego et al. are secure against a non-signalling adversary, and have significant physical interpretations.\nThe first construction that does not require any structural assumptions on the weak source is due to Chung, Shi, and Wu.\n"}
{"id": "3156850", "url": "https://en.wikipedia.org/wiki?curid=3156850", "title": "Discount points", "text": "Discount points\n\nDiscount points, also called mortgage points or simply points, are a form of pre-paid interest available in the United States when arranging a mortgage. One point equals one percent of the loan amount. By charging a borrower points, a lender effectively increases the yield on the loan above the amount of the stated interest rate. Borrowers can offer to pay a lender points as a method to reduce the interest rate on the loan, thus obtaining a lower monthly payment in exchange for this up-front payment. For each point purchased, the loan rate is typically reduced by anywhere from 1/8% (0.125%) to 1/4% (0.25%).\n\nSelling the property or refinancing prior to this break-even point will result in a net financial loss for the buyer while keeping the loan for longer than this break-even point will result in a net financial savings for the buyer. The longer you keep the property financed under the loan with purchased points, the more the money spent on the points will pay off. Accordingly, if the intention is to buy and sell the property or refinance in a rapid fashion, paying points is actually going to end up costing more than just paying the loan at the higher interest rate.\n\nPoints may also be purchased to reduce the monthly payment for the purpose of qualifying for a loan. Loan qualification based on monthly income versus the monthly loan payment may sometimes only be achievable by reducing the monthly payment through the purchasing of points to buy down the interest rate, thereby reducing the monthly loan payment.\n\nDiscount points may be different from origination fee, mortgage arrangement fee or broker fee. Discount points are always used to buy down the interest rates, while origination fees sometimes are fees the lender charges for the loan or sometimes just another name for buying down the interest rate. Origination fee and discount points are both items listed under lender-charges on the HUD-1 Settlement Statement.\n\nThe difference in savings over the life of the loan can make paying points a benefit to the borrower. If you intend to stay in your home for an extended period of time, it may be worthwhile to pay additional points in order to obtain a lower interest rate. Any significant changes in fees should be re-disclosed in the final good faith estimate (GFE).\n\nAlso directly related to points is the concept of the 'no closing cost loan', in which the consumer accepts a higher interest rate in return for the lender paying the loan's closing costs up front. In some cases a purchaser can negotiate with the seller to get them to pay seller's points which can be used to pay mortgage points.\n\n"}
{"id": "30628775", "url": "https://en.wikipedia.org/wiki?curid=30628775", "title": "Earthscore", "text": "Earthscore\n\nEarthscore is a notational system that enables collaborating videographers to produce a shared perception of environmental realities. The system optimizes the use of video and television in the context of the environmental movement by incorporating the cybernetic ideas of Gregory Bateson and the semiotics of Charles Sanders Peirce. The intent of the system is to generate human behaviors that comply with the self-correcting mechanisms of the\nEarth. Earthscore has been studied and utilized by university students and academics worldwide since 1992.\n\nEarthscore was developed and created by The New School professor and artist Paul Ryan, and originally published by NASA in 1990.\n\nIn its most complete and succinct iteration, Earthscore is a notational system with five components. These components are:\n\nUsing the trikonic categories of firstness, secondness, and thirdness developed by American philosopher Charles Sanders Peirce, Earthscore splits knowledge into three modes of being: firstness (positive quality), secondness (actual fact), and thirdness (laws that will govern facts in the future), and defines these categories as \"a theory of everything\".\n\nFirstness is positive quality in the realm of spontaneity, freshness, possibility, and freedom. Earthscore defines firstness as being \"as is\", without regard for any other. Examples include: the taste of banana, warmth, redness, and feeling gloomy.\n\nSecondness is a two-sided consciousness of effort and resistance engendered by being up against brute facts. Earthscore defines secondness as the \"facticity\", or \"thisness\", of something, as it exists, here and now, without rhyme or reason. Examples include: pushing against an unlocked door and meeting silent, unseen resistance.\n\nThirdness mediates between secondness and firstness, between fact and possibility. \"Earthscore\" defines thirdness as the realm of habit and laws that will govern facts in the future, and posits that a knowledge of thirdness can allow predictions of how certain future events will turn out. It is an 'if...then' sort of knowledge. Thirdness consists in the reality that future facts of secondness will conform to general laws.\n\nThe relational circuit is a self-penetrating, tubular continuum with six unambiguous positions which attempts to supply a topological continuum to the trikonic categories. The circuit organizes differences in terms of firstness, secondness, and thirdness, and three \"in-between\" positions that connect them within the continuum. The relational circuit is to the Earthscore System what the staff and bars are to classical music notation, and is the basis of the third component, threeing.\n\nThreeing is a practice in which three people take turns playing three different roles; initiator, respondent and mediator, in an attempt to solve relational confusion. The roles correspond to the categories of firstness, secondness, and thirdness. Through role playing, the three individuals interact with each other spontaneously and recursively, following the relational circuit.\n\nThreeing is further broken down into ten different subsections:\n\n\nEarthscore utilizes the Firstness of Thirdness as a means of creativity, in an attempt to imagine an ecologically sustainable life before living it. By incorporating pure firstness as the realm of spontaneity, Earthscore uses the disciplines of Zen, yoga, and T'ai chi to cultivate the human capacity to be comfortable in pure firstness and yield new insights and visions.\n\nEarthscore approaches knowledge as a process of generating signs, and incorporates the semiotic system of Charles Sanders Peirce consistent with the trikonic categories: A sign (firstness) representing an object (secondness) for an interpretant (thirdness). Earthscore exfoliates the threefold division into a sixty-six-fold classification of signs that is inclusive of everything in nature, in order to systematize both interdisciplinary and multimedia representations of ecosystems.\n\n\n"}
{"id": "28197279", "url": "https://en.wikipedia.org/wiki?curid=28197279", "title": "Efficient frontier", "text": "Efficient frontier\n\nIn modern portfolio theory, the efficient frontier (or portfolio frontier) is an investment portfolio which occupies the 'efficient' parts of the risk-return spectrum. Formally, it is the set of portfolios which satisfy the condition that no other portfolio exists with a higher expected return but with the same standard deviation of return. The efficient frontier was first formulated by Harry Markowitz in 1952. \n\nA combination of assets, i.e. a portfolio, is referred to as \"efficient\" if it has the best possible expected level of return for its level of risk (which is represented by the standard deviation of the portfolio's return). Here, every possible combination of risky assets can be plotted in risk–expected return space, and the collection of all such possible portfolios defines a region in this space. In the absence of the opportunity to hold a risk-free asset, this region is the opportunity set (the feasible set). The positively sloped (upward-sloped) top boundary of this region is a portion of a hyperbola and is called the \"efficient frontier.\" \n\nIf a risk-free asset is also available, the opportunity set is larger, and its upper boundary, the efficient frontier, is a straight line segment emanating from the vertical axis at the value of the risk-free asset's return and tangent to the risky-assets-only opportunity set. All portfolios between the risk-free asset and the tangency portfolio are portfolios composed of risk-free assets and the tangency portfolio, while all portfolios on the linear frontier above and to the right of the tangency portfolio are generated by borrowing at the risk-free rate and investing the proceeds into the tangency portfolio.\n\n"}
{"id": "1440695", "url": "https://en.wikipedia.org/wiki?curid=1440695", "title": "Elementary symmetric polynomial", "text": "Elementary symmetric polynomial\n\nIn mathematics, specifically in commutative algebra, the elementary symmetric polynomials are one type of basic building block for symmetric polynomials, in the sense that any symmetric polynomial can be expressed as a polynomial in elementary symmetric polynomials. That is, any symmetric polynomial is given by an expression involving only additions and multiplication of constants and elementary symmetric polynomials. There is one elementary symmetric polynomial of degree in variables for each nonnegative integer , and it is formed by adding together all distinct products of distinct variables.\n\nThe elementary symmetric polynomials in variables , written for , are defined by\nand so forth, ending with\nIn general, for we define\nso that if .\n\nThus, for each non-negative integer less than or equal to there exists exactly one elementary symmetric polynomial of degree in variables. To form the one that has degree , we take the sum of all products of -subsets of the variables. (By contrast, if one performs the same operation using \"multisets\" of variables, that is, taking variables with repetition, one arrives at the complete homogeneous symmetric polynomials.)\n\nGiven an integer partition (that is, a finite decreasing sequence of positive integers) , one defines the symmetric polynomial , also called an elementary symmetric polynomial, by\n\nSometimes the notation is used instead of .\n\nThe following lists the elementary symmetric polynomials for the first four positive values of . (In every case, is also one of the polynomials.)\n\nFor :\n\nFor : \n\nFor :\n\nFor :\n\nThe elementary symmetric polynomials appear when we expand a linear factorization of a monic polynomial: we have the identity \nThat is, when we substitute numerical values for the variables , we obtain the monic univariate polynomial (with variable ) whose roots are the values substituted for and whose coefficients are up to their sign the elementary symmetric polynomials. These relations between the roots and the coefficients of a polynomial are called Vieta's formulas.\n\nThe characteristic polynomial of a square matrix is an example of application of Vieta's formulas. The roots of this polynomial are the eigenvalues of the matrix. When we substitute these eigenvalues into the elementary symmetric polynomials, we obtain, up to their sign, the coefficients of the characteristic polynomial, which are invariants of the matrix. In particular, the trace (the sum of the elements of the diagonal) is the value of , and thus the sum of the eigenvalues. Similarly, the determinant is, up to the sign, the constant term of the characteristic polynomial; more precisely the determinant is the value of . Thus the determinant of a square matrix is the product of the eigenvalues.\n\nThe set of elementary symmetric polynomials in variables generates the ring of symmetric polynomials in variables. More specifically, the ring of symmetric polynomials with integer coefficients equals the integral polynomial ring . (See below for a more general statement and proof.) This fact is one of the foundations of invariant theory. For other systems of symmetric polynomials with a similar property see power sum symmetric polynomials and complete homogeneous symmetric polynomials.\n\nFor any commutative ring , denote the ring of symmetric polynomials in the variables with coefficients in by . This is a polynomial ring in the \"n\" elementary symmetric polynomials for . (Note that is not among these polynomials; since , it cannot be member of \"any\" set of algebraically independent elements.)\n\nThis means that every symmetric polynomial has a unique representation\nfor some polynomial . Another way of saying the same thing is that is isomorphic to the polynomial ring through an isomorphism that sends to for .\n\nThe theorem may be proved for symmetric homogeneous polynomials by a double mathematical induction with respect to the number of variables and, for fixed , with respect to the degree of the homogeneous polynomial. The general case then follows by splitting an arbitrary symmetric polynomial into its homogeneous components (which are again symmetric). \n\nIn the case the result is obvious because every polynomial in one variable is automatically symmetric.\n\nAssume now that the theorem has been proved for all polynomials for variables and all symmetric polynomials in variables with degree . Every homogeneous symmetric polynomial in can be decomposed as a sum of homogeneous symmetric polynomials\nHere the \"lacunary part\" is defined as the sum of all monomials in which contain only a proper subset of the variables , i.e., where at least one variable is missing. \n\nBecause is symmetric, the lacunary part is determined by its terms containing only the variables , i.e., which do not contain . More precisely: If and are two homogeneous symmetric polynomials in having the same degree, and if the coefficient of before each monomial which contains only the variables equals the corresponding coefficient of , then and have equal lacunary parts. (This is because every monomial which can appear in a lacunary part must lack at least one variable, and thus can be transformed by a permutation of the variables into a monomial which contains only the variables .)\n\nBut the terms of which contain only the variables are precisely the terms that survive the operation of setting to 0, so their sum equals , which is a symmetric polynomial in the variables that we shall denote by . By the inductive assumption, this polynomial can be written as \nfor some . Here the doubly indexed denote the elementary symmetric polynomials in variables. \n\nConsider now the polynomial \nThen is a symmetric polynomial in , of the same degree as , which satisfies\n(the first equality holds because setting to 0 in gives , for all ). In other words, the coefficient of before each monomial which contains only the variables equals the corresponding coefficient of . As we know, this shows that the lacunary part of coincides with that of the original polynomial . Therefore the difference has no lacunary part, and is therefore divisible by the product of all variables, which equals the elementary symmetric polynomial . Then writing , the quotient is a homogeneous symmetric polynomial of degree less than (in fact degree at most ) which by the inductive assumption can be expressed as a polynomial in the elementary symmetric functions. Combining the representations for and one finds a polynomial representation for . \n\nThe uniqueness of the representation can be proved inductively in a similar way. (It is equivalent to the fact that the polynomials are algebraically independent over the ring .) The fact that the polynomial representation is unique implies that is isomorphic to .\n\nThe following proof is also inductive, but does not involve other polynomials than those symmetric in , and also leads to a fairly direct procedure to effectively write a symmetric polynomial as a polynomial in the elementary symmetric ones. Assume the symmetric polynomial to be homogeneous of degree ; different homogeneous components can be decomposed separately. Order the monomials in the variables lexicographically, where the individual variables are ordered , in other words the dominant term of a polynomial is one with the highest occurring power of , and among those the one with the highest power of , etc. Furthermore parametrize all products of elementary symmetric polynomials that have degree (they are in fact homogeneous) as follows by partitions of . Order the individual elementary symmetric polynomials in the product so that those with larger indices come first, then build for each such factor a column of boxes, and arrange those columns from left to right to form a Young diagram containing boxes in all. The shape of this diagram is a partition of , and each partition of arises for exactly one product of elementary symmetric polynomials, which we shall denote by ) (the is present only because traditionally this product is associated to the transpose partition of ). The essential ingredient of the proof is the following simple property, which uses multi-index notation for monomials in the variables .\n\nLemma. The leading term of is .\n\nNow one proves by induction on the leading monomial in lexicographic order, that any nonzero homogeneous symmetric polynomial of degree can be written as polynomial in the elementary symmetric polynomials. Since is symmetric, its leading monomial has weakly decreasing exponents, so it is some with a partition of . Let the coefficient of this term be , then is either zero or a symmetric polynomial with a strictly smaller leading monomial. Writing this difference inductively as a polynomial in the elementary symmetric polynomials, and adding back to it, one obtains the sought for polynomial expression for .\n\nThe fact that this expression is unique, or equivalently that all the products (monomials) of elementary symmetric polynomials are linearly independent, is also easily proved. The lemma shows that all these products have different leading monomials, and this suffices: if a nontrivial linear combination of the were zero, one focuses on the contribution in the linear combination with nonzero coefficient and with (as polynomial in the variables ) the largest leading monomial; the leading term of this contribution cannot be cancelled by any other contribution of the linear combination, which gives a contradiction.\n\n\n"}
{"id": "35052454", "url": "https://en.wikipedia.org/wiki?curid=35052454", "title": "Eli Shamir", "text": "Eli Shamir\n\nEliahu (Eli) Shamir () is an Israeli mathematician and computer scientist, the Jean and Helene Alfassa Professor Emeritus of Computer Science at the Hebrew University of Jerusalem.\n\nShamir earned his Ph.D. from the Hebrew University in 1963, under the supervision of Shmuel Agmon. After briefly holding faculty positions at the University of California, Berkeley and Northwestern University, he returned to the Hebrew University in 1966, and was promoted to full professor in 1972.\n\nShamir was one of the discoverers of the pumping lemma for context-free languages. He did research in partial differential equations, automata theory, random graphs, computational learning theory, and computational linguistics. He was (with Michael O. Rabin) one of the founders of the computer science program at the Hebrew University.\n\nHe was given his named chair in 1987, and in 2002 a workshop on learning and formal verification was held in his honor at Neve Ilan, Israel.\n\n"}
{"id": "9780066", "url": "https://en.wikipedia.org/wiki?curid=9780066", "title": "Euler summation", "text": "Euler summation\n\nIn the mathematics of convergent and divergent series, Euler summation is a summability method. That is, it is a method for assigning a value to a series, different from the conventional method of taking limits of partial sums. Given a series ∑\"a\", if its Euler transform converges to a sum, then that sum is called the Euler sum of the original series. As well as being used to define values for divergent series, Euler summation can be used to speed the convergence of series.\n\nEuler summation can be generalized into a family of methods denoted (E, \"q\"), where \"q\" ≥ 0. The (E, 1) sum is the ordinary Euler sum. All of these methods are strictly weaker than Borel summation; for \"q\" > 0 they are incomparable with Abel summation.\n\nFor some value \"y\" we may define the Euler sum (if it converges for that value of \"y\") corresponding to a particular formal summation as:\n\nIf the formal sum actually converges, an Euler sum will equal it. But Euler summation is particularly used to accelerate the convergence of alternating series and sometimes it can give a useful meaning to divergent sums.\n\nTo justify the approach notice that for interchanged sum, Euler's summation reduces to the initial series, because\n\nThis method itself cannot be improved by iterated application, as \n\n\n\n"}
{"id": "5508726", "url": "https://en.wikipedia.org/wiki?curid=5508726", "title": "Expansive homeomorphism", "text": "Expansive homeomorphism\n\nIn mathematics, the notion of expansivity formalizes the notion of points moving away from one another under the action of an iterated function. The idea of expansivity is fairly rigid, as the definition of positive expansivity, below, as well as the Schwarz–Ahlfors–Pick theorem demonstrate.\n\nIf formula_1 is a metric space, a homeomorphism formula_2 is said to be expansive if there is a constant \n\ncalled the expansivity constant, such that for every pair of points formula_4 in formula_5 there is an integer formula_6 such that \n\nNote that in this definition, formula_6 can be positive or negative, and so formula_9 may be expansive in the forward or backward directions.\n\nThe space formula_5 is often assumed to be compact, since under that assumption expansivity is a topological property; i.e. if formula_11 is any other metric generating the same topology as formula_12, and if formula_9 is expansive in formula_1, then formula_9 is expansive in formula_16 (possibly with a different expansivity constant).\n\nIf \n\nis a continuous map, we say that formula_5 is positively expansive (or forward expansive) if there is a \n\nsuch that, for any formula_4 in formula_5, there is an formula_22 such that formula_23.\n\nGiven \"f\" an expansive homeomorphism of a compact metric space, the theorem of uniform expansivity states that for every formula_24 and formula_25 there is an formula_26 such that for each pair formula_27 of points of formula_5 such that formula_29, there is an formula_30 with formula_31 such that \n\nwhere formula_33 is the expansivity constant of formula_9 (proof).\n\nPositive expansivity is much stronger than expansivity. In fact, one can prove that if formula_5 is compact and formula_9 is a positively\nexpansive homeomorphism, then formula_5 is finite (proof).\n\n"}
{"id": "11742", "url": "https://en.wikipedia.org/wiki?curid=11742", "title": "Finite set", "text": "Finite set\n\nIn mathematics, a finite set is a set that has a finite number of elements. Informally, a finite set is a set which one could in principle count and finish counting. For example,\nis a finite set with five elements. The number of elements of a finite set is a natural number (a non-negative integer) and is called the cardinality of the set. A set that is not finite is called infinite. For example, the set of all positive integers is infinite:\nFinite sets are particularly important in combinatorics, the mathematical study of counting. Many arguments involving finite sets rely on the pigeonhole principle, which states that there cannot exist an injective function from a larger finite set to a smaller finite set.\n\nFormally, a set is called finite if there exists a bijection\nfor some natural number . The number is the set's cardinality, denoted as ||. The empty set {} or Ø is considered finite, with cardinality zero.\n\nIf a set is finite, its elements may be written — in many ways — in a sequence:\nIn combinatorics, a finite set with elements is sometimes called an \"-set\" and a subset with elements is called a \"-subset\". For example, the set {5,6,7} is a 3-set – a finite set with three elements – and {6,7} is a 2-subset of it.\n\nAny proper subset of a finite set \"S\" is finite and has fewer elements than \"S\" itself. As a consequence, there cannot exist a bijection between a finite set \"S\" and a proper subset of \"S\". Any set with this property is called Dedekind-finite. Using the standard ZFC axioms for set theory, every Dedekind-finite set is also finite, but this implication cannot be proved in ZF (Zermelo Fraenkel axioms with the axiom of choice removed) alone. \nThe axiom of countable choice, a weak version of the axiom of choice, is sufficient to prove this equivalence.\n\nAny injective function between two finite sets of the same cardinality is also a surjective function (a surjection). Similarly, any surjection between two finite sets of the same cardinality is also an injection.\n\nThe union of two finite sets is finite, with\nIn fact:\nMore generally, the union of any finite number of finite sets is finite. The Cartesian product of finite sets is also finite, with:\nSimilarly, the Cartesian product of finitely many finite sets is finite. A finite set with \"n\" elements has 2 distinct subsets. That is, the\npower set of a finite set is finite, with cardinality 2.\n\nAny subset of a finite set is finite. The set of values of a function when applied to elements of a finite set is finite.\n\nAll finite sets are countable, but not all countable sets are finite. (Some authors, however, use \"countable\" to mean \"countably infinite\", so do not consider finite sets to be countable.)\n\nThe free semilattice over a finite set is the set of its non-empty subsets, with the join operation being given by set union.\n\nIn Zermelo–Fraenkel set theory without the axiom of choice (ZF), the following conditions are all equivalent:\n\n\nIf the axiom of choice is also assumed (the axiom of countable choice is sufficient), then the following conditions are all equivalent:\n\n\nGeorg Cantor initiated his theory of sets in order to provide a mathematical treatment of infinite sets. Thus the distinction between the finite and the infinite lies at the core of set theory. Certain foundationalists, the strict finitists, reject the existence of infinite sets and thus recommend a mathematics based solely on finite sets. Mainstream mathematicians consider strict finitism too confining, but acknowledge its relative consistency: the universe of hereditarily finite sets constitutes a model of Zermelo–Fraenkel set theory with the axiom of infinity replaced by its negation.\n\nEven for those mathematicians who embrace infinite sets, in certain important contexts, the formal distinction between the finite and the infinite can remain a delicate matter. The difficulty stems from Gödel's incompleteness theorems. One can interpret the theory of hereditarily finite sets within Peano arithmetic (and certainly also vice versa), so the incompleteness of the theory of Peano arithmetic implies that of the theory of hereditarily finite sets. In particular, there exists a plethora of so-called non-standard models of both theories. A seeming paradox, non-standard models of the theory of hereditarily finite sets contain infinite sets --- but these infinite sets look finite from within the model. (This can happen when the model lacks the sets or functions necessary to witness the infinitude of these sets.) On account of the incompleteness theorems, no first-order predicate, nor even any recursive scheme of first-order predicates, can characterize the standard part of all such models. So, at least from the point of view of first-order logic, one can only hope to describe finiteness approximately.\n\nMore generally, informal notions like set, and particularly finite set, may receive interpretations across a range of formal systems varying in their axiomatics and logical apparatus. The best known axiomatic set theories include Zermelo-Fraenkel set theory (ZF), Zermelo-Fraenkel set theory with the Axiom of Choice (ZFC), Von Neumann–Bernays–Gödel set theory (NBG), Non-well-founded set theory, Bertrand Russell's Type theory and all the theories of their various models. One may also choose among classical first-order logic, various higher-order logics and intuitionistic logic.\n\nA formalist might see the meaning of \"set\" varying from system to system. Some kinds of Platonists might view particular formal systems as approximating an underlying reality.\n\nIn contexts where the notion of natural number sits logically prior to any notion of set, one can define a set \"S\" as finite if \"S\" admits a bijection to some set of natural numbers of the form formula_9. Mathematicians more typically choose to ground notions of number in set theory, for example they might model natural numbers by the order types of finite well-ordered sets. Such an approach requires a structural definition of finiteness that does not depend on natural numbers.\n\nVarious properties that single out the finite sets among all sets in the theory ZFC turn out logically inequivalent in weaker systems such as ZF or intuitionistic set theories. Two definitions feature prominently in the literature, one due to Richard Dedekind, the other to Kazimierz Kuratowski. (Kuratowski's is the definition used above.)\n\nA set \"S\" is called Dedekind infinite if there exists an injective, non-surjective function formula_10. Such a function exhibits a bijection between \"S\" and a proper subset of \"S\", namely the image of \"f\". Given a Dedekind infinite set \"S\", a function \"f\", and an element \"x\" that is not in the image of \"f\", we can form an infinite sequence of distinct elements of \"S\", namely formula_11. Conversely, given a sequence in \"S\" consisting of distinct elements formula_12, we can define a function \"f\" such that on elements in the sequence formula_13 and \"f\" behaves like the identity function otherwise. Thus Dedekind infinite sets contain subsets that correspond bijectively with the natural numbers. Dedekind finite naturally means that every injective self-map is also surjective.\n\nReaders unfamiliar with semi-lattices and other notions of abstract algebra may prefer an entirely elementary formulation. Kuratowski finite means \"S\" lies in the set \"K\"(\"S\"), constructed as follows. Write \"M\" for the set of all subsets \"X\" of \"P\"(\"S\") such that:\nThen \"K\"(\"S\") may be defined as the intersection of \"M\".\n\nIn ZF, Kuratowski finite implies Dedekind finite, but not vice versa. In the parlance of a popular pedagogical formulation, when the axiom of choice fails badly, one may have an infinite family of socks with no way to choose one sock from more than finitely many of the pairs. That would make the set of such socks Dedekind finite: there can be no infinite sequence of socks, because such a sequence would allow a choice of one sock for infinitely many pairs by choosing the first sock in the sequence. However, Kuratowski finiteness would fail for the same set of socks.\n\nIn ZF set theory without the axiom of choice, the following concepts of finiteness for a set \"S\" are distinct. They are arranged in strictly decreasing order of strength. In other words, if a set \"S\" meets one of the criteria in this list, it meets all of the criteria which follow that one. In the absence of the axiom of choice, the reverse implications are all unprovable. If the axiom of choice is assumed, then all of these concepts are equivalent. (Note that none of these definitions need the set of finite ordinal numbers to be defined first. They are all pure \"set-theoretic\" definitions in terms of the equality and element-of relations, not involving ω.)\n\n\nThe forward implications (from strong to weak) are theorems within ZF. Counter-examples to the reverse implications (from weak to strong) are found using model theory.\n\nMost of these finiteness definitions and their names are attributed to by . However, definitions I, II, III, IV and V were presented in , together with proofs (or references to proofs) for the forward implications. At that time, model theory was not sufficiently advanced to find the counter-examples.\n\n\n"}
{"id": "38520847", "url": "https://en.wikipedia.org/wiki?curid=38520847", "title": "Finitely generated object", "text": "Finitely generated object\n\nIn category theory, a finitely generated object is the quotient of a free object over a finite set, in the sense that it is the target of a regular epimorphism from a free object that is free on a finite set.\n\nFor instance, one way of defining a finitely generated group is that it is the image of a group homomorphism from a finitely generated free group.\n\n"}
{"id": "9550415", "url": "https://en.wikipedia.org/wiki?curid=9550415", "title": "Generator (category theory)", "text": "Generator (category theory)\n\nIn category theory in mathematics a family of generators (or family of separators) of a category formula_1 is a collection formula_2 of objects, indexed by some set \"I\", such that for any two morphisms formula_3 in formula_1, if formula_5 then there is some \"i∈I\" and morphism formula_6, such that the compositions formula_7. If the family consists of a single object \"G\", we say it is a generator (or separator). \n\nGenerators are central to the definition of Grothendieck categories.\n\nThe dual concept is called a cogenerator or coseparator.\n\n\n"}
{"id": "10473265", "url": "https://en.wikipedia.org/wiki?curid=10473265", "title": "Harald Ganzinger", "text": "Harald Ganzinger\n\nHarald Ganzinger (31 October 1950, Werneck – 3 June 2004, Saarbrücken) was a German computer scientist who together with Leo Bachmair developed the superposition calculus, which is (as of 2007) used in most of the state-of-the-art automated theorem provers for first-order logic.\n\nHe received his Ph.D. from the Technical University of Munich in 1978. Before 1991 he was a Professor of Computer Science at University of Dortmund. Then he joined the Max Planck Institute for Computer Science in Saarbrücken shortly after it was founded in 1991. Until 2004 he was the Director of the Programming Logics department of the Max Planck Institute for Computer Science and honorary professor at Saarland University. His research group created the SPASS automated theorem prover.\n\nHe received the Herbrand Award in 2004 (posthumous) for his important contributions to automated theorem proving.\n\n\n"}
{"id": "50769397", "url": "https://en.wikipedia.org/wiki?curid=50769397", "title": "Jean-Michel Bony", "text": "Jean-Michel Bony\n\nJean-Michel Bony (born 1 February 1942 in Paris) is a French mathematician, specializing in mathematical analysis. He is known for his work on microlocal analysis and pseudodifferential operators.\n\nBony completed his undergraduate and graduate studies at the École Normale Supérieure, where he received his Ph.D in 1972 with thesis advisor Gustave Choquet. Bony became a professor at the University of Paris-Sud and is now a professor at the École Polytechnique.\n\nHis research deals with microlocal analysis, partial differential equations and potential theory. In 1981 he published important results on paradifferential operators, extending the theory of pseudifferential operators published by Ronald Coifman and Yves Meyer in 1979. Bony applied his theory to the propagation of singularities in solutions of semilinear wave equations.\n\nHe was elected in 1990 a corresponding member and in 2000 a full member of the French Academy of Sciences.\n\nHe was an Invited Speaker at the ICM in 1970 in Nice and in 1983 in Warsaw.\n\n\n\n\n\n"}
{"id": "11195483", "url": "https://en.wikipedia.org/wiki?curid=11195483", "title": "Jeffrey Adams (mathematician)", "text": "Jeffrey Adams (mathematician)\n\nJeffrey David Adams (born 1955) is a mathematician at the University of Maryland who works on unitary representations of reductive Lie groups, and who led the project \"Atlas of Lie groups and representations\" that calculated the characters of the representations of E. The project to calculate the representations of E has been compared to the Human Genome Project in scope. Together with Dan Barbasch and David Vogan, he co-authored a monograph on a geometric approach to the Langlands classification and Arthur's conjectures in the real case.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "50959785", "url": "https://en.wikipedia.org/wiki?curid=50959785", "title": "KHOPCA clustering algorithm", "text": "KHOPCA clustering algorithm\n\nKHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. KHOPCA (formula_1-hop clustering algorithm) provides a fully distributed and localized approach to group elements such as nodes in a network according to their distance from each other. KHOPCA operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.\n\nKHOPCA's clustering process explicitly supports joining and leaving of nodes, which makes KHOPCA suitable for highly dynamic networks. However, it has been demonstrated that KHOPCA also performs in static networks.\n\nBesides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, networked swarming, and real-time data clustering and analysis.\n\nKHOPCA (formula_1-hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable formula_1-hops. A set of local rules describes the state transition between nodes. A node's weight is determined only depending on the current state of its neighbors in communication range. Each node of the network is continuously involved in this process. As result, formula_1-hop clusters are formed and maintained in static as well as dynamic networks.\n\nKHOPCA does not require any predetermined initial configuration. Therefore, a node can potentially choose any weight (between formula_5 and formula_6). However, the choice of the initial configuration does influence the convergence time.\n\nThe prerequisites in the start configuration for the application of the rules are the following.\nThe following rules describe the state transition for a node formula_9 with weight formula_25. These rules have to be executed on each node in the order described here.\n\nThe first rule has the function of constructing an order within the cluster. This happens through a node formula_9 detects the direct neighbor with the highest weight formula_27, which is higher than the node's own weight formula_25. If such a direct neighbor is detected, the node formula_9 changes its own weight to be the weight of the highest weight within the neighborhood subtracted by 1. Applied iteratively, this process creates a top-to-down hierarchical cluster structure.\nif max(W(N(n))) > w_n\n\nThe second rule deals with the situation where nodes in a neighborhood are on the minimum weight level. This situation can happen if, for instance, the initial configuration assigns the minimum weight to all nodes. If there is a neighborhood with all nodes having the minimum weight level, the node formula_9 declares itself as cluster center. Even if coincidently all nodes declare themselves as cluster centers, the conflict situation will be resolved by one of the other rules.\nif max(W(N(n)) == MIN & w_n == MIN\n\nThe third rule describes situations where nodes with leveraged weight values, which are not cluster centers, attract surrounding nodes with lower weights. This behavior can lead to fragmented clusters without a cluster center. In order to avoid fragmented clusters, the node with higher weight value is supposed to successively decrease its own weight with the objective to correct the fragmentation by allowing the other nodes to reconfigure according to the rules. \nif max(W(N(n))) <= w_n && w_n != MAX\n\nThe fourth rule resolves the situation where two cluster centers connect in 1-hop neighborhood and need to decide which cluster center should continue its role as cluster center. Given any specific criterion (e.g., device ID, battery power), one cluster center remains while the other cluster center is hierarchized in 1-hop neighborhood to that new cluster center. The choice of the specific criterion to resolve the decision-making depends on the used application scenario and on the available information. \nif max(W(N(n)) == MAX && w_n == MAX\n\nAn exemplary sequence of state transitions applying the described four rules is illustrated below.\n\nKHOPCA acting in a dynamic 2-D simulation. The geometry is based on a geometric random graph; all existing links are drawn in this network.\n\nKHOPCA also works in a dynamic 3-D environment. The cluster connections are illustrated with bold lines.\n\nIt has been demonstrated that KHOPCA terminates after a finite number of state transitions in static networks.\n"}
{"id": "23864530", "url": "https://en.wikipedia.org/wiki?curid=23864530", "title": "Learning with errors", "text": "Learning with errors\n\nLearning with errors (LWE) is a problem in machine learning that is conjectured to be hard to solve. Introduced by Oded Regev in 2005 (who won the 2018 Gödel Prize for this work), it is a generalization of the parity learning problem. Regev showed, furthermore, that the LWE problem is as hard to solve as several worst-case lattice problems. The LWE problem has recently been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert.\n\nAn algorithm is said to solve the LWE problem if, when given access to samples formula_1 where formula_2 (a vector of formula_3 integers modulo formula_4) and formula_5, with the assurance, for some fixed linear function formula_6 that formula_7 with high probability and deviates from it according to some known noise model, the algorithm can recreate formula_8 or some close approximation of it with high probability.\n\nDenote by formula_9 the additive group on reals modulo one. Denote by formula_10 the distribution on formula_11 obtained by choosing a vector formula_12 uniformly at random, choosing a number formula_13 according to a probability distribution formula_14 on formula_15 and outputting formula_16 for some fixed vector formula_17. Here formula_18 is the standard inner product formula_19, the division is done in the field of reals (or more formally, this \"division by formula_4\" is notation for the group homomorphism formula_21 mapping formula_22 to formula_23), and the final addition is in formula_15.\n\nThe learning with errors problem formula_25 is to find formula_17, given access to polynomially many samples of choice from formula_10.\n\nFor every formula_28, denote by formula_29 the one-dimensional Gaussian with density function formula_30 where formula_31, and let formula_32 be the distribution on formula_15 obtained by considering formula_29 modulo one. The version of LWE considered in most of the results would be formula_35\n\nThe LWE problem described above is the \"search\" version of the problem. In the \"decision\" version (DLWE), the goal is to distinguish between noisy inner products and uniformly random samples from formula_11 (practically, some discretized version of it). Regev showed that the \"decision\" and \"search\" versions are equivalent when formula_4 is a prime bounded by some polynomial in formula_3.\n\nIntuitively, if we have a procedure for the search problem, the decision version can be solved easily: just feed the input samples for the decision problem to the solver for the search problem. Denote the given samples by formula_39. If the solver returns a candidate formula_40, for all formula_41, calculate formula_42. If the samples are from an LWE distribution, then the results of this calculation will be distributed according formula_43, but if the samples are uniformly random, these quantities will be distributed uniformly as well.\n\nFor the other direction, given a solver for the decision problem, the search version can be solved as follows: Recover formula_40 one coordinate at a time. To obtain the first coordinate, formula_45, make a guess formula_46, and do the following. Choose a number formula_47 uniformly at random. Transform the given samples formula_39 as follows. Calculate formula_49. Send the transformed samples to the decision solver.\n\nIf the guess formula_50 was correct, the transformation takes the distribution formula_51 to itself, and otherwise, since formula_4 is prime, it takes it to the uniform distribution. So, given a polynomial-time solver for the decision problem that errs with very small probability, since formula_4 is bounded by some polynomial in formula_3, it only takes polynomial time to guess every possible value for formula_50 and use the solver to see which one is correct.\n\nAfter obtaining formula_45, we follow an analogous procedure for each other coordinate formula_57. Namely, we transform our formula_58 samples the same way, and transform our formula_59 samples by calculating formula_60, where the formula_61 is in the formula_62 coordinate.\n\nPeikert showed that this reduction, with a small modification, works for any formula_4 that is a product of distinct, small (polynomial in formula_3) primes. The main idea is if formula_65, for each formula_66, guess and check to see if formula_57 is congruent to formula_68, and then use the Chinese remainder theorem to recover formula_57.\n\nRegev showed the random self-reducibility of the LWE and DLWE problems for arbitrary formula_4 and formula_43. Given samples formula_72 from formula_51, it is easy to see that formula_74 are samples from formula_75.\n\nSo, suppose there was some set formula_76 such that formula_77, and for distributions formula_78, with formula_79, DLWE was easy.\n\nThen there would be some distinguisher formula_80, who, given samples formula_81, could tell whether they were uniformly random or from formula_78. If we need to distinguish uniformly random samples from formula_51, where formula_40 is chosen uniformly at random from formula_85, we could simply try different values formula_86 sampled uniformly at random from formula_85, calculate formula_74 and feed these samples to formula_80. Since formula_90 comprises a large fraction of formula_85, with high probability, if we choose a polynomial number of values for formula_92, we will find one such that formula_93, and formula_80 will successfully distinguish the samples.\n\nThus, no such formula_90 can exist, meaning LWE and DLWE are (up to a polynomial factor) as hard in the average case as they are in the worst case.\n\nFor a n-dimensional lattice formula_96, let \"smoothing parameter\" formula_97 denote the smallest formula_98 such that formula_99 where formula_100 is the dual of formula_96 and formula_31 is extended to sets by summing over function values at each element in the set. Let formula_103 denote the discrete Gaussian distribution on formula_96 of width formula_61 for a lattice formula_96 and real formula_107. The probability of each formula_108 is proportional to formula_109.\n\nThe \"discrete Gaussian sampling problem\"(DGS) is defined as follows: An instance of formula_110 is given by an formula_3-dimensional lattice formula_96 and a number formula_113. The goal is to output a sample from formula_103. Regev shows that there is a reduction from formula_115 to formula_116 for any function formula_117.\n\nRegev then shows that there exists an efficient quantum algorithm for formula_118 given access to an oracle for formula_35 for integer formula_4 and formula_121 such that formula_122. This implies the hardness for LWE. Although the proof of this assertion works for any formula_4, for creating a cryptosystem, the formula_4 has to be polynomial in formula_3.\n\nPeikert proves that there is a probabilistic polynomial time reduction from the formula_126 problem in the worst case to solving formula_35 using formula_128 samples for parameters formula_121, formula_130, formula_131 and formula_132.\n\nThe LWE problem serves as a versatile problem used in construction of several cryptosystems. In 2005, Regev showed that the decision version of LWE is hard assuming quantum hardness of the lattice problems formula_133 (for formula_134 as above) and formula_135 with formula_136). In 2009, Peikert proved a similar result assuming only the classical hardness of the related problem formula_137. The disadvantage of Peikert's result is that it bases itself on a non-standard version of an easier (when compared to SIVP) problem GapSVP.\n\nRegev proposed a public-key cryptosystem based on the hardness of the LWE problem. The cryptosystem as well as the proof of security and correctness are completely classical. The system is characterized by formula_138 and a probability distribution formula_43 on formula_15. The setting of the parameters used in proofs of correctness and security is\n\nThe cryptosystem is then defined by:\n\nThe proof of correctness follows from choice of parameters and some probability analysis. The proof of security is by reduction to the decision version of LWE: an algorithm for distinguishing between encryptions (with above parameters) of formula_160 and formula_164 can be used to distinguish between formula_167 and the uniform distribution over formula_168\n\nPeikert proposed a system that is secure even against any chosen-ciphertext attack.\n\nThe idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper appeared in 2012 after a provisional patent application was filed in 2012.\n\nThe security of the protocol is proven based on the hardness of solving the LWE problem. In 2014, Peikert presented a key-transport scheme following the same basic idea of Ding's, where the new idea of sending an additional 1-bit signal for rounding in Ding's construction is also used. The \"new hope\" implementation selected for Google's post-quantum experiment, uses Peikert's scheme with variation in the error distribution.\n\n"}
{"id": "1211913", "url": "https://en.wikipedia.org/wiki?curid=1211913", "title": "Least fixed point", "text": "Least fixed point\n\nIn order theory, a branch of mathematics, the least fixed point (lfp or LFP, sometimes also smallest fixed point) of a function from a partially ordered set to itself is the fixed point which is less than each other fixed point, according to the set's order. A function need not have a least fixed point, and cannot have more than one.\n\nFor example, with the usual order on the real numbers, the least fixed point of the real function \"f\"(\"x\") = \"x\"² is \"x\" = 0 (since the only other fixed point is 1 and 0 < 1). In contrast, \"f\"(\"x\") = \"x\"+1 has no fixed point at all, let alone a least one, and \"f\"(\"x\")=\"x\" has infinitely many fixed points, but no least one.\n\nMany fixed-point theorems yield algorithms for locating the least fixed point. Least fixed points often have desirable properties that arbitrary fixed points do not.\n\nIn mathematical logic and computer science, the least fixed point is related to making recursive definitions (see domain theory and/or denotational semantics for details).\n\nImmerman \nand Vardi \nindependently showed the descriptive complexity result that the polynomial-time computable properties of linearly ordered structures are definable in FO(LFP), i.e. in first-order logic with a least fixed point operator. However, FO(LFP) is too weak to express all polynomial-time properties of unordered structures (for instance that a structure has even size).\n\nLet \"G=(V,A)\" be a directed graph and \"v\" be a vertex. The set of nodes accessible from \"v\" can be defined as the set \"S\" which is the least fixed-point for the property: \"v\" belongs to \"S\" and if \"w\" belongs to \"S\" and there is an edge from \"w\" to \"x\", then \"x\" belongs to \"S\".\nThe set of nodes which are co-accessible from \"v\" is defined by a similar least fix-point. On the one hand the strongly connected component of \"v\" is the intersection of those two least fixed-point.\n\nLet \"G\" be a proper context-free grammar. The set \"E\" of symbols which produces the emptyword is defined as the least fixed-point which contains the symbols \"S\" such that formula_1, or such that formula_2 where all symbols formula_3 belongs to \"E\".\n\nGreatest fixed points can also be determined, but they are less commonly used than least fixed points. However, in computer science they, analogously to the least fixed point, give rise to corecursion and codata.\n\n\n"}
{"id": "3125155", "url": "https://en.wikipedia.org/wiki?curid=3125155", "title": "Left-right planarity test", "text": "Left-right planarity test\n\nIn graph theory, a branch of mathematics, the left-right planarity test\nor de Fraysseix–Rosenstiehl planarity criterion is a characterization of planar graphs based on the properties of the depth-first search trees, published by and used by them with Patrice Ossona de Mendez to develop a linear time planarity testing algorithm. In a 2003 experimental comparison of six planarity testing algorithms, this was one of the fastest algorithms tested.\n\nFor any depth-first search of a graph \"G\", the edges\nencountered when discovering a vertex for the first time define a depth-first search tree \"T\" of \"G\". This is a Trémaux tree, meaning that the remaining edges (the cotree) each connect a pair of vertices that are related to each other as an ancestor and descendant in \"T\". Three types of patterns can be used to define two relations between pairs of cotree edges, named the T\"-alike and T\"-opposite relations.\n\nIn the following figures, simple circle nodes represent vertices, double circle nodes represent subtrees, twisted segments represent tree paths, and curved arcs represent cotree edges. The root of each tree is shown at the bottom of the figure. In the first figure, the edges labeled formula_1 and formula_2 are \"T\"-alike, meaning that, at the endpoints nearest the root of the tree, they will both be on the same side of the tree in every planar drawing. In the next two figures, the edges with the same labels are \"T\"-opposite, meaning that they will be on different sides of the tree in every planar drawing.\n\nLet \"G\" be a graph and let \"T\" be a Trémaux tree of \"G\". The graph \"G\" is planar if and only if there exists a partition of the cotree edges of \"G\" into two classes so that any two edges belong to a same class if they are \"T\"-alike and any two edges belong to different classes if they are \"T\"-opposite.\n\nThis characterization immediately leads to an (inefficient) planarity test: determine for all pairs of edges whether they are \"T\"-alike or \"T\"-opposite, form an auxiliary graph that has a vertex for each\nconnected component of \"T\"-alike edges and an edge for each pair of \"T\"-opposite edges, and check whether this auxiliary graph is bipartite. Making this algorithm efficient involves finding a subset of the \"T\"-alike and \"T\"-opposite pairs that is sufficient to carry out this method without determining the relation between all edge pairs in the input graph.\n"}
{"id": "734881", "url": "https://en.wikipedia.org/wiki?curid=734881", "title": "List of examples in general topology", "text": "List of examples in general topology\n\nThis is a list of useful examples in general topology, a field of mathematics.\n\n\n"}
{"id": "19636", "url": "https://en.wikipedia.org/wiki?curid=19636", "title": "Mathematical logic", "text": "Mathematical logic\n\nMathematical logic is a subfield of mathematics exploring the applications of formal logic to mathematics. It bears close connections to metamathematics, the foundations of mathematics, and theoretical computer science. The unifying themes in mathematical logic include the study of the expressive power of formal systems and the deductive power of formal proof systems.\nMathematical logic is often divided into the fields of set theory, model theory, recursion theory, and proof theory. These areas share basic results on logic, particularly first-order logic, and definability. In computer science (particularly in the ACM Classification) mathematical logic encompasses additional topics not detailed in this article; see Logic in computer science for those.\n\nSince its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.\n\nThe \"Handbook of Mathematical Logic\" makes a rough division of contemporary mathematical logic into four areas:\nEach area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.\n\nThe mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.\n\nMathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics (Ferreirós 2001, p. 443). \"Mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.\" Before this emergence, logic was studied with rhetoric, with \"calculationes\", through the syllogism, and with philosophy. The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.\n\nTheories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.\n\nIn the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic. Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics (Katz 1998, p. 686).\n\nCharles Sanders Peirce built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\nGottlob Frege presented an independent development of logic with quantifiers in his \"Begriffsschrift\", published in 1879, a work generally considered as marking a turning point in the history of logic. Frege's work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century. The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.\n\nFrom 1890 to 1905, Ernst Schröder published \"Vorlesungen über die Algebra der Logik\" in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\n\nConcerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.\nIn logic, the term \"arithmetic\" refers to the theory of the natural numbers. Giuseppe Peano (1889) published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege's work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind (1888) proposed a different characterization, which lacked the formal logical character of Peano's axioms. Dedekind's work, however, proved theorems inaccessible in Peano's system, including the uniqueness of the set of natural numbers (up to isomorphism) and the recursive definitions of addition and multiplication from the successor function and mathematical induction.\nIn the mid-19th century, flaws in Euclid's axioms for geometry became known (Katz 1998, p. 774). In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826 (Lobachevsky 1840), mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert (1899) developed a complete set of axioms for geometry, building on previous work by Pasch (1882). The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line. This would prove to be a major area of research in the first half of the 20th century.\n\nThe 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate. Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817 (Felscher 2000), but remained relatively unknown.\nCauchy in 1821 defined continuity in terms of infinitesimals (see Cours d'Analyse, page 34). In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers (Dedekind 1872), a definition still employed in contemporary texts.\n\nGeorg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities (Cantor 1874). Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor's theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895 (Katz 1998, p. 807).\n\nIn the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.\n\nIn 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert's \"Entscheidungsproblem\", posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.\n\nErnst Zermelo (1904) gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof (Zermelo 1908a). This paper led to the general acceptance of the axiom of choice in the mathematics community.\n\nSkepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti (1897) was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell's paradox in 1901, and Jules Richard (1905) discovered Richard's paradox.\n\nZermelo (1908b) provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo's axioms incorporated the principle of limitation of size to avoid Russell's paradox.\n\nIn 1910, the first volume of \"Principia Mathematica\" by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. \"Principia Mathematica\" is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics (Ferreirós 2001, p. 445).\n\nFraenkel (1922) proved that the axiom of choice cannot be proved from the axioms of Zermelo's set theory with urelements. Later work by Paul Cohen (1966) showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen's proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.\n\nLeopold Löwenheim (1915) and Thoralf Skolem (1920) obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem's paradox.\n\nIn his doctoral thesis, Kurt Gödel (1929) proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.\n\nIn 1931, Gödel published \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems\", which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel's incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert's program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic. Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.\n\nGödel's theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen (1936) proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen's result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory. Gödel (1958) gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.\n\nAlfred Tarski developed the basics of model theory.\n\nBeginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words \"bijection\", \"injection\", and \"surjection\", and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.\n\nThe study of computability came to be known as recursion theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing's formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.\n\nNumerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene (1943) introduced the concepts of relative computability, foreshadowed by Turing (1939), and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.\n\nAt its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language. The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties. Stronger classical logics such as second-order logic or infinitary logic are also studied, along with nonclassical logics such as intuitionistic logic.\n\nFirst-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.\n\nEarly results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.\n\nGödel's completeness theorem (Gödel 1929) established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel's proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.\n\nGödel's incompleteness theorems (Gödel 1931) establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.\n\nHere a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called \"sufficiently strong.\" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert's program cannot be completed.\n\nMany logics besides first-order logic are studied. These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.\n\nThe most well studied infinitary logic is formula_1. In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of formula_1 such as\n\nHigher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type. The logics studied before the development of first-order logic, for example Frege's logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.\n\nAnother type of logics are s that allow inductive definitions, like one writes for primitive recursive functions.\n\nOne can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic. Lindström's theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the Downward Löwenheim–Skolem theorem is first-order logic.\n\nModal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability (Solovay 1976) and set-theoretic forcing (Hamkins and Löwe 2007).\n\nIntuitionistic logic was developed by Heyting to study Brouwer's program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true. Kleene's work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.\n\nAlgebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.\n\nSet theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo (1908b), was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.\n\nOther formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF). Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.\n\nTwo famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo (1904), was proved independent of ZF by Fraenkel (1922), but has come to be widely accepted by mathematicians. It states that given a collection of nonempty sets there is a single set \"C\" that contains exactly one element from each set in the collection. The set \"C\" is said to \"choose\" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski (1924) showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.\n\nThe continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory (Cohen 1966). This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear (Woodin 2001).\n\nContemporary research in set theory includes the study of large cardinals and determinacy. Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC. Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line. \"Determinacy\" refers to the possible existence of winning strategies for certain two-player games (the games are said to be \"determined\"). The existence of these strategies implies structural properties of the real line and other Polish spaces.\n\nModel theory studies the models of various formal theories. Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.\n\nThe set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.\n\nThe method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski (1948) established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. (He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic.) A modern subfield developing from this is concerned with o-minimal structures.\n\nMorley's categoricity theorem, proved by Michael D. Morley (1965), states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.\n\nA trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis. Many special cases of this conjecture have been established.\n\nRecursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability. Recursion theory also includes the study of generalized computability and definability. Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.\n\nClassical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems. More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.\n\nGeneralized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.\n\nContemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.\n\nAn important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.\n\nThere are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959. The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.\n\nHilbert's tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970 (Davis 1973).\n\nProof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques. Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.\n\nThe study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems. An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods (Weyl 1918).\n\nBecause proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability. The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest. Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or \"translate\") classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.\n\nRecent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.\n\n\"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski). Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls).\" \"Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).\"\n\nThe study of computability theory in computer science is closely related to the study of computability in mathematical logic. There is a difference of emphasis, however. Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.\n\nThe theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard isomorphism between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.\n\nComputer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.\n\nDescriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.\n\nIn the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.\n\nCantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated \"God made the integers; all else is the work of man,\" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying \"No one shall expel us from the Paradise that Cantor has created.\"\n\nMathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs. In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining \"point\" to mean a point on a fixed sphere and \"line\" to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.\n\nWith the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term \"finitary\" to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.\n\nA second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of \"constructive\". At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. \n\nIn the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a philosophy of mathematics. This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to \"intuit\" the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true. Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.\n\n\n\n\n\n\n\n"}
{"id": "25069727", "url": "https://en.wikipedia.org/wiki?curid=25069727", "title": "Maximum theorem", "text": "Maximum theorem\n\nThe maximum theorem provides conditions for the continuity of an optimized function and the set of its maximizers as a parameter changes. The statement was first proven by Claude Berge in 1959. The theorem is primarily used in mathematical economics.\n\nLet formula_1 and formula_2 be metric spaces, formula_3 be a function jointly continuous in its two arguments, and formula_4 be a compact-valued correspondence.\n\nFor formula_5 in formula_1 and formula_7 in formula_2, let\n\nIf formula_11 is continuous (i.e. both upper and lower hemicontinuous) at some formula_7, then formula_13 is continuous at formula_7 and formula_15 is non-empty, compact-valued, and upper hemicontinuous at formula_7.\n\nThe theorem is typically interpreted as providing conditions for a parametric optimization problem to have continuous solutions with regard to the parameter. In this case, formula_2 is the parameter space, formula_18 is the function to be maximized, and formula_19 gives the constraint set that formula_20 is maximized over. Then, formula_21 is the maximized value of the function and formula_15 is the set of points that maximize formula_20.\n\nThe result is that if the elements of an optimization problem are sufficiently continuous, then some, but not all, of that continuity is preserved in the solutions.\n\nThe proof relies primarily on the sequential definitions of upper and lower hemicontinuity.\n\nBecause formula_11 is compact-valued and formula_20 is continuous, the extreme value theorem guarantees the constrained maximum of formula_20 is well-defined and formula_27 is non-empty for all formula_7 in formula_2. Then, let formula_30 be a sequence converging to formula_7 and formula_32 be a sequence in formula_1. Since formula_11 is upper hemicontinuous, there exists a convergent subsequence formula_35. \n\nIf it is shown that formula_36, then\nwhich would simultaneously prove the continuity of formula_13 and the upper hemicontinuity of formula_15.\n\nSuppose to the contrary that formula_40, i.e. there exists an formula_41 such that formula_42. Because formula_11 is lower hemicontinuous, there is a further subsequence of formula_44 such that formula_45 and formula_46. By the continuity of formula_20 and the contradiction hypothesis,\nBut this implies that for sufficiently large formula_49,\nwhich would mean formula_51 is not a maximizer, a contradiction of formula_32. This establishes the continuity of formula_13 and the upper hemicontinuity of formula_15.\n\nBecause formula_55 and formula_19 is compact, it is sufficient to show formula_15 is closed-valued for it to be compact-valued. This can be done by contradiction using sequences similar to above.\n\nIf in addition to the conditions above, formula_20 is quasiconcave in formula_5 for each formula_7 and formula_11 is convex-valued, then formula_15 is also convex-valued. If formula_20 is strictly quasiconcave in formula_5 for each formula_7 and formula_11 is convex-valued, then formula_15 is single-valued, and thus is a continuous function rather than a correspondence.\n\nIf formula_20 is concave and formula_11 has a convex graph, then formula_13 is concave and formula_15 is convex-valued. Similarly to above, if formula_20 is strictly concave, then formula_15 is a continuous function.\n\nIt is also possible to generalize Berge's theorem to non-compact set-valued correspondences if the objective function is K-inf-compact.\n\nConsider a utility maximization problem where a consumer makes a choice from their budget set. Translating from the notation above to the standard consumer theory notation,\n\nThen, \n\nProofs in general equilibrium theory often apply the Brouwer or Kakutani fixed point theorems to the consumer's demand, which require compactness and continuity, and the maximum theorem provides the sufficient conditions to do so.\n\n\n"}
{"id": "2252716", "url": "https://en.wikipedia.org/wiki?curid=2252716", "title": "Michel Chasles", "text": "Michel Chasles\n\nMichel Floréal Chasles (15 November 1793 – 18 December 1880) was a French mathematician.\n\nHe was born at Épernon in France and studied at the École Polytechnique in Paris under Siméon Denis Poisson. In the War of the Sixth Coalition he was drafted to fight in the defence of Paris in 1814. After the war, he gave up on a career as an engineer or stockbroker in order to pursue his mathematical studies.\n\nIn 1837 he published his \"Historical view of the origin and development of methods in geometry\", a study of the method of reciprocal polars in projective geometry. The work gained him considerable fame and respect and he was appointed Professor at the École Polytechnique in 1841, then he was awarded a chair at the Sorbonne in 1846. A second edition of his book was published in 1875, and Leonhard Sohncke translated the work into German.\n\nJakob Steiner had proposed Steiner's conic problem of enumerating the number of conic sections tangent to each of five given conics, and had answered it incorrectly. Chasles developed a theory of characteristics that enabled the correct enumeration of the conics (there are 3264) (see enumerative geometry). He established several important theorems (all called Chasles's theorem). In kinematics, Chasles's description of a Euclidean motion in space as screw displacement was seminal to the development of the theories of dynamics of rigid bodies.\n\nChasles was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1864. In 1865 he was awarded the Copley Medal.\n\nAs described in \"A Treasury of Deception\", by Michael Farquhar (Peguin Books, 2005), between 1861 and 1869 Chasles purchased some of the 27,000 forged letters from Frenchman Denis Vrain-Lucas. Included in this trove were letters from Alexander the Great to Aristotle, from Cleopatra to Julius Caesar, and from Mary Magdalene to a revived Lazarus, all in a fake medieval French. In 2004, the journal \"Critical Inquiry\" published a recently \"discovered\" 1871 letter written by Vrain-Lucas (from prison) to Chasles, conveying Vrain-Lucas's perspective on these events, itself an invention.\n\nIn 1986, Alexander Jones published a commentary on Book 7 of the \"Collection\" of Pappus of Alexandria, which Chasles had referred to in his history of geometric methods. Jones makes these comments about Chasles, Pappus and Euclid:\n\nChasles's name is one of the 72 names inscribed on the Eiffel Tower.\n\n\n\n"}
{"id": "5007562", "url": "https://en.wikipedia.org/wiki?curid=5007562", "title": "Oblate spheroidal coordinates", "text": "Oblate spheroidal coordinates\n\nOblate spheroidal coordinates are a three-dimensional orthogonal coordinate system that results from rotating the two-dimensional elliptic coordinate system about the non-focal axis of the ellipse, i.e., the symmetry axis that separates the foci. Thus, the two foci are transformed into a ring of radius formula_1 in the \"x\"-\"y\" plane. (Rotation about the other axis produces prolate spheroidal coordinates.) Oblate spheroidal coordinates can also be considered as a limiting case of ellipsoidal coordinates in which the two largest semi-axes are equal in length.\n\nOblate spheroidal coordinates are often useful in solving partial differential equations when the boundary conditions are defined on an oblate spheroid or a hyperboloid of revolution. For example, they played an important role in the calculation of the Perrin friction factors, which contributed to the awarding of the 1926 Nobel Prize in Physics to Jean Baptiste Perrin. These friction factors determine the rotational diffusion of molecules, which affects the feasibility of many techniques such as protein NMR and from which the hydrodynamic volume and shape of molecules can be inferred. Oblate spheroidal coordinates are also useful in problems of electromagnetism (e.g., dielectric constant of charged oblate molecules), acoustics (e.g., scattering of sound through a circular hole), fluid dynamics (e.g., the flow of water through a firehose nozzle) and the diffusion of materials and heat (e.g., cooling of a red-hot coin in a water bath)\n\nThe most common definition of oblate spheroidal coordinates formula_2 is\n\nwhere formula_6 is a nonnegative real number and the angle formula_7. The azimuthal angle formula_8 can fall anywhere on a full circle, between formula_9. These coordinates are favored over the alternatives below because they are not degenerate; the set of coordinates formula_2 describes a unique point in Cartesian coordinates formula_11. The reverse is also true, except on the formula_12-axis and the disk in the formula_13 plane inside the focal ring.\n\nThe surfaces of constant μ form oblate spheroids, by the trigonometric identity\n\nsince they are ellipses rotated about the \"z\"-axis, which separates their foci. An ellipse in the \"x\"-\"z\" plane (Figure 2) has a major semiaxis of length \"a\" cosh μ along the \"x\"-axis, whereas its minor semiaxis has length \"a\" sinh μ along the \"z\"-axis. The foci of all the ellipses in the \"x\"-\"z\" plane are located on the \"x\"-axis at ±\"a\".\n\nSimilarly, the surfaces of constant ν form one-sheet half hyperboloids of revolution by the hyperbolic trigonometric identity\n\nFor positive ν, the half-hyperboloid is above the \"x\"-\"y\" plane (i.e., has positive \"z\") whereas for negative ν, the half-hyperboloid is below the \"x\"-\"y\" plane (i.e., has negative \"z\"). Geometrically, the angle ν corresponds to the angle of the asymptotes of the hyperbola. The foci of all the hyperbolae are likewise located on the \"x\"-axis at ±\"a\".\n\nThe (μ, ν, φ) coordinates may be calculated from the Cartesian coordinates (\"x\", \"y\", \"z\") as follows. The azimuthal angle φ is given by the formula\n\nThe cylindrical radius ρ of the point P is given by\n\nand its distances to the foci in the plane defined by φ is given by\n\nThe remaining coordinates μ and ν can be calculated from the equations\n\nwhere the sign of μ is always non-negative, and the sign of ν is the same as that of \"z\".\n\nAnother method to compute the inverse transform is\n\nwhere\n\nThe scale factors for the coordinates μ and ν are equal\n\nwhereas the azimuthal scale factor equals\n\nConsequently, an infinitesimal volume element equals\n\nand the Laplacian can be written\n\nOther differential operators such as formula_30 and formula_31 can be expressed in the coordinates (μ, ν, φ) by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nThe orthonormal basis vectors for the formula_32 coordinate system can be expressed in Cartesian coordinates as\n\nwhere formula_36 are the Cartesian unit vectors. Here, formula_37 is the outward normal vector to the oblate spheroidal surface of constant formula_6, formula_39 is the same azimuthal unit vector from spherical coordinates, and formula_40 lies in the tangent plane to the oblate spheroid surface and completes the right-handed basis set.\n\nAnother set of oblate spheroidal coordinates formula_41 are sometimes used where formula_42 and formula_43 (Smythe 1968). The curves of constant formula_44 are oblate spheroids and the curves of constant formula_45 are the hyperboloids of revolution. The coordinate formula_44 is restricted by formula_47 and formula_45 is restricted by formula_49.\n\nThe relationship to Cartesian coordinates is\n\nThe scale factors for formula_53 are:\n\nKnowing the scale factors, various functions of the coordinates can be calculated by the general method outlined in the orthogonal coordinates article. The infinitesimal volume element is:\n\nThe gradient is:\n\nThe divergence is:\n\nand the Laplacian equals\n\nAs is the case with spherical coordinates and spherical harmonics, Laplace's equation may be solved by the method of separation of variables to yield solutions in the form of oblate spheroidal harmonics, which are convenient to use when boundary conditions are defined on a surface with a constant oblate spheroidal coordinate.\n\nFollowing the technique of separation of variables, a solution to Laplace's equation is written:\n\nThis yields three separate differential equations in each of the variables:\n\nwhere \"m\" is a constant which is an integer because the φ variable is periodic with period 2π. \"n\" will then be an integer. The solution to these equations are:\n\nwhere the formula_68 are constants and formula_69 and formula_70 are associated Legendre polynomials of the first and second kind respectively. The product of the three solutions is called an \"oblate spheroidal harmonic\" and the general solution to Laplace's equation is written:\n\nThe constants will combine to yield only four independent constants for each harmonic.\n\nAn alternative and geometrically intuitive set of oblate spheroidal coordinates (σ, τ, φ) are sometimes used, where σ = cosh μ and τ = cos ν. Therefore, the coordinate σ must be greater than or equal to one, whereas τ must lie between ±1, inclusive. The surfaces of constant σ are oblate spheroids, as were those of constant μ, whereas the curves of constant τ are full hyperboloids of revolution, including the half-hyperboloids corresponding to ±ν. Thus, these coordinates are degenerate; \"two\" points in Cartesian coordinates (\"x\", \"y\", ±\"z\") map to \"one\" set of coordinates (σ, τ, φ). This two-fold degeneracy in the sign of \"z\" is evident from the equations transforming from oblate spheroidal coordinates to the Cartesian coordinates\n\nThe coordinates formula_75 and formula_76 have a simple relation to the distances to the focal ring. For any point, the \"sum\" formula_77 of its distances to the focal ring equals formula_78, whereas their \"difference\" formula_79 equals formula_80. Thus, the \"far\" distance to the focal ring is formula_81, whereas the \"near\" distance is formula_82.\n\nSimilar to its counterpart μ, the surfaces of constant σ form oblate spheroids\n\nSimilarly, the surfaces of constant τ form full one-sheet hyperboloids of revolution\n\nThe scale factors for the alternative oblate spheroidal coordinates formula_85 are\n\nwhereas the azimuthal scale factor is formula_88.\n\nHence, the infinitesimal volume element can be written\n\nand the Laplacian equals\n\nOther differential operators such as formula_30 and formula_31 can be expressed in the coordinates formula_93 by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nAs is the case with spherical coordinates, Laplaces equation may be solved by the method of separation of variables to yield solutions in the form of oblate spheroidal harmonics, which are convenient to use when boundary conditions are defined on a surface with a constant oblate spheroidal coordinate (See Smythe, 1968).\n\n\n\n\n"}
{"id": "35762367", "url": "https://en.wikipedia.org/wiki?curid=35762367", "title": "Petr–Douglas–Neumann theorem", "text": "Petr–Douglas–Neumann theorem\n\nIn geometry, the Petr–Douglas–Neumann theorem (or the PDN-theorem) is a result concerning arbitrary planar polygons. The theorem asserts that a certain procedure when applied to an arbitrary polygon always yields a regular polygon having the same number of sides as the initial polygon. The theorem was first published by Karel Petr (1868–1950) of Prague in 1908. The theorem was independently rediscovered by Jesse Douglas (1897–1965) in 1940 and also by B H Neumann (1909–2002) in 1941. The naming of the theorem as \"Petr–Douglas–Neumann theorem\", or as the \"PDN-theorem\" for short, is due to Stephen B Gray. This theorem has also been called Douglas’s theorem, the Douglas–Neumann theorem, the Napoleon–Douglas–Neumann theorem and Petr’s theorem.\n\nThe PDN-theorem is a generalisation of the Napoleon's theorem which is concerned about arbitrary triangles and of the van Aubel's theorem which is related to arbitrary quadrilaterals.\n\nThe Petr–Douglas–Neumann theorem asserts the following.\n\nIn the case of triangles, the value of \"n\" is 3 and that of \"n\" − 2 is 1. Hence there is only one possible value for \"k\", namely 1. The specialisation of the theorem to triangles asserts that the triangle A is a regular 3-gon, that is, an equilateral triangle.\n\nA is formed by the apices of the isosceles triangles with apex angle 2π/3 erected over the sides of the triangle A. The vertices of A are the centers of equilateral triangles erected over the sides of triangle A. Thus the specialisation of the PDN theorem to a triangle can be formulated as follows:\n\nThe last statement is the assertion of the Napoleon's theorem.\n\nIn the case of quadrilaterals, the value of \"n\" is 4 and that of \"n\" − 2 is 2. There are two possible values for \"k\", namely 1 and 2, and so two possible apex angles, namely:\n\nAccording to the PDN-theorem the quadrilateral A is a regular 4-gon, that is, a square. The two-stage process yielding the square A can be carried out in two different ways. (The apex \"Z\" of an isosceles triangle with apex angle π erected over a line segment \"XY\" is the midpoint of the line segment \"XY\".)\n\nIn this case the vertices of A are the free apices of isosceles triangles with apex angles π/2 erected over the sides of the quadrilaetral A. The vertices of the quadrilateral A are the midpoints of the sides of the quadrilateral A. By the PDN theorem, A is a square.\n\nThe vertices of the quadrilateral A are the centers of squares erected over the sides of the quadrilateral A. The assertion that quadrilateral A is a square is equivalent to the assertion that the diagonals of A are equal and perpendicular to each other. The latter assertion is the content of van Aubel's theorem.\n\nThus van Aubel's theorem is a special case of the PDN-theorem.\n\nIn this case the vertices of A are the midpoints of the sides of the quadrilateral A and those of A are the apices of the triangles with apex angles π/2 erected over the sides of A. The PDN-theorem asserts that A is a square in this case also.\n\nIn the case of pentagons, we have \"n\" = 5 and \"n\" − 2 = 3. So there are three possible values for \"k\", namely 1, 2 and 3, and hence three possible apex angles for isosceles triangles:\n\nAccording to the PDN-theorem, A is a regular pentagon. The three-stage process leading to the construction of the regular pentagon A can be performed in six different ways depending on the order in which the apex angles are selected for the construction of the isosceles triangles. \n\nThe theorem can be proved using some elementary concepts from linear algebra.\n\nThe proof begins by encoding an \"n\"-gon by a list complex numbers representing the vertices of the \"n\"-gon. This list can be thought of as a vector in the \"n\"-dimensional complex linear space C. Take an \"n\"-gon \"A\" and let it be represented by the complex vector\n\nLet the polygon \"B\" be formed by the free vertices of similar triangles built on the sides of \"A\" and let it be represented by the complex vector\n\nThen we have\n\nThis yields the following expression to compute the \"b\" ' s:\n\nIn terms of the linear operator \"S\" : C → C that cyclically permutes the coordinates one place, we have \n\nThis means that the polygon \"A\" that we need to show is regular is obtained from \"A\" by applying the composition of the following operators:\n\nA polygon \"P\" = ( \"p\", \"p\", ..., \"p\" ) is a regular \"n\"-gon if each side of \"P\" is obtained from the next by rotating through an angle of 2π/\"n\", that is, if \nThis condition can be formulated in terms of S as follows:\n\nOr equivalently as\n\nPetr–Douglas–Neumann theorem now follows from the following computations.\n"}
{"id": "10195749", "url": "https://en.wikipedia.org/wiki?curid=10195749", "title": "Power sum symmetric polynomial", "text": "Power sum symmetric polynomial\n\nIn mathematics, specifically in commutative algebra, the power sum symmetric polynomials are a type of basic building block for symmetric polynomials, in the sense that every symmetric polynomial with rational coefficients can be expressed as a sum and difference of products of power sum symmetric polynomials with rational coefficients. However, not every symmetric polynomial with integral coefficients is generated by integral combinations of products of power-sum polynomials: they are a generating set over the \"rationals,\" but not over the \"integers.\"\n\nThe power sum symmetric polynomial of degree \"k\" in formula_1 variables \"x\", ..., \"x\", written \"p\" for \"k\" = 0, 1, 2, ..., is the sum of all \"k\"th powers of the variables. Formally,\nThe first few of these polynomials are \nThus, for each nonnegative integer formula_7, there exists exactly one power sum symmetric polynomial of degree formula_7 in formula_1 variables. \n\nThe polynomial ring formed by taking all integral linear combinations of products of the power sum symmetric polynomials is a commutative ring.\n\nThe following lists the formula_1 power sum symmetric polynomials of positive degrees up to \"n\" for the first three positive values of formula_11 In every case, formula_12 is one of the polynomials. The list goes up to degree \"n\" because the power sum symmetric polynomials of degrees 1 to \"n\" are basic in the sense of the Main Theorem stated below.\n\nFor \"n\" = 1:\n\nFor \"n\" = 2: \n\nFor \"n\" = 3:\n\nThe set of power sum symmetric polynomials of degrees 1, 2, ..., \"n\" in \"n\" variables generates the ring of symmetric polynomials in \"n\" variables. More specifically: \n\nHowever, this is not true if the coefficients must be integers. For example, for \"n\" = 2, the symmetric polynomial \nhas the expression \nwhich involves fractions. According to the theorem this is the only way to represent formula_22 in terms of \"p\" and \"p\". Therefore, \"P\" does not belong to the integral polynomial ring formula_23\nFor another example, the elementary symmetric polynomials \"e\", expressed as polynomials in the power sum polynomials, do not all have integral coefficients. For instance, \n\nThe theorem is also untrue if the field has characteristic different from 0. For example, if the field \"F\" has characteristic 2, then formula_25, so \"p\" and \"p\" cannot generate \"e\" = \"x\"\"x\".\n\n\"Sketch of a partial proof of the theorem\": By Newton's identities the power sums are functions of the elementary symmetric polynomials; this is implied by the following recurrence relation, though the explicit function that gives the power sums in terms of the \"e\" is complicated:\n\nRewriting the same recurrence, one has the elementary symmetric polynomials in terms of the power sums (also implicitly, the explicit formula being complicated):\n\nThis implies that the elementary polynomials are rational, though not integral, linear combinations of the power sum polynomials of degrees 1, ..., \"n\". \nSince the elementary symmetric polynomials are an algebraic basis for all symmetric polynomials with coefficients in a field, it follows that every symmetric polynomial in \"n\" variables is a polynomial function formula_28 of the power sum symmetric polynomials \"p\", ..., \"p\". That is, the ring of symmetric polynomials is contained in the ring generated by the power sums, formula_19 Because every power sum polynomial is symmetric, the two rings are equal.\n\nFor another system of symmetric polynomials with similar properties see complete homogeneous symmetric polynomials.\n\n\n"}
{"id": "1622235", "url": "https://en.wikipedia.org/wiki?curid=1622235", "title": "Probability distribution function", "text": "Probability distribution function\n\nA probability distribution function is some function that may be used to define a particular probability distribution. Depending upon which text is consulted, the term may refer to:\nThe similar term probability function may mean any of the above and, in addition,\n"}
{"id": "23493177", "url": "https://en.wikipedia.org/wiki?curid=23493177", "title": "Proofs involving ordinary least squares", "text": "Proofs involving ordinary least squares\n\nThe purpose of this page is to provide supplementary materials for the ordinary least squares article, reducing the load of the main article with mathematics and improving its accessibility, while at the same time retaining the completeness of exposition.\n\nDefine the formula_1th residual to be\n\nThen formula_3 can be rewritten\n\nGiven that \"S\" is convex, it is minimized when its gradient vector is zero (This follows by definition: if the gradient vector is not zero, there is a direction in which we can move to minimize it further – see maxima and minima.) The elements of the gradient vector are the partial derivatives of \"S\" with respect to the parameters:\n\nThe derivatives are\n\nSubstitution of the expressions for the residuals and the derivatives into the gradient equations gives\n\nThus if formula_8 minimizes \"S\", we have\n\nUpon rearrangement, we obtain the normal equations:\n\nThe normal equations are written in matrix notation as\n\nThe solution of the normal equations yields the vector formula_12 of the optimal parameter values.\n\nThe normal equations can be derived directly from a matrix representation of the problem as follows. The objective is to minimize\n\nHere formula_14 has the dimension 1x1 (the number of columns of formula_15), so it is a scalar and equal to its own transpose, hence formula_16\nand the quantity to minimize becomes\n\nDifferentiating this with respect to formula_18 and equating to zero to satisfy the first-order conditions gives\n\nwhich is equivalent to the above-given normal equations. A sufficient condition for satisfaction of the second-order conditions for a minimum is that formula_20 have full column rank, in which case formula_21 is positive definite.\n\nWhen formula_21 is positive definite, the formula for the minimizing value of formula_23 can be derived without the use of derivatives. The quantity\n\ncan be written as\n\nwhere formula_26 depends only on formula_27 and formula_28, and formula_29 is the inner product defined by\n\nIt follows that formula_31 is equal to\n\nand therefore minimized exactly when\n\nIn general, the coefficients of the matrices formula_34 and formula_35 can be complex. By using a Hermitian transpose instead of a simple transpose, it is possible to find a vector formula_36 which minimizes formula_37 , just as for the real matrix case. In order to get the normal equations we follow a similar path as in previous derivations:\n\nwhere formula_39 stands for Hermitian transpose.\n\nWe should now take derivatives of formula_40 with respect to each of the coefficients formula_41, but first we separate real and imaginary parts to deal with the conjugate factors in above expression. For the formula_42 we have\n\nand the derivatives change into\n\nAfter rewriting formula_40 in the summation form and writing formula_42 explicitly, we can calculate both partial derivatives with result:\n\nwhich, after adding it together and comparing to zero (minimization condition for formula_36) yields\n\nIn matrix form:\n\nUsing matrix notation, the sum of squared residuals is given by\n\nSince this is a quadratic expression, the vector which gives the global minimum may be found via matrix calculus by differentiating with respect to the vector formula_53 (using denominator layout) and setting equal to zero:\n\nBy assumption matrix \"X\" has full column rank, and therefore \"XX\" is invertible and the least squares estimator for \"β\" is given by\n\nPlug \"y\" = \"Xβ\" + \"ε\" into the formula for formula_56 and then use the Law of iterated expectation:\n\nwhere E[\"ε\"|\"X\"] = 0 by assumptions of the model.\n\nFor the variance, let the covariance matrix of formula_59 be formula_60\n(where formula_61 is the identity formula_62 matrix).\nThen,\n\nwhere we used the fact that formula_64 is just an affine transformation of formula_59 by the matrix formula_66 ( see article on the multivariate normal distribution under the affine transformation section).\n\nFor a simple linear regression model, where formula_67 (formula_68 is the y-intercept and formula_69 is the slope), one obtains\n\nFirst we will plug in the expression for \"y\" into the estimator, and use the fact that \"X'M\" = \"MX\" = 0 (matrix \"M\" projects onto the space orthogonal to \"X\"):\n\nNow we can recognize \"ε'Mε\" as a 1×1 matrix, such matrix is equal to its own trace. This is useful because by properties of trace operator, tr(\"AB\")=tr(\"BA\"), and we can use this to separate disturbance \"ε\" from matrix \"M\" which is a function of regressors \"X\":\n\nUsing the Law of iterated expectation this can be written as\n\nRecall that \"M\" = \"I\" − \"P\" where \"P\" is the projection onto linear space spanned by columns of matrix \"X\". By properties of a projection matrix, it has \"p\" = rank(\"X\") eigenvalues equal to 1, and all other eigenvalues are equal to 0. Trace of a matrix is equal to the sum of its characteristic values, thus tr(\"P\")=\"p\", and tr(\"M\") = \"n\" − \"p\". Therefore,\n\nNote: in the later section “Maximum likelihood” we show that under the additional assumption that errors are distributed normally, the estimator formula_72 is proportional to a chi-squared distribution with \"n\" – \"p\" degrees of freedom, from which the formula for expected value would immediately follow. However the result we have shown in this section is valid regardless of the distribution of the errors, and thus has importance on its own.\n\nEstimator formula_56 can be written as\nWe can use the law of large numbers to establish that \nBy Slutsky's theorem and continuous mapping theorem these results can be combined to establish consistency of estimator formula_56:\n\nThe central limit theorem tells us that\n\nApplying Slutsky's theorem again we'll have\n\nMaximum likelihood estimation is a generic technique for estimating the unknown parameters in a statistical model by constructing a log-likelihood function corresponding to the joint distribution of the data, then maximizing this function over all possible parameter values. In order to apply this method, we have to make an assumption about the distribution of y given X so that the log-likelihood function can be constructed. The connection of maximum likelihood estimation to OLS arises when this distribution is modeled as a multivariate normal.\n\nSpecifically, assume that the errors ε have multivariate normal distribution with mean 0 and variance matrix \"σ\"\"I\". Then the distribution of \"y\" conditionally on \"X\" is\nand the log-likelihood function of the data will be\nDifferentiating this expression with respect to \"β\" and \"σ\" we'll find the ML estimates of these parameters:\nWe can check that this is indeed a maximum by looking at the Hessian matrix of the log-likelihood function.\n\nSince we have assumed in this section that the distribution of error terms is known to be normal, it becomes possible to derive the explicit expressions for the distributions of estimators formula_56 and formula_72:\nso that by the affine transformation properties of multivariate normal distribution\n\nSimilarly the distribution of formula_72 follows from\n\nwhere formula_96 is the symmetric projection matrix onto subspace orthogonal to \"X\", and thus \"MX = X'M = \"0. We have argued before that this matrix rank \"n\" – \"p\", and thus by properties of chi-squared distribution,\n\nMoreover, the estimators formula_56 and formula_72 turn out to be independent (conditional on \"X\"), a fact which is fundamental for construction of the classical t- and F-tests. The independence can be easily seen from following: the estimator formula_56 represents coefficients of vector decomposition of formula_101 by the basis of columns of \"X\", as such formula_56 is a function of \"Pε\". At the same time, the estimator formula_72 is a norm of vector \"Mε\" divided by \"n\", and thus this estimator is a function of \"Mε\". Now, random variables (\"Pε\", \"Mε\") are jointly normal as a linear transformation of \"ε\", and they are also uncorrelated because \"PM\" = 0. By properties of multivariate normal distribution, this means that \"Pε\" and \"Mε\" are independent, and therefore estimators formula_56 and formula_72 will be independent as well.\n\nWe look for formula_106 and formula_107 that minimize the sum of squared errors (SSE):\n\nTo find a minimum take partial derivatives with respect to formula_106 and formula_107\n\nBefore taking partial derivative with respect to formula_107, substitute the previous result for formula_106.\n\nNow, take the derivative with respect to formula_107:\n\nAnd finally substitute formula_107 to determine formula_106\n"}
{"id": "15634017", "url": "https://en.wikipedia.org/wiki?curid=15634017", "title": "Ramified forcing", "text": "Ramified forcing\n\nIn the mathematical discipline of set theory, ramified forcing is the original form of forcing introduced by to prove the independence of the continuum hypothesis from Zermelo–Fraenkel set theory. Ramified forcing starts with a model of set theory in which the axiom of constructibility, , holds, and then builds up a larger model of Zermelo–Fraenkel set theory by adding a generic subset of a partially ordered set to , imitating Kurt Gödel's constructible hierarchy.\n\nDana Scott and Robert Solovay realized that the use of constructible sets was an unnecessary complication, and could be replaced by a simpler construction similar to John von Neumann's construction of the universe as a union of sets for ordinals . Their simplification was originally called \"unramified forcing\" , but is now usually just called \"forcing\". As a result, ramified forcing is only rarely used.\n\n"}
{"id": "31103500", "url": "https://en.wikipedia.org/wiki?curid=31103500", "title": "Reasoning system", "text": "Reasoning system\n\nIn information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.\n\nBy the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance.\n\nReasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing.\n\nThe first reasoning systems were theorem provers, systems that represent axioms and statements in First Order Logic and then use rules of logic such as modus ponens to infer new statements. Another early type of reasoning system were general problem solvers. These were systems such as the General Problem Solver designed by Newell and Simon. General problem solvers attempted to provide a generic planning engine that could represent and solve structured problems. They worked by decomposing problems into smaller more manageable sub-problems, solving each sub-problem and assembling the partial answers into one final answer. Another example general problem solver was the SOAR family of systems.\n\nIn practice these theorem provers and general problem solvers were seldom useful for practical applications and required specialized users with knowledge of logic to utilize. The first practical application of automated reasoning were expert systems. Expert systems focused on much more well defined domains than general problem solving such as medical diagnosis or analyzing faults in an aircraft. Expert systems also focused on more limited implementations of logic. Rather than attempting to implement the full range of logical expressions they typically focused on modus-ponens implemented via IF-THEN rules. Focusing on a specific domain and allowing only a restricted subset of logic improved the performance of such systems so that they were practical for use in the real world and not merely as research demonstrations as most previous automated reasoning systems had been. The engine used for automated reasoning in expert systems were typically called inference engines. Those used for more general logical inferencing are typically called theorem provers.\n\nWith the rise in popularity of expert systems many new types of automated reasoning were applied to diverse problems in government and industry. Some such as case-based reasoning were off shoots of expert systems research. Others such as constraint satisfaction algorithms were also influenced by fields such as decision technology and linear programming. Also, a completely different approach, one not based on symbolic reasoning but on a connectionist model has also been extremely productive. This latter type of automated reasoning is especially well suited to pattern matching and signal detection types of problems such as text searching and face matching.\n\nThe term reasoning system can be used to apply to just about any kind of sophisticated decision system as illustrated by the specific areas described below. However, the most common use of the term reasoning system implies the computer representation of logic. Various implementations demonstrate significant variation in terms of systems of logic and formality. Most reasoning systems implement variations of propositional and symbolic (predicate) logic. These variations may be mathematically precise representations of formal logic systems (e.g., FOL), or extended and hybrid versions of those systems (e.g., Courteous logic). Reasoning systems may explicitly implement additional logic types (e.g., modal, deontic, temporal logics). However, many reasoning systems implement imprecise and semi-formal approximations to recognised logic systems. These systems typically support a variety of procedural and semi-declarative techniques in order to model different reasoning strategies. They emphasise pragmatism over formality and may depend on custom extensions and attachments in order to solve real-world problems.\n\nMany reasoning systems employ deductive reasoning to draw inferences from available knowledge. These inference engines support forward reasoning or backward reasoning to infer conclusions via modus ponens. The recursive reasoning methods they employ are termed ‘forward chaining’ and ‘backward chaining’, respectively. Although reasoning systems widely support deductive inference, some systems employ abductive, inductive, defeasible and other types of reasoning. Heuristics may also be employed to determine acceptable solutions to intractable problems.\n\nReasoning systems may employ the closed world assumption (CWA) or open world assumption (OWA). The OWA is often associated with ontological knowledge representation and the Semantic Web. Different systems exhibit a variety of approaches to negation. As well as logical or bitwise complement, systems may support existential forms of strong and weak negation including negation-as-failure and ‘inflationary’ negation (negation of non-ground atoms). Different reasoning systems may support monotonic or non-monotonic reasoning, stratification and other logical techniques.\n\nMany reasoning systems provide capabilities for reasoning under uncertainty. This is important when building situated reasoning agents which must deal with uncertain representations of the world. There are several common approaches to handling uncertainty. These include the use of certainty factors, probabilistic methods such as Bayesian inference or Dempster–Shafer theory, multi-valued (‘fuzzy’) logic and various connectionist approaches.\n\nThis section provides a non-exhaustive and informal categorisation of common types of reasoning system. These categories are not absolute. They overlap to a significant degree and share a number of techniques, methods and algorithms.\n\nConstraint solvers solve constraint satisfaction problems (CSPs). They support constraint programming. A constraint is a condition which must be met by any valid solution to a problem. Constraints are defined declaratively and applied to variables within given domains. Constraint solvers use search, backtracking and constraint propagation techniques to find solutions and determine optimal solutions. They may employ forms of linear and nonlinear programming. They are often used to perform optimization within highly combinatorial problem spaces. For example, they may be used to calculate optimal scheduling, design efficient integrated circuits or maximise productivity in a manufacturing process.\n\nTheorem provers use automated reasoning techniques to determine proofs of mathematical theorems. They may also be used to verify existing proofs. In addition to academic use, typical applications of theorem provers include verification of the correctness of integrated circuits, software programs, engineering designs, etc.\n\nLogic programs (LPs) are software programs written using programming languages whose primitives and expressions provide direct representations of constructs drawn from mathematical logic. An example of a general-purpose logic programming language is Prolog. LPs represent the direct application of logic programming to solve problems. Logic programming is characterised by highly declarative approaches based on formal logic, and has wide application across many disciplines.\n\nRule engines represent conditional logic as discrete rules. Rule sets can be managed and applied separately to other functionality. They have wide applicability across many domains. Many rule engines implement reasoning capabilities. A common approach is to implement production systems to support forward or backward chaining. Each rule (‘production’) binds a conjunction of predicate clauses to a list of executable actions. At run-time, the rule engine matches productions against facts and executes (‘fires’) the associated action list for each match. If those actions remove or modify any facts, or assert new facts, the engine immediately re-computes the set of matches. Rule engines are widely used to model and apply business rules, to control decision-making in automated processes and to enforce business and technical policies.\n\nDeductive classifiers arose slightly later than rule-based systems and were a component of a new type of artificial intelligence knowledge representation tool known as frame languages. A frame language describes the problem domain as a set of classes, subclasses, and relations among the classes. It is similar to the object-oriented model. Unlike object-oriented models however, frame languages have a formal semantics based on first order logic. They utilize this semantics to provide input to the deductive classifier. The classifier in turn can analyze a given model (known as an ontology) and determine if the various relations described in the model are consistent. If the ontology is not consistent the classifier will highlight the declarations that are inconsistent. If the ontology is consistent the classifier can then do further reasoning and draw additional conclusions about the relations of the objects in the ontology. For example, it may determine that an object is actually a subclass or instance of additional classes as those described by the user. Classifiers are an important technology in analyzing the ontologies used to describe models in the Semantic web.\nMachine learning systems evolve their behavior over time based on experience. This may involve reasoning over observed events or example data provided for training purposes. For example, machine learning systems may use inductive reasoning to generate hypotheses for observed facts. Learning systems search for generalised rules or functions that yield results in line with observations and then use these generalisations to control future behavior.\n\nCase-based reasoning (CBR) systems provide solutions to problems by analysing similarities to other problems for which known solutions already exist. They use analogical reasoning to infer solutions based on case histories. CBR systems are commonly used in customer/technical support and call centre scenarios and have applications in industrial manufacture, agriculture, medicine, law and many other areas.\n\nA procedural reasoning system (PRS) uses reasoning techniques to select plans from a procedural knowledge base. Each plan represents a course of action for achievement of a given goal. The PRS implements a belief-desire-intention model by reasoning over facts (‘beliefs’) to select appropriate plans (‘intentions’) for given goals (‘desires’). Typical applications of PRS include management, monitoring and fault detection systems.\n"}
{"id": "5028401", "url": "https://en.wikipedia.org/wiki?curid=5028401", "title": "Sergei Adian", "text": "Sergei Adian\n\nSergei Ivanovich Adian, also Adyan (; , born 1 January 1931), is a Soviet and Russian mathematician. He is a professor at the Moscow State University and is known for his work in group theory, especially on the Burnside problem.\n\nAdian was born near Elizavetpol. He grew up there in an Armenian family. He studied at Yerevan and Moscow pedagogical institutes. His advisor was Pyotr Novikov. He has been working at Moscow State University (MSU) since 1965. Alexander Razborov was one of his students.\n\nIn his first work as a student in 1950, Adian proved that the graph of a function formula_1 of a real variable satisfying the functional equation formula_2 and having discontinuities is dense in the plane. (Clearly, all continuous solutions of the equation are linear functions.) This result was not published at the time. About 25 years later the American mathematician Edwin Hewitt from the University of Washington gave preprints of some of his papers to Adian during a visit to MSU, one of which was devoted to exactly the same result, which was published by Hewitt much later.\n\nBy the beginning of 1955, Adian had managed to prove the undecidability of practically all non-trivial invariant group properties, including the undecidability of being isomorphic to a fixed group formula_3, for any group formula_3. These results constituted his Ph.D. thesis and his first published work. This is one of the most remarkable, beautiful, and general results in algorithmic group theory and is now known as the Adian–Rabin theorem. What distinguishes the first published work by Adian, is its completeness. In spite of numerous attempts, nobody has added anything fundamentally new to the results during the past 50 years. Adian’s result was immediately used by Andrey Markov in his proof of the algorithmic unsolvability of the classical problem of deciding when topological manifolds are homeomorphic.\n\nAbout the Burnside problem:\n\nVery much like Fermat's Last Theorem in number theory, Burnside’s\nproblem has acted as a catalyst for research in group theory. The fascination exerted by a problem with an extremely simple formulation which then turns out to be extremely difficult has something irresistible about it to the mind of the mathematician.\nBefore the work of Novikov and Adian an affirmative answer to the problem was known only for formula_5 and the matrix groups. However, this did not hinder the belief in an affirmative answer for any period formula_6. The only question was\nto find the right methods for proving it. As later developments showed, this belief was too naive. This just demonstrates that before their work nobody even came close to imagining the nature of the free Burnside group, or the extent to which subtle structures inevitably arose in any serious attempt to investigate it. In fact, there were no methods for proving inequalities in groups given by identities of the\nform formula_7.\n\nAn approach to solving the problem in the negative was first outlined by P. S. Novikov in his note, which appeared in 1959. However, the concrete realization of his ideas encountered serious difficulties, and in 1960, at the insistence of Novikov and his wife Lyudmila Keldysh, Adian settled down to work on the Burnside problem. Completing the project took intensive efforts from both\ncollaborators in the course of eight years, and in 1968 their famous paper appeared, containing a negative solution of the problem for all odd periods formula_8, and hence for all multiples of those odd integers as well.\n\nThe solution of the Burnside problem was certainly one of the most outstanding\nand deep mathematical results of the past century. At the same time, this result\nis one of the hardest theorems: just the inductive step of a complicated induction\nused in the proof took up a whole issue of volume 32 of Izvestiya, even lengthened\nby 30 pages. In many respects the work was literally carried to its conclusion by\nthe exceptional persistence of Adian. In that regard it is worth recalling the words\nof Novikov, who said that he had never met a mathematician more ‘penetrating’\nthan Adian.\n\nIn contrast to the Adian–Rabin theorem, the paper of Adian and Novikov in no way ‘closed’ the Burnside problem. Moreover,\nover a long period of more than ten years Adian continued to improve and simplify\nthe method they had created and also to adapt the method for solving some other\nfundamental problems in group theory.\n\nBy the beginning of the 1980s, when other contributors\nappeared who mastered the Novikov–Adian method, the theory already\nrepresented a powerful method for constructing and investigating new groups (both\nperiodic and non-periodic) with interesting properties prescribed.\n\n"}
{"id": "34479518", "url": "https://en.wikipedia.org/wiki?curid=34479518", "title": "Spectral test", "text": "Spectral test\n\nThe spectral test is a statistical test for the quality of a class of pseudorandom number generators (PRNGs), the linear congruential generators (LCGs). LCGs have a property that when plotted in 2 or more dimensions, lines or hyperplanes will form, on which all possible outputs can be found. The spectral test compares the distance between these planes; the further apart they are, the worse the generator is. As this test is devised to study the lattice structures of LCGs, it can not be applied to other families of PRNGs. \n\nAccording to Donald Knuth, this is by far the most powerful test known, because it can fail LCGs which pass most statistical tests. The IBM subroutine RANDU LCG fails in this test for 3 dimensions and above.\n"}
{"id": "41527719", "url": "https://en.wikipedia.org/wiki?curid=41527719", "title": "Stationary-wave integrated Fourier transform spectrometry", "text": "Stationary-wave integrated Fourier transform spectrometry\n\nStationary-wave integrated Fourier transform spectrometry (SWIFTS) is an analytical technique used for measuring the distribution of light across an optical spectrum. SWIFTS technology is based on a near-field Lippmann architecture. An optical signal is injected into a waveguide and ended by a mirror (true Lippman configuration). The input signal interferes with the reflected signal, creating a stationary wave. \n\nIn a counter-propagative architecture, the two optical signals are injected at the opposite ends of the waveguide. The evanescent waves propagating within the waveguide are then sampled by optical probes. This results in an interferogram. A mathematical function known as a Lippmann transform, similar to a Fourier transform, is later used to give the spectrum of the light.\n\nIn 1891, at the Académie des Sciences in Paris, Gabriel Lippmann presented a colour photograph of the Sun’s spectrum obtained with his new photographic plate. Later, in 1894, he published an article on how his plate was able to record colour information in the depth of photographic grainless gelatin and how the same plate after processing could restore the original colour image merely through light reflection. He was thus the inventor of true interferential colour photography. He received the Nobel Prize in Physics in 1908 for this breakthrough. Unfortunately, this principle was too complex to use. The method was abandoned a few years after its discovery. \n\nOne aspect of the Lippmann concept that was ignored at that time relates to spectroscopic applications. Early in 1933, Ives proposed to use a photoelectric device to probe stationary waves to make spectrometric measurements. In 1995, P. Connes proposed to use the emerging new technology of detectors for three-dimensional Lippmann-based spectrometry. Following this, a first realization of a very compact spectrometer based on a micro-opto-electromechanical system (MOEMS) was reported by Knipp et al. in 2005, but it had a very limited spectral resolution. In 2004, two French researchers, Etienne Le Coarer from Joseph Fourier University and Pierre Benech from INP Grenoble, coupled sensing elements to the evanescent part of stationary waves within a single-mode waveguide. In 2007, those two researchers reported a near-field method to probe the interferogram within a waveguide. The first SWIFTS-based spectrometers appeared in 2011 based on a SWIFTS linear configuration.\n\nThe technology works by probing an optical stationary wave, or the sum of the standing waves in the case of polychromatic light, created by a light to be analyzed. In a SWIFTS linear configuration (true Lippman configuration), the stationary wave is created by a single-mode waveguide ended by a fixed mirror. The stationary wave is regularly sampled on one side of a waveguide using nano-scattering dots. These dots are located in the evanescent field. These nanodots are characterized by an optical index difference with the medium in which the evanescent field is located. The light is then scattered around an axis perpendicular to the waveguide. For each dot, this scattered light is detected by a pixel aligned with this axis. The intensity detected is therefore proportional to the intensity inside the waveguide at the exact location of the dot. This results in a linear image of the interferogram. No moving parts are used. A mathematical function known as a Lippmann transform, similar to a Fourier transform, is then applied to this linear image and gives the spectrum of the light.\n\nIt should be noted that the interferogram is truncated. Only the frequencies corresponding to the zero Optical Path Difference (OPD) at the mirror, up to the farthest dots are sampled. Higher frequencies are rejected. This interferogram’s truncation determines the spectral resolution. It is also to be noted that the interferogram is under-sampled. A consequence of this under-sampling is a limitation of the wavelength bandwidth to which the mathematical function is applied.\n\nSWIFTS technology displays the Fellgett’s advantage, which is derived from the fact that an interferometer measures wavelengths simultaneously with the same elements of the detector, whereas a dispersive spectrometer measures them successively. Fellgett’s advantage also states that when collecting a spectrum whose measurement noise is dominated by detector noise, a multiplex spectrometer such as a Fourier Transform Spectrometer will produce a relative improvement in the signal-to-noise ratio, with respect to an equivalent scanning monochromator, that is approximately equal to the square root of the number of sample points comprising the spectrum. The Connes advantage states that the wavenumber scale of an interferometer, derived from a helium-neon (HeNe) laser, is more accurate and boasts better long-term stability than the calibration of dispersive instruments.\n\n"}
{"id": "2472919", "url": "https://en.wikipedia.org/wiki?curid=2472919", "title": "Theta graph", "text": "Theta graph\n\nIn computational geometry, the Theta graph, or formula_1-graph, is a type of geometric spanner similar to a Yao graph. The basic method of construction involves partitioning the space around each vertex into a set of \"cones\", which themselves partition the remaining vertices of the graph. Like Yao Graphs, a formula_1-graph contains at most one edge per cone; where they differ is how that edge is selected. Whereas Yao Graphs will select the nearest vertex according to the metric space of the graph, the formula_1-graph defines a fixed ray contained within each cone (conventionally the bisector of the cone) and selects the nearest neighbour with respect to orthogonal projections to that ray. The resulting graph exhibits several good spanner properties\n\nformula_1-graphs were first described by \nClarkson in 1987 \nand independently by\nKeil in 1988.\n\nformula_1-graphs are specified with a few parameters which determine their construction. The most obvious parameter is formula_6, which corresponds to the number of equal angle cones that partition the space around each vertex. In particular, for a vertex formula_7, a cone about formula_7 can be imagined as two infinite rays emanating from it with angle formula_9 between them. With respect to formula_7, we can label these cones as formula_11 through formula_12 in an counterclockwise pattern from formula_11, which conventionally opens so that its bisector has angle 0 with respect to the plane. As these cones partition the plane, they also partition the remaining vertex set of the graph (assuming general position) into the sets formula_14 through formula_15, again with respect to formula_7. Every vertex in the graph gets the same number of cones in the same orientation, and we can consider the set of vertices that fall into each.\n\nConsidering a single cone, we need to specify another ray emanating from formula_7, which we will label formula_18. For every vertex in formula_19, we consider the orthogonal projection of each formula_20 onto formula_18. Suppose that formula_22 is the vertex with the closest such projection, then the edge formula_23 is added to the graph. This is the primary difference from Yao Graphs which always select the nearest vertex; in the example image, a Yao Graph would include the edge formula_24 instead.\n\nConstruction of a formula_1-graph is possible with a sweepline algorithm in formula_26 time.\n\nformula_1-graphs exhibit several good geometric spanner properties.\n\nWhen the parameter formula_6 is a constant, the formula_1-graph is a sparse spanner. As each cone generates at most one edge per cone, most vertices will have small degree, and the overall graph will have at most formula_30 edges.\n\nThe stretch factor between any pair of points in a spanner is defined as the ratio between their metric space distance, and their distance within the spanner (i.e. from following edges of the spanner). The stretch factor of the entire spanner is the maximum stretch factor over all pairs of points within it. Recall from above that formula_9, then when formula_32, the formula_1-graph has a stretch factor of at most formula_34. If the orthogonal projection line formula_18 in each cone is chosen to be the bisector, then for formula_36, the spanning ratio is at most formula_37.\n\nFor formula_38, the formula_1-graph forms a nearest neighbour graph. For formula_40, it is easy to see that the graph is connected, as each vertex will connect to something to its left, and something to its right, if they exist. For formula_41,\nformula_42, \nformula_43,\nand formula_44,\nthe formula_1-graph is known to be connected. As yet unpublished results indicate that formula_1-graphs are connected for formula_47, as well. Many of these results also give upper and/or lower bounds on their spanning ratios.\n\nWhen formula_6 is an even number, we can create a variant of the formula_49-graph known as the \"half-formula_49-graph\", where the cones themselves are partitioned into \"even\" and \"odd\" sets in an alternating fashion, and edges are only considered in the even cones (or, only the odd cones). Half-formula_49-graphs are known to have some very nice properties of their own. For example, the half-formula_52-graph (and, consequently, the formula_52-graph, which is just the union of two complimentary half-formula_52-graphs) is known to be a 2-spanner.\n\n\n"}
{"id": "30367", "url": "https://en.wikipedia.org/wiki?curid=30367", "title": "Trigonometric functions", "text": "Trigonometric functions\n\nIn mathematics, the trigonometric functions (also called circular functions, angle functions or goniometric functions) are functions of an angle. They relate the angles of a triangle to the lengths of its sides. Trigonometric functions are important in the study of triangles and modeling periodic phenomena, among many other applications.\n\nThe most familiar trigonometric functions are the sine, cosine, and tangent. In the context of the standard unit circle (a circle with radius 1 unit), where a triangle is formed by a ray starting at the origin and making some angle with the -axis, the sine of the angle gives the -component (the opposite to the angle or the rise) of the triangle, the cosine gives the -component (the adjacent of the angle or the run), and the tangent function gives the slope (-component divided by the -component). For angles less than a right angle, trigonometric functions are commonly defined as ratios of two sides of a right triangle containing the angle, and their values can be found in the lengths of various line segments around a unit circle. Modern definitions express trigonometric functions as infinite series or as solutions of certain differential equations, allowing the extension of the arguments to the whole number line and to the complex numbers.\n\nTrigonometric functions have a wide range of uses including computing unknown lengths and angles in triangles (often right triangles). In this use, trigonometric functions are used, for instance, in navigation, engineering, and physics. A common use in elementary physics is resolving a vector into Cartesian coordinates. The sine and cosine functions are also commonly used to model periodic function phenomena such as sound and light waves, the position and velocity of harmonic oscillators, sunlight intensity and day length, and average temperature variations through the year.\n\nIn modern usage, there are six basic trigonometric functions, tabulated here with equations that relate them to one another. Especially with the last four, these relations are often taken as the \"definitions\" of those functions, but one can define them equally well geometrically, or by other means, and then derive these relations.\n\nThe notion that there should be some standard correspondence between the lengths of the sides of a triangle and the angles of the triangle comes as soon as one recognizes that similar triangles maintain the same ratios between their sides. That is, for any similar triangle the ratio of the hypotenuse (for example) and another of the sides remains the same. If the hypotenuse is twice as long, so are the sides. It is these ratios that the trigonometric functions express.\n\nTo define the trigonometric functions for the angle \"A\", start with any right triangle that contains the angle \"A\". The three sides of the triangle are named as follows:\n\nIn ordinary Euclidean geometry, according to the triangle postulate, the inside angles of every triangle total 180° ( radians). Therefore, in a right-angled triangle, the two non-right angles total 90° ( radians), so each of these angles must be in the range of as expressed in interval notation. The following definitions apply to angles in this range. They can be extended to the full set of real arguments by using the unit circle, or by requiring certain symmetries and that they be periodic functions. For example, the figure shows for angles , , , and depicted on the unit circle (top) and as a graph (bottom). The value of the sine repeats itself apart from sign in all four quadrants, and if the range of is extended to additional rotations, this behavior repeats periodically with a period 2.\n\nThe trigonometric functions are summarized in the following table and described in more detail below. The angle is the angle between the hypotenuse and the adjacent line – the angle at \"A\" in the accompanying diagram.\n\nThe sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. The word comes from the Latin \"sinus\" for gulf or bay, since, given a unit circle, it is the side of the triangle on which the angle \"opens\". In our case:\n\nThe cosine (\"sine complement\", Latin: \"cosinus\", \"sinus complementi\") of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse, so called because it is the sine of the complementary or co-angle, the other non-right angle. Because the angle sum of a triangle is radians, the co-angle is equal to ; so . In our case:\n\nThe tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side, so called because it can be represented as a line segment tangent to the circle, i.e. the line that touches the circle, from Latin \"linea tangens\" or touching line (cf. \"tangere\", to touch). In our case:\n\nTangent may also be represented in terms of sine and cosine. That is:\n\nThese ratios do not depend on the size of the particular right triangle chosen, as long as the focus angle is equal, since all such triangles are similar.\n\nThe acronyms \"SOH-CAH-TOA\" (\"soak-a-toe\", \"sock-a-toa\", \"so-kah-toa\") and \"OHSAHCOAT\" are commonly used trigonometric mnemonics for these ratios.\n\nThe remaining three functions are best defined using the three functions above and can be considered their reciprocals.\n\nThe secant of an angle is the reciprocal of its cosine, that is, the ratio of the length of the hypotenuse to the length of the adjacent side, so called because it represents the secant line that \"cuts\" the circle (from Latin: \"secare\", to cut):\n\nThe cosecant (\"secant complement\", Latin: \"cosecans\", \"secans complementi\") of an angle is the reciprocal of its sine, that is, the ratio of the length of the hypotenuse to the length of the opposite side, so called because it is the secant of the complementary or co-angle:\n\nThe cotangent (\"tangent complement\", Latin: \"cotangens\", \"tangens complementi\") of an angle is the reciprocal of its tangent, that is, the ratio of the length of the adjacent side to the length of the opposite side, so called because it is the tangent of the complementary or co-angle:\n\nEquivalent to the right-triangle definitions, the trigonometric functions can also be defined in terms of the \"rise\", \"run\", and \"slope\" of a line segment relative to horizontal. The slope is commonly taught as \"rise over run\" or . The three main trigonometric functions are commonly taught in the order sine, cosine and tangent. With a line segment length of 1 (as in a unit circle), the following mnemonic devices show the correspondence of definitions:\n\n\nThis shows the main use of tangent and arctangent: converting between the two ways of telling the slant of a line, i.e. angles and slopes. (The arctangent or \"inverse tangent\" is not to be confused with the \"cotangent\", which is cosine divided by sine.)\n\nWhile the length of the line segment makes no difference for the slope (the slope does not depend on the length of the slanted line), it does affect rise and run. To adjust and find the actual rise and run when the line does not have a length of 1, just multiply the sine and cosine by the line length. For instance, if the line segment has length 5, the run at an angle of 7° is 5cos(7°).\n\nThe six trigonometric functions can be defined as coordinate values of points on the Euclidean plane that are related to the unit circle, which is the circle of radius one centered at the origin of this coordinate system. While right-angled triangle definitions permit the definition of the trigonometric functions for angles between and formula_8 radian the unit circle definitions allow to extend the domain of the trigonometric functions to all positive and negative real numbers.\n\nRotating a ray from the direction of the positive half of the \"x\"-axis by an angle (counterclockwise for formula_9 and clockwise for formula_10) yields intersection points of this ray (see the figure) with the unit and, by extending the ray to a line if necessary, with the and with the The tangent line to the unit circle in point , which is orthogonal to this ray, intersects the \"y\"- and \"x\"-axis in points formula_11 and formula_12. The coordinate values of these points give all the existing values of the trigonometric functions for arbitrary real values of in the following manner.\n\nThe trigonometric functions and are defined, respectively, as the \"x\"- and \"y\"-coordinate values of point , i.e.,\n\nIn the range formula_15 this definition coincides with the right-angled triangle definition by taking the right-angled triangle to have the unit radius as hypotenuse, and since for all points formula_16 on the unit circle the equation formula_17 holds, this definition of cosine and sine also satisfies the Pythagorean identity \n\nThe other trigonometric functions can be found along the unit circle as \n\nBy applying the Pythagorean identity and geometric proof methods, these definitions can readily be shown to coincide with the definitions of tangent, cotangent, secant and cosecant in terms of sine and cosine, that is\n\nAs a rotation of an angle of formula_24 does not change the position or size of a shape, the points , , , , and are the same for two angles whose difference is an integer multiple of formula_25. Thus trigonometric functions are periodic functions with period formula_25. That is, the equalities\nhold for any angle and any integer . The same is true for the four other trigonometric functions. Observing the sign and the monotonicity of the functions sine, cosine, cosecant, and secant in the four quadrants, shows that is the smallest value for which they are periodic, i.e., is the fundamental period of these functions. However, already after a rotation by an angle formula_29 the points and return to their original position, so that the tangent function and the cotangent function have a fundamental period of . That is, the equalities\nhold for any angle and any integer .\n\nThe algebraic expressions for and are\n\nrespectively. Writing the numerators as square roots of consecutive natural numbers (formula_33) provides an easy way to remember the values.\nSuch simple expressions generally do not exist for other angles which are rational multiples of a straight angle.\n\nFor an angle which, measured in degrees, is a multiple of three, the sine and the cosine may be expressed in terms of square roots, as shown below. These values of the sine and the cosine may thus be constructed by ruler and compass.\n\nFor an angle of an integer number of degrees, the sine and the cosine may be expressed in terms of square roots and the cube root of a non-real complex number. Galois theory allows proof that, if the angle is not a multiple of 3°, non-real cube roots are unavoidable.\n\nFor an angle which, measured in degrees, is a rational number, the sine and the cosine are algebraic numbers, which may be expressed in terms of th roots. This results from the fact that the Galois groups of the cyclotomic polynomials are cyclic.\n\nFor an angle which, measured in degrees, is not a rational number, then either the angle or both the sine and the cosine are transcendental numbers. This is a corollary of Baker's theorem, proved in 1966.\n\nAlgebraic expressions for 15°, 18°, 36°, 54°, 72° and 75° are as follows:\n\nFrom these, the algebraic expressions for all multiples of 3° can be computed. For example:\n\nAlgebraic expressions can be deduced for other angles of an integer number of degrees, for example,\nwhere , and and are the above algebraic expressions for, respectively, and , and the principal cube root (that is, the cube root with the largest real part) is to be taken.\n\nTrigonometric functions are analytic functions. Using only geometry and properties of limits, it can be shown that the derivative of sine is cosine and the derivative of cosine is the negative of sine. One can then use the theory of Taylor series to show that the following identities hold for all real numbers . Here, and generally in calculus, all angles are measured in radians.\n\nThe infinite series appearing in these identities are convergent in the whole complex plane and are often taken as the definitions of the sine and cosine functions of a complex variable. Another standard (and equivalent) definition of the sine and the cosine as functions of a complex variable is through their differential equation, below.\n\nOther series can be found. For the following trigonometric functions:\n\nWhen the series for the tangent and secant functions are expressed in a form in which the denominators are the corresponding factorials, the numerators, called the \"tangent numbers\" and \"secant numbers\" respectively, have a combinatorial interpretation: they enumerate alternating permutations of finite sets, of odd cardinality for the tangent series and even cardinality for the secant series. The series itself can be found by a power series solution of the aforementioned differential equation.\n\nFrom a theorem in complex analysis, there is a unique analytic continuation of this real function to the domain of complex numbers. They have the same Taylor series, and so the trigonometric functions are defined on the complex numbers using the Taylor series above.\n\nThere is a series representation as partial fraction expansion where just translated reciprocal functions are summed up, such that the poles of the cotangent function and the reciprocal functions match:\nThis identity can be proven with the Herglotz trick.\nCombining the th with the th term lead to absolutely convergent series:\n\nIt can be shown from the series definitions that the sine and cosine functions are respectively the imaginary and real parts of the exponential function of a purely imaginary argument. That is, if is real, we have \nand\n\nThe latter identity, although primarily established for real , remains valid for every complex , and is called Euler's formula.\n\nEuler's formula can be used to derive most trigonometric identities from the properties of the exponential function, by writing sine and cosine as:\n\nIt is also sometimes useful to express the complex sine and cosine functions in terms of the real and imaginary parts of their arguments.\n\nThis exhibits a deep relationship between the complex sine and cosine functions and their real (sin, cos) and hyperbolic real (sinh, cosh) counterparts.\n\nIn the following graphs the domain is the complex plane pictured with domain coloring, and the range values are indicated at each point by color. Brightness indicates the size (absolute value) of the range value, with black being zero. Hue varies with argument, or angle, measured from the positive real axis. \n\nBoth the sine and cosine functions satisfy the linear differential equation:\n\nThat is to say, each is the additive inverse of its own second derivative. Within the 2-dimensional function space consisting of all solutions of this equation,\n\nSince the sine and cosine functions are linearly independent, together they form a basis of . This method of defining the sine and cosine functions is essentially equivalent to using Euler's formula. (See linear differential equation.) It turns out that this differential equation can be used not only to define the sine and cosine functions but also to prove the trigonometric identities for the sine and cosine functions.\n\nFurther, the observation that sine and cosine satisfies means that they are eigenfunctions of the second-derivative operator.\n\nThe tangent function is the unique solution of the nonlinear differential equation\nsatisfying the initial condition . There is a very interesting visual proof that the tangent function satisfies this differential equation.\n\nRadians specify an angle by measuring the length around the path of the unit circle and constitute a special argument to the sine and cosine functions. In particular, only sines and cosines that map radians to ratios satisfy the differential equations that classically describe them. If an argument to sine or cosine in radians is scaled by frequency,\n\nthen the derivatives will scale by amplitude.\n\nHere, is a constant that represents a mapping between units. If is in degrees, then\n\nThis means that the second derivative of a sine in degrees does not satisfy the differential equation\n\nbut rather\n\nThe cosine's second derivative behaves similarly.\n\nThis means that these sines and cosines are different functions, and that the fourth derivative of sine will be sine again only if the argument is in radians.\n\nMany identities interrelate the trigonometric functions. Among the most frequently used is the Pythagorean identity, which states that for any angle, the square of the sine plus the square of the cosine is 1. This is easy to see by studying a right triangle of hypotenuse 1 and applying the Pythagorean theorem. In symbolic form, the Pythagorean identity is written\n\nwhich is standard shorthand notation for\n\nOther key relationships are the sum and difference formulas, which give the sine and cosine of the sum and difference of two angles in terms of sines and cosines of the angles themselves. These can be derived geometrically, using arguments that date to Ptolemy. One can also produce them algebraically using Euler's formula.\n\nThese in turn lead to the following three-angle formulae:\n\nWhen the two angles are equal, the sum formulas reduce to simpler equations known as the double-angle formulae.\n\nWhen three angles are equal, the three-angle formulae simplify to\n\nThese identities can also be used to derive the product-to-sum identities that were used in antiquity to transform the product of two numbers into a sum of numbers and greatly speed operations, much like the logarithm function.\n\nFor integrals and derivatives of trigonometric functions, see the relevant sections of Differentiation of trigonometric functions, Lists of integrals and List of integrals of trigonometric functions. Below is the list of the derivatives and integrals of the six basic trigonometric functions. The number  is a constant of integration.\n\nIn mathematical analysis, one can define the trigonometric functions using functional equations based on properties like the difference formula. Taking as given these formulas, one can prove that only two continuous functions satisfy those conditions. Formally, there exists exactly one pair of continuous functions—sin and cos—such that for all real numbers and , the following equation holds:\n\nwith the added condition that\n\nThis may also be used for extending sine and cosine to the complex numbers. Other functional equations are also possible for defining trigonometric functions.\n\nThe computation of trigonometric functions is a complicated subject, which can today be avoided by most people because of the widespread availability of computers and scientific calculators that provide built-in trigonometric functions for any angle. This section, however, describes details of their computation in three important contexts: the historical use of trigonometric tables, the modern techniques used by computers, and a few \"important\" angles where simple exact values are easily found.\n\nThe first step in computing any trigonometric function is range reduction—reducing the given angle to a \"reduced angle\" inside a small range of angles, say 0 to , using the periodicity and symmetries of the trigonometric functions.\n\nPrior to computers, people typically evaluated trigonometric functions by interpolating from a detailed table of their values, calculated to many significant figures. Such tables have been available for as long as trigonometric functions have been described (see History below), and were typically generated by repeated application of the half-angle and angle-addition identities starting from a known value (such as sin() = 1).\n\nModern computers use a variety of techniques. One common method, especially on higher-end processors with floating point units, is to combine a polynomial or rational approximation (such as Chebyshev approximation, best uniform approximation, and Padé approximation, and typically for higher or variable precisions, Taylor and Laurent series) with range reduction and a table lookup—they first look up the closest angle in a small table, and then use the polynomial to compute the correction. Devices that lack hardware multipliers often use an algorithm called CORDIC (as well as related techniques), which uses only addition, subtraction, bitshift, and table lookup. These methods are commonly implemented in hardware floating-point units for performance reasons.\n\nFor very high precision calculations, when series expansion convergence becomes too slow, trigonometric functions can be approximated by the arithmetic-geometric mean, which itself approximates the trigonometric function by the (complex) elliptic integral.\n\nFinally, for some simple angles, the values can be easily computed by hand using the Pythagorean theorem, as in the following examples. For example, the sine, cosine and tangent of any integer multiple of radians (3°) can be found exactly by hand.\n\nConsider a right triangle where the two other angles are equal, and therefore are both radians (45°). Then the length of side and the length of side are equal; we can choose . The values of sine, cosine and tangent of an angle of radians (45°) can then be found using the Pythagorean theorem:\n\nTherefore:\n\nTo determine the trigonometric functions for angles of radians (60°) and radians (30°), we start with an equilateral triangle of side length 1. All its angles are radians (60°). By dividing it into two, we obtain a right triangle with radians (30°) and radians (60°) angles. For this triangle, the shortest side is , the next largest side is and the hypotenuse is 1. This yields:\n\nThere are some commonly used special values in trigonometric functions, as shown in the following table.\n\nThe symbol here represents the point at infinity on the projectively extended real line, the limit on the extended real line is on one side and on the other.\n\nThe trigonometric functions are periodic, and hence not injective, so strictly they do not have an inverse function. Therefore, to define an inverse function we must restrict their domains so that the trigonometric function is bijective. In the following, the functions on the left are \"defined\" by the equation on the right; these are not proved identities. The principal inverses are usually defined as:\n\nThe notations sin and cos are often used for arcsin and arccos, etc. When this notation is used, the inverse functions could be confused with the multiplicative inverses of the functions. The notation using the \"arc-\" prefix avoids such confusion, though \"arcsec\" for arcsecant can be confused with \"arcsecond\".\n\nJust like the sine and cosine, the inverse trigonometric functions can also be defined in terms of infinite series. For example,\nThese functions may also be defined by proving that they are antiderivatives of other functions. The arcsine, for example, can be written as the following integral:\n\nAnalogous formulas for the other functions can be found at inverse trigonometric functions. Using the complex logarithm, one can generalize all these functions to complex arguments:\n\nIn an inner product space, the angle between two non-zero vectors is defined to be\n\nThe trigonometric functions, as the name suggests, are of crucial importance in trigonometry, mainly because of the following two results.\n\nThe law of sines states that for an arbitrary triangle with sides , , and and angles opposite those sides , and :\n\nwhere is the area of the triangle,\nor, equivalently,\n\nwhere is the triangle's circumradius.\n\nIt can be proven by dividing the triangle into two right ones and using the above definition of sine. The law of sines is useful for computing the lengths of the unknown sides in a triangle if two angles and one side are known. This is a common situation occurring in \"triangulation\", a technique to determine unknown distances by measuring two angles and an accessible enclosed distance.\n\nThe law of cosines (also known as the cosine formula or cosine rule) is an extension of the Pythagorean theorem:\n\nor equivalently,\n\nIn this formula the angle at is opposite to the side . This theorem can be proven by dividing the triangle into two right ones and using the Pythagorean theorem.\n\nThe law of cosines can be used to determine a side of a triangle if two sides and the angle between them are known. It can also be used to find the cosines of an angle (and consequently the angles themselves) if the lengths of all the sides are known.\n\nThe following all form the law of tangents\n\nThe explanation of the formulae in words would be cumbersome, but the patterns of sums and differences, for the lengths and corresponding opposite angles, are apparent in the theorem.\n\nIf\n\nand\n\nthen the following all form the law of cotangents\n\nIt follows that\n\nIn words the theorem is: the cotangent of a half-angle equals the ratio of the semi-perimeter minus the opposite side to the said angle, to the inradius for the triangle.\n\nThe trigonometric functions are also important in physics. The sine and the cosine functions, for example, are used to describe simple harmonic motion, which models many natural phenomena, such as the movement of a mass attached to a spring and, for small angles, the pendular motion of a mass hanging by a string. The sine and cosine functions are one-dimensional projections of uniform circular motion.\n\nTrigonometric functions also prove to be useful in the study of general periodic functions. The characteristic wave patterns of periodic functions are useful for modeling recurring phenomena such as sound or light waves.\n\nUnder rather general conditions, a periodic function can be expressed as a sum of sine waves or cosine waves in a Fourier series. Denoting the sine or cosine basis functions by , the expansion of the periodic function takes the form:\n\nFor example, the square wave can be written as the Fourier series\n\nIn the animation of a square wave at top right it can be seen that just a few terms already produce a fairly good approximation. The superposition of several terms in the expansion of a sawtooth wave are shown underneath.\n\nWhile the early study of trigonometry can be traced to antiquity, the trigonometric functions as they are in use today were developed in the medieval period. The chord function was discovered by Hipparchus of Nicaea (180–125 BCE) and Ptolemy of Roman Egypt (90–165 CE).\n\nThe functions sine and cosine can be traced to the \"jyā\" and \"koti-jyā\" functions used in Gupta period Indian astronomy (\"Aryabhatiya\", \"Surya Siddhanta\"), via translation from Sanskrit to Arabic and then from Arabic to Latin.\n\nAll six trigonometric functions in current use were known in Islamic mathematics by the 9th century, as was the law of sines, used in solving triangles. al-Khwārizmī produced tables of sines, cosines and tangents.\nThey were studied by authors including Omar Khayyám, Bhāskara II, Nasir al-Din al-Tusi, Jamshīd al-Kāshī (14th century), Ulugh Beg (14th century), Regiomontanus (1464), Rheticus, and Rheticus' student Valentinus Otho.\n\nMadhava of Sangamagrama (c. 1400) made early strides in the analysis of trigonometric functions in terms of infinite series.\n\nThe terms \"tangent\" and \"secant\" were first introduced by the Danish mathematician Thomas Fincke in his book \"Geometria rotundi\" (1583).\n\nThe first published use of the abbreviations \"sin\", \"cos\", and \"tan\" is probably by the 16th century French mathematician Albert Girard.\n\nIn a paper published in 1682, Leibniz proved that is not an algebraic function of .\n\nLeonhard Euler's \"Introductio in analysin infinitorum\" (1748) was mostly responsible for establishing the analytic treatment of trigonometric functions in Europe, also defining them as infinite series and presenting \"Euler's formula\", as well as near-modern abbreviations (\"sin.\", \"cos.\", \"tang.\", \"cot.\", \"sec.\", and \"cosec.\").\n\nA few functions were common historically, but are now seldom used, such as the chord (), the versine () (which appeared in the earliest tables), the coversine (), the haversine (), the exsecant (), and the excosecant (). See List of trigonometric identities for more relations between these functions.\n\nThe word \"sine\" derives from Latin \"sinus\", meaning \"bend; bay\", and more specifically \"the hanging fold of the upper part of a toga\", \"the bosom of a garment\", which was chosen as the translation of what was interpreted as the Arabic word \"jaib\", meaning \"pocket\" or \"fold\" in the twelfth-century translations of works by Al-Battani and al-Khwārizmī into Medieval Latin.\nThe choice was based on a misreading of the Arabic written form \"j-y-b\" (), which itself originated as a transliteration from Sanskrit ', which along with its synonym ' (the standard Sanskrit term for the sine) translates to \"bowstring\", being in turn adopted from Ancient Greek \"string\".\n\nThe word \"tangent\" comes from Latin \"tangens\" meaning \"touching\", since the line \"touches\" the circle of unit radius, whereas \"secant\" stems from Latin \"secans\"—\"cutting\"—since the line \"cuts\" the circle.\n\nThe prefix \"co-\" (in \"cosine\", \"cotangent\", \"cosecant\") is found in Edmund Gunter's \"Canon triangulorum\" (1620), which defines the \"cosinus\" as an abbreviation for the \"sinus complementi\" (sine of the complementary angle) and proceeds to define the \"cotangens\" similarly.\n\n"}
{"id": "31276757", "url": "https://en.wikipedia.org/wiki?curid=31276757", "title": "Yao graph", "text": "Yao graph\n\nIn computational geometry, the Yao graph, named after Andrew Yao, is a kind of geometric spanner, a weighted undirected graph connecting a set of geometric points with the property that, for every pair of points in the graph, their shortest path has a length that is within a constant factor of their Euclidean distance.\n\nThe basic idea underlying the two-dimensional Yao graph is to surround each of the given points by equally spaced rays, partitioning the plane into sectors with equal angles, and to connect each point to its nearest neighbor in each of these sectors. Associated with a Yao graph is an integer parameter which is the number of rays and sectors described above; larger values of produce closer approximations to the Euclidean distance. The stretch factor is at most formula_1, where formula_2 is the angle of the sectors. The same idea can be extended to point sets in more than two dimensions, but the number of sectors required grows exponentially with the dimension.\n\nAndrew Yao used these graphs to construct high-dimensional Euclidean minimum spanning trees.\n\n\n"}
{"id": "20805702", "url": "https://en.wikipedia.org/wiki?curid=20805702", "title": "Zero-forcing precoding", "text": "Zero-forcing precoding\n\nZero-forcing (or null-steering) precoding is a method of spatial signal processing by which the multiple antenna transmitter can null multiuser interference signals in wireless communications. Regularized zero-forcing precoding is enhanced processing to consider the impact on a background noise and unknown user interference, where the background noise and the unknown user interference can be emphasized in the result of (known) interference signal nulling.\n\nIn particular, null-steering is a method of beamforming for narrowband signals where we want to have a simple way of compensating delays of receiving signals from a specific source at different elements of the antenna array. In general to make better use of the antenna arrays, we sum and average the signals coming to different elements, but this is only possible when delays are equal. Otherwise, we first need to compensate the delays and then sum them up. To reach this goal, we may only add the weighted version of the signals with appropriate weight values. We do this in such a way that the frequency domain output of this weighted sum produces a zero result. This method is called null steering. The generated weights are of course related to each other and this relation is a function of delay and central working frequency of the source.\n\nIf the transmitter knows the downlink channel state information (CSI) perfectly, ZF-precoding can achieve almost the system capacity when the number of users is large. On the other hand, with limited channel state information at the transmitter (CSIT) the performance of ZF-precoding decreases depending on the accuracy of CSIT. ZF-precoding requires the significant feedback overhead with respect to signal-to-noise-ratio (SNR) so as to achieve the full multiplexing gain. Inaccurate CSIT results in the significant throughput loss because of residual multiuser interferences. Multiuser interferences remain since they can not be nulled with beams generated by imperfect CSIT.\n\nIn a multiple antenna downlink system which comprises an formula_1 transmit antenna access point (AP) and formula_2 single receive antenna users, the received signal of user formula_3 is described as\n\nwhere formula_5 is the formula_6 vector of transmitted symbols, formula_7 is the noise signal, formula_8 is the formula_6 channel vector and formula_10 is the formula_6 linear precoding vector. From the fact that each beam generated by ZF-precoding is orthogonal to all the other user channel vectors, one can rewrite the received signal as\n\nFor comparison purpose, we describe the received signal model for multiple antenna uplink systems. In the uplink system with a formula_13 receiver antenna AP and formula_2 K single transmit antenna user, the received signal at the AP is described as\nwhere formula_16 is the transmitted signal of user formula_17, formula_18 is the formula_19 noise vector, formula_8 is the formula_19 channel vector.\n\nQuantify the amount of the feedback resource required to maintain at least a given throughput performance gap between zero-forcing with perfect feedback and with limited feedback, i.e.,\n\nJindal showed that the required feedback bits of a spatially uncorrelated channel should be scaled according to SNR of the downlink channel, which is given by:\n\nwhere \"M\" is the number of transmit antennas and formula_24 is the SNR of the downlink channel.\n\nTo feed back \"B\" bits though the uplink channel, the throughput performance of the uplink channel should be larger than or equal to 'B'\n\nwhere formula_26 is the feedback resource consisted by multiplying the feedback frequency resource and the frequency temporal resource subsequently and formula_27 is SNR of the feedback channel. Then, the required feedback resource to satisfy formula_28 is \nNote that differently from the feedback bits case, the required feedback resource is a function of both downlink and uplink channel conditions. It is reasonable to include the uplink channel status in the calculation of the feedback resource since the uplink channel status determines the capacity, i.e., bits/second per unit frequency band (Hz), of the feedback link. Consider a case when SNR of the downlink and uplink are proportion such that formula_30 is constant and both SNRs are sufficiently high. Then, the feedback resource will be only proportional to the number of transmit antennas\n\nIt follows from the above equation that the feedback resource (formula_32) is not necessary to scale according to SNR of the downlink channel, which is almost contradict to the case of the feedback bits. One, hence, sees that the whole systematic analysis can reverse the facts resulted from each reductioned situation.\n\n\n"}
{"id": "693197", "url": "https://en.wikipedia.org/wiki?curid=693197", "title": "Μ operator", "text": "Μ operator\n\nIn computability theory, the μ operator, minimization operator, or unbounded search operator searches for the least natural number with a given property. Adding the μ-operator to the five primitive recursive operators makes it possible to define all computable functions.\n\nSuppose that R( y, x, . . ., x ) is a fixed \"k+1\"-ary relation on the natural numbers. The mu operator \"μy\", in either the unbounded or bounded form, is a \"number theoretic function\" defined from the natural numbers to the natural numbers. However, \"μy\" contains a \"predicate\" over the natural numbers that delivers \"true\" when the predicate is satisfied and \"false\" when it is not.\n\nThe \"bounded\" mu operator appears earlier in Kleene (1952) \"Chapter IX Primitive Recursive Functions, §45 Predicates, prime factor representation\", as:\n\nStephen Kleene notes that any of the six inequality restrictions on the range of the variable y is permitted, i.e. \"y < z\", \"y ≤ z\", \"w < y < z\", \"w < y ≤ z\", \"w ≤ y < z\", \"w ≤ y ≤ z\". \"When the indicated range contains no y such that R(y) [is \"true\"], the value of the \"μy\" expression is the cardinal number of the range\"(p. 226); this is why the default \"z\" appears in the definition above. As shown below, the bounded mu operator \"μy\" is defined in terms of two primitive recursive functions called the finite sum Σ and finite product Π, a predicate function that \"does the test\" and a representing function that converts { t, f } to { 0, 1 }.\n\nIn Chapter XI §57 General Recursive Functions, Kleene defines the \"unbounded\" μ-operator over the variable y in the following manner,\n\nIn this instance R itself, or its representing function, delivers 0 when it is satisfied (i.e. delivers \"true\"); the function then delivers the number y. No upper bound exists on y, hence no inequality expressions appear in its definition.\n\nFor a given R(y) the unbounded mu operator μyR(y) (note no requirement for \"(Ey)\" ) is a partial function. Kleene makes it as a total function instead (cf. p. 317):\nThe total version of the unbounded mu operator is studied in \"higher-order\" reverse mathematics () in the following form:\n\nformula_4\n\nThis axiom gives rise to the Big Five system ACA when combined with the usual base theory of higher-order reverse mathematics.\n\n(i) In context of the primitive recursive functions, where the search variable y of the μ-operator is bounded, e.g. y<z in the formula below, if the predicate R is primitive recursive (Kleene Proof #E p. 228), then\n\n(ii) In the context of the (total) recursive functions: Where the search variable y is \"unbounded\" but guaranteed to exist for \"all\" values x of the total recursive predicate R's parameters,\n\nthen the five primitive recursive operators plus the unbounded-but-total μ-operator give rise to what Kleene called the \"general\" recursive functions (i.e. total functions defined by the six recursion operators).\n\n(iii) In the context of the partial recursive functions: Suppose that the relation \"R\" holds if and only if a partial recursive function converges to zero. And suppose that that partial recursive function converges (to something, not necessarily zero) whenever formula_5 is defined and \"y\" is formula_5 or smaller. Then the function formula_5 is also a partial recursive function.\nThe μ operator is used in the characterization of the computable functions as the μ recursive functions.\n\nIn constructive mathematics, the unbounded search operator is related to Markov's principle.\n\nThe \"bounded\" μ-operator can be expressed rather simply in terms of two primitive recursive functions (hereafter \"prf\") that also are used to define the CASE function—the product-of-terms Π and the sum-of-terms Σ (cf Kleene #B page 224). (As needed, any boundary for the variable such as s≤t or t<z, or 5<x<17 etc. is appropriate). For example:\n\nBefore we proceed we need to introduce a function ψ called \"the representing function\" of predicate R. Function ψ is defined from inputs ( t= \"truth\", f=\"falsity\" ) to outputs ( 0, 1 ) (\"Observe the order!\"). In this case the input to ψ i.e. { t, f } is coming from the output of R:\n\nKleene demonstrates that μy R(y) is defined as follows; we see the product function Π is acting like a Boolean OR operator, and the sum Σ is acting somewhat like a Boolean AND but is producing { Σ≠0, Σ=0 } rather than just { 1, 0 }:\n\nThe equation is easier if observed with an example, as given by Kleene. He just made up the entries for the representing function ψ(R(y)). He designated the representing functions χ(y) rather than ψ( x, y ):\nThe unbounded μ operator—the function μy—is the one commonly defined in the texts. But the reader may wonder why—the modern texts do not state the reason—the unbounded μ-operator is searching for a function R(x, y) to yield \"zero\", rather than some other natural number.\n\nThe reason for \"zero\" is that the unbounded operator μy will be defined in terms of the function \"product\" Π with its index y allowed to \"grow\" as the μ operator searches. As noted in the example above, the product Π of a string of numbers ψ(x, 0) *, . . ., * ψ(x, y) yields zero whenever one of its members ψ(x, i) is zero:\n\nif any ψ(x, i)=0 where 0 ≤ i ≤ s. Thus the Π is acting like a Boolean AND.\n\nThe function μy produces as \"output\" a single natural number y = { 0, 1, 2, 3 ... }. However, inside the operator one of a couple \"situations\" can appear: (a) a \"number-theoretic function\" χ that produces a single natural number, or (b) a \"predicate\" R that produces either { t= true, f = false }. (And, in the context of \"partial\" recursive functions Kleene later admits a third outcome: \"μ = undecided\", pp. 332ff ).\n\nKleene splits his definition of the unbounded μ operator to handle the two situations (a) and (b). For situation (b), before the predicate R(x, y) can serve in an arithmetic capacity in the product Π, its output { t, f } must first be \"operated on\" by its \"representing function χ\" to yield { 0, 1 }. And for situation (a) if one definition is to be used then the \"number theoretic function χ\" must produce zero to \"satisfy\" the μ operator. With this matter settled, he demonstrates with single \"Proof III\" that either types (a) or (b) together with the five primitive recursive operators yield the (total) recursive functions ... with this proviso for a total function:\nKleene also admits a third situation (c) that does not require the demonstration of \"for all x a y exists such that ψ(x, y).\" He uses this in his proof that more total recursive functions exist than can be enumerated;\" cf footnote Total function demonstration.\n\nKleene's proof is informal and uses an example similar to the first example. Fbut first he casts the μ-operator into a different form that uses the \"product-of-terms\" Π operating on function χ that yields a natural number n where n can be any natural number, and 0 in the instance when the u operator's test is \"satisfied\".\n\nThis is subtle. At first glance the equations seem to be using primitive recursion. But Kleene has not provided us with a base step and an induction step of the general form:\n\nWhat is going on? First, we have to remind ourselves that we have assigned a parameter (a natural number) to every variable x. Second, we do see a successor-operator at work iterating y (i.e. the y'). And third, we see that the function μy χ(y, x) is just producing instances of χ(y,x) i.e. χ(0,x), χ(1,x), ... until an instance yields 0. Fourth, when an instance χ(n,x) yields 0 it causes the middle term of τ, i.e. v = π( x, y' ) to yield 0. Finally, when the middle term v = 0, μy χ(y) executes line (iii) and \"exits\". Kleene's presentation of equations (ii) and (iii) have been exchanged to make this point that line (iii) represents an \"exit\"—an exit taken only when the search successfully finds a y to satisfy χ(y) and the middle product-term π(x, y' ) is 0; the operator then terminates its search with τ(z', 0, y) = y.\n\nFor the example Kleene \"...consider[s] any fixed values of x, ... x) and write[s] simply \"χ(y)\" for \"χ(x, ... x),y)\":\n\nBoth Minsky (1967) p. 21 and Boolos-Burgess-Jeffrey (2002) p. 60-61 provide definitions of the μ operator as an abstract machine; see footnote Alternative definitions of μ.\n\nThe following demonstration follows Minsky without \"the peculiarity\" mentioned in the footnote. The demonstration will use a \"successor\" counter machine model closely related to the Peano Axioms and the primitive recursive functions. The model consists of (i) a finite state machine with a TABLE of instructions and a so-called 'state register' that we will rename \"the Instruction Register\" (IR), (ii) a few \"registers\" each of which can contain only a single natural number, and (iii) an instruction set of four \"commands\" described in the following table:\n\nThe algorithm for the minimization operator μy [φ( x, y )] will, in essence, create a sequence of instances of the function φ( x, y ) as the value of parameter y (a natural number) increases; the process will continue (see Note † below) until a match occurs between the output of function φ( x, y ) and some pre-established number (usually 0). Thus the evaluation of φ(x, y) requires, at the outset, assignment of a natural number to each of its variables x and an assignment of a \"match-number\" (usually 0) to a register \"w\", and a number (usually 0) to register y.\n\nIn the following we are assuming that the Instruction Register (IR) encounters the μy \"routine\" at instruction number \"n\". Its first action will be to establish a number in a dedicated \"w\" register—an \"example of\" the number that function φ( x, y ) must produce before the algorithm can terminate (classically this is the number zero, but see the footnote about the use of numbers other than zero). The algorithm's next action at instructiton \"n+1\" will be to clear the \"y\" register -- \"y\" will act as an \"up-counter\" that starts from 0. Then at instruction \"n+2\" the algorithm evaluates its function φ( x, y ) -- we assume this takes j instructions to accomplish—and at the end of its evaluation φ( x, y ) deposits its output in register \"φ\". At the n+j+3rd instruction the algorithm compares the number in the \"w\" register (e.g. 0) to the number in the \"φ\" register—if they are the same the algorithm has succeeded and it escapes through \"exit\"; otherwise it increments the contents of the \"y\" register and \"loops\" back with this new y-value to test function φ( x, y ) again.\n\n\nWhat is \"mandatory\" if the function is to be a total function is a demonstration \"by some other method\" (e.g. induction) that for each and every combination of values of its parameters x some natural number y will satisfy the μ-operator so that the algorithm that represents the calculation can terminate:\n\nFor an example of what this means in practice see the examples at mu recursive functions—even the simplest (\"improper\") subtraction algorithm \"x - y = d\" can yield, for the undefined cases when x < y, (1) no termination, (2) no numbers (i.e. something wrong with the format so the yield is not considered a natural number), or (3) deceit: wrong numbers in the correct format. The \"proper\" subtraction algorithm requires careful attention to all the \"cases\"\n\nThe unbounded μ operator is also defined by Boolos-Burgess-Jeffrey (2002) p. 60-61 for a counter machine with an instruction set equivalent to the following:\n\nIn this version the counter \"y\" is called \"r2\", and the function f( x, r2 ) deposits its number in register \"r3\". Perhaps the reason Boolos-Burgess-Jeffrey clear r3 is to facilitate an unconditional jump to \"loop\"; this is often done by use of a dedicated register \"0\" that contains \"0\":\n\n\n"}
