{"id": "391919", "url": "https://en.wikipedia.org/wiki?curid=391919", "title": "34 (number)", "text": "34 (number)\n\n34 (thirty-four) is the natural number following 33 and preceding 35.\n\n34 is the ninth distinct semiprime and has four divisors including one and itself. Its neighbors, 33 and 35, also are distinct semiprimes, having four divisors each, and 34 is the smallest number to be surrounded by numbers with the same number of divisors as it has.\n\nIt is the ninth Fibonacci number and a companion Pell number. Since it is an odd-indexed Fibonacci number, 34 is a Markov number, appearing in solutions with other Fibonacci numbers, such as (1, 13, 34), (1, 34, 89), etc.\n\nThis number is the magic constant of a 4 by 4 normal magic square:\n\nThirty-four is a heptagonal number.\n\nThere is no solution to the equation φ(\"x\") = 34, making 34 a nontotient. Nor is there a solution to the equation \"x\" − φ(\"x\") = 34, making 34 a noncototient.\n\n\n\n\n34 is also:\n\nRule 34 (Internet meme)\n\n"}
{"id": "6901703", "url": "https://en.wikipedia.org/wiki?curid=6901703", "title": "Algorithm characterizations", "text": "Algorithm characterizations\n\nAlgorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the \"characterizations\" of the notion of \"algorithm\" in more detail.\n\nOver the last 200 years the definition of algorithm has become more complicated and detailed as researchers have tried to pin down the term. Indeed, there may be more than one type of \"algorithm\". But most agree that algorithm has something to do with defining generalized processes for the creation of \"output\" integers from other \"input\" integers – \"input parameters\" arbitrary and infinite in extent, or limited in extent but still variable—by the manipulation of distinguishable symbols (counting numbers) with finite collections of rules that a person can perform with paper and pencil.\n\nThe most common number-manipulation schemes—both in formal mathematics and in routine life—are: (1) the recursive functions calculated by a person with paper and pencil, and (2) the Turing machine or its Turing equivalents—the primitive register machine or \"counter machine\" model, the Random Access Machine model (RAM), the Random access stored program machine model (RASP) and its functional equivalent \"the computer\".\n\nWhen we are doing \"arithmetic\" we are really calculating by the use of \"recursive functions\" in the shorthand algorithms we learned in grade-school, for example, adding and subtracting.\n\nThe proofs that every \"recursive function\" we can \"calculate by hand\" we can \"compute by machine\" and vice versa—note the usage of the words \"calculate\" versus \"compute\"—is remarkable. But this equivalence together with the \"thesis\" (unproven assertion) that this includes \"every\" calculation/computation indicates why so much emphasis has been placed upon the use of Turing-equivalent machines in the definition of specific algorithms, and why the definition of \"algorithm\" itself often refers back to \"the Turing machine\". This is discussed in more detail under Stephen Kleene's characterization.\n\nThe following are summaries of the more famous characterizations (Kleene, Markov, Knuth) together with those that introduce novel elements—elements that further expand the definition or contribute to a more precise definition.\n\nThere is more consensus on the \"characterization\" of the notion of \"simple algorithm\".\n\nAll algorithms need to be specified in a formal language, and the \"simplicity notion\" arises from the simplicity of the language. The Chomsky (1956) hierarchy is a containment hierarchy of classes of formal grammars that generate formal languages. It is used for classifying of programming languages and abstract machines.\n\nFrom the \"Chomsky hierarchy\" perspective, if the algorithm can be specified on a simpler language (than unrestricted), it can be characterized by this kind of language, else it is a typical \"unrestricted algorithm\".\n\nExamples: a \"general purpose\" macro language, like M4 is unrestricted (Turing complete), but the C preprocessor macro language is not, so any algorithm expressed in \"C preprocessor\" is a \"simple algorithm\".\n\nSee also Relationships between complexity classes.\n\nThe following are the features of a good algorithm.\n1. Precision: a good algorithm must have a certain outlined steps. The steps should be exact enough, and not varying.\n2. Uniqueness: each step taken in the algorithm should give a definite result as stated by the writer of the algorithm. The results should not fluctuate by any means.\n3. Feasibility: the algorithm should be possible and practicable in real life. It should not be abstract or imaginary.\n4. Input: a good algorithm must be able to accept a set of defined input.\n5. Output: a good algorithm should be able to produce results as output, preferably solutions.\n6. Finiteness: the algorithm should have a stop after a certain number of instructions.\n7. Generality: the algorithm must apply to a set of defined inputs.\n\nIn early 1870 W. Stanley Jevons presented a \"Logical Machine\" (Jevons 1880:200) for analyzing a syllogism or other logical form e.g. an argument reduced to a Boolean equation. By means of what Couturat (1914) called a \"sort of \"logical piano\" [,] ... the equalities which represent the premises ... are \"played\" on a keyboard like that of a typewriter. ... When all the premises have been \"played\", the panel shows only those constituents whose sum is equal to 1, that is, ... its logical whole. This mechanical method has the advantage over VENN's geometrical method...\" (Couturat 1914:75).\n\nFor his part John Venn, a logician contemporary to Jevons, was less than thrilled, opining that \"it does not seem to me that any contrivances at present known \"or likely to be discovered\" really deserve the name of logical machines\" (italics added, Venn 1881:120). But of historical use to the developing notion of \"algorithm\" is his explanation for his negative reaction with respect to a machine that \"may subserve a really valuable purpose by enabling us to avoid otherwise inevitable labor\":\nHe concludes that \"I cannot see that any machine can hope to help us except in the third of these steps; so that it seems very doubtful whether any thing of this sort really deserves the name of a logical engine.\"(Venn 1881:119–121).\n\nThis section is longer and more detailed than the others because of its importance to the topic: Kleene was the first to propose that \"all\" calculations/computations—of \"every\" sort, the \"totality\" of—can \"equivalently\" be (i) \"calculated\" by use of five \"primitive recursive operators\" plus one special operator called the mu-operator, or be (ii) \"computed\" by the actions of a Turing machine or an equivalent model.\n\nFurthermore, he opined that either of these would stand as a definition of algorithm.\n\nA reader first confronting the words that follow may well be confused, so a brief explanation is in order. \"Calculation\" means done by hand, \"computation\" means done by Turing machine (or equivalent). (Sometimes an author slips and interchanges the words). A \"function\" can be thought of as an \"input-output box\" into which a person puts natural numbers called \"arguments\" or \"parameters\" (but only the counting numbers including 0—the nonnegative integers) and gets out a single nonnegative integer (conventionally called \"the answer\"). Think of the \"function-box\" as a little man either calculating by hand using \"general recursion\" or computing by Turing machine (or an equivalent machine).\n\n\"Effectively calculable/computable\" is more generic and means \"calculable/computable by \"some\" procedure, method, technique ... whatever...\". \"General recursive\" was Kleene's way of writing what today is called just \"recursion\"; however, \"primitive recursion\"—calculation by use of the five recursive operators—is a lesser form of recursion that lacks access to the sixth, additional, mu-operator that is needed only in rare instances. Thus most of life goes on requiring only the \"primitive recursive functions.\"\n\nIn 1943 Kleene proposed what has come to be known as Church's thesis:\n\nIn a nutshell: to calculate \"any\" function the only operations a person needs (technically, formally) are the 6 primitive operators of \"general\" recursion (nowadays called the operators of the mu recursive functions).\n\nKleene's first statement of this was under the section title \"12. Algorithmic theories\". He would later amplify it in his text (1952) as follows:\n\nThis is not as daunting as it may sound – \"general\" recursion is just a way of making our everyday arithmetic operations from the five \"operators\" of the primitive recursive functions together with the additional mu-operator as needed. Indeed, Kleene gives 13 examples of primitive recursive functions and Boolos–Burgess–Jeffrey add some more, most of which will be familiar to the reader—e.g. addition, subtraction, multiplication and division, exponentiation, the CASE function, concatenation, etc., etc.; for a list see Some common primitive recursive functions.\nWhy general-recursive functions rather than primitive-recursive functions?\n\nKleene et al. (cf §55 General recursive functions p. 270 in Kleene 1952) had to add a sixth recursion operator called the minimization-operator (written as μ-operator or mu-operator) because Ackermann (1925) produced a hugely growing function—the Ackermann function—and Rózsa Péter (1935) produced a general method of creating recursive functions using Cantor's diagonal argument, neither of which could be described by the 5 primitive-recursive-function operators. With respect to the Ackermann function:\nBut the need for the mu-operator is a rarity. As indicated above by Kleene's list of common calculations, a person goes about their life happily computing primitive recursive functions without fear of encountering the monster numbers created by Ackermann's function (e.g. super-exponentiation ).\n\nTuring's Thesis hypothesizes the computability of \"all computable functions\" by the Turing machine model and its equivalents.\n\nTo do this in an effective manner, Kleene extended the notion of \"computable\" by casting the net wider—by allowing into the notion of \"functions\" both \"total functions\" and \"partial functions\". A \"total function\" is one that is defined \"for all natural numbers\" (positive integers including 0). A partial function is defined for \"some\" natural numbers but not all—the specification of \"some\" has to come \"up front\". Thus the inclusion of \"partial function\" extends the notion of function to \"less-perfect\" functions. Total- and partial-functions may either be calculated by hand or computed by machine.\n\nWe now observe Kleene's definition of \"computable\" in a formal sense:\n\nThus we have arrived at \"Turing's Thesis\":\n\nAlthough Kleene did not give examples of \"computable functions\" others have. For example, Davis (1958) gives Turing tables for the Constant, Successor and Identity functions, three of the five operators of the primitive recursive functions:\n\nBoolos–Burgess–Jeffrey (2002) give the following as prose descriptions of Turing machines for:\n\nWith regards to the counter machine, an abstract machine model equivalent to the Turing machine:\n\nDemonstrations of computability by abacus machine (Boolos–Burgess–Jeffrey (2002)) and by counter machine (Minsky 1967):\n\nThe fact that the abacus/counter machine models can simulate the recursive functions provides the proof that: If a function is \"machine computable\" then it is \"hand-calculable by partial recursion\". Kleene's Theorem XXIX :\n\nThe converse appears as his Theorem XXVIII. Together these form the proof of their equivalence, Kleene's Theorem XXX.\n\nWith his Theorem XXX Kleene proves the \"equivalence\" of the two \"Theses\"—the Church Thesis and the Turing Thesis. (Kleene can only hypothesize (conjecture) the truth of both thesis – \"these he has not proven\"):\n\nThus by Kleene's Theorem XXX: either method of making numbers from input-numbers—recursive functions calculated by hand or computated by Turing-machine or equivalent—results in an \"\"effectively calculable/computable\" function\". If we accept the hypothesis that \"every\" calculation/computation can be done by either method equivalently we have accepted both Kleene's Theorem XXX (the equivalence) and the Church–Turing Thesis (the hypothesis of \"every\").\n\nThe notion of separating out Church's and Turing's theses from the \"Church–Turing thesis\" appears not only in Kleene (1952) but in Blass-Gurevich (2003) as well. But while there are agreements, there are disagreements too:\n\nA. A. Markov (1954) provided the following definition of algorithm:\n\nHe admitted that this definition \"does not pretend to mathematical precision\" (p. 1). His 1954 monograph was his attempt to define algorithm more accurately; he saw his resulting definition—his \"normal\" algorithm—as \"equivalent to the concept of a recursive function\" (p. 3). His definition included four major components (Chapter II.3 pp. 63ff):\n\nIn his Introduction Markov observed that \"the entire significance for mathematics\" of efforts to define algorithm more precisely would be \"in connection with the problem of a constructive foundation for mathematics\" (p. 2). Ian Stewart (cf Encyclopædia Britannica) shares a similar belief: \"...constructive analysis is very much in the same algorithmic spirit as computer science...\". For more see constructive mathematics and Intuitionism.\n\nDistinguishability and Locality: Both notions first appeared with Turing (1936–1937) --\n\nLocality appears prominently in the work of Gurevich and Gandy (1980) (whom Gurevich cites). Gandy's \"Fourth Principle for Mechanisms\" is \"The Principle of Local Causality\":\n\n1936: A rather famous quote from Kurt Gödel appears in a \"Remark added in proof [of the original German publication] in his paper \"On the Length of Proofs\" translated by Martin Davis appearing on pp. 82–83 of \"The Undecidable\". A number of authors—Kleene, Gurevich, Gandy etc. -- have quoted the following:\n\n1963: In a \"Note\" dated 28 August 1963 added to his famous paper \"On Formally Undecidable Propositions\" (1931) Gödel states (in a footnote) his belief that \"formal systems\" have \"the characteristic property that reasoning in them, in principle, can be completely replaced by mechanical devices\" (p. 616 in van Heijenoort). \". . . due to \"A. M. Turing's work a precise and unquestionably adequate definition of the general notion of formal system can now be given [and] a completely general version of Theorems VI and XI is now possible.\" (p. 616). In a 1964 note to another work he expresses the same opinion more strongly and in more detail.\n\n1964: In a Postscriptum, dated 1964, to a paper presented to the Institute for Advanced Study in spring 1934, Gödel amplified his conviction that \"formal systems\" are those that can be mechanized:\n\nThe * indicates a footnote in which Gödel cites the papers by Alan Turing (1937) and Emil Post (1936) and then goes on to make the following intriguing statement:\n\nChurch's definitions encompass so-called \"recursion\" and the \"lambda calculus\" (i.e. the λ-definable functions). His footnote 18 says that he discussed the relationship of \"effective calculatibility\" and \"recursiveness\" with Gödel but that he independently questioned \"effectively calculability\" and \"λ-definability\":\n\nIt would appear from this, and the following, that far as Gödel was concerned, the Turing machine was sufficient and the lambda calculus was \"much less suitable.\" He goes on to make the point that, with regards to limitations on human reason, the jury is still out:\n\nMinsky (1967) baldly asserts that \"an algorithm is \"an effective procedure\" and declines to use the word \"algorithm\" further in his text; in fact his index makes it clear what he feels about \"Algorithm, \"synonym\" for Effective procedure\"(p. 311):\n\nOther writers (see Knuth below) use the word \"effective procedure\". This leads one to wonder: What is Minsky's notion of \"an effective procedure\"? He starts off with:\n\nBut he recognizes that this is subject to a criticism:\n\nHis refinement? To \"specify, along with the statement of the rules, \"the details of the mechanism that is to interpret them\"\". To avoid the \"cumbersome\" process of \"having to do this over again for each individual procedure\" he hopes to identify a \"reasonably \"uniform\" family of rule-obeying mechanisms\". His \"formulation\":\n\nIn the end, though, he still worries that \"there remains a subjective aspect to the matter. Different people may not agree on whether a certain procedure should be called effective\" (p. 107)\n\nBut Minsky is undeterred. He immediately introduces \"Turing's Analysis of Computation Process\" (his chapter 5.2). He quotes what he calls \"Turing's \"thesis\"\"\n\nAfter an analysis of \"Turing's Argument\" (his chapter 5.3)\nhe observes that \"equivalence of many intuitive formulations\" of Turing, Church, Kleene, Post, and Smullyan \"...leads us to suppose that there is really here an 'objective' or 'absolute' notion. As Rogers [1959] put it:\n\nIn his 1967 \"Theory of Recursive Functions and Effective Computability\" Hartley Rogers' characterizes \"algorithm\" roughly as \"a clerical (i.e., deterministic, bookkeeping) procedure . . . applied to . . . symbolic \"inputs\" and which will eventually yield, for each such input, a corresponding symbolic \"output\"\"(p. 1). He then goes on to describe the notion \"in approximate and intuitive terms\" as having 10 \"features\", 5 of which he asserts that \"virtually all mathematicians would agree [to]\" (p. 2). The remaining 5 he asserts \"are less obvious than *1 to *5 and about which we might find less general agreement\" (p. 3).\n\nThe 5 \"obvious\" are:\n\nThe remaining 5 that he opens to debate, are:\n\nKnuth (1968, 1973) has given a list of five properties that are widely accepted as requirements for an algorithm:\n\n\nKnuth offers as an example the Euclidean algorithm for determining the greatest common divisor of two natural numbers (cf. Knuth Vol. 1 p. 2).\n\nKnuth admits that, while his description of an algorithm may be intuitively clear, it lacks formal rigor, since it is not exactly clear what \"precisely defined\" means, or \"rigorously and unambiguously specified\" means, or \"sufficiently basic\", and so forth. He makes an effort in this direction in his first volume where he defines \"in detail\" what he calls the \"machine language\" for his \"mythical MIX...the world's first polyunsaturated computer\" (pp. 120ff). Many of the algorithms in his books are written in the MIX language. He also uses tree diagrams, flow diagrams and state diagrams.\n\n\"Goodness\" of an algorithm, \"best\" algorithms: Knuth states that \"In practice, we not only want algorithms, we want \"good\" algorithms...\" He suggests that some criteria of an algorithm's goodness are the number of steps to perform the algorithm, its \"adaptability to computers, its simplicity and elegance, etc.\" Given a number of algorithms to perform the same computation, which one is \"best\"? He calls this sort of inquiry \"algorithmic analysis: given an algorithm, to determine its performance characteristcis\" (all quotes this paragraph: Knuth Vol. 1 p. 7)\n\nStone (1972) and Knuth (1968, 1973) were professors at Stanford University at the same time so it is not surprising if there are similarities in their definitions (boldface added for emphasis):\n\nStone is noteworthy because of his detailed discussion of what constitutes an “effective” rule – his robot, or person-acting-as-robot, must have some information and abilities within them, and if not the information and the ability must be provided in \"the algorithm\":\n\nFurthermore, \"...not all instructions are acceptable, because they may require the robot to have abilities beyond those that we consider reasonable.” He gives the example of a robot confronted with the question is “Henry VIII a King of England?” and to print 1 if yes and 0 if no, but the robot has not been previously provided with this information. And worse, if the robot is asked if Aristotle was a King of England and the robot only had been provided with five names, it would not know how to answer. Thus:\n\nAfter providing us with his definition, Stone introduces the Turing machine model and states that the set of five-tuples that are the machine’s instructions are “an algorithm ... known as a Turing machine program” (p. 9). Immediately thereafter he goes on say that a “\"computation\" of a Turing machine is \"described\" by stating:\n\nThis precise prescription of what is required for \"a computation\" is in the spirit of what will follow in the work of Blass and Gurevich.\n\nWhile a student at Princeton in the mid-1960s, David Berlinski was a student of Alonzo Church (cf p. 160). His year-2000 book \"The Advent of the Algorithm: The 300-year Journey from an Idea to the Computer\" contains the following definition of algorithm:\n\nA careful reading of Gurevich 2000 leads one to conclude (infer?) that he believes that \"an algorithm\" is actually \"a Turing machine\" or \"a pointer machine\" doing a computation. An \"algorithm\" is not just the symbol-table that guides the behavior of the machine, nor is it just one instance of a machine doing a computation given a particular set of input parameters, nor is it a suitably programmed machine with the power off; rather \"an algorithm is the machine actually doing any computation of which it is capable\". Gurevich does not come right out and say this, so as worded above this conclusion (inference?) is certainly open to debate:\n\nIn Blass and Gurevich 2002 the authors invoke a dialog between \"Quisani\" (\"Q\") and \"Authors\" (A), using Yiannis Moshovakis as a foil, where they come right out and flatly state:\n\nThis use of the word \"implementation\" cuts straight to the heart of the question. Early in the paper, Q states his reading of Moshovakis:\n\nBut the authors waffle here, saying \"[L]et's stick to \"algorithm\" and \"machine\", and the reader is left, again, confused. We have to wait until Dershowitz and Gurevich 2007 to get the following footnote comment:\n\nBlass and Gurevich describe their work as evolved from consideration of Turing machines and pointer machines, specifically Kolmogorov-Uspensky machines (KU machines), Schönhage Storage Modification Machines (SMM), and linking automata as defined by Knuth. The work of Gandy and Markov are also described as influential precursors.\n\nGurevich offers a 'strong' definition of an algorithm (boldface added):\n\nThe above phrase computation as an evolution of the state differs markedly from the definition of Knuth and Stone—the \"algorithm\" as a Turing machine program. Rather, it corresponds to what Turing called \"the complete configuration\" (cf Turing's definition in Undecidable, p. 118) -- and includes \"both\" the current instruction (state) \"and\" the status of the tape. [cf Kleene (1952) p. 375 where he shows an example of a tape with 6 symbols on it—all other squares are blank—and how to Gödelize its combined table-tape status].\n\nIn Algorithm examples we see the evolution of the state first-hand.\n\nPhilosopher Daniel Dennett analyses the importance of evolution as an algorithmic process in his 1995 book \"Darwin's Dangerous Idea\". Dennett identifies three key features of an algorithm:\n\nIt is on the basis of this analysis that Dennett concludes that \"According to Darwin, evolution is an algorithmic process\". (p. 60).\n\nHowever, in the previous page he has gone out on a much-further limb. In the context of his chapter titled \"Processes as Algorithms\", he states:\n\nIt is unclear from the above whether Dennett is stating that the physical world by itself and without observers is intrinsically algorithmic (computational) or whether a symbol-processing observer is what is adding \"meaning\" to the observations.\n\nDaniel Dennett is a proponent of strong artificial intelligence: the idea that the logical structure of an algorithm is sufficient to explain mind. John Searle, the creator of the Chinese room thought experiment, claims that \"syntax [that is, logical structure] is by itself not sufficient for semantic content [that is, meaning]\" . In other words, the \"meaning\" of symbols is relative to the mind that is using them; an algorithm—a logical construct—by itself is insufficient for a mind.\n\nSearle cautions those who claim that algorithmic (computational) processes are intrinsic to nature (for example, cosmologists, physicists, chemists, etc.):\n\nAn example in Boolos-Burgess-Jeffrey (2002) (pp. 31–32) demonstrates the precision required in a complete specification of an algorithm, in this case to add two numbers: m+n. It is similar to the Stone requirements above.\n\n(i) They have discussed the role of \"number format\" in the computation and selected the \"tally notation\" to represent numbers:\n\n(ii) At the outset of their example they specify the machine to be used in the computation as a Turing machine. They have previously specified (p. 26) that the Turing-machine will be of the 4-tuple, rather than 5-tuple, variety. For more on this convention see Turing machine.\n\n(iii) Previously the authors have specified that the tape-head's position will be indicated by a subscript to the \"right\" of the scanned symbol. For more on this convention see Turing machine. (In the following, boldface is added for emphasis):\n\nThis specification is incomplete: it requires the location of where the instructions are to be placed and their format in the machine--\n\nThis later point is important. Boolos-Burgess-Jeffrey give a demonstration (p. 36) that the predictability of the entries in the table allow one to \"shrink\" the table by putting the entries in sequence and omitting the input state and the symbol. Indeed, the example Turing machine computation required only the 4 columns as shown in the table below (but note: these were presented to the machine in \"rows\"):\n\nSipser begins by defining '\"algorithm\" as follows:\n\nDoes Sipser mean that \"algorithm\" is just \"instructions\" for a Turing machine, or is the combination of \"instructions + a (specific variety of) Turing machine\"? For example, he defines the two standard variants (multi-tape and non-deterministic) of his particular variant (not the same as Turing's original) and goes on, in his Problems (pages 160-161), to describes four more variants (write-once, doubly infinite tape (i.e. left- and right-infinite), left reset, and \"stay put instead of left). In addition, he imposes some constraints. First, the input must be encoded as a string (p. 157) and says of numeric encodings in the context of complexity theory:\n\nvan Emde Boas comments on a similar problem with respect to the random access machine (RAM) abstract model of computation sometimes used in place of the Turing machine when doing \"analysis of algorithms\":\n\"The absence or presence of multiplicative and parallel bit manipulation operations is of relevance for the correct understanding of some results in the analysis of algorithms.\n\n\". . . [T]here hardly exists such as a thing as an \"innocent\" extension of the standard RAM model in the uniform time measures; either one only has additive arithmetic or one might as well include all reasonable multiplicative and/or bitwise Boolean instructions on small operands.\" (van Emde Boas, 1990:26)\n\nWith regards to a \"description language\" for algorithms Sipser finishes the job that Stone and Boolos-Burgess-Jeffrey started (boldface added). He offers us three levels of description of Turing machine algorithms (p. 157):\n\n"}
{"id": "1186896", "url": "https://en.wikipedia.org/wiki?curid=1186896", "title": "Ample line bundle", "text": "Ample line bundle\n\nIn algebraic geometry, a very ample line bundle is one with enough global sections to set up an embedding of its base variety or manifold formula_1 into projective space. An ample line bundle is one such that some positive power is very ample. Globally generated sheaves are those with enough sections to define a morphism to projective space.\n\nGiven a morphism formula_2, any vector bundle formula_3 on \"Y\", or more generally any sheaf in formula_4 modules, \"e.g.\" a coherent sheaf, can be pulled back to \"X\", (see Inverse image functor). This construction preserves the condition of being a line bundle, and more generally the rank.\n\nThe notions described in this article are related to this construction in the case of morphisms to projective spaces \nthe line bundle corresponding to the hyperplane divisor, whose sections are the 1-homogeneous regular functions. See Algebraic geometry of projective spaces#Divisors and twisting sheaves.\n\nLet \"X\" be a scheme or a complex manifold and \"F\" a sheaf on \"X\". One says that \"F\" is generated by (finitely many) global sections formula_7, if every stalk of \"F\" is generated as a module over the stalk of the structure sheaf by the germs of the \"a\". For example, if \"F\" happens to be a line bundle, i.e. locally free of rank 1, this amounts to having finitely many global sections, such that for any point \"x\" in \"X\", there is at least one section not vanishing at this point. In this case a choice of such global generators \"a\", ..., \"a\" gives a morphism\nsuch that the pullback \"f\"*(\"O\"(1)) is \"F\" (Note that this evaluation makes sense when \"F\" is a subsheaf of the constant sheaf of rational functions on \"X\"). The converse statement is also true: given such a morphism \"f\", the pullback of \"O\"(1) is generated by its global sections (on \"X\").\n\nMore generally, a sheaf generated by global sections is a sheaf \"F\" on a locally ringed space \"X\", with structure sheaf \"O\" that is of a rather simple type. Assume \"F\" is a sheaf of abelian groups. Then it is asserted that if \"A\" is the abelian group of global sections, i.e.\n\nthen for any open set \"U\" of \"X\", ρ(\"A\") spans \"F\"(\"U\") as an \"O\"-module. Here\n\nis the restriction map. In words, all sections of \"F\" are locally generated by the global sections.\n\nAn example of such a sheaf is that associated in algebraic geometry to an \"R\"-module \"M\", \"R\" being any commutative ring, on the spectrum of a ring \"Spec\"(\"R\").\nAnother example: according to Cartan's theorem A, any coherent sheaf on a Stein manifold is spanned by global sections.\n\nGiven a scheme \"X\" over a base scheme \"S\" or a complex manifold, a line bundle (or in other words an invertible sheaf, that is, a locally free sheaf of rank one) \"L\" on \"X\" is said to be very ample, if there is an embedding \"i : X → \"P, the \"n\"-dimensional projective space over \"S\" for some \"n\", such that the pullback of the standard twisting sheaf \"O\"(1) on P is isomorphic to \"L\":\n\nHence this notion is a special case of the previous one, namely a line bundle is very ample if it is globally generated and the morphism given by some global generators is an embedding.\n\nGiven a very ample sheaf \"L\" on \"X\" and a coherent sheaf \"F\", a theorem of Serre shows that (the coherent sheaf) \"F ⊗ L\" is generated by finitely many global sections for sufficiently large \"n\". This in turn implies that global sections and higher (Zariski) cohomology groups \nformula_12\nare finitely generated. This is a distinctive feature of the projective situation. For example, for the affine \"n\"-space \"A\" over a field \"k\", global sections of the structure sheaf \"O\" are polynomials in \"n\" variables, thus not a finitely generated \"k\"-vector space, whereas for P, global sections are just constant functions, a one-dimensional \"k\"-vector space.\n\nThe notion of ample line bundles \"L\" is slightly weaker than very ample line bundles: a line bundle \"L\" is ample if for any coherent sheaf \"F\" on \"X\", there exists an integer \"n(F)\", such that \"F\" ⊗ \"L\" is generated by its global sections for \"n > n(F)\".\n\nAn equivalent, maybe more intuitive, definition of the ampleness of the line bundle formula_13 is its having a positive tensorial power that is very ample. In other words, for formula_14 there exists a projective embedding formula_15 such that formula_16, that is the zero divisors of global sections of formula_17\nare hyperplane sections.\n\nThis definition makes sense for the underlying \"divisors\" (Cartier divisors) formula_18; an ample formula_18 is one where formula_20 \"moves in a large enough linear system\". Such divisors form a cone in all divisors of those that are, in some sense, \"positive enough\". The relationship with projective space is that the formula_18 for a very ample formula_22 corresponds to the hyperplane sections (intersection with some hyperplane) of the embedded formula_1.\n\nThe equivalence between the two definitions is credited to Jean-Pierre Serre in Faisceaux algébriques cohérents.\n\nAmplitude is an open condition under small perturbations of a divisor. If \"H\" is an ample \"Q\"-divisor on \"X\", and \"E\" an arbitrary \"Q-divisor.\" Then formula_24 is ample for all sufficiently small rational numbers formula_25\n\n\nTo decide in practice when a Cartier divisor \"D\" corresponds to an ample line bundle, there are some geometric criteria.\n\nFor curves, a divisor \"D\" is very ample if and only if\n\"l\"(\"D\") = 2 + \"l\"(\"D\" − \"A\" − \"B\") whenever \"A\" and \"B\" are points. By the Riemann–Roch theorem every divisor of degree\nat least 2\"g\" + 1 satisfies this condition so is very ample. This implies that a divisor is ample if and only if it has positive degree. The canonical divisor of degree 2\"g\" − 2 is very ample if and only if the curve is not\na hyperelliptic curve.\n\nThe Nakai–Moishezon criterion (, ) states that a Cartier divisor \"D\" on a proper scheme \"X\" over an algebraically closed field is ample if and only if \"D\".\"Y\" > 0 for every closed integral subscheme \"Y\" of \"X\". In the special case of curves this says that a divisor is ample if and only if it has positive degree, and for a smooth projective algebraic surface \"S\", the Nakai–Moishezon criterion states that \"D\" is ample if and only if its self-intersection number \"D\".\"D\" is strictly positive, and for any irreducible curve \"C\" on \"S\" we have \"D\".\"C\" > 0.\n\nFor an formula_37-divisor \"D\" Nakai criterion is slightly more subtle. In fact, if \"D\" is an ample formula_37 divisor, then certainly we have Nakai inequality, but it is no longer clear that Nakai inequality characterize amplitude. In the case when \"X\" is projective, then Nakai inequality characterize amplitude. \nThe Kleiman condition states that for any projective scheme \"X\", a divisor \"D\" on \"X\" is ample if and only if \"D\".\"C\" > 0 for any nonzero element \"C\" in the closure of NE(\"X\"), the cone of curves of \"X\". In other words, a divisor is ample if and only if it is in the interior of the real cone generated by nef divisors.\n\nThis shows that the condition \"D\".\"D\" > 0 cannot be omitted in the Nakai–Moishezon criterion, and it is necessary to use the closure of NE(\"X\") rather than NE(\"X\") in the Kleiman condition.\n\ndeg(\"L\"|) ≥ ε\"m\"(\"C\") for all integral curves \"C\" in \"X\", where \"m\"(\"C\") is the\nmaximum of the multiplicities at the points of \"C\".\n\nThe theorem of Cartan-Serre-Grothendieck states that for a line bundle formula_13 on a variety formula_40, the following conditions are equivalent:\n\nIf formula_40 is proper over some noetherian ring, this is also equivalent to:\n\nLet \"D\" be a Cartier divisor on a projective algebraic scheme \"X\". Then \"D\" is ample if and only if it satisfies either of the following properties:\n\n(I). For every irreducible subvariety formula_48 of positive dimension, there is a positive integer \"m\", together with a non-zero section formula_49, such that \"s\" vanishes at some point of \"V\".\n\n(II) For every irreducible subvariety formula_48 of positive dimension,\n\nformula_51 as formula_52\n\nA locally free sheaf (vector bundle) formula_53 on a variety is called ample if the invertible sheaf formula_54 on formula_55 is ample .\n\nAmple vector bundles inherit many of the properties of ample line bundles.\n\nAn important generalization, notably in birational geometry, is that of a big line bundle. A line bundle formula_13 on \"X\" is said to be big if the equivalent following conditions are satisfied:\nThe interest of this notion is its stability with respect to rational transformations.\n\n\n\n\n"}
{"id": "29866108", "url": "https://en.wikipedia.org/wiki?curid=29866108", "title": "B-theorem", "text": "B-theorem\n\nB-Theorem is a mathematical finite group theory formerly known as the B-Conjecture.\n\nThe theorem states that if formula_1 is the centralizer of an involution of a finite group, then every component of formula_2 is the image of a component of formula_1. \n"}
{"id": "39329230", "url": "https://en.wikipedia.org/wiki?curid=39329230", "title": "Bipartite half", "text": "Bipartite half\n\nIn graph theory, the bipartite half or half-square of a bipartite graph \"G\" = (\"U\",\"V\",\"E\") is a graph whose vertex set is one of the two sides of the bipartition (without loss of generality, \"U\") and in which there is an edge \"u\"\"u\" for each two vertices \"u\" and \"u\" in \"U\" that are at distance two from each other in \"G\". That is, in a more compact notation, the bipartite half is \"G\"[\"U\"] where the superscript 2 denotes the square of a graph and the square brackets denote an induced subgraph.\n\nFor instance, the bipartite half of the complete bipartite graph \"K\" is the complete graph \"K\" and the bipartite half of the hypercube graph is the halved cube graph.\nWhen \"G\" is a distance-regular graph, its two bipartite halves are both distance-regular.\n\nThe map graphs, that is, the intersection graphs of interior-disjoint simply-connected regions in the plane, are exactly the bipartite halves of bipartite planar graphs.\n\n"}
{"id": "13931788", "url": "https://en.wikipedia.org/wiki?curid=13931788", "title": "Bob Hale (philosopher)", "text": "Bob Hale (philosopher)\n\nBob Hale, FRSE (1945 – 12 December 2017), was a British philosopher, known for his contributions to the development of the neo-Fregean (neo-logicist) philosophy of mathematics in collaboration with Crispin Wright, and for his works in modality and philosophy of language.\n\nHale obtained a BSc in Philosophy in 1967 from Linacre College, University of Oxford. From 2006 until his death, he was a professor of philosophy in the department of philosophy at the University of Sheffield. Prior to that, he taught in the University of Glasgow, the University of St. Andrews and the University of Lancaster.\n\nHale produced the first published neo-Fregean construction of the real numbers. In his book (\"Necessary Beings\"), he argues for an essentialist theory of necessity and possibility.\n\n\n\n"}
{"id": "17941791", "url": "https://en.wikipedia.org/wiki?curid=17941791", "title": "Book of Lemmas", "text": "Book of Lemmas\n\nThe Book of Lemmas is a book attributed to Archimedes by Thābit ibn Qurra, though the authorship of the book is questionable. It consists of fifteen propositions (lemmas) on circles.\n\nThe \"Book of Lemmas\" was first introduced in Arabic by Thābit ibn Qurra; he attributed the work to Archimedes. In 1661, the Arabic manuscript was translated into Latin by Abraham Ecchellensis and edited by Giovanni A. Borelli. The Latin version was published under the name \"Liber Assumptorum\". T. L. Heath translated Heiburg's Latin work into English in his \"The Works of Archimedes\".\n\nThe original authorship of the \"Book of Lemmas\" has been in question because in proposition four, the book refers to Archimedes in third person; however, it has been suggested that it may have been added by the translator. Another possibility is that the \"Book of Lemmas\" may be a collection of propositions by Archimedes later collected by a Greek writer.\n\nThe Book of Lemmas introduces several new geometrical figures.\n\nArchimedes first introduced the arbelos in proposition four of his book:\n\nThe figure is used in propositions four through eight. In propositions five, Archimedes introduces the Archimedes's twin circles, and in proposition eight, he makes use what would be the Pappus chain, formally introduced by Pappus of Alexandria.\n\nArchimedes first introduced the salinon in proposition fourteen of his book:\n\nArchimedes proved that the salinon and the circle are equal in area.\n\n"}
{"id": "3203851", "url": "https://en.wikipedia.org/wiki?curid=3203851", "title": "Bragg plane", "text": "Bragg plane\n\nIn physics, a Bragg plane is a plane in reciprocal space which bisects a reciprocal lattice vector, formula_1, at right angles. The Bragg plane is defined as part of the Von Laue condition for diffraction peaks in x-ray diffraction crystallography.\n\nConsidering the adjacent diagram, the arriving x-ray plane wave is defined by:\n\nWhere formula_3 is the incident wave vector given by:\n\nwhere formula_5 is the wavelength of the incident photon. While the Bragg formulation assumes a unique choice of direct lattice planes and specular reflection of the incident X-rays, the Von Laue formula only assumes monochromatic light and that each scattering center acts as a source of secondary wavelets as described by the Huygens principle. Each scattered wave contributes to a new plane wave given by:\n\nThe condition for constructive interference in the formula_7 direction is that the path difference between the photons is an integer multiple (m) of their wavelength. We know then that for constructive interference we have:\n\nwhere formula_9. Multiplying the above by formula_10 we formulate the condition in terms of the wave vectors, formula_3 and formula_12:\n\nNow consider that a crystal is an array of scattering centres, each at a point in the Bravais lattice. We can set one of the scattering centres as the origin of an array. Since the lattice points are displaced by the Bravais lattice vectors, formula_14, scattered waves interfere constructively when the above condition holds simultaneously for all values of formula_14 which are Bravais lattice vectors, the condition then becomes:\n\nAn equivalent statement (see mathematical description of the reciprocal lattice) is to say that:\n\nBy comparing this equation with the definition of a reciprocal lattice vector, we see that constructive interference occurs if formula_18 is a vector of the reciprocal lattice. We notice that formula_3 and formula_12 have the same magnitude, we can restate the Von Laue formulation as requiring that the tip of incident wave vector, formula_3, must lie in the plane that is a perpendicular bisector of the reciprocal lattice vector, formula_1. This reciprocal space plane is the \"Bragg plane\".\n\n"}
{"id": "5648782", "url": "https://en.wikipedia.org/wiki?curid=5648782", "title": "Cologarithm", "text": "Cologarithm\n\nIn mathematics, the base-\"b\" cologarithm, sometimes shortened to colog, of a number is the base-\"b\" logarithm of the reciprocal of the number. It is equal to the \"negative\" base-\"b\" logarithm of the number.\n\nThe cologarithm in base b of a number is also equal to the logarithm of the same number having the reciprocal of b as the base:\n\nIn chemistry, a decimal cologarithm is indicated by the letter p. This usage originated with the quantity pH, defined as –log [HO]. Based on pH, the quantity p\"K\" was later defined as –log \"K\".\n\n\n"}
{"id": "32376085", "url": "https://en.wikipedia.org/wiki?curid=32376085", "title": "Consumer math", "text": "Consumer math\n\nConsumer math comprises practical mathematical techniques used in commerce and everyday life. In the United States, consumer math is typically offered in high schools, some elementary schools, or in some colleges which grant associate's degrees. \n\nA U.S. consumer math course might include a review of elementary arithmetic, including fractions, decimals, and percentages. Elementary algebra is often included as well, in the context of solving practical business problems. The practical applications typically include: changing money, checking accounts, budgeting, price discounts, markups and markdowns, payroll calculations, investing (simple and compound interest), taxes, consumer and business credit, and mortgages.\n\nThe emphasis in these courses is on computational skills and their practical application, with practical application being predominant. For instance, while computational formulas are covered in the material on interest and mortgages, the use of prepared tables based on those formulas is also presented and emphasized.\n\nSome consumer math courses are designed to simulate a real life environment. At ConsumerMath.org, high school students are able to \"purchase\" their grades instead of working for points. Students earn money by completing assignments, and paying their bills. Students must learn to stay out of debt as they buy cars, pay for insurance, and learn about credit cards. Success is determined by the student successfully staying out of debt during the course.\n\n\n"}
{"id": "19859862", "url": "https://en.wikipedia.org/wiki?curid=19859862", "title": "Cosmic space", "text": "Cosmic space\n\nIn mathematics, particularly topology, a cosmic space is any topological space that is a continuous image of some separable metric space. Equivalently (for regular \"T\" spaces but not in general), a space is cosmic if and only if it has a countable network; namely a countable collection of subsets of the space such that any open set is the union of a subcollection of these sets.\n\nCosmic spaces have several interesting properties. There are a number of unsolved problems about them.\n\n\nIt is unknown as to whether \"X\" is cosmic if:\n\na) \"X\" contains no uncountable discrete space;\n\nb) the countable product of \"X\" with itself is hereditarily separable and hereditarily Lindelöf.\n\n\n"}
{"id": "31320359", "url": "https://en.wikipedia.org/wiki?curid=31320359", "title": "Credal set", "text": "Credal set\n\nA credal set is a set of probability distributions or, more generally, a set of (possibly finitely additive) probability measures. A credal set is often assumed or constructed to be a closed convex set. It is intended to express uncertainty or doubt about the probability model that should be used, or to convey the beliefs of a Bayesian agent about the possible states of the world.\n\nIf a credal set formula_1 is closed and convex, then, by the Krein–Milman theorem, it can be equivalently described by its extreme points formula_2. In that case, the expectation for a function formula_3 of formula_4 with respect to the credal set formula_1 forms a closed interval formula_6, whose lower bound is called the lower prevision of formula_3, and whose upper bound is called the upper prevision of formula_3:\nwhere formula_10 denotes a probability measure, and with a similar expression for formula_11 (just replace formula_12 by formula_13 in the above expression).\n\nIf formula_4 is a categorical variable, then the credal set formula_1 can be considered as a set of probability mass functions over formula_4.\nIf additionally formula_1 is also closed and convex, then the lower prevision of a function formula_3 of formula_4 can be simply evaluated as:\nwhere formula_21 denotes a probability mass function.\nIt is easy to see that a credal set over a Boolean variable formula_4 cannot have more than two extreme points (because the only closed convex sets in formula_23 are closed intervals), while credal sets over variables formula_4 that can take three or more values can have any arbitrary number of extreme points.\n\n"}
{"id": "7404967", "url": "https://en.wikipedia.org/wiki?curid=7404967", "title": "DLOGTIME", "text": "DLOGTIME\n\nIn computational complexity theory, DLOGTIME is the complexity class of all computational problems solvable in a logarithmic amount of computation time on a deterministic Turing machine. It must be defined on a random-access Turing machine, since otherwise the input tape is longer than the range of cells that can be accessed by the machine. It is a very weak model of time complexity: no random-access Turing machine with a smaller deterministic time bound can access the whole input.\n\nDLOGTIME-uniformity is important in circuit complexity.\n"}
{"id": "19287844", "url": "https://en.wikipedia.org/wiki?curid=19287844", "title": "DNA sequencing theory", "text": "DNA sequencing theory\n\nDNA sequencing theory is the broad body of work that attempts to lay analytical foundations for determining the order of specific nucleotides in a sequence of DNA, otherwise known as DNA sequencing. The practical aspects revolve around designing and optimizing sequencing projects (known as \"strategic genomics\"), predicting project performance, troubleshooting experimental results, characterizing factors such as sequence bias and the effects of software processing algorithms, and comparing various sequencing methods to one another. In this sense, it could be considered a branch of systems engineering or operations research. The permanent archive of work is primarily mathematical, although numerical calculations are often conducted for particular problems too. DNA sequencing theory addresses \"physical processes\" related to sequencing DNA and should not be confused with theories of analyzing resultant DNA sequences, e.g. sequence alignment. Publications sometimes do not make a careful distinction, but the latter are primarily concerned with algorithmic issues. Sequencing theory is based on elements of mathematics, biology, and systems engineering, so it is highly interdisciplinary. The subject may be studied within the context of computational biology.\n\nAll mainstream methods of DNA sequencing rely on reading small fragments of DNA and subsequently reconstructing these data to infer the original DNA target, either via assembly or alignment to a reference. The abstraction common to these methods is that of a mathematical covering problem. For example, one can imagine a line segment representing the target and a subsequent process where smaller segments are \"dropped\" onto random locations of the target. The target is considered \"sequenced\" when adequate coverage accumulates (e.g., when no gaps remain).\n\nThe abstract properties of covering have been studied by mathematicians for over a century. However, direct application of these results has not generally been possible. Closed-form mathematical solutions, especially for probability distributions, often cannot be readily evaluated. That is, they involve inordinately large amounts of computer time for parameters characteristic of DNA sequencing. Stevens' configuration is one such example. Results obtained from the perspective of pure mathematics also do not account for factors that are actually important in sequencing, for instance detectable overlap in sequencing fragments, double-stranding, edge-effects, and target multiplicity. Consequently, development of sequencing theory has proceeded more according to the philosophy of applied mathematics. In particular, it has been problem-focused and makes expedient use of approximations, simulations, etc.\n\nThe earliest result may be found directly from elementary probability theory. Suppose we model the above process taking formula_1 and formula_2 as the fragment length and target length, respectively. The probability of \"covering\" any given location on the target \"with one particular fragment\" is then formula_3. (This presumes formula_4, which is valid often, but not for all real-world cases.) The probability of a single fragment not covering a given location on the target is therefore formula_5, and formula_6 for formula_7 fragments. The probability of covering a given location on the target with \"at least one\" fragment is therefore\n\nThis equation was first used to characterize plasmid libraries, but it may appear in a modified form. For most projects formula_9, so that, to a good degree of approximation\n\nwhere formula_11 is called the \"redundancy\". Note the significance of redundancy as representing the average number of times a position is covered with fragments. Note also that in considering the covering process over all positions in the target, this probability is identical to the expected value of the random variable formula_12, the fraction of the target coverage. The final result,\n\nremains in widespread use as a \"back of the envelope\" estimator and predicts that coverage for all projects evolves along a universal curve that is a function only of the redundancy.\n\nIn 1988, Eric Lander and Michael Waterman published an important paper examining the covering problem from the standpoint of gaps. Although they focused on the so-called mapping problem, the abstraction to sequencing is much the same. They furnished a number of useful results that were adopted as the standard theory from the earliest days of \"large-scale\" genome sequencing. Their model was also used in designing the Human Genome Project and continues to play an important role in DNA sequencing.\n\nUltimately, the main goal of a sequencing project is to close all gaps, so the \"gap perspective\" was a logical basis of developing a sequencing model. One of the more frequently used results from this model is the expected number of contigs, given the number of fragments sequenced. If one neglects the amount of sequence that is essentially \"wasted\" by having to detect overlaps, their theory yields\n\nIn 1995, Roach published improvements to this theory, enabling it to be applied to sequencing projects in which the goal was to completely sequence a target genome. Michael Wendl and Bob Waterston confirmed, based on Stevens' method, that both models produced similar results when the number of contigs was substantial, such as in low coverage mapping or sequencing projects. As sequencing projects ramped up in the 1990s, and projects approached completion, low coverage approximations became inadequate, and the exact model of Roach was necessary. However, as the cost of sequencing dropped, parameters of sequencing projects became easier to directly test empirically, and interest and funding for strategic genomics diminished\n\nThe basic ideas of Lander–Waterman theory led to a number of additional results for particular variations in mapping techniques. However, technological advancements have rendered mapping theories largely obsolete except in organisms other than highly studied model organisms (e.g., yeast, flies, mice, and humans).\n\nThe parking strategy for sequencing resembles the process of parking cars along a curb. Each car is a sequenced clone, and the curb is the genomic target. Each clone sequenced is screened to ensure that subsequently sequenced clones do not overlap any previously sequenced clone. No sequencing effort is redundant in this strategy. However, much like the gaps between parked cars, unsequenced gaps less than the length of a clone accumulate between sequenced clones. There can be considerable cost to close such gaps.\n\nIn 1995, Roach \"et al.\" proposed and demonstrated through simulations a generalization of a set of strategies explored earlier by Edwards and Caskey. This whole-genome sequencing method became immensely popular as it was championed by Celera and used to sequence several model organisms before Celera applied it to the human genome. Today, most sequencing projects employ this strategy, often called paired end sequencing.\n\nThe physical processes and protocols of DNA sequencing have continued to evolve, largely driven by advancements in bio-chemical methods, instrumentation, and automation techniques. There is now a wide range of problems that DNA sequencing has made in-roads into, including metagenomics and medical (cancer) sequencing. There are important factors in these scenarios that classical theory does not account for. Recent work has begun to focus on resolving the effects of some of these issues. The level of mathematics becomes commensurately more sophisticated.\n\nBiologists have developed methods to filter highly-repetitive, essentially un-sequenceable regions of genomes. These procedures are important for organisms whose genomes consist mostly of such DNA, for example corn. They yield multitudes of small islands of sequenceable DNA products. Wendl and Barbazuk proposed an extension to Lander–Waterman Theory to account for \"gaps\" in the target due to filtering and the so-called \"edge-effect\". The latter is a position-specific sampling bias, for example the terminal base position has only a formula_15 chance of being covered, as opposed to formula_3 for interior positions. For formula_17, classical Lander–Waterman Theory still gives good predictions, but dynamics change for higher redundancies.\n\nModern sequencing methods usually sequence both ends of a larger fragment, which provides linking information for \"de novo\" assembly and improved probabilities for alignment to reference sequence. Researchers generally believe that longer lengths of data (read lengths) enhance performance for very large DNA targets, an idea consistent with predictions from distribution models. However, Wendl showed that smaller fragments provide better coverage on small, linear targets because they reduce the edge effect in linear molecules. These findings have implications for sequencing the products of DNA filtering procedures. Read-pairing and fragment size evidently have negligible influence for large, whole-genome class targets.\n\nSequencing is emerging as an important tool in medicine, for example in cancer research. Here, the ability to detect heterozygous mutations is important and this can only be done if the sequence of the diploid genome is obtained. In the pioneering efforts to sequence individuals, Levy \"et al.\" and Wheeler \"et al.\", who sequenced Craig Venter and Jim Watson, respectively, outlined models for covering both alleles in a genome. Wendl and Wilson followed with a more general theory that allowed for an arbitrary number of coverings of each allele and arbitrary ploidy. These results point to the general conclusion that the amount of data needed for such projects is significantly higher than for traditional haploid projects. Generally, at least 30-fold redundancy, i.e. each nucleotide spanned by an average of 30 sequence reads, is now standard.\nHowever, requirements can be even greater, depending upon what kinds of genomic events are to be found. For example, in the so-called \"discordant read pairs method\", DNA insertions can be inferred if the distance between read pairs is larger than expected. Calculations show that around 50-fold redundancy is needed to avoid false-positive errors at 1% threshold.\n\nThe advent of next-generation sequencing has also made large-scale population sequencing feasible, for example the 1000 Genomes Project to characterize variation in human population groups. While common variation is easily captured, rare variation poses a design challenge: too few samples with significant sequence redundancy risks not having a variant in the sample group, but large samples with light redundancy risk not capturing a variant in the read set that is actually in the sample group. Wendl and Wilson report a simple set of optimization rules that maximize the probability of discovery for a given set of parameters. For example, for observing a rare allele at least twice (to eliminate the possibility is unique to an individual) a little less than 4-fold redundancy should be used, regardless of the sample size.\n\nNext-generation instruments are now also enabling the sequencing of whole uncultured metagenomic communities. The sequencing scenario is more complicated here and there are various ways of framing design theories for a given project. For example, Stanhope developed a probabilistic model for the amount of sequence needed to obtain at least one contig of a given size from each novel organism of the community, while Wendl et al. reported analysis for the average contig size or the probability of completely recovering a novel organism for a given rareness within the community. Conversely, Hooper et al. propose a semi-empirical model based on the gamma distribution.\n\nDNA sequencing theories often invoke the assumption that certain random variables in a model are independent and identically distributed. For example, in Lander–Waterman Theory, a sequenced fragment is presumed to have the same probability of covering each region of a genome and all fragments are assumed to be independent of one another. In actuality, sequencing projects are subject to various types of bias, including differences of how well regions can be cloned, sequencing anomalies, biases in the target sequence (which is \"not\" random), and software-dependent errors and biases. In general, theory will agree well with observation up to the point that enough data have been generated to expose latent biases. The kinds of biases related to the underlying target sequence are particularly difficult to model, since the sequence itself may not be known \"a priori\". This presents a type of Catch-22 (logic) problem.\n\n"}
{"id": "41375260", "url": "https://en.wikipedia.org/wiki?curid=41375260", "title": "Differentiable stack", "text": "Differentiable stack\n\nIn differential geometry, a differentiable stack is a stack over the category of differentiable manifolds (with the usual open covering topology) which admits an atlas. In other words, a differentiable stack is a stack that can be represented by a Lie groupoid.\n\nEvery Lie groupoid Γ gives rise to a differentiable stack that is the category of Γ-torsors. In fact, every differentiable stack is of this form. Hence, roughly, \"a differentiable stack is a Lie groupoid up to Morita equivalence.\"\n\nA differentiable space is a differentiable stack with trivial stabilizers. For example, if a Lie group acts freely but not necessarily properly on a manifold, then the quotient by it is in general not a manifold but a differentiable space.\n\nA differentiable stack \"X\" may be equipped with Grothendieck topology in a certain way (see the reference). This gives the notion of a sheaf over \"X\". For example, the sheaf formula_1 of differential \"p\"-forms over \"X\" is given by, for any \"x\" in \"X\" over a manifold \"U\", letting formula_2 be the space of \"p\"-forms on \"U\". The sheaf formula_3 is called the structure sheaf on \"X\" and is denoted by formula_4. formula_5 comes with exterior derivative and thus is a complex of sheaves of vector spaces over \"X\": one thus has the notion of de Rham cohomology of \"X\".\n\nAn epimorphism between differentiable stacks formula_6 is called a gerbe over \"X\" if formula_7 is also an epimorphism. For example, if \"X\" is a stack, formula_8 is a gerbe. A theorem of Giraud says that formula_9 corresponds one-to-one to the set of gerbes over \"X\" that are locally isomorphic to formula_8 and that come with trivializations of their bands.\n\n\n"}
{"id": "627828", "url": "https://en.wikipedia.org/wiki?curid=627828", "title": "Double Mersenne number", "text": "Double Mersenne number\n\nIn mathematics, a double Mersenne number is a Mersenne number of the form\n\nwhere \"p\" is prime.\n\nThe first four terms of the sequence of double Mersenne numbers are :\n\nA double Mersenne number that is prime is called a double Mersenne prime. Since a Mersenne number \"M\" can be prime only if \"p\" is prime, (see Mersenne prime for a proof), a double Mersenne number formula_6 can be prime only if \"M\" is itself a Mersenne prime. For the first values of \"p\" for which \"M\" is prime, formula_7 is known to be prime for \"p\" = 2, 3, 5, 7 while explicit factors of formula_7 have been found for \"p\" = 13, 17, 19, and 31.\n\nThus, the smallest candidate for the next double Mersenne prime is formula_9, or 2 − 1.\nBeing approximately 1.695,\nthis number is far too large for any currently known primality test. It has no prime factor below 4×10. There are probably no other double Mersenne primes than the four known.\n\nSmallest prime factor of formula_7 (where \"p\" is the \"n\"th prime) are\n\nWrite formula_11 instead of formula_12. A special case of the double Mersenne numbers, namely the recursively defined sequence\nis called the Catalan–Mersenne numbers. Catalan came up with this sequence after the discovery of the primality of \"M\"(127) = \"M\"(\"M\"(\"M\"(\"M\"(2)))) by Lucas in 1876. Catalan conjectured that they are prime \"up to a certain limit\". Although the first five terms (below \"M\") are prime, no known methods can prove that any further terms are prime (in any reasonable time) simply because they are too huge. However, if \"M\" is not prime, there is a chance to discover this by computing \"M\" modulo some small prime \"p\" (using recursive modular exponentiation. If the resulting residue is zero, \"p\" represents a factor of \"M\" and thus would disprove its primality. Since \"M\" is a Mersenne number, such prime factor \"p\" must be of the form 2·\"k\"·\"M\"+1.)\n\nIn the Futurama movie , the double Mersenne number formula_13 is briefly seen in \"an elementary proof of the Goldbach conjecture\". In the movie, this number is known as a \"martian prime\".\n\n\n"}
{"id": "2967256", "url": "https://en.wikipedia.org/wiki?curid=2967256", "title": "Engel expansion", "text": "Engel expansion\n\nThe Engel expansion of a positive real number \"x\" is the unique non-decreasing sequence of positive integers formula_1 such that\n\nFor instance, Euler's constant \"e\" has the Engel expansion\ncorresponding to the infinite series\n\nRational numbers have a finite Engel expansion, while irrational numbers have an infinite Engel expansion. If \"x\" is rational, its Engel expansion provides a representation of \"x\" as an Egyptian fraction. Engel expansions are named after Friedrich Engel, who studied them in 1913.\n\nAn expansion analogous to an Engel expansion, in which alternating terms are negative, is called a Pierce expansion.\n\n observe that an Engel expansion can also be written as an ascending variant of a continued fraction:\n\nThey claim that ascending continued fractions such as this have been studied as early as Fibonacci's \"Liber Abaci\" (1202). This claim appears to refer to Fibonacci's compound fraction notation in which a sequence of numerators and denominators sharing the same fraction bar represents an ascending continued fraction:\n\nIf such a notation has all numerators 0 or 1, as occurs in several instances in \"Liber Abaci\", the result is an Engel expansion. However, Engel expansion as a general technique does not seem to be described by Fibonacci.\n\nTo find the Engel expansion of \"x\", let\n\nand\n\nwhere formula_9 is the ceiling function (the smallest integer not less than \"r\").\n\nIf formula_10 for any \"i\", halt the algorithm. \nAnother equivalent method is to consider the map \n\nand set\n\nwhere\n\nYet another equivalent method, called the modified Engel expansion calculated by \n\nformula_15\n\nand\n\nformula_16\nThe Frobenius-Perron Transfer operator of the Engel map formula_17 acts on functions formula_18 with\n\nformula_19\n\nsince\n\nformula_20 and the inverse of the n-th component is formula_21 which is found by solving formula_22 for formula_23.\n\nThe Mellin transform of the map formula_17 is related to the Riemann zeta function by the formula\n\nTo find the Engel expansion of 1.175, we perform the following steps.\n\nThe series ends here. Thus,\n\nand the Engel expansion of 1.175 is {1, 6, 20}.\n\nEvery positive rational number has a unique finite Engel expansion. In the algorithm for Engel expansion, if \"u\" is a rational number \"x\"/\"y\", then \"u\" = (−\"y\" mod \"x\")/\"y\". Therefore, at each step, the numerator in the remaining fraction \"u\" decreases and the process of constructing the Engel expansion must terminate in a finite number of steps. Every rational number also has a unique infinite Engel expansion: using the identity\n\nthe final digit \"n\" in a finite Engel expansion can be replaced by an infinite sequence of (\"n\" + 1)s without changing its value. For example,\n\nThis is analogous to the fact that any rational number with a finite decimal representation also has an infinite decimal representation (see 0.999...).\nAn infinite Engel expansion in which all terms are equal is a geometric series.\n\nErdős, Rényi, and Szüsz asked for nontrivial bounds on the length of the finite Engel expansion of a rational number \"x\"/\"y\"; this question was answered by Erdős and Shallit, who proved that the number of terms in the expansion is O(\"y\") for any ε > 0.\n\nAnd in general,\n\nMore Engel expansions for constants can be found here.\n\nThe coefficients \"a\" of the Engel expansion typically exhibit exponential growth; more precisely, for almost all numbers in the interval (0,1], the limit formula_38 exists and is equal to \"e\". However, the subset of the interval for which this is not the case is still large enough that its Hausdorff dimension is one.\n\nThe same typical growth rate applies to the terms in expansion generated by the greedy algorithm for Egyptian fractions. However, the set of real numbers in the interval (0,1] whose Engel expansions coincide with their greedy expansions has measure zero, and Hausdorff dimension 1/2.\n"}
{"id": "18095448", "url": "https://en.wikipedia.org/wiki?curid=18095448", "title": "Epitome (data processing)", "text": "Epitome (data processing)\n\nAn epitome, in data processing, is a condensed digital representation of the essential statistical properties of ordered datasets such as matrices that represent images, audio signals, videos or genetic sequences. Although much smaller than the data, the epitome contains many of its smaller overlapping parts with much less repetition and with some level of generalization. As such, it can be used in tasks such as data mining, machine learning and signal processing.\n\nThe first use of epitomic analysis was with image textures for the purposes of image parsing. Epitomes have also been used in video processing to replace, remove or superresolve imagery.\n\nEpitomes are also being investigated as tools for vaccine design.\n\n"}
{"id": "842387", "url": "https://en.wikipedia.org/wiki?curid=842387", "title": "Evolute", "text": "Evolute\n\nIn the differential geometry of curves, the evolute of a curve is the locus of all its centers of curvature. That is to say that when the center of curvature of each point on a curve is drawn, the resultant shape will be the evolute of that curve. The evolute of a circle is therefore a single point at its center. Equivalently, an evolute is the envelope of the normals to a curve.\n\nThe evolute of a curve, a surface, or more generally a submanifold, is the caustic of the normal map. Let be a smooth, regular submanifold in . For each point in and each vector , based at and normal to , we associate the point . This defines a Lagrangian map, called the normal map. The caustic of the normal map is the evolute of .\n\nEvolutes are closely connected to involutes: A curve is the evolute of any of its involutes.\n\nApollonius ( 200 BC) discussed evolutes in Book V of his \"Conics\". However, Huygens is sometimes credited with being the first to study them (1673). Huygens formulated his theory of evolutes sometime around 1659 to help solve the problem of finding the tautochrone curve, which in turn helped him construct an isochronous pendulum. This was because the tautochrone curve is a cycloid, and the cycloid has the unique property that its evolute is also a cycloid. The theory of evolutes, in fact, allowed Huygens to achieve many results that would later be found using calculus.\n\nIf formula_1 is the parametric representation of a regular curve in the plane with its curvature nowhere 0 and formula_2 its curvature radius and formula_3 the unit normal pointing to the curvature center, then\ndescribes the evolute of the given curve.\n\nFor formula_5 and formula_6 one gets\n\nIn order to derive properties of a regular curve it is advantageous to use the arc length formula_9 of the given curve as its parameter, because of formula_10 and formula_11 (see Frenet–Serret formulas). Hence the tangent vector of the evolute formula_12 is:\nFrom this equation one gets the following properties of the evolute:\n\n\"Proof\" of the last property:\nLet be formula_15 at the section of consideration. An involute of the evolute can be described as follows:\nwhere formula_19 is a fixed string extension (see Involute of a parameterized curve ).\nWith formula_20 and formula_15 one gets \nThat means: For the string extension formula_23 the given curve is reproduced.\n\n\"Proof:\" A parallel curve with distance formula_24 off the given curve has the parametric representation formula_25 and the radius of curvature formula_26 (see parallel curve). Hence the evolute of the parallel curve is formula_27\n\nFor the parabola with the parametric representation formula_28 one gets from the formulae above the equations:\nwhich describes a semicubic parabola\nFor the ellipse with the parametric representation formula_31 one gets:\nThese are the equations of a non symmetric astroid. \nEliminating parameter formula_34 leads to the implicit representation\n\nFor the cycloid with the parametric representation formula_36 the evolute will be:\nwhich describes a transposed replica of itself.\n\nThe evolute \n\nA curve with a similar definition is the radial of a given curve. For each point on the curve take the vector from the point to the center of curvature and translate it so that it begins at the origin. Then the locus of points at the end of such vectors is called the radial of the curve. The equation for the radial is obtained by removing the and terms from the equation of the evolute. This produces\n"}
{"id": "23967473", "url": "https://en.wikipedia.org/wiki?curid=23967473", "title": "Fibrifold", "text": "Fibrifold\n\nIn mathematics, a fibrifold is (roughly) a fiber space whose fibers and base spaces are orbifolds. They were introduced by , who introduced a system of notation for 3-dimensional fibrifolds and used this to assign names to the 219 affine space group types. 184 of these are considered reducible, and 35 irreducible.\n\nThe 35 irreducible space groups correspond to the cubic space group.\n\nIrreducible group symbols (indexed 195−230) in Hermann–Mauguin notation, Fibrifold notation, geometric notation, and Coxeter notation:\n"}
{"id": "226981", "url": "https://en.wikipedia.org/wiki?curid=226981", "title": "Finite mathematics", "text": "Finite mathematics\n\nIn mathematics education, Finite Mathematics is a syllabus in college and university mathematics that is independent of calculus. A course in precalculus may be a prerequisite for Finite Mathematics.\n\nContents of the course include an eclectic selection of topics often applied in social science and business, such as finite probability spaces, matrix multiplication, Markov processes, finite graphs, or mathematical models. These topics were used in Finite Mathematics courses at Dartmouth College (home of Tuck School of Business) as developed by John G. Kemeny, Gerald L. Thompson, and J. Laurie Snell and published by Prentice-Hall. Other publishers followed with their own topics. With the arrival of software to facilitate computations, teaching and usage shifted from a broad-spectrum Finite Mathematics with paper and pen, into development and usage of software.\n\n"}
{"id": "10144353", "url": "https://en.wikipedia.org/wiki?curid=10144353", "title": "Fraser Filter", "text": "Fraser Filter\n\nA Fraser Filter, named after Douglas Fraser, is typically used in geophysics when displaying VLF data. It is effectively the first derivative of the data.\n\nIf formula_1 represents the collected data then formula_2 is the average of two values. Consider this value to be plotted between point 1 and point 2 and do the same with points 3 and 4: formula_3\n\nIf formula_4 represents the space between each station along the line then\nformula_5 is the Fraser Filter of those four values.\n\nSince formula_6 is constant, it can be ignored and the Fraser Filter considered to be\nformula_7.\n"}
{"id": "234232", "url": "https://en.wikipedia.org/wiki?curid=234232", "title": "Graham's number", "text": "Graham's number\n\nGraham's number is an enormous number that arises as an upper bound on the answer of a problem in the mathematical field of Ramsey theory. It is named after mathematician Ronald Graham, who used the number as a simplified explanation of the upper bounds of the problem he was working on in conversations with popular science writer Martin Gardner. Gardner later described the number in \"Scientific American\" in 1977, introducing it to the general public. At the time of its introduction, it was the largest specific positive integer ever to have been used in a published mathematical proof. The number was published in the 1980 \"Guinness Book of World Records\", adding to its popular interest. Other specific integers (such as TREE(3)) known to be far larger than Graham's number have since appeared in many serious mathematical proofs, for example in connection with Harvey Friedman's various finite forms of Kruskal's theorem. Additionally, smaller upper bounds on the Ramsey theory problem from which Graham's number derived have since been proven to be valid.\n\nGraham's number is much larger than many other large numbers such as Skewes' number and Moser's number, both of which are in turn much larger than a googolplex. As with these, it is so large that the observable universe is far too small to contain an ordinary digital representation of Graham's number, assuming that each digit occupies one Planck volume, possibly the smallest measurable space. But even the number of digits in this digital representation of Graham's number would itself be a number so large that its digital representation cannot be represented in the observable universe. Nor even can the number of digits of \"that\" number—and so forth, for a number of times far exceeding the total number of Planck volumes in the observable universe. Thus Graham's number cannot even be expressed in this way by power towers of the form formula_1. \n\nHowever, Graham's number can be explicitly given by computable recursive formulas using Knuth's up-arrow notation or equivalent, as was done by Graham. As there is a recursive formula to define it, it is much smaller than typical busy beaver numbers. Though too large to be computed in full, the sequence of digits of Graham's number can be computed explicitly through simple algorithms. The last 12 digits are ...262464195387. With Knuth's up-arrow notation, Graham's number is formula_2, where\n\nformula_3\n\nGraham's number is connected to the following problem in Ramsey theory:\nIn 1971, Graham and Rothschild proved that this problem has a solution \"N*\", giving as a bound 6 ≤ \"N*\" ≤ \"N\", with \"N\" being a large but explicitly defined number formula_4, where formula_5 in Knuth's up-arrow notation; the number is between 4 → 2 → 8 → 2 and 2 → 3 → 9 → 2 in Conway chained arrow notation. This was reduced in 2014 via upper bounds on the Hales–Jewett number to formula_6. The lower bound of 6 was later improved to 11 by Geoffrey Exoo in 2003, and to 13 by Jerome Barkley in 2008. Thus, the best known bounds for \"N*\" are 13 ≤ \"N*\" ≤ \"N\"'.\n\nGraham's number, \"G\", is much larger than \"N\": formula_7, where formula_8. This weaker upper bound for the problem, attributed to an unpublished work of Graham, was eventually published and named by Martin Gardner in \"Scientific American\" in November 1977.\n\nThe number gained a degree of popular attention when Martin Gardner described it in the \"Mathematical Games\" section of \"Scientific American\" in November 1977, writing that Graham had recently established, in an unpublished proof, \"a bound so vast that it holds the record for the largest number ever used in a serious mathematical proof.\" The 1980 \"Guinness Book of World Records\" repeated Gardner's claim, adding to the popular interest in this number. According to physicist John Baez, Graham invented the quantity now known as Graham's number in conversation with Gardner. While Graham was trying to explain a result in Ramsey theory which he had derived with his collaborator Bruce Lee Rothschild, Graham found that the quantity now known as Graham's number was easier to explain than the actual number appearing in the proof. Because the number which Graham described to Gardner is larger than the number in the paper itself, both are valid upper bounds for the solution to the problem studied by Graham and Rothschild.\n\nUsing Knuth's up-arrow notation, Graham's number \"G\" (as defined in Gardner's \"Scientific American\" article) is\nformula_9\n\nwhere the number of \"arrows\" in each subsequent layer is specified by the value of the next layer below it; that is,\n\nformula_10 where formula_11 formula_12\n\nand where a superscript on an up-arrow indicates how many arrows there are. In other words, \"G\" is calculated in 64 steps: the first step is to calculate \"g\" with four up-arrows between 3s; the second step is to calculate \"g\" with \"g\" up-arrows between 3s; the third step is to calculate \"g\" with \"g\" up-arrows between 3s; and so on, until finally calculating \"G\" = \"g\" with \"g\" up-arrows between 3s.\n\nEquivalently,\nformula_13\n\nand the superscript on \"f\" indicates an iteration of the function, e.g., formula_14. Expressed in terms of the family of hyperoperations formula_15, the function \"f\" is the particular sequence formula_16, which is a version of the rapidly growing Ackermann function \"A\"(\"n\", \"n\"). (In fact, formula_17 for all \"n\".) The function \"f\" can also be expressed in Conway chained arrow notation as formula_18, and this notation also provides the following bounds on \"G\":\n\nformula_19\n\nTo convey the difficulty of appreciating the enormous size of Graham's number, it may be helpful to express—in terms of exponentiation alone—just the first term (\"g\") of the rapidly growing 64-term sequence. First, in terms of tetration (formula_20) alone:\nformula_21\n\nwhere the number of 3s in the expression on the right is\nformula_22\n\nNow each tetration (formula_23) operation reduces to a power tower (formula_24) according to the definition\nformula_25 where there are \"X\" 3s.\n\nThus,\nformula_26\n\nbecomes, solely in terms of repeated \"exponentiation towers\",\nformula_27\n\nand where the number of 3s in each tower, starting from the leftmost tower, is specified by the value of the next tower to the right.\n\nIn other words, \"g\" is computed by first calculating the number of towers, formula_28 (where the number of 3s is formula_29), and then computing the \"n\" tower in the following sequence:\n\nwhere the number of 3s in each successive tower is given by the tower just before it. Note that the result of calculating the third tower is the value of \"n\", the number of towers for \"g\".\n\nThe magnitude of this first term, \"g\", is so large that it is practically incomprehensible, even though the above display is relatively easy to comprehend. Even \"n\", the mere \"number of towers\" in this formula for \"g\", is far greater than the number of Planck volumes (roughly 10 of them) into which one can imagine subdividing the observable universe. And after this first term, still another 63 terms remain in the rapidly growing \"g\" sequence before Graham's number \"G\" = \"g\" is reached. To illustrate just how fast this sequence grows, while \"g\" is equal to formula_30 with only four up arrows, the number of up arrows in \"g\" is this incomprehensibly large number \"g\".\n\nGraham's number is a \"power tower\" of the form 3↑↑\"n\" (with a very large value of \"n\"), so its rightmost decimal digits must satisfy certain properties common to all such towers. One of these properties is that \"all such towers of height greater than d (say), have the same sequence of d rightmost decimal digits\". This is a special case of a more general property: The \"d\" rightmost decimal digits of all such towers of height greater than \"d\"+2, are \"independent\" of the topmost \"3\" in the tower; i.e., the topmost \"3\" can be changed to any other non-negative integer without affecting the \"d\" rightmost digits.\n\nThe following table illustrates, for a few values of \"d\", how this happens. For a given height of tower and number of digits \"d\", the full range of \"d\"-digit numbers (10 of them) does \"not\" occur; instead, a certain smaller subset of values repeats itself in a cycle. The length of the cycle and some of the values (in parentheses) are shown in each cell of this table:\n\nThe particular rightmost \"d\" digits that are ultimately shared by all sufficiently tall towers of 3s are in bold text, and can be seen developing as the tower height increases. For any fixed number of digits \"d\" (row in the table), the number of values possible for 3formula_313↑…3↑\"x\" mod 10, as \"x\" ranges over all nonnegative integers, is seen to decrease steadily as the height increases, until eventually reducing the \"possibility set\" to a single number (colored cells) when the height exceeds \"d\"+2.\n\nA simple algorithm for computing these digits may be described as follows: let x = 3, then iterate, \"d\" times, the assignment \"x\" = 3 mod 10. Except for omitting any leading 0s, the final value assigned to \"x\" (as a base-ten numeral) is then composed of the \"d\" rightmost decimal digits of 3↑↑\"n\", for all \"n\" > \"d\". (If the final value of \"x\" has fewer than \"d\" digits, then the required number of leading 0s must be added.)\n\nLet \"k\" be the numerousness of these \"stable\" digits, which satisfy the congruence relation G(mod 10)≡[G](mod 10).\n\n\"k\"=\"t\"-1, where G(\"t\"):=3↑↑\"t\". It follows that, .\n\nThe algorithm above produces the following 500 rightmost decimal digits of Graham's number (or of any tower of more than 500 3s):\n\n\n"}
{"id": "56030165", "url": "https://en.wikipedia.org/wiki?curid=56030165", "title": "Gyárfás–Sumner conjecture", "text": "Gyárfás–Sumner conjecture\n\nIn graph theory, the Gyárfás–Sumner conjecture asks whether, for every tree formula_1 and complete graph formula_2, the graphs with neither formula_1 nor formula_2 as induced subgraphs have can be properly colored using only a constant number of colors. Equivalently, it asks whether the formula_1-free graphs are formula_6-bounded.\nIt is named after András Gyárfás and David Sumner, who formulated it independently in 1975 and 1981 respectively. It remains unproven.\n\nIn this conjecture, it is not possible to replace formula_1 by a graph with cycles. As Paul Erdős and András Hajnal have shown, there exist triangle-free graphs with arbitrarily large chromatic number and, at the same time, arbitrarily large girth. Using these graphs, one can obtain graphs that avoid any fixed choice of a cyclic graph and clique (of more than two vertices) as induced subgraphs, and exceed any fixed bound on the chromatic number.\n\nThe conjecture is known to be true for certain special choices of formula_1, including paths, stars, and trees of radius two.\nIt is also known that, for any tree formula_1, the graphs that do not contain a subdivision of formula_1 are formula_6-bounded.\n\n"}
{"id": "17422461", "url": "https://en.wikipedia.org/wiki?curid=17422461", "title": "Indecomposability", "text": "Indecomposability\n\nIn constructive mathematics, indecomposability or indivisibility (, from the adjective \"unzerlegbar\") is the principle that the continuum cannot be partitioned into two nonempty pieces. This principle was established by Brouwer in 1928 using intuitionistic principles, and can also be proven using Church's thesis. The analogous property in classical analysis is the fact that any continuous function from the continuum to {0,1} is constant.\n\nIt follows from the indecomposability principle that any property of real numbers that is \"decided\" (each real number either has or does not have that property) is in fact trivial (either all the real numbers have that property, or else none of them do). Conversely, if a property of real numbers is not trivial, then the property is not decided for all real numbers. This contradicts the law of the excluded middle, according to which every property of the real numbers is decided; so, since there are many nontrivial properties, there are many nontrivial partitions of the continuum.\n\nIn CZF, it is consistent to assume the universe of all sets is indecomposable—so that any class for which membership is decided (every set is either a member of the class, or else not a member of the class) is either empty or the entire universe.\n\n\n"}
{"id": "11930843", "url": "https://en.wikipedia.org/wiki?curid=11930843", "title": "Infinity (philosophy)", "text": "Infinity (philosophy)\n\nIn philosophy and theology, infinity is explored in articles under headings such as the Ultimate, the Absolute, God, and Zeno's paradoxes. In Greek philosophy, for example in Anaximander, 'the Boundless' is the origin of all that is. He took the beginning or first principle to be an endless, unlimited primordial mass (ἄπειρον, \"apeiron\"). The Jain metaphysics and mathematics was the first to define and delineate different \"types\" of infinities. The work of the mathematician Georg Cantor first placed infinity into a coherent mathematical framework. Keenly aware of his departure from traditional wisdom, Cantor also presented a comprehensive historical and philosophical discussion of infinity. In Judeo-Christian theology, for example in the work of Duns Scotus, the infinite nature of God invokes a sense of being without constraint, rather than a sense of being unlimited in quantity. In ethics infinity plays an important role designating that which cannot be defined or reduced to knowledge or power.\n\n\"Main: Ḥeḥu and Ḥeḥut\" - (Brugsch 1885)\n\nAn early engagement with the idea of infinity was made by Anaximander who considered infinity to be a foundational and primitive basis of reality. Anaximander was the first in the Greek philosophical tradition to propose that the universe was infinite.\n\nAnaxagoras (500–428 BCE) was of the opinion that matter of the universe had an innate capacity for infinite division.\n\nA group of thinkers of ancient Greece (later identified as the Atomists) all similarly considered matter to be made of an infinite number of structures as considered by imagining an dividing or separating matter from itself an infinite number of times.\n\nAristotle, alive for the period 384–322 BCE, is credited with being the root of a field of thought, in his influence of succeeding thinking for a period spanning more than one subsequent millennium, by his rejection of the idea of actual infinity.\n\nIn Book 3 of the work entitled Physics, written by Aristotle, Aristotle deals with the concept of infinity in terms of his notion of actuality and of potentiality.\n\nThis is often called potential infinity; however, there are two ideas mixed up with this. One is that it is always possible to find a number of things that surpasses any given number, even if there are not actually such things. The other is that we may quantify over infinite sets without restriction. For example, formula_1, which reads, \"for any integer n, there exists an integer m > n such that P(m)\". The second view is found in a clearer form by medieval writers such as William of Ockham:\n\nThe parts are actually there, in some sense. However, on this view, no infinite magnitude can have a number, for whatever number we can imagine, there is always a larger one: \"There are not so many (in number) that there are no more.\"\n\nAristotle's views on the continuum foreshadow some topological aspects of modern mathematical theories of the continuum. Aristotle's emphasis on the connectedness of the continuum may have inspired—in different ways—modern philosophers and mathematicians such as Charles Sanders Peirce, Cantor, and LEJ Brouwer.\n\nAmong the scholastics, Aquinas also argued against the idea that infinity could be in any sense complete or a totality.\n\nAristotle deals with infinity in the context of the prime mover, in Book 7 of the same work, the reasoning of which was later studied and commented on by Simplicius.\n\nPlotinus considered infinity, while he was alive, during the 3rd century A.D.\n\nSimplicius, alive circa 490 to 560 AD, thought the concept \"Mind\" was infinite.\n\nAugustine thought infinity to be \"incomprehensible for the human mind\".\n\nThe Jain upanga āgama Surya Prajnapti (c. 400 BC) classifies all numbers into three sets: enumerable, innumerable, and infinite. Each of these was further subdivided into three orders:\n\nThe Jains were the first to discard the idea that all infinities were the same or equal. They recognized different types of infinities: infinite in length (one dimension), infinite in area (two dimensions), infinite in volume (three dimensions), and infinite perpetually (infinite number of dimensions).\n\nAccording to Singh (1987), Joseph (2000) and Agrawal (2000), the highest enumerable number \"N\" of the Jains corresponds to the modern concept of aleph-null formula_2 (the cardinal number of the infinite set of integers 1, 2, ...), the smallest cardinal transfinite number. The Jains also defined a whole system of infinite cardinal numbers, of which the highest enumerable number \"N\" is the smallest.\n\nIn the Jaina work on the theory of sets, two basic types of infinite numbers are distinguished. On both physical and ontological grounds, a distinction was made between (\"countless, innumerable\") and \"ananta\" (\"endless, unlimited\"), between rigidly bounded and loosely bounded infinities.\n\nGalileo Galilei (February 1564 - January 1642 ) discussed the example of comparing the square numbers {1, 4, 9, 16, ...} with the natural numbers {1, 2, 3, 4, ...} as follows:\n\nIt appeared by this reasoning as though a \"set\" (Galileo did not use the terminology) which is naturally smaller than the \"set\" of which it is a part (since it does not contain all the members) is in some sense the same \"size\". Galileo found no way around this problem:\n\nThe idea that size can be measured by one-to-one correspondence is today known as Hume's principle, although Hume, like Galileo, believed the principle could not be applied to the infinite. The same concept, applied by Georg Cantor, is used in relation to infinite sets.\n\nFamously, the ultra-empiricist Hobbes ( April 1588 - December 1679 ) tried to defend the idea of a potential infinity in light of the discovery, by Evangelista Torricelli, of a figure (Gabriel's Horn) whose surface area is infinite, but whose volume is finite. Not reported, this motivation of Hobbes came too late as curves having infinite length yet bounding finite areas were known much before.\n\nLocke ( August 1632 - October 1704 ) in common with most of the empiricist philosophers, also believed that we can have no proper idea of the infinite. They believed all our ideas were derived from sense data or \"impressions,\" and since all sensory impressions are inherently finite, so too are our thoughts and ideas. Our idea of infinity is merely negative or privative.\n\nHe considered that in considerations on the subject of eternity, which he classified as an infinity, humans are likely to make mistakes.\n\nModern discussion of the infinite is now regarded as part of set theory and mathematics. Contemporary philosophers of mathematics engage with the topic of infinity and generally acknowledge its role in mathematical practice. But, although set theory is now widely accepted, this was not always so. Influenced by L.E.J Brouwer and verificationism in part, Wittgenstein (April 1889 Vienna - April 1951 Cambridge, England ), made an impassioned attack upon axiomatic set theory, and upon the idea of the actual infinite, during his \"middle period\".\n\nUnlike the traditional empiricists, he thought that the infinite was in some way given to sense experience.\n\nThe philosopher Emmanuel Levinas ( January 1906, Lithuania - December 25 1995, Paris ) uses infinity to designate that which cannot be defined or reduced to knowledge or power. In Levinas' magnum opus Totality and Infinity he says :\n\nLevinas also wrote a work entitled \"Philosophy and the Idea of Infinity\", which was published during 1957.\n\n\n\n"}
{"id": "40581191", "url": "https://en.wikipedia.org/wiki?curid=40581191", "title": "Infinity mirror", "text": "Infinity mirror\n\nAn infinity mirror is a pair of parallel mirrors, which create a series of smaller and smaller reflections that appear to recede to infinity. They are used as room accents and in artwork.\n\nIn a classic self-contained infinity mirror, a set of light bulbs, LEDs, or other point-source lights are placed around the periphery of a fully reflective mirror, and a second, partially reflective \"one-way mirror\" is placed a short distance in front of it, in a parallel alignment. When an outside observer looks into the surface of the partially reflective mirror, the lights appear to recede into infinity, creating the appearance of a tunnel of lights of great depth.\n\nAlternatively, this effect can also be seen when an observer stands \"between\" two parallel fully reflective mirrors, as in some dressing rooms, some elevators, or a house of mirrors. A weaker version of this effect can be seen by standing between any two parallel reflective surfaces, such as the glass walls of a small entry lobby into some buildings. The partially reflective glass produces this sensation, diluted by the visual noise of the views through the glass into the surrounding environment.\n\nThe 3D illusion mirror effect is produced whenever there are two parallel reflective surfaces which can bounce a beam of light back and forth an indefinite (theoretically infinite) number of times. The reflections appear to recede into the distance because the light actually is traversing the distance it appears to be traveling. \n\nFor example, in a two-centimeter-thick infinity mirror, with the light sources halfway between, light from the source initially travels one centimeter. The first reflection travels one centimeter to the rear mirror and then two centimeters to, and through the front mirror, a total of three centimeters. The second reflection travels two centimeters from front mirror to back mirror, and again two centimeters from the back mirror to, and through the front mirror, totaling four centimeters, plus the first reflection (three centimeters) making the second reflection seven centimeters away from the front mirror. Each successive reflection adds four more centimeters to the total (the third reflection appears 11 centimeters deep, fourth 15 centimeters deep, and so on).\n\nEach additional reflection adds length to the path the light must travel before exiting the mirror. If the mirrors are not precisely parallel, but instead are canted at a slight angle, the \"visual tunnel\" will be perceived to be curved (off to one side) as it recedes into infinity.\n\nWhen studied using the principles of geometrical optics, the series of repeating images forms the infinite mathematical surface known as Gabriel's Horn, or Torricelli's Trumpet, named in honor of Italian mathematician Evangelista Torricelli, who first studied it. In theory, such a surface is infinite in area, but encloses a finite volume.\n\nVisual artists, especially contemporary sculptors, have made use of infinity mirrors. Yayoi Kusama, Josiah McElheny, Ivan Navarro, and Taylor Davis have all produced works that use the infinity mirror to expand the sensation of unlimited space in their artworks.\n\nSome amusement park dark rides, such as Disney's \"Space Mountain\" roller coaster attraction, use infinity mirrors to create the impression of flying through space.\n\n"}
{"id": "25831625", "url": "https://en.wikipedia.org/wiki?curid=25831625", "title": "James Wood (mathematician)", "text": "James Wood (mathematician)\n\nJames Wood (14 December 1760 – 23 April 1839) was a mathematician, and Master of St John's College, Cambridge. In his later years he was Dean of Ely.\n\nWood was born in Holcombe, Bury where his father ran an evening school and taught his son the elements of arithmetic and algebra. From Bury Grammar School he proceeded to St John's College, Cambridge in 1778, graduating as senior wrangler in 1782. On graduating he became a fellow of the college and in his long tenure there produced several successful academic textbooks for students of mathematics. Between 1795 and 1799 his \"The principles of mathematics and natural philosophy\", was printed, in four volumes, by J. Burges. Vol.I: 'The elements of algebra', by Wood; Vol.II: 'The principles of fluxions' by Samuel Vince; Vol.III Part I: 'The principles of mechanics\" by Wood; and Vol.III Part II: \"The principles of hydrostatics\" by Samuel Vince; Vol.IV \"The principles of astronomy\" by Samuel Vince. Three other volumes -\"A treatise on plane and spherical trigonometry\" and \"The elements of the conic sections\" by Samuel Vince (1800) and \"The elements of optics\" by Wood (1801\" may have been issued as part of the series.\n\nWood remained for sixty years at St. John's, serving as both President (1802–1815) and Master (1815–1839); on his death in 1839 he was interred in the college chapel and bequeathed his extensive library to the college, comprising almost 4,500 printed books on classics, history, mathematics, theology and travel, dating from the 17th to the 19th centuries.\n\nWood was also ordained as a priest in 1787 and served as Dean of Ely from 1820 until his death.\n\n\n"}
{"id": "387571", "url": "https://en.wikipedia.org/wiki?curid=387571", "title": "Jesse Douglas", "text": "Jesse Douglas\n\nJesse Douglas (3 July 1897 – 7 September 1965) was an American mathematician and Fields Medalist known for his general solution of the Problem of Plateau.\n\nHe was born in New York City, the son of Sarah (née Kommel) and Louis Douglas. He attended City College of New York as an undergraduate, graduating with honors in Mathematics in 1916. He then moved to Columbia University as a graduate student, obtaining a PhD in mathematics in 1920.\n\nDouglas was one of two winners of the first Fields Medals, awarded in 1936. He was honored for solving, in 1930, the problem of Plateau, which asks whether a minimal surface exists for a given boundary. The problem, open since 1760 when Lagrange raised it, is part of the calculus of variations and is also known as the \"soap bubble problem\". Douglas also made significant contributions to the inverse problem of the calculus of variations. The American Mathematical Society awarded him the Bôcher Memorial Prize in 1943.\n\nDouglas later became a full professor at the City College of New York (CCNY), where he taught until his death. At the time CCNY only offered undergraduate degrees and Professor Douglas taught the advanced calculus course. Sophomores (and freshmen with advanced placement) were privileged to get their introduction to real analysis from a Fields medalist.\n\n\n\n\n"}
{"id": "2777137", "url": "https://en.wikipedia.org/wiki?curid=2777137", "title": "Keynesian beauty contest", "text": "Keynesian beauty contest\n\nA Keynesian beauty contest is a concept developed by John Maynard Keynes and introduced in Chapter 12 of his work, \"The General Theory of Employment, Interest and Money\" (1936), to explain price fluctuations in equity markets.\n\nKeynes described the action of rational agents in a market using an analogy based on a fictional newspaper contest, in which entrants are asked to choose the six most attractive faces from a hundred photographs. Those who picked the most popular faces are then eligible for a prize.\n\nA naive strategy would be to choose the face that, in the opinion of the entrant, is the most handsome. A more sophisticated contest entrant, wishing to maximize the chances of winning a prize, would think about what the majority perception of attractive is, and then make a selection based on some inference from his knowledge of public perceptions. This can be carried one step further to take into account the fact that other entrants would each have their own opinion of what public perceptions are. Thus the strategy can be extended to the next order and the next and so on, at each level attempting to predict the eventual outcome of the process based on the reasoning of other rational agents.\n\"It is not a case of choosing those [faces] that, to the best of one's judgment, are really the prettiest, nor even those that average opinion genuinely thinks the prettiest. We have reached the third degree where we devote our intelligences to anticipating what average opinion expects the average opinion to be. And there are some, I believe, who practice the fourth, fifth and higher degrees.\" (Keynes, General Theory of Employment, Interest and Money, 1936).\nKeynes believed that similar behavior was at work within the stock market. This would have people pricing shares not based on what they think their fundamental value is, but rather on what they think everyone else thinks their value is, or what everybody else would predict the average assessment of value to be.\n\nNational Public Radio's \"Planet Money\" tested the theory by having its listeners select the cutest of three animal videos. The listeners were broken into two groups. One selected the animal they thought was cutest, and the other selected the one they thought most participants would think was the cutest. The results showed significant differences between the groups. Fifty percent of the first group selected a video with a kitten, compared to seventy-six percent of the second selecting the same video. Individuals in the second group were generally able to disregard their own preferences and accurately make a decision based on the expected preferences of others. The results were considered to be consistent with Keynes' theory.\n\nOther, more explicit scenarios help to convey the notion of the contest as a convergence to Nash Equilibrium. For instance in the \"p\"-beauty contest game (Moulin 1986), all participants are asked to simultaneously pick a number between 0 and 100. The winner of the contest is the person(s) whose number is closest to p times the average of all numbers submitted, where p is some fraction, typically 2/3 or 1/2. If there are only two players and p<1, the only Nash equilibrium solution is for all to guess 0 or 1. By contrast, in Keynes' formulation, p=1 and there are many possible Nash equilibria.\n\nIn play of the p-beauty contest game (where p differs from 1), players exhibit distinct, boundedly rational levels of reasoning as first documented in an experimental test by Nagel (1995). The lowest, 'Level 0' players, choose numbers randomly from the interval [0,100]. The next higher, 'Level 1' players believe that all other players are Level 0. These Level 1 players therefore reason that the average of all numbers submitted should be around 50. If p=2/3, for instance, these Level 1 players choose, as their number, 2/3 of 50, or 33. Similarly, the next higher 'Level 2' players in the 2/3-the average game believe that all other players are Level 1 players. These Level 2 players therefore reason that the average of all numbers submitted should be around 33, and so they choose, as their number, 2/3 of 33 or 22. Similarly, the next higher 'Level 3' players play a best response to the play of Level 2 players and so on. The Nash equilibrium of this game, where all players choose the number 0, is thus associated with an infinite level of reasoning. Empirically, in a single play of the game, the typical finding is that most participants can be classified from their choice of numbers as members of the lowest Level types 0, 1, 2 or 3, in line with Keynes' observation.\n\nIn another variation of reasoning towards the beauty contest, the players may begin to judge contestants based on the most distinguishable unique property found scarcely clustered in the group. As an analogy, imagine the contest where the player is instructed to choose the most attractive six faces out of a set of hundred faces. Under special circumstances, the player may ignore all judgment-based instructions in a search for the six most unusual faces (interchanging concepts of high demand and low supply). Ironic to the situation, if the player finds it much easier to find a consensus solution for judging the six ugliest contestants, she may apply this property instead of attractiveness level in choosing six faces. In this line of reasoning, the player is looking for other players overlooking the instructions (which can often be based on random selection) to a transformed set of instructions only elite players would solicit, giving them an advantage. As an example, imagine a contest where contestants are asked to pick the two best numbers in the list: {1, 2, 3, 4, 5, 6, 7, 8, 2345, 6435, 9, 10, 11, 12, 13}. All judgment based instructions can likely be ignored since by consensus two of the numbers do not belong in the set.\n\n\n\n"}
{"id": "2069644", "url": "https://en.wikipedia.org/wiki?curid=2069644", "title": "Lebesgue's lemma", "text": "Lebesgue's lemma\n\n\"For Lebesgue's lemma for open covers of compact spaces in topology see Lebesgue's number lemma\"\n\nIn mathematics, Lebesgue's lemma is an important statement in approximation theory. It provides a bound for the projection error.\n\nLet (\"V\", ||·||) be a normed vector space, \"U\" be a subspace of \"V\" and let formula_1 be a linear projector on formula_2. Then, for each \"v\" in \"V\":\n\n"}
{"id": "384058", "url": "https://en.wikipedia.org/wiki?curid=384058", "title": "Madhu Sudan", "text": "Madhu Sudan\n\nMadhu Sudan (born 12 September 1966) is an Indian-American computer scientist. He has been a Gordon McKay Professor of Computer Science at the Harvard John A. Paulson School of Engineering and Applied Sciences since 2015.\n\nHe received his bachelor's degree in computer science from IIT Delhi in 1987 and his doctoral degree in computer science at the University of California, Berkeley in 1992. He was a research staff member at the IBM Thomas J. Watson Research Center in Yorktown Heights, New York from 1992 to 1997 and moved to MIT after that. From 2009 to 2015 he was a permanent researcher at Microsoft Research New England before joining Harvard University in 2015.\n\nHe was awarded the Rolf Nevanlinna Prize at the 24th International Congress of Mathematicians in 2002. The prize recognizes outstanding work in the mathematical aspects of computer science. Sudan was honored for his work in advancing the theory of probabilistically checkable proofs—a way to recast a mathematical proof in computer language for additional checks on its validity—and developing error-correcting codes. For the same work, he received the ACM's Distinguished Doctoral Dissertation Award in 1993 and the Gödel Prize in 2001. He is a Fellow of the ACM (2008). In 2012 he became a fellow of the American Mathematical Society. In 2014 he won the Infosys Prize in the mathematical sciences.\nIn 2017 he was elected to the National Academy of Sciences.\n\nSudan has made important contributions to several areas of theoretical computer science, including probabilistically checkable proofs, non-approximability of optimization problems, list decoding, and error-correcting codes.\n\n"}
{"id": "45111462", "url": "https://en.wikipedia.org/wiki?curid=45111462", "title": "Michael Sadowsky", "text": "Michael Sadowsky\n\nMichael A. Sadowsky (1902 – December 31, 1967) was a researcher in solid mechanics, particularly the mathematical theory of elasticity and materials science. Born in Estonia, he earned his doctorate in 1927 under the applied mathematician Georg Hamel at the Technical University of Berlin with a dissertation entitled \"Spatially periodic solutions in the theory of elasticity\" (in German). He made contributions in the use of potential functions in elasticity and force transfer mechanisms in composites. Many of his early papers were written in German and are now being translated.\n\n"}
{"id": "4977512", "url": "https://en.wikipedia.org/wiki?curid=4977512", "title": "Notices of the American Mathematical Society", "text": "Notices of the American Mathematical Society\n\nNotices of the American Mathematical Society is the membership journal of the American Mathematical Society (AMS), published monthly except for the combined June/July issue. The first volume appeared in 1953. Each issue of the magazine since January 1995 is available in its entirety on the journal web site. Articles are peer-reviewed by an editorial board of mathematical experts. Since 2016, the editor-in-chief is Frank Morgan. The cover regularly features mathematical visualizations.\n\nThe \"Notices\" is the world's most widely read mathematical journal. As the membership journal of the American Mathematical Society, the \"Notices\" is sent to the approximately 30,000 AMS members worldwide, one-third of whom reside outside the United States. By publishing high-level exposition, the \"Notices\" provides opportunities for mathematicians to find out what is going on in the field. Each issue contains one or two such expository articles that describe current developments in mathematical research, written by professional mathematicians. The \"Notices\" also carries articles on the history of mathematics, mathematics education, and professional issues facing mathematicians, as well as reviews of books and other works.\n\n"}
{"id": "5847302", "url": "https://en.wikipedia.org/wiki?curid=5847302", "title": "Outline of algebraic structures", "text": "Outline of algebraic structures\n\nIn mathematics, there are many types of algebraic structures which are studied. Abstract algebra is primarily the study of specific algebraic structures and their properties. Algebraic structures may be viewed in different ways, however the common starting point of algebra texts is that an algebraic object incorporates one or more sets with one or more binary operations or unary operations satisfying a collection of axioms.\n\nAnother branch of mathematics known as universal algebra studies algebraic structures in general. From the universal algebra viewpoint, most structures can be divided into varieties and quasivarieties depending on the axioms used. Some axiomatic formal systems that are neither varieties nor quasivarieties, called \"nonvarieties\", are sometimes included among the algebraic structures by tradition.\n\nConcrete examples of each structure will be found in the articles listed.\n\nAlgebraic structures are so numerous today that this article will inevitably be incomplete. In addition to this, there are sometimes multiple names for the same structure, and sometimes one name will be defined by disagreeing axioms by different authors. Most structures appearing on this page will be common ones which most authors agree on. Other web lists of algebraic structures, organized more or less alphabetically, include Jipsen and PlanetMath. These lists mention many structures not included below, and may present more information about some structures than is presented here.\n\nAlgebraic structures appear in most branches of mathematics, and one can encounter them in many different ways.\n\nIn full generality, an algebraic structure may use any number of sets and any number of axioms in its definition. The most commonly studied structures, however, usually involve only one or two sets and one or two binary operations. The structures below are organized by how many sets are involved, and how many binary operations are used. Increased indentation is meant to indicate a more exotic structure, and the least indented levels are the most basic.\n\nThe following structures consist of a set with a binary operation. The most common structure is that of a \"group\". Other structures involve weakening or strengthening the axioms for groups, and may additionally use unary operations.\n\n\nThe main types of structures with one set having two binary operations are \"rings\" and \"lattices\". The axioms defining many of the other structures are modifications of the axioms for rings and lattices. One major difference between rings and lattices is that their two operations are related to each other in different ways. In ring-like structures, the two operations are linked by the distributive law; in lattice-like structures, the operations are linked by the absorption law.\n\n\nThe following structures have the common feature of having two sets, \"A\" and \"B\", so that there is a binary operation from \"A\"×\"A\" into \"A\" and another operation from \"A\"×\"B\" into \"A\".\n\n\nMany structures here are actually hybrid structures of the previously mentioned ones.\n\n\nThere are many examples of mathematical structures where algebraic structure exists alongside non-algebraic structure.\n\n\nSome algebraic structures find uses in disciplines outside of abstract algebra. The following is meant to demonstrate some specific applications in other fields.\n\nIn Physics:\n\nIn Mathematical logic:\n\nIn Computer science:\n\nA monograph available free online:\n"}
{"id": "3222625", "url": "https://en.wikipedia.org/wiki?curid=3222625", "title": "Pauling's rules", "text": "Pauling's rules\n\nPauling's rules are five rules published by Linus Pauling in 1929 for predicting and rationalizing the crystal structures of ionic compounds.\n\nFor typical ionic solids, the cations are smaller than the anions, and each cation is surrounded by coordinated anions which form a polyhedron. The sum of the ionic radii determines the cation-anion distance, while the cation-anion radius ratio formula_1 (or formula_2) determines the coordination number (C.N.) of the cation, as well as the shape of the coordinated polyhedron of anions.\n\nFor the coordination numbers and corresponding polyhedra in the table below, Pauling mathematically derived the \"minimum\" radius ratio for which the cation is in contact with the given number of anions (considering the ions as rigid spheres). If the cation is smaller, it will not be in contact with the anions which results in instability leading to a lower coordination number.\n\nThe three diagrams at right correspond to octahedral coordination with a coordination number of six: four anions in the plane of the diagrams, and two (not shown) above and below this plane. The central diagram shows the minimal radius ratio. The cation and any two anions form a right triangle, with formula_3, or formula_4. Then formula_5. Similar geometrical proofs yield the minimum radius ratios for the highly symmetrical cases C.N. = 3, 4 and 8.\n\nFor C.N. = 6 and a radius ratio greater than the minimum, the crystal is more stable since the cation is still in contact with six anions, but the anions are further from each other so that their mutual repulsion is reduced. An octahedron may then form with a radius ratio greater than or equal to .414, but as the ratio rises above .732, a cubic geometry becomes more stable. This explains why Na in NaCl with a radius ratio of 0.55 has octahedral coordination, whereas Cs in CsCl with a radius ratio of 0.93 has cubic coordination.\n\nIf the radius ratio is less than the minimum, two anions will tend to depart and the remaining four will rearrange into a tetrahedral geometry where they are all in contact with the cation.\n\nThe radius ratio rules are a first approximation which have some success in predicting coordination numbers, but many exceptions do exist.\n\nFor a given cation, Pauling defined the \"electrostatic bond strength\" to each coordinated anion as formula_6, where z is the cation charge and ν is the cation coordination number. A stable ionic structure is arranged to preserve \"local electroneutrality\", so that the sum of the strengths of the electrostatic bonds to an anion equals the charge on that anion.\n\nwhere formula_8 is the anion charge and the summation is over the adjacent cations. For simple solids, the formula_9 are equal for all cations coordinated to a given anion, so that the anion coordination number is the anion charge divided by each electrostatic bond strength. Some examples are given in the table.\n\nPauling showed that this rule is useful in limiting the possible structures to consider for more complex crystals such as the aluminosilicate mineral orthoclase, KAlSiO, with three different cations.\n\nThe sharing of edges and particularly faces by two anion polyhedra decreases the stability of an ionic structure. Sharing of corners does not decrease stability as much, so (for example) octahedra may share corners with one another.\n\nThe decrease in stability is due to the fact that sharing edges and faces places cations in closer proximity to each other, so that cation-cation electrostatic repulsion is increased. The effect is largest for cations with high charge and low C.N. (especially when r+/r- approaches the lower limit of the polyhedral stability).\n\nAs one example, Pauling considered the three mineral forms of titanium dioxide, each with a coordination number of 6 for the Ti cations. The most stable (and most abundant) form is rutile, in which the coordination octahedra are arranged so that each one shares only two edges (and no faces) with adjoining octahedra. The other two, less stable, forms are brookite and anatase, in which each octahedron shares three and four edges respectively with adjoining octahedra.\n\nIn a crystal containing different cations, those of high valency and small coordination number tend not to share polyhedron elements with one another. This rule tends to increase the distance between highly charged cations, so as to reduce the electrostatic repulsion between them.\n\nOne of Pauling's examples is olivine, MSiO, where M is a mixture of Mg at some sites and Fe at others. The structure contains distinct SiO tetrahedra which do not share any oxygens (at corners, edges or faces) with each other. The lower-valence Mg and Fe cations are surrounded by polyhedra which do share oxygens.\n\nThe number of essentially different kinds of constituents in a crystal tends to be small. The repeating units will tend to be identical because each atom in the structure is most stable in a specific environment. There may be two or three types of polyhedra, such as tetrahedra or octahedra, but there will not be many different types.\n"}
{"id": "21569386", "url": "https://en.wikipedia.org/wiki?curid=21569386", "title": "Pixel connectivity", "text": "Pixel connectivity\n\nIn image processing and image recognition, pixel connectivity is the way in which pixels in 2-dimensional (or voxels in 3-dimensional) images relate to their neighbors.\n\n4-connected pixels are neighbors to every pixel that touches one of their edges. These pixels are connected horizontally and vertically. In terms of pixel coordinates, every pixel that has the coordinates\n\nis connected to the pixel at formula_3.\n\n6-connected pixels are neighbors to every pixel that touches one of their corners (which includes pixels that touch one of their edges) in a hexagonal grid or stretcher bond rectangular grid.\n\nThere are several ways to map hexagonal tiles to integer pixel coordinates. With one method, in addition to the 4-connected pixels, the two pixels at coordinates formula_4 and formula_5 are connected to the pixel at formula_6.\n\n8-connected pixels are neighbors to every pixel that touches one of their edges or corners. These pixels are connected horizontally, vertically, and diagonally. In addition to 4-connected pixels, each pixel with coordinates formula_7 is connected to the pixel at formula_6.\n\n6-connected pixels are neighbors to every pixel that touches one of their faces. These pixels are connected along one of the primary axes. Each pixel with coordinates formula_9, formula_10, or formula_11 is connected to the pixel at formula_12.\n\n18-connected pixels are neighbors to every pixel that touches one of their faces or edges. These pixels are connected along either one or two of the primary axes. In addition to 6-connected pixels, each pixel with coordinates formula_13, formula_14, formula_15, formula_16, formula_17, or formula_18 is connected to the pixel at formula_12.\n\n26-connected pixels are neighbors to every pixel that touches one of their faces, edges, or corners. These pixels are connected along either one, two, or all three of the primary axes. In addition to 18-connected pixels, each pixel with coordinates formula_20, formula_21, formula_22, or formula_23 is connected to the pixel at formula_12.\n\n\n"}
{"id": "2833097", "url": "https://en.wikipedia.org/wiki?curid=2833097", "title": "Reachability", "text": "Reachability\n\nIn graph theory, reachability refers to the ability to get from one vertex to another within a graph. A vertex formula_1 can reach a vertex formula_2 (and formula_2 is reachable from formula_1) if there exists a sequence of adjacent vertices (i.e. a path) which starts with formula_1 and ends with formula_2.\n\nIn an undirected graph, reachability between all pairs of vertices can be determined by identifying the connected components of the graph. Any pair of vertices in such a graph can reach each other if and only if they belong to the same connected component. The connected components of an undirected graph can be identified in linear time. The remainder of this article focuses on the more difficult problem of determining pairwise reachability in a directed graph.\n\nFor a directed graph formula_7, with vertex set formula_8 and edge set formula_9, the reachability relation of formula_10 is the transitive closure of formula_9, which is to say the set of all ordered pairs formula_12 of vertices in formula_8 for which there exists a sequence of vertices formula_14 such that the edge formula_15 is in formula_9 for all formula_17.\n\nIf formula_10 is acyclic, then its reachability relation is a partial order; any partial order may be defined in this way, for instance as the reachability relation of its transitive reduction.\nA noteworthy consequence of this is that since partial orders are anti-symmetric, if formula_1 can reach formula_2, then we know that formula_2 \"cannot\" reach formula_1. Intuitively, if we could travel from formula_1 to formula_2 and back to formula_1, then formula_10 would contain a cycle, contradicting that it is acyclic.\nIf formula_10 is directed but \"not\" acyclic (i.e. it contains at least one cycle), then its reachability relation will correspond to a preorder instead of a partial order.\nAlgorithms for determining reachability fall into two classes: those that require preprocessing and those that do not.\n\nIf you have only one (or a few) queries to make, it may be more efficient to forgo the use of more complex data structures and compute the reachability of the desired pair directly. This can be accomplished in linear time using algorithms such as breadth first search or iterative deepening depth-first search.\n\nIf you will be making many queries, then a more sophisticated method may be used; the exact choice of method depends on the nature of the graph being analysed. In exchange for preprocessing time and some extra storage space, we can create a data structure which can then answer reachability queries on any pair of vertices in as low as formula_28 time. Three different algorithms and data structures for three different, increasingly specialized situations are outlined below.\n\nThe Floyd–Warshall algorithm \ncan be used to compute the transitive closure of any directed graph, which gives rise to the reachability relation as in the definition, above.\n\nThe algorithm requires formula_29 time and formula_30 space in the worst case. This algorithm is not solely interested in reachability as it also computes the shortest path distance between all pairs of vertices. For graphs containing negative cycles, shortest paths may be undefined, but reachability between pairs can still be noted.\n\nFor planar digraphs, a much faster method is available, as described by Mikkel Thorup in 2004.\nThis method can answer reachability queries on a planar graph in\nformula_28 time after spending formula_32 preprocessing time to create a data\nstructure of formula_32 size. This algorithm can also supply approximate\nshortest path distances, as well as route information.\n\nThe overall approach is to associate with each vertex a relatively small set of\nso-called separator paths such that any path from a vertex formula_34 to any other\nvertex formula_35 must go through at least one of the separators associated with formula_34 or\nformula_35. An outline of the reachability related sections follows.\n\nGiven a graph formula_10, the algorithm begins by organizing the vertices into layers starting from an\narbitrary vertex formula_39. The layers are built in alternating steps by first\nconsidering all vertices reachable \"from\" the previous step (starting with\njust formula_39) and then all vertices which reach \"to\" the previous step until\nall vertices have been assigned to a layer. By construction of the layers, every\nvertex appears at most two layers, and every directed path, or dipath, in formula_10 is contained within\ntwo adjacent layers formula_42 and formula_43. Let formula_44 be the last layer created, that\nis, the lowest value for formula_44 such that formula_46.\n\nThe graph is then re-expressed as a series of digraphs formula_47 where each formula_48 and where formula_49 is the\ncontraction of all previous levels formula_50 into a single vertex.\nBecause every dipath appears in at most two consecutive layers, and because\neach formula_51 is formed by two consecutive layers, every dipath in formula_10 appears in\nits entirety in at least one formula_51 (and no more than 2 consecutive such graphs)\n\nFor each formula_51, three separators are identified which, when removed, break the\ngraph into three components which each contain at most formula_55 the vertices of the\noriginal. As formula_51 is built from two layers of opposed dipaths, each separator\nmay consist of up to 2 dipaths, for a total of up to 6 dipaths over all of the\nseparators. Let formula_57 be this set of dipaths. The proof that such separators can always be\nfound is related to the Planar Separator Theorem of Lipton and Tarjan, and these\nseparators can be located in linear time.\n\nFor each formula_58, the directed nature of formula_59 provides for a natural indexing\nof its vertices from the start to the end of the path. For each vertex formula_34\nin formula_51, we locate the first vertex in formula_59 reachable by formula_34, and the last\nvertex in formula_59 that reaches to formula_34.\nThat is, we are looking at how early into formula_59 we can get from formula_34, and how far\nwe can stay in formula_59 and still get back to formula_34. This information is stored with\neach formula_34. Then for any pair of vertices formula_71 and formula_35, formula_71 can reach formula_35 \"via\" formula_59 if formula_71 connects to formula_59 earlier than formula_35 connects from formula_59.\n\nEvery vertex is labelled as above for each step of the recursion which builds\nformula_80. As this recursion has logarithmic depth, a total of\nformula_81 extra information is stored per vertex. From this point, a\nlogarithmic time query for reachability is as simple as looking over each pair\nof labels for a common, suitable formula_59. The original paper then works to tune the\nquery time down to formula_28.\n\nIn summarizing the analysis of this method, first consider that the layering\napproach partitions the vertices so that each vertex is considered only formula_28\ntimes. The separator phase of the algorithm breaks the graph into components\nwhich are at most formula_55 the size of the original graph, resulting in a\nlogarithmic recursion depth. At each level of the recursion, only linear work\nis needed to identify the separators as well as the connections possible between\nvertices. The overall result is formula_86 preprocessing time with only\nformula_81 additional information stored for each vertex.\n\nAn even faster method for pre-processing, due to T. Kameda in 1975,\ncan be used if the graph is planar, acyclic, and also exhibits the following additional properties: all 0-indegree and all 0-outdegree vertices appear on the same face (often assumed to be the outer face), and it is possible to partition the boundary of that face into two parts such that all 0-indegree vertices appear on one part, and all\n0-outdegree vertices appear on the other (i.e. the two types of vertices do not alternate).\n\nIf formula_10 exhibits these properties, then we can preprocess the graph in only\nformula_89 time, and store only formula_81 extra bits per vertex, answering\nreachability queries for any pair of vertices in formula_28 time with a simple\ncomparison.\n\nPreprocessing performs the following steps. We add a new vertex formula_1 which has an edge to each 0-indegree vertex, and another new vertex formula_2 with edges from each 0-outdegree vertex. Note that the properties of formula_10 allow us to do so while maintaining planarity, that is, there will still be no edge crossings after these additions. For each vertex we store the list of adjacencies (out-edges) in order of the planarity of the graph (for example, clockwise with respect to the graph's embedding). We then initialize a counter formula_95 and begin a Depth-First Traversal from formula_1. During this traversal, the adjacency list of each vertex is visited from left-to-right as needed. As vertices are popped from the traversal's stack, they are labelled with the value formula_97, and formula_97 is then decremented. Note that formula_2 is always labelled with the value formula_100 and formula_1 is always labelled with formula_102. The depth-first traversal is then repeated, but this time the adjacency list of each vertex is visited from right-to-left.\n\nWhen completed, formula_1 and formula_2, and their incident edges, are removed. Each\nremaining vertex stores a 2-dimensional label with values from formula_105 to formula_106.\nGiven two vertices formula_71 and formula_34, and their labels formula_109 and formula_110, we say that formula_111 if and only if formula_112, formula_113, and there exists at least one component formula_114 or formula_115 which is strictly\nless than formula_116 or formula_117, respectively.\n\nThe main result of this method then states that formula_34 is reachable from formula_71 if and\nonly if formula_111, which is easily calculated in formula_28 time.\n\nA related problem is to solve reachability queries with some number formula_44 of vertex failures. For example: \"Can vertex formula_71 still reach vertex formula_34 even though vertices formula_125 have failed and can no longer be used?\" A similar problem may consider edge failures rather than vertex failures, or a mix of the two. The breadth-first search technique works just as well on such queries, but constructing an efficient oracle is more challenging.\n\nAnother problem related to reachability queries is in quickly recalculating changes to reachability relationships when some portion of the graph is changed. For example, this is a relevant concern to garbage collection which needs to balance the reclamation of memory (so that it may be reallocated) with the performance concerns of the running application.\n\n"}
{"id": "41144748", "url": "https://en.wikipedia.org/wiki?curid=41144748", "title": "Redshift conjecture", "text": "Redshift conjecture\n\nIn mathematics, more specifically in chromatic homotopy theory, the redshift conjecture states, roughly, that algebraic K-theory formula_1 has chromatic level one higher than that of a complex-oriented ring spectrum \"R\".\nIt was formulated by John Rognes in a lecture at Schloss Ringberg, Germany, in January 1999, and made more precise by him in a lecture at Mathematische Forschungsinstitut Oberwolfach, Germany, in September 2000.\n\n"}
{"id": "46948278", "url": "https://en.wikipedia.org/wiki?curid=46948278", "title": "SQLf", "text": "SQLf\n\nSQLf is a SQL extended with fuzzy set theory application for expressing flexible (fuzzy) queries to traditional (or ″Regular″) Relational Databases. Among the known extensions proposed to SQL, at the present time, this is the most complete, because it allows the use of diverse fuzzy elements in all the constructions of the language SQL.\n\nSQLf is the only known proposal of flexible query system allowing linguistic quantification over set of rows in queries, achieved through the extension of SQL nesting and partitioning structures with fuzzy quantifiers. It also allows the use of quantifiers to qualify the quantity of search criteria satisfied by single rows. Several mechanisms are proposed for query evaluation, the most important being the one based on the derivation principle. This consists in deriving classic queries that produce, given a threshold \"t\", a \"t\"-cut of the result of the fuzzy query, so that the additional processing cost of using a fuzzy language is diminished.\n\nThe fundamental querying structure of SQLf is the multi-relational block. The conception of this structure is based on the three basic operations of the relational algebra: projection, cartesian product and selection, and the application of fuzzy sets’ concepts. The result of a SQLf query is a fuzzy set of rows that is a fuzzy relation instead of a regular relation.\nA basic block in SQLf consists of a codice_1 clause, a codice_2 clause and an optional codice_3 clause. The semantic of this query structure is:\n\nThe following is an example of a codice_1 query that returns a list of hotels that are cheap. The query retrieves all rows from the \"Hotels\" table that satisfice the fuzzy predicate \"cheap\" defined by the fuzzy set μ=(, , 25, 30). The result is sorted in descending order by the membership degree of the query.\n"}
{"id": "361598", "url": "https://en.wikipedia.org/wiki?curid=361598", "title": "Second Hardy–Littlewood conjecture", "text": "Second Hardy–Littlewood conjecture\n\nIn number theory, the second Hardy–Littlewood conjecture concerns the number of primes in intervals. The conjecture states that\n\nfor \"x\", \"y\" ≥ 2, where π(\"x\") denotes the prime-counting function, giving the number of prime numbers up to and including \"x\".\n\nThis means that the number of primes from \"x\" + 1 to \"x\" + \"y\" is always less than or equal to the number of primes from 1 to \"y\". This was proved to be inconsistent with the first Hardy–Littlewood conjecture on prime \"k\"-tuples, and the first violation is expected to likely occur for very large values of \"x\". For example, an admissible \"k\"-tuple (or prime constellation) of 447 primes can be found in an interval of \"y\" = 3159 integers, while π(3159) = 446. If the first Hardy–Littlewood conjecture holds, then the first such \"k\"-tuple is expected for \"x\" greater than 1.5 × 10 but less than 2.2 × 10.\n\n"}
{"id": "33985750", "url": "https://en.wikipedia.org/wiki?curid=33985750", "title": "Semën Samsonovich Kutateladze", "text": "Semën Samsonovich Kutateladze\n\nSemën Samsonovich Kutateladze (born October 2, 1945 in Leningrad, now St. Petersburg) is a mathematician. He is known for contributions to functional analysis and its applications to vector lattices and optimization.\n\nHe is professor of mathematics at Novosibirsk State University, where he has continued and enriched the scientific tradition of Leonid Kantorovich. \n\n\n\n"}
{"id": "9722901", "url": "https://en.wikipedia.org/wiki?curid=9722901", "title": "Shop drawing", "text": "Shop drawing\n\nA shop drawing is a drawing or set of drawings produced by the contractor, supplier, manufacturer, subcontractor, or fabricator. Shop drawings are typically required for prefabricated components. Examples of these include: elevators, structural steel, trusses, pre-cast concrete, windows, appliances, cabinets, air handling units, and millwork. Also critical are the installation and coordination shop drawings of the MEP trades such as sheet metal ductwork, piping, plumbing, fire protection, and electrical. Shop drawings are produced by contractors and suppliers under their contract with the owner. The shop drawing is the manufacturer’s or the contractor’s drawn version of information shown in the construction documents. The shop drawing normally shows more detail than the construction documents. It is drawn to explain the fabrication and/or installation of the items to the manufacturer’s production crew or contractor's installation crews. The style of the shop drawing is usually very different from that of the architect’s drawing. The shop drawing’s primary emphasis is on the particular product or installation and excludes notation concerning other products and installations, unless integration with the subject product is necessary.\n\nThe shop drawings should include information for the architect and engineer to compare to the specifications and drawings. The shop drawing should address the appearance, performance, and prescriptive descriptions in the specifications and construction drawings.\nThe shop drawing often is more detailed than the information shown in the construction documents to give the architect and engineer the opportunity to review the fabricator’s version of the product, prior to fabrication. References to the construction documents, drawings, and specifications assist the architect and engineer in their review of the shop drawings. Attachment of manufacturer’s material specifications, “catalog cut sheets,” and other manufacturer’s information may be helpful to accompany these drawings. Because shop drawings facilitate the architect’s and engineer’s approval of the product, they should be as clear and complete as possible.\n\nNotes concerning changes or differences from the original documents should be made on the shop drawing for the architect’s and engineer’s approval. Ultimately, they are responsible for changes in these drawings and should have the opportunity to analyze any modifications. A dialogue should occur between the fabricator and the architect and engineer about any areas needing clarification. Successful installations are the result of collaboration between the designer, fabricator, and contractor.\n\nDimensions, manufacturing conventions, and special fabrication instructions should be included on the shop drawing. It should be clear to fabrication personnel what will be manufactured from the shop drawings alone. The construction documents are rarely used as a reference in fabrication, with the fabricators relying on the shop drawing for all information.\n\nMost jobsite dimensions, such as the dimensions between two surfaces on the jobsite, need to be verified. A dimension may be shown on the construction drawings, but the actual dimension may vary, from very small to large increments, depending on jobsite conditions. It is extremely important that the fabricated item arrive on the jobsite ready to be installed without field modification. Special care must be taken by the contractor to measure and verify dimensions. In new construction, plan dimensions usually are sufficient for ordering many fabricated items such as structural steel or precast concrete.\n\nIn remodeling and renovation work, it is essential that field dimensions be verified prior to fabrication. Some fabricators, such as cabinet and casework suppliers, prefer not to rely on the contractor’s verification and will verify the dimensions with their own personnel.\n\nSome fabricators and manufacturers will provide symbols, data, or instructions concerning installation. This can include a list of other materials, such as fasteners or adhesives, appropriate but not included for the product.\n\nThird party review may be required for major building systems. An example of this would be a commercial chiller which would be furnished by the mechanical contractor, but would require electrical connections, plumbing, rigging, insulation and commissioning. Various third parties will need to review the installation information and confirm they are furnishing compatible equipment and proper layout of services. Review of installation information for major equipment should be reviewed with field supervisors including the project superintendent, trade foremen and field engineer. Installation of major equipment will dictate structural clearances and temporary openings.\n\nSome fabrications will require a sample submittal with the shop drawing, primarily for color and texture selection of finishes.\n\nProblems with design coordination, such as time consumption and ineffectiveness related to the current 2D paper-based process, are some of the top concerns of a general contractor, since late conflict correction increases the potential for errors in the field. In order to address this issue, there have been an increasing number of Architecture, Engineering and Construction (AEC) firms utilizing building information models (BIM) in their coordination and clash detection processes, which according to practitioners, allows for increased coordination and fewer field problems. Leite et al. compared types of clashes identified in a manual coordination process (overlay of 2D drawings on a light table by pairs of subcontractors) and through automatic clash detection using a Building Information Model (BIM). The automatic clash detection identified several clashes that were missed by the subcontractors, who were performing this task manually. Also, the manual clash detection identified clashes which could not possibly be found by the automatic clash detection software, since one of the clashing objects (e.g. cable trays) was not modeled in the BIM. This study also included site observations of field detected clashes, some of which were not identified in either manual or automatic processes. Leite noted that the combination of clashes identified in coordination meetings, those automatically detected, as well as those identified in the field enable identification of objects that need to be modeled in order to capture the largest possible number of clashes. This paper was limited to the comparison of types of clashes identified in each of the three methods during a specific project. Although their results cannot be generalized, they still provide insight towards the need to identify what needs to be modeled in a BIM for MEP coordination prior to the start of the coordination process.\n\nBecause writing comments on eight to ten copies is a tedious process and an inefficient use of the architect and engineer's time, they will frequently specify other \nmethods for distributing comments. Quick review is essential during the approval process.\nAny method that facilitates this, while providing ample opportunity for comment and complete distribution, should be considered. Although a procedure may be specified in the contract drawings, most architects and engineers are open to suggestions and innovations that speed up the process.\n\nShop drawings are required, in various forms, depending upon the practice of the \narchitect and engineer. A specific number of copies may be required by the \nspecification. An example distribution of the completed and corrected shop \ndrawings may include the:\n\n\nCorrections are made by the architect and engineer, and the shop drawing is corrected by the supplier, then the appropriate number of copies is distributed. This method can be time consuming, as the shop drawing is not approved until the corrections are made on it.\n\nThe architect and engineer make comments on the reproducible, then copies are distributed. This method facilitates the timely approval and distribution of the shop drawing. Review comments usually are obvious on the reproducible copy. When sepia copies are used, \nthe reproduction of the sepia often is not as clear as a normal blue-line print.\n\nWhen the supplier and designer have compatible CAD software or when universal file formats such as IFC, PDF or DWG are utilized, the review can be made from a CD, email or FTP transfer. Comments can be made by the designer in a bold font or changes can be boxed for emphasis.\n\nPopular CAD platforms used for generating shop drawings are Advance Steel, AutoCAD, Revit, CATIA, Creo Elements/Pro, Inventor, Solidworks and Tekla Structures.\n\nConcrete reinforcing is one of the many items requiring specialized shop drawings for the fabrication of the material. Concrete reinforcing is custom-fabricated from 60-foot-long reinforcing bars.\nThe reinforcing bars are cut to length and bent to specific configurations. The shop drawing and the accompanying “cut sheet” lists the quantity, sizes, lengths, and shapes of the reinforcing bar. This information is provided for review by the structural engineer to ensure that sufficient reinforcing is being supplied; fabrication of the bar by the supplier’s shop; an inventory list for the contractor, upon delivery the typical project has thousands of pieces of reinforcing steel that need to be organized for storage and installation; and placement by the ironworker. The Concrete Reinforcing Steel Institute (CRSI) has developed standard symbols, graphics, and formats for shop drawings and cut sheets that generally are used by reinforcing steel fabricators. Each fabricator, has particular style for shop drawings and cut sheets, depending on the draftspeople and Computer-aided design systems. Examples of software used are Advance Concrete, AutoCAD, MicroStation, ProConcrete and Tekla Structures.\n\n"}
{"id": "7771625", "url": "https://en.wikipedia.org/wiki?curid=7771625", "title": "Sir Frederick Pollock, 1st Baronet", "text": "Sir Frederick Pollock, 1st Baronet\n\nSir Jonathan Frederick Pollock, 1st Baronet, PC (23 September 1783 – 28 August 1870) was a British lawyer and Tory politician.\n\nPollock was the son of David Pollock, of Charing Cross, London, and the elder brother of Field Marshal Sir George Pollock, 1st Baronet. He was educated at St Paul's School and Trinity College, Cambridge. He was Senior Wrangler at Cambridge University. He is also thought to be one of the founding members of the Cambridge Union Society, along with Henry Bickersteth and Sir Edward Hall Alderson, both of Gonville and Caius College.\n\nPollock was Member of Parliament (MP) for Huntingdon from 1831 to 1844. He served as Attorney General between 1834 and 1835 and 1841 and 1844 in the Tory administrations of Sir Robert Peel. In 1841 he was admitted to the Privy Council and appointed Lord Chief Baron of the Exchequer, a post he held until 1868. Having been knighted on 29 December 1834, Pollock was created a Baronet, of Hatton in the County of Middlesex, on 2 August 1866. Apart from his political and legal career Pollock was elected a Fellow of the Royal Society in 1816. He contributed a number of papers in mathematics to the Royal Society, including one on what is now known as the Pollock's conjecture.\n\nPollock died in August 1870, aged 86, and was succeeded in the baronetcy by his eldest son, William. His fourth son, Charles Edward Pollock, apprenticed to his father, had no university education. He became a law reporter then co-serving Baron of the Court of Exchequer, becoming the last in that appeal court.\n\nTwo of Pollock's grandsons became prominent lawyers: Sir Frederick Pollock, 3rd Baronet (d.1937), was Professor of Jurisprudence at the University of Oxford; Ernest Pollock, 1st Viscount Hanworth (d.1936), served as Master of the Rolls.\n\n"}
{"id": "49923425", "url": "https://en.wikipedia.org/wiki?curid=49923425", "title": "Split (graph theory)", "text": "Split (graph theory)\n\nIn graph theory, a split of an undirected graph is a cut whose cut-set forms a complete bipartite graph. A graph is prime if it has no splits. The splits of a graph can be collected into a tree-like structure called the split decomposition or join decomposition, which can be constructed in linear time. This decomposition has been used for fast recognition of circle graphs and distance-hereditary graphs, as well as for other problems in graph algorithms.\n\nSplits and split decompositions were first introduced by , who also studied variants of the same notions for directed graphs.\n\nA cut of an undirected graph is a partition of the vertices into two nonempty subsets, the sides of the cut. The subset of edges that have one endpoint in each side is called a cut-set. When a cut-set forms a complete bipartite graph, its cut is called a split. Thus, a split can be described as a partition of the vertices of the graph into two subsets and , such that every neighbor of in is adjacent to every neighbor of in .\n\nA cut or split is trivial when one of its two sides has only one vertex in it; every trivial cut is a split. A graph is said to be prime (with respect to splits) if it has no nontrivial splits.\n\nTwo splits are said to cross if each side of one split has a non-empty intersection with each side of the other split. A split is called strong when it is not crossed by any other split. As a special case, every trivial split is strong. The strong splits of a graph give rise to a structure called the split decomposition or join decomposition of the graph. This decomposition can be represented by a tree whose leaves correspond one-to-one with the given graph, and whose edges correspond one-to-one with the strong splits of the graph, such that the partition of leaves formed by removing any edge from the tree is the same as the partition of vertices given by the associated strong split.\n\nEach internal node of the split decomposition tree of a graph is associated with a graph , called the quotient graph for node . The quotient graph can be formed by deleting from the tree, forming subsets of vertices in corresponding to the leaves in each of the resulting subtrees, and collapsing each of these vertex sets into a single vertex. Every quotient graph has one of three forms: it may be a prime graph, a complete graph, or a star.\n\nA graph may have exponentially many different splits, but they are all represented in the split decomposition tree, either as an edge of the tree (for a strong split) or as an arbitrary partition of a complete or star quotient graph (for a split that is not strong).\n\nIn a complete graph or complete bipartite graph, every cut is a split.\n\nIn a cycle graph of length four, the partition of the vertices given by 2-coloring the cycle is a nontrivial split, but for cycles of any longer length there are no nontrivial splits.\n\nA bridge of a graph that is not 2-edge-connected corresponds to a split, with each side of the split formed by the vertices on one side of the bridge. The cut-set of the split is just the single bridge edge, which is a special case of a complete bipartite subgraph. Similarly, if is an articulation point of a graph that is not 2-vertex-connected, then the graph has multiple splits in which and some but not all of the components formed by its deletion are on one side, and the remaining components are on the other side. In these examples, the cut-set of the split forms a star.\n\n already showed that it is possible to find the split decomposition in polynomial time. After subsequent improvements to the algorithm, linear time algorithms were discovered by and .\n\nSplit decomposition has been applied in the recognition of several important graph classes:\n\nSplit decomposition has also been used to simplify the solution of some problems that are NP-hard on arbitrary graphs:\nThese methods can lead to polynomial time algorithms for graphs in which each quotient graph has a simple structure that allows its subproblem to be computed efficiently. For instance, this is true of the graphs in which each quotient graph has constant size.\n"}
{"id": "19762817", "url": "https://en.wikipedia.org/wiki?curid=19762817", "title": "Steinitz's theorem", "text": "Steinitz's theorem\n\nIn polyhedral combinatorics, a branch of mathematics, Steinitz's theorem is a characterization of the undirected graphs formed by the edges and vertices of three-dimensional convex polyhedra: they are exactly the (simple) 3-vertex-connected planar graphs (with at least four \nvertices). That is, every convex polyhedron forms a 3-connected planar graph, and every 3-connected planar graph can be represented as the graph of a convex polyhedron. For this reason, the 3-connected planar graphs are also known as polyhedral graphs. Branko Grünbaum has called this theorem “the most important and deepest known result on 3-polytopes.”\n\nThe theorem appears in a 1922 paper of Ernst Steinitz, after whom it is named. It can be proven by mathematical induction (as Steinitz did), by finding the minimum-energy state of a two-dimensional spring system into three dimensions, or by using the circle packing theorem.\nSeveral extensions of the theorem are known, in which the polyhedron that realizes a given graph has additional constraints; for instance, every polyhedral graph is the graph of a convex polyhedron with integer coordinates, or the graph of a convex polyhedron all of whose edges are tangent to a common midsphere.\n\nIn higher dimensions, the problem of characterizing the graphs of convex polytopes remains open.\n\nAn undirected graph is a system of vertices and edges, each edge connecting two of the vertices. From any polyhedron one can form a graph, by letting the vertices of the graph correspond to the vertices of the polyhedron and by connecting any two graph vertices by an edge whenever the corresponding two polyhedron vertices are the endpoints of an edge of the polyhedron. This graph is known as the skeleton of the polyhedron.\n\nA graph is planar if it can be drawn with its vertices as points in the Euclidean plane, and its edges as curves that connect these points, such that no two edge curves cross each other and such that the point representing a vertex lies on the curve representing an edge only when the vertex is an endpoint of the edge. By Fáry's theorem, it is sufficient to consider only planar drawings in which the curves representing the edges are line segments. A graph is 3-connected if, after the removal of any two of its vertices, any other pair of vertices remain connected by a path.\nSteinitz's theorem states that these two conditions are both necessary and sufficient to characterize the skeletons of three-dimensional convex polyhedra: a given graph is the graph of a convex three-dimensional polyhedron, if and only if is planar and 3-vertex-connected.\n\nSteinitz's theorem is named after Ernst Steinitz, who submitted its first proof for publication in 1916.\n\nThe name \"Steinitz's theorem\" has also been applied to other results of Steinitz: \n\nOne direction of Steinitz's theorem (the easier direction to prove) states that the graph of every convex polyhedron is planar and 3-connected. As shown in the illustration, planarity can be shown by using a Schlegel diagram: if one places a light source near one face of the polyhedron, and a plane on the other side, the shadows of the polyhedron edges will form a planar graph, embedded in such a way that the edges are straight line segments. The 3-connectivity of a polyhedral graph is a special case of Balinski's theorem that the graph of any \"k\"-dimensional convex polytope is \"k\"-connected.\n\nThe other, more difficult, direction of Steinitz's theorem states that every planar 3-connected graph is the graph of a convex polyhedron. There are three standard approaches for this part: proofs by induction, lifting two-dimensional Tutte embeddings into three dimensions using the Maxwell–Cremona correspondence, and methods using the circle packing theorem to generate a canonical polyhedron.\n\nSteinitz' original proof involved finding a sequence of Δ-Y and Y-Δ transforms that reduce any 3-connected planar graph to \"K\", the graph of the tetrahedron. A Y-Δ transform removes a degree-three vertex from a graph, adding edges between all of its former neighbors if those edges did not already exist; the reverse transformation, a Δ-Y transform, removes the edges of a triangle from a graph and replaces them by a new degree-three vertex adjacent to the same three vertices. Once such a sequence is found, it can be reversed to give a sequence of Δ-Y and Y-Δ transforms that build up the desired polyhedron step by step starting from a polyhedron. Each Y-Δ transform in this sequence can be performed by slicing off a degree-three vertex from a polyhedron. A Δ-Y transform can be performed by removing a triangular face from a polyhedron and extending its neighboring faces until the point where they meet, but only when that triple intersection point of the three neighboring faces is on the correct side of the polyhedron; when the triple intersection point is not on the correct side, a projective transformation of the polyhedron suffices to move it to the correct side. Therefore, by induction on the number of Δ-Y and Y-Δ transforms needed to reduce a given graph to \"K\", every polyhedral graph can be realized as a polyhedron.\n\nA later work by Epifanov strengthened Steinitz's proof that every polyhedral graph can be reduced to \"K\" by Δ-Y and Y-Δ transforms. Epifanov proved that if two vertices are specified in a planar graph, then the graph can be reduced to a single edge between those terminals by combining Δ-Y and Y-Δ transforms with series-parallel reductions. Epifanov's proof was complicated and non-constructive, but it was simplified by Truemper using methods based on graph minors. Truemper observed that every grid graph is reducible by Δ-Y and Y-Δ transforms in this way, that this reducibility is preserved by graph minors, and that every planar graph is a minor of a grid graph. This idea can be used to replace Steinitz's lemma that a reduction sequence exists, in a proof of Steinitz's theorem using induction in the same way. However, there exist graphs that require a nonlinear number of steps in any sequence of Δ-Y and Y-Δ transforms. More precisely, \"Ω\"(\"n\") steps are sometimes necessary, and the best known upper bound on the number of steps is even worse, \"O\"(\"n\").\n\nAn alternative form of induction proof is based on removing edges (and compressing out the degree-two vertices that might be performed by this removal) or contracting edges and forming a minor of the given planar graph. Any polyhedral graph can be reduced to \"K\" by a linear number of these operations, and again the operations can be reversed and the reversed operations performed geometrically, giving a polyhedral realization of the graph. However, while it is simpler to prove that a reduction sequence exists for this type of argument, and the reduction sequences are shorter, the geometric steps needed to reverse the sequence are more complicated.\n\nIf a graph is drawn in the plane with straight line edges, then an equilibrium stress is defined as an assignment of nonzero real numbers (weights) to the edges, with the property that each vertex is in the position given by the weighted sum of its neighbors. According to the Maxwell–Cremona correspondence, an equilibrium stress can be lifted to a piecewise linear continuous three-dimensional surface such that the edges forming the boundaries between the flat parts of the surface project to the given drawing. The weight and length of each edge determines the difference in slopes of the surface on either side of the edge, and the condition that each vertex is in equilibrium with its neighbors is equivalent to the condition that these slope differences cause the surface to meet up with itself correctly in the neighborhood of the vertex. Positive weights translate to convex dihedral angles between two faces of the piecewise linear surface, and negative weights translate to concave dihedral angles. Conversely, every continuous piecewise-linear surface comes from an equilibrium stress in this way. If a finite planar graph is drawn and given an equilibrium stress in such a way that all interior edges of the drawing have positive weights, and all exterior edges have negative weights, then by translating this stress into a three-dimensional surface in this way, and then replacing the flat surface representing the exterior of the graph by its complement in the same plane, one obtains a convex poyhedron, with the additional property that its perpendicular projection onto the plane has no crossings.\n\nThe Maxwell–Cremona correspondence has been used to obtain polyhedral realizations of polyhedral graphs by combining it with a planar graph drawing method of W. T. Tutte, the Tutte embedding. Tutte's method begins by fixing one face of a polyhedral graph into convex position in the plane. This face will become the outer face of a drawing of a graph. The method continues by setting up a system of linear equations in the vertex coordinates, according to which each remaining vertex should be placed at the average of its neighbors. Then as Tutte showed, this system of equations will have a unique solution in which each face of the graph is drawn as a convex polygon. The result is almost an equilibrium stress: if one assigns weight one to each interior edge, then each interior vertex of the drawing is in equilibrium. However, it is not always possible to assign negative numbers to the exterior edges so that they, too, are in equilibrium.\nSuch an assignment is always possible when the outer face is a triangle, and so this method can be used to realize any polyhedral graph that has a triangular face.\nIf a polyhedral graph does not contain a triangular face, its dual graph does contain a triangle and is also polyhedral, so one can realize the dual in this way and then realize the original graph as the polar polyhedron of the dual realization. It is also possible to realize any polyhedral graph directly by choosing the outer face to be any face with at most five vertices (something that exists in all polyhedral graphs) and choosing more carefully the fixed shape of this face in such a way that the Tutte embedding can be lifted, or by using an incremental method instead of Tutte's method to find a liftable planar drawing that does not have equal weights for all the interior edges.\n\nAccording to one variant of the circle packing theorem, for every polyhedral graph and its dual graph, there exists a system of circles in the plane or on any sphere,\nrepresenting the vertices of both graphs, so that two adjacent vertices in the same graph are represented by tangent circles, a primal and dual vertex that represent a vertex and face that touch each other are represented by orthogonal circles, and all remaining pairs of circles are disjoint. From such a representation on a sphere, one can find a polyhedral realization of the given graph as the intersection of a collection of halfspaces, one for each circle that represents a dual vertex, with the boundary of the halfspace containing the circle. Alternatively and equivalently, one can find the same polyhedron as the convex hull of a collection of points (its vertices), such that the horizon seen when viewing the sphere from any vertex equals the circle that corresponds to that vertex. The sphere becomes the midsphere of the realization: each edge of the polyhedron is tangent to it, at the point where two tangent primal circles and two dual circles orthogonal to the primal circles and tangent to each other all meet.\n\nIt is possible to prove a stronger form of Steinitz's theorem, that any polyhedral graph can be realized by a convex polyhedron for which all of the vertex coordinates are integers. For instance,\nSteinitz's original induction-based proof can be strengthened in this way. However, the integers that would result from this construction are doubly exponential in the number of vertices of the given polyhedral graph. Writing down numbers of this magnitude in binary notation would require an exponential number of bits.\n\nSubsequent researchers have found lifting-based realization algorithms that use only O(\"n\") bits per vertex. It is also possible to relax the requirement that the coordinates be integers, and assign coordinates in such a way that the \"x\"-coordinates of the vertices are distinct integers in the range [0,2\"n\" − 4] and the other two coordinates are real numbers in the range [0,1], so that each edge has length at least one while the overall polyhedron has volume O(\"n\"). Some polyhedral graphs are known to be realizable on grids of only polynomial size; in particular this is true for the pyramids (realizations of wheel graphs), prisms (realizations of prism graphs), and stacked polyhedra (realizations of Apollonian networks).\n\nA Halin graph is a planar graph formed from a planar-embedded tree (with no degree-two vertices) by connecting the leaves of the tree into a cycle. Every Halin graph can be realized by a polyhedron in which this cycle forms a horizontal base face, every other face lies directly above the base face (as in the polyhedra realized through lifting), and every face has the same slope. Equivalently, the straight skeleton of the base face is combinatorially equivalent to the tree from which the Halin graph was formed. The proof of this result uses induction: any rooted tree may reduced to a smaller tree by removing the leaves from an internal node whose children are all leaves, the Halin graph formed from the smaller tree has a realization by the induction hypothesis, and it is possible to modify this realization in order to add any number of leaf children to the tree node whose children were removed.\n\nIn any polyhedron that represents a given polyhedral graph \"G\", the faces of \"G\" are exactly the cycles in \"G\" that do not separate \"G\" into two components: that is, removing a facial cycle from \"G\" leaves the rest of \"G\" as a connected subgraph. Thus, the faces are uniquely determined from the graph structure.\nAnother strengthening of Steinitz's theorem, by Barnette and Grünbaum, states that for any polyhedral graph, any face of the graph, and any convex polygon representing that face, it is possible to find a polyhedral realization of the whole graph that has the specified shape for the designated face. This is related to a theorem of Tutte, that any polyhedral graph can be drawn in the plane with all faces convex and any specified shape for its outer face. However, the planar graph drawings produced by Tutte's method do not necessarily lift to convex polyhedra. Instead, Barnette and Grünbaum prove this result using an inductive method It is also always possible, given a polyhedral graph \"G\" and an arbitrary cycle \"C\", to find a realization such that \"C\" forms the silhouette of the realization under parallel projection.\n\nThe Koebe–Andreev–Thurston circle packing theorem can be interpreted as providing another strengthening of Steinitz's theorem, that every 3-connected planar graph may be represented as a convex polyhedron in such a way that all of its edges are tangent to the same unit sphere. By performing a carefully chosen Möbius transformation of a circle packing before transforming it into a polyhedron, it is possible to find a polyhedral realization that realizes all the symmetries of the underlying graph, in the sense that every graph automorphism is a symmetry of the polyhedral realization. More generally, if \"G\" is a polyhedral graph and \"K\" is any smooth three-dimensional convex body, it is possible to find a polyhedral representation of \"G\" in which all edges are tangent to \"K\".\n\nCircle packing methods can also be used to characterize the graphs of polyhedra that have a circumsphere or insphere. The characterization involves edge weights, constrained by systems of linear inequalities. These weights correspond to the angles made by adjacent circles in a system of circles, made by the intersections of the faces of the polyhedron with their circumsphere or the horizons of the vertices of the polyhedron on its insphere.\n\nIn any dimension higher than three, the algorithmic Steinitz problem (given a lattice, determine whether it is the face lattice of a convex polytope) is complete for the existential theory of the reals by Richter-Gebert's universality theorem. However, because a given graph may correspond to more than one face lattice, it is difficult to extend this completeness result to the problem of recognizing the graphs of 4-polytopes, and this problem's complexity remains open.\n\nResearchers have also found graph-theoretic characterizations of the graphs of certain special classes of three-dimensional non-convex polyhedra and four-dimensional convex polytopes. However, in both cases, the general problem remains unsolved. Indeed, even the problem of determining which complete graphs are the graphs of non-convex polyhedra (other than \"K\" for the tetrahedron and \"K\" for the Császár polyhedron) remains unsolved.\n\nLászló Lovász has shown a correspondence between polyhedral representations of graphs and matrices realizing the Colin de Verdière graph invariants of the same graphs.\n"}
{"id": "10805909", "url": "https://en.wikipedia.org/wiki?curid=10805909", "title": "Subgroups of cyclic groups", "text": "Subgroups of cyclic groups\n\nIn abstract algebra, every subgroup of a cyclic group is cyclic. Moreover, for a finite cyclic group of order \"n\", every subgroup's order is a divisor of \"n\", and there is exactly one subgroup for each divisor. This result has been called the fundamental theorem of cyclic groups.\n\nFor every finite group \"G\" of order \"n\", the following statements are equivalent:\nThis statement is known by various names such as characterization by subgroups. (See also cyclic group for some characterization.)\n\nThere exist finite groups other than cyclic groups with the property that all proper subgroups are cyclic; the Klein group is an example. However, the Klein group has more than one subgroup of order 2, so it does not meet the conditions of the characterization.\n\nThe infinite cyclic group is isomorphic to the additive subgroup Z of the integers. There is one subgroup \"d\"Z for each integer \"d\" (consisting of the multiples of \"d\"), and with the exception of the trivial group (generated by \"d\" = 0) every such subgroup is itself an infinite cyclic group. Because the infinite cyclic group is a free group on one generator (and the trivial group is a free group on no generators), this result can be seen as a special case of the Nielsen–Schreier theorem that every subgroup of a free group is itself free.\n\nThe fundamental theorem for finite cyclic groups can be established from the same theorem for the infinite cyclic groups, by viewing each finite cyclic group as a quotient group of the infinite cyclic group.\n\nIn both the finite and the infinite case, the lattice of subgroups of a cyclic group is isomorphic to the dual of a divisibility lattice. In the finite case, the lattice of subgroups of a cyclic group of order \"n\" is isomorphic to the dual of the lattice of divisors of \"n\", with a subgroup of order \"n\"/\"d\" for each divisor \"d\". The subgroup of order \"n\"/\"d\" is a subgroup of the subgroup of order \"n\"/\"e\" if and only if \"e\" is a divisor of \"d\". The lattice of subgroups of the infinite cyclic group can be described in the same way, as the dual of the divisibility lattice of all positive integers. If the infinite cyclic group is represented as the additive group on the integers, then the subgroup generated by \"d\" is a subgroup of the subgroup generated by \"e\" if and only if \"e\" is a divisor of \"d\".\n\nDivisibility lattices are distributive lattices, and therefore so are the lattices of subgroups of cyclic groups. This provides another alternative characterization of the finite cyclic groups: they are exactly the finite groups whose lattices of subgroups are distributive. More generally, a finitely generated group is cyclic if and only if its lattice of subgroups is distributive and an arbitrary group is locally cyclic if and only its lattice of subgroups is distributive. The additive group of the rational numbers provides an example of a group that is locally cyclic, and that has a distributive lattice of subgroups, but that is not itself cyclic.\n"}
{"id": "44362809", "url": "https://en.wikipedia.org/wiki?curid=44362809", "title": "Symmetry (geometry)", "text": "Symmetry (geometry)\n\nA geometric object has symmetry if there is an \"operation\" or \"transformation\" (such as an isometry or affine map) that maps the figure/object onto itself; i.e., it is said that the object has an invariance under the transform. For instance, a circle rotated about its center will have the same shape and size as the original circle—all points before and after the transform would be indistinguishable. A circle is said to be \"symmetric under rotation\" or to have \"rotational symmetry\". If the isometry is the reflection of a plane figure, the figure is said to have reflectional symmetry or line symmetry; moreover, it is possible for a figure/object to have more than one line of symmetry.\n\nThe types of symmetries that are possible for a geometric object depend on the set of geometric transforms available, and on what object properties should remain unchanged after a transform. Because the composition of two transforms is also a transform and every transform has an inverse transform that undoes it, the set of transforms under which an object is symmetric form a mathematical group, the symmetry group of the object.\n\nThe most common group of transforms applied to objects are termed the Euclidean group of \"isometries,\" which are distance-preserving transformations in space commonly referred to as two-dimensional or three-dimensional (i.e., in plane geometry or solid geometry Euclidean spaces). These isometries consist of reflections, rotations, translations, and combinations of these basic operations. Under an isometric transformation, a geometric object is said to be symmetric if, after transformation, the object is indistinguishable from the object before the transformation. A geometric object is typically symmetric only under a subset or \"subgroup\" of all isometries. The kinds of isometry subgroups are described below, followed by other kinds of transform groups and by the types of object invariance that are possible in geometry.\n\nBy the Cartan–Dieudonné theorem, an orthogonal transformation in \"n\"-dimensional space can be represented by the composition of at most \"n\" reflections.\nReflectional symmetry, linear symmetry, mirror symmetry, mirror-image symmetry, or bilateral symmetry is symmetry with respect to reflection.\n\nIn one dimension, there is a point of symmetry about which reflection takes place; in two dimensions there is an axis of symmetry, and in three dimensions there is a plane of symmetry. An object or figure for which every point has a one-to-one mapping onto another, equidistant from and on opposite sides of a common plane is called mirror symmetric (see mirror image).\n\nThe axis of symmetry of a two-dimensional figure is a line such that, if a perpendicular is constructed, any two points lying on the perpendicular at equal distances from the axis of symmetry are identical. Another way to think about it is that if the shape were to be folded in half over the axis, the two halves would be identical: the two halves are each other's mirror image. Thus a square has four axes of symmetry, because there are four different ways to fold it and have the edges all match. A circle has infinitely many axes of symmetry passing through its center, for the same reason.\n\nIf the letter T is reflected along a vertical axis, it appears the same. This is sometimes called vertical symmetry. One can better use an unambiguous formulation; e.g., \"T has a vertical symmetry axis\" or \"T has left-right symmetry\".\n\nThe triangles with reflection symmetry are isosceles, the quadrilaterals with this symmetry are the kites and the isosceles trapezoids.\n\nFor each line or plane of reflection, the symmetry group is isomorphic with C (see point groups in three dimensions), one of the three types of order two (involutions), hence algebraically isomorphic to C. The fundamental domain is a half-plane or half-space.\n\nReflection symmetry can be generalized to other isometries of -dimensional space which are involutions, such as\n\nin a certain system of Cartesian coordinates. This reflects the space along an -dimensional affine subspace. If  = , then such a transformation is known as a point reflection, or an \"inversion through a point\". On the plane ( = 2) a point reflection is the same as a half-turn (180°) rotation; see below. \"Antipodal symmetry\" is an alternative name for a point reflection symmetry through the origin.\n\nSuch a \"reflection\" preserves orientation if and only if is an even number. This implies that for  = 3 (as well as for other odd ) a point reflection changes the orientation of the space, like a mirror-image symmetry. That is why in physics the term \"P-symmetry\" is used for both point reflection and mirror symmetry (P stands for parity). As a point reflection in three dimensions changes a left-handed coordinate system into a right-handed coordinate system, symmetry under a point reflection is also called a left-right symmetry.\n\nRotational symmetry is symmetry with respect to some or all rotations in -dimensional Euclidean space. Rotations are direct isometries; i.e., isometries preserving orientation. Therefore, a symmetry group of rotational symmetry is a subgroup of the special Euclidean group E().\n\nSymmetry with respect to all rotations about all points implies translational symmetry with respect to all translations (because translations are compositions of rotations about distinct points), and the symmetry group is the whole E(). This does not apply for objects because it makes space homogeneous, but it may apply for physical laws.\n\nFor symmetry with respect to rotations about a point we can take that point as origin. These rotations form the special orthogonal group SO(), which can be represented by the group of orthogonal matrices with determinant 1. For  = 3 this is the rotation group SO(3).\n\nIn another meaning of the word, the rotation group of an object is the symmetry group within E(), the group of rigid motions; in other words, the intersection of the full symmetry group and the group of rigid motions. For chiral objects it is the same as the full symmetry group.\n\nLaws of physics are SO(3)-invariant if they do not distinguish different directions in space. Because of Noether's theorem, rotational symmetry of a physical system is equivalent to the angular momentum conservation law. See also rotational invariance.\n\nTranslational symmetry leaves an object invariant under a discrete or continuous group of translations formula_1. The illustration on the right shows four congruent triangles generated by translations along the arrow. If the line of triangles extended to infinity in both directions, they would have a discrete translational symmetry; any translation that mapped one triangle onto another would leave the whole line unchanged.\n\nIn 2D, a glide reflection symmetry (in 3D it is called a glide plane symmetry, and a transflection in general) means that a reflection in a line or plane combined with a translation along the line / in the plane, results in the same object. The composition of two glide reflections results in a translation symmetry with twice the translation vector. The symmetry group comprising glide reflections and associated translations is the frieze group p11g and is isomorphic with the infinite cyclic group Z.\n\nIn 3D, a rotary reflection, rotoreflection or improper rotation is a rotation about an axis combined with reflection in a plane perpendicular to that axis. The symmetry groups associated with rotoreflections include:\n\nIn 3D geometry and higher, a screw axis (or rotary translation) is a combination of a rotation and a translation along the rotation axis.\n\nHelical symmetry is the kind of symmetry seen in such everyday objects as springs, Slinky toys, drill bits, and augers. The concept of helical symmetry can be visualized as the tracing in three-dimensional space that results from rotating an object at a constant angular speed while simultaneously translating at a constant linear speed along its axis of rotation. At any one point in time, these two motions combine to give a \"coiling angle\" that helps define the properties of the traced helix. When the tracing object rotates quickly and translates slowly, the coiling angle will be close to 0°. Conversely, if the rotation is slow and the translation is speedy, the coiling angle will approach 90°.\n\nThree main classes of helical symmetry can be distinguished based on the interplay of the angle of coiling and translation symmetries along the axis:\n\nIn 4D, a double rotation symmetry can be generated as the composite of two orthogonal rotations. It is similar to 3D screw axis which is the composite of a rotation and an orthogonal translation.\n\nA wider definition of geometric symmetry allows operations from a larger group than the Euclidean group of isometries. Examples of larger geometric symmetry groups are:\nIn Felix Klein's Erlangen program, each possible group of symmetries defines a geometry in which objects that are related by a member of the symmetry group are considered to be equivalent. For example, the Euclidean group defines Euclidean geometry, whereas the group of Möbius transformations defines projective geometry.\n\nScale symmetry means that if an object is expanded or reduced in size, the new object has the same properties as the original. This is \"not\" true of most physical systems, as witness the difference in the shape of the legs of an elephant and a mouse (so-called allometric scaling). Similarly, if a soft wax candle were enlarged to the size of a tall tree, it would immediately collapse under its own weight.\n\nA more subtle form of scale symmetry is demonstrated by fractals. As conceived by Benoît Mandelbrot, fractals are a mathematical concept in which the structure of a complex form looks similar at any degree of magnification, well seen in the Mandelbrot set. A coast is an example of a naturally occurring fractal, since it retains similar-appearing complexity at every level from the view of a satellite to a microscopic examination of how the water laps up against individual grains of sand. The branching of trees, which enables small twigs to stand in for full trees in dioramas, is another example.\n\nBecause fractals can generate the appearance of patterns in nature, they have a beauty and familiarity not typically seen with mathematically generated functions. Fractals have also found a place in computer-generated movie effects, where their ability to create complex curves with fractal symmetries results in more realistic virtual worlds.\n\nWith every geometry, Felix Klein associated an underlying group of symmetries. The hierarchy of geometries is thus mathematically represented as a hierarchy of these groups, and hierarchy of their invariants. For example, lengths, angles and areas are preserved with respect to the Euclidean group of symmetries, while only the incidence structure and the cross-ratio are preserved under the most general projective transformations. A concept of parallelism, which is preserved in affine geometry, is not meaningful in projective geometry. Then, by abstracting the underlying groups of symmetries from the geometries, the relationships between them can be re-established at the group level. Since the group of affine geometry is a subgroup of the group of projective geometry, any notion invariant in projective geometry is \"a priori\" meaningful in affine geometry; but not the other way round. If you add required symmetries, you have a more powerful theory but fewer concepts and theorems (which will be deeper and more general).\n\nWilliam Thurston introduced a similar version of symmetries in geometry. A model geometry is a simply connected smooth manifold \"X\" together with a transitive action of a Lie group \"G\" on \"X\" with compact stabilizers. The Lie group can be thought of as the group of symmetries of the geometry.\n\nA model geometry is called maximal if \"G\" is maximal among groups acting smoothly and transitively on \"X\" with compact stabilizers, i.e. if it is the maximal group of symmetries. Sometimes this condition is included in the definition of a model geometry.\n\nA geometric structure on a manifold \"M\" is a diffeomorphism from \"M\" to \"X\"/Γ for some model geometry \"X\", where Γ is a discrete subgroup of \"G\" acting freely on \"X\". If a given manifold admits a geometric structure, then it admits one whose model is maximal.\n\nA 3-dimensional model geometry \"X\" is relevant to the geometrization conjecture if it is maximal and if there is at least one compact manifold with a geometric structure modelled on \"X\". Thurston classified the 8 model geometries satisfying these conditions; they are listed below and are sometimes called Thurston geometries. (There are also uncountably many model geometries without compact quotients.)\n\n"}
{"id": "8802504", "url": "https://en.wikipedia.org/wiki?curid=8802504", "title": "Voltage graph", "text": "Voltage graph\n\nIn graph theory, a voltage graph is a directed graph whose edges are labelled invertibly by elements of a group. It is formally identical to a gain graph, but it is generally used in topological graph theory as a concise way to specify another graph called the derived graph of the voltage graph.\n\nTypical choices of the groups used for voltage graphs include the two-element group ℤ (for defining the bipartite double cover of a graph), free groups (for defining the universal cover of a graph), \"d\"-dimensional integer lattices ℤ (viewed as a group under vector addition, for defining periodic structures in \"d\"-dimensional Euclidean space), and finite cyclic groups ℤ for \"n\" > 2. When Π is a cyclic group, the voltage graph may be called a \"cyclic-voltage graph\".\n\nFormal definition of a Π-voltage graph, for a given group Π:\n\nNote that the voltages of a voltage graph need not satisfy Kirchhoff's voltage law, that the sum of voltages around a closed path is 0 (the identity element of the group), although this law does hold for the derived graphs described below. Thus, the name may be somewhat misleading. It results from the origin of voltage graphs as dual to the current graphs of topological graph theory.\n\nThe derived graph of a voltage graph formula_4 is the graph formula_5 whose vertex set is formula_6 and whose edge set is formula_7, where the endpoints of an edge (\"e\", \"k\") such that \"e\" has tail \"v\" and head \"w\" are formula_8 and formula_9.\n\nAlthough voltage graphs are defined for digraphs, they may be extended to undirected graphs by replacing each undirected edge by a pair of oppositely ordered directed edges and by requiring that these edges have labels that are inverse to each other in the group structure. In this case, the derived graph will also have the property that its directed edges form pairs of oppositely oriented edges, so the derived graph may itself be interpreted as being an undirected graph.\n\nThe derived graph is a covering graph of the given voltage graph. If no edge label of the voltage graph is the identity element, then the group elements associated with the vertices of the derived graph provide a coloring of the derived graph with a number of colors equal to the group order. An important special case is the bipartite double cover, the derived graph of a voltage graph in which all edges are labeled with the non-identity element of a two-element group. Because the order of the group is two, the derived graph in this case is guaranteed to be bipartite.\n\nPolynomial time algorithms are known for determining whether the derived graph of a formula_10-voltage graph contains any directed cycles.\n\nAny Cayley graph of a group Π, with a given set Γ of generators, may be defined as the derived graph for a Π-voltage graph having one vertex and |Γ| self-loops, each labeled with one of the generators in Γ.\n\nThe Petersen graph is the derived graph for a ℤ-voltage graph in the shape of a dumbbell with two vertices and three edges: one edge connecting the two vertices, and one self-loop on each vertex. One self-loop is labeled with 1, the other with 2, and the edge connecting the two vertices is labeled 0. More generally, the same construction allows any generalized Petersen graph GP(\"n\",\"k\") to be constructed as a derived graph of the same dumbbell graph with labels 1, 0, and \"k\" in the group ℤ.\n\nThe vertices and edges of any periodic tessellation of the plane may be formed as the derived graph of a finite graph, with voltages in ℤ.\n\n"}
{"id": "5468083", "url": "https://en.wikipedia.org/wiki?curid=5468083", "title": "Weitzenböck identity", "text": "Weitzenböck identity\n\nIn mathematics, in particular in differential geometry, mathematical physics, and representation theory a Weitzenböck identity, named after Roland Weitzenböck, expresses a relationship between two second-order elliptic operators on a manifold with the same leading symbol. (The origins of this terminology seem doubtful, however, as there does not seem to be any evidence that such identities ever appeared in Weitzenböck's work.) Usually Weitzenböck formulae are implemented for \"G\"-invariant self-adjoint operators between vector bundles associated to some principal \"G\"-bundle, although the precise conditions under which such a formula exists are difficult to formulate. This article focuses on three examples of Weitzenböck identities: from Riemannian geometry, spin geometry, and complex analysis.\n\nIn Riemannian geometry there are two notions of the Laplacian on differential forms over an oriented compact Riemannian manifold \"M\". The first definition uses the divergence operator \"δ\" defined as the formal adjoint of the de Rham operator \"d\":\nwhere \"α\" is any \"p\"-form and \"β\" is any ()-form, and formula_2 is the metric induced on the bundle of ()-forms. The usual form Laplacian is then given by\n\nOn the other hand, the Levi-Civita connection supplies a differential operator\n\nwhere Ω\"M\" is the bundle of \"p\"-forms and \"T\"\"M\" is the cotangent bundle of \"M\". The Bochner Laplacian is given by\n\nwhere formula_6 is the adjoint of formula_7.\n\nThe Weitzenböck formula then asserts that\n\nwhere \"A\" is a linear operator of order zero involving only the curvature.\n\nThe precise form of \"A\" is given, up to an overall sign depending on curvature conventions, by\n\nwhere\n\nIf \"M\" is an oriented spin manifold with Dirac operator ð, then one may form the spin Laplacian Δ = ð on the spin bundle. On the other hand, the Levi-Civita connection extends to the spin bundle to yield a differential operator\nAs in the case of Riemannian manifolds, let formula_5. This is another self-adjoint operator and, moreover, has the same leading symbol as the spin Laplacian. The Weitzenböck formula yields:\n\nwhere \"Sc\" is the scalar curvature. This result is also known as the Lichnerowicz formula.\n\nIf \"M\" is a compact Kähler manifold, there is a Weitzenböck formula relating the formula_15-Laplacian (see Dolbeault complex) and the Euclidean Laplacian on (\"p\",\"q\")-forms. Specifically, let\n\nAccording to the Weitzenböck formula, if α ε Ω\"M\", then\n\nwhere \"A\" is an operator of order zero involving the curvature. Specifically,\n\n\n"}
