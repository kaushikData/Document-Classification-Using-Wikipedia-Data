{"id": "486493", "url": "https://en.wikipedia.org/wiki?curid=486493", "title": "Action semantics", "text": "Action semantics\n\nAction semantics is a framework for the formal specification of semantics of programming languages invented by David Watt and Peter D. Mosses in the 1990s. It is a mixture of denotational, operational and algebraic semantics.\n\nAction Semantics aims to be pragmatic. Action-Semantic Descriptions (ASDs) are designed to scale up to handle realistic programming languages. This is aided by the extensibility and modifiability of ASDs. This helps to ensure that extensions and changes do not require too many changes in the description. This is in contrast to the typical case when extending denotational or operational semantics, which may require reformulation of the entire description.\n\nThe Action Semantics framework was originally developed at the University of Aarhus and the University of Glasgow. Groups and individuals around the world have since contributed further to the approach.\n\nAn important part of action semantics that gives it a modularity not seen in previous programming language semantics is the use of first-order semantic entities. First-order refers to how, unlike in denotational semantics, where a semantic function can be applied to another semantic function, in action semantics, a semantic entity cannot be applied to another semantic entity of its kind. Furthermore, the semantic entities utilized by action semantics broaden the framework’s ability to describe a programming language’s constructs by serving to denote both program behavior that is independent of any particular implementation and the way in which parts of a program influence the overall performance of the whole. The appropriately named action notation is employed to express the three types of semantic entities found in action semantics: \"actions\", \"data\", and \"yielders\". The central semantic entity in this framework is actions, with data and yielders occupying supplementary roles. More specifically, actions are the mechanisms through which yielders and data are processed. An action, which can occur within another action, is a step-by-step representation of program behavior, where each step accesses current information, changes current information, or does both. Yielders appear within actions and only access current information. A yielder entity is capable of being evaluated, and when it is, the product is a datum entity.\n\nAction entities can directly represent programs’ semantics by describing possible program behaviors or represent, in a more indirect way, the impact that individual pieces of a program, like statements or expressions, have on the semantics of the program as a whole. They model computational behavior by indicating changes in state through their generation of new values from passed values. Specifically, an action accepts data passed to it via the current information — the transient data given to it, the bindings received by it, and the current state of storage — and, from this, gives new transient data, creates new bindings, updates the state of storage, or any combination of these. An action entity can culminate in three possible ways. It can: \"complete\" (terminate normally), \"escape\" (terminate in an exception), \"fail\" (alternative is discarded), or \"diverge\" (not terminate).\n\nThere are four categories of information that are processed by action performance. \"Transient information\" corresponds to intermediate results and is accessible for immediate use by the action. The data that comprises transient information encompasses the values given by expressions. If these values are not immediately used, they are lost. \"Scoped information\" corresponds to symbol tables and can be referenced from anywhere within the action and its sub-actions. It is also possible for such information to be hidden within a sub-action, via the creation of an inner scope, in which case it would be only locally accessible within that scope, to that sub-action. \"Stable information\" corresponds to values assigned to variables and can be modified in the action performance. Because alterations to storage during the performance of an action are persistent, only explicit actions can cause such modifications. In accordance with this, stable information is available until it is explicitly destroyed. And, unlike scoped information, it cannot be hidden. \"Permanent information\" corresponds to data exchanged between actions and can be extended but not modified. Transient information is produced only when an action completes or escapes, and scoped information is produced only when an action completes. The modification of stable information and the extension of permanent information must take place during action performance.\n\nAn action entity has five different facets, one for processing that does not rely on information, and four for processing each of the four different types of information. The \"basic\" facet, an example of which would be control flows, is not tied to information of any kind. The \"functional\" facet deals with the processing of transient information and is characterized by actions giving and accepting data. The \"declarative\" facet deals with the processing of scoped information and is characterized by actions creating and receiving bindings. The \"imperative\" facet deals with the processing of stable information and is characterized by actions allocating and freeing storage cells, and fetching and modifying the data stored in them. The \"communicative\" facet deals with processing permanent information and is characterized by actions sending and receiving messages and “offer[ing] contracts to agents.” There are two different kinds of actions in terms of their effect on the information in each facet. \"Primitive actions\" only affect the information in one facet. Action \"combinators\" permit actions that involve multiple facets, governing how control and information flows for each facet involved in a combined action. In combining actions, action combinators governor the sequencing of sub-action performances and the incoming and outgoing flows of data for each sub-action.\n\nData entities are the items that comprise the information processed in action entities. The data is arranged into structures known as sorts. Sorts are sets of mathematical objects, include operations that can be performed on those objects, and are defined according to algebraic criteria. These structures allow access to each individual entity. Examples of data entities can include concrete elements like maps, lists, sets, strings, characters, numbers, and truth values, more abstract elements used solely for the purpose of some computational operation, namely data access, like agents, cells corresponding to memory locations, and tokens, or elements like contracts and messages that are a composite of data components. An \"abstraction\" is a data entity that encapsulates an action entity, in which case enacting the abstraction results in the action being performed. This is the technique by which action semantics represents the declaring and invoking of subprograms.\nYielder entities consist of unevaluated quantities of data. The values of these quantities are contingent on the current information and state of computation. Yielders draw on transient data, bindings, and storage to select the information to be processed by actions. It is during action performance that yielders are evaluated, and their evaluation results in data entities. While the current information can influence the data produced by the evaluation of a yielder entity, the evaluation cannot influence the current information. If data operations are employed on yielder entities, compound yielder entities may be formed as a result.\n\nRegular English words serve as the symbols of action notation. Action notation is designed to simulate natural language, which is illustrated in the parts of speech used to denote semantic entities. Action entities are represented by verb phrases and data and yielder entities by noun phrases. The result of this choice of symbols is a framework that is highly readable and no less formal than other frameworks since it remains precisely defined.\n\nAction semantics embodies a level of abstraction that increases its comprehensibility. The specifics of the control and data flows that an action involves are implicitly incorporated in the action, as opposed to being explicitly expressed as the details of semantic functions are in denotational semantics. When an action is performed, most information processing and manipulation occurs automatically.\n\nProgram phrases are mapped to actions when constructing a definition of a programming language’s meaning in action semantics. The execution of a programming phrase corresponds to the performance of the action to which it maps.\n\nThe programming language specification generated from the application of action semantics can be broken down into a lower level (\"microsemantics\") and an upper level (\"macrosemantics\"). The lower level consists of defining the meaning of action notation, while the upper level consists of defining the meaning of a programming language, using action notation to do so.\n"}
{"id": "2986091", "url": "https://en.wikipedia.org/wiki?curid=2986091", "title": "Alfred Young", "text": "Alfred Young\n\nAlfred Young, FRS (16 April 1873 – 15 December 1940) was a British mathematician.\n\nHe was born in Widnes, Lancashire, England, and educated at Monkton Combe School in Somerset and Clare College, Cambridge, graduating BA as 10th Wrangler in 1895. He is known for his work in the area of group theory. Both Young diagrams and Young tableaux (which he introduced in 1900) are named after him.\n\nYoung was appointed to the position of lecturer at Selwyn College, Cambridge, in 1901, transferring to Clare College in 1905. In 1902 he collaborated with John Hilton Grace on the book \"The Algebra of Invariants\".\n\nIn 1907 he married Edith Clara \"née\" Wilson. In 1908 he became an ordained clergyman, and in 1910 became parish priest at Birdbrook in Essex, a village 25 miles east of Cambridge. He lived there the rest of his life, but in 1926 began lecturing again at Cambridge.\n\nMost of his long series of papers on invariant theory and the symmetric group were written while he was a clergyman.\n\n\n"}
{"id": "18716923", "url": "https://en.wikipedia.org/wiki?curid=18716923", "title": "Algebra", "text": "Algebra\n\nAlgebra (from Arabic \"al-jabr\", literally meaning \"reunion of broken parts\") is one of the broad parts of mathematics, together with number theory, geometry and analysis. In its most general form, algebra is the study of mathematical symbols and the rules for manipulating these symbols; it is a unifying thread of almost all of mathematics. It includes everything from elementary equation solving to the study of abstractions such as groups, rings, and fields. The more basic parts of algebra are called elementary algebra; the more abstract parts are called abstract algebra or modern algebra. Elementary algebra is generally considered to be essential for any study of mathematics, science, or engineering, as well as such applications as medicine and economics. Abstract algebra is a major area in advanced mathematics, studied primarily by professional mathematicians.\n\nElementary algebra differs from arithmetic in the use of abstractions, such as using letters to stand for numbers that are either unknown or allowed to take on many values. For example, in formula_1 the letter formula_2 is unknown, but the law of inverses can be used to discover its value: formula_3. In , the letters formula_4 and formula_5 are variables, and the letter formula_6 is a constant, the speed of light in a vacuum. Algebra gives methods for writing formulas and solving equations that are much clearer and easier than the older method of writing everything out in words.\n\nThe word \"algebra\" is also used in certain specialized ways. A special kind of mathematical object in abstract algebra is called an \"algebra\", and the word is used, for example, in the phrases linear algebra and algebraic topology.\n\nA mathematician who does research in algebra is called an algebraist.\n\nThe word \"algebra\" comes from the Arabic (\"\" lit. \"the reunion of broken parts\") from the title of the book \"Ilm al-jabr wa'l-muḳābala\" by the Persian mathematician and astronomer al-Khwarizmi. The word entered the English language during the fifteenth century, from either Spanish, Italian, or Medieval Latin. It originally referred to the surgical procedure of setting broken or dislocated bones. The mathematical meaning was first recorded in the sixteenth century.\n\nThe word \"algebra\" has several related meanings in mathematics, as a single word or with qualifiers.\n\n\nAlgebra began with computations similar to those of arithmetic, with letters standing for numbers. This allowed proofs of properties that are true no matter which numbers are involved. For example, in the quadratic equation\nformula_8 can be any numbers whatsoever (except that formula_9 cannot be formula_10), and the quadratic formula can be used to quickly and easily find the values of the unknown quantity formula_2 which satisfy the equation. That is to say, to find all the solutions of the equation.\n\nHistorically, and in current teaching, the study of algebra starts with the solving of equations such as the quadratic equation above. Then more general questions, such as \"does an equation have a solution?\", \"how many solutions does an equation have?\", \"what can be said about the nature of the solutions?\" are considered. These questions led extending algebra to non-numerical objects, such as permutations, vectors, matrices, and polynomials. The structural properties of these non-numerical objects were then abstracted into algebraic structures such as groups, rings, and fields.\n\nBefore the 16th century, mathematics was divided into only two subfields, arithmetic and geometry. Even though some methods, which had been developed much earlier, may be considered nowadays as algebra, the emergence of algebra and, soon thereafter, of infinitesimal calculus as subfields of mathematics only dates from the 16th or 17th century. From the second half of 19th century on, many new fields of mathematics appeared, most of which made use of both arithmetic and geometry, and almost all of which used algebra.\n\nToday, algebra has grown until it includes many branches of mathematics, as can be seen in the Mathematics Subject Classification\nwhere none of the first level areas (two digit entries) is called \"algebra\". Today algebra includes section 08-General algebraic systems, 12-Field theory and polynomials, 13-Commutative algebra, 15-Linear and multilinear algebra; matrix theory, 16-Associative rings and algebras, 17-Nonassociative rings and algebras, 18-Category theory; homological algebra, 19-K-theory and 20-Group theory. Algebra is also used extensively in 11-Number theory and 14-Algebraic geometry.\n\nThe roots of algebra can be traced to the ancient Babylonians, who developed an advanced arithmetical system with which they were able to do calculations in an algorithmic fashion. The Babylonians developed formulas to calculate solutions for problems typically solved today by using linear equations, quadratic equations, and indeterminate linear equations. By contrast, most Egyptians of this era, as well as Greek and Chinese mathematics in the 1st millennium BC, usually solved such equations by geometric methods, such as those described in the \"Rhind Mathematical Papyrus\", Euclid's \"Elements\", and \"The Nine Chapters on the Mathematical Art\". The geometric work of the Greeks, typified in the \"Elements\", provided the framework for generalizing formulae beyond the solution of particular problems into more general systems of stating and solving equations, although this would not be realized until mathematics developed in medieval Islam.\n\nBy the time of Plato, Greek mathematics had undergone a drastic change. The Greeks created a geometric algebra where terms were represented by sides of geometric objects, usually lines, that had letters associated with them. Diophantus (3rd century AD) was an Alexandrian Greek mathematician and the author of a series of books called \"Arithmetica\". These texts deal with solving algebraic equations, and have led, in number theory to the modern notion of Diophantine equation.\n\nEarlier traditions discussed above had a direct influence on the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī (c. 780–850). He later wrote \"The Compendious Book on Calculation by Completion and Balancing\", which established algebra as a mathematical discipline that is independent of geometry and arithmetic.\n\nThe Hellenistic mathematicians Hero of Alexandria and Diophantus as well as Indian mathematicians such as Brahmagupta continued the traditions of Egypt and Babylon, though Diophantus' \"Arithmetica\" and Brahmagupta's \"Brāhmasphuṭasiddhānta\" are on a higher level. For example, the first complete arithmetic solution (including zero and negative solutions) to quadratic equations was described by Brahmagupta in his book \"Brahmasphutasiddhanta\". Later, Persian and Arabic mathematicians developed algebraic methods to a much higher degree of sophistication. Although Diophantus and the Babylonians used mostly special \"ad hoc\" methods to solve equations, Al-Khwarizmi's contribution was fundamental. He solved linear and quadratic equations without algebraic symbolism, negative numbers or zero, thus he had to distinguish several types of equations.\n\nIn the context where algebra is identified with the theory of equations, the Greek mathematician Diophantus has traditionally been known as the \"father of algebra\" and in context where it is identified with rules for manipulating and solving equations, Persian mathematician al-Khwarizmi is regarded as \"the father of algebra\". A debate now exists whether who (in general sense) is more entitled to be known as \"the father of algebra\". Those who support Diophantus point to the fact that the algebra found in \"Al-Jabr\" is slightly more elementary than the algebra found in \"Arithmetica\" and that \"Arithmetica\" is syncopated while \"Al-Jabr\" is fully rhetorical. Those who support Al-Khwarizmi point to the fact that he introduced the methods of \"reduction\" and \"balancing\" (the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation) which the term \"al-jabr\" originally referred to, and that he gave an exhaustive explanation of solving quadratic equations, supported by geometric proofs, while treating algebra as an independent discipline in its own right. His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study\". He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems\".\n\nAnother Persian mathematician Omar Khayyam is credited with identifying the foundations of algebraic geometry and found the general geometric solution of the cubic equation. His book \"Treatise on Demonstrations of Problems of Algebra\" (1070), which laid down the principles of algebra, is part of the body of Persian mathematics that was eventually transmitted to Europe. Yet another Persian mathematician, Sharaf al-Dīn al-Tūsī, found algebraic and numerical solutions to various cases of cubic equations. He also developed the concept of a function. The Indian mathematicians Mahavira and Bhaskara II, the Persian mathematician Al-Karaji, and the Chinese mathematician Zhu Shijie, solved various cases of cubic, quartic, quintic and higher-order polynomial equations using numerical methods. In the 13th century, the solution of a cubic equation by Fibonacci is representative of the beginning of a revival in European algebra. Abū al-Ḥasan ibn ʿAlī al-Qalaṣādī (1412–1486) took \"the first steps toward the introduction of algebraic symbolism\". He also computed ∑\"n\", ∑\"n\" and used the method of successive approximation to determine square roots. As the Islamic world was declining, the European world was ascending. And it is here that algebra was further developed.\n\nFrançois Viète's work on new algebra at the close of the 16th century was an important step towards modern algebra. In 1637, René Descartes published \"La Géométrie\", inventing analytic geometry and introducing modern algebraic notation. Another key event in the further development of algebra was the general algebraic solution of the cubic and quartic equations, developed in the mid-16th century. The idea of a determinant was developed by Japanese mathematician Seki Kōwa in the 17th century, followed independently by Gottfried Leibniz ten years later, for the purpose of solving systems of simultaneous linear equations using matrices. Gabriel Cramer also did some work on matrices and determinants in the 18th century. Permutations were studied by Joseph-Louis Lagrange in his 1770 paper \"Réflexions sur la résolution algébrique des équations\" devoted to solutions of algebraic equations, in which he introduced Lagrange resolvents. Paolo Ruffini was the first person to develop the theory of permutation groups, and like his predecessors, also in the context of solving algebraic equations.\n\nAbstract algebra was developed in the 19th century, deriving from the interest in solving equations, initially focusing on what is now called Galois theory, and on constructibility issues. George Peacock was the founder of axiomatic thinking in arithmetic and algebra. Augustus De Morgan discovered relation algebra in his \"Syllabus of a Proposed System of Logic\". Josiah Willard Gibbs developed an algebra of vectors in three-dimensional space, and Arthur Cayley developed an algebra of matrices (this is a noncommutative algebra).\n\nSome areas of mathematics that fall under the classification abstract algebra have the word algebra in their name; linear algebra is one example. Others do not: group theory, ring theory, and field theory are examples. In this section, we list some areas of mathematics with the word \"algebra\" in the name.\n\n\nMany mathematical structures are called algebras:\n\n\nElementary algebra is the most basic form of algebra. It is taught to students who are presumed to have no knowledge of mathematics beyond the basic principles of arithmetic. In arithmetic, only numbers and their arithmetical operations (such as +, −, ×, ÷) occur. In algebra, numbers are often represented by symbols called variables (such as \"a\", \"n\", \"x\", \"y\" or \"z\"). This is useful because:\n\nA polynomial is an expression that is the sum of a finite number of non-zero terms, each term consisting of the product of a constant and a finite number of variables raised to whole number powers. For example, \"x\" + 2\"x\" − 3 is a polynomial in the single variable \"x\". A polynomial expression is an expression that may be rewritten as a polynomial, by using commutativity, associativity and distributivity of addition and multiplication. For example, (\"x\" − 1)(\"x\" + 3) is a polynomial expression, that, properly speaking, is not a polynomial. A polynomial function is a function that is defined by a polynomial, or, equivalently, by a polynomial expression. The two preceding examples define the same polynomial function.\n\nTwo important and related problems in algebra are the factorization of polynomials, that is, expressing a given polynomial as a product of other polynomials that can not be factored any further, and the computation of polynomial greatest common divisors. The example polynomial above can be factored as (\"x\" − 1)(\"x\" + 3). A related class of problems is finding algebraic expressions for the roots of a polynomial in a single variable.\n\nIt has been suggested that elementary algebra should be taught to students as young as eleven years old, though in recent years it is more common for public lessons to begin at the eighth grade level (≈ 13 y.o. ±) in the United States. However, in some US schools, algebra is started in ninth grade.\n\nAbstract algebra extends the familiar concepts found in elementary algebra and arithmetic of numbers to more general concepts. Here are listed fundamental concepts in abstract algebra.\n\nSets: Rather than just considering the different types of numbers, abstract algebra deals with the more general concept of \"sets\": a collection of all objects (called elements) selected by property specific for the set. All collections of the familiar types of numbers are sets. Other examples of sets include the set of all two-by-two matrices, the set of all second-degree polynomials (\"ax\" + \"bx\" + \"c\"), the set of all two dimensional vectors in the plane, and the various finite groups such as the cyclic groups, which are the groups of integers modulo \"n\". Set theory is a branch of logic and not technically a branch of algebra.\n\nBinary operations: The notion of addition (+) is abstracted to give a \"binary operation\", ∗ say. The notion of binary operation is meaningless without the set on which the operation is defined. For two elements \"a\" and \"b\" in a set \"S\", \"a\" ∗ \"b\" is another element in the set; this condition is called closure. Addition (+), subtraction (−), multiplication (×), and division (÷) can be binary operations when defined on different sets, as are addition and multiplication of matrices, vectors, and polynomials.\n\nIdentity elements: The numbers zero and one are abstracted to give the notion of an \"identity element\" for an operation. Zero is the identity element for addition and one is the identity element for multiplication. For a general binary operator ∗ the identity element \"e\" must satisfy \"a\" ∗ \"e\" = \"a\" and \"e\" ∗ \"a\" = \"a\", and is necessarily unique, if it exists. This holds for addition as \"a\" + 0 = \"a\" and 0 + \"a\" = \"a\" and multiplication \"a\" × 1 = \"a\" and 1 × \"a\" = \"a\". Not all sets and operator combinations have an identity element; for example, the set of positive natural numbers (1, 2, 3, ...) has no identity element for addition.\n\nInverse elements: The negative numbers give rise to the concept of \"inverse elements\". For addition, the inverse of \"a\" is written −\"a\", and for multiplication the inverse is written \"a\". A general two-sided inverse element \"a\" satisfies the property that \"a\" ∗ \"a\" = \"e\" and \"a\" ∗ \"a\" = \"e\", where \"e\" is the identity element.\n\nAssociativity: Addition of integers has a property called associativity. That is, the grouping of the numbers to be added does not affect the sum. For example: . In general, this becomes (\"a\" ∗ \"b\") ∗ \"c\" = \"a\" ∗ (\"b\" ∗ \"c\"). This property is shared by most binary operations, but not subtraction or division or octonion multiplication.\n\nCommutativity: Addition and multiplication of real numbers are both commutative. That is, the order of the numbers does not affect the result. For example: 2 + 3 = 3 + 2. In general, this becomes \"a\" ∗ \"b\" = \"b\" ∗ \"a\". This property does not hold for all binary operations. For example, matrix multiplication and quaternion multiplication are both non-commutative.\n\nCombining the above concepts gives one of the most important structures in mathematics: a group. A group is a combination of a set \"S\" and a single binary operation ∗, defined in any way you choose, but with the following properties:\n\nIf a group is also commutative—that is, for any two members \"a\" and \"b\" of \"S\", \"a\" ∗ \"b\" is identical to \"b\" ∗ \"a\"—then the group is said to be abelian.\n\nFor example, the set of integers under the operation of addition is a group. In this group, the identity element is 0 and the inverse of any element \"a\" is its negation, −\"a\". The associativity requirement is met, because for any integers \"a\", \"b\" and \"c\", (\"a\" + \"b\") + \"c\" = \"a\" + (\"b\" + \"c\")\n\nThe nonzero rational numbers form a group under multiplication. Here, the identity element is 1, since 1 × \"a\" = \"a\" × 1 = \"a\" for any rational number \"a\". The inverse of \"a\" is 1/\"a\", since \"a\" × 1/\"a\" = 1.\n\nThe integers under the multiplication operation, however, do not form a group. This is because, in general, the multiplicative inverse of an integer is not an integer. For example, 4 is an integer, but its multiplicative inverse is ¼, which is not an integer.\n\nThe theory of groups is studied in group theory. A major result in this theory is the classification of finite simple groups, mostly published between about 1955 and 1983, which separates the finite simple groups into roughly 30 basic types.\n\nSemigroups, quasigroups, and monoids are structures similar to groups, but more general. They comprise a set and a closed binary operation, but do not necessarily satisfy the other conditions. A semigroup has an \"associative\" binary operation, but might not have an identity element. A monoid is a semigroup which does have an identity but might not have an inverse for every element. A quasigroup satisfies a requirement that any element can be turned into any other by either a unique left-multiplication or right-multiplication; however the binary operation might not be associative.\n\nAll groups are monoids, and all monoids are semigroups.\n\nGroups just have one binary operation. To fully explain the behaviour of the different types of numbers, structures with two operators need to be studied. The most important of these are rings, and fields.\n\nA ring has two binary operations (+) and (×), with × distributive over +. Under the first operator (+) it forms an \"abelian group\". Under the second operator (×) it is associative, but it does not need to have identity, or inverse, so division is not required. The additive (+) identity element is written as 0 and the additive inverse of \"a\" is written as −\"a\".\n\nDistributivity generalises the \"distributive law\" for numbers. For the integers and and × is said to be \"distributive\" over +.\n\nThe integers are an example of a ring. The integers have additional properties which make it an integral domain.\n\nA field is a \"ring\" with the additional property that all the elements excluding 0 form an \"abelian group\" under ×. The multiplicative (×) identity is written as 1 and the multiplicative inverse of \"a\" is written as \"a\".\n\nThe rational numbers, the real numbers and the complex numbers are all examples of fields.\n\n\n\n"}
{"id": "31453337", "url": "https://en.wikipedia.org/wiki?curid=31453337", "title": "Algebraic geometry of projective spaces", "text": "Algebraic geometry of projective spaces\n\nProjective space plays a central role in algebraic geometry. The aim of this article is to define the notion in terms of abstract algebraic geometry and to describe some basic uses of projective space.\n\nLet k be an algebraically closed field, and \"V\" be a finite-dimensional vector space over k. The symmetric algebra of the dual vector space \"V*\" is called the polynomial ring on \"V\" and denoted by k[\"V\"]. It is a naturally graded algebra by the degree of polynomials.\n\nThe projective Nullstellensatz states that, for any homogeneous ideal \"I\" that does not contain all polynomials of a certain degree (referred to as an irrelevant ideal), the common zero locus of all polynomials in \"I\" (or \"Nullstelle\") is non-trivial (i.e. the common zero locus contains more than the single element {0}), and, more precisely, the ideal of polynomials that vanish on that locus coincides with the radical of the ideal \"I\".\n\nThis last assertion is best summarized by the formula : for any relevant ideal \"I\", \n\nIn particular, maximal homogeneous relevant ideals of k[\"V\"] are one-to-one with lines through the origin of \"V\".\n\nLet \"V\" be a finite-dimensional vector space over a field k. The scheme over k defined by Proj(k[\"V\"]) is called projectivization of \"V\". The projective \"n\"-space on k is the projectivization of the vector space formula_2.\n\nThe definition of the sheaf is done on the base of open sets of principal open sets \"D\"(\"P\"), where \"P\" varies over the set of homogeneous polynomials, by setting the sections \n\nto be the ring formula_4, the zero degree component of the ring obtained by localization at \"P\". Its elements are therefore the rational functions with homogeneous numerator and some power of \"P\" as the denominator, with same degree as the numerator.\n\nThe situation is most clear at a non-vanishing linear form φ. The restriction of the structure sheaf to the open set \"D\"(φ) is then canonically identified with the affine scheme spec(k[ker φ]). Since the \"D\"(\"φ\") form an open cover of \"X\" the projective schemes can be thought of as being obtained by the gluing via projectivization of isomorphic affine schemes.\n\nIt can be noted that the ring of global sections of this scheme is a field, which implies that the scheme is not affine. Any two open sets intersect non-trivially: \"ie\" the scheme is irreducible. When the field k is algebraically closed, formula_5 is in fact an abstract variety, that furthermore is complete. \"cf. \" Glossary of scheme theory\n\nThe Proj functor in fact gives more than a mere scheme: a sheaf in graded modules over the structure sheaf is defined in the process. The homogeneous components of this graded sheaf are denoted formula_6, the Serre twisting sheaves. All of these sheaves are in fact line bundles. By the correspondence between Cartier divisors and line bundles, the first twisting sheaf formula_7 is equivalent to hyperplane divisors.\n\nSince the ring of polynomials is a unique factorization domain, any prime ideal of height 1 is principal, which shows that any Weil divisor is linearly equivalent to some power of a hyperplane divisor. This consideration proves that the Picard group of a projective space is free of rank 1. That is formula_8, and the isomorphism is given by the degree of divisors.\n\nThe invertible sheaves, or \"line bundles\", on the projective space formula_9 for \"k\" a field, are exactly the twisting sheaves formula_10 so the Picard group of formula_11 is isomorphic to formula_12. The isomorphism is given by the first Chern class.\n\nThe space of local sections on an open set formula_13 of the line bundle formula_14 is the space of homogeneous degree \"k\" regular functions on the cone in \"V\" associated to \"U\". In particular, the space of global sections \nvanishes if \"m < 0\", and consists of constants in k for \"m=0\" and of homogeneous polynomials of degree \"m\" for \"m > 0\". (Hence has dimension formula_16).\n\nThe Birkhoff-Grothendieck theorem states that on the projective line, any vector bundle splits in a unique way as a direct sum of the line bundles.\n\nThe tautological bundle, which appears for instance as the exceptional divisor of the blowing up of a smooth point is the sheaf formula_17. The canonical bundle \nThis fact derives from a fundamental geometric statement on projective spaces: the Euler sequence.\n\nThe negativity of the canonical line bundle makes projective spaces prime examples of Fano varieties, equivalently, their anticanonical line bundle is ample (in fact very ample). Their index (\"cf.\" Fano varieties) is given by formula_20, and, by a theorem of Kobayashi-Ochiai, projective spaces are \"characterized\" amongst Fano varieties by the property \n\nAs affine spaces can be embedded in projective spaces, all affine varieties can be embedded in projective spaces too.\n\nAny choice of a finite system of nonsimultaneously vanishing global sections of a globally generated line bundle defines a morphism to a projective space. A line bundle whose base can be embedded in a projective space by such a morphism is called very ample.\n\nThe group of symmetries of the projective space formula_22 is the group of projectivized linear automorphisms formula_23. The choice of a morphism to a projective space formula_24 \"modulo\" the action of this group is in fact \"equivalent\" to the choice of a globally generating \"n\"-dimensional linear system of divisors on a line bundle on \"X\". The choice of a projective embedding of \"X\", \"modulo\" projective transformations is likewise equivalent to the choice of a very ample line bundle on \"X\".\n\nA morphism to a projective space formula_24 defines a globally generated line bundle by formula_26 and a linear system \n\nIf the range of the morphism formula_28 is not contained in a hyperplane divisor, then the pull-back is an injection and the linear system of divisors \n\nThe Veronese embeddings are embeddings formula_30 for formula_31\n\nSee the answer on MathOverflow for an application of the Veronese embedding to the calculation of cohomology groups of smooth projective hypersurfaces (smooth divisors).\n\nAs Fano varieties, the projective spaces are ruled varieties. The intersection theory of curves in the projective plane yields the Bézout theorem.\n\n\n\n"}
{"id": "7870034", "url": "https://en.wikipedia.org/wiki?curid=7870034", "title": "Algebraic logic", "text": "Algebraic logic\n\nIn mathematical logic, algebraic logic is the reasoning obtained by manipulating equations with free variables.\n\nWhat is now usually called classical algebraic logic focuses on the identification and algebraic description of models appropriate for the study of various logics (in the form of classes of algebras that constitute the algebraic semantics for these deductive systems) and connected problems like representation and duality. Well known results like the representation theorem for Boolean algebras and Stone duality fall under the umbrella of classical algebraic logic .\n\nWorks in the more recent abstract algebraic logic (AAL) focus on the process of algebraization itself, like classifying various forms of algebraizability using the Leibniz operator .\n\nA homogeneous binary relation is found in the power set of \"X\" × \"X\" for some set \"X\", while a heterogeneous relation is found in the power set of \"X\" × \"Y\", where \"X\" ≠ \"Y\". Whether a given relation holds for two individuals is one bit of information, so relations are studied with Boolean arithmetic. Elements of the power set are partially ordered by inclusion, and lattice of these sets becomes an algebra through \"relative multiplication\" or composition of relations.\n\n\"The basic operations are set-theoretic union, intersection and complementation, the relative multiplication, and conversion.\"\n\nThe \"conversion\" refers to the converse relation that always exists, contrary to function theory. A given relation may be represented by a logical matrix; then the converse relation is represented by the transpose matrix. A relation obtained as the composition of two others is then represented by the logical matrix obtained by matrix multiplication using Boolean arithmetic.\n\nFor instance, a function is a relation \"R\" with range \"B\" such that \"R\" \"R\" = I, the identity relation on \"B\".\n\nThis relation algebra structure based in set theory was transcended by Tarski with axioms describing it. Then he asked if every algebra satisfying the axioms could be represented by a set relation. The negative answer opened the frontier of abstract algebraic logic.\n\nAlgebraic logic treats algebraic structures, often bounded lattices, as models (interpretations) of certain logics, making logic a branch of the order theory.\n\nIn algebraic logic:\n\nIn the table below, the left column contains one or more logical or mathematical systems, and the algebraic structure which are its models are shown on the right in the same row. Some of these structures are either Boolean algebras or proper extensions thereof. Modal and other nonclassical logics are typically modeled by what are called \"Boolean algebras with operators.\"\n\nAlgebraic formalisms going beyond first-order logic in at least some respects include:\n\nAlgebraic logic is, perhaps, the oldest approach to formal logic, arguably beginning with a number of memoranda Leibniz wrote in the 1680s, some of which were published in the 19th century and translated into English by Clarence Lewis in 1918. But nearly all of Leibniz's known work on algebraic logic was published only in 1903 after Louis Couturat discovered it in Leibniz's Nachlass. and translated selections from Couturat's volume into English.\n\nModern mathematical logic began in 1847, with two pamphlets whose respective authors were George Boole and Augustus De Morgan. In 1870 Charles Sanders Peirce published the first of several works on the logic of relatives, and in 1883, Christine Ladd, a student of Peirce at Johns Hopkins University, published \"On the Algebra of Logic\". Logic turned more algebraic when binary relations were combined with composition of relations. For sets \"A\" and \"B\", relations were first understood as elements of the power set of \"A\"×\"B\" with properties described by Boolean algebra. The \"calculus of relations\" is arguably the culmination of Leibniz's approach to logic. At the Hochschule Karlsruhe the calculus of relations was described by Ernst Schröder. In particular he formulated Schröder rules, though De Morgan had anticipated them with his Theorem K. \n\nThe \"Boole-Schröder algebra of logic\" was developed at University of California, Berkeley in a textbook by Clarence Lewis in 1918. He treated the logic of relations as derived from the propositional functions of two or more variables.\n\nHugh MacColl, Frege, Peano, Bertrand Russell, and A. N. Whitehead all shared Leibniz's dream of combining symbolic logic, mathematics, and philosophy. \n\nSome writings by Leopold Loewenheim and Thoralf Skolem on algebraic logic appeared after the 1910–13 publication of \"Principia Mathematica\", and Tarski revived interest in relations with his 1941 essay \"On the Calculus of Relations\".\n\nAccording to Helena Rasiowa, \"The years 1920-40 saw, in particular in the Polish school of logic, researches on non-classical propositional calculi conducted by what is termed the logical matrix method. Since logical matrices are certain abstract algebras, this led to the use of an algebraic method in logic.\"\n\n\nLeibniz had no influence on the rise of algebraic logic because his logical writings were little studied before the Parkinson and Loemker translations. Our present understanding of Leibniz as a logician stems mainly from the work of Wolfgang Lenzen, summarized in . To see how present-day work in logic and metaphysics can draw inspiration from, and shed light on, Leibniz's thought, see .\n\n\n\n\nHistorical perspective\n"}
{"id": "34471935", "url": "https://en.wikipedia.org/wiki?curid=34471935", "title": "Arithmetic zeta function", "text": "Arithmetic zeta function\n\nIn mathematics, the arithmetic zeta function is a zeta function associated with a scheme of finite type over integers. The arithmetic zeta function generalizes the Riemann zeta function and Dedekind zeta function to higher dimensions. The arithmetic zeta function is one of the most-fundamental objects of number theory.\n\nThe arithmetic zeta function is defined by an Euler product analogous to the Riemann zeta function:\n\nwhere the product is taken over all closed points of the scheme . Equivalently, the product is over all points whose residue field is finite. The cardinality of this field is denoted .\n\nIf is the spectrum of a finite field with elements, then \n\nFor a variety \"X\" over a finite field, it is known by Grothendieck's trace formula that\nwhere formula_4 is a rational function (i.e., a quotient of polynomials).\n\nGiven two varieties \"X\" and \"Y\" over a finite field, the zeta function of formula_5 is given by\nwhere formula_7 denotes the multiplication in the ring formula_8 of Witt vectors of the integers.\n\nIf is the spectrum of the ring of integers, then is the Riemann zeta function. More generally, if is the spectrum of the ring of integers of an algebraic number field, then is the Dedekind zeta function.\n\nThe zeta function of affine and projective spaces over a scheme are given by\n\nThe latter equation can be deduced from the former using that, for any that is the disjoint union of a closed and open subscheme and , respectively,\n\nEven more generally, a similar formula holds for infinite disjoint unions. In particular, this shows that the zeta function of is the product of the ones of the reduction of modulo the primes :\n\nSuch an expression ranging over each prime number is sometimes called Euler product and each factor is called Euler factor. In many cases of interest, the generic fiber is smooth. Then, only finitely many are singular (bad reduction). For almost all primes, namely when has good reduction, the Euler factor is known to agree with the corresponding factor of the Hasse-Weil zeta function of . Therefore, these two functions are closely related.\n\nThere are a number of conjectures concerning the behavior of the zeta function of a regular irreducible equidimensional scheme (of finite type over the integers). Many (but not all) of these conjectures generalize the one-dimensional case of well known theorems about the Euler-Riemann-Dedekind zeta function.\n\nThe scheme need not be flat over , in this case it is a scheme of finite type over some . This is referred to as the characteristic case below. In the latter case, many of these conjectures (with the most notable exception of the Birch and Swinnerton-Dyer conjecture, i.e. the study of special values) are known. Very little is known for schemes that are flat over and are of dimension two and higher.\n\nHasse and Weil conjectured that has a meromorphic continuation to the complex plane and satisfies a functional equation with respect to where is the absolute dimension of . \n\nThis is proven for and some very special cases when for flat schemes over and for all in positive characteristic. It is a consequence of the Weil conjectures (more precisely, the Riemann hypothesis part thereof) that the zeta function has a meromorphic continuation up to formula_12.\n\nAccording to the generalized Riemann Hypothesis the zeros of are conjectured to lie inside the critical strip lie on the vertical lines and the poles of inside the critical strip lie on the vertical lines . \n\nThis was proved (Emil Artin, Helmut Hasse, André Weil, Alexander Grothendieck, Pierre Deligne) in positive characteristic for all . It is not proved for any scheme that is flat over . The Riemann hypothesis is a partial case of Conjecture 2.\n\nSubject to the analytic continuation, the order of the zero or pole and the residue of at integer points inside the critical strip is conjectured to be expressible by important arithmetic invariants of . An argument due to Serre based on the above elementary properties and Noether normalization shows that the zeta function of has a pole at whose order equals the number of irreducible components of with maximal dimension. Secondly, Tate conjectured\n\ni.e., the pole order is expressible by the rank of the groups of invertible regular functions and the Picard group. The Birch and Swinnerton-Dyer conjecture is a partial case this conjecture. In fact, this conjecture of Tate's is equivalent to a generalization of Birch and Swinnerton-Dyer.\n\nMore generally, Soulé conjectured\n\nThe right hand side denotes the Adams eigenspaces of algebraic -theory of . These ranks are finite under the Bass conjecture.\n\nThese conjectures are known when , that is, the case of number rings and curves over finite fields. As for , partial cases of the Birch and Swinnerton-Dyer conjecture have been proven, but even in positive characteristic the conjecture remains open.\n\nThe arithmetic zeta function of a regular connected equidimensional arithmetic scheme of Kronecker dimension can be factorized into the product of appropriately defined -factors and an auxiliary factor. Hence, results on -functions imply corresponding results for the arithmetic zeta functions. However, there is still very little amount of proven results about the -factors of arithmetic schemes in characteristic zero and dimensions 2 and higher. Ivan Fesenko initiated a theory which studies the arithmetic zeta functions directly, without working with their -factors. It is a higher-dimensional generalisation of Tate's thesis, i.e. it uses higher adele groups, higher zeta integral and objects which come from higher class field theory. In this theory, the meromorphic continuation and functional equation of proper regular models of elliptic curves over global fields is related to mean-periodicity property of a boundary function. In his joint work with M. Suzuki and G. Ricotta a new correspondence in number theory is proposed, between the arithmetic zeta functions and mean-periodic functions in the space of smooth functions on the real line of not more than exponential growth. This correspondence is related to the Langlands correspondence. Two other applications of Fesenko's theory are to the poles of the zeta function of proper models of elliptic curves over global fields and to the special value at the central point.\n\nSources\n"}
{"id": "18017949", "url": "https://en.wikipedia.org/wiki?curid=18017949", "title": "Atiyah–Segal completion theorem", "text": "Atiyah–Segal completion theorem\n\nThe Atiyah–Segal completion theorem is a theorem in mathematics about equivariant K-theory in homotopy theory. Let \"G\" be a compact Lie group and let \"X\" be a \"G\"-CW-complex. The theorem then states that the projection map\n\ninduces an isomorphism of prorings\n\nHere, the induced map has as domain the completion of the \"G\"-equivariant K-theory of \"X\" with respect to \"I\", where \"I\" denotes the augmentation ideal of the representation ring of \"G\".\n\nIn the special case of \"X\" a point, the theorem specializes to give an isomorphism formula_3 between the K-theory of the classifying space of \"G\" and the completion of the representation ring.\n\nThe theorem can be interpreted as giving a comparison between the geometrical process of completing a \"G\"-space by making the action free and the algebraic process of completing with respect to an ideal.\nThe theorem was first proved for finite groups by Michael Atiyah in 1961,\n\nand a proof of the general case was published by Atiyah together with Graeme Segal in 1969.\n\nDifferent proofs have since appeared generalizing the theorem to completion with respect to families of subgroups.\nThe corresponding statement for algebraic K-theory was proven by Merkujev, holding in the case that the group is algebraic over the complex numbers.\n\n"}
{"id": "27594026", "url": "https://en.wikipedia.org/wiki?curid=27594026", "title": "Banach bundle (non-commutative geometry)", "text": "Banach bundle (non-commutative geometry)\n\nIn mathematics, a Banach bundle is a fiber bundle over a topological Hausdorff space, such that each fiber has the structure of a Banach space.\n\nLet formula_1 be a topological Hausdorff space, a (continuous) Banach bundle over formula_2 is a tuple formula_3, where formula_4 is a topological Hausdorff space, and formula_5 is a continuous, open surjection, such that each fiber formula_6 is a Banach space. Which satisfies the following conditions:\nIf the map formula_20 is only upper semi-continuous, formula_21 is called upper semi-continuous bundle.\n\nLet \"A\" be a Banach space, \"X\" be a topological Hausdorff space. Define formula_22 and formula_5 by formula_24. Then formula_25 is a Banach bundle, called the trivial bundle\n\n"}
{"id": "35573062", "url": "https://en.wikipedia.org/wiki?curid=35573062", "title": "Blahut–Arimoto algorithm", "text": "Blahut–Arimoto algorithm\n\nThe Blahut–Arimoto algorithm, is often used to refer to a class of algorithms for computing numerically either the information theoretic capacity of a channel, or the rate-distortion function of a source. They are iterative algorithms that eventually converge to the optimal solution of the convex optimization problem that is associated with these information theoretic concepts.\n\nFor the case of channel capacity, the algorithm was independently invented by Suguru Arimoto and Richard Blahut. In the case of lossy compression, the corresponding algorithm was invented by Richard Blahut. The algorithm is most applicable to the case of arbitrary finite alphabet sources. Much work has been done to extend it to more general problem instances.\n\nSuppose we have a source formula_1 with probability formula_2 of any given symbol. We wish to find an encoding formula_3 that generates a compressed signal formula_4 from the original signal while minimizing the expected distortion formula_5, where the expectation is taken over the joint probability of formula_1 and formula_4. We can find an encoding that minimizes the rate-distortion functional locally by repeating the following iteration until convergence:\n\nwhere formula_10 is a parameter related to the slope in the rate-distortion curve that we are targeting and thus is related to how much we favor compression versus distortion (higher formula_10 means less compression).\n"}
{"id": "56442223", "url": "https://en.wikipedia.org/wiki?curid=56442223", "title": "Blocknots", "text": "Blocknots\n\nBlocknots were random sequences of numbers contained in a book and organized by numbered rows and columns and were used as additives in the recyphering of Soviet Union codes, during World War II. The Blocknot consisted of fifty sheets of 5-figure random additive, 100 additive groups to a sheet. No sheet was used more than once, thus the blocknots were in effect a form of One-time pad. The Soviet Unions highest grade ciphers that were used in the East, were the 5-figure codebook enciphered with the Blocknot book, and were generally considered unbreakable.\n\nBlocknots were distributed centrally from an office in Moscow. Every Blocknot contained 5-figure groups in a number of sheets, for the enciphering of 5-figure messages. The encipherment was effected by applying additives taken from the pad, of which 50-100 5-figure groups appeared. Each pad had a 5-figure number and each sheet had a 2-figure number running consecutively. There were 5 different types of Blocknots, in two different categories\n\n\n\nTwo other types were used, in lower echelons.\n\nThe distribution of Blocknots was carried out centrally from Moscow to Army Groups then to Armies. The Army was responsible for their distribution throughout the lower levels of the army down to company level. Independent units took their cipher material with them. Occasionally the same blocknot was distributed to two units on different parts of the front, which enabled Depth to be established. Records of all Blocknots used were kept in Berlin and when a repeat was noticed a \"BLOCKNOT ANGEBOT\" message was sent out to all German Signals units, to indicate that it may have been possible to break the code using it. There was no certainty in this.\n\nCorporal Althans, a cryptanalyst with the General der Nachrichtenaufklärung stated while being interrogated by TICOM:\n\nGerman cryptanalysts who were prisoners of war stated under interrogation, that each of the figures 0 to 9 were placed en clair usually within the first ten groups of the text or sometimes at the end. One indicator was the Blocknot number and the consisted of two random figures, the figure representing the type, and the remaining two, the page of the Blocknot being used.\n\nIn long messages, \"000000\" was placed in the message when the end of a page had been reached.\n\nThe Chi-number was the serial numbering of all 5-figure messages passing through the hands of the Cipher Officer, starting on the first of January and ending on thirty-first December of the current year. It always appeared as the last group in an intercepted message, e.g. 00001 on the 1st January, or when the unit was newly set up. The progression of Chi-numbers was carefully observed and recorded in the form of a graph. A Russian corps had about 10 5-figure messages per day, and Army about 20-30 and a Front about 60-100. After only a relatively short time, the individual curves separated sharply and the type of formation could be recognized by the height of the Chi-number alone.\n\nBlocknots were tracked in a card index, that was maintained by the Signal Intelligence Evaluation Centre (NAAS). The NAAS functionality included evaluation and traffic analysis, cryptanalysis, collation and dissemination of intelligence. The card index, which was one amongst several Card Indexes. A careful recording and study of blocks provided the positive clues in the identification and tracking of formations using 5-figure ciphers. The index was subdivided into two files: \n\n\nInspector Berger, who was the chief cryptanalyst of NAAS 1 stated that the two files formed:\n\nThe Blocknots were also used in the Stationary Intercept Company (Feste), the military unit that were designed to work at a lower level to the NAAS, at the Army level and were semi-motorized, and closer to the front. The Feste used the Blocknot value along with several other parameters to build a network diagram. The network diagram was studied extensively, as part of a 6-stage process, that involved several departments within the Feste. The final outcome was a metric which dermined the most interesting circuit for traffic monitoring, and least interesting, were monitoring of traffic should cease.\n\nJohannes Marquart was a mathematician and cryptanalyst who initially worked for Inspectorate 7/VI and later led Referat Ia of Group IV of the General der Nachrichtenaufklärung. Marquart was assigned the study of the Soviet Union Blocknot traffic. Marquart and his unit conducted extensive research in an attempt to discover the method by which they were produced. All the counts which they made, however, failed to reveal any non-random characteristics in the design of the tables, and while they thought the Blocknots must have been generated by machine, they were never able to draw any concrete deductions as a result of their research.\n\nThe Soviet 3rd Guard Tank Army transmits a 5-figure message with the Blocknot of \"37581\" (one of the first 10 groups in the message). On the same day the Block \"37582\" was used by the same formation. The next day \"37583\" appeared. Thereafter, for a period, the Army was not heard by German Wireless telegraphy intercept operators, as it was maintaining wireless silence. After a few days, an unidentified net with the Blocknot \"37588\" is picked up. This message net is claimed, because of the proximity of the blocks (88/83) to be the 3rd Guard Tank Army. The missing Blocknots 84-87 were presumably used in telegraphic, telephonic or courier communications. The Chi number provides confirmation of the first assumption, based on proximity of blocknots in most cases.\n"}
{"id": "1286813", "url": "https://en.wikipedia.org/wiki?curid=1286813", "title": "Boris Delaunay", "text": "Boris Delaunay\n\nBoris Nikolaevich Delaunay or Delone (; March 15, 1890 – July 17, 1980) was one of the first Russian mountain climbers and a Soviet/Russian mathematician, and the father of physicist Nikolai Borisovich Delone.\n\nThe spelling \"Delone\" is a straightforward transliteration from Cyrillic he often used in later publications, while \"Delaunay\" is the French version he used in the early French and German publications. \n\nBoris Delone got his surname from his ancestor French Army officer De Launay, who was captured in Russia during Napoleon's invasion of 1812. De Launay was a nephew of the Bastille governor marquis de Launay. He married a woman from the Tukhachevsky noble family and stayed in Russia. \n\nWhen Boris was a young boy his family spent summers in the Alps where he learned mountain climbing. By 1913, he became one of the top three Russian mountain climbers. After the Russian revolution, he climbed mountains in the Caucasus and Altai. One of the mountains (4300 m) near Belukha is named after him. In the 1930s, he was among the first to receive a qualification of Master of mountain climbing of the USSR. Future Nobel laureate in physics Igor Tamm was his associate in setting tourist camps in the mountains.\n\nBoris Delaunay worked in the fields of modern algebra, the geometry of numbers. He used the results of Evgraf Fedorov, Hermann Minkowski, Georgy Voronoy, and others in his development of modern mathematical crystallography and general mathematical model of crystals. \nHe invented what is now called Delaunay triangulation in 1934; Delone sets are also named after him. Among his best students are the mathematicians Aleksandr Aleksandrov and Igor Shafarevich. \n\nDelaunay was elected the corresponding member of the USSR Academy of Sciences in 1929.\nDelaunay is credited as being an organizer, in Leningrad in 1934, of the first mathematical olympiad for high school students in the Soviet Union.\n\n\n"}
{"id": "6319245", "url": "https://en.wikipedia.org/wiki?curid=6319245", "title": "Camera resectioning", "text": "Camera resectioning\n\nCamera resectioning is the process of estimating the parameters of a pinhole camera model approximating the camera that produced a given photograph or video. Usually, the pinhole camera parameters are represented in a 3 × 4 matrix called the camera matrix.\n\nThis process is often called camera calibration, although that term can also refer to photometric camera calibration.\n\nOften, we use formula_1 to represent a 2D point position in pixel coordinates. Here formula_2 is used to represent a 3D point position in World coordinates. Note: they were expressed in augmented notation of homogeneous coordinates which is the most common notation in robotics and rigid body transforms.\nReferring to the pinhole camera model, a camera matrix is used to denote a projective mapping from World coordinates to Pixel coordinates.\n\nThe intrinsic matrix formula_5 contains 5 intrinsic parameters. These parameters encompass focal length, image sensor format, and principal point. \nThe parameters formula_6 and formula_7 represent focal length in terms of pixels, where formula_8 and formula_9 are the scale factors relating pixels to distance and formula_10 is the focal length in terms of distance.\n\nformula_11 represents the skew coefficient between the x and the y axis, and is often 0.\nformula_12 and formula_13 represent the principal point, which would be ideally in the centre of the image.\n\nNonlinear intrinsic parameters such as lens distortion are also important although they cannot be included in the linear camera model described by the intrinsic parameter matrix. Many modern camera calibration algorithms estimate these intrinsic parameters as well in the form of non-linear optimisation techniques. This is done in the form of optimising the camera and distortion parameters in the form of what is generally known as bundle adjustment.\n\nformula_14 are the extrinsic parameters which denote the coordinate system transformations from 3D world coordinates to 3D camera coordinates. Equivalently, the extrinsic parameters define the position of the camera center and the camera's heading in world coordinates. formula_15 is the position of the origin of the world coordinate system expressed in coordinates of the camera-centered coordinate system. formula_15 is often mistakenly considered the position of the camera. The position, formula_17, of the camera expressed in world coordinates is formula_18 (since formula_19 is a rotation matrix).\n\nCamera calibration is often used as an early stage in computer vision.\n\nWhen a camera is used, light from the environment is focused on an image plane and captured. This process reduces the dimensions of the data taken in by the camera from three to two (light from a 3D scene is stored on a 2D image). Each pixel on the image plane therefore corresponds to a shaft of light from the original scene. Camera resectioning determines which incoming light is associated with each pixel on the resulting image. In an ideal pinhole camera, a simple projection matrix is enough to do this. With more complex camera systems, errors resulting from misaligned lenses and deformations in their structures can result in more complex distortions in the final image.\nThe camera projection matrix is derived from the intrinsic and extrinsic parameters of the camera, and is often represented by the series of transformations; e.g., a matrix of camera intrinsic parameters, a 3 × 3 rotation matrix, and a translation vector. The camera projection matrix can be used to associate points in a camera's image space with locations in 3D world space.\n\nCamera resectioning is often used in the application of stereo vision where the camera projection matrices of two cameras are used to calculate the 3D world coordinates of a point viewed by both cameras.\n\nSome people call this camera calibration, but many restrict the term camera calibration for the estimation of internal or intrinsic parameters only.\n\nThere are many different approaches to calculate the intrinsic and extrinsic parameters for a specific camera setup. The most common ones are: \n\nZhang model is a camera calibration method that uses traditional calibration techniques (known calibration points) and self-calibration techniques (correspondence between the calibration points when they are in different positions). To perform a full calibration by the Zhang method at least three different images of the calibration target/gauge are required, either by moving the gauge or the camera itself. If some of the intrinsic parameters are given as data (orthogonality of the image or optical center coordinates) the number of images required can be reduced to two.\nIn a first step, an approximation of the estimated projection matrix formula_20 between the calibration target and the image plane is determined using DLT method. Subsequently, applying self-calibration techniques to obtained the image of the absolute conic matrix [Link]. The main contribution of Zhang method is how to extract a constrained instrinsic formula_5 and formula_22 numbers of formula_19 and formula_15 calibration parameters from formula_22 pose of the calibration target.\n\nAssume we have a homography formula_26 that maps points formula_27 on a \"probe plane\" formula_28 to points formula_29 on the image.\n\nThe circular points formula_30 lie on both our probe plane formula_28 and on the absolute conic formula_32. Lying on formula_32 of course means they are also projected onto the \"image\" of the absolute conic (IAC) formula_34, thus formula_35 and formula_36. The circular points project as\n\nWe can actually ignore formula_38 while substituting our new expression for formula_39 as follows:\n\nIt is a 2-stage algorithm, calculating the pose (3D Orientation, and x-axis and y-axis translation) in first stage. In second stage it computes the focal length, distortion coefficients and the z-axis translation.\n\nSelby's camera calibration method addresses the auto-calibration of X-ray camera systems.\nX-ray camera systems, consisting of the X-ray generating tube and a solid state detector can be modelled as pinhole camera systems, comprising 9 intrinsic and extrinsic camera parameters.\nIntensity based registration based on an arbitrary X-ray image and a reference model (as a tomographic dataset) can then be used to determine the relative camera parameters without the need of a special calibration body or any ground-truth data.\n\n\n"}
{"id": "1839944", "url": "https://en.wikipedia.org/wiki?curid=1839944", "title": "Complete Boolean algebra", "text": "Complete Boolean algebra\n\nIn mathematics, a complete Boolean algebra is a Boolean algebra in which every subset has a supremum (least upper bound). Complete Boolean algebras are used to construct Boolean-valued models of set theory in the theory of forcing. Every Boolean algebra \"A\" has an essentially unique completion, which is a complete Boolean algebra containing \"A\" such that every element is the supremum of some subset of \"A\". As a partially ordered set, this completion of \"A\" is the Dedekind–MacNeille completion.\n\nMore generally, if κ is a cardinal then a Boolean algebra is called κ-complete if every subset of cardinality less than κ has a supremum.\n\n\n\nif \"A\" is a subalgebra of a Boolean algebra \"B\", then any homomorphism from \"A\" to a complete Boolean algebra \"C\" can be extended to a morphism from \"B\" to \"C\".\n\n\nThe completion of a Boolean algebra can be defined in several equivalent ways:\n\nThe completion of a Boolean algebra \"A\" can be constructed in several ways:\n\nIf \"A\" is a metric space and \"B\" its completion then any isometry from \"A\" to a complete metric space \"C\" can be extended to a unique isometry from \"B\" to \"C\". The analogous statement for complete Boolean algebras is not true: a homomorphism from a Boolean algebra \"A\" to a complete Boolean algebra \"C\" cannot necessarily be extended to a (supremum preserving) homomorphism of complete Boolean algebras from the completion \"B\" of \"A\" to \"C\". (By Sikorski's extension theorem it can be extended to a homomorphism of Boolean algebras from \"B\" to \"C\", but this will not in general be a homomorphism of complete Boolean algebras; in other words, it need not preserve suprema.)\n\nUnless the Axiom of Choice is relaxed, free complete boolean algebras generated by a set do not exist (unless the set is finite). More precisely, for any cardinal κ, there is a complete Boolean algebra of cardinality 2 greater than κ that is generated as a complete Boolean algebra by a countable subset; for example the Boolean algebra of regular open sets in the product space κ, where κ has the discrete topology. A countable generating set consists of all sets \"a\" for \"m\", \"n\" integers, consisting of the elements \"x\"∈κ such that \"x\"(\"m\")<\"x\"(\"n\"). (This boolean algebra is called a collapsing algebra, because forcing with it collapses the cardinal κ onto ω.)\n\nIn particular the forgetful functor from complete Boolean algebras to sets has no left adjoint, even though it is continuous and the category of Boolean algebras is small-complete. This shows that the \"solution set condition\" in Freyd's adjoint functor theorem is necessary.\n\nGiven a set \"X\", one can form the free Boolean algebra \"A\" generated by this set and then take its completion \"B\". However \"B\" is not a \"free\" complete Boolean algebra generated by \"X\" (unless \"X\" is finite or AC is omitted), because a function from \"X\" to a free Boolean algebra \"C\" cannot in general be extended to a (supremum-preserving) morphism of Boolean algebras from \"B\" to \"C\".\n\nOn the other hand, for any fixed cardinal κ, there is a free (or universal) κ-complete Boolean algebra generated by any given set.\n\n\n"}
{"id": "1081538", "url": "https://en.wikipedia.org/wiki?curid=1081538", "title": "Complex polygon", "text": "Complex polygon\n\nThe term complex polygon can mean two different things:\n\n\nIn geometry, a complex polygon is a polygon in the complex Hilbert plane, which has two complex dimensions.\n\nA complex number may be represented in the form formula_1, where formula_2 and formula_3 are real numbers, and formula_4 is the square root of formula_5. Multiples of formula_4 such as formula_7 are called \"imaginary numbers\". A complex number lies in a complex plane having one real and one imaginary dimension, which may be represented as an Argand diagram. So a single complex dimension comprises two spatial dimensions, but of different kinds - one real and the other imaginary.\n\nThe unitary plane comprises two such complex planes, which are orthogonal to each other. Thus it has two real dimensions and two imaginary dimensions.\n\nA complex polygon is a (complex) two-dimensional (i.e. four spatial dimensions) analogue of a real polygon. As such it is an example of the more general complex polytope in any number of complex dimensions.\n\nIn a \"real\" plane, a visible figure can be constructed as the \"real conjugate\" of some complex polygon.\n\nIn computer graphics, a complex polygon is a polygon which has a boundary comprising discrete circuits, such as a polygon with a hole in it.\n\nSelf-intersecting polygons are also sometimes included among the complex polygons. Vertices are only counted at the ends of edges, not where edges intersect in space.\n\nA formula relating an integral over a bounded region to a closed line integral may still apply when the \"inside-out\" parts of the region are counted negatively.\n\nMoving around the polygon, the total amount one \"turns\" at the vertices can be any integer times 360°, e.g. 720° for a pentagram and 0° for an angular \"eight\".\n\n\n\n"}
{"id": "8581934", "url": "https://en.wikipedia.org/wiki?curid=8581934", "title": "Complex polytope", "text": "Complex polytope\n\nIn geometry, a complex polytope is a generalization of a polytope in real space to an analogous structure in a complex Hilbert space, where each real dimension is accompanied by an imaginary one.\n\nA complex polytope may be understood as a collection of complex points, lines, planes, and so on, where every point is the junction of multiple lines, every line of multiple planes, and so on.\n\nPrecise definitions exist only for the regular complex polytopes, which are configurations. The regular complex polytopes have been completely characterized, and can be described using a symbolic notation developed by Coxeter.\n\nSome complex polytopes which are not fully regular have also been described.\n\nThe complex line formula_1 has one dimension with real coordinates and another with imaginary coordinates. Applying real coordinates to both dimensions is said to give it two dimensions over the real numbers. A real plane, with the imaginary axis labelled as such, is called an Argand diagram. Because of this it is sometimes called the complex plane. Complex 2-space (also sometimes called the complex plane) is thus a four-dimensional space over the reals, and so on in higher dimensions.\n\nA complex \"n\"-polytope in complex \"n\"-space is the analogue of a real \"n\"-polytope in real \"n\"-space.\n\nThere is no natural complex analogue of the ordering of points on a real line (or of the associated combinatorial properties). Because of this a complex polytope cannot be seen as a contiguous surface and it does not bound an interior in the way that a real polytope does.\n\nIn the case of \"regular\" polytopes, a precise definition can be made by using the notion of symmetry. For any regular polytope the symmetry group (here a complex reflection group, called a Shephard group) acts transitively on the flags, that is, on the nested sequences of a point contained in a line contained in a plane and so on.\n\nMore fully, say that a collection \"P\" of affine subspaces (or \"flats\") of a complex unitary space \"V\" of dimension \"n\" is a regular complex polytope if it meets the following conditions:\n\n(Here, a flat of dimension −1 is taken to mean the empty set.) Thus, by definition, regular complex polytopes are configurations in complex unitary space.\n\nThe regular complex polytopes were discovered by Shephard (1952), and the theory was further developed by Coxeter (1974).\n\nA complex polytope exists in the complex space of equivalent dimension. For example, the vertices of a complex polygon are points in the complex plane formula_2, and the edges are complex lines formula_1 existing as (affine) subspaces of the plane and intersecting at the vertices. Thus, an edge can be given a coordinate system consisting of a single complex number.\n\nIn a regular complex polytope the vertices incident on the edge are arranged symmetrically about their centroid, which is often used as the origin of the edge's coordinate system (in the real case the centroid is just the midpoint of the edge). The symmetry arises from a complex reflection about the centroid; this reflection will leave the magnitude of any vertex unchanged, but change its argument by a fixed amount, moving it to the coordinates of the next vertex in order. So we may assume (after a suitable choice of scale) that the vertices on the edge satisfy the equation formula_4 where \"p\" is the number of incident vertices. Thus, in the Argand diagram of the edge, the vertex points lie at the vertices of a regular polygon centered on the origin.\n\nThree real projections of regular complex polygon 4{4}2 are illustrated above, with edges \"a, b, c, d, e, f, g, h\". It has 16 vertices, which for clarity have not been individually marked. Each edge has four vertices and each vertex lies on two edges, hence each edge meets four other edges. In the first diagram, each edge is represented by a square. The sides of the square are \"not\" parts of the polygon but are drawn purely to help visually relate the four vertices. The edges are laid out symmetrically. (Note that the diagram looks similar to the B Coxeter plane projection of the tesseract, but it is structurally different).\n\nThe middle diagram abandons octagonal symmetry in favour of clarity. Each edge is shown as a real line, and each meeting point of two lines is a vertex. The connectivity between the various edges is clear to see.\n\nThe last diagram gives a flavour of the structure projected into three dimensions: the two cubes of vertices are in fact the same size but are seen in perspective at different distances away in the fourth dimension.\n\nA real 1-dimensional polytope exists as a closed segment in the real line formula_5, defined by its two end points or vertices in the line. Its Schläfli symbol is {} .\n\nAnalogously, a complex 1-polytope exists as a set of \"p\" vertex points in the complex line formula_1. These may be represented as a set of points in an Argand diagram (\"x\",\"y\")=\"x\"+\"iy\". A regular complex 1-dimensional polytope {} has \"p\" (\"p\" ≥ 2) vertex points arranged to form a convex regular polygon {\"p\"} in the Argand plane.\n\nUnlike points on the real line, points on the complex line have no natural ordering. Thus, unlike real polytopes, no interior can be defined. Despite this, complex 1-polytopes are often drawn, as here, as a bounded regular polygon in the Argand plane.\nA regular real 1-dimensional polytope is represented by an empty Schläfli symbol {}, or Coxeter-Dynkin diagram . The dot or node of the Coxeter-Dynkin diagram itself represents a reflection generator while the circle around the node means the generator point is not on the reflection, so its reflective image is a distinct point from itself. By extension, a regular complex 1-dimensional polytope in formula_1 has Coxeter-Dynkin diagram , for any positive integer \"p\", 2 or greater, containing \"p\" vertices. \"p\" can be suppressed if it is 2. It can also be represented by an empty Schläfli symbol {}, }{, {}, or {2}. The 1 is a notational placeholder, representing a nonexistent reflection, or a period 1 identity generator. (A 0-polytope, real or complex is a point, and is represented as } {, or {2}.)\n\nThe symmetry is denoted by the Coxeter diagram , and can alternatively be described in Coxeter notation as [], [] or ][, [2] or [1]. The symmetry is isomorphic to the cyclic group, order \"p\". The subgroups of [] are any whole divisor \"d\", [], where \"d\"≥2.\n\nA unitary operator generator for is seen as a rotation by 2π/\"p\" radians counter clockwise, and a edge is created by sequential applications of a single unitary reflection. A unitary reflection generator for a 1-polytope with \"p\" vertices is . When \"p\" = 2, the generator is \"e\" = –1, the same as a point reflection in the real plane.\n\nIn higher complex polytopes, 1-polytopes form \"p\"-edges. A 2-edge is similar to an ordinary real edge, in that it contains two vertices, but need not exist on a real line.\n\nWhile 1-polytopes can have unlimited \"p\", finite regular complex polygons, excluding the double prism polygons {4}, are limited to 5-edge (pentagonal edges) elements, and infinite regular aperiogons also include 6-edge (hexagonal edges) elements.\n\nShephard originally devised a modified form of Schläfli's notation for regular polytopes. For a polygon bounded by \"p\"-edges, with a \"p\"-set as vertex figure and overall symmetry group of order \"g\", we denote the polygon as \"p\"(\"g\")\"p\".\n\nThe number of vertices \"V\" is then \"g\"/\"p\" and the number of edges \"E\" is \"g\"/\"p\".\n\nThe complex polygon illustrated above has eight square edges (\"p\"=4) and sixteen vertices (\"p\"=2). From this we can work out that \"g\" = 32, giving the modified Schläfli symbol 4(32)2.\n\nA more modern notation {\"q\"} is due to Coxeter, and is based on group theory. As a symmetry group, its symbol is [\"q\"].\n\nThe symmetry group [\"q\"] is represented by 2 generators R, R, where: R = R = I. If \"q\" is even, (RR) = (RR). If \"q\" is odd, (RR)R = (RR)R. When \"q\" is odd, \"p\"=\"p\".\n\nFor [4] has R = R = I, (RR) = (RR).\n\nFor [5] has R = R = I, (RR)R = (RR)R.\n\nCoxeter also generalised the use of Coxeter-Dynkin diagrams to complex polytopes, for example the complex polygon {\"q\"} is represented by and the equivalent symmetry group, [\"q\"], is a ringless diagram . The nodes \"p\" and \"r\" represent mirrors producing \"p\" and \"r\" images in the plane. Unlabeled nodes in a diagram have implicit 2 labels. For example, a real regular polygon is {\"q\"} or {\"q\"} or .\n\nOne limitation, nodes connected by odd branch orders must have identical node orders. If they do not, the group will create \"starry\" polygons, with overlapping element. So and are ordinary, while is starry.\n\nCoxeter enumerated this list of regular complex polygons in formula_2. A regular complex polygon, {\"q\"} or , has \"p\"-edges, and \"q\"-gonal vertex figures. {\"q\"} is a finite polytope if (\"p\"+\"r\")\"q\">\"pr\"(\"q\"-2).\n\nIts symmetry is written as [\"q\"], called a \"Shephard group\", analogous to a Coxeter group, while also allowing real and unitary reflections.\n\nFor nonstarry groups, the order of the group [\"q\"] can be computed as formula_9.\n\nThe Coxeter number for [\"q\"] is formula_10, so the group order can also be computed as formula_11. A regular complex polygon can be drawn in orthogonal projection with \"h\"-gonal symmetry.\n\nThe rank 2 solutions that generate complex polygons are: \nExcluded solutions with odd \"q\" and unequal \"p\" and \"r\" are: [3], [3], [3], [3], ..., [5], [5], [5], [5], [7], [5], [9], and [11].\n\nOther whole \"q\" with unequal \"p\" and \"r\", create starry groups with overlapping fundamental domains: , , , , , and .\n\nThe dual polygon of {\"q\"} is {\"q\"}. A polygon of the form {\"q\"} is self-dual. Groups of the form [2\"q\"] have a half symmetry [\"q\"], so a regular polygon is the same as quasiregular . As well, regular polygon with the same node orders, , have an alternated construction , allowing adjacent edges to be two different colors.\n\nThe group order, \"g\", is used to compute the total number of vertices and edges. It will have \"g\"/\"r\" vertices, and \"g\"/\"p\" edges. When \"p\"=\"r\", the number of vertices and edges are equal. This condition is required when \"q\" is odd.\n\nPolygons of the form {2\"r\"} can be visualized by \"q\" color sets of \"p\"-edge. Each \"p\"-edge is seen as a regular polygon, while there are no faces.\n\n\nPolygons of the form {4} are called generalized orthoplexes. They share vertices with the 4D \"q\"-\"q\" duopyramids, vertices connected by 2-edges. \n\nPolygons of the form {4} are called generalized hypercubes (squares for polygons). They share vertices with the 4D \"p\"-\"p\" duoprisms, vertices connected by p-edges. Vertices are drawn in green, and \"p\"-edges are drawn in alternate colors, red and blue. The perspective is distorted slightly for odd dimensions to move overlappng vertices from the center.\n\nPolygons of the form {\"r\"} have equal number of vertices and edges. They are also self-dual.\n\nIn general, a regular complex polytope is represented by Coxeter as {\"z\"}{z}{z}… or Coxeter diagram …, having symmetry [\"z\"][\"z\"][\"z\"]… or ….\n\nThere are infinite families of regular complex polytopes that occur in all dimensions, generalizing the hypercubes and cross polytopes in real space. Shephard's \"generalized orthotope\" generalizes the hypercube; it has symbol given by γ = {4}{3}…{3} and diagram …. Its symmetry group has diagram [4][3]…[3]; in the Shephard–Todd classification, this is the group G(\"p\", 1, \"n\") generalizing the signed permutation matrices. Its dual regular polytope, the \"generalized cross polytope\", is represented by the symbol β = {3}{3}…{4} and diagram ….\n\nA 1-dimensional \"regular complex polytope\" in formula_1 is represented as , having \"p\" vertices, with its real representation a regular polygon, {\"p\"}. Coxeter also gives it symbol γ or β as 1-dimensional generalized hypercube or cross polytope. Its symmetry is [] or , a cyclic group of order \"p\". In a higher polytope, {} or represents a \"p\"-edge element, with a 2-edge, {} or , representing an ordinary real edge between two vertices.\n\nA dual complex polytope is constructed by exchanging \"k\" and (\"n\"-1-\"k\")-elements of an \"n\"-polytope. For example, a dual complex polygon has vertices centered on each edge, and new edges are centered at the old vertices. A \"v\"-valence vertex creates a new \"v\"-edge, and \"e\"-edges become \"e\"-valance vertices. The dual of a regular complex polytope has a reversed symbol. Regular complex polytopes with symmetric symbols, i.e. {\"q\"}, {\"q\"}{\"q\"}, {\"q\"}{\"s\"}{\"q\"}, etc. are self dual.\n\nCoxeter enumerated this list of nonstarry regular complex polyhedra in formula_13, including the 5 platonic solids in formula_14.\n\nA regular complex polyhedron, {\"n\"}{\"n\"} or , has faces, edges, and vertex figures.\n\nA complex regular polyhedron {\"n\"}{\"n\"} requires both \"g\" = order([\"n\"]) and \"g\" = order([\"n\"]) be finite.\n\nGiven \"g\" = order([\"n\"][\"n\"]), the number of vertices is \"g\"/\"g\", and the number of faces is \"g\"/\"g\". The number of edges is \"g\"/\"pr\".\n\n\n\nGeneralized octahedra have a regular construction as and quasiregular form as . All elements are simplexes.\n\nGeneralized cubes have a regular construction as and prismatic construction as , a product of three \"p\"-gonal 1-polytopes. Elements are lower dimensional generalized cubes.\n\nCoxeter enumerated this list of nonstarry regular complex 4-polytopes in formula_15, including the 6 convex regular 4-polytopes in formula_16.\n\nGeneralized 4-orthoplexes have a regular construction as and quasiregular form as . All elements are simplexes.\n\nGeneralized tesseracts have a regular construction as and prismatic construction as , a product of four \"p\"-gonal 1-polytopes. Elements are lower dimensional generalized cubes.\n\nRegular complex 5-polytopes in formula_17 or higher exist in three families, the real simplexes and the generalized hypercube, and orthoplex.\n\nGeneralized 5-orthoplexes have a regular construction as and quasiregular form as . All elements are simplexes.\n\n\nGeneralized 5-cubes have a regular construction as and prismatic construction as , a product of five \"p\"-gonal 1-polytopes. Elements are lower dimensional generalized cubes.\n\nGeneralized 6-orthoplexes have a regular construction as and quasiregular form as . All elements are simplexes.\n\n\nGeneralized 6-cubes have a regular construction as and prismatic construction as , a product of six \"p\"-gonal 1-polytopes. Elements are lower dimensional generalized cubes.\n\nCoxeter enumerated this list of nonstarry regular complex apeirotopes or honeycombs.\n\nFor each dimension there are 12 apeirotopes symbolized as δ exists in any dimensions formula_18, or formula_19 if \"p\"=\"q\"=2. Coxeter calls these generalized cubic honeycombs for \"n\">2.\n\nEach has proportional element counts given as:\n\nThe only regular complex 1-polytope is {}, or . Its real representation is an apeirogon, {∞}, or .\n\nRank 2 complex apeirogons have symmetry [\"q\"], where 1/\"p\" + 2/\"q\" + 1/\"r\" = 1. Coxeter expresses them as δ where \"q\" is constrained to satisfy .\n\nThere are 8 solutions: \nThere are two excluded solutions odd \"q\" and unequal \"p\" and \"r\": [5] and [3], or and .\n\nA regular complex apeirogon {\"q\"} has \"p\"-edges and \"q\"-gonal vertex figures. The dual apeirogon of {\"q\"} is {\"q\"}. An apeirogon of the form {\"q\"} is self-dual. Groups of the form [2\"q\"] have a half symmetry [\"q\"], so a regular apeirogon is the same as quasiregular .\n\nApeirogons can be represented on the Argand plane share four different vertex arrangements. Apeirogons of the form {\"q\"} have a vertex arrangement as {\"q\"/2,\"p\"}. The form {\"q\"} have vertex arrangement as r{\"p\",\"q\"/2}. Apeirogons of the form {4} have vertex arrangements {\"p\",\"r\"}.\n\nIncluding affine nodes, and formula_2, there are 3 more infinite solutions: [2], [4], [3], and , , and . The first is an index 2 subgroup of the second. The vertices of these apeirogons exist in formula_1.\n\nThere are 22 regular complex apeirohedra, of the form {\"a\"}{\"b\"}. 8 are self-dual (\"p\"=\"r\" and \"a\"=\"b\"), while 14 exist as dual polytope pairs. Three are entirely real (\"p\"=\"q\"=\"r\"=2).\n\nCoxeter symbolizes 12 of them as δ or {4}{4} is the regular form of the product apeirotope δ × δ or {\"q\"} × {\"q\"}, where \"q\" is determined from \"p\" and \"r\".\n\nThere are 16 regular complex apeirotopes in formula_13. Coxeter expresses 12 of them by δ where \"q\" is constrained to satisfy . These can also be decomposed as product apeirotopes: = . The first case is the formula_14 cubic honeycomb.\n\nThere are 15 regular complex apeirotopes in formula_15. Coxeter expresses 12 of them by δ where \"q\" is constrained to satisfy . These can also be decomposed as product apeirotopes: = . The first case is the formula_16 tesseractic honeycomb. The 16-cell honeycomb and 24-cell honeycomb are real solutions. The last solution is generated has Witting polytope elements.\n\nThere are only 12 regular complex apeirotopes in formula_17 or higher, expressed δ where \"q\" is constrained to satisfy . These can also be decomposed a product of \"n\" apeirogons: ... = ... . The first case is the real formula_19 hypercube honeycomb.\nA van Oss polygon is a regular polygon in the plane (real plane formula_30, or unitary plane formula_2) in which both an edge and the centroid of a regular polytope lie, and formed of elements of the polytope. Not all regular polytopes have Van Oss polygons.\n\nFor example, the van Oss polygons of a real octahedron are the three squares whose planes pass through its center. In contrast a cube does not have a van Oss polygon because the edge-to-center plane cuts diagonally across two square faces and the two edges of the cube which lie in the plane do not form a polygon.\n\nInfinite honeycombs also have van Oss apeirogons. For example, the real square tiling and triangular tiling have apeirogons {∞} van Oss apeirogons.\n\nIf it exists, the \"van Oss polygon\" of regular complex polytope of the form {\"q\"}{\"s\"}... has \"p\"-edges.\n\nSome complex polytopes can be represented as Cartesian products. These product polytopes are not strictly regular since they'll have more than one facet type, but some can represent lower symmetry of regular forms if all the orthogonal polytopes are identical. For example, the product {}×{} or of two 1-dimensional polytopes is the same as the regular {4} or . More general products, like {}×{} have real representations as the 4-dimensional \"p\"-\"q\" duoprisms. The dual of a product polytope can be written as a sum {}+{} and have real representations as the 4-dimensional \"p\"-\"q\" duopyramid. The {}+{} can have its symmetry doubled as a regular complex polytope {4} or .\n\nSimilarly, a formula_13 complex polyhedron can be constructed as a triple product: {}×{}×{} or is the same as the regular \"generalized cube\", {4}{3} or , as well as product {4}×{} or .\n\nA quasiregular polygon is a truncation of a regular polygon. A quasiregular polygon contains alternate edges of the regular polygons and . The quasiregular polygon has \"p\" vertices on the p-edges of the regular form.\n\nThere are 7 quasiregular complex apeirogons which alternate edges of a regular apeirogon and its regular dual. The vertex arrangements of these apeirogon have real representations with the regular and uniform tilings of the Euclidean plane. The last column for the 6{3}6 apeirogon is not only self-dual, but the dual coincides with itself with overlapping hexagonal edges, thus their quasiregular form also has overlapping hexagonal edges, so it can't be drawn with two alternating colors like the others. The symmetry of the self-dual families can be doubled, so creating an identical geometry as the regular forms: = \n\nLike real polytopes, a complex quasiregular polyhedron can be constructed as a rectification (a complete truncation) of a regular polyhedron. Vertices are created mid-edge of the regular polyhedron and faces of the regular polyhedron and its dual are positioned alternating across common edges.\n\nFor example, a p-generalized cube, , has \"p\" vertices, 3\"p\" edges, and 3\"p\" \"p\"-generalized square faces, while the \"p\"-generalized octahedron, , has 3\"p\" vertices, 3\"p\" edges and \"p\" triangular faces. The middle quasiregular form \"p\"-generalized cuboctahedron, , has 3\"p\" vertices, 3\"p\" edges, and 3\"p\"+\"p\" faces.\n\nAlso the rectification of the Hessian polyhedron , is , a quasiregular form sharing the geometry of the regular complex polyhedron .\n\nOther nonregular complex polytopes can be constructed within unitary reflection groups that don't make linear Coxeter graphs. In Coxeter diagrams with loops Coxeter marks a special period interior, like or symbol (1 1 1), and group [1 1 1]. These complex polytopes have not been systematically explored beyond a few cases.\n\nThe group is defined by 3 unitary reflections, R, R, R, all order 2: R = R = R = (RR) = (RR) = (RR) = (RRRR) = 1. The period \"p\" can be seen as a double rotation in real formula_16.\n\nAs with all Wythoff constructions, polytopes generated by reflections, the number of vertices of a single-ringed Coxeter diagram polytope is equal to the order of the group divided by the order of the subgroup where the ringed node is removed. For example, a real cube has Coxeter diagram , with octahedral symmetry order 48, and subgroup dihedral symmetry order 6, so the number of vertices of a cube is 48/6=8. Facets are constructed by removing one node furthest from the ringed node, for example for the cube. Vertex figures are generated by removing a ringed node and ringing one or more connected nodes, and for the cube.\n\nCoxeter represents these groups by the following symbols. Some groups have the same order, but a different structure, defining the same vertex arrangement in complex polytopes, but different edges and higher elements, like and with \"p\"≠3.\n\nCoxeter calls some of these complex polyhedra \"almost regular\" because they have regular facets and vertex figures. The first is a lower symmetry form of the generalized cross-polytope in formula_13. The second is a fractional generalized cube, reducing \"p\"-edges into single vertices leaving ordinary 2-edges. Three of them are related to the finite regular skew polyhedron in formula_16.\nCoxeter defines other groups with anti-unitary constructions, for example these three. The first was discovered and drawn by Peter McMullen in 1966.\n\n\n\n"}
{"id": "5407575", "url": "https://en.wikipedia.org/wiki?curid=5407575", "title": "Cryptographic log on", "text": "Cryptographic log on\n\nCryptographic log-on (CLO) is a process that uses Common Access Cards (CAC) and embedded Public Key Infrastructure (PKI) certificates to authenticate a user's identification to a workstation and network. It replaces the username and passwords for identifying and authenticating users. To log-on cryptographically to a CLO-enabled workstation, users simply insert their CAC into their workstation’s CAC reader and provide their Personal Identification Number (PIN).\n\nThe Navy/Marine Corps Intranet, among many other secure networks, uses CLO.\n"}
{"id": "30368248", "url": "https://en.wikipedia.org/wiki?curid=30368248", "title": "DiProDB", "text": "DiProDB\n\nDiProDB is a database designed to collect and analyse thermodynamic, structural and other dinucleotide properties.\n\n\n"}
{"id": "38344422", "url": "https://en.wikipedia.org/wiki?curid=38344422", "title": "Discrete Mathematics &amp; Theoretical Computer Science", "text": "Discrete Mathematics &amp; Theoretical Computer Science\n\nDiscrete Mathematics & Theoretical Computer Science is a peer-reviewed open access scientific journal covering discrete mathematics and theoretical computer science. It was established in 1997 by Daniel Krob (Paris Diderot University). Since 2001, the editor-in-chief is Jens Gustedt (Institut National de Recherche en Informatique et en Automatique).\n\nThe journal is abstracted and indexed in \"Mathematical Reviews\" and the Science Citation Index Expanded. According to the \"Journal Citation Reports\", the journal has a 2011 impact factor of 0.465.\n"}
{"id": "3127321", "url": "https://en.wikipedia.org/wiki?curid=3127321", "title": "Dual curve", "text": "Dual curve\n\nIn projective geometry, a dual curve of a given plane curve is a curve in the dual projective plane consisting of the set of lines tangent to . There is a map from a curve to its dual, sending each point to the point dual to its tangent line. If is algebraic then so is its dual and the degree of the dual is known as the \"class\" of the original curve. The equation of the dual of , given in line coordinates, is known as the \"tangential equation\" of .\n\nThe construction of the dual curve is the geometrical underpinning for the Legendre transformation in the context of Hamiltonian mechanics.\n\nLet be the equation of a curve in homogeneous coordinates. Let be the equation of a line, with being designated its line coordinates. The condition that the line is tangent to the curve can be expressed in the form which is the tangential equation of the curve.\n\nLet be the point on the curve, then the equation of the tangent at this point is given by\nSo is a tangent to the curve if\n\nEliminating , , , and from these equations, along with , gives the equation in , and of the dual curve.\nFor example, let be the conic . Then dual is found by eliminating , , , and from the equations\nThe first three equations are easily solved for , , , and substituting in the last equation produces\nClearing from the denominators, the equation of the dual is\n\nFor a parametrically defined curve its dual curve is defined by the following parametric equations:\nThe dual of an inflection point will give a cusp and two points sharing the same tangent line will give a self intersection point on the dual.\n\nIf is a plane algebraic curve then the degree of the dual is the number of points intersection with a line in the dual plane. Since a line in the dual plane corresponds to a point in the plane, the degree of the dual is the number of tangents to the that can be drawn through a given point. The points where these tangents touch the curve are the points of intersection between the curve and the polar curve with respect to the given point. If the degree of the curve is then the degree of the polar is and so the number of tangents that can be drawn through the given point is at most .\n\nThe dual of a line (a curve of degree 1) is an exception to this and is taken to be a point in the dual space (namely the original line). The dual of a single point is taken to be the collection of lines though the point; this forms a line in the dual space which corresponds to the original point.\n\nIf is smooth, i.e. there are no singular points then the dual of has the maximum degree . If is a conic this implies its dual is also a conic. This can also be seen geometrically: the map from a conic to its dual is one-to-one (since no line is tangent to two points of a conic, as that requires degree 4), and tangent line varies smoothly (as the curve is convex, so the slope of the tangent line changes monotonically: cusps in the dual require an inflection point in the original curve, which requires degree 3).\n\nFor curves with singular points, these points will also lie on the intersection of the curve and its polar and this reduces the number of possible tangent lines. The degree of the dual given in terms of the \"d\" and the number and types of singular points of is one of the Plücker formulas.\n\nThe dual can be visualized as a locus in the plane in the form of the \"polar reciprocal\". This is defined with reference to a fixed conic as the locus of the poles of the tangent lines of the curve . The conic is nearly always taken to be a circle and this case the polar reciprocal is the inverse of the pedal of .\n\nProperties of the original curve correspond to dual properties on the dual curve. In the image at right, the red curve has three singularities – a node in the center, and two cusps at the lower right and lower left. The black curve has no singularities, but has four distinguished points: the two top-most points have the same tangent line (a horizontal line), while there are two inflection points on the upper curve. The two top-most points correspond to the node (double point), as they both have the same tangent line, hence map to the same point in the dual curve, while the inflection points correspond to the cusps, corresponding to the tangent lines first going one way, then the other (slope increasing, then decreasing).\n\nBy contrast, on a smooth, convex curve the angle of the tangent line changes monotonically, and the resulting dual curve is also smooth and convex.\n\nFurther, both curves have a reflectional symmetry, corresponding to the fact that symmetries of a projective space correspond to symmetries of the dual space, and that duality of curves is preserved by this, so dual curves have the same symmetry group. In this case both symmetries are realized as a left-right reflection; this is an artifact of how the space and the dual space have been identified – in general these are symmetries of different spaces.\n\nSimilarly, generalizing to higher dimensions, given a hypersurface, the tangent space at each point gives a family of hyperplanes, and thus defines a dual hypersurface in the dual space. For any closed subvariety in a projective space, the set of all hyperplanes tangent to some point of is a closed subvariety of the dual of the projective space, called the dual variety of .\n\nExamples\n\n\nThe dual curve construction works even if the curve is piecewise linear (or piecewise differentiable, but the resulting map is degenerate (if there are linear components) or ill-defined (if there are singular points).\n\nIn the case of a polygon, all points on each edge share the same tangent line, and thus map to the same vertex of the dual, while the tangent line of a vertex is ill-defined, and can be interpreted as all the lines passing through it with angle between the two edges. This accords both with projective duality (lines map to points, and points to lines), and with the limit of smooth curves with no linear component: as a curve flattens to an edge, its tangent lines map to closer and closer points; as a curve sharpens to a vertex, its tangent lines spread further apart.\n\n\n"}
{"id": "34914591", "url": "https://en.wikipedia.org/wiki?curid=34914591", "title": "European Conference on Computational Biology", "text": "European Conference on Computational Biology\n\nThe European Conference on Computational Biology (ECCB) is a scientific meeting on the subjects of bioinformatics and computational biology. It covers a wide spectrum of disciplines, including bioinformatics, computational biology, genomics, computational structural biology, and systems biology. ECCB is organized annually in different European cities. Since 2007, the conference has been held jointly with Intelligent Systems for Molecular Biology (ISMB) every second year. The conference also hosts the European ISCB Student Council Symposium. The proceedings of the conference are published by the journal \"Bioinformatics\".\n\nECCB was formed with the intent of providing a European conference focusing on advances in computational biology and their application to problems in molecular biology. The conference was initially to be held on a rotating basis, with the idea that previously successful regional conferences (for instance, the German Conference on Bioinformatics (GCB), the French Journées Ouvertes Biologie, Informatique et Mathématiques (JOBIM) conference and the British Genes, Proteins & Computers (GPC) conference) would be jointly held with ECCB if that region was hosting ECCB in that particular year. The first ECCB conference was held in October 2002 in Saarbrücken, Germany and was chaired by Hans-Peter Lenhof. 69 scientific papers were submitted to the conference.\n\nIn 2004, ECCB was jointly held with the Intelligent Systems for Molecular Biology (ISMB) conference for the first time. It was also co-located with the Genes, Proteins & Computers conference. This meeting, held in Glasgow, UK, was the largest bioinformatics conference ever held, attended by 2,136 delegates, submitting 496 scientific papers. ISCB Board member and Director of the Spanish National Bioinformatics Institute Alfonso Valencia considers ISMB/ECCB 2004 to be an important milestone in the history of ISMB: \"it was the first one where the balance between Europe and the States became an important part of the conference. It was here that we established the rules and the ways and the spirit of collaboration between the Americans and the Europeans.\" The success of the joint conference paved the way for future European ISMB meetings to be held jointly with ECCB. In January 2007, ISMB and ECCB agreed to hold joint conferences in Europe every other year, beginning with ISMB/ECCB 2007. This pattern has been confirmed to continue until at least 2017.\n"}
{"id": "22751892", "url": "https://en.wikipedia.org/wiki?curid=22751892", "title": "Fake projective space", "text": "Fake projective space\n\nIn mathematics, a fake projective space is a complex algebraic variety that has the same Betti numbers as some projective space, but is not isomorphic to it.\n\nThere are exactly 50 fake projective planes. found four examples of fake projective 4-folds, and showed that no arithmetic examples exist in dimensions other than 2 and 4.\n"}
{"id": "48389", "url": "https://en.wikipedia.org/wiki?curid=48389", "title": "Galactic coordinate system", "text": "Galactic coordinate system\n\nThe galactic coordinate system is a celestial coordinate system in spherical coordinates, with the Sun as its center, the primary direction aligned with the approximate center of the Milky Way galaxy, and the fundamental plane parallel to an approximation of the galactic plane but offset to its north. It uses the right-handed convention, meaning that coordinates are positive toward the north and toward the east in the fundamental plane.\n\nLongitude (symbol ) measures the angular distance of an object eastward along the galactic equator from the galactic center. Analogous to terrestrial longitude, galactic longitude is usually measured in degrees (°).\n\nLatitude (symbol ) measures the angle of an object north or south of the galactic equator (or midplane) as viewed from Earth; positive to the north, negative to the south. For example, the north galactic pole has a latitude of +90°. Analogous to terrestrial latitude, galactic latitude is usually measured in degrees (°).\n\nThe first galactic coordinate system was used by William Herschel in 1785. A number of different coordinate systems, each differing by a few degrees, were used until 1932, when Lund Observatory assembled a set of conversion tables that defined a standard galactic coordinate system based on a galactic north pole at RA , dec +28° (in the B1900.0 epoch convention) and a 0° longitude at the point where the galactic plane and equatorial plane intersected.\n\nIn 1958, the International Astronomical Union (IAU) defined the galactic coordinate system in reference to radio observations of galactic neutral hydrogen through the hydrogen line, changing the definition of the Galactic longitude by 32° and the latitude by 1.5°. In the equatorial coordinate system, for equinox and equator of 1950.0, the north galactic pole is defined at right ascension , declination +27.4°, in the constellation Coma Berenices, with a probable error of ±0.1°. Longitude 0° is the great semicircle that originates from this point along the line in position angle 123° with respect to the equatorial pole. The galactic longitude increases in the same direction as right ascension. Galactic latitude is positive towards the north galactic pole, with a plane passing through the Sun and parallel to the galactic equator being 0°, whilst the poles are ±90°. Based on this definition, the galactic poles and equator can be found from spherical trigonometry and can be precessed to other epochs; see the table.\n\nThe IAU recommended that during the transition period from the old, pre-1958 system to the new, the old longitude and latitude should be designated and while the new should be designated and . This convention is occasionally seen.\n\nRadio source Sagittarius A*, which is the best physical marker of the true galactic center, is located at , (J2000). Rounded to the same number of digits as the table, , −29.01° (J2000), there is an offset of about 0.07° from the defined coordinate center, well within the 1958 error estimate of ±0.1°. Due to the Sun's position, which currently lies north of the midplane, and the heliocentric definition adopted by the IAU, the galactic coordinates of Sgr A* are latitude south, longitude . Since as defined the galactic coordinate system does not rotate with time, Sgr A* is actually decreasing in longitude at the rate of galactic rotation at the sun, , approximately 5.7 milliarcseconds per year (see Oort constants).\n\nIn some applications use is made of rectangular coordinates based on galactic longitude and latitude and distance. In some work regarding the distant past or future the galactic coordinate system is taken as rotating so that the -axis always goes to the centre of the galaxy.\n\nThere are two major rectangular variations of galactic coordinates, commonly used for computing space velocities of galactic objects. In these systems the -axes are designated , but the definitions vary by author. In one system, the axis is directed toward the galactic center ( = 0°), and it is a right-handed system (positive towards the east and towards the north galactic pole); in the other, the axis is directed toward the galactic anticenter ( = 180°), and it is a left-handed system (positive towards the east and towards the north galactic pole).\n\nThe galactic equator runs through the following constellations:\n\n\n"}
{"id": "3108737", "url": "https://en.wikipedia.org/wiki?curid=3108737", "title": "Hermitian function", "text": "Hermitian function\n\nIn mathematical analysis, a Hermitian function is a complex function with the property that its complex conjugate is equal to the original function with the variable changed in sign:\n\n(where the formula_2 indicates the complex conjugate) for all formula_3 in the domain of formula_4.\n\nThis definition extends also to functions of two or more variables, e.g., in the case that formula_4 is a function of two variables it is Hermitian if\n\nfor all pairs formula_7 in the domain of formula_4.\n\nFrom this definition it follows immediately that: formula_4 is a Hermitian function if and only if\n\n\nHermitian functions appear frequently in mathematics, physics, and signal processing. For example, the following two statements follow from basic properties of the Fourier transform:\n\nSince the Fourier transform of a real signal is guaranteed to be Hermitian, it can be compressed using the Hermitian even/odd symmetry. This, for example, allows the discrete Fourier transform of a signal (which is in general complex) to be stored in the same space as the original real signal.\n\n\nWhere the formula_17 is cross-correlation, and formula_18 is convolution.\n\n\n"}
{"id": "21348562", "url": "https://en.wikipedia.org/wiki?curid=21348562", "title": "Homeokinetics", "text": "Homeokinetics\n\nHomeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.\n\nArthur Iberall, Warren McCulloch and Harry Soodak developed the concept of homeokinetics as a new branch of physics. It began through Iberall's biophysical research for the NASA exobiology program into the dynamics of mammalian physiological processes They were observing an area that physics has neglected, that of complex systems with their very long internal factory day delays. They were observing systems associated with nested hierarchy and with an extensive range of time scale processes. It was such connections, referred to as both up-down or in-out connections (as nested hierarchy) and side-side or flatland physics among atomistic-like components (as heterarchy) that became the hallmark of homeokinetic problems. By 1975, they began to put a formal catch-phrase name on those complex problems, associating them with nature, life, human, mind, and society. The major method of exposition that they began using was a combination of engineering physics and a more academic pure physics. In 1981, Iberall was invited to the Crump Institute for Medical Engineering of UCLA, where he further refined the key concepts of homeokinetics, developing a physical scientific foundation for complex systems.\n\nA system is a collective of interacting ‘atomistic’-like entities. The word ‘atomism’ is used to stand both for the entity and the doctrine. As is known from ‘kinetic’ theory, in mobile or simple systems, the atomisms share their ‘energy’ in interactive collisions. That so-called ‘equipartitioning’ process takes place within a few collisions. Physically, if there is little or no interaction, the process is considered to be very weak. Physics deals basically with the forces of interaction—few in number—that influence the interactions. They all tend to emerge with considerable force at high ‘density’ of atomistic interaction. In complex systems, there is also a result of internal processes in the atomisms. They exhibit, in addition to the pair-by-pair interactions, internal actions such as vibrations, rotations, and association. If the energy and time involved internally creates a very large—in time—cycle of performance of their actions compared to their pair interactions, the collective system is complex. If you eat a cookie and you do not see the resulting action for hours, that is complex; if boy meets girl and they become ‘engaged’ for a protracted period, that is complex. What emerges from that physics is a broad host of changes in state and stability transitions in state. Viewing Aristotle as having defined a general basis for systems in their static-logical states and trying to identify a logic-metalogic for physics, e.g., metaphysics, then homeokinetics is viewed to be an attempt to define the dynamics of all those systems in the universe.\n\nOrdinary physics is a flatland physics, a physics at some particular level. Examples include nuclear and atomic physics, biophysics, social physics, and stellar physics. Homeokinetic physics combines flatland physics with the study of the up down processes that binds the levels. Tools, such as mechanics, quantum field theory, and the laws of thermodynamics, provide key relationships for the binding of the levels, how they connect, and how the energy flows up and down. And whether the atomisms are atoms, molecules, cells, people, stars, galaxies, or universes, the same tools can be used to understand them. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.\n\nA homeokinetic approach to complex systems has been applied to ecological psychology, anthropology, geology,\nbioenergetics, and political science.\n\nIt has also been applied to social physics where a homeokinetics analysis shows that one must account for flow variables such as the flow of energy, of materials, of action, reproduction rate, and value-in-exchange.\n\n"}
{"id": "53659697", "url": "https://en.wikipedia.org/wiki?curid=53659697", "title": "Infinite chess", "text": "Infinite chess\n\nInfinite chess is any variation of the game chess played on an unbounded chessboard. Versions of infinite chess have been introduced independently by multiple players, chess theorists, and mathematicians, both as a playable game and as a model for theoretical study. It has been found that even though the board is unbounded, there are ways in which a player can win the game in a finite number of moves.\n\nClassical (FIDE) chess is played on an 8×8 board (64 squares). However, the history of chess includes variants of the game played on boards of various sizes. A predecessor game called Courier chess was played on a slightly larger 12×8 board (96 squares) in the 12th century, and continued to be played for at least six hundred years. Japanese chess (shogi) has been played historically on boards of various sizes; the largest is taikyoku shōgi (\"ultimate chess\"). This chess-like game, which dates to the mid 16th century, was played on a 36×36 board (1296 squares). Each player starts with 402 pieces of 209 different types, and a well-played game would require several days of play, possibly requiring each player to make over a thousand moves.\n\nChess player Jianying Ji was one of many to propose infinite chess, suggesting a setup with the chess pieces in the same relative positions as in classical chess, with knights replaced by nightriders and a rule preventing pieces from travelling too far from opposing pieces. Numerous other chess players, chess theorists, and mathematicians who study game theory have conceived of variations of infinite chess, often with different objectives in mind. Chess players sometimes use the scheme simply to alter the strategy; since chess pieces, and in particular the king, cannot be trapped in corners on an infinite board, new patterns are required to form a checkmate. Theorists conceive of infinite chess variations to expand the theory of chess in general, or as a model to study other mathematical, economic, or game-playing strategies.\n\nFor infinite chess, mathematical investigations have shown that in a general endgame, one player can force a win in a finite number of moves. More specifically, it has been found that infinite chess is decidable; that is, given a position (such as formula_1, and assuming pawns do not promote) of a finite number of chess pieces which are uniformly mobile and with constant and linear freedom, and (for example) White to move, there is an algorithm that will answer if White can win or force a draw, against any defense by Black.\n\n\n\n"}
{"id": "567445", "url": "https://en.wikipedia.org/wiki?curid=567445", "title": "Jakob Steiner", "text": "Jakob Steiner\n\nJakob Steiner (18 March 1796 – 1 April 1863) was a Swiss mathematician who worked primarily in geometry.\n\nSteiner was born in the village of Utzenstorf, Canton of Bern. At 18, he became a pupil of Heinrich Pestalozzi and afterwards studied at Heidelberg. Then, he went to Berlin, earning a livelihood there, as in Heidelberg, by tutoring. Here he became acquainted with A. L. Crelle, who, encouraged by his ability and by that of N. H. Abel, then also staying at Berlin, founded his famous \"Journal\" (1826).\n\nAfter Steiner's publication (1832) of his \"Systematische Entwickelungen\" he received, through C. G. J. Jacobi, who was then professor at Königsberg University, and earned an honorary degree there; and through the influence of Carl Gustav Jacob Jacobi and of the brothers Alexander and Wilhelm von Humboldt a new chair of geometry was founded for him at Berlin (1834). This he occupied until his death in Bern on 1 April 1863.\n\nHe was described by Thomas Hirst as follows: \n\nSteiner's mathematical work was mainly confined to geometry. This he treated synthetically, to the total exclusion of analysis, which he hated, and he is said to have considered it a disgrace to synthetic geometry if equal or higher results were obtained by analytical geometry methods. In his own field he surpassed all his contemporaries. His investigations are distinguished by their great generality, by the fertility of his resources, and by the rigour in his proofs. He has been considered the greatest pure geometer since Apollonius of Perga.\n\nIn his \"Systematische Entwickelung der Abhängigkeit geometrischer Gestalten von einander\" he laid the foundation of modern synthetic geometry. He introduces what are now called the geometrical forms (the \"row\", \"flat pencil\", etc.), and establishes between their elements a one-to-one correspondence, or, as he calls it, makes them projective. He next gives by aid of these projective rows and pencils a new generation of conics and ruled quadric surfaces, which leads quicker and more directly than former methods into the inner nature of conics and reveals to us the organic connection of their innumerable properties and mysteries. In this work also, of which only one volume appeared instead of the projected five, we see for the first time the principle of duality introduced from the very beginning as an immediate outflow of the most fundamental properties of the plane, the line and the point.\n\nIn a second little volume, \"Die geometrischen Constructionen ausgeführt mittels der geraden Linie und eines festen Kreises\" (1833), republished in 1895 by Ottingen, he shows, what had been already suggested by J. V. Poncelet, how all problems of the second order can be solved by aid of the straight edge alone without the use of compasses, as soon as one circle is given on the drawing-paper. He also wrote \"Vorlesungen über synthetische Geometrie\", published posthumously at Leipzig by C. F. Geiser and H. Schroeter in 1867; a third edition by R. Sturm was published in 1887-1898.\n\nOther geometric results by Steiner include development of a formula for the partitioning of space by planes (the maximal number of parts created by n planes), several theorems about the famous Steiner's chain of tangential circles, and a proof of the isoperimetric theorem (later a flaw was found in the proof, but was corrected by Weierstrass).\n\nThe rest of Steiner's writings are found in numerous papers mostly published in \"Crelle's Journal\", the first volume of which contains his first four papers. The most important are those relating to algebraic curves and surfaces, especially the short paper \"Allgemeine Eigenschaften algebraischer Curven\". This contains only results, and there is no indication of the method by which they were obtained, so that, according to L. O. Hosse, they are, like Fermat's theorems, riddles to the present and future generations. Eminent analysts succeeded in proving some of the theorems, but it was reserved to Luigi Cremona to prove them all, and that by a uniform synthetic method, in his book on algebraic curves.\n\nOther important investigations relate to maxima and minima. Starting from simple elementary propositions, Steiner advances to the solution of problems which analytically require the calculus of variations, but which at the time altogether surpassed the powers of that calculus. Connected with this is the paper \"Vom Krümmungsschwerpuncte ebener Curven\", which contains numerous properties of pedals and roulettes, especially of their areas.\n\nSteiner also made a small but important contribution to combinatorics. In 1853, Steiner published a two pages article in \"Crelle's Journal\" on what nowadays is called Steiner systems, a basic kind of block design.\n\nHis oldest papers and manuscripts (1823-1826) were published by his admirer Fritz Bützberger on the request of the Bernese Society for Natural Scientists.\n\n\n"}
{"id": "24687007", "url": "https://en.wikipedia.org/wiki?curid=24687007", "title": "Jenő Egerváry", "text": "Jenő Egerváry\n\nJenő Egerváry (or Eugene Egerváry) (April 16, 1891 – November 30, 1958) was a Hungarian mathematician.\n\nEgerváry was born in Debrecen in 1891. In 1914, he received his doctorate at the Pázmány Péter University in Budapest, where he studied under the supervision of Lipót Fejér. He then worked as an assistant at the Seismological Observatory in Budapest, and since 1918 as a professor at the Superior Industrial School in Budapest. In 1938 he was appointed Privatdozent at the Pázmány Péter University in Budapest.\n\nIn 1941 he became full professor at the Technical University of Budapest, and in 1950 he was appointed Chairman of the Scientific Council of the Research Institute for Applied Mathematics of the Hungarian Academy of Sciences.\n\nEgerváry received the Gyula Kőnig Prize in 1932 and the Kossuth Prize in 1949 and 1953.\n\nHe committed suicide in 1958 because of the troubles caused to him by the communist bureaucracy.\n\nEgerváry's interests spanned the theory of algebraic equations, geometry, differential equations, and matrix theory.\n\nIn what later became a classic result in the field of combinatorial optimization, Egerváry generalized Kőnig's theorem to the case of weighted graphs. This contribution was translated and published in 1955 by Harold W. Kuhn, who also showed how to apply Kőnig's and Egerváry's method to solve the assignment problem; the resulting algorithm has since been known as the \"Hungarian method\". \n\n"}
{"id": "20183658", "url": "https://en.wikipedia.org/wiki?curid=20183658", "title": "Linear programming decoding", "text": "Linear programming decoding\n\nIn information theory and coding theory, linear programming decoding (LP decoding) is a decoding method which uses concepts from linear programming (LP) theory to solve decoding problems. This approach was first used by Jon Feldman \"et al.\" They showed how the LP can be used to decodes block codes.\n\nThe basic idea behind LP decoding is to first represent the maximum likelihood decoding of a linear code as an integer linear program, and then relax the integrality constraints on the variables into linear inequalities.\n"}
{"id": "11353703", "url": "https://en.wikipedia.org/wiki?curid=11353703", "title": "List of Jewish mathematicians", "text": "List of Jewish mathematicians\n\nThis list of Jewish mathematicians includes mathematicians and statisticians who are or were verifiably Jewish or of Jewish descent. In 1933, when the Nazis rose to power in Germany, one-third of all mathematics professors in the country were Jewish, while Jews constituted less than one percent of the population. Jewish mathematicians made major contributions throughout the 20th century and into the 21st, as is evidenced by their high representation among the winners of major mathematics awards: 27% for the Fields Medal, 30% for the Abel Prize, and 40% for the Wolf Prize.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "241820", "url": "https://en.wikipedia.org/wiki?curid=241820", "title": "List of mathematical shapes", "text": "List of mathematical shapes\n\nFollowing is a list of some mathematically well-defined shapes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"See the list of algebraic surfaces.\"\n\n\n\nThis table shows a summary of regular polytope counts by dimension.\n\nThere are no nonconvex Euclidean regular tessellations in any number of dimensions.\n\nThe elements of a polytope can be considered according to either their own dimensionality or how many dimensions \"down\" they are from the body.\n\n\nFor example, in a polyhedron (3-dimensional polytope), a face is a facet, an edge is a ridge, and a vertex is a peak.\n\n\nThe classical convex polytopes may be considered tessellations, or tilings, of spherical space. Tessellations of euclidean and hyperbolic space may also be considered regular polytopes. Note that an 'n'-dimensional polytope actually tessellates a space of one dimension less. For example, the (three-dimensional) platonic solids tessellate the 'two'-dimensional 'surface' of the sphere.\n\n\nThere is only one polytope in 1 dimension, whose boundaries are the two endpoints of a line segment, represented by the empty Schläfli symbol {}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolygons named for their number of sides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "12154411", "url": "https://en.wikipedia.org/wiki?curid=12154411", "title": "List of problems in loop theory and quasigroup theory", "text": "List of problems in loop theory and quasigroup theory\n\nIn mathematics, especially abstract algebra, loop theory and quasigroup theory are active research areas with many open problems. As in other areas of mathematics, such problems are often made public at professional conferences and meetings. Many of the problems posed here first appeared in the \"Loops (Prague)\" conferences and the \"Mile High (Denver)\" conferences.\n\nLet \"L\" be a Moufang loop with normal abelian subgroup (associative subloop) \"M\" of odd order such that \"L\"/\"M\" is a cyclic group of order bigger than 3. (i) Is \"L\" a group? (ii) If the orders of \"M\" and \"L\"/\"M\" are relatively prime, is L a group?\n\n\nConjecture: Any finite commutative Moufang loop of period 3 can be embedded into a commutative alternative algebra.\n\n\nConjecture: Let \"L\" be a finite Moufang loop and Φ(\"L\") the intersection of all maximal subloops of \"L\". Then Φ(\"L\") is a normal nilpotent subloop of \"L\".\n\n\nFor a group formula_1, define formula_2 on formula_1 x formula_4 by\nformula_5, formula_6, formula_7, formula_8. Find a minimal presentation for the Moufang loop formula_2 with respect to a presentation for formula_1.\n\n\nLet \"p\" and \"q\" be distinct odd primes. If \"q\" is not congruent to 1 modulo \"p\", are all Moufang loops of order \"p\"\"q\" groups? What about \"pq\"?\n\n\nIs there a Moufang loop of odd order with trivial nucleus?\n\n\nFind presentations for all nonassociative finite simple Moufang loops in the variety of Moufang loops.\n\n\nConjecture: Let \"M\" be a finite Moufang loop of exponent \"n\" with \"m\" generators. Then there exists a function \"f\"(\"n\",\"m\") such that |\"M\"| < \"f\"(\"n\",\"m\").\n\n\nConjecture: Let \"L\" be a finitely generated Moufang loop of exponent 4 or 6. Then \"L\" is finite.\n\n\nLet MF be the free Moufang loop with \"n\" generators.\n\nConjecture: MF is torsion free but MF with \"n\" > 4 is not.\n\n\nFor a left Bol loop \"Q\", find some relation between the nilpotency degree of the left multiplication group of \"Q\" and the structure of \"Q\".\n\n\nLet formula_15, formula_16 be two quasigroups defined on the same underlying set formula_17. The distance formula_18 is the number of pairs formula_19 in formula_20 such that formula_21. Call a class of finite quasigroups \"quadratic\" if there is a positive real number formula_22 such that any two quasigroups formula_15, formula_16 of order formula_25 from the class satisfying formula_26 are isomorphic. Are Moufang loops quadratic? Are Bol loops quadratic?\n\n\nDetermine the Campbell–Hausdorff series for analytic Bol loops.\n\n\nA loop is \"universally flexible\" if every one of its loop isotopes is flexible, that is, satisfies (\"xy\")\"x\" = \"x\"(\"yx\"). A loop is \"middle Bol\" if every one of its loop isotopes has the antiautomorphic inverse property, that is, satisfies (\"xy\") = \"y\"\"x\". Is there a finite, universally flexible loop that is not middle Bol?\n\n\nIs there a finite simple nonassociative Bol loop with nontrivial conjugacy classes?\n\n\nLet \"Q\" be a loop whose inner mapping group is nilpotent. Is \"Q\" nilpotent? Is \"Q\" solvable?\n\n\nLet \"Q\" be a loop with abelian inner mapping group. Is \"Q\" nilpotent? If so, is there a bound on the nilpotency class of \"Q\"? In particular, can the nilpotency class of \"Q\" be higher than 3?\n\n\nDetermine the number of nilpotent loops of order 24 up to isomorphism.\n\n\nClassify the finite simple paramedial quasigroups.\n\n\nAre there infinite simple paramedial quasigroups?\n\n\nA variety \"V\" of quasigroups is \"isotopically universal\" if every quasigroup is isotopic to a member of \"V\". Is the variety of loops a minimal isotopically universal variety? Does every isotopically universal variety contain the variety of loops or its parastrophes?\n\nDoes there exist a quasigroup \"Q\" of order \"q\" = 14, 18, 26 or 42 such that the operation * defined on \"Q\" by \"x\" * \"y\" = \"y\" − \"xy\" is a quasigroup operation?\n\nConstruct a latin square \"L\" of order \"n\" as follows: Let G = K be the complete bipartite graph with distinct weights on its n edges. Let M be the cheapest matching in \"G\", \"M\" the cheapest matching in G with M removed, and so on. Each matching \"M\" determines a permutation p of 1, ..., \"n\". Let \"L\" be obtained from \"G\" by placing the permutation \"p\" into row \"i\" of \"L\". Does this procedure result in a uniform distribution on the space of latin squares of order \"n\"?\n\nFor a loop \"Q\", let Mlt(Q) denote the multiplication group of \"Q\", that is, the group generated by all left and right translations. Is |Mlt(\"Q\")| < \"f\"(|\"Q\"|) for some variety of loops and for some polynomial \"f\"?\n\nDoes every finite alternative loop, that is, every loop satisfying \"x\"(\"xy\") = (\"xx\")\"y\" and \"x\"(\"yy\") = (\"xy\")\"y\", have 2-sided inverses?\n\nFind a nonassociative finite simple automorphic loop, if such a loop exists.\n\nWe say that a variety \"V\" of loops satisfies the Moufang theorem if for every loop \"Q\" in \"V\" the following implication holds: for every \"x\", \"y\", \"z\" in \"Q\", if \"x\"(\"yz\") = (\"xy\")\"z\" then the subloop generated by \"x\", \"y\", \"z\" is a group. Is every variety that satisfies Moufang theorem contained in the variety of Moufang loops?\n\nA loop is \"Osborn\" if it satisfies the identity \"x\"((\"yz\")\"x\") = (\"x\"\\\"y\")(\"zx\"). Is every Osborn loop universal, that is, is every isotope of an Osborn loop Osborn? If not, is there a nice identity characterizing universal Osborn loops?\n\nThe following problems were posed as open at various conferences and have since been solved.\n\nIs there a Buchsteiner loop that is not conjugacy closed? Is there a finite simple Buchsteiner loop that is not conjugacy closed?\n\nClassify nonassociative Moufang loops of order 64.\n\nConstruct a conjugacy closed loop whose left multiplication group is not isomorphic to its right multiplication group.\n\nIs there a finite simple Bol loop that is not Moufang?\n\nIs there a finite non-Moufang left Bol loop with trivial right nucleus?\n\nDoes every finite Moufang loop have the strong Lagrange property?\n\nIs there a Moufang loop whose commutant is not normal?\n\nIs the class of cores of Bol loops a quasivariety?\n\nLet I(n) be the number of isomorphism classes of quasigroups of order n. Is I(n) odd for every n?\n\nClassify the finite simple paramedial quasigroups.\n\n\n\n"}
{"id": "46356463", "url": "https://en.wikipedia.org/wiki?curid=46356463", "title": "Local criterion for flatness", "text": "Local criterion for flatness\n\nIn algebra, local criterion for flatness gives conditions one can check to show flatness of a module.\n\nGiven a commutative ring \"A\", an ideal \"I\" and an \"A\"-module \"M\", suppose either\nor\nThen the following are equivalent:\nThe assumption that “\"A\" is a Noetherian ring” is used to invoke the Artin–Rees lemma and can be weakened; see \n\nFollowing SGA 1, Exposé IV, we first prove a few lemmas, which are interesting themselves. (See also this blog post by Akhil Mathew for a proof of a special case.)\n\n\"Proof\": The assumption implies that formula_3 and so, since tensor product commutes with base extension,\n\n\"Proof\": To see the equivalence of the first two, study Tor spectral sequence. (Editorial note: give more details after adding the discussion of the Tor spectral sequence to Wikipedia.) To see the \"Moreover\" part, if 1. is valid, then formula_6 and so\nBy descending induction, this implies 3. The converse is trivial. formula_5\n\n\"Proof\": In view of Lemma 1, the only converse needs to be shown. Let formula_9 denote the exact sequence formula_10 and formula_11. Consider the exact sequence of complexes:\nThen formula_13 (it is so for large formula_14 and then use descending induction). 3. of Lemma 2 then implies that formula_15 is flat. formula_5\n\n\"Proof of the main statement\".\n\nformula_17: If formula_18 is nilpotent, then, by Lemma 2, formula_19 and formula_15 is flat over formula_21. Thus, assume that the first assumption is valid. Let formula_22 be an ideal and we shall show formula_23 is injective. For an integer formula_24, consider the exact sequence\nSince formula_26 by Lemma 2 (note: formula_27 kills formula_28), tensoring the above with formula_15, we get:\nTensoring formula_15 with formula_32, we also have:\nWe combine the two to get:\n\nNow, if formula_35 is in the kernel of formula_23, then, a fortiori, formula_35 is in formula_38. By the Artin–Rees lemma, given formula_39, we can find formula_24 such that formula_41. Since formula_42, we conclude formula_43.\n\nformula_44 follows from Lemma 1.\n\nformula_45: Since formula_46, the condition 4. is still valid with formula_47 replaced by formula_48. Then Lemma 3 says that formula_49 is flat over formula_50.\n\nformula_51 Tensoring formula_52 with \"M\", we see formula_53 is the kernel of formula_54. Thus, the implication is established by an argument similar to that of formula_17formula_5\n\nB. Conrad calls the next theorem \"the miracle flatness theorem\".\n\n"}
{"id": "9938473", "url": "https://en.wikipedia.org/wiki?curid=9938473", "title": "Logica nova", "text": "Logica nova\n\nIn the history of logic, the term logica nova (Latin, meaning new logic) refers to a subdivision of the logical tradition of Western Europe, as it existed around the middle of the thirteenth century. According to the availability at the time of the logical works of Aristotle (written in Greek) in Latin translation, there was a \"logica vetus\" (old logic) and the \"logica nova\".\n\nThe division of works was as follows:\n\n\nThese works, excluding the \"Liber sex principiorum\", were already canonical in the time of Abelard. He wrote his so-called \"Logica Ingredientibus\" on the scheme of a set of seven commentaries.\n\n\nThe advent of the \"logica nova\" was the result of new Latin translations, particularly by James of Venice. The combination of the two logics was termed the \"logica antiquorum\" (logic of the ancients). Restricting just to the works of Aristotle, the whole \"Organon\" of six works was split by the historical accidents of transmission into two books in the \"logica vetus\", and four in the \"logica nova\".\n\nSome of the religious orders organized special \"studia\" for the formation of their members dedicated to the study of the new logic. For example, after the theology component of the \"studium provinciale\" of the Dominican Order at the Roman convent of Santa Sabina was transferred in 1288 to the convent of Santa Maria sopra Minerva, which would develop into the College of Saint Thomas in the 15th century and into the Pontifical University of Saint Thomas Aquinas, \"Angelicum\", the Santa Sabina \"studium\" was redesignated in 1291 as one of three \"studia nove logice\" of the Order. These \"studia\" were intended to offer courses of advanced logic covering the logica nova, the Aristotelian texts recovered in the West only in the second half of the 12th century, the \"Topics\", \"Sophistical Refutations\", and the \"First and Second Analytics\" of Aristotle. This was an advance over the \"logica antiqua\", which treated the \"Isagoge of Porphyry\", \"Divisions\" and \"Topics\" of Boethius, the \"Categories\" and \"On Interpretation\" of Aristotle, and the \"Summule logicales\" of Peter of Spain. Milone da Velletri was lector there in 1293 In 1310 the Florentine Giovanni dei Tornaquinci was lector there. In 1331 Nerius de Tertia was lector, and Giovanni Zocco da Spoleto was a student of logic there.\n\nAnother usage for \"logica nova\" is for the later theories of Ramón Lull. The \"logica parva\" refers to an important textbook of Paul of Venice.\n\nThe terminology had some currency at least until the seventeenth century, and Johannes Clauberg's \"Logica vetus et nova\".\n"}
{"id": "8459839", "url": "https://en.wikipedia.org/wiki?curid=8459839", "title": "Marjorie Lee Browne", "text": "Marjorie Lee Browne\n\nMarjorie Lee Browne (September 9, 1914 – October 19, 1979) was a noted mathematics educator. She was one of the first African-American women to receive a Ph.D in mathematics.\n\nMarjorie Lee Browne was born in Tennessee in 1914. Her mother died when she was only two years old,and she was raised by her stepmother and her father, Lawrence Johnson Lee. Her father, a railway postal clerk, was also a \"math wizard\" who shared his passion for mathematics with his children. Through her daughter Eddi Beatrice Brown she is the grandmother of president of the National Association of Mathematicians Edray Herber Goins. She attended LeMoyne High School, a private Methodist school started after the Civil War to offer education for African Americans. She won the Memphis city women's tennis singles championship while she was in high school.\n\nShe attended Howard University, majoring in mathematics and graduating cum laude in 1935. After receiving her bachelor's degree, she taught high school and college for a short term, including at Gilbert Academy in New Orleans.\n\nShe then applied to the University of Michigan graduate program in mathematics. Michigan accepted African Americans, while many other US educational institutions did not at the time. After working full-time at the historically black Wiley College in Marshall, Texas, and attending Michigan only during the summer, Browne's work paid off and she received a teaching fellowship at Michigan, attending full-time and completing her dissertation in 1949. Her dissertation, \"Studies of One Parameter Subgroups of Certain Topological and Matrix Groups,\" was supervised by George Yuri Rainich. She was one of the first African-American women in the US to earn a doctorate in mathematics, along with Evelyn Boyd Granville, who also earned a Ph.D. in 1949. Euphemia Haynes was the very first African-American woman in the US to earn a doctorate in mathematics, having earned hers in 1943.\n\nAfter receiving her doctorate, Browne was unable to keep a teaching position at a research institution. As a result of this she worked with secondary school mathematics teachers, instructing them in \"modern math.\" She focused especially on encouraging math education for minorities and women.\n\nBrowne then joined the faculty at North Carolina College (now North Carolina Central University (NCCU)), where she taught and researched for thirty years. She was also the head of the department for much of her time at NCCU, from 1951 to 1970. There she worked a principal investigator, coordinator or the mathematics section, and lecturer for the Summer Institute for Secondary School Science and Mathematics Teachers.\n\nBrowne's work on classical groups demonstrated simple proofs of important topological properties of and relations between classical groups. Her work in general focused on linear and matrix algebra.\n\nBrowne saw the importance of computer science early on, writing a $60,000 grant to IBM to bring a computer to NCCU in 1960—one of the first computers in academic computing, and probably the first at a historically black school.\n\nThroughout her career, Browne worked to help gifted mathematics students, educating them and offering them financial support to pursue higher education. Notable students included Joseph Battle, William Fletcher, Asamoah Nkwanta, and Nathan Simms. She established summer institutes to provide continuing education in mathematics for high school teachers. In 1974 she was awarded the first W. W. Rankin Memorial Award from the North Carolina Council of Teachers of Mathematics for her work with mathematics education.\n\nShe was a member of the Women's Research Society, American Mathematical Society, Mathematical Association of America, and the International Congress of Mathematicians.\n\nMarjorie Lee Browne died of a heart attack in Durham, North Carolina, on October 19, 1979. After her death, four of her students established the Marjorie Lee Brown Trust Fund at North Carolina Central University which sponsors the Marjorie Lee Browne Scholarship and the Marjorie Lee Browne Distinguished Alumni Lecture Series. Since 1999, the Mathematics Department at the University of Michigan has hosted the Marjorie Lee Browne Colloquium, which annually brings a speaker \"to present a talk that highlights their research but also addresses the issue of diversity in the sciences.\" \n\n\n\nWhile discrimination against African Americans and women was significant during Browne's early career, she was recognized for her achievements in education and mathematics.\n\n\n\n"}
{"id": "1944319", "url": "https://en.wikipedia.org/wiki?curid=1944319", "title": "Menaechmus", "text": "Menaechmus\n\nMenaechmus (, 380–320 BC) was an ancient Greek mathematician and geometer born in Alopeconnesus in the Thracian Chersonese, who was known for his friendship with the renowned philosopher Plato and for his apparent discovery of conic sections and his solution to the then-long-standing problem of doubling the cube using the parabola and hyperbola.\n\nMenaechmus is remembered by mathematicians for his discovery of the conic sections and his solution to the problem of doubling the cube. Menaechmus likely discovered the conic sections, that is, the ellipse, the parabola, and the hyperbola, as a by-product of his search for the solution to the Delian problem. Menaechmus knew that in a parabola y = \"L\"x, where \"L\" is a constant called the \"latus rectum\", although he was not aware of the fact that any equation in two unknowns determines a curve. He apparently derived these properties of conic sections and others as well. Using this information it was now possible to find a solution to the problem of the duplication of the cube by solving for the points at which two parabolas intersect, a solution equivalent to solving a cubic equation.\n\nThere are few direct sources for Menaechmus's work; his work on conic sections is known primarily from an epigram by Eratosthenes, and the accomplishment of his brother (of devising a method to create a square equal in area to a given circle using the quadratrix), Dinostratus, is known solely from the writings of Proclus. Proclus also mentions that Menaechmus was taught by Eudoxus. There is a curious statement by Plutarch to the effect that Plato disapproved of Menaechmus achieving his doubled cube solution with the use of mechanical devices; the proof currently known appears to be purely algebraic.\n\nMenaechmus was said to have been the tutor of Alexander the Great; this belief derives from the following anecdote: supposedly, once, when Alexander asked him for a shortcut to understanding geometry, he replied \"O King, for traveling over the country, there are royal road and roads for common citizens, but in geometry there is one road for all.\" (Beckmann, \"A History of Pi\", 1989, p. 34) However, this quote is first attested by Stobaeus, about 500 AD, and so whether Menaechmus really taught Alexander is uncertain.\n\nWhere precisely he died is uncertain as well, though modern scholars believe that he eventually expired in Cyzicus.\n\n\n"}
{"id": "1432127", "url": "https://en.wikipedia.org/wiki?curid=1432127", "title": "Mode (statistics)", "text": "Mode (statistics)\n\nThe mode of a set of data values is the value that appears most often. It is the value \"x\" at which its probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled.\n\nLike the statistical mean and median, the mode is a way of expressing, in a (usually) single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions.\n\nThe mode is not necessarily unique to a given discrete distribution, since the probability mass function may take the same maximum value at several points \"x\", \"x\", etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently.\n\nWhen the probability density function of a continuous distribution has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal). A mode of a continuous probability distribution is often considered to be any value \"x\" at which its probability density function has a locally maximum value, so any peak is a mode.\n\nIn symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric unimodal distribution, the sample mean can be used as an estimate of the population mode.\n\nThe mode of a sample is the element that occurs most often in the collection. For example, the mode of the sample [1, 3, 6, 6, 6, 6, 7, 7, 12, 12, 17] is 6. Given the list of data [1, 1, 2, 4, 4] the mode is not unique – the dataset may be said to be bimodal, while a set with more than two modes may be described as multimodal.\n\nFor a sample from a continuous distribution, such as [0.935..., 1.211..., 2.430..., 3.668..., 3.874...], the concept is unusable in its raw form, since no two values will be exactly the same, so each value will occur precisely once. In order to estimate the mode of the underlying distribution, the usual practice is to discretize the data by assigning frequency values to intervals of equal distance, as for making a histogram, effectively replacing the values by the midpoints of the\nintervals they are assigned to. The mode is then the value where the histogram reaches its peak. For small or middle-sized samples the outcome of this procedure is sensitive to the choice of interval width if chosen too narrow or too wide; typically one should have a sizable fraction of the data concentrated in a relatively small number of intervals (5 to 10), while the fraction of the data falling outside these intervals is also sizable. An alternate approach is kernel density estimation, which essentially blurs point samples to produce a continuous estimate of the probability density function which can provide an estimate of the mode.\n\nThe following MATLAB (or Octave) code example computes the mode of a sample:\n\nThe algorithm requires as a first step to sort the sample in ascending order. It then computes the discrete derivative of the sorted list, and finds the indices where this derivative is positive. Next it computes the discrete derivative of this set of indices, locating the maximum of this derivative of indices, and finally evaluates the sorted sample at the point where that maximum occurs, which corresponds to the last member of the stretch of repeated values.\n\nUnlike mean and median, the concept of mode also makes sense for \"nominal data\" (i.e., not consisting of numerical values in the case of mean, or even of ordered values in the case of median). For example, taking a sample of Korean family names, one might find that \"Kim\" occurs more often than any other name. Then \"Kim\" would be the mode of the sample. In any voting system where a plurality determines victory, a single modal value determines the victor, while a multi-modal outcome would require some tie-breaking procedure to take place.\n\nUnlike median, the concept of mode makes sense for any random variable assuming values from a vector space, including the real numbers (a one-dimensional vector space) and the integers (which can be considered embedded in the reals). For example, a distribution of points in the plane will typically have a mean and a mode, but the concept of median does not apply. The median makes sense when there is a linear order on the possible values. Generalizations of the concept of median to higher-dimensional spaces are the geometric median and the centerpoint.\n\nFor some probability distributions, the expected value may be infinite or undefined, but if defined, it is unique. The mean of a (finite) sample is always defined. The median is the value such that the fractions not exceeding it and not falling below it are each at least 1/2. It is not necessarily unique, but never infinite or totally undefined. For a data sample it is the \"halfway\" value when the list of values is ordered in increasing value, where usually for a list of even length the numerical average is taken of the two values closest to \"halfway\". Finally, as said before, the mode is not necessarily unique. Certain pathological distributions (for example, the Cantor distribution) have no defined mode at all. For a finite data sample, the mode is one (or more) of the values in the sample.\n\nAssuming definedness, and for simplicity uniqueness, the following are some of the most interesting properties.\n\nAn example of a skewed distribution is personal wealth: Few people are very rich, but among those some are extremely rich. However, many are rather poor.\nA well-known class of distributions that can be arbitrarily skewed is given by the log-normal distribution. It is obtained by transforming a random variable \"X\" having a normal distribution into random variable \"Y\" = \"e\". Then the logarithm of random variable \"Y\" is normally distributed, hence the name.\n\nTaking the mean μ of \"X\" to be 0, the median of \"Y\" will be 1, independent of the standard deviation σ of \"X\". This is so because \"X\" has a symmetric distribution, so its median is also 0. The transformation from \"X\" to \"Y\" is monotonic, and so we find the median \"e\" = 1 for \"Y\".\n\nWhen \"X\" has standard deviation σ = 0.25, the distribution of \"Y\" is weakly skewed. Using formulas for the log-normal distribution, we find:\nIndeed, the median is about one third on the way from mean to mode.\n\nWhen \"X\" has a larger standard deviation, σ = 1, the distribution of \"Y\" is strongly skewed. Now\nHere, Pearson's rule of thumb fails.\n\nVan Zwet derived an inequality which provides sufficient conditions for this inequality to hold. The inequality\n\nholds if\n\nfor all \"x\" where F() is the cumulative distribution function of the distribution.\n\nIt can be shown for a unimodal distribution that the median formula_4 and the mean formula_5 lie within (3/5) ≈ 0.7746 standard deviations of each other. In symbols,\n\nwhere formula_7 is the absolute value.\n\nA similar relation holds between the median and the mode: they lie within 3 ≈ 1.732 standard deviations of each other:\n\nThe term mode originates with Karl Pearson in 1895.\n\n\n"}
{"id": "18972438", "url": "https://en.wikipedia.org/wiki?curid=18972438", "title": "Naylor Prize and Lectureship", "text": "Naylor Prize and Lectureship\n\nThe Naylor Prize and lectureship in Applied Mathematics is a prize of the London Mathematical Society awarded every two years in memory of Dr V.D. Naylor. Only those who reside in the United Kingdom are eligible for the prize. The \"grounds for award can include work in, and influence on, and contributions to applied mathematics and/or the applications of mathematics, and lecturing gifts.\"\n\n\n"}
{"id": "42012723", "url": "https://en.wikipedia.org/wiki?curid=42012723", "title": "Order-7 heptagrammic tiling", "text": "Order-7 heptagrammic tiling\n\nIn geometry, the order-7 heptagrammic tiling is a tiling of the hyperbolic plane by overlapping heptagrams.\n\nThis tiling is a regular star-tiling, and has Schläfli symbol of {7/2,7}. The heptagrams forming the tiling are of type {7/2}, . The overlapping heptagrams subdivide the hyperbolic plane into isosceles triangles, 14 of which form each heptagram.\n\nEach point of the hyperbolic plane that does not lie on a heptagram edge belongs to the central heptagon of one heptagram, and is in one of the points of exactly one other heptagram. The winding number of each heptagram around its points is one, and the winding number around the central heptagon is two, so adding these two numbers together, each point of the plane is surrounded three times; that is, the density of the tiling is 3.\n\nIn the Euclidean plane, a heptagram of type {7/2} would have angles of 3/7 at its vertices, but in the hyperbolic plane heptagrams can have the sharper vertex angle 2/7 that is needed to make exactly seven other heptagrams meet up at the center of each heptagram of the tiling.\n\nIt has the same vertex arrangement as the regular order-7 triangular tiling, {3,7}. The full set of edges coincide with the edges of a heptakis heptagonal tiling. The valance 6 vertices in this tiling are false-vertices in the heptagrammic one caused by crossed edges.\n\nIt is related to a Kepler-Poinsot polyhedron, the small stellated dodecahedron, {5/2,5}, which is polyhedron and a density-3 regular star-tiling on the sphere:\n\n\n"}
{"id": "194743", "url": "https://en.wikipedia.org/wiki?curid=194743", "title": "Orthonormality", "text": "Orthonormality\n\nIn linear algebra, two vectors in an inner product space are orthonormal if they are orthogonal and unit vectors. A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. An orthonormal set which forms a basis is called an orthonormal basis.\n\nThe construction of orthogonality of vectors is motivated by a desire to extend the intuitive notion of perpendicular vectors to higher-dimensional spaces. In the Cartesian plane, two vectors are said to be \"perpendicular\" if the angle between them is 90° (i.e. if they form a right angle). This definition can be formalized in Cartesian space by defining the dot product and specifying that two vectors in the plane are orthogonal if their dot product is zero.\n\nSimilarly, the construction of the norm of a vector is motivated by a desire to extend the intuitive notion of the length of a vector to higher-dimensional spaces. In Cartesian space, the \"norm\" of a vector is the square root of the vector dotted with itself. That is,\n\nMany important results in linear algebra deal with collections of two or more orthogonal vectors. But often, it is easier to deal with vectors of unit length. That is, it often simplifies things to only consider vectors whose norm equals 1. The notion of restricting orthogonal pairs of vectors to only those of unit length is important enough to be given a special name. Two vectors which are orthogonal and of length 1 are said to be \"orthonormal\".\n\nWhat does a pair of orthonormal vectors in 2-D Euclidean space look like?\n\nLet u = (x, y) and v = (x, y).\nConsider the restrictions on x, x, y, y required to make u and v form an orthonormal pair.\n\n\nExpanding these terms gives 3 equations:\nforms an orthonormal set.\n\nHowever, this is of little consequence, because C[−π,π] is infinite-dimensional, and a finite set of vectors cannot span it. But, removing the restriction that \"n\" be finite makes the set dense in C[−π,π] and therefore an orthonormal basis of C[−π,π].\n\n"}
{"id": "1015401", "url": "https://en.wikipedia.org/wiki?curid=1015401", "title": "Possible Worlds (play)", "text": "Possible Worlds (play)\n\nPossible Worlds is a play written in 1990 by John Mighton. It is part murder mystery, part science-fiction, and part mathematical philosophy and follows the multiple parallel lives of the main character George Barber. At the play's beginning, George is found dead, with his brain missing. Two detectives set out to uncover the truth behind his grisly death, and stumble upon several strange characters.\n\nMighton, a mathematician from University of Toronto's Fields Institute, brought his considerable professional experience to bear on the writing of the play.\n\nThe play bears many conceptual similarities to Tom Stoppard's \"Hapgood\", a play about spies and secret agents that takes place primarily in the men's changingroom of a municipal swimming baths.\n\nA film adaptation of the same name was released in 2000. Directed by Robert Lepage and starring Tom McCamus and Tilda Swinton, it garnered wide critical acclaim, won two Genie Awards, and was nominated for a further four. \n\nThe theatre book was published in 1997 by Playwrights Canada Press.\n\nThe author, John Mighton, is a mathematician and philosopher. His plays tend to meld science, drama and math into one cohesive piece. Possible Worlds won a Governor General's Literary Award for Drama in 1992 alongside Short History of Night.\n\nThe play begins with dectectives Berkley and Williams at a crime scene where a man, identified as George Barber, has been murdered, the top of his head cut off and his brain stolen.\n\nOne of the detective is interviewing a neuroscientist (who we learn at the end of the play is named Pensfield) specializing on research of the nervous system. Pensfield has many brains in jars hooked up to life support and lights in his lab. He tells how a rat can only imagine so much and is limited by the structure of its brain. Creatures like humans that can anticipate possible futures and make contingency plans have an evolutionary advantage, according to him.\n\nIn what seems a flashback, George meets Joyce, a scientist, at a bar. This time he is luckier because she does not flat out reject his advances, rather invites George to call her. Joyce is doing research on how to improve intelligence and her specialization is in rat cortexes. Williams and Berkley are in their office with a rat brain from Pensfield's lab. Williams tells Berkley about a course his wife wants him to take to increase intelligence and imagination.\n\nGeorge is back with Joyce, this time a stockbroker. He mismatches Joyce the stockbroker with Joyce the scientist about where they were born. George also tells Joyce about a dream with the slab and block men. In this dream, there is the Guide who is Pensfield. George and Joyce, now the scientist, are at her apartment filled with the same kind of stones that were in George's dream. George convinces the workaholic Joyce to take time off from work and go to the beach with him.\n\nWilliams is listening to a tape from meditation teacher Jocelyn when Berkley interrupts him. They are visited by the caretaker of George’s Building. He tells them about seeing a UFO the night that George died and fears that “they” will kill him too. Later, they will find him frozen to death without a logical explanation.\n\nJoyce is looking at a picture of the beach while telling George about a demonstration that was being held outside her lab in response to one of colleague’s work. Her college is keeping an ape’s brain alive. George is back with stockbroker Joyce who breaks up with him because she has been seeing someone else. Williams and Berkley talk about Louise, the rat brain. William feels bad for Louise and decides to take the brain back to the scientist because he is the only one who can “help” her.\n\nGeorge meets Joyce the scientist on a beach. She does not know George and she has a boyfriend. George tries to tell her that they were once married and lived together and it freaks her out. Joyce insists that he has her mixed up with someone else and tries to run from George. George attacks Joyce. He is then with a doctor who is Pensfield. George says when he’s dreaming, he sometimes thinks he is falling asleep.\n\nWilliams comes into the office and tells Berkley he has found the missing brains and Pensfield had them all along. He is arrested. Joyce Barber, George’s real life wife, is met by the detectives and told that her husband's brain is alive and producing rudimentary consciousness in a very discontinuous “fluctuating dream state”.\n\nGeorge is again with Joyce on the beach. They see a blinking light in the ocean that goes out shortly afterwards before they can figure out what it was. The light is reminiscent of the light that came on with brain activity, suggesting the real Joyce agreed to terminate George consciousness.\n\nPossible Worlds was written in such a way that it will be relevant to any era that is making advancements in neuroscience. This play opens up your mind to the possibilities of alternate dimensions, social constructs or worlds and then throws it to the ground in the end when all of it was a construct of the imagination. This play explores the human consciousness, morals and scientific advancement all in one, while adding a bit of romance. It is very discontinuous and hard to follow at times and the viewer must pay close attention to pick up the very subtle messages and themes.\n\n\nGeorge is the main character. He is in his 30’s and in his “real life” is married to Joyce. George is a stock broker and is brilliant at math. He discovered at a young age about alternate lives that he could live when he was young doing a math problem, or so he says. In reality all of this is constructed inside George’s brain and never really takes place as far as we can tell. George is the Protagonist of the play although some of his actions throughout the play pose as blocks to what he wants. What he wants is to be loved by Joyce and be in a relationship with Joyce in every world that he encounters. George is also a semi-static character as far as the worlds go. George is always George while other characters appear in way that they would not in the real world. George stays the same through the play so that the viewer can connect with him and feel bad for him at the end. He is the only main “human” character because of how he does not really change from one world to the next.\n\nThis play may be classified as a sci-fi tragic drama\n\nPossible Worlds is a Post Modern Expressionist piece in that neither the relationship with the viewer or the subject is stable. The scenes are disjointed and it disrupts continuity which is what creates that unstable relationship with the viewer which can also put the play under the Style of Modernism. This play has many elements of many styles. At times it is mood driven and we are dealing with the hidden world inside george’s brain which would symbolism. But George is on a quest towards clarity and the setting is inside the main character’s mind, although most of the play we do not know that but nonetheless the play is mostly expressionistic. Possible worlds is also specifically post modern in that the setting varies and is somewhat unpredictable, the world are artificial and the meaning of the play comes with knowing the context.\n\nThe language used in the play is modern but uses very little slang. During the scene with the block and slab men you hear throughout the scene the men saying “block” and “slab”. The Language that is used does not tend to have any elements of euphonic, but symbols tend to be fairly prevalent. Subtle hints in the script are what let us know what is really going on. Without certain lines like when George says, “I’m in a case.” He is not saying figuratively but it sees to be so, but he is literally in a case, his body in a casket and his brain in a jar at a science lab.\n\nPossible Worlds addresses many controversial topics regarding today’s scientific advancements and the moral issues that relate to it. A main overriding theme is human emotions and how they are constructed and what they mean. This theme is there to be questioned not to be answered. Another main theme is that of the human consciousness and how the brain is able to construct it. This play is asking whether we should be allowed to figure out exactly how the brain works, essentially pitting every emotion thought and movement to a function in the brain, essentially turning all the mystery in to the light and making the brain seem like a machine. This is a controversial idea because to many people who study religion this could debunk their religious teachings. Religions tend to heavily rely on the idea of a spirit or a soul and if the brain were demystified then there would most likely be no room for a soul and this could lead to a large rebellion against the church. By keeping the brain human and not a machine it leaves room for a “miracle” or other unexplained happening which leaves room for religion. With the advancements in science we will most likely some day know exactly how the brain works but right now we are far from the answer but get closer all the time. The biggest road block is going to be with human testing because it is seen as immoral but without it the advancements in science may not happen. Whether that is to be allowed is a question up to the moral of the human populous as a whole.\n\nThe biggest Spectacle on stage in this play would be the image of the human and rat brain in glass containers hooked up to wires and lights.\n\nThe play itself does not have any music.\n\n\n\n"}
{"id": "15536340", "url": "https://en.wikipedia.org/wiki?curid=15536340", "title": "Post's lattice", "text": "Post's lattice\n\nIn logic and universal algebra, Post's lattice denotes the lattice of all clones on a two-element set {0, 1}, ordered by inclusion. It is named for Emil Post, who published a complete description of the lattice in 1941. The relative simplicity of Post's lattice is in stark contrast to the lattice of clones on a three-element (or larger) set, which has the cardinality of the continuum, and a complicated inner structure. A modern exposition of Post's result can be found in Lau (2006).\n\nA Boolean function, or logical connective, is an \"n\"-ary operation for some , where 2 denotes the two-element set {0, 1}. Particular Boolean functions are the projections\nand given an \"m\"-ary function \"f\", and \"n\"-ary functions \"g\", ..., \"g\", we can construct another \"n\"-ary function\ncalled their composition. A set of functions closed under composition, and containing all projections, is called a clone.\n\nLet \"B\" be a set of connectives. The functions which can be defined by a formula using propositional variables and connectives from \"B\" form a clone [\"B\"], indeed it is the smallest clone which includes \"B\". We call [\"B\"] the clone \"generated\" by \"B\", and say that \"B\" is the \"basis\" of [\"B\"]. For example, [¬, ∧] are all Boolean functions, and [0, 1, ∧, ∨] are the monotone functions.\n\nWe use the operations ¬, N\"p\", (negation), ∧, K\"pq\", (conjunction or meet), ∨, A\"pq\", (disjunction or join), →, C\"pq\", (implication), ↔, E\"pq\", (biconditional), +, J\"pq\" (exclusive disjunction or Boolean ring addition), ↛, L\"pq\", (nonimplication), ?: (the ternary ) and the constant unary functions 0 and 1. Moreover, we need the threshold functions\nFor example, th is the large disjunction of all the variables \"x\", and th is the large conjunction. Of particular importance is the majority function\n\nWe denote elements of 2 (i.e., truth-assignments) as vectors: . The set 2 carries a natural product Boolean algebra structure. That is, ordering, meets, joins, and other operations on \"n\"-ary truth assignments are defined pointwise:\n\nIntersection of an arbitrary number of clones is again a clone. It is convenient to denote intersection of clones by simple juxtaposition, i.e., the clone is denoted by \"C\"\"C\"...\"C\". Some special clones are introduced below:\n\nThe set of all clones is a closure system, hence it forms a complete lattice. The lattice is countably infinite, and all its members are finitely generated. All the clones are listed in the table below.\n\nThe eight infinite families have actually also members with \"k\" = 1, but these appear separately in the table: , , , , , .\n\nThe lattice has a natural symmetry mapping each clone \"C\" to its dual clone }, where is the de Morgan dual of a Boolean function \"f\". For example, , , and .\n\nThe complete classification of Boolean clones given by Post helps to resolve various questions about classes of Boolean functions. For example:\n\nPost originally did not work with the modern definition of clones, but with the so-called \"iterative systems\", which are sets of operations closed under substitution\nas well as permutation and identification of variables. The main difference is that iterative systems do not necessarily contain all projections. Every clone is an iterative system, and there are 20 non-empty iterative systems which are not clones. (Post also excluded the empty iterative system from the classification, hence his diagram has no least element and fails to be a lattice.) As another alternative, some authors work with the notion of a \"closed class\", which is an iterative system closed under introduction of dummy variables. There are four closed classes which are not clones: the empty set, the set of constant 0 functions, the set of constant 1 functions, and the set of all constant functions.\n\nComposition alone does not allow to generate a nullary function from the corresponding unary constant function, this is the technical reason why nullary functions are excluded from clones in Post's classification. If we lift the restriction, we get more clones. Namely, each clone \"C\" in Post's lattice which contains at least one constant function corresponds to two clones under the less restrictive definition: \"C\", and \"C\" together with all nullary functions whose unary versions are in \"C\".\n"}
{"id": "1359832", "url": "https://en.wikipedia.org/wiki?curid=1359832", "title": "Pregeometry (model theory)", "text": "Pregeometry (model theory)\n\nPregeometry, and in full combinatorial pregeometry, are essentially synonyms for \"matroid\". They were introduced by G.-C. Rota with the intention of providing a less \"ineffably cacophonous\" alternative term. Also, the term combinatorial geometry, sometimes abbreviated to geometry, was intended to replace \"simple matroid\". These terms are now infrequently used in the study of matroids.\n\nIn the branch of mathematical logic called model theory, infinite finitary matroids, there called \"pregeometries\" (and \"geometries\" if they are simple matroids), are used in the discussion of independence phenomena.\n\nIt turns out that many fundamental concepts of linear algebra – closure, independence, subspace, basis, dimension – are preserved in the framework of abstract geometries.\n\nThe study of how pregeometries, geometries, and abstract closure operators influence the structure of first-order models is called geometric stability theory.\n\nA combinatorial pregeometry (also known as a finitary matroid), is a second-order structure: formula_1, where formula_2 (called the closure map) satisfies the following axioms. For all formula_3 and formula_4:\n\n\nA geometry is a pregeometry in which the closure of singletons are singletons and the closure of the empty set is the empty set.\n\nGiven sets formula_15, formula_16 is independent over formula_17 if formula_18 for any formula_19.\n\nA set formula_20 is a basis for formula_16 over formula_17 if it is independent over formula_17 and formula_24.\n\nSince a pregeometry satisfies the Steinitz exchange property all bases are of the same cardinality, hence the definition of the dimension of formula_16 over formula_17 as formula_27 has no ambiguity.\n\nThe sets formula_28 are independent over formula_29 if formula_30 whenever formula_31 is a finite subset of formula_16. Note that this relation is symmetric.\n\nIn minimal sets over stable theories the independence relation coincides with the notion of forking independence.\n\nA geometry automorphism of a geometry formula_33 is a bijection formula_34 such that formula_35 for any formula_36.\n\nA pregeometry formula_33 is said to be homogeneous if for any closed formula_36 and any two elements formula_39 there is an automorphism of formula_33 which maps formula_41 to formula_42 and fixes formula_43 pointwise.\n\nGiven a pregeometry formula_44 its associated geometry (sometimes referred in the literature as the canonical geometry) is the geometry formula_45 where\n\n\nIts easy to see that the associated geometry of a homogeneous pregeometry is homogeneous.\n\nGiven formula_49 the localization of formula_33 is the geometry formula_51 where formula_52.\n\nLet formula_44 be a pregeometry, then it is said to be:\n\n\nTriviality, modularity and local modularity pass to the associated geometry and are preserved under localization.\n\nIf formula_33 is a locally modular homogeneous pregeometry and formula_61 then the localization of formula_33 in formula_42 is modular.\n\nThe geometry formula_33 is modular if and only if whenever formula_65, formula_49, formula_67 and formula_68 then formula_69.\n\nIf formula_33 is any set we may define formula_71. This pregeometry is a trivial, homogeneous, locally finite geometry.\n\nLet formula_72 be a field (a division ring actually suffices) and let formula_73 be a formula_74-dimensional vector space over formula_72. Then formula_73 is a pregeometry where closures of sets are defined to be their span.\n\nThis pregeometry is homogeneous and modular. Vector spaces are considered to be the prototypical example of modularity.\n\nformula_73 is locally finite if and only if formula_72 is finite.\n\nformula_73 is not a geometry, as the closure of any nontrivial vector is a subspace of size at least formula_80.\n\nThe associated geometry of a formula_74-dimensional vector space over formula_72 is the formula_83-dimensional projective space over formula_72. It is easy to see that this pregeometry is a projective geometry.\n\nLet formula_73 be a formula_74-dimensional affine space over a field formula_72. Given a set define its closure to be its affine hull (i.e. the smallest affine subspace containing it).\n\nThis forms a homogeneous formula_88-dimensional geometry.\n\nAn affine space is not modular (for example, if formula_43 and formula_58 be parallel lines then the formula in the definition of modularity fails). However, it is easy to check that all localizations are modular.\n\nLet formula_91 be an algebraically closed field with formula_92, and define the closure of a set to be its algebraic closure.\n\nWhile vector spaces are modular and affine spaces are \"almost\" modular (i.e. everywhere locally modular), algebraically closed fields are examples of the other extremity, not being even locally modular (i.e. none of the localizations is modular).\n\nH.H. Crapo and G.-C. Rota (1970), \"On the Foundations of Combinatorial Theory: Combinatorial Geometries\". M.I.T. Press, Cambridge, Mass.\n\nPillay, Anand (1996), \"Geometric Stability Theory\". Oxford Logic Guides. Oxford University Press.\n"}
{"id": "1842075", "url": "https://en.wikipedia.org/wiki?curid=1842075", "title": "Pro-p group", "text": "Pro-p group\n\nIn mathematics, a pro-\"p\" group (for some prime number \"p\") is a profinite group formula_1 such that for any open normal subgroup formula_2 the quotient group formula_3 is a \"p\"-group. Note that, as profinite groups are compact, the open subgroups are exactly the closed subgroups of finite index, so that the discrete quotient group is always finite.\n\nAlternatively, one can define a pro-\"p\" group to be the inverse limit of an inverse system of discrete finite \"p\"-groups.\n\nThe best-understood (and historically most important) class of pro-\"p\" groups is the \"p\"-adic analytic groups: groups with the structure of an analytic manifold over formula_4 such that group multiplication and inversion are both analytic functions.\nThe work of Lubotzky and Mann, combined with Michel Lazard's solution to Hilbert's fifth problem over the \"p\"-adic numbers, shows that a pro-\"p\" group is \"p\"-adic analytic if and only if it has finite rank, i.e. there exists a positive integer formula_5 such that any closed subgroup has a topological generating set with no more than formula_5 elements.\n\nThe Coclass Theorems have been proved in 1994 by A. Shalev and independently by C. R. Leedham-Green. Theorem D is one of these theorems and asserts that, for any prime number \"p\" and any positive integer \"r\", there exist only finitely many pro-\"p\" groups of coclass \"r\". This finiteness result is fundamental for the classification of finite \"p\"-groups by means of directed coclass graphs.\n\n\n\n"}
{"id": "33452109", "url": "https://en.wikipedia.org/wiki?curid=33452109", "title": "Quasi-relative interior", "text": "Quasi-relative interior\n\nIn topology, a branch of mathematics, the quasi-relative interior of a subset of a vector space is a refinement of the concept of the interior. Formally, if formula_1 is a linear space then the quasi-relative interior of formula_2 is\nwhere formula_4 denotes the closure of the conic hull.\n\nLet formula_1 is a normed vector space, if formula_6 is a convex finite-dimensional set then formula_7 such that formula_8 is the relative interior.\n\n"}
{"id": "20748371", "url": "https://en.wikipedia.org/wiki?curid=20748371", "title": "Real RAM", "text": "Real RAM\n\nIn computing, especially computational geometry, a real RAM (random access machine) is a mathematical model of a computer that can compute with exact real numbers instead of the binary numbers used by most actual computers.\nThe real RAM was formulated by Michael Ian Shamos in his 1978 Ph.D. dissertation.\n\nThe \"RAM\" part of the real RAM model name stands for \"random access machine\". This is a model of computing that resembles a simplified version of a standard computer architecture. It consists of a stored program, a computer memory unit consisting of an array of cells, and a central processing unit with a bounded number of registers. Each memory cell or register can store a real number. Under the control of the program, the real RAM can transfer real numbers between memory and registers, and perform arithmetic operations on the values stored in the registers.\n\nThe allowed operations typically include addition, subtraction, multiplication, and division, as well as comparisons, but not modulus or rounding to integers. The reason for avoiding integer rounding and modulus operations is that allowing these operations could give the real RAM unreasonable amounts of computational power, enabling it to solve PSPACE-complete problems in polynomial time.\n\nWhen analyzing algorithms for the real RAM, each allowed operation is typically assumed to take constant time.\n\nSoftware libraries such as LEDA have been developed which allow programmers to write computer programs that work as if they were running on a real RAM.\nThese libraries represent real values using data structures which allow them to perform arithmetic and comparisons with the same results as a real RAM would produce. The time analysis of the underlying real RAM algorithm\ncan be interpreted as counting the number of library calls needed by a given algorithm.\n\nThe real RAM closely resembles the later Blum–Shub–Smale machine, which however lacks the memory unit that gives the real RAM the \"RAM\" part of its name. However, the real RAM is typically used for the analysis of concrete algorithms in computational geometry, while the Blum–Shub–Smale machine instead forms the basis for extensions of the theory of NP-completeness to real-number computation.\n\nAn alternative to the real RAM is the word RAM, in which both the inputs to a problem and the values stored in memory and registers are assumed to be integers with a fixed number of bits. The word RAM model can perform some operations more quickly than the real RAM; for instance, it allows fast integer sorting algorithms, while sorting on the real RAM must be done with slower comparison sorting algorithms. However, some computational geometry problems have inputs or outputs that cannot be represented exactly using integer coordinates; see for instance the Perles configuration, an arrangement of points and line segments that has no integer-coordinate representation.\n\n"}
{"id": "19667704", "url": "https://en.wikipedia.org/wiki?curid=19667704", "title": "Rogers–Ramanujan continued fraction", "text": "Rogers–Ramanujan continued fraction\n\nThe Rogers–Ramanujan continued fraction is a continued fraction discovered by and independently by Srinivasa Ramanujan, and closely related to the Rogers–Ramanujan identities. It can be evaluated explicitly for a broad class of values of its argument.\n\nGiven the functions \"G\"(\"q\") and \"H\"(\"q\") appearing in the Rogers–Ramanujan identities,\n\nand,\n\nIf formula_5, then formula_6 and formula_7, as well as their quotient formula_8, are modular functions of formula_9. Since they have integral coefficients, the theory of complex multiplication implies that their values for formula_9 an imaginary quadratic irrational are algebraic numbers that can be evaluated explicitly.\n\nwhere formula_13 is the golden ratio.\n\nIt can be related to the Dedekind eta function, a modular form of weight 1/2, as,\n\nAmong the many formulas of the j-function, one is,\n\nwhere\n\nEliminating the eta quotient, one can then express \"j\"(\"τ\") in terms of formula_18 as,\n\nwhere the numerator and denominator are polynomial invariants of the icosahedron. Using the modular equation between formula_8 and formula_21, one finds that,\n\nlet formula_23,thenformula_24\n\nwhere\n\nwhich in fact is the j-invariant of the elliptic curve,\n\nparameterized by the non-cusp points of the modular curve formula_27.\n\nFor convenience, one can also use the notation formula_28 when \"q\" = e. While other modular functions like the j-invariant satisfies,\n\nand the Dedekind eta function has,\n\nthe functional equation of the Rogers–Ramanujan continued fraction involves the golden ratio formula_31,\n\nIncidentally,\n\nThere are modular equations between formula_8 and formula_35. Elegant ones for small prime \"n\" are as follows.\n\nFor formula_36, let formula_37 and formula_38, then formula_39\n\nFor formula_40, let formula_37 and formula_42, then formula_43\n\nFor formula_44, let formula_37 and formula_46, then formula_47\n\nFor formula_48, let formula_37 and formula_50, then formula_51\n\nRegarding formula_44, note that\n\nRamanujan found many other interesting results regarding \"R\"(\"q\"). Let formula_54, formula_55, and formula_31 as the golden ratio.\n\nThe powers of \"R\"(\"q\") also can be expressed in unusual ways. For its cube,\n\nwhere,\n\nFor its fifth power, let formula_64, then,\n\n\n"}
{"id": "8984861", "url": "https://en.wikipedia.org/wiki?curid=8984861", "title": "Spectral set", "text": "Spectral set\n\nIn operator theory, a set formula_1 is said to be a spectral set for a (possibly unbounded) linear operator formula_2 on a Banach space if the spectrum of formula_2 is in formula_4 and von-Neumann's inequality holds for formula_2 on formula_4 - i.e. for all rational functions formula_7 with no poles on formula_4\n\nThis concept is related to the topic of analytic functional calculus\nof operators. In general, one wants to get more details about the operators constructed from functions with the original operator as the variable.\n"}
{"id": "368621", "url": "https://en.wikipedia.org/wiki?curid=368621", "title": "Sphere packing", "text": "Sphere packing\n\nIn geometry, a sphere packing is an arrangement of non-overlapping spheres within a containing space. The spheres considered are usually all of identical size, and the space is usually three-dimensional Euclidean space. However, sphere packing problems can be generalised to consider unequal spheres, \"n\"-dimensional Euclidean space (where the problem becomes circle packing in two dimensions, or hypersphere packing in higher dimensions) or to non-Euclidean spaces such as hyperbolic space.\n\nA typical sphere packing problem is to find an arrangement in which the spheres fill as large a proportion of the space as possible. The proportion of space filled by the spheres is called the density of the arrangement. As the local density of a packing in an infinite space can vary depending on the volume over which it is measured, the problem is usually to maximise the average or asymptotic density, measured over a large enough volume.\n\nFor equal spheres in three dimensions the densest packing uses approximately 74% of the volume. A random packing of equal spheres generally has a density around 64%.\n\nA lattice arrangement (commonly called a regular arrangement) is one in which the centers of the spheres form a very symmetric pattern which needs only \"n\" vectors to be uniquely defined (in \"n\"-dimensional Euclidean space). Lattice arrangements are periodic. Arrangements in which the spheres do not form a lattice (often referred to as irregular) can still be periodic, but also aperiodic (properly speaking non-periodic) or random. Lattice arrangements are easier to handle than irregular ones—their high degree of symmetry makes it easier to classify them and to measure their densities.\n\nIn three-dimensional Euclidean space, the densest packing of equal spheres is achieved by a family of structures called close-packed structures. One method for generating such a structure is as follows. Consider a plane with a compact arrangement of spheres on it. For any three neighbouring spheres, a fourth sphere can be placed on top in the hollow between the three bottom spheres. If we do this \"everywhere\" in a second plane above the first, we create a new compact layer. A third layer can be placed directly above the first one, or the spheres can be offset, vertically above another set of hollows of the first layer. There are thus three types of planes, called A, B and C.\n\nTwo simple arrangements within the close-packed family correspond to regular lattices. One is called cubic close packing (or face centred cubic, \"FCC\")—where the layers are alternated in the ABCABC... sequence. The other is called hexagonal close packing (\"HCP\")—where the layers are alternated in the ABAB... sequence. But many layer stacking sequences are possible (ABAC, ABCBA, ABCBAC, etc.), and still generate a close-packed structure. In all of these arrangements each sphere is surrounded by 12 other spheres, and the average density is \n\nCarl Friedrich Gauss proved in 1831 that these packings have the highest density amongst all possible lattice packings.\n\nIn 1611 Johannes Kepler had conjectured that this is the maximum possible density amongst both regular and irregular arrangements—this became known as the Kepler conjecture. In 1998, Thomas Callister Hales, following the approach suggested by László Fejes Tóth in 1953, announced a proof of the Kepler conjecture. Hales' proof is a proof by exhaustion involving checking of many individual cases using complex computer calculations. Referees said that they were \"99% certain\" of the correctness of Hales' proof. On 10 August 2014 Hales announced the completion of a formal proof using automated proof checking, removing any doubt.\n\nSome other lattice packings are often found in physical systems. These include the cubic lattice with a density of formula_2, the hexagonal lattice with a density of formula_3 and the tetrahedral lattice with a density of formula_4, and loosest possible at a density of 0.0555.\n\nPackings where all spheres are constrained by their neighbours to stay in one location are called rigid or jammed. The strictly jammed sphere packing with the lowest density is a diluted (\"tunneled\") fcc crystal with a density of only 0.49365.\n\nIf we attempt to build a densely packed collection of spheres, we will be tempted to always place the next sphere in a hollow between three packed spheres. If five spheres are assembled in this way, they will be consistent with one of the regularly packed arrangements described above. However, the sixth sphere placed in this way will render the structure inconsistent with any regular arrangement. This results in the possibility of a \"random close packing\" of spheres which is stable against compression.\n\nWhen spheres are randomly added to a container and then compressed, they will generally form what is known as an \"irregular\" or \"jammed\" packing configuration when they can be compressed no more. This irregular packing will generally have a density of about 64%. Recent research predicts analytically that it cannot exceed a density limit of 63.4% This situation is unlike the case of one or two dimensions, where compressing a collection of 1-dimensional or 2-dimensional spheres (that is, line segments or circles) will yield a regular packing.\n\nThe sphere packing problem is the three-dimensional version of a class of ball-packing problems in arbitrary dimensions. In two dimensions, the equivalent problem is packing circles on a plane. In one dimension it is packing line segments into a linear universe.\n\nIn dimensions higher than three, the densest regular packings of hyperspheres are known up to 8 dimensions. Very little is known about irregular hypersphere packings; it is possible that in some dimensions the densest packing may be irregular. Some support for this conjecture comes from the fact that in certain dimensions (e.g. 10) the densest known irregular packing is denser than the densest known regular packing.\n\nIn 2016, Maryna Viazovska announced a proof that the E lattice provides the optimal packing (regardless of regularity) in eight-dimensional space, and soon afterwards she and a group of collaborators announced a similar proof that the Leech lattice is optimal in 24 dimensions. This result built on and improved previous methods which showed that these two lattices are very close to optimal.\nThe new proofs involve using the Laplace transform of a carefully-chosen modular function to construct a radially-symmetric function such that and its Fourier transform both equal one at the origin, and both vanish at all other points of the optimal lattice, with negative outside the central sphere of the packing and positive. Then, the Poisson summation formula for is used to compare the density of the optimal lattice with that of any other packing. Before the proof had been formally refereed and published, mathematician Peter Sarnak called the proof \"stunningly simple\" and wrote that \"You just start reading the paper and you know this is correct.\"\n\nAnother line of research in high dimensions is trying to find asymptotic bounds for the density of the densest packings. Currently the best known result is that there exists a lattice in dimension with density bigger or equal to for some number .\n\nMany problems in the chemical and physical sciences can be related to packing problems where more than one size of sphere is available. Here there is a choice between separating the spheres into regions of close-packed equal spheres, or combining the multiple sizes of spheres into a compound or interstitial packing. When many sizes of spheres (or a distribution) are available, the problem quickly becomes intractable, but some studies of binary hard spheres (two sizes) are available.\n\nWhen the second sphere is much smaller than the first, it is possible to arrange the large spheres in a close-packed arrangement, and then arrange the small spheres within the octahedral and tetrahedral gaps. The density of this interstitial packing depends sensitively on the radius ratio, but in the limit of extreme size ratios, the smaller spheres can fill the gaps with the same density as the larger spheres filled space. Even if the large spheres are not in a close-packed arrangement, it is always possible to insert some smaller spheres of up to 0.29099 of the radius of the larger sphere.\n\nWhen the smaller sphere has a radius greater than 0.41421 of the radius of the larger sphere, it is no longer possible to fit into even the octahedral holes of the close-packed structure. Thus, beyond this point, either the host structure must expand to accommodate the interstitials (which compromises the overall density), or rearrange into a more complex crystalline compound structure. Structures are known which exceed the close packing density for radius ratios up to 0.659786.\n\nUpper bounds for the density that can be obtained in such binary packings have also been obtained.\n\nIn many chemical situations such as ionic crystals, the stoichiometry is constrained by the charges of the constituent ions. This additional constraint on the packing, together with the need to minimize the Coulomb energy of interacting charges leads to a diversity of optimal packing arrangements.\n\nAlthough the concept of circles and spheres can be extended to hyperbolic space, finding the densest packing becomes much more difficult. In a hyperbolic space there is no limit to the number of spheres that can surround another sphere (for example, Ford circles can be thought of as an arrangement of identical hyperbolic circles in which each circle is surrounded by an infinite number of other circles). The concept of average density also becomes much more difficult to define accurately. The densest packings in any hyperbolic space are almost always irregular.\n\nDespite this difficulty, K. Böröczky gives a universal upper bound for the density of sphere packings of hyperbolic \"n\"-space where \"n\" ≥ 2. In three dimensions the Böröczky bound is approximately 85.327613%, and is realized by the horosphere packing of the order-6 tetrahedral honeycomb with Schläfli symbol {3,3,6}. In addition to this configuration at least three other horosphere packings are known to exist in hyperbolic 3-space that realize the density upper bound.\n\nThe contact graph of an arbitrary finite packing of unit balls is the graph whose vertices correspond to the packing elements and whose two vertices are connected by an edge if the corresponding two packing elements touch each other. The cardinality of the edge set of the contact graph gives the number of touching pairs, the number of 3-cycles in the contact graph gives the number of touching triplets, and the number of tetrahedrons in the contact graph gives the number of touching quadruples (in general for a contact graph associated with a sphere packing in \"n\" dimensions that the cardinality of the set of \"n\"-simplices in the contact graph gives the number of touching (\"n\" + 1)-tuples in the sphere packing). In the case of 3-dimensional Euclidean space, non-trivial upper bounds on the number of touching pairs, triplets, and quadruples were proved by Karoly Bezdek and Samuel Reid at the University of Calgary.\n\nSphere packing on the corners of a hypercube (with the spheres defined by Hamming distance) corresponds to designing error-correcting codes: if the spheres have radius \"t\", then their centers are codewords of a (2\"t\" + 1)-error-correcting code. Lattice packings correspond to linear codes. There are other, subtler relationships between Euclidean sphere packing and error-correcting codes. For example, the binary Golay code is closely related to the 24-dimensional Leech lattice.\n\nFor further details on these connections, see the book \"Sphere Packings, Lattices and Groups\" by Conway and Sloane.\n\n\n\n"}
{"id": "2352847", "url": "https://en.wikipedia.org/wiki?curid=2352847", "title": "Strategic dominance", "text": "Strategic dominance\n\nIn game theory, strategic dominance (commonly called simply dominance) occurs when one strategy is better than another strategy for one player, no matter how that player's opponents may play. Many simple games can be solved using dominance.\nThe opposite, intransitivity, occurs in games where one strategy may be better or worse than another strategy for one player, depending on how the player's opponents may play.\nWhen a player tries to choose the \"best\" strategy among a multitude of options, that player may compare two strategies A and B to see which one is better.\nThe result of the comparison is one of:\nThis notion can be generalized beyond the comparison of two strategies.\n\nStrategy: A complete contingent plan for a player in the game. A complete contingent plan is a full specification of a player's behavior, describing each action a player would take at every possible decision point. Because information sets represent points in a game where a player must make a decision, a player's strategy describes what that player will do at each information set.\n\nRationality: The assumption that each player acts to in a way that is designed to bring about what she most prefers given probabilities of various outcomes; von Neumann and Morgenstern showed that if these preferences satisfy certain conditions, this is mathematically equivalent to maximizing a payoff. A straightforward example of maximizing payoff is that of monetary gain, but for the purpose of a game theory analysis, this payoff can take any form. Be it a cash reward, minimization of exertion or discomfort, promoting justice, spread of ones genes, or amassing overall “utility” - the assumption of rationality states that\nplayers will always act in the way that best satisfies their ordering from best to worst of various possible outcomes.\n\nCommon Knowledge: The assumption that each player has knowledge of the game, knows the rules and payoffs associated with each course of action, and realizes that every other player has this same level of understanding. This is the premise that allows a player to make a value judgment on the actions of another player, backed by the assumption of rationality, into\nconsideration when selecting an action.\n\nIf a strictly dominant strategy exists for one player in a game, that player will play that strategy in each of the game's Nash equilibria. If both players have a strictly dominant strategy, the game has only one unique Nash equilibrium. However, that Nash equilibrium is not necessarily \"efficient\", meaning that there may be non-equilibrium outcomes of the game that would be better for both players. The classic game used to illustrate this is the Prisoner's Dilemma.\n\nStrictly dominated strategies cannot be a part of a Nash equilibrium, and as such, it is irrational for any player to play them. On the other hand, weakly dominated strategies may be part of Nash equilibria. For instance, consider the payoff matrix pictured at the right.\n\nStrategy \"C\" weakly dominates strategy \"D.\" Consider playing \"C\": If one's opponent plays \"C,\" one gets 1; if one's opponent plays \"D,\" one gets 0. Compare this to \"D,\" where one gets 0 regardless. Since in one case, one does better by playing \"C\" instead of \"D\" and never does worse, \"C\" weakly dominates \"D\". Despite this, is a Nash equilibrium. Suppose both players choose \"D\". Neither player will do any better by unilaterally deviating—if a player switches to playing \"C,\" they will still get 0. This satisfies the requirements of a Nash equilibrium. Suppose both players choose C. Neither player will do better by unilaterally deviating—if a player switches to playing D, they will get 0. This also satisfies the requirements of a Nash equilibrium.\n\nThe iterated elimination (or deletion) of dominated strategies is one common technique for solving games that involves iteratively removing dominated strategies. In the first step, at most one dominated strategy is removed from the strategy space of each of the players since no rational player would ever play these strategies. This results in a new, smaller game. Some strategies—that were not dominated before—may be dominated in the smaller game. The first step is repeated, creating a new even smaller game, and so on. The process stops when no dominated strategy is found for any player. This process is valid since it is assumed that rationality among players is common knowledge, that is, each player knows that the rest of the players are rational, and each player knows that the rest of the players know that he knows that the rest of the players are rational, and so on ad infinitum (see Aumann, 1976).\n\nThere are two versions of this process. One version involves only eliminating strictly dominated strategies. If, after completing this process, there is only one strategy for each player remaining, that strategy set is the unique Nash equilibrium.\n\nAnother version involves eliminating both strictly and weakly dominated strategies. If, at the end of the process, there is a single strategy for each player, this strategy set is also a Nash equilibrium. However, unlike the first process, elimination of weakly dominated strategies may eliminate some Nash equilibria. As a result, the Nash equilibrium found by eliminating weakly dominated strategies may not be the \"only\" Nash equilibrium. (In some games, if we remove weakly dominated strategies in a different order, we may end up with a different Nash equilibrium.)\n\nIn any case, if by iterated elimination of dominated strategies there is only one strategy left for each player, the game is called a dominance-solvable game.\n\n\n"}
{"id": "27873", "url": "https://en.wikipedia.org/wiki?curid=27873", "title": "Surjective function", "text": "Surjective function\n\nIn mathematics, a function \"f\" from a set \"X\" to a set \"Y\" is surjective (or onto), or a surjection, if for every element \"y\" in the codomain \"Y\" of \"f\" there is at least one element \"x\" in the domain \"X\" of \"f\" such that \"f\"(\"x\") = \"y\". It is not required that \"x\" be unique; the function \"f\" may map one or more elements of \"X\" to the same element of \"Y\".\n\nThe term \"surjective\" and the related terms \"injective\" and \"bijective\" were introduced by Nicolas Bourbaki, a group of mainly French 20th-century mathematicians who under this pseudonym wrote a series of books presenting an exposition of modern advanced mathematics, beginning in 1935. The French word \"sur\" means \"over\" or \"above\" and relates to the fact that the image of the domain of a surjective function completely covers the function's codomain.\n\nAny function induces a surjection by restricting its codomain to its range. Every surjective function has a right inverse, and every function with a right inverse is necessarily a surjection. The composite of surjective functions is always surjective. Any function can be decomposed into a surjection and an injection.\n\nA surjective function is a function whose image is equal to its codomain. Equivalently, a function \"f\" with domain \"X\" and codomain \"Y\" is surjective if for every \"y\" in \"Y\" there exists at least one \"x\" in \"X\" with formula_1. Surjections are sometimes denoted by a two-headed rightwards arrow (), as in \"f\" : \"X\" ↠ \"Y\".\n\nSymbolically,\n\nFor any set \"X\", the identity function id on \"X\" is surjective.\n\nThe function defined by \"f\"(\"n\") = \"n\" mod 2 (that is, even integers are mapped to 0 and odd integers to 1) is surjective.\n\nThe function defined by \"f\"(\"x\") = 2\"x\" + 1 is surjective (and even bijective), because for every real number \"y\" we have an \"x\" such that \"f\"(\"x\") = \"y\": an appropriate \"x\" is (\"y\" − 1)/2.\n\nThe function defined by \"f\"(\"x\") = \"x\" − 3\"x\" is surjective, because the pre-image of any real number \"y\" is the solution set of the cubic polynomial equation \"x\" − 3\"x\" − \"y\" = 0 and every cubic polynomial with real coefficients has at least one real root. However, this function is not injective (and hence not bijective) since e.g. the pre-image of \"y\" = 2 is {\"x\" = −1, \"x\" = 2}. (In fact, the pre-image of this function for every \"y\", −2 ≤ \"y\" ≤ 2 has more than one element.)\n\nThe function defined by \"g\"(\"x\") = \"x\" is \"not\" surjective, because there is no real number \"x\" such that \"x\" = −1. However, the function defined by \"g\"(\"x\") = \"x\" (with restricted codomain) \"is\" surjective because for every \"y\" in the nonnegative real codomain \"Y\" there is at least one \"x\" in the real domain \"X\" such that \"x\" = \"y\".\n\nThe natural logarithm function is a surjective and even bijective mapping from the set of positive real numbers to the set of all real numbers. Its inverse, the exponential function, is not surjective as its range is the set of positive real numbers, and its domain is usually defined to be the set of all real numbers. The matrix exponential is not surjective when seen as a map from the space of all \"n\"×\"n\" matrices to itself. It is, however, usually defined as a map from the space of all \"n\"×\"n\" matrices to the general linear group of degree \"n\", i.e. the group of all \"n\"×\"n\" invertible matrices. Under this definition the matrix exponential is surjective for complex matrices, although still not surjective for real matrices.\n\nThe projection from a cartesian product to one of its factors is surjective unless the other factor is empty.\n\nIn a 3D video game, vectors are projected onto a 2D flat screen by means of a surjective function.\n\nA function is bijective if and only if it is both surjective and injective.\n\nIf (as is often done) a function is identified with its graph, then surjectivity is not a property of the function itself, but rather a property of the mapping. This is, the function together with its codomain. Unlike injectivity, surjectivity cannot be read off of the graph of the function alone.\n\nThe function is said to be a right inverse of the function if \"f\"(\"g\"(\"y\")) = \"y\" for every \"y\" in \"Y\" (\"g\" can be undone by \"f\"). In other words, \"g\" is a right inverse of \"f\" if the composition of \"g\" and \"f\" in that order is the identity function on the domain \"Y\" of \"g\". The function \"g\" need not be a complete inverse of \"f\" because the composition in the other order, , may not be the identity function on the domain \"X\" of \"f\". In other words, \"f\" can undo or \"reverse\" \"g\", but cannot necessarily be reversed by it.\n\nEvery function with a right inverse is necessarily a surjection. The proposition that every surjective function has a right inverse is equivalent to the axiom of choice.\n\nIf is surjective and \"B\" is a subset of \"Y\", then \"f\"(\"f\"(\"B\")) = \"B\". Thus, \"B\" can be recovered from its preimage .\n\nFor example, in the first illustration, above, there is some function \"g\" such that \"g\"(\"C\") = 4. There is also some function \"f\" such that \"f\"(4) = \"C\". It doesn't matter that \"g\"(\"C\") can also equal 3; it only matters that \"f\" \"reverses\" \"g\".\n\nA function is surjective if and only if it is right-cancellative: given any functions , whenever \"g\" \"f\" = \"h\" \"f\", then \"g\" = \"h\". This property is formulated in terms of functions and their composition and can be generalized to the more general notion of the morphisms of a category and their composition. Right-cancellative morphisms are called epimorphisms. Specifically, surjective functions are precisely the epimorphisms in the category of sets. The prefix \"epi\" is derived from the Greek preposition \"ἐπί\" meaning \"over\", \"above\", \"on\".\n\nAny morphism with a right inverse is an epimorphism, but the converse is not true in general. A right inverse \"g\" of a morphism \"f\" is called a section of \"f\". A morphism with a right inverse is called a split epimorphism.\n\nAny function with domain \"X\" and codomain \"Y\" can be seen as a left-total and right-unique binary relation between \"X\" and \"Y\" by identifying it with its function graph. A surjective function with domain \"X\" and codomain \"Y\" is then a binary relation between \"X\" and \"Y\" that is right-unique and both left-total and right-total.\n\nThe cardinality of the domain of a surjective function is greater than or equal to the cardinality of its codomain: If is a surjective function, then \"X\" has at least as many elements as \"Y\", in the sense of cardinal numbers. (The proof appeals to the axiom of choice to show that a function\n\nSpecifically, if both \"X\" and \"Y\" are finite with the same number of elements, then is surjective if and only if \"f\" is injective.\n\nGiven two sets \"X\" and \"Y\", the notation is used to say that either \"X\" is empty or that there is a surjection from \"Y\" onto \"X\". Using the axiom of choice one can show that and together imply that |\"Y\"| = |\"X\"|, a variant of the Schröder–Bernstein theorem.\n\nThe composite of surjective functions is always surjective: If \"f\" and \"g\" are both surjective, and the codomain of \"g\" is equal to the domain of \"f\", then is surjective. Conversely, if is surjective, then \"f\" is surjective (but \"g\", the function applied first, need not be). These properties generalize from surjections in the category of sets to any epimorphisms in any category.\n\nAny function can be decomposed into a surjection and an injection: For any function there exist a surjection and an injection such that \"h\" = \"g\" \"f\". To see this, define \"Y\" to be the set of preimages where \"z\" is in . These preimages are disjoint and partition \"X\". Then \"f\" carries each \"x\" to the element of \"Y\" which contains it, and \"g\" carries each element of \"Y\" to the point in \"Z\" to which \"h\" sends its points. Then \"f\" is surjective since it is a projection map, and \"g\" is injective by definition.\n\nAny function induces a surjection by restricting its codomain to its range. Any surjective function induces a bijection defined on a quotient of its domain by collapsing all arguments mapping to a given fixed image. More precisely, every surjection can be factored as a projection followed by a bijection as follows. Let \"A\"/~ be the equivalence classes of \"A\" under the following equivalence relation: \"x\" ~ \"y\" if and only if \"f\"(\"x\") = \"f\"(\"y\"). Equivalently, \"A\"/~ is the set of all preimages under \"f\". Let \"P\"(~) : \"A\" → \"A\"/~ be the projection map which sends each \"x\" in \"A\" to its equivalence class [\"x\"], and let \"f\" : \"A\"/~ → \"B\" be the well-defined function given by \"f\"([\"x\"]) = \"f\"(\"x\"). Then \"f\" = \"f\" o \"P\"(~).\n\n"}
