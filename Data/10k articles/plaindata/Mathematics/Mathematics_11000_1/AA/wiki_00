{"id": "407357", "url": "https://en.wikipedia.org/wiki?curid=407357", "title": "102 (number)", "text": "102 (number)\n\n102 (one hundred [and] two) is the natural number following 101 and preceding 103.\n\n102 is an abundant number and semiperfect number. It is a sphenic number. It is the sum of four consecutive prime numbers (19 + 23 + 29 + 31).\n\nThe sum of Euler's totient function φ(\"x\") over the first eighteen integers is 102.\n\n102 is the third base 10 polydivisible number, since 1 is divisible by 1, 10 is divisible by 2 and 102 is divisible by 3. This also shows that 102 is a Harshad number. 102 is the first 3-digit number divisible by the numbers 3, 6, 17, 34 and 51.\n\n\n102 is also:\n\n\n\n"}
{"id": "3795398", "url": "https://en.wikipedia.org/wiki?curid=3795398", "title": "192 (number)", "text": "192 (number)\n\n192 (one hundred [and] ninety-two) is the natural number following 191 and preceding 193.\n\n\n\n\n\n\n\n\n\n 192 is also:\n\n\n"}
{"id": "25860534", "url": "https://en.wikipedia.org/wiki?curid=25860534", "title": "3D pose estimation", "text": "3D pose estimation\n\n3D pose estimation is the problem of determining the transformation of an object in a 2D image which gives the 3D object. The need for 3D pose estimation arises from the limitations of feature based pose estimation. There exist environments where it is difficult to extract corners or edges from an image. To circumvent these issues, the object is dealt with as a whole through the use of free-form contours.\n\nIt is possible to estimate the 3D rotation and translation of a 3D object from a single 2D photo, if an approximate 3D model of the object is known and the corresponding points in the 2D image are known. A common technique for solving this has recently been \"POSIT\", where the 3D pose is estimated directly from the 3D model points and the 2D image points, and corrects the errors iteratively until a good estimate is found from a single image. Most implementations of POSIT only work on non-coplanar points (in other words, it won't work with flat objects or planes).\n\nAnother approach is to register a 3D CAD model over the photograph of a known object by optimizing a suitable distance measure with respect to the pose parameters.\nThe distance measure is computed between the object in the photograph and the 3D CAD model projection at a given pose.\nPerspective projection or orthogonal projection is possible depending on the pose representation used.\nThis approach is appropriate for applications where a 3D CAD model of a known object (or object category) is available.\n\nGiven a 2D image of an object, and the camera that is calibrated with respect to a world coordinate system, it is also possible to find the pose which gives the 3D object in its object coordinate system. This works as follows.\n\nStarting with a 2D image, image points are extracted which correspond to corners in an image. The projection rays from the image points are reconstructed from the 2D points so that the 3D points, which must be incident with the reconstructed rays, can be determined.\n\nThe algorithm for determining pose estimation is based on the iterative closest point algorithm. The main idea is to determine the correspondences between 2D image features and points on the 3D model curve. \ncodice_1\n\nThe above algorithm does not account for images containing an object that is partially occluded. The following algorithm assumes that all contours are rigidly coupled, meaning the pose of one contour defines the pose of another contour.\n\ncodice_2\n\nIn practice, using a 2 GHz Intel Pentium processor, average speeds of 29fps have been reached using the above algorithm.\n\nSystems exist which use a database of an object at different rotations and translations to compare an input image against to estimate pose. These systems accuracy is limited to situations which are represented in their database of images, however the goal is to recognize a pose, rather than determine it.\n\n\n\n"}
{"id": "7427109", "url": "https://en.wikipedia.org/wiki?curid=7427109", "title": "A Metric America", "text": "A Metric America\n\nA Metric America: A Decision Whose Time Has Come was a 1971 book by the United States National Bureau of Standards (now the National Institute of Standards and Technology) printed by the Government Printing Office.\n\nIn 1968, in the Metric Study Act (Pub. L. 90-472, August 9, 1968, 82 Stat. 693), Congress authorized a three-year study of systems of measurement in the U.S., with particular emphasis on the feasibility of adopting the SI. This detailed U.S. Metric Study was conducted by the Department of Commerce. A 45-member advisory panel consulted with and took testimony from hundreds of consumers, business organizations, labor groups, manufacturers, and state and local officials.\n\nA Metric America: \"A Decision Whose Time Has Come\" – For Real (NISTIR 4858) was a June 1992 followup to this book.\n\n\n"}
{"id": "60476", "url": "https://en.wikipedia.org/wiki?curid=60476", "title": "Augmented Backus–Naur form", "text": "Augmented Backus–Naur form\n\nIn computer science, augmented Backus–Naur form (ABNF) is a metalanguage based on Backus–Naur form (BNF), but consisting of its own syntax and derivation rules. The motive principle for ABNF is to describe a formal system of a language to be used as a bidirectional communications protocol. It is defined by \"Internet Standard 68\" (\"STD 68\", type case sic), which is , and it often serves as the definition language for IETF communication protocols.\n\nAn ABNF specification is a set of derivation rules, written as\n\nwhere rule is a case-insensitive nonterminal, the definition consists of sequences of symbols that define the rule, a comment for documentation, and ending with a carriage return and line feed.\n\nRule names are case-insensitive: codice_1, codice_2, codice_3, and codice_4 all refer to the same rule. Rule names consist of a letter followed by letters, numbers, and hyphens.\n\nAngle brackets (codice_5, codice_6) are not required around rule names (as they are in BNF). However, they may be used to delimit a rule name when used in prose to discern a rule name.\n\nTerminals are specified by one or more numeric characters.\n\nNumeric characters may be specified as the percent sign codice_7, followed by the base (codice_8 = binary, codice_9 = decimal, and codice_10 = hexadecimal), followed by the value, or concatenation of values (indicated by codice_11). For example, a carriage return is specified by codice_12 in decimal or codice_13 in hexadecimal. A carriage return followed by a line feed may be specified with concatenation as codice_14.\n\nLiteral text is specified through the use of a string enclosed in quotation marks (codice_15). These strings are case-insensitive, and the character set used is (US-)ASCII. Therefore, the string codice_16 will match “abc”, “Abc”, “aBc”, “abC”, “ABc”, “AbC”, “aBC”, and “ABC”. RFC 7405 added a syntax for case-sensitive strings: codice_17 will only match \"aBc\". Prior to that, a case-sensitive string could only be specified by listing the individual characters: to match “aBc”, the definition would be codice_18. A string can also be explicitly specified as case-insensitive with a codice_19 prefix.\n\nWhite space is used to separate elements of a definition; for space to be recognized as a delimiter, it must be explicitly included. The explicit reference for a single whitespace character is codice_20 (linear white space), and codice_21 is for zero or more whitespace characters with newlines permitted. The codice_21 definition in RFC5234 is controversial because at least one whitespace character is needed to form a delimiter between two fields.\n\nDefinitions are left-aligned. When multiple lines are required (for readability), continuation lines are indented by whitespace.\n\ncodice_23\n\nA semicolon (codice_24) starts a comment that continues to the end of the line.\n\ncodice_25\n\nA rule may be defined by listing a sequence of rule names.\n\nTo match the string “aba”, the following rules could be used:\n\ncodice_29\n\nA rule may be defined by a list of alternative rules separated by a solidus (codice_30).\n\nTo accept the rule \"fu\" or the rule \"bar\", the following rule could be constructed:\n\ncodice_32\n\nAdditional alternatives may be added to a rule through the use of codice_33 between the rule name and the definition.\n\nThe rule\nis equivalent to \n\ncodice_38\n\nA range of numeric values may be specified through the use of a hyphen (codice_39).\n\nThe rule\nis equivalent to\n\ncodice_42\n\nElements may be placed in parentheses to group rules in a definition.\n\nTo match “elem fubar snafu” or “elem tarfu snafu”, the following rule could be constructed:\n\nTo match “elem fubar” or “tarfu snafu”, the following rules could be constructed:\n\ncodice_46\n\nTo indicate repetition of an element, the form codice_47 is used. The optional codice_48 gives the minimal number of elements to be included (with the default of 0). The optional codice_49 gives the maximal number of elements to be included (with the default of infinity).\n\nUse codice_50 for zero or more elements, codice_51 for zero or one element, codice_52 for one or more elements, and codice_53 for two or three elements, cf. regular expressions codice_54, codice_55, codice_56 and codice_57.\n\ncodice_58\n\nTo indicate an explicit number of elements, the form codice_59 is used and is equivalent to codice_60.\n\nUse codice_61 to get two numeric digits, and codice_62 to get three numeric digits. (codice_63 is defined below under \"Core rules\". Also see \"zip-code\" in the example below.)\n\ncodice_64\n\nTo indicate an optional element, the following constructions are equivalent:\n\nThe following operators have the given precedence from tightest binding to loosest binding:\n\nUse of the alternative operator with concatenation may be confusing, and it is recommended that grouping be used to make explicit concatenation groups.\n\nThe core rules are defined in the ABNF standard.\nThe postal address example given in the augmented Backus–Naur form (ABNF) page may be specified as follows:\n\nRFC 5234 adds a warning in conjunction to the definition of LWSP as follows: \n"}
{"id": "40351757", "url": "https://en.wikipedia.org/wiki?curid=40351757", "title": "BREACH", "text": "BREACH\n\nBREACH (a backronym: Browser Reconnaissance and Exfiltration via Adaptive Compression of Hypertext) is a security exploit against HTTPS when using HTTP compression. BREACH is built based on the CRIME security exploit. BREACH was announced at the August 2013 Black Hat conference by security researchers Angelo Prado, Neal Harris and Yoel Gluck. The idea had been discussed in community before the announcement.\n\nWhile the CRIME attack was presented as a general attack that could work effectively against a large number of protocols, only exploits against SPDY request compression and TLS compression were demonstrated and largely mitigated in browsers and servers. The CRIME exploit against HTTP compression has not been mitigated at all, even though the authors of CRIME have warned that this vulnerability might be even more widespread than SPDY and TLS compression combined.\n\nBREACH is an instance of the CRIME attack against HTTP compression—the use of gzip or DEFLATE data compression algorithms via the content-encoding option within HTTP by many web browsers and servers. Given this compression oracle, the rest of the BREACH attack follows the same general lines as the CRIME exploit, by performing an initial blind brute-force search to guess a few bytes, followed by divide-and-conquer search to expand a correct guess to an arbitrarily large amount of content.\n\nBREACH exploits the compression in the underlying HTTP protocol. Therefore, turning off TLS compression makes no difference to BREACH, which can still perform a chosen-plaintext attack against the HTTP payload.\n\nAs a result, clients and servers are either forced to disable HTTP compression completely (thus reducing performance), or to adopt workarounds to try to foil BREACH in individual attack scenarios, such as using cross-site request forgery (CSRF) protection.\n\nAnother suggested approach is to disable HTTP compression whenever the referrer header indicates a cross-site request, or when the header is not present. This approach allows effective mitigation of the attack without losing functionality, only incurring a performance penalty on affected requests.\n\nAround 2013-2014, there was an IETF draft proposal for a TLS extension for length-hiding padding that, in theory, could be used as a mitigation against this attack. It allows the actual length of the TLS payload to be disguised by the insertion of padding to round it up to a fixed set of lengths, or to randomize the external length, thereby decreasing the likelihood of detecting small changes in compression ratio that is the basis for the BREACH attack. However, this draft has since expired without further action.\n\n"}
{"id": "3573834", "url": "https://en.wikipedia.org/wiki?curid=3573834", "title": "Branching quantifier", "text": "Branching quantifier\n\nIn logic a branching quantifier, also called a Henkin quantifier, finite partially ordered quantifier or even nonlinear quantifier, is a partial ordering\n\nof quantifiers for \"Q\" ∈ {∀,∃}. It is a special case of generalized quantifier. In classical logic, quantifier prefixes are linearly ordered such that the value of a variable \"y\" bound by a quantifier \"Q\" depends on the value of the variables\n\nbound by quantifiers\n\npreceding \"Q\". In a logic with (finite) partially ordered quantification this is not in general the case.\n\nBranching quantification first appeared in a 1959 conference paper of Leon Henkin. Systems of partially ordered quantification are intermediate in strength between first-order logic and second-order logic. They are being used as a basis for Hintikka's and Gabriel Sandu's independence-friendly logic.\n\nThe simplest Henkin quantifier formula_2 is\n\nIt (in fact every formula with a Henkin prefix, not just the simplest one) is equivalent to its second-order Skolemization, i.e.\n\nIt is also powerful enough to define the quantifier formula_5 (i.e. \"there are infinitely many\") defined as\n\nSeveral things follow from this, including the nonaxiomatizability of first-order logic with formula_2 (first observed by Ehrenfeucht), and its equivalence to the formula_8-fragment of second-order logic (existential second-order logic)—the latter result published independently in 1970 by Herbert Enderton and W. Walkoe.\n\nThe following quantifiers are also definable by formula_2.\n\n\n\n\nThe Henkin quantifier formula_2 can itself be expressed as a type (4) Lindström quantifier.\n\nHintikka in a 1973 paper advanced the hypothesis that some sentences in natural languages are best understood in terms of branching quantifiers, for example: \"some relative of each villager and some relative of each townsman hate each other\" is supposed to be interpreted, according to Hintikka, as:\n\nwhich is known to have no first-order logic equivalent.\n\nThe idea of branching is not necessarily restricted to using the classical quantifiers as leaves. In a 1979 paper, Jon Barwise proposed variations of Hintikka sentences (as the above is sometimes called) in which the inner quantifiers are themselves generalized quantifiers, for example: \"Most villagers and most townsmen hate each other.\" Observing that formula_8 is not closed under negation, Barwise also proposed a practical test to determine whether natural language sentences really involve branching quantifiers, namely to test whether their natural-language negation involves universal quantification over a set variable (a formula_16 sentence).\n\nHintikka's proposal was met with skepticism by a number of logicians because some first-order sentences like the one below appear to capture well enough the natural language Hintikka sentence.\n\nwhere\n\ndenotes\n\nAlthough much purely theoretical debate followed, it wasn't until 2009 that some empirical tests with students trained in logic found that they are more likely to assign models matching the \"bidirectional\" first-order sentence rather than branching-quantifier sentence to several natural-language constructs derived from the Hintikka sentence. For instance students were shown undirected bipartite graphs—with squares and circles as vertices—and asked to say whether sentences like \"more than 3 circles and more than 3 squares are connected by lines\" were correctly describing the diagrams.\n\n\n"}
{"id": "3191883", "url": "https://en.wikipedia.org/wiki?curid=3191883", "title": "Bring radical", "text": "Bring radical\n\nIn algebra, the Bring radical or ultraradical of a real number \"a\" is the unique real root of the polynomial\n\nThe Bring radical of a complex number \"a\" is either any of the five roots of the above polynomial (it is thus multi-valued), or a specific root, which is usually chosen in order that the Bring radical is a function of \"a\", which is real-valued when \"a\" is real, and is an analytic function in a neighborhood of the real line. Because of the existence of four branch points, the Bring radical cannot be defined as a function that is continuous over the whole complex plane, and its domain of continuity must exclude four branch cuts.\n\nGeorge Jerrard showed that some quintic equations can be solved in closed form using radicals and Bring radicals, which had been introduced by Erland Bring.\n\nIn this article, the Bring radical of \"a\" is denoted formula_2 For real argument, it is odd, monotonically decreasing, and unbounded, with asymptotic behavior formula_3 for large formula_4.\n\nThe quintic equation is rather difficult to obtain solutions for directly, with five independent coefficients in its most general form:\n\nThe various methods for solving the quintic that have been developed generally attempt to simplify the quintic using Tschirnhaus transformations to reduce the number of independent coefficients.\n\nThe general quintic may be reduced into what is known as the principal quintic form, with the quartic and cubic terms removed:\n\nIf the roots of a general quintic and a principal quintic are related by a quadratic Tschirnhaus transformation\n\nthe coefficients \"α\" and \"β\" may be determined by using the resultant, or by means of the power sums of the roots and Newton's identities. This leads to a system of equations in \"α\" and \"β\" consisting of a quadratic and a linear equation, and either of the two sets of solutions may be used to obtain the corresponding three coefficients of the principal quintic form.\n\nThis form is used by Felix Klein's solution to the quintic.\n\nIt is possible to simplify the quintic still further and eliminate the quadratic term, producing the Bring–Jerrard normal form:\nUsing the power-sum formulae again with a cubic transformation as Tschirnhaus tried does not work, since the resulting system of equations results in a sixth-degree equation. But in 1796 Bring found a way around this by using a quartic Tschirnhaus transformation to relate the roots of a principal quintic to those of a Bring–Jerrard quintic:\n\nThe extra parameter this fourth-order transformation provides allowed Bring to decrease the degrees of the other parameters. This leads to a system of five equations in six unknowns, which then requires the solution of a cubic and a quadratic equation. This method was also discovered by Jerrard in 1852, but it is likely that he was unaware of Bring's previous work in this area. The full transformation may readily be accomplished using a computer algebra package such as Mathematica or Maple. As might be expected from the complexity of these transformations, the resulting expressions can be enormous, particularly when compared to the solutions in radicals for lower degree equations, taking many megabytes of storage for a general quintic with symbolic coefficients.\n\nRegarded as an algebraic function, the solutions to\n\ninvolve two variables, \"d\" and \"d\"; however, the reduction is actually to an algebraic function of one variable, very much analogous to a solution in radicals, since we may further reduce the Bring–Jerrard form. If we for instance set\n\nthen we reduce the equation to the form\n\nwhich involves \"z\" as an algebraic function of a single variable \"t\", where formula_13. A similar transformation suffices to reduce the equation to\n\nwhich is the form required by the Hermite–Kronecker–Brioschi method, Glasser's method, and the Cockle–Harley method of differential resolvents described below.\n\nThere is another one-parameter normal form for the quintic equation, known as Brioschi normal form\nwhich can be derived by using the rational Tschirnhaus transformation\n\nto relate the roots of a general quintic to a Brioschi quintic. The values of the parameters formula_17 and formula_18 may be derived by using polyhedral functions on the Riemann sphere, and are related to the partition of an object of icosahedral symmetry into five objects of tetrahedral symmetry.\n\nIt is to be noted that this Tschirnhaus transformation is rather simpler than the difficult one used to transform a principal quintic into Bring–Jerrard form. This normal form is used by the Doyle–McMullen iteration method and the Kiepert method.\n\nA Taylor series for Bring radicals, as well as a representation in terms of hypergeometric functions can be derived as follows. The equation formula_19 can be rewritten as formula_20; by setting formula_21, the desired solution is formula_22.\n\nThe series for formula_23 can then be obtained by reversion of the Taylor series for formula_24 (which is simply formula_25), giving:\n\nwhere the absolute values of the coefficients are sequence in the OEIS. The series confirms that formula_27 is odd. This gives\n\nThe series converges for formula_29 and can be analytically continued in the complex plane. The above result can be written in hypergeometric form as:\n\nCompare with the hypergeometric functions that arise in Glasser's derivation and the method of differential resolvents below.\n\nWe now may express the roots of any polynomial\n\nin terms of the Bring radical as\n\nand its four conjugates. We have a reduction to the Bring–Jerrard form in terms of solvable polynomial equations, and we used transformations involving polynomial expressions in the roots only up to the fourth degree, which means inverting the transformation may be done by finding the roots of a polynomial solvable in radicals. This procedure produces extraneous solutions, but when we have found the correct ones by numerical means we can also write down the roots of the quintic in terms of square roots, cube roots, and the Bring radical, which is therefore an algebraic solution in terms of algebraic functions of a single variable — an algebraic solution of the general quintic.\n\nMany other characterizations of the Bring radical have been developed, the first of which is in terms of elliptic modular functions by Charles Hermite in 1858, and further methods later developed by other mathematicians.\n\nIn 1858, Charles Hermite published the first known solution to the general quintic equation in terms of elliptic transcendents, and at around the same time Francesco Brioschi and Leopold Kronecker came upon equivalent solutions. Hermite arrived at this solution by generalizing the well-known solution to the cubic equation in terms of trigonometric functions and finds the solution to a quintic in Bring–Jerrard form:\n\ninto which any quintic equation may be reduced by means of Tschirnhaus transformations as has been shown. He observed that elliptic functions had an analogous role to play in the solution of the Bring–Jerrard quintic as the trigonometric functions had for the cubic. If formula_34 and formula_35 are the periods of an elliptic integral of the first kind:\nthe elliptic nome is given by:\nand\n\nWith\ndefine the two elliptic modular functions:\nwhere formula_43 and similar are Jacobi theta functions.\n\nIf \"n\" is a prime number, we can define two values \"u\" and \"v\" as follows:\n\nand\n\nThe parameters formula_46 and formula_47 are linked by an equation of degree \"n\" + 1 known as the modular equation, whose \"n\" + 1 roots are given by:\n\nand\n\nwhere ε is 1 or −1 depending on whether 2 is a quadratic residue with respect to \"n\" or not, and \"m\" is an integer modulo \"n\". For \"n\" = 5, we have the modular equation of the sixth degree:\n\nwith six roots as shown above.\n\nThe modular equation of the sixth degree may be related to the Bring–Jerrard quintic by the following function of the six roots of the modular equation:\n\nThe five quantities formula_52, formula_53, formula_54, formula_55, formula_56 are the roots of a quintic equation with coefficients rational in formula_57:\n\nwhich may be readily converted into the Bring–Jerrard form by the substitution:\n\nleading to the Bring–Jerrard quintic:\n\nwhere\n\nThe Hermite–Kronecker–Brioschi method then amounts to finding a value for τ that corresponds to the value of \"a\", and then using that value of τ to obtain the roots of the corresponding modular equation. To do this, let\n\nand calculate the required elliptic modulus formula_63 by solving the quartic equation:\n\nThe roots of this equation are:\n\nwhere formula_66 (note that some important references erroneously give it as formula_67\n). Any of these roots may be used as the elliptic modulus for the purposes of the method. The value of formula_68 may be easily obtained from the elliptic modulus formula_69 by the relations given above. The roots of the Bring–Jerrard quintic are then given by:\nfor formula_71.\n\nIt may be seen that this process uses a generalization of the nth root, which may be expressed as:\nor more to the point, as\nThe Hermite–Kronecker–Brioschi method essentially replaces the exponential by an elliptic modular function, and the integral formula_74 by an elliptic integral. Kronecker thought that this generalization was a special case of a still more general theorem, which would be applicable to equations of arbitrarily high degree. This theorem, known as Thomae's formula, was fully expressed by Hiroshi Umemura in 1984, who used Siegel modular forms in place of the exponential/elliptic modular function, and the integral by a hyperelliptic integral.\n\nThis derivation due to M. L. Glasser generalizes the series method presented earlier in this article to find a solution to any trinomial equation of the form:\n\nIn particular, the quintic equation can be reduced to this form by the use of Tschirnhaus transformations as shown above. Let formula_76, the general form becomes:\n\nwhere\n\nA formula due to Lagrange states that for any analytic function formula_79, in the neighborhood of a root of the transformed general equation in terms of formula_80, above may be expressed as an infinite series:\n\nIf we let formula_82 in this formula, we can come up with the root:\n\nBy the use of the Gauss multiplication theorem the infinite series above may be broken up into a finite series of hypergeometric functions:\n\nand the trinomial of the form has roots\n\nA root of the equation can thus be expressed as the sum of at most \"N\" − 1 hypergeometric functions. Applying this method to the reduced Bring–Jerrard quintic, define the following functions:\n\nwhich are the hypergeometric functions that appear in the series formula above. The roots of the quintic are thus:\n\nThis is essentially the same result as that obtained by the following method.\n\nJames Cockle and Robert Harley developed, in 1860, a method for solving the quintic by means of differential equations. They consider the roots as being functions of the coefficients, and calculate a differential resolvent based on these equations. The Bring–Jerrard quintic is expressed as a function:\nand a function formula_96 is to be determined such that:\n\nThe function formula_98 must also satisfy the following four differential equations:\n\nExpanding these and combining them together yields the differential resolvent:\n\nThe solution of the differential resolvent, being a fourth order ordinary differential equation, depends on four constants of integration, which should be chosen so as to satisfy the original quintic. This is a Fuchsian ordinary differential equation of hypergeometric type, whose solution turns out to be identical to the series of hypergeometric functions that arose in Glasser's derivation above.\n\nThis method may also be generalized to equations of arbitrarily high degree, with differential resolvents which are partial differential equations, whose solutions involve hypergeometric functions of several variables.\nA general formula for differential resolvents of arbitrary univariate polynomials is given by Nahay's powersum formula.\nIn 1989, Peter Doyle and Curt McMullen derived an iteration method that solves a quintic in Brioschi normal form:\nThe iteration algorithm proceeds as follows:\n\n1. Set formula_102\n\n2. Compute the rational function\n3. Iterate formula_108 on a random starting guess until it converges. Call the limit point formula_109 and let formula_110.\n4. Compute\n5. Finally, compute\n\nThe two polynomial functions formula_104 and formula_112 are as follows:\n\nThis iteration method produces two roots of the quintic. The remaining three roots can be obtained by using synthetic division to divide the two roots out, producing a cubic equation. It is to be noted that due to the way the iteration is formulated, this method seems to always find two complex conjugate roots of the quintic even when all the quintic coefficients are real and the starting guess is real. This iteration method is derived by from the symmetries of the icosahedron and is closely related to the method Felix Klein describes in his book.\n\n\n\n\n"}
{"id": "13115105", "url": "https://en.wikipedia.org/wiki?curid=13115105", "title": "Caloric polynomial", "text": "Caloric polynomial\n\nIn differential equations, the \"m\"th-degree caloric polynomial (or heat polynomial) is a \"parabolically \"m\"-homogeneous\" polynomial \"P\"(\"x\", \"t\") that satisfies the heat equation\n\n\"Parabolically \"m\"-homogeneous\" means\n\nThe polynomial is given by\n\nIt is unique up to a factor.\n\nWith \"t\" = −1, this polynomial reduces to the \"m\"th-degree Hermite polynomial in \"x\".\n\n\n"}
{"id": "173547", "url": "https://en.wikipedia.org/wiki?curid=173547", "title": "Cayley–Hamilton theorem", "text": "Cayley–Hamilton theorem\n\nIn linear algebra, the Cayley–Hamilton theorem (named after the mathematicians Arthur Cayley and William Rowan Hamilton) states that every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic equation.\n\nIf is a given matrix and is the identity matrix, then the characteristic polynomial of is defined as\nwhere is the determinant operation and λ is a scalar element of the base ring. Since the entries of the matrix are (linear or constant) polynomials in , the determinant is also an -th order monic polynomial in . The Cayley–Hamilton theorem states that substituting the matrix for in this polynomial results in the zero matrix,\n\nThe powers of , obtained by substitution from powers of , are defined by repeated matrix multiplication; the constant term of gives a multiple of the power , which is defined as the identity matrix.\nThe theorem allows to be expressed as a linear combination of the lower matrix powers of . When the ring is a field, the Cayley–Hamilton theorem is equivalent to the statement that the minimal polynomial of a square matrix divides its characteristic polynomial.\n\nThe theorem was first proved in 1853 in terms of inverses of linear functions of quaternions, a \"non-commutative\" ring, by Hamilton. This corresponds to the special case of certain real or complex matrices. The theorem holds for general quaternionic matrices. Cayley in 1858 stated it for and smaller matrices, but only published a proof for the case. The general case was first proved by Frobenius in 1878.\n\nFor a matrix , the characteristic polynomial is given by , and so is obvious.\n\nAs a concrete example, let\nIts characteristic polynomial is given by\n\nThe Cayley–Hamilton theorem claims that, if we \"define\"\nthen\nWe can verify by computation that indeed,\n\nFor a generic matrix,\n\nthe characteristic polynomial is given by , so the Cayley–Hamilton theorem states that\nwhich is indeed always the case, evident by working out the entries of .\n\nFor a general invertible matrix , i.e., one with nonzero determinant, can thus be written as an order polynomial expression in : As indicated, the Cayley–Hamilton theorem amounts to the identity \n\nThe coefficients are given by the elementary symmetric polynomials of the eigenvalues of . Using Newton identities, the elementary symmetric polynomials can in turn be expressed in terms of power sum symmetric polynomials of the eigenvalues: \nwhere is the trace of the matrix . Thus, we can express in terms of the trace of powers of .\n\nIn general, the formula for the coefficients is given in terms of complete exponential Bell polynomials as \n\nIn particular, the determinant of corresponds to . Thus, the determinant can be written as a trace identity\n\nLikewise, the characteristic polynomial can be written as\nand, by multiplying both sides by (note ), one is led to an expression for the inverse of as a trace identity,\n\nFor instance, the first few Bell polynomials are = 1, , , and .\n\nUsing these to specify the coefficients of the characteristic polynomial of a matrix yields\n\nThe coefficient gives the determinant of the matrix, minus its trace, while its inverse is given by\n\nIt is apparent from the general formula for \"c\", expressed in terms of Bell polynomials, that the expressions\n\nalways give the coefficients of and of in the characteristic polynomial of any matrix, respectively. So, for a matrix , the statement of the Cayley–Hamilton theorem can also be written as\nwhere the right-hand side designates a matrix with all entries reduced to zero. Likewise, this determinant in the case, is now\nThis expression gives the negative of coefficient of in the general case, as seen below.\n\nSimilarly, one can write for a matrix ,\nwhere, now, the determinant is , \nand so on for larger matrices. The increasingly complex expressions for the coefficients is deducible from Newton's identities or the Faddeev–LeVerrier algorithm.\n\nAnother method for obtaining these coefficients for a general matrix, provided no root be zero, relies on the following alternative expression for the determinant,\n\nHence, by virtue of the Mercator series,\nwhere the exponential \"only\" needs be expanded to order , since is of order , the net negative powers of automatically vanishing by the C–H theorem. (Again, this requires a ring containing the rational numbers.) The coefficients of can be directly written in terms of complete Bell polynomials by comparing this expression with the generating function of the Bell polynomial.\n\nDifferentiation of this expression with respect to allows determination of the generic coefficients of the characteristic polynomial for general , as\ndeterminants of matrices,\n\nThe Cayley–Hamilton theorem always provides a relationship between the powers of (though not always the simplest one), which allows one to simplify expressions involving such powers, and evaluate them without having to compute the power or any higher powers of .\n\nAs an example, for formula_28 the theorem gives \n\nThen, to calculate , observe\nLikewise,\n\nNotice that we have been able to write the matrix power as the sum of two terms. In fact, matrix power of any order can be written as a matrix polynomial of degree at most , where is the size of a square matrix. This is an instance where Cayley–Hamilton theorem can be used to express a matrix function, which we will discuss below systematically.\n\nGiven an analytic function \nand the characteristic polynomial of degree of an matrix , the function can be expressed using long division as \nwhere is some quotient polynomial and is a remainder polynomial such that . By the Cayley–Hamilton theorem, replacing by the matrix gives , so one has\n\nThus, the analytic function of matrix can be expressed as a matrix polynomial of degree less than .\n\nLet the remainder polynomial be\nSince , evaluating the function at the eigenvalues of , yields\nThis amounts to a system of linear equations, which can be solved to determine the coefficients . Thus, one has \n\nWhen the eigenvalues are repeated, that is for some , two or more equations are identical; and hence the linear equations cannot be solved uniquely. For such cases, for an eigenvalue with multiplicity , the first derivative of vanishes at the eigenvalues. Thus, there are the extra linearly independent solutions \nwhich, when combined with others, yield the required equations to solve for .\n\nFinding a polynomial that passes through the points is essentially an interpolation problem, and can be solved using Lagrange or Newton interpolation techniques, leading to Sylvester's formula.\n\nFor example, suppose the task is to find the polynomial representation of \n\nThe characteristic polynomial is , and the eigenvalues are . Let . Evaluating at the eigenvalues, one obtains two linear equations and . Solving the equations yields and . Thus, it follows that\n\nIf, instead, the function were , then the coefficients would have been and ; hence\n\nAs a further example, when considering\nthen the characteristic polynomial is , and the eigenvalues are . As before, evaluating the function at the eigenvalues gives us the linear equations and ; the solution of which gives, and . Thus, for this case,\nwhich is a rotation matrix.\n\nStandard examples of such usage is the exponential map from the Lie algebra of a matrix Lie group into the group. It is given by a matrix exponential,\nSuch expressions have long been known for ,\nwhere the are the Pauli matrices and for ,\nwhich is Rodrigues' rotation formula. For the notation, see rotation group SO(3)#A note on Lie algebra.\n\nMore recently, expressions have appeared for other groups, like the Lorentz group , and , as well as . The group is the conformal group of spacetime, its simply connected cover (to be precise, the simply connected cover of the connected component of ). The expressions obtained apply to the standard representation of these groups. They require knowledge of (some of) the eigenvalues of the matrix to exponentiate. For (and hence for ), closed expressions have recently been obtained for all irreducible representations, i.e. of any spin.\n\nThe Cayley–Hamilton theorem is an effective tool for computing the minimal polynomial of algebraic integers. For example, given a finite extension formula_48 of formula_49 and an algebraic integer formula_50 which is a non-zero linear combination of the formula_51 we can compute the minimal polynomial of formula_52 by finding a matrix representing the formula_49-linear transformation\nIf we call this transformation matrix formula_55, then we can find the minimal polynomial by applying the Cayley–Hamilton theorem to formula_55.\n\nThe Cayley-Hamilton is an immediate consequence of the existence of the Jordan normal form for matrices over algebraically closed fields. In this section direct proofs are presented. \n\nAs the examples above show, obtaining the statement of the Cayley–Hamilton theorem for an matrix\n\nrequires two steps: first the coefficients of the characteristic polynomial are determined by development as a polynomial in of the determinant\n\nand then these coefficients are used in a linear combination of powers of that is equated to the null matrix:\n\nThe left hand side can be worked out to an matrix whose entries are (enormous) polynomial expressions in the set of entries of , so the Cayley–Hamilton theorem states that each of these expressions are equal to . For any fixed value of these identities can be obtained by tedious but completely straightforward algebraic manipulations. None of these computations can show however why the Cayley–Hamilton theorem should be valid for matrices of all possible sizes , so a uniform proof for all is needed.\n\nIf a vector of size happens to be an eigenvector of with eigenvalue , in other words if , then\nwhich is the null vector since (the eigenvalues of are precisely the roots of ). This holds for all possible eigenvalues , so the two matrices equated by the theorem certainly give the same (null) result when applied to any eigenvector. Now if admits a basis of eigenvectors, in other words if is diagonalizable, then the Cayley–Hamilton theorem must hold for , since two matrices that give the same values when applied to each element of a basis must be equal. \n\nConsider now the function formula_67which maps formula_68matrices to formula_68matrices given by the formula formula_70, i.e. which takes a matrix formula_55and plugs it into its own characteristic polynomial. Not all matrices are diagonalizable, but for matrices with complex coefficients many of them are: the set of formula_72 diagonalizable complex square matrices of a given size is dense in the set of all such square matrices (for a matrix to be diagonalizable it suffices for instance that its characteristic polynomial not have any multiple roots). Now viewed as a function formula_73(since matrices have formula_74entries) we see that this function is continuous. This is true because the entries of the image of a matrix are given by polynomials in the entries of the matrix. Since \n\nformula_75 \n\nand since formula_72is dense, by continuity this function must map the entire set of formula_68matrices to the zero matrix. Therefore the Cayley-Hamilton theorem is true for complex numbers, and must therefore also hold for formula_78or formula_79valued matrices..\n\nWhile this provides a valid proof, the argument is not very satisfactory, since the identities represented by the theorem do not in any way depend on the nature of the matrix (diagonalizable or not), nor on the kind of entries allowed (for matrices with real entries the diagonalizable ones do not form a dense set, and it seems strange one would have to consider complex matrices to see that the Cayley–Hamilton theorem holds for them). We shall therefore now consider only arguments that prove the theorem directly for any matrix using algebraic manipulations only; these also have the benefit of working for matrices with entries in any commutative ring.\n\nThere is a great variety of such proofs of the Cayley–Hamilton theorem, of which several will be given here. They vary in the amount of abstract algebraic notions required to understand the proof. The simplest proofs use just those notions needed to formulate the theorem (matrices, polynomials with numeric entries, determinants), but involve technical computations that render somewhat mysterious the fact that they lead precisely to the correct conclusion. It is possible to avoid such details, but at the price of involving more subtle algebraic notions: polynomials with coefficients in a non-commutative ring, or matrices with unusual kinds of entries.\n\nAll proofs below use the notion of the adjugate matrix of an matrix , the transpose of its cofactor matrix.\n\nThis is a matrix whose coefficients are given by polynomial expressions in the coefficients of (in fact, by certain determinants), in such a way that \nthe following fundamental relations hold,\nThese relations are a direct consequence of the basic properties of determinants: evaluation of the entry of the matrix product on the left gives the expansion by column of the determinant of the matrix obtained from by replacing column by a copy of column , which is if and zero otherwise; the matrix product on the right is similar, but for expansions by rows.\n\nBeing a consequence of just algebraic expression manipulation, these relations are valid for matrices with entries in any commutative ring (commutativity must be assumed for determinants to be defined in the first place). This is important to note here, because these relations will be applied below for matrices with non-numeric entries such as polynomials.\n\nThis proof uses just the kind of objects needed to formulate the Cayley–Hamilton theorem: matrices with polynomials as entries. The matrix whose determinant is the characteristic polynomial of is such a matrix, and since polynomials form a commutative ring, it has an adjugate\nThen, according to the right-hand fundamental relation of the adjugate, one has\n\nSince is also a matrix with polynomials in as entries, one can, for each , collect the coefficients of in each entry to form a matrix of numbers, such that one has\n(The way the entries of are defined makes clear that no powers higher than occur). While this \"looks\" like a polynomial with matrices as coefficients, we shall not consider such a notion; it is just a way to write a matrix with polynomial entries as a linear combination of constant matrices, and the coefficient has been written to the left of the matrix to stress this point of view.\n\nNow, one can expand the matrix product in our equation by bilinearity\n\nWriting\none obtains an equality of two matrices with polynomial entries, written as linear combinations of constant matrices with powers of as coefficients.\n\nSuch an equality can hold only if in any matrix position the entry that is multiplied by a given power is the same on both sides; it follows that the constant matrices with coefficient in both expressions must be equal. Writing these equations then for from down to 0, one finds\n\nFinally, multiply the equation of the coefficients of from the left by , and sum up:\n\nformula_87\n\nThe left-hand sides form a telescoping sum and cancel completely; the right-hand sides add up to formula_88:\nThis completes the proof.\n\nThis proof is similar to the first one, but tries to give meaning to the notion of polynomial with matrix coefficients that was suggested by the expressions occurring in that proof. This requires considerable care, since it is somewhat unusual to consider polynomials with coefficients in a non-commutative ring, and not all reasoning that is valid for commutative polynomials can be applied in this setting.\n\nNotably, while arithmetic of polynomials over a commutative ring models the arithmetic of polynomial functions, this is not the case over a non-commutative ring (in fact there is no obvious notion of polynomial function in this case that is closed under multiplication). So when considering polynomials in with matrix coefficients, the variable must not be thought of as an \"unknown\", but as a formal symbol that is to be manipulated according to given rules; in particular one cannot just set to a specific value.\n\nLet \"M\"(\"n\", \"R\") be the ring of \"n\"×\"n\" matrices with entries in some ring \"R\" (such as the real or complex numbers) that has as an element. Matrices with as coefficients polynomials in , such as formula_91 or its adjugate \"B\" in the first proof, are elements of \"M\"(\"n\", \"R\"[\"t\"]).\n\nBy collecting like powers of , such matrices can be written as \"polynomials\" in with constant matrices as coefficients; write \"M\"(\"n\", \"R\")[\"t\"] for the set of such polynomials. Since this set is in bijection with \"M\"(\"n\", \"R\"[\"t\"]), one defines arithmetic operations on it correspondingly, in particular multiplication is given by\nrespecting the order of the coefficient matrices from the two operands; obviously this gives a non-commutative multiplication.\n\nThus, the identity\nfrom the first proof can be viewed as one involving a multiplication of elements in \"M\"(\"n\", \"R\")[\"t\"].\n\nAt this point, it is tempting to simply set equal to the matrix , which makes the first factor on the left equal to the null matrix, and the right hand side equal to ; however, this is not an allowed operation when coefficients do not commute. It is possible to define a \"right-evaluation map\" ev : M[\"t\"] → M, which replaces each \"t\" by the matrix power of , where one stipulates that the power is always to be multiplied on the right to the corresponding coefficient.\n\nBut this map is not a ring homomorphism: the right-evaluation of a product differs in general from the product of the right-evaluations. This is so because multiplication of polynomials with matrix coefficients does not model multiplication of expressions containing unknowns: a product formula_94 is defined assuming that commutes with , but this may fail if is replaced by the matrix .\n\nOne can work around this difficulty in the particular situation at hand, since the above right-evaluation map does become a ring homomorphism if the matrix is in the center of the ring of coefficients, so that it commutes with all the coefficients of the polynomials (the argument proving this is straightforward, exactly because commuting with coefficients is now justified after evaluation).\n\nNow, is not always in the center of M, but we may replace M with a smaller ring provided it contains all the coefficients of the polynomials in question: formula_95, , and the coefficients formula_96 of the polynomial \"B\". The obvious choice for such a subring is the centralizer \"Z\" of , the subring of all matrices that commute with ; by definition is in the center of \"Z\".\n\nThis centralizer obviously contains formula_95, and , but one has to show that it contains the matrices formula_96. To do this, one combines the two fundamental relations for adjugates, writing out the adjugate \"B\" as a polynomial:\n\nEquating the coefficients shows that for each \"i\", we have \"B\" = \"B\" as desired. Having found the proper setting in which ev is indeed a homomorphism of rings, one can complete the proof as suggested above:\nThis completes the proof.\n\nIn the first proof, one was able to determine the coefficients of based on the right-hand fundamental relation for the adjugate only. In fact the first equations derived can be interpreted as determining the quotient of the Euclidean division of the polynomial on the left by the monic polynomial , while the final equation expresses the fact that the remainder is zero. This division is performed in the ring of polynomials with matrix coefficients. Indeed, even over a non-commutative ring, Euclidean division by a monic polynomial is defined, and always produces a unique quotient and remainder with the same degree condition as in the commutative case, provided it is specified at which side one wishes to be a factor (here that is to the left).\n\nTo see that quotient and remainder are unique (which is the important part of the statement here), it suffices to write formula_101 as formula_102 and observe that since is monic, cannot have a degree less than that of , unless .\n\nBut the dividend and divisor used here both lie in the subring , where is the subring of the matrix ring generated by : the -linear span of all powers of . Therefore, the Euclidean division can in fact be performed within that \"commutative\" polynomial ring, and of course it then gives the same quotient and remainder 0 as in the larger ring; in particular this shows that in fact lies in .\n\nBut, in this commutative setting, it is valid to set to in the equation \nwhich is a ring homomorphism, giving\njust like in the second proof, as desired.\n\nIn addition to proving the theorem, the above argument tells us that the coefficients of are polynomials in , while from the second proof we only knew that they lie in the centralizer of ; in general is a larger subring than , and not necessarily commutative. In particular the constant term lies in . Since is an arbitrary square matrix, this proves that can always be expressed as a polynomial in (with coefficients that depend on .\n\nIn fact, the equations found in the first proof allow successively expressing formula_106 as polynomials in , which leads to the identity\n\nvalid for all matrices, where \nis the characteristic polynomial of .\n\nNote that this identity also implies the statement of the Cayley–Hamilton theorem: one may move to the right hand side, multiply the resulting equation (on the left or on the right) by , and use the fact that\n\nAs was mentioned above, the matrix \"p\"(\"A\") in statement of the theorem is obtained by first evaluating the determinant and then substituting the matrix \"A\" for \"t\"; doing that substitution into the matrix formula_109 before evaluating the determinant is not meaningful. Nevertheless, it is possible to give an interpretation where \"p\"(\"A\") is obtained directly as the value of a certain determinant, but this requires a more complicated setting, one of matrices over a ring in which one can interpret both the entries formula_110 of \"A\", and all of \"A\" itself. One could take for this the ring \"M\"(\"n\", \"R\") of \"n\"×\"n\" matrices over \"R\", where the entry formula_110 is realised as formula_112, and \"A\" as itself. But considering matrices with matrices as entries might cause confusion with block matrices, which is not intended, as that gives the wrong notion of determinant (recall that the determinant of a matrix is defined as a sum of products of its entries, and in the case of a block matrix this is generally not the same as the corresponding sum of products of its blocks!). It is clearer to distinguish \"A\" from the endomorphism φ of an \"n\"-dimensional vector space \"V\" (or free \"R\"-module if \"R\" is not a field) defined by it in a basis \"e\", ..., \"e\", and to take matrices over the ring End(\"V\") of all such endomorphisms. Then φ ∈ End(\"V\") is a possible matrix entry, while \"A\" designates the element of \"M\"(\"n\", End(\"V\")) whose \"i\",\"j\" entry is endomorphism of scalar multiplication by formula_110; similarly \"I\" will be interpreted as element of \"M\"(\"n\", End(\"V\")). However, since End(\"V\") is not a commutative ring, no determinant is defined on \"M\"(\"n\", End(\"V\")); this can only be done for matrices over a commutative subring of End(\"V\"). Now the entries of the matrix formula_114 all lie in the subring \"R\"[φ] generated by the identity and φ, which is commutative. Then a determinant map \"M\"(\"n\", \"R\"[φ]) → \"R\"[φ] is defined, and formula_115 evaluates to the value \"p\"(φ) of the characteristic polynomial of \"A\" at φ (this holds independently of the relation between \"A\" and φ); the Cayley–Hamilton theorem states that \"p\"(φ) is the null endomorphism.\n\nIn this form, the following proof can be obtained from that of (which in fact is the more general statement related to the Nakayama lemma; one takes for the ideal in that proposition the whole ring \"R\"). The fact that \"A\" is the matrix of φ in the basis \"e\", ..., \"e\" means that\nOne can interpret these as \"n\" components of one equation in \"V\", whose members can be written using the matrix-vector product \"M\"(\"n\", End(\"V\")) × \"V\" → \"V\" that is defined as usual, but with individual entries ψ ∈ End(\"V\") and \"v\" in \"V\" being \"multiplied\" by forming formula_117; this gives:\nwhere formula_119 is the element whose component \"i\" is \"e\" (in other words it is the basis \"e\", ..., \"e\" of \"V\" written as a column of vectors). Writing this equation as\none recognizes the transpose of the matrix formula_114 considered above, and its determinant (as element of \"M\"(\"n\", \"R\"[φ])) is also \"p\"(φ). To derive from this equation that \"p\"(φ) = 0 ∈ End(\"V\"), one left-multiplies by the adjugate matrix of formula_122, which is defined in the matrix ring \"M\"(\"n\", \"R\"[φ]), giving\nthe associativity of matrix-matrix and matrix-vector multiplication used in the first step is a purely formal property of those operations, independent of the nature of the entries. Now component \"i\" of this equation says that \"p\"(φ)(\"e\") = 0 ∈ \"V\"; thus \"p\"(φ) vanishes on all \"e\", and since these elements generate \"V\" it follows that \"p\"(φ) = 0 ∈ End(\"V\"), completing the proof.\n\nOne additional fact that follows from this proof is that the matrix \"A\" whose characteristic polynomial is taken need not be identical to the value φ substituted into that polynomial; it suffices that φ be an endomorphism of \"V\" satisfying the initial equations\n\nfor \"some\" sequence of elements \"e\"...,\"e\" that generate \"V\" (which space might have smaller dimension than \"n\", or in case the ring \"R\" is not a field it might not be a free module at all).\n\nOne persistent elementary but incorrect argument for the theorem is to \"simply\" take the definition\nand substitute for , obtaining\n\nThere are many ways to see why this argument is wrong. First, in Cayley–Hamilton theorem, \"p\"(\"A\") is an \"n×n matrix\". However, the right hand side of the above equation is the value of a determinant, which is a \"scalar\". So they cannot be equated unless \"n\" = 1 (i.e. \"A\" is just a scalar). Second, in the expression formula_127, the variable λ actually occurs at the diagonal entries of the matrix formula_128. To illustrate, consider the characteristic polynomial in the previous example again:\n\nIf one substitutes the entire matrix \"A\" for λ in those positions, one obtains\n\nin which the \"matrix\" expression is simply not a valid one. Note, however, that if scalar multiples of identity matrices\ninstead of scalars are subtracted in the above, i.e. if the substitution is performed as\n\nthen the determinant is indeed zero, but the expanded matrix in question does not evaluate to formula_132; nor can its determinant (a scalar) be compared to \"p\"(\"A\") (a matrix). So the argument that formula_133 still does not apply.\n\nActually, if such an argument holds, it should also hold when other multilinear forms instead of determinant is used. For instance, if we consider the permanent function and define formula_134, then by the same argument, we should be able to \"prove\" that \"q\"(\"A\") = 0. But this statement is demonstrably wrong. In the 2-dimensional case, for instance, the permanent of a matrix is given by\n\nSo, for the matrix \"A\" in the previous example,\n\nYet one can verify that\n\nOne of the proofs for Cayley–Hamilton theorem above bears some similarity to the argument that formula_133. By introducing a matrix with non-numeric coefficients, one can actually let \"A\" live inside a matrix entry, but then formula_139 is not equal to \"A\", and the conclusion is reached differently.\n\nThe above proofs show that the Cayley–Hamilton theorem holds for matrices with entries in any commutative ring \"R\", and that \"p\"(φ) = 0 will hold whenever φ is an endomorphism of an \"R\" module generated by elements \"e\"...,\"e\" that satisfies\n\nThis more general version of the theorem is the source of the celebrated Nakayama lemma in commutative algebra and algebraic geometry.\n\n\n\n"}
{"id": "1374931", "url": "https://en.wikipedia.org/wiki?curid=1374931", "title": "Charlotte Froese Fischer", "text": "Charlotte Froese Fischer\n\nCharlotte Froese Fischer (born 1929) is a Canadian-American applied mathematician and computer scientist who gained world recognition for the development and implementation of the Multi-Configurational Hartree–Fock (MCHF) approach to atomic-structure calculations and for her theoretical prediction concerning the existence of the negative calcium ion. For this last accomplishment, she was elected to grade of Fellow of the American Physical Society.\n\nCharlotte Froese was born on September 21, 1929, in the village of Pravdivka (formerly Nikolayevka), in the Donetsk region, in the present-day Ukraine, to parents of Mennonite descent. Her parents immigrated to Germany in 1929 on the last train allowed to cross the border before its closure by Soviet authorities. After a few months in a refugee camp, her family was allowed to immigrate to Canada, where they eventually established themselves in Chilliwack, British Columbia.\n\nShe obtained both a B.A. degree, with honors, in Mathematics and Chemistry and an M.A. degree in Applied Mathematics from the University of British Columbia in 1952 and 1954, respectively. She then obtained her Ph.D. in Applied Mathematics and Computing at Cambridge University in 1957, pursuing coursework in quantum theory with Paul Dirac. She worked under the supervision of Douglas Hartree, whom she assisted in programming the Electronic Delay Storage Automatic Calculator (EDSAC) for atomic-structure calculations.\n\nShe served on the mathematics faculty of the University of British Columbia from 1957 till 1968, where she introduced numerical analysis and computer courses into the curriculum and was instrumental in the formation of the Computer Science Department.\n\nThe years 1963-64 were spent by Froese Fischer at the Harvard College Observatory, where she extended her research on atomic-structure calculations. While at Harvard, she was the first woman-scientist to be awarded an Alfred P. Sloan Fellowship. In 1991 she became a Fellow of the American Physical Society, in part for her contribution to the discovery of negative calcium. In 1995 she was elected a member of the Royal Physiographic Society in Lund, in 2004 a foreign member of the Lithuanian Academy of Sciences, and in 2015 she was awarded an Honorary Doctorate in Technology from Malmö University, Sweden.\n\nShe is the author of over 300 research articles on computational atomic theory, many of which have had far-reaching impact in the area of atomic-structure calculations. The early version of the MCHF program, published in the first volume of Computer Physics Communications received two Citation Classics Awards in 1987. One of her largest efforts in the field is the recent calculation of the complete lower spectra of the beryllium-like to argon-like isoelectronic sequences, amounting to the publication of data covering 400 journal pages and a total of over 150 ions.\n\nShe is currently an emerita research professor of computer science at Vanderbilt University and a guest scientist of the Atomic Spectroscopy Laboratory at NIST. She is the widow of Patrick C. Fischer, himself a noted computer scientist and former professor at Vanderbilt.\n\n"}
{"id": "2288644", "url": "https://en.wikipedia.org/wiki?curid=2288644", "title": "Concrete security", "text": "Concrete security\n\nIn cryptography, concrete security or exact security is a practice-oriented approach that aims to give more precise estimates of the computational complexities of adversarial tasks than polynomial equivalence would allow.\n\nTraditionally, provable security is asymptotic: it classifies the hardness of computational problems using polynomial-time reducibility. Secure schemes are defined to be those in which the advantage of any computationally bounded adversary is negligible. While such a theoretical guarantee is important, in practice one needs to know exactly how efficient a reduction is because of the need to instantiate the security parameter - it is not enough to know that \"sufficiently large\" security parameters will do. An inefficient reduction results either in the success probability for the adversary or the resource requirement of the scheme being greater than desired.\n\nConcrete security parametrizes all the resources available to the adversary, such as running time and memory, and other resources specific to the system in question, such as the number of plaintexts it can obtain or the number of queries it can make to any oracles available. Then the advantage of the adversary is upper bounded as a function of these resources and of the problem size. It is often possible to give a lower bound (i.e. an adversarial strategy) matching the upper bound, hence the name exact security.\n\n"}
{"id": "250323", "url": "https://en.wikipedia.org/wiki?curid=250323", "title": "Covering space", "text": "Covering space\n\nIn mathematics, more specifically algebraic topology, a covering map (also covering projection) is a continuous function \"p\" from a topological space \"C\" to a topological space \"X\" such that each point in \"X\" has an open neighbourhood evenly covered by \"p\" (as shown in the image); the precise definition is given below. In this case, \"C\" is called a covering space and \"X\" the base space of the covering projection. The definition implies that every covering map is a local homeomorphism.\n\nCovering spaces play an important role in homotopy theory, harmonic analysis, Riemannian geometry and differential topology. In Riemannian geometry for example, ramification is a generalization of the notion of covering maps. Covering spaces are also deeply intertwined with the study of homotopy groups and, in particular, the fundamental group. An important application comes from the result that, if \"X\" is a \"sufficiently good\" topological space, there is a bijection between the collection of all isomorphism classes of connected coverings of \"X\" and the conjugacy classes of subgroups of the fundamental group of \"X\".\n\nLet \"X\" be a topological space. A covering space of \"X\" is a topological space \"C\" together with a continuous surjective map\n\nsuch that for every formula_2, there exists an open neighborhood formula_3 of formula_4, such that formula_5 (the inverse image of \"U\" under \"p\") is a union of disjoint open sets in \"C\", each of which is mapped homeomorphically onto \"U\" by \"p\".\n\nThe map \"p\" is called the covering map, the space \"X\" is often called the base space of the covering, and the space \"C\" is called the total space of the covering. For any point \"x\" in the base the inverse image of \"x\" in \"C\" is necessarily a discrete space called the fiber over \"x\".\n\nThe special open neighborhoods \"U\" of \"x\" given in the definition are called evenly covered neighborhoods. The evenly covered neighborhoods form an open cover of the space \"X\". The homeomorphic copies in \"C\" of an evenly covered neighborhood \"U\" are called the sheets over \"U\". One generally pictures \"C\" as \"hovering above\" \"X\", with \"p\" mapping \"downwards\", the sheets over \"U\" being horizontally stacked above each other and above \"U\", and the fiber over \"x\" consisting of those points of \"C\" that lie \"vertically above\" \"x\". In particular, covering maps are locally trivial. This means that locally, each covering map is 'isomorphic' to a projection in the sense that there is a homeomorphism, \"h\", from the pre-image formula_5, of an evenly covered neighbourhood \"U\", onto formula_7, where \"F\" is the fiber, satisfying the local trivialization condition, which is that, if we project formula_7 onto \"U\", formula_9, so the composition of the projection with the homeomorphism \"h\" will be a map formula_10 from the pre-image formula_5 onto \"U\", then the derived composition formula_10 will equal \"p\" locally (within formula_5).\n\nMany authors impose some connectivity conditions on the spaces \"X\" and \"C\" in the definition of a covering map. In particular, many authors require both spaces to be path-connected and locally path-connected. This can prove helpful because many theorems hold only if the spaces in question have these properties. Some authors omit the assumption of surjectivity, for if \"X\" is connected and \"C\" is nonempty then surjectivity of the covering map actually follows from the other axioms.\n\nFor instance the diamond crystal as an abstract graph is the maximal abelian covering graph of the dipole graph D\n\n\nFor every \"x\" in \"X\", the fiber over \"x\" is a discrete subset of \"C\". On every connected component of \"X\", the fibers are homeomorphic.\n\nIf \"X\" is connected, there is a discrete space \"F\" such that for every \"x\" in \"X\" the fiber over \"x\" is homeomorphic to \"F\" and, moreover, for every \"x\" in \"X\" there is a neighborhood \"U\" of \"x\" such that its full pre-image \"p\"(\"U\") is homeomorphic to . In particular, the cardinality of the fiber over \"x\" is equal to the cardinality of \"F\" and it is called the degree of the cover . Thus, if every fiber has \"n\" elements, we speak of an \"n\"-fold covering (for the case , the covering is trivial; when , the covering is a double cover; when , the covering is a triple cover and so on).\n\nIf is a cover and γ is a path in \"X\" (i.e. a continuous map from the unit interval into \"X\") and is a point \"lying over\" γ(0) (i.e. , then there exists a unique path Γ in \"C\" lying over γ (i.e. ) such that . The curve Γ is called the lift of γ. If \"x\" and \"y\" are two points in \"X\" connected by a path, then that path furnishes a bijection between the fiber over \"x\" and the fiber over \"y\" via the lifting property.\n\nMore generally, let be a continuous map to \"X\" from a path connected and locally path connected space \"Z\". Fix a base-point , and choose a point \"lying over\" \"f\"(\"z\") (i.e. ). Then there exists a lift of \"f\" (that is, a continuous map for which and ) if and only if the induced homomorphisms and at the level of fundamental groups satisfy\n\nMoreover, if such a lift \"g\" of \"f\" exists, it is unique.\n\nIn particular, if the space \"Z\" is assumed to be simply connected (so that is trivial), condition is automatically satisfied, and every continuous map from \"Z\" to \"X\" can be lifted. Since the unit interval is simply connected, the lifting property for paths is a special case of the lifting property for maps stated above.\n\nIf is a covering and and are such that , then \"p\" is injective at the level of fundamental groups, and the induced homomorphisms are isomorphisms for all . Both of these statements can be deduced from the lifting property for continuous maps. Surjectivity of \"p\" for follows from the fact that for all such \"n\", the \"n\"-sphere S is simply connected and hence every continuous map from S to \"X\" can be lifted to \"C\".\n\nLet and be two coverings. One says that the two coverings \"p\" and \"p\" are equivalent if there exists a homeomorphism and such that . Equivalence classes of coverings correspond to conjugacy classes of subgroups of the fundamental group of \"X\", as discussed below. If is a covering (rather than a homeomorphism) and , then one says that \"p\" dominates \"p\".\n\nSince coverings are local homeomorphisms, a covering of a topological \"n\"-manifold is an \"n\"-manifold. (One can prove that the covering space is second-countable from the fact that the fundamental group of a manifold is always countable.) However a space covered by an \"n\"-manifold may be a non-Hausdorff manifold. An example is given by letting \"C\" be the plane with the origin deleted and \"X\" the quotient space obtained by identifying every point with . If is the quotient map then it is a covering since the action of \"Z\" on \"C\" generated by is properly discontinuous. The points and do not have disjoint neighborhoods in \"X\".\n\nAny covering space of a differentiable manifold may be equipped with a (natural) differentiable structure that turns \"p\" (the covering map in question) into a local diffeomorphism – a map with constant rank \"n\".\n\nA covering space is a universal covering space if it is simply connected. The name \"universal cover\" comes from the following important property: if the mapping is a universal cover of the space \"X\" and the mapping is any cover of the space \"X\" where the covering space \"C\" is connected, then there exists a covering map such that . This can be phrased as\n\nThe universal cover (of the space \"X\") covers any connected cover (of the space \"X\").</BLOCKQUOTE >\n\nThe map \"f\" is unique in the following sense: if we fix a point \"x\" in the space \"X\" and a point \"d\" in the space \"D\" with and a point \"c\" in the space \"C\" with , then there exists a unique covering map such that and .\n\nIf the space \"X\" has a universal cover then that universal cover is essentially unique: if the mappings and are two universal covers of the space \"X\" then there exists a homeomorphism such that .\n\nThe space \"X\" has a universal cover if it is connected, locally path-connected and semi-locally simply connected. The universal cover of the space \"X\" can be constructed as a certain space of paths in the space \"X\". More explicitly, it forms a principal bundle with the fundamental group as structure group.\n\nThe example given above is a universal cover. The map from unit quaternions to rotations of 3D space described in quaternions and spatial rotation is also a universal cover.\n\nIf the space formula_27 carries some additional structure, then its universal cover usually inherits that structure:\n\nThe universal cover first arose in the theory of analytic functions as the natural domain of an analytic continuation.\n\nLet \"G\" be a discrete group acting on the topological space \"X\". It is natural to ask under what conditions the projection from \"X\" to the orbit space \"X\"/\"G\" is a covering map. This is not always true since the action may have fixed points. An example for this is the cyclic group of order 2 acting on a product by the twist action where the non-identity element acts by . Thus the study of the relation between the fundamental groups of \"X\" and \"X\"/\"G\" is not so straightforward.\n\nHowever the group \"G\" does act on the fundamental groupoid of \"X\", and so the study is best handled by considering groups acting on groupoids, and the corresponding \"orbit groupoids\". The theory for this is set down in Chapter 11 of the book \"Topology and groupoids\" referred to below. The main result is that for discontinuous actions of a group \"G\" on a Hausdorff space \"X\" which admits a universal cover, then the fundamental groupoid of the orbit space \"X\"/\"G\" is isomorphic to the orbit groupoid of the fundamental groupoid of \"X\", i.e. the quotient of that groupoid by the action of the group \"G\". This leads to explicit computations, for example of the fundamental group of the symmetric square of a space.\n\nA deck transformation or automorphism of a cover is a homeomorphism such that . The set of all deck transformations of \"p\" forms a group under composition, the deck transformation group Aut(\"p\"). Deck transformations are also called covering transformations. Every deck transformation permutes the elements of each fiber. This defines a group action of the deck transformation group on each fiber. Note that by the unique lifting property, if \"f\" is not the identity and \"C\" is path connected, then \"f\" has no fixed points.\n\nNow suppose is a covering map and \"C\" (and therefore also \"X\") is connected and locally path connected. The action of Aut(\"p\") on each fiber is free. If this action is transitive on some fiber, then it is transitive on all fibers, and we call the cover regular (or normal or Galois). Every such regular cover is a principal \"G\"-bundle, where is considered as a discrete topological group.\n\nEvery universal cover is regular, with deck transformation group being isomorphic to the fundamental group (\"X\").\n\nAs another important example, consider C the complex plane and C the complex plane minus the origin. Then the map with is a regular cover. The deck transformations are multiplications with \"n\"-th roots of unity and the deck transformation group is therefore isomorphic to the cyclic group \"C\". Likewise, the map with is the universal cover.\n\nAgain suppose is a covering map and \"C\" (and therefore also \"X\") is connected and locally path connected. If \"x\" is in \"X\" and \"c\" belongs to the fiber over \"x\" (i.e. ), and is a path with , then this path lifts to a unique path in \"C\" with starting point \"c\". The end point of this lifted path need not be \"c\", but it must lie in the fiber over \"x\". It turns out that this end point only depends on the class of γ in the fundamental group . In this fashion we obtain a right group action of on the fiber over \"x\". This is known as the monodromy action.\n\nThere are two actions on the fiber over acts on the left and acts on the right. These two actions are compatible in the following sense:\nformula_37 for all \"f\" in Aut(\"p\"), \"c\" in \"p\"(\"x\") and γ in .\n\nIf \"p\" is a universal cover, then Aut(\"p\") can be naturally identified with the opposite group of so that the left action of the opposite group of coincides with the action of Aut(\"p\") on the fiber over \"x\". Note that Aut(\"p\") and are naturally isomorphic in this case (as a group is always naturally isomorphic to its opposite through .\n\nIf \"p\" is a regular cover, then Aut(\"p\") is naturally isomorphic to a quotient of .\n\nIn general (for good spaces), Aut(\"p\") is naturally isomorphic to the quotient of the normalizer of in over , where .\n\nLet be a covering map where both \"X\" and \"C\" are path-connected. Let be a basepoint of \"X\" and let be one of its pre-images in \"C\", that is . There is an induced homomorphism of fundamental groups which is injective by the lifting property of coverings. Specifically if \"γ\" is a closed loop at \"c\" such that , that is is null-homotopic in \"X\", then consider a null-homotopy of as a map from the 2-disc \"D\" to \"X\" such that the restriction of \"f\" to the boundary S of \"D\" is equal to . By the lifting property the map \"f\" lifts to a continuous map such that the restriction of \"g\" to the boundary S of \"D\" is equal to \"γ\". Therefore, \"γ\" is null-homotopic in \"C\", so that the kernel of is trivial and thus is an injective homomorphism.\n\nTherefore, is isomorphic to the subgroup of . If is another pre-image of \"x\" in \"C\" then the subgroups and are conjugate in by \"p\"-image of a curve in \"C\" connecting \"c\" to \"c\". Thus a covering map defines a conjugacy class of subgroups of and one can show that equivalent covers of \"X\" define the same conjugacy class of subgroups of .\n\nFor a covering the group can also be seen to be equal to\n\nthe set of homotopy classes of those closed curves γ based at \"x\" whose lifts \"γ\" in \"C\", starting at \"c\", are closed curves at \"c\". If \"X\" and \"C\" are path-connected, the degree of the cover \"p\" (that is, the cardinality of any fiber of \"p\") is equal to the index [] of the subgroup in .\n\nA key result of the covering space theory says that for a \"sufficiently good\" space \"X\" (namely, if \"X\" is path-connected, locally path-connected and semi-locally simply connected) there is in fact a bijection between equivalence classes of path-connected covers of \"X\" and the conjugacy classes of subgroups of the fundamental group . The main step in proving this result is establishing the existence of a universal cover, that is a cover corresponding to the trivial subgroup of . Once the existence of a universal cover \"C\" of \"X\" is established, if \"H\" ≤ (\"X\", \"x\") is an arbitrary subgroup, the space \"C\"/\"H\" is the covering of \"X\" corresponding to \"H\". One also needs to check that two covers of \"X\" corresponding to the same (conjugacy class of) subgroup of are equivalent. Connected cell complexes and connected manifolds are examples of \"sufficiently good\" spaces.\n\nLet \"N\"(\"Γ\") be the normalizer of Γ in . The deck transformation group Aut(\"p\") is isomorphic to the quotient group \"N\"(Γ)/Γ. If \"p\" is a universal covering, then \"Γ\" is the trivial group, and Aut(\"p\") is isomorphic to (\"X\").\n\nLet us reverse this argument. Let \"N\" be a normal subgroup of . By the above arguments, this defines a (regular) covering . Let \"c\" in \"C\" be in the fiber of \"x\". Then for every other \"c\" in the fiber of \"x\", there is precisely one deck transformation that takes \"c\" to \"c\". This deck transformation corresponds to a curve \"g\" in \"C\" connecting \"c\" to \"c\".\n\nOne of the ways of expressing the algebraic content of the theory of covering spaces is using groupoids and the fundamental groupoid. The latter functor gives an equivalence of categories\n\nbetween the category of covering spaces of a reasonably nice space \"X\" and the category of groupoid covering morphisms of (\"X\"). Thus a particular kind of \"map\" of spaces is well modelled by a particular kind of \"morphism\" of groupoids. The category of covering morphisms of a groupoid \"G\" is also equivalent to the category of actions of \"G\" on sets, and this allows the recovery of more traditional classifications of coverings. Proofs of these facts are given in the book 'Topology and Groupoids' referenced below.\n\nIf \"X\" is a connected cell complex with homotopy groups for all , then the universal covering space \"T\" of \"X\" is contractible, as follows from applying the Whitehead theorem to \"T\". In this case \"X\" is a classifying space or for .\n\nMoreover, for every the group of cellular \"n\"-chains \"C\"(\"T\") (that is, a free abelian group with basis given by \"n\"-cells in \"T\") also has a natural ZG\"-module structure. Here for an \"n\"-cell \"σ\" in \"T\" and for \"g\" in \"G\" the cell \"g\" σ is exactly the translate of σ by a covering transformation of \"T\" corresponding to \"g\". Moreover, \"C\"(\"T\") is a free ZG\"-module with free Z\"G\"-basis given by representatives of \"G\"-orbits of \"n\"-cells in \"T\". In this case the standard topological chain complex\n\nwhere ε is the augmentation map, is a free ZG\"-resolution of Z (where Z is equipped with the trivial ZG\"-module structure, for every and every ). This resolution can be used to compute group cohomology of \"G\" with arbitrary coefficients.\n\nThe method of Graham Ellis for computing group resolutions and other aspects of homological algebra, as shown in his paper in J. Symbolic Comp. and his web page listed below, is to build a universal cover of a prospective inductively at the same time as a contracting homotopy of this universal cover. It is the latter which gives the computational method.\n\nAs a homotopy theory, the notion of covering spaces works well when the deck transformation group is discrete, or, equivalently, when the space is locally path-connected. However, when the deck transformation group is a topological group whose topology is not discrete, difficulties arise. Some progress has been made for more complex spaces, such as the Hawaiian earring; see the references there for further information.\n\nA number of these difficulties are resolved with the notion of \"semicovering\" due to Jeremy Brazas, see the paper cited below. Every covering map is a semicovering, but semicoverings satisfy the \"2 out of 3\" rule: given a composition of maps of spaces, if two of the maps are semicoverings, then so also is the third. This rule does not hold for coverings, since the composition of covering maps need not be a covering map.\n\nAnother generalisation is to actions of a group which are not free. Ross Geoghegan in his 1986 review () of two papers by M.A. Armstrong on the fundamental groups of orbit spaces wrote: \"These two papers show which parts of elementary covering space theory carry over from the free to the nonfree case. This is the kind of basic material that ought to have been in standard textbooks on fundamental groups for the last fifty years.\" At present, \"Topology and Groupoids\" listed below seems to be the only basic topology text to cover such results.\n\nAn important practical application of covering spaces occurs in charts on SO(3), the rotation group. This group occurs widely in engineering, due to 3-dimensional rotations being heavily used in navigation, nautical engineering, and aerospace engineering, among many other uses. Topologically, SO(3) is the real projective space RP, with fundamental group Z/2, and only (non-trivial) covering space the hypersphere \"S\", which is the group Spin(3), and represented by the unit quaternions. Thus quaternions are a preferred method for representing spatial rotations – see quaternions and spatial rotation.\n\nHowever, it is often desirable to represent rotations by a set of three numbers, known as Euler angles (in numerous variants), both because this is conceptually simpler for someone familiar with planar rotation, and because one can build a combination of three gimbals to produce rotations in three dimensions. Topologically this corresponds to a map from the 3-torus \"T\" of three angles to the real projective space RP of rotations, and the resulting map has imperfections due to this map being unable to be a covering map. Specifically, the failure of the map to be a local homeomorphism at certain points is referred to as gimbal lock, and is demonstrated in the animation at the right – at some points (when the axes are coplanar) the rank of the map is 2, rather than 3, meaning that only 2 dimensions of rotations can be realized from that point by changing the angles. This causes problems in applications, and is formalized by the notion of a covering space.\n\n\n"}
{"id": "2621463", "url": "https://en.wikipedia.org/wiki?curid=2621463", "title": "Crystal Ball function", "text": "Crystal Ball function\n\nThe Crystal Ball function, named after the Crystal Ball Collaboration (hence the capitalized initial letters), is a probability density function commonly used to model various lossy processes in high-energy physics. It consists of a Gaussian core portion and a power-law low-end tail, below a certain threshold. The function itself and its first derivative are both continuous.\n\nThe Crystal Ball function is given by:\n\nwhere\n\nformula_7 (Skwarnicki 1986) is a normalization factor and formula_8, formula_9, formula_10 and formula_11 are parameters which are fitted with the data. erf is the error function.\n\n"}
{"id": "50416", "url": "https://en.wikipedia.org/wiki?curid=50416", "title": "Differential calculus", "text": "Differential calculus\n\nIn mathematics, differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change. It is one of the two traditional divisions of calculus, the other being integral calculus, the study of the area beneath a curve.\n\nThe primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. For a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point.\n\nDifferential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration.\n\nDifferentiation has applications to nearly all quantitative disciplines. For example, in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body equals the force applied to the body; rearranging this derivative statement leads to the famous equation associated with Newton's second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research, derivatives determine the most efficient ways to transport materials and design factories.\n\nDerivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory, and abstract algebra.\n\nSuppose that and are real numbers and that is a function of , that is, for every value of , there is a corresponding value of . This relationship can be written as . If is the equation for a straight line (called a linear equation), then there are two real numbers and such that . In this \"slope-intercept form\", the term is called the slope and can be determined from the formula:\n\nwhere the symbol (the uppercase form of the Greek letter Delta) is an abbreviation for \"change in\". It follows that .\n\nA general function is not a line, so it does not have a slope. Geometrically, the derivative of at the point is the slope of the tangent line to the function at the point (see figure). This is often denoted in Lagrange's notation or in Leibniz's notation. Since the derivative is the slope of the linear approximation to at the point , the derivative (together with the value of at ) determines the best linear approximation, or linearization, of near the point .\n\nIf every point in the domain of has a derivative, there is a function that sends every point to the derivative of at . For example, if , then the derivative function .\n\nA closely related notion is the differential of a function. When and are real variables, the derivative of at is the slope of the tangent line to the graph of at . Because the source and target of are one-dimensional, the derivative of is a real number. If and are vectors, then the best linear approximation to the graph of depends on how changes in several directions at once. Taking the best linear approximation in a single direction determines a partial derivative, which is usually denoted . The linearization of in all directions at once is called the total derivative.\n\nThe concept of a derivative in the sense of a tangent line is a very old one, familiar to Greek geometers such as \nEuclid (c. 300 BC), Archimedes (c. 287–212 BC) and Apollonius of Perga (c. 262–190 BC). Archimedes also introduced the use of infinitesimals, although these were primarily used to study areas and volumes rather than derivatives and tangents; see Archimedes' use of infinitesimals.\n\nThe use of infinitesimals to study rates of change can be found in Indian mathematics, perhaps as early as 500 AD, when the astronomer and mathematician Aryabhata (476–550) used infinitesimals to study the orbit of the Moon. The use of infinitesimals to compute rates of change was developed significantly by Bhāskara II (1114–1185); indeed, it has been argued that many of the key notions of differential calculus can be found in his work, such as \"Rolle's theorem\".\n\nThe Persian mathematician, Sharaf al-Dīn al-Tūsī (1135–1213), in his \"Treatise on Equations\", established conditions for some cubic equations to have solutions, by finding the maxima of appropriate cubic polynomials. He proved, for example, that the maximum of the cubic occurs when , and concluded therefrom that the equation has exactly one positive solution when , and two positive solutions whenever . The historian of science, Roshdi Rashed, has argued that al-Tūsī must have used the derivative of the cubic to obtain this result. Rashed's conclusion has been contested by other scholars, however, who argue that he could have obtained the result by other methods which do not require the derivative of the function to be known.\n\nThe modern development of calculus is usually credited to Isaac Newton (1643–1727) and Gottfried Wilhelm Leibniz (1646–1716), who provided independent and unified approaches to differentiation and derivatives. The key insight, however, that earned them this credit, was the fundamental theorem of calculus relating differentiation and integration: this rendered obsolete most previous methods for computing areas and volumes, which had not been significantly extended since the time of Ibn al-Haytham (Alhazen). For their ideas on derivatives, both Newton and Leibniz built on significant earlier work by mathematicians such as Pierre de Fermat (1607-1665), Isaac Barrow (1630–1677), René Descartes (1596–1650), Christiaan Huygens (1629–1695), Blaise Pascal (1623–1662) and John Wallis (1616–1703). Regarding Fermat's influence, Newton once wrote in a letter that \"I had the hint of this method [of fluxions] from Fermat's way of drawing tangents, and by applying it to abstract equations, directly and invertedly, I made it general.\" Isaac Barrow is generally given credit for the early development of the derivative. Nevertheless, Newton and Leibniz remain key figures in the history of differentiation, not least because Newton was the first to apply differentiation to theoretical physics, while Leibniz systematically developed much of the notation still used today.\n\nSince the 17th century many mathematicians have contributed to the theory of differentiation. In the 19th century, calculus was put on a much more rigorous footing by mathematicians such as Augustin Louis Cauchy (1789–1857), Bernhard Riemann (1826–1866), and Karl Weierstrass (1815–1897). It was also during this period that the differentiation was generalized to Euclidean space and the complex plane.\n\nIf is a differentiable function on (or an open interval) and is a local maximum or a local minimum of , then the derivative of at is zero. Points where are called \"critical points\" or \"stationary points\" (and the value of at is called a \"critical value\"). If is not assumed to be everywhere differentiable, then points at which it fails to be differentiable are also designated critical points.\n\nIf is twice differentiable, then conversely, a critical point of can be analysed by considering the second derivative of at :\nThis is called the second derivative test. An alternative approach, called the first derivative test, involves considering the sign of the on each side of the critical point.\n\nTaking derivatives and solving for critical points is therefore often a simple way to find local minima or maxima, which can be useful in optimization. By the extreme value theorem, a continuous function on a closed interval must attain its minimum and maximum values at least once. If the function is differentiable, the minima and maxima can only occur at critical points or endpoints.\n\nThis also has applications in graph sketching: once the local minima and maxima of a differentiable function have been found, a rough plot of the graph can be obtained from the observation that it will be either increasing or decreasing between critical points.\n\nIn higher dimensions, a critical point of a scalar valued function is a point at which the gradient is zero. The second derivative test can still be used to analyse critical points by considering the eigenvalues of the Hessian matrix of second partial derivatives of the function at the critical point. If all of the eigenvalues are positive, then the point is a local minimum; if all are negative, it is a local maximum. If there are some positive and some negative eigenvalues, then the critical point is called a \"saddle point\", and if none of these cases hold (i.e., some of the eigenvalues are zero) then the test is considered to be inconclusive.\n\nOne example of an optimization problem is: Find the shortest curve between two points on a surface, assuming that the curve must also lie on the surface. If the surface is a plane, then the shortest curve is a line. But if the surface is, for example, egg-shaped, then the shortest path is not immediately clear. These paths are called geodesics, and one of the simplest problems in the calculus of variations is finding geodesics. Another example is: Find the smallest area surface filling in a closed curve in space. This surface is called a minimal surface and it, too, can be found using the calculus of variations.\n\nCalculus is of vital importance in physics: many physical processes are described by equations involving derivatives, called differential equations. Physics is particularly concerned with the way quantities change and develop over time, and the concept of the \"time derivative\" — the rate of change over time — is essential for the precise definition of several important concepts. In particular, the time derivatives of an object's position are significant in Newtonian physics:\n\n\nFor example, if an object's position on a line is given by\n\nthen the object's velocity is\n\nand the object's acceleration is\n\nwhich is constant.\n\nA differential equation is a relation between a collection of functions and their derivatives. An ordinary differential equation is a differential equation that relates functions of one variable to their derivatives with respect to that variable. A partial differential equation is a differential equation that relates functions of more than one variable to their partial derivatives. Differential equations arise naturally in the physical sciences, in mathematical modelling, and within mathematics itself. For example, Newton's second law, which describes the relationship between acceleration and force, can be stated as the ordinary differential equation\nThe heat equation in one space variable, which describes how heat diffuses through a straight rod, is the partial differential equation\nHere is the temperature of the rod at position and time and is a constant that depends on how fast heat diffuses through the rod.\n\nThe mean value theorem gives a relationship between values of the derivative and values of the original function. If is a real-valued function and and are numbers with , then the mean value theorem says that under mild hypotheses, the slope between the two points and is equal to the slope of the tangent line to at some point between and . In other words,\nIn practice, what the mean value theorem does is control a function in terms of its derivative. For instance, suppose that has derivative equal to zero at each point. This means that its tangent line is horizontal at every point, so the function should also be horizontal. The mean value theorem proves that this must be true: The slope between any two points on the graph of must equal the slope of one of the tangent lines of . All of those slopes are zero, so any line from one point on the graph to another point will also have slope zero. But that says that the function does not move up or down, so it must be a horizontal line. More complicated conditions on the derivative lead to less precise but still highly useful information about the original function.\n\nThe derivative gives the best possible linear approximation of a function at a given point, but this can be very different from the original function. One way of improving the approximation is to take a quadratic approximation. That is to say, the linearization of a real-valued function at the point is a linear polynomial , and it may be possible to get a better approximation by considering a quadratic polynomial . Still better might be a cubic polynomial , and this idea can be extended to arbitrarily high degree polynomials. For each one of these polynomials, there should be a best possible choice of coefficients , , , and that makes the approximation as good as possible.\n\nIn the neighbourhood of , for the best possible choice is always , and for the best possible choice is always . For , , and higher-degree coefficients, these coefficients are determined by higher derivatives of . should always be , and should always be . Using these coefficients gives the Taylor polynomial of . The Taylor polynomial of degree is the polynomial of degree which best approximates , and its coefficients can be found by a generalization of the above formulas. Taylor's theorem gives a precise bound on how good the approximation is. If is a polynomial of degree less than or equal to , then the Taylor polynomial of degree equals .\n\nThe limit of the Taylor polynomials is an infinite series called the Taylor series. The Taylor series is frequently a very good approximation to the original function. Functions which are equal to their Taylor series are called analytic functions. It is impossible for functions with discontinuities or sharp corners to be analytic, but there are smooth functions which are not analytic.\n\nSome natural geometric shapes, such as circles, cannot be drawn as the graph of a function. For instance, if , then the circle is the set of all pairs such that . This set is called the zero set of . It is not the same as the graph of , which is a paraboloid. The implicit function theorem converts relations such as into functions. It states that if is continuously differentiable, then around most points, the zero set of looks like graphs of functions pasted together. The points where this is not true are determined by a condition on the derivative of . The circle, for instance, can be pasted together from the graphs of the two functions . In a neighborhood of every point on the circle except and , one of these two functions has a graph that looks like the circle. (These two functions also happen to meet and , but this is not guaranteed by the implicit function theorem.)\n\nThe implicit function theorem is closely related to the inverse function theorem, which states when a function looks like graphs of invertible functions pasted together.\n\n\n"}
{"id": "149896", "url": "https://en.wikipedia.org/wiki?curid=149896", "title": "Emmy Noether", "text": "Emmy Noether\n\nAmalie Emmy Noether (; 23 March 1882 – 14 April 1935) was a German mathematician who made important contributions to abstract algebra and theoretical physics. She invariably used the name \"Emmy Noether\" in her life and publications. She was described by Pavel Alexandrov, Albert Einstein, Jean Dieudonné, Hermann Weyl and Norbert Wiener as the most important woman in the history of mathematics. As one of the leading mathematicians of her time, she developed the theories of rings, fields, and algebras. In physics, Noether's theorem explains the connection between symmetry and conservation laws.\n\nNoether was born to a Jewish family in the Franconian town of Erlangen; her father was a mathematician, Max Noether. She originally planned to teach French and English after passing the required examinations, but instead studied mathematics at the University of Erlangen, where her father lectured. After completing her dissertation in 1907 under the supervision of Paul Gordan, she worked at the Mathematical Institute of Erlangen without pay for seven years. At the time, women were largely excluded from academic positions. In 1915, she was invited by David Hilbert and Felix Klein to join the mathematics department at the University of Göttingen, a world-renowned center of mathematical research. The philosophical faculty objected, however, and she spent four years lecturing under Hilbert's name. Her \"habilitation\" was approved in 1919, allowing her to obtain the rank of \"Privatdozent\".\n\nNoether remained a leading member of the Göttingen mathematics department until 1933; her students were sometimes called the \"Noether boys\". In 1924, Dutch mathematician B. L. van der Waerden joined her circle and soon became the leading expositor of Noether's ideas: Her work was the foundation for the second volume of his influential 1931 textbook, \"Moderne Algebra\". By the time of her plenary address at the 1932 International Congress of Mathematicians in Zürich, her algebraic acumen was recognized around the world. The following year, Germany's Nazi government dismissed Jews from university positions, and Noether moved to the United States to take up a position at Bryn Mawr College in Pennsylvania. In 1935 she underwent surgery for an ovarian cyst and, despite signs of a recovery, died four days later at the age of 53.\n\nNoether's mathematical work has been divided into three \"epochs\". In the first (1908–1919), she made contributions to the theories of algebraic invariants and number fields. Her work on differential invariants in the calculus of variations, \"Noether's theorem\", has been called \"one of the most important mathematical theorems ever proved in guiding the development of modern physics\". In the second epoch (1920–1926), she began work that \"changed the face of [abstract] algebra\". In her classic 1921 paper \"Idealtheorie in Ringbereichen\" (\"Theory of Ideals in Ring Domains\") Noether developed the theory of ideals in commutative rings into a tool with wide-ranging applications. She made elegant use of the ascending chain condition, and objects satisfying it are named \"Noetherian\" in her honor. In the third epoch (1927–1935), she published works on noncommutative algebras and hypercomplex numbers and united the representation theory of groups with the theory of modules and ideals. In addition to her own publications, Noether was generous with her ideas and is credited with several lines of research published by other mathematicians, even in fields far removed from her main work, such as algebraic topology.\n\nEmmy's father, Max Noether, was descended from a family of wholesale traders in Germany. At age 14, he had been paralyzed by polio. He regained mobility, but one leg remained affected. Largely self-taught, he was awarded a doctorate from the University of Heidelberg in 1868. After teaching there for seven years, he took a position in the Bavarian city of Erlangen, where he met and married Ida Amalia Kaufmann, the daughter of a prosperous merchant. \n\nMax Noether's mathematical contributions were to algebraic geometry mainly, following in the footsteps of Alfred Clebsch. His best known results are the Brill–Noether theorem and the residue, or AF+BG theorem; several other theorems are associated with him; see Max Noether's theorem.\n\nEmmy Noether was born on 23 March 1882, the first of four children. Her first name was \"Amalie\", after her mother and paternal grandmother, but she began using her middle name at a young age.\n\nAs a girl, Noether was well liked. She did not stand out academically although she was known for being clever and friendly. She was near-sighted and talked with a minor lisp during childhood. A family friend recounted a story years later about young Noether quickly solving a brain teaser at a children's party, showing logical acumen at that early age. She was taught to cook and clean, as were most girls of the time, and she took piano lessons. She pursued none of these activities with passion, although she loved to dance.\n\nShe had three younger brothers: The eldest, Alfred, was born in 1883, was awarded a doctorate in chemistry from Erlangen in 1909, but died nine years later. Fritz Noether, born in 1884, is remembered for his academic accomplishments; after studying in Munich he made a reputation for himself in applied mathematics. The youngest, Gustav Robert, was born in 1889. Very little is known about his life; he suffered from chronic illness and died in 1928.\n\nNoether showed early proficiency in French and English. In the spring of 1900, she took the examination for teachers of these languages and received an overall score of \"sehr gut\" (very good). Her performance qualified her to teach languages at schools reserved for girls, but she chose instead to continue her studies at the University of Erlangen.\n\nThis was an unconventional decision; two years earlier, the Academic Senate of the university had declared that allowing mixed-sex education would \"overthrow all academic order\". One of only two women in a university of 986 students, Noether was only allowed to audit classes rather than participate fully, and required the permission of individual professors whose lectures she wished to attend. Despite these obstacles, on 14 July 1903 she passed the graduation exam at a \"Realgymnasium\" in Nuremberg.\n\nDuring the 1903–1904 winter semester, she studied at the University of Göttingen, attending lectures given by astronomer Karl Schwarzschild and mathematicians Hermann Minkowski, Otto Blumenthal, Felix Klein, and David Hilbert. Soon thereafter, restrictions on women's participation in that university were rescinded.\n\nNoether returned to Erlangen. She officially reentered the university in October 1904, and declared her intention to focus solely on mathematics. Under the supervision of Paul Gordan she wrote her dissertation, \"Über die Bildung des Formensystems der ternären biquadratischen Form\" (\"On Complete Systems of Invariants for Ternary Biquadratic Forms\", 1907). Gordan was a member of the \"computational\" school of invariant researchers, and Noether's thesis ended with a list of over 300 explicitly worked out invariants. This approach to invariants was later superseded by the more abstract and general approach pioneered by Hilbert. Although it had been well received, Noether later described her thesis and a number of subsequent similar papers she produced as \"crap\".\n\nFor the next seven years (1908–1915) she taught at the University of Erlangen's Mathematical Institute without pay, occasionally substituting for her father when he was too ill to lecture. In 1910 and 1911 she published an extension of her thesis work from three variables to \"n\" variables.\nGordan retired in the spring of 1910, but continued to teach occasionally with his successor, Erhard Schmidt, who left shortly afterward for a position in Breslau. Gordan retired from teaching altogether in 1911 when Schmidt's successor Ernst Fischer arrived; Gordan died a year later in December 1912.\n\nAccording to Hermann Weyl, Fischer was an important influence on Noether, in particular by introducing her to the work of David Hilbert. From 1913 to 1916 Noether published several papers extending and applying Hilbert's methods to mathematical objects such as fields of rational functions and the invariants of finite groups. This phase marks the beginning of her engagement with abstract algebra, the field of mathematics to which she would make groundbreaking contributions.\n\nNoether and Fischer shared lively enjoyment of mathematics and would often discuss lectures long after they were over; Noether is known to have sent postcards to Fischer continuing her train of mathematical thoughts.\n\nIn the spring of 1915, Noether was invited to return to the University of Göttingen by David Hilbert and Felix Klein. Their effort to recruit her, however, was blocked by the philologists and historians among the philosophical faculty: Women, they insisted, should not become \"privatdozent\". One faculty member protested: \"\"What will our soldiers think when they return to the university and find that they are required to learn at the feet of a woman?\" Hilbert responded with indignation, stating, \"I do not see that the sex of the candidate is an argument against her admission as \"privatdozent\". After all, we are a university, not a bath house.\"\"\nNoether left for Göttingen in late April; two weeks later her mother died suddenly in Erlangen. She had previously received medical care for an eye condition, but its nature and impact on her death is unknown. At about the same time Noether's father retired and her brother joined the German Army to serve in World War I. She returned to Erlangen for several weeks, mostly to care for her aging father.\n\nDuring her first years teaching at Göttingen she did not have an official position and was not paid; her family paid for her room and board and supported her academic work. Her lectures often were advertised under Hilbert's name, and Noether would provide \"assistance\".\n\nSoon after arriving at Göttingen, however, she demonstrated her capabilities by proving the theorem now known as Noether's theorem, which shows that a conservation law is associated with any differentiable symmetry of a physical system. The paper was presented by a colleague, F. Klein on 26 July 1918 to a meeting of the Royal Society of Sciences at Göttingen. Noether presumably did not present it herself because she was not a member of the society. American physicists Leon M. Lederman and Christopher T. Hill argue in their book \"Symmetry and the Beautiful Universe\" that Noether's theorem is \"certainly one of the most important mathematical theorems ever proved in guiding the development of modern physics, possibly on a par with the Pythagorean theorem\".\nWhen World War I ended, the German Revolution of 1918–1919 brought a significant change in social attitudes, including more rights for women. In 1919 the University of Göttingen allowed Noether to proceed with her \"habilitation\" (eligibility for tenure). Her oral examination was held in late May, and she successfully delivered her \"habilitation\" lecture in June 1919.\n\nThree years later she received a letter from , the Prussian Minister for Science, Art, and Public Education, in which he conferred on her the title of \"nicht beamteter ausserordentlicher Professor\" (an untenured professor with limited internal administrative rights and functions). This was an unpaid \"extraordinary\" professorship, not the higher \"ordinary\" professorship, which was a civil-service position. Although it recognized the importance of her work, the position still provided no salary. Noether was not paid for her lectures until she was appointed to the special position of \"Lehrbeauftragte für Algebra\" a year later.\n\nAlthough Noether's theorem had a significant effect upon classical and quantum mechanics, among mathematicians she is best remembered for her contributions to abstract algebra. In his introduction to Noether's \"Collected Papers\", Nathan Jacobson wrote thatThe development of abstract algebra, which is one of the most distinctive innovations of twentieth century mathematics, is largely due to her – in published papers, in lectures, and in personal influence on her contemporaries. She sometimes allowed her colleagues and students to receive credit for her ideas, helping them develop their careers at the expense of her own.\n\nNoether's work in algebra began in 1920. In collaboration with W. Schmeidler, she then published a paper about the theory of ideals in which they defined left and right ideals in a ring.\n\nThe following year she published a paper called \"Idealtheorie in Ringbereichen\", analyzing ascending chain conditions with regard to (mathematical) ideals. Noted algebraist Irving Kaplansky called this work \"revolutionary\"; the publication gave rise to the term \"Noetherian ring\" and the naming of several other mathematical objects as \"Noetherian\".\n\nIn 1924 a young Dutch mathematician, B. L. van der Waerden, arrived at the University of Göttingen. He immediately began working with Noether, who provided invaluable methods of abstract conceptualization. Van der Waerden later said that her originality was \"absolute beyond comparison\". In 1931 he published \"Moderne Algebra\", a central text in the field; its second volume borrowed heavily from Noether's work. Although Noether did not seek recognition, he included as a note in the seventh edition \"based in part on lectures by E. Artin and E. Noether\".\n\nVan der Waerden's visit was part of a convergence of mathematicians from all over the world to Göttingen, which became a major hub of mathematical and physical research. From 1926 to 1930 Russian topologist Pavel Alexandrov lectured at the university, and he and Noether quickly became good friends. He began referring to her as \"der Noether\", using the masculine German article as a term of endearment to show his respect. She tried to arrange for him to obtain a position at Göttingen as a regular professor, but was only able to help him secure a scholarship from the Rockefeller Foundation. They met regularly and enjoyed discussions about the intersections of algebra and topology. In his 1935 memorial address, Alexandrov named Emmy Noether \"the greatest woman mathematician of all time\".\n\nIn addition to her mathematical insight, Noether was respected for her consideration of others. Although she sometimes acted rudely toward those who disagreed with her, she nevertheless gained a reputation for constant helpfulness and patient guidance of new students. Her loyalty to mathematical precision caused one colleague to name her \"a severe critic\", but she combined this demand for accuracy with a nurturing attitude. A colleague later described her this way:Completely unegotistical and free of vanity, she never claimed anything for herself, but promoted the works of her students above all.\n\nIn Göttingen, Noether supervised more than a dozen doctoral students; her first was Grete Hermann, who defended her dissertation in February 1925. She later spoke reverently of her \"dissertation-mother\". Noether also supervised Max Deuring, who distinguished himself as an undergraduate and went on to contribute significantly to the field of arithmetic geometry; Hans Fitting, remembered for Fitting's theorem and the Fitting lemma; and Zeng Jiongzhi (also rendered \"Chiungtze C. Tsen\" in English), who proved Tsen's theorem. She also worked closely with Wolfgang Krull, who greatly advanced commutative algebra with his \"Hauptidealsatz\" and his dimension theory for commutative rings.\n\nHer frugal lifestyle at first was due to being denied pay for her work; however, even after the university began paying her a small salary in 1923, she continued to live a simple and modest life. She was paid more generously later in her life, but saved half of her salary to bequeath to her nephew, Gottfried E. Noether.\n\nMostly unconcerned about appearance and manners, biographers suggest she focused on her studies. A distinguished algebraist Olga Taussky-Todd described a luncheon, during which Noether, wholly engrossed in a discussion of mathematics, \"gesticulated wildly\" as she ate and \"spilled her food constantly and wiped it off from her dress, completely unperturbed\". Appearance-conscious students cringed as she retrieved the handkerchief from her blouse and ignored the increasing disarray of her hair during a lecture. Two female students once approached her during a break in a two-hour class to express their concern, but they were unable to break through the energetic mathematics discussion she was having with other students.\n\nAccording to van der Waerden's obituary of Emmy Noether, she did not follow a lesson plan for her lectures, which frustrated some students. Instead, she used her lectures as a spontaneous discussion time with her students, to think through and clarify important problems in mathematics. Some of her most important results were developed in these lectures, and the lecture notes of her students formed the basis for several important textbooks, such as those of van der Waerden and Deuring.\n\nSeveral of her colleagues attended her lectures, and she allowed some of her ideas, such as the crossed product (\"verschränktes Produkt\" in German) of associative algebras, to be published by others. Noether was recorded as having given at least five semester-long courses at Göttingen:\n\nThese courses often preceded major publications on the same subjects.\n\nNoether spoke quickly – reflecting the speed of her thoughts, many said – and demanded great concentration from her students. Students who disliked her style often felt alienated. Some pupils felt that she relied too much on spontaneous discussions. Her most dedicated students, however, relished the enthusiasm with which she approached mathematics, especially since her lectures often built on earlier work they had done together.\n\nShe developed a close circle of colleagues and students who thought along similar lines and tended to exclude those who did not. \"Outsiders\" who occasionally visited Noether's lectures usually spent only 30 minutes in the room before leaving in frustration or confusion. A regular student said of one such instance: \"The enemy has been defeated; he has cleared out.\"\n\nNoether showed a devotion to her subject and her students that extended beyond the academic day. Once, when the building was closed for a state holiday, she gathered the class on the steps outside, led them through the woods, and lectured at a local coffee house. Later, after she had been dismissed by the Third Reich, she invited students into her home to discuss their plans for the future and mathematical concepts.\n\nIn the winter of 1928–1929 Noether accepted an invitation to Moscow State University, where she continued working with P. S. Alexandrov. In addition to carrying on with her research, she taught classes in abstract algebra and algebraic geometry. She worked with the topologists, Lev Pontryagin and Nikolai Chebotaryov, who later praised her contributions to the development of Galois theory.\nAlthough politics was not central to her life, Noether took a keen interest in political matters and, according to Alexandrov, showed considerable support for the Russian Revolution. She was especially happy to see Soviet advances in the fields of science and mathematics, which she considered indicative of new opportunities made possible by the Bolshevik project. This attitude caused her problems in Germany, culminating in her eviction from a pension lodging building, after student leaders complained of living with \"a Marxist-leaning Jewess\".\n\nNoether planned to return to Moscow, an effort for which she received support from Alexandrov. After she left Germany in 1933 he tried to help her gain a chair at Moscow State University through the Soviet Education Ministry. Although this effort proved unsuccessful, they corresponded frequently during the 1930s, and in 1935 she made plans for a return to the Soviet Union. Meanwhile, her brother Fritz accepted a position at the Research Institute for Mathematics and Mechanics in Tomsk, in the Siberian Federal District of Russia, after losing his job in Germany, and was subsequently executed during the Great Purge.\n\nIn 1932 Emmy Noether and Emil Artin received the Ackermann–Teubner Memorial Award for their contributions to mathematics. The prize carried a monetary reward of 500 Reichsmarks and was seen as a long-overdue official recognition of her considerable work in the field. Nevertheless, her colleagues expressed frustration at the fact that she was not elected to the Göttingen \"Gesellschaft der Wissenschaften\" (academy of sciences) and was never promoted to the position of \"Ordentlicher Professor\" (full professor).\nNoether's colleagues celebrated her fiftieth birthday in 1932, in typical mathematicians' style. Helmut Hasse dedicated an article to her in the \"Mathematische Annalen\", wherein he confirmed her suspicion that some aspects of noncommutative algebra are simpler than those of commutative algebra, by proving a noncommutative reciprocity law. This pleased her immensely. He also sent her a mathematical riddle, which he called the \"m-riddle of syllables\". She solved immediately, but the riddle has been lost.\n\nIn November of the same year, Noether delivered a plenary address (\"großer Vortrag\") on \"Hyper-complex systems in their relations to commutative algebra and to number theory\" at the International Congress of Mathematicians in Zürich. The congress was attended by 800 people, including Noether's colleagues Hermann Weyl, Edmund Landau, and Wolfgang Krull. There were 420 official participants and twenty-one plenary addresses presented. Apparently, Noether's prominent speaking position was a recognition of the importance of her contributions to mathematics. The 1932 congress is sometimes described as the high point of her career.\n\nWhen Adolf Hitler became the German \"Reichskanzler\" in January 1933, Nazi activity around the country increased dramatically. At the University of Göttingen the German Student Association led the attack on the \"un-German spirit\" attributed to Jews and was aided by a privatdozent named Werner Weber, a former student of Noether. Antisemitic attitudes created a climate hostile to Jewish professors. One young protester reportedly demanded: \"\"Aryan students want Aryan mathematics and not Jewish mathematics\".\"\n\nOne of the first actions of Hitler's administration was the Law for the Restoration of the Professional Civil Service which removed Jews and politically suspect government employees (including university professors) from their jobs unless they had \"demonstrated their loyalty to Germany\" by serving in World War I. In April 1933 Noether received a notice from the Prussian Ministry for Sciences, Art, and Public Education which read: \"\"On the basis of paragraph 3 of the Civil Service Code of 7 April 1933, I hereby withdraw from you the right to teach at the University of Göttingen\".\" Several of Noether's colleagues, including Max Born and Richard Courant, also had their positions revoked.\n\nNoether accepted the decision calmly, providing support for others during this difficult time. Hermann Weyl later wrote that \"Emmy Noether—her courage, her frankness, her unconcern about her own fate, her conciliatory spirit—was in the midst of all the hatred and meanness, despair and sorrow surrounding us, a moral solace.\" Typically, Noether remained focused on mathematics, gathering students in her apartment to discuss class field theory. When one of her students appeared in the uniform of the Nazi paramilitary organization \"Sturmabteilung\" (SA), she showed no sign of agitation and, reportedly, even laughed about it later. This, however, was before the bloody events of Kristallnacht in 1938, and their praise from Propaganda Minister Joseph Goebbels.\n\nAs dozens of newly unemployed professors began searching for positions outside of Germany, their colleagues in the United States sought to provide assistance and job opportunities for them. Albert Einstein and Hermann Weyl were appointed by the Institute for Advanced Study in Princeton, while others worked to find a sponsor required for legal immigration. Noether was contacted by representatives of two educational institutions: Bryn Mawr College, in the United States, and Somerville College at the University of Oxford, in England. After a series of negotiations with the Rockefeller Foundation, a grant to Bryn Mawr was approved for Noether and she took a position there, starting in late 1933.\n\nAt Bryn Mawr, Noether met and befriended Anna Wheeler, who had studied at Göttingen just before Noether arrived there. Another source of support at the college was the Bryn Mawr president, Marion Edwards Park, who enthusiastically invited mathematicians in the area to \"see Dr. Noether in action!\" Noether and a small team of students worked quickly through van der Waerden's 1930 book \"Moderne Algebra I\" and parts of Erich Hecke's \"Theorie der algebraischen Zahlen\" (\"Theory of algebraic numbers\").\n\nIn 1934, Noether began lecturing at the Institute for Advanced Study in Princeton upon the invitation of Abraham Flexner and Oswald Veblen. She also worked with and supervised Abraham Albert and Harry Vandiver. However, she remarked about Princeton University that she was not welcome at \"the men's university, where nothing female is admitted\".\n\nHer time in the United States was pleasant, surrounded as she was by supportive colleagues and absorbed in her favorite subjects. In the summer of 1934 she briefly returned to Germany to see Emil Artin and her brother Fritz before he left for Tomsk. Although many of her former colleagues had been forced out of the universities, she was able to use the library as a \"foreign scholar\".\n\nIn April 1935 doctors discovered a tumor in Noether's pelvis. Worried about complications from surgery, they ordered two days of bed rest first. During the operation they discovered an ovarian cyst \"the size of a large cantaloupe\". Two smaller tumors in her uterus appeared to be benign and were not removed, to avoid prolonging surgery. For three days she appeared to convalesce normally, and she recovered quickly from a circulatory collapse on the fourth. On 14 April she fell unconscious, her temperature soared to , and she died. \"[I]t is not easy to say what had occurred in Dr. Noether\", one of the physicians wrote. \"It is possible that there was some form of unusual and virulent infection, which struck the base of the brain where the heat centers are supposed to be located.\"\n\nA few days after Noether's death her friends and associates at Bryn Mawr held a small memorial service at College President Park's house. Hermann Weyl and Richard Brauer traveled from Princeton and spoke with Wheeler and Taussky about their departed colleague. In the months that followed, written tributes began to appear around the globe: Albert Einstein joined van der Waerden, Weyl, and Pavel Alexandrov in paying their respects. Her body was cremated and the ashes interred under the walkway around the cloisters of the M. Carey Thomas Library at Bryn Mawr.\n\nNoether's work in abstract algebra and topology was influential in mathematics, while in physics, Noether's theorem has consequences for theoretical physics and dynamical systems. She showed an acute propensity for abstract thought, which allowed her to approach problems of mathematics in fresh and original ways. Her friend and colleague Hermann Weyl described her scholarly output in three epochs:\nIn the first epoch (1907–1919), Noether dealt primarily with differential and algebraic invariants, beginning with her dissertation under Paul Gordan. Her mathematical horizons broadened, and her work became more general and abstract, as she became acquainted with the work of David Hilbert, through close interactions with a successor to Gordan, Ernst Sigismund Fischer. After moving to Göttingen in 1915, she produced her work for physics, the two Noether's theorems.\n\nIn the second epoch (1920–1926), Noether devoted herself to developing the theory of mathematical rings.\n\nIn the third epoch (1927–1935), Noether focused on noncommutative algebra, linear transformations, and commutative number fields.\n\nAlthough the results of Noether's first epoch were impressive and useful, her fame among mathematicians rests more on the groundbreaking work she did in her second and third epochs, as noted by Hermann Weyl and B. L. van der Waerden in their obituaries of her.\n\nIn these epochs, she was not merely applying ideas and methods of earlier mathematicians; rather, she was crafting new systems of mathematical definitions that would be used by future mathematicians. In particular, she developed a completely new theory of ideals in rings, generalizing earlier work of Richard Dedekind. She is also renowned for developing ascending chain conditions, a simple finiteness condition that yielded powerful results in her hands. Such conditions and the theory of ideals enabled Noether to generalize many older results and to treat old problems from a new perspective, such as elimination theory and the algebraic varieties that had been studied by her father.\n\nIn the century from 1832 to Noether's death in 1935, the field of mathematics – specifically algebra – underwent a profound revolution, whose reverberations are still being felt. Mathematicians of previous centuries had worked on practical methods for solving specific types of equations, e.g., cubic, quartic, and quintic equations, as well as on the related problem of constructing regular polygons using compass and straightedge. Beginning with Carl Friedrich Gauss's 1832 proof that prime numbers such as five can be factored in Gaussian integers, Évariste Galois's introduction of permutation groups in 1832 (although, because of his death, his papers were published only in 1846, by Liouville), William Rowan Hamilton's discovery of quaternions in 1843, and Arthur Cayley's more modern definition of groups in 1854, research turned to determining the properties of ever-more-abstract systems defined by ever-more-universal rules. Noether's most important contributions to mathematics were to the development of this new field, abstract algebra.\n\nTwo of the most basic objects in abstract algebra are groups and rings.\n\nA \"group\" consists of a set of elements and a single operation which combines a first and a second element and returns a third. The operation must satisfy certain constraints for it to determine a group: It must be closed (when applied to any pair of elements of the associated set, the generated element must also be a member of that set), it must be associative, there must be an identity element (an element which, when combined with another element using the operation, results in the original element, such as adding zero to a number or multiplying it by one), and for every element there must be an inverse element.\n\nA \"ring\" likewise, has a set of elements, but now has \"two\" operations. The first operation must make the set a group, and the second operation is associative and distributive with respect to the first operation. It may or may not be commutative; this means that the result of applying the operation to a first and a second element is the same as to the second and first – the order of the elements does not matter. If every non-zero element has a multiplicative inverse (an element \"x\" such that \"a x\" = \"x a\" = 1 ), the ring is called a \"division ring\". A \"field\" is defined as a commutative division ring.\n\nGroups are frequently studied through \"group representations\". In their most general form, these consist of a choice of group, a set, and an \"action\" of the group on the set, that is, an operation which takes an element of the group and an element of the set and returns an element of the set. Most often, the set is a vector space, and the group represents symmetries of the vector space. For example, there is a group which represents the rigid rotations of space. This is a type of symmetry of space, because space itself does not change when it is rotated even though the positions of objects in it do. Noether used these sorts of symmetries in her work on invariants in physics.\n\nA powerful way of studying rings is through their \"modules\". A module consists of a choice of ring, another set, usually distinct from the underlying set of the ring and called the underlying set of the module, an operation on pairs of elements of the underlying set of the module, and an operation which takes an element of the ring and an element of the module and returns an element of the module.\n\nThe underlying set of the module and its operation must form a group. A module is a ring-theoretic version of a group representation: Ignoring the second ring operation and the operation on pairs of module elements determines a group representation. The real utility of modules is that the kinds of modules that exist and their interactions, reveal the structure of the ring in ways that are not apparent from the ring itself. An important special case of this is an \"algebra\". (The word algebra means both a subject within mathematics as well as an object studied in the subject of algebra.) An algebra consists of a choice of two rings and an operation which takes an element from each ring and returns an element of the second ring. This operation makes the second ring into a module over the first. Often the first ring is a field.\n\nWords such as \"element\" and \"combining operation\" are very general, and can be applied to many real-world and abstract situations. Any set of things that obeys all the rules for one (or two) operation(s) is, by definition, a group (or ring), and obeys all theorems about groups (or rings). Integer numbers, and the operations of addition and multiplication, are just one example. For example, the elements might be computer data words, where the first combining operation is exclusive or and the second is logical conjunction. Theorems of abstract algebra are powerful because they are general; they govern many systems. It might be imagined that little could be concluded about objects defined with so few properties, but precisely therein lay Noether's gift to discover the maximum that could be concluded from a given set of properties, or conversely, to identify the minimum set, the essential properties responsible for a particular observation. Unlike most mathematicians, she did not make abstractions by generalizing from known examples; rather, she worked directly with the abstractions. In his obituary of Noether, her student van der Waerden recalled that\nThis is the \"begriffliche Mathematik\" (purely conceptual mathematics) that was characteristic of Noether. This style of mathematics was consequently adopted by other mathematicians, especially in the (then new) field of abstract algebra.\n\nThe integers form a commutative ring whose elements are the integers, and the combining operations are addition and multiplication. Any pair of integers can be added or multiplied, always resulting in another integer, and the first operation, addition, is commutative, i.e., for any elements \"a\" and \"b\" in the ring, \"a\" + \"b\" = \"b\" + \"a\". The second operation, multiplication, also is commutative, but that need not be true for other rings, meaning that \"a\" combined with \"b\" might be different from \"b\" combined with \"a\". Examples of noncommutative rings include matrices and quaternions. The integers do not form a division ring, because the second operation cannot always be inverted; there is no integer \"a\" such that 3 × \"a\" = 1.\n\nThe integers have additional properties which do not generalize to all commutative rings. An important example is the fundamental theorem of arithmetic, which says that every positive integer can be factored uniquely into prime numbers. Unique factorizations do not always exist in other rings, but Noether found a unique factorization theorem, now called the \"Lasker–Noether theorem\", for the ideals of many rings. Much of Noether's work lay in determining what properties \"do\" hold for all rings, in devising novel analogs of the old integer theorems, and in determining the minimal set of assumptions required to yield certain properties of rings.\n\nMuch of Noether's work in the first epoch of her career was associated with invariant theory, principally algebraic invariant theory. Invariant theory is concerned with expressions that remain constant (invariant) under a group of transformations. As an everyday example, if a rigid yardstick is rotated, the coordinates (\"x\", \"y\", \"z\") and (\"x\", \"y\", \"z\") of its endpoints change, but its length \"L\" given by the formula \"L\" = Δ\"x\" + Δ\"y\" + Δ\"z\" remains the same. Invariant theory was an active area of research in the later nineteenth century, prompted in part by Felix Klein's Erlangen program, according to which different types of geometry should be characterized by their invariants under transformations, e.g., the cross-ratio of projective geometry.\n\nAn example of an \"invariant\" is the discriminant \"B\" − 4 \"A C\" of a binary quadratic form x·A x + y·B x + y·C y , where x and y are vectors and \"·\" is the dot product or \"inner product\" for the vectors. A, B, and C are linear operators on the vectors – typically matrices.\n\nThe discriminant is called \"invariant\" because it is not changed by linear substitutions x → \"a\" x + \"b\" y, \"y\" → \"c\" x + \"d\" y with determinant \"a\" \"d\" − \"b\" \"c\" = 1 . These substitutions form the special linear group \"SL\".\n\nOne can ask for all polynomials in A, B, and C that are unchanged by the action of \"SL\"; these are called the invariants of binary quadratic forms and turn out to be the polynomials in the discriminant.\n\nMore generally, one can ask for the invariants of homogeneous polynomials A \"x\" \"y\" + ... + A x \"y\" of higher degree, which will be certain polynomials in the coefficients A, ..., A, and more generally still, one can ask the similar question for homogeneous polynomials in more than two variables.\n\nOne of the main goals of invariant theory was to solve the \"finite basis problem\". The sum or product of any two invariants is invariant, and the finite basis problem asked whether it was possible to get all the invariants by starting with a finite list of invariants, called \"generators\", and then, adding or multiplying the generators together. For example, the discriminant gives a finite basis (with one element) for the invariants of binary quadratic forms.\n\nNoether's advisor, Paul Gordan, was known as the \"king of invariant theory\", and his chief contribution to mathematics was his 1870 solution of the finite basis problem for invariants of homogeneous polynomials in two variables. He proved this by giving a constructive method for finding all of the invariants and their generators, but was not able to carry out this constructive approach for invariants in three or more variables. In 1890, David Hilbert proved a similar statement for the invariants of homogeneous polynomials in any number of variables. Furthermore, his method worked, not only for the special linear group, but also for some of its subgroups such as the special orthogonal group.\n\nGalois theory concerns transformations of number fields that permute the roots of an equation. Consider a polynomial equation of a variable \"x\" of degree \"n\", in which the coefficients are drawn from some ground field, which might be, for example, the field of real numbers, rational numbers, or the integers modulo 7. There may or may not be choices of \"x\", which make this polynomial evaluate to zero. Such choices, if they exist, are called roots. If the polynomial is \"x\" + 1 and the field is the real numbers, then the polynomial has no roots, because any choice of \"x\" makes the polynomial greater than or equal to one. If the field is extended, however, then the polynomial may gain roots, and if it is extended enough, then it always has a number of roots equal to its degree.\n\nContinuing the previous example, if the field is enlarged to the complex numbers, then the polynomial gains two roots, +\"i\" and −\"i\", where \"i\" is the imaginary unit, that is, \"i\" = −1 . More generally, the extension field in which a polynomial can be factored into its roots is known as the splitting field of the polynomial.\n\nThe Galois group of a polynomial is the set of all transformations of the splitting field which preserve the ground field and the roots of the polynomial. (In mathematical jargon, these transformations are called automorphisms.) The Galois group of consists of two elements: The identity transformation, which sends every complex number to itself, and complex conjugation, which sends +\"i\" to −\"i\". Since the Galois group does not change the ground field, it leaves the coefficients of the polynomial unchanged, so it must leave the set of all roots unchanged. Each root can move to another root, however, so transformation determines a permutation of the \"n\" roots among themselves. The significance of the Galois group derives from the fundamental theorem of Galois theory, which proves that the fields lying between the ground field and the splitting field are in one-to-one correspondence with the subgroups of the Galois group.\n\nIn 1918, Noether published a paper on the inverse Galois problem. Instead of determining the Galois group of transformations of a given field and its extension, Noether asked whether, given a field and a group, it always is possible to find an extension of the field that has the given group as its Galois group. She reduced this to \"Noether's problem\", which asks whether the fixed field of a subgroup \"G\" of the permutation group \"S\" acting on the field \"k\"(\"x\", ... , \"x\") always is a pure transcendental extension of the field \"k\". (She first mentioned this problem in a 1913 paper, where she attributed the problem to her colleague Fischer.) She showed this was true for \"n\" = 2, 3, or 4. In 1969, R. G. Swan found a counter-example to Noether's problem, with \"n\" = 47 and \"G\" a cyclic group of order 47 (although this group can be realized as a Galois group over the rationals in other ways). The inverse Galois problem remains unsolved.\n\nNoether was brought to Göttingen in 1915 by David Hilbert and Felix Klein, who wanted her expertise in invariant theory to help them in understanding general relativity, a geometrical theory of gravitation developed mainly by Albert Einstein. Hilbert had observed that the conservation of energy seemed to be violated in general relativity, because gravitational energy could itself gravitate. Noether provided the resolution of this paradox, and a fundamental tool of modern theoretical physics, with Noether's first theorem, which she proved in 1915, but did not publish until 1918. She not only solved the problem for general relativity, but also determined the conserved quantities for \"every\" system of physical laws that possesses some continuous symmetry. Upon receiving her work, Einstein wrote to Hilbert:\n\nFor illustration, if a physical system behaves the same, regardless of how it is oriented in space, the physical laws that govern it are rotationally symmetric; from this symmetry, Noether's theorem shows the angular momentum of the system must be conserved. The physical system itself need not be symmetric; a jagged asteroid tumbling in space conserves angular momentum despite its asymmetry. Rather, the symmetry of the \"physical laws\" governing the system is responsible for the conservation law. As another example, if a physical experiment has the same outcome at any place and at any time, then its laws are symmetric under continuous translations in space and time; by Noether's theorem, these symmetries account for the conservation laws of linear momentum and energy within this system, respectively.\n\nNoether's theorem has become a fundamental tool of modern theoretical physics, both because of the insight it gives into conservation laws, and also, as a practical calculation tool. Her theorem allows researchers to determine the conserved quantities from the observed symmetries of a physical system. Conversely, it facilitates the description of a physical system based on classes of hypothetical physical laws. For illustration, suppose that a new physical phenomenon is discovered. Noether's theorem provides a test for theoretical models of the phenomenon:If the theory has a continuous symmetry, then Noether's theorem guarantees that the theory has a conserved quantity, and for the theory to be correct, this conservation must be observable in experiments.\n\nIn this epoch, Noether became famous for her deft use of ascending (\"Teilerkettensatz\") or descending (\"Vielfachenkettensatz\") chain conditions. A sequence of non-empty subsets \"A\", \"A\", \"A\", etc. of a set \"S\" is usually said to be \"ascending\", if each is a subset of the next\n\nConversely, a sequence of subsets of \"S\" is called \"descending\" if each contains the next subset:\n\nA chain \"becomes constant after a finite number of steps\" if there is an \"n\" such that formula_3 for all \"m\" ≥ \"n\". A collection of subsets of a given set satisfies the ascending chain condition if any ascending sequence becomes constant after a finite number of steps. It satisfies the descending chain condition if any descending sequence becomes constant after a finite number of steps.\n\nAscending and descending chain conditions are general, meaning that they can be applied to many types of mathematical objects—and, on the surface, they might not seem very powerful. Noether showed how to exploit such conditions, however, to maximum advantage.\n\nFor example: How to use chain conditions to show that every set of sub-objects has a maximal/minimal element or that a complex object can be generated by a smaller number of elements. These conclusions often are crucial steps in a proof.\n\nMany types of objects in abstract algebra can satisfy chain conditions, and usually if they satisfy an ascending chain condition, they are called \"Noetherian\" in her honor. By definition, a Noetherian ring satisfies an ascending chain condition on its left and right ideals, whereas a Noetherian group is defined as a group in which every strictly ascending chain of subgroups is finite. A Noetherian module is a module in which every strictly ascending chain of submodules becomes constant after a finite number of steps. A Noetherian space is a topological space in which every strictly ascending chain of open subspaces becomes constant after a finite number of steps; this definition makes the spectrum of a Noetherian ring a Noetherian topological space.\n\nThe chain condition often is \"inherited\" by sub-objects. For example, all subspaces of a Noetherian space, are Noetherian themselves; all subgroups and quotient groups of a Noetherian group are likewise, Noetherian; and, \"mutatis mutandis\", the same holds for submodules and quotient modules of a Noetherian module. All quotient rings of a Noetherian ring are Noetherian, but that does not necessarily hold for its subrings. The chain condition also may be inherited by combinations or extensions of a Noetherian object. For example, finite direct sums of Noetherian rings are Noetherian, as is the ring of formal power series over a Noetherian ring.\n\nAnother application of such chain conditions is in Noetherian induction—also known as well-founded induction—which is a generalization of mathematical induction. It frequently is used to reduce general statements about collections of objects to statements about specific objects in that collection. Suppose that \"S\" is a partially ordered set. One way of proving a statement about the objects of \"S\" is to assume the existence of a counterexample and deduce a contradiction, thereby proving the contrapositive of the original statement. The basic premise of Noetherian induction is that every non-empty subset of \"S\" contains a minimal element. In particular, the set of all counterexamples contains a minimal element, the \"minimal counterexample\". In order to prove the original statement, therefore, it suffices to prove something seemingly much weaker: For any counter-example, there is a smaller counter-example.\n\nNoether's paper, \"Idealtheorie in Ringbereichen\" (\"Theory of Ideals in Ring Domains\", 1921), is the foundation of general commutative ring theory, and gives one of the first general definitions of a commutative ring. Before her paper, most results in commutative algebra were restricted to special examples of commutative rings, such as polynomial rings over fields or rings of algebraic integers. Noether proved that in a ring which satisfies the ascending chain condition on ideals, every ideal is finitely generated. In 1943, French mathematician Claude Chevalley coined the term, \"Noetherian ring\", to describe this property. A major result in Noether's 1921 paper is the Lasker–Noether theorem, which extends Lasker's theorem on the primary decomposition of ideals of polynomial rings to all Noetherian rings. The Lasker–Noether theorem can be viewed as a generalization of the fundamental theorem of arithmetic which states that any positive integer can be expressed as a product of prime numbers, and that this decomposition is unique.\n\nNoether's work \"Abstrakter Aufbau der Idealtheorie in algebraischen Zahl- und Funktionenkörpern\" (\"Abstract Structure of the Theory of Ideals in Algebraic Number and Function Fields\", 1927) characterized the rings in which the ideals have unique factorization into prime ideals as the Dedekind domains: integral domains that are Noetherian, 0- or 1-dimensional, and integrally closed in their quotient fields. This paper also contains what now are called the isomorphism theorems, which describe some fundamental natural isomorphisms, and some other basic results on Noetherian and Artinian modules.\n\nIn 1923–1924, Noether applied her ideal theory to elimination theory in a formulation that she attributed to her student, Kurt Hentzelt. She showed that fundamental theorems about the factorization of polynomials could be carried over directly. Traditionally, elimination theory is concerned with eliminating one or more variables from a system of polynomial equations, usually by the method of resultants.\n\nFor illustration, a system of equations often can be written in the form   M v = 0   where a matrix (or linear transform)   M   (without the variable \"x\") times a vector v (that only has non-zero powers of \"x\") is equal to the zero vector, 0. Hence, the determinant of the matrix   M   must be zero, providing a new equation in which the variable \"x\" has been eliminated.\n\nTechniques such as Hilbert's original non-constructive solution to the finite basis problem could not be used to get quantitative information about the invariants of a group action, and furthermore, they did not apply to all group actions. In her 1915 paper, Noether found a solution to the finite basis problem for a finite group of transformations   \"G\"   acting on a finite-dimensional vector space over a field of characteristic zero. Her solution shows that the ring of invariants is generated by homogeneous invariants whose degree is less than, or equal to, the order of the finite group; this is called Noether's bound. Her paper gave two proofs of Noether's bound, both of which also work when the characteristic of the field is coprime to   |\"G\"|!   (the factorial of the order   |\"G\"|   of the group \"G\"). The degrees of generators need not satisfy Noether's bound when the characteristic of the field divides the number   |\"G\"| , but Noether was not able to determine whether this bound was correct when the characteristic of the field divides   |\"G\"|!   but not   |\"G\"| . For many years, determining the truth or falsehood of this bound for this particular case was an open problem, called \"Noether's gap\". It was finally solved independently by Fleischmann in 2000 and Fogarty in 2001, who both showed that the bound remains true.\n\nIn her 1926 paper, Noether extended Hilbert's theorem to representations of a finite group over any field; the new case that did not follow from Hilbert's work is when the characteristic of the field divides the order of the group. Noether's result was later extended by William Haboush to all reductive groups by his proof of the Mumford conjecture. In this paper Noether also introduced the \"Noether normalization lemma\", showing that a finitely generated domain \"A\" over a field \"k\" has a set   { \"x\", ... , \"x\" }   of algebraically independent elements such that \"A\" is integral over   \"k\" [\"x\", ... , \"x\"] .\n\nAs noted by Pavel Alexandrov and Hermann Weyl in their obituaries, Noether's contributions to topology illustrate her generosity with ideas and how her insights could transform entire fields of mathematics. In topology, mathematicians study the properties of objects that remain invariant even under deformation, properties such as their connectedness. An old joke is that \"a topologist cannot distinguish a donut from a coffee mug\", since they can be continuously deformed into one another.\n\nNoether is credited with fundamental ideas that led to the development of algebraic topology from the earlier combinatorial topology, specifically, the idea of homology groups. According to the account of Alexandrov, Noether attended lectures given by Heinz Hopf and by him in the summers of 1926 and 1927, where \"she continually made observations which were often deep and subtle\" and he continues that,\nNoether's suggestion that topology be studied algebraically was adopted immediately by Hopf, Alexandrov, and others, and it became a frequent topic of discussion among the mathematicians of Göttingen. Noether observed that her idea of a Betti group makes the Euler–Poincaré formula simpler to understand, and Hopf's own work on this subject \"bears the imprint of these remarks of Emmy Noether\". Noether mentions her own topology ideas only as an aside in a 1926 publication, where she cites it as an application of group theory.\n\nThis algebraic approach to topology was also developed independently in Austria. In a 1926–1927 course given in Vienna, Leopold Vietoris defined a homology group, which was developed by Walther Mayer, into an axiomatic definition in 1928.\n\nMuch work on hypercomplex numbers and group representations was carried out in the nineteenth and early twentieth centuries, but remained disparate. Noether united these results and gave the first general representation theory of groups and algebras.\n\nBriefly, Noether subsumed the structure theory of associative algebras and the representation theory of groups into a single arithmetic theory of modules and ideals in rings satisfying ascending chain conditions. This single work by Noether was of fundamental importance for the development of modern algebra.\n\nNoether also was responsible for a number of other advances in the field of algebra. With Emil Artin, Richard Brauer, and Helmut Hasse, she founded the theory of central simple algebras.\n\nA paper by Noether, Helmut Hasse, and Richard Brauer pertains to division algebras, which are algebraic systems in which division is possible. They proved two important theorems: a local-global theorem stating that if a finite-dimensional central division algebra over a number field splits locally everywhere then it splits globally (so is trivial), and from this, deduced their \"Hauptsatz\" (\"main theorem\"):every finite dimensional central division algebra over an algebraic number field F splits over a cyclic cyclotomic extension.These theorems allow one to classify all finite-dimensional central division algebras over a given number field. A subsequent paper by Noether showed, as a special case of a more general theorem, that all maximal subfields of a division algebra \"D\" are splitting fields. This paper also contains the Skolem–Noether theorem which states that any two embeddings of an extension of a field \"k\" into a finite-dimensional central simple algebra over \"k\", are conjugate. The Brauer–Noether theorem gives a characterization of the splitting fields of a central division algebra over a field.\n\nNoether's work continues to be relevant for the development of theoretical physics and mathematics and she is consistently ranked as one of the greatest mathematicians of the twentieth century. In his obituary, fellow algebraist BL van der Waerden says that her mathematical originality was \"absolute beyond comparison\", and Hermann Weyl said that Noether \"changed the face of algebra by her work\". During her lifetime and even until today, Noether has been characterized as the greatest woman mathematician in recorded history by mathematicians such as Pavel Alexandrov, Hermann Weyl, and Jean Dieudonné.\n\nIn a letter to \"The New York Times\", Albert Einstein wrote:\nOn 2 January 1935, a few months before her death, mathematician Norbert Wiener wrote that \nAt an exhibition at the 1964 World's Fair devoted to , Noether was the only woman represented among the notable mathematicians of the modern world.\n\nNoether has been honored in several memorials,\n\nIn fiction, Emmy Nutter, the physics professor in \"The God Patent\" by Ransom Stephens, is based on Emmy Noether.\n\nFarther from home,\n\n\n\n\n\n"}
{"id": "10469986", "url": "https://en.wikipedia.org/wiki?curid=10469986", "title": "Facility location problem", "text": "Facility location problem\n\nThe study of facility location problems, also known as location analysis, is a branch of operations research and computational geometry concerned with the optimal placement of facilities to minimize transportation costs while considering factors like avoiding placing hazardous materials near housing, and competitors' facilities. The techniques also apply to cluster analysis.\n\nA simple facility location problem is the Weber problem, in which a single facility is to be placed, with the only optimization criterion being the minimization of the weighted sum of distances from a given set of point sites. More complex problems considered in this discipline include the placement of multiple facilities, constraints on the locations of facilities, and more complex optimization criteria.\n\nIn a basic formulation, the facility location problem consists of a set of potential facility sites \"L\" where a facility can be opened, and a set of demand points \"D\" that must be serviced. The goal is to pick a subset \"F\" of facilities to open, to minimize the sum of distances from each demand point to its nearest facility, plus the sum of opening costs of the facilities.\n\nThe facility location problem on general graphs is NP-hard to solve optimally, by reduction from (for example) the set cover problem. A number of approximation algorithms have been developed for the facility location problem and many of its variants.\n\nWithout assumptions on the set of distances between clients and sites (in particular, without assuming that the distances satisfy the triangle inequality), the problem is known as non-metric facility location and can be approximated to within a factor O(log \"n\"). This factor is tight, via an approximation-preserving reduction from the set cover problem.\n\nIf we assume distances between clients and sites are undirected and satisfy the triangle inequality, we are talking about a metric facility location (MFL) problem. The MFL is still NP-hard and hard to approximate within factor better than 1.463. The currently best known approximation algorithm achieves approximation ratio of 1.488.\n\nThe minimax facility location problem seeks a location which minimizes the maximum distance to the sites, where the distance from one point to the sites is the distance from the point to its nearest site. A formal definition is as follows:\nGiven a point set P ⊂ ℝ, find a point set S ⊂ ℝ, |S| = \"k\", so that max(min(d(p, q)) ) is minimized.\n\nIn the case of the Euclidean metric for \"k\" = 1, it is known as the smallest enclosing sphere problem or 1-center problem. Its study traced at least to the year of 1860. see smallest enclosing circle and bounding sphere for more details.\n\nIt has been proved that exact solution of \"k\"-center problem is NP hard.\nApproximation to the problem was found to be also NP hard when the error is small. The error level in the approximation algorithm is measured as an approximation factor, which is defined as the ratio between the approximation and the optimum. It's proved that the \"k\"-center problem approximation is NP hard when approximation factor is less than 1.822 (dimension = 2) or 2 (dimension > 2).\n\nExact solver\n\nThere exist algorithms to produce exact solutions to this problem. One exact solver runs in time formula_1.\n\n1 + \"ε\" approximation\n\n1 + \"ε\" approximation is to find a solution with approximation factor no greater than 1 + \"ε\". This approximation is NP hard as \"ε\" is arbitrary. One approach based on the coreset concept is proposed with execution complexity of formula_2.\nAs an alternative, another algorithm also based on coresets is available. It runs in formula_3. The author claims that the running time is much less than the worst case and thus it's possible to solve some problems when \"k\" is small (say \"k\" < 5).\n\nFarthest-point clustering\n\nFor the hardness of the problem, it's impractical to get an exact solution or precise approximation. Instead, an approximation with factor = 2 is widely used for large \"k\" cases. The approximation is referred to as the farthest-point clustering (FPC) algorithm, or farthest-first traversal. The algorithm is quite simple: pick any point from the set as one center; search for the farthest point from remaining set as another center; repeat the process until \"k\" centers are found.\n\nIt is easy to see that this algorithm runs in linear time. As approximation with factor less than 2 is proved to be NP hard, FPC was regarded as the best approximation one can find.\n\nAs per the performance of execution, the time complexity is later improved to O(\"n\" log \"k\") with box decomposition technique.\n\nThe maxmin facility location or obnoxious facility location problem seeks a location which maximizes the minimum distance to the sites. In the case of the Euclidean metric, it is known as the largest empty sphere problem. The planar case (largest empty circle problem) may be solved in optimal time Θ(\"n\" log n).\n\nDynamic facility location problems allow considering a time-dependent plan for the optimal placement of facilities to minimize transportation costs, while serving customers in some area or region. This class of problems emerges as appropriate when changes in demands or transportation costs are known. As an extension to their static counterparts, a general mathematical programming framework for dynamic facility location problems has been recently introduced by Laporte et al. \nEfficient approaches for solving large instances of those problems have been recently proposed by Castro et al., based on a specialized application of the Benders decomposition.\n\nFacility location problems are often solved as integer programs. In this context, facility location problems are often posed as follows: suppose there are formula_4 facilities and formula_5 customers. We wish to choose (1) which of the formula_4 facilities to open, and (2) which (open) facilities to use to supply the formula_5 customers, in order to satisfy some fixed demand at minimum cost. We introduce the following notation: let formula_8 denote the (fixed) cost of opening facility formula_9, for formula_10. Let formula_11denote the cost to ship a product from facility formula_9 to customer formula_13 for formula_10 and formula_15. Let formula_16 denote the demand of customer formula_13 for formula_15. Further suppose that each facility has a maximum output. Let formula_19 denote the maximum amount of product that can be produced by facility formula_9, that is, let formula_19 denote the \"capacity\" of facility formula_9. The remainder of this section follows\n\nIn our initial formulation, introduce a binary variable formula_23 for formula_10, where formula_25 if facility formula_9 is open, and formula_27 otherwise. Further introduce the variable formula_28 for formula_10 and formula_15 which represents the fraction of the demand formula_16 filled by facility formula_9. The so-called capacitated facility location problem is then given byformula_33\n\nNote that the second set of constraints ensure that if formula_27, that is, facility formula_9 isn't open, then formula_36 for all formula_13, that is, no demand for any customer can be filled from facility formula_9.\n\nA common case of the capacitated facility location problem above is the case when formula_39 for all formula_10. In this case, it is always optimal to satisfy all of the demand from customer formula_13 from the nearest open facility. Because of this, we may replace the continuous variables formula_28 from above with the binary variables formula_43, where formula_44 if customer formula_13 is supplied by facility formula_9, and formula_47 otherwise. The uncapacitated facility location problem is then given byformula_48\n\nwhere formula_49 is a constant chosen to be suitably large. The choice of formula_49 can affect computation results--the best choice in this instance is obvious: take formula_51. Then, if formula_25, any choice of the formula_43 will satisfy the second set of constraints.\n\nAnother formulation possibility for the uncapacitated facility location problem is to \"disaggregate\" the capacity constraints (the big-formula_49 constraints). That is, replace the constraintsformula_55with the constraintsformula_56In practice, this new formulation performs significantly better, in the sense that it has a tighter Linear programming relaxation than the first formulation. Notice that summing the new constraints together yields the original big-formula_49 constraints. In the capacitated case, these formulations are not equivalent. More information about the uncapacitated facility location problem can be found in Chapter 3 of \"Discrete location theory\".\n\nhttp://www.opendoorlogistics.com/tutorials/tutorial-territory-design/step-3-automated-territory-design/\n\nLocation problems have widely been used in placing healthcare facilities. The recent review paper surveys studies on this topic.\n\n"}
{"id": "27455341", "url": "https://en.wikipedia.org/wiki?curid=27455341", "title": "Fold-and-cut theorem", "text": "Fold-and-cut theorem\n\nThe fold-and-cut theorem states that any shape with straight sides can be cut from a single (idealized) sheet of paper by folding it flat and making a single straight complete cut. Such shapes include polygons, which may be concave, shapes with holes, and collections of such shapes (i.e. the regions need not be connected).\n\nThe corresponding problem that the theorem solves is known as the fold-and-cut problem, which asks what shapes can be obtained by the so-called fold-and-cut method. A particular instance of the problem, which asks how a particular shape can be obtained by the fold-and-cut method, is known as \"a\" fold-and-cut problem.\n\nThe earliest known description of a fold-and-cut problem appears in \"Wakoku Chiyekurabe\" (Mathematical Contests), a book that was published in 1721 by Kan Chu Sen in Japan.\n\nAn 1873 article in \"Harper's New Monthly Magazine\" describes how Betsy Ross may have proposed that stars on the American flag have five points, because such a shape can easily be obtained by the fold-and-cut method.\n\nIn the 20th century, several magicians published books containing examples of fold-and-cut problems, including Will Blyth, Harry Houdini, and Gerald Loe (1955).\n\nInspired by Loe, Martin Gardner wrote about the fold-and-cut problems in \"Scientific American\" in 1960. Examples mentioned by Gardner include separating the red squares from the black squares of a checkerboard with one cut, and \"an old paper-cutting stunt, of unknown origin\" in which one cut splits a piece of paper into both a Latin cross and a set of smaller pieces that can be rearranged to spell the word \"hell\". Foreshadowing work on the general fold-and-cut theorem, he writes that \"more complicated designs present formidable problems\".\n\nThe first proof of the fold-and-cut theorem, solving the problem, was published in 1999 by Erik Demaine, Martin Demaine, and Anna Lubiw.\n\nThere are two general methods known for solving instances of the fold-and-cut problem, based on straight skeletons and on circle packing respectively.\n\n"}
{"id": "32779191", "url": "https://en.wikipedia.org/wiki?curid=32779191", "title": "Hahn–Exton q-Bessel function", "text": "Hahn–Exton q-Bessel function\n\nIn mathematics, the Hahn–Exton \"q\"-Bessel function or the third Jackson \"q\"-Bessel function is a \"q\"-analog of the Bessel function, and satisfies the Hahn-Exton \"q\"-difference equation (). This function was introduced by in a special case and by in general.\n\nThe Hahn–Exton \"q\"-Bessel function is given by \nformula_2 is the basic hypergeometric function.\n\nKoelink and Swarttouw proved that formula_3 has infinite number of real zeros.\nThey also proved that for formula_4 all non-zero roots of formula_3 are real (). For more details, see and . Zeros of the Hahn-Exton q-Bessel function appear in a discrete analog of Daniel Bernoulli's problem about free vibrations of a lump loaded chain (, )\n\nFor the (usual) derivative and q-derivative of formula_3, see . The symmetric q-derivative of formula_3 is described on .\n\nThe Hahn–Exton q-Bessel function has the following recurrence relation (see ):\n\nThe Hahn–Exton q-Bessel function has the following integral representation (see ):\nFor a contour integral representation, see .\n\nThe Hahn–Exton q-Bessel function has the following hypergeometric representation (see ):\nThis converges fast at formula_12. It is also an asymptotic expansion for formula_13.\n\n"}
{"id": "35171991", "url": "https://en.wikipedia.org/wiki?curid=35171991", "title": "Hyperkähler quotient", "text": "Hyperkähler quotient\n\nIn mathematics, the hyperkähler quotient of a hyperkähler manifold acted on by a group \"G\" is the quotient of a fiber of the hyperkähler moment map \"M\"→R⊗\"g\"* over a \"G\"-fixed point (or more generally a \"G\"-orbit) by the action of \"G\". It was introduced by . It is a hyperkähler analogue of the Kähler quotient.\n"}
{"id": "41694231", "url": "https://en.wikipedia.org/wiki?curid=41694231", "title": "Hölder summation", "text": "Hölder summation\n\nIn mathematics, Hölder summation is a method for summing divergent series introduced by .\n\nGiven a series\ndefine\nIf the limit\nexists for some \"k\", this is called the Hölder sum, or the (\"H\",\"k\") sum, of the series.\n\nParticularly, since the Cesàro sum of a convergent series always exists, the Hölder sum of a series (that is Hölder summable) can be written in the following form:\n\n\n"}
{"id": "22763305", "url": "https://en.wikipedia.org/wiki?curid=22763305", "title": "IEEE Information Theory Society", "text": "IEEE Information Theory Society\n\nThe IEEE Information Theory Society (ITS or ITSoc), formerly the IEEE Information Theory Group, is a professional society of the Institute of Electrical and Electronics Engineers (IEEE) focused on several aspects of information: its processing, transmission, storage, and usage; and the \"foundations of the communication process\".\n\nThe foundation of the society was made in 1951 when the \"IRE Professional Group on Information Theory\" (PGIT) came together for the first time. This professional group was part of the Institute of Radio Engineers (IRE). With the merge of the IRE into the Institute of Electrical and Electronics Engineers (IEEE) in 1963, the name was changed into \"IEEE Professional Technical Group on Information Theory\", but one year later simplyfied into \"IEEE Information Theory Group\". The final name \"IEEE Information Theory Society\" was taken in 1989.\n\nIEEE ITS publishes the IEEE Transactions on Information Theory and the IEEE ITS Newsletter.\n\nIEEE ITS sponsors widely attended conferences and workshops internationally each year.\nThe flagship meeting of the Information Theory Society is the IEEE International Symposium on Information Theory (ISIT).\n\nThe IEEE Information Theory Society confers several awards to recognize members and groups within the IT community for their excellence in research as well as their dedicated efforts on behalf of the Society.\n\n\n"}
{"id": "45168279", "url": "https://en.wikipedia.org/wiki?curid=45168279", "title": "Israel Kleiner (mathematician)", "text": "Israel Kleiner (mathematician)\n\nIsrael Kleiner is a Canadian mathematician and historian of mathematics.\n\nKleiner earned an MA at Yale University (1963) and a PhD at McGill University (1967) under Joachim Lambek with a thesis \"Lie modules and rings of quotients\". Before his retirement as professor emeritus, he spent his career as a mathematics professor at York University, where he was a member of the faculty since 1965 and where he coordinated the training program for mathematics teachers teaching at the secondary school level. He is noted for his work on the history of algebra and on the combination of the history of mathematics and mathematics education.\n\nHe received the Carl B. Allendoerfer Award in 1987 and again in 1992, the George Pólya Award in 1990, and the Lester Randolph Ford Award in 1995. He was in the mid 2000s vice-president of the Canadian Society for the History and Philosophy of Mathematics.\n\n\n\n"}
{"id": "31745436", "url": "https://en.wikipedia.org/wiki?curid=31745436", "title": "Iterated filtering", "text": "Iterated filtering\n\nIterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems. Stochastic perturbations to the unknown parameters are used to explore the parameter space. Applying sequential Monte Carlo (the particle filter) to this extended model results in the selection of the parameter values that are more consistent with the data. Appropriately constructed procedures, iterating with successively diminished perturbations, converge to the maximum likelihood estimate. Iterated filtering methods have so far been used most extensively to study infectious disease transmission dynamics. Case studies include cholera, Ebola virus, influenza, malaria, HIV, pertussis, poliovirus and measles. Other areas which have been proposed to be suitable for these methods include ecological dynamics and finance. \n\nThe perturbations to the parameter space play several different roles. Firstly, they smooth out the likelihood surface, enabling the algorithm to overcome small-scale features of the likelihood during early stages of the global search. Secondly, Monte Carlo variation allows the search to escape from local minima. Thirdly, the iterated filtering update uses the perturbed parameter values to construct an approximation to the derivative of the log likelihood even though this quantity is not typically available in closed form. Fourthly, the parameter perturbations help to overcome numerical difficulties that can arise during sequential Monte Carlo.\n\nThe data are a time series formula_1 collected at times formula_2. The dynamic system is modeled by a Markov process formula_3 which is generated by a function formula_4 in the sense that\n\nwhere formula_6 is a vector of unknown parameters and formula_7 is some random quantity that is drawn independently each time formula_8 is evaluated. An initial condition formula_9 at some time formula_10 is specified by an initialization function, formula_11. A measurement density formula_12 completes the specification of a partially observed Markov process. We present a basic iterated filtering algorithm (IF1) followed by an iterated filtering algorithm implementing an iterated, perturbed Bayes map (IF2).\n\n\n\"pomp: statistical inference for observed Markov processes\" : R package.\n"}
{"id": "57182978", "url": "https://en.wikipedia.org/wiki?curid=57182978", "title": "Jacques Deruyts Prize", "text": "Jacques Deruyts Prize\n\nThe Jacques Deruyts Prize, or Prix Jacques Deruyts, is a monetary prize that recognizes progress in mathematical analysis. It was first awarded in 1952 by the Academie Royale de Belgique, Classe des Sciences and is named for Jacques Deruyts. Recipients must be Belgian.\n\nThe recipients of the Jacques Deruyts Prize are:\n\n"}
{"id": "33815350", "url": "https://en.wikipedia.org/wiki?curid=33815350", "title": "Jennifer Quinn", "text": "Jennifer Quinn\n\nJennifer J. (Massey) Quinn is an American mathematician specializing in combinatorics. She is a professor of mathematics at the University of Washington Tacoma, and sits on the board of governors of the Mathematical Association of America. From 2004 to 2008 she was co-editor-in-chief of \"Math Horizons\".\nQuinn went to Williams College as an undergraduate, graduating in 1985. She earned a master's degree from the University of Illinois at Chicago in 1987, and completed her doctorate at the University of Wisconsin–Madison in 1993. Her dissertation, \"Colorings and Cycle Packings in Graphs and Digraphs\", was supervised by Richard A. Brualdi.\n\nShe taught at Occidental College until 2005, when she gave up her position as full professor and department chair to move with her husband, biologist Mark Martin, to Washington. She became a part-time lecturer, and executive director of the Association for Women in Mathematics, until being given her faculty position at Tacoma in 2007.\n\nQuinn won a Distinguished Teaching Award from the Mathematical Association of America in 2001, and the Deborah and Franklin Tepper Haimo Award for Distinguished College or University Teaching of Mathematics of the association in 2007.\n\nQuinn's book with Arthur T. Benjamin, \"Proofs that Really Count: The Art of Combinatorial Proof\" (2003) won the CHOICE Award for Outstanding Academic Title of the American Library Association and the Beckenbach Book Prize of the Mathematical Association of America.\n\nIn 2018, Quinn was elected an officer-at-large member of the board of directors of the Mathematical Association of America.\n"}
{"id": "76861", "url": "https://en.wikipedia.org/wiki?curid=76861", "title": "Julius Petersen", "text": "Julius Petersen\n\nJulius Peter Christian Petersen (16 June 1839, Sorø, West Zealand – 5 August 1910, Copenhagen) was a Danish mathematician. His contributions to the field of mathematics led to the birth of graph theory.\n\nPetersen's interests in mathematics were manifold, including: geometry, complex analysis, number theory, mathematical physics, mathematical economics, cryptography and graph theory.\nHis famous paper \"Die Theorie der regulären graphs\" was a fundamental contribution to modern graph theory as we know it today. In 1898, he presented a counterexample to Tait's claimed theorem about 1-factorability of 3-regular graphs, which is nowadays known as the \"Petersen graph\". In cryptography and mathematical economics he made contributions which today are seen as pioneering.\n\nHe published a systematic treatment of geometrical constructions (with straightedge and compass) in 1880. A French translation was reprinted in 1990.\n\nA special issue of Discrete Mathematics has been dedicated to the 150th birthday of Petersen.\n\nPetersen, as he claimed, had a very independent way of thinking. In order to preserve this independence he made a habit to read as little as possible of other people’s mathematics, pushing it to extremes. The consequences for his lack of knowledge of the literature of the time were severe. He spent a significant part of his time rediscovering already known results, in other cases already existing results had to be removed from a submitted paper and in other more serious cases a paper did not get published at all.\n\nHe started from very modest beginnings, and by hard work, some luck and some good connections, moved steadily upward to a station of considerable importance. In 1891 his work received royal recognition through the award of the Order of the Dannebrog. Among mathematicians he enjoyed an international reputation. At his death –which was front page news in Copenhagen– the socialist newspaper Social-Demokraten correctly sensed the popular appeal of his story: here was a kind of Hans Christian Andersen of science, a child of the people who had made good in the intellectual world.\n\nPeter Christian Julius Petersen was born on the 16th of June 1839 in Sorø on Zealand. His parents were Jens Petersen (1803–1873), a dyer by profession, and Anna Cathrine Petersen (1813–1896), born Wiuff. He had two younger brothers, Hans Christian Rudolf Petersen (1844–1868) and Carl Sophus Valdemar Petersen (1846–1935), and two sisters, Nielsine Cathrine Marie Petersen (1837–?) and Sophie Caroline Petersen (1842–?). After preparation in a private school, he was admitted in 1849 into second grade at the Sorø Academy, a prestigious boarding school. He was taken out of school after his confirmation in 1854, because his parents could not afford to keep him there, and he worked as an apprentice for almost a year in an uncle’s grocery in Kolding, Jutland. The uncle died, however, and left Petersen a sum of money that enabled him to return to Sorø, pass the real-examination in 1856 with distinction, and begin his studies at the Polytechnical College in Copenhagen. In 1860 Petersen passed the first part of the civil engineering examination. By that same year he had decided to study mathematics at the university, rather than to continue with the more practical second part of the engineering education. However, his inheritance was used up and he now had to teach to make a living. From 1859 to 1871 he taught at one of Copenhagen's most prestigious private high-schools, the von Westenske Institut, with occasional part-time teaching jobs at other private schools. In 1862 he passed the student-examination, and could now enter the university. In 1866 Julius Petersen obtained the degree of magister in mathematics at the University, and by 1871 he obtained the Dr. Phil. Degree at Copenhagen University. In his \"doctorvita\" written for the university, Petersen wrote: \"\"Mathematics had, from the time I started to learn it, taken my complete interest, and most of my work consisted in solving problems of my own and my friends, and in seeking the trisection of the angle, a problem that has had a great influence on my whole development\".\"\n\nIn the summer of 1871 he married Laura Kirstine Bertelsen (1837–1901) and seven months later the couple had their first son Aage Wiuff-Petersen (1863–1927). Later the family increased with another son, Thor Ejnar Petersen (1867–1946), and a daughter, Agnete Helga Kathrine Petersen (1872–1941).\n\nMany of Petersen’s early contributions to mathematics were mainly focused on geometry. During the 1860s he wrote five textbooks along with some papers, all on geometry. One of his most remarkable works was a book, \"‘Methods and Theories’\". The first edition of this book appeared only in Danish, but the 1879 edition was translated into eight different languages including English, French, and Spanish, earning him an international reputation more than any of his other works.\n\nIn graph theory, two of Petersen’s most famous contributions are: the Petersen graph, exhibited in 1898, served as a counterexample to Tait’s ‘theorem’ on the 4-colour problem: a bridgeless 3-regular graph is factorable into three 1-factors and the theorem: \"‘a connected 3-regular graph with at most two leaves contains a 1-factor’\".\n\nIn 1891 Petersen published a paper in the Acta Mathematica (volume 15, pages 193–220) entitled \"‘Die Theorie der regularen graphs’\". It was the first paper containing (correct) results explicitly in graph theory. The paper consisted of four major parts:\n\n(i) The transformation of the original algebraic problem into a graph theoretical one\n\n(ii) The problem of factorizing regular graphs of even degree. Here Petersen proves his first major result, viz. that any such graph has a 2-factorization (2-factor theorem).\n\n(iii) Criteria for the existence of edge-separating factorizations of 4-regular graphs.\n\n(iv) The factorization of regular graphs of odd degree, in particular, the theorem that any bridgeless 3-regular graph can be decomposed into a l-factor and a 2-factor (Petersen's theorem).\n\nBetween 1887 and 1895 Petersen also contributed to mathematics with different models and instruments. one of these models was a ‘eine Serie von kinematischen Modellen’ which in 1888 was asked by ‘Verlagsbuchhandler L. Brill’ for permission to produce and sell. In 1887 Petersen had constructed another model; a planimeter which was presented to the Royal Danish Academy of Science and Letters. It consisted of an arm, of, whose one end o is fixed to the paper by a lead cylinder with a pin p, and whose other end f is connected to a second arm dc (or df) of length L. When the stylus d is moved around the domain once, the area is measured as L∫dh, where dh is the differential displacement of the arm dc orthogonal to itself.\n\nIn the spring of 1908 Petersen suffered a stroke. But even in this condition his optimism and desire to work did not stop him. In a letter to Mittag-Leffler in Stockholm he wrote: \n\"“I feel in all respects rather well, it is only that I cannot walk and have difficulties in talking. However I hope to get so far this summer that I can resume my lectures in the autumn”.\" \nHis last two years became a period of physical and mental debility, where, towards the end, he hardly had any memory left of his wide interests and the rich work which had filled his life. In 1909 he retired from his professorship. He died on August 5, 1910, after having been hospitalized for five months. He was buried at Vestre Kirkegaard, where Copenhagen University cared for his grave until 1947.\n\n\n"}
{"id": "4277566", "url": "https://en.wikipedia.org/wiki?curid=4277566", "title": "Milliken–Taylor theorem", "text": "Milliken–Taylor theorem\n\nIn mathematics, the Milliken–Taylor theorem in combinatorics is a generalization of both Ramsey's theorem and Hindman's theorem. It is named after Keith Milliken and Alan D. Taylor.\n\nLet formula_1 denote the set of finite subsets of formula_2, and define a partial order on formula_1 by α<β if and only if max α<min β. Given a sequence of integers formula_4 and , let \nLet formula_6 denote the \"k\"-element subsets of a set \"S\". The Milliken–Taylor theorem says that for any finite partition formula_7, there exist some and a sequence formula_8 such that formula_9.\n\nFor each formula_4, call formula_11 an \"MT set\". Then, alternatively, the Milliken–Taylor theorem asserts that the collection of MT sets is partition regular for each \"k\".\n\n"}
{"id": "51072", "url": "https://en.wikipedia.org/wiki?curid=51072", "title": "Natural deduction", "text": "Natural deduction\n\nIn logic and proof theory, natural deduction is a kind of proof calculus in which logical reasoning is expressed by inference rules closely related to the \"natural\" way of reasoning. This contrasts with Hilbert-style systems, which instead use axioms as much as possible to express the logical laws of deductive reasoning.\n\nNatural deduction grew out of a context of dissatisfaction with the axiomatizations of deductive reasoning common to the systems of Hilbert, Frege, and Russell (see, e.g., Hilbert system). Such axiomatizations were most famously used by Russell and Whitehead in their mathematical treatise \"Principia Mathematica\". Spurred on by a series of seminars in Poland in 1926 by Łukasiewicz that advocated a more natural treatment of logic, Jaśkowski made the earliest attempts at defining a more natural deduction, first in 1929 using a diagrammatic notation, and later updating his proposal in a sequence of papers in 1934 and 1935. His proposals led to different notations\nsuch as Fitch-style calculus (or Fitch's diagrams) or Suppes' method of which e.g. Lemmon gave a variant called system L.\n\nNatural deduction in its modern form was independently proposed by the German mathematician Gerhard Gentzen in 1934, in a dissertation delivered to the faculty of mathematical sciences of the University of Göttingen. The term \"natural deduction\" (or rather, its German equivalent \"natürliches Schließen\") was coined in that paper:\n\nGentzen was motivated by a desire to establish the consistency of number theory. He was unable to prove the main result required for the consistency result, the cut elimination theorem—the Hauptsatz—directly for natural deduction. For this reason he introduced his alternative system, the sequent calculus, for which he proved the Hauptsatz both for classical and intuitionistic logic. In a series of seminars in 1961 and 1962 Prawitz gave a comprehensive summary of natural deduction calculi, and transported much of Gentzen's work with sequent calculi into the natural deduction framework. His 1965 monograph \"Natural deduction: a proof-theoretical study\" was to become a reference work on natural deduction, and included applications for modal and second-order logic.\n\nIn natural deduction, a proposition is deduced from a collection of premises by applying inference rules repeatedly. The system presented in this article is a minor variation of Gentzen's or Prawitz's formulation, but with a closer adherence to Martin-Löf's description of logical judgments and connectives.\n\nA \"judgment\" is something that is knowable, that is, an object of knowledge. It is \"evident\" if one in fact knows it. Thus \"\"it is raining\" is a judgment, which is evident for the one who knows that it is actually raining; in this case one may readily find evidence for the judgment by looking outside the window or stepping out of the house. In mathematical logic however, evidence is often not as directly observable, but rather deduced from more basic evident judgments. The process of deduction is what constitutes a \"proof\"; in other words, a judgment is evident if one has a proof for it.\n\nThe most important judgments in logic are of the form \"A is true\". The letter \"A\" stands for any expression representing a \"proposition\"; the truth judgments thus require a more primitive judgment: \"A is a proposition\". Many other judgments have been studied; for example, \"A is false\" (see classical logic), \"A is true at time t\" (see temporal logic), \"A is necessarily true\" or \"A is possibly true\" (see modal logic), \"the program M has type τ\" (see programming languages and type theory), \"A is achievable from the available resources\" (see linear logic), and many others. To start with, we shall concern ourselves with the simplest two judgments \"A is a proposition\" and \"A is true\", abbreviated as \"A\" prop\" and \"\"A\" true\" respectively.\n\nThe judgment \"\"A\" prop\" defines the structure of valid proofs of \"A\", which in turn defines the structure of propositions. For this reason, the inference rules for this judgment are sometimes known as \"formation rules\". To illustrate, if we have two propositions \"A\" and \"B\" (that is, the judgments \"\"A\" prop\" and \"\"B\" prop\" are evident), then we form the compound proposition \"A and B\", written symbolically as \"formula_1\". We can write this in the form of an inference rule:\n\nwhere the parentheses are omitted to make the inference rule more succinct:\n\nThis inference rule is \"schematic\": \"A\" and \"B\" can be instantiated with any expression. The general form of an inference rule is:\n\nwhere each formula_2 is a judgment and the inference rule is named \"name\". The judgments above the line are known as \"premises\", and those below the line are \"conclusions\". Other common logical propositions are disjunction (formula_3), negation (formula_4), implication (formula_5), and the logical constants truth (formula_6) and falsehood (formula_7). Their formation rules are below.\nNow we discuss the \"\"A\" true\" judgment. Inference rules that introduce a logical connective in the conclusion are known as \"introduction rules\". To introduce conjunctions, \"i.e.\", to conclude \"\"A and B\" true\" for propositions \"A\" and \"B\", one requires evidence for \"\"A\" true\" and \"\"B\" true\". As an inference rule:\n\nIt must be understood that in such rules the objects are propositions. That is, the above rule is really an abbreviation for:\n\nThis can also be written:\n\nIn this form, the first premise can be satisfied by the formula_8 formation rule, giving the first two premises of the previous form. In this article we shall elide the \"prop\" judgments where they are understood. In the nullary case, one can derive truth from no premises.\n\nIf the truth of a proposition can be established in more than one way, the corresponding connective has multiple introduction rules.\n\nNote that in the nullary case, \"i.e.\", for falsehood, there are \"no\" introduction rules. Thus one can never infer falsehood from simpler judgments.\n\nDual to introduction rules are \"elimination rules\" to describe how to deconstruct information about a compound proposition into information about its constituents. Thus, from \"\"A ∧ B\" true\", we can conclude \"\"A\" true\" and \"\"B\" true\":\n\nAs an example of the use of inference rules, consider commutativity of conjunction. If \"A\" ∧ \"B\" is true, then \"B\" ∧ \"A\" is true; This derivation can be drawn by composing inference rules in such a fashion that premises of a lower inference match the conclusion of the next higher inference.\n\nThe inference figures we have seen so far are not sufficient to state the rules of implication introduction or disjunction elimination; for these, we need a more general notion of \"hypothetical derivation\".\n\nA pervasive operation in mathematical logic is \"reasoning from assumptions\". For example, consider the following derivation:\n\nThis derivation does not establish the truth of \"B\" as such; rather, it establishes the following fact:\nIn logic, one says \"\"assuming A ∧ (B ∧ C) is true, we show that B is true\"; in other words, the judgment \"B\" true\" depends on the assumed judgment \"\"A ∧ (B ∧ C)\" true\". This is a \"hypothetical derivation\", which we write as follows:\n\nThe interpretation is: \"\"B true\" is derivable from \"A ∧ (B ∧ C) true\". Of course, in this specific example we actually know the derivation of \"B\" true\" from \"\"A ∧ (B ∧ C)\" true\", but in general we may not \"a priori\" know the derivation. The general form of a hypothetical derivation is:\n\nEach hypothetical derivation has a collection of \"antecedent\" derivations (the \"D\") written on the top line, and a \"succedent\" judgment (\"J\") written on the bottom line. Each of the premises may itself be a hypothetical derivation. (For simplicity, we treat a judgment as a premise-less derivation.)\n\nThe notion of hypothetical judgment is \"internalised\" as the connective of implication. The introduction and elimination rules are as follows.\nIn the introduction rule, the antecedent named \"u\" is \"discharged\" in the conclusion. This is a mechanism for delimiting the \"scope\" of the hypothesis: its sole reason for existence is to establish \"\"B\" true\"; it cannot be used for any other purpose, and in particular, it cannot be used below the introduction. As an example, consider the derivation of \"\"A ⊃ (B ⊃ (A ∧ B))\" true\":\n\nThis full derivation has no unsatisfied premises; however, sub-derivations \"are\" hypothetical. For instance, the derivation of \"\"B ⊃ (A ∧ B)\" true\" is hypothetical with antecedent \"\"A\" true\" (named \"u\").\n\nWith hypothetical derivations, we can now write the elimination rule for disjunction:\n\nIn words, if \"A ∨ B\" is true, and we can derive \"\"C\" true\" both from \"\"A\" true\" and from \"\"B\" true\", then \"C\" is indeed true. Note that this rule does not commit to either \"\"A\" true\" or \"\"B\" true\". In the zero-ary case, \"i.e.\" for falsehood, we obtain the following elimination rule:\n\nThis is read as: if falsehood is true, then any proposition \"C\" is true.\n\nNegation is similar to implication.\n\nThe introduction rule discharges both the name of the hypothesis \"u\", and the succedent \"p\", \"i.e.\", the proposition \"p\" must not occur in the conclusion \"A\". Since these rules are schematic, the interpretation of the introduction rule is: if from \"\"A\" true\" we can derive for every proposition \"p\" that \"\"p\" true\", then \"A\" must be false, \"i.e.\", \"\"not A\" true\". For the elimination, if both \"A\" and \"not A\" are shown to be true, then there is a contradiction, in which case every proposition \"C\" is true. Because the rules for implication and negation are so similar, it should be fairly easy to see that \"not A\" and \"A ⊃ ⊥\" are equivalent, i.e., each is derivable from the other.\n\nA theory is said to be consistent if falsehood is not provable (from no assumptions) and is complete if every theorem or its negation is provable using the inference rules of the logic. These are statements about the entire logic, and are usually tied to some notion of a model. However, there are local notions of consistency and completeness that are purely syntactic checks on the inference rules, and require no appeals to models. The first of these is local consistency, also known as local reducibility, which says that any derivation containing an introduction of a connective followed immediately by its elimination can be turned into an equivalent derivation without this detour. It is a check on the \"strength\" of elimination rules: they must not be so strong that they include knowledge not already contained in their premises. As an example, consider conjunctions.\nDually, local completeness says that the elimination rules are strong enough to decompose a connective into the forms suitable for its introduction rule. Again for conjunctions:\nThese notions correspond exactly to β-reduction (beta reduction) and η-conversion (eta conversion) in the lambda calculus, using the Curry–Howard isomorphism. By local completeness, we see that every derivation can be converted to an equivalent derivation where the principal connective is introduced. In fact, if the entire derivation obeys this ordering of eliminations followed by introductions, then it is said to be \"normal\". In a normal derivation all eliminations happen above introductions. In most logics, every derivation has an equivalent normal derivation, called a \"normal form\". The existence of normal forms is generally hard to prove using natural deduction alone, though such accounts do exist in the literature, most notably by Dag Prawitz in 1961. It is much easier to show this indirectly by means of a cut-free sequent calculus presentation.\n\nThe logic of the earlier section is an example of a \"single-sorted\" logic, \"i.e.\", a logic with a single kind of object: propositions. Many extensions of this simple framework have been proposed; in this section we will extend it with a second sort of \"individuals\" or \"terms\". More precisely, we will add a new kind of judgment, \"\"t is a term\" (or \"t term\") where \"t\" is schematic. We shall fix a countable set \"V\" of \"variables\", another countable set \"F\" of \"function symbols\", and construct terms as follows:\n\nFor propositions, we consider a third countable set \"P\" of \"predicates\", and define \"atomic predicates over terms\" with the following formation rule:\n\nIn addition, we add a pair of \"quantified\" propositions: universal (∀) and existential (∃):\n\nThese quantified propositions have the following introduction and elimination rules.\n\nIn these rules, the notation [\"t\"/\"x\"] \"A\" stands for the substitution of \"t\" for every (visible) instance of \"x\" in \"A\", avoiding capture; see the article on lambda calculus for more detail about this standard operation. As before the superscripts on the name stand for the components that are discharged: the term \"a\" cannot occur in the conclusion of ∀I (such terms are known as \"eigenvariables\" or \"parameters\"), and the hypotheses named \"u\" and \"v\" in ∃E are localised to the second premise in a hypothetical derivation. Although the propositional logic of earlier sections was decidable, adding the quantifiers makes the logic undecidable.\n\nSo far the quantified extensions are \"first-order\": they distinguish propositions from the kinds of objects quantified over. Higher-order logic takes a different approach and has only a single sort of propositions. The quantifiers have as the domain of quantification the very same sort of propositions, as reflected in the formation rules:\n\nA discussion of the introduction and elimination forms for higher-order logic is beyond the scope of this article. It is possible to be in-between first-order and higher-order logics. For example, second-order logic has two kinds of propositions, one kind quantifying over terms, and the second kind quantifying over propositions of the first kind.\n\nGentzen's discharging annotations used to internalise hypothetical judgments can be avoided by representing proofs as a tree of sequents \"Γ ⊢A\" instead of a tree of \"A true\" judgments.\n\nJaśkowski's representations of natural deduction led to different notations such as Fitch-style calculus (or Fitch's diagrams) or Suppes' method, of which Lemmon gave a variant called system L. Such presentation systems, which are more accurately described as tabular, include the following.\n\nThe presentation of natural deduction so far has concentrated on the nature of propositions without giving a formal definition of a \"proof\". To formalise the notion of proof, we alter the presentation of hypothetical derivations slightly. We label the antecedents with \"proof variables\" (from some countable set \"V\" of variables), and decorate the succedent with the actual proof. The antecedents or \"hypotheses\" are separated from the succedent by means of a \"turnstile\" (⊢). This modification sometimes goes under the name of \"localised hypotheses\". The following diagram summarises the change.\n\nThe collection of hypotheses will be written as Γ when their exact composition is not relevant.\nTo make proofs explicit, we move from the proof-less judgment \"A true\"\" to a judgment: \"π \"is a proof of (A true)\"\", which is written symbolically as \"π : \"A true\"\". Following the standard approach, proofs are specified with their own formation rules for the judgment \"π \"proof\"\". The simplest possible proof is the use of a labelled hypothesis; in this case the evidence is the label itself.\n\nFor brevity, we shall leave off the judgmental label \"true\" in the rest of this article, \"i.e.\", write \"Γ ⊢ π : \"A\"\". Let us re-examine some of the connectives with explicit proofs. For conjunction, we look at the introduction rule ∧I to discover the form of proofs of conjunction: they must be a pair of proofs of the two conjuncts. Thus:\n\nThe elimination rules ∧E and ∧E select either the left or the right conjunct; thus the proofs are a pair of projections—first (fst) and second (snd).\n\nFor implication, the introduction form localises or \"binds\" the hypothesis, written using a λ; this corresponds to the discharged label. In the rule, \"Γ, \"u\":\"A\"\" stands for the collection of hypotheses Γ, together with the additional hypothesis \"u\".\n\nWith proofs available explicitly, one can manipulate and reason about proofs. The key operation on proofs is the substitution of one proof for an assumption used in another proof. This is commonly known as a \"substitution theorem\", and can be proved by induction on the depth (or structure) of the second judgment.\n\n\nSo far the judgment \"Γ ⊢ π : \"A\"\" has had a purely logical interpretation. In type theory, the logical view is exchanged for a more computational view of objects. Propositions in the logical interpretation are now viewed as \"types\", and proofs as programs in the lambda calculus. Thus the interpretation of \"π : \"A\" is \"the program\" π has type \"A\"\". The logical connectives are also given a different reading: conjunction is viewed as product (×), implication as the function arrow (→), etc. The differences are only cosmetic, however. Type theory has a natural deduction presentation in terms of formation, introduction and elimination rules; in fact, the reader can easily reconstruct what is known as \"simple type theory\" from the previous sections.\n\nThe difference between logic and type theory is primarily a shift of focus from the types (propositions) to the programs (proofs). Type theory is chiefly interested in the convertibility or reducibility of programs. For every type, there are canonical programs of that type which are irreducible; these are known as \"canonical forms\" or \"values\". If every program can be reduced to a canonical form, then the type theory is said to be \"normalising\" (or \"weakly normalising\"). If the canonical form is unique, then the theory is said to be \"strongly normalising\". Normalisability is a rare feature of most non-trivial type theories, which is a big departure from the logical world. (Recall that almost every logical derivation has an equivalent normal derivation.) To sketch the reason: in type theories that admit recursive definitions, it is possible to write programs that never reduce to a value; such looping programs can generally be given any type. In particular, the looping program has type ⊥, although there is no logical proof of \"⊥ \"true\". For this reason, the \"propositions as types; proofs as programs\" paradigm only works in one direction, if at all: interpreting a type theory as a logic generally gives an inconsistent logic.\n\nLike logic, type theory has many extensions and variants, including first-order and higher-order versions. One branch, known as dependent type theory, is used in a number of computer-assisted proof systems. Dependent type theory allows quantifiers to range over programs themselves. These quantified types are written as Π and Σ instead of ∀ and ∃, and have the following formation rules:\n\nThese types are generalisations of the arrow and product types, respectively, as witnessed by their introduction and elimination rules.\nDependent type theory in full generality is very powerful: it is able to express almost any conceivable property of programs directly in the types of the program. This generality comes at a steep price — either typechecking is undecidable (extensional type theory), or extensional reasoning is more difficult (intensional type theory). For this reason, some dependent type theories do not allow quantification over arbitrary programs, but rather restrict to programs of a given decidable \"index domain\", for example integers, strings, or linear programs.\n\nSince dependent type theories allow types to depend on programs, a natural question to ask is whether it is possible for programs to depend on types, or any other combination. There are many kinds of answers to such questions. A popular approach in type theory is to allow programs to be quantified over types, also known as \"parametric polymorphism\"; of this there are two main kinds: if types and programs are kept separate, then one obtains a somewhat more well-behaved system called \"predicative polymorphism\"; if the distinction between program and type is blurred, one obtains the type-theoretic analogue of higher-order logic, also known as \"impredicative polymorphism\". Various combinations of dependency and polymorphism have been considered in the literature, the most famous being the lambda cube of Henk Barendregt.\n\nThe intersection of logic and type theory is a vast and active research area. New logics are usually formalised in a general type theoretic setting, known as a logical framework. Popular modern logical frameworks such as the calculus of constructions and LF are based on higher-order dependent type theory, with various trade-offs in terms of decidability and expressive power. These logical frameworks are themselves always specified as natural deduction systems, which is a testament to the versatility of the natural deduction approach.\n\nFor simplicity, the logics presented so far have been intuitionistic. Classical logic extends intuitionistic logic with an additional axiom or principle of excluded middle:\n\nThis statement is not obviously either an introduction or an elimination; indeed, it involves two distinct connectives. Gentzen's original treatment of excluded middle prescribed one of the following three (equivalent) formulations, which were already present in analogous forms in the systems of Hilbert and Heyting:\n(XM is merely XM expressed in terms of E.) This treatment of excluded middle, in addition to being objectionable from a purist's standpoint, introduces additional complications in the definition of normal forms.\n\nA comparatively more satisfactory treatment of classical natural deduction in terms of introduction and elimination rules alone was first proposed by Parigot in 1992 in the form of a classical lambda calculus called λμ. The key insight of his approach was to replace a truth-centric judgment \"A true\" with a more classical notion, reminiscent of the sequent calculus: in localised form, instead of Γ ⊢ \"A\", he used Γ ⊢ Δ, with Δ a collection of propositions similar to Γ. Γ was treated as a conjunction, and Δ as a disjunction. This structure is essentially lifted directly from classical sequent calculi, but the innovation in λμ was to give a computational meaning to classical natural deduction proofs in terms of a callcc or a throw/catch mechanism seen in LISP and its descendants. (See also: first class control.)\n\nAnother important extension was for modal and other logics that need more than just the basic judgment of truth. These were first described, for the alethic modal logics S4 and S5, in a natural deduction style by Prawitz in 1965, and have since accumulated a large body of related work. To give a simple example, the modal logic S4 requires one new judgment, \"A valid\"\", that is categorical with respect to truth:\n\nThis categorical judgment is internalised as a unary connective ◻\"A\" (read \"\"necessarily A\") with the following introduction and elimination rules:\nNote that the premise \"A valid\"\" has no defining rules; instead, the categorical definition of validity is used in its place. This mode becomes clearer in the localised form when the hypotheses are explicit. We write \"Ω;Γ ⊢ \"A true\" where Γ contains the true hypotheses as before, and Ω contains valid hypotheses. On the right there is just a single judgment \"A true\"\"; validity is not needed here since \"Ω ⊢ \"A valid\"\" is by definition the same as \"Ω;⋅ ⊢ \"A true\"\". The introduction and elimination forms are then:\nThe modal hypotheses have their own version of the hypothesis rule and substitution theorem.\n\nThis framework of separating judgments into distinct collections of hypotheses, also known as \"multi-zoned\" or \"polyadic\" contexts, is very powerful and extensible; it has been applied for many different modal logics, and also for linear and other substructural logics, to give a few examples. However, relatively few systems of modal logic can be formalised directly in natural deduction. To give proof-theoretic characterisations of these systems, extensions such as labelling or systems of deep inference.\n\nThe addition of labels to formulae permits much finer control of the conditions under which rules apply, allowing the more flexible techniques of analytic tableaux to be applied, as has been done in the case of labelled deduction. Labels also allow the naming of worlds in Kripke semantics; presents an influential technique for converting frame conditions of modal logics in Kripke semantics into inference rules in a natural deduction formalisation of hybrid logic. surveys the application of many proof theories, such as Avron and Pottinger's hypersequents and Belnap's display logic to such modal logics as S5 and B.\n\nThe sequent calculus is the chief alternative to natural deduction as a foundation of mathematical logic. In natural deduction the flow of information is bi-directional: elimination rules flow information downwards by deconstruction, and introduction rules flow information upwards by assembly. Thus, a natural deduction proof does not have a purely bottom-up or top-down reading, making it unsuitable for automation in proof search. To address this fact, Gentzen in 1935 proposed his sequent calculus, though he initially intended it as a technical device for clarifying the consistency of predicate logic. Kleene, in his seminal 1952 book \"Introduction to Metamathematics\", gave the first formulation of the sequent calculus in the modern style.\n\nIn the sequent calculus all inference rules have a purely bottom-up reading. Inference rules can apply to elements on both sides of the turnstile. (To differentiate from natural deduction, this article uses a double arrow ⇒ instead of the right tack ⊢ for sequents.) The introduction rules of natural deduction are viewed as \"right rules\" in the sequent calculus, and are structurally very similar. The elimination rules on the other hand turn into \"left rules\" in the sequent calculus. To give an example, consider disjunction; the right rules are familiar:\n\nOn the left:\n\nRecall the ∨E rule of natural deduction in localised form:\n\nThe proposition \"A ∨ B\", which is the succedent of a premise in ∨E, turns into a hypothesis of the conclusion in the left rule ∨L. Thus, left rules can be seen as a sort of inverted elimination rule. This observation can be illustrated as follows:\n\nIn the sequent calculus, the left and right rules are performed in lock-step until one reaches the \"initial sequent\", which corresponds to the meeting point of elimination and introduction rules in natural deduction. These initial rules are superficially similar to the hypothesis rule of natural deduction, but in the sequent calculus they describe a \"transposition\" or a \"handshake\" of a left and a right proposition:\n\nThe correspondence between the sequent calculus and natural deduction is a pair of soundness and completeness theorems, which are both provable by means of an inductive argument.\n\n\nIt is clear by these theorems that the sequent calculus does not change the notion of truth, because the same collection of propositions remain true. Thus, one can use the same proof objects as before in sequent calculus derivations. As an example, consider the conjunctions. The right rule is virtually identical to the introduction rule\n\nThe left rule, however, performs some additional substitutions that are not performed in the corresponding elimination rules.\nThe kinds of proofs generated in the sequent calculus are therefore rather different from those of natural deduction. The sequent calculus produces proofs in what is known as the \"β-normal η-long\" form, which corresponds to a canonical representation of the normal form of the natural deduction proof. If one attempts to describe these proofs using natural deduction itself, one obtains what is called the \"intercalation calculus\" (first described by John Byrnes), which can be used to formally define the notion of a \"normal form\" for natural deduction.\n\nThe substitution theorem of natural deduction takes the form of a structural rule or structural theorem known as \"cut\" in the sequent calculus.\n\n\nIn most well behaved logics, cut is unnecessary as an inference rule, though it remains provable as a meta-theorem; the superfluousness of the cut rule is usually presented as a computational process, known as \"cut elimination\". This has an interesting application for natural deduction; usually it is extremely tedious to prove certain properties directly in natural deduction because of an unbounded number of cases. For example, consider showing that a given proposition is \"not\" provable in natural deduction. A simple inductive argument fails because of rules like ∨E or E which can introduce arbitrary propositions. However, we know that the sequent calculus is complete with respect to natural deduction, so it is enough to show this unprovability in the sequent calculus. Now, if cut is not available as an inference rule, then all sequent rules either introduce a connective on the right or the left, so the depth of a sequent derivation is fully bounded by the connectives in the final conclusion. Thus, showing unprovability is much easier, because there are only a finite number of cases to consider, and each case is composed entirely of sub-propositions of the conclusion. A simple instance of this is the \"global consistency\" theorem: \"⋅ ⊢ ⊥ \"true\"\" is not provable. In the sequent calculus version, this is manifestly true because there is no rule that can have \"⋅ ⇒ ⊥\" as a conclusion! Proof theorists often prefer to work on cut-free sequent calculus formulations because of such properties.\n\n\n"}
{"id": "9236652", "url": "https://en.wikipedia.org/wiki?curid=9236652", "title": "Néron–Tate height", "text": "Néron–Tate height\n\nIn number theory, the Néron–Tate height (or canonical height) is a quadratic form on the Mordell-Weil group of rational points of an abelian variety defined over a global field. It is named after André Néron and John Tate.\n\nNéron defined the Néron–Tate height as a sum of local heights. Although the global Néron–Tate height is quadratic, the constituent local heights are not quite quadratic. Tate (unpublished) defined it globally by observing that the logarithmic height formula_1 associated to a symmetric invertible sheaf formula_2 on an abelian variety formula_3 is “almost quadratic,” and used this to show that the limit\n\nexists, defines a quadratic form on the Mordell-Weil group of rational points, and satisfies\n\nwhere the implied formula_6 constant is independent of formula_7. If formula_2 is anti-symmetric, that is formula_9, then the analogous limit \n\nconverges and satisfies formula_11, but in this case formula_12 is a linear function on the Mordell-Weil group. For general invertible sheaves, one writes formula_13 as a product of a symmetric sheaf and an anti-symmetric sheaf, and then\n\nis the unique quadratic function satisfying\n\nThe Néron–Tate height depends on the choice of an invertible sheaf on the abelian variety, although the associated bilinear form depends only on the image of formula_2 in\nthe Néron–Severi group of formula_3. If the abelian variety formula_3 is defined over a number field \"K\" and the invertible sheaf is symmetric and ample, then the Néron–Tate height is positive definite in the sense that it vanishes only on torsion elements of the Mordell-Weil group formula_19. More generally, formula_12 induces a positive definite quadratic form on the real vector space formula_21.\n\nOn an elliptic curve, the Néron-Severi group is of rank one and has a unique ample generator, so this generator is often used to define the Néron–Tate height, which is denoted formula_22 without reference to a particular line bundle. (However, the height that naturally appears in the statement of the Birch–Swinnerton-Dyer conjecture is twice this height.) On abelian varieties of higher dimension, there need not be a particular choice of smallest ample line bundle to be used in defining the Néron–Tate height, and the height used in the statement of the Birch–Swinnerton-Dyer conjecture is the Néron–Tate height associated to the Poincaré line bundle on formula_23, the product of formula_3 with its dual.\n\nThe bilinear form associated to the canonical height formula_22 on an elliptic curve \"E\" is \n\nThe elliptic regulator of \"E/K\" is\n\nwhere \"P,…,P\" is a basis for the Mordell-Weil group \"E\"(\"K\") modulo torsion (cf. Gram determinant). The elliptic regulator does not depend on the choice of basis. \n\nMore generally, let \"A/K\" be an abelian variety, let \"B\" ≅ Pic(\"A\") be the dual abelian variety to \"A\", and let \"P\" be the Poincaré line bundle on \"A\" × \"B\". Then the abelian regulator of \"A/K\" is defined by choosing a basis \"Q,…,Q\" for the Mordell-Weil group \"A\"(\"K\") modulo torsion and a basis η,…,η for the Mordell-Weil group \"B\"(\"K\") modulo torsion and setting\n\nThe elliptic and abelian regulators appear in the Birch–Swinnerton-Dyer conjecture.\n\nThere are two fundamental conjectures that give lower bounds for the Néron–Tate height. In the first, the field \"K\" is fixed and the elliptic curve \"E/K\" and point \"P ∈ E(K)\" vary, while in the second, the elliptic Lehmer conjecture, the curve \"E/K\" is fixed while the field of definition of the point \"P\" varies. \n\n\nIn both conjectures, the constants are positive and depend only on the indicated quantities. (A stronger form of Lang's conjecture asserts that formula_34 depends only on the degree formula_35.) It is known that the \"abc\" conjecture implies Lang's conjecture, and that the analogue of Lang's conjecture over one dimensional characteristic 0 function fields is unconditionally true. The best general result on Lehmer's conjecture is the weaker estimate formula_36 due to Masser. When the elliptic curve has complex multiplication, this has been improved to formula_37 by Laurent. There are analogous conjectures for abelian varieties, with the nontorsion condition replaced by the condition that the multiples of formula_7 form a Zariski dense subset of formula_3, and the lower bound in Lang's conjecture replaced by formula_40, where formula_41 is the Faltings height of formula_42.\n\nA polarized algebraic dynamical system is a triple (\"V\",φ,\"L\") consisting of a (smooth projective) algebraic variety \"V\", a self-morphism φ : V → V, and a line bundle \"L\" on \"V\" with the property that formula_43 for some integer \"d\" > 1. The associated canonical height is given by the Tate limit\n\nwhere φ = φ o φ o … o φ is the \"n\"-fold iteration of φ. For example, any morphism φ : P → P of degree \"d\" > 1 yields a canonical height associated to the line bundle relation φ*\"O\"(1) = \"O\"(\"d\"). If \"V\" is defined over a number field and \"L\" is ample, then the canonical height is non-negative, and\n\nGeneral references for the theory of canonical heights\n"}
{"id": "208515", "url": "https://en.wikipedia.org/wiki?curid=208515", "title": "Object-oriented software engineering", "text": "Object-oriented software engineering\n\nObject-oriented software engineering (commonly known by acronym OOSE) is an object-modeling language and methodology.\n\nOOSE was developed by Ivar Jacobson in 1992 while at Objectory AB. It is the first object-oriented design methodology to employ use cases to drive software design. It also uses other design products similar to those used by Object-modeling technique.\n\nIt was documented in the 1992 book \"Object-Oriented Software Engineering: A Use Case Driven Approach\", \n\nThe tool Objectory was created by the team at Objectory AB to implement the OOSE methodology. After success in the marketplace, other tool vendors also supported OOSE.\n\nAfter Rational Software bought Objectory AB, the OOSE notation, methodology, and tools became superseded.\n\n\nOOSE has been largely replaced by the UML notation and by the RUP methodology.\n\nMain Issues:\n1. Software products can get very complex.\n2. High-quality results are expected.\n3. The development team can be large and distributed.\n4. Most projects add functionality to an existing product.\n"}
{"id": "2535430", "url": "https://en.wikipedia.org/wiki?curid=2535430", "title": "Object Modeling in Color", "text": "Object Modeling in Color\n\nUML color standards are a set of four colors associated with Unified Modeling Language (UML) diagrams. The coloring system indicates which of several archetypes apply to the UML object. UML typically identifies a stereotype with a bracketed comment for each object identifying whether it is a class, interface, etc.\n\nThese colors were first suggested by Peter Coad, Eric Lefebvre, and Jeff De Luca in a series of articles in \"The Coad Letter\", and later published in their book \"Java Modeling In Color With UML\".\n\nOver hundreds of domain models, it became clear that four major \"types\" of classes appeared again and again, though they had different names in different domains. After much discussion, these were termed archetypes, which is meant to convey that the classes of a given archetype follow more or less the same form. That is, attributes, methods, associations, and interfaces are fairly similar among classes of a given archetype.\n\nWhen attempting to classify a given domain class, one typically asks about the color standards in this order:\n\nAlthough the actual colors vary, most systems tend to use lighter color palettes so that black text can also be easily read on a colored background. Coad, et al., used the 4-color pastel Post-it notes, and later had UML modeling tools support the color scheme by associating a color to one or more class stereotypes.\n\nMany people feel colored objects appeal to the pattern recognition section of the brain. Others advocate that you can begin a modeling process with a stack of four-color note cards or colored sticky notes.\n\nThe value of color modeling was especially obvious when standing back from a model drawn or projected on a wall. That extra dimension allowed modelers to see important aspects of the models (the pink classes, for instance), and to spot areas that may need reviewing (unusual combinations of color classes linked together).\n\nThe technique also made it easy to help determine aspects of the domain model – especially for newcomers to modeling. For example, by simply looking first for \"pinks\" in the domain, it was easy to begin to get some important classes identified for a given domain. It was also easy to review the standard types of attributes, methods, and so on, for applicability to the current domain effort.\n\n\n\n"}
{"id": "256737", "url": "https://en.wikipedia.org/wiki?curid=256737", "title": "Ordered exponential", "text": "Ordered exponential\n\nThe ordered exponential, also called the path-ordered exponential, is a mathematical operation defined in non-commutative algebras, equivalent to the exponential of the integral in the commutative algebras. In practice the ordered exponential is used in matrix and operator algebras.\n\nLet \"A\" be an algebra over a real or complex field \"K\", and \"a\"(\"t\") be a parameterized element of \"A\",\n\nThe parameter \"t\" in \"a\"(\"t\") is often referred to as the \"time parameter\" in this context.\n\nThe ordered exponential of \"a\" is denoted\n\nwhere the term \"n\" = 0 is equal to 1 and\nwhere formula_3 is a higher-order operation that ensures the exponential is time-ordered: any product of \"a\"(\"t\") that occurs in the expansion of the exponential must be ordered such that the value of \"t\" is increasing from right to left of the product; a schematic example:\nThis restriction is necessary as products in the algebra are not necessarily commutative.\n\nThe operation maps a parameterized element onto another parameterized element, or symbolically,\n\nThere are various ways to define this integral more rigorously.\n\nThe ordered exponential can be defined as the left product integral of the infinitesimal exponentials, or equivalently, as an ordered product of exponentials in the limit as the number of terms grows to infinity:\n\nwhere the time moments are defined as for , and .\n\nThe ordered exponential is in fact a geometric integral. \n\nThe ordered exponential is unique solution of the initial value problem:\n\nThe ordered exponential is the solution to the integral equation:\n\nThis equation is equivalent to the previous initial value problem.\n\nThe ordered exponential can be defined as an infinite sum,\n\nThis can be derived by recursively substituting the integral equation into itself.\n\nGiven a manifold formula_10 where for a formula_11 with group transformation formula_12 it holds at a point formula_13:\n\nHere, formula_15 denotes exterior differentiation and formula_16 is the connection operator (1-form field) acting on formula_17. When integrating above equation it holds (now, formula_16 is the connection operator expressed in a coordinate basis)\n\nwith the path-ordering operator formula_20 that orders factors in order of the path formula_21. For the special case that formula_16 is an antisymmetric operator and formula_23 is an infinitesimal rectangle with edge lengths formula_24 and corners at points formula_25 above expression simplifies as follows :\n\nHence, it holds the group transformation identity formula_27. If formula_28 is a smooth connection, expanding above quantity to second order in infinitesimal quantities formula_24 one obtains for the ordered exponential the identity with a correction term that is proportional to the curvature tensor.\n\n\n"}
{"id": "1346015", "url": "https://en.wikipedia.org/wiki?curid=1346015", "title": "PORS", "text": "PORS\n\nPORS stands for Plus One Recall Store. It is a problem used in evolutionary computation and genetic programming.\n\nThe PORS language consists of two terminal nodes (1 and recall), one unary operation (store) and one binary operation (plus) that together make up a parse tree that calculates a number.\n"}
{"id": "592505", "url": "https://en.wikipedia.org/wiki?curid=592505", "title": "Padding (cryptography)", "text": "Padding (cryptography)\n\nIn cryptography, padding refers to a number of distinct practices.\n\nOfficial messages often start and end in predictable ways: \"My dear ambassador, Weather report, Sincerely yours\", etc. The primary use of padding with classical ciphers is to prevent the cryptanalyst from using that predictability to find known plaintext that aids in breaking the encryption. Random length padding also prevents an attacker from knowing the exact length of the plaintext message.\n\nMany classical ciphers arrange the plaintext into particular patterns (e.g., squares, rectangles, etc.) and if the plaintext doesn't exactly fit, it is often necessary to supply additional letters to fill out the pattern. Using nonsense letters for this purpose has a side benefit of making some kinds of cryptanalysis more difficult.\n\nA famous example of classical padding which caused a great misunderstanding is \"the world wonders\".\n\nMost modern cryptographic hash functions process messages in fixed-length blocks; all but the earliest hash functions include some sort of padding scheme. It is critical for cryptographic hash functions to employ termination schemes that prevent a hash from being vulnerable to length extension attacks.\n\nMany padding schemes are based on appending predictable data to the final block. For example, the pad could be derived from the total length of the message. This kind of padding scheme is commonly applied to hash algorithms that use the Merkle–Damgård construction.\n\nElectronic codebook and cipher-block chaining (CBC) mode are examples of block cipher mode of operation. Block cipher modes for symmetric-key encryption algorithms require plain text input that is a multiple of the block size, so messages may have to be padded to bring them to this length.\nThere is currently a shift to use streaming mode of operation instead of block mode of operation. An example of streaming mode encryption is the counter mode of operation . Streaming modes of operation can encrypt and decrypt messages of any size and therefore do not require padding. More intricate ways of ending a message such as ciphertext stealing or residual block termination avoid the need for padding.\n\nA disadvantage of padding is that it makes the plain text of the message susceptible to padding oracle attacks. Padding oracle attacks allow the attacker to gain knowledge of the plain text without attacking the block cipher primitive itself. Padding oracle attacks can be avoided by making sure that an attacker cannot gain knowledge about the removal of the padding bytes. This can be accomplished by verifying a message authentication code (MAC) or digital signature \"before\" removal of the padding bytes, or by switching to a streaming mode of operation.\n\nBit padding can be applied to messages of any size.\n\nA single set ('1') bit is added to the message and then as many reset ('0') bits as required (possibly none) are added. The number of reset ('0') bits added will depend on the block boundary to which the message needs to be extended. In bit terms this is \"1000 ... 0000\".\n\nThis method can be used to pad messages which are any number of bits long, not necessarily a whole number of bytes long. For example, a message of 23 bits that is padded with 9 bits in order to fill a 32-bit block:\n\nThis padding is the first step of a two-step padding scheme used in many hash functions including MD5 and SHA. In this context, it is specified by RFC1321 step 3.1.\n\nThis padding scheme is defined by ISO/IEC 9797-1 as Padding Method 2.\n\nByte padding can be applied to messages that can be encoded as an integral number of bytes.\n\nIn ANSI X.923 bytes filled with zeros are padded and the last byte defines the padding boundaries or the number of padded bytes.\n\nExample:\nIn the following example the block size is 8 bytes, and padding is required for 4 bytes (in hexadecimal format)\n\nISO 10126 (withdrawn, 2007) specifies that the padding should be done at the end of that last block with random bytes, and the padding boundary should be specified by the last byte.\n\nExample:\nIn the following example the block size is 8 bytes and padding is required for 4 bytes\n\nPKCS#7 is described in RFC 5652.\n\nPadding is in whole bytes. The value of each added byte is the number of bytes that are added, i.e. bytes, each of value are added. The number of bytes added will depend on the block boundary to which the message needs to be extended.\n\nThe padding will be one of:\n\nThis padding method (as well as the previous two) is well-defined if and only if is less than 256.\n\nExample:\nIn the following example the block size is 8 bytes and padding is required for 4 bytes\n\nIf the original data is an integer multiple of bytes, then an extra block of bytes with value is added. This is necessary so the deciphering algorithm can determine with certainty whether the last byte of the last block is a pad byte indicating the number of padding bytes added or part of the plaintext message. Consider a plaintext message that is an integer multiple of bytes with the last byte of plaintext being 01. With no additional information, the deciphering algorithm will not be able to determine whether the last byte is a plaintext byte or a pad byte. However, by adding bytes each of value after the 01 plaintext byte, the deciphering algorithm can always treat the last byte as a pad byte and strip the appropriate number of pad bytes off the end of the ciphertext; said number of bytes to be stripped based on the value of the last byte.\n\nPKCS#5 padding is identical to PKCS#7 padding, except that it has only been defined for block ciphers that use a 64-bit (8-byte) block size. In practice the two can be used interchangeably.\n\nISO/IEC 7816-4:2005 is identical to the bit padding scheme, applied to a plain text of N bytes. This means in practice that the first byte is a mandatory byte valued '80' (Hexadecimal) followed, if needed, by 0 to N-1 bytes set to '00', until the end of the block is reached. ISO/IEC 7816-4 itself is a communication standard for smart cards containing a file system, and in itself does not contain any cryptographic specifications.\n\nExample:\nIn the following example the block size is 8 bytes and padding is required for 4 bytes\n\nThe next example shows a padding of just one byte\n\nAll the bytes that are required to be padded are padded with zero. The zero padding scheme has not been standardized for encryption, although it is specified for hashes and MACs as Padding Method 1 in ISO/IEC 10118-1 and ISO/IEC 9797-1.\n\nExample:\nIn the following example the block size is 8 bytes and padding is required for 4 bytes\n\nZero padding may not be reversible if the original file ends with one or more zero bytes, making it impossible to distinguish between plaintext data bytes and padding bytes. It may be used when the length of the message can be derived out-of-band. It is often applied to binary encoded strings as the null character can usually be stripped off as whitespace.\n\nZero padding is sometimes also referred to as \"null padding\" or \"zero byte padding\". Some implementations may add an additional block of zero bytes if the plaintext is already divisible by the block size.\n\nIn public key cryptography, padding is the process of preparing a message for encryption or signing using a specification or scheme such as PKCS#1 v1.5, OAEP, PSS, PSSR, IEEE P1363 EMSA2 and EMSA5. A modern form of padding for asymmetric primitives is OAEP applied to the RSA algorithm, when it is used to encrypt a limited number of bytes.\n\nThe operation is referred to as \"padding\" because originally, random material was simply appended to the message to make it long enough for the primitive. This form of padding is not secure and is therefore no longer applied. A modern padding scheme aims to ensure that the attacker cannot manipulate the plaintext to exploit the mathematical structure of the primitive and will usually be accompanied by a proof, often in the random oracle model, that breaking the padding scheme is as hard as solving the hard problem underlying the primitive.\n\nEven if perfect cryptographic routines are used, the attacker can gain knowledge of the amount of traffic that was generated. The attacker might not know what Alice and Bob were talking about, but can know that they \"were\" talking and \"how much\" they talked. In certain circumstances this can be very bad. Consider for example when a military is organising a secret attack against another nation: it may suffice to alert the other nation for them to know merely that there \"is\" a lot of secret activity going on.\n\nAs another example, when encrypting Voice Over IP streams that use variable bit rate encoding, the number of bits per unit of time is not obscured, and this can be exploited to guess spoken phrases.\n\nPadding messages makes traffic analysis much harder. Normally, a number of random bits are appended to the end of the message with an indication at the end how much this random data is. The randomness should have a minimum value of 0, a maximum number of N and an even distribution between the two extremes. Note that increasing 0 does not help, only increasing N helps, though that also means that a lower percentage of the channel will be used to transmit real data. Also note that, since the cryptographic routine is assumed to be uncrackable (otherwise the padding length itself is crackable), it does not help to put the padding anywhere else, e.g. at the beginning, in the middle, or in a sporadic manner. For the same reason, padding can be structured (e.g. it can simply be a set of zeros) – though structured padding can be hazard, as explained in timing attack.\n\n\n"}
{"id": "5075551", "url": "https://en.wikipedia.org/wiki?curid=5075551", "title": "Perfect set", "text": "Perfect set\n\nIn mathematics, in the field of topology, a subset of a topological space is perfect if it is closed and has no isolated points. Equivalently: the set formula_1 is perfect if formula_2, where formula_3 denotes the set of all limit points of formula_1, also known as the derived set of formula_1.\n\nIn a perfect set, every point can be approximated arbitrarily well by other points from the set: given any point of formula_1 and any topological neighborhood of the point, there is another point of formula_1 that lies within the neighborhood.\n\nNote that the term \"perfect space\" is also used, incompatibly, to refer to other properties of a topological space, such as being a G space. \n\nExamples of perfect subsets of the real line formula_8 are: the empty set, all closed intervals, the real line itself, and the Cantor set. The latter is noteworthy in that it is totally disconnected.\n\nCantor proved that every closed subset of the real line can be uniquely written as the disjoint union of a perfect set and a countable set. This is also true more generally for all closed subsets of Polish spaces, in which case the theorem is known as the Cantor–Bendixson theorem.\n\nCantor also showed that every non-empty perfect subset of the real line has cardinality formula_9, the cardinality of the continuum. These results are extended in descriptive set theory as follows:\n\n\n"}
{"id": "7576966", "url": "https://en.wikipedia.org/wiki?curid=7576966", "title": "Pierre de Fermat", "text": "Pierre de Fermat\n\nPierre de Fermat () (Between 31 October and 6 December 1607 – 12 January 1665) was a French lawyer at the \"Parlement\" of Toulouse, France, and a mathematician who is given credit for early developments that led to infinitesimal calculus, including his technique of adequality. In particular, he is recognized for his discovery of an original method of finding the greatest and the smallest ordinates of curved lines, which is analogous to that of differential calculus, then unknown, and his research into number theory. He made notable contributions to analytic geometry, probability, and optics. He is best known for his Fermat's principle for light propagation and his Fermat's Last Theorem in number theory, which he described in a note at the margin of a copy of Diophantus' \"Arithmetica\".\n\nFermat was born in the first decade of the 17th century in Beaumont-de-Lomagne, France—the late 15th-century mansion where Fermat was born is now a museum. He was from Gascony, where his father, Dominique Fermat, was a wealthy leather merchant, and served three one-year terms as one of the four consuls of Beaumont-de-Lomagne. His mother was Claire de Long. Pierre had one brother and two sisters and was almost certainly brought up in the town of his birth. There is little evidence concerning his school education, but it was probably at the Collège de Navarre in Montauban.\n\nHe attended the University of Orléans from 1623 and received a bachelor in civil law in 1626, before moving to Bordeaux. In Bordeaux he began his first serious mathematical researches, and in 1629 he gave a copy of his restoration of Apollonius's \"De Locis Planis\" to one of the mathematicians there. Certainly in Bordeaux he was in contact with Beaugrand and during this time he produced important work on maxima and minima which he gave to Étienne d'Espagnet who clearly shared mathematical interests with Fermat. There he became much influenced by the work of François Viète.\n\nIn 1630, he bought the office of a councillor at the Parlement de Toulouse, one of the High Courts of Judicature in France, and was sworn in by the Grand Chambre in May 1631. He held this office for the rest of his life. Fermat thereby became entitled to change his name from Pierre Fermat to Pierre de Fermat. Fluent in six languages (French, Latin, Occitan, classical Greek, Italian and Spanish), Fermat was praised for his written verse in several languages and his advice was eagerly sought regarding the emendation of Greek texts.\n\nHe communicated most of his work in letters to friends, often with little or no proof of his theorems. In some of these letters to his friends he explored many of the fundamental ideas of calculus before Newton or Leibniz. Fermat was a trained lawyer making mathematics more of a hobby than a profession. Nevertheless, he made important contributions to analytical geometry, probability, number theory and calculus. Secrecy was common in European mathematical circles at the time. This naturally led to priority disputes with contemporaries such as Descartes and Wallis.\n\nAnders Hald writes that, \"The basis of Fermat's mathematics was the classical Greek treatises combined with Vieta's new algebraic methods.\"\n\nFermat's pioneering work in analytic geometry (\"Methodus ad disquirendam maximam et minimam et de tangentibus linearum curvarum\") was circulated in manuscript form in 1636 (based on results achieved in 1629), predating the publication of Descartes' famous \"La géométrie\" (1637), which exploited the work. This manuscript was published posthumously in 1679 in \"Varia opera mathematica\", as \"Ad Locos Planos et Solidos Isagoge\" (\"Introduction to Plane and Solid Loci\").\n\nIn \"Methodus ad disquirendam maximam et minimam\" and in \"De tangentibus linearum curvarum\", Fermat developed a method (adequality) for determining maxima, minima, and tangents to various curves that was equivalent to differential calculus. In these works, Fermat obtained a technique for finding the centers of gravity of various plane and solid figures, which led to his further work in quadrature.\nFermat was the first person known to have evaluated the integral of general power functions. With his method, he was able to reduce this evaluation to the sum of geometric series. The resulting formula was helpful to Newton, and then Leibniz, when they independently developed the fundamental theorem of calculus.\n\nIn number theory, Fermat studied Pell's equation, perfect numbers, amicable numbers and what would later become Fermat numbers. It was while researching perfect numbers that he discovered Fermat's little theorem. He invented a factorization method—Fermat's factorization method—as well as the proof technique of infinite descent, which he used to prove Fermat's right triangle theorem which includes as a corollary Fermat's Last Theorem for the case \"n\" = 4. Fermat developed the two-square theorem, and the polygonal number theorem, which states that each number is a sum of three triangular numbers, four square numbers, five pentagonal numbers, and so on.\n\nAlthough Fermat claimed to have proven all his arithmetic theorems, few records of his proofs have survived. Many mathematicians, including Gauss, doubted several of his claims, especially given the difficulty of some of the problems and the limited mathematical methods available to Fermat. His famous Last Theorem was first discovered by his son in the margin in his father's copy of an edition of Diophantus, and included the statement that the margin was too small to include the proof. It seems that he had not written to Marin Mersenne about it. It was first proven in 1994, by Sir Andrew Wiles, using techniques unavailable to Fermat.\n\nAlthough he carefully studied and drew inspiration from Diophantus, Fermat began a different tradition. Diophantus was content to find a single solution to his equations, even if it were an undesired fractional one. Fermat was interested only in integer solutions to his Diophantine equations, and he looked for all possible general solutions. He often proved that certain equations had no solution, which usually baffled his contemporaries.\n\nThrough their correspondence in 1654, Fermat and Blaise Pascal helped lay the foundation for the theory of probability. From this brief but productive collaboration on the problem of points, they are now regarded as joint founders of probability theory. Fermat is credited with carrying out the first ever rigorous probability calculation. In it, he was asked by a professional gambler why if he bet on rolling at least one six in four throws of a die he won in the long term, whereas betting on throwing at least one double-six in 24 throws of two dice resulted in his losing. Fermat showed mathematically why this was the case.\n\nThe first variational principle in physics was articulated by Euclid in his \"Catoptrica\". It says that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection. Hero of Alexandria later showed that this path gave the shortest length and the least time. Fermat refined and generalized this to \"light travels between two given points along the path of shortest \"time\"\" now known as the \"principle of least time\". For this, Fermat is recognized as a key figure in the historical development of the fundamental principle of least action in physics. The terms Fermat's principle and \"Fermat functional\" were named in recognition of this role.\n\nPierre de Fermat died on January 12, 1665 at Castres, in the present-day department of Tarn. The oldest and most prestigious high school in Toulouse is named after him: the . French sculptor Théophile Barrau made a marble statue named \"Hommage à Pierre Fermat\" as a tribute to Fermat, now at the Capitole de Toulouse.\n\nTogether with René Descartes, Fermat was one of the two leading mathematicians of the first half of the 17th century. According to Peter L. Bernstein, in his book \"Against the Gods\", Fermat \"was a mathematician of rare power. He was an independent inventor of analytic geometry, he contributed to the early development of calculus, he did research on the weight of the earth, and he worked on light refraction and optics. In the course of what turned out to be an extended correspondence with Pascal, he made a significant contribution to the theory of probability. But Fermat's crowning achievement was in the theory of numbers.\"\n\nRegarding Fermat's work in analysis, Isaac Newton wrote that his own early ideas about calculus came directly from \"Fermat's way of drawing tangents.\"\n\nOf Fermat's number theoretic work, the 20th-century mathematician André Weil wrote that: \"what we possess of his methods for dealing with curves of genus 1 is remarkably coherent; it is still the foundation for the modern theory of such curves. It naturally falls into two parts; the first one ... may conveniently be termed a method of ascent, in contrast with the descent which is rightly regarded as Fermat's own.\" Regarding Fermat's use of ascent, Weil continued: \"The novelty consisted in the vastly extended use which Fermat made of it, giving him at least a partial equivalent of what we would obtain by the systematic use of the group theoretical properties of the rational points on a standard cubic.\" With his gift for number relations and his ability to find proofs for many of his theorems, Fermat essentially created the modern theory of numbers.\n\n\n\n"}
{"id": "1161104", "url": "https://en.wikipedia.org/wiki?curid=1161104", "title": "Power of 10", "text": "Power of 10\n\nIn mathematics, a power of 10 is any of the integer powers of the number ten; in other words, ten multiplied by itself a certain number of times (when the power is a positive integer). By definition, the number one is a power (the zeroth power) of ten. The first few non-negative powers of ten are:\n\nIn decimal notation the \"n\"th power of ten is written as '1' followed by \"n\" zeroes. It can also be written as 10 or as 1E\"n\" in E notation. See order of magnitude and orders of magnitude (numbers) for named powers of ten. There are two conventions for naming positive powers of ten, called the long and short scales. Where a power of ten has different names in the two conventions, the long scale name is shown in brackets. \n\nThe positive 10 power related to a short scale name can be determined based on its Latin name-prefix using the following formula:\n10\n\nExamples: Billion = 10 = 10 ; Octillion = 10 = 10\n\nThe sequence of powers of ten can also be extended to negative powers.\n\nSimilarly to above, the negative 10 power related to a short scale name can be determined based on its Latin name-prefix using the following formula:\n10\n\nExamples: billionth = 10 = 10 ; quintillionth = 10 = 10\n\nThe number googol is 10. The term was coined by 9-year-old Milton Sirotta, nephew of American mathematician Edward Kasner, popularized in his 1940 book \"Mathematics and the Imagination\", it was used to compare and illustrate very large numbers. Googolplex, a much larger power of ten (10 to the googol power, or 10), was also introduced in that book.\n\nScientific notation is a way of writing numbers of very large and very small sizes compactly when precision is less important.\n\nA number written in scientific notation has a significand (sometime called a mantissa) multiplied by a power of ten.\n\nSometimes written in the form:\n\nOr more compactly as:\n\nThis is generally used to denote powers of 10. Where \"n\" is positive, this indicates the number zeros after the number, and where the \"n\" is negative, this indicates the number of decimal places before the number.\n\nAs an example:\n\nThe notation of \"m\"E\"n\", known as \"E notation\", is used in computer programming, spreadsheets and databases, but is not used in scientific papers.\n\n\n"}
{"id": "13473221", "url": "https://en.wikipedia.org/wiki?curid=13473221", "title": "Quantum bus", "text": "Quantum bus\n\nA quantum bus is a device which can be used to store or transfer information between independent qubits in a quantum computer, or combine two qubits into a superposition. It is the quantum analog of a classical bus.\n\nThe concept was first demonstrated by researchers at Yale University and the National Institute of Standards and Technology (NIST) in 2007. Prior to this experimental demonstration, the quantum bus had been described by scientists at NIST as one of the possible cornerstone building blocks in quantum computing architectures.\n\nA quantum bus for superconducting qubits can be built with a resonance cavity. The hamiltonian for a system with qubit A, qubit B, and the resonance cavity or quantum bus connecting the two is formula_1 where formula_2 is the single qubit hamiltonian, formula_3 is the raising or lowering operator for creating or destroying excitations in the formula_4th qubit, and formula_5 is controlled by the amplitude of the D.C. and radio frequency flux bias.\n"}
{"id": "2025989", "url": "https://en.wikipedia.org/wiki?curid=2025989", "title": "Shell theorem", "text": "Shell theorem\n\nIn classical mechanics, the shell theorem gives gravitational simplifications that can be applied to objects inside or outside a spherically symmetrical body. This theorem has particular application to astronomy.\n\nIsaac Newton proved the shell theorem and stated that:\n\nA corollary is that inside a solid sphere of constant density, the gravitational force within the object varies linearly with distance from the centre, becoming zero by symmetry at the centre of mass. This can be seen as follows: take a point within such a sphere, at a distance formula_1 from the centre of the sphere. Then you can ignore all the shells of greater radius, according to the shell theorem. So, the remaining mass formula_2 is proportional to formula_3 (because it is based on volume), and the gravitational force exerted on it is proportional to formula_4 (the inverse square law), so the overall gravitational effect is proportional to formula_5, so is linear in formula_1.\n\nThese results were important to Newton's analysis of planetary motion; they are not immediately obvious, but they can be proven with calculus. (Alternatively, Gauss's law for gravity offers a much simpler way to prove the same results.)\n\nIn addition to gravity, the shell theorem can also be used to describe the electric field generated by a static spherically symmetric charge density, or similarly for any other phenomenon that follows an inverse square law. The derivations below focus on gravity, but the results can easily be generalized to the electrostatic force. Moreover, the results can be generalized to the case of general ellipsoidal bodies.\n\nA solid, spherically symmetric body can be modelled as an infinite number of concentric, infinitesimally thin spherical shells. If one of these shells can be treated as a point mass, then a system of shells (i.e. the sphere) can also be treated as a point mass. Consider one such shell (the diagram shows a cross-section):\n\nApplying Newton's Universal Law of Gravitation, the sum of the forces due to mass elements in the shaded band is\n\nHowever, since there is partial cancellation due to the vector nature of the force in conjunction with the circular band's symmetry, the leftover component (in the direction pointing toward \"m\") is given by\n\nThe total force on \"m\", then, is simply the sum of the force exerted by all the bands. By shrinking the width of each band, and increasing the number of bands, the sum becomes an integral expression:\n\nSince \"G\" and \"m\" are constants, they may be taken out of the integral:\n\nTo evaluate this integral, one must first express \"dM\" as a function of \"dθ\"\n\nThe total surface of a spherical shell is\n\nwhile the surface of the thin slice between \"θ\" and \"θ\" + \"dθ\" is\nIf the mass of the shell is \"M\", one therefore has that\n\nand\n\nBy the law of cosines,\n\nThese two relations link the three parameters \"θ\", \"ϕ\" and \"s\" that appear in the integral together. When \"θ\" increases from 0 to π radians, \"ϕ\" varies from the initial value 0 to a maximal value to finally return to zero for \"θ\" = π. \"s\" on the other hand increases from the initial value \"r\" − \"R\" to the final value \"r\" + \"R\" when \"θ\" increases from 0 to π radians. \nThis is illustrated in the following animation:\n\nTo find a primitive function to the integrand, one has to make \"s\" the independent integration variable instead of \"θ\".\n\nPerforming an implicit differentiation of the second of the \"cosine law\" expressions above yields\n\nand one gets that\n\nwhere the new integration variable \"s\" increases from \"r\" − \"R\" to \"r\" + \"R\".\n\nInserting the expression for cos(\"φ\") using the first of the \"cosine law\" expressions\nabove, one finally gets that\n\nA primitive function to the integrand is\n\nand inserting the bounds \"r\" − \"R\", \"r\" + \"R\" for the integration variable \"s\" in this primitive function, one gets that\n\nsaying that the gravitational force is the same as that of a point mass in the centre of the shell with the same mass.\n\nFinally, integrate all infinitesimally thin spherical shell with mass of \"dM\", and we can obtain the total gravity contribution of a solid ball to the object outside the ball\n\nBetween the radius of \"x\" to \"x\" + \"dx\", \"dM\" can be expressed as a function of \"x\", i.e.,\n\nTherefore, the total gravity is\n\nwhich suggests that the gravity of a solid spherical ball to an exterior object can be simplified as that of a point mass in the centre of the ball with the same mass.\n\nFor a point inside the shell, the difference is that when \"θ\" is equal to zero, \"ϕ\" takes the value π radians and \"s\" the value \"R\" - \"r\". When \"θ\" increases from 0 to\nπ radians, \"ϕ\" decreases from the initial value π radians to zero and \"s\" increases from the initial value \"R\" - \"r\" to the value \"R\" + \"r\".\n\nThis can all be seen in the following figure\nInserting these bounds into the primitive function\n\none gets that, in this case\n\nsaying that the net gravitational forces acting on the point mass from the mass elements of the shell, outside the measurement point, cancel out.\n\nGeneralization: If formula_7, the resultant force inside the shell is:\n\nThe above results into formula_8 being identically zero if and only if formula_9\n\nOutside the shell (i.e. r>R or r<-R):\n\nThe shell theorem is an immediate consequence of Gauss's law for gravity saying that\n\nwhere \"M\" is the mass of the part of the spherically symmetric mass distribution that is inside the sphere with radius \"r\" and\n\nis the surface integral of the gravitational field g over any closed surface inside which the total mass is \"M\", the unit vector formula_10 being the outward normal to the surface.\n\nThe gravitational field of a spherically symmetric mass distribution like a mass point, a spherical shell or a homogenous sphere must also be spherically symmetric. If formula_11 is a unit vector in the direction from the point of symmetry to another point the gravitational field at this other point must therefore be\n\nwhere \"g\"(\"r\") only depends on the distance \"r\" to the point of symmetry\n\nSelecting the closed surface as a sphere with radius \"r\" with center at the point of symmetry the outward normal to a point on the surface, formula_12, is precisely the direction pointing away from the point of symmetry of the mass distribution.\n\nOne, therefore, has that\n\nand\n\nas the area of the sphere is 4π\"r\".\n\nFrom Gauss's law it then follows that\ni.e. that\n\nIt is natural to ask whether the converse of the shell theorem is true, namely whether the result of the theorem implies the law of universal gravitation, or if there is some more general force law for which the theorem holds. More specifically, one may ask the question:\n\nIn fact, this allows exactly one more class of force than the (Newtonian) inverse square. The most general force as derived in is:\n\nwhere formula_13 and formula_14 can be constants taking any value. The first term is the familiar law of universal gravitation; the second is an additional force, analogous to the cosmological constant term in general relativity.\n\nIf we further constrain the force by requiring that the second part of the theorem also holds, namely that there is no force inside a hollow ball, we exclude the possibility of the additional term, and the inverse square law is indeed the unique force law satisfying the theorem.\n\nOn the other hand, if we relax the conditions, and require only that the field everywhere outside a spherically symmetric body is the same as the field from some point mass at the centre (of any mass), we allow a new class of solutions given by the Yukawa potential, of which the inverse square law is a special case.\n\nAnother generalization can be made for a disc by observing that \n\nso:\n\nwhere formula_15\n\nDoing all the intermediate calculations we get:\n\nNote that formula_16 in this example is expressed in formula_17\n\nPropositions 70 and 71 consider the force acting on a particle from a hollow sphere with an infinitesimally thin surface, whose mass density is constant over the surface. The force on the particle from a small area of the surface of the sphere is proportional to the mass of the area and inversely as the square of its distance from the particle. The first proposition considers the case when the particle is inside the sphere, the second when it is outside. The use of infinitesimals and limiting processes in geometrical constructions are simple and elegant and avoid the need for any integrations. They well illustrate Newton's method of proving many of the propositions in the \"Principia\". \n\nHis proof of Propositions 70 is trivial. In the following, it is considered in slightly greater detail than Newton provides.\n\nThe proof of Proposition 71 is more historically significant. It forms the first part of his proof that the gravitational force of a solid sphere acting on a particle outside it is inversely proportional to the square of its distance from the centre of the sphere, provided the density at any point inside the sphere is a function only of its distance from the centre of the sphere.\n\nAlthough the following are completely faithful to Newton's proofs, very minor changes have been made to attempt to make them clearer.\n\nFig. 2 is a cross-section of the hollow sphere through the centre, S and an arbitrary point, P, inside the sphere. Through P draw two lines IL and HK such that the angle KPL is very small. JM is the line through P that bisects that angle. From the geometry of circles, the triangles IPH and KPL are similar. \nThe lines KH and IL are rotated about the axis JM to form 2 cones that intersect the sphere in 2 closed curves. In Fig. 1 the sphere is seen from a distance along the line PE and is assumed transparent so both curves can be seen. \n\nThe surface of the sphere that the cones intersect can be considered to be flat, and angles formula_18\n\nSince the intersection of a cone with a plane is an ellipse, in this case the intersections form two ellipses with major axes IH and KL, where formula_19\n\nBy a similar argument, the minor axes are in the same ratio. This is clear if the sphere is viewed from above. Therefore the two ellipses are similar, so their areas are as the squares of their major axes. As the mass of any section of the surface is proportional to the area of that section, for the 2 elliptical areas the ratios of their masses formula_20.\n\nSince the force of attraction on P in the direction JM from either of the elliptic areas, is direct as the mass of the area and inversely as the square of its distance from P, it is independent of the distance of P from the sphere. Hence, the forces on P from the 2 infinitesimal elliptical areas are equal and opposite and there is no net force in the direction JM. \n\nAs the position of P and the direction of JM are both arbitrary, it follows that any particle inside a hollow sphere experiences no net force from the mass of the sphere.\n\nNote: Newton simply describes the arcs IH and KL as 'minimally small' and the areas traced out by the lines IL and HK can be any shape, not necessarily elliptic, but they will always be similar.\n\nFig. 1 is a cross-section of the hollow sphere through the centre, S with an arbitrary point, P, outside the sphere. PT is the tangent to the circle at T which passes through P. HI is a small arc on the surface such that PH is less than PT. Extend PI to intersect the sphere at L and draw SF to the point F that bisects IL. Extend PH to intersect the sphere at K and draw SE to the point E that bisects HK, and extend SF to intersect HK at D. Drop a perpendicular IQ on to the line PS joining P to the centre S. Let the radius of the sphere be a and the distance PS be D.\n\nLet arc IH be extended perpendicularly out of the plane of the diagram, by a small distance ζ. The area of the figure generated is IH.ζ, and its mass is proportional to this product.\n\nThe force due to this mass on the particle at P formula_21 and is along the line PI. \n\nThe component of this force towards the centre formula_22.\n\nIf now the arc HI is rotated completely about the line PS to form a ring of width HI and radius IQ, the length of the ring is 2π.IQ and its area is 2π.IQ.IH. The component of the force due to this ring on the particle at P in the direction PS becomes formula_23.\n\nThe perpendicular components of the force directed towards PS cancel out since the mass in the ring is distributed symmetrically about PS. Therefore, the component in the direction PS is the total force on P due to the ring formed by rotating arc HI about PS.\n\nFrom similar triangles: formula_24; formula_25, and formula_26\n\nIf HI is sufficiently small that it can be taken as a straight line, SIH is a right angle, and the angles formula_27, so that formula_28.\n\nHence the force on P due to the ring formula_29.\n\nAssume now in Fig. 2 that another particle is outside the sphere at a point, p, a different distance, d, from the centre of the sphere, with corresponding points lettered in lower case. For easy comparison, the construction of P in Fig. 1 is also shown in Fig. 2. As before, ph is less than pt.\n\nGenerate a ring with width ih and radius iq by making angle formula_30 and the slightly larger Angle formula_31, so that the distance PS is subtended by the same angle at I as is pS at i. The same holds for H and h, respectively.\n\nThe total force on p due to this ring is \n\nClearly formula_32, formula_33, and formula_34.\n\nNewton claims that DF and df can be taken as equal in the limit as the angles DPF and dpf 'vanish together'. Note that angles DPF and dpf are not equal. Although DS and dS become equal in the limit, this does not imply that the ratio of DF to df becomes equal to unity, when DF and df both approach zero. In the finite case DF depends on D, and df on d, so they are not equal.\nSince the ratio of DF to df in the limit is crucial, more detailed analysis is required. From the similar right triangles, formula_35 and formula_36, giving formula_37. Solving the quadratic for DF, in the limit as ES approaches FS, the smaller root, formula_38. More simply, as DF approaches zero, in the limit the formula_39 term can be ignored: formula_40 leading to the same result. Clearly df has the same limit, justifying Newton’s claim.\n\nComparing the force from the ring HI rotated about PS to the ring hi about pS, the ratio of these 2 forces equals formula_41.\n\nBy dividing up the arcs AT and Bt into corresponding infinitesimal rings, it follows that the ratio of the force due to the arc AT rotated about PS to that of Bt rotated about pS is in the same ratio, and similarly, the ratio of the forces due to arc TB to that of tA both rotated are in the same ratio.\n\nTherefore, the force on a particle any distance D from the centre of the hollow sphere is inversely proportional to formula_42, which proves the proposition.\n\nThere is an analog of the shell theorem in general relativity (GR). The shell theorem shows that the gravitational potential outside a spherically symmetric body is given by -GM/r—whether or not the body is changing with time. In general relativity, there is a similar theorem that demonstrating that the spacetime geometry \"outside\" a spherically symmetric mass-energy distribution is the time-independent Schwarzschild geometry—even if the central mass is undergoing gravitational collapse (Misner et al. 1973). \n\nThis equivalent of the shell theorem in GR is very useful because it allows to use the machinery of the Schwarzschild metric to understand the gravitational collapse leading to a black hole, and its effect on the motion of light-rays and particles outside and inside the event horizon (Hartle 2003, chapter 12). \n\n"}
{"id": "29208", "url": "https://en.wikipedia.org/wiki?curid=29208", "title": "Square root", "text": "Square root\n\nIn mathematics, a square root of a number \"a\" is a number \"y\" such that ; in other words, a number \"y\" whose \"square\" (the result of multiplying the number by itself, or ) is \"a\". For example, 4 and −4 are square roots of 16 because .\nEvery nonnegative real number \"a\" has a unique nonnegative square root, called the \"principal square root\", which is denoted by , where √ is called the \"radical sign\" or \"radix\". For example, the principal square root of 9 is 3, which is denoted by = 3, because and 3 is nonnegative. The term (or number) whose square root is being considered is known as the \"radicand\". The radicand is the number or expression underneath the radical sign, in this example 9.\n\nEvery positive number \"a\" has two square roots: , which is positive, and −, which is negative. Together, these two roots are denoted as ± (see ± shorthand). Although the principal square root of a positive number is only one of its two square roots, the designation \"\"the\" square root\" is often used to refer to the \"principal square root\". For positive \"a\", the principal square root can also be written in exponent notation, as \"a\".\n\nSquare roots of negative numbers can be discussed within the framework of complex numbers. More generally, square roots can be considered in any context in which a notion of \"squaring\" of some mathematical objects is defined (including algebras of matrices, endomorphism rings, etc.)\n\nThe Yale Babylonian Collection YBC 7289 clay tablet was created between 1800 BC and 1600 BC, showing and /2 = 1/ as 1;24,51,10 and 0;42,25,35 base 60 numbers on a square crossed by two diagonals. (1;24,51,10) base 60 corresponds to 1.41421296 which is a correct value to 5 decimal points (1.41421356...).\n\nThe Rhind Mathematical Papyrus is a copy from 1650 BC of an earlier Berlin Papyrus and other textspossibly the Kahun Papyrusthat shows how the Egyptians extracted square roots by an inverse proportion method.\n\nIn Ancient India, the knowledge of theoretical and applied aspects of square and square root was at least as old as the \"Sulba Sutras\", dated around 800–500 BC (possibly much earlier). A method for finding very good approximations to the square roots of 2 and 3 are given in the \"Baudhayana Sulba Sutra\". Aryabhata in the \"Aryabhatiya\" (section 2.4), has given a method for finding the square root of numbers having many digits.\n\nIt was known to the ancient Greeks that square roots of positive whole numbers that are not perfect squares are always irrational numbers: numbers not expressible as a ratio of two integers (that is to say they cannot be written exactly as \"m/n\", where \"m\" and \"n\" are integers). This is the theorem \"Euclid X, 9\" almost certainly due to Theaetetus dating back to circa 380 BC.\nThe particular case is assumed to date back earlier to the Pythagoreans and is traditionally attributed to Hippasus. It is exactly the length of the diagonal of a square with side length 1.\n\nIn the Chinese mathematical work \"Writings on Reckoning\", written between 202 BC and 186 BC during the early Han Dynasty, the square root is approximated by using an \"excess and deficiency\" method, which says to \"...combine the excess and deficiency as the divisor; (taking) the deficiency numerator multiplied by the excess denominator and the excess numerator times the deficiency denominator, combine them as the dividend.\"\n\nA symbol for square roots, written as an elaborate R, was invented by Regiomontanus (1436–1476). An R was also used for Radix to indicate square roots in Gerolamo Cardano's \"Ars Magna\".\n\nAccording to historian of mathematics D.E. Smith, Aryabhata's method for finding the square root was first introduced in Europe by Cataneo in 1546.\n\nAccording to Jeffrey A. Oaks, Arabs used the letter \"jīm/ĝīm\" (), the first letter of the word “” (variously transliterated as \"jaḏr\", \"jiḏr\", \"ǧaḏr\" or \"ǧiḏr\", “root”), placed in its initial form () over a number to indicate its square root. The letter \"jīm\" resembles the present square root shape. Its usage goes as far as the end of the twelfth century in the works of the Moroccan mathematician Ibn al-Yasamin.\n\nThe symbol '√' for the square root was first used in print in 1525 in Christoph Rudolff's \"Coss\".\n\nThe principal square root function \"f\"(\"x\") = (usually just referred to as the \"square root function\") is a function that maps the set of nonnegative real numbers onto itself. In geometrical terms, the square root function maps the area of a square to its side length.\n\nThe square root of \"x\" is rational if and only if \"x\" is a rational number that can be represented as a ratio of two perfect squares. (See square root of 2 for proofs that this is an irrational number, and quadratic irrational for a proof for all non-square natural numbers.) The square root function maps rational numbers into algebraic numbers (a superset of the rational numbers).\n\nFor all real numbers \"x\",\n\nFor all nonnegative real numbers \"x\" and \"y\",\n\nand\n\nThe square root function is continuous for all nonnegative \"x\" and differentiable for all positive \"x\". If \"f\" denotes the square root function, its derivative is given by:\n\nThe Taylor series of about \"x\" = 0 converges for ≤ 1 and is given by\n\nThe square root of a nonnegative number is used in the definition of Euclidean norm (and distance), as well as in generalizations such as Hilbert spaces. It defines an important concept of standard deviation used in probability theory and statistics. It has a major use in the formula for roots of a quadratic equation; quadratic fields and rings of quadratic integers, which are based on square roots, are important in algebra and have uses in geometry. Square roots frequently appear in mathematical formulas elsewhere, as well as in many physical laws.\n\nMost pocket calculators have a square root key. Computer spreadsheets and other software are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the Newton's method (frequently with an initial guess of 1), to compute the square root of a positive real number. When computing square roots with logarithm tables or slide rules, one can exploit the identities\n\nwhere and are the natural and base-10 logarithms.\n\nBy trial-and-error, one can square an estimate for and raise or lower the estimate until it agrees to sufficient accuracy. For this technique it's prudent to use the identity\n\nas it allows one to adjust the estimate \"x\" by some amount \"c\" and measure the square of the adjustment in terms of the original estimate and its square. Furthermore, (\"x\" + \"c\") ≈ \"x\" + 2\"xc\" when \"c\" is close to 0, because the tangent line to the graph of \"x\" + 2\"xc\" + \"c\" at \"c\" = 0, as a function of \"c\" alone, is \"y\" = 2\"xc\" + \"x\". Thus, small adjustments to \"x\" can be planned out by setting 2\"xc\" to \"a\", or \"c\" = \"a\"/(2\"x\").\n\nThe most common iterative method of square root calculation by hand is known as the \"Babylonian method\" or \"Heron's method\" after the first-century Greek philosopher Heron of Alexandria, who first described it.\nThe method uses the same iterative scheme as the Newton–Raphson method yields when applied to the function y = \"f\"(\"x\") = \"x\" − \"a\", using the fact that its slope at any point is \"dy\"/\"dx\" = \"\"(\"x\") = 2\"x\", but predates it by many centuries.\nThe algorithm is to repeat a simple calculation that results in a number closer to the actual square root each time it is repeated with its result as the new input. The motivation is that if \"x\" is an overestimate to the square root of a nonnegative real number \"a\" then \"a\"/\"x\" will be an underestimate and so the average of these two numbers is a better approximation than either of them. However, the inequality of arithmetic and geometric means shows this average is always an overestimate of the square root (as noted below), and so it can serve as a new overestimate with which to repeat the process, which converges as a consequence of the successive overestimates and underestimates being closer to each other after each iteration. To find \"x\":\n\n\nThat is, if an arbitrary guess for is \"x\", and , then each x is an approximation of which is better for large \"n\" than for small \"n\". If \"a\" is positive, the convergence is quadratic, which means that in approaching the limit, the number of correct digits roughly doubles in each next iteration. If , the convergence is only linear.\n\nUsing the identity\n\nthe computation of the square root of a positive number can be reduced to that of a number in the range . This simplifies finding a start value for the iterative method that is close to the square root, for which a polynomial or piecewise-linear approximation can be used.\n\nThe time complexity for computing a square root with \"n\" digits of precision is equivalent to that of multiplying two \"n\"-digit numbers.\n\nAnother useful method for calculating the square root is the shifting nth root algorithm, applied for .\n\nThe name of the square root function varies from programming language to programming language, with codice_1 (often pronounced \"squirt\" ) being common, used in C, C++, and derived languages like JavaScript, PHP, and Python.\n\nThe square of any positive or negative number is positive, and the square of 0 is 0. Therefore, no negative number can have a real square root. However, it is possible to work with a more inclusive set of numbers, called the complex numbers, that does contain solutions to the square root of a negative number. This is done by introducing a new number, denoted by \"i\" (sometimes \"j\", especially in the context of electricity where \"i\" traditionally represents electric current) and called the imaginary unit, which is \"defined\" such that . Using this notation, we can think of \"i\" as the square root of −1, but notice that we also have and so −\"i\" is also a square root of −1. By convention, the principal square root of −1 is \"i\", or more generally, if \"x\" is any nonnegative number, then the principal square root of −\"x\" is\n\nThe right side (as well as its negative) is indeed a square root of −\"x\", since\n\nFor every non-zero complex number \"z\" there exist precisely two numbers \"w\" such that : the principal square root of \"z\" (defined below), and its negative.\n\nThere are two complex numbers that square to a given arbitrary non-zero imaginary number formula_11 with real formula_12\n\nwhere the two-digit pattern {3, 6} repeats over and over again in the partial denominators. Since , the above is also identical to the following generalized continued fractions:\n\nThe square root of a positive number is usually defined as the side length of a square with the area equal to the given number. But the square shape is not necessary for it: if one of two similar planar Euclidean objects has the area \"a\" times greater than another, then the ratio of their linear sizes is .\n\nA square root can be constructed with a compass and straightedge. In his Elements, Euclid (fl. 300 BC) gave the construction of the geometric mean of two quantities in two different places: Proposition II.14 and Proposition VI.13. Since the geometric mean of \"a\" and \"b\" is , one can construct simply by taking .\n\nThe construction is also given by Descartes in his \"La Géométrie\", see figure 2 on page 2. However, Descartes made no claim to originality and his audience would have been quite familiar with Euclid.\n\nEuclid's second proof in Book VI depends on the theory of similar triangles. Let AHB be a line segment of length with and . Construct the circle with AB as diameter and let C be one of the two intersections of the perpendicular chord at H with the circle and denote the length CH as \"h\". Then, using Thales' theorem and, as in the proof of Pythagoras' theorem by similar triangles, triangle AHC is similar to triangle CHB (as indeed both are to triangle ACB, though we don't need that, but it is the essence of the proof of Pythagoras' theorem) so that AH:CH is as HC:HB, i.e. , from which we conclude by cross-multiplication that , and finally that . When marking the midpoint O of the line segment AB and drawing the radius OC of length , then clearly OC > CH, i.e. (with equality if and only if ), which is the arithmetic–geometric mean inequality for two variables and, as noted above, is the basis of the Ancient Greek understanding of \"Heron's method\".\n\nAnother method of geometric construction uses right triangles and induction: can, of course, be constructed, and once has been constructed, the right triangle with 1 and for its legs has a hypotenuse of . The Spiral of Theodorus is constructed using successive square roots in this manner.\n\n\n\n"}
{"id": "19351165", "url": "https://en.wikipedia.org/wiki?curid=19351165", "title": "Timeline of category theory and related mathematics", "text": "Timeline of category theory and related mathematics\n\nThis is a timeline of category theory and related mathematics. Its scope ('related mathematics') is taken as:\n\nIn this article and in category theory in general ∞ = \"ω\".\n\n\n"}
{"id": "7282654", "url": "https://en.wikipedia.org/wiki?curid=7282654", "title": "Trailing zero", "text": "Trailing zero\n\nIn mathematics, trailing zeros are a sequence of 0 in the decimal representation (or more generally, in any positional representation) of a number, after which no other digits follow.\n\nTrailing zeros to the right of a decimal point, as in 12.3400, do not affect the value of a number and may be omitted if all that is of interest is its numerical value. This is true even if the zeros recur infinitely. For example, in pharmacy, trailing zeros are omitted from dose values to prevent misreading. However, trailing zeros may be useful for indicating the number of significant figures, for example in a measurement. In such a context, \"simplifying\" a number by removing trailing zeros would be incorrect. \n\nThe number of trailing zeros in a non-zero base-\"b\" integer \"n\" equals the exponent of the highest power of \"b\" that divides \"n\". For example, 14000 has three trailing zeros and is therefore divisible by 1000 = 10, but not by 10. This property is useful when looking for small factors in integer factorization. Some computer architectures have a count trailing zeros operation in their instruction set for efficiently determining the number of trailing zero bits in a machine word.\n\nThe number of trailing zeros in the decimal representation of \"n\"!, the factorial of a non-negative integer \"n\", is simply the multiplicity of the prime factor 5 in \"n\"<nowiki>!</nowiki>. This can be determined with this special case of de Polignac's formula:\n\nwhere \"k\" must be chosen such that\n\nmore precisely\n\nand formula_5 denotes the floor function applied to \"a\". For \"n\" = 0, 1, 2, ... this is\n\nFor example, 5 > 32, and therefore 32! = 263130836933693530167218012160000000 ends in\n\nzeros. If \"n\" < 5, the inequality is satisfied by \"k\" = 0; in that case the sum is empty, giving the answer 0.\n\nThe formula actually counts the number of factors 5 in \"n\"<nowiki>!</nowiki>, but since there are at least as many factors 2, this is equivalent to the number of factors 10, each of which gives one more trailing zero.\n\nDefining\n\nthe following recurrence relation holds:\n\nThis can be used to simplify the computation of the terms of the summation, which can be stopped as soon as \"q\" reaches zero. The condition is equivalent to \n\n\n"}
{"id": "18609360", "url": "https://en.wikipedia.org/wiki?curid=18609360", "title": "Ultralimit", "text": "Ultralimit\n\nIn mathematics, an ultralimit is a geometric construction that assigns to a sequence of metric spaces \"X\" a limiting metric space. The notion of an ultralimit captures the limiting behavior of finite configurations in the spaces \"X\" and uses an ultrafilter to avoid the process of repeatedly passing to subsequences to ensure convergence. An ultralimit is a generalization of the notion of Gromov–Hausdorff convergence of metric spaces.\n\nRecall that an ultrafilter \"ω\" on the set of natural numbers is a set of nonempty subsets of (whose inclusion function can thought of as a measure) which is closed under finite intersection, upwards-closed, and which, given any subset \"X\" of , contains either \"X\" or . An ultrafilter \"ω\" on is \"non-principal\" if it contains no finite set.\n\nLet \"ω\" be a non-principal ultrafilter on formula_1.\nIf formula_2 is a sequence of points in a metric space (\"X\",\"d\") and \"x\"∈ \"X\", the point \"x\" is called the \"ω\" -\"limit\" of \"x\", denoted formula_3, if for every formula_4 we have:\n\nIt is not hard to see the following:\n\nAn important basic fact states that, if (\"X\",\"d\") is compact and \"ω\" is a non-principal ultrafilter on formula_1, the \"ω\"-limit of any sequence of points in \"X\" exists (and is necessarily unique).\n\nIn particular, any bounded sequence of real numbers has a well-defined \"ω\"-limit in formula_9 (as closed intervals are compact).\n\nLet \"ω\" be a non-principal ultrafilter on formula_1. Let (\"X\",\"d\") be a sequence of metric spaces with specified base-points \"p\"∈\"X\".\n\nLet us say that a sequence formula_11, where \"x\"∈\"X\", is \"admissible\", if the sequence of real numbers (\"d\"(\"x\",\"p\")) is bounded, that is, if there exists a positive real number \"C\" such that formula_12.\nLet us denote the set of all admissible sequences by formula_13.\n\nIt is easy to see from the triangle inequality that for any two admissible sequences formula_14 and formula_15 the sequence (\"d\"(\"x\",\"y\")) is bounded and hence there exists an \"ω\"-limit formula_16. Let us define a relation formula_17 on the set formula_13 of all admissible sequences as follows. For formula_19 we have formula_20 whenever formula_21 It is easy to show that formula_17 is an equivalence relation on formula_23\n\nThe ultralimit with respect to \"ω\" of the sequence (\"X\",\"d\", \"p\") is a metric space formula_24 defined as follows.\n\nAs a set, we have formula_25 .\n\nFor two formula_17-equivalence classes formula_27 of admissible sequences formula_14 and formula_15 we have formula_30\n\nIt is not hard to see that formula_31 is well-defined and that it is a metric on the set formula_32.\n\nDenote formula_33 .\n\nSuppose that (\"X\",\"d\") is a sequence of metric spaces of uniformly bounded diameter, that is, there exists a real number \"C\">0 such that diam(\"X\")≤\"C\" for every formula_34. Then for any choice \"p\" of base-points in \"X\" \"every\" sequence formula_35 is admissible. Therefore, in this situation the choice of base-points does not have to be specified when defining an ultralimit, and the ultralimit formula_24 depends only on (\"X\",\"d\") and on \"ω\" but does not depend on the choice of a base-point sequence formula_37. In this case one writes formula_38.\n\nActually, by construction, the limit space is always complete, even when (\"X\",\"d\")\nis a repeating sequence of a space (\"X\",\"d\") which is not complete.\n\nAn important class of ultralimits are the so-called \"asymptotic cones\" of metric spaces. Let (\"X\",\"d\") be a metric space, let \"ω\" be a non-principal ultrafilter on formula_1 and let \"p\" ∈ \"X\" be a sequence of base-points. Then the \"ω\"–ultralimit of the sequence formula_48 is called the asymptotic cone of \"X\" with respect to \"ω\" and formula_49 and is denoted formula_50. One often takes the base-point sequence to be constant, \"p\" = \"p\" for some \"p ∈ X\"; in this case the asymptotic cone does not depend on the choice of \"p ∈ X\" and is denoted by formula_51 or just formula_52.\n\nThe notion of an asymptotic cone plays an important role in geometric group theory since asymptotic cones (or, more precisely, their topological types and bi-Lipschitz types) provide quasi-isometry invariants of metric spaces in general and of finitely generated groups in particular. Asymptotic cones also turn out to be a useful tool in the study of relatively hyperbolic groups and their generalizations.\n\n\n\n"}
{"id": "1564194", "url": "https://en.wikipedia.org/wiki?curid=1564194", "title": "Universal instantiation", "text": "Universal instantiation\n\nIn predicate logic universal instantiation (UI; also called universal specification or universal elimination, and sometimes confused with \"dictum de omni\") is a valid rule of inference from a truth about each member of a class of individuals to the truth about a particular individual of that class. It is generally given as a quantification rule for the universal quantifier but it can also be encoded in an axiom. It is one of the basic principles used in quantification theory.\n\nExample: \"All dogs are mammals. Fido is a dog. Therefore Fido is a mammal.\"\n\nIn symbols the rule as an axiom schema is\nfor some term \"a\" and where formula_2 is the result of substituting \"a\" for all \"free\" occurrences of \"x\" in \"A\". formula_3 is an instance of formula_4\n\nAnd as a rule of inference it is\nwith \"A\"(\"a\"/\"x\") the same as above.\n\nIrving Copi noted that universal instantiation \"...follows from variants of rules for 'natural deduction', which were devised independently by Gerhard Gentzen and Stanisław Jaśkowski in 1934.\" \n\nAccording to Willard Van Orman Quine, universal instantiation and existential generalization are two aspects of a single principle, for instead of saying that \"∀\"x\" \"x\" = \"x\"\" implies \"Socrates = Socrates\", we could as well say that the denial \"Socrates ≠ Socrates\" implies \"∃\"x\" \"x\" ≠ \"x\"\". The principle embodied in these two operations is the link between quantifications and the singular statements that are related to them as instances. Yet it is a principle only by courtesy. It holds only in the case where a term names and, furthermore, occurs referentially.\n\n"}
{"id": "145094", "url": "https://en.wikipedia.org/wiki?curid=145094", "title": "XML Metadata Interchange", "text": "XML Metadata Interchange\n\nThe XML Metadata Interchange (XMI) is an Object Management Group (OMG) standard for exchanging metadata information via Extensible Markup Language (XML).\n\nIt can be used for any metadata whose metamodel can be expressed in Meta-Object Facility (MOF).\n\nThe most common use of XMI is as an interchange format for UML models, although it can also be used for serialization of models of other languages (metamodels).\n\nIn the OMG vision of modeling, data is split into abstract models and concrete models. The abstract models represent the semantic information, whereas the concrete models represent visual diagrams. Abstract models are instances of arbitrary MOF-based modeling languages such as UML or SysML. For diagrams, the Diagram Interchange (DI, XMI[DI]) standard is used. At the moment there are several incompatibilities between different modeling tool vendor implementations of XMI, even between interchange of abstract model data. The usage of Diagram Interchange is almost nonexistent. This means exchanging files between UML modeling tools using XMI is rarely possible.\n\nOne purpose of XML Metadata Interchange (XMI) is to enable easy interchange of metadata between UML-based modeling tools and MOF-based metadata repositories in distributed heterogeneous environments. XMI is also commonly used as the medium by which models are passed from modeling tools to software generation tools as part of model-driven engineering.\n\nXMI integrates four industry standards:\n\nThe integration of these four standards into XMI allows tool developers of distributed systems to share object models and other metadata.\n\nSeveral versions of XMI have been created: 1.0, 1.1, 1.2, 2.0, 2.1, 2.1.1, 2.4, 2.4.1, 2.4.2. and 2 5.1. The 2.x versions are radically different from the 1.x series.\nThere are now other XML standards for representing metadata. One of the most recent is the Web Ontology Language (OWL) (but ontologies are a very specialized kind of metadata, and OWL has no built-in support for most of the information represented in UML). OWL is built upon the Resource Description Framework (RDF).\n\nThe Diagram Definition OMG project is another alternative, which can also express the layout and graphical representation.\n\nXMI is an international standard:\n\n\n"}
{"id": "29676565", "url": "https://en.wikipedia.org/wiki?curid=29676565", "title": "Yusnier Viera", "text": "Yusnier Viera\n\nYusnier Viera (born April 26, 1982) is a Cuban American mental calculator. He is well known as \"The Human Calendar\" for his world record on calendar dates. On October 31, 2005 he broke for first time the World Record for calendar calculations. At the Mental Calculation World Cup in 2010 he won the calendar category. He currently has three World Records for calendar calculations.\n\nViera has appeared in prestigious TV channels like CNN & ABC and has starred on the international Discovery Channel Series \"Superhuman Showdown\" (trailer). In early 2014, he participated in the Latin American show \"Super Cerebros\", of NatGeo. He won the first round and four thousand five hundred dollars of cash prize, reaching the final round of the show.\n\nDue to his extraordinary skills, University of Sussex neuroscientists took fMRI scans of his brain. In the study, he also completed a computerized version of the Raven's Progressive Matrices Test with an IQ score of 157 (standard deviation of 15). The scientists concluded that his expertise is a result of long-term practice and motivation.\n\nOn 2016, Yusnier participated in the Fox show \"Superhumans\" where he showed a new skill called \"flash math\". Later, he was invited to \"The Ellen DeGeneres Show\" for an impressive demonstration.\n\nRecently, he published the books \"Basic Course of Mental Arithmetic\" and \"Master the Multiplication Tables\".\n"}
{"id": "8072034", "url": "https://en.wikipedia.org/wiki?curid=8072034", "title": "Zoia Ceaușescu", "text": "Zoia Ceaușescu\n\nZoia Ceaușescu (, 28 February 1949 – 20 November 2006) was a Romanian mathematician, the daughter of Communist leader Nicolae Ceaușescu and his wife, Elena.\n\nShe studied at High School nr. 24 (now Jean Monnet High School) in Bucharest, finishing in 1966. She then continued her studies at the Faculty of Mathematics, University of Bucharest. After completing her Ph.D. in mathematics, she worked as a researcher at the Institute of Mathematics of the Romanian Academy in Bucharest. Her field of specialization was functional analysis. Allegedly, her parents were unhappy with their daughter's choice of doing research in mathematics, so the Institute was disbanded in 1975. She moved on to work for Institutul pentru Creație Științifică și Tehnică (INCREST, Institute for Scientific and Technical Creativity), where she eventually started and headed a new department of mathematics. In 1976, Ceausesca received the Simion Stoilow Prize for her outstanding contributions to the mathematical sciences.\n\nShe was married in 1980 to Mircea Oprean, an engineer and professor at the Polytechnic University of Bucharest.\n\nDuring the Romanian Revolution, on 24 December 1989 she was arrested for \"undermining the Romanian economy\" and was released only eight months later, on 18 August 1990. After she was freed, she tried unsuccessfully to return to her former job at INCREST, then gave up and retired. After the revolution, some newspapers reported that she had lived a wild life, having plenty of lovers and often being drunk.\n\nAfter her parents were executed, the new government confiscated the house where she and her husband lived (the house was used as proof of allegedly stolen wealth), so she had to live with friends.\n\nAfter the revolution that ousted her parents, Zoia reported that during her parents' time in power her mother had asked the Securitate to keep an eye on the Ceaușescu children, perhaps she felt, out of a \"sense of love\". The Securitate \"could not touch\" the children she said, but the information they provided created a lot of problems for the children. She also remarked that power had a \"destructive effect\" on her father and that he \"lost his sense of judgment\".\n\nZoia Ceaușescu believed that her parents were not buried in Ghencea Cemetery; she attempted to have their remains exhumed, but a military court refused her request.\n\nZoia was known to be an inveterate smoker. She died of lung cancer in 2006, at age 57.\n\nZoia Ceaușescu published 22 scientific papers between 1976 and 1988. Some of those are:\n"}
