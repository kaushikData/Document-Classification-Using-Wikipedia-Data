{"id": "159285", "url": "https://en.wikipedia.org/wiki?curid=159285", "title": "Abel Prize", "text": "Abel Prize\n\nThe Abel Prize () is a Norwegian prize awarded annually by the King of Norway to one or more outstanding mathematicians. It is named after Norwegian mathematician Niels Henrik Abel (1802–1829) and directly modeled after the Nobel Prizes. It comes with a monetary award of 6 million Norwegian kroner (NOK) (€635,000 or $740,000).\n\nThe Abel Prize's history dates back to 1899, when its establishment was proposed by the Norwegian mathematician Sophus Lie when he learned that Alfred Nobel's plans for annual prizes would not include a prize in mathematics. In 1902 King Oscar II of Sweden and Norway indicated his willingness to finance a mathematics prize to complement the Nobel Prizes, but the establishment of the prize was prevented by the dissolution of the union between Norway and Sweden in 1905. It took almost a century before the prize was finally established by the Government of Norway in 2001, and it was specifically intended \"to give the mathematicians their own equivalent of a Nobel Prize.\" The laureates are selected by the Abel Committee, the members of which are appointed by the Norwegian Academy of Science and Letters.\n\nThe award ceremony takes place in the Aula of the University of Oslo, where the Nobel Peace Prize was awarded between 1947 and 1989. The Abel Prize board has also established an Abel symposium, administered by the Norwegian Mathematical Society.\n\nThe prize was first proposed in 1899, to be part of the celebration of the 100th anniversary of Niels Henrik Abel's birth in 1902. Shortly before his death in 1899, the Norwegian mathematician Sophus Lie proposed establishing an Abel Prize when he learned that Alfred Nobel's plans for annual prizes would not include a prize in mathematics. King Oscar II was willing to finance a mathematics prize in 1902, and the mathematicians Ludwig Sylow and Carl Størmer drew up statutes and rules for the proposed prize. However, Lie's influence waned after his death, and the dissolution of the union between Sweden and Norway in 1905 ended the first attempt to create an Abel Prize.\n\nAfter interest in the concept of the prize had risen in 2001, a working group was formed to develop a proposal, which was presented to the Prime Minister of Norway in May. In August 2001, the Norwegian government announced that the prize would be awarded beginning in 2002, the two-hundredth anniversary of Abel's birth. Atle Selberg received an honorary Abel Prize in 2002, but the first actual Abel Prize was awarded in 2003.\n\nA book series presenting Abel Prize laureates and their research was commenced in 2010. The first two volumes cover the years 2003–2007 and 2008–2012 respectively.\n\nAnyone may submit a nomination for the Abel Prize, however, self-nominations are not permitted. The nominee must be alive; however, if the awardee dies after being declared as the winner, the prize will be awarded posthumously. \n\nThe Norwegian Academy of Science and Letters declares the winner of the Abel Prize each March after recommendation by the Abel Committee, which consists of five leading mathematicians. Both Norwegians and non-Norwegians may serve on the Committee. They are elected by the Norwegian Academy of Science and Letters and nominated by the International Mathematical Union and the European Mathematical Society. The committee is of 2018 chaired by Norwegian mathematician Hans Munthe-Kaas (University of Bergen), and was before that, headed by Professor John Rognes.\n\nThe Norwegian Government gave the prize an initial funding of NOK 200 million (about €21.7 million) in 2001. Previously, the funding came from the Abel foundation, but today the prize is financed directly through the national budget. \n\nThe funding is controlled by the Board, which consists of members elected by the Norwegian Academy of Science and Letters.. The current leader of the Board is John Grue. \n\n\n"}
{"id": "2194283", "url": "https://en.wikipedia.org/wiki?curid=2194283", "title": "Actor model theory", "text": "Actor model theory\n\nIn theoretical computer science, Actor model theory concerns theoretical issues for the Actor model.\n\nActors are the primitives that form the basis of the Actor model of concurrent digital computation. In response to a message that it receives, an Actor can make local decisions, create more Actors, send more messages, and designate how to respond to the next message received. Actor model theory incorporates theories of the events and structures of Actor computations, their proof theory, and denotational models.\n\nFrom the definition of an Actor, it can be seen that numerous events take place: local decisions, creating Actors, sending messages, receiving messages, and designating how to respond to the next message received.\n\nHowever, this article focuses on just those events that are the arrival of a message sent to an Actor.\n\nThis article reports on the results published in Hewitt [2006].\n\nThe activation ordering (codice_1) is a fundamental ordering that models one event activating another (there must be energy flow in the message passing from an event to an event which it activates).\n\n\nThe arrival ordering of an Actor codice_10 ( codice_11) models the (total) ordering of events in which a message arrives at codice_10. Arrival ordering is determined by \"arbitration\" in processing messages (often making use of a digital circuit called an arbiter). The arrival events of an Actor are on its world line. The arrival ordering means that the Actor model inherently has indeterminacy (see Indeterminacy in concurrent computation).\n\n\nThe combined ordering (denoted by codice_24) is defined to be the transitive closure of the activation ordering and the arrival orderings of all Actors.\n\n\nThe combined ordering is obviously transitive by definition.\n\nIn [Baker and Hewitt 197?], it was conjectured that the above laws might entail the following law:\n\nHowever, [Clinger 1981] surprisingly proved that the Law of Finite Chains Between Events in the Combined Ordering is independent of the previous laws, \"i.e.\",\n\nTheorem. \"The Law of Finite Chains Between Events in the Combined Ordering does not follow from the previously stated laws.\"\n\nProof. It is sufficient to show that there is an Actor computation that satisfies the previously stated laws but violates the Law of Finite Chains Between Events in the Combined Ordering.\n\nHowever, we know from physics that infinite energy cannot be expended along a finite trajectory. Therefore, since the Actor model is based on physics, the Law of Finite Chains Between Events in the Combined Ordering was taken as an axiom of the Actor model.\n\nThe Law of Finite Chains Between Events in the Combined Ordering is closely related to the following law:\n\nIn fact the previous two laws have been shown to be equivalent:\n\nThe law of discreteness rules out Zeno machines and is related to results on Petri nets [Best \"et al.\" 1984, 1987].\n\nThe Law of Discreteness implies the property of unbounded nondeterminism. The combined ordering is used by [Clinger 1981] in the construction of a denotational model of Actors (see denotational semantics).\n\nClinger [1981] used the Actor event model described above to construct a denotational model for Actors using power domains. Subsequently Hewitt [2006] augmented the diagrams with arrival times to construct a technically simpler denotational model that is easier to understand.\n\n\n"}
{"id": "42305243", "url": "https://en.wikipedia.org/wiki?curid=42305243", "title": "Age at risk", "text": "Age at risk\n\nAge at Risk (AaR) is a time-based risk measure designed to measure longevity risk in actuarial models.\n\nAaR represents certain quantile for a given probability distribution, so is similar to Value at Risk (VaR).\nBut, AaR measures risk amount as time(time until an adverse event) rather than value(loss amount).\n\nAge at Risk is a special case of Time at Risk (TaR). \nWhen TaR is applied to a household's financial planning rather than corporate finance,\nTaR in this case is referred to as AaR.\n\nIn fact, TaR is an expanded idea from AaR which is originated by actuaries.\nIn actuaries's view, loss model and survival model are identical, so they could naturally transform VaR into time-based measure.\nSurvival model is actively used in life insurance modeling, but the application is not limited to this area. Anything that has lifespan can be a subject of survival model; for example, durability of products, default of bonds, bankruptcy of companies etc.\nAaR had been used in models that include a person's lifetime as a random variable, and it was naturally expanded to TaR as actuaries generalized subjects of models.\n\nMathematical definition of AaR is same as that of VaR.\n\nHowever, value-based random variable is replaced with time-based one (age), and given time-horizon is replaced with given finance structure of a household.\n"}
{"id": "348973", "url": "https://en.wikipedia.org/wiki?curid=348973", "title": "Antichain", "text": "Antichain\n\nIn mathematics, in the area of order theory, an antichain is a subset of a partially ordered set such that any two distinct elements in the subset are incomparable. (Some authors use the term \"antichain\" to mean strong antichain, a subset such that there is no element of the poset smaller than two distinct elements of the antichain.)\n\nLet \"S\" be a partially ordered set. We say two elements \"a\" and \"b\" of a partially ordered set are comparable if \"a\" ≤ \"b\" or \"b\" ≤ \"a\". If two elements are not comparable, we say they are incomparable; that is, \"x\" and \"y\" are incomparable if neither \"x\" ≤ \"y\" nor \"y\" ≤ \"x\".\n\nA chain in \"S\" is a subset \"C\" of \"S\" in which each pair of elements is comparable; that is, \"C\" is totally ordered. An antichain in \"S\" is a subset \"A\" of \"S\" in which each pair of different elements is incomparable; that is, there is no order relation between any two different elements in \"A\".\n\nA maximal antichain is an antichain that is not a proper subset of any other antichain. A maximum antichain is an antichain that has cardinality at least as large as every other antichain. The \"width\" of a partially ordered set is the cardinality of a maximum antichain. Any antichain can intersect any chain in at most one element, so, if we can partition the elements of an order into \"k\" chains then the width of the order must be at most \"k\" (if the antichain has more than \"k\" elements, by the Pigeonhole Principle, there would be 2 of its elements belonging to the same chain, contradiction). Dilworth's theorem states that this bound can always be reached: there always exists an antichain, and a partition of the elements into chains, such that the number of chains equals the number of elements in the antichain, which must therefore also equal the width. Similarly, we can define the \"height\" of a partial order to be the maximum cardinality of a chain. Mirsky's theorem states similarly that in any partial order of finite height, the height equals the smallest number of antichains into which the order may be partitioned.\n\nAn antichain in the inclusion ordering of subsets of an \"n\"-element set is known as a Sperner family. The number of different Sperner families is counted by the Dedekind numbers, the first few of which numbers are\nEven the empty set has two antichains in its power set: one containing a single set (the empty set itself) and one containing no sets.\n\nAny antichain \"A\" corresponds to a lower set\nIn a finite partial order (or more generally a partial order satisfying the ascending chain condition) all lower sets have this form. The union of any two lower sets is another lower set, and the union operation corresponds in this way to a join operation\non antichains:\nSimilarly, we can define a meet operation on antichains, corresponding to the intersection of lower sets:\nThe join and meet operations on all finite antichains of finite subsets of a set \"X\" define a distributive lattice, the free distributive lattice generated by \"X\". Birkhoff's representation theorem for distributive lattices states that every finite distributive lattice can be represented via join and meet operations on antichains of a finite partial order, or equivalently as union and intersection operations on the lower sets of the partial order.\n\n\n"}
{"id": "998824", "url": "https://en.wikipedia.org/wiki?curid=998824", "title": "Atom (order theory)", "text": "Atom (order theory)\n\nIn the mathematical field of order theory, an element \"a\" of a partially ordered set with least element 0 is an atom if 0 < \"a\" and there is no \"x\" such that 0 < \"x\" < \"a\". \n\nEquivalently, one may define an atom to be an element that is minimal among the non-zero elements, or alternatively an element that covers the least element 0.\n\nLet <: denote the cover relation in a partially ordered set.\n\nA partially ordered set with a least element 0 is atomic if every element \"b\" > 0 has an atom \"a\" below it, that is, there is some \"a\" such that \"b\" ≥ \"a\" :> \"0\". Every finite partially ordered set with 0 is atomic, but the set of nonnegative real numbers (ordered in the usual way) is not atomic (and in fact has no atoms).\n\nA partially ordered set is relatively atomic (or \"strongly atomic\") if for all \"a\" < \"b\" there is an element \"c\" such that \"a\" <: \"c\" ≤ \"b\" or, equivalently, if every interval [\"a\", \"b\"] is atomic. Every relatively atomic partially ordered set with a least element is atomic. Every finite poset is relatively atomic,\n\nA partially ordered set with least element 0 is called atomistic if every element is the least upper bound of a set of atoms. The linear order with three elements is not atomistic (see Fig.2).\n\nAtoms in partially ordered sets are abstract generalizations of singletons in set theory (see Fig.1). Atomicity (the property of being atomic) provides an abstract generalization in the context of order theory of the ability to select an element from a non-empty set.\n\nThe terms \"coatom\", \"coatomic\", and \"coatomistic\" are defined dually. Thus, in a partially ordered set with greatest element 1, one says that\n\n"}
{"id": "26747160", "url": "https://en.wikipedia.org/wiki?curid=26747160", "title": "Bihari–LaSalle inequality", "text": "Bihari–LaSalle inequality\n\nThe Bihari–LaSalle inequality, was proved by the American mathematician\nJoseph P. LaSalle (1916–1983) in 1949 and by the Hungarian mathematician\nImre Bihari (1915–1998) in 1956. It is the following nonlinear generalization of Grönwall's lemma.\n\nLet \"u\" and \"ƒ\" be non-negative continuous functions defined on the half-infinite ray [0, ∞), and let \"w\" be a continuous non-decreasing function defined on [0, ∞) and \"w\"(\"u\") > 0 on (0, ∞). If \"u\" satisfies the following integral inequality,\n\nwhere \"α\" is a non-negative constant, then\n\nwhere the function \"G\" is defined by\n\nand \"G\" is the inverse function of \"G\" and \"T\" is chosen so that\n"}
{"id": "12365918", "url": "https://en.wikipedia.org/wiki?curid=12365918", "title": "Camera matrix", "text": "Camera matrix\n\nIn computer vision a camera matrix or (camera) projection matrix is a formula_1 matrix which describes the mapping of a pinhole camera from 3D points in the world to 2D points in an image.\n\nLet formula_2 be a representation of a 3D point in homogeneous coordinates (a 4-dimensional vector), and let formula_3 be a representation of the image of this point in the pinhole camera (a 3-dimensional vector). Then the following relation holds\n\nwhere formula_5 is the camera matrix and the formula_6 sign implies that the left and right hand sides are equal up to a non-zero scalar multiplication.\n\nSince the camera matrix formula_5 is involved in the mapping between elements of two projective spaces, it too can be regarded as a projective element. This means that it has only 11 degrees of freedom since any multiplication by a non-zero scalar results in an equivalent camera matrix.\n\nThe mapping from the coordinates of a 3D point P to the 2D image coordinates of the point's projection onto the image plane, according to the pinhole camera model is given by\n\nwhere formula_9 are the 3D coordinates of P relative to a camera centered coordinate system, formula_10 are the resulting image coordinates, and \"f\" is the camera's focal length for which we assume \"f\" > 0. Furthermore, we also assume that \"x > 0\".\n\nTo derive the camera matrix this expression is rewritten in terms of homogeneous coordinates. Instead of the 2D vector formula_11 we consider the projective element (a 3D vector) formula_12 and instead of equality we consider equality up to scaling by a non-zero number, denoted formula_6. First, we write the homogeneous image coordinates as expressions in the usual 3D coordinates.\n\nFinally, also the 3D coordinates are expressed in a homogeneous representation formula_2 and this is how the camera matrix appears:\n\nwhere formula_5 is the camera matrix, which here is given by\n\nand the corresponding camera matrix now becomes\n\nThe last step is a consequence of formula_5 itself being a projective element.\n\nThe camera matrix derived here may appear trivial in the sense that it contains very few non-zero elements. This depends to a large extent on the particular coordinate systems which have been chosen for the 3D and 2D points. In practice, however, other forms of camera matrices are common, as will be shown below.\n\nThe camera matrix formula_5 derived in the previous section has a null space which is spanned by the vector\n\nThis is also the homogeneous representation of the 3D point which has coordinates (0,0,0), that is, the \"camera center\" (aka the entrance pupil; the position of the pinhole of a pinhole camera) is at O. This means that the camera center (and only this point) cannot be mapped to a point in the image plane by the camera (or equivalently, it maps to all points on the image as every ray on the image goes through this point).\n\nFor any other 3D point with formula_24, the result formula_25 is well-defined and has the form formula_26. This corresponds to a point at infinity in the projective image plane (even though, if the image plane is taken to be a Euclidean plane, no corresponding intersection point exists).\n\nThe camera matrix derived above can be simplified even further if we assume that \"f = 1\":\n\nwhere formula_28 here denotes a formula_29 identity matrix. Note that formula_30 matrix formula_5 here is divided into a concatenation of a formula_29 matrix and a 3-dimensional vector. The camera matrix formula_33 is sometimes referred to as a \"canonical form\".\n\nSo far all points in the 3D world have been represented in a \"camera centered\" coordinate system, that is, a coordinate system which has its origin at the camera center (the location of the pinhole of a pinhole camera). In practice however, the 3D points may be represented in terms of coordinates relative to an arbitrary coordinate system (X1',X2',X3'). Assuming that the camera coordinate axes (X1,X2,X3) and the axes (X1',X2',X3') are of Euclidean type (orthogonal and isotropic), there is a unique Euclidean 3D transformation (rotation and translation) between the two coordinate systems. In other words, the camera is not necessarily at the origin looking along the \"z\" axis.\n\nThe two operations of rotation and translation of 3D coordinates can be represented as the two formula_34 matrices\n\nwhere formula_37 is a formula_29 rotation matrix and formula_39 is a 3-dimensional translation vector. When the first matrix is multiplied onto the homogeneous representation of a 3D point, the result is the homogeneous representation of the rotated point, and the second matrix performs instead a translation. Performing the two operations in sequence, i.e. first the rotation and then the translation (with translation vector given in the already rotated coordinate system), gives a combined rotation and translation matrix\n\nAssuming that formula_37 and formula_39 are precisely the rotation and translations which relate the two coordinate system (X1,X2,X3) and (X1',X2',X3') above, this implies that\n\nwhere formula_44 is the homogeneous representation of the point P in the coordinate system (X1',X2',X3').\n\nAssuming also that the camera matrix is given by formula_33, the mapping from the coordinates in the (X1',X2',X3') system to homogeneous image coordinates becomes\n\nConsequently, the camera matrix which relates points in the coordinate system (X1',X2',X3') to image coordinates is\n\na concatenation of a 3D rotation matrix and a 3-dimensional translation vector.\n\nThis type of camera matrix is referred to as a \"normalized camera matrix\", it assumes focal length = 1 and that image coordinates are measured in a coordinate system where the origin is located at the intersection between axis X3 and the image plane and has the same units as the 3D coordinate system. The resulting image coordinates are referred to as \"normalized image coordinates\".\n\nAgain, the null space of the normalized camera matrix, formula_48 described above, is spanned by the 4-dimensional vector\n\nThis is also, again, the coordinates of the camera center, now relative to the (X1',X2',X3') system. This can be seen by applying first the rotation and then the translation to the 3-dimensional vector formula_50 and the result is the homogeneous representation of 3D coordinates (0,0,0).\n\nThis implies that the camera center (in its homogeneous representation) lies in the null space of the camera matrix, provided that it is represented in terms of 3D coordinates relative to the same coordinate system as the camera matrix refers to.\n\nThe normalized camera matrix formula_48 can now be written as\n\nwhere formula_50 is the 3D coordinates of the camera relative to the (X1',X2',X3') system.\n\nGiven the mapping produced by a normalized camera matrix, the resulting normalized image coordinates can be transformed by means of an arbitrary 2D homography. This includes 2D translations and rotations as well as scaling (isotropic and anisotropic) but also general 2D perspective transformations. Such a transformation can be represented as a formula_29 matrix formula_55 which maps the homogeneous normalized image coordinates formula_3 to the homogeneous transformed image coordinates formula_57:\n\nInserting the above expression for the normalized image coordinates in terms of the 3D coordinates gives\n\nThis produces the most general form of camera matrix\n\n"}
{"id": "44929782", "url": "https://en.wikipedia.org/wiki?curid=44929782", "title": "Carol Wood", "text": "Carol Wood\n\nCarol Saunders Wood (born February 9, 1945, in Pennington Gap, Virginia) is a retired American mathematician, the Edward Burr Van Vleck Professor of Mathematics, \"Emerita\", at Wesleyan University. Her research concerns mathematical logic and model-theoretic algebra, and in particular the theory of differentially closed fields.\n\nWood graduated in 1966 from Randolph-Macon Woman's College, a small United Methodist college in Lynchburg, Virginia. She earned her doctorate in 1971 from Yale University with a dissertation on forcing supervised by Abraham Robinson. At Wesleyan, she served three times as department chair. She was president of the Association for Women in Mathematics from 1991 to 1993, and served on the board of trustees of the American Mathematical Society from 2002 to 2007. She has served on the AMS Committee on Women in Mathematics since it was formed in 2012 and was chair from 2012 to 2015. She supervised 4 doctoral students at Wesleyan.\n\nWood was the 1998 commencement speaker for mathematics at the University of California, Berkeley. In 2012, she became one of the inaugural fellows of the American Mathematical Society. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
{"id": "37264795", "url": "https://en.wikipedia.org/wiki?curid=37264795", "title": "Cross-sequential study", "text": "Cross-sequential study\n\nA cross-sequential design is a research method that combines both a longitudinal design and a cross-sectional design. It aims to correct for some of the problems inherent in the cross-sectional and longitudinal designs.\n\nIn a cross-sequential design (also called an \"accelerated longitudinal\" or \"convergence\" design), a researcher wants to study development over some large period of time within the lifespan. Rather than studying particular individuals across that whole period of time (e.g. 20–60 years) as in a longitudinal design, or multiple individuals of different ages at one time (e.g. 20, 25, 30, 35, 40, 45, 50, 55, and 60 years) as in a cross-sectional design, the researcher chooses a smaller time window (e.g. 20 years) to study multiple individuals of different starting ages. An example of a cross-sequential design is shown in the table below.\nIn this table, over a span of 10 years, from 1990 to 2000, 7 overlapping cohorts with different starting ages could be studied to provide information on the whole span of development from ages 20 to 60.\n\nThis design has been used in studies to investigate career trajectories in academia and other phenomena.\n"}
{"id": "5930652", "url": "https://en.wikipedia.org/wiki?curid=5930652", "title": "Degree of a polynomial", "text": "Degree of a polynomial\n\nThe degree of a polynomial is the highest degree of its monomials (individual terms) with non-zero coefficients. The degree of a term is the sum of the exponents of the variables that appear in it, and thus is a non-negative integer. The term order has been used as a synonym of \"degree\" but, nowadays, may refer to several other concepts (see order of a polynomial).\nFor example, the polynomial formula_1 which can also be expressed as formula_2 has three terms. The first term has a degree of 5 (the sum of the powers 2 and 3), the second term has a degree of 1, and the last term has a degree of 0. Therefore, the polynomial has a degree of 5, which is the highest degree of any term.\n\nTo determine the degree of a polynomial that is not in standard form (for example:formula_3), one has to put it first in standard form by expanding the products (by distributivity) and combining the like terms; for example formula_4 is of degree 1, even though each summand has degree 2. However, this is not needed when the polynomial is expressed as a product of polynomials in standard form, because the degree of a product is the sum of the degrees of the factors.\n\nThe following names are assigned to polynomials according to their degree:\nFor higher degrees, names have sometimes been proposed, but they are rarely used:\n\nNames for degree above three are based on Latin ordinal numbers, and end in \"-ic\". This should be distinguished from the names used for the number of variables, the arity, which are based on Latin distributive numbers, and end in \"-ary\". For example, a degree two polynomial in two variables, such as formula_5, is called a \"binary quadratic\": \"binary\" due to two variables, \"quadratic\" due to degree two. There are also names for the number of terms, which are also based on Latin distributive numbers, ending in \"-nomial\"; the common ones are \"monomial\", \"binomial\", and (less commonly) \"trinomial\"; thus formula_6 is a \"binary quadratic binomial\".\n\n\nThe canonical forms of the three examples above are:\n\n\nThe degree of the sum, the product or the composition of two polynomials is strongly related to the degree of the input polynomials.\nThe degree of the sum (or difference) of two polynomials is less than or equal to the greater of their degrees; the equality always holds when the degrees of the polynomials are different i.e.\n\nE.g.\n\n\nThe degree of the product of a polynomial by a non-zero scalar is equal to the degree of the polynomial, i.e.\n\nE.g.\n\n\nNote that for polynomials over a ring containing divisors of zero, this is not necessarily true. For example, in formula_24, formula_25, but formula_26.\n\nThe set of polynomials with coefficients from a given field F and degree smaller than or equal to a given number n thus forms a vector space. (Note, however, that this set is not a ring, as it is not closed under multiplication, as is seen below.)\n\nThe degree of the product of two polynomials over a field or an integral domain is the sum of their degrees\n\nE.g.\n\nNote that for polynomials over an arbitrary ring, this is not necessarily true. For example, in formula_24, formula_30, but formula_31.\n\nThe degree of the composition of two non-constant polynomials formula_32 and formula_33 over a field or integral domain is the product of their degrees:\n\nE.g.\n\nNote that for polynomials over an arbitrary ring, this is not necessarily true. For example, in formula_24, formula_39, but formula_40.\n\nThe degree of the zero polynomial is either left undefined, or is defined to be negative (usually −1 or formula_41).\n\nLike any constant value, the value 0 can be considered as a (constant) polynomial, called the zero polynomial. It has no nonzero terms, and so, strictly speaking, it has no degree either. As such, its degree is undefined. The propositions for the degree of sums and products of polynomials in the above section do not apply if any of the polynomials involved is the zero polynomial.\n\nIt is convenient, however, to define the degree of the zero polynomial to be \"negative infinity\", formula_42 and introduce the arithmetic rules\nand\n\nThese examples illustrate how this extension satisfies the behavior rules above:\n\nA number of formulae exist which will evaluate the degree of a polynomial function \"f\". One based on asymptotic analysis is\nthis is the exact counterpart of the method of estimating the slope in a log–log plot.\n\nThis formula generalizes the concept of degree to some functions that are not polynomials.\nFor example:\nNote that the formula also gives sensible results for many combinations of such functions, e.g., the degree of formula_59 is formula_60.\n\nAnother formula to compute the degree of \"f\" from its values is\nthis second formula follows from applying L'Hôpital's rule to the first formula. Intuitively though, it is more about exhibiting the degree \"d\" as the extra constant factor in the derivative formula_62 of formula_63.\n\nA more fine grained (than a simple numeric degree) description of the asymptotics of a function can be had by using big O notation. In the analysis of algorithms, it is for example often relevant to distinguish between the growth rates of formula_64 and formula_65, which would both come out as having the \"same\" degree according to the above formulae.\n\nFor polynomials in two or more variables, the degree of a term is the \"sum\" of the exponents of the variables in the term; the degree (sometimes called the total degree) of the polynomial is again the maximum of the degrees of all terms in the polynomial. For example, the polynomial \"x\"\"y\" + 3\"x\" + 4\"y\" has degree 4, the same degree as the term \"x\"\"y\".\n\nHowever, a polynomial in variables \"x\" and \"y\", is a polynomial in \"x\" with coefficients which are polynomials in \"y\", and also a polynomial in \"y\" with coefficients which are polynomials in \"x\". The polynomial\nhas degree 3 in \"x\" and degree 2 in \"y\".\n\nGiven a ring R, the polynomial ring R[\"x\"] is the set of all polynomials in \"x\" that have coefficients chosen from R. In the special case that R is also a field, then the polynomial ring R[\"x\"] is a principal ideal domain and, more importantly to our discussion here, a Euclidean domain.\n\nIt can be shown that the degree of a polynomial over a field satisfies all of the requirements of the \"norm\" function in the euclidean domain. That is, given two polynomials \"f\"(\"x\") and \"g\"(\"x\"), the degree of the product \"f\"(\"x\")\"g\"(\"x\") must be larger than both the degrees of \"f\" and \"g\" individually. In fact, something stronger holds:\n\nFor an example of why the degree function may fail over a ring that is not a field, take the following example. Let R = formula_67, the ring of integers modulo 4. This ring is not a field (and is not even an integral domain) because 2 × 2 = 4 ≡ 0 (mod 4). Therefore, let \"f\"(\"x\") = \"g\"(\"x\") = 2\"x\" + 1. Then, \"f\"(\"x\")\"g\"(\"x\") = 4\"x\" + 4\"x\" + 1 = 1. Thus deg(\"f\"⋅\"g\") = 0 which is not greater than the degrees of \"f\" and \"g\" (which each had degree 1).\n\nSince the \"norm\" function is not defined for the zero element of the ring, we consider the degree of the polynomial \"f\"(\"x\") = 0 to also be undefined so that it follows the rules of a norm in a euclidean domain.\n\n\n"}
{"id": "361449", "url": "https://en.wikipedia.org/wiki?curid=361449", "title": "Descent (mathematics)", "text": "Descent (mathematics)\n\nIn mathematics, the idea of descent extends the intuitive idea of 'gluing' in topology. Since the topologists' glue is the use of equivalence relations on topological spaces, the theory starts with some ideas on identification.\n\nThe case of the construction of vector bundles from data on a disjoint union of topological spaces is a straightforward place to start.\n\nSuppose \"X\" is a topological space covered by open sets \"X\". Let \"Y\" be the disjoint union of the \"X\", so that there is a natural mapping\n\nWe think of \"Y\" as 'above' \"X\", with the \"X\" projection 'down' onto \"X\". With this language, \"descent\" implies a vector bundle on \"Y \"(so, a bundle given on each \"X\"), and our concern is to 'glue' those bundles \"V\", to make a single bundle \"V\" on X. What we mean is that \"V\" should, when restricted to \"X\", give back \"V\", up to a bundle isomorphism.\n\nThe data needed is then this: on each overlap\n\nintersection of \"X\" and \"X\", we'll require mappings\n\nto use to identify \"V\" and \"V\" there, fiber by fiber. Further the \"f\" must satisfy conditions based on the reflexive, symmetric and transitive properties of an equivalence relation (gluing conditions). For example, the composition\n\nfor transitivity (and choosing apt notation). The \"f\" should be identity maps and hence symmetry becomes formula_5 (so that it is fiberwise an isomorphism).\n\nThese are indeed standard conditions in fiber bundle theory (see transition map). One important application to note is \"change of fiber\": if the \"f\" are all you need to make a bundle, then there are many ways to make an associated bundle. That is, we can take essentially same \"f\", acting on various fibers.\n\nAnother major point is the relation with the chain rule: the discussion of the way there of constructing tensor fields can be summed up as 'once you learn to descend the tangent bundle, for which transitivity is the Jacobian chain rule, the rest is just 'naturality of tensor constructions'.\n\nTo move closer towards the abstract theory we need to interpret the disjoint union of the\n\nnow as\n\nthe fiber product (here an equalizer) of two copies of the projection p. The bundles on the \"X\" that we must control are \"V\"′ and \"V\"\", the pullbacks to the fiber of \"V\" via the two different projection maps to \"X\".\n\nTherefore, by going to a more abstract level one can eliminate the combinatorial side (that is, leave out the indices) and get something that makes sense for \"p\" not of the special form of covering with which we began. This then allows a category theory approach: what remains to do is to re-express the gluing conditions.\n\nThe ideas were developed in the period 1955–1965 (which was roughly the time at which the requirements of algebraic topology were met but those of algebraic geometry were not). From the point of view of abstract category theory the work of comonads of Beck was a summation of those ideas; see Beck's monadicity theorem.\n\nThe difficulties of algebraic geometry with passage to the quotient are acute. The urgency (to put it that way) of the problem for the geometers accounts for the title of the 1959 Grothendieck seminar \"TDTE\" on \"theorems of descent and techniques of existence\" (see FGA) connecting the descent question with the representable functor question in algebraic geometry in general, and the moduli problem in particular.\n\nLet formula_8. Each sheaf \"F\" on \"X\" gives rise to a descent data:\nwhere formula_10 satisfies the cocycle condition:\n\nThe fully faithful descent says: formula_12 is fully faithful. The descent theory tells conditions for which there is a fully faithful descent.\n\n\n\nOther possible sources include:\n\n"}
{"id": "22272370", "url": "https://en.wikipedia.org/wiki?curid=22272370", "title": "Fibonomial coefficient", "text": "Fibonomial coefficient\n\nIn mathematics, the Fibonomial coefficients or Fibonacci-binomial coefficients are defined as\n\nwhere \"n\" and \"k\" are non-negative integers, 0 ≤ \"k\" ≤ \"n\", \"F\" is the \"j\"-th Fibonacci number and \"n\"! is the \"n\"th Fibonorial, where 0!, being the empty product, evaluates to 1.\n\nThe Fibonomial coefficients are all integers. Some special values are:\n\nThe Fibonomial coefficients are similar to binomial coefficients and can be displayed in a triangle similar to Pascal's triangle. The first eight rows are shown below.\n\nThe recurrence relation\n\nimplies that the Fibonomial coefficients are always integers.\n\nThe fibonomial coefficients can be expressed in terms of the Gaussian binomial coefficients and the golden ratio formula_8:\n\n"}
{"id": "8887084", "url": "https://en.wikipedia.org/wiki?curid=8887084", "title": "Function series", "text": "Function series\n\nIn calculus, a function series is a series, where the summands are not just real or complex numbers but functions.\n\nExamples of function series include power series, Laurent series, Fourier series, etc.\n\nAs for sequences of functions, and unlike for series of numbers, there exist many types of convergence for a function series, such as uniform convergence, pointwise convergence, almost everywhere convergence, etc. \n\nThe Weierstrass M-test is a useful result in studying convergence of function series.\n\n"}
{"id": "50718738", "url": "https://en.wikipedia.org/wiki?curid=50718738", "title": "Generating set of a module", "text": "Generating set of a module\n\nIn algebra, a generating set \"G\" of a module \"M\" over a ring \"R\" is a subset of \"M\" such that the smallest submodule of \"M\" containing \"G\" is \"M\" itself (the smallest submodule containing a subset is the intersection of all submodules containing the set). The set \"G\" is then said to generate \"M\". For example, the ring \"R\" is generated by the identity element 1 as a left \"R\"-module over itself. If there is a finite generating set, then a module is said to be finitely generated.\n\nExplicitly, if \"G\" is a generating set of a module \"M\", then every element of \"M\" is a (finite) \"R\"-linear combination of some elements of \"G\"; i.e., for each \"x\" in \"M\", there are \"r\", ..., \"r\" in \"R\" and \"g\", ..., \"g\" in \"G\" such that\n\nPut in another way, there is a surjection\n\nwhere we wrote \"r\" for an element in the \"g\"-th component of the direct sum. (Coincidentally, since a generating set always exists; for example, \"M\" itself, this shows that a module is a quotient of a free module, a useful fact.)\n\nA generating set of a module is said to be minimal if no proper subset of the set generates the module. If \"R\" is a field, then it is the same thing as a basis. Unless the module is finitely-generated, there may exist no minimal generating set.\n\nThe cardinality of a minimal generating set need not be an invariant of the module; Z is generated as a principal ideal by 1, but it is also generated by, say, a minimal generating set }. What is uniquely determined by a module is the infimum of the numbers of the generators of the module.\n\nLet \"R\" be a local ring with maximal ideal \"m\" and residue field \"k\" and \"M\" finitely generated module. Then Nakayama's lemma says that \"M\" has a minimal generating set whose cardinality is formula_3. If \"M\" is flat, then this minimal generating set is linearly independent (so \"M\" is free). See also: minimal resolution.\n\nA more refined information is obtained if one considers the relations between the generators; cf. free presentation of a module.\n\n\n"}
{"id": "26412383", "url": "https://en.wikipedia.org/wiki?curid=26412383", "title": "GraphCrunch", "text": "GraphCrunch\n\nGraphCrunch is a comprehensive, parallelizable, and easily extendible open source software tool for analyzing and modeling large biological networks (or graphs); it compares real-world networks against a series of random graph models with respect to a multitude of local and global network properties. It is available at http://bio-nets.doc.ic.ac.uk/graphcrunch2/.\n\nRecent technological advances in experimental biology have yielded large amounts of biological network data. Many other real-world phenomena have also been described in terms of large networks (also called graphs), such as various types of social and technological networks. Thus, understanding these complex phenomena has become an important scientific problem that has led to intensive research in network modeling and analyses.\n\nAn important step towards understanding biological networks is finding an adequate network model. Evaluating the fit of a model network to the data is a formidable challenge, since network comparisons are computationally infeasible and thus have to rely on heuristics, or \"network properties.\" GraphCrunch automates the process of generating random networks drawn from a series of random graph models and evaluating the fit of the network models to a real-world network with respect to a variety of global and local network properties.\n\nGraphCrunch performs the following tasks: \n1) computes user specified global and local properties of an input real-world network, \n2) creates a user specified number of random networks belonging to user specified random graph models, \n3) compares how closely each model network reproduces a range of global and local properties (specified in point 1 above) of the real-world network, and \n4) produces the statistics of network property similarities between the data and the model networks.\n\nGraphCrunch currently supports five different types of random graph models:\n\nGraphCrunch currently supports seven global and local network properties:\n\nInstructions on how to install and run GraphCrunch are available at http://www.ics.uci.edu/~bio-nets/graphcrunch/.\n\nGraphCrunch has been used to find an optimal network model for protein-protein interaction networks, as well as for protein structure networks.\n\n"}
{"id": "728209", "url": "https://en.wikipedia.org/wiki?curid=728209", "title": "Half-period ratio", "text": "Half-period ratio\n\nIn mathematics, the half-period ratio τ of an elliptic function \"j\" is the ratio\n\nof the two half-periods formula_2 and formula_3 of \"j\", where \"j\" is defined in such a way that\n\nis in the upper half-plane.\n\nQuite often in the literature, ω and ω are defined to be the periods of an elliptic function rather than its half-periods. Regardless of the choice of notation, the ratio ω/ω of periods is identical to the ratio (ω/2)/(ω/2) of half-periods. Hence the period ratio is the same as the \"half-period ratio\".\n\nNote that the half-period ratio can be thought of as a simple number, namely, one of the parameters to elliptic functions, or it can be thought of as a function itself, because the half periods can be given in terms of the elliptic modulus or in terms of the nome.\n\nSee the pages on quarter period and elliptic integrals for additional definitions and relations on the arguments and parameters to elliptic functions.\n\n"}
{"id": "16288924", "url": "https://en.wikipedia.org/wiki?curid=16288924", "title": "Hjelmslev's theorem", "text": "Hjelmslev's theorem\n\nIn geometry, Hjelmslev's theorem, named after Johannes Hjelmslev, is the statement that if points P, Q, R... on a line are isometrically mapped to points P´, Q´, R´... of another line in the same plane, then the midpoints of the segments PP`, QQ´, RR´... also lie on a line.\n\nThe proof is easy if one assumes the classification of plane isometries. If the given isometry is odd, in which case it is necessarily either a reflection in a line or a glide-reflection (the product of three reflections in a line and two perpendiculars to it), then the statement is true of any points in the plane whatsoever: the midpoint of PP´ lies upon the axis of the (glide-)reflection for any P. If the isometry is even, compose it with reflection in line PQR to obtain an odd isometry with the same effect on P, Q, R... and apply the previous remark.\n\nThe importance of the theorem lies in the fact that it has a different proof that does \"not\" presuppose the parallel postulate and is therefore valid in non-Euclidean geometry as well. By its help, the mapping that maps every point P of the plane to the midpoint of the segment P´P´´, where P´and P´´ are the images of P under a rotation (in either sense) by a given acute angle about a given center, is seen to be a collineation mapping the whole hyperbolic plane in a 1-1 way onto the inside of a disk, thus providing a good intuitive notion of the linear structure of the hyperbolic plane. In fact, this is called the Hjelmslev transformation.\n\n"}
{"id": "5576186", "url": "https://en.wikipedia.org/wiki?curid=5576186", "title": "Index arbitrage", "text": "Index arbitrage\n\nIndex arbitrage is a subset of statistical arbitrage focusing on index components.\n\nThe idea is that an index (such as S&P 500 or Russell 2000) is made up of several components (in the example, 500 large US stocks picked by S&P to represent the US market) that influence the index price in a different manner.\n\nFor instance, there are leaders (components that react first to market impact) and laggers (the opposite). As the index is the weighted sum of all components, identifying leaders and laggers can provide a proprietary trader with the opportunity to take positions in these and make money if he/she believes the laggers will eventually rally on the leaders. The challenge being of course to correctly identify these, and to have the technology to act in the marketplace before the price correction takes place.\n\nTraders buying into stocks in advance of their joining an index and increasing weight will profit from the rise in demand for the stock when the change takes place. This effect is largely due to the popularity of index tracker funds which buy automatically on these events. The arbitrage opportunity is thus a zero-sum transfer of wealth from passive index investors to arbitrageurs. As index arbitrage becomes more common, and when reweightings occur more frequently, the loss to tracker investors increases. The presence of index arbitrageurs is an argument for active investment which is less vulnerable to this exploitation (but however incurs higher management fees) or for simple buy and hold strategies.\n\nOther types of index arbitrage include basis trading, the arbitrage between a current index value (synthetically replicated) and that of its future.\n\n"}
{"id": "17545919", "url": "https://en.wikipedia.org/wiki?curid=17545919", "title": "Institute of Combinatorics and its Applications", "text": "Institute of Combinatorics and its Applications\n\nThe Institute of Combinatorics and its Applications (ICA) is an international scientific organization formed in 1990 to increase the visibility and influence of the combinatorial community. In pursuit of this goal, the ICA sponsors conferences, publishes a bulletin and awards a number of medals, including the Euler, Hall, Kirkman, and Stanton Medels. It is based in Boca Raton, Florida and its operation office is housed at Florida Atlantic University.\nThe institute was minimally active between 2010 and 2016 and resumed its full activities in March 2016.\n\nThe ICA has over 800 members in over forty countries. Membership is at three levels. \"Members\" are those who have not yet completed a Ph.D. \"Associate Fellows\" are younger members who have received the Ph.D. or have published extensively; normally an Associate Fellow should hold the rank of Assistant Professor. \"Fellows\" are expected to be established scholars and typically have the rank of Associate Professor or higher.\nSome members are involved in highly theoretical research; there are members whose primary interest lies in education and instruction; and there are members who are heavily involved in the applications of combinatorics in statistical design, communications theory, cryptography, computer security, and other practical areas.\n\nAlthough being a fellow of the ICA is not itself a highly selective honor, the ICA also maintains another class of members, \"honorary fellows\", people who have made \"pre-eminent contributions to combinatorics or its applications\". The number of living honorary fellows is limited to ten at any time. The deceased honorary fellows include \nH. S. M. Coxeter, Paul Erdős, Haim Hanani, Bernhard Neumann, D. H. Lehmer,\nLeonard Carlitz, Robert Frucht, E. M. Wright, and Horst Sachs.\nLiving honorary fellows include\nS. S. Shrikhande, C. R. Rao, G. J. Simmons, Vera Sós, Henry Gould, and Neil Robertson.\n\nThe ICA publishes the \"Bulletin of the ICA\" (), a journal that combines publication of survey and research papers with news of members and accounts of future and past conferences. It appears three times a year, in January, May and September and usually consists of 128 pages.\n\nBeginning in 2017, the research articles in the \"Bulletin\" have been made available on an open access basis.\n\nThe ICA awards the Euler Medals annually for distinguished career contributions to combinatorics by a member of the institute who is still active in research. It is named after the 18th century mathematician Leonhard Euler.\n\nThe ICA awards the Hall Medals, named after Marshall Hall, Jr., to recognize outstanding achievements by members who are not over age 40.\n\nThe ICA awards the Kirkman Medals, named after Thomas Kirkman, to recognize outstanding achievements by members who are within four years past their Ph.D.\n\nThe winners of the medals for the years between 2010 and 2015 were decided by the ICA Medals Committee between November 2016 and February 2017 after the ICA resumed its activities in 2016.\n\nIn 2016, the ICA voted to institute an ICA medal to be known as the Stanton Medal, named after Ralph Stanton, in recognition of substantial and sustained contributions, other than research, to promoting the discipline of combinatorics. The Stanton Medal honours significant lifetime contributions to promoting the discipline of combinatorics through advocacy, outreach, service, teaching and/or mentoring. At most one medal per year is to be awarded, typically to a Fellow of the ICA. \n\n"}
{"id": "41780044", "url": "https://en.wikipedia.org/wiki?curid=41780044", "title": "Intersection", "text": "Intersection\n\nIn mathematics, the intersection of two or more objects is another, usually \"smaller\" object. All objects are presumed to lie in a certain common space except in set theory, where the intersection of arbitrary sets is defined. The intersection is one of basic concepts of geometry. Intuitively, the intersection of two or more objects is a new object that lies in each of original objects. An intersection can have various geometric shapes, but a point is the most common in a plane geometry.\n\nDefinitions vary in different contexts: set theory formalizes the idea that a smaller object lies in a larger object with inclusion, and the intersection of sets is formed of elements that belong to all intersecting sets. It is always defined, but may be empty. Incidence geometry defines an intersection (usually, of flats) as an object of lower dimension that is incident to each of original objects. In this approach an intersection can be sometimes undefined, such as for parallel lines. In both cases the concept of intersection relies on logical conjunction.\n\nAlgebraic geometry defines intersections in its own way with intersection theory.\nThere can be more than one primitive object, such as points (pictured above), that form an intersection. The intersection can be viewed collectively as all of the shared objects (i.e., the intersection operation results in a set, possibly empty), or as several intersection objects (possibly zero).\n\nThe intersection of two sets \"A\" and \"B\" is the set of elements which are in both \"A\" and \"B\". In symbols,\n\nFor example, if \"A\" = {1, 3, 5, 7} and \"B\" = {1, 2, 4, 6} then \"A\" ∩ \"B\" = {1}. A more elaborate example (involving infinite sets) is:\n\nAs another example, the number 9 is \"not\" contained in the intersection of the set of prime numbers {2, 3, 5, 7, 11, …} and the set of even numbers {2, 4, 6, 8, 10, …}, because 9 is neither prime nor even.\n\n\nIntersection is denoted by the from Unicode Mathematical Operators. \n"}
{"id": "1614128", "url": "https://en.wikipedia.org/wiki?curid=1614128", "title": "Life table", "text": "Life table\n\nIn actuarial science and demography, a life table (also called a mortality table or actuarial table) is a table which shows, for each age, what the probability is that a person of that age will die before his or her next birthday (\"probability of death\"). In other words, it represents the survivorship of people from a certain population. They can also be explained as a long-term mathematical way to measure a population's longevity. Tables have been created by demographers including Graunt, Reed and Merrell, Keyfitz, and Greville.\n\nThere are two types of life tables used in actuarial science. The period life table represents mortality rates during a specific time period of a certain population. A cohort life table, often referred to as a generation life table, is used to represent the overall mortality rates of a certain population's entire lifetime. They must have had to be born during the same specific time interval. A cohort life table is more frequently used because it is able to make a prediction of any expected changes in mortality rates of a population in the future. This type of table also analyzes patterns in mortality rates that can be observed over time. Both of these types of life tables are created based on an actual population from the present, as well as an educated prediction of the experience of a population in the near future. In order to find the true life expectancy average, 100 years would need to pass and by then finding that data would be of no use as healthcare is continually advancing. \n\nOther life tables in historical demography may be based on historical records, although these often undercount infants and understate infant mortality, on comparison with other regions with better records, and on mathematical adjustments for varying mortality levels and life expectancies at birth. \n\nFrom this starting point, a number of inferences can be derived.\n\n\nLife tables are also used extensively in biology and epidemiology. An area that uses this tool is Social Security. It examines the mortality rates of all the people who have Social Security to decide which actions to take.\n\nThe concept is also of importance in product life cycle management.\n\nThere are two types of life tables:\n\nStatic life tables sample individuals assuming a stationary population with overlapping generations. \"Static life tables\" and \"cohort life tables\" will be identical if population is in equilibrium and environment does not change. If a population were to have a constant number of people each year, it would mean that the probabilities of death from the life table were completely accurate. Also, an exact number of 100,000 people were born each year with no immigration or emigration involved. \n\"Life table\" primarily refers to \"period\" life tables, as cohort life tables can only be constructed using data up to the current point, and distant projections for future mortality.\n\nLife tables can be constructed using projections of future mortality rates, but more often they are a snapshot of age-specific mortality rates in the recent past, and do not necessarily purport to be projections. For these reasons, the older ages represented in a life table may have a greater chance of not being representative of what lives at these ages may experience in future, as it is predicated on current advances in medicine, public health, and safety standards that did not exist in the early years of this cohort. A life table is created by mortality rates and census figures from a certain population, ideally under a closed demographic system. This means that immigration and emigration do not exist when analyzing a cohort. A closed demographic system assumes that migration flows are random and not significant, and that immigrants from other populations have the same risk of death as an individual from the new population.\nAnother benefit from mortality tables is that they can be used to make predictions on demographics or different populations.\n\nHowever, there are also weaknesses of the information displayed on life tables. One being that they do not state the overall health of the population. There is more than one disease present in the world, and a person can have more than one disease at different stages simultaneously, introducing the term comorbidity. Therefore, life tables also do not show the direct correlation of mortality and morbidity. \nThe life table observes the mortality experience of a single generation, consisting of 100,000 births, at every age number they can live through.\n\nLife tables are usually constructed separately for men and for women because of their substantially different mortality rates. Other characteristics can also be used to distinguish different risks, such as smoking status, occupation, and socioeconomic class.\n\nLife tables can be extended to include other information in addition to mortality, for instance health information to calculate health expectancy. Health expectancies such as disability-adjusted life year and Healthy Life Years are the remaining number of years a person can expect to live in a specific health state, such as free of disability. Two types of life tables are used to divide the life expectancy into life spent in various states:\n\n\nLife tables that relate to maternal deaths and infant moralities are important, as they help form family planning programs that work with particular populations. They also help compare a country's average life expectancy with other countries. Comparing life expectancy globally helps countries understand why one country's life expectancy is rising substantially by looking at each other's healthcare, and adopting ideas to their own systems.\n\nIn order to price insurance products, and ensure the solvency of insurance companies through adequate reserves, actuaries must develop projections of future insured events (such as death, sickness, and disability). To do this, actuaries develop mathematical models of the rates and timing of the events. They do this by studying the incidence of these events in the recent past, and sometimes developing expectations of how these past events will change over time (for example, whether the progressive reductions in mortality rates in the past will continue) and deriving expected rates of such events in the future, usually based on the age or other relevant characteristics of the population. An actuary's job is to form a comparison between people at risk of death and people who actually died to come up with a probability of death for a person at each age number, defined as qx in an equation. When analyzing a population, one of the main sources used to gather the required information is insurance by obtaining individual records that belong to a specific population. These are called mortality tables if they show death rates, and morbidity tables if they show various types of sickness or disability rates.\n\nThe availability of computers and the proliferation of data gathering about individuals has made possible calculations that are more voluminous and intensive than those used in the past (i.e. they crunch more numbers) and it is more common to attempt to provide different tables for different uses, and to factor in a range of non-traditional behaviors (e.g. gambling, debt load) into specialized calculations utilized by some institutions for evaluating risk. This is particularly the case in non-life insurance (e.g. the pricing of motor insurance can allow for a large number of risk factors, which requires a correspondingly complex table of expected claim rates). However the expression \"life table\" normally refers to human survival rates and is not relevant to non-life insurance.\n\nThe basic algebra used in life tables is as follows.\n\n\n\n\n\n\n\nAnother common variable is\nThis symbol refers to central rate of mortality. It is approximately equal to the average force of mortality, averaged over the year of age.\n\nFurther descriptions:\nThe variable dx stands for the number of deaths that would occur within two consecutive age numbers. An example of this is the number of deaths in a cohort that were recorded between the age of seven and the age of eight. \nThe variable \"ℓx\", which stands for the opposite of \"dx\", represents the number of people who lived between two consecutive age numbers. \"ℓ\" of zero is equal to 100,000. The variable \"Tx\" stands for the years lived beyond each age number x by all members in the generation. \"Ėx\" represents the life expectancy for members already at a specific age number.\n\nIn practice, it is useful to have an ultimate age associated with a mortality table. Once the ultimate age is reached, the mortality rate is assumed to be 1.000. This age may be the point at which life insurance benefits are paid to a survivor or annuity payments cease.\n\nFour methods can be used to end mortality tables:\n\nIn epidemiology and public health, both standard life tables (used to calculate life expectancy), as well as the Sullivan and multi-state life tables (used to calculate health expectancy), are the most commonly mathematical used devices. The latter includes information on health in addition to mortality. By watching over the life expectancy of any year(s) being studied, epidemiologists can see if diseases are contributing to the overall increase in mortality rates. Epidemiologists are able to help demographers understand the sudden decline of life expectancy by linking it to the health problems that are arising in certain populations.\n\n\n\n"}
{"id": "454351", "url": "https://en.wikipedia.org/wiki?curid=454351", "title": "List of important publications in computer science", "text": "List of important publications in computer science\n\nThis is a list of important publications in computer science, organized by field.\n\nSome reasons why a particular publication might be regarded as important:\n\n\nDescription: This paper discusses whether machines can think and suggested the Turing test as a method for checking it.\n\n\nDescription: This summer research proposal inaugurated and defined the field. It contains the first use of the term artificial intelligence and this succinct description of the philosophical foundation of the field: \"every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it.\" (See philosophy of AI) The proposal invited researchers to the Dartmouth conference, which is widely considered the \"birth of AI\". (See history of AI.)\n\n\nDescription: The seminal paper published in 1965 provides details on the mathematics of fuzzy set theory.\n\n\nDescription: This book introduced Bayesian methods to AI.\n\n\nDescription: The standard textbook in Artificial Intelligence. The book web site lists over 1100 colleges.\n\n\nDescription: The first paper written on machine learning. Emphasized the importance of training sequences, and the use of parts of previous solutions to problems in constructing trial solutions to new problems.\n\n\nDescription: This paper created Algorithmic learning theory.\n\n\nDescription: Computational learning theory, VC theory, statistical uniform convergence and the VC dimension.\n\n\nDescription: The Probably approximately correct learning (PAC learning) framework.\n\n\nSeppo Linnainmaa's reverse mode of automatic differentiation (first applied to neural networks by Paul Werbos) is used in experiments by David Rumelhart, Geoff Hinton and Ronald J. Williams to learn internal representations.\n\n\nDescription: Decision Trees are a common learning algorithm and a decision representation tool. Development of decision trees was done by many researchers in many areas, even before this paper. Though this paper is one of the most influential in the field.\n\n\nDescription: One of the papers that started the field of on-line learning. In this learning setting, a learner receives a sequence of examples, making predictions after each one, and receiving feedback after each prediction. Research in this area is remarkable because (1) the algorithms and proofs tend to be very simple and beautiful, and (2) the model makes no statistical assumptions about the data. In other words, the data need not be random (as in nearly all other learning models), but can be chosen arbitrarily by \"nature\" or even an adversary. Specifically, this paper introduced the winnow algorithm.\n\n\nDescription: The Temporal difference method for reinforcement learning.\n\n\nDescription: The complete characterization of PAC learnability using the VC dimension.\n\n\nDescription: Proving negative results for PAC learning.\n\n\nDescription: Proving that weak and strong learnability are equivalent in the noise free PAC framework. The proof was done by introducing the boosting method.\n\n\nDescription: This paper presented support vector machines, a practical and popular machine learning algorithm. Support vector machines often use the kernel trick.\n\n\nDescription: This paper presented a tractable greedy layer-wise learning algorithm for deep belief networks which led to great advancement in the field of deep learning.\n\n\nDescription: The first application of supervised learning to gene expression data, in particular Support Vector Machines. The method is now standard, and the paper one of the most cited in the area.\n\n\nDescription: LR parser, which does bottom up parsing for deterministic context-free languages. Later derived parsers, such as the LALR parser, have been and continue to be standard practice, such as in Yacc and descendents.\n\n\nDescription: About grammar attribution, the base for yacc's s-attributed and zyacc's LR-attributed approach.\n\n\nDescription: From the abstract: \"The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly reach each node of the control flow graph of the program and all the definitions that are live on each edge of the graph.\"\n\n\nDescription: Formalized the concept of data-flow analysis as fixpoint computation over lattices, and showed that most static analyses used for program optimization can be uniformly expressed within this framework.\n\n\nDescription: Yacc is a tool that made compiler writing much easier.\n\n\nDescription: The gprof profiler\n\n\nDescription: This book became a classic in compiler writing. It is also known as the , after the (red) dragon that appears on its cover.\n\n\nDescription: The \"Colossus\" machines were early computing devices used by British codebreakers to break German messages encrypted with the Lorenz Cipher during World War II. Colossus was an early binary electronic digital computer. The design of Colossus was later described in the referenced paper.\n\n\nDescription: It contains the first published description of the logical design of a computer using the stored-program concept, which has come to be known as the von Neumann architecture.\n\n\nDescription: The IBM System/360 (S/360) is a mainframe computer system family announced by IBM on April 7, 1964. It was the first family of computers making a clear distinction between architecture and implementation.\n\n\nDescription: The \"reduced instruction set computer\"( \"RISC\") CPU design philosophy. The RISC is a CPU design philosophy that favors a reduced set of simpler instructions.\n\n\nDescription:\n\n\nDescription: The Cray-1 was a supercomputer designed by a team including Seymour Cray for Cray Research. The first Cray-1 system was installed at Los Alamos National Laboratory in 1976, and it went on to become one of the best known and most successful supercomputers in history.\n\n\nDescription: The Amdahl's Law.\n\n\nDescription: This paper discusses the concept of RAID disks, outlines the different levels of RAID, and the benefits of each level. It is a good paper for discussing issues of reliability and fault tolerance of computer systems, and the cost of providing such fault-tolerance.\n\n\nDescription: This paper argues that the approach taken to improving the performance of processors by adding multiple instruction issue and out-of-order execution cannot continue to provide speedups indefinitely. It lays out the case for making single chip processors that contain multiple \"cores\". With the mainstream introduction of multicore processors by Intel in 2005, and their subsequent domination of the market, this paper was shown to be prescient.\n\n\n\nDescription: The Academy of Motion Picture Arts and Sciences cited this paper as a \"milestone in computer graphics\".\n\n\nDescription: A correlation method based upon the inverse Fourier transform\n\n\nDescription: A method for estimating the image motion of world points between 2 frames of a video sequence.\n\n\nDescription: This paper provides efficient technique for image registration\n\n\nDescription: A technique for image encoding using local operators of many scales.\n\n\nDescription: introduced 1) MRFs for image analysis \n2) the Gibbs sampling which revolutionized computational Bayesian statistics and thus had paramount impact in many other fields in addition to Computer Vision.\n\nDescription: An interactive variational technique for image segmentation and visual tracking.\n\n\nDescription: A technique for visual tracking\n\n\nDescription: A technique (scale-invariant feature transform) for robust feature description\n\nTopics covered: concurrent computing, parallel computing, and distributed computing.\n\n\nDescription: This paper introduced the relational model for databases. This model became the number one model.\n\n\nDescription: This paper introduced the B-Trees data structure. This model became the number one model.\n\n\nDescription: Completeness of Data Base Sublanguages\n\n\nDescription: This paper introduced the entity-relationship diagram(ERD) method of database design.\n\n\nDescription: This paper introduced the SQL language.\n\n\nDescription: This paper defined the concepts of transaction, consistency and schedule. It also argued that a transaction needs to lock a logical rather than a physical subset of the database.\n\n\nDescription: Introduced federated database systems concept leading huge impact on data interoperability and integration of hetereogenous data sources.\n\n\nDescription: Association rules, a very common method for data mining.\n\nDescription: Perhaps the first book on the history of computation.\n\nedited by: \n\nDescription: Several chapters by pioneers of computing.\n\n\nDescription: Presented the vector space model.\n\n\nDescription: Presented the inverted index\n\n\nDescription: Conceived a statistical interpretation of term specificity called Inverse document frequency (IDF), which became a cornerstone of term weighting.\n\n\nDescription: This book presents a comprehensive and accessible approach to data communications and networking that has made this book a favorite with students and professionals alike. More than 830 figures and 150 tables accompany the text and provide a visual and intuitive opportunity for understanding the material.\n\n\nDescription: This paper discuss time-sharing as a method of sharing computer resource. This idea changed the interaction with computer systems.\n\n\nDescription: The beginning of cache. For more information see SIGOPS Hall of Fame.\n\n\nDescription: The classic paper on Multics, the most ambitious operating system in the early history of computing. Difficult reading, but it describes the implications of trying to build a system that takes information sharing to its logical extreme. Most operating systems since Multics have incorporated a subset of its facilities.\n\nDescription: Classic paper on the extensible nucleus architecture of the RC 4000 multiprogramming system, and what became known as the operating system kernel and microkernel architecture.\n\nDescription: The first comprehensive textbook on operating systems. Includes the first monitor notation (Chapter 7).\n\n\nDescription: This paper addresses issues in constraining the flow of information from untrusted programs. It discusses covert channels, but more importantly it addresses the difficulty in obtaining full confinement without making the program itself effectively unusable. The ideas are important when trying to understand containment of malicious code, as well as aspects of trusted computing.\n\n\nDescription: The Unix operating system and its principles were described in this paper. The main importance is not of the paper but of the operating system, which had tremendous effect on operating system and computer technology.\n\n\nDescription: This paper describes the consistency mechanism known as quorum consensus. It is a good example of algorithms that provide a continuous set of options between two alternatives (in this case, between the read-one write-all, and the write-one read-all consistency methods). There have been many variations and improvements by researchers in the years that followed, and it is one of the consistency algorithms that should be understood by all. The options available by choosing different size quorums provide a useful structure for discussing of the core requirements for consistency in distributed systems.\n\n\nDescription: This is the classic paper on synchronization techniques, including both alternate approaches and pitfalls.\n\n\nDescription: Algorithms for coscheduling of related processes were given\n\n\nDescription: The file system of UNIX. One of the first papers discussing how to manage disk storage for high-performance file systems. Most file-system research since this paper has been influenced by it, and most high-performance file systems of the last 20 years incorporate techniques from this paper.\n\n\nThis definitive description principally covered the System V Release 2 kernel, with some new features from Release 3 and BSD.\n\n\nDescription: Log-structured file system.\n\n\nDescription: This is a good paper discussing one particular microkernel architecture and contrasting it with monolithic kernel design. Mach underlies Mac OS X, and its layered architecture had a significant impact on the design of the Windows NT kernel and modern microkernels like L4. In addition, its memory-mapped files feature was added to many monolithic kernels.\n\n\nDescription: The paper was the first production-quality implementation of that idea which spawned much additional discussion of the viability and short-comings of log-structured filesystems. While \"The Design and Implementation of a Log-Structured File System\" was certainly the first, this one was important in bringing the research idea to a usable system.\n\n\nDescription: A new way of maintaining filesystem consistency.\n\n\nDescription: This paper describes the design and implementation of the first FORTRAN compiler by the IBM team. Fortran is a general-purpose, procedural, imperative programming language that is especially suited to numeric computation and scientific computing.\n\n\nDescription: This paper introduced LISP, the first functional programming language, which was used heavily in many areas of computer science, especially in AI. LISP also has powerful features for manipulating LISP programs within the language.\n\n\nDescription: Algol 60 introduced block structure.\n\n\nDescription: This seminal paper proposed an ideal language ISWIM, which without being ever implemented influenced the whole later development.\n\nDescription:\n\"Fundamental Concepts in Programming Languages\" introduced much programming language terminology still in use today, including \"R-values\", \"L-values\", \"parametric polymorphism\", and \"ad hoc polymorphism\".\n\n\nDescription: This series of papers and reports first defined the influential Scheme programming language and questioned the prevailing practices in programming language design, employing lambda calculus extensively to model programming language concepts and guide efficient implementation without sacrificing expressive power.\n\n\nDescription: This textbook explains core computer programming concepts, and is widely considered a classic text in computer science.\n\nOnline course\n\n\nDescription: This paper introduced monads to functional programming.\n\n\nDescription: This paper introduced System F and created the modern notion of Parametric polymorphism\n\n\nDescription: This paper introduce Hoare logic, which forms the foundation of program verification\n\n\n\nDescription: Conference of leading people in software field c. 1968<br>\nThe paper defined the field of Software engineering\n\n\nDescription: A description of the system that originated the (now dominant) GUI programming paradigm of Model–view–controller\n\n\nDescription: Don't use goto – the beginning of structured programming.\n\n\nDescription: The importance of modularization and information hiding. Note that information hiding was first presented in a different paper of the same author – \"Information Distributions Aspects of Design Methodology\", Proceedings of IFIP Congress '71, 1971, Booklet TA-3, pp. 26–30\n\n\nDescription: The beginning of Object-oriented programming. This paper argued that programs should be decomposed to independent components with small and simple interfaces. They also argued that objects should have both data and related methods.\n\n\nDescription: software specification.\n\n\nDescription: Seminal paper on Structured Design, data flow diagram, coupling, and cohesion.\n\n\nDescription: Illustrates the \"second-system effect\" and the importance of simplicity.\n\n\nDescription: Throwing more people at the task will not speed its completion...\n\n\nDescription: Open source methodology.\n\n\nDescription: This book was the first to define and list design patterns in computer science.\n\n\nDescription: Statecharts are a visual modeling method. They are an extension of state machine that might be exponentially more efficient. Therefore, statcharts enable formal modeling of applications that were too complex before. Statecharts are part of the UML diagrams.\n\n\n\n\n\n\nTopics covered: theoretical computer science, including computability theory, computational complexity theory, algorithms, algorithmic information theory, information theory and formal verification.\n\n\n\n"}
{"id": "30377917", "url": "https://en.wikipedia.org/wiki?curid=30377917", "title": "Mathematics and Computer Education", "text": "Mathematics and Computer Education\n\nMathematics and Computer Education is a triannual peer-reviewed academic journal in the fields of mathematics and computer science education. The editor-in-chief is George M. Miller Jr. (Nassau Community College).\n\n"}
{"id": "17651431", "url": "https://en.wikipedia.org/wiki?curid=17651431", "title": "Newcastle–Ottawa scale", "text": "Newcastle–Ottawa scale\n\nIn statistics, the Newcastle–Ottawa scale is a tool used for assessing the quality of non-randomized studies included in a systematic review and/or meta-analyses. Using the tool, each study is judged on eight items, categorized into three groups: the selection of the study groups; the comparability of the groups; and the ascertainment of either the exposure or outcome of interest for case-control or cohort studies respectively. Stars awarded for each quality item serve as a quick visual assessment. Stars are awarded such that the highest quality studies are awarded up to nine stars. The method was developed as a collaboration between the University of Newcastle, Australia, and the University of Ottawa, Canada, using a Delphi process to define variables for data extraction. The scale was then tested on systematic reviews and further refined. Separate tools were developed for cohort and case–control studies. It has also been adapted for prevalence studies.\n\n"}
{"id": "16728666", "url": "https://en.wikipedia.org/wiki?curid=16728666", "title": "Parser combinator", "text": "Parser combinator\n\nIn computer programming, a parser combinator is a higher-order function that accepts several parsers as input and returns a new parser as its output. In this context, a parser is a function accepting strings as input and returning some structure as output, typically a parse tree or a set of indices representing locations in the string where parsing stopped successfully. Parser combinators enable a recursive descent parsing strategy that facilitates modular piecewise construction and testing. This parsing technique is called combinatory parsing.\n\nParsers built using combinators are straightforward to construct, readable, modular, well-structured, and easily maintainable. They have been used extensively in the prototyping of compilers and processors for domain-specific languages such as natural-language interfaces to databases, where complex and varied semantic actions are closely integrated with syntactic processing. In 1989, Richard Frost and John Launchbury demonstrated use of parser combinators to construct natural-language interpreters. Graham Hutton also used higher-order functions for basic parsing in 1992. S.D. Swierstra also exhibited the practical aspects of parser combinators in 2001. In 2008, Frost, Hafiz and Callaghan described a set of parser combinators in Haskell that solve the long-standing problem of accommodating left recursion, and work as a complete top-down parsing tool in polynomial time and space.\n\nIn any programming language that has first-class functions, parser combinators can be used to combine basic parsers to construct parsers for more complex rules. For example, a production rule of a context-free grammar (CFG) may have one or more alternatives and each alternative may consist of a sequence of non-terminal(s) and/or terminal(s), or the alternative may consist of a single non-terminal or terminal or the empty string. If a simple parser is available for each of these alternatives, a parser combinator can be used to combine each of these parsers, returning a new parser which can recognise any or all of the alternatives.\n\nIn languages that support operator overloading, a parser combinator can take the form of an infix operator, used to glue different parsers to form a complete rule. Parser combinators thereby enable parsers to be defined in an embedded style, in code which is similar in structure to the rules of the formal grammar. As such, implementations can be thought of as executable specifications with all the associated advantages. (Notably: readability)\n\nTo keep the discussion relatively straightforward, we discuss parser combinators in terms of \"recognizers\" only. If the input string is of length codice_1 and its members are accessed through an index codice_2, a recognizer is a parser which returns, as output, a set of indices representing positions at which the parser successfully finished recognizing a sequence of tokens that began at position codice_2. An empty result set indicates that the recognizer failed to recognize any sequence beginning at index codice_2. A non-empty result set indicates the recognizer ends at different positions successfully.\n\n\n\nNote that there may be multiple distinct ways to parse a string while finishing at the same index: this indicates an ambiguous grammar. Simple recognizers do not acknowledge these ambiguities; each possible finishing index is listed only once in the result set. For a more complete set of results, a more complicated object such as a parse tree must be returned.\n\nFollowing the definitions of two basic recognizers codice_11 and codice_12, we can define two major parser combinators for alternative and sequencing:\n\n\n\nConsider a highly ambiguous context-free grammar, codice_21. Using the combinators defined earlier, we can modularly define executable notations of this grammar in a modern functional language (e.g. Haskell) as codice_22. When the recognizer codice_23 is applied on an input sequence codice_24 at position codice_25, according to the above definitions it would return a result set codice_26.\n\nParser combinators, like all recursive descent parsers, are not limited to the context-free grammars and thus do no global search for ambiguities in the LL(\"k\") parsing First and Follow sets. Thus, ambiguities are not known until run-time if and until the input triggers them. In such cases, the recursive descent parser may default (perhaps unknown to the grammar designer) to one of the possible ambiguous paths, resulting in semantic confusion (aliasing) in the use of the language. This leads to bugs by users of ambiguous programming languages, which are not reported at compile-time, and which are introduced not by human error, but by ambiguous grammar. The only solution that eliminates these bugs is to remove the ambiguities and use a context-free grammar.\n\nThe simple implementations of parser combinators have some shortcomings, which are common in top-down parsing. Naïve combinatory parsing requires exponential time and space when parsing an ambiguous context-free grammar. In 1996, Frost and Szydlowski demonstrated how memoization can be used with parser combinators to reduce the time complexity to polynomial. Later Frost used monads to construct the combinators for systematic and correct threading of memo-table throughout the computation.\n\nLike any top-down recursive descent parsing, the conventional parser combinators (like the combinators described above) will not terminate while processing a left-recursive grammar (e.g. codice_27). A recognition algorithm that accommodates ambiguous grammars with direct left-recursive rules is described by Frost and Hafiz in 2006. The algorithm curtails the otherwise ever-growing left-recursive parse by imposing depth restrictions. That algorithm was extended to a complete parsing algorithm to accommodate indirect as well as direct left-recursion in polynomial time, and to generate compact polynomial-size representations of the potentially exponential number of parse trees for highly ambiguous grammars by Frost, Hafiz and Callaghan in 2007. This extended algorithm accommodates indirect left recursion by comparing its ‘computed context’ with ‘current context’. The same authors also described their implementation of a set of parser combinators written in the Haskell programming language based on the same algorithm.\n\n\n"}
{"id": "34494899", "url": "https://en.wikipedia.org/wiki?curid=34494899", "title": "Piecewise-deterministic Markov process", "text": "Piecewise-deterministic Markov process\n\nIn probability theory, a piecewise-deterministic Markov process (PDMP) is a process whose behaviour is governed by random jumps at points in time, but whose evolution is deterministically governed by an ordinary differential equation between those times. The class of models is \"wide enough to include as special cases virtually all the non-diffusion models of applied probability.\" The process is defined by three quantities: the ﬂow, the jump rate, and the transition measure.\n\nThe model was first introduced in a paper by Mark H. A. Davis in 1984.\n\nPiecewise linear models such as Markov chains, continuous-time Markov chains, the M/G/1 queue, the GI/G/1 queue and the fluid queue can be encapsulated as PDMPs with simple differential equations.\n\nPDMPs have been shown useful in ruin theory, queueing theory, for modelling biochemical processes such as subtilin production by the organism B. subtilis and DNA replication in eukaryotes for modelling earthquakes. Moreover, this class of processes has been shown to be appropriate for biophysical neuron models with stochastic ion channels.\n\nLöpker and Palmowski have shown conditions under which a time reversed PDMP is a PDMP. General conditions are known for PDMPs to be stable.\n"}
{"id": "403143", "url": "https://en.wikipedia.org/wiki?curid=403143", "title": "Plane at infinity", "text": "Plane at infinity\n\nIn projective geometry, a plane at infinity is the hyperplane at infinity of a three dimensional projective space or to any plane contained in the hyperplane at infinity of any projective space of higher dimension. This article will be concerned solely with the three-dimensional case.\n\nThere are two approaches to defining the \"plane at infinity\" which depend on whether one starts with a projective 3-space or an affine 3-space.\n\nIf a projective 3-space is given, the \"plane at infinity\" is any distinguished projective plane of the space. This point of view emphasizes the fact that this plane is not geometrically different than any other plane. On the other hand, given an affine 3-space, the \"plane at infinity\" is a projective plane which is added to the affine 3-space in order to give it closure of incidence properties. Meaning that the points of the \"plane at infinity\" are the points where parallel lines of the affine 3-space will meet, and the lines are the lines where parallel planes of the affine 3-space will meet. The result of the addition is the projective 3-space, formula_1. This point of view emphasizes the internal structure of the plane at infinity, but does make it look \"special\" in comparison to the other planes of the space.\n\nIf the affine 3-space is real, formula_2, then the addition of a real projective plane formula_3 at infinity produces the real projective 3-space formula_4.\n\nSince any two projective planes in a projective 3-space are equivalent, we can choose a homogeneous coordinate system so that any point on the plane at infinity is represented as (\"X\":\"Y\":\"Z\":0).\nAny point in the affine 3-space will then be represented as (\"X\":\"Y\":\"Z\":1). The points on the plane at infinity seem to have three degrees of freedom, but homogeneous coordinates are equivalent up to any rescaling:\n\nso that the coordinates (\"X\":\"Y\":\"Z\":0) can be normalized, thus reducing the degrees of freedom to two (thus, a surface, namely a projective plane).\n\n\"Proposition\": Any line which passes through the origin (0:0:0:1) and through a point (\"X\":\"Y\":\"Z\":1) will intersect the plane at infinity at the point (\"X\":\"Y\":\"Z\":0).\n\n\"Proof\": A line which passes through points (0:0:0:1) and (\"X\":\"Y\":\"Z\":1) will consist of points which are linear combinations of the two given points:\nFor such a point to lie on the plane at infinity we must have, formula_7. So, by choosing formula_8, we obtain the point \nformula_9, as required. Q.E.D.\n\nAny pair of parallel lines in 3-space will intersect each other at a point on the plane at infinity. Also, every line in 3-space intersects the plane at infinity at a unique point. This point is determined by the direction—and only by the direction—of the line. To determine this point, consider a line parallel to the given line, but passing through the origin, if the line does not already pass through the origin. Then choose any point, other than the origin, on this second line. If the homogeneous coordinates of this point are (\"X\":\"Y\":\"Z\":1), then the homogeneous coordinates of the point at infinity through which the first and second line both pass is (\"X\":\"Y\":\"Z\":0).\n\n\"Example\": Consider a line passing through the points (0:0:1:1) and (3:0:1:1). A parallel line passes through points (0:0:0:1) and (3:0:0:1). This second line intersects the plane at infinity at the point (3:0:0:0). But the first line also passes through this point:\nwhen formula_13. ■\n\nAny pair of parallel planes in affine 3-space will intersect each other in a projective line (a line at infinity) in the plane at infinity. Also, every plane in the affine 3-space intersects the plane at infinity in a unique line. This line is determined by the direction—and only by the direction—of the plane.\n\nSince the plane at infinity is a projective plane, it is homeomorphic to the surface of a \"sphere modulo antipodes\", i.e. a sphere in which antipodal points are equivalent: S/{1,-1} where the quotient is understood as a quotient by a group action (see quotient space). \n\n"}
{"id": "30405742", "url": "https://en.wikipedia.org/wiki?curid=30405742", "title": "Plug &amp; Pray", "text": "Plug &amp; Pray\n\nPlug & Pray is a 2010 documentary film about the promise, problems and ethics of artificial intelligence and robotics. The main protagonists are the former MIT professor Joseph Weizenbaum and the futurist Raymond Kurzweil. The title is a pun on the computer hardware phrase \"Plug and Play\".\n\nComputer experts around the world strive towards the development of intelligent robots. Pioneers like Raymond Kurzweil and Hiroshi Ishiguro dream of fashioning intelligent machines that will equal their human creators. In this potential reality, man and machine merge as a single unity. Rejecting evolution's biological shackles tantalisingly dangles the promise of eternal life for those bold enough to seize it. But others, like Joseph Weizenbaum, counterattack against society's limitless faith in the redemptive powers of technology, questioning the prevailing discourses on new technologies and their ethical relationships to human life. The film delves into a world where computer technology, robotics, biology, neuroscience, and developmental psychology merge, and features roboticists in their laboratories in Japan, the USA, Italy and Germany.\n\nSince antiquity, mankind has dreamed of creating brilliant machines. The invention of the computer and the breathtaking pace of technological progress appear to be bringing the realisation of this dream within the grasp of humans. Robots were to do the housework, look after the children, care for the elderly, and go to war. Former MIT professor Joseph Weizenbaum, creator of ELIZA, has become a harsh critic of their visions of technological omnipotence.\n\nProduction of the film started in 2006 and ended in 2009. The death of the main protagonist Joseph Weizenbaum on March 5, 2008, fell in this period. The international festival premiere was at FIPA 2010 in Biarritz, France. Since then the film has been invited to 27 film festivals, among them the Seattle International Film Festival, Vancouver Film Festival, Visions du Réel. The theatrical release in Germany was on Nov. 11, 2010.\n\nThe film won the Bavarian Film Award 2010 for \"best documentary\", the Grand Prix of the Jury for the best film at the Paris International Science Film Festival, the Primer Premio for best film at the Mostra de Ciencia e Cinema in La Coruña (Spain), and the Science Communication Award at the International Science Film Festival Athens. It was also chosen as the best international film at the 46th AFO, Science Documentary Festival in Olomouc, Czech Republic, in 2011.\n\n"}
{"id": "11702062", "url": "https://en.wikipedia.org/wiki?curid=11702062", "title": "Problems involving arithmetic progressions", "text": "Problems involving arithmetic progressions\n\nProblems involving arithmetic progressions are of interest in number theory, combinatorics, and computer science, both from theoretical and applied points of view.\n\nFind the cardinality (denoted by \"A\"(\"m\")) of the largest subset of {1, 2, ..., \"m\"} which contains no progression of \"k\" distinct terms. The elements of the forbidden progressions are not required to be consecutive.\n\nFor example, \"A\"(10) = 8, because {1, 2, 3, 5, 6, 8, 9, 10} has no arithmetic progressions of length 4, while all 9-element subsets of {1, 2, ..., 10} have one. Paul Erdős set a $1000 prize for a question related to this number, collected by Endre Szemerédi for what has become known as Szemerédi's theorem.\n\nSzemerédi's theorem states that a set of natural numbers of non-zero upper asymptotic density contains finite arithmetic progressions, of any arbitrary length \"k\".\n\nErdős made a more general conjecture from which it would follow that\n\nThis result was proven by Ben Green and Terence Tao in 2004 and is now known as the Green–Tao theorem.\n\nSee also Dirichlet's theorem on arithmetic progressions.\n\n, the longest known arithmetic progression of primes has length 26:\n\nAs of 2011, the longest known arithmetic progression of \"consecutive\" primes has length 10. It was found in 1998. The progression starts with a 93-digit number\n\nand has the common difference 210.\n\nSource about Erdős-Turán Conjecture of 1936:\n\n\nThe prime number theorem for arithmetic progressions deals with the asymptotic distribution of prime numbers in an arithmetic progression.\n\n\n"}
{"id": "15906926", "url": "https://en.wikipedia.org/wiki?curid=15906926", "title": "Ratner's theorems", "text": "Ratner's theorems\n\nIn mathematics, Ratner's theorems are a group of major theorems in ergodic theory concerning unipotent flows on homogeneous spaces proved by Marina Ratner around 1990. The theorems grew out of Ratner's earlier work on horocycle flows. The study of the dynamics of unipotent flows played a decisive role in the proof of the Oppenheim conjecture by Grigory Margulis. Ratner's theorems have guided key advances in the understanding of the dynamics of unipotent flows. Their later generalizations provide ways to both sharpen the results and extend the theory to the setting of arbitrary semisimple algebraic groups over a local field.\n\nThe Ratner orbit closure theorem asserts that the closures of orbits of unipotent flows on the quotient of a Lie group by a lattice are nice, geometric subsets. The Ratner equidistribution theorem further asserts that each such orbit is equidistributed in its closure. The Ratner measure classification theorem is the weaker statement that every ergodic invariant probability measure is homogeneous, or \"algebraic\": this turns out to be an important step towards proving the more general equidistribution property. There is no universal agreement on the names of these theorems: they are variously known as the \"measure rigidity theorem\", the \"theorem on invariant measures\" and its \"topological version\", and so on.\n\nLet formula_1 be a Lie group, formula_2 a lattice in formula_1, and formula_4 a one-parameter subgroup of formula_1 consisting of unipotent elements, with the associated flow formula_6 on formula_7. Then the closure of every orbit formula_8 of formula_6 is homogeneous. More precisely, there exists a connected, closed subgroup formula_10 of formula_1 such that the image of the orbit formula_12 for the action of formula_10 by right translations on formula_1 under the canonical projection to formula_7 is closed, has a finite formula_10-invariant measure, and contains the closure of the formula_6-orbit of formula_18 as a dense subset.\n\n\n"}
{"id": "210091", "url": "https://en.wikipedia.org/wiki?curid=210091", "title": "Reflexive space", "text": "Reflexive space\n\nIn the area of mathematics known as functional analysis, a reflexive space is a Banach space (or more generally a locally convex topological vector space) that coincides with the continuous dual of its continuous dual space, both as linear space and as topological space. Reflexive Banach spaces are often characterized by their geometric properties.\n\nSuppose formula_1 is a normed vector space over the number field formula_2 or formula_3 (the real or complex numbers), with a norm formula_4. Consider its dual normed space formula_5, that consists of all continuous linear functionals formula_6 and is equipped with the dual norm formula_7 defined by\n\nThe dual formula_5 is a normed space (a Banach space to be precise), and its dual normed space formula_10 is called bidual space for formula_1. The bidual consists of all continuous linear functionals formula_12 and is equipped with the norm formula_13 dual to formula_7. Each vector formula_15 generates a scalar function formula_16 by the formula:\nand formula_18 is a continuous linear functional on formula_5, \"i.e.\", formula_20. One obtains in this way a map\n\ncalled evaluation map, that is linear. It follows from the Hahn–Banach theorem that formula_22 is injective and preserves norms:\n\"i.e.\", formula_22 maps formula_1 isometrically onto its image formula_26 in formula_27. Furthermore, the image formula_26 is closed in formula_27, but it need not be equal to formula_27.\n\nA normed space formula_1 is called reflexive if it satisfies the following equivalent conditions:\nA reflexive space formula_1 is a Banach space, since formula_1 is then isometric to the Banach space formula_27.\n\nA Banach space \"X\" is reflexive if it is linearly isometric to its bidual under this canonical embedding \"J\". James' space is an example of a non-reflexive space which is linearly isometric to its bidual. Furthermore, the image of James' space under the canonical embedding \"J\" has codimension one in its bidual.\n\nA Banach space \"X\" is called quasi-reflexive (of order \"d\") if the quotient has finite dimension \"d\".\n\n1) Every finite-dimensional normed space is reflexive, simply because in this case, the space, its dual and bidual all have the same linear dimension, hence the linear injection \"J\" from the definition is bijective, by the rank–nullity theorem.\n\n2) The Banach space \"c\" of scalar sequences tending to 0 at infinity, equipped with the supremum norm, is not reflexive. It follows from the general properties below that ℓ and ℓ are not reflexive, because ℓ is isomorphic to the dual of \"c\", and ℓ is isomorphic to the dual of ℓ.\n\n3) All Hilbert spaces are reflexive, as are the \"L\" spaces for . More generally: all uniformly convex Banach spaces are reflexive according to the Milman–Pettis theorem. The \"L\"(\"μ\") and \"L\"(\"μ\") spaces are not reflexive (unless they are finite dimensional, which happens for example when \"μ\" is a measure on a finite set). Likewise, the Banach space \"C\"([0, 1]) of continuous functions on [0, 1] is not reflexive.\n\n4) The spaces \"S\"(\"H\") of operators in the Schatten class on a Hilbert space \"H\" are uniformly convex, hence reflexive, when . When the dimension of \"H\" is infinite, then \"S\"(\"H\") (the trace class) is not reflexive, because it contains a subspace isomorphic to ℓ, and \"S\"(\"H\") = \"L\"(\"H\") (the bounded linear operators on \"H\") is not reflexive, because it contains a subspace isomorphic to ℓ. In both cases, the subspace can be chosen to be the operators diagonal with respect to a given orthonormal basis of \"H\".\n\nIf a Banach space \"Y\" is isomorphic to a reflexive Banach space \"X\", then \"Y\" is reflexive.\n\nEvery closed linear subspace of a reflexive space is reflexive. The continuous dual of a reflexive space is reflexive. Every quotient of a reflexive space by a closed subspace is reflexive.\n\nLet \"X\" be a Banach space. The following are equivalent.\n\nSince norm-closed convex subsets in a Banach space are weakly closed, \nit follows from the third property that closed bounded convex subsets of a reflexive space \"X\" are weakly compact. Thus, for every decreasing sequence of non-empty closed bounded convex subsets of \"X\", the intersection is non-empty. As a consequence, every continuous convex function \"f\" on a closed convex subset \"C\" of \"X\", such that the set\nis non-empty and bounded for some real number \"t\", attains its minimum value on \"C\".\n\nThe promised geometric property of reflexive Banach spaces is the following: if \"C\" is a closed non-empty convex subset of the reflexive space \"X\", then for every \"x\" in \"X\" there exists a \"c\" in \"C\" such that minimizes the distance between \"x\" and points of \"C\". This follows from the preceding result for convex functions, applied to . Note that while the minimal distance between \"x\" and \"C\" is uniquely defined by \"x\", the point \"c\" is not. The closest point \"c\" is unique when \"X\" is uniformly convex.\n\nA reflexive Banach space is separable if and only if its continuous dual is separable. This follows from the fact that for every normed space \"Y\", separability of the continuous dual implies separability .\n\nInformally, a super-reflexive Banach space \"X\" has the following property: given an arbitrary Banach space \"Y\", if all finite-dimensional subspaces of \"Y\" have a very similar copy sitting somewhere in \"X\", then \"Y\" must be reflexive. By this definition, the space \"X\" itself must be reflexive. As an elementary example, every Banach space \"Y\" whose two dimensional subspaces are isometric to subspaces of satisfies the parallelogram law, hence \n\"Y\" is a Hilbert space, therefore \"Y\" is reflexive. So ℓ is super-reflexive.\n\nThe formal definition does not use isometries, but almost isometries. A Banach space \"Y\" is finitely representable \nin a Banach space \"X\" if for every finite-dimensional subspace \"Y\" of \"Y\" and every , there is a subspace \"X\" of \"X\" such that the multiplicative Banach–Mazur distance between \"X\" and \"Y\" satisfies\n\nA Banach space finitely representable in ℓ is a Hilbert space. Every Banach space is finitely representable in \"c\". The space \"L\"([0, 1]) is finitely representable in ℓ.\n\nA Banach space \"X\" is super-reflexive if all Banach spaces \"Y\" finitely representable in \"X\" are reflexive, or, in other words, if no non-reflexive space \"Y\" is finitely representable in \"X\". The notion of ultraproduct of a family of Banach spaces \nallows for a concise definition: the Banach space \"X\" is super-reflexive when its ultrapowers are reflexive.\n\nJames proved that a space is super-reflexive if and only if its dual is super-reflexive.\n\nOne of James' characterizations of super-reflexivity uses the growth of separated trees.\nThe description of a vectorial binary tree begins with a rooted binary tree labeled by vectors: a tree of height \"n\" in a Banach space \"X\" is a family of vectors of \"X\", that can be organized in successive levels, starting with level 0 that consists of a single vector \"x\", the root of the tree, followed, for , by a family of 2 vectors forming level \"k\":\n\nthat are the children of vertices of level . In addition to the tree structure, it is required here that each vector that is an internal vertex of the tree be the midpoint between its two children:\n\nGiven a positive real number \"t\", the tree is said to be \"t\"-separated if for every internal vertex, the two children are \"t\"-separated in the given space norm:\n\nTheorem.\nThe Banach space \"X\" is super-reflexive if and only if for every , there is a number \"n\"(\"t\") such that every \"t\"-separated tree contained in the unit ball of \"X\" has height less than \"n\"(\"t\").\n\nUniformly convex spaces are super-reflexive. \nLet \"X\" be uniformly convex, with modulus of convexity δ and let \"t\" be a real number in . By the properties of the modulus of convexity, a \"t\"-separated tree of height \"n\", contained in the unit ball, must have all points of level  contained in the ball of radius . By induction, it follows that all points of level  are contained in the ball of radius\n\nIf the height \"n\" was so large that\n\nthen the two points \"x\", \"x\" of the first level could not be \"t\"-separated, contrary to the assumption. This gives the required bound \"n\"(\"t\"), function of δ(\"t\") only.\n\nUsing the tree-characterization, Enflo proved \nthat super-reflexive Banach spaces admit an equivalent uniformly convex norm. Trees in a Banach space are a special instance of vector-valued martingales. Adding techniques from scalar martingale theory, Pisier improved Enflo's result by showing \nthat a super-reflexive space \"X\" admits an equivalent uniformly convex norm for which the modulus of convexity satisfies, for some constant  and some real number ,\n\nThe notion of reflexive Banach space can be generalized to topological vector spaces in the following way.\n\nLet formula_1 be a topological vector space over a number field formula_47 (of real numbers formula_48 or complex numbers formula_49). Consider its strong dual space formula_50, which consists of all continuous linear functionals formula_6 and is equipped with the strong topology formula_52, \"i.e.\", the topology of uniform convergence on bounded subsets in formula_1. The space formula_50 is a topological vector space (to be more precise, a locally convex space), so one can consider its strong dual space formula_55, which is called the strong bidual space for formula_1. It consists of all \ncontinuous linear functionals formula_57 and is equipped with the strong topology formula_58. Each vector formula_15 generates a map formula_60 by the following formula:\nThis is a continuous linear functional on formula_50, \"i.e.\", formula_63. One obtains a map called evaluation map:\nThis map is linear. If formula_1 is locally convex, from the Hahn–Banach theorem it follows that formula_22 is injective and open (\"i.e.\", for each neighbourhood of zero formula_67 in formula_1 there is a neighbourhood of zero formula_69 in formula_55 such that formula_71). But it can be non-surjective and/or discontinuous.\n\nA locally convex space formula_1 is called \n\nTheorem. \"A locally convex Hausdorff space formula_1 is semi-reflexive if and only if formula_1 with the formula_78-topology has the Heine-Borel property (i.e. weakly closed and bounded subsets of formula_1 are weakly compact).\"\n\nTheorem. \"A locally convex space formula_1 is reflexive if and only if it is semi-reflexive and barreled.\"\n\nTheorem. \"The strong dual of a semireflexive space is barrelled.\"\n\n1) Every finite-dimensional Hausdorff topological vector space is reflexive, because \"J\" is bijective by linear algebra, and because there is a unique Hausdorff vector space topology on a finite dimensional vector space.\n\n2) A normed space formula_1 is reflexive as a normed space if and only if it is reflexive as a locally convex space. This follows from the fact that for a normed space formula_1 its dual normed space formula_5 coincides as a topological vector space with the strong dual space formula_50. As a corollary, the evaluation map formula_32 coincides with the evaluation map formula_73, and the following conditions become equivalent:\n\n3) A (somewhat artificial) example of a semi-reflexive space that is not reflexive is obtained as follows: let \"Y\" be an infinite dimensional reflexive Banach space, and let \"X\" be the topological vector space , that is, the vector space \"Y\" equipped with the weak topology. Then the continuous dual of \"X\" and are the same set of functionals, and bounded subsets of \"X\" (\"i.e.\", weakly bounded subsets of \"Y\") are norm-bounded, hence the Banach space is the strong dual of \"X\". Since \"Y\" is reflexive, the continuous dual of is equal to the image \"J\"(\"X\") of \"X\" under the canonical embedding \"J\", but the topology on \"X\" (the weak topology of \"Y\") is not the strong topology , that is equal to the norm topology of \"Y\".\n\n4) Montel spaces are reflexive locally convex topological vector spaces. In particular, the following functional spaces frequently used in functional analysis are reflexive locally convex spaces:\n\n\nAmong all locally convex spaces (even among all Banach spaces) used in functional analysis the class of reflexive spaces is too narrow to represent a self-sufficient category in any sense. On the other hand, the idea of duality reflected in this notion is so natural that it gives rise to intuitive expectations that appropriate changes in the definition of reflexivity can lead to another notion, more convenient for some goals of mathematics. One of such goals is the idea of approaching analysis to the other parts of mathematics, like algebra and geometry, by reformulating its results in the purely algebraic language of category theory.\n\nThis program is being worked out in the theory of stereotype spaces, which are defined as topological vector spaces satisfying a similar condition of reflexivity, but with the topology of uniform convergence on totally bounded subsets (instead of bounded subsets) in the definition of dual space X’. More precisely, a topological vector space formula_1 is called stereotype if the evaluation map into the stereotype second dual space\n\nis an isomorphism of topological vector spaces. Here the \"stereotype dual space\" formula_111 is defined as the space of continuous linear functionals formula_5 endowed with the topology of uniform convergence on totally bounded sets in formula_1 (and the \"stereotype second dual space\" formula_114 is the space dual to formula_115 in the same sense).\n\nIn contrast to the classical reflexive spaces the class Ste of stereotype spaces is very wide (it contains, in particular, all Fréchet spaces and thus, all Banach spaces), it forms a closed monoidal category, and it admits standard operations (defined inside of Ste) of constructing new spaces, like taking closed subspaces, quotient spaces, projective and injective limits, the space of operators, tensor products, etc. The category Ste have applications in duality theory for non-commutative groups.\n\nSimilarly, one can replace the class of bounded (and totally bounded) subsets in X in the definition of dual space X’, by other classes of subsets, for example, by the class of compact subsets in X -- the spaces defined by the corresponding reflexivity condition are called \"reflective\", and they form an even wider class than Ste, but it is not clear (2012), whether this class forms a category with properties similar to those of Ste.\n\n\n"}
{"id": "58684510", "url": "https://en.wikipedia.org/wiki?curid=58684510", "title": "Seneca effect", "text": "Seneca effect\n\nThe Seneca effect, or Seneca cliff or Seneca collapse, is a mathematical model, proposed by Ugo Bardi. This model is aimed to describe a class of problems in nature in which the decline is faster than growth, under the condition of some constraints. This model is closely related to the work \"The Limits to Growth\" issued by the Club of Rome in the Seventies and its main application is to describe various kind of economics under the condition of shortage of fossil fuels, e.g. in relation to the Hubbert curve.\n\nThe reference to the Latin philosopher and writer Seneca of the model's name lies in the verse: \"the raise is gradual, the ruin is precipitous\" (Lucius Anneus Seneca, \"Letters to Lucilius\", 91–63).\n\n\n"}
{"id": "55001159", "url": "https://en.wikipedia.org/wiki?curid=55001159", "title": "Separating lattice homomorphism", "text": "Separating lattice homomorphism\n\nLet formula_1 and formula_2 be two lattices with 0 and 1. A homomorphism from formula_1 to formula_2 is called 0,1-\"separating\" iff formula_5 (formula_6 separates 0) and formula_7 (formula_6 separates 1).\n"}
{"id": "33530042", "url": "https://en.wikipedia.org/wiki?curid=33530042", "title": "Shioda modular surface", "text": "Shioda modular surface\n\nIn mathematics, a Shioda modular surface is one of the elliptic surfaces studied by .\n\n"}
{"id": "24816442", "url": "https://en.wikipedia.org/wiki?curid=24816442", "title": "SigSpec", "text": "SigSpec\n\nSigSpec is an acronym of \"SIGnificance SPECtrum\" and addresses a statistical technique to provide the reliability of periodicities in a measured (noisy and not necessarily equidistant) time series. It relies on the amplitude spectrum obtained by the Discrete Fourier transform (DFT) and assigns a quantity called the spectral significance (frequently abbreviated by “sig”) to each amplitude. This quantity is a logarithmic measure of the probability that the given amplitude level is due to white noise, in the sense of a type I error. It represents the answer to the question, “What would be the chance to obtain an amplitude like the measured one or higher, if the analysed time series were random?”\n\nSigSpec may be considered a formal extension to the Lomb-Scargle periodogram, appropriately incorporating a time series to be averaged to zero before applying the DFT, which is done in many practical applications. When a zero-mean corrected dataset has to be statistically compared to a random sample, the sample mean (rather than the population mean only) has to be zero.\n\nConsidering a time series to be represented by a set of formula_1 pairs formula_2, the amplitude pdf of white noise in Fourier space, depending on frequency and phase angle may be described in terms of three parameters, formula_3, formula_4, formula_5, defining the “sampling profile”, according to\n\nIn terms of the phase angle in Fourier space, formula_9, with\n\nthe probability density of amplitudes is given by\n\nwhere the sock function is defined by\n\nand formula_13 denotes the variance of the dependent variable formula_14.\n\nIntegration of the pdf yields the false-alarm probability that white noise in the time domain produces an amplitude of at least formula_15,\n\nThe sig is defined as the negative logarithm of the false-alarm probability and evaluates to\n\nIt returns the number of random time series one would have to examine to obtain one amplitude exceeding formula_15 at the given frequency and phase.\n\nSigSpec is primarily used in asteroseismology to identify variable stars and to classify stellar pulsation (see references below). The fact that this method incorporates the properties of the time-domain sampling appropriately makes it a valuable tool for typical astronomical measurements containing data gaps.\n\n\n"}
{"id": "18412684", "url": "https://en.wikipedia.org/wiki?curid=18412684", "title": "Spectral layout", "text": "Spectral layout\n\nSpectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.\n\nThe idea of the layout is to compute the two largest (or smallest) eigenvalues and corresponding eigenvectors of the laplacian matrix of the graph and then use those for actually placing the nodes.\nUsually nodes are placed in the 2 dimensional plane, an embedding into more dimensions can be found by using more eigenvectors.\nFor actually placing a node from a graph which corresponds to a row/column i in the (symmetric) laplacian matrix, the x-coordinate is the value of the i-th coordinate of the first eigenvector.\nCorrespondingly the i-th component of the second eigenvector describes the y-coordinate of the point i.\n\n"}
{"id": "7271261", "url": "https://en.wikipedia.org/wiki?curid=7271261", "title": "Statistical semantics", "text": "Statistical semantics\n\nIn linguistics, statistical semantics applies the methods of statistics to the problem of determining the meaning of words or phrases, ideally through unsupervised learning, to a degree of precision at least sufficient for the purpose of information retrieval. \n\nThe term \"statistical semantics\" was first used by Warren Weaver in his well-known paper on machine translation. He argued that word sense disambiguation for machine translation should be based on the co-occurrence frequency of the context words near a given target word. The underlying assumption that \"a word is characterized by the company it keeps\" was advocated by J.R. Firth. This assumption is known in linguistics as the distributional hypothesis. Emile Delavenay defined \"statistical semantics\" as the \"statistical study of meanings of words and their frequency and order of recurrence\". \"Furnas et al. 1983\" is frequently cited as a foundational contribution to statistical semantics. An early success in the field was latent semantic analysis.\n\nResearch in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of semantics, by applying statistical techniques to large corpora:\n\nStatistical semantics focuses on the meanings of common words and the relations between common words, unlike text mining, which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical semantics is a subfield of computational semantics, which is in turn a subfield of computational linguistics and natural language processing.\n\nMany of the applications of statistical semantics (listed above) can also be addressed by lexicon-based algorithms, instead of the corpus-based algorithms of statistical semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.\n\n"}
{"id": "55733410", "url": "https://en.wikipedia.org/wiki?curid=55733410", "title": "Stereotype algebra", "text": "Stereotype algebra\n\nIn functional analysis and related areas of mathematics, stereotype algebras are topological algebras defined as stereotype spaces with a (unital and associative) multiplication satisfying the condition of separate uniform continuity on compact sets.\n\nThere are two equivalent ways to define stereotype algebra. \n\n\n\nA \"morphism of stereotype algebras\" formula_4 and formula_16 is a continuous unital homomorphism formula_17.\n\nStereotype algebras form a subclass in the class of \"separately continuous\" topological algebras.\n\nA stereotype space formula_18 is called a \"left stereotype module\" over a stereotype algebra formula_4, if a bilinear operation formula_41 is defined that turns formula_18 into a left module over formula_4, and this operation is a '%22%60UNIQ--postMath-0000014E-QINU%60%22' continuous bilinear map in the stereotype sense: for each compact set formula_11 and for each neighbourhood of zero formula_45 there is a neighbourhood of zero formula_46 such that \n\nformula_47\n\nand at the same time for each compact set formula_48 and for each neighbourhood of zero formula_45 there is a neighbourhood of zero formula_50 such that \n\nformula_51\n\nThe notion of a \"right stereotype module\" over a stereotype algebra formula_4 is defined by analogy.\n\nThe theory of stereotype algebras allows to simplify the formulations in the theory of topological algebras and to bring this field closer to abstract algebra. The following two results (which do not hold for general jointly or separately continuous algebras) are typical illustrations.\n\nTheorem. \"A stereotype space formula_18 with a structure of left (right) module over a stereotype algebra formula_4 is a stereotype module over formula_4 if and only if the operation of multiplication formula_41 (respectively, formula_57) defines a continuous map (a representation) formula_58.\"\n\nTheorem. \"For each stereotype algebra formula_4 the categories Ste and Ste of left and right stereotype modules over formula_4 are enriched categories over the monoidal category (Ste,formula_1,formula_2) of stereotype spaces.\"\n\nThe notion of stereotype algebra is used in the generalizations of the Pontryagin duality theory to the classes of non-commutative groups based on the notion of envelope: the holomorphic, the smooth and the continuous envelopes of stereotype algebras lead respectively to the constructions of the holomorphic, the smooth and the continuous dualities in \"big geometric disciplines\" – complex geometry, differential geometry, and topology – for certain classes of (not necessarily commutative) topological groups considered in these disciplines (affine algebraic groups, and some classes of Lie groups and Moore groups).\n\n\n"}
{"id": "2615145", "url": "https://en.wikipedia.org/wiki?curid=2615145", "title": "Steven Kerckhoff", "text": "Steven Kerckhoff\n\nSteven Paul Kerckhoff (born 1952) is a professor of mathematics at Stanford University, who works on hyperbolic 3-manifolds and Teichmüller spaces. \n\nHe received his Ph.D. in mathematics from Princeton University in 1978, under the direction of William Thurston. Among his most famous results is his resolution of the Nielsen realization problem, a 1932 conjecture by Jakob Nielsen. Along with William J. Floyd, he wrote large parts of Thurston's influential Princeton lecture notes, and he is well known for his work (some of which is joint with Craig Hodgson) in exploring and clarifying Thurston's hyperbolic Dehn surgery. \n\nKerckhoff is one of four academics from Stanford University, along with Gunnar Carlsson, Ralph Cohen, and James Milgram, who were instrumental in developing the controversial California Mathematics Academic Content Standards for the State Board of Education.\n\n\n"}
{"id": "20989214", "url": "https://en.wikipedia.org/wiki?curid=20989214", "title": "Susanne Albers", "text": "Susanne Albers\n\nSusanne Albers is a German theoretical computer scientist and professor of computer science at Technische Universität München. She is a recipient of the Otto Hahn Medal and the Gottfried Wilhelm Leibniz Prize.\n\nAlbers studied mathematics, computer science, and business administration in Osnabrück and received her Ph.D. (Dr. rer. nat.) in 1993 at Saarland University under the supervision of Kurt Mehlhorn. Until 1999 she was associated with the Max Planck Institute for Computer Science and held visiting and postdoctoral positions at the International Computer Science Institute in Berkeley, Free University of Berlin, and University of Paderborn. In 1999 she received her habilitation and accepted a position at Dortmund University. From 2001 to 2009 she was professor of computer science at Albert-Ludwigs-Universität Freiburg. From 2009 to 2013 she has been at Humboldt-Universität zu Berlin. Since 2013 Albers holds a chair for efficient algorithms at the Technische Universität München.\n\nAlbers’ research is in the design and analysis of algorithms, especially online algorithms, approximation algorithms, algorithmic game theory and algorithm engineering.\n\nIn 1993 she received the Otto Hahn Medal from the Max Planck Society, and in 2008 the Gottfried Wilhelm Leibniz Prize from the German Research Foundation, considered the highest German research prize and including a grant of 2.5 million euro. In 2011 she was elected as a fellow of the Gesellschaft für Informatik. In 2014 she became one of ten inaugural fellows of the European Association for Theoretical Computer Science.\n\n"}
{"id": "7689061", "url": "https://en.wikipedia.org/wiki?curid=7689061", "title": "Terminal and nonterminal symbols", "text": "Terminal and nonterminal symbols\n\nIn computer science, terminal and nonterminal symbols are the lexical elements used in specifying the production rules constituting a formal grammar. \"Terminal symbols\" are the elementary symbols of the language defined by a formal grammar. \"Nonterminal symbols\" (or \"syntactic variables\") are replaced by groups of terminal symbols according to the production rules.\n\nThe terminals and nonterminals of a particular grammar are two disjoint sets.\n\nTerminal symbols are literal symbols which may appear in the outputs of the production rules of a formal grammar and which cannot be changed using the rules of the grammar. Applying the rules recursively to a source string of symbols will usually terminate in a final output string consisting only of terminal symbols.\n\nConsider a grammar defined by two rules. Using pictoric marks interacting with each other:\n\nHere codice_4 is a terminal symbol because no rule exists which would change it into something else. On the other hand, codice_1 has two rules that can change it, thus it is nonterminal. A formal language defined or \"generated\" by a particular grammar is the set of strings that can be produced by the grammar \"and that consist only of terminal symbols\".\n\nNonterminal symbols are those symbols which can be replaced. They may also be called simply \"syntactic variables\". A formal grammar includes a \"start symbol\", a designated member of the set of nonterminals from which all the strings in the language may be derived by successive applications of the production rules. In fact, the language defined by a grammar is precisely the set of \"terminal\" strings that can be so derived.\n\nContext-free grammars are those grammars in which the left-hand side of each production rule consists of only a single nonterminal symbol. This restriction is non-trivial; not all languages can be generated by context-free grammars. Those that can are called context-free languages. \nThese are exactly the languages that can be recognized by a non-deterministic push down automaton. Context-free languages are the theoretical basis for the syntax of most programming languages.\n\nA grammar is defined by production rules (or just 'productions') that specify which symbols may replace which other symbols; these rules may be used to generate strings, or to parse them. Each such rule has a \"head\", or left-hand side, which consists of the string that may be replaced, and a \"body\", or right-hand side, which consists of a string that may replace it. Rules are often written in the form \"head\" → \"body\"; e.g., the rule \"a\" → \"b\" specifies that \"a\" can be replaced by \"b\".\n\nIn the classic formalization of generative grammars first proposed by Noam Chomsky in the 1950s, a grammar \"G\" consists of the following components:\nA grammar is formally defined as the ordered quadruple formula_14. Such a formal grammar is often called a rewriting system or a phrase structure grammar in the literature.\n\nFor instance, the following represents an integer (which may be signed) expressed in a variant of Backus–Naur form:\n<digit> ::= '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'\nIn this example, the symbols (-,0,1,2,3,4,5,6,7,8,9) are terminal symbols and <digit> and <integer> are nonterminal symbols. \n\n\"Note: This example supports strings with leading zeroes like \"0056\" or \"0000\", as well as negative zero strings like \"-0\" and \"-00000\".\"\n\nAnother example is:\n\nS -> cAd\n\nA -> a | ab\n\nIn this example, the symbols a,b,c,d are terminal symbols and S,A are nonterminal symbols.\n\n"}
{"id": "30773", "url": "https://en.wikipedia.org/wiki?curid=30773", "title": "Theoretical ecology", "text": "Theoretical ecology\n\nTheoretical ecology is the scientific discipline devoted to the study of ecological systems using theoretical methods such as simple conceptual models, mathematical models, computational simulations, and advanced data analysis. Effective models improve understanding of the natural world by revealing how the dynamics of species populations are often based on fundamental biological conditions and processes. Further, the field aims to unify a diverse range of empirical observations by assuming that common, mechanistic processes generate observable phenomena across species and ecological environments. Based on biologically realistic assumptions, theoretical ecologists are able to uncover novel, non-intuitive insights about natural processes. Theoretical results are often verified by empirical and observational studies, revealing the power of theoretical methods in both predicting and understanding the noisy, diverse biological world.\n\nThe field is broad and includes foundations in applied mathematics, computer science, biology, statistical physics, genetics, chemistry, evolution, and conservation biology. Theoretical ecology aims to explain a diverse range of phenomena in the life sciences, such as population growth and dynamics, fisheries, competition, evolutionary theory, epidemiology, animal behavior and group dynamics, food webs, ecosystems, spatial ecology, and the effects of climate change.\n\nTheoretical ecology has further benefited from the advent of fast computing power, allowing the analysis and visualization of large-scale computational simulations of ecological phenomena. Importantly, these modern tools provide quantitative predictions about the effects of human induced environmental change on a diverse variety of ecological phenomena, such as: species invasions, climate change, the effect of fishing and hunting on food network stability, and the global carbon cycle.\n\nAs in most other sciences, mathematical models form the foundation of modern ecological theory.\n\nEcological models can be deterministic or stochastic.\n\n\nSpecies can be modelled in continuous or discrete time.\n\n\nModels are often used to describe real ecological reproduction processes of single or multiple species.\nThese can be modelled using stochastic branching processes. Examples are the dynamics of interacting populations (predation competition and mutualism), which, depending on the species of interest, may best be modeled over either continuous or discrete time. Other examples of such models may be found in the field of mathematical epidemiology where the dynamic relationships that are to be modeled are host–pathogen interactions.\n\nBifurcation theory is used to illustrate how small changes in parameter values can give rise to dramatically different long run outcomes, a mathematical fact that may be used to explain drastic ecological differences that come about in qualitatively very similar systems. Logistic maps are polynomial mappings, and are often cited as providing archetypal examples of how chaotic behaviour can arise from very simple non-linear dynamical equations. The maps were popularized in a seminal 1976 paper by the theoretical ecologist Robert May. The difference equation is intended to capture the two effects of reproduction and starvation.\n\nIn 1930, R.A. Fisher published his classic \"The Genetical Theory of Natural Selection\", which introduced the idea that frequency-dependent fitness brings a strategic aspect to evolution, where the payoffs to a particular organism, arising from the interplay of all of the relevant organisms, are the number of this organism' s viable offspring. In 1961, Richard Lewontin applied game theory to evolutionary biology in his \"Evolution and the Theory of Games\",\nfollowed closely by John Maynard Smith, who in his seminal 1972 paper, “Game Theory and the Evolution of Fighting\", defined the concept of the evolutionarily stable strategy.\n\nBecause ecological systems are typically nonlinear, they often cannot be solved analytically and in order to obtain sensible results, nonlinear, stochastic and computational techniques must be used. One class of computational models that is becoming increasingly popular are the agent-based models. These models can simulate the actions and interactions of multiple, heterogeneous, organisms where more traditional, analytical techniques are inadequate. Applied theoretical ecology yields results which are used in the real world. For example, optimal harvesting theory draws on optimization techniques developed in economics, computer science and operations research, and is widely used in fisheries.\n\nPopulation ecology is a sub-field of ecology that deals with the dynamics of species populations and how these populations interact with the environment. It is the study of how the population sizes of species living together in groups change over time and space, and was one of the first aspects of ecology to be studied and modelled mathematically.\n\nThe most basic way of modeling population dynamics is to assume that the rate of growth of a population depends only upon the population size at that time and the per capita growth rate of the organism. In other words, if the number of individuals in a population at a time t, is N(t), then the rate of population growth is given by:\nwhere r is the per capita growth rate, or the intrinsic growth rate of the organism. It can also be described as r = b-d, where b and d are the per capita time-invariant birth and death rates, respectively. This first order linear differential equation can be solved to yield the solution\na trajectory known as Malthusian growth, after Thomas Malthus, who first described its dynamics in 1798. A population experiencing Malthusian growth follows an exponential curve, where N(0) is the initial population size. The population grows when r > 0, and declines when r < 0. The model is most applicable in cases where a few organisms have begun a colony and are rapidly growing without any limitations or restrictions impeding their growth (e.g. bacteria inoculated in rich media).\n\nThe exponential growth model makes a number of assumptions, many of which often do not hold. For example, many factors affect the intrinsic growth rate and is often not time-invariant. A simple modification of the exponential growth is to assume that the intrinsic growth rate varies with population size. This is reasonable: the larger the population size, the fewer resources available, which can result in a lower birth rate and higher death rate. Hence, we can replace the time-invariant r with r’(t) = (b –a*N(t)) – (d + c*N(t)), where a and c are constants that modulate birth and death rates in a population dependent manner (e.g. intraspecific competition). Both a and c will depend on other environmental factors which, we can for now, assume to be constant in this approximated model. The differential equation is now:\nThis can be rewritten as:\nwhere r = b-d and K = (b-d)/(a+c).\n\nThe biological significance of K becomes apparent when stabilities of the equilibria of the system are considered. The constant K is the carrying capacity of the population. The equilibria of the system are N = 0 and N = K. If the system is linearized, it can be seen that N = 0 is an unstable equilibrium while K is a stable equilibrium.\n\nAnother assumption of the exponential growth model is that all individuals within a population are identical and have the same probabilities of surviving and of reproducing. This is not a valid assumption for species with complex life histories. The exponential growth model can be modified to account for this, by tracking the number of individuals in different age classes (e.g. one-, two-, and three-year-olds) or different stage classes (juveniles, sub-adults, and adults) separately, and allowing individuals in each group to have their own survival and reproduction rates.\nThe general form of this model is\nwhere N is a vector of the number of individuals in each class at time \"t\" and L is a matrix that contains the survival probability and fecundity for each class. The matrix L is referred to as the Leslie matrix for age-structured models, and as the Lefkovitch matrix for stage-structured models.\n\nIf parameter values in L are estimated from demographic data on a specific population, a structured model can then be used to predict whether this population is expected to grow or decline in the long-term, and what the expected age distribution within the population will be. This has been done for a number of species including loggerhead sea turtles and right whales.\n\nAn ecological community is a group of trophically similar, sympatric species that actually or potentially compete in a local area for the same or similar resources. Interactions between these species form the first steps in analyzing more complex dynamics of ecosystems. These interactions shape the distribution and dynamics of species. Of these interactions, predation is one of the most widespread population activities.\nTaken in its most general sense, predation comprises predator–prey, host–pathogen, and host–parasitoid interactions.\n\nPredator–prey interactions exhibit natural oscillations in the populations of both predator and the prey. In 1925, the American mathematician Alfred J. Lotka developed simple equations for predator–prey interactions in his book on biomathematics. The following year, the Italian mathematician Vito Volterra, made a statistical analysis of fish catches in the Adriatic and independently developed the same equations. It is one of the earliest and most recognised ecological models, known as the Lotka-Volterra model:\n\nwhere N is the prey and P is the predator population sizes, r is the rate for prey growth, taken to be exponential in the absence of any predators, α is the prey mortality rate for per-capita predation (also called ‘attack rate’), c is the efficiency of conversion from prey to predator, and d is the exponential death rate for predators in the absence of any prey.\n\nVolterra originally used the model to explain fluctuations in fish and shark populations after fishing was curtailed during the First World War. However, the equations have subsequently been applied more generally. Other examples of these models include the Lotka-Volterra model of the snowshoe hare and Canadian lynx in North America, any infectious disease modeling such as the recent outbreak of SARS\n\nand biological control of California red scale by the introduction of its parasitoid, \"Aphytis melinus\"\n\nA credible, simple alternative to the Lotka-Volterra predator–prey model and their common prey dependent generalizations is the ratio dependent or Arditi-Ginzburg model. The two are the extremes of the spectrum of predator interference models. According to the authors of the alternative view, the data show that true interactions in nature are so far from the Lotka–Volterra extreme on the interference spectrum that the model can simply be discounted as wrong. They are much closer to the ratio-dependent extreme, so if a simple model is needed one can use the Arditi–Ginzburg model as the first approximation.\n\nThe second interaction, that of host and pathogen, differs from predator–prey interactions in that pathogens are much smaller, have much faster generation times, and require a host to reproduce. Therefore, only the host population is tracked in host–pathogen models. Compartmental models that categorize host population into groups such as susceptible, infected, and recovered (SIR) are commonly used.\n\nThe third interaction, that of host and parasitoid, can be analyzed by the Nicholson-Bailey model, which differs from Lotka-Volterra and SIR models in that it is discrete in time. This model, like that of Lotka-Volterra, tracks both populations explicitly. Typically, in its general form, it states:\nwhere f(N, P) describes the probability of infection (typically, Poisson distribution), λ is the per-capita growth rate of hosts in the absence of parasitoids, and c is the conversion efficiency, as in the Lotka-Volterra model.\n\nIn studies of the populations of two species, the Lotka-Volterra system of equations has been extensively used to describe dynamics of behavior between two species, N and N. Examples include relations between \"D. discoiderum\" and \"E. coli\",\nas well as theoretical analysis of the behavior of the system.\nThe r coefficients give a “base” growth rate to each species, while K coefficients correspond to the carrying capacity. What can really change the dynamics of a system, however are the α terms. These describe the nature of the relationship between the two species. When α is negative, it means that N has a negative effect on N, by competing with it, preying on it, or any number of other possibilities. When α is positive, however, it means that N has a positive effect on N, through some kind of mutualistic interaction between the two.\nWhen both α and α are negative, the relationship is described as competitive. In this case, each species detracts from the other, potentially over competition for scarce resources.\nWhen both α and α are positive, the relationship becomes one of mutualism. In this case, each species provides a benefit to the other, such that the presence of one aids the population growth of the other.\n\nUnified neutral theory is a hypothesis proposed by Stephen Hubbell in 2001. The hypothesis aims to explain the diversity and relative abundance of species in ecological communities, although like other neutral theories in ecology, Hubbell's hypothesis assumes that the differences between members of an ecological community of trophically similar species are \"neutral,\" or irrelevant to their success. Neutrality means that at a given trophic level in a food web, species are equivalent in birth rates, death rates, dispersal rates and speciation rates, when measured on a per-capita basis. This implies that biodiversity arises at random, as each species follows a random walk. This can be considered a null hypothesis to niche theory. The hypothesis has sparked controversy, and some authors consider it a more complex version of other null models that fit the data better.\n\nUnder unified neutral theory, complex ecological interactions are permitted among individuals of an ecological community (such as competition and cooperation), providing all individuals obey the same rules. Asymmetric phenomena such as parasitism and predation are ruled out by the terms of reference; but cooperative strategies such as swarming, and negative interaction such as competing for limited food or light are allowed, so long as all individuals behave the same way. The theory makes predictions that have implications for the management of biodiversity, especially the management of rare species. It predicts the existence of a fundamental biodiversity constant, conventionally written \"θ\", that appears to govern species richness on a wide variety of spatial and temporal scales.\n\nHubbell built on earlier neutral concepts, including MacArthur & Wilson's theory of island biogeography and Gould's concepts of symmetry and null models.\n\nBiogeography is the study of the distribution of species in space and time. It aims to reveal where organisms live, at what abundance, and why they are (or are not) found in a certain geographical area.\n\nBiogeography is most keenly observed on islands, which has led to the development of the subdiscipline of island biogeography. These habitats are often a more manageable areas of study because they are more condensed than larger ecosystems on the mainland. In 1967, Robert MacArthur and E.O. Wilson published \"The Theory of Island Biogeography\". This showed that the species richness in an area could be predicted in terms of factors such as habitat area, immigration rate and extinction rate. The theory is considered one of the fundamentals of ecological theory. The application of island biogeography theory to habitat fragments spurred the development of the fields of conservation biology and landscape ecology.\n\nA population ecology concept is r/K selection theory, one of the first predictive models in ecology used to explain life-history evolution. The premise behind the r/K selection model is that natural selection pressures change according to population density. For example, when an island is first colonized, density of individuals is low. The initial increase in population size is not limited by competition, leaving an abundance of available resources for rapid population growth. These early phases of population growth experience \"density-independent\" forces of natural selection, which is called \"r\"-selection. As the population becomes more crowded, it approaches the island's carrying capacity, thus forcing individuals to compete more heavily for fewer available resources. Under crowded conditions, the population experiences density-dependent forces of natural selection, called \"K\"-selection.\n\nSpatial analysis of ecological systems often reveals that assumptions that are valid for spatially homogenous populations – and indeed, intuitive – may no longer be valid when migratory subpopulations moving from one patch to another are considered. In a simple one-species formulation, a subpopulation may occupy a patch, move from one patch to another empty patch, or die out leaving an empty patch behind. In such a case, the proportion of occupied patches may be represented as\nwhere m is the rate of colonization, and e is the rate of extinction. In this model, if e < m, the steady state value of p is 1 – (e/m) while in the other case, all the patches will eventually be left empty. This model may be made more complex by addition of another species in several different ways, including but not limited to game theoretic approaches, predator–prey interactions, etc. We will consider here an extension of the previous one-species system for simplicity. Let us denote the proportion of patches occupied by the first population as p, and that by the second as p. Then,\nIn this case, if e is too high, p and p will be zero at steady state. However, when the rate of extinction is moderate, p and p can stably coexist. The steady state value of p is given by\n(p* may be inferred by symmetry).\nIf e is zero, the dynamics of the system favor the species that is better at colonizing (i.e. has the higher m value). This leads to a very important result in theoretical ecology known as the Intermediate Disturbance Hypothesis, where the biodiversity (the number of species that coexist in the population) is maximized when the disturbance (of which e is a proxy here) is not too high or too low, but at intermediate levels.\n\nThe form of the differential equations used in this simplistic modelling approach can be modified. For example:\n\nThe model can also be extended to combinations of the four possible linear or non-linear dependencies of colonization and extinction on p are described in more detail in.\n\nIntroducing new elements, whether biotic or abiotic, into ecosystems can be disruptive. In some cases, it leads to ecological collapse, trophic cascades and the death of many species within the ecosystem. The abstract notion of ecological health attempts to measure the robustness and recovery capacity for an ecosystem; i.e. how far the ecosystem is away from its steady state. Often, however, ecosystems rebound from a disruptive agent. The difference between collapse or rebound depends on the toxicity of the introduced element and the resiliency of the original ecosystem.\n\nIf ecosystems are governed primarily by stochastic processes, through which its subsequent state would be determined by both predictable and random actions, they may be more resilient to sudden change than each species individually. In the absence of a balance of nature, the species composition of ecosystems would undergo shifts that would depend on the nature of the change, but entire ecological collapse would probably be infrequent events. In 1997, Robert Ulanowicz used information theory tools to describe the structure of ecosystems, emphasizing mutual information (correlations) in studied systems. Drawing on this methodology and prior observations of complex ecosystems, Ulanowicz depicts approaches to determining the stress levels on ecosystems and predicting system reactions to defined types of alteration in their settings (such as increased or reduced energy flow, and eutrophication.\n\nEcopath is a free ecosystem modelling software suite, initially developed by NOAA, and widely used in fisheries management as a tool for modelling and visualising the complex relationships that exist in real world marine ecosystems.\n\nFood webs provide a framework within which a complex network of predator–prey interactions can be organised. A food web model is a network of food chains. Each food chain starts with a primary producer or autotroph, an organism, such as a plant, which is able to manufacture its own food. Next in the chain is an organism that feeds on the primary producer, and the chain continues in this way as a string of successive predators. The organisms in each chain are grouped into trophic levels, based on how many links they are removed from the primary producers. The length of the chain, or trophic level, is a measure of the number of species encountered as energy or nutrients move from plants to top predators. Food energy flows from one organism to the next and to the next and so on, with some energy being lost at each level. At a given trophic level there may be one species or a group of species with the same predators and prey.\n\nIn 1927, Charles Elton published an influential synthesis on the use of food webs, which resulted in them becoming a central concept in ecology. In 1966, interest in food webs increased after Robert Paine's experimental and descriptive study of intertidal shores, suggesting that food web complexity was key to maintaining species diversity and ecological stability. Many theoretical ecologists, including Sir Robert May and Stuart Pimm, were prompted by this discovery and others to examine the mathematical properties of food webs. According to their analyses, complex food webs should be less stable than simple food webs. The apparent paradox between the complexity of food webs observed in nature and the mathematical fragility of food web models is currently an area of intensive study and debate. The paradox may be due partially to conceptual differences between persistence of a food web and equilibrial stability of a food web.\n\nSystems ecology can be seen as an application of general systems theory to ecology. It takes a holistic and interdisciplinary approach to the study of ecological systems, and particularly ecosystems. Systems ecology is especially concerned with the way the functioning of ecosystems can be influenced by human interventions. Like other fields in theoretical ecology, it uses and extends concepts from thermodynamics and develops other macroscopic descriptions of complex systems. It also takes account of the energy flows through the different trophic levels in the ecological networks. In systems ecology the principles of ecosystem energy flows are considered formally analogous to the principles of energetics. Systems ecology also considers the external influence of ecological economics, which usually is not otherwise considered in ecosystem ecology. For the most part, systems ecology is a subfield of ecosystem ecology.\n\nThis is the study of how \"the environment, both physical and biological, interacts with the physiology of an organism. It includes the effects of climate and nutrients on physiological processes in both plants and animals, and has a particular focus on how physiological processes scale with organism size\".\n\nSwarm behaviour is a collective behaviour exhibited by animals of similar size which aggregate together, perhaps milling about the same spot or perhaps migrating in some direction. Swarm behaviour is commonly exhibited by insects, but it also occurs in the flocking of birds, the schooling of fish and the herd behaviour of quadrupeds. It is a complex emergent behaviour that occurs when individual agents follow simple behavioral rules.\n\nRecently, a number of mathematical models have been discovered which explain many aspects of the emergent behaviour. Swarm algorithms follow a Lagrangian approach or an Eulerian approach. The Eulerian approach views the swarm as a field, working with the density of the swarm and deriving mean field properties. It is a hydrodynamic approach, and can be useful for modelling the overall dynamics of large swarms. However, most models work with the Lagrangian approach, which is an agent-based model following the individual agents (points or particles) that make up the swarm. Individual particle models can follow information on heading and spacing that is lost in the Eulerian approach. Examples include ant colony optimization, self-propelled particles and particle swarm optimization\n\nThe British biologist Alfred Russel Wallace is best known for independently proposing a theory of evolution due to natural selection that prompted Charles Darwin to publish his own theory. In his famous 1858 paper, Wallace proposed natural selection as a kind of feedback mechanism which keeps species and varieties adapted to their environment.\n\n\"The action of this principle is exactly like that of the centrifugal governor of the steam engine, which checks and corrects any irregularities almost before they become evident; and in like manner no unbalanced deficiency in the animal kingdom can ever reach any conspicuous magnitude, because it would make itself felt at the very first step, by rendering existence difficult and extinction almost sure soon to follow.\"\n\nThe cybernetician and anthropologist Gregory Bateson observed in the 1970s that, though writing it only as an example, Wallace had \"probably said the most powerful thing that’d been said in the 19th Century\". Subsequently, the connection between natural selection and systems theory has become an area of active research.\n\nIn contrast to previous ecological theories which considered floods to be catastrophic events, the river flood pulse concept argues that the annual flood pulse is the most important aspect and the most biologically productive feature of a river's ecosystem.\n\nTheoretical ecology draws on pioneering work done by G. Evelyn Hutchinson and his students. Brothers H.T. Odum and E.P. Odum are generally recognised as the founders of modern theoretical ecology. Robert MacArthur brought theory to community ecology. Daniel Simberloff was the student of E.O. Wilson, with whom MacArthur collaborated on \"The Theory of Island Biogeography\", a seminal work in the development of theoretical ecology.\n\nSimberloff added statistical rigour to experimental ecology and was a key figure in the SLOSS debate, about whether it is preferable to protect a single large or several small reserves. This resulted in the supporters of Jared Diamond's community assembly rules defending their ideas through Neutral Model Analysis. Simberloff also played a key role in the (still ongoing) debate on the utility of corridors for connecting isolated reserves.\n\nStephen Hubbell and Michael Rosenzweig combined theoretical and practical elements into works that extended MacArthur and Wilson's Island Biogeography Theory - Hubbell with his Unified Neutral Theory of Biodiversity and Biogeography and Rosenzweig with his Species Diversity in Space and Time.\n\nA tentative distinction can be made between mathematical ecologists, ecologists who apply mathematics to ecological problems, and mathematicians who develop the mathematics itself that arises out of ecological problems.\n\nSome notable theoretical ecologists can be found in these categories:\n\n\n"}
{"id": "1633057", "url": "https://en.wikipedia.org/wiki?curid=1633057", "title": "Toroidal graph", "text": "Toroidal graph\n\nIn mathematics, a toroidal graph is a graph that can be embedded on a torus. In other words, the graph's vertices can be placed on a torus such that no edges cross.\n\nAny graph which can be embedded in a plane can also be embedded in a torus. A toroidal graph of genus 1 can be embedded in a torus but not in a plane. The Heawood graph, the complete graph K (and hence K and K), the Petersen graph (and hence the complete bipartite graph K, since the Petersen graph contains a subdivision of it), one of the Blanuša snarks, and all Möbius ladders are toroidal. More generally, any graph with crossing number 1 is toroidal. Some graphs with greater crossing numbers are also toroidal: the Möbius–Kantor graph, for example, has crossing number 4 and is toroidal.\n\nAny toroidal graph has chromatic number at most 7. The complete graph K provides an example of toroidal graph with chromatic number 7.\n\nAny triangle-free toroidal graph has chromatic number at most 4.\n\nBy a result analogous to Fáry's theorem, any toroidal graph may be drawn with straight edges in a rectangle with periodic boundary conditions. Furthermore, the analogue of Tutte's spring theorem applies in this case.\nToroidal graphs also have book embeddings with at most 7 pages.\n\nBy the Robertson–Seymour theorem, there exists a finite set \"H\" of minimal non-toroidal graphs, such that a graph is toroidal if and only if it has no graph minor in \"H\".\nThat is, \"H\" forms the set of forbidden minors for the toroidal graphs.\nThe complete set \"H\" is not known, but it has at least 17,523 graphs. Alternatively, there are at least 250,815 non-toroidal graphs that are minimal in the topological minor ordering.\nA graph is toroidal if and only if it has none of these graphs as a topological minor.\n\n\n"}
{"id": "683570", "url": "https://en.wikipedia.org/wiki?curid=683570", "title": "Total variation distance of probability measures", "text": "Total variation distance of probability measures\n\nIn probability theory, the total variation distance is a distance measure for probability distributions. It is an example of a statistical distance metric, and is sometimes called the statistical distance or variational distance.\n\nThe total variation distance between two probability measures \"P\" and \"Q\" on a sigma-algebra \"formula_1\" of subsets of the sample space formula_2 is defined via\n\nInformally, this is the largest possible difference between the probabilities that the two probability distributions can assign to the same event.\n\nThe total variation distance is related to the Kullback–Leibler divergence by Pinsker's inequality:\n\nWhen the set is countable, the total variation distance is related to the L norm by the identity:\n\nThe total variation distance (or half the norm) arises as the optimal transportation cost, when the cost function is formula_6, that is,\n\nwhere the infimum is taken over all formula_8 probability distributions with marginals formula_9 and formula_10, respectively.\n\n"}
{"id": "34666424", "url": "https://en.wikipedia.org/wiki?curid=34666424", "title": "Trombi–Varadarajan theorem", "text": "Trombi–Varadarajan theorem\n\nIn mathematics, the Trombi–Varadarajan theorem, introduced by , gives an isomorphism between a certain space of spherical functions on a semisimple Lie group, and a certain space of holomorphic functions defined on a tubular neighborhood of the dual of a Cartan subalgebra.\n"}
{"id": "31262846", "url": "https://en.wikipedia.org/wiki?curid=31262846", "title": "WORHP", "text": "WORHP\n\nWORHP ( \"warp\"), also referred to as eNLP (European NLP solver) by ESA, is a mathematical software library for solving continuous large scale nonlinear optimization problems numerically. The acronym WORHP is sometimes spelled out as \"We Optimize Really Huge Problems\", its primary intended application. WORHP is a hybrid Fortran and C implementation and can be used from C/C++ and Fortran programs using different interfaces of varying complexity and flexibility. In addition interfaces for the modelling environments MATLAB, CasADi and AMPL exist.\n\nWORHP is designed to solve problems of the form\nwith sufficiently smooth functions formula_3 (objective) and formula_4 (constraints) that may be nonlinear, and need not necessarily be convex. Even problems with large dimensions formula_5 and formula_6 can be solved efficiently, if the problem is sufficiently sparse.\nCases where objective and constraints cannot be evaluated separately, or where constraints can be evaluated element-wise can be exploited by WORHP to increase the computational efficiency.\n\nWORHP requires the first derivative (Gradient) of formula_7 and of formula_8 (Jacobian) and second derivatives (Hessian matrix) of the Lagrange function; in a modelling environment like AMPL, these are provided by automatic differentiation methods, but need to be provided by the caller in other environments. First and second derivatives can be approximated by WORHP using finite differences. To reduce the otherwise prohibitively high number of necessary function evaluations in large scale sparse problems, graph colouring theory is used to group first and second partial derivatives. Second derivatives may also be approximated using variations of the classic BFGS method, including block-diagonal or sparse BFGS matrices.\n\nThe NLP level of WORHP is based on SQP, while the quadratic subproblems are solved using an interior point method. This approach was chosen to benefit from the robustness of SQP methods and the reliable runtime complexity of IP methods, since traditional active set methods may be unsuitable for large-scale problems.\n\nDevelopment of WORHP started in 2006 with funding from DLR and was continued under the \"eNLP\" label after 2008 with support by ESA / ESTEC together with the Interior-Point solver ipfilter\n(whose inclusion in eNLP was discontinued after 2010) to develop a European NLP solver for use in trajectory optimisation, mission analysis and aerospace applications in general.\n\nThe development of WORHP is led by the Steinbeis-Forschungszentrum Optimierung, Steuerung und Regelung and scientists of the Optimization and Optimal Control Group at the University of Bremen, and at the Bundeswehr University of Munich.\nThe developers stress that WORHP, despite its academic roots, is intended as industrial-grade tool rather than an academic research platform.\n\nWORHP has been integrated into trajectory analysis tools such as LOTNAV\nand ASTOS, and is being used at ESOC and ESTEC. It can be used as optimiser in CasADi (since version 1.5.0beta)\nand as local optimiser in SVAGO MDO tool developed at University of Bremen and Politecnico di Milano on Multidisciplinary design optimization through the ESA PRESTIGE program.\n\n"}
{"id": "1974015", "url": "https://en.wikipedia.org/wiki?curid=1974015", "title": "Wilson quotient", "text": "Wilson quotient\n\nThe Wilson quotient \"W\"(\"p\") is defined as:\n\nIf \"p\" is a prime number, the quotient is an integer by Wilson's theorem; moreover, if \"p\" is composite, the quotient is not an integer. If \"p\" divides \"W\"(\"p\"), it is called a Wilson prime. The integer values of \"W\"(\"p\") are :\n\nIt is known that\n\nwhere formula_4 is the \"k\"-th Bernoulli number. Note that the first relation comes from the second one by subtraction, after substituting formula_5 and formula_6.\n\n\n"}
{"id": "25640189", "url": "https://en.wikipedia.org/wiki?curid=25640189", "title": "Workshop on Logic, Language, Information and Computation", "text": "Workshop on Logic, Language, Information and Computation\n\nWoLLIC, the Workshop on Logic, Language, Information and Computation is an academic conference in the field of pure and applied logic and theoretical computer science. WoLLIC has been organised annually since 1994, typically in June or July; the conference is scientifically sponsored by the Association for Logic, Language and Information, the Association for Symbolic Logic, the European Association for Theoretical Computer Science and the European Association for Computer Science Logic.\n\nAccording to Computer Science Conference Ranking 2010, the conference is ranked \"B\" among over 1900 international conferences across the world. It is also ranked \"B\" on The CORE Conference Ranking Exercise - CORE Portal (2017). It is currently ranked 9th (Last 5 years), Field-Rating 1, Algorithms & Theory, at Microsoft Academic Search - Conferences. On Google Scholar, the conference gets a score of 11 as its h5-index, and a score of 19 as its h5-median.\n\n\nThe meetings alternate between Latin America and US/Europe/Asia. The following locations are planned for future meetings:\n\n\n\n\n"}
{"id": "264210", "url": "https://en.wikipedia.org/wiki?curid=264210", "title": "Zero of a function", "text": "Zero of a function\n\nIn mathematics, a zero, also sometimes called a root, of a real-, complex- or generally vector-valued function formula_1 is a member formula_2 of the domain of formula_1 such that formula_4 \"vanishes\" at formula_2; that is, formula_2 is a solution of the equation formula_7.\nIn other words, a \"zero\" of a function is an input value that produces an output of formula_8.\n\nA root of a polynomial is a zero of the corresponding polynomial function.\nThe fundamental theorem of algebra shows that any non-zero polynomial has a number of roots at most equal to its degree and that the number of roots and the degree are equal when one considers the complex roots (or more generally the roots in an algebraically closed extension) counted with their multiplicities. For example, the polynomial formula_1 of degree two, defined by \nhas the two roots formula_11 and formula_12, since\n\nIf the function maps real numbers to real numbers, its zeros are the formula_2-coordinates of the points where its graph meets the \"x\"-axis. An alternative name for such a point formula_15 in this context is an formula_2-intercept.\n\nEvery equation in the unknown may be rewritten as\nby regrouping all terms in the left-hand side. It follows that the solutions of such an equation are exactly the zeros of the function formula_1. In other words, \"zero of a function\" is a phrase denoting a \"solution of the equation obtained by equating the function to 0,\" and the study of zeros of functions is exactly the same as the study of solutions of equations.\n\nEvery real polynomial of odd degree has an odd number of real roots (counting multiplicities); likewise, a real polynomial of even degree must have an even number of real roots. Consequently, real odd polynomials must have at least one real root (because one is the smallest odd whole number), whereas even polynomials may have none. This principle can be proven by reference to the intermediate value theorem: since polynomial functions are continuous, the function value must cross zero in the process of changing from negative to positive or vice versa.\n\nThe fundamental theorem of algebra states that every polynomial of degree formula_19 has formula_19 complex roots, counted with their multiplicities. The non-real roots of polynomials with real coefficients come in conjugate pairs. Vieta's formulas relate the coefficients of a polynomial to sums and products of its roots.\n\nComputing roots of functions, for example polynomial functions, frequently requires the use of specialised or approximation techniques (for example, Newton's method). However, some polynomial functions, including all those of degree no greater than 4, can have all their roots expressed algebraically in terms of their coefficients. (See algebraic solution.)\n\nIn various areas of mathematics, the zero set of a function is the set of all its zeros. More precisely, if formula_21 is a real-valued function (or, more generally, a function taking values in some additive group), its zero set is formula_22 of formula_23, the inverse image of formula_24).\n\nThe term \"zero set\" is generally used when there are infinitely many zeros, and they have some non-trivial topological properties. For example, a level set of a function formula_1 is the zero set of formula_26. The cozero set of formula_1 is the complement of the zero set of formula_1 (i.e., the subset of formula_23 on which formula_1 is nonzero).\n\nIn algebraic geometry, the first definition of an algebraic variety is through zero sets. Specifically, an affine algebraic set is the intersection of the zero sets of several polynomials in a polynomial ring formula_31 over a field. In this context, a zero set is sometimes called a \"zero locus\".\n\nIn analysis and geometry, any closed subset of formula_32 is the zero set of a smooth function defined on all of formula_32. This extends to any smooth manifold as a corollary of paracompactness. \n\nIn differential geometry, zero sets are frequently used to define manifolds. An important special case is the case that formula_1 is a smooth function from formula_35 to formula_32. If zero is a regular value of formula_1, then the zero set of formula_1 is a smooth manifold of dimension formula_39 by the regular value theorem.\n\nFor example, the unit formula_40-sphere in formula_41 is the zero set of the real-valued function formula_42.\n\n"}
