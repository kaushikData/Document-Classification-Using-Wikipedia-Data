{"id": "19321930", "url": "https://en.wikipedia.org/wiki?curid=19321930", "title": "Acta Biotheoretica", "text": "Acta Biotheoretica\n\n\"Acta Biotheoretica: Mathematical and philosophical foundations of biological and biomedical science\" is a quarterly peer-reviewed scientific journal published by Springer Science+Business Media. It is the official journal of the Jan van der Hoeven Society for Theoretical Biology. The editor-in-chief is D.J. Kornet (Leiden University).\n\nThe journal's focus is theoretical biology which includes mathematical representation, treatment, and modeling for simulations and quantitative descriptions. The journal's focus also includes the philosophy of biology which emphasizes looking at the methods developed to form biological theory. Topical coverage also includes biomathematics, computational biology, genetics, ecology, and morphology.\n\nThis journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 0.950.\n"}
{"id": "25384599", "url": "https://en.wikipedia.org/wiki?curid=25384599", "title": "Adequate equivalence relation", "text": "Adequate equivalence relation\n\nIn algebraic geometry, a branch of mathematics, an adequate equivalence relation is an equivalence relation on algebraic cycles of smooth projective varieties used to obtain a well-working theory of such cycles, and in particular, well-defined intersection products. Pierre Samuel formalized the concept of an adequate equivalence relation in 1958. Since then it has become central to theory of motives. For every adequate equivalence relation, one may define the category of pure motives with respect to that relation.\n\nPossible (and useful) adequate equivalence relations include \"rational\", \"algebraic\", \"homological\" and \"numerical equivalence\". They are called \"adequate\" because dividing out by the equivalence relation is functorial, i.e. push-forward (with change of co-dimension) and pull-back of cycles is well-defined. Codimension one cycles modulo rational equivalence form the classical group of divisors. All cycles modulo rational equivalence form the Chow ring.\n\nLet \"Z(X)\" := Z[\"X\"] be the free abelian group on the algebraic cycles of \"X\". Then an adequate equivalence relation is a family of equivalence relations, \"∼\" on \"Z(X)\", one for each smooth projective variety \"X\", satisfying the following three conditions:\n\nThe push-forward cycle in the last axiom is often denoted\nIf formula_6 is the graph of a function, then this reduces to the push-forward of the function. The generalizations of functions from \"X\" to \"Y\" to cycles on \"X × Y\" are known as correspondences. The last axiom allows us to push forward cycles by a correspondence.\n\nThe most common equivalence relations, listed from strongest to weakest, are gathered below in a table.\n"}
{"id": "1048680", "url": "https://en.wikipedia.org/wiki?curid=1048680", "title": "Algebraic normal form", "text": "Algebraic normal form\n\nIn Boolean algebra, the algebraic normal form (ANF), ring sum normal form (RSNF or RNF), \"Zhegalkin normal form\", or \"Reed–Muller expansion\" is a way of writing logical formulas in one of three subforms:\n\n\nFormulas written in ANF are also known as Zhegalkin polynomials () and Positive Polarity (or Parity) Reed–Muller expressions (PPRM).\n\nANF is a normal form, which means that two equivalent formulas will convert to the same ANF, easily showing whether two formulas are equivalent for automated theorem proving. Unlike other normal forms, it can be represented as a simple list of lists of variable names. Conjunctive and disjunctive normal forms also require recording whether each variable is negated or not. Negation normal form is unsuitable for that purpose, since it doesn't use equality as its equivalence relation: a ∨ ¬a isn't reduced to the same thing as 1, even though they're equal.\n\nPutting a formula into ANF also makes it easy to identify linear functions (used, for example, in linear feedback shift registers): a linear function is one that is a sum of single literals. Properties of nonlinear feedback shift registers can also be deduced from certain properties of the feedback function in ANF.\n\nThere are straightforward ways to perform the standard boolean operations on ANF inputs in order to get ANF results.\n\nXOR (logical exclusive disjunction) is performed directly:\n\nNOT (logical negation) is XORing 1:\n\nAND (logical conjunction) is distributed algebraically\n\nOR (logical disjunction) uses either 1 ⊕ (1 ⊕ a)(1 ⊕ b) (easier when both operands have purely true terms) or a ⊕ b ⊕ ab (easier otherwise):\n\nEach variable in a formula is already in pure ANF, so you only need to perform the formula's boolean operations as shown above to get the entire formula into ANF. For example:\n\nANF is sometimes described in an equivalent way:\n\nThere are only four functions with one argument:\n\nTo represent a function with multiple arguments one can use the following equality:\n\nIndeed,\n\nSince both formula_17 and formula_18 have fewer arguments than formula_3 it follows that using this process recursively we will finish with functions with one variable. For example, let us construct ANF of formula_20 (logical or):\n\n\n"}
{"id": "35986396", "url": "https://en.wikipedia.org/wiki?curid=35986396", "title": "Almost commutative ring", "text": "Almost commutative ring\n\nIn algebra, a filtered ring \"A\" is said to be almost commutative if the associated graded ring formula_1 is commutative.\n\nBasic examples of almost commutative rings involve differential operators. For example, the enveloping algebra of a complex Lie algebra is almost commutative by the PBW theorem. Similarly, a Weyl algebra is almost commutative.\n\n\n"}
{"id": "1145733", "url": "https://en.wikipedia.org/wiki?curid=1145733", "title": "BIBO stability", "text": "BIBO stability\n\nIn signal processing, specifically control theory, bounded-input, bounded-output (BIBO) stability is a form of stability for linear signals and systems that take inputs. If a system is BIBO stable, then the output will be bounded for every input to the system that is bounded.\n\nA signal is bounded if there is a finite value formula_1 such that the signal magnitude never exceeds formula_2, that is\n\nFor a continuous time linear time-invariant (LTI) system, the condition for BIBO stability is that the impulse response be absolutely integrable, i.e., its L norm exists.\n\nFor a discrete time LTI system, the condition for BIBO stability is that the impulse response be absolutely summable, i.e., its formula_6 norm exists.\n\nGiven a discrete time LTI system with impulse response formula_8 the relationship between the input formula_9 and the output formula_10 is\n\nwhere formula_12 denotes convolution. Then it follows by the definition of convolution\n\nLet formula_14 be the maximum value of formula_15, i.e., the formula_16-norm.\n\nIf formula_20 is absolutely summable, then formula_21 and\n\nSo if formula_20 is absolutely summable and formula_24 is bounded, then formula_25 is bounded as well because formula_26\n\nThe proof for continuous-time follows the same arguments.\n\nFor a rational and continuous-time system, the condition for stability is that the region of convergence (ROC) of the Laplace transform includes the imaginary axis. When the system is causal, the ROC is the open region to the right of a vertical line whose abscissa is the real part of the \"largest pole\", or the pole that has the greatest real part of any pole in the system. The real part of the largest pole defining the ROC is called the abscissa of convergence. Therefore, all poles of the system must be in the strict left half of the s-plane for BIBO stability.\n\nThis stability condition can be derived from the above time-domain condition as follows:\n\nwhere formula_28 and formula_29\n\nThe region of convergence must therefore include the imaginary axis.\n\nFor a rational and discrete time system, the condition for stability is that the region of convergence (ROC) of the z-transform includes the unit circle. When the system is causal, the ROC is the open region outside a circle whose radius is the magnitude of the pole with largest magnitude. Therefore, all poles of the system must be inside the unit circle in the z-plane for BIBO stability.\n\nThis stability condition can be derived in a similar fashion to the continuous-time derivation:\n\nwhere formula_31 and formula_32.\n\nThe region of convergence must therefore include the unit circle.\n\n"}
{"id": "1461105", "url": "https://en.wikipedia.org/wiki?curid=1461105", "title": "Borel's lemma", "text": "Borel's lemma\n\nIn mathematics, Borel's lemma, named after Émile Borel, is an important result used in the theory of asymptotic expansions and partial differential equations.\n\nSuppose \"U\" is an open set in the Euclidean space R, and suppose that \"f\", \"f\" ... is a sequence of smooth functions on \"U\".\n\nIf \"I\" is an any open interval in R containing 0 (possibly \"I\" = R), then there exists a smooth function \"F\"(\"t\", \"x\") defined on \"I\"×\"U\", such that\n\nfor \"k\" ≥ 0 and \"x\" in \"U\".\n\nProofs of Borel's lemma can be found in many text books on analysis, including and , from which the proof below is taken.\n\nNote that it suffices to prove the result for a small interval \"I\" = (−ε,ε), since if ψ(\"t\") is a smooth bump function with compact support in (−ε,ε) equal identically to 1 near 0, then ψ(\"t\") ⋅ \"F\"(\"t\", \"x\") gives a solution on R × \"U\". Similarly using a smooth partition of unity on R subordinate to a covering by open balls with centres at δ⋅Z, it can be assumed that all the \"f\" have compact support in some fixed closed ball \"C\". For each \"m\", let\n\nwhere ε is chosen sufficiently small that\n\nfor |α| < \"m\". These estimates imply that each sum\n\nis uniformly convergent and hence that\n\nis a smooth function with\n\nBy construction\n\nNote: Exactly the same construction can be applied, without the auxiliary space \"U\", to produce a smooth function on the interval \"I\" for which the derivatives at 0 form an arbitrary sequence.\n\n\n"}
{"id": "12217323", "url": "https://en.wikipedia.org/wiki?curid=12217323", "title": "Bounded deformation", "text": "Bounded deformation\n\nIn mathematics, a function of bounded deformation is a function whose distributional derivatives are not quite well-behaved-enough to qualify as functions of bounded variation, although the symmetric part of the derivative matrix does meet that condition. Thought of as deformations of elasto-plastic bodies, functions of bounded deformation play a major role in the mathematical study of materials, e.g. the Francfort-Marigo model of brittle crack evolution.\n\nMore precisely, given an open subset Ω of R, a function \"u\" : Ω → R is said to be of bounded deformation if the symmetrized gradient \"ε\"(\"u\") of \"u\",\n\nis a bounded, symmetric \"n\" × \"n\" matrix-valued Radon measure. The collection of all functions of bounded deformation is denoted BD(Ω; R), or simply BD, introduced essentially by P.-M. Suquet in 1978. BD is a strictly larger space than the space BV of functions of bounded variation.\n\nOne can show that if \"u\" is of bounded deformation then the measure \"ε\"(\"u\") can be decomposed into three parts: one absolutely continuous with respect to Lebesgue measure, denoted \"e\"(\"u\") d\"x\"; a jump part, supported on a rectifiable (\"n\" − 1)-dimensional set \"J\" of points where \"u\" has two different approximate limits \"u\" and \"u\", together with a normal vector \"ν\"; and a \"Cantor part\", which vanishes on Borel sets of finite \"H\"-measure (where \"H\" denotes \"k\"-dimensional Hausdorff measure).\n\nA function \"u\" is said to be of special bounded deformation if the Cantor part of \"ε\"(\"u\") vanishes, so that the measure can be written as\n\nwhere \"H\" | \"J\" denotes \"H\" on the jump set \"J\" and formula_3 denotes the symmetrized dyadic product:\n\nThe collection of all functions of special bounded deformation is denoted SBD(Ω; R), or simply SBD.\n\n"}
{"id": "12288578", "url": "https://en.wikipedia.org/wiki?curid=12288578", "title": "Calderón–Zygmund lemma", "text": "Calderón–Zygmund lemma\n\nIn mathematics, the Calderón–Zygmund lemma is a fundamental result in Fourier analysis, harmonic analysis, and singular integrals. It is named for the mathematicians Alberto Calderón and Antoni Zygmund.\n\nGiven an integrable function , where denotes Euclidean space and denotes the complex numbers, the lemma gives a precise way of partitioning into two sets: one where is essentially small; the other a countable collection of cubes where is essentially large, but where some control of the function is retained.\n\nThis leads to the associated Calderón–Zygmund decomposition of , wherein is written as the sum of \"good\" and \"bad\" functions, using the above sets.\n\nLet be integrable and be a positive constant. Then there exists an open set such that:\n\nGiven as above, we may write as the sum of a \"good\" function and a \"bad\" function , . To do this, we define\n\nand let . Consequently we have that\n\nfor each cube .\n\nThe function is thus supported on a collection of cubes where is allowed to be \"large\", but has the beneficial property that its average value is zero on each of these cubes. Meanwhile, for almost every in , and on each cube in , is equal to the average value of over that cube, which by the covering chosen is not more than .\n\n\n"}
{"id": "41282222", "url": "https://en.wikipedia.org/wiki?curid=41282222", "title": "Calvin C. Moore", "text": "Calvin C. Moore\n\nCalvin C. Moore (born November 2, 1936 in New York City) is an American mathematician who works in the theory of operator algebras and topological groups.\n\nMoore graduated from Harvard University with a bachelor's degree in 1958 and with a Ph.D. in 1960 under the supervision of George Mackey (\"Extensions and cohomology theory of locally compact groups\"). In 1961 he became assistant professor at the University of California, Berkeley and professor in 1966. From 1977 to 1980, he was director of the Center for Pure and Applied mathematics.\n\nWith Shiing-Shen Chern and Isadore Singer, he co-founded Mathematical Sciences Research Institute in 1982. From 1964 to 1965 he was at the Institute for Advanced Study at Princeton.\n\nHe is a fellow of the American Academy of Arts and Sciences. From 1965 to 1967 he was a Sloan Fellow. From 1971 to 1979 he was a member of the Board of Trustees of the American Mathematical Society, whose fellow he is. Since 1977, he is co-editor of the \"Pacific Journal of Mathematics\". From 1978 to 1979 he was a Miller research professor at Berkeley.\n\nHe has written on a history of mathematics at Berkeley. \n\nHis students include Roger Howe and .\n\n\n"}
{"id": "1917276", "url": "https://en.wikipedia.org/wiki?curid=1917276", "title": "Circular shift", "text": "Circular shift\n\nIn combinatorial mathematics, a circular shift is the operation of rearranging the entries in a tuple, either by moving the final entry to the first position, while shifting all other entries to the next position, or by performing the inverse operation. A circular shift is a special kind of cyclic permutation, which in turn is a special kind of permutation. Formally, a circular shift is a permutation σ of the \"n\" entries in the tuple such that either \nor\n\nThe result of repeatedly applying circular shifts to a given tuple are also called the circular shifts of the tuple.\n\nFor example, repeatedly applying circular shifts to the four-tuple (\"a\", \"b\", \"c\", \"d\") successively gives\nand then the sequence repeats; this four-tuple therefore has four distinct circular shifts. However, not all \"n\"-tuples have \"n\" distinct circular shifts. For instance, the 4-tuple (\"a\", \"b\", \"a\", \"b\") only has 2 distinct circular shifts. In general the number of circular shifts of an \"n\"-tuple could be any divisor of \"n\", depending on the entries of the tuple.\n\nIn computer programming, a circular shift (or bitwise rotation) is a shift operator that shifts all bits of its operand. Unlike an arithmetic shift, a circular shift does not preserve a number's sign bit or distinguish a number's exponent from its significand (sometimes referred to as the mantissa). Unlike a logical shift, the vacant bit positions are not filled in with zeros but are filled in with the bits that are shifted out of the sequence.\n\nCircular shifts are used often in cryptography in order to permute bit sequences. Unfortunately, many programming languages, including C, do not have operators or standard functions for circular shifting, even though virtually all processors have bitwise operation instructions for it (e.g. Intel x86 has ROL and ROR).\nHowever, some compilers may provide access to the processor instructions by means of intrinsic functions. In addition, it is possible to write standard ANSI C code that compiles down to the \"rotate\" assembly language instruction (on CPUs that have such an instruction). Most C compilers recognize the following idiom, and compile it to a single 32-bit rotate instruction.\n\nThis safe and compiler-friendly implementation was developed by John Regehr, and further polished by Peter Cordes.\n\nA simpler version is often seen when the codice_1 is limited to the range of 1 to 31 bits:\n\nThis version is dangerous because if the codice_1 is 0 or 32, it asks for a 32-bit shift, which is undefined behaviour in the C language standard. However, it tends to work anyway, because most microprocessors implement codice_3 as either a 32-bit shift (producing 0) or a 0-bit shift (producing the original codice_4), and either one produces the correct result in this application.\n\nFor C++, the use of templates can expand the support to all integer types:\n\nIf the bit sequence 0001 0111 were subjected to a circular shift of one bit position... (see images below)\n\nIf the bit sequence 1001 0110 were subjected to the following operations:\n\nCyclic codes are a kind of block code with the property that the circular shift of a codeword will always yield another codeword. This motivates the following general definition: For a string \"s\" over an alphabet \"Σ\", let \"shift(s)\" denote the set of circular shifts of \"s\",\nand for a set \"L\" of strings, let \"shift\"(\"L\") denote the set of all circular shifts of strings in \"L\". If \"L\" is a cyclic code, then \"shift\"(\"L\") ⊆ \"L\"; this is a necessary condition for \"L\" being a cyclic language. The operation \"shift\"(\"L\") has been studied in formal language theory. For instance, if \"L\" is a context-free language, then \"shift\"(\"L\") is again context-free. Also, if \"L\" is described by a regular expression of length \"n\", there is a regular expression of length \"O\"(\"n\") describing \"shift\"(\"L\").\n\n"}
{"id": "20426081", "url": "https://en.wikipedia.org/wiki?curid=20426081", "title": "Clairaut's relation", "text": "Clairaut's relation\n\nClairaut's relation, named after Alexis Claude de Clairaut, is a formula in classical differential geometry. The formula relates the distance \"r\"(\"t\") from a point on a great circle of the unit sphere to the \"z\"-axis, and the angle \"θ\"(\"t\") between the tangent vector and the latitudinal circle:\n\nThe relation remains valid for a geodesic on an arbitrary surface of revolution.\n\nA formal mathematical statement of Clairaut's relation is:\n\nPressley (p. 185) explains this theorem as an expression of conservation of angular momentum about the axis of revolution when a particle slides along a geodesic under no forces other than those that keep it on the surface.\n\n"}
{"id": "2606797", "url": "https://en.wikipedia.org/wiki?curid=2606797", "title": "Creative and productive sets", "text": "Creative and productive sets\n\nIn computability theory, productive sets and creative sets are types of sets of natural numbers that have important applications in mathematical logic. They are a standard topic in mathematical logic textbooks such as and .\n\nFor the remainder of this article, assume that formula_1 is an acceptable numbering of the computable functions and \"W\" the corresponding numbering of the recursively enumerable sets.\n\nA set \"A\" of natural numbers is called productive if there exists a total recursive (computable) function formula_2 so that for all formula_3, if formula_4 then formula_5 The function formula_2 is called the productive function for formula_7\n\nA set \"A\" of natural numbers is called creative if \"A\" is recursively enumerable and its complement formula_8 is productive. Not every productive set has a recursively enumerable complement, however, as illustrated below.\n\nThe archetypal creative set is formula_9, the set representing the halting problem. Its complement formula_10 is productive with productive function \"f\"(\"i\") = \"i\" (the identity function).\n\nTo see this, we apply the definition of productivity function and show separately that formula_11 and formula_12:\n\nNo productive set \"A\" can be recursively enumerable, because whenever \"A\" contains every number in an r.e. set \"W\" it contains other numbers, and moreover there is an effective procedure to produce an example of such a number from the index \"i\". Similarly, no creative set can be decidable, because this would imply that its complement, a productive set, is recursively enumerable.\n\nAny productive set has a productive function that is injective and total.\n\nThe following theorems, due to Myhill (1955), show that in a sense all creative sets are like formula_23 and all productive sets are like formula_24.\n\nTheorem. Let \"P\" be a set of natural numbers. The following are equivalent:\n\nTheorem. Let \"C\" be a set of natural numbers. The following are equivalent:\n\nThe set of all provable sentences in an effective axiomatic system is always a recursively enumerable set. If the system is suitably complex, like first-order arithmetic, then the set \"T\" of Gödel numbers of true sentences in the system will be a productive set, which means that whenever \"W\" is a recursively enumerable set of true sentences, there is at least one true sentence that is not in \"W\". This can be used to give a rigorous proof of Gödel's first incompleteness theorem, because no recursively enumerable set is productive. The complement of the set \"T\" will not be recursively enumerable, and thus \"T\" is an example of a productive set whose complement is not creative.\n\nThe seminal paper of defined the concept he called a Creative set. Reiterating, the set formula_27 referenced above and defined as the domain of the function formula_28 that takes the diagonal of all enumerated 1-place computable partial functions and adds 1 to them is an example of a creative set. Post gave a version of Gödel's Incompleteness Theorem using his creative sets, where originally Gödel had in some sense constructed a sentence that could be freely translated as saying \"I am unprovable in this axiomatic theory.\" However, Gödel's proof did not work from the concept of true sentences, and rather used the concept of a consistent theory, which led to the Second incompleteness theorem. After Post completed his version of incompleteness he then added the following:\n\n\"The conclusion is unescapable that even for such a fixed, well defined body of mathematical propositions, mathematical thinking is, and must remain, essentially creative.\"\n\nThe usual creative set formula_29 defined using the diagonal function formula_30 has its own historical development. Alan Turing in a 1936 article on the Turing machine showed the existence of a universal computer that computes the formula_31 function. The function formula_32 is defined such that\nformula_33 (\"the result of applying the instructions coded by\" formula_34 \"to the input\" formula_35 ), and is universal in the sense that any calculable partial function formula_36 is given by formula_37 for all formula_38 where formula_39 codes the instructions for formula_2. Using the above notation formula_41, and the diagonal function arises quite naturally as formula_42. Ultimately, these ideas are connected to Church's thesis that says the mathematical notion of computable partial functions is the \"correct\" formalization of an effectively calculable partial function, which can neither be proved or disproved. Church used Lambda calculus, Turing an idealized computer, and later Emil Post in his approach, all of which are equivalent.\n\n"}
{"id": "442961", "url": "https://en.wikipedia.org/wiki?curid=442961", "title": "Cryptographic Message Syntax", "text": "Cryptographic Message Syntax\n\nThe Cryptographic Message Syntax (CMS) is the IETF's standard for cryptographically protected messages. It can be used to digitally sign, digest, authenticate or encrypt any form of digital data.\n\nCMS is based on the syntax of PKCS #7, which in turn is based on the Privacy-Enhanced Mail standard. The newest version of CMS () is specified in (but see also for updated ASN.1 modules conforming to ASN.1 2002).\n\nThe architecture of CMS is built around certificate-based key management, such as the profile defined by the PKIX working group.\n\nCMS is used as the key cryptographic component of many other cryptographic standards, such as S/MIME, PKCS #12 and the Digital timestamping protocol.\n\nOpenSSL is open source software that can encrypt, decrypt, sign and verify, compress and uncompress CMS documents.\n\n\n"}
{"id": "5280515", "url": "https://en.wikipedia.org/wiki?curid=5280515", "title": "Exceptional object", "text": "Exceptional object\n\nMany branches of mathematics study objects of a given type and prove a classification theorem. A common theme is that the classification results in a number of series of objects and a finite number of exceptions that do not fit into any series. These are known as exceptional objects. Frequently these exceptional objects play a further and important role in the subject. Furthermore, the exceptional objects in one branch of mathematics are often related to the exceptional objects in others.\n\nA related phenomenon is exceptional isomorphism, when two series are in general different, but agree for some small values.\n\nThe prototypical examples of exceptional objects arise in the classification of regular polytopes. In two dimensions there is a series of regular \"n\"-gons for \"n\" ≥ 3. In every dimension above 2 we find analogues of the cube, tetrahedron and octahedron. In three dimensions we find two more regular polyhedra — the dodecahedron (12-hedron) and the icosahedron (20-hedron) — making five Platonic solids. In four dimensions we have a total of six regular polytopes including the 120-cell, the 600-cell and the 24-cell. There are no other regular polytopes; in higher dimensions the only regular polytopes are of the hypercube, simplex, orthoplex series. In all dimensions combined, there are therefore three series and five exceptional polytopes.\n\nThe pattern is similar if non-convex polytopes are included. In two dimensions there is a regular star polygon for every rational number \"p\"/\"q\" > 2. In three dimensions there are four Kepler–Poinsot polyhedra, and in four dimensions ten Schläfli–Hess polychora; in higher dimensions there are no non-convex regular figures.\n\nThese can be generalized to tessellations of other spaces, especially uniform tessellations, notably tilings of Euclidean space (honeycombs), which have exceptional objects, and tilings of hyperbolic space. There are various exceptional objects in dimension below 6, but in dimension 6 and above the only regular polyhedra/tilings/hyperbolic tilings are the simplex, hypercube, cross-polytope, and hypercube lattice.\n\nRelated to tilings and the regular polyhedra, there are exceptional Schwarz triangles (triangles that tile the sphere, or more generally Euclidean plane or hyperbolic plane via their triangle group of reflections in their edges), particularly the Möbius triangles. In the sphere there are 3 Möbius triangles (and 1 1-parameter family), corresponding to the 3 exceptional Platonic solid groups, while in the Euclidean plane there are 3 Möbius triangles, corresponding to the 3 special triangles: 60-60-60 (equilateral), 45-45-90 (isosceles right), and 30-60-90. There are additional exceptional Schwarz triangles in the sphere and Euclidean plane. By contrast, in the hyperbolic plane there is a 3-parameter family of Möbius triangles, and none exceptional.\n\nThe finite simple groups have been classified into a number of series as well as 26 sporadic groups. Of these, 20 are subgroups or subquotients of the monster group, referred to as the \"Happy Family\", while 6 are not, and are referred to as \"pariahs\".\n\nSeveral of the sporadic groups are related to the Leech lattice, most notably the Conway group Co, which is the automorphism group of the Leech lattice, quotiented out by its center.\n\nThere are only three finite-dimensional associative division algebras over the reals — the real numbers, the complex numbers and the quaternions. The only non-associative division algebra is the algebra of octonions. The octonions are connected to a wide variety of exceptional objects. For example, the exceptional formally real Jordan algebra is the Albert algebra of 3 by 3 self-adjoint matrices over the octonions.\n\nThe simple Lie groups form a number of series (classical Lie groups) labelled A, B, C and D. In addition there are the exceptional groups G (the automorphism group of the octonions), F, E, E, E. These last four groups can be viewed as the symmetry groups of projective planes over O, C⊗O, H⊗O and O⊗O respectively, where O is the octonions and the tensor products are over the reals.\n\nThe classification of Lie groups corresponds to the classification of root systems and thus the exceptional Lie groups correspond to exceptional root systems and exceptional Dynkin diagrams.\n\nThere are a few exceptional objects with supersymmetry. The classification of superalgebras by Kac and Tierry-Mieg indicates that the Lie superalgebras G(3) in 31 dimensions and F(4) in 40 dimensions, and the Jordan superalgebras K and K, are examples of exceptional objects.\n\nUp to isometry there is only one even unimodular lattice in 15 dimensions or less — the E lattice. Up to dimension 24 there is only one even unimodular lattice without roots, the Leech lattice. Three of the sporadic simple groups were discovered by Conway while investigating the automorphism group of the Leech lattice. For example, Co is the automorphism group itself modulo ±1. The groups Co and Co, as well as a number of other sporadic groups, arise as stabilisers of various subsets of the Leech lattice.\n\nSome codes also stand out as exceptional objects, in particular the perfect binary Golay code which is closely related to the Leech lattice. The Mathieu group formula_1, one of the sporadic simple groups, is the group of automorphisms of the extended binary Golay code, and four more of the sporadic simple groups arise as various types of stabilizer subgroup of formula_1.\n\nAn exceptional block design is the Steiner system S(5,8,24) whose automorphism group is the sporadic simple Mathieu group formula_1.\n\nThe codewords of the extended binary Golay code have a length of 24 bits and have weights 0, 8, 12, 16, or 24. This code can correct up to three errors. So every 24-bit word with weight 5 can be corrected to a codeword with weight 8. The bits of a 24-bit word can be thought of as specifying the possible subsets of a 24 element set. So the extended binary Golay code gives a unique 8 element subset for each 5 element subset. In fact, it defines S(5,8,24).\n\nCertain families of groups generically have a certain outer automorphism group, but in particular cases they have other, exceptional outer automorphisms.\n\nAmong families of finite simple groups, the only example is in the automorphisms of the symmetric and alternating groups: for formula_4 the alternating group formula_5 has one outer automorphism (corresponding to conjugation by an odd element of formula_6) and the symmetric group formula_6 has no outer automorphisms. However, for formula_8 there is an exceptional outer automorphism of formula_9 (of order 2), and correspondingly, the outer automorphism group of formula_10 is not formula_11 (the group of order 2) but rather formula_12, the Klein four-group.\n\nIf one instead considers A as the (isomorphic) projective special linear group PSL(2,9), then the outer automorphism is not exceptional; thus the exceptionalness can be seen as due to the exceptional isomorphism formula_13 This exceptional outer automorphism is realized inside of the Mathieu group M and similarly, M acts on a set of 12 elements in 2 different ways.\n\nAmong Lie groups, the spin group Spin(8) has an exceptionally large outer automorphism group (namely formula_14), which corresponds to the exceptional symmetries of the Dynkin diagram D. This phenomenon is referred to as \"triality.\"\n\nThe exceptional symmetry of the D diagram also gives rise to the Steinberg groups.\n\nThe Kervaire invariant is an invariant of a (4\"k\"+2)-dimensional manifold that measures whether the manifold could be surgically converted into a sphere. This invariant evaluates to 0 if the manifold can be converted to a sphere, and 1 otherwise. More specifically, the Kervaire invariant applies to a framed manifold, that is, to a manifold equipped with an embedding into Euclidean space and a trivialization of the normal bundle. The Kervaire invariant problem is the problem of determining in which dimensions the Kervaire invariant can be nonzero. For differentiable manifolds, this can happen in dimensions 2, 6, 14, 30, 62, and possibly 126, and in no other dimensions. The final case of dimension 126 remains open. These five or six framed cobordism classes of manifolds having Kervaire invariant 1 are exceptional objects related to exotic spheres. The first three cases are related to the complex numbers, quaternions and octonions respectively: a manifold of Kervaire invariant 1 can be constructed as the product of two spheres, with its exotic framing determined by the normed division algebra.\n\nDue to similarities of dimensions, it is conjectured that the remaining cases (dimensions 30, 62 and 126) are related to the Rosenfeld projective planes, which are defined over algebras constructed from the octonions. Specifically, it has been conjectured that there is a construction that takes these projective planes and produces a manifold with nonzero Kervaire invariant in two dimensions lower, but this remains unconfirmed.\n\nIn quantum information theory, there exist structures known as SIC-POVMs or SICs, which correspond to maximal sets of complex equiangular lines. Some of the known SICs—those in vector spaces of 2 and 3 dimensions, as well as certain solutions in 8 dimensions—are considered exceptional objects and called \"sporadic SICs\". They differ from the other known SICs in ways that involve their symmetry groups, the Galois theory of the numerical values of their vector components, and so forth. The sporadic SICs in dimension 8 are related to the integral octonions.\n\nNumerous connections have been observed between some, though not all, of these exceptional objects. Most common are objects related to 8 and 24 dimensions, noting that 24 = 8 · 3. By contrast, the pariah groups stand apart, as the name suggests.\n\nExceptional objects related to the number 8 include the following.\nLikewise, exceptional objects related to the number 24 include the following.\n\nThese objects are connected to various other phenomena in math which may be considered surprising but not themselves \"exceptional\". For example, in algebraic topology, 8-fold real Bott periodicity can be seen as coming from the octonions. In the theory of modular forms, the 24-dimensional nature of the Leech lattice underlies the presence of 24 in the formulas for the Dedekind eta function and the modular discriminant, which connection is deepened by Monstrous moonshine, a development that related modular functions to the Monster group.\n\nIn string theory and superstring theory we often find that particular dimensions are singled out as a result of exceptional algebraic phenomena. For example, bosonic string theory requires a spacetime of dimension 26 which is directly related to the presence of 24 in the Dedekind eta function. Similarly, the possible dimensions of supergravity are related to the dimensions of the division algebras.\n\nMany of the exceptional objects in mathematics and physics have been found to be connected to each other. Developments such as the Monstrous moonshine conjectures show how, for example, the Monster group is connected to string theory. The theory of modular forms shows how the algebra E is connected to the Monster group. (In fact, well before the proof of the Monstrous moonshine conjecture, the elliptic \"j\"-function was discovered to encode the representations of E.) Other interesting connections include how the Leech lattice is connected via the Golay code to the adjacency matrix of the dodecahedron (another exceptional object). Below is a mind map showing how some of the exceptional objects in mathematics and mathematical physics are related.\n\nThe connections can partly be explained by thinking of the algebras as a tower of lattice vertex operator algebras. It just so happens that the vertex algebras at the bottom are so simple that they are isomorphic to familiar non-vertex algebras. Thus the connections can be seen simply as the consequence of some lattices being sub-lattices of others.\n\nThe Jordan superalgebras are a parallel set of exceptional objects with supersymmetry. These are the Lie superalgebras which are related to Lorentzian lattices. This subject is less explored, and the connections between the objects are less well established. There are new conjectures parallel to the Monstrous moonshine conjectures for these super-objects, involving different sporadic groups.\n\n\"Exceptional\" object is reserved for objects that are unusual, meaning rare, the exception, not for \"unexpected\" or \"non-standard\" objects. These unexpected-but-typical (or common) phenomena are generally referred to as pathological, such as nowhere differentiable functions, or \"exotic\", as in exotic spheres — there are exotic spheres in arbitrarily high dimension (not only a finite set of exceptions), and in many dimensions most (differential structures on) spheres are exotic.\n\nExceptional objects must be distinguished from \"extremal\" objects: those that fall in a family and are the most extreme example by some measure are of interest, but not unusual in the way exceptional objects are. For example, the golden ratio \"φ\" has the simplest continued fraction approximation, and accordingly is most difficult to approximate by rationals; however, it is but one of infinitely many such quadratic numbers (continued fractions).\n\nSimilarly, the (2,3,7) Schwarz triangle is the smallest hyperbolic Schwarz triangle, and the associated (2,3,7) triangle group is of particular interest, being the universal Hurwitz group, and thus being associated with the Hurwitz curves, the maximally symmetric algebraic curves. However, it falls in a family of such triangles ((2,4,7), (2,3,8), (3,3,7), etc.), and while the smallest, is not exceptional or unlike the others.\n\n"}
{"id": "404456", "url": "https://en.wikipedia.org/wiki?curid=404456", "title": "Finitary", "text": "Finitary\n\nIn mathematics or logic, a finitary operation is an operation of finite arity, that is an operation that takes a finite number of input values. By contrast, an operation that may take an infinite number of input values is said to be infinitary. In standard mathematics, an operation is, by definition, finitary. Therefore these terms are used only in the context of infinitary logic.\n\nA finitary argument is one which can be translated into a finite set of symbolic propositions starting from a finite set of axioms. In other words, it is a proof (including all assumptions) that can be written on a large enough sheet of paper.\n\nBy contrast, infinitary logic studies logics that allow infinitely long statements and proofs. In such a logic, one can regard the existential quantifier, for instance, as derived from an infinitary disjunction.\n\nThe emphasis on finitary methods has historical roots.\n\nIn the early 20th century, logicians aimed to solve the problem of foundations; that is, answer the question: \"What is the true base of mathematics?\" The program was to be able to rewrite all mathematics using an entirely syntactical language \"without semantics\". In the words of David Hilbert (referring to geometry), \"it does not matter if we call the things \"chairs\", \"tables\" and \"beer mugs\" or \"points\", \"lines\" and \"planes\".\"\n\nThe stress on finiteness came from the idea that human \"mathematical\" thought is based on a finite number of principles and all the reasonings follow essentially one rule: the \"modus ponens\". The project was to fix a finite number of symbols (essentially the numerals 1, 2, 3, ... the letters of alphabet and some special symbols like \"+\", \"->\", \"(\", \")\", etc.), give a finite number of propositions expressed in those symbols, which were to be taken as \"foundations\" (the axioms), and some rules of inference which would model the way humans make conclusions. From these, \"regardless of the semantic interpretation of the symbols\" the remaining theorems should follow \"formally\" using only the stated rules (which make mathematics look like a \"game with symbols\" more than a \"science\") without the need to rely on ingenuity. The hope was to prove that from these axioms and rules \"all\" the theorems of mathematics could be deduced. That aim is known as logicism.\n\nKurt Gödel's incompleteness theorem is sometimes alleged to undermine logicism because it shows that no particular axiomatization of mathematics can decide all statements, although such theorem itself is based in logic.\n\n"}
{"id": "8547005", "url": "https://en.wikipedia.org/wiki?curid=8547005", "title": "Frontal solver", "text": "Frontal solver\n\nA frontal solver, conceived by Bruce Irons, is an approach to solving sparse linear systems which is used extensively in finite element analysis. It is a variant of Gauss elimination that automatically avoids a large number of operations involving zero terms.\n\nA frontal solver builds a LU or Cholesky decomposition of a sparse matrix given as the assembly of element matrices by assembling the matrix and eliminating equations only on a subset of elements at a time. However, elements can be stored in-core in a clique sequence as recently proposed by Areias. This subset is called the front and it is essentially the transition region between the part of the system already finished and the part not touched yet. The whole sparse matrix is never created explicitly. Only parts of the matrix are assembled as they enter the front. Processing the front involves dense matrix operations, which use the CPU efficiently. In a typical implementation, only the front is in memory, while the factors in the decomposition are written into files. The element matrices are read from files or created as needed and discarded.\n\nA multifrontal solver of Duff and Reid is an improvement of the frontal solver that uses several independent fronts at the same time. The fronts can be worked on by different processors, which enables parallel computing.\n\nSee for a monograph exposition.\n\n"}
{"id": "113564", "url": "https://en.wikipedia.org/wiki?curid=113564", "title": "General linear group", "text": "General linear group\n\nIn mathematics, the general linear group of degree \"n\" is the set of invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.\n\nTo be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of invertible matrices of real numbers, and is denoted by GL(R) or .\n\nMore generally, the general linear group of degree \"n\" over any field \"F\" (such as the complex numbers), or a ring \"R\" (such as the ring of integers), is the set of invertible matrices with entries from \"F\" (or \"R\"), again with matrix multiplication as the group operation. Typical notation is GL(\"F\") or , or simply GL(\"n\") if the field is understood.\n\nMore generally still, the general linear group of a vector space GL(\"V\") is the abstract automorphism group, not necessarily written as matrices.\n\nThe special linear group, written or SL(\"F\"), is the subgroup of consisting of matrices with a determinant of 1.\n\nThe group and its subgroups are often called linear groups or matrix groups (the abstract group GL(\"V\") is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group .\n\nIf , then the group is not abelian.\n\nIf \"V\" is a vector space over the field \"F\", the general linear group of \"V\", written GL(\"V\") or Aut(\"V\"), is the group of all automorphisms of \"V\", i.e. the set of all bijective linear transformations , together with functional composition as group operation. If \"V\" has finite dimension \"n\", then GL(\"V\") and are isomorphic. The isomorphism is not canonical; it depends on a choice of basis in \"V\". Given a basis of \"V\" and an automorphism \"T\" in GL(\"V\"), we have then for every basis vector \"e\" that\nfor some constants \"a\" in \"F\"; the matrix corresponding to \"T\" is then just the matrix with entries given by the \"a\".\n\nIn a similar way, for a commutative ring \"R\" the group may be interpreted as the group of automorphisms of a \"free\" \"R\"-module \"M\" of rank \"n\". One can also define GL(\"M\") for any \"R\"-module, but in general this is not isomorphic to (for any \"n\").\n\nOver a field \"F\", a matrix is invertible if and only if its determinant is nonzero. Therefore, an alternative definition of is as the group of matrices with nonzero determinant.\n\nOver a commutative ring \"R\", more care is needed: a matrix over \"R\" is invertible if and only if its determinant is a unit in \"R\", that is, if its determinant is invertible in \"R\". Therefore, may be defined as the group of matrices whose determinants are units.\n\nOver a non-commutative ring \"R\", determinants are not at all well behaved. In this case, may be defined as the unit group of the matrix ring .\n\nThe general linear group over the field of real numbers is a real Lie group of dimension \"n\". To see this, note that the set of all real matrices, M(R), forms a real vector space of dimension \"n\". The subset consists of those matrices whose determinant is non-zero. The determinant is a polynomial map, and hence is an open affine subvariety of M(R) (a non-empty open subset of M(R) in the Zariski topology), and therefore\na smooth manifold of the same dimension.\n\nThe Lie algebra of , denoted formula_2 consists of all real matrices with the commutator serving as the Lie bracket.\n\nAs a manifold, is not connected but rather has two connected components: the matrices with positive determinant and the ones with negative determinant. The identity component, denoted by , consists of the real matrices with positive determinant. This is also a Lie group of dimension \"n\"; it has the same Lie algebra as .\n\nThe group is also noncompact. \"The\" maximal compact subgroup of is the orthogonal group O(\"n\"), while \"the\" maximal compact subgroup of is the special orthogonal group SO(\"n\"). As for SO(\"n\"), the group is not simply connected (except when , but rather has a fundamental group isomorphic to Z for or Z for .\n\nThe general linear group over the field of complex numbers, , is a \"complex\" Lie group of complex dimension \"n\". As a real Lie group (through realification) it has dimension 2\"n\". The set of all real matrices forms a real Lie subgroup. These correspond to the inclusions\nwhich have real dimensions \"n\", 2\"n\", and . Complex \"n\"-dimensional matrices can be characterized as real 2\"n\"-dimensional matrices that preserve a linear complex structure — concretely, that commute with a matrix \"J\" such that , where \"J\" corresponds to multiplying by the imaginary unit \"i\".\n\nThe Lie algebra corresponding to consists of all complex matrices with the commutator serving as the Lie bracket.\n\nUnlike the real case, is connected. This follows, in part, since the multiplicative group of complex numbers C is connected. The group manifold is not compact; rather its maximal compact subgroup is the unitary group U(\"n\"). As for U(\"n\"), the group manifold is not simply connected but has a fundamental group isomorphic to Z.\n\nIf \"F\" is a finite field with \"q\" elements, then we sometimes write instead of . When \"p\" is prime, is the outer automorphism group of the group Z, and also the automorphism group, because Z is abelian, so the inner automorphism group is trivial.\n\nThe order of is: \n\nThis can be shown by counting the possible columns of the matrix: the first column can be anything but the zero vector; the second column can be anything but the multiples of the first column; and in general, the \"k\"th column can be any vector not in the linear span of the first columns. In \"q\"-analog notation, this is formula_4.\n\nFor example, has order . It is the automorphism group of the Fano plane and of the group Z, and is also known as .\n\nMore generally, one can count points of Grassmannian over \"F\": in other words the number of subspaces of a given dimension \"k\". This requires only finding the order of the stabilizer subgroup of one such subspace and dividing into the formula just given, by the orbit-stabilizer theorem.\n\nThese formulas are connected to the Schubert decomposition of the Grassmannian, and are \"q\"-analogs of the Betti numbers of complex Grassmannians. This was one of the clues leading to the Weil conjectures.\n\nNote that in the limit the order of goes to 0! – but under the correct procedure (dividing by ) we see that it is the order of the symmetric group (See Lorscheid's article) – in the philosophy of the field with one element, one thus interprets the symmetric group as the general linear group over the field with one element: .\n\nThe general linear group over a prime field, , was constructed and its order computed by Évariste Galois in 1832, in his last letter (to Chevalier) and second (of three) attached manuscripts, which he used in the context of studying the Galois group of the general equation of order \"p\".\n\nThe special linear group, , is the group of all matrices with determinant 1. They are special in that they lie on a subvariety – they satisfy a polynomial equation (as the determinant is a polynomial in the entries). Matrices of this type form a group as the determinant of the product of two matrices is the product of the determinants of each matrix. is a normal subgroup of .\n\nIf we write \"F\" for the multiplicative group of \"F\" (excluding 0), then the determinant is a group homomorphism\nthat is surjective and its kernel is the special linear group. Therefore, by the first isomorphism theorem, is isomorphic to \"F\". In fact, can be written as a semidirect product:\n\nThe special linear group is also the derived group (also known as commutator subgroup) of the GL(\"n\", \"F\") (for a field or a division ring \"F\") provided that formula_5 or \"k\" is not the field with two elements.\n\nWhen \"F\" is R or C, is a Lie subgroup of of dimension . The Lie algebra of consists of all matrices over \"F\" with vanishing trace. The Lie bracket is given by the commutator.\n\nThe special linear group can be characterized as the group of \"volume and orientation preserving\" linear transformations of R.\n\nThe group is simply connected, while is not. has the same fundamental group as , that is, Z for and Z for .\n\nThe set of all invertible diagonal matrices forms a subgroup of isomorphic to (\"F\"). In fields like R and C, these correspond to rescaling the space; the so-called dilations and contractions.\n\nA scalar matrix is a diagonal matrix which is a constant times the identity matrix. The set of all nonzero scalar matrices forms a subgroup of isomorphic to \"F\" . This group is the center of . In particular, it is a normal, abelian subgroup.\n\nThe center of is simply the set of all scalar matrices with unit determinant, and is isomorphic to the group of \"n\"th roots of unity in the field \"F\".\n\nThe so-called classical groups are subgroups of GL(\"V\") which preserve some sort of bilinear form on a vector space \"V\". These include the\nThese groups provide important examples of Lie groups.\n\nThe projective linear group and the projective special linear group are the quotients of and by their centers (which consist of the multiples of the identity matrix therein); they are the induced action on the associated projective space.\n\nThe affine group is an extension of by the group of translations in \"F\". It can be written as a semidirect product:\nwhere acts on \"F\" in the natural manner. The affine group can be viewed as the group of all affine transformations of the affine space underlying the vector space \"F\".\n\nOne has analogous constructions for other subgroups of the general linear group: for instance, the special affine group is the subgroup defined by the semidirect product, , and the Poincaré group is the affine group associated to the Lorentz group, .\n\nThe general semilinear group is the group of all invertible semilinear transformations, and contains GL. A semilinear transformation is a transformation which is linear \"up to a twist\", meaning \"up to a field automorphism under scalar multiplication\". It can be written as a semidirect product:\nwhere Gal(\"F\") is the Galois group of \"F\" (over its prime field), which acts on by the Galois action on the entries.\n\nThe main interest of is that the associated projective semilinear group (which contains is the collineation group of projective space, for , and thus semilinear maps are of interest in projective geometry.\n\nIf one removes the restriction of the determinant being non-zero, the resulting algebraic structure is a monoid, usually called the full linear monoid, but occasionally also \"full linear semigroup\", \"general linear monoid\" etc. It is actually a regular semigroup.\n\nThe infinite general linear group or stable general linear group is the direct limit of the inclusions as the upper left block matrix. It is denoted by either GL(\"F\") or , and can also be interpreted as invertible infinite matrices which differ from the identity matrix in only finitely many places.\n\nIt is used in algebraic K-theory to define K, and over the reals has a well-understood topology, thanks to Bott periodicity.\n\nIt should not be confused with the space of (bounded) invertible operators on a Hilbert space, which is a larger group, and topologically much simpler, namely contractible – see Kuiper's theorem.\n\n\n"}
{"id": "45558284", "url": "https://en.wikipedia.org/wiki?curid=45558284", "title": "Geometric phase analysis", "text": "Geometric phase analysis\n\nGeometric phase analysis is a digital signal processing method used with Fast Fourier transform algorithms in high-resolution transmission electron microscopy images to quantify displacement and strain fields in crystalline lattices at nanoscale resolution.\n"}
{"id": "14471145", "url": "https://en.wikipedia.org/wiki?curid=14471145", "title": "George Henry Law", "text": "George Henry Law\n\nGeorge Henry Law (12 September 1761 – 22 September 1845) was the Bishop of Chester (1812) and then, from 1824, Bishop of Bath and Wells. He was the son of Edmund Law, Bishop of Carlisle.\n\nBorn at the lodge of Peterhouse, Cambridge, of which his father Edmund Law was Master, Law was educated at Charterhouse School and at Queens' College, Cambridge, where he was second wrangler. His main claim to fame was the way in which he introduced a systematic and rigorous training system for parish priests.\n\nHe founded a theological college at St Bees in Cumbria. There had been once been a monastery at St Bees, but since the dissolution in 1539 many of the monastic buildings had disappeared and chancel stood roofless when Bishop Law visited Whitehaven in 1816. He was short of good clergy for the diocese, which included Lancashire, and was at that time the powerhouse of the industrial revolution. The consequent growth in population increased the demand for clergymen. Up until Bishop Law's college, training for clergy was haphazard. Most were ordained on the strength of a degree from Oxford or Cambridge, whilst some were ordained after individual instruction from a member of the clergy. Resulting clergy were variable and did not meet a reliable standard. Law was determined to improve the supply situation so when Law visited Whitehaven and met the influential Lowther family and they agreed to pay for restoration of the chancel for a new theogical college he accepted the offer. The agreement allowed Law to appoint the new vicar for St Bees and Principal of the College, contrary to the practice of patronage at the time, and so the St Bees Theological College was born. It was the first theological training institution of the Anglican Church outside Oxford or Cambridge.\n\nThe Lowthers did not act out of pure generosity. They were keen to improve their public image having been accused of acquiring the mineral rights to Whitehaven for a pittance from St Bees School, and were also suspected of having tried to keep the matter quiet by arranging the sacking the headmaster.\n\nHe was the younger brother of Bishop John Law (1745–1810) and of Edward Law, 1st Baron Ellenborough (1750–1818).\n\n"}
{"id": "12955", "url": "https://en.wikipedia.org/wiki?curid=12955", "title": "George Pólya", "text": "George Pólya\n\nGeorge Pólya (; ; December 13, 1887 – September 7, 1985) was a Hungarian mathematician. He was a professor of mathematics from 1914 to 1940 at ETH Zürich and from 1940 to 1953 at Stanford University. He made fundamental contributions to combinatorics, number theory, numerical analysis and probability theory. He is also noted for his work in heuristics and mathematics education. According to György Marx he was one of The Martians.\n\nPólya was born in Budapest, Austria-Hungary to Anna Deutsch and Jakab Pólya, Hungarian Jews who had converted to the Roman Catholic faith in 1886. Although his parents were religious and he was baptized into the Roman Catholic Church, George Pólya grew up to be an agnostic. He was a professor of mathematics from 1914 to 1940 at ETH Zürich in Switzerland and from 1940 to 1953 at Stanford University. He remained Stanford Professor Emeritus for the rest of his life and career. He worked on a range of mathematical topics, including series, number theory, mathematical analysis, geometry, algebra, combinatorics, and probability. He was an Invited Speaker of the ICM in 1928 at Bologna, in 1936 at Oslo, and in 1950 at Cambridge, Massachusetts.\n\nHe died in Palo Alto, California, United States.\n\nEarly in his career, Pólya wrote with Gábor Szegő two influential problem books \"Problems and Theorems in Analysis\" (\"I: Series, Integral Calculus, Theory of Functions\" and \"II: Theory of Functions. Zeros. Polynomials. Determinants. Number Theory. Geometry\"). Later in his career, he spent considerable effort to identify systematic methods of problem-solving to further discovery and invention in mathematics for students, teachers, and researchers. He wrote five books on the subject: \"How to Solve It\", \"Mathematics and Plausible Reasoning\" (\"Volume I: Induction and Analogy in Mathematics\", and \"Volume II: Patterns of Plausible Inference\"), and \"Mathematical Discovery: On Understanding, Learning, and Teaching Problem Solving\" (volumes 1 and 2).\n\nIn \"How to Solve It\", Pólya provides general heuristics for solving a gamut of problems, including both mathematical and non-mathematical problems. The book includes advice for teaching students of mathematics and a mini-encyclopedia of heuristic terms. It was translated into several languages and has sold over a million copies. Russian physicist Zhores I. Alfyorov (Nobel laureate in 2000) praised it, noting that he was a fan. The American mathematician Terence Tao used the book to prepare for the International Mathematical Olympiad. The book is still used in mathematical education. Douglas Lenat's Automated Mathematician and Eurisko artificial intelligence programs were inspired by Pólya's work.\n\nIn addition to his works directly addressing problem solving, Pólya wrote another short book called \"Mathematical Methods in Science\", based on a 1963 work supported by the National Science Foundation, edited by Leon Bowden, and published by the Mathematical Association of America (MAA) in 1977. As Pólya notes in the preface, Professor Bowden carefully followed a tape recording of a course Pólya gave several times at Stanford in order to put the book together. Pólya notes in the preface \"that the following pages will be useful, yet they should not be regarded as a finished expression.\"\n\nThere are three prizes named after Pólya, causing occasional confusion of one for another. In 1969 the Society for Industrial and Applied Mathematics (SIAM) established the George Pólya Prize, given alternately in two categories for \"a notable application of combinatorial theory\" and for \"a notable contribution in another area of interest to George Pólya.\" In 1976 the Mathematical Association of America (MAA) established the George Pólya Award \"for articles of expository excellence\" published in the \"College Mathematics Journal\". In 1987 the London Mathematical Society (LMS) established the Pólya Prize for \"outstanding creativity in, imaginative exposition of, or distinguished contribution to, mathematics within the United Kingdom.\"\n\nA mathematics center has been named in Pólya's honor at the University of Idaho in Moscow, Idaho. The mathematics center focuses mainly on tutoring students in the subjects of algebra and calculus.\n\n\n\n\n"}
{"id": "3828409", "url": "https://en.wikipedia.org/wiki?curid=3828409", "title": "Glivenko–Cantelli theorem", "text": "Glivenko–Cantelli theorem\n\nIn the theory of probability, the Glivenko–Cantelli theorem, named after Valery Ivanovich Glivenko and Francesco Paolo Cantelli, determines the asymptotic behaviour of the empirical distribution function as the number of independent and identically distributed observations grows. The uniform convergence of more general empirical measures becomes an important property of the Glivenko–Cantelli classes of functions or sets. The Glivenko–Cantelli classes arise in Vapnik–Chervonenkis theory, with applications to machine learning. Applications can be found in econometrics making use of M-estimators.\n\nAssume that formula_1 are independent and identically-distributed random variables in formula_2 with common cumulative distribution function formula_3. The \"empirical distribution function\" for formula_4 is defined by\n\nwhere formula_6 is the indicator function of the set formula_7. For every (fixed) formula_8, formula_9 is a sequence of random variables which converge to formula_3 almost surely by the strong law of large numbers, that is, formula_11 converges to formula_12 pointwise. Glivenko and Cantelli strengthened this result by proving uniform convergence of formula_11 to formula_12.\n\nTheorem\n\nThis theorem originates with Valery Glivenko, and Francesco Cantelli, in 1933.\n\nRemarks\n\nFor simplicity, consider a case of continuous random variable formula_19. Fix formula_20 such that formula_21 for formula_22. Now for all formula_23 there exists formula_24 such that formula_25. Note that\n\nformula_26\n\nTherefore, almost surely\n\nformula_27\n\nSince formula_28 by strong law of large numbers, we can guarantee that for any integer formula_29 we can find formula_30 such that for all formula_31\n\nformula_32,\n\nwhich is the definition of almost sure convergence.\n\nOne can generalize the \"empirical distribution function\" by replacing the set formula_33 by an arbitrary set \"C\" from a class of sets formula_34 to obtain an empirical measure indexed by sets formula_35\n\nWhere formula_37 is the indicator function of each set formula_7.\n\nFurther generalization is the map induced by formula_39 on measurable real-valued functions \"f\", which is given by\n\nThen it becomes an important property of these classes that the strong law of large numbers holds uniformly on formula_41 or formula_34.\n\nConsider a set formula_43 with a sigma algebra of Borel subsets \"A\" and a probability measure \"P\". For a class of subsets,\n\nand a class of functions\n\ndefine random variables\n\nwhere formula_48 is the empirical measure, formula_49 is the corresponding map, and\n\nDefinitions\n\nTheorem (Vapnik and Chervonenkis, 1968)\n\n\n\n\n"}
{"id": "49514", "url": "https://en.wikipedia.org/wiki?curid=49514", "title": "Henri Lebesgue", "text": "Henri Lebesgue\n\nHenri Léon Lebesgue (; June 28, 1875 – July 26, 1941) was a French mathematician most famous for his theory of integration, which was a generalization of the 17th century concept of integration—summing the area between an axis and the curve of a function defined for that axis. His theory was published originally in his dissertation \"Intégrale, longueur, aire\" (\"Integral, length, area\") at the University of Nancy during 1902.\n\nHenri Lebesgue was born on 28 June 1875 in Beauvais, Oise. Lebesgue's father was a typesetter and his mother was a school teacher. His parents assembled at home a library that the young Henri was able to use. His father died of tuberculosis when Lebesgue was still very young and his mother had to support him by herself. As he showed a remarkable talent for mathematics in primary school, one of his instructors arranged for community support to continue his education at the Collège de Beauvais and then at Lycée Saint-Louis and Lycée Louis-le-Grand in Paris.\n\nIn 1894 Lebesgue was accepted at the École Normale Supérieure, where he continued to focus his energy on the study of mathematics, graduating in 1897. After graduation he remained at the École Normale Supérieure for two years, working in the library, where he became aware of the research on discontinuity done at that time by René-Louis Baire, a recent graduate of the school. At the same time he started his graduate studies at the Sorbonne, where he learned about Émile Borel's work on the incipient measure theory and Camille Jordan's work on the Jordan measure. In 1899 he moved to a teaching position at the Lycée Central in Nancy, while continuing work on his doctorate. In 1902 he earned his Ph.D. from the Sorbonne with the seminal thesis on \"Integral, Length, Area\", submitted with Borel, four years older, as advisor.\n\nLebesgue married the sister of one of his fellow students, and he and his wife had two children, Suzanne and Jacques.\n\nAfter publishing his thesis, Lebesgue was offered in 1902 a position at the University of Rennes, lecturing there until 1906, when he moved to the Faculty of Sciences of the University of Poitiers. In 1910 Lebesgue moved to the Sorbonne as a maître de conférences, being promoted to professor starting with 1919. In 1921 he left the Sorbonne to become professor of mathematics at the Collège de France, where he lectured and did research for the rest of his life. In 1922 he was elected a member of the Académie des Sciences. Henri Lebesgue died on 26 July 1941 in Paris.\n\nLebesgue's first paper was published in 1898 and was titled \"Sur l'approximation des fonctions\". It dealt with Weierstrass' theorem on approximation to continuous functions by polynomials. Between March 1899 and April 1901 Lebesgue published six notes in \"Comptes Rendus.\" The first of these, unrelated to his development of Lebesgue integration, dealt with the extension of Baire's theorem to functions of two variables. The next five dealt with surfaces applicable to a plane, the area of skew polygons, surface integrals of minimum area with a given bound, and the final note gave the definition of Lebesgue integration for some function f(x). Lebesgue's great thesis, \"Intégrale, longueur, aire\", with the full account of this work, appeared in the Annali di Matematica in 1902. The first chapter develops the theory of measure (see Borel measure). In the second chapter he defines the integral both geometrically and analytically. The next chapters expand the \"Comptes Rendus\" notes dealing with length, area and applicable surfaces. The final chapter deals mainly with Plateau's problem. This dissertation is considered to be one of the finest ever written by a mathematician.\n\nHis lectures from 1902 to 1903 were collected into a \"Borel tract\" \"Leçons sur l'intégration et la recherche des fonctions primitives\". The problem of integration regarded as the search for a primitive function is the keynote of the book. Lebesgue presents the problem of integration in its historical context, addressing Augustin-Louis Cauchy, Peter Gustav Lejeune Dirichlet, and Bernhard Riemann. Lebesgue presents six conditions which it is desirable that the integral should satisfy, the last of which is \"If the sequence f(x) increases to the limit f(x), the integral of f(x) tends to the integral of f(x).\" Lebesgue shows that his conditions lead to the theory of measure and measurable functions and the analytical and geometrical definitions of the integral.\n\nHe turned next to trigonometric functions with his 1903 paper \"Sur les séries trigonométriques\". He presented three major theorems in this work: that a trigonometrical series\nrepresenting a bounded function is a Fourier series, that the n Fourier coefficient tends to zero (the Riemann–Lebesgue lemma), and that a Fourier series is integrable term by term. In 1904-1905 Lebesgue lectured once again at the Collège de France, this time on trigonometrical series and he went on to publish his lectures in another of the \"Borel tracts\". In this tract he once again treats the subject in its historical context. He expounds on Fourier series, Cantor-Riemann theory, the Poisson integral and the Dirichlet problem.\n\nIn a 1910 paper, \"Représentation trigonométrique approchée des fonctions satisfaisant a une condition de Lipschitz\" deals with the Fourier series of functions satisfying a Lipschitz condition, with an evaluation of the order of magnitude of the remainder term. He also proves that the Riemann–Lebesgue lemma is a best possible result for continuous functions, and gives some treatment to Lebesgue constants.\n\nLebesgue once wrote, \"Réduites à des théories générales, les mathématiques seraient une belle forme sans contenu.\" (\"Reduced to general theories, mathematics would be a beautiful form without content.\")\n\nIn measure-theoretic analysis and related branches of mathematics, the Lebesgue–Stieltjes integral generalizes Riemann–Stieltjes and Lebesgue integration, preserving the many advantages of the latter in a more general measure-theoretic framework.\n\nDuring the course of his career, Lebesgue also made forays into the realms of complex analysis and topology. He also had a disagreement with Émile Borel about whose integral was more general. However, these minor forays pale in comparison to his contributions to real analysis; his contributions to this field had a tremendous impact on the shape of the field today and his methods have become an essential part of modern analysis. These have important practical implications for fundamental physics of which Lebesgue would have been completely unaware, as noted below.\n\nIntegration is a mathematical operation that corresponds to the informal idea of finding the area under the graph of a function. The first theory of integration was developed by Archimedes in the 3rd century BC with his method of quadratures, but this could be applied only in limited circumstances with a high degree of geometric symmetry. In the 17th century, Isaac Newton and Gottfried Wilhelm Leibniz discovered the idea that integration was intrinsically linked to differentiation, the latter being a way of measuring how quickly a function changed at any given point on the graph. This surprising relationship between two major geometric operations in calculus, differentiation and integration, is now known as the Fundamental Theorem of Calculus. It has allowed mathematicians to calculate a broad class of integrals for the first time. However, unlike Archimedes' method, which was based on Euclidean geometry, mathematicians felt that Newton's and Leibniz's integral calculus did not have a rigorous foundation.\n\nIn the 19th century, Augustin Cauchy developed epsilon-delta limits, and Bernhard Riemann followed up on this by formalizing what is now called the Riemann integral. To define this integral, one fills the area under the graph with smaller and smaller rectangles and takes the limit of the sums of the areas of the rectangles at each stage. For some functions, however, the total area of these rectangles does not approach a single number. As such, they have no Riemann integral.\n\nLebesgue invented a new method of integration to solve this problem.\nInstead of using the areas of rectangles, which put the focus on the domain of the function, Lebesgue looked at the codomain of the function for his fundamental unit of area.\nLebesgue's idea was to first define measure, for both sets and functions on those sets. He then proceeded to build the integral for what he called simple functions; measurable functions that take only finitely many values.\nThen he defined it for more complicated functions as the least upper bound of all the integrals of simple functions smaller than the function in question.\n\nLebesgue integration has the property that every function defined over a bounded interval with a Riemann integral also has a Lebesgue integral, and for those functions the two integrals agree. Furthermore, every bounded function on a closed bounded interval has a Lebesgue integral and there are many functions with a Lebesgue integral that have no Riemann integral.\n\nAs part of the development of Lebesgue integration, Lebesgue invented the concept of measure, which extends the idea of length from intervals to a very large class of sets, called measurable sets (so, more precisely, simple functions are functions that take a finite number of values, and each value is taken on a measurable set).\nLebesgue's technique for turning a measure into an integral generalises easily to many other situations, leading to the modern field of measure theory.\n\nThe Lebesgue integral is deficient in one respect.\nThe Riemann integral generalises to the improper Riemann integral to measure functions whose domain of definition is not a closed interval.\nThe Lebesgue integral integrates many of these functions (always reproducing the same answer when it did), but not all of them.\nFor functions on the real line, the Henstock integral is an even more general notion of integral (based on Riemann's theory rather than Lebesgue's) that subsumes both Lebesgue integration and improper Riemann integration.\nHowever, the Henstock integral depends on specific ordering features of the real line and so does not generalise to allow integration in more\ngeneral spaces (say, manifolds), while the Lebesgue integral extends to such spaces quite naturally.\n\n\n"}
{"id": "4187137", "url": "https://en.wikipedia.org/wiki?curid=4187137", "title": "Higman's lemma", "text": "Higman's lemma\n\nIn mathematics, Higman's lemma states that the set of finite sequences over a finite alphabet, as partially ordered by the subsequence relation, is well-quasi-ordered. That is, if formula_1 is an infinite sequence of words over some fixed finite alphabet, then there exist indices formula_2 such that formula_3 can be obtained from formula_4 by deleting some (possibly none) symbols. More generally this remains true when the alphabet is not necessarily finite, but is itself well-quasi-ordered, and the subsequence relation allows the replacement of symbols by earlier symbols in the well-quasi-ordering of labels. This is a special case of the later Kruskal's tree theorem. It is named after Graham Higman, who published it in 1952.\n"}
{"id": "1192944", "url": "https://en.wikipedia.org/wiki?curid=1192944", "title": "Horace Lamb", "text": "Horace Lamb\n\nSir Horace Lamb (27 November 1849 – 4 December 1934) was an English applied mathematician and author of several influential texts on classical physics, among them \"Hydrodynamics\" (1895) and \"Dynamical Theory of Sound\" (1910). Both of these books remain in print. The word vorticity was coined by Lamb in 1916.\n\nLamb was born in Stockport, Cheshire, the son of John Lamb and his wife Elizabeth, \"née\" Rangeley, the latter a foreman in a cotton mill, who had gained some distinction by an invention for the improvement of spinning machines. John Lamb died while his son was a child. Lamb's mother married again, and shortly afterwards Horace went to live with his strict but maternal aunt, Mrs. Holland. He studied at Stockport Grammar School, where he made the acquaintance of a wise and kindly headmaster in the Rev. Charles Hamilton, and a graduate of classics, Frederic Slaney Poole, who in his final year became a good friend. It was from these two tutors that Lamb acquired his taste for mathematics and, to a somewhat lesser extent, classical literature.\n\nIn 1867, he gained a classical scholarship at Queens' College, Cambridge. Since Lamb's inclination, however, was to pursue a career in engineering, he chose to decline the offer, and instead worked for a year at the Owens College in nearby Manchester, as a means of developing his mathematical proficiency further.\n\nAt that time, the Chair of Pure Mathematics at Owens College was held by Thomas Barker, an eminent Scottish mathematician, who graduated as Senior Wrangler and first Smith's prizeman from the Cambridge Mathematical Tripos in 1862. An acknowledged lecturer of high quality, Lamb prospered under the guidance of Barker, and was elected to a minor scholarship at Trinity College, Cambridge.\n\nAt Trinity, he was Second Wrangler in the Mathematical Tripos, 2nd Smith's prizeman and elected fellow in 1872. Among his professors were James Clerk Maxwell and George Gabriel Stokes. He was soon elected both a Fellow and a tutor in the college.\n\nBy 1874, Lamb had become thoroughly invested in his work at Trinity, preparing there an innovative and original series of lectures on the subject of hydrodynamics for third-year students. Richard Glazebrook, a final-year student at the time, wrote of them that they were 'a revelation', and praised Lamb for his lucid presentation of the properties of liquids in rotational motion. However, Lamb soon became romantically involved with Elizabeth Foot, sister-in-law to his former headmaster, and, since the conditions of his position at Trinity stipulated that he should hold it only so long as he was unmarried, he was compelled, in 1875, to resign and continue his work elsewhere.\n\nLamb's acquaintance from Stockport, Frederic Slaney Poole, had by now for some years lived in South Australia; hearing of his engagement, Poole suggested in a letter that he should apply for the chair at the recently founded University of Adelaide. In 1875, he was appointed the first (Sir Thomas) Elder Professor of Mathematics there, and took up the chair in March, 1876. Lamb was instrumental in the establishment of the academic and administrative structure of the university, and lectured in pure and applied mathematics, also giving practical demonstrations in physics. For the next ten years the average number of students enrolled in the Bachelor of Arts course at Adelaide was fewer than twelve; though Lamb also gave some public lectures in the evenings, his workload was relatively light. His deftly rendered and original \"A Treatise on the Mathematical Theory of the Motions of Fluids\" (which would later be reprinted as \"Hydrodynamics\" in 1895) was first published in 1878.\n\nIn 1883, Lamb published a paper in the \"Philosophical Transactions of the Royal Society\" applying Maxwell's equations to the problem of oscillatory current flow in spherical conductors, an early examination of what was later to be known as the skin effect. Lamb was elected a Fellow of the Royal Society in 1884.\n\nLamb was appointed to the Chair of Mathematics at Owens College, Manchester, in 1885 and which became the Beyer Chair in 1888, a position Lamb held until retirement in 1920 (Owens College was merged with the Victoria University of Manchester in 1904). His \"Hydrodynamics\" appeared in 1895 (6th ed. 1933), and other works included \"An Elementary Course of Infinitesimal Calculus\" (1897, 3rd ed. 1919), \"Propagation of Tremors over the Surface of an Elastic Solid\" (1904), \"The Dynamical Theory of Sound\" (1910, 2nd ed. 1925), \"Statics\" (1912, 3rd ed. 1928), \"Dynamics\" (1914), \"Higher Mechanics\" (1920) and \"The Evolution of Mathematical Physics\" (1924).\n\nIn 1932 Lamb, in an address to the British Association for the Advancement of Science, wittily expressed on the difficulty of explaining and studying turbulence in fluids.\nHe reportedly said, \"I am an old man now, and when I die and go to heaven there are two matters on which I hope for enlightenment. One is quantum electrodynamics, and the other is the turbulent motion of fluids. And about the former I am rather optimistic.\"\n\nLamb is also known for description of special waves in thin solid layers. These are now known as Lamb waves.\n\nLamb was survived by three sons and four daughters. The sons (who included the painter Henry Lamb) were born at Adelaide, South Australia, and all became distinguished. He is buried at the Parish of the Ascension Burial Ground in Cambridge, with his wife.\n\nLamb was elected a Fellow of the Royal Society in 1884, was twice vice-president, received its Royal Medal in 1902 and, its highest honour, the Copley Medal in 1924. He was president of the London Mathematical Society 1902–1904, president of the Manchester Literary and Philosophical Society, and president of the British Association in 1925. He was knighted in 1931. A room in the Alan Turing Building at the University of Manchester is named in his honour and in 2013 the Sir Horace Lamb Chair was created at Manchester. A building at the University of Adelaide also bears his name.\n\n\n\n"}
{"id": "485424", "url": "https://en.wikipedia.org/wiki?curid=485424", "title": "Householder transformation", "text": "Householder transformation\n\nIn linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. The Householder transformation was introduced in 1958 by Alston Scott Householder.\n\nIts analogue over general inner product spaces is the Householder operator.\n\nThe reflection hyperplane can be defined by a unit vector formula_1 (a vector with length formula_2) which is orthogonal to the hyperplane. The reflection of a point formula_3 about this hyperplane is the linear transformation:\n\nwhere formula_1 is given as a column unit vector with Hermitian transpose formula_6.\n\nThe matrix constructed from this transformation can be expressed in terms of an outer product as:\n\nis known as the Householder matrix, where formula_8 is the identity matrix\n\nThe Householder matrix has the following properties:\n\nIn geometric optics, specular reflection can be expressed in terms of the Householder matrix (specular reflection#Vector formulation).\n\nHouseholder transformations are widely used in numerical linear algebra, to perform QR decompositions and is the first step of the QR algorithm. They are also widely used for tridiagonalization of symmetric matrices and for transforming non-symmetric matrices to a Hessenberg form.\n\nHouseholder reflections can be used to calculate QR decompositions by reflecting first one column of a matrix onto a multiple of a standard basis vector, calculating the transformation matrix, multiplying it with the original matrix and then recursing down the formula_26 minors of that product.\n\nThis procedure is taken from the book: Numerical Analysis, Burden and Faires, 8th Edition.\nIn the first step, to form the Householder matrix in each step we need to determine formula_27 and formula_28, which are:\n\nFrom formula_27 and formula_28, construct vector formula_1:\n\nwhere formula_34, formula_35, and \n\nThen compute:\n\nHaving found formula_39 and computed formula_40 the process is repeated for formula_41 as follows:\n\nContinuing in this manner, the tridiagonal and symmetric matrix is formed.\n\nThis example is taken from the book \"Numerical Analysis\" by Richard L. Burden (Author), J. Douglas Faires. In this example, the given matrix is transformed to the similar tridiagonal matrix A by using the Householder method.\n\nFollowing those steps in the Householder method, we have:\n\nThe first Householder matrix:\n\nUsed formula_45 to form\n\nAs we can see, the final result is a tridiagonal symmetric matrix which is similar to the original one. The process is finished after two steps.\n\nThe Householder transformation is a reflection about a hyperplane with unit normal vector formula_1, as stated earlier. An formula_48-by-formula_48 unitary transformation formula_50 satisfies formula_51. Taking the determinant (formula_48-th power of the geometric mean) and trace (proportional to arithmetic mean) of a unitary matrix reveals that its eigenvalues formula_53 have unit modulus. This can be seen directly and swiftly:\n\nSince arithmetic and geometric means are equal if the variables are constant (see inequality of arithmetic and geometric means), we establish the claim of unit modulus.\n\nFor the case of real valued unitary matrices we obtain orthogonal matrices, formula_55. It follows rather readily (see orthogonal matrix) that any orthogonal matrix can be decomposed into a product of 2 by 2 rotations, called Givens Rotations, and Householder reflections. This is appealing intuitively since multiplication of a vector by an orthogonal matrix preserves the length of that vector, and rotations and reflections exhaust the set of (real valued) geometric operations that render invariant a vector's length.\n\nThe Householder transformation was shown to have a one-to-one relationship with the canonical coset decomposition of unitary matrices defined in group theory, which can be used to parametrize unitary operators in a very efficient manner.\n\nFinally we note that a single Householder transform, unlike a solitary Givens transform, can act on all columns of a matrix, and as such exhibits the lowest computational cost for QR decomposition and tridiagonalization. The penalty for this \"computational optimality\" is, of course, that Householder operations cannot be as deeply or efficiently parallelized. As such Householder is preferred for dense matrices on sequential machines, whilst Givens is preferred on sparse matrices, and/or parallel machines.\n\n"}
{"id": "38277667", "url": "https://en.wikipedia.org/wiki?curid=38277667", "title": "Hypertopology", "text": "Hypertopology\n\nIn the mathematical branch of topology, a hyperspace (or a space equipped with a hypertopology) is a topological space, which consists of the set \"CL(X)\" of all closed subsets of another topological space \"X\", equipped with a topology so that the canonical map\n\nformula_1\n\nis a homeomorphism onto its image. As a consequence, a copy of the original space \"X\" lives inside hyperspace \"CL(X)\". \nEarly examples of hypertopology include the Hausdorff metric and Vietoris topology.\n\n\n"}
{"id": "3999498", "url": "https://en.wikipedia.org/wiki?curid=3999498", "title": "IP set", "text": "IP set\n\nIn mathematics, an IP set is a set of natural numbers which contains all finite sums of some infinite set.\n\nThe finite sums of a set \"D\" of natural numbers are all those numbers that can be obtained by adding up the elements of some finite nonempty subset of \"D\".\nThe set of all finite sums over \"D\" is often denoted as FS(\"D\"). Slightly more generally, for a sequence of natural numbers (\"n\"), one can consider the set of finite sums FS((\"n\")), consisting of the sums of all finite length subsequences of (\"n\").\n\nA set \"A\" of natural numbers is an IP set if there exists an infinite set \"D\" such that FS(\"D\") is a subset of \"A\". Equivalently, one may require that \"A\" contains all finite sums FS((\"n\")) of a sequence (\"n\").\n\nSome authors give a slightly different definition of IP sets: They require that FS(\"D\") equal \"A\" instead of just being a subset.\n\nThe term IP set was coined by Furstenberg and Weiss to abbreviate \"infinite-dimensional parallelepiped\". Serendipitously, the abbreviation IP can also be expanded to \"idempotent\" (a set is IP if and only if it is a member of an idempotent ultrafilter).\n\nIf formula_1 is an IP set and formula_2, then at least one formula_3 contains an IP set.\nThis is known as \"Hindman's theorem\" or the \"finite sums theorem\". In different terms, Hindman's theorem states that the class of IP sets is partition regular.\n\nSince the set of natural numbers itself is an IP set and partitions can also be seen as colorings, one can reformulate a special case of Hindman's theorem in more familiar terms: Suppose the natural numbers are \"colored\" with \"n\" different colors; each natural number gets one and only one of the \"n\" colors. Then there exists a color \"c\" and an infinite set \"D\" of natural numbers, all colored with \"c\", such that every finite sum over \"D\" also has color \"c\".\n\nThe Milliken–Taylor theorem is a common generalisation of Hindman's theorem and Ramsey's theorem.\n\nThe definition of being IP has been extended from subsets of the special semigroup of natural numbers with addition to subsets of semigroups and partial semigroups in general. A variant of Hindman's theorem is true for arbitrary semigroups.\n\n\n"}
{"id": "58151566", "url": "https://en.wikipedia.org/wiki?curid=58151566", "title": "InfoQ", "text": "InfoQ\n\nInformation quality (InfoQ) is the potential of a data set to achieve a specific (scientific or practical) goal using a given empirical analysis method.\n\nFormally, the definition is InfoQ = U(X,f|g) where X is the data, f the analysis method, g the goal and U the utility function. InfoQ is different from data quality and analysis quality, but is dependent on these components and on the relationship between them. \nInfoQ has been applied in a wide range of domains like healthcare, customer surveys, data science programs, advanced manufacturing and Bayesian network applications.\n\nKenett and Shmueli (2014) proposed eight dimensions to help assess InfoQ and various methods for increasing InfoQ: Data resolution, Data structure, Data integration, Temporal relevance, Chronology of data and goal, Generalization, Operationalization, Communication.\n"}
{"id": "5825279", "url": "https://en.wikipedia.org/wiki?curid=5825279", "title": "James Earl Baumgartner", "text": "James Earl Baumgartner\n\nJames Earl Baumgartner (March 23, 1943 – December 28, 2011) was an American mathematician who worked in set theory, mathematical logic and foundations, and topology.\n\nBaumgartner was born in Wichita, Kansas, began his undergraduate study at the California Institute of Technology in 1960, then transferred to the University of California, Berkeley, from which he received his PhD in 1970 from for a dissertation entitled \"Results and Independence Proofs in Combinatorial Set Theory\". His advisor was Robert Vaught. He became a professor at Dartmouth College in 1969, and spent there his entire career.\n\nOne of Baumgartner's results is the consistency of the statement that any two formula_1-dense sets of reals are order isomorphic (a set of reals is formula_1-dense if it has exactly formula_1 points in every open interval). With András Hajnal he proved the Baumgartner–Hajnal theorem, which states that the partition relation formula_4 holds for formula_5 and formula_6. He died in 2011 of a heart attack at his home in Hanover, New Hampshire.\n\nThe mathematical context in which Baumgartner worked spans Suslin's problem, Ramsey theory, uncountable order types, disjoint refinements, almost disjoint families, cardinal arithmetics, filters, ideals, and partition relations, iterated forcing and Axiom A, proper forcing and the proper forcing axiom, chromatic number of graphs, a thin very-tall superatomic Boolean algebra, closed unbounded sets, and partition relations\n\n\n"}
{"id": "25646402", "url": "https://en.wikipedia.org/wiki?curid=25646402", "title": "Jensen's covering theorem", "text": "Jensen's covering theorem\n\nIn set theory, Jensen's covering theorem states that if 0 does not exist then every uncountable set of ordinals is contained in a constructible set of the same cardinality. Informally this conclusion says that the constructible universe is close to the universe of all sets. The first proof appeared in . Silver later gave a fine structure free proof using his machines and finally gave an even simpler proof.\n\nThe converse of Jensen's covering theorem is also true: if 0 exists then the countable set of all cardinals less than ℵ cannot be covered by a constructible set of cardinality less than ℵ.\n\nIn his book \"Proper Forcing\", Shelah proved a strong form of Jensen's covering lemma.\n\n"}
{"id": "82285", "url": "https://en.wikipedia.org/wiki?curid=82285", "title": "Mathematical proof", "text": "Mathematical proof\n\nIn mathematics, a proof is an inferential argument for a mathematical statement. In the argument, other previously established statements, such as theorems, can be used. In principle, a proof can be traced back to self-evident or assumed statements, known as axioms, along with accepted rules of inference. Axioms may be treated as conditions that must be met before the statement applies. Proofs are examples of exhaustive deductive reasoning or inductive reasoning and are distinguished from empirical arguments or non-exhaustive inductive reasoning (or \"reasonable expectation\"). A proof must demonstrate that a statement is always true (occasionally by listing \"all\" possible cases and showing that it holds in each), rather than enumerate many confirmatory cases. An unproved proposition that is believed to be true is known as a conjecture.\n\nProofs employ logic but usually include some amount of natural language which usually admits some ambiguity. In fact, the vast majority of proofs in written mathematics can be considered as applications of rigorous informal logic. Purely formal proofs, written in symbolic language instead of natural language, are considered in proof theory. The distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics (in both senses of that term). The philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language.\n\nThe word \"proof\" comes from the Latin \"probare\" meaning \"to test\". Related modern words are the English \"probe\", \"probation\", and \"probability\", the Spanish \"probar\" (to smell or taste, or (lesser use) touch or test), Italian \"provare\" (to try), and the German \"probieren\" (to try). The early use of \"probity\" was in the presentation of legal evidence. A person of authority, such as a nobleman, was said to have probity, whereby the evidence was by his relative authority, which outweighed empirical testimony.\n\nPlausibility arguments using heuristic devices such as pictures and analogies preceded strict mathematical proof. It is likely that the idea of demonstrating a conclusion first arose in connection with geometry, which originally meant the same as \"land measurement\". The development of mathematical proof is primarily the product of ancient Greek mathematics, and one of the greatest achievements thereof. Thales (624–546 BCE) and Hippocrates of Chios (c470-410 BCE) proved some theorems in geometry. Eudoxus (408–355 BCE) and Theaetetus (417–369 BCE) formulated theorems but did not prove them. Aristotle (384–322 BCE) said definitions should describe the concept being defined in terms of other concepts already known. Mathematical proofs were revolutionized by Euclid (300 BCE), who introduced the axiomatic method still in use today, starting with undefined terms and axioms (propositions regarding the undefined terms assumed to be self-evidently true from the Greek \"axios\" meaning \"something worthy\"), and used these to prove theorems using deductive logic. His book, the \"Elements\", was read by anyone who was considered educated in the West until the middle of the 20th century. In addition to theorems of geometry, such as the Pythagorean theorem, the \"Elements\" also covers number theory, including a proof that the square root of two is irrational and that there are infinitely many prime numbers.\n\nFurther advances took place in medieval Islamic mathematics. While earlier Greek proofs were largely geometric demonstrations, the development of arithmetic and algebra by Islamic mathematicians allowed more general proofs that no longer depended on geometry. In the 10th century CE, the Iraqi mathematician Al-Hashimi provided general proofs for numbers (rather than geometric demonstrations) as he considered multiplication, division, etc. for \"lines.\" He used this method to provide a proof of the existence of irrational numbers. An inductive proof for arithmetic sequences was introduced in the \"Al-Fakhri\" (1000) by Al-Karaji, who used it to prove the binomial theorem and properties of Pascal's triangle. Alhazen also developed the method of proof by contradiction, as the first attempt at proving the Euclidean parallel postulate.\n\nModern proof theory treats proofs as inductively defined data structures. There is no longer an assumption that axioms are \"true\" in any sense; this allows for parallel mathematical theories built on alternate sets of axioms (see Axiomatic set theory and Non-Euclidean geometry for examples).\n\nAs practiced, a proof is expressed in natural language and is a rigorous argument intended to convince the audience of the truth of a statement. The standard of rigor is not absolute and has varied throughout history. A proof can be presented differently depending on the intended audience. In order to gain acceptance, a proof has to meet communal statements of rigor; an argument considered vague or incomplete may be rejected.\n\nThe concept of a proof is formalized in the field of mathematical logic. A formal proof is written in a formal language instead of a natural language. A formal proof is defined as sequence of formulas in a formal language, in which each formula is a logical consequence of preceding formulas. Having a definition of formal proof makes the concept of proof amenable to study. Indeed, the field of proof theory studies formal proofs and their properties, for example, the property that a statement has a formal proof. An application of proof theory is to show that certain undecidable statements are not provable.\n\nThe definition of a formal proof is intended to capture the concept of proofs as written in the practice of mathematics. The soundness of this definition amounts to the belief that a published proof can, in principle, be converted into a formal proof. However, outside the field of automated proof assistants, this is rarely done in practice. A classic question in philosophy asks whether mathematical proofs are analytic or synthetic. Kant, who introduced the analytic-synthetic distinction, believed mathematical proofs are synthetic. \n\nProofs may be viewed as aesthetic objects, admired for their mathematical beauty. The mathematician Paul Erdős was known for describing proofs he found particularly elegant as coming from \"The Book\", a hypothetical tome containing the most beautiful method(s) of proving each theorem. The book \"Proofs from THE BOOK\", published in 2003, is devoted to presenting 32 proofs its editors find particularly pleasing.\n\nIn direct proof, the conclusion is established by logically combining the axioms, definitions, and earlier theorems. For example, direct proof can be used to establish that the sum of two even integers is always even:\n\nThis proof uses the definition of even integers, the integer properties of closure under addition and multiplication, and distributivity.\n\nDespite its name, mathematical induction is a method of deduction, not a form of inductive reasoning. In proof by mathematical induction, a single \"base case\" is proved, and an \"induction rule\" is proved that establishes that any arbitrary case implies the next case. Since in principle the induction rule can be applied repeatedly starting from the proved base case, we see that all (usually infinitely many) cases are provable. This avoids having to prove each case individually. A variant of mathematical induction is proof by infinite descent, which can be used, for example, to prove the irrationality of the square root of two.\n\nA common application of proof by mathematical induction is to prove that a property known to hold for one number holds for all natural numbers:\nLet } be the set of natural numbers, and be a mathematical statement involving the natural number belonging to such that\n\nFor example, we can prove by induction that all positive integers of the form are odd. Let represent \" is odd\":\n\nThe shorter phrase \"proof by induction\" is often used instead of \"proof by mathematical induction\".\n\nProof by contraposition infers the conclusion \"if \"p\" then \"q\"\" from the premise \"if \"not q\" then \"not p\"\". The statement \"if \"not q\" then \"not p\"\" is called the contrapositive of the statement \"if \"p\" then \"q\"\". For example, contraposition can be used to establish that, given an integer formula_1, if formula_2 is even, then formula_1 is even:\n\nIn proof by contradiction (also known as \"reductio ad absurdum\", Latin for \"by reduction to the absurd\"), it is shown that if some statement were true, a logical contradiction occurs, hence the statement must be false. A famous example of proof by contradiction shows that formula_10 is an irrational number:\n\nProof by construction, or proof by example, is the construction of a concrete example with a property to show that something having that property exists. Joseph Liouville, for instance, proved the existence of transcendental numbers by constructing an explicit example. It can also be used to construct a counterexample to disprove a proposition that all elements have a certain property.\n\nIn proof by exhaustion, the conclusion is established by dividing it into a finite number of cases and proving each one separately. The number of cases sometimes can become very large. For example, the first proof of the four color theorem was a proof by exhaustion with 1,936 cases. This proof was controversial because the majority of the cases were checked by a computer program, not by hand. The shortest known proof of the four color theorem still has over 600 cases.\n\nA probabilistic proof is one in which an example is shown to exist, with certainty, by using methods of probability theory. Probabilistic proof, like proof by construction, is one of many ways to show existence theorems.\n\nThis is not to be confused with an argument that a theorem is 'probably' true, a 'plausibility argument'. The work on the Collatz conjecture shows how far plausibility is from genuine proof.\n\nWhile most mathematicians do not think that probabilistic evidence ever counts as a genuine mathematical proof, a few mathematicians and philosophers have argued that at least some types of probabilistic evidence (such as Rabin's probabilistic algorithm for testing primality) are as good as genuine mathematical proofs. \n\nA combinatorial proof establishes the equivalence of different expressions by showing that they count the same object in different ways. Often a bijection between two sets is used to show that the expressions for their two sizes are equal. Alternatively, a double counting argument provides two different expressions for the size of a single set, again showing that the two expressions are equal.\n\nA nonconstructive proof establishes that a mathematical object with a certain property exists without explaining how such an object can be found. Often, this takes the form of a proof by contradiction in which the nonexistence of the object is proved to be impossible. In contrast, a constructive proof establishes that a particular object exists by providing a method of finding it. A famous example of a\nnonconstructive proof shows that there exist two irrational numbers \"a\" and \"b\" such that formula_15 is a rational number:\n\nThe expression \"statistical proof\" may be used technically or colloquially in areas of pure mathematics, such as involving cryptography, chaotic series, and probabilistic or analytic number theory. It is less commonly used to refer to a mathematical proof in the branch of mathematics known as mathematical statistics. See also \"Statistical proof using data\" section below.\n\nUntil the twentieth century it was assumed that any proof could, in principle, be checked by a competent mathematician to confirm its validity. However, computers are now used both to prove theorems and to carry out calculations that are too long for any human or team of humans to check; the first proof of the four color theorem is an example of a computer-assisted proof. Some mathematicians are concerned that the possibility of an error in a computer program or a run-time error in its calculations calls the validity of such computer-assisted proofs into question. In practice, the chances of an error invalidating a computer-assisted proof can be reduced by incorporating redundancy and self-checks into calculations, and by developing multiple independent approaches and programs. Errors can never be completely ruled out in case of verification of a proof by humans either, especially if the proof contains natural language and requires deep mathematical insight.\n\nA statement that is neither provable nor disprovable from a set of axioms is called undecidable (from those axioms). One example is the parallel postulate, which is neither provable nor refutable from the remaining axioms of Euclidean geometry.\n\nMathematicians have shown there are many statements that are neither provable nor disprovable in Zermelo-Fraenkel set theory with the axiom of choice (ZFC), the standard system of set theory in mathematics (assuming that ZFC is consistent); see list of statements undecidable in ZFC.\n\nGödel's (first) incompleteness theorem shows that many axiom systems of mathematical interest will have undecidable statements.\n\nWhile early mathematicians such as Eudoxus of Cnidus did not use proofs, from Euclid to the foundational mathematics developments of the late 19th and 20th centuries, proofs were an essential part of mathematics. With the increase in computing power in the 1960s, significant work began to be done investigating mathematical objects outside of the proof-theorem framework, in experimental mathematics. Early pioneers of these methods intended the work ultimately to be embedded in a classical proof-theorem framework, e.g. the early development of fractal geometry, which was ultimately so embedded.\n\nAlthough not a formal proof, a visual demonstration of a mathematical theorem is sometimes called a \"proof without words\". The left-hand picture below is an example of a historic visual proof of the Pythagorean theorem in the case of the (3,4,5) triangle.\nSome illusory visual proofs, such as the missing square puzzle, can be constructed in a way which appear to prove a supposed mathematical fact but only do so under the presence of tiny errors (for example, supposedly straight lines which actually bend slightly) which are unnoticeable until the entire picture is closely examined, with lengths and angles precisely measured or calculated.\n\nAn elementary proof is a proof which only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using \"higher\" mathematics. However, over time, many of these results have been reproved using only elementary techniques.\n\nA particular way of organising a proof using two parallel columns is often used in elementary geometry classes in the United States. The proof is written as a series of lines in two columns. In each line, the left-hand column contains a proposition, while the right-hand column contains a brief explanation of how the corresponding proposition in the left-hand column is either an axiom, a hypothesis, or can be logically derived from previous propositions. The left-hand column is typically headed \"Statements\" and the right-hand column is typically headed \"Reasons\".\n\nThe expression \"mathematical proof\" is used by lay people to refer to using mathematical methods or arguing with mathematical objects, such as numbers, to demonstrate something about everyday life, or when data used in an argument is numerical. It is sometimes also used to mean a \"statistical proof\" (below), especially when used to argue from data.\n\n\"Statistical proof\" from data refers to the application of statistics, data analysis, or Bayesian analysis to infer propositions regarding the probability of data. While \"using\" mathematical proof to establish theorems in statistics, it is usually not a mathematical proof in that the \"assumptions\" from which probability statements are derived require empirical evidence from outside mathematics to verify. In physics, in addition to statistical methods, \"statistical proof\" can refer to the specialized \"mathematical methods of physics\" applied to analyze data in a particle physics experiment or observational study in physical cosmology. \"Statistical proof\" may also refer to raw data or a convincing diagram involving data, such as scatter plots, when the data or diagram is adequately convincing without further analysis.\n\nProofs using inductive logic, while considered mathematical in nature, seek to establish propositions with a degree of certainty, which acts in a similar manner to probability, and may be less than full certainty. Inductive logic should not be confused with mathematical induction.\n\nBayesian analysis uses Bayes' theorem to update a person's assessment of likelihoods of hypotheses when new evidence or information is acquired.\n\nPsychologism views mathematical proofs as psychological or mental objects. Mathematician philosophers, such as Leibniz, Frege, and Carnap have variously criticized this view and attempted to develop a semantics for what they considered to be the language of thought, whereby standards of mathematical proof might be applied to empirical science.\n\nPhilosopher-mathematicians such as Spinoza have attempted to formulate philosophical arguments in an axiomatic manner, whereby mathematical proof standards could be applied to argumentation in general philosophy. Other mathematician-philosophers have tried to use standards of mathematical proof and reason, without empiricism, to arrive at statements outside of mathematics, but having the certainty of propositions deduced in a mathematical proof, such as Descartes' \"cogito\" argument.\n\nSometimes, the abbreviation \"Q.E.D.\" is written to indicate the end of a proof. This abbreviation stands for \"Quod Erat Demonstrandum\", which is Latin for \"that which was to be demonstrated\". A more common alternative is to use a square or a rectangle, such as □ or ∎, known as a \"tombstone\" or \"halmos\" after its eponym Paul Halmos. Often, \"which was to be shown\" is verbally stated when writing \"QED\", \"□\", or \"∎\" during an oral presentation.\n\n\n"}
{"id": "3983499", "url": "https://en.wikipedia.org/wiki?curid=3983499", "title": "Mathematics of Computation", "text": "Mathematics of Computation\n\nMathematics of Computation is a bimonthly mathematics journal focused on computational mathematics. It was established in 1943 as \"Mathematical Tables and other Aids to Computation\", obtaining its current name in 1960. Articles older than five years are available electronically free of charge.\n\nThe journal is abstracted and indexed in Mathematical Reviews, Zentralblatt MATH, Science Citation Index, CompuMath Citation Index, and Current Contents/Physical, Chemical & Earth Sciences. According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 1.409.\n"}
{"id": "48624090", "url": "https://en.wikipedia.org/wiki?curid=48624090", "title": "Mireille Bousquet-Mélou", "text": "Mireille Bousquet-Mélou\n\nMireille Bousquet-Mélou (born May 12, 1967) is a French mathematician who specializes in enumerative combinatorics and who works as a senior researcher for the Centre national de la recherche scientifique (CNRS) at the computer science department (LaBRI) of the University of Bordeaux.\n\nBousquet-Mélou was born in Albi, the second daughter of two high school teachers, and grew up in Pau where her family moved when she was three.\nShe studied at the École Normale Supérieure in Paris from 1986 to 1990, as the only woman in her entering class of mathematicians, and earned an agrégation in mathematics in 1989, with Xavier Gérard Viennot as her mentor in combinatorics. She completed her Ph.D. at the University of Bordeaux in 1991, with a dissertation on the enumeration of orthogonally convex polyominos supervised by Viennot. She joined CNRS as a junior researcher in 1990, and completed a habilitation at Bordeaux in 1996.\n\nBousquet-Mélou won the bronze medal of the CNRS in 1993, and the silver medal in 2014. Linköping University gave her an honorary doctorate in 2005, and the French Academy of Sciences gave her their Charles-Louis de Saulces de Freycinet Prize in 2009. In 2006, she was an invited speaker at the International Congress of Mathematicians in the section on combinatorics.\nHer presentation at the congress concerned connections between enumerative combinatorics, formal language theory, and the algebraic structure of generating functions, according to which enumeration problems whose generating functions are rational functions are often isomorphic to regular languages, and problems whose generating functions are algebraic are often isomorphic to unambiguous context-free languages.\n\n\n"}
{"id": "222367", "url": "https://en.wikipedia.org/wiki?curid=222367", "title": "Multivalued function", "text": "Multivalued function\n\nIn mathematics, a multivalued function from a domain to a codomain is a heterogeneous relation. However, in some contexts such as the complex plane (\"X\" = \"Y\" = ℂ), authors prefer to mimic function theory as they extend concepts of the ordinary (single-valued) functions.\n\nIn this context, an ordinary function is often called a single-valued function to avoid confusion.\n\nThe term \"multivalued function\" originated in complex analysis, from analytic continuation. It often occurs that one knows the value of a complex analytic function formula_1 in some neighbourhood of a point formula_2. This is the case for functions defined by the implicit function theorem or by a Taylor series around formula_2. In such a situation, one may extend the domain of the single-valued function formula_1 along curves in the complex plane starting at formula_5. In doing so, one finds that the value of the extended function at a point formula_6 depends on the chosen curve from formula_5 to formula_8; since none of the new values is more natural than the others, all of them are incorporated into a multivalued function. For example, let formula_9 be the usual square root function on positive real numbers. One may extend its domain to a neighbourhood of formula_10 in the complex plane, and then further along curves starting at formula_10, so that the values along a given curve vary continuously from formula_12. Extending to negative real numbers, one gets two opposite values of the square root such as formula_13, depending on whether the domain has been extended through the upper or the lower half of the complex plane. This phenomenon is very frequent, occurring for th roots, logarithms and inverse trigonometric functions.\n\nTo define a single-valued function from a complex multivalued function, one may distinguish one of the multiple values as the principal value, producing a single-valued function on the whole plane which is discontinuous along certain boundary curves. Alternatively, dealing with the multivalued function allows having something that is everywhere continuous, at the cost of possible value changes when one follows a closed path (monodromy). These problems are resolved in the theory of Riemann surfaces: to consider a multivalued function formula_1 as an ordinary function without discarding any values, one multiplies the domain into a many-layered covering space, a manifold which is the Riemann surface associated to formula_1.\n\n\nThese are all examples of multivalued functions that come about from non-injective functions. Since the original functions do not preserve all the information of their inputs, they are not reversible. Often, the restriction of a multivalued function is a partial inverse of the original function.\n\nMultivalued functions of a complex variable have branch points. For example, for the \"n\"th root and logarithm functions, 0 is a branch point; for the arctangent function, the imaginary units \"i\" and −\"i\" are branch points. Using the branch points, these functions may be redefined to be single-valued functions, by restricting the range. A suitable interval may be found through use of a branch cut, a kind of curve that connects pairs of branch points, thus reducing the multilayered Riemann surface of the function to a single layer. As in the case with real functions, the restricted range may be called the \"principal branch\" of the function.\n\nSet-valued analysis is the study of sets in the spirit of mathematical analysis and general topology.\n\nInstead of considering collections of only points, set-valued analysis considers collections of sets. If a collection of sets is endowed with a topology, or inherits an appropriate topology from an underlying topological space, then the convergence of sets can be studied.\n\nMuch of set-valued analysis arose through the study of mathematical economics and optimal control, partly as a generalization of convex analysis; the term \"variational analysis\" is used by authors such as R. T. Rockafellar and Roger Wets, Jon Borwein and Adrian Lewis, and Boris Mordukhovich. In optimization theory, the convergence of approximating subdifferentials to a subdifferential is important in understanding necessary or sufficient conditions for any minimizing point.\n\nThere exist set-valued extensions of the following concepts from point-valued analysis: continuity, differentiation, integration, implicit function theorem, contraction mappings, measure theory, fixed-point theorems, optimization, and topological degree theory.\n\nEquations are generalized to inclusions.\n\nOne can differentiate many continuity concepts, primarily closed graph property and upper and lower hemicontinuity. (One should be warned that often the terms upper and lower semicontinuous are used instead of upper and lower hemicontinuous reserved for the case of weak topology in domain; yet we arrive at the collision with the reserved names for upper and lower semicontinuous real-valued function). There exist also various definitions for measurability of multifunction.\n\nThe practice of allowing \"function\" in mathematics to mean also \"multivalued function\" dropped out of usage at some point in the first half of the twentieth century. Some evolution can be seen in different editions of \"A Course of Pure Mathematics\" by G. H. Hardy, for example. It probably persisted longest in the theory of special functions, for its occasional convenience.\n\nThe theory of multivalued functions was fairly systematically developed for the first time \nin Claude Berge's \"Topological spaces\" (1963).\n\nMultifunctions arise in optimal control theory, especially differential inclusions and related subjects as game theory, where the Kakutani fixed point theorem for multifunctions has been applied to prove existence of Nash equilibria (note: in the context of game theory, a multivalued function is usually referred to as a correspondence.) This among many other properties loosely associated with approximability of upper hemicontinuous multifunctions via continuous functions explains why upper hemicontinuity is more preferred than lower hemicontinuity.\n\nNevertheless, lower semicontinuous multifunctions usually possess continuous selections as stated in the Michael selection theorem, which provides another characterisation of paracompact spaces. Other selection theorems, like Bressan-Colombo directional continuous selection, Kuratowski and Ryll-Nardzewski measurable selection theorem, Aumann measurable selection, Fryszkowski selection for decomposable maps are important in optimal control and the theory of differential inclusions.\n\nIn physics, multivalued functions play an increasingly important role. They form the mathematical basis for Dirac's magnetic monopoles, for the theory of defects in crystals and the resulting plasticity of materials, for vortices in superfluids and superconductors, and for phase transitions in these systems, for instance melting and quark confinement. They are the origin of gauge field structures in many branches of physics.\n\n\n\n"}
{"id": "5643937", "url": "https://en.wikipedia.org/wiki?curid=5643937", "title": "Music and mathematics", "text": "Music and mathematics\n\nMusic theory has no axiomatic foundation in modern mathematics, yet the basis of musical sound can be described mathematically (in acoustics) and exhibits \"a remarkable array of number properties\". Elements of music such as its form, rhythm and metre, the pitches of its notes and the tempo of its pulse can be related to the measurement of time and frequency, offering ready analogies in geometry. \n\nThe attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work.\n\nThough ancient Chinese, Indians, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios, particularly the ratios of small integers. Their central doctrine was that \"all nature consists of harmony arising out of numbers\".\n\nFrom the time of Plato, harmony was considered a fundamental branch of physics, now known as musical acoustics. Early Indian and Chinese theorists show similar approaches: all sought to show that the mathematical laws of harmonics and rhythms were fundamental not only to our understanding of the world but to human well-being. Confucius, like Pythagoras, regarded the small numbers 1,2,3,4 as the source of all perfection.\n\nWithout the boundaries of rhythmic structure – a fundamental equal and regular arrangement of pulse repetition, accent, phrase and duration – music would not be possible. Modern musical use of terms like meter and measure also reflects the historical importance of music, along with astronomy, in the development of counting, arithmetic and the exact measurement of time and periodicity that is fundamental to physics.\n\nThe elements of musical form often build strict proportions or hypermetric structures (powers of the numbers 2 and 3).\n\nMusical form is the plan by which a short piece of music is extended. The term \"plan\" is also used in architecture, to which musical form is often compared. Like the architect, the composer must take into account the function for which the work is intended and the means available, practicing economy and making use of repetition and order. The common types of form known as binary and ternary (\"twofold\" and \"threefold\") once again demonstrate the importance of small integral values to the intelligibility and appeal of music.\n\nA musical scale is a discrete set of pitches used in making or describing music. The most important scale in the Western tradition is the diatonic scale but many others have been used and proposed in various historical eras and parts of the world. Each pitch corresponds to a particular frequency, expressed in hertz (Hz), sometimes referred to as cycles per second (c.p.s.). A scale has an interval of repetition, normally the octave. The octave of any pitch refers to a frequency exactly twice that of the given pitch.\n\nSucceeding superoctaves are pitches found at frequencies four, eight, sixteen times, and so on, of the fundamental frequency. Pitches at frequencies of half, a quarter, an eighth and so on of the fundamental are called suboctaves. There is no case in musical harmony where, if a given pitch be considered accordant, that its octaves are considered otherwise. Therefore, any note and its octaves will generally be found similarly named in musical systems (e.g. all will be called doh or A or Sa, as the case may be).\n\nWhen expressed as a frequency bandwidth an octave A–A spans from 110 Hz to 220 Hz (span=110 Hz). The next octave will span from 220 Hz to 440 Hz (span=220 Hz). The third octave spans from 440 Hz to 880 Hz (span=440 Hz) and so on. Each successive octave spans twice the frequency range of the previous octave.\n\nBecause we are often interested in the relations or ratios between the pitches (known as intervals) rather than the precise pitches themselves in describing a scale, it is usual to refer to all the scale pitches in terms of their ratio from a particular pitch, which is given the value of one (often written 1/1), generally a note which functions as the tonic of the scale. For interval size comparison, cents are often used.\n\nThere are two main families of tuning systems: equal temperament and just tuning. Equal temperament scales are built by dividing an octave into intervals which are equal on a logarithmic scale, which results in perfectly evenly divided scales, but with ratios of frequencies which are irrational numbers. Just scales are built by multiplying frequencies by rational numbers, which results in simple ratios between frequencies, but with scale divisions that are uneven.\n\nOne major difference between equal temperament tunings and just tunings is differences in acoustical beat when two notes are sounded together, which affects the subjective experience of consonance and dissonance. Both of these systems, and the vast majority of music in general, have scales that repeat on the interval of every octave, which is defined as frequency ratio of 2:1. In other words, every time the frequency is doubled, the given scale repeats.\n\nBelow are Ogg Vorbis files demonstrating the difference between just intonation and equal temperament. You may need to play the samples several times before you can pick the difference.\n\n5-limit tuning, the most common form of just intonation, is a system of tuning using tones that are regular number harmonics of a single fundamental frequency. This was one of the scales Johannes Kepler presented in his Harmonices Mundi (1619) in connection with planetary motion. The same scale was given in transposed form by Scottish mathematician and musical theorist, Alexander Malcolm, in 1721 in his 'Treatise of Musick: Speculative, Practical and Historical', and by theorist Jose Wuerschmidt in the 20th century. A form of it is used in the music of northern India.\n\nAmerican composer Terry Riley also made use of the inverted form of it in his \"Harp of New Albion\". Just intonation gives superior results when there is little or no chord progression: voices and other instruments gravitate to just intonation whenever possible. However, it gives two different whole tone intervals (9:8 and 10:9) because a fixed tuned instrument, such as a piano, cannot change key. To calculate the frequency of a note in a scale given in terms of ratios, the frequency ratio is multiplied by the tonic frequency. For instance, with a tonic of A4 (A natural above middle C), the frequency is 440 Hz, and a justly tuned fifth above it (E5) is simply 440×(3:2) = 660 Hz.\n\nPythagorean tuning is tuning based only on the perfect consonances, the (perfect) octave, perfect fifth, and perfect fourth. Thus the major third is considered not a third but a ditone, literally \"two tones\", and is (9:8) = 81:64, rather than the independent and harmonic just 5:4 = 80:64 directly below. A whole tone is a secondary interval, being derived from two perfect fifths, (3:2) = 9:8.\n\nThe just major third, 5:4 and minor third, 6:5, are a syntonic comma, 81:80, apart from their Pythagorean equivalents 81:64 and 32:27 respectively. According to Carl , \"the dependent third conforms to the Pythagorean, the independent third to the harmonic tuning of intervals.\"\n\nWestern common practice music usually cannot be played in just intonation but requires a systematically tempered scale. The tempering can involve either the irregularities of well temperament or be constructed as a regular temperament, either some form of equal temperament or some other regular meantone, but in all cases will involve the fundamental features of meantone temperament. For example, the root of chord ii, if tuned to a fifth above the dominant, would be a major whole tone (9:8) above the tonic. If tuned a just minor third (6:5) below a just subdominant degree of 4:3, however, the interval from the tonic would equal a minor whole tone (10:9). Meantone temperament reduces the difference between 9:8 and 10:9. Their ratio, (9:8)/(10:9) = 81:80, is treated as a unison. The interval 81:80, called the syntonic comma or comma of Didymus, is the key comma of meantone temperament.\n\nIn equal temperament, the octave is divided into equal parts on the logarithmic scale. While it is possible to construct equal temperament scale with any number of notes (for example, the 24-tone Arab tone system), the most common number is 12, which makes up the equal-temperament chromatic scale. In western music, a division into twelve intervals is commonly assumed unless it is specified otherwise.\n\nFor the chromatic scale, the octave is divided into twelve equal parts, each semitone (half-step) is an interval of the twelfth root of two so that twelve of these equal half steps add up to exactly an octave. With fretted instruments it is very useful to use equal temperament so that the frets align evenly across the strings. In the European music tradition, equal temperament was used for lute and guitar music far earlier than for other instruments, such as musical keyboards. Because of this historical force, twelve-tone equal temperament is now the dominant intonation system in the Western, and much of the non-Western, world.\n\nEqually tempered scales have been used and instruments built using various other numbers of equal intervals. The 19 equal temperament, first proposed and used by Guillaume Costeley in the 16th century, uses 19 equally spaced tones, offering better major thirds and far better minor thirds than normal 12-semitone equal temperament at the cost of a flatter fifth. The overall effect is one of greater consonance. Twenty-four equal temperament, with twenty-four equally spaced tones, is widespread in the pedagogy and notation of Arabic music. However, in theory and practice, the intonation of Arabic music conforms to rational ratios, as opposed to the irrational ratios of equally tempered systems.\n\nWhile any analog to the equally tempered quarter tone is entirely absent from Arabic intonation systems, analogs to a three-quarter tone, or neutral second, frequently occur. These neutral seconds, however, vary slightly in their ratios dependent on maqam, as well as geography. Indeed, Arabic music historian Habib Hassan Touma has written that \"the breadth of deviation of this musical step is a crucial ingredient in the peculiar flavor of Arabian music. To temper the scale by dividing the octave into twenty-four quarter-tones of equal size would be to surrender one of the most characteristic elements of this musical culture.\"\n\nMusical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set.\n\nExpanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.\n\nTransformational theory is a branch of music theory developed by David Lewin. The theory allows for great generality because it emphasizes transformations between musical objects, rather than the musical objects themselves.\n\nTheorists have also proposed musical applications of more sophisticated algebraic concepts. The theory of regular temperaments has been extensively developed with a wide range of sophisticated mathematics, for example by associating each regular temperament with a rational point on a Grassmannian.\n\nThe chromatic scale has a free and transitive action of the cyclic group formula_1, with the action being defined via transposition of notes. So the chromatic scale can be thought of as a torsor for the group formula_1.\n\nReal and complex analysis have also been made use of, for instance by applying the theory of the Riemann zeta function to the study of equal divisions of the octave.\n\n\n"}
{"id": "5351448", "url": "https://en.wikipedia.org/wiki?curid=5351448", "title": "Myerson–Satterthwaite theorem", "text": "Myerson–Satterthwaite theorem\n\nThe Myerson–Satterthwaite theorem is an important result in mechanism design and the economics of asymmetric information, due to Roger Myerson and Mark Satterthwaite. Informally, the result says that there is no efficient way for two parties to trade a good when they each have secret and probabilistically varying valuations for it, without the risk of forcing one party to trade at a loss.\n\nThe Myerson–Satterthwaite theorem is among the most remarkable and universally applicable negative results in economics—a kind of negative mirror to the fundamental theorems of welfare economics. It is, however, much less famous than those results or Arrow's earlier result on the impossibility of satisfactory electoral systems.\n\nThere are two agents: Sally (the seller) and Bob (the buyer). Sally holds an item that is valuable for both her and Bob. Each agent values the item differently: Bob values it as formula_1 and Sally as formula_2. Each agent knows his/her own valuation with certainty, but knows the valuation of the other agent only probabilistically:\n\nA \"direct bargaining mechanism\" is a mechanism which asks each agent to report his/her valuation of the item, then decides whether the item will be traded and at what price. Formally, it is represented by two functions:\nNote that, thanks to the revelation principle, the assumption that the mechanism is direct does not lose generality.\n\nEvery agent knows his value and knows the mechanism. Hence, every agent can calculate his expected gain from the trade. Since we are interested in mechanisms which are truthful in equilibrium, we assume that each agent assumes that the other agent is truthful. Hence:\n\nMyerson and Satterthwaite study the following requirements that an ideal mechanism should satisfy (see also Double auction#requirements):\n\n1. ex-ante Individual Rationality (IR): The expected value of both Bob and Sally should be non-negative (so that they have an initial incentive to participate). Formally: formula_14 and formula_15.\n\n2. Weak Balanced Budget (WBB): The auctioneer should not have to bring money from home in order to subsidize the trade.\n\n3. Nash equilibrium Incentive compatibility (NEIC): for every agent, if the other agent reports the true value, then the best response is to report the true value too. Formally: formula_16 and formula_17.\n\n4. ex-post Pareto efficiency (PE): the item should be finally given to the agent who values it the most. Formally: formula_18 if formula_19 and formula_20 if formula_21.\n\nIf the following two assumptions are true:\n\nthen, there is no mechanism which satisfies the four properties mentioned above (IR, WBB, NEIC and PE).\n\nVarious variants of the Myerson–Satterthwaite setting have been studied.\n\n1. Myerson and Satterthwaite considered a single buyer and a single seller. When there are many buyers and sellers, the inefficiency asymptotically disappears.\nHowever, this is only true in the case of private goods; in the case of public goods the inefficiency is aggravated when the number of agents becomes large. \n\n2. Myerson and Satterthwaite considered an asymmetric initial situation, in the sense that at the outset one party has 100% of the good and the other party has 0% of the good. It has been shown that ex post efficiency can be attained if initially both parties own 50% of the good to be traded. \n\n3. The latter result has been extended to settings in which the parties can make unobservable ex ante investments in order to increase their own valuations. Yet, ex post efficiency cannot be achieved if the seller's unobservable investment increases the buyer's valuation, even if only the buyer has private information about his or her valuation. \n\n4. Another impossibility result where only one party has private information about its valuation can be shown to hold when the outside option payoffs are not exogenously given.\n\n"}
{"id": "30849196", "url": "https://en.wikipedia.org/wiki?curid=30849196", "title": "Numa Edward Hartog", "text": "Numa Edward Hartog\n\nNuma Edward Hartog (May 20, 1846 – June 19, 1871) was a Jewish British mathematician who attracted attention in 1869 for graduating from Cambridge University as Senior Wrangler and Smith's Prizeman but as a Jew had not been admitted to a fellowship. Hartog's case led to the passage of the Universities Tests Act of 1871, which removed religious barriers to holding fellowships at Oxford and Cambridge.\n\nHartog was born in London on 20 May 1846 to Alfonse Hartog and Marion Moss. He was the elder brother of Héléna, Marcus, and Philip Hartog, and the cousin of Henri Bergson.\n\nIn his earlier academic career, he attended University College School in London, and then University College London. At Cambridge, he attended Trinity College. He was a trailblazing figure in overcoming religious obstacles to academic achievement in the UK. For example, when his B.A. was awarded, the words \"In nomine Patris et Filii et Spiritus Sancti\" were omitted. However, he was unable to accept a fellowship due to being unable to subscribe to the required test on account of his religion.\n\nWithin weeks, Solicitor-General John Coleridge of the Gladstone government introduced legislation to rectify the situation. The House of Lords twice rejected bills passed by the House of Commons before finally accepting the Universities Tests Act of 1871; Hartog's testimony before the Lords helped secure its passage.\n\nHe was a member of the Council of Jews' College and an Honorary Secretary of the Society of Hebrew Literature.\n\nHartog died of smallpox at the age of only 25.\n\n"}
{"id": "17565856", "url": "https://en.wikipedia.org/wiki?curid=17565856", "title": "Parthasarathy's theorem", "text": "Parthasarathy's theorem\n\nIn mathematics -- and in particular the study of games on the unit square -- Parthasarathy's theorem is a generalization of Von Neumann's minimax theorem. It states that a particular class of games has a mixed value, provided that at least one of the players has a strategy that is restricted to absolutely continuous distributions with respect to the Lebesgue measure (in other words, one of the players is forbidden to use a pure strategy).\n\nThe theorem is attributed to the Indian mathematician Thiruvenkatachari Parthasarathy.\n\nLet formula_1 and formula_2 stand for the unit interval formula_3; formula_4 denote the set of probability distributions on formula_1 (with formula_6 defined similarly); and formula_7 denote the set of absolutely continuous distributions on formula_1 (with formula_9 defined similarly).\n\nSuppose that formula_10 is bounded on the unit square formula_11 and that formula_10 is continuous except possibly on a number of curves of the form formula_13 (with formula_14) where the formula_15 are continuous functions. For formula_16, define\n\nThen\n\nThis is equivalent to the statement that the game induced by formula_19 has a value. Note that one player (WLOG formula_2) is forbidden from using a pure strategy.\n\nParthasarathy goes on to exhibit a game in which\n\nwhich thus has no value. There is no contradiction because in this case neither player is restricted to absolutely continuous distributions (and the demonstration that the game has no value requires both players to use pure strategies).\n\n"}
{"id": "1377241", "url": "https://en.wikipedia.org/wiki?curid=1377241", "title": "Phragmén–Lindelöf principle", "text": "Phragmén–Lindelöf principle\n\nIn complex analysis, the Phragmén–Lindelöf principle (or method), first formulated by Lars Edvard Phragmén (1863–1937) and Ernst Leonard Lindelöf (1870-1946) in 1908, is a technique which employs an auxiliary, parameterized function to prove the boundedness of a holomorphic function formula_1 (i.e, formula_2) on an unbounded domain formula_3 when an additional (usually mild) condition constraining the growth of formula_4 on formula_3 is given. It is a generalization of the maximum modulus principle, which is only applicable to bounded domains.\n\nIn the theory of complex functions, it is known that the modulus (absolute value) of a holomorphic (complex differentiable) function in the interior of a \"bounded\" region is bounded by its modulus on the boundary of the region. More precisely, if a non-constant function formula_6 is holomorphic in a bounded region formula_3 and continuous on its closure formula_8, then formula_9 for all formula_10. This is known as the \"maximum modulus principle.\" (In fact, since formula_11 is compact and formula_4 is continuous, there actually exists some formula_13 such that formula_14.) The maximum modulus principle is generally used to conclude that a holomorphic function is bounded in a region after showing that it is bounded on its boundary.\n\nHowever, the maximum modulus principle cannot be applied to an unbounded region of the complex plane. As a concrete example, let us examine the behavior of the holomorphic function formula_15 in the unbounded strip\n\nAlthough formula_17, so that formula_4 is bounded on boundary formula_19, formula_4 grows rapidly without bound when formula_21 along the positive real axis. The difficulty here stems from the extremely fast growth of formula_4 along the positive real axis. If the growth rate of formula_4 is guaranteed to not be \"too fast,\" as specified by an appropriate growth condition, the \"Phragmén–Lindelöf principle\" can be applied to show that boundedness of formula_1 on the region's boundary implies that formula_1 is in fact bounded in the whole region, effectively extending the maximum modulus principle to unbounded regions.\n\n\"The technique:\" Suppose we are given a holomorphic function formula_1 and an unbounded region formula_27. In a typical Phragmén–Lindelöf argument, we introduce a certain multiplicative factor formula_28 satisfying formula_29 to \"subdue\" the growth of formula_1, such that formula_31 on the boundary of a bounded subregion formula_32. This allows us to apply the maximum modulus principle to formula_33 and conclude that it is bounded on formula_34. We then argue that the subregion could be expanded so as to encompass all points in formula_27, establishing the boundedness of formula_33 on formula_27. Finally, we let formula_38 so that formula_39 in order to conclude that formula_1 must also be bounded on formula_27.\n\nIn the literature of complex analysis, there are many examples of the Phragmén–Lindelöf principle applied to unbounded regions of differing types, and also a version of this principle may be applied in a similar fashion to subharmonic and superharmonic functions.\n\nTo continue the example above, we can impose a growth condition on a holomorphic function formula_1 that prevents it from \"blowing up\" and allows the Phragmén–Lindelöf principle to be applied. To this end, we now include the condition that \n\nfor some real constants formula_44 and formula_45, for all formula_46. It can then be shown that formula_47 for all formula_48 implies that formula_47 in fact holds for all formula_46. Thus, we have the following proposition:\n\nProposition. \"Let\" \n\n\"Let\" formula_1 \"be holomorphic on formula_27\" \"and continuous on formula_54, and suppose there exist real constants formula_55\" \"such that\"\n\n\"for all formula_46\" \"and formula_47 for all formula_59. Then\" \"formula_47\" \"for all formula_46\".\n\nNote that this conclusion fails when formula_62, precisely as the motivating counterexample in the previous section demonstrates. The proof of this statement employs a typical Phragmén–Lindelöf argument:\n\n\"Sketch of Proof:\" We fix formula_63, define for each formula_64 the auxiliary function formula_28 by formula_66, and consider the function formula_33. The growth properties of formula_33 allow us to find an formula_69 such that formula_70 whenever formula_71 and formula_72. In particular, formula_73 holds for all formula_74, where formula_34 is the open rectangle in the complex plane defined by the vertices formula_76. Because formula_34is a bounded region, the maximum modulus principle implies that formula_73 for all formula_79. But since formula_69 can be made arbitrarily large, this result extends to all of formula_27, and formula_73 for all formula_46. Finally, since formula_84 as formula_38, we conclude that formula_86 for all formula_46. □\n\nA particularly useful statement proved using the Phragmén–Lindelöf principle bounds holomorphic functions on a sector of the complex plane if it is bounded on its boundary. This statement can be used to give a complex analytic proof of the Hardy's uncertainty principle, which states that a function and its Fourier transform cannot both decay faster than exponentially. \n\nProposition. \"Let formula_88 be a function that is holomorphic in a sector\"\n\n\"of central angle formula_90, and continuous on its boundary. If\"\n\n\"for formula_48, and\"\n\"for all formula_46, where formula_93 and formula_94, then () holds also for all formula_46.\"\n\n\nwith the same conclusion.\n\nIn practice the point 0 is often transformed into the point ∞ of the Riemann sphere. This gives a version of the principle that applies to strips, for example bounded by two lines of constant real part in the complex plane. This special case is sometimes known as Lindelöf's theorem.\n\nCarlson's theorem is an application of the principle to functions bounded on the imaginary axis.\n\n"}
{"id": "15282871", "url": "https://en.wikipedia.org/wiki?curid=15282871", "title": "Predicate functor logic", "text": "Predicate functor logic\n\nIn mathematical logic, predicate functor logic (PFL) is one of several ways to express first-order logic (also known as predicate logic) by purely algebraic means, i.e., without quantified variables. PFL employs a small number of algebraic devices called predicate functors (or predicate modifiers) that operate on terms to yield terms. PFL is mostly the invention of the logician and philosopher Willard Quine.\n\nThe source for this section, as well as for much of this entry, is Quine (1976). Quine proposed PFL as a way of algebraizing first-order logic in a manner analogous to how Boolean algebra algebraizes propositional logic. He designed PFL to have exactly the expressive power of first-order logic with identity. Hence the metamathematics of PFL are exactly those of first-order logic with no interpreted predicate letters: both logics are sound, complete, and undecidable. Most work Quine published on logic and mathematics in the last 30 years of his life touched on PFL in some way.\n\nQuine took \"functor\" from the writings of his friend Rudolf Carnap, the first to employ it in philosophy and mathematical logic, and defined it as follows:\n\n\"The word \"functor\", grammatical in import but logical in habitat... is a sign that attaches to one or more expressions of given grammatical kind(s) to produce an expression of a given grammatical kind.\" (Quine 1982: 129)\nWays other than PFL to algebraize first-order logic include:\nPFL is arguably the simplest of these formalisms, yet also the one about which the least has been written.\n\nQuine had a lifelong fascination with combinatory logic, attested to by his (1976) and his introduction to the translation in Van Heijenoort (1967) of the paper by the Russian logician Moses Schönfinkel founding combinatory logic. When Quine began working on PFL in earnest, in 1959, combinatory logic was commonly deemed a failure for the following reasons:\n\nThe PFL syntax, primitives, and axioms described in this section are largely Kuhn's (1983). The semantics of the functors are Quine's (1982). The rest of this entry incorporates some terminology from Bacon (1985).\n\nAn \"atomic term\" is an upper case Latin letter, \"I\" and \"S\" excepted, followed by a numerical superscript called its \"degree\", or by concatenated lower case variables, collectively known as an \"argument list\". The degree of a term conveys the same information as the number of variables following a predicate letter. An atomic term of degree 0 denotes a Boolean variable or a truth value. The degree of \"I\" is invariably 2 and so is not indicated.\n\nThe \"combinatory\" (the word is Quine's) predicate functors, all monadic and peculiar to PFL, are Inv, inv, ∃, +, and p. A term is either an atomic term, or constructed by the following recursive rule. If τ is a term, then Invτ, invτ, ∃τ, +τ, and pτ are terms. A functor with a superscript \"n\", \"n\" a natural number > 1, denotes \"n\" consecutive applications (iterations) of that functor.\n\nA formula is either a term or defined by the recursive rule: if α and β are formulas, then αβ and ~(α) are likewise formulas. Hence \"~\" is another monadic functor, and concatenation is the sole dyadic predicate functor. Quine called these functors \"alethic.\" The natural interpretation of \"~\" is negation; that of concatenation is any connective that, when combined with negation, forms a functionally complete set of connectives. Quine's preferred functionally complete set was conjunction and negation. Thus concatenated terms are taken as conjoined. The notation + is Bacon's (1985); all other notation is Quine's (1976; 1982). The alethic part of PFL is identical to the \"Boolean term schemata\" of Quine (1982).\n\nAs is well known, the two alethic functors could be replaced by a single dyadic functor with the following syntax and semantics: if α and β are formulas, then (αβ) is a formula whose semantics are \"not (α and/or β)\" (see NAND and NOR).\n\nQuine set out neither axiomatization nor proof procedure for PFL. The following axiomatization of PFL, one of two proposed in Kuhn (1983), is concise and easy to describe, but makes extensive use of free variables and so does not do full justice to the spirit of PFL. Kuhn gives another axiomatization dispensing with free variables, but that is harder to describe and that makes extensive use of defined functors. Kuhn proved both of his PFL axiomatizations sound and complete.\n\nThis section is built around the primitive predicate functors and a few defined ones. The alethic functors can be axiomatized by any set of axioms for sentential logic whose primitives are negation and one of ∧ or ∨. Equivalently, all tautologies of sentential logic can be taken as axioms.\n\nQuine's (1982) semantics for each predicate functor are stated below in terms of abstraction (set builder notation), followed by either the relevant axiom from Kuhn (1983), or a definition from Quine (1976). The notation formula_1 denotes the set of n-tuples satisfying the atomic formula formula_2\n\nIdentity is reflexive (\"Ixx\"), symmetric (\"Ixy\"→\"Iyx\"), transitive ( (\"Ixy\"∧\"Iyz\") → \"Ixz\"), and obeys the substitution property:\n\n\"Cropping\" enables two useful defined functors:\nS generalizes the notion of reflexivity to all terms of any finite degree greater than 2. N.B: S should not be confused with the primitive combinator S of combinatory logic.\nHere only, Quine adopted an infix notation, because this infix notation for Cartesian product is very well established in mathematics. Cartesian product allows restating conjunction as follows:\nReorder the concatenated argument list so as to shift a pair of duplicate variables to the far left, then invoke S to eliminate the duplication. Repeating this as many times as required results in an argument list of length max(\"m\",\"n\").\n\nThe next three functors enable reordering argument lists at will. \nGiven an argument list consisting of \"n\" variables, p implicitly treats the last \"n\"-1 variables like a bicycle chain, with each variable constituting a link in the chain. One application of p advances the chain by one link. \"k\" consecutive applications of p to \"F\" moves the \"k\"+1 variable to the second argument position in \"F\".\n\nWhen \"n\"=2, Inv and inv merely interchange \"x\" and \"x\". When \"n\"=1, they have no effect. Hence p has no effect when \"n\"<3.\n\nKuhn (1983) takes \"Major inversion\" and \"Minor inversion\" as primitive. The notation p in Kuhn corresponds to inv; he has no analog to \"Permutation\" and hence has no axioms for it. If, following Quine (1976), p is taken as primitive, Inv and inv can be defined as nontrivial combinations of +, ∃, and iterated p.\n\nThe following table summarizes how the functors affect the degrees of their arguments.\n\nAll instances of a predicate letter may be replaced by another predicate letter of the same degree, without affecting validity. The rules are:\n\nInstead of axiomatizing PFL, Quine (1976) proposed the following conjectures as candidate axioms.\n\n\"n\"-1 consecutive iterations of p restores the \"status quo ante\":\n+ and ∃ annihilate each other:\n\n\nNegation distributes over +, ∃, and p:\n\n\n\n+ and p distributes over conjunction:\n\n\nIdentity has the interesting implication:\n\nQuine also conjectured the rule: If formula_34 is a PFL theorem, then so are formula_35 formula_36 and formula_37.\n\nBacon (1985) takes the conditional, negation, \"Identity\", \"Padding\", and \"Major\" and \"Minor inversion\" as primitive, and \"Cropping\" as defined. Employing terminology and notation differing somewhat from the above, Bacon (1985) sets out two formulations of PFL:\n\nBacon also:\n\nThe following algorithm is adapted from Quine (1976: 300-2). Given a closed formula of first-order logic, first do the following:\n\nNow apply the following algorithm to the preceding result:\n\n1. Translate the matrices of the most deeply nested quantifiers into disjunctive normal form, consisting of disjuncts of conjuncts of terms, negating atomic terms as required. The resulting subformula contains only negation, conjunction, disjunction, and existential quantification.\n\n2. Distribute the existential quantifiers over the disjuncts in the matrix using the rule of passage (Quine 1982: 119):\n\n3. Replace conjunction by Cartesian product, by invoking the fact:\n\n4. Concatenate the argument lists of all atomic terms, and move the concatenated list to the far right of the subformula.\n\n5. Use Inv and inv to move all instances of the quantified variable (call it \"y\") to the left of the argument list.\n\n6. Invoke S as many times as required to eliminate all but the last instance of \"y\". Eliminate \"y\" by prefixing the subformula with one instance of ∃.\n\n7. Repeat (1)-(6) until all quantified variables have been eliminated. Eliminate any disjunctions falling within the scope of a quantifier by invoking the equivalence:\n\nThe reverse translation, from PFL to first-order logic, is discussed in Quine (1976: 302-4).\n\nThe canonical foundation of mathematics is axiomatic set theory, with a background logic consisting of first-order logic with identity, with a universe of discourse consisting entirely of sets. There is a single predicate letter of degree 2, interpreted as set membership. The PFL translation of the canonical axiomatic set theory ZFC is not difficult, as no ZFC axiom requires more than 6 quantified variables.\n\n\n\n"}
{"id": "177700", "url": "https://en.wikipedia.org/wiki?curid=177700", "title": "Risk aversion", "text": "Risk aversion\n\nIn economics and finance, risk aversion is the behavior of humans (especially consumers and investors), who, when exposed to uncertainty, attempt to lower that uncertainty. It is the hesitation of a person to agree to a situation with an unknown payoff rather than another situation with a more predictable payoff but possibly lower expected payoff. For example, a risk-averse investor might choose to put their money into a bank account with a low but guaranteed interest rate, rather than into a stock that may have high expected returns, but also involves a chance of losing value.\n\nA person is given the choice between two scenarios, one with a guaranteed payoff and one without. In the guaranteed scenario, the person receives $50. In the uncertain scenario, a coin is flipped to decide whether the person receives $100 or nothing. The expected payoff for both scenarios is $50, meaning that an individual who was insensitive to risk would not care whether they took the guaranteed payment or the gamble. However, individuals may have different risk attitudes.\n\nA person is said to be:\n\n\nThe average payoff of the gamble, known as its expected value, is $50. The smallest dollar amount that the individual would accept instead of the bet is called the certainty equivalent, and the difference between the expected value and the certainty equivalent is called the risk premium. For risk-averse individuals, it is positive, for risk-neutral persons it is zero, and for risk-loving individuals their risk premium is negative.\n\nIn expected utility theory, an agent has a utility function \"u\"(\"c\") where \"c\" represents the value that he might receive in money or goods (in the above example \"c\" could be $0 or $40 or $100).\n\nThe utility function \"u\"(\"c\") is defined only up to positive affine transformation – in other words, a constant could be added to the value of \"u\"(\"c\") for all \"c\", and/or \"u\"(\"c\") could be multiplied by a positive constant factor, without affecting the conclusions.\n\nAn agent possesses risk aversion if and only if the utility function is concave. For instance \"u\"(0) could be 0, \"u\"(100) might be 10, \"u\"(40) might be 5, and for comparison \"u\"(50) might be 6.\n\nThe expected utility of the above bet (with a 50% chance of receiving 100 and a 50% chance of receiving 0) is\n\nand if the person has the utility function with \"u\"(0)=0, \"u\"(40)=5, and \"u\"(100)=10 then the expected utility of the bet equals 5, which is the same as the known utility of the amount 40. Hence the certainty equivalent is 40.\n\nThe risk premium is ($50 minus $40)=$10, or in proportional terms\n\nor 25% (where $50 is the expected value of the risky bet: (formula_3). This risk premium means that the person would be willing to sacrifice as much as $10 in expected value in order to achieve perfect certainty about how much money will be received. In other words, the person would be indifferent between the bet and a guarantee of $40, and would prefer anything over $40 to the bet.\n\nIn the case of a wealthier individual, the risk of losing $100 would be less significant, and for such small amounts his utility function would be likely to be almost linear, for instance if u(0) = 0 and u(100) = 10, then u(40) might be 4.0001 and u(50) might be 5.0001.\n\nThe utility function for perceived gains has two key properties: an upward slope, and concavity. (i) The upward slope implies that the person feels that more is better: a larger amount received yields greater utility, and for risky bets the person would prefer a bet which is first-order stochastically dominant over an alternative bet (that is, if the probability mass of the second bet is pushed to the right to form the first bet, then the first bet is preferred). (ii) The concavity of the utility function implies that the person is risk averse: a sure amount would always be preferred over a risky bet having the same expected value; moreover, for risky bets the person would prefer a bet which is a mean-preserving contraction of an alternative bet (that is, if some of the probability mass of the first bet is spread out without altering the mean to form the second bet, then the first bet is preferred).\n\nThere are multiple measures of the risk aversion expressed by a given utility function. Several functional forms often used for utility functions are expressed in terms of these measures.\n\nThe higher the curvature of formula_4, the higher the risk aversion. However, since expected utility functions are not uniquely defined (are defined only up to affine transformations), a measure that stays constant with respect to these transformations is needed. One such measure is the Arrow–Pratt measure of absolute risk aversion (ARA), after the economists Kenneth Arrow and John W. Pratt, also known as the coefficient of absolute risk aversion, defined as\n\nwhere formula_6 and formula_7 denote the first and second derivatives with respect to formula_8 of formula_4.\n\nThe following expressions relate to this term:\n\nThe solution to this differential equation (omitting additive and multiplicative constant terms, which do not affect the behavior implied by the utility function) is:\n\nwhere formula_14 and formula_15.\nNote that when formula_16, this is CARA, as formula_17, and when formula_18, this is CRRA (see below), as formula_19.\nSee \n\nand this can hold only if formula_22. Therefore, DARA implies that the utility function is positively skewed; that is, formula_22. Analogously, IARA can be derived with the opposite directions of inequalities, which permits but does not require a negatively skewed utility function (formula_24). An example of a DARA utility function is formula_25, with formula_26, while formula_27 formula_28, with formula_29 would represent a quadratic utility function exhibiting IARA.\n\nThe Arrow-Pratt measure of relative risk aversion (RRA) or coefficient of relative risk aversion is defined as\n\nLike for absolute risk aversion, the corresponding terms \"constant relative risk aversion\" (CRRA) and \"decreasing/increasing relative risk aversion\" (DRRA/IRRA) are used. This measure has the advantage that it is still a valid measure of risk aversion, even if the utility function changes from risk averse to risk loving as \"c\" varies, i.e. utility is not strictly convex/concave over all \"c\". A constant RRA implies a decreasing ARA, but the reverse is not always true. As a specific example of constant relative risk aversion, the utility function formula_32 implies RRA = 1.\n\nIn intertemporal choice problems, the elasticity of intertemporal substitution often cannot be disentangled from the coefficient of relative risk aversion. The isoelastic utility function\nexhibits constant relative risk aversion with formula_34 and the elasticity of intertemporal substitution formula_35. When formula_36 using l'Hôpital's rule shows that this simplifies to the case of \"log utility,\" \"u\"(\"c\") = log \"c\", and the income effect and substitution effect on saving exactly offset.\n\nA time-varying relative risk aversion can be considered.\n\nThe most straightforward implications of increasing or decreasing absolute or relative risk aversion, and the ones that motivate a focus on these concepts, occur in the context of forming a portfolio with one risky asset and one risk-free asset. If the person experiences an increase in wealth, he/she will choose to increase (or keep unchanged, or decrease) the \"number of dollars\" of the risky asset held in the portfolio if \"absolute\" risk aversion is decreasing (or constant, or increasing). Thus economists avoid using utility functions such as the quadratic, which exhibit increasing absolute risk aversion, because they have an unrealistic behavioral implication.\n\nSimilarly, if the person experiences an increase in wealth, he/she will choose to increase (or keep unchanged, or decrease) the \"fraction\" of the portfolio held in the risky asset if \"relative\" risk aversion is decreasing (or constant, or increasing).\n\nIn one model in monetary economics, an increase in relative risk aversion increases the impact of households' money holdings on the overall economy. In other words, the more the relative risk aversion increases, the more money demand shocks will impact the economy.\n\nIn modern portfolio theory, risk aversion is measured as the additional expected reward an investor requires to accept additional risk. Here risk is measured as the standard deviation of the return on investment, i.e. the square root of its variance. In advanced portfolio theory, different kinds of risk are taken into consideration. They are measured as the n-th root of the n-th central moment. The symbol used for risk aversion is A or A.\n\nThe notion of using expected utility theory to analyze risk aversion has come under criticism from behavioral economics. Matthew Rabin has showed that a risk-averse, expected-utility-maximizing individual who,\n\n\"from any initial wealth level [...] turns down gambles where she loses $100 or gains $110, each with 50% probability [...] will turn down 50-50 bets of losing $1,000 or gaining any sum of money.\"\n\nRabin criticizes this implication of expected utility theory on grounds of implausibility. One solution to the problem observed by Rabin is that proposed by prospect theory and cumulative prospect theory, where outcomes are considered relative to a reference point (usually the status quo), rather than to consider only the final wealth.\n\nAnother limitation is the reflection effect which demonstrates the reversing of risk aversion. This effect was first presented by Kahneman and Tversky as a part of the prospect theory, in the behavioral economics domain.\nThe reflection effect is an identified pattern of opposite preferences between negative prospects as opposed to positive prospects. According to this effect, people tend to avoid risks under the gain domain, and to seek risks under the loss domain. Meaning, no risk aversion is expected under the loss domain. For example, in the gain domain, most people prefer a certain gain of 3000, than a gain of 4000 with a risk of 80 percent. When posing the same problem under the loss domain - with negative values, most people prefer a loss of 4000 with 80 percent chance, over a certain loss of 3000.\n\nThe reflection effect (as well as the certainty effect) is inconsistent with the expected utility hypothesis. It is assumed that the psychological principle which stands behind this kind of behavior is the overweighting of certainty. Meaning, options which are perceived as certain, are over-weighted relative to uncertain options. This pattern is an indication of a risk seeking behavior in negative prospects and eliminates other explanations for the certainty effect such as aversion for uncertainty or variability.\n\nThe initial findings regarding the reflection effect faced criticism regarding its validity, as it was claimed that there are insufficient evidence to support the effect on the individual level. Subsequently, an extensive investigation revealed its possible limitations, suggesting that the effect is most prevalent when either small or large amounts and extreme probabilities are involved.\n\nAttitudes towards risk have attracted the interest of the field of neuroeconomics and behavioral economics. A 2009 study by Christopoulos et al. suggested that the activity of a specific brain area (right inferior frontal gyrus) correlates with risk aversion, with more risk averse participants (i.e. those having higher risk premia) also having higher responses to safer options. This result coincides with other studies, that show that neuromodulation of the same area results in participants making more or less risk averse choices, depending on whether the modulation increases or decreases the activity of the target area.\n\nIn the real world, many government agencies, e.g. Health and Safety Executive, are fundamentally risk-averse in their mandate. This often means that they demand (with the power of legal enforcement) that risks be minimized, even at the cost of losing the utility of the risky activity.\nIt is important to consider the opportunity cost when mitigating a risk; the cost of not taking the risky action. Writing laws focused on the risk without the balance of the utility may misrepresent society's goals. The public understanding of risk, which influences political decisions, is an area which has recently been recognised as deserving focus. In 2007 Cambridge University initiated the Winton Professorship of the Public Understanding of Risk, a role described as outreach rather than traditional academic research by the holder, David Spiegelhalter.\n\nChildren's services such as schools and playgrounds have become the focus of much risk-averse planning, meaning that children are often prevented from benefiting from activities that they would otherwise have had. Many playgrounds have been fitted with impact-absorbing matting surfaces. However, these are only designed to save children from death in the case of direct falls on their heads and do not achieve their main goals. They are expensive, meaning that less resources are available to benefit users in other ways (such as building a playground closer to the child's home, reducing the risk of a road traffic accident on the way to it), and—some argue—children may attempt more dangerous acts, with confidence in the artificial surface. Shiela Sage, an early years school advisor, observes \"Children who are only ever kept in very safe places, are not the ones who are able to solve problems for themselves. Children need to have a certain amount of risk taking ... so they'll know how to get out of situations.\"\n\nA vaccine to protect children against the three common diseases measles, mumps and rubella was developed and recommended for all children in several countries including the UK. However, a controversy arose around fraudulent allegations that it caused autism. This alleged causal link was thoroughly disproved, and the doctor who made the claims was expelled from the General Medical Council. Even years after the claims were disproved, some parents wanted to avert the risk of causing autism in their own children. They chose to spend significant amounts of their own money on alternatives from private doctors. These alternatives carried their own risks which were not balanced fairly, most often that the children were not properly immunized against the more common diseases of measles, mumps and rubella.\n\nMobile phones may carry some small health risk. While most people would accept that unproven risk to gain the benefit of improved communication, others remain so risk averse that they do not. (The COSMOS cohort study continues to study the actual risks of mobile phones.)\n\nOne experimental study with student-subject playing the game of the TV show Deal or No Deal finds that people are more risk averse in the limelight than in the anonymity of a typical behavioral laboratory. In the laboratory treatments, subjects made decisions in a standard, computerized laboratory setting as typically employed in behavioral experiments. In the limelight treatments, subjects made their choices in a simulated game show environment, which included a live audience, a game show host, and video cameras. In line with this, studies on investor behavior find that investors trade more and more speculatively after switching from phone-based to online trading and that investors tend to keep their core investments with traditional brokers and use a small fraction of their wealth to speculate online.\n\n\n"}
{"id": "2804687", "url": "https://en.wikipedia.org/wiki?curid=2804687", "title": "Set notation", "text": "Set notation\n\nSets are fundamental objects in mathematics. Intuitively, a set is merely a collection of elements or \"members\". There are various conventions for textually denoting sets. In any particular situation, an author typically chooses from among these conventions depending on which properties of the set are most relevant to the immediate context or on which perspective is most useful.\n\nWhere it is desirable to refer to a set as an indivisible entity, one typically denotes it by a single capital letter. In referring to an arbitrary, generic set, a typical notational choice is . When several sets are being discussed simultaneously, they are often denoted by the first few capitals: , , , and so forth. By convention, particular symbols are reserved for the most important sets of numbers:\n\nSome authors use the blackboard bold font for these particular sets (formula_3, formula_4, etc.). This usage is widely accepted in handwriting, but many mathematicians, and such experts on mathematical typography as Donald Knuth, advise against its use in print.\n\nIn many contexts one is interested more in the elements that constitute the set than in the single entity comprising those elements, for instance where stating an extensional definition of the set. Here the elements, whether expressed discretely or in some aggregate manner, are enclosed in braces.\n\nThe simplest notational approach of this type, which is feasible only for fairly small sets, is to enumerate the elements exhaustively. Thus the set of suits in a standard deck of playing cards is denoted by {♠, ♦, ♥, ♣} and the set of even prime numbers is denoted by . This approach also provides the notation for the empty set.\n\nThe semantics of the term \"set\" imposes certain syntactic constraints on these notations. The only information that is fundamental for a set is which particular objects are, or are not, elements. As a result, the order in which elements appear in an enumeration is irrelevant: and are two enumerations of the same set. Likewise, repeated mention of an element is also irrelevant, so . To deal with collections for which members' multiplicity \"is\" significant, there is a generalization of sets called \"multisets\".\n\nA variant of this explicitly exhaustive enumeration uses ranges of elements and features the ellipsis. Thus the set of the first ten natural numbers is . Here, of course, the ellipsis means \"and so forth.\" Note that wherever an ellipsis is used to denote a range, it is punctuated as though it were an element of the set. If either extreme of a range is indeterminate, it may be denoted by a mathematical expression giving a formula to compute it. As an example, if is known from context to be a positive integer, then the set of the first perfect squares may be denoted by .\n\nSome infinite sets, too, can be represented in this way. An example is denoting the set of natural numbers (for which one notation described above is ) by . In cases where the infinitely repeating pattern is not obvious, one can insert an expression to represent a generic element of the set, as with .\n\nA more powerful mechanism for denoting a set in terms of its elements is set-builder notation. Here the general pattern is , which denotes the set of all elements (from some universal set) for which the assertion about is true. For example, when understood as a set of points, the circle with radius and center , may be denoted as .\n\nA notable exception to the braces notation is used to express intervals on the real line. It makes use of the fact that any such interval is completely determined by its left and right endpoints: the unit interval, for instance, is the set of reals between 0 and 1 (inclusive). The convention for denoting intervals uses brackets and parentheses, depending as the corresponding endpoint is included in or excluded from the set, respectively. Thus the set of reals with absolute value less than one is denoted by — note that this is very different from the ordered pair with first entry −1 and second entry 1. As other examples, the set of reals that satisfy is denoted by , and the set of nonnegative reals is denoted by .\n\nSince so much of mathematics consists in discovering and exploiting patterns, it is perhaps not surprising that there should have arisen various set-denotational conventions that strike practitioners as obvious or natural—if sometimes only once the pattern has been pointed out.\n\nOne class comprises those notations deriving the symbol for a set from the algebraic form of a representative element of the set. As an example, consider the set of even numbers. Since a number is even precisely if there exists some integer such that , the following variation on set-builder notation could be used to denote this set: (compare this with the formal set-builder notation: ). Alternatively, a single symbol for the set of even numbers is . Likewise, since any odd number must have the form for some integer , the set of odd numbers may be denoted .\n\nA second class is based on a strong logical relationship between a set and a particular integer. One example is the bracket notation, in which the set of the first positive integers is denoted by (as a related point, when endowed with the standard less-than-or-equal relation , the set yields the poset denoted by .) Another example arises from modular arithmetic, where equivalence classes are denoted by formula_5, which may be understood to represent the set of integers that leave remainder on division by . Thus yet another notation for the set of even numbers is formula_6.\n\nAnother set-denotational convention that relies on metaphor comes from enumerative combinatorics. It derives a symbol for a set from an expression for the set's cardinality, or size, . Perhaps the simplest and best known example is the Cartesian product of sets and , which is the set . Since, in this set, every element of gets paired exactly once with every element of , its cardinality is . For this reason, the set is denoted by . In fact, that same fact about its cardinality is why this set is called a \"product\".\n\nThere are many other examples of this convention. One is the set of functions from set to set . When and are finite, specifying any such function amounts to choosing for each element of which element of should be its image. So, the number of these functions is . Thus, one denotes the set of all functions from to as . Another example is the power set of a set , which, having cardinality , is denoted by . Note, though, that since any subset of may be seen as a function assigning to each element of one or the other element of {include, exclude}, the notation may be seen as a special case of . The cardinality metaphor has also been used to derive from the standard notation for binomial coefficients the notation formula_7 for the set of all -element subsets of a set . An alternative notation to denote the all formula_8-subsets of formula_9 is formula_10 \n\nAn example where this cardinality-based convention appears not to have been used yet is to denote the set of all permutations of a set . Since it is usually seen as the underlying set of a symmetric group, this set is typically denoted by a symbol for the group itself, either or .\n\nFurther conventions are also sometimes seen, including one based on relations. For a relation on a set , one may denote the set of objects related by \"R\" to some element of by . So from the notation | for the \"divides\" relation of number theory, one may denote the set of factors of an integer by . Similarly, a subset of is a \"principal\" lower set of a poset precisely if it can be denoted by for some in . And since is the symbol for the adjacency relation, the subset of a collection of vertices of a graph that includes exactly those adjacent to a vertex (namely, the intersection of with the open neighborhood of may be denoted by .\n\n"}
{"id": "148555", "url": "https://en.wikipedia.org/wiki?curid=148555", "title": "Spin glass", "text": "Spin glass\n\nA spin glass is a disordered magnet, where the magnetic spins of the component atoms (the orientation of the north and south magnetic poles in three-dimensional space) are not aligned in a regular pattern. The term \"glass\" comes from an analogy between the \"magnetic\" disorder in a spin glass and the \"positional\" disorder of a conventional, chemical glass, e.g., a window glass. In window glass or any amorphous solid the atomic bond structure is highly irregular; in contrast, a crystal has a uniform pattern of atomic bonds. In ferromagnetic solid, magnetic spins all align in the same direction; this would be analogous to a crystal.\n\nThe individual atomic bonds in a spin glass are a mixture of roughly equal numbers of ferromagnetic bonds (where neighbors have the same orientation) and antiferromagnetic bonds (where neighbors have exactly the opposite orientation: north and south poles are flipped 180 degrees). These patterns of aligned and misaligned atomic magnets create what are known as frustrated interactions - distortions in the geometry of atomic bonds compared to what would be seen in a regular, fully aligned solid. They may also create situations where more than one geometric arrangement of atoms is stable.\n\nSpin glasses and the complex internal structures that arise within them are termed \"metastable\" because they are \"stuck\" in stable configurations other than the lowest-energy configuration (which would be aligned and ferromagnetic). The mathematical complexity of these structures is difficult but fruitful to study experimentally or in simulations, with applications to artificial neural networks in computer science, in addition to physics, chemistry, and materials science.\n\nIt is the time dependence which distinguishes spin glasses from other magnetic systems.\n\nAbove the spin glass transition temperature, \"T\", the spin glass exhibits typical magnetic behaviour (such as paramagnetism).\n\nIf a magnetic field is applied as the sample is cooled to the transition temperature, magnetization of the sample increases as described by the Curie law. Upon reaching \"T\", the sample becomes a spin glass and further cooling results in little change in magnetization. This is referred to as the \"field-cooled\" magnetization.\n\nWhen the external magnetic field is removed, the magnetization of the spin glass falls rapidly to a lower value known as the \"remanent\" magnetization.\n\nMagnetization then decays slowly as it approaches zero (or some small fraction of the original value—this remains unknown). This decay is non-exponential and no simple function can fit the curve of magnetization versus time adequately. This slow decay is particular to spin glasses. Experimental measurements on the order of days have shown continual changes above the noise level of instrumentation.\n\nSpin glasses differ from ferromagnetic materials by the fact that after the external magnetic field is removed from a ferromagnetic substance, the magnetization remains indefinitely at the remanent value. Paramagnetic materials differ from spin glasses by the fact that, after the external magnetic field is removed, the magnetization rapidly falls to zero, with no remanent magnetization. The decay is rapid and exponential.\n\nIf the sample is cooled below \"T\" in the absence of an external magnetic field and a magnetic field is applied after the transition to the spin glass phase, there is a rapid initial increase to a value called the \"zero-field-cooled\" magnetization. A slow upward drift then occurs toward the field-cooled magnetization.\n\nSurprisingly, the sum of the two complicated functions of time (the zero-field-cooled and remanent magnetizations) is a constant, namely the field-cooled value, and thus both share identical functional forms with time, at least in the limit of very small external fields.\n\nIn this model, we have spins arranged on a formula_1-dimensional lattice with only nearest neighbor interactions similar to the Ising model. This model can be solved exactly for the critical temperatures and a glassy phase is observed to exist at low temperatures. The Hamiltonian for this spin system is given by:\n\nwhere formula_3 refers to the Pauli spin matrix for the spin-half particle at lattice point formula_4. A negative value of formula_5 denotes an antiferromagnetic type interaction between spins at points formula_4 and formula_7. The sum runs over all nearest neighbor positions on a lattice, of any dimension. \nThe variables formula_5 representing the magnetic nature of the spin-spin interactions are called bond or link variables. In order to determine the partition function for this system, one needs to average the free energy formula_9 where formula_10, over all possible values of formula_5. The distribution of values of formula_5 is taken to be a gaussian with a mean formula_13 and a variance formula_14:\n\nSolving for the free energy using the replica method, below a certain temperature, a new magnetic phase called the spin glass phase (or glassy phase) of the system is found to exist which is characterized by a vanishing magnetization formula_16 along with a non-vanishing value of the two point correlation function between spins at the same lattice point but at two different replicas: formula_17, where formula_18 are replica indices. The order parameter for the ferromagnetic to spin glass phase transition is therefore formula_19, and that for paramagnetic to spin glass is again formula_19. Hence the new set of order parameters describing the three magnetic phases consists of both formula_21 and formula_19.\nFree energy of this system can be found, both under assumptions of replica symmetry as well as considering replica symmetry breaking. Under the assumption of replica symmetry, the free energy is given by the expression:\n\nIn addition to unusual experimental properties, spin glasses are the subject of extensive theoretical and computational investigations. A substantial part of early theoretical work on spin glasses dealt with a form of mean field theory based on a set of replicas of the partition function of the system.\n\nAn important, exactly solvable model of a spin glass was introduced by D. Sherrington and S. Kirkpatrick in 1975. It is an Ising model with long range frustrated ferro- as well as antiferromagnetic couplings. It corresponds to a mean field approximation of spin glasses describing the slow dynamics of the magnetization and the complex non-ergodic equilibrium state.\n\nUnlike the Edwards–Anderson (EA) model, in the system though only two-spin interactions are considered, the range of each interaction can be potentially infinite (of the order of the size of the lattice). Therefore, we see that any two spins can be linked with a ferromagnetic or an antiferromagnetic bond and the distribution of these is given exactly as in the case of Edwards–Anderson model. The Hamiltonian for SK model is very similar to the EA model:\n\nwhere formula_25 have same meanings as in the EA model. The equilibrium solution of the model, after some initial attempts by Sherrington, Kirkpatrick and others, was found by Giorgio Parisi in 1979 with the replica method. The subsequent work of interpretation of the Parisi solution—by M. Mezard, G. Parisi, M.A. Virasoro and many others—revealed the complex nature of a glassy low temperature phase characterized by ergodicity breaking, ultrametricity and non-selfaverageness. Further developments led to the creation of the cavity method, which allowed study of the low temperature phase without replicas. A rigorous proof of the Parisi solution has been provided in the work of Francesco Guerra and Michel Talagrand.\n\nThe formalism of replica mean field theory has also been applied in the study of neural networks, where it has enabled calculations of properties such as the storage capacity of simple neural network architectures without requiring a training algorithm (such as backpropagation) to be designed or implemented.\n\nMore realistic spin glass models with short range frustrated interactions and disorder, like the Gaussian model where the couplings between neighboring spins follow a Gaussian distribution, have been studied extensively as well, especially using Monte Carlo simulations. These models display spin glass phases bordered by sharp phase transitions.\n\nBesides its relevance in condensed matter physics, spin glass theory has acquired a strongly interdisciplinary character, with applications to neural network theory, computer \nscience, theoretical biology, econophysics etc.\n\nThe infinite-range model is a generalization of the Sherrington–Kirkpatrick model where we not only consider two spin interactions but formula_26-spin interactions, where formula_27 and formula_28 is the total number of spins. Unlike the Edwards–Anderson model, similar to the SK model, the interaction range is still infinite. The Hamiltonian for this model is described by:\n\nwhere formula_30 have similar meanings as in the EA model. The formula_31 limit of this model is known as the random energy model. In this limit, it can be seen that the probability of the spin glass existing in a particular state, depends only on the energy of that state and not on the individual spin configurations in it.\nA gaussian distribution of magnetic bonds across the lattice is assumed usually to solve this model. Any other distribution is expected to give the same result, as a consequence of the central limit theorem. The gaussian distribution function, with mean formula_32 and variance formula_33, is given as:\n\nThe order parameters for this system are given by the magnetization formula_21 and the two point spin correlation between spins at the same site formula_19, in two different replicas, which are the same as for the SK model. This infinite range model can be solved explicitly for the free energy in terms of formula_21 and formula_19, under the assumption of replica symmetry as well as 1-Replica Symmetry Breaking.\n\nA so-called non-ergodic behavior happens in spin glasses below the freezing temperature formula_40, since below that temperature the system cannot escape from the ultradeep minima of the hierarchically disordered energy landscape. Although the freezing temperature is typically as low as 30 kelvins (−240 °C), so that the spin-glass magnetism appears to be practically without applications in daily life, there are applications in different contexts, e.g. in the already mentioned theory of neural networks, i.e. in theoretical brain research, and in the mathematical-economical theory of optimization.\n\nA detailed account of the history of spin glasses from the early 1960s to the late 1980s can be found in a series of popular articles by Philip W. Anderson in \"Physics Today\".\n\n\n\n"}
{"id": "42081371", "url": "https://en.wikipedia.org/wiki?curid=42081371", "title": "Stable ∞-category", "text": "Stable ∞-category\n\nIn category theory, a branch of mathematics, a stable ∞-category is an ∞-category such that\n\nThe homotopy category of a stable ∞-category is triangulated. A stable ∞-category admits finite limits and colimits.\n\nExamples: the derived category of an abelian category and the ∞-category of spectra are both stable.\n\nA stabilization of an ∞-category \"C\" having finite limits and base point is a functor from the stable ∞-category \"S\" to \"C\". It preserves limit. The objects in the image have the structure of infinite loop spaces; whence, the notion is a generalization of the corresponding notion (stabilization (topology)) in classical algebraic topology.\n\nBy definition, the t-structure of an stable ∞-category is the t-structure of its homotopy category. Let \"C\" be a stable ∞-category with a t-structure. Then every filtered object formula_1 in \"C\" gives rise to a spectral sequence formula_2, which, under some conditions, converges to formula_3 By the Dold–Kan correspondence, this generalizes the construction of the spectral sequence associated to a filtered chain complex of abelian groups.\n"}
{"id": "7282499", "url": "https://en.wikipedia.org/wiki?curid=7282499", "title": "Surface-area-to-volume ratio", "text": "Surface-area-to-volume ratio\n\nThe surface-area-to-volume ratio, also called the surface-to-volume ratio and variously denoted sa/vol or SA:V, is the amount of surface area per unit volume of an object or collection of objects.\nIn chemical reactions involving a solid material, the surface area to volume ratio is an important factor for the reactivity, that is, the rate at which the chemical reaction will proceed.\n\nFor a given volume, the object with the smallest surface area (and therefore with the smallest SA:V) is the sphere, a consequence of the isoperimetric inequality in 3 dimensions. By contrast, objects with tiny spikes will have very large surface area for a given volume.\n\nThe surface-area-to-volume ratio has physical dimension L (inverse length) and is therefore expressed in units of inverse distance. As an example, a cube with sides of length 1 cm will have a surface area of 6 cm and a volume of 1 cm. The surface to volume ratio for this cube is thus\n\nFor a given shape, SA:V is inversely proportional to size. A cube 2 cm on a side has a ratio of 3 cm, half that of a cube 1 cm on a side. Conversely, preserving SA:V as size increases requires changing to a less compact shape.\n\nMaterials with high surface area to volume ratio (e.g. very small diameter, very porous, or otherwise not compact) react at much faster rates than monolithic materials, because more surface is available to react. Examples include grain dust; while grain isn't typically flammable, grain dust is explosive. Finely ground salt dissolves much more quickly than coarse salt.\n\nHigh surface area to volume ratio provides a strong \"driving force\" to speed up thermodynamic processes that minimize free energy.\n\nThe ratio between the surface area and volume of cells and organisms has an enormous impact on their biology, including their physiology and behavior. For example, many aquatic microorganisms have increased surface area to increase their drag in the water. This reduces their rate of sink and allows them to remain near the surface with less energy expenditure.\n\nAn increased surface area to volume ratio also means increased exposure to the environment. The finely-branched appendages of filter feeders such as krill provide a large surface area to sift the water for food.\n\nIndividual organs like the lung have numerous internal branchings that increase the surface area; in the case of the lung, the large surface supports gas exchange, bringing oxygen into the blood and releasing carbon dioxide from the blood. Similarly, the small intestine has a finely wrinkled internal surface, allowing the body to absorb nutrients efficiently.\n\nCells can achieve a high surface area to volume ratio with an elaborately convoluted surface, like the microvilli lining the small intestine.\n\nIncreased surface area can also lead to biological problems. More contact with the environment through the surface of a cell or an organ (relative to its volume) increases loss of water and dissolved substances. High surface area to volume ratios also present problems of temperature control in unfavorable environments.\n\nThe surface to volume ratios of organisms of different sizes also leads to some biological rules such as Bergmann's rule and gigantothermy.\n\nIn the context of wildfires, the ratio of the surface area of a solid fuel to its volume is an important measurement. Fire spread behavior is frequently correlated to the surface-area-to-volume ratio of the fuel (e.g. leaves and branches). The higher its value, the faster a particle responds to changes in environmental conditions, such as temperature or moisture. Higher values are also correlated to shorter fuel ignition times, and hence faster fire spread rates.\n\nA body of icy or rocky material in outer space may, if it can build and retain sufficient heat, develop a differentiated interior and alter its surface through volcanic or tectonic activity. The length of time through which a planetary body can maintain surface-altering activity depends on how well it retains heat, and this is governed by its surface area-to-volume ratio. For Vesta (r=263 km), the ratio is so high that astronomers were surprised to find that it \"did\" differentiate and have brief volcanic activity. The moon, Mercury and Mars have radii in the low thousands of kilometers; all three retained heat well enough to be thoroughly differentiated although after a billion years or so they became too cool to show anything more than very localized and infrequent volcanic activity, of which none is evident at present. Venus and Earth (r>6,000 km) have sufficiently low surface area-to-volume ratios (roughly half that of Mars and much lower than all other known rocky bodies) so that their heat loss is minimal.\n\n\n\n\n"}
{"id": "47978056", "url": "https://en.wikipedia.org/wiki?curid=47978056", "title": "Synthetic control method", "text": "Synthetic control method\n\nThe synthetic control method is a statistical method used to evaluate the effect of an intervention in comparative case studies. It involves the construction of a weighted combination of groups used as controls, to which the treatment group is compared. This comparison is used to estimate what would have happened to the treatment group if it had not received the treatment.\nUnlike difference in differences approaches, this method can account for the effects of confounders changing over time, by weighting the control group to better match the treatment group before the intervention. Another advantage of the synthetic control method is that it allows researchers to systematically select comparison groups. It has been applied to the fields of political science, health policy, criminology, and economics.\n\nThe synthetic control method combines elements from matching and difference-in-differences techniques. Difference-in-differences methods are often-used policy evaluation tools that estimate the effect of an intervention at an aggregate level (e.g. state, country, age group etc.) by averaging over a set of unaffected units. Famous examples include studies of the employment effects of a raise in the minimum wage in New Jersey fast food restaurants by comparing them to fast food restaurants just across the border in Philadelphia that were unaffected by a minimum wage raise, and studies that look at crime rates in southern cities to evaluate the impact of the Mariel boat lift on crime. The control group in this specific scenario can be interpreted as a weighted average, where some units effectively receive zero weight while others get an equal, non-zero weight.\n\nThe synthetic control method tries to offer a more systematic way to assign weights to the control group. It typically uses a relatively long time series of the outcome prior to the intervention and estimates weights in such a way that the control group mirrors the treatment group as closely as possible. In particular, assume we have \"J\" observations over \"T\" time periods where the relevant treatment occurs at time\n\nformula_1 where formula_2 Let formula_3 where formula_4 the outcome in absence of the treatment, be the treatment effect for unit i at time t. Without loss of generality, if unit 1 receives the relevant treatment, only formula_5is not observed for formula_6 and we aim to estimate (formula_7. Imposing some structure\n\nformula_8 and assuming there exist some optimal weights \"w\" such that formula_9 for formula_10, to the synthetic controls approach suggests using these weights to estimate the counterfactual formula_11 for formula_6. So under some regularity conditions, such weights would provide estimators for the treatment effects of interest. In essence, the method uses the idea of matching and using the training data pre-intervention to set up the weights and hence a relevant control post-intervention.\n\nSynthetic controls have been used in a number of empirical applications, ranging from studies examining natural catastrophies and growth, and studies linking political murders to house prices. \n"}
{"id": "2696215", "url": "https://en.wikipedia.org/wiki?curid=2696215", "title": "Thébault's theorem", "text": "Thébault's theorem\n\nThébault's theorem is the name given variously to one of the geometry problems proposed by the French mathematician Victor Thébault, individually known as Thébault's problem I, II, and III.\n\nGiven any parallelogram, construct on its sides four squares external to the parallelogram. The quadrilateral formed by joining the centers of those four squares is a square.\n\nIt is a special case of van Aubel's theorem and a square version of the Napoleon's theorem.\n\nGiven a square, construct equilateral triangles on two adjacent edges, either both inside or both outside the square. Then the triangle formed by joining the vertex of the square distant from both triangles and the vertices of the triangles distant from the square is equilateral.\n\nGiven any triangle ABC, and any point M on BC, construct the incircle and circumcircle of the triangle. Then construct two additional circles, each tangent to AM, BC, and to the circumcircle. Then their centers and the center of the incircle are colinear.\n\nUntil 2003, academia thought this third problem of Thébault the most difficult to prove. It was published in the American Mathematical Monthly in 1938, and proved by Dutch mathematician H. Streefkerk in 1973. However, in 2003, Jean-Louis Ayme discovered that Y. Sawayama, an instructor at The Central Military School of Tokyo, independently proposed and solved this problem in 1905.\n\nAn \"external\" version of this theorem, where the incircle is replaced by an excircle and the two additional circles are external to the circumcircle, is found in Shay Gueron (2002). A proof based on Casey's theorem is in the paper.\n\n"}
{"id": "1937658", "url": "https://en.wikipedia.org/wiki?curid=1937658", "title": "Time reversibility", "text": "Time reversibility\n\nA mathematical or physical process is time-reversible if the dynamics of the process remain well-defined when the sequence of time-states is reversed.\n\nA deterministic process is time-reversible if the time-reversed process satisfies the same dynamic equations as the original process; in other words, the equations are invariant or symmetrical under a change in the sign of time. A stochastic process is reversible if the statistical properties of the process are the same as the statistical properties for time-reversed data from the same process.\n\nIn mathematics, a dynamical system is time-reversible if the forward evolution is one-to-one, so that for every state there exists a transformation (an involution) π which gives a one-to-one mapping between the time-reversed evolution of any one state and the forward-time evolution of another corresponding state, given by the operator equation:\n\nAny time-independent structures (e.g. critical points or attractors) which the dynamics give rise to must therefore either be self-symmetrical or have symmetrical images under the involution π.\n\nIn physics, the laws of motion of classical mechanics exhibit time reversibility, as long as the operator π reverses the conjugate momenta of all the particles of the system, i.e. formula_2 (T-symmetry).\n\nIn quantum mechanical systems, however, the weak nuclear force is not invariant under T-symmetry alone; if weak interactions are present reversible dynamics are still possible, but only if the operator π also reverses the signs of all the charges and the parity of the spatial co-ordinates (C-symmetry and P-symmetry). This reversibility of several linked properties is known as CPT symmetry.\n\nThermodynamic processes can be reversible or irreversible, depending on the change in entropy during the process.\n\nA stochastic process is time-reversible if the joint probabilities of the forward and reverse state sequences are the same for all sets of time increments { \"τ\" }, for \"s\" = 1, ..., \"k\" for any \"k\":\n\nA univariate stationary Gaussian process is time-reversible. Markov processes can only be reversible if their stationary distributions have the property of detailed balance:\n\nKolmogorov's criterion defines the condition for a Markov chain or continuous-time Markov chain to be time-reversible.\n\nTime reversal of numerous classes of stochastic processes has been studied, including Lévy processes, stochastic networks (Kelly's lemma), birth and death processes, Markov chains, and piecewise deterministic Markov processes.\n\nTime reversal method works based on the linear reciprocity of the wave equation, which states that the time reversed solution of a wave equation is also a solution to the wave equation since standard wave equations only contain even derivatives of the unknown variables. Thus, the wave equation is symmetrical under time reversal, so the time reversal of any valid solution is also a solution. This means that a wave's path through space is valid when travelled in either direction.\n\nTime reversal signal processing is a process in which this property is used to reverse a received signal; this signal is then re-emitted and a temporal compression occurs, resulting in a reverse of the initial excitation waveform being played at the initial source.\n\n"}
{"id": "4906174", "url": "https://en.wikipedia.org/wiki?curid=4906174", "title": "Unrestricted grammar", "text": "Unrestricted grammar\n\nIn formal language theory, the class of unrestricted grammars (also called semi-Thue, type 0, or phrase structure grammars by some authors) is the most general class of grammars in the Chomsky–Schützenberger hierarchy. On the productions of an unrestricted grammar, no other restrictions are made than each of their left-hand sides being non-empty. This grammar class can generate arbitrary recursively enumerable languages.\n\nAn unrestricted grammar is a formal grammar formula_1, where formula_2 is a set of nonterminal symbols, formula_3 is a set of terminal symbols, formula_2 and formula_3 are disjoint, formula_6 is a set of production rules of the form formula_7 where formula_8 and formula_9 are strings of symbols in formula_10 and formula_8 is not the empty string, and formula_12 is a specially designated start symbol. As the name implies, there are no real restrictions on the types of production rules that unrestricted grammars can have.\n\nThe unrestricted grammars characterize the recursively enumerable languages. This is the same as saying that for every unrestricted grammar formula_13 there exists some Turing machine capable of recognizing formula_14 and vice versa. Given an unrestricted grammar, such a Turing machine is simple enough to construct, as a two-tape nondeterministic Turing machine. The first tape contains the input word formula_15 to be tested, and the second tape is used by the machine to generate sentential forms from formula_13. The Turing machine then does the following:\n\n\nIt is easy to see that this Turing machine will generate all and only the sentential forms of formula_13 on its second tape after the last step is executed an arbitrary number of times, thus the language formula_14 must be recursively enumerable.\n\nThe reverse construction is also possible. Given some Turing machine, it is possible to create an equivalent unrestricted grammar which even uses only productions with one or more non-terminal symbols on their left-hand sides. Therefore, an arbitrary unrestricted grammar can always be equivalently converted to obey the latter form, by converting it to a Turing machine and back again. Some authors use the latter form as definition of \"unrestricted grammar\".\n\nThe decision problem of whether a given string formula_28 can be generated by a given unrestricted grammar is equivalent to the problem of whether it can be accepted by the Turing machine equivalent to the grammar. The latter problem is called the Halting problem and is undecidable.\n\nRecursively enumerable languages are closed under Kleene star, concatenation, union, and intersection, but not under set difference; see Recursively enumerable language#Closure properties.\n\nThe equivalence of unrestricted grammars to Turing machines implies the existence of a universal unrestricted grammar, a grammar capable of accepting any other unrestricted grammar's language given a description of the language. For this reason, it is theoretically possible to build a programming language based on unrestricted grammars (e.g. Thue).\n\n"}
{"id": "24734804", "url": "https://en.wikipedia.org/wiki?curid=24734804", "title": "Uriel Feige", "text": "Uriel Feige\n\nUriel Feige () is an Israeli computer scientist who was a doctoral student of Adi Shamir.\n\nUriel Feige currently holds the post of Professor at the Department of Computer Science and Applied Mathematics, the Weizmann Institute of Science, Rehovot in Israel.\n\nHe is notable for co-inventing the Feige–Fiat–Shamir identification scheme along with Amos Fiat and Adi Shamir.\n\nHe won the Gödel Prize in 2001 \"for the PCP theorem and its applications to hardness of approximation\".\n"}
