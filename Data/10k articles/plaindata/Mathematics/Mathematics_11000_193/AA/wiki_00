{"id": "3642210", "url": "https://en.wikipedia.org/wiki?curid=3642210", "title": "162 (number)", "text": "162 (number)\n\n162 (one hundred [and] sixty-two) is the natural number between 161 and 163.\n\nHaving only 2 and 3 as its prime divisors, 162 is a 3-smooth number. 162 is also a polygonal number and an abundant number, since formula_1\n\n"}
{"id": "378186", "url": "https://en.wikipedia.org/wiki?curid=378186", "title": "72 (number)", "text": "72 (number)\n\n72 (seventy-two) is the natural number following 71 and preceding 73. It is half a gross or 6 dozen (i.e., 60 in duodecimal).\n\nSeventy-two is the sum of four consecutive primes (13 + 17 + 19 + 23), as well as the sum of six consecutive primes (5 + 7 + 11 + 13 + 17 + 19). The product of 8 and 9, 72 is a pronic number.\n\nThe sum of Euler's totient function φ(\"x\") over the first fifteen integers is 72. There are 17 solutions to the equation φ(\"x\") = 72, more than any integer below 72, making it a highly totient number.\n\n72 is the smallest number whose fifth power is the sum of five smaller fifth powers: 19 + 43 + 46 + 47 + 67 = 72.\n\nThe sum of the eighth row of Lozanić's triangle is 72.\n\nIn a plane, the exterior angles of a regular pentagon measure 72 degrees each.\n\nIn base 10, the number 72 is a Harshad number.\n\n\n\n\nSeventy-two is also:\n\n\n"}
{"id": "391877", "url": "https://en.wikipedia.org/wiki?curid=391877", "title": "84 (number)", "text": "84 (number)\n\n84 (eighty-four) is the natural number following 83 and preceding 85.\n\n84 is:\n\n\nA hepteract is a seven-dimensional hypercube with 84 penteract 5-faces.\n\n\nEighty-four is also:\n\n\n"}
{"id": "407350", "url": "https://en.wikipedia.org/wiki?curid=407350", "title": "98 (number)", "text": "98 (number)\n\n98 (ninety-eight) is the natural number following 97 and preceding 99.\n\n\n\n\nNinety-eight is:\n\n\n\n"}
{"id": "50723592", "url": "https://en.wikipedia.org/wiki?curid=50723592", "title": "Bootstrap percolation", "text": "Bootstrap percolation\n\nIn statistical mechanics, bootstrap percolation is a percolation process in which a random initial configuration of active cells is selected from a lattice or other space, and then cells with few active neighbors are successively removed from the active set until the system stabilizes. The order in which this removal occurs makes no difference to the final stable state.\n\nWhen the threshold of active neighbors needed for an active cell to survive is high enough (depending on the lattice), the only stable states are states with no active cells, or states in which every cluster of active cells is infinitely large. For instance, on the square lattice with the von Neumann neighborhood, there are finite clusters with at least two active neighbors per cluster cell, but when three or four active neighbors are required, any stable cluster must be infinite. With three active neighbors needed to stay active, an infinite cluster must stretch infinitely in three or four of the possible cardinal directions, and any finite holes it contains will necessarily be rectangular. In this case, the critical probability is 1, meaning that when the probability of each cell being active in the initial state is anything less than 1, then almost surely there is no infinite cluster. If the initial state is active everywhere except for an square, within which one cell in each row and column is inactive, then these single-cell voids will merge to form a void that covers the whole square if and only if the inactive cells have the pattern of a separable permutation. In any higher dimension, for any threshold, there is an analogous critical probability below which all cells almost surely become inactive and above which some clusters almost surely survive.\n\nBootstrap percolation can be interpreted as a cellular automaton, resembling Conway's Game of Life, in which live cells die when they have too few live neighbors. However, unlike Conway's Life, cells that have become dead never become alive again. It can also be viewed as an epidemic model in which inactive cells are considered as infected and active cells with too many infected neighbors become infected themselves. The smallest threshold that allows some cells of an initial cluster to survive is the degeneracy of its adjacency graph, and the remnant of a cluster that survives with threshold \"k\" is the \"k\"-core of this graph.\n\nOne application of bootstrap percolation arises in the study of fault tolerance for distributed computing. If some processors in a large grid of processors fail (become inactive), then it may also be necessary to inactivate other processors with too few active neighbors, in order to preserve the high connectivity of the remaining network. The analysis of bootstrap percolation can be used to determine the failure probability that can be tolerated by the system.\n"}
{"id": "12941912", "url": "https://en.wikipedia.org/wiki?curid=12941912", "title": "CAT(k) space", "text": "CAT(k) space\n\nIn mathematics, a formula_1 space, where formula_2 is a real number, is a specific type of metric space. Intuitively, triangles in a formula_3 space are \"slimmer\" than corresponding \"model triangles\" in a standard space of constant curvature formula_2. In a formula_3 space, the curvature is bounded from above by formula_2. A notable special case is formula_7 complete formula_8 spaces are known as Hadamard spaces after the French mathematician Jacques Hadamard.\n\nOriginally, Aleksandrov called these spaces “formula_9 domain”.\nThe terminology formula_3 was coined by Mikhail Gromov in 1987 and is an acronym for Élie Cartan, Aleksandr Danilovich Aleksandrov and Victor Andreevich Toponogov (although Toponogov never explored curvature bounded above in publications).\n\nFor a real number formula_2, let formula_12 denote the unique simply connected surface (real 2-dimensional Riemannian manifold) with constant curvature formula_2. Denote by formula_14 the diameter of formula_12, which is formula_16 if formula_17 and formula_18 for formula_19.\n\nLet formula_20 be a geodesic metric space, i.e. a metric space for which every two points formula_21 can be joined by a geodesic segment, an arc length parametrized continuous curve formula_22, whose length\n\nis precisely formula_24. Let formula_25 be a triangle in formula_26 with geodesic segments as its sides. formula_25 is said to satisfy the formula_1 inequality if there is a comparison triangle formula_29 in the model space formula_12, with sides of the same length as the sides of formula_25, such that distances between points on formula_25 are less than or equal to the distances between corresponding points on formula_29.\n\nThe geodesic metric space formula_20 is said to be a formula_1 space if every geodesic triangle formula_25 in formula_26 with perimeter less than formula_38 satisfies the formula_3 inequality. A (not-necessarily-geodesic) metric space formula_40 is said to be a space with curvature formula_41 if every point of formula_26 has a geodesically convex formula_3 neighbourhood. A space with curvature formula_44 may be said to have non-positive curvature.\n\n\n\nAs a special case, a complete CAT(0) space is also known as a Hadamard space; this is by analogy with the situation for Hadamard manifolds. A Hadamard space is contractible (it has the homotopy type of a single point) and, between any two points of a Hadamard space, there is a unique geodesic segment connecting them (in fact, both properties also hold for general, possibly incomplete, CAT(0) spaces). Most importantly, distance functions in Hadamard spaces are convex: if \"σ\", \"σ\" are two geodesics in \"X\" defined on the same interval of time \"I\", then the function \"I\" → R given by\n\nis convex in \"t\".\n\nLet formula_20 be a formula_3 space. Then the following properties hold:\n\n\n\n\n"}
{"id": "46410903", "url": "https://en.wikipedia.org/wiki?curid=46410903", "title": "Cheryl's Birthday", "text": "Cheryl's Birthday\n\nCheryl's Birthday is the unofficial name given to a mathematics brain teaser that was asked in the Singapore and Asian Schools Math Olympiad, and was posted online on 10 April 2015 by Singapore TV presenter Kenneth Kong. It went viral in a matter of days. The quiz asked readers to determine the birthday of a girl named Cheryl using a handful of clues given to her friends Albert and Bernard.\n\nThe question was posted on Facebook by Singapore TV presenter Kenneth Kong. The posting drew thousands of comments, including several humorous ones—many aimed at Cheryl who apparently didn't want to disclose her birthday straight away. Kong posted it out of his debate with his wife, and he incorrectly thought it to be part of a mathematics question for a primary school examination, aimed at 10- to 11-year-old students, although it was actually part of the 2015 Singapore and Asian Schools Math Olympiad (SASMO) meant for 14-year-old students, a fact later acknowledged by Kong. The competition was held on 8 April 2015, with 28,000 participants from Singapore, Thailand, Vietnam, China and the UK. According to SASMO's organisers, the quiz was aimed at the top 40% of the contestants and aimed to \"sift out the better students\". SASMO's executive director told the BBC that \"there was a place for some kind of logical and analytical thinking in the workplace and in our daily lives\". \n\nThe question is number 24 in a list of 25 questions, and reads as follows:\n\n\"Albert and Bernard just become friends with Cheryl, and they want to know when her birthday is. Cheryl gives them a list of 10 possible dates:\nCheryl then tells Albert and Bernard separately the month and the day of her birthday respectively.\n\nAlbert: I don't know when Cheryl's birthday is, but I know that Bernard doesn't know too.<br>\nBernard: At first I don't know when Cheryl's birthday is, but I know now.<br>\nAlbert: Then I also know when Cheryl's birthday is.<br>\nSo when is Cheryl's birthday?\"\n\nThe answer to the question is July 16.\n\nThe answer can be deduced by progressively eliminating impossible dates. This is how Alex Bellos in the UK newspaper \"The Guardian\" presented its outcome:\n\nAfter the question went viral, some people suggested August 17 was an alternative answer to the question. This is rejected by the Singapore and Asian School Math Olympiads as a valid answer.\n\nThe solutions which arrive at this answer ignore that the latter part of:\nconveys information to Bernard about how Albert was able to deduce this. Bernard would only have known the birthday if the date was unique, 18 or 19. Albert therefore is able to deduce that \"Bernard doesn't know\" because he heard a month that does not contain those dates (July or August). Realizing this, Bernard can rule out May and June, which allows him to arrive at a unique birthday even if he is given the dates 15 or 16, not just 17.\n\nAs the SASMO organizers have pointed out, August 17 would be the solution if the sequence of statements instead began with Bernard saying that he did not know Cheryl's birthday:\n\nIt would also be the answer if the first statement was made by Cheryl instead:\nNote: The final statements by Albert in the two alternative examples only completes a dialogue, they are not needed by the reader to determine Cheryl's birthday as August 17.\n\nOn May 14, 2015, Nanyang Technological University uploaded a second part to the question on Facebook, this time titled \"Cheryl's Age\". It reads as follows:\n\nAlbert and Bernard now want to know how old Cheryl is.<br>\nCheryl: I have two younger brothers. The product of all our ages (i.e. my age and the ages of my two brothers) is 144, assuming that we use whole numbers for our ages. <br>\nAlbert: We still don't know your age. What other hints can you give us? <br>\nCheryl: The sum of all our ages is the bus number of this bus that we are on. <br>\nBernard: Of course we know the bus number, but we still don't know your age. <br>\nCheryl: Oh, I forgot to tell you that my brothers have the same age. <br>\nAlbert and Bernard: Oh, now we know your age.\n\nSo what is Cheryl's age?\n\nCheryl first says that she is the oldest of three siblings, and that their ages multiplied makes 144. 144 can be decomposed into prime number factors by the fundamental theorem of arithmetic (144 = 2 x 2 x 2 x 2 x 3 x 3), and all possible ages for Cheryl and her two brothers examined (for example, 16, 9, 1, or 8, 6, 3, and so on). The sums of the ages can then be computed. Because Bernard (who knows the bus number) cannot determine Cheryl's age despite having been told this sum, it must be a sum that is not unique among the possible solutions. On examining all the possible ages, it turns out there are two pairs of sets of possible ages that produce the same sum as each other: 9, 4, 4 and 8, 6, 3, which sum to 17, and 12, 4, 3 and 9, 8, 2, which sum to 19. Cheryl then says that her brothers are the same age, which eliminates the last three possibilities and leaves only 9, 4, 4, so we can deduce that Cheryl is 9 years old and her brothers are 4 years old, and the bus the three of them are on has the number 17.\n\n"}
{"id": "6626644", "url": "https://en.wikipedia.org/wiki?curid=6626644", "title": "Comparison theorem", "text": "Comparison theorem\n\nA comparison theorem is any of a variety of theorems that compare properties of various mathematical objects.\n\nIn the theory of differential equations, comparison theorems assert particular properties of solutions of a differential equation (or of a system thereof) provided that an auxiliary equation/inequality (or a system thereof) possesses a certain property. See also Lyapunov comparison principle\n\n\nIn Riemannian geometry it is a traditional name for a number of theorems that compare various metrics and provide various estimates in Riemannian geometry. \n\n\n"}
{"id": "52621851", "url": "https://en.wikipedia.org/wiki?curid=52621851", "title": "Convex layers", "text": "Convex layers\n\nIn computational geometry, the convex layers of a set of points in the Euclidean plane are a sequence of nested convex polygons having the points as their vertices. The outermost one is the convex hull of the points and the rest are formed in the same way recursively. The innermost layer may be degenerate, consisting only of one or two points.\nThe problem of constructing convex layers has also been called onion peeling or onion decomposition.\n\nAlthough constructing the convex layers by repeatedly finding convex hulls would be slower, it is possible to partition any set of formula_1 points into its convex layers in time formula_2.\n\nAn early application of the convex layers was in robust statistics, as a way of identifying outliers and measuring the central tendency of a set of sample points. In this context, the number of convex layers surrounding a given point is called its convex hull peeling depth, and the convex layers themselves are the depth contours for this notion of data depth.\n\nConvex layers may be used as part of an efficient range reporting data structure for listing all of the points in a query half-plane. The points in the half-plane from each successive layer may be found by a binary search to find the most extreme point in the direction of the half-plane, and then searching sequentially from there. Fractional cascading can be used to speed up the binary searches, giving total query time formula_3 to find formula_4 points out of a set of formula_1.\n\nThe points of an formula_6 grid have formula_7 convex layers, as do the same number of uniformly random points within any convex shape.\n"}
{"id": "26536158", "url": "https://en.wikipedia.org/wiki?curid=26536158", "title": "Cooperative coevolution", "text": "Cooperative coevolution\n\nCooperative Coevolution (CC) is an evolutionary computation method that divides a large problem into subcomponents and solves them independently in order to solve the large problem. \n\nThe subcomponents are also called species. The subcomponents are implemented as subpopulations and the only interaction between subpopulations is in the cooperative evaluation of each individual of the subpopulations. The general CC framework is nature inspired where the individuals of a particular group of species mate amongst themselves, however, mating in between different species is not feasible. The cooperative evaluation of each individual in a subpopulation is done by concatenating the current individual with the best individuals from the rest of the subpopulations as described by M. Potter.\n\nThe cooperative coevolution framework has been applied to real world problems such as pedestrian detection systems, large-scale function optimization and neural network training.\nIt has also be further extended into another method, called Constructive cooperative coevolution.\n\n \"i\" = 0 \n\n"}
{"id": "42012361", "url": "https://en.wikipedia.org/wiki?curid=42012361", "title": "Cosheaf", "text": "Cosheaf\n\nIn topology, a branch of mathematics, a cosheaf with values in an ∞-category \"C\" that admits colimits is a functor \"F\" from the category of open subsets of a topological space \"X\" (more precisely its nerve) to \"C\" such that\n\nThe basic example is formula_6 where on the right is the singular chain complex of \"U\" with coefficients in an abelian group \"A\".\n\nExample: If \"f\" is a continuous map, then formula_7 is a cosheaf.\n\n\n"}
{"id": "46877921", "url": "https://en.wikipedia.org/wiki?curid=46877921", "title": "Deterministic scale-free network", "text": "Deterministic scale-free network\n\nA scale-free network is a type of networks that is of particular interest of network science. It is characterized by its degree distribution following a power law. While the most widely known generative models for scale-free networks are stochastic, such as the Barabási–Albert model or the Fitness model can reproduce many properties of real-life networks by assuming preferential attachment and incremental growth, the understanding of deterministic scale-free networks leads to valuable, analytical results.\n\nAlthough there are multiple deterministic models to generate scale-free networks, it is common, that they define a simple algorithm of adding nodes, which is then iteratively repeated and thus leads to a complex network. As these models are deterministic, it is possible to get analytic results about the degree distribution, clustering coefficient, average shortest path length, random walk centrality and other relevant network metrics.\n\nDeterministic models are especially useful to explain empirically observed phenomena and demonstrate the existence of networks with certain properties. For example, the Barabási-Albert model predicts a decreasing average clustering coefficient as the number of nodes increases, whereas empirical evidence suggests otherwise. Hierarchical network models can explain this phenomenon while also retaining the scale-free property. Another notable example is that it is possible to generate networks deterministically, which are scale-free and linear world at the same time, showing that small-world property is not a necessary consequence of the scale-free property.\n\nThe exact properties of the generated networks depend on the particular algorithm with which they are constructed, therefore there are not many common properties. The single unifying property is scale-freeness, that is, the degree distribution of the nodes always follows a power law at least asymptotically, which means that a randomly selected node has k edges with probability\n\nformula_1\n\nwhere the degree coefficient (λ) depends on the model parameters.\n\nAlso, because of the iterative construction, many of the models produce hierarchical networks with fractal-like properties. Other properties, such as network diameter, average path length, clustering coefficient vary across models depending on the construction.\n\nOne of the first deterministic scale-free network models was proposed by Barabási, Ravasz and Vicsek. It involved the generation of a hierarchical, scale-free network by following a set of simple steps:\n\nStep 0: We start from a single node, that we designate as the root of the graph.\n\nStep 1: We add two more nodes, and connect each of them to the root.\n\nStep 2: We add two units of three nodes, each unit identical to the network created in the previous iteration (step 1), and we connect each of the bottom nodes [...] of these two units to the root. That is, the root will gain four more new links.\n\nStep 3: We add two units of nine nodes each, identical to the units generated in the previous iteration, and connect all eight bottom nodes of the two new units to the root.\n\nStep n: Add two units of 3n−1 nodes each, identical to the network created in the previous iteration (step n−1), and connect each of the 2n bottom nodes of these two units to the root of the network.\nIt has been analytically shown, that the degree distribution of this network asymptotically follows a power law with a degree exponent of ln(3)/ln(2). Iguchi and Yamada has shown that most of the important network statistics can be derived analytically in this model.\n\nThis model based on a binary tree exhibits both the scale-free property and the ultra small-world property (the average shorthest path increases more slowly than the logarithm of the number of nodes) for an arbitrary number of nodes.\n\nThe authors propose two models which exhibit the power law in the degree distribution while the average shortest path grows linearly with the number of nodes. This proves that not every scale-free network has the small-world property. One of their models shows that this can also be true in the absence of degree correlation.\n"}
{"id": "5944768", "url": "https://en.wikipedia.org/wiki?curid=5944768", "title": "Differential (infinitesimal)", "text": "Differential (infinitesimal)\n\nThe term differential is used in calculus to refer to an infinitesimal (infinitely small) change in some varying quantity. For example, if \"x\" is a variable, then a change in the value of \"x\" is often denoted Δ\"x\" (pronounced \"delta x\"). The differential \"dx\" represents an infinitely small change in the variable \"x\". The idea of an infinitely small or infinitely slow change is extremely useful intuitively, and there are a number of ways to make the notion mathematically precise.\n\nUsing calculus, it is possible to relate the infinitely small changes of various variables to each other mathematically using derivatives. If \"y\" is a function of \"x\", then the differential \"dy\" of \"y\" is related to \"dx\" by the formula\nwhere \"dy\"/\"dx\" denotes the derivative of \"y\" with respect to \"x\". This formula summarizes the intuitive idea that the derivative of \"y\" with respect to \"x\" is the limit of the ratio of differences Δ\"y\"/Δ\"x\" as Δ\"x\" becomes infinitesimal.\n\nThere are several approaches for making the notion of differentials mathematically precise.\n\nThese approaches are very different from each other, but they have in common the idea to be \"quantitative\", i.e., to say not just that a differential is infinitely small, but \"how\" small it is.\n\nInfinitesimal quantities played a significant role in the development of calculus. Archimedes used them, even though he didn't believe that arguments involving infinitesimals were rigorous. Isaac Newton referred to them as fluxions. However, it was Gottfried Leibniz who coined the term \"differentials\" for infinitesimal quantities and introduced the notation for them which is still used today.\n\nIn Leibniz's notation, if \"x\" is a variable quantity, then \"dx\" denotes an infinitesimal change in the variable \"x\". Thus, if \"y\" is a function of \"x\", then the derivative of \"y\" with respect to \"x\" is often denoted \"dy\"/\"dx\", which would otherwise be denoted (in the notation of Newton or Lagrange) \"ẏ\" or \"y\". The use of differentials in this form attracted much criticism, for instance in the famous pamphlet The Analyst by Bishop Berkeley. Nevertheless, the notation has remained popular because it suggests strongly the idea that the derivative of \"y\" at \"x\" is its instantaneous rate of change (the slope of the graph's tangent line), which may be obtained by taking the limit of the ratio Δ\"y\"/Δ\"x\" of the change in \"y\" over the change in \"x\", as the change in \"x\" becomes arbitrarily small. Differentials are also compatible with dimensional analysis, where a differential such as \"dx\" has the same dimensions as the variable \"x\".\n\nDifferentials are also used in the notation for integrals because an integral can be regarded as an infinite sum of infinitesimal quantities: the area under a graph is obtained by subdividing the graph into infinitely thin strips and summing their areas. In an expression such as\nthe integral sign (which is a modified long s) denotes the infinite sum, \"f\"(\"x\") denotes the \"height\" of a thin strip, and the differential \"dx\" denotes its infinitely thin width.\n\nThere is a simple way to make precise sense of differentials by regarding them as linear maps. To illustrate, suppose \"f\"(\"x\") is a real-valued function on R. We can reinterpret the variable \"x\" in \"f\"(\"x\") as being a function rather than a number, namely the identity map on the real line, which takes a real number \"p\" to itself: \"x\"(\"p\") = \"p\". Then \"f\"(\"x\") is the composite of \"f\" with \"x\", whose value at \"p\" is \"f\"(\"x\"(\"p\")) = \"f\"(\"p\"). The differential \"df\" (which of course depends on \"f\") is then a function whose value at \"p\" (usually denoted \"df\") is not a number, but a linear map from R to R. Since a linear map from R to R is given by a 1×1 matrix, it is essentially the same thing as a number, but the change in the point of view allows us to think of \"df\" as an infinitesimal and \"compare\" it with the \"standard infinitesimal\" \"dx\", which is again just the identity map from R to R (a 1×1 matrix with entry 1). The identity map has the property that if ε is very small, then \"dx\"(ε) is very small, which enables us to regard it as infinitesimal. The differential \"df\" has the same property, because it is just a multiple of \"dx\", and this multiple is the derivative \"f\" ′(\"p\") by definition. We therefore obtain that \"df\" = \"f\" ′(\"p\") \"dx\", and hence \"df\" = \"f\" ′ \"dx\". Thus we recover the idea that \"f\" ′ is the ratio of the differentials \"df\" and \"dx\".\n\nThis would just be a trick were it not for the fact that:\n\nFor instance, if \"f\" is a function from R to R, then we say that \"f\" is \"differentiable\" at \"p\" ∈ R if there is a linear map \"df\" from R to R such that for any ε > 0, there is a neighbourhood \"N\" of \"p\" such that for \"x\" ∈ \"N\",\n\nWe can now use the same trick as in the one-dimensional case and think of the expression \"f\"(\"x\", \"x\", …, \"x\") as the composite of \"f\" with the standard coordinates \"x\", \"x\", …, \"x\" on R (so that \"x\"(\"p\") is the \"j\"-th component of \"p\" ∈ R). Then the differentials (\"dx\"), (\"dx\"), (\"dx\") at a point \"p\" form a basis for the vector space of linear maps from R to R and therefore, if \"f\" is differentiable at \"p\", we can write \"df\" as a linear combination of these basis elements:\n\nThe coefficients \"D\"\"f\"(\"p\") are (by definition) the partial derivatives of \"f\" at \"p\" with respect to \"x\", \"x\", …, \"x\". Hence, if \"f\" is differentiable on all of R, we can write, more concisely:\n\nIn the one-dimensional case this becomes\nas before.\n\nThis idea generalizes straightforwardly to functions from R to R. Furthermore, it has the decisive advantage over other definitions of the derivative that it is invariant under changes of coordinates. This means that the same idea can be used to define the differential of smooth maps between smooth manifolds.\n\nAside: Note that the existence of all the partial derivatives of \"f\"(\"x\") at \"x\" is a necessary condition for the existence of a differential at \"x\". However it is not a sufficient condition. For counterexamples, see Gâteaux derivative.\n\nIn algebraic geometry, differentials and other infinitesimal notions are handled in a very explicit way by accepting that the coordinate ring or structure sheaf of a space may contain nilpotent elements. The simplest example is the ring of dual numbers R[\"ε\"], where \"ε\" = 0.\n\nThis can be motivated by the algebro-geometric point of view on the derivative of a function \"f\" from R to R at a point \"p\". For this, note first that \"f\" − \"f\"(\"p\") belongs to the ideal \"I\" of functions on R which vanish at \"p\". If the derivative \"f\" vanishes at \"p\", then \"f\" − \"f\"(\"p\") belongs to the square \"I\" of this ideal. Hence the derivative of \"f\" at \"p\" may be captured by the equivalence class [\"f\" − \"f\"(\"p\")] in the quotient space \"I\"/\"I\", and the 1-jet of \"f\" (which encodes its value and its first derivative) is the equivalence class of \"f\" in the space of all functions modulo \"I\". Algebraic geometers regard this equivalence class as the \"restriction\" of \"f\" to a \"thickened\" version of the point \"p\" whose coordinate ring is not R (which is the quotient space of functions on R modulo \"I\") but R[\"ε\"] which is the quotient space of functions on R modulo \"I\". Such a thickened point is a simple example of a scheme.\n\nA third approach to infinitesimals is the method of synthetic differential geometry or smooth infinitesimal analysis. This is closely related to the algebraic-geometric approach, except that the infinitesimals are more implicit and intuitive. The main idea of this approach is to replace the category of sets with another category of \"smoothly varying sets\" which is a topos. In this category, one can define the real numbers, smooth functions, and so on, but the real numbers \"automatically\" contain nilpotent infinitesimals, so these do not need to be introduced by hand as in the algebraic geometric approach. However the logic in this new category is not identical to the familiar logic of the category of sets: in particular, the law of the excluded middle does not hold. This means that set-theoretic mathematical arguments only extend to smooth infinitesimal analysis if they are \"constructive\" (e.g., do not use proof by contradiction). Some regard this disadvantage as a positive thing, since it forces one to find constructive arguments wherever they are available.\n\nThe final approach to infinitesimals again involves extending the real numbers, but in a less drastic way. In the nonstandard analysis approach there are no nilpotent infinitesimals, only invertible ones, which may be viewed as the reciprocals of infinitely large numbers. Such extensions of the real numbers may be constructed explicitly using equivalence classes of sequences of real numbers, so that, for example, the sequence (1, 1/2, 1/3, …, 1/\"n\", …) represents an infinitesimal. The first-order logic of this new set of hyperreal numbers is the same as the logic for the usual real numbers, but the completeness axiom (which involves second-order logic) does not hold. Nevertheless, this suffices to develop an elementary and quite intuitive approach to calculus using infinitesimals, see transfer principle.\n\n\n"}
{"id": "21988115", "url": "https://en.wikipedia.org/wiki?curid=21988115", "title": "Discrete Mathematics (journal)", "text": "Discrete Mathematics (journal)\n\nDiscrete Mathematics is a biweekly peer-reviewed scientific journal in the broad area of discrete mathematics, combinatorics, graph theory, and their applications. It was established in 1971 and is published by North-Holland Publishing Company. It publishes both short notes, full length contributions, as well as survey articles. In addition, the journal publishes a number of special issues each year dedicated to a particular topic. Although originally it published articles in French and German, it now allows only English language articles. The editor-in-chief is Douglas West (University of Illinois, Urbana).\n\nThe journal was established in 1971. The very first article it published was written by Paul Erdős, who went on to publish a total of 84 papers in the journal.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2012 impact factor of 0.578.\n\n"}
{"id": "54365238", "url": "https://en.wikipedia.org/wiki?curid=54365238", "title": "Erdős–Tetali theorem", "text": "Erdős–Tetali theorem\n\nIn additive number theory, an area of mathematics, the Erdős–Tetali theorem is an existence theorem concerning economical additive basis of every order. More specifically, it states that for every fixed integer formula_1, there exists a subset of the natural numbers formula_2 satisfyingformula_3where formula_4 denotes the number of ways that a natural number \"n\" can be expressed as the sum of \"h\" elements of \"B.\"\n\nThe theorem is named after Paul Erdős and Prasad V. Tetali, who published it in 1990.\n\nThe original motivation for this result is attributed to a problem posed by S. Sidon in 1932 on \"economical bases\". An additive basis formula_5 is called \"economical\" (or sometimes \"thin\") when it is an additive basis of order \"h\" andformula_6that is, formula_7 for every formula_8. In other words, these are additive bases that use as few numbers as possible to represent a given \"n\", and yet represent every natural number. Related concepts include formula_9-sequences and the Erdős–Turán conjecture on additive bases.\n\nSidon's question was whether an economical basis of order 2 exists. A positive answer was given by P. Erdős in 1956, settling the yet-to-be-called Erdős–Tetali theorem for the case formula_10. Although the general version was believed to be true, no complete proof appeared in the literature before the paper from Erdős & Tetali (1990).\n\nThe proof is an instance of the probabilistic method, and can be divided into three main steps. First, one start by defining a \"random sequence\" formula_11 byformula_12where formula_13 is some large real constant, formula_14 is a fixed integer and \"n\" is sufficiently large so that the above formula is well-defined. A detailed discussion on the probability space associated with this type of construction may be found on Halberstam & Roth (1983).\n\nSecondly, one then shows that the expected value of the random variable formula_15 has the order of log. That is,formula_16Finally, one shows that formula_15 almost surely concentrates around its mean. More explicitly:formula_18This is the critical step of the proof. Originally it was dealt with by means of Janson's inequality, a type of concentration inequality for multivariate polynomials. Tao & Vu (2006) present this proof with a more sophisticated two-sided concentration inequality by V. Vu (2000), thus relatively simplifying this step. Alon & Spencer (2016) classify this proof as an instance of the Poisson paradigm.\n\nAll the known proofs of Erdős–Tetali theorem are, by the nature of the infinite probability space used, non-constructive proofs. However, Kolountzakis (1995) showed the existence of a recursive set formula_19 satisfying formula_20 such that formula_21 takes polynomial time in \"n\" to be computed. The question for formula_22 remains open.\n\nGiven an arbitrary additive basis formula_23, one can ask whether there exists formula_24 such that formula_25 is an economical basis. V. Vu (2000) showed that this is the case for Waring bases formula_26, where for every fixed \"k\" there are economical subbases of formula_27 of order formula_28 for every formula_29, for some large computable constant formula_30.\n\nThe original Erdős–Turán conjecture on additive bases states, in its most general form, that if formula_5 is an additive basis of order \"h\" then formula_32. Nonetheless, in his 1956 paper on the case formula_10 of Erdős–Tetali, P. Erdős asked whether it could be the case that actually formula_34 whenever formula_5 is an additive basis of order 2. The question naturally extends to formula_22, making it a way stronger assertion than that of Erdős–Turán. In some sense, what is being conjectured is that there are no additive bases substantially more economical than those guaranteed to exist by the Erdős–Tetali theorem.\n\n"}
{"id": "12219849", "url": "https://en.wikipedia.org/wiki?curid=12219849", "title": "Extension by definitions", "text": "Extension by definitions\n\nIn mathematical logic, more specifically in the proof theory of first-order theories, extensions by definitions formalize the introduction of new symbols by means of a definition. For example, it is common in naive set theory to introduce a symbol formula_1 for the set which has no member. In the formal setting of first-order theories, this can be done by adding to the theory a new constant formula_1 and the new axiom formula_3, meaning 'for all \"x\", \"x\" is not a member of formula_1'. It can then be proved that doing so adds essentially nothing to the old theory, as should be expected from a definition. More precisely, the new theory is a conservative extension of the old one.\n\n\"Let\" formula_5 be a first-order theory and formula_6 a formula of formula_5 such that formula_8, ..., formula_9 are distinct and include the variables free in formula_6. Form a new first-order theory formula_11 from formula_5 by adding a new formula_13-ary relation symbol formula_14, the logical axioms featuring the symbol formula_14 and the new axiom\ncalled the \"defining axiom\" of formula_14.\n\nIf formula_18 is a formula of formula_11, let formula_20 be the formula of formula_5 obtained from formula_18 by replacing any occurrence of formula_23 by formula_24 (changing the bound variables in formula_25 if necessary so that the variables occurring in the formula_26 are not bound in formula_24). Then the following hold:\n\n\nThe fact that formula_11 is a conservative extension of formula_5 shows that the defining axiom of formula_14 cannot be used to prove new theorems. The formula formula_20 is called a \"translation\" of formula_18 into formula_5. Semantically, the formula formula_20 has the same meaning as formula_18, but the defined symbol formula_14 has been eliminated.\n\nLet formula_5 be a first-order theory (with equality) and formula_42 a formula of formula_5 such that formula_44, formula_8, ..., formula_9 are distinct and include the variables free in formula_42. Assume that we can prove\nin formula_5, i.e. for all formula_8, ..., formula_9, there exists a unique \"y\" such that formula_42. Form a new first-order theory formula_11 from formula_5 by adding a new formula_13-ary function symbol formula_56, the logical axioms featuring the symbol formula_56 and the new axiom\ncalled the \"defining axiom\" of formula_56.\n\nLet formula_18 be any formula of formula_11. We define formula formula_20 of formula_5 recursively as follows. If the new symbol formula_56 does not occur in formula_18, let formula_20 be formula_18. Otherwise, choose an occurrence of formula_68 in formula_18 such that formula_56 does not occur in the terms formula_26, and let formula_72 be obtained from formula_18 by replacing that occurrence by a new variable formula_74. Then since formula_56 occurs in formula_72 one less time than in formula_18, the formula formula_78 has already been defined, and we let formula_20 be \n(changing the bound variables in formula_25 if necessary so that the variables occurring in the formula_26 are not bound in formula_83). For a general formula formula_18, the formula formula_20 is formed by replacing every occurrence of an atomic subformula formula_72 by formula_78. Then the following hold:\n\n\nThe formula formula_20 is called a \"translation\" of formula_18 into formula_5. As in the case of relation symbols, the formula formula_20 has the same meaning as formula_18, but the new symbol formula_56 has been eliminated.\n\nThe construction of this paragraph also works for constants, which can be viewed as 0-ary function symbols.\n\nA first-order theory formula_11 obtained from formula_5 by successive introductions of relation symbols and function symbols as above is called an extension by definitions of formula_5. Then formula_11 is a conservative extension of formula_5, and for any formula formula_18 of formula_11 we can form a formula formula_20 of formula_5, called a \"translation\" of formula_18 into formula_5, such that formula_28 is provable in formula_11. Such a formula is not unique, but any two of them can be proved to be equivalent in \"T\".\n\nIn practice, an extension by definitions formula_11 of \"T\" is not distinguished from the original theory \"T\". In fact, the formulas of formula_11 can be thought of as \"abbreviating\" their translations into \"T\". The manipulation of these abbreviations as actual formulas is then justified by the fact that extensions by definitions are conservative.\n\n\n"}
{"id": "35755218", "url": "https://en.wikipedia.org/wiki?curid=35755218", "title": "France Križanič", "text": "France Križanič\n\nFrance Križanič (3 March 1928 – 17 January 2002) was a Slovene mathematician, author of numerous books and textbooks on mathematics. He was professor of mathematical analysis at the Faculty of Mathematics and Physics of the University of Ljubljana.\n\nKrižanič won the Levstik Award twice, in 1951 for his book \"Kratkočasna matematika\" (Maths for Fun) and in 1960 for \"Križem po matematiki\" and \"Elektronski aritmetični računalniki\" (Criss Cross Across Maths \"and\" Electronic Calculators).\n\n"}
{"id": "3086309", "url": "https://en.wikipedia.org/wiki?curid=3086309", "title": "Fujita conjecture", "text": "Fujita conjecture\n\nIn mathematics, Fujita's conjecture is a problem in the theories of algebraic geometry and complex manifolds, unsolved . It is named after Takao Fujita, who formulated it in 1985.\n\nIn complex geometry, the conjecture states that for a positive holomorphic line bundle \"L\" on a compact complex manifold \"M\", the line bundle \"K\" ⊗ \"L\" (where \"K\" is a canonical line bundle of \"M\") is \n\nwhere \"n\" is the complex dimension of \"M\".\n\nNote that for large \"m\" the line bundle \"K\" ⊗ \"L\" is very ample by the standard Serre's vanishing theorem (and its complex analytic variant). Fujita conjecture provides an explicit bound on \"m\", which is optimal for projective spaces.\n\nFor surfaces the Fujita conjecture follows from Reider's theorem. For three-dimensional algebraic varieties Ein and Lazarsfeld in 1993 proved the first part of the Fujita conjecture, i.e. that \"m\"≥4 implies global generation.\n\n\n"}
{"id": "4183150", "url": "https://en.wikipedia.org/wiki?curid=4183150", "title": "Gamma process", "text": "Gamma process\n\nA gamma process is a random process with independent gamma distributed increments. Often written as formula_1, it is a pure-jump increasing Lévy process with intensity measure formula_2 for positive formula_3. Thus jumps whose size lies in the interval formula_4 occur as a Poisson process with intensity formula_5 The parameter formula_6 controls the rate of jump arrivals and the scaling parameter formula_7 inversely controls the jump size. It is assumed that the process starts from a value 0 at \"t\"=0.\n\nThe gamma process is sometimes also parameterised in terms of the mean (formula_8) and variance (formula_9) of the increase per unit time, which is equivalent to formula_10 and formula_11.\n\nSince we use the Gamma function in these properties, we may write the process at time formula_12 as formula_13 to eliminate ambiguity.\n\nSome basic properties of the gamma process are:\n\nThe marginal distribution of a gamma process at time formula_12 is a gamma distribution with mean formula_15 and variance formula_16 \n\nThat is, its density formula_17 is given by formula_18\n\nMultiplication of a gamma process by a scalar constant formula_19 is again a gamma process with different mean increase rate.\n\nThe sum of two independent gamma processes is again a gamma process.\n\nThe gamma process is used as the distribution for random time change in the variance gamma process.\n\n"}
{"id": "340621", "url": "https://en.wikipedia.org/wiki?curid=340621", "title": "George Boolos", "text": "George Boolos\n\nGeorge Stephen Boolos (; September 4, 1940 – May 27, 1996) was an American philosopher and a mathematical logician who taught at the Massachusetts Institute of Technology.\n\nBoolos graduated from Princeton University in 1961 with an A.B. in mathematics. Oxford University awarded him the B.Phil. in 1963. In 1966, he obtained the first Ph.D. in philosophy ever awarded by the Massachusetts Institute of Technology, under the direction of Hilary Putnam. After teaching three years at Columbia University, he returned to MIT in 1969, where he spent the rest of his career until his death from cancer.\n\nA charismatic speaker well known for his clarity and wit, he once delivered a lecture (1994b) giving an account of Gödel's second incompleteness theorem, employing only words of one syllable. At the end of his viva, Hilary Putnam asked him, \"And tell us, Mr. Boolos, what does the analytical hierarchy have to do with the real world?\" Without hesitating Boolos replied, \"It's part of it\".\n\nAn expert on puzzles of all kinds, in 1993 Boolos reached the London Regional Final of \"The Times\" crossword competition. His score was one of the highest ever recorded by an American. He wrote a paper on \"The Hardest Logic Puzzle Ever\"—one of many puzzles created by Raymond Smullyan.\n\nBoolos coauthored with Richard Jeffrey the first three editions of the classic university text on mathematical logic, \"Computability and Logic\". The book is now in its fifth edition, the last two editions updated by John P. Burgess.\n\nKurt Gödel wrote the first paper on provability logic, which applies modal logic—the logic of necessity and possibility—to the theory of mathematical proof, but Gödel never developed the subject to any significant extent. Boolos was one of its earliest proponents and pioneers, and he produced the first book-length treatment of it, \"The Unprovability of Consistency\", published in 1979. The solution of a major unsolved problem some years later led to a new treatment, \"The Logic of Provability\", published in 1993. The modal-logical treatment of provability helped demonstrate the \"intensionality\" of Gödel's Second Incompleteness Theorem, meaning that the theorem's correctness depends on the precise formulation of the provability predicate. These conditions were first identified by David Hilbert and Paul Bernays in their \"Grundlagen der Arithmetik\". The unclear status of the Second Theorem was noted for several decades by logicians such as Georg Kreisel and Leon Henkin, who asked whether the formal sentence expressing \"This sentence is provable\" (as opposed to the Gödel sentence, \"This sentence is not provable\") was provable and hence true. Martin Löb showed Henkin's conjecture to be true, as well as identifying an important \"reflection\" principle also neatly codified using the modal logical approach. Some of the key provability results involving the representation of provability predicates had been obtained earlier using very different methods by Solomon Feferman.\n\nBoolos was an authority on the 19th-century German mathematician and philosopher Gottlob Frege. Boolos proved a conjecture due to Crispin Wright (and also proved, independently, by others), that the system of Frege's \"Grundgesetze\", long thought vitiated by Russell's paradox, could be freed of inconsistency by replacing one of its axioms, the notorious Basic Law V with Hume's Principle. The resulting system has since been the subject of intense work.\n\nBoolos argued that if one reads the second-order variables in monadic second-order logic plurally, then second-order logic can be interpreted as having no ontological commitment to entities other than those over which the first-order variables range. The result is plural quantification. David Lewis employed plural quantification in his \"Parts of Classes\" to derive a system in which Zermelo–Fraenkel set theory and the Peano axioms were all theorems. While Boolos is usually credited with plural quantification, Peter Simons (1982) has argued that the essential idea can be found in the work of Stanislaw Leśniewski.\n\nShortly before his death, Boolos chose 30 of his papers to be published in a book. The result is perhaps his most highly regarded work, his posthumous \"Logic, Logic, and Logic\". This book reprints much of Boolos's work on the rehabilitation of Frege, as well as a number of his papers on set theory, second-order logic and nonfirstorderizability, plural quantification, proof theory, and three short insightful papers on Gödel's Incompleteness Theorem. There are also papers on Dedekind, Cantor, and Russell.\n\n\n\n\n\n"}
{"id": "58729748", "url": "https://en.wikipedia.org/wiki?curid=58729748", "title": "Giulia Di Nunno", "text": "Giulia Di Nunno\n\nGiulia Di Nunno (born 1973) is an Italian mathematician specializing in stochastic analysis and financial mathematics who works as a professor of mathematics at the University of Oslo, with an adjunct appointment at the Norwegian School of Economics.\nAs well as for her research, Di Nunno is known for promoting mathematics in Africa.\n\nDi Nunno earned a degree in mathematics from the University of Milan in 1998, including research on stochastic functions with Yurii Rozanov. She moved to the University of Pavia for doctoral studies, continuing with Rozanov as an informal mentor but under the official supervision of Eugenio Regazzini. She completed her Ph.D. in 2003; her dissertation was \"On stochastic differentiation with applications to minimal variance hedging\".\nShe joined the University of Oslo in 2003, and added her affiliation with the Norwegian School of Economics in 2009.\n\nDi Nunno is the chair of the European Mathematical Society's Committee for Developing Countries, and has worked to promote young researchers to visit Africa and to establish \"Emerging Regional Centres of Excellence\" there. The International Council for Industrial and Applied Mathematics gave her their 2019 Su Buchin Prize for this work, citing her \"long-lasting record actively and efficiently encouraging top-level mathematical research and education in developing African countries\".\n\nWith Bernt Karsten Øksendal and Frank Proske, Di Nunno is a co-author of the book \"Malliavin calculus for Lévy processes with applications to finance\" (Springer, 2009). She also co-edited \"Advanced Mathematical Methods for Finance\" (Springer, 1011) with Øksendal.\n\n"}
{"id": "5108937", "url": "https://en.wikipedia.org/wiki?curid=5108937", "title": "Heteroclinic cycle", "text": "Heteroclinic cycle\n\nIn mathematics, a heteroclinic cycle is an invariant set in the phase space of a dynamical system. It is a topological circle of equilibrium points and connecting heteroclinic orbits. If a heteroclinic cycle is asymptotically stable, approaching trajectories spend longer and longer periods of time in a neighbourhood of successive equilibria.\n\nA robust heteroclinic cycle is one which persists under small changes in the underlying dynamical system. Robust cycles often arise in the presence of symmetry or other constraints which force the existence of invariant hyperplanes. A prototypical example of a robust heteroclinic cycle is the Guckenheimer–Holmes cycle.\n\n\n"}
{"id": "82208", "url": "https://en.wikipedia.org/wiki?curid=82208", "title": "International Mathematical Olympiad", "text": "International Mathematical Olympiad\n\nThe International Mathematical Olympiad (IMO) is an annual six-problem mathematical olympiad for pre-college students, and is the oldest of the International Science Olympiads. The first IMO was held in Romania in 1959. It has since been held annually, except in 1980. More than 100 countries, representing over 90% of the world's population, send teams of up to six students, plus one team leader, one deputy leader, and observers.\n\nThe content ranges from extremely difficult algebra and pre-calculus problems to problems on branches of mathematics not conventionally covered at school and often not at university level either, such as projective and complex geometry, functional equations, combinatorics, and well-grounded number theory, of which extensive knowledge of theorems is required. Calculus, though allowed in solutions, is never required, as there is a principle that anyone with a basic understanding of mathematics should understand the problems, even if the solutions require a great deal more knowledge. Supporters of this principle claim that this allows more universality and creates an incentive to find elegant, deceptively simple-looking problems which nevertheless require a certain level of ingenuity.\n\nThe selection process differs by country, but it often consists of a series of tests which admit fewer students at each progressing test. Awards are given to approximately the top-scoring 50% of the individual contestants. Teams are not officially recognized—all scores are given only to individual contestants, but team scoring is unofficially compared more than individual scores. Contestants must be under the age of 20 and must not be registered at any tertiary institution. Subject to these conditions, an individual may participate any number of times in the IMO.\n\nThe International Mathematical Olympiad is one of the most prestigious mathematical competitions in the world. In January 2011, Google sponsored €1 million to the International Mathematical Olympiad organization.\n\nThe first IMO was held in Romania in 1959. Since then it has been held every year except in 1980. That year, it was cancelled due to internal strife in Mongolia. It was initially founded for eastern European member countries of the Warsaw Pact, under the USSR bloc of influence, but later other countries participated as well. Because of this eastern origin, the IMOs were first hosted only in eastern European countries, and gradually spread to other nations.\n\nSources differ about the cities hosting some of the early IMOs. This may be partly because leaders are generally housed well away from the students, and partly because after the competition the students did not always stay based in one city for the rest of the IMO. The exact dates cited may also differ, because of leaders arriving before the students, and at more recent IMOs the IMO Advisory Board arriving before the leaders.\n\nSeveral students, such as Zhuo Qun Song, Teodor von Burg, Lisa Sauermann, John Lian, Josh Li and Christian Reiher, have performed exceptionally well in the IMO, winning multiple gold medals. Others, such as Grigory Margulis, Jean-Christophe Yoccoz, Laurent Lafforgue, Stanislav Smirnov, Terence Tao, Sucharit Sarkar, Grigori Perelman, Ngô Bảo Châu and Maryam Mirzakhani have gone on to become notable mathematicians. Several former participants have won awards such as the Fields Medal.\n\nThe examination consists of six problems. Each problem is worth seven points, so the maximum total score is 42 points. No calculators are allowed. The examination is held over two consecutive days; each day the contestants have four-and-a-half hours to solve three problems. The problems chosen are from various areas of secondary school mathematics, broadly classifiable as geometry, number theory, algebra, and combinatorics. They require no knowledge of higher mathematics such as calculus and analysis, and solutions are often short and elementary. However, they are usually disguised so as to make the solutions difficult. Prominently featured are algebraic inequalities, complex numbers, and construction-oriented geometrical problems, though in recent years the latter has not been as popular as before.\n\nEach participating country, other than the host country, may submit suggested problems to a Problem Selection Committee provided by the host country, which reduces the submitted problems to a shortlist. The team leaders arrive at the IMO a few days in advance of the contestants and form the IMO Jury which is responsible for all the formal decisions relating to the contest, starting with selecting the six problems from the shortlist. The Jury aims to order the problems so that the order in increasing difficulty is Q1, Q4, Q2, Q5, Q3 and Q6. As the leaders know the problems in advance of the contestants, they are kept strictly separated and observed.\n\nEach country's marks are agreed between that country's leader and deputy leader and coordinators provided by the host country (the leader of the team whose country submitted the problem in the case of the marks of the host country), subject to the decisions of the chief coordinator and ultimately a jury if any disputes cannot be resolved.\n\nThe selection process for the IMO varies greatly by country. In some countries, especially those in East Asia, the selection process involves several tests of a difficulty comparable to the IMO itself. The Chinese contestants go through a camp. In others, such as the United States, possible participants go through a series of easier standalone competitions that gradually increase in difficulty. In the United States, the tests include the American Mathematics Competitions, the American Invitational Mathematics Examination, and the United States of America Mathematical Olympiad, each of which is a competition in its own right. For high scorers in the final competition for the team selection, there also is a summer camp, like that of China.\n\nIn countries of the former Soviet Union and other eastern European countries, a team has in the past been chosen several years beforehand, and they are given special training specifically for the event. However, such methods have been discontinued in some countries. In Ukraine, for instance, selection tests consist of four olympiads comparable to the IMO by difficulty and schedule. While identifying the winners, only the results of the current selection olympiads are considered.\n\nThe participants are ranked based on their individual scores. Medals are awarded to the highest ranked participants; slightly fewer than half of them receive a medal. The cutoffs (minimum scores required to receive a gold, silver or bronze medal respectively) are then chosen so that the numbers of gold, silver and bronze medals awarded are approximately in the ratios 1:2:3. Participants who do not win a medal but who score seven points on at least one problem receive an honorable mention.\n\nSpecial prizes may be awarded for solutions of outstanding elegance or involving good generalisations of a problem. This last happened in 1995 (Nikolay Nikolov, Bulgaria) and 2005 (Iurie Boreico), but was more frequent up to the early 1980s. The special prize in 2005 was awarded to Iurie Boreico, a student from Moldova, who came up with a brilliant solution to question 3, which was an inequality involving three variables.\n\nThe rule that at most half the contestants win a medal is sometimes broken if it would cause the total number of medals to deviate too much from half the number of contestants. This last happened in 2010 (when the choice was to give either 226 (43.71%) or 266 (51.45%) of the 517 contestants (excluding the 6 from North Korea — see below) a medal), 2012 (when the choice was to give either 226 (41.24%) or 277 (50.55%) of the 548 contestants a medal), and 2013, when the choice was to give either 249 (47.16%) or 278 (52.65%) of the 528 contestants a medal.\n\nNorth Korea was disqualified for cheating at the 32nd IMO in 1991 and again at the 51st IMO in 2010. It is the only country to have been accused of cheating.\n\nThe following nations have achieved the highest team score in the respective competition:\n\n\nThe following nations have achieved an all-members-gold IMO with a full team:\n\n\nThe only countries to have their entire team score perfectly in the IMO were the United States in 1994 (they were coached by Paul Zeitz); and Luxembourg, whose 1-member team had a perfect score in 1981. The US's success earned a mention in \"TIME Magazine\". Hungary won IMO 1975 in an unorthodox way when none of the eight team members received a gold medal (five silver, three bronze). Second place team East Germany also did not have a single gold medal winner (four silver, four bronze).\n\nSeveral individuals have consistently scored highly and/or earned medals on the IMO: As of July 2015 Zhuo Qun Song (Canada) is the most successful participant with five gold medals (including one perfect score in 2015) and one bronze medal. Reid Barton (United States) was the first participant to win a gold medal four times (1998-2001). Barton is also one of only eight four-time Putnam Fellows (2001–04). Christian Reiher (Germany), Lisa Sauermann (Germany), Teodor von Burg (Serbia), and Nipun Pitimanaaree (Thailand) are the only other participants to have won four gold medals (2000–03, 2008–11, 2009–12, 2010–13, and 2011–14 respectively); Reiher also received a bronze medal (1999), Sauermann a silver medal (2007), von Burg a silver medal (2008) and a bronze medal (2007), and Pitimanaaree a silver medal (2009). Wolfgang Burmeister (East Germany), Martin Härterich (West Germany), Iurie Boreico (Moldova), and Lim Jeck (Singapore) are the only other participants besides Reiher, Sauermann, von Burg, and Pitimanaaree to win five medals with at least three of them gold. Ciprian Manolescu (Romania) managed to write a perfect paper (42 points) for gold medal more times than anybody else in the history of the competition, doing it all three times he participated in the IMO (1995, 1996, 1997). Manolescu is also a three-time Putnam Fellow (1997, 1998, 2000). Eugenia Malinnikova (Soviet Union) is the highest-scoring female contestant in IMO history. She has 3 gold medals in IMO 1989 (41 points), IMO 1990 (42) and IMO 1991 (42), missing only 1 point in 1989 to precede Manolescu's achievement.\n\nTerence Tao (Australia) participated in IMO 1986, 1987 and 1988, winning bronze, silver and gold medals respectively. He won a gold medal when he just turned thirteen in IMO 1988, becoming the youngest person at that time to receive a gold medal (a feat matched in 2011 by Zhuo Qun Song of Canada). Tao also holds the distinction of being the youngest medalist with his 1986 bronze medal, alongside 2009 bronze medalist Raúl Chávez Sarmiento (Peru), at the age of 10 and 11 respectively. Representing the United States, Noam Elkies won a gold medal with a perfect paper at the age of 14 in 1981. Note that both Elkies and Tao could have participated in the IMO multiple times following their success, but entered university and therefore became ineligible.\n\nThe current ten countries with the best all-time results are as follows:\n\n\n\n"}
{"id": "5387218", "url": "https://en.wikipedia.org/wiki?curid=5387218", "title": "Introduction to Mathematical Philosophy", "text": "Introduction to Mathematical Philosophy\n\nIntroduction to Mathematical Philosophy is a book by Bertrand Russell, published in 1919, written in part to exposit in a less technical way the main ideas of his and Whitehead's \"Principia Mathematica\" (1910–13), including the theory of descriptions.\nMathematics and logic, historically speaking, have been entirely distinct studies. Mathematics has been connected with science, logic with Greek. But both have developed in modern times: logic has become more mathematical and mathematics has become more logical. The consequence is that it has now become wholly impossible to draw a line between the two; in fact, the two are one. They differ as boy and man: logic is the youth of mathematics and mathematics is the manhood of logic. This view is resented by logicians who, having spent their time in the study of classical texts, are incapable of following a piece of symbolic reasoning, and by mathematicians who have learnt a technique without troubling to inquire into its meaning or justification. Both types are now fortunately growing rarer. So much of modern mathematical work is obviously on the border-line of logic, so much of modern logic is symbolic and formal, that the very close relationship of logic and mathematics has become obvious to every instructed student. The proof of their identity is, of course, a matter of detail: starting with premises which would be universally admitted to belong to logic, and arriving by deduction at results which as obviously belong to mathematics, we find that there is no point at which a sharp line can be drawn, with logic to the left and mathematics to the right. If there are still those who do not admit the identity of logic and mathematics, we may challenge them to indicate at what point, in the successive definitions and deductions of \"Principia Mathematica\", they consider that logic ends and mathematics begins. It will then be obvious that any answer must be quite arbitrary. (Russell 1919, 194–195).\n\n\n"}
{"id": "8110359", "url": "https://en.wikipedia.org/wiki?curid=8110359", "title": "John Ernest", "text": "John Ernest\n\nJohn Ernest (May 6, 1922 – July 21, 1994) was an American-born constructivist abstract artist. He was born in Philadelphia, in 1922. After living and working in Sweden and Paris from 1946 to 1951, he moved to London, England, where he lived and worked from 1951. As a mature student at Saint Martin's School of Art he came under the influence of Victor Pasmore and other proponents of constructivism. During the 1950s together with Anthony Hill, Kenneth Martin, Mary Martin, Stephen Gilbert and Gillian Wise he became a key member of the British constructivist (a.k.a. constructionist) art movement.\n\nJohn Ernest created both reliefs and free standing constructions. Several of his works are held at Tate Britain, including the Moebius Strip sculpture. He designed both a tower and a large wall relief at the International Union of Architects congress, South Bank, London, 1961. The exhibition structure also housed works by several of the other British constructivists.\n\nJohn Ernest had a lifelong fascination with mathematics that is reflected in his work, and together with constructivist artist Anthony Hill he made contributions to graph theory, studying crossing numbers of complete graphs.\n\nErnest was an atheist.\n"}
{"id": "4827464", "url": "https://en.wikipedia.org/wiki?curid=4827464", "title": "Lacunary value", "text": "Lacunary value\n\nIn complex analysis, a subfield of mathematics, a lacunary value or gap of a complex-valued function defined on a subset of the complex plane is a complex number which is not in the image of the function.\n\nMore specifically, given a subset \"X\" of the complex plane C and a function \"f\" : \"X\" → C, a complex number \"z\" is called a \"lacunary value\" of \"f\" if \"z\" ∉ image(\"f\").\n\nNote, for example, that 0 is the only lacunary value of the complex exponential function. The two Picard theorems limit the number of possible lacunary values of certain types of holomorphic functions.\n"}
{"id": "22783408", "url": "https://en.wikipedia.org/wiki?curid=22783408", "title": "Light's associativity test", "text": "Light's associativity test\n\nIn mathematics, Light's associativity test is a procedure invented by F. W. Light for testing whether a binary operation defined in a finite set by a Cayley multiplication table is associative. The naive procedure for verification of the associativity of a binary operation specified by a Cayley table, which compares the two products that can be formed from each triple of elements, is cumbersome. Light's associativity test simplifies the task in some instances (although it does not improve the worst-case runtime of the naive algorithm, namely formula_1 for sets of size formula_2).\n\nLet a binary operation ' · ' be defined in a finite set \"A\" by a Cayley table. Choosing some element \"a\" in \"A\", two new binary operations are defined in \"A\" as follows:\nThe Cayley tables of these operations are constructed and compared. If the tables coincide then \"x\" · ( \"a\" · \"y\" ) = ( \"x\" · \"a\" ) · \"y\" for all \"x\" and \"y\". This is repeated for every element of the set \"A\".\n\nThe example below illustrates a further simplification in the procedure for the construction and comparison of the Cayley tables of the operations ' formula_3 ' and ' formula_4 '.\n\nIt is not even necessary to construct the Cayley tables of ' formula_3 ' and ' formula_4 ' for \"all\" elements of \"A\". It is enough to compare Cayley tables of ' formula_3 ' and ' formula_4 ' corresponding to the elements in a proper generating subset of \"A\".\n\nConsider the binary operation ' · ' in the set \"A\" = { \"a\", \"b\", \"c\", \"d\", \"e\" } defined by the following Cayley table (Table 1):\n\nThe set { \"c\", \"e\" } is a generating set for the set \"A\" under the binary operation defined by the above table, for, \"a\" = \"e\" · \"e\", \"b\" = \"c\" · \"c\", \"d\" = \"c\" · \"e\". Thus it is enough to verify that the binary operations ' formula_3 ' and ' formula_4 ' corresponding to \"c\" coincide and also that the binary operations ' formula_3 ' and ' formula_4 ' corresponding to \"e\" coincide.\n\nTo verify that the binary operations ' formula_3 ' and ' formula_4 ' corresponding to \"c\" coincide, choose the row in Table 1 corresponding to the element \"c\" :\n\nThis row is copied as the header row of a new table (Table 3):\n\nUnder the header \"a\" copy the corresponding column in Table 1, under the header \"b\" copy the corresponding column in Table 1, etc., and construct Table 4.\n\nThe column headers of Table 4 are now deleted to get Table 5:\n\nThe Cayley table of the binary operation ' formula_3 ' corresponding to the element \"c\" is given by Table 6.\n\nNext choose the \"c\" column of Table 1:\n\nCopy this column to the index column to get Table 8:\n\nAgainst the index entry \"a\" in Table 8 copy the corresponding row in Table 1, against the index entry \"b\" copy the corresponding row in Table 1, etc., and construct Table 9.\n\nThe index entries in the first column of Table 9 are now deleted to get Table 10:\n\nThe Cayley table of the binary operation ' formula_4 ' corresponding to the element \"c\" is given by Table 11.\n\nOne can verify that the entries in the various cells in Table 6 agrees with the entries in the corresponding cells of Table 11. This shows that \"x\" · ( \"c\" · \"y\" ) = ( \"x\" · \"c\" ) · \"y\" for all \"x\" and \"y\" in \"A\". If there were some discrepancy then it would not be true that \"x\" · ( \"c\" · \"y\" ) = ( \"x\" · \"c\" ) · \"y\" for all \"x\" and \"y\" in \"A\".\n\nThat \"x\" · ( \"e\" · \"y\" ) = ( \"x\" · \"e\" ) · \"y\" for all \"x\" and \"y\" in \"A\" can be verified in a similar way by constructing the following tables (Table 12 and Table 13):\n\nIt is not necessary to construct the Cayley tables (Table 6 and table 11) of the binary operations ' formula_3 ' and ' formula_4 '. It is enough to copy the column corresponding to the header \"c\" in Table 1 to the index column in Table 5 and form the following table (Table 14) and verify that the \"a\" -row of Table 14 is identical with the \"a\" -row of Table 1, the \"b\" -row of Table 14 is identical with the \"b\" -row of Table 1, etc. This is to be repeated \"mutatis mutandis\" for all the elements of the generating set of \"A\".\n\nComputer software can be written to carry out Light's associativity test. Kehayopulu and Argyris have developed such a program for Mathematica.\n\nLight's associativity test can be extended to test associativity in a more general context.\n\nLet \"T\" = { \"t\", \"t\", formula_21, \"t\" } be a magma in which the operation is denoted by juxtaposition. Let \"X\" = { \"x\", \"x\", formula_21, \"x\" } be a set. Let there be a mapping from the Cartesian product \"T\" × \"X\" to \"X\" denoted by (\"t\", \"x\") \"tx\" and let it be required to test whether this map has the property\n\nA generalization of Light's associativity test can be applied to verify whether the above property holds or not. In mathematical notations, the generalization runs as follows: For each \"t\" in \"T\", let \"L\"(\"t\") be the \"m\" × \"n\" matrix of elements of \"X\" whose \"i\" - th row is\n\nand let \"R\"(\"t\") be the \"m\" × \"n\" matrix of elements of \"X\", the elements of whose \"j\" - th column are\n\nAccording to the generalised test (due to Bednarek), that the property to be verified holds if and only if \"L\"(\"t\") = \"R\"(\"t\") for all \"t\" in \"T\". When \"X\" = \"T\", Bednarek's test reduces to Light's test.\n\nThere is a randomized algorithm by Rajagopalan and Schulman to test associativity in time proportional to the input size. (The method also works for testing certain other identities.) Specifically, the runtime is formula_27 for an formula_28 table and error probability formula_29. \nThe algorithm can be modified to produce a triple formula_30 for which formula_31, if there is one, in time formula_32.\n\n"}
{"id": "23216258", "url": "https://en.wikipedia.org/wiki?curid=23216258", "title": "Log sum inequality", "text": "Log sum inequality\n\nThe log sum inequality is an inequality, which is useful for proving several theorems in information theory.\n\nLet formula_1 and formula_2 be nonnegative numbers. Denote the sum of all formula_3s by formula_4 and the sum of all formula_5s by formula_6. The log sum inequality states that\n\nwith equality if and only if formula_8 are equal for all formula_9, in other words formula_10 for all formula_9.\n\nNotice that after setting formula_12 we have\n\nwhere the inequality follows from Jensen's inequality since formula_14, formula_15, and formula_16 is convex.\n\nThe log sum inequality can be used to prove several inequalities in information theory such as Gibbs' inequality or the convexity of Kullback-Leibler divergence.\n\nFor example, to prove Gibbs' inequality it is enough to substitute formula_17s for formula_3s, and formula_19s for formula_5s to get\n\nThe inequality remains valid for formula_22 provided that formula_23 and formula_24.\nThe proof above holds for any function formula_25 such that formula_26 is convex, such as all continuous non-decreasing functions. Generalizations to convex functions other than the logarithm is given in Csiszár, 2004.\n\n"}
{"id": "4771336", "url": "https://en.wikipedia.org/wiki?curid=4771336", "title": "Mathematical sociology", "text": "Mathematical sociology\n\nMathematical sociology is the area of sociology that uses mathematics to construct social theories. Mathematical sociology aims to take sociological theory, which is strong in intuitive content but weak from a formal point of view, and to express it in formal terms. The benefits of this approach include increased clarity and the ability to use mathematics to derive implications of a theory that cannot be arrived at intuitively. In mathematical sociology, the preferred style is encapsulated in the phrase \"constructing a mathematical model.\" This means making specified assumptions about some social phenomenon, expressing them in formal mathematics, and providing an empirical interpretation for the ideas. It also means deducing properties of the model and comparing these with relevant empirical data. Social network analysis is the best-known contribution of this subfield to sociology as a whole and to the scientific community at large. The models typically used in mathematical sociology allow sociologists to understand how predictable local interactions are and they are often able to elicit global patterns of social structure.\n\nStarting in the early 1940s, Nicolas Rashevsky, and subsequently in the late 1940s, Anatol Rapoport and others, developed a relational and probabilistic approach to the characterization of large social networks in which the nodes are persons and the links are acquaintanceship. During the late 1940s, formulas were derived that connected local parameters such as closure of contacts – if A is linked to both B and C, then there is a greater than chance probability that B and C are linked to each other – to the global network property of connectivity.\n\nMoreover, acquaintanceship is a \"positive tie\", but what about \"negative ties\" such as animosity among persons? To tackle this problem, graph theory, which is the mathematical study of abstract representations of networks of points and lines, can be extended to include these two types of links and thereby to create models that represent both positive and negative sentiment relations, which are represented as signed graphs. A signed graph is called \"balanced\" if the product of the signs of all relations in every cycle (links in every graph cycle) is positive. Through formalization by mathematician Frank Harary this work produced the fundamental theorem of this theory. It says that if a network of interrelated positive and negative ties is balanced, e.g. as illustrated by the psychological principle that \"my friend's enemy is my enemy\", then it consists of two subnetworks such that each has positive ties among its nodes and there are only negative ties between nodes in distinct subnetworks. The imagery here is of a social system that splits into two cliques. There is, however, a special case where one of the two subnetworks is empty, which might occur in very small networks.\nIn another model, ties have relative strengths. 'Acquaintanceship' can be viewed as a 'weak' tie and 'friendship' is represented as a strong tie. Like its uniform cousin discussed above, there is a concept of closure, called strong triadic closure. A graph satisfies strong triadic closure If A is strongly connected to B, and B is strongly connected to C, then A and C must have a tie (either weak or strong).\n\nn these two developments we have mathematical models bearing upon the analysis of structure. Other early influential developments in mathematical sociology pertained to process. For instance, in 1952 Herbert A. Simon produced a mathematical formalization of a published theory of social groups by constructing a model consisting of a deterministic system of differential equations. A formal study of the system led to theorems about the dynamics and the implied equilibrium states of any group.\n\nThe emergence of mathematical models in the social sciences was part of the zeitgeist in the 1940s and 1950s in which a variety of new interdisciplinary scientific innovations occurred, such as information theory, game theory, cybernetics and mathematical model building in the social and behavioral sciences.\n\nIn 1954, a critical expository analysis of Rashevsky's social behavior models was written by sociologist James S. Coleman. Rashevsky's models and as well as the model constructed by Simon raise a question: how can one connect such theoretical models to the data of sociology, which often take the form of surveys in which the results are expressed in the form of proportions of people believing or doing something. This suggests deriving the equations from assumptions about the chances of an individual changing state in a small interval of time, a procedure well known in the mathematics of stochastic processes.\n\nColeman embodied this idea in his 1964 book \"Introduction to Mathematical Sociology\", which showed how stochastic processes in social networks could be analyzed in such a way as to enable testing of the constructed model by comparison with the relevant data. The same idea can and has been applied to processes of change in social relations, an active research theme in the study of social networks, illustrated by an empirical study appearing in the journal Science. \n\nIn other work, Coleman employed mathematical ideas drawn from economics, such as general equilibrium theory, to argue that general social theory should begin with a concept of purposive action and, for analytical reasons, approximate such action by the use of rational choice models (Coleman, 1990). This argument is similar to viewpoints expressed by other sociologists in their efforts to use rational choice theory in sociological analysis although such efforts have met with substantive and philosophical criticisms.\n\nMeanwhile, structural analysis of the type indicated earlier received a further extension to social networks based on institutionalized social relations, notably those of kinship. The linkage of mathematics and sociology here involved abstract algebra, in particular, group theory. This, in turn, led to a focus on a data-analytical version of homomorphic reduction of a complex social network (which along with many other techniques is presented in Wasserman and Faust 1994).\n\nIn regard to Rapoport's random and biased net theory, his 1961 study of a large sociogram, co-authored with Horvath turned out to become a very influential paper. There was early evidence of this influence. In 1964, Thomas Fararo and a co-author analyzed another large friendship sociogram using a biased net model. Later in the 1960s, Stanley Milgram described the small world problem and undertook a field experiment dealing with it. A highly fertile idea was suggested and applied by Mark Granovetter in which he drew upon Rapoport's 1961 paper to suggest and apply a distinction between weak and strong ties. The key idea was that there was \"strength\" in weak ties. \n\nSome programs of research in sociology employ experimental methods to study social interaction processes. Joseph Berger and his colleagues initiated such a program in which the central idea is the use of the theoretical concept \"expectation state\" to construct theoretical models to explain interpersonal processes, e.g., those linking external status in society to differential influence in local group decision-making. Much of this theoretical work is linked to mathematical model building, especially after the late 1970s adoption of a graph theoretic representation of social information processing, as Berger (2000) describes in looking back upon the development of his program of research. In 1962 he and his collaborators explained model building by reference to the goal of the model builder, which could be explication of a concept in a theory, representation of a single recurrent social process, or a broad theory based on a theoretical construct, such as, respectively, the concept of balance in psychological and social structures, the process of conformity in an experimental situation, and stimulus sampling theory. \n\nThe generations of mathematical sociologists that followed Rapoport, Simon, Harary, Coleman, White and Berger, including those entering the field in the 1960s such as Thomas Fararo, Philip Bonacich, and Tom Mayer, among others, drew upon their work in a variety of ways.\n\nMathematical sociology remains a small subfield within the discipline, but it has succeeded in spawning a number of other subfields which share its goals of formally modeling social life. The foremost of these fields is social network analysis, which has become among the fastest growing areas of sociology in the 21st century. The other major development in the field is the rise of computational sociology, which expands the mathematical toolkit with the use of computer simulations, artificial intelligence and advanced statistical methods. The latter subfield also makes use of the vast new data sets on social activity generated by social interaction on the internet.\n\nOne important indicator of the significance of mathematical sociology is that the general interest journals in the field, including such central journals as \"The American Journal of Sociology\" and \"The American Sociological Review\", have published mathematical models that became influential in the field at large. \n\nMore recent trends in mathematical sociology are evident in contributions to \"The Journal of Mathematical Sociology\" (JMS). Several trends stand out: the further development of formal theories that explain experimental data dealing with small group processes, the continuing interest in structural balance as a major mathematical and theoretical idea, the interpenetration of mathematical models oriented to theory and innovative quantitative techniques relating to methodology, the use of computer simulations to study problems in social complexity, interest in micro–macro linkage and the problem of emergence, and ever-increasing research on networks of social relations. \nThus, topics from the earliest days, like balance and network models, continue to be of contemporary interest. The formal techniques employed remain many of the standard and well-known methods of mathematics: differential equations, stochastic processes and game theory. Newer tools like agent-based models used in computer simulation studies are prominently represented. Perennial substantive problems still drive research: social diffusion, social influence, social status origins and consequences, segregation, cooperation, collective action, power, and much more.\nMany of the developments in mathematical sociology, including formal theory, have exhibited notable decades-long advances that began with path-setting contributions by leading mathematical sociologists and formal theorists. This provides another way of taking note of recent contributions but with an emphasis on continuity with early work through the use of the idea of “research program,” which is a coherent series of theoretical and empirical studies based on some fundamental principle or approach. There are more than a few of these programs and what follows is no more than a brief capsule description of leading exemplars of this idea in which there is an emphasis on the originating leadership in each program and its further development over decades.\n\n(1) Rational Choice Theory and James S. Coleman: After his 1964 pioneering \"Introduction to Mathematical Sociology\", Coleman continued to make contributions to social theory and mathematical model building and his 1990 volume, \"Foundations of Social Theory\" was the major theoretical work of a career that spanned the period from 1950s to 1990s and included many other research-based contributions.. The Foundation book combined accessible examples of how rational choice theory could function in the analysis of such sociological topics as authority, trust, social capital and the norms (in particular, their emergence). In this way, the book showed how rational choice theory could provide an effective basis for making the transition from micro to macro levels of sociological explanation. An important feature of the book is its use of mathematical ideas in generalizing the rational choice model to include interpersonal sentiment relations as modifiers of outcomes and doing so such that the generalized theory captures the original more self-oriented theory as a special case, as point emphasized in a later analysis of the theory. The rationality presupposition of the theory led to debates among sociological theorists. Nevertheless, many sociologists drew upon Coleman’s formulation of a general template for micro-macro transition to gain leverage on the continuation of topics central to his and the discipline's explanatory focus on a variety of macrosocial phenomena in which rational choice simplified the micro level in the interest of combining individual actions to account for macro outcomes of social processes. \n(2) Structuralism (Formal) and Harrison C. White: In the decades since his earliest contributions, Harrison White has led the field in putting social structural analysis on a mathematical and empirical basis, including the 1970 publication of \"Chains of Opportunity: System Models of Mobility in Organization\"s which set out and applied to data a vacancy chain model for mobility in and across organizations. His very influential other work includes the operational concepts of blockmodel and structural equivalence which start from a body of social relational data to produce analytical results using these procedures and concepts. These ideas and methods were developed in collaboration with his former students François Lorraine, Ronald Breiger, and Scott Boorman. These three are among the more than 30 students who earned their doctorates under White in the period 1963-1986. The theory and application of blockmodels has been set out in detail in a recent monograph.. White's later contributions include a structuralist approach to markets and, in 1992, a general theoretical framework, later appearing in a revised edition.\n(3) Expectation states theory and Joseph Berger: Under Berger’s intellectual and organizational leadership, Expectation States Theory branched out into a large number of specific programs of research on specific problems, each treated in terms of the master concept of expectation states. He and his colleague and frequent collaborator Morris Zelditch Jr not only produced work of their own but created a doctoral program at Stanford University that led to an enormous outpouring of research by notable former students, including Murray Webster, David Wagner, and Hamit Fisek. Collaboration with mathematician Robert Z. Norman led to the use of mathematical graph theory as a way of representing and analyzing social information processing in self-other(s) interactions. Berger and Zelditch also advanced work in formal theorizing and mathematical model building as early as 1962 with a collaborative expository analysis of types of models. Berger and Zelditch stimulated advances in other theoretical research programs by providing outlets for the publication of new work, culminating in a 2002 edited volume that includes a chapter that presents an authoritative overview of Expectation states theory as a program of cumulative research dealing with group processes.\n(4) Formalization in Theoretical Sociology and Thomas J. Fararo: Many of this sociologist’s contributions have been devoted to bringing mathematical thinking into greater contact with sociological theory. He organized a symposium attended by sociological theorists in which formal theorists delivered papers that were subsequently published in 2000. Through collaborations with students and colleagues his own theoretical research program dealt with such topics as macrostructural theory and E-state structuralism (both with former student John Skvoretz), subjective images of stratification (with former student Kenji Kosaka), tripartite structural analysis (with colleague Patrick Doreian) and computational sociology (with colleague Norman P. Hummon). Two of his books are extended treatments of his approach to theoretical sociology.\n(5) Social Network Analysis and Linton C. Freeman: In the early 1960s Freeman directed a sophisticated empirical study of community power structure. In 1978 he established the journal \"Social Networks.\" It rapidly became a major outlet for original research papers that used mathematical techniques to analyze network data. The journal also publishes conceptual and theoretical contributions, including his paper “Centrality in Social Networks: Conceptual Clarification.” The paper has been cited more than 13,000 times. In turn, the mathematical concept defined in that paper led to further elaborations of the ideas, to experimental tests, and to numerous applications in empirical studies. He is the author of a study of the history and sociology of the field of social network analysis.\n(6) Quantitative Methodology and Kenneth C. Land: Kenneth Land has been on the frontier of quantitative methodology in sociology as well as formal theoretical model building. The influential yearly volume \"Sociological Methodology\" has been one of Land’s favorite outlets for the publication of papers that often lie in the intersection of quantitative methodology and mathematical sociology. Two of his theoretical papers appeared early in this journal: “Mathematical Formalization of Durkheim's Theory of Division of Labor” (1970) and “Formal Theory” (1971). His decades-long research program includes contributions relating to numerous special topics and methods, including social statistics, social indicators, stochastic processes, mathematical criminology, demography and social forecasting. Thus Land brings to these fields the skills of a statistician, a mathematician and a sociologist, combined. \n(7) Affect Control Theory and David R. Heise: In 1979, Heise published a groundbreaking formal and empirical study in the tradition of interpretive sociology, especially symbolic interactionism,\"Understanding Events: Affect and the Construction of Social Action.\" It was the origination of a research program that has included his further theoretical and empirical studies and those of other sociologists, such as Lynn Smith-Lovin, Dawn Robinson and Neil MacKinnon. Definition of the situation and self-other definitions are two of the leading concepts in affect control theory. The formalism used by Heise and other contributors uses a validated form of measurement and a cybernetic control mechanism in which immediate feelings and compared with fundamental sentiments in such a way as to generate an effort to bring immediate feelings in a situation into correspondence with sentiments. In the simplest models, each person in an interactive pair, is represented in terms of one side of a role relationship in which fundamental sentiments associated with each role guide the process of immediate interaction. A higher level of the control process can be activated in which the definition of the situation is transformed. This research program comprises several of the key chapters in a 2006 volume of contributions to control systems theory (in the sense of Powers 1975 ) in sociology.\n(8) \"Distributive Justice Theory\" and Guillermina Jasso: Since 1980, Jasso has treated problems of distributive justice with an original theory that uses mathematical methods. She has elaborated upon and applied this theory to a wide range of social phenomena. Her most general mathematical apparatus – with the theory of distributive justice as a special case -- deals with any subjective comparison between some actual state and some reference level for it, e.g., a comparison of an actual reward with an expected reward. In her justice theory, she starts with a very simple premise, the justice evaluation function (the natural logarithm of the ratio of actual to just reward) and then derives numerous empirically testable implications. \n(9) Collaborative research and John Skvoretz. A major feature of modern science is collaborative research in which the distinctive skills of the participants combine to produce original research. Skvoretz, in addition to this other contributions, has been a frequent collaborator in a variety of theoretical research programs, often using mathematical expertise as well as skills in experimental design, statistical data analysis and simulation methods. Some examples are: (1) Collaborative work on theoretical, statistical and mathematical problems in biased net theory. (2) Collaborative contributions to Expectation States Theory. (3) Collaborative contributions to Elementary Theory. (4) Collaboration with Bruce Mayhew in a structuralist research program. From the early 1970s, Skvoretz has been one of the most prolific of contributors to the advance of mathematical sociology.\nThe above discussion could be expanded to include many other programs and individuals including European sociologists such as Peter Abell and the late Raymond Boudon.\n\nThe Mathematical Sociology section of The American Sociological Association in 2002 initiated awards for contributions to the field, including \"The James S. Coleman Distinguished Career Achievement Award.\" (Coleman had died in 1995 before the section had been established.) Given every other year, the awardees include some of those just listed in terms of their career-long research programs:\n\nThe section's other categories of awards and their recipients are listed at ASA Section on Mathematical Sociology\n\nMathematical sociology textbooks cover a variety of models, usually explaining the required mathematical background before discussing important work in the literature (Fararo 1973, Leik and Meeker 1975, Bonacich and Lu 2012). An earlier text by Otomar Bartos (1967) is still of relevance. Of wider scope and mathematical sophistication is the text by Rapoport (1983). A very reader-friendly and imaginative introduction to explanatory thinking leading to models is Lave and March (1975, reprinted 1993). The \"Journal of Mathematical Sociology\" (started in 1971) has been open to papers covering a broad spectrum of topics employing a variety of types of mathematics, especially through frequent special issues. Other journals in sociology who publish papers with substantial use of mathematics are Computational and Mathematical Organization Theory, Journal of social structure, Journal of Artificial Societies and Social Simulation \n\nArticles in \"Social Networks,\" a journal devoted to social structural analysis, very often employ mathematical models and related structural data analyses. In addition – importantly indicating the penetration of mathematical model building into sociological research – the major comprehensive journals in sociology, especially \"The American Journal of Sociology\" and \"The American Sociological Review,\" regularly publish articles featuring mathematical formulations.\n\n\n\n"}
{"id": "1422483", "url": "https://en.wikipedia.org/wiki?curid=1422483", "title": "Mathlete", "text": "Mathlete\n\nA mathlete is a person who competes in mathematics competitions at any level or any age. More specifically, a Mathlete is a student who participates in any of the MATHCOUNTS programs, as Mathlete is a registered trademark of the MATHCOUNTS Foundation in the United States. The term is a portmanteau of the words mathematics and athlete. \n\nTop Mathletes from MATHCOUNTS often go on to compete in the AIME, USAMO, and ARML competitions in the United States. Those in other countries generally participate in national olympiads to qualify for the International Mathematical Olympiad.\n\nParticipants in World Math Day also are commonly referred to as mathletes.\n\nThe Putnam Exam: The William Lowell Putnam Competition is the preeminent undergraduate level mathletic competition in the United States. Administered by the Mathematical Association of America, students compete as individuals and as teams (as chosen by their Institution) for scholarships and team prize money. The exam is administered on the first Saturday in December.\n\nThe academic off-season (traditionally referred to as \"summer\") can be especially difficult on mathletes, though various training regimens have been proposed to keep mathletic ability at its peak. Publications such as the MAA's \"The American Mathematical Monthly\" and the AMS's \"Notices of the American Mathematical Society\" are widely read to maintain and hone mathematical ability. Some coaches suggest seeking research internships or grants, many of which are funded by the National Science Foundation.\n\nAt higher levels, mathletes can obtain funding from host institutions to work on summer research projects. For example, the University of Delaware offers the Groups Exploring the Mathematical Sciences project (GEMS project) to first year graduate students. The students act as the principal investigator and work with an undergraduate research assistant and a faculty adviser who will oversee their summer research.\n\n"}
{"id": "826363", "url": "https://en.wikipedia.org/wiki?curid=826363", "title": "Morris Kline", "text": "Morris Kline\n\nMorris Kline (May 1, 1908 – June 10, 1992) was a Professor of Mathematics, a writer on the history, philosophy, and teaching of mathematics, and also a popularizer of mathematical subjects.\n\nKline was born to a Jewish family in Brooklyn and resided in Jamaica, Queens. After graduating from Boys High School in Brooklyn, he studied mathematics at New York University, earning a bachelor's degree in 1930, a master's degree in 1932, and a doctorate (Ph. D) in 1936. He continued at NYU as an instructor until 1942.\n\nDuring World War II, Kline was posted to the Signal Corps (United States Army) stationed at Belmar, New Jersey. Designated a physicist, he worked in the engineering lab where radar was developed. After the war he continued investigating electromagnetism, and from 1946 to 1966 was director of the division for electromagnetic research at the Courant Institute of Mathematical Sciences.\n\nKline resumed his mathematical teaching at NYU, becoming a full professor in 1952.\nHe taught at New York University until 1975, and wrote many papers and more than a dozen books on various aspects of mathematics and particularly teaching of mathematics. He repeatedly stressed the need to teach the applications and usefulness of mathematics rather than expecting students to enjoy it for its own sake. Similarly, he urged that mathematical research concentrate on solving problems posed in other fields rather than building structures of interest only to other mathematicians.\nOne can get a sense of Kline's views on teaching from the following:\n\nMorris Kline was a protagonist in the curriculum reform in mathematics education that occurred in the second half of the twentieth century, a period including the programs of the new math. An article by Kline in 1956 in \"The Mathematics Teacher\", the main journal of the National Council of Teachers of Mathematics, was titled \"Mathematical texts and teachers: a tirade\". Calling out teachers blaming students for failures, he wrote \"There is a student problem, but there are also three other factors which are responsible for the present state of mathematical learning, namely, the curricula, the texts, and the teachers.\" The tirade touched a nerve, and changes started to happen. But then Kline switched to being a critic of some of the changes. In 1958 he wrote \"Ancients versus moderns: a new battle of the books\". The article was accompanied with a rebuttal by Albert E. Meder Jr. of Rutgers University. He says, \"I find objectionable: first, vague generalizations, entirely undocumented, concerning views held by ‘modernists’, and second, the inferences drawn from what has not been said by the ‘modernists’.\" \nBy 1966 Kline proposed an eight-page high school plan. The rebuttal for this article was by James H. Zant; it asserted that Kline had \"a general lack of knowledge of what was going on in schools with reference to textbooks, teaching, and curriculum.\" Zant criticized Kline’s writing for \"vagueness, distortion of facts, undocumented statements and overgeneralization.\"\n\nIn 1966 and 1970 Kline issued two further criticisms. In 1973 St. Martin’s Press contributed to the dialogue by publishing Kline’s critique, \"Why Johnny Can’t Add: the Failure of the New Math\". Its opening chapter is a parody of instruction as students’ intuitions are challenged by the new jargon. The book recapitulates the debates from \"Mathematics Teacher\", with Kline conceding some progress: He cites Howard Fehr of Columbia University who sought to unify the subject through its general concepts, sets, operations, mappings, relations, and structure in the Secondary School Mathematics Curriculum Improvement Study.\n\nIn 1977 Kline turned to undergraduate university education; he took on the academic mathematics establishment with his \"Why the Professor Can’t Teach: the dilemma of university education\". Kline argues that onus to conduct research misdirects the scholarly method that characterizes good teaching. He lauds scholarship as expressed by expository writing or reviews of original work of others. For scholarship he expects critical attitudes to topics, materials and methods. Among the rebuttals are those by D.T. Finkbeiner, Harry Pollard, and Peter Hilton. Pollard conceded, \"The society in which learning is admired and pursued for its own sake has disappeared.\" The Hilton review was more direct: Kline has \"placed in the hand of enemies…[a] weapon\". Having started in 1956 as an agitator for change in mathematics education, he became a critic of some trends. Skilled expositor that he was, editors frequently felt his expressions were best tempered with rebuttal.\n\nIn considering what motivated Morris Kline to protest, consider Professor Meder’s opinion:I am wondering whether in point of fact, Professor Kline really likes mathematics [...] I think that he is at heart a physicist, or perhaps a ‘natural philosopher’, not a mathematician, and that the reason he does not like the proposals for orienting the secondary school college preparatory mathematics curriculum to the diverse needs of the twentieth century by making use of some concepts developed in mathematics in the last hundred years or so is not that this is bad mathematics, but that it minimizes the importance of physics.\n\nIt might appear so, as Kline recalls E. H. Moore’s recommendation to combine science and mathematics at the high school level. But closer reading shows Kline calling mathematics a \"part of man’s efforts to understand and master his world\", and he sees that role in a broad spectrum of sciences.\n\nIn \"\" (ch. XIII: \"The Isolation of Mathematics\"), Kline deplored the way mathematics research was being conducted, complaining that often mathematicians, not willing to become acquainted with the (sometimes deep) context needed to solve applied problems in sciences, prefer to invent pure mathematics problems that are not necessarily of any consequence. Kline also blamed the publish or perish academic culture for this state of affairs.\n\n\n\n"}
{"id": "38557859", "url": "https://en.wikipedia.org/wiki?curid=38557859", "title": "N-ary associativity", "text": "N-ary associativity\n\nIn algebra, \"n\"-ary associativity is a generalization of the associative law to \"n\"-ary operations. Ternary associativity is\n\ni.e. the string \"abcde\" with any three adjacent elements bracketed. \"n\"-ary associativity is a string of length \"n\" + (\"n\" − 1) with any \"n\" adjacent elements bracketed.\n"}
{"id": "26641574", "url": "https://en.wikipedia.org/wiki?curid=26641574", "title": "Narrow escape problem", "text": "Narrow escape problem\n\nThe narrow escape problem is a ubiquitous problem in biology, biophysics and cellular biology.\n\nThe mathematical formulation is the following: a Brownian particle (ion, molecule, or protein) is confined to a bounded domain (a compartment or a cell) by a reflecting boundary, except for a small window through which it can escape. The narrow escape problem is that of calculating the mean escape time. This time diverges as the window shrinks, thus rendering the calculation a singular perturbation problem \n\nWhen escape is even more stringent due to severe geometrical restrictions at the place of escape, the Narrow Escape Problem becomes the Dire Strait Problem .\n\nThe Narrow escape problem was proposed in the context of biology and biophysics by D. Holcman and Z. Schuss and later on with A.Singer and lead to the Narrow Escape Theory in Applied Mathematics and Computational Biology \nThe motion of a particle is described by the Smoluchowski limit of the Langevin equation :\n\nformula_1\n\nwhere formula_2 is the diffusion coefficient of the particle, formula_3 is the friction coefficient \nper unit of mass, formula_4 the force per unit of mass, and formula_5 is a Brownian motion.\n\nA common question is to estimate the mean sojourn time of a particle diffusing in a bounded domain formula_6 before it escapes through a small absorbing window formula_7 in its boundary formula_8. The time is estimated asymptotically in the limit formula_9\n\nThe probability density function (pdf) formula_10 is the probability of finding the particle at position formula_11 at time formula_12.\n\nThe pdf satisfies the Fokker–Planck equation :formula_13\nwith initial condition\n\nand mixed Dirichlet–Neumann boundary conditions (formula_15)\n\nThe function\n\nrepresents the mean sojourn time of particle, conditioned on the initial position formula_19. It is the solution of the boundary value problem\n\nThe solution depends on the dimension of the domain. For a particle diffusing on a two-dimensional disk\n\nwhere formula_24 is the surface of the domain. The function formula_25does not depend on the initial position formula_19, except for a small boundary layer near the absorbing boundary due to the asymptotic form.\n\nThe first order term matters in dimension 2: for a circular disk of radius formula_27, the mean escape time of a particle starting in the center is\n\nThe escape time averaged with respect to a uniform initial distribution of the particle is given by\n\nThe geometry of the small opening can affect the escape time: if the absorbing window is located at a corner \nof angle formula_30, then:\n\nformula_31\n\nMore surprising, near a cusp in a two dimensional domain, the\nescape time formula_32 grows algebraically, rather than\nlogarithmically: in the domain bounded between two tangent circles,\nthe escape time is :\n\nformula_33\n\nwhere \"d\" > 1 is the ratio of the radii. Finally, when\nthe domain is an annulus, the escape time to a small opening located\non the inner circle involves a second parameter which is \nformula_34 the ratio of the inner to the outer\nradii, the escape time, averaged with respect to a uniform initial\ndistribution, is:\n\nformula_35\n\nThis equation contains two terms of the asymptotic expansion of formula_36 and formula_37 is the angle of the absorbing boundary. The case formula_38 close to 1 remains open, and for general domains, the asymptotic expansion of the escape time remains an open problem. So does the problem of computing the escape time near a cusp point in three-dimensional domains. For Brownian motion in a field of force\n\nthe gap in the spectrum is not necessarily small between the first and the second eigenvalues, depending on the relative size of the small hole and the force barriers, the particle has to overcome in order to escape. The escape stream is not necessarily Poissonian.\n\nA theorem that relates the Brownian motion escape problem to a (deterministic) partial differential equation problem is the following.\n\nHere formula_54 is the derivative in the direction formula_55, the exterior normal to formula_56 Moreover, the average of the variance can be calculated from the formula\n\nThe first part of the theorem is a classical result, while the average variance was proved in 2011 by Carey Caginalp and Xinfu Chen [1,3].\n\nThe escape time has been the subject of a number of studies using the small gate as an asymptotically small parameter. The following closed form result [1,3] gives an exact solution that confirms these asymptotic formulae and extends them to gates that are not necessarily small.\n\nAnother set of results concerns the probability density of the location of exit [2]\n\nThat is, for any (Borel set) formula_63, the probability that a particle, starting either at the origin or uniformly distributed in formula_6, exhibiting Brownian motion in formula_6, reflecting when it hits formula_66, and escaping once it hits formula_42, ends up escaping from formula_3 is\n\nwhere formula_70 is the surface element of formula_8 at formula_72.\n\nIn simulation there is a random error due to the statistical sampling process. This error can be limited by appealing to the Central Limit Theorem and using a large number of samples. There is also a discretization error due to the finite size approximation of the step size in approximating the Brownian motion. One can then obtain empirical results as step size and gate size vary. Using the exact result quoted above for the particular case of the circle, it is possible to make a careful comparison of the exact solution with the numerical solution. This illuminates the distinction between finite steps and continuous diffusion. A distribution of exit locations was also obtained through simulations for this problem.\n\nThe forward rate of chemical reactions is the reciprocal of the narrow escape time, which generalizes the classical Smoluchowski formula for Brownian particles located in an infinite medium. A Markov description can be used to estimate the binding and unbinding to a small number of sites.\n\n[1] Carey Caginalp and Xinfu Chen: Analytical and numerical results for first\nescape time in 2D (with Xinfu Chen) Comptes Rendus, C. R. Acad. Sci. Paris,\nSer. I 349 191–194 (2011).\n\n[2] Carey Caginalp and Xinfu Chen: Analytical and numerical results for an\nescape problem (with Xinfu Chen) Archive for Rat. Mech. Analysis 203 329–342 (2012).\n\n[3] Carey Caginalp:\\ Analytical and numerical results on escape, B. Phil.\nThesis University of Pittsburgh (2011).\n\nExternal Links. Carey Caginalp publications and lectures\nhttp://www.pitt.edu/~careycag/\n\nComptes Rendus paper http://www.pitt.edu/~careycag/paper1.pdf\n\nARMA paper http://www.pitt.edu/~careycag/paper2.pdf\n"}
{"id": "56028417", "url": "https://en.wikipedia.org/wiki?curid=56028417", "title": "Optical cluster state", "text": "Optical cluster state\n\nOptical cluster states are a proposed tool to achieve quantum computational universality in linear optical quantum computing (LOQC). As direct entangling operations with photons often require nonlinear effects, probabilistic generation of entangled resource states has been proposed as an alternative path to the direct approach.\n\nOn a silicon photonic chip, one of the most common platforms for implementing LOQC, there are two typical choices for encoding quantum information, though many more options exist. Photons have useful degrees of freedom in the spatial modes of the possible photon paths or in the polarization of the photons themselves. The way in which a cluster state is generated varies with which encoding has been chosen for implementation.\n\nStoring information in the spatial modes of the photon paths is often referred to as dual rail encoding. In a simple case, one might consider the situation where a photon has two possible paths, a horizontal path with creation operator formula_1 and a vertical path with creation operator formula_2, where the logical zero and one states are then represented by\n\nand\n\nSingle qubit operations are then performed by beam splitters, which allow manipulation of the relative superposition weights of the modes, and phase shifters, which allow manipulation of the relative phases of the two modes. This type of encoding lends itself to the Nielsen protocol for generating cluster states. In encoding with photon polarization, logical zero and one can be encoded via the horizontal and vertical states of a photon, e.g.\n\nand\n\nGiven this encoding, single qubit operations can be performed using waveplates. This encoding can be used with the Browne-Rudolph protocol.\n\nIn 2004, Nielsen proposed a protocol to create cluster states, borrowing techniques from the Knill-Laflamme-Milburn protocol (KLM protocol) to probabilistically create controlled-Z connections between qubits which, when performed on a pair of formula_7 states (normalization being ignored), forms the basis for cluster states. While the KLM protocol requires error correction and a fairly large number of modes in order to get very high probability two-qubit gate, Neilsen's protocol only requires a success probability per gate of greater than one half. Given that the success probability for a connection using formula_8 ancilla photons is formula_9, relaxation of the success probability from nearly one to anything over one half presents a major advantage in resources, as well as simply reducing the number of required elements in the photonic circuit.\n\nTo see how Nielsen brought about this improvement, consider the photons being generated for qubits as vertices on a two dimensional grid, and the controlled-Z operations being probabilistically added edges between nearest neighbors. Using results from percolation theory, it can be shown that as long as the probability of adding edges is above a certain threshold, there will exist a complete grid as a sub-graph with near unit probability. Because of this, Nielsen's protocol doesn't rely on every individual connection being successful, just enough of them that the connections between photons allow a grid.\n\nAmong the first proposals of utilizing resource states for optical quantum computing was the Yoran-Reznik protocol in 2003. While the proposed resource in this protocol was not exactly a cluster state, it brought many of the same key concepts to the attention of those considering the possibilities of optical quantum computing and still required connecting multiple separate one-dimensional chains of entangled photons via controlled-Z operations. This protocol is somewhat unique in that it utilizes both the spatial mode degree of freedom along with the polarization degree of freedom to help entanglement between qubits.\n\nGiven a horizontal path, denoted by formula_10, and a vertical path, denoted by formula_11, a 50:50 beam splitter connecting the paths followed by a formula_12-phase shifter on path formula_10, we can perform the transformations\n\nwhere formula_18 denotes a photon with polarization formula_19 on path formula_20. In this way, we have the path of the photon entangled with its polarization. This is sometimes referred to as hyperentanglement, a situation in which the degrees of freedom of a single particle are entangled with each other. This, paired with the Hong-Ou-Mandel effect and projective measurements on the polarization state, can be used to create path entanglement between photons in a linear chain.\n\nThese one-dimensional chains of entangled photons still need to be connected via controlled-Z operations, similar to the KLM protocol. These controlled-Z connection s between chains are still probabilistic, relying on measurement dependent teleportation with special resource states. However, due to the fact that this method does not include Fock measurements on the photons being used for computation as the KLM protocol does, the probabilistic nature of implementing controlled-Z operations presents much less of a problem. In fact, as long as connections occur with probability greater than one half, the entanglement present between chains will be enough to perform useful quantum computation, on average.\n\nAn alternative approach to building cluster states that focuses entirely on photon polarization is the Browne-Rudolph protocol. This method rests on performing parity checks on a pair of photons to stitch together already entangled sets of photons, meaning that this protocol requires entangled photon sources. Browne and Rudolph proposed two ways of doing this, called type-I and type-II fusion.\n\nIn type-I fusion, photons with either vertical or horizontal polarization are injected into modes formula_10 and formula_11, connected by a polarizing beam splitter. Each of the photons sent into this system is part of a Bell pair that this method will try to entangle. Upon passing through the polarizing beam splitter, the two photons will go opposite ways if they have the same polarization or the same way if they have the same polarization, e.g.\n\nor\n\nThen on one of these modes, a projective measurement onto the basis formula_25 is performed. If the measurement is successful, i.e. if it detects anything, then the detected photon is destroyed, but the remaining photons from the Bell pairs become entangled. Failure to detect anything results in an effective loss of the involved photons in a way that breaks any chain of entangled photons they were on. This can make attempting to make connections between already developed chains potentially risky.\n\nType-II fusion works similarly to type-I fusion, with the differences being that a diagonal polarizing beam splitter is used and the pair of photons is measured in the two-qubit Bell basis. A successful measurement here involves measuring the pair to be in a Bell state with no relative phase between the superposition of states (e.g. formula_26 as opposed to formula_27). This again entangles any two clusters already formed. A failure here performs local complementation on the local subgraph, making an existing chain shorter rather than cutting it in half. In this way, while it requires the use of more qubits in combining entangled resources, the potential loss for attempts to connect two chains together are not as expensive for type-II fusion as they are for type-I fusion.\n\nOnce a cluster state has been successfully generated, computation can be done with the resource state directly by applying measurements to the qubits on the lattice. This is the model of measurement-based quantum computation (MQC), and it is equivalent to the circuit model.\n\nLogical operations in MQC come about from the byproduct operators that occur during quantum teleportation. For example, given a single qubit state formula_28, one can connect this qubit to a plus state (formula_7) via a two-qubit controlled-Z operation. Then, upon measuring the first qubit (the original formula_28) in the Pauli-X basis, the original state of the first qubit is teleported to the second qubit with a measurement outcome dependent extra rotation, which one can see from the partial inner product of the measurement acting on the two-qubit state:\n\nfor formula_32 denoting the measurement outcome as either the formula_33 eigenstate of Pauli-X for formula_34 or the formula_35 eigenstate for formula_36. A two qubit state formula_37 connected by a pair of controlled-Z operations to the state formula_38 yields a two-qubit operation on the teleported formula_37 state after measuring the original qubits:\n\nfor measurement outcomes formula_41 and formula_42. This basic concept extends to arbitrarily many qubits, and thus computation is performed by the byproduct operators of teleportation down a chain. Adjusting the desired single-qubit gates is simply a matter of adjusting the measurement basis on each qubit, and non-Pauli measurements are necessary for universal quantum computation.\n\nPath-entangled two qubit states have been generated in laboratory settings on silicon photonic chips in recent years, making important steps in the direction of generating optical cluster states. Among methods of doing this, it has been shown experimentally that spontaneous four-wave mixing can be used with the appropriate use of microring resonators and other waveguides for filtering to perform on-chip generation of two-photon Bell states, which are equivalent to two-qubit cluster states up to local unitary operations.\n\nTo do this, a short laser pulse is injected into an on-chip waveguide that splits into two paths. This forces the pulse into a superposition of the possible directions it could go. The two paths are coupled to microring resonators that allow circulation of the laser pulse until spontaneous four-wave mixing occurs, taking two photons from the laser pulse and converting them into a pair of photons, called the signal formula_43 and idler formula_44 with different frequencies in a way that conserves energy. In order to prevent the generation of multiple photon pairs at once, the procedure takes advantage of the conservation of energy and ensures that there is only enough energy in the laser pulse to create a single pair of photons. Because of this restriction, spontaneous four-wave mixing can only occur in one of the microring resonators at a time, meaning that the superposition of paths that the laser pulse could take is converted into a superposition of paths the two photons could be on. Mathematically, if formula_45 denotes the laser pulse, the paths are labeled as formula_10 and formula_11, the process can be written as\n\nwhere formula_49 is the representation of having formula_8 of photon formula_51 on path formula_52. With the state of the two photons being in this kind of superposition, they are entangled, which can be verified by tests of Bell inequalities.\n\nPolarization entangled photon pairs have also been produced on-chip. The setup involves a silicon wire waveguide that is split in half by a polarization rotator. This process, like the entanglement generation described for the dual rail encoding, makes use of the nonlinear process of spontaneous four-wave mixing, which can occur in the silicon wire on either side of the polarization rotator. However, the geometry of these wires are designed such that horizontal polarization is preferred in the conversion of laser pump photons to signal and idler photons. Thus when the photon pair is generated, both photons should have the same polarization, i.e.\n\nThe polarization rotator is then designed with the specific dimensions such that horizontal polarization is switched to vertical polarization. Thus any pairs of photons generated before the rotator exit the waveguide with vertical polarization and any pairs generated on the other end of the wire exit the waveguide still having horizontal polarization. Mathematically, the process is, up to overall normalization,\n\nAssuming that equal space on each side of the rotator makes spontaneous four-wave mixing equally likely one each side, the output state of the photons is maximally entangled:\n\nStates generated this way could potentially be used to build a cluster state using the Browne-Rudolph protocol.\n"}
{"id": "5988691", "url": "https://en.wikipedia.org/wiki?curid=5988691", "title": "Overconvergent modular form", "text": "Overconvergent modular form\n\nIn mathematics, overconvergent modular forms are special p-adic modular forms that are elements of certain \"p\"-adic Banach spaces (usually infinite dimensional) \ncontaining classical spaces of modular forms as subspaces. They were introduced by Nicholas M. Katz in 1972.\n\n"}
{"id": "2556580", "url": "https://en.wikipedia.org/wiki?curid=2556580", "title": "Paul Bernays", "text": "Paul Bernays\n\nPaul Isaac Bernays (17 October 1888 – 18 September 1977) was a Swiss mathematician, who made significant contributions to mathematical logic, axiomatic set theory, and the philosophy of mathematics. He was an assistant and close collaborator of David Hilbert.\n\nBernays was born into a distinguished German-Jewish family of scholars and businessmen. His great-grandfather, Isaac ben Jacob Bernays, served as chief rabbi of Hamburg from 1821 to 1849.\n\nBernays spent his childhood in Berlin, and attended the Köllner Gymnasium, 1895–1907. At the University of Berlin, he studied mathematics under Issai Schur, Edmund Landau, Ferdinand Georg Frobenius, and Friedrich Schottky; philosophy under Alois Riehl, Carl Stumpf and Ernst Cassirer; and physics under Max Planck. At the University of Göttingen, he studied mathematics under David Hilbert, Edmund Landau, Hermann Weyl, and Felix Klein; physics under Voigt and Max Born; and philosophy under Leonard Nelson.\n\nIn 1912, the University of Berlin awarded him a Ph.D. in mathematics, for a thesis, supervised by Landau, on the analytic number theory of binary quadratic forms. That same year, the University of Zurich awarded him the Habilitation for a thesis on complex analysis and Picard's theorem. The examiner was Ernst Zermelo. Bernays was Privatdozent at the University of Zurich, 1912–17, where he came to know George Pólya.\n\nStarting in 1917, David Hilbert employed Bernays to assist him with his investigations of the foundations of arithmetic. Bernays also lectured on other areas of mathematics at the University of Göttingen. In 1918, that university awarded him a second Habilitation, for a thesis on the axiomatics of the propositional calculus of \"Principia Mathematica\".\n\nIn 1922, Göttingen appointed Bernays extraordinary professor without tenure. His most successful student there was Gerhard Gentzen. After the passing of the Law for the Restoration of the Professional Civil Service in 1933, he was dismissed from this post because of his Jewish ancestry. After working privately for Hilbert for six months, Bernays and his family moved to Switzerland, whose nationality he had inherited from his father, and where the ETH employed him on occasion. He also visited the University of Pennsylvania and was a visiting scholar at the Institute for Advanced Study in 1935–36 and again in 1959–60.\n\nBernays's collaboration with Hilbert culminated in the two volume work Grundlagen der Mathematik by , discussed in Sieg and Ravaglia (2005). In seven papers, published between 1937 and 1954 in the \"Journal of Symbolic Logic\", republished in , Bernays set out an axiomatic set theory whose starting point was a related theory John von Neumann had set out in the 1920s. Von Neumann's theory took the notions of function and argument as primitive; Bernays recast von Neumann's theory so that classes and sets were primitive. Bernays's theory, with some modifications by Kurt Gödel, is now known as von Neumann–Bernays–Gödel set theory.\nA proof from the \"Grundlagen der Mathematik\" that a sufficiently strong consistent theory cannot contain its own reference functor is now known as the Hilbert–Bernays paradox.\n\n\n\n"}
{"id": "187805", "url": "https://en.wikipedia.org/wiki?curid=187805", "title": "Percolation theory", "text": "Percolation theory\n\nIn statistical physics and mathematics, percolation theory describes the behaviour of connected clusters in a random graph. The applications of percolation theory to materials science and other domains are discussed in the article percolation.\n\nA representative question (and the source of the name) is as follows. Assume that some liquid is poured on top of some porous material. Will the liquid be able to make its way from hole to hole and reach the bottom? This physical question is modelled mathematically as a three-dimensional network of vertices, usually called \"sites\", in which the edge or \"bonds\" between each two neighbors may be open (allowing the liquid through) with probability , or closed with probability , and they are assumed to be independent. Therefore, for a given , what is the probability that an open path (meaning a path, each of whose links is an \"open\" bond) exists from the top to the bottom? The behavior for large  is of primary interest. This problem, called now bond percolation, was introduced in the mathematics literature by , and has been studied intensively by mathematicians and physicists since then.\n\nIn a slightly different mathematical model for obtaining a random graph, a site is \"occupied\" with probability or \"empty\" (in which case its edges are removed) with probability ; the corresponding problem is called site percolation. The question is the same: for a given \"p\", what is the probability that a path exists between top and bottom? Similarly, one can ask, given a connected graph at what fraction of failures the graph will become disconnected (no large component).\n\nThe same questions can be asked for any lattice dimension. As is quite typical, it is actually easier to examine infinite networks than just large ones. In this case the corresponding question is: does an infinite open cluster exist? That is, is there a path of connected points of infinite length \"through\" the network? By Kolmogorov's zero–one law, for any given , the probability that an infinite cluster exists is either zero or one. Since this probability is an increasing function of (proof via coupling argument), there must be a critical (denoted by ) below which the probability is always 0 and above which the probability is always 1. In practice, this criticality is very easy to observe. Even for as small as 100, the probability of an open path from the top to the bottom increases sharply from very close to zero to very close to one in a short span of values of .\n\nThe universality principle states that the numerical value of is determined by the local structure of the graph, whereas the kind of behavior of clusters that is observed below, at, and above is independent of the local structure, and therefore, in some sense these behaviors are more natural to consider than itself. This universality also means that for a given dimension, the various critical exponents, the fractal dimension of the clusters at is independent of the lattice type and percolation type (e.g., bond or site). However, recently percolation has been performed on a weighted planar stochastic lattice (WPSL) and found that although the dimension of the WPSL coincides with the dimension of the space where it is embedded, its universality class is different from that of all the known planar lattices.\n\nThe main fact in the subcritical phase is \"exponential decay\". That is, when , the probability that a specific point (for example, the origin) is contained in an open cluster (meaning a maximal connected set of \"open\" edges of the graph) of size decays to zero exponentially in . This was proved for percolation in three and more dimensions by and independently by . In two dimensions, it formed part of Kesten's proof that .\n\nThe dual graph of the square lattice is also the square lattice. It follows that, in two dimensions, the supercritical phase is dual to a subcritical percolation process. This provides essentially full information about the supercritical model with . The main result for the supercritical phase in three and more dimensions is that, for sufficiently large , there is an infinite open cluster in the two-dimensional slab . This was proved by .\n\nIn two dimensions with , there is with probability one a unique infinite closed cluster (a closed cluster is a maximal connected set of \"closed\" edges of the graph). Thus the subcritical phase may be described as finite open islands in an infinite closed ocean. When just the opposite occurs, with finite closed islands in an infinite open ocean. The picture is more complicated when since , and there is coexistence of infinite open and closed clusters for between and .\n\nThe model has a singularity at the critical point believed to be of power-law type. Scaling theory predicts the existence of critical exponents, depending on the number \"d\" of dimensions, that determine the class of the singularity. When these predictions are backed up by arguments from conformal field theory and Schramm–Loewner evolution, and include predicted numerical values for the exponents. Most of these predictions are conjectural except when the number of dimensions satisfies either or . They include:\n\n\nSee . In 11 or more dimensions, these facts are largely proved using a technique known as the lace expansion. It is believed that a version of the lace expansion should be valid for 7 or more dimensions, perhaps with implications also for the threshold case of 6 dimensions. The connection of percolation to the lace expansion is found in .\n\nIn two dimensions, the first fact (\"no percolation in the critical phase\") is proved for many lattices, using duality. Substantial progress has been made on two-dimensional percolation through the conjecture of Oded Schramm that the scaling limit of a large cluster may be described in terms of a Schramm–Loewner evolution. This conjecture was proved by in the special case of site percolation on the triangular lattice.\n\n\nPercolation theory has been applied to studies of how environment fragmentation impacts animal habitats, models of how the plague bacterium \"Yersinia pestis\" spreads, and how amounts of certain elastic fibers may aid in diagnosing chronic obstructive pulmonary disease (COPD).\n\n"}
{"id": "24314", "url": "https://en.wikipedia.org/wiki?curid=24314", "title": "Planar graph", "text": "Planar graph\n\nIn graph theory, a planar graph is a graph that can be embedded in the plane, i.e., it can be drawn on the plane in such a way that its edges intersect only at their endpoints. In other words, it can be drawn in such a way that no edges cross each other. Such a drawing is called a plane graph or planar embedding of the graph. A plane graph can be defined as a planar graph with a mapping from every node to a point on a plane, and from every edge to a plane curve on that plane, such that the extreme points of each curve are the points mapped from its end nodes, and all curves are disjoint except on their extreme points.\n\nEvery graph that can be drawn on a plane can be drawn on the sphere as well, and vice versa, by means of stereographic projection.\n\nPlane graphs can be encoded by combinatorial maps.\n\nThe equivalence class of topologically equivalent drawings on the sphere is called a planar map. Although a plane graph has an external or unbounded face, none of the faces of a planar map have a particular status.\n\nPlanar graphs generalize to graphs drawable on a surface of a given genus. In this terminology, planar graphs have graph genus 0, since the plane (and the sphere) are surfaces of genus 0. See \"graph embedding\" for other related topics.\n\nThe Polish mathematician Kazimierz Kuratowski provided a characterization of planar graphs in terms of forbidden graphs, now known as Kuratowski's theorem:\n\nA subdivision of a graph results from inserting vertices into edges (for example, changing an edge •——• to •—•—•) zero or more times.\nInstead of considering subdivisions, Wagner's theorem deals with minors:\n\nA minor of a graph results from taking a subgraph and repeatedly contracting an edge into a vertex, with each neighbor of the original end-vertices becoming a neighbor of the new vertex.\n\nKlaus Wagner asked more generally whether any minor-closed class of graphs is determined by a finite set of \"forbidden minors\". This is now the Robertson–Seymour theorem, proved in a long series of papers. In the language of this theorem, \"K\" and \"K\" are the forbidden minors for the class of finite planar graphs.\n\nIn practice, it is difficult to use Kuratowski's criterion to quickly decide whether a given graph is planar. However, there exist fast algorithms for this problem: for a graph with \"n\" vertices, it is possible to determine in time O(\"n\") (linear time) whether the graph may be planar or not (see planarity testing).\n\nFor a simple, connected, planar graph with \"v\" vertices and \"e\" edges and \"f\" faces, the following simple conditions hold for \"v\" ≥ 3:\n\n\nIn this sense, planar graphs are sparse graphs, in that they have only O(\"v\") edges, asymptotically smaller than the maximum O(\"v\"). The graph \"K\", for example, has 6 vertices, 9 edges, and no cycles of length 3. Therefore, by Theorem 2, it cannot be planar. Note that these theorems provide necessary conditions for planarity that are not sufficient conditions, and therefore can only be used to prove a graph is not planar, not that it is planar. If both theorem 1 and 2 fail, other methods may be used.\n\n\nEuler's formula states that if a finite, connected, planar graph is drawn in the plane without any edge intersections, and \"v\" is the number of vertices, \"e\" is the number of edges and \"f\" is the number of faces (regions bounded by edges, including the outer, infinitely large region), then\n\nAs an illustration, in the butterfly graph given above, \"v\" = 5, \"e\" = 6 and \"f\" = 3. \nIn general, if the property holds for all planar graphs of \"f\" faces, any change to the graph that creates an additional face while keeping the graph planar would keep \"v\" − \"e\" + \"f\" an invariant. Since the property holds for all graphs with \"f\" = 2, by mathematical induction it holds for all cases. Euler's formula can also be proved as follows: if the graph isn't a tree, then remove an edge which completes a cycle. This lowers both \"e\" and \"f\" by one, leaving \"v\" − \"e\" + \"f\" constant. Repeat until the remaining graph is a tree; trees have \"v\" =  \"e\" + 1 and \"f\" = 1, yielding \"v\" − \"e\" + \"f\" = 2, i. e., the Euler characteristic is 2.\n\nIn a finite, connected, \"simple\", planar graph, any face (except possibly the outer one) is bounded by at least three edges and every edge touches at most two faces; using Euler's formula, one can then show that these graphs are \"sparse\" in the sense that if \"v\" ≥ 3:\n\nEuler's formula is also valid for convex polyhedra. This is no coincidence: every convex polyhedron can be turned into a connected, simple, planar graph by using the Schlegel diagram of the polyhedron, a perspective projection of the polyhedron onto a plane with the center of perspective chosen near the center of one of the polyhedron's faces. Not every planar graph corresponds to a convex polyhedron in this way: the trees do not, for example. Steinitz's theorem says that the polyhedral graphs formed from convex polyhedra are precisely the finite 3-connected simple planar graphs. More generally, Euler's formula applies to any polyhedron whose faces are simple polygons that form a surface topologically equivalent to a sphere, regardless of its convexity.\n\nConnected planar graphs with more than one edge obey the inequality formula_3, because each face has at least three face-edge incidences and each edge contributes exactly two incidences. It follows via algebraic transformations of this inequality with Euler's formula formula_4 that for finite planar graphs the average degree is strictly less than 6. Graphs with higher average degree cannot be planar.\n\nWe say that two circles drawn in a plane \"kiss\" (or \"osculate\") whenever they intersect in exactly one point. A \"coin graph\" is a graph formed by a set of circles, no two of which have overlapping interiors, by making a vertex for each circle and an edge for each pair of circles that kiss. The circle packing theorem, first proved by Paul Koebe in 1936, states that a graph is planar if and only if it is a coin graph.\n\nThis result provides an easy proof of Fáry's theorem, that every planar graph can be embedded in the plane in such a way that its edges are straight line segments that do not cross each other. If one places each vertex of the graph at the center of the corresponding circle in a coin graph representation, then the line segments between centers of kissing circles do not cross any of the other edges.\n\nThe density formula_5 of a planar graph, or network, is defined as a ratio of the number of edges formula_6 to the number of possible edges in a network with formula_7 nodes, given by a planar graph formula_8, giving formula_9. A completely sparse planar graph has formula_10, alternatively a completely dense planar graph has formula_11\n\nA simple graph is called maximal planar if it is planar but adding any edge (on the given vertex set) would destroy that property. All faces (including the outer one) are then bounded by three edges, explaining the alternative term plane triangulation. The alternative names \"triangular graph\" or \"triangulated graph\" have also been used, but are ambiguous, as they more commonly refer to the line graph of a complete graph and to the chordal graphs respectively. Every maximal planar is 3-vertex-connected.\n\nIf a maximal planar graph has \"v\" vertices with \"v\" > 2, then it has precisely 3\"v\" − 6 edges and 2\"v\" − 4 faces.\n\nApollonian networks are the maximal planar graphs formed by repeatedly splitting triangular faces into triples of smaller triangles. Equivalently, they are the planar 3-trees.\n\nStrangulated graphs are the graphs in which every peripheral cycle is a triangle. In a maximal planar graph (or more generally a polyhedral graph) the peripheral cycles are the faces, so maximal planar graphs are strangulated. The strangulated graphs include also the chordal graphs, and are exactly the graphs that can be formed by clique-sums (without deleting edges) of complete graphs and maximal planar graphs.\n\nOuterplanar graphs are graphs with an embedding in the plane such that all vertices belong to the unbounded face of the embedding. Every outerplanar graph is planar, but the converse is not true: \"K\" is planar but not outerplanar. A theorem similar to Kuratowski's states that a finite graph is outerplanar if and only if it does not contain a subdivision of \"K\" or of \"K\".\n\nA 1-outerplanar embedding of a graph is the same as an outerplanar embedding. For \"k\" > 1 a planar embedding is \"k\"-outerplanar if removing the vertices on the outer face results in a (\"k\" − 1)-outerplanar embedding. A graph is \"k\"-outerplanar if it has a \"k\"-outerplanar embedding.\n\nA Halin graph is a graph formed from an undirected plane tree (with no degree-two nodes) by connecting its leaves into a cycle, in the order given by the plane embedding of the tree. Equivalently, it is a polyhedral graph in which one face is adjacent to all the others. Every Halin graph is planar. Like outerplanar graphs, Halin graphs have low treewidth, making many algorithmic problems on them more easily solved than in unrestricted planar graphs.\n\nAn apex graph is a graph that may be made planar by the removal of one vertex, and a \"k\"-apex graph is a graph that may be made planar by the removal of at most \"k\" vertices.\n\nA 1-planar graph is a graph that may be drawn in the plane with at most one simple crossing per edge, and a \"k\"-planar graph is a graph that may be drawn with at most \"k\" simple crossings per edge.\n\nA map graph is a graph formed from a set of finitely many simply-connected interior-disjoint regions in the plane by connecting two regions when they share at least one boundary point. When at most three regions meet at a point, the result is a planar graph, but when four or more regions meet at a point, the result can be nonplanar.\n\nA toroidal graph is a graph that can be embedded without crossings on the torus. More generally, the genus of a graph is the minimum genus of a two-dimensional surface into which the graph may be embedded; planar graphs have genus zero and nonplanar toroidal graphs have genus one.\n\nAny graph may be embedded into three-dimensional space without crossings. However, a three-dimensional analogue of the planar graphs is provided by the linklessly embeddable graphs, graphs that can be embedded into three-dimensional space in such a way that no two cycles are topologically linked with each other. In analogy to Kuratowski's and Wagner's characterizations of the planar graphs as being the graphs that do not contain \"K\" or \"K\" as a minor, the linklessly embeddable graphs may be characterized as the graphs that do not contain as a minor any of the seven graphs in the Petersen family. In analogy to the characterizations of the outerplanar and planar graphs as being the graphs with Colin de Verdière graph invariant at most two or three, the linklessly embeddable graphs are the graphs that have Colin de Verdière invariant at most four.\n\nAn upward planar graph is a directed acyclic graph that can be drawn in the plane with its edges as non-crossing curves that are consistently oriented in an upward direction. Not every planar directed acyclic graph is upward planar, and it is NP-complete to test whether a given graph is upward planar.\n\nThe asymptotic for the number of (labeled) planar graphs on formula_12 vertices is formula_13, where formula_14 and formula_15.\n\nAlmost all planar graphs have an exponential number of automorphisms.\n\nThe number of unlabeled (non-isomorphic) planar graphs on formula_12 vertices is between formula_17 and formula_18.\n\nThe Four Color Theorem states that every planar graph is 4-colorable (i.e. 4-partite).\n\nFáry's theorem states that every simple planar graph admits an embedding in the plane such that all edges are straight line segments which don't intersect. A universal point set is a set of points such that every planar graph with \"n\" vertices has such an embedding with all vertices in the point set; there exist universal point sets of quadratic size, formed by taking a rectangular subset of the integer lattice. Every simple outerplanar graph admits an embedding in the plane such that all vertices lie on a fixed circle and all edges are straight line segments that lie inside the disk and don't intersect, so \"n\"-vertex regular polygons are universal for outerplanar graphs.\nGiven an embedding \"G\" of a (not necessarily simple) connected graph in the plane without edge intersections, we construct the dual graph \"G\"* as follows: we choose one vertex in each face of \"G\" (including the outer face) and for each edge \"e\" in \"G\" we introduce a new edge in \"G\"* connecting the two vertices in \"G\"* corresponding to the two faces in \"G\" that meet at \"e\". Furthermore, this edge is drawn so that it crosses \"e\" exactly once and that no other edge of \"G\" or \"G\"* is intersected. Then \"G\"* is again the embedding of a (not necessarily simple) planar graph; it has as many edges as \"G\", as many vertices as \"G\" has faces and as many faces as \"G\" has vertices. The term \"dual\" is justified by the fact that \"G\"** = \"G\"; here the equality is the equivalence of embeddings on the sphere. If \"G\" is the planar graph corresponding to a convex polyhedron, then \"G\"* is the planar graph corresponding to the dual polyhedron.\n\nDuals are useful because many properties of the dual graph are related in simple ways to properties of the original graph, enabling results to be proven about graphs by examining their dual graphs.\n\nWhile the dual constructed for a particular embedding is unique (up to isomorphism), graphs may have different (i.e. non-isomorphic) duals, obtained from different (i.e. non-homeomorphic) embeddings.\nA \"Euclidean graph\" is a graph in which the vertices represent points in the plane, and the edges are assigned lengths equal to the Euclidean distance between those points; see Geometric graph theory.\n\nA plane graph is said to be \"convex\" if all of its faces (including the outer face) are convex polygons. A planar graph may be drawn convexly if and only if it is a subdivision of a 3-vertex-connected planar graph.\n\nScheinerman's conjecture (now a theorem) states that every planar graph can be represented as an intersection graph of line segments in the plane.\n\nThe planar separator theorem states that every \"n\"-vertex planar graph can be partitioned into two subgraphs of size at most 2\"n\"/3 by the removal of O() vertices. As a consequence, planar graphs also have treewidth and branch-width O().\n\nFor two planar graphs with \"v\" vertices, it is possible to determine in time O(\"v\") whether they are isomorphic or not (see also graph isomorphism problem).\n\nThe meshedness coefficient of a planar graph normalizes its number of bounded faces (the same as the circuit rank of the graph, by Mac Lane's planarity criterion) by dividing it by 2\"n\" − 5, the maximum possible number of bounded faces in a planar graph with \"n\" vertices. Thus, it ranges from 0 for trees to 1 for maximal planar graphs.\n\n\n\n"}
{"id": "57185095", "url": "https://en.wikipedia.org/wiki?curid=57185095", "title": "Prix Francoeur", "text": "Prix Francoeur\n\nThe Prix Francoeur, or Francoeur Prize, was an award granted by the Institut de France, Academie des Sciences, Fondation Francoeur to authors of works useful to the progress of pure and applied mathematics. Preference was given to young scholars or to geometricians not yet established. It was established in 1882 and has been discontinued.\n"}
{"id": "18409940", "url": "https://en.wikipedia.org/wiki?curid=18409940", "title": "Quantum threshold theorem", "text": "Quantum threshold theorem\n\nIn quantum computing, the (quantum) threshold theorem (or quantum fault-tolerance theorem), proved by Michael Ben-Or and Dorit Aharonov (along with other groups), states that a quantum computer with a physical error rate below a certain threshold can, through application of Quantum error correction schemes, suppress the logical error rate to arbitrarily low levels. Current estimates put the threshold for the surface code on the order of 1%, though estimates range widely and are difficult to calculate due to the exponential difficulty of simulating large quantum systems. At a 0.1% probability of a depolarizing error, the surface code would require approximately 1,000-10,000 physical qubits per logical data qubit, though more pathological error types could change this figure drastically.\n\n\nBooks:\n\nPapers from : https://journals.aps.org/\n\nPapers from : https://arxiv.org/\n\n"}
{"id": "8712432", "url": "https://en.wikipedia.org/wiki?curid=8712432", "title": "Rafail Ostrovsky", "text": "Rafail Ostrovsky\n\nRafail Ostrovsky is a professor of computer science and mathematics at UCLA and a well-known researcher in algorithms and cryptography.\n\nRafail Ostrovsky received his Ph.D. from MIT in 1992.\n\nHe is a member of the Editorial Board of Algorithmica \n, Editorial Board of Journal of Cryptology and Editorial and Advisory Board of the International Journal of Information and Computer Security .\n\n\nSome of Ostrovsky's contributions to computer science include:\n\n"}
{"id": "2985811", "url": "https://en.wikipedia.org/wiki?curid=2985811", "title": "Signed measure", "text": "Signed measure\n\nIn mathematics, signed measure is a generalization of the concept of measure by allowing it to have negative values. Some authors may call it a charge, by analogy with electric charge, which is a familiar distribution that takes on positive and negative values.\n\nThere are two slightly different concepts of a signed measure, depending on whether or not one allows it to take infinite values. In research papers and advanced books signed measures are usually only allowed to take finite values, while undergraduate textbooks often allow them to take infinite values. To avoid confusion, this article will call these two cases \"finite signed measures\" and \"extended signed measures\".\n\nGiven a measurable space (\"X\", Σ), that is, a set \"X\" with a sigma algebra Σ on it, an extended signed measure is a function\nsuch that formula_2 and formula_3 is sigma additive, that is, it satisfies the equality\nwhere the series on the right must converge absolutely, for any sequence \"A\", \"A\", ..., \"A\", ... of disjoint sets in Σ. One consequence is that any extended signed measure can take +∞ as value, or it can take −∞ as value, but both are not available. The expression ∞ − ∞ is undefined and must be avoided.\n\nA finite signed measure (aka. real measure) is defined in the same way, except that it is only allowed to take real values. That is, it cannot take +∞ or −∞.\n\nFinite signed measures form a vector space, while extended signed measures are not even closed under addition, which makes them rather hard to work with. On the other hand, measures are extended signed measures, but are not in general finite signed measures.\n\nConsider a nonnegative measure ν on the space (\"X\", Σ) and a measurable function \"f\":\"X\"→ R such that\n\nThen, a finite signed measure is given by\n\nfor all \"A\" in Σ.\n\nThis signed measure takes only finite values. To allow it to take +∞ as a value, one needs to replace the assumption about \"f\" being absolutely integrable with the more relaxed condition\n\nwhere \"f\"(\"x\") = max(−\"f\"(\"x\"), 0) is the negative part of \"f\".\n\nWhat follows are two results which will imply that an extended signed measure is the difference of two nonnegative measures, and a finite signed measure is the difference of two finite non-negative measures.\n\nThe Hahn decomposition theorem states that given a signed measure μ, there exist two measurable sets \"P\" and \"N\" such that:\n\nMoreover, this decomposition is unique up to adding to/subtracting μ-null sets from \"P\" and \"N\".\n\nConsider then two nonnegative measures μ and μ defined by\n\nand\n\nfor all measurable sets \"E\", that is, \"E\" in Σ.\n\nOne can check that both μ and μ are nonnegative measures, with one taking only finite values, and are called the \"positive part\" and \"negative part\" of μ, respectively. One has that μ = μ - μ. The measure |μ| = μ + μ is called the \"variation\" of μ, and its maximum possible value, ||μ|| = |μ|(\"X\"), is called the \"total variation\" of μ.\n\nThis consequence of the Hahn decomposition theorem is called the \"Jordan decomposition\". The measures μ, μ and |μ| are independent of the choice of \"P\" and \"N\" in the Hahn decomposition theorem.\n\nThe sum of two finite signed measures is a finite signed measure, as is the product of a finite signed measure by a real number: they are closed under linear combination. It follows that the set of finite signed measures on a measurable space (\"X\", Σ) is a real vector space; this is in contrast to positive measures, which are only closed under conical combination, and thus form a convex cone but not a vector space. Furthermore, the total variation defines a norm in respect to which the space of finite signed measures becomes a Banach space. This space has even more structure, in that it can be shown to be a Dedekind complete Banach lattice and in so doing the Radon–Nikodym theorem can be shown to be a special case of the Freudenthal spectral theorem.\n\nIf \"X\" is a compact separable space, then the space of finite signed Baire measures is the dual of the real Banach space of all continuous real-valued functions on \"X\", by the Riesz–Markov–Kakutani representation theorem.\n\n\n"}
{"id": "236098", "url": "https://en.wikipedia.org/wiki?curid=236098", "title": "Subsequence", "text": "Subsequence\n\nIn mathematics, a subsequence is a sequence that can be derived from another sequence by deleting some or no elements without changing the order of the remaining elements. For example, the sequence formula_1 is a subsequence of formula_2 obtained after removal of elements formula_3, formula_4, and formula_5. The relation of one sequence being the subsequence of another is a preorder.\n\nThe subsequence should not be confused with substring formula_6 which can be derived from the above string formula_2 by deleting substring formula_8. The substring is a refinement of the subsequence. \n\nThe list of all subsequences for the word \"apple\" would be \"a, ap, al, ae, app, apl, ape, ale, appl, appe, aple, apple, p, pp, pl, pe, ppl, ppe, ple, pple, l, le, e\". \n\nGiven two sequences \"X\" and \"Y\", a sequence \"Z\" is said to be a \"common subsequence\" of \"X\" and \"Y\", if \"Z\" is a subsequence of both \"X\" and \"Y\". For example, if\n\nthen a common subsequence of \"X\" and \"Y\" could be\n\nThis would \"not\" be the \"longest common subsequence\", since \"Z\" only has length 3, and the common subsequence formula_12 has length 4. The longest common subsequence of \"X\" and \"Y\" is formula_13.\n\nSubsequences have applications to computer science, especially in the discipline of bioinformatics, where computers are used to compare, analyze, and store DNA, RNA, and protein sequences.\n\nTake two sequences of DNA containing 37 elements, say:\n\nThe longest common subsequence of sequences 1 and 2 is:\n\nThis can be illustrated by highlighting the 27 elements of the longest common subsequence into the initial sequences:\n\nAnother way to show this is to \"align\" the two sequences, \"i.e.\", to position elements of the longest common subsequence in a same column (indicated by the vertical bar) and to introduce a special character (here, a dash) in one sequence when two elements in the same column differ:\n\nSubsequences are used to determine how similar the two strands of DNA are, using the DNA bases: adenine, guanine, cytosine and thymine.\n\n\n"}
{"id": "46919225", "url": "https://en.wikipedia.org/wiki?curid=46919225", "title": "Suzanne Lenhart", "text": "Suzanne Lenhart\n\nSuzanne Marie Lenhart (born November 19, 1954) is an American mathematician who works in partial differential equations and mathematical biology. She is a Chancellor's Professor of mathematics at the University of Tennessee, an associate director for education and outreach at the National Institute for Mathematical and Biological Synthesis, and a part-time researcher at the Oak Ridge National Laboratory.\n\nLenhart grew up in Louisville, Kentucky, and was educated in the Catholic school system there. She did her undergraduate studies at Bellarmine College in Louisville, where Ralph Grimaldi encouraged her to prepare for graduate studies in mathematics and gave her additional tutoring in number theory. She entered graduate school at the University of Kentucky not knowing what she would specialize in, but in her second year chose partial differential equations. She completed her doctorate in 1981 under the supervision of Lawrence C. Evans, and immediately took a tenure-track faculty position at the University of Tennessee. She added a second part-time position at Oak Ridge in 1987.\n\nLenhart was AWM/MAA Falconer Lecturer in 1997, president of the Association for Women in Mathematics in 2001–2003, and AWM/SIAM Sonia Kovalevsky Lecturer in 2010 \"in recognition of her significant research in partial differential equations, ordinary differential equations, and optimal control\". She was elected as a fellow of the American Association for the Advancement of Science in 2010, and became a Chancellor's Professor and SIAM Fellow in 2011. In 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n\n"}
{"id": "740362", "url": "https://en.wikipedia.org/wiki?curid=740362", "title": "Tathagat Avatar Tulsi", "text": "Tathagat Avatar Tulsi\n\nTathagat Avatar Tulsi (born 9 September 1987) is an Indian physicist, best known as a child prodigy. He completed high school at the age of 9, earned a B.Sc. at the age of 10 and a M.Sc. at the age of 12 from Patna Science College (Patna University). In August 2009, he got his Ph.D. from the Indian Institute of Science, Bangalore at the age of 22. In July 2010, he was offered a position as Assistant Professor on contract (a non-permanent teaching position for fresh Ph.D. graduates) at IIT Bombay.\n\nWhile he was admitted for a Ph.D. program at 17, to a journalist query the then dean of the physics dept at IISc described him as a \"good boy, very lovable and working to achieve his goals\"; however, he declined to comment on the description of Tulsi as a prodigy.\n\nHe received wide public attention in 2001, when he was shortlisted by the Indian Government's Department of Science and Technology (DST) to participate in a Nobel laureates conference in Germany. Later, the DST officials claimed that it was a mistake to shortlist him, and that he was a \"fake prodigy\". Tulsi claimed that he had impressed three Nobel laureates but two of them could not recall having talked to him and the third Brian Josephson did not find anything promising in his ideas. Following the controversy, Tulsi is said to have suffered from depression, which he says, cost him a Ph.D. seat at IIT. He decided to \"fight back\" by earning a Ph.D. and gaining recognition as a scientist.\n\nTulsi was admitted by the Indian Institute of Science (IISc)., where he wrote a 33-page long Ph.D. thesis on \"Generalizations of the Quantum Search Algorithm\". He has the special distinction of being one of the world's youngest scientists. At the age of 17, he co-authored a research manuscript (\"A New Algorithm for Fixed-point Quantum Search\") with Lov Grover, the inventor of a quantum search algorithm that goes by his name.\n\nTulsi is listed as one of the most gifted Asian youngsters by \"TIME\" magazine, mentioned as \"Superteen\" by \"Science\", \"Physics Prodigy\" by \"The TIMES\", \"Master Mind\" by \"The WEEK\" and listed by \"Outlook\" as one of the smartest Indian youngsters. Tathagat Avatar Tulsi participated in the Stock Exchange of Visions project of Fabrica, Benetton's research centre in 2007. He was invited by Luciano Benetton for a dinner in honor of Al Gore on 14 June 2007 in Milano, Italy. Tathagat's story was showcased by National Geographic Channel in the program \"My Brilliant Brain\". The episode named \"India's Geniuses\" was aired on 13 December 2007 and was hosted by Bollywood actress Konkona Sen Sharma. He was interviewed by 14-year-old Trishit Banerjee for his magazine \"Young Chronicle\".\n\n\n"}
{"id": "24134845", "url": "https://en.wikipedia.org/wiki?curid=24134845", "title": "Technical drawing tool", "text": "Technical drawing tool\n\nTechnical drawing tools include and are not limited to: pens, rulers, compasses, protractors and drawing utilities. Drafting tools may be used for measurement and layout of drawings, or to improve the consistency and speed of creation of standard drawing elements. The tools used for manual technical drawing have been displaced by the advent of the personal computer and its common utilization as the main tool in computer-aided drawing, draughting and design (CADD).\n\nThe ancient Egyptians are known to have used wooden corner rulers. Ancient Nuragic people in Sardinia used compasses made of bronze, like the one displayed in showcase 25 in the Nuragic department of the National Archeological Museum G. A. Sanna in Sassari. In ancient Greece, evidence has been found of the use of styli and metal chisels, scale rulers and triangle rulers. Excavations in Pompeii have found a bronze tool kit used by the Romans, which contained triangle rulers, compasses and a ruler to use with a pen.\n\nAlthough a variety of styli were developed in ancient times and were still being used in the 18th century, quills were generally used as the main drawing tool. Styli were also used in the form of ivory or ebony pencils.\n\nProtractors have been used to measure and draw angles and arcs of a circle accurately since about the 13th century, although mathematics and science demanded more detailed drawing instruments. The adjustable corner ruler was developed in the 17th century, but a feasible screw-tightened version not until the 1920s.\n\nIn the 17th century, a stylus that could draw a line with a specific width called a ruling pen was developed. The stylus had two curved metal pieces which were joined by a screw. Ink was trickled between the blades, from which it flowed evenly across the paper. The basic model was maintained for a long time, with minor modifications, until the 1930s when the German technical drawing pens came to the market.\n\nArtists (including Leonardo da Vinci and Albrecht Dürer, Nicholas Bion and George Adams) generally made drawing tools for themselves. Industrial production of technical drawing instruments started in 1853, when Englishman William Stanley (1829–1909) founded a technical manufacturing company in London. Even then, however, most tools were still made by hand.\n\nIn the 1930s the equipment available expanded: drawing apparatus and Rapidograph-drawing pens appeared, improving the line quality and, especially, producing consistent line width. In addition to the Rapidograph stylus, a more traditional Grafos-type stylus was used for a long time, where different line widths were achieved by changing the pen nib. For instance in Finland Grafos was commonly used as a primary drawing tool still in the early 1970s.\n\nEquipment changed radically during the 1990s, when computer-aided design almost completely ousted drawing by hand. Technical design has changed from drawing by hand to producing computer-aided design drawings, where drawings are no longer \"drawn\", but are built from a virtually-produced model. Drawings are not necessarily produced in hard copy at all, and if they are needed they are printed automatically by a computer program. Hand-drawn designs, however, are still widely used in the draft design stage.\n\nTraditional and typical styli used for technical drawing are pencils and technical pens.\n\nPencils in use are usually mechanical pencils with a standard lead thickness. The usual line widths are 0.18 mm, 0.25 mm, 0.5 mm and 0.7 mm. Hardness varies usually from HB to 2H. Softer lead gives a better contrast, but harder lead gives a more accurate line. Bad contrast of the lead line in general is problematic when photocopying, but new scanning copy techniques have improved the final result. Paper or plastic surfaces require their own lead types.\n\nIn most cases, the final drawings are drawn with ink, on either plastic or tracing paper. The pen is generally a Rapidograph-type technical pen, a marker pen that draws lines of consistent width (so-called steel marker pen). The pen has an ink container which contains a metal tube, inside which is a thin metal needle or wire, the soul. Ink is absorbed between the needle and the tube wall, preventing an excessive amount of ink from being released. The needle has a weight and by waving the pen back and forth the needle is released and the ink can run. Originally, the tank was filled from an ink bottle; newer pens use ink cartridges.\n\nEach line width has its own stylus. The line width is standardized: In Finland, the most commonly used set is 0.13 mm, 0.18 mm, 0.25 mm, 0.35 mm, 0.50 mm and 0.70 mm. Separate styli are used for tracing paper and plastic, because plastic requires a harder pen tip. To function well they require regular maintenance, the finest marker pens in particular.\n\nThe drawing board is an essential tool. Paper will be attached and kept straight and still, so that the drawing can be done with accuracy. Generally, different kind of assistance rulers are used in drawing. The drawing board is usually mounted to a floor pedestal in which the board turns to a different position, and also its height can be adjustable. Smaller drawing boards are produced for table-top use. In the 18th and 19th centuries,drawing paper was dampened and then its edges glued to the drawing board. After drying the paper would be flat and smooth. The completed drawing was then cut free. Paper could also be secured to the drawing board with drawing pins or even C-clamps. More recent practice is to use self-adhesive tape to secure paper to the board, including the sophisticated use of individualized adhesive dots from a dispensing roll. Some drawing boards are magnetized, allowing paper to be held down by long steel strips. Boards used for overlay drafting or animation may include registration pins or peg bars to ensure alignment of multiple layers of drawing media.\n\nA T-square is a straightedge which uses the edge of the drawing board as a support. It is used with the drafting board to draw horizontal lines and to align other drawing instruments. Wooden, metal, or plastic triangles with 30° and 60° angles or with two 45° angles are used to speed drawing of lines at these commonly used angles. A continuously adjustable 0–90° protractor is also in use. An alternative to the T-square is the parallel bar which is permanently attached to the drawing board. It has a set of cables and pulleys to allow it to be positioned anywhere on the drawing surface while still remaining parallel to the bottom of the board. The drafting machine replaces the T-square and triangles.\n\nA drafting machine is a device which is mounted to the drawing board. It has rulers whose angles can be precisely adjusted with a controlling mechanism.\n\nThere are two main types of apparatus: an arm-type parallelogram apparatus based on a hinged arm; and a track-type apparatus which moves on a rail mounted to the top of the drawing board. The accuracy of the arm type apparatus is better in the middle of the board, decreasing towards the edges, whereas a track machine has a constant accuracy over the whole board. The drawing head of a track-type drafting machine slides on bearings in a vertical rail, which in turn is moved along a horizontal, top-mounted rail. Both apparatus types have an adjustable drawing-head with rules attached to a protractor scale so that the angle of the rules may be adjusted.\n\nA drafting machine allows easy drawing of parallel lines over the paper. The adjustable angle between the rulers allows the lines to be drawn in varying accurate angles. Rulers may also be used as a support for separate special rulers and letter templates. The rules are replaceable and they can be for example scale-rules.\n\nDrawing apparatus has evolved from a drawing board mounted parallel ruler and a pantograph, which is a device used for copying objects in an adjustable ratio of sizes.\n\nFrench curves are made of wood, plastic or celluloid. Some set squares also have these curves cut in the middle. French curves are used for drawing curves which cannot be drawn with compasses. A faint freehand curve is first drawn through the known points; the longest possible curve that coincides exactly with the freehand curve is then found out from the French curves. Finally, a neat continuous curve is drawn with the aid of the French curves.\n\nRulers used in technical drawing are usually made of polystyrene.It is used for drawing lines and connecting points. Rulers come in two types according to the design of their edge. A ruler with a straight edge can be used with lead pencils and felt pens, whereas when a technical pen is used the edge must be grooved to prevent the spread of the ink.\n\nA scale ruler is a scaled, three-edged ruler which has six different scales marked to its sides. A typical combination for building details is 1:20, 1:50, 1:100, 1:25, 1:75 and 1:125. There are separate rulers for zoning work as well as for inch units. Today scale rulers are made of plastic, formerly they were made of hardwood. A pocket-sized version is also available, with scales printed on flexible plastic strips.\n\nCompasses are used for drawing circles or arc segments of circles. One form has two straight legs joined by a hinge; one leg has a sharp pivot point and the other has a holder for a technical pen or pencil. Another form, the beam compass, has the pivot point and pen holder joined by a trammel bar, useful when drawing very large radius arcs. Often a circle template is used instead of a compass when predefined circle sizes are required.\n\nTemplates contain pre-dimensioned holes in the right scale to accurately draw a symbol or sign.\n\nLetter templates are used for drawing text, including digits and letter characters. Diagrams are usually of a standard letter shape and size to conform to standards of encodings (e.g. DIN or ANSI). For example, in Finland the series used is 1.8 mm, 2.5 mm, 3.5 mm, 5.0 mm and 7.0 mm. Except for the very biggest ones, the templates are only suitable for technical pen drawing.\n\nFor drawing circles and circle-arcs, circle templates which contain a set of suitably-sized holes are used. Templates are also available for other geometric shapes such as squares and for drawing ellipses, as well as many specialized varieties for other purposes.\n\nThere are also specific templates to provide user with the most common symbols in use in different branches of designing.\n\nFor example, the architect templates can be used to draw different sized doors with their \"opening arcs\",\n\nbuilding and equipment symbols and furniture. The templates also provide the symbols for thermal insulation.\n\nTwo methods of drawing smooth curves in manual drafting are the use of French curves and flat splines (flexible curves). A French curve is a drawing aid with many different smoothly-varying radiused curves on it; the manual drafter can fit the French curve to some known reference points and draw a smooth curved line between them. A spline is a flexible ruler, usually rubber or plastic coated with a metal \"backbone\", which can be smoothly shaped to follow a desired curve and allows drawing a smooth line between initial reference points. Sometimes a spline is temporarily held in position with small weights.\n\nA perspective machine is an instrument designed to create perspective drawings.\n\nSilk-paper -like translucent drafting paper that wrinkles when wetted. It is primarily suitable for pencils and felt tip pens. Pencil marks can be corrected to some extent with an eraser.\n\nSandwich paper -like, thin translucent sheet of paper.\n\nManufactured in different strengths, the surface may be slightly polished. This paper also wrinkles upon wetting. Suitable for pencil and felt tipped pens, and with limitations for technical pens. An eraser can be used for pencil lines. Ink is difficult to erase without damage.\n\nDrafting linen was formerly used for technical drawings. It was durable and held up to handling, but it was difficult to use in modern whiteprints for reproduction, and shrinking was a concern.\n\nPolished sandwich paper -like, translucent thick paper, which comes in different strengths. Wrinkles upon wetting. Suitable for both graphite pencils and technical pens. An eraser or sharp scraper tool is used for corrections.\n\nTranslucent plastic film, which is usually of gray or a light khaki shade. Common types are 0.05, 0.07 and 0.10 mm thick. These films are also used in photocopying. The most commonly used materials are polyesters, and sometimes also PVC or polycarbonate; arguably, a proprietary eponym or genericized trademark for this is called Mylar.\n\nIn drawing, plastic's specific advantages over translucent paper are higher mechanical strength and dimensional accuracy; plastic does not, unlike paper, shrink or stretch with changing air humidity. Plastic is also as a surface completely flat, while the surface of paper is relatively rough. Plastic is suitable for both pencils and drawing pens. However, the surface tends to wear the pen tips, which must be made of hard-metal alloy. Ordinary ink is not absorbed into the plastic at all, so the lines can easily be removed with an eraser. Photocopier marks can be removed by scraping.\n\nDrawing inks can be divided into two groups: India ink and polymer inks. India ink is used on paper and drafting film plastics. The most commonly used India ink is a colloidal mixture of water and carbon black.\n\nDry transfer decals can speed the production of repetitive drawing elements such as borders, title blocks, line types, shading, and symbols. They were frequently used in the production of schematic drawings, maps, and printed circuit board artwork, for example. Dry transfer lettering such as Letraset was used especially in lettering larger size document annotations, or when consistency of lettering was especially required.\n\nMany copies of technical drawings may be required in the construction of a project. Reproductions must be accurate as to size and shape, but for many purposes need not be permanent. The blueprint process was first used for mechanical reproduction of drawings. Drawing offices may use diazo or whiteprint processes. Where the volume of drawings reproduced justifies the cost of the machine, a large format photocopier using xerography can reproduce drawings at lower cost than re-plotting them.\n\n\n"}
{"id": "32550400", "url": "https://en.wikipedia.org/wiki?curid=32550400", "title": "Volatility risk premium", "text": "Volatility risk premium\n\nIn mathematical finance, the volatility risk premium is a measure of the extra amount investors demand in order to hold a volatile security, above what can be computed based on expected returns.\n\nIt can be defined as the compensation for inherent volatility risk divided by the volatility beta.\n\n"}
{"id": "48217935", "url": "https://en.wikipedia.org/wiki?curid=48217935", "title": "Yael Dowker", "text": "Yael Dowker\n\nYael Naim Dowker (1919–2016) was an Israeli-English mathematician, prominent especially due to her work in the fields of measure theory, ergodic theory and topological dynamics.\n\nYael Naim (later Dowker) was born in Tel Aviv. She left for the United States to study at Johns Hopkins University in Baltimore, Maryland. In 1941, as a graduate student, she met Clifford Hugh Dowker, a Canadian topologist working as an instructor there. The couple married in 1944. From 1943 to 1946 they worked together at the Radiation Laboratory at Massachusetts Institute of Technology. Clifford also worked as a civilian adviser for the United States Air Force during World War II.\n\nDowker did her doctorate at Radcliffe College (in Cambridge, Massachusetts) under Witold Hurewicz (a Polish mathematician known for the Hurewicz theorem). She published her thesis \"Invariant measure and the ergodic theorems\" in 1947 and received her Ph.D in 1948. \nIn the period between 1948 and 1949, she did post-doctoral work at the Institute for Advanced Study, located in Princeton, New Jersey. A few years after the war, McCarthyism became a common phenomenon in the academic world, with several of the Dowker couple's friends in the mathematical community harassed and one arrested. In 1950, they emigrated to the United Kingdom.\n\nIn 1951 Dowker served as a professor at the University of Manchester, and later went on as a professor at the Imperial College London. \nWhile there, among the students she advised was Bill Parry, who published his thesis in 1960. She also cooperated on some of her work with the Hungarian mathematician Paul Erdős (Erdős' number of one). With her husband, she helped educate over thirty \"gifted children\".\n\n\n"}
