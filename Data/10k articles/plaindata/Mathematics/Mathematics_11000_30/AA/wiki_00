{"id": "2130559", "url": "https://en.wikipedia.org/wiki?curid=2130559", "title": "Activity (UML)", "text": "Activity (UML)\n\nAn activity in Unified Modeling Language (UML) is a major task that must take place in order to fulfill an operation contract. Activities can be represented in activity diagrams\n\nAn activity can represent:\nActivities can be decomposed into subactivities, until at the bottom we find atomic actions. \n\nThe underlying conception of an activity has changed between UML 1.5 and UML 2.0. In UML 2.0 an activity is no longer based on the state-chart rather it is based on a Petri net like coordination mechanism. There the activity represents user-defined behavior coordinating actions. Actions in turn are pre-defined (UML offers a series of actions for this).\n"}
{"id": "3108888", "url": "https://en.wikipedia.org/wiki?curid=3108888", "title": "Alexander duality", "text": "Alexander duality\n\nIn mathematics, Alexander duality refers to a duality theory presaged by a result of 1915 by J. W. Alexander, and subsequently further developed, particularly by P. S. Alexandrov and Lev Pontryagin. It applies to the homology theory properties of the complement of a subspace \"X\" in Euclidean space, a sphere, or other manifold. It is generalized by Spanier-Whitehead duality.\n\nLet \"X\" be a compact, locally contractible subspace of the sphere \"S\" of dimension \"n\". Let \"Y\" be the complement of \"X\" in \"S\". Then if \"H\" stands for reduced homology or reduced cohomology, with coefficients in a given abelian group, there is an isomorphism between \n\nand \n\nNote that we can drop local contractibility as part of the hypothesis, if we use Čech cohomology, which is designed to deal with local pathologies.\n\nTo go back to Alexander's original work, it is assumed that \"X\" is a simplicial complex. \n\nAlexander had little of the modern apparatus, and his result was only for the Betti numbers, with coefficients taken \"modulo\" 2. What to expect comes from examples. For example the Clifford torus construction in the 3-sphere shows that the complement of a solid torus is another solid torus; which will be open if the other is closed, but this does not affect its homology. Each of the solid tori is from the homotopy point of view a circle. If we just write down the Betti numbers\n\nof the circle (up to \"H\", since we are in the 3-sphere), then reverse as\n\nand then shift one to the left to get\n\nthere is a difficulty, since we are not getting what we started with. On the other hand the same procedure applied to the \"reduced\" Betti numbers, for which the initial Betti number is decremented by 1, starts with\n\nand gives\n\nwhence\n\nThis \"does\" work out, predicting the complement's reduced Betti numbers.\n\nThe prototype here is the Jordan curve theorem, which topologically concerns the complement of a circle in the Riemann sphere. It also tells the same story. We have the honest Betti numbers\n\nof the circle, and therefore\n\nby flipping over and\n\nby shifting to the left. This gives back something different from what the Jordan theorem states, which is that there are two components, each contractible (Schoenflies theorem, to be accurate about what is used here). That is, the correct answer in honest Betti numbers is \n\nOnce more, it is the reduced Betti numbers that work out. With those, we begin with\n\nto finish with\n\nFrom these two examples, therefore, Alexander's formulation can be inferred: reduced Betti numbers \"b\"* are related in complements by\n\n\n"}
{"id": "3932587", "url": "https://en.wikipedia.org/wiki?curid=3932587", "title": "Attribute domain", "text": "Attribute domain\n\nIn computing, the attribute domain is the set of values allowed in an attribute.\n\nFor example: \n\nFor the relational model it is a requirement that each part of a tuple be atomic. The consequence is that each value in the tuple must be of some basic type, like a string or an integer. For the elementary type to be atomic it cannot be broken into more pieces. Alas, the domain is an elementary type, and attribute domain the domain a given attribute belongs to an abstraction belonging to or characteristic of an entity.\n\nFor example in SQL ,one can create their own domain for an attribute with the command\n\nThe above command says : \"Create a datatype SSN_TYPE that is of character type with size 9 \"\n"}
{"id": "4607798", "url": "https://en.wikipedia.org/wiki?curid=4607798", "title": "Australian Mathematics Competition", "text": "Australian Mathematics Competition\n\nThe Australian Mathematics Competition is a mathematics competition run by the Australian Mathematics Trust for students from year 3 up to year 12 in Australia, and their equivalent grades in other countries. Since its inception in 1976 in the Australian Capital Territory, the participation numbers have increased to around 600,000, with around 100,000 being from outside Australia, making it the world's largest mathematics competition.\n\nThe fore-runner of the competition, first held in 1976, was open to students within the Australian Capital Territory, and attracted 1200 entries. In 1976 and 1977 the outstanding entrants were awarded the Burroughs medal. In 1978, the competition became a nationwide event, and became known as the Australian Mathematics Competition for the Wales awards with 60,000 students from Australia and New Zealand participating. In 1983 the medals were renamed the Westpac awards following a change to the name of the title sponsor Westpac Banking Corporation (previously known as the Bank of New South Wales). Other sponsors since the inception of the competition have been the Canberra Mathematical Association and the University of Canberra (previously known as the Canberra College of Advanced Education).\n\nThe competition has since spread to countries such as New Zealand, Singapore, Fiji, Taiwan and Malaysia, which submit thousands of entries each. A French translation of the paper has been available since the current competition was established in 1978, with Chinese translation being made available to Hong Kong (Traditional Chinese Characters) and Taiwan (Traditional Chinese Characters) students in 2000. Large print and braille versions are also available.\n\nIn 2004, the competition was expanded to allow two more divisions, one for year five and six students, and another for year three and four students.\n\nIn 2005, students from 38 different countries entered the competition.\n\nThe competition paper consists of twenty-five multiple-choice questions and five integer questions, which are ordered in increasing difficulty. Students record their personal details and mark their answers by pencil on a carbon-mark answer sheet, which is marked by computer. There are five divisions in total: Senior (for years 11 and 12), Intermediate (for years 9 and 10), Junior (for years 7 and 8), Upper Primary (for years 5 and 6) and Middle Primary (for years 3 and 4). \n\nStudents are allowed 75 minutes (60 minutes for the two primary papers) to read and answer the questions. Calculators are not permitted for secondary-level entrants, but geometrical aids such as rulers, compasses, protractors and paper for working are permitted. Primary-level entrants may use calculators and any aids normally found in a classroom.\n\nThe original points scheme, which was in operation from inception until 2001, consisted of three groups of ten questions. The first ten questions were worth three marks each, the next ten four marks each, and the last ten five marks each. Students were deducted a quarter of the marks for a given question if they answered incorrectly, so that a student randomly guessing the answers would gain no numerical benefit (on statistical average). Students started with 30 marks, so that a student who answered all questions incorrectly would record a total score of zero, while one who answered all questions correctly would record a score of 150.\n\nIn 2002, the format was changed so that no penalties were incurred for incorrect answers to the first twenty questions, and for each of the last ten questions, a correct answer gave eight marks, no answer gave three marks, and no marks were given for an incorrect answer; the total score remained the same at 150.\n\nIn 2005, the format was changed once more. This time the first ten questions are still worth three marks each and the next ten are still worth four marks each, however the last ten are now once again worth 5 marks each. To make it harder to guess the most difficult questions, the last 5 questions required integer answers between 0 and 999 inclusive. The total score possible was thus reduced to 120. \n\nIt has since been changed yet again. The first 25 questions have remained with the same mark allocation, however the last 5 questions have been altered. Although still requiring integer answers between 0 and 999, the mark allocation has been changed to 6 marks for Q26, 7 marks for Q27, 8 marks for Q28, 9 marks for Q29 and 10 marks for Q30, bringing the total marks to 135.\n\nThe competition is supervised by staff of the individual educational institutions, and the Australian Mathematics Trust reserves the right to conduct re-examinations in order to preserve the integrity of the competition, if it believes that students have not attempted the paper under sufficiently stringent conditions.\n\nThere is no official declared syllabus which determines the scope of the problems presented to the students. However, all problems can be solved without the use of calculus.\n\nDespite the name of the competition, students are allocated awards for their performance relative to other students in their region, of the same year level. For Australian students, this means their State or Territory, and for other students, their country. Although the personal data such as date of birth and gender are collected, this is not used in the percentile ranking, which is only determined by the raw score. The award scheme is as such\n\nStudents who have won a prize may also receive a medal if they are determined to have performed outstandingly well with respect to their region and the competition as a whole. All students receive a certificate, and prizewinners are awarded an additional monetary sum or book voucher. Students who achieve the maximum score are awarded the Bernhard Neumann certificate. From 2008, this award has been renamed the Peter O'Halloran Certificate in honour of the foundation Executive Director of the Trust. In 1998, a record 10 students in Australia, and 23 in Singapore achieved the maximum attainable score. A re-examination was carried out in order to determine the Singaporean medallists. \n\nAll students receive an analysis sheet along with their certificate, which records their answers for each question, along with the correct answers. The questions are divided into four categories: arithmetic, algebra, geometry and problem solving, and the number of questions that the student answered correctly for each category is listed along with the regional mean.\n\nEvery school receives a more comprehensive analysis, with a complete record of answers given by all students, as well as the percentage of students choosing any given answer for a given question, and a comparison to the percentage of students choosing any given answer for a given question in the whole region. Schools also receive analysis of their students by mathematical topic, compared to the entire region.\n\nThree students have won medals on all six of their opportunities to participate:\n\nShane Booth, Wanganui Park High School, Shepparton, Victoria was the first to win five consecutive medals (1981–1985).\n\nIvan Guo, Sydney Boys High School, New South Wales was the first person to win three consecutive BH Neumann certificates, which are only awarded to those who achieve a perfect score.\n\n"}
{"id": "17971241", "url": "https://en.wikipedia.org/wiki?curid=17971241", "title": "Barwise compactness theorem", "text": "Barwise compactness theorem\n\nIn mathematical logic, the Barwise compactness theorem, named after Jon Barwise, is a generalization of the usual compactness theorem for first-order logic to a certain class of infinitary languages. It was stated and proved by Barwise in 1967.\n\nLet formula_1 be a countable admissible set. Let formula_2 be an formula_1-finite relational language. Suppose formula_4 is a set of formula_5-sentences, where formula_4 is a formula_7 set with parameters from formula_1, and every formula_1-finite subset of formula_4 is satisfiable. Then formula_4 is satisfiable.\n\n\n"}
{"id": "31118503", "url": "https://en.wikipedia.org/wiki?curid=31118503", "title": "Beta negative binomial distribution", "text": "Beta negative binomial distribution\n\n} & \\text{if}\\ \\alpha>3 \\\\\n\nIn probability theory, a beta negative binomial distribution is the probability distribution of a discrete random variable \"X\" equal to the number of failures needed to get \"r\" successes in a sequence of independent Bernoulli trials where the probability \"p\" of success on each trial is constant within any given experiment but is itself a random variable following a beta distribution, varying between different experiments. Thus the distribution is a compound probability distribution.\n\nThis distribution has also been called both the inverse Markov-Pólya distribution and the generalized Waring distribution. A shifted form of the distribution has been called the beta-Pascal distribution.\n\nIf parameters of the beta distribution are \"α\" and \"β\", and if\nwhere\nthen the marginal distribution of \"X\" is a beta negative binomial distribution:\n\nIn the above, NB(\"r\", \"p\") is the negative binomial distribution and B(\"α\", \"β\") is the beta distribution.\n\nIf formula_5 is an integer, then the PMF can be written in terms of the beta function,:\nMore generally the PMF can be written \n\nUsing the properties of the Beta function, the PMF with integer formula_5 can be rewritten as:\n\nMore generally, the PMF can be written as\n\nThe PMF is often also presented in terms of the Pochammer symbol for integer formula_5\n\nThe beta negative binomial distribution contains the beta geometric distribution as a special case when formula_13. It can therefore approximate the geometric distribution arbitrarily well. It also approximates the negative binomial distribution arbitrary well for large formula_14 and formula_15. It can therefore approximate the Poisson distribution arbitrarily well for large formula_14, formula_15 and formula_5.\n\nBy Stirling's approximation to the beta function, it can be easily shown that\nwhich implies that the beta negative binomial distribution is heavy tailed.\n\n\n"}
{"id": "43350772", "url": "https://en.wikipedia.org/wiki?curid=43350772", "title": "Bioinformatics Open Source Conference", "text": "Bioinformatics Open Source Conference\n\nThe Bioinformatics Open Source Conference (BOSC) is an academic conference on open-source programming in bioinformatics organised by the Open Bioinformatics Foundation. The conference has been held annually since 2000 and is run as a two-day satellite meeting preceding the Intelligent Systems for Molecular Biology (ISMB) conference.\n\nThe conference is held as a single track consisting of presentations, poster sessions and two keynote talks by people of influence in open-source bioinformatics.\n\nSince 2010, an informal two-day Codefest has been held directly preceding the conference.\n\nNIH Associate Director for Data Science Philip Bourne and C. Titus Brown gave keynote talks at BOSC 2014.\n\nBOSC 2016 was organized in Orlando, Florida from July 8–9 before the main ISMB conference.\n"}
{"id": "46757894", "url": "https://en.wikipedia.org/wiki?curid=46757894", "title": "Björn Sandstede", "text": "Björn Sandstede\n\nBjörn Sandstede is a professor in the Division of Applied Mathematics at Brown University. Sandstede earned his Ph.D. in 1993 from the University of Stuttgart, under the supervision of Bernold Fiedler. In 2001 he was awarded the J.D. Crawford Prize of the Society for Industrial and Applied Mathematics for outstanding research in nonlinear science. In 2014 Sandstede was awarded the Jack K. Hale Award for his contributions to partial differential equations and the study of spiral waves in reaction diffusion systems.\n\n"}
{"id": "2572603", "url": "https://en.wikipedia.org/wiki?curid=2572603", "title": "Chaos game", "text": "Chaos game\n\nIn mathematics, the term chaos game originally referred to a method of creating a fractal, using a polygon and an initial point selected at random inside it. The fractal is created by iteratively creating a sequence of points, starting with the initial random point, in which each point in the sequence is a given fraction of the distance between the previous point and one of the vertices of the polygon; the vertex is chosen at random in each iteration. Repeating this iterative process a large number of times, selecting the vertex at random on each iteration, and throwing out the first few points in the sequence, will often (but not always) produce a fractal shape. Using a regular triangle and the factor 1/2 will result in the Sierpinski triangle, while creating the proper arrangement with four points and a factor 1/2 will create a display of a \"Sierpinski Tetrahedron\", the three-dimensional analogue of the Sierpinski triangle. As the number of points is increased to a number N, the arrangement forms a corresponding (N-1)-dimensional Sierpinski Simplex. An interactive version of the Chaos Game can be found here.\n\nThe term has been generalized to refer to a method of generating the attractor, or the fixed point, of any iterated function system (IFS). Starting with any point x, successive iterations are formed as x = f(x), where f is a member of the given IFS randomly selected for each iteration. The iterations converge to the fixed point of the IFS. Whenever x belongs to the attractor of the IFS, all iterations x stay inside the attractor and, with probability 1, form a dense set in the latter.\n\nThe \"chaos game\" method plots points in random order all over the attractor. This is in contrast to other methods of drawing fractals, which test each pixel on the screen to see whether it belongs to the fractal. The general shape of a fractal can be plotted quickly with the \"chaos game\" method, but it may be difficult to plot some areas of the fractal in detail.\n\nThe \"chaos game\" method is mentioned in Tom Stoppard's 1993 play Arcadia.\n\nWith the aid of the \"chaos game\" a new fractal can be made and while making the new fractal some parameters can be obtained. These parameters are useful for applications of fractal theory such as classification and identification. The new fractal is self-similar to the original in some important features such as fractal dimension.\n\nIf in the \"chaos game\" you start at each vertex and go through all possible paths that the game can take, you will get the same image as with only taking one random path. However, taking more than one path is rarely done since the overhead for keeping track of every path makes it far slower to calculate. This method does have the advantages of illustrating how the fractal is formed more clearly than the standard method as well as being deterministic.\n\nIf the chaos game is run with a square, no fractal appears and the interior of the square fills evenly with points. However, if restrictions are placed on the choice of vertices, fractals will appear in the square. For example, if the current vertex cannot be chosen in the next iteration, this fractal appears:\n\nIf the current vertex cannot be one place away (anti-clockwise) from the previously chosen vertex, this fractal appears:\n\nIf the point is prevented from landing on a particular region of the square, the shape of that region will be reproduced as a fractal in other and apparently unrestricted parts of the square. Here, for example, is the fractal produced when the point cannot jump so as to land on a red Om symbol at the center of the square:\n\n"}
{"id": "43200575", "url": "https://en.wikipedia.org/wiki?curid=43200575", "title": "Cheyette model", "text": "Cheyette model\n\nCheyette Model is a quasi-Gaussian quadratic volatility model of interest rates which is aiming to overcome certain limitations of the Heath-Jarrow-Morton framework.\n\n"}
{"id": "31261684", "url": "https://en.wikipedia.org/wiki?curid=31261684", "title": "Circuit quantum electrodynamics", "text": "Circuit quantum electrodynamics\n\nCircuit quantum electrodynamics (circuit QED) provides a means of studying the fundamental interaction between light and matter. As in the field of cavity quantum electrodynamics, a single photon within a single mode cavity coherently couples to a quantum object (atom). In contrast to cavity QED, the photon is stored in a one-dimensional on-chip resonator and the quantum object is no natural atom but an artificial one. These artificial atoms usually are mesoscopic devices which exhibit an atom-like energy spectrum.\nThe field of circuit QED is a prominent example for quantum information processing and a promising candidate for future quantum computation.\n\nThe resonant devices used for circuit QED are superconducting coplanar waveguide microwave resonators, which are two-dimensional microwave analogues of the Fabry–Pérot interferometer. Coplanar waveguides consist of a signal carrying centerline flanked by two grounded planes. This planar structure is put on a dielectric substrate by a photolithographic process. Superconducting materials used are mostly aluminium (Al) or niobium (Nb). Dielectrics typically used as substrates are either surface oxidized silicon (Si) or sapphire (AlO).\nThe line impedance is given by the geometric properties, which are chosen to match the 50 formula_1 of the peripheric microwave equipment to avoid partial reflection of the signal.\nThe electric field is basically confined between the center conductor and the ground planes resulting in a very small mode volume formula_2 which gives rise to very high electric fields per photon formula_3 (compared to three-dimensional cavities).\n\nformula_4\n\nOne can distinguish between two different types of resonators: formula_5 and formula_6 resonators. Half-wavelength resonators are made by breaking the center conductor at two spots with the distance formula_7. The resulting piece of center conductor is in this way capacitively coupled to the input and output and represents a resonator with formula_8-field antinodes at its ends. Quarter-wavelength resonators are short pieces of a coplanar line, which are shorted to ground on one end and capacitively coupled to a feed line on the other. The resonance frequencies are given by\n\nformula_9\n\nwith formula_10 being the effective dielectric permittivity of the device.\n\nThe first realized artificial atom in circuit QED was the so-called Cooper-pair box. In this device, a reservoir of Cooper-pairs is coupled via Josephson junctions to a gated superconducting island. The state of the Cooper-pair box (qubit) is given by the number of Cooper pairs on the island (formula_11 Cooper pairs for the ground state formula_12 and formula_13 for the excited state formula_14). By controlling the Coulomb energy (bias voltage) and the Josephson energy (flux bias) the transition frequency formula_15 is tuned. Due to the nonlinearity of the Josephson junctions the Cooper-pair box shows an atom like energy spectrum.\nOther more recent examples for qubits used in circuit QED are so called transmon qubits (more charge noise insensitive compared to the Cooper-pair box) and flux qubits (the state is given by the direction of a supercurrent in a superconducting loop intersected by Josephson junctions).\nAll of these devices feature very large dipole moments formula_16 (up to 10 that of large formula_17 Rydberg atoms), which qualifies them as extremely suitable coupling counterparts for the light field in circuit QED.\n\nThe full quantum description of matter-light interaction is given by the Jaynes-Cummings model. The three terms of the Jaynes-Cummings model can be ascribed to a cavity term, which is mimicked by a harmonic oscillator, an atomic term and an interaction term.\n\nformula_18\n\nIn this formulation formula_19 is the resonance frequency of the cavity and formula_20 and formula_21 are photon creation and annihilation operators, respectively. The atomic term is given by the Hamiltonian of a spin 1/2 system with formula_15 being the transition frequency and formula_23 the Pauli matrix. The operators formula_24 are raising and lowering operators (ladder operators) for the atomic states.\nFor the case of zero detuning (formula_25) the interaction lifts the degeneracy of the photon number state formula_26 and the atomic states formula_12 and formula_14 and pairs of dressed states are formed. These new states are superpositions of cavity and atom states\n\nformula_29\n\nand are energetically split by formula_30.\nIf the detuning is significantly larger than the combined cavity and atomic linewidth the cavity states are merely shifted by formula_31 (with the detuning formula_32) depending on the atomic state. This provides the possibility to read out the atomic (qubit) state by measuring the transition frequency.\n\nThe coupling is given by formula_33 (for electric dipolar coupling). If the coupling is much larger than the cavity loss rate formula_34 (quality factor formula_35; the higher formula_35, the longer the photon remains inside the resonator) as well as the decoherence rate formula_37 (rate at which the qubit relaxes into modes other than the resonator mode) the strong coupling regime is reached. Due to the high fields and low losses of the coplanar resonators together with the large dipole moments and long decoherence times of the qubits, the strong coupling regime can easily be reached in the field of circuit QED. Combination of the Jaynes–Cummings model and the coupled cavities leads to the Jaynes-Cummings-Hubbard model.\n"}
{"id": "442136", "url": "https://en.wikipedia.org/wiki?curid=442136", "title": "Computability", "text": "Computability\n\nComputability is the ability to solve a problem in an effective manner. It is a key topic of the field of computability theory within mathematical logic and the theory of computation within computer science. The computability of a problem is closely linked to the existence of an algorithm to solve the problem.\n\nThe most widely studied models of computability are the Turing-computable and μ-recursive functions, and the lambda calculus, all of which have computationally equivalent power. Other forms of computability are studied as well: computability notions weaker than Turing machines are studied in automata theory, while computability notions stronger than Turing machines are studied in the field of hypercomputation.\n\nA central idea in computability is that of a (computational) problem, which is a task whose computability can be explored.\n\nThere are two key types of problems:\nOther types of problems include search problems and optimization problems.\n\nOne goal of computability theory is to determine which problems, or classes of problems, can be solved in each model of computation.\n\nA model of computation is a formal description of a particular type of computational process. The description often takes the form of an abstract machine that is meant to perform the task at hand. General models of computation equivalent to a Turing machine (see Church–Turing thesis) include:\n\n\nIn addition to the general computational models, some simpler computational models are useful for special, restricted applications. Regular expressions, for example, specify string patterns in many contexts, from office productivity software to programming languages. Another formalism mathematically equivalent to regular expressions, Finite automata are used in circuit design and in some kinds of problem-solving. Context-free grammars specify programming language syntax. Non-deterministic pushdown automata are another formalism equivalent to context-free grammars.\n\nDifferent models of computation have the ability to do different tasks. One way to measure the power of a computational model is to study the class of formal languages that the model can generate; in such a way is the Chomsky hierarchy of languages is obtained.\n\nOther restricted models of computation include:\n\nWith these computational models in hand, we can determine what their limits are. That is, what classes of languages can they accept?\n\nComputer scientists call any language that can be accepted by a finite-state machine a regular language. Because of the restriction that the number of possible states in a finite state machine is finite, we can see that to find a language that is not regular, we must construct a language that would require an infinite number of states.\n\nAn example of such a language is the set of all strings consisting of the letters 'a' and 'b' which contain an equal number of the letter 'a' and 'b'. To see why this language cannot be correctly recognized by a finite state machine, assume first that such a machine \"M\" exists. \"M\" must have some number of states \"n\". Now consider the string \"x\" consisting of formula_3 'a's followed by formula_3 'b's.\n\nAs \"M\" reads in \"x\", there must be some state in the machine that is repeated as it reads in the first series of 'a's, since there are formula_3 'a's and only \"n\" states by the pigeonhole principle. Call this state \"S\", and further let \"d\" be the number of 'a's that our machine read in order to get from the first occurrence of \"S\" to some subsequent occurrence during the 'a' sequence. We know, then, that at that second occurrence of \"S\", we can add in an additional \"d\" (where formula_6) 'a's and we will be again at state \"S\". This means that we know that a string of formula_7 'a's must end up in the same state as the string of formula_3 'a's. This implies that if our machine accepts \"x\", it must also accept the string of formula_7 'a's followed by formula_3 'b's, which is not in the language of strings containing an equal number of 'a's and 'b's. In other words, \"M\" cannot correctly distinguish between a string of equal number of 'a's and 'b's and a string with formula_7 'a's and formula_12 'b's.\n\nWe know, therefore, that this language cannot be accepted correctly by any finite-state machine, and is thus not a regular language. A more general form of this result is called the Pumping lemma for regular languages, which can be used to show that broad classes of languages cannot be recognized by a finite state machine.\n\nComputer scientists define a language that can be accepted by a pushdown automaton as a Context-free language, which can be specified as a Context-free grammar. The language consisting of strings with equal numbers of 'a's and 'b's, which we showed was not a regular language, can be decided by a push-down automaton. Also, in general, a push-down automaton can behave just like a finite-state machine, so it can decide any language which is regular. This model of computation is thus strictly more powerful than finite state machines.\n\nHowever, it turns out there are languages that cannot be decided by push-down automaton either. The result is similar to that for regular expressions, and won't be detailed here. There exists a Pumping lemma for context-free languages. An example of such a language is the set of prime numbers.\n\nTuring machines can decide any context-free language, in addition to languages not decidable by a push-down automaton, such as the language consisting of prime numbers. It is therefore a strictly more powerful model of computation.\n\nBecause Turing machines have the ability to \"back up\" in their input tape, it is possible for a Turing machine to run for a long time in a way that is not possible with the other computation models previously described. It is possible to construct a Turing machine that will never finish running (halt) on some inputs. We say that a Turing machine can decide a language if it eventually will halt on all inputs and give an answer. A language that can be so decided is called a recursive language. We can further describe Turing machines that will eventually halt and give an answer for any input in a language, but which may run forever for input strings which are not in the language. Such Turing machines could tell us that a given string is in the language, but we may never be sure based on its behavior that a given string is not in a language, since it may run forever in such a case. A language which is accepted by such a Turing machine is called a recursively enumerable language.\n\nThe Turing machine, it turns out, is an exceedingly powerful model of automata. Attempts to amend the definition of a Turing machine to produce a more powerful machine have surprisingly met with failure. For example, adding an extra tape to the Turing machine, giving it a two-dimensional (or three- or any-dimensional) infinite surface to work with can all be simulated by a Turing machine with the basic one-dimensional tape. These models are thus not more powerful. In fact, a consequence of the Church–Turing thesis is that there is no reasonable model of computation which can decide languages that cannot be decided by a Turing machine.\n\nThe question to ask then is: do there exist languages which are recursively enumerable, but not recursive? And, furthermore, are there languages which are not even recursively enumerable?\n\nThe halting problem is one of the most famous problems in computer science, because it has profound implications on the theory of computability and on how we use computers in everyday practice. The problem can be phrased:\n\nHere we are asking not a simple question about a prime number or a palindrome, but we are instead turning the tables and asking a Turing machine to answer a question about another Turing machine. It can be shown (See main article: Halting problem) that it is not possible to construct a Turing machine that can answer this question in all cases.\n\nThat is, the only general way to know for sure if a given program will halt on a particular input in all cases is simply to run it and see if it halts. If it does halt, then you know it halts. If it doesn't halt, however, you may never know if it will eventually halt. The language consisting of all Turing machine descriptions paired with all possible input streams on which those Turing machines will eventually halt, is not recursive. The halting problem is therefore called non-computable or undecidable.\n\nAn extension of the halting problem is called Rice's theorem, which states that it is undecidable (in general) whether a given language possesses any specific nontrivial property.\n\nThe halting problem is easy to solve, however, if we allow that the Turing machine that decides it may run forever when given input which is a representation of a Turing machine that does not itself halt. The halting language is therefore recursively enumerable. It is possible to construct languages which are not even recursively enumerable, however.\n\nA simple example of such a language is the complement of the halting language; that is the language consisting of all Turing machines paired with input strings where the Turing machines do \"not\" halt on their input. To see that this language is not recursively enumerable, imagine that we construct a Turing machine \"M\" which is able to give a definite answer for all such Turing machines, but that it may run forever on any Turing machine that does eventually halt. We can then construct another Turing machine formula_13 that simulates the operation of this machine, along with simulating directly the execution of the machine given in the input as well, by interleaving the execution of the two programs. Since the direct simulation will eventually halt if the program it is simulating halts, and since by assumption the simulation of \"M\" will eventually halt if the input program would never halt, we know that formula_13 will eventually have one of its parallel versions halt. formula_13 is thus a decider for the halting problem. We have previously shown, however, that the halting problem is undecidable. We have a contradiction, and we have thus shown that our assumption that \"M\" exists is incorrect. The complement of the halting language is therefore not recursively enumerable.\n\nA number of computational models based on concurrency have been developed, including the parallel random-access machine and the Petri net. These models of concurrent computation still do not implement any mathematical functions that cannot be implemented by Turing machines.\n\nThe Church–Turing thesis conjectures that there is no effective model of computing that can compute more mathematical functions than a Turing machine. Computer scientists have imagined many varieties of hypercomputers, models of computation that go beyond Turing computability.\n\nImagine a machine where each step of the computation requires half the time of the previous step (and hopefully half the energy of the previous step...). If we normalize to 1/2 time unit the amount of time required for the first step (and to 1/2 energy unit the amount of energy required for the first step...), the execution would require\n\ntime unit (and 1 energy unit...) to run. This infinite series converges to 1, which means that this Zeno machine can execute a countably infinite number of steps in 1 time unit (using 1 energy unit...). This machine is capable of deciding the halting problem by directly simulating the execution of the machine in question. By extension, any convergent infinite [must be provably infinite] series would work. Assuming that the infinite series converges to a value \"n\", the Zeno machine would complete a countably infinite execution in \"n\" time units.\n\nSo-called Oracle machines have access to various \"oracles\" which provide the solution to specific undecidable problems. For example, the Turing machine may have a \"halting oracle\" which answers immediately whether a given Turing machine will ever halt on a given input. These machines are a central topic of study in recursion theory.\n\nEven these machines, which seemingly represent the limit of automata that we could imagine, run into their own limitations. While each of them can solve the halting problem for a Turing machine, they cannot solve their own version of the halting problem. For example, an Oracle machine cannot answer the question of whether a given Oracle machine will ever halt.\n\n\n"}
{"id": "52098442", "url": "https://en.wikipedia.org/wiki?curid=52098442", "title": "Critical mathematics pedagogy", "text": "Critical mathematics pedagogy\n\nCritical mathematics pedagogy is an approach to mathematics education that includes a practical and philosophical commitment to liberation. Approaches that involve critical mathematics pedagogy give special attention to the social, political, cultural and economic contexts of oppression, as they can be understood through mathematics. They also analyze the role that mathematics plays in producing and maintaining potentially oppressive social, political, cultural or economic structures. Finally, critical mathematics pedagogy demands that critique is connected to action promoting more just and equitable social, political or economic reform.\n\nCritical mathematics pedagogy builds on critical theory developed in the post-Marxist Frankfurt School, as well as critical pedagogy developed out of critical theory by Brazilian educator and educational theorist Paulo Freire. Definitions of critical mathematics pedagogy and critical mathematics education differ among those who practice it and write about it in their work. The focus of critical mathematics pedagogy shifts between three core tenets, but always includes some attention to all three: (1) analysis of injustice and inequitable relations of power made possible through mathematics, (2) critiques of the ways in which mathematics is used to structure and maintain power, and (3) critiques toward plans of action for change and the use of mathematics to reveal and oppose injustices, as well as imagine proposals for more equitable and just relations.\n\nThose who build their critical mathematics pedagogy with close relations to critical theory, focus on the analysis of mathematics as having \"formatting power\" that shapes the way we understand and organize the world. The assumption underlying critical mathematics pedagogy that comes from critical theory is the notion that mathematics is not neutral. According to critical mathematics, neither mathematics itself nor the teaching or learning of mathematics can be value-neutral, or free of interpretation. The critical mathematics group (est. 1990), one of the first groups of teachers and researchers to convene around the work of critical mathematics, state that mathematics is (1) knowledge constructed by humans, (2) the set of knowledges constructed by all groups of humans, not only the Eurocentric knowledge traditionally included in academic texts and (3) a human enterprise in which understanding results from action in social, cultural, political and economic context.\n\nMarilyn Frankenstein, the first educator to coin the term critical mathematics pedagogy in the United States in her 1983 article \"Critical Mathematics Pedagogy: An Application of Paulo Freire's Epistemology,\" illustrates one way in which mathematics is not neutral using the example of the world map. She explains that in order to represent a three-dimensional object on a two dimensional surface, such as is necessary when mapping the earth, map-makers must make decisions about which types of distortions to allow. For example, the most traditionally accepted and commonly used world map is the Mercator map which enlarges the size of Europe and shrinks the size of Africa. This representation can be read to suggest that certain parts of the world are larger, and therefore more important or more powerful than others via the (inaccurate) size comparison presented in the map. The decisions with regards to distortions are the result of political struggles and choices. Yet the map is most often read as a direct and neutral representation of reality.\n\nOle Skovsmose's first publication on critical mathematics pedagogy in Europe coincided with Marilyn Frankenstein's in the United States. It refers to \"mathemacy\" which would parallel critical literacy for mathematics. He explains that \"mathematics colonizes part of reality and reorders it.\" Therefore, \"the goal of mathematics education should be to understand the formatting power of mathematics and to empower people to examine this formatting power so they will not be controlled by it.\" According to him, mathemacy would consist of three components (1) mathematical knowing, or the skills developed in traditional mathematics classrooms, (2) technological knowing, or the ability to build models with mathematics and (3) reflective knowing, or competency in evaluating applications of mathematics. It is specifically the third component that makes this approach to mathematical literacy a critical one.\n\nThose who build their critical mathematics pedagogy out of critical pedagogy focus on empowerment of the learners as experts and actors for change in their own world. Critical mathematics pedagogy demands that students and teachers use mathematics to understand \"relations of power, resource inequalities between different social groups and explicit discrimination\" in order to take action for change. Paolo Freire (1921–1997), Brazilian educator and educational theorist, commonly regarded as the originator of critical pedagogy, suggests that most teaching happens in a \"banking\" model where teachers hold the information and students are assumed to be passive receptacles for that knowledge. Freire's alternative to the banking method is a \"problem-posing\" model of education. Through this model students and teachers participate together in a mutually humanizing process of dialogue. With the support of their teacher, students examine problems from their own lives and work collaboratively to generate solutions. One goal of critical pedagogy, according to Freire, is to develop critical consciousness or \"conscientização\" (Portuguese). Both teachers and students are expected to challenge their own \"well-established ways of thinking that frequently limit their own potential\" and that of others. They are especially expected to challenge those ways of thinking that might reproduce instead of challenge oppressive ways of thinking and being. This commitment to learning and critique for the purpose of action for change is also known as praxis, the intersection of theory and practice, another core tenet of the critical pedagogy of Paulo Freire.\n\nMarilyn Frankenstien argues that \"most current uses of mathematics support hegemonic ideologies.\" In particular, she focuses on the mathematical science of statistics which supports the unquestioned acceptance of uncertain conclusions. She argues that the use of the banking model in mathematics education (memorization and procedural focus) produces \"math anxiety\" in many people, especially and disproportionately those in non-dominant groups (women, people of color, lower income students). This math anxiety then leads people to \"not probe the mathematical mystifications\" that drive industrial society.\n\nEric (Rico) Gutstein applies Freire's notion of the inherent connection between \"reading the word and the world\" to mathematical literacy. He suggests that teaching mathematics for social justice involves both reading the world with mathematics, or more explicitly, \"using mathematics to understand relations of power, resource inequalities between different social groups and explicit discrimination,\" as well as writing the world with mathematics, or developing the tools of social agency in young people for acting in their own worlds. Mathematical literacy according to Gutstein must include both the capacity to \"read the mathematical world,\" necessary for traditional academic and economic success, as well as the capacity to \"read the world with mathematics,\" meaning the use of mathematics to understand and interrogate potentially problematic or unjust structures in their own lives.\n\nBecause critical mathematics pedagogy is designed to be responsive to the lives of the students in a given classroom and their local context, there is no set curriculum. Some educators re-use lessons or units from year to year that may apply to multiple groups of students, while other educators develop projects that respond directly to the concerns of a particular group of students, building a project together around a problem the students have posed. Precisely for this reason it is pertinent to consider a few examples of what critical mathematics pedagogy might look like in action.\n\nWilliam Tate, critical race theorist and promoter of culturally relevant teaching, describes the work of one teacher who brought together many of the core components of critical mathematics pedagogy. This teacher elicited concerns from her students about their own neighborhood and lives, and found out that one concern was the prevalence of liquor stores in the neighborhood. Students were being harassed on their way to and from school, having to step over or walk past drunk individuals, making them feel uncomfortable and unsafe. This teacher led her students through the process of in-depth research to better understand the distribution of liquor licenses and the reasons behind the concentration in their neighborhood. The class then met with local journalists to discuss the use of different types of graphic for representing statistics to the general public. The class then considered and determined which graphics and statistical representations (decimals, fractions, percents) might be the strongest for communicating their findings. Finally, the students used their research to produce a policy solution which they presented to the local community council. The work of this group of students and their teacher succeeded in leading to the closing of two of the nearby liquor stores in the neighborhood.\n\nOle Skovsmose describes a classroom in Denmark in which students learned about the use of algorithms for distribution of welfare support to families by attempting to create their own algorithms. The class worked in groups, where each group came up with a family profile to serve under the supervision of the instructor. Groups then were given a budget for welfare distributions to families and had to come up with how to distribute the money among all the families in their \"town\" made up of all the created family profiles. The task led them to develop ways of categorizing people in families by age, and families type, by income amount and type, by labor and employment, by possible productivity to society, and more. Some groups distributed the money without building a distribution algorithm, using trial and error and attempting to balance the distribution by more intuitive means. Others built algorithms, working backwards, attempting to break down the distribution using percentages. Many groups were surprised to find that their algorithms did not function comprehensively, and did not fully distribute the amount they were budgeted, and that the outcomes by group were vastly different. Perhaps more importantly, students gained an awareness of the choices and decision making that goes into how policies such as welfare for families are complex and human-created, not simply existing structures. This project is an example of the way in which critical mathematics pedagogy can reveal the role that humans play in mathematizing the world. It is different from Tate's example because it does not explicitly include an action component.\n\nFor a collection of sample lessons that address mathematics teaching through a critical lens see the book, \"Rethinking Mathematics: Teaching Social Justice by the Numbers\" (Eds. Gutstein and Peterson, 2005).\n\nOther work in the field of mathematics education that often overlaps at least in part with critical mathematics pedagogy includes the work of ethnomathematics, culturally relevant teaching in mathematics, and work for educational equity in mathematics.\n\nThe concept of ethnomathematics was introduced by D'Ambrosio in 1978, in response to the reliance on Eurocentric models for academic mathematics teaching to the exclusion of other cultural models. The goal of work in ethnomathematics is to de-center mathematics as a European dominated discipline by contributing research and teaching that highlights the contributions of many different cultures to mathematics as a discipline, and validating a wide range of mathematical practices. Ethnomathematics work notices, recognizes, reclaims, and celebrates the ways in which non-European communities and cultures are now and have throughout their histories been creating, using, and innovating with mathematics. It differs from critical mathematics pedagogy in that its focus is on cultural and social aspects of mathematics, where critical mathematics work also includes an explicit focus on politics and power structures. Though differences exist, those who work in either field oftentimes publish in similar publications and both consider their work mathematics for social justice.\n\nCulturally relevant teaching in mathematics was developed initially to support the success of African-American students, frequently poorly served by the American public school system which has a long history of educational inequality. The liquor store example provided above is shared by Tate as an example of culturally relevant teaching, but might likewise be seen to embody the tenets of critical pedagogy. He cites six core practices of the teacher from the example that make her work culturally relevant: (1) communication between students, teacher, and outside entities, (2) cooperative group work, (3) investigative research throughout the learning process, (4) questioning content, people, and institutions, (5) open-ended problem solving connected to student realities, and (6) social action. While the practices listed by Tate resonate profoundly with those of critical mathematics pedagogy, the difference (if there is any) is in the goals of the two approaches. The focus of culturally relevant teaching is on the empowerment and liberation of a cultural or racial group, whereas the goals of critical pedagogy include empowerment and liberation of individuals as well as groups, in the face of any form of oppression, not only cultural or racial oppression.\n\nThe notion of educational equity in mathematics education promotes the provision of high quality mathematics education to all groups and individuals in an attempt to narrow achievement gaps, for example gaps related to race and gender. This approach does not include a critical approach to mathematics itself, or the notion that mathematics education should include the learning of mathematics for the purpose of being able to analyze and change structures of power and injustice in the world. The National Council of Teachers of Mathematics, the world's largest mathematics education organization, has placed equity as one of its top priorities. However, critical mathematics educators suggest that the NCTM standards \"fail to define equity in applicable terms for classroom teachers, and it overemphasized the economic aspects of equity.\"\n\nLogistically, implementation of critical pedagogy is a challenge because there is and can be no \"how-to recipe.\" If the curriculum must be built out of students’ lives then it will necessarily change each year and with each group of students.\n\nCritiques are widespread, suggesting that mathematics is unbiased and not bound to culture, society or politics and therefore should not be wrongly politicized in the classroom. It is argued that this politicization is a distraction from achievement and risks holding students back, most specifically those it purports to support.\n\n"}
{"id": "4706825", "url": "https://en.wikipedia.org/wiki?curid=4706825", "title": "Cyclic code", "text": "Cyclic code\n\nIn coding theory, a cyclic code is a block code, where the circular shifts of each codeword gives another word that belongs to the code. They are error-correcting codes that have algebraic properties that are convenient for efficient error detection and correction.\nLet formula_1 be a linear code over a finite field (also called \" Galois field\") formula_2 of block length \"n\". formula_1 is called a cyclic code if, for every codeword \"c\"=(\"c\"...,\"c\") from \"C\", the word (\"c\",\"c\"...,\"c\") in formula_4 obtained by a cyclic right shift of components is again a codeword. Because one cyclic right shift is equal to \"n\" − 1 cyclic left shifts, a cyclic code may also be defined via cyclic left shifts. Therefore the linear code formula_1 is cyclic precisely when it is invariant under all cyclic shifts.\n\nCyclic Codes have some additional structural constraint on the codes. They are based on Galois fields and because of their structural properties they are very useful for error controls. Their structure is strongly related to Galois fields because of which the encoding and decoding algorithms for cyclic codes are computationally efficient.\n\nCyclic codes can be linked to ideals in certain rings. Let formula_6 be a polynomial ring over the finite field formula_7. Identify the elements of the cyclic code \"C\" with polynomials in \"R\" such that \nformula_8 maps to the polynomial \nformula_9: thus multiplication by \"x\" corresponds to a cyclic shift. Then \"C\" is an ideal in \"R\", and hence principal, since \"R\" is a principal ideal ring. The ideal is generated by the unique monic element in \"C\" of minimum degree, the \"generator polynomial\" \"g\".\nThis must be a divisor of formula_10. It follows that every cyclic code is a polynomial code.\nIf the generator polynomial \"g\" has degree \"d\" then the rank of the code \"C\" is formula_11.\n\nThe idempotent of \"C\" is a codeword \"e\" such that \"e\" = \"e\" (that is, \"e\" is an idempotent element of \"C\") and \"e\" is an identity for the code, that is \"e\" \"c\" = \"c\" for every codeword \"c\". If \"n\" and \"q\" are coprime such a word always exists and is unique; it is a generator of the code.\n\nAn irreducible code is a cyclic code in which the code, as an ideal is irreducible, i.e. is minimal in \"R\", so that its check polynomial is an irreducible polynomial.\n\nFor example, if \"A\"=formula_12 and \"n\"=3, the set of codewords contained in cyclic code generated by (1,1,0) is precisely\n\nIt corresponds to the ideal in formula_14 generated by formula_15.\n\nNote that formula_15 is an irreducible polynomial in the polynomial ring, and hence the code is an irreducible code.\n\nThe idempotent of this code is the polynomial formula_17, corresponding to the codeword (0,1,1).\n\nTrivial examples of cyclic codes are \"A\" itself and the code containing only the zero codeword. These correspond to generators 1 and formula_10 respectively: these two polynomials must always be factors of formula_10.\n\nOver GF(2) the parity bit code, consisting of all words of even weight, corresponds to generator formula_20. Again over GF(2) this must always be a factor of formula_10.\n\nBefore delving into the details of cyclic codes first we will discuss quasi-cyclic and shortened codes which are closely related to the cyclic codes and they all can be converted into each other.\n\nQuasi-cyclic codes:\n\nAn formula_22 \"quasi-cyclic code\" is a linear block code such that, for some formula_23 which is coprime to formula_24, the polynomial formula_25 is a \"codeword polynomial\" whenever formula_26 is a codeword polynomial.\n\nHere, \"codeword polynomial\" is an element of a linear code whose code words are polynomials that are divisible by a polynomial of shorter length called the \"generator polynomial\". Every codeword polynomial can be expressed in the form formula_27, where formula_28 is the generator polynomial. Any codeword formula_29 of a cyclic code formula_30 can be associated with a codeword polynomial, namely, formula_31. A quasi-cyclic code with formula_23 equal to formula_33 is a cyclic code.\n\nShortened codes: \n\nAn formula_34 linear code is called a \"proper shortened cyclic code\" if it can be obtained by deleting formula_23 positions from an formula_36 cyclic code.\n\nIn shortened codes information symbols are deleted to obtain a desired blocklength smaller than the design blocklength. The missing information symbols are usually imagined to be at the beginning of the codeword and are considered to be 0. Therefore, formula_24−formula_38 is fixed, and then formula_38 is decreased which eventually decreases formula_24. Note that it is not necessary to delete the starting symbols. Depending on the application sometimes consecutive positions are considered as 0 and are deleted.\n\nAll the symbols which are dropped need not be transmitted and at the receiving end can be reinserted. To convert formula_22 cyclic code to formula_42 shortened code, set formula_23 symbols to zero and drop them from each codeword. Any cyclic code can be converted to quasi-cyclic codes by dropping every formula_23th symbol where formula_23 is a factor of formula_24. If the dropped symbols are not check symbols then this cyclic code is also a shortened code.\n\nNow, we will begin the discussion of cyclic codes explicitly with error detection and correction. Cyclic codes can be used to correct errors, like Hamming codes as a cyclic codes can be used for correcting single error. Likewise, they are also used to correct double errors and burst errors. All types of error corrections are covered briefly in the further subsections.\n\nThe (7,4) Hamming code has a generator polynomial formula_47. This polynomial has a zero in Galois extension field formula_48 at the primitive element formula_49, and all codewords satisfy formula_50. Cyclic codes can also be used to correct double errors over the field formula_51. Blocklength will be formula_24 equal to formula_53 and primitive elements formula_49 and formula_55 as zeros in the formula_56 because we are considering the case of two errors here, so each will represent one error.\n\nThe received word is a polynomial of degree formula_57 given as\nformula_58\n\nwhere formula_59 can have at most two nonzero coefficients corresponding to 2 errors.\n\nWe define the Syndrome Polynomial, formula_60 as the remainder of polynomial formula_61 when divided by the generator polynomial formula_28 i.e. \n\nformula_63 as formula_64.\n\nLet the field elements formula_65 and formula_66 be the two error location numbers. If only one error occurs then formula_66 is equal to zero and if none occurs both are zero.\n\nLet formula_68 and formula_69.\n\nThese field elements are called \"syndromes\". Now because formula_28 is zero at primitive elements formula_49 and formula_55, so we can write formula_73 and formula_74. If say two errors occur, then\n\nformula_75 and \nformula_76.\n\nAnd these two can be considered as two pair of equations in formula_56 with two unknowns and hence we can write\n\nformula_78 and \nformula_79.\nHence if the two pair of nonlinear equations can be solved cyclic codes can used to correct two errors.\n\nThe Hamming(7,4) code may be written as a cyclic code over GF(2) with generator formula_80. In fact, any binary Hamming code of the form Ham(r, 2) is equivalent to a cyclic code, and any Hamming code of the form Ham(r,q) with r and q-1 relatively prime is also equivalent to a cyclic code. Given a Hamming code of the form Ham(r,2) with formula_81, the set of even codewords forms a cyclic formula_82-code.\n\nA code whose minimum distance is at least 3, have a check matrix all of whose columns are distinct and non zero. If a check matrix for a binary code has formula_83 rows, then each column is an formula_83-bit binary number. There are formula_85 possible columns. Therefore if a check matrix of a binary code with formula_86 at least 3 has formula_83 rows, then it can only have formula_85 columns, not more than that. This defines a formula_89 code, called Hamming code.\n\nIt is easy to define Hamming codes for large alphabets of size formula_90. We need to define one formula_91 matrix with linearly independent columns. For any word of size formula_90 there will be columns who are multiples of each other. So, to get linear independence all non zero formula_83-tuples with one as a top most non zero element will be chosen as columns. Then two columns will never be linearly dependent because three columns could be linearly dependent with the minimum distance of the code as 3.\n\nSo, there are formula_94 nonzero columns with one as top most non zero element. Therefore, Hamming code is a formula_95 code.\n\nNow, for cyclic codes, Let formula_49 be primitive element in formula_97, and let formula_98. Then formula_99 and thus formula_100 is a zero of the polynomial formula_101 and is a generator polynomial for the cyclic code of block length formula_102.\n\nBut for formula_103, formula_104. And the received word is a polynomial of degree formula_57 given as \n\nformula_58\n\nwhere, formula_107 or formula_108 where formula_109 represents the error locations.\n\nBut we can also use formula_110 as an element of formula_56 to index error location. Because formula_112, we have formula_113 and all powers of formula_49 from formula_115 to formula_116 are distinct. Therefore we can easily determine error location formula_109 from formula_110 unless formula_119 which represents no error. So, hamming code is a single error correcting code over formula_51 with formula_121 and formula_122.\n\nFrom Hamming distance concept, a code with minimum distance formula_123 can correct any formula_124 errors. But in many channels error pattern is not very arbitrary, it occurs within very short segment of the message. Such kind of errors are called burst errors. So, for correcting such errors we will get a more efficient code of higher rate because of the less constraints. Cyclic codes are used for correcting burst error. In fact, cyclic codes can also correct cyclic burst errors along with burst errors. Cyclic burst errors are defined as\n\nA cyclic burst of length formula_124 is a vector whose nonzero components are among formula_124 (cyclically) consecutive components, the first and the last of which are nonzero.\n\nIn polynomial form cyclic burst of length formula_124 can be described as formula_128 with formula_129 as a polynomial of degree formula_130 with nonzero coefficient formula_131. Here formula_129 defines the pattern and formula_133 defines the starting point of error. Length of the pattern is given by degformula_134. The syndrome polynomial is unique for each pattern and is given by \n\nformula_135\n\nA linear block code that corrects all burst errors of length formula_124 or less must have at least formula_137 check symbols. Proof: Because any linear code that can correct burst pattern of length formula_124 or less cannot have a burst of length formula_137 or less as a codeword because if it did then a burst of length formula_124 could change the codeword to burst pattern of length formula_124, which also could be obtained by making a burst error of length formula_124 in all zero codeword. Now, any two vectors that are non zero in the first formula_137 components must be from different co-sets of an array to avoid their difference being a codeword of bursts of length formula_137. Therefore number of such co-sets are equal to number of such vectors which are formula_145. Hence at least formula_145 co-sets and hence at least formula_137 check symbol.\n\nThis property is also known as Rieger bound and it is similar to the singleton bound for random error correcting.\n\nIn 1959, Philip Fire presented a construction of cyclic codes generated by a product of a binomial and a primitive polynomial. The binomial has the form formula_148 for some positive odd integer formula_149. \"Fire code\" is a cyclic burst error correcting code over formula_2 with the generator polynomial\n\nformula_151\n\nwhere formula_152 is a prime polynomial with degree formula_83 not smaller than formula_124 and formula_152 does not divide formula_156. Block length of the fire code is the smallest integer formula_24 such that formula_28 divides\nformula_10.\n\nA fire code can correct all burst errors of length t or less if no two bursts formula_129 and formula_161 appear in the same co-set. This can be proved by contradiction. Suppose there are two distinct nonzero bursts formula_129 and formula_161 of length formula_124 or less and are in the same co-set of the code. So, their difference is a codeword. As the difference is a multiple of formula_28 it is also a multiple of formula_156. Therefore,\n\nformula_167.\n\nThis shows that formula_168 is a multiple of formula_169, So\n\nformula_170 \n\nfor some formula_171. Now, as formula_172 is less than formula_124 and formula_171 is less than formula_175 so formula_176 is a codeword. Therefore, \n\nformula_177.\n\nSince formula_129 degree is less than degree of formula_152,formula_152 cannot divide formula_129. If formula_171 is not zero, then formula_152 also cannot divide formula_184 as formula_171 is less than formula_186 and by definition of formula_83, formula_152 divides formula_184 for no formula_171 smaller than formula_186. Therefore formula_171 and formula_168 equals to zero. That means both that both the bursts are same, contrary to assumption. \n\nFire codes are the best single burst correcting codes with high rate and they are constructed analytically. They are of very high rate and when formula_83 and formula_195 are equal, redundancy is least and is equal to formula_196. By using multiple fire codes longer burst errors can also be corrected.\n\nFor error detection cyclic codes are widely used and are called formula_130 cyclic redundancy codes.\n\nApplications of Fourier transform are widespread in signal processing. But their applications are not limited to the complex fields only; Fourier transforms also exist in the Galois field formula_2. Cyclic codes using Fourier transform can be described in a setting closer to the signal processing.\n\nFourier transform over finite fields\nThe discrete Fourier transform of vector formula_199 is given by a vector formula_200 where,\n\nformula_201 = formula_202 where,\n\nformula_203\n\nwhere exp(formula_204) is an formula_24th root of unity. Similarly in the finite field formula_24th root of unity is element formula_207 of order formula_24. Therefore\n\n\"If formula_209 is a vector over formula_2, and formula_207 be an element of formula_2 of order formula_24, then Fourier transform of the vector formula_214 is the vector formula_215 and components are given by\"\n\nformula_216 = formula_217 where,\n\nformula_203\n\nHere formula_109 is \"time\" index, formula_168 is \"frequency\" and formula_221 is the \"spectrum\". One important difference between Fourier transform in complex field and Galois field is that complex field formula_207 exists for every value of formula_24 while in Galois field formula_207 exists only if formula_24 divides formula_226. In case of extension fields, there will be a Fourier transform in the extension field formula_97 if formula_24 divides formula_175 for some formula_83. \nIn Galois field time domain vector formula_214 is over the field formula_2 but the spectrum formula_221 may be over the extension field formula_97.\n\nAny codeword of cyclic code of blocklength formula_24 can be represented by a polynomial formula_26 of degree at most formula_57. Its encoder can be written as formula_27. Therefore in frequency domain encoder can be written as formula_239. Here \"codeword spectrum\" formula_240 has a value in formula_97 but all the components in the time domain are from formula_2. As the data spectrum formula_243 is arbitrary, the role of formula_244 is to specify those formula_168 where formula_240 will be zero.\n\nThus, cyclic codes can also be defined as \n\n\"Given a set of spectral indices,\" formula_247, \" whose elements are called check frequencies, the cyclic code\" formula_30 \"is the set of words over\" formula_2 \"whose spectrum is zero in the components indexed by\" formula_250. \"Any such spectrum\" formula_30 \"will have components of the form\" formula_252.\n\nSo, cyclic codes are vectors in the field formula_2 and the spectrum given by its inverse fourier transform is over the field formula_97 and are constrained to be zero at certain components. But note that every spectrum in the field formula_97 and zero at certain components may not have inverse transforms with components in the field formula_2. Such spectrum can not be used as cyclic codes.\n\nFollowing are the few bounds on the spectrum of cyclic codes.\n\nIf formula_24 be a factor of formula_258 for some formula_83. The only vector in formula_4 of weight formula_261 or less that has formula_261 consecutive components of its spectrum equal to zero is all-zero vector.\n\nIf formula_24 be a factor of formula_258 for some formula_83, and formula_23 an integer that is coprime with formula_24. The only vector formula_214 in formula_4 of weight formula_261 or less whose spectral \ncomponents formula_216 equal zero for formula_272, where formula_273 and formula_274, is the all zero vector.\n\nIf formula_24 be a factor of formula_175 for some formula_83 and formula_278. The only vector in \nformula_4 of weight formula_261 or less whose spectral components formula_216 equal to zero for formula_282, where formula_283 and formula_284 takes at least formula_285 values in the range formula_286, is the all-zero vector.\n\nWhen the prime formula_171 is a quadratic residue modulo the prime formula_288 there is a quadratic residue code which is a cyclic code of length formula_288, dimension formula_290 and minimum weight at least formula_291 over formula_292.\n\nA constacyclic code is a linear code with the property that for some constant λ if (\"c\",c...,\"c\") is a codeword then so is (λ\"c\",c...,\"c\"). A negacyclic code is a constacyclic code with λ=-1. A quasi-cyclic code has the property that for some \"s\", any cyclic shift of a codeword by \"s\" places is again a codeword. A double circulant code is a quasi-cyclic code of even length with \"s\"=2.\n\n\n\n\n"}
{"id": "8625", "url": "https://en.wikipedia.org/wiki?curid=8625", "title": "Differential geometry", "text": "Differential geometry\n\nDifferential geometry is a mathematical discipline that uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra to study problems in geometry. The theory of plane and space curves and surfaces in the three-dimensional Euclidean space formed the basis for development of differential geometry during the 18th century and the 19th century.\n\nSince the late 19th century, differential geometry has grown into a field concerned more generally with the geometric structures on differentiable manifolds. Differential geometry is closely related to differential topology and the geometric aspects of the theory of differential equations. The differential geometry of surfaces captures many of the key ideas and techniques endemic to this field.\n\nDifferential geometry arose and developed as a result of and in connection to the mathematical analysis of curves and surfaces. Mathematical analysis of curves and surfaces had been developed to answer some of the nagging and unanswered questions that appeared in calculus, like the reasons for relationships between complex shapes and curves, series and analytic functions. These unanswered questions indicated greater, hidden relationships.\n\nThe general idea of natural equations for obtaining curves from local curvature appears to have been first considered by Leonhard Euler in 1736, and many examples with fairly simple behavior were studied in the 1800s.\n\nWhen curves, surfaces enclosed by curves, and points on curves were found to be quantitatively, and generally, related by mathematical forms, the formal study of the nature of curves and surfaces became a field of study in its own right, with Monge's paper in 1795, and especially, with Gauss's publication of his article, titled 'Disquisitiones Generales Circa Superficies Curvas', in \"Commentationes Societatis Regiae Scientiarum Gottingesis Recentiores\" in 1827.\n\nInitially applied to the Euclidean space, further explorations led to non-Euclidean space, and metric and topological spaces.\n\nRiemannian geometry studies Riemannian manifolds, smooth manifolds with a \"Riemannian metric\". This is a concept of distance expressed by means of a smooth positive definite symmetric bilinear form defined on the tangent space at each point. Riemannian geometry generalizes Euclidean geometry to spaces that are not necessarily flat, although they still resemble the Euclidean space at each point infinitesimally, i.e. in the first order of approximation. Various concepts based on length, such as the arc length of curves, area of plane regions, and volume of solids all possess natural analogues in Riemannian geometry. The notion of a directional derivative of a function from multivariable calculus is extended in Riemannian geometry to the notion of a covariant derivative of a tensor. Many concepts and techniques of analysis and differential equations have been generalized to the setting of Riemannian manifolds.\n\nA distance-preserving diffeomorphism between Riemannian manifolds is called an isometry. This notion can also be defined \"locally\", i.e. for small neighborhoods of points. Any two regular curves are locally isometric. However, the Theorema Egregium of Carl Friedrich Gauss showed that for surfaces, the existence of a local isometry imposes strong compatibility conditions on their metrics: the Gaussian curvatures at the corresponding points must be the same. In higher dimensions, the Riemann curvature tensor is an important pointwise invariant associated with a Riemannian manifold that measures how close it is to being flat. An important class of Riemannian manifolds is the Riemannian symmetric spaces, whose curvature is not necessarily constant. These are the closest analogues to the \"ordinary\" plane and space considered in Euclidean and non-Euclidean geometry.\n\nPseudo-Riemannian geometry generalizes Riemannian geometry to the case in which the metric tensor need not be positive-definite. \nA special case of this is a Lorentzian manifold, which is the mathematical basis of Einstein's general relativity theory of gravity.\n\nFinsler geometry has \"Finsler manifolds\" as the main object of study. This is a differential manifold with a \"Finsler metric\", that is, a Banach norm defined on each tangent space. Riemannian manifolds are special cases of the more general Finsler manifolds. A Finsler structure on a manifold is a function such that:\n\nSymplectic geometry is the study of symplectic manifolds. An almost symplectic manifold is a differentiable manifold equipped with a smoothly varying non-degenerate skew-symmetric bilinear form on each tangent space, i.e., a nondegenerate 2-form \"ω\", called the \"symplectic form\". A symplectic manifold is an almost symplectic manifold for which the symplectic form \"ω\" is closed: .\n\nA diffeomorphism between two symplectic manifolds which preserves the symplectic form is called a symplectomorphism. Non-degenerate skew-symmetric bilinear forms can only exist on even-dimensional vector spaces, so symplectic manifolds necessarily have even dimension. In dimension 2, a symplectic manifold is just a surface endowed with an area form and a symplectomorphism is an area-preserving diffeomorphism. The phase space of a mechanical system is a symplectic manifold and they made an implicit appearance already in the work of Joseph Louis Lagrange on analytical mechanics and later in Carl Gustav Jacobi's and William Rowan Hamilton's formulations of classical mechanics.\n\nBy contrast with Riemannian geometry, where the curvature provides a local invariant of Riemannian manifolds, Darboux's theorem states that all symplectic manifolds are locally isomorphic. The only invariants of a symplectic manifold are global in nature and topological aspects play a prominent role in symplectic geometry. The first result in symplectic topology is probably the Poincaré–Birkhoff theorem, conjectured by Henri Poincaré and then proved by G.D. Birkhoff in 1912. It claims that if an area preserving map of an annulus twists each boundary component in opposite directions, then the map has at least two fixed points.\n\nContact geometry deals with certain manifolds of odd dimension. It is close to symplectic geometry and like the latter, it originated in questions of classical mechanics. A \"contact structure\" on a -dimensional manifold \"M\" is given by a smooth hyperplane field \"H\" in the tangent bundle that is as far as possible from being associated with the level sets of a differentiable function on \"M\" (the technical term is \"completely nonintegrable tangent hyperplane distribution\"). Near each point \"p\", a hyperplane distribution is determined by a nowhere vanishing 1-form formula_1, which is unique up to multiplication by a nowhere vanishing function:\n\nA local 1-form on \"M\" is a \"contact form\" if the restriction of its exterior derivative to \"H\" is a non-degenerate two-form and thus induces a symplectic structure on \"H\" at each point. If the distribution \"H\" can be defined by a global one-form formula_1 then this form is contact if and only if the top-dimensional form\n\nis a volume form on \"M\", i.e. does not vanish anywhere. A contact analogue of the Darboux theorem holds: all contact structures on an odd-dimensional manifold are locally isomorphic and can be brought to a certain local normal form by a suitable choice of the coordinate system.\n\n\"Complex differential geometry\" is the study of complex manifolds.\nAn almost complex manifold is a \"real\" manifold formula_5, endowed with a tensor of type (1, 1), i.e. a vector bundle endomorphism (called an \"almost complex structure\")\n\nIt follows from this definition that an almost complex manifold is even-dimensional.\n\nAn almost complex manifold is called \"complex\" if formula_8, where formula_9 is a tensor of type (2, 1) related to formula_10, called the Nijenhuis tensor (or sometimes the \"torsion\").\nAn almost complex manifold is complex if and only if it admits a holomorphic coordinate atlas.\nAn \"almost Hermitian structure\" is given by an almost complex structure \"J\", along with a Riemannian metric \"g\", satisfying the compatibility condition\n\nAn almost Hermitian structure defines naturally a differential two-form\n\nThe following two conditions are equivalent:\n\n\nwhere formula_15 is the Levi-Civita connection of formula_16. In this case, formula_17 is called a \"Kähler structure\", and a \"Kähler manifold\" is a manifold endowed with a Kähler structure. In particular, a Kähler manifold is both a complex and a symplectic manifold. A large class of Kähler manifolds (the class of Hodge manifolds) is given by all the smooth complex projective varieties.\n\nCR geometry is the study of the intrinsic geometry of boundaries of domains in complex manifolds.\n\nDifferential topology is the study of global geometric invariants without a metric or symplectic form.\n\nDifferential topology starts from the natural operations such as Lie derivative of natural vector bundles and de Rham differential of forms. Beside Lie algebroids, also Courant algebroids start playing a more important role.\n\nA Lie group is a group in the category of smooth manifolds. Beside the algebraic properties this enjoys also differential geometric properties. The most obvious construction is that of a Lie algebra which is the tangent space at the unit endowed with the Lie bracket between left-invariant vector fields. Beside the structure theory there is also the wide field of representation theory.\n\nThe apparatus of vector bundles, principal bundles, and connections on bundles plays an extraordinarily important role in modern differential geometry. A smooth manifold always carries a natural vector bundle, the tangent bundle. Loosely speaking, this structure by itself is sufficient only for developing analysis on the manifold, while doing geometry requires, in addition, some way to relate the tangent spaces at different points, i.e. a notion of parallel transport. An important example is provided by affine connections. For a surface in R, tangent planes at different points can be identified using a natural path-wise parallelism induced by the ambient Euclidean space, which has a well-known standard definition of metric and parallelism. In Riemannian geometry, the Levi-Civita connection serves a similar purpose. (The Levi-Civita connection defines path-wise parallelism in terms of a given arbitrary Riemannian metric on a manifold.) More generally, differential geometers consider spaces with a vector bundle and an arbitrary affine connection which is not defined in terms of a metric. In physics, the manifold may be the space-time continuum and the bundles and connections are related to various physical fields.\n\nFrom the beginning and through the middle of the 18th century, differential geometry was studied from the \"extrinsic\" point of view: curves and surfaces were considered as lying in a Euclidean space of higher dimension (for example a surface in an ambient space of three dimensions). The simplest results are those in the differential geometry of curves and differential geometry of surfaces. Starting with the work of Riemann, the \"intrinsic\" point of view was developed, in which one cannot speak of moving \"outside\" the geometric object because it is considered to be given in a free-standing way. The fundamental result here is Gauss's theorema egregium, to the effect that Gaussian curvature is an intrinsic invariant.\n\nThe intrinsic point of view is more flexible. For example, it is useful in relativity where space-time cannot naturally be taken as extrinsic (what would be \"outside\" of it?). However, there is a price to pay in technical complexity: the intrinsic definitions of curvature and connections become much less visually intuitive.\n\nThese two points of view can be reconciled, i.e. the extrinsic geometry can be considered as a structure additional to the intrinsic one. (See the Nash embedding theorem.) In the formalism of geometric calculus both extrinsic and intrinsic geometry of a manifold can be characterized by a single bivector-valued one-form called the shape operator.\n\nBelow are some examples of how differential geometry is applied to other fields of science and mathematics.\n\n\n"}
{"id": "33522862", "url": "https://en.wikipedia.org/wiki?curid=33522862", "title": "Equational logic", "text": "Equational logic\n\nFirst-order equational logic consists of quantifier-free terms of ordinary first-order logic, with equality as the only predicate symbol. The model theory of this logic was developed into Universal algebra by Birkhoff, Grätzer and Cohn. It was later made into a branch of category theory by Lawvere (\"algebraic theories\").\nThe terms of equational logic are built up from variables and constants using function symbols (or operations).\n\nHere are the four inference rules of logic formula_1. formula_2 denotes textual substitution of expression formula_1 for variable formula_4 in expression formula_5. formula_6 denotes equality, for formula_7 and formula_8 of the same type, while formula_9, or equivalence, is defined only for formula_7 and formula_8 of type boolean. For formula_7 and formula_8 of type boolean, formula_6 and formula_9 have the same meaning.\n\nEquational logic was developed over the years (beginning in the early 1980s) by researchers in the formal development of programs, who felt a need for an effective style of manipulation, of calculation. Involved were people like Roland Carl Backhouse, Edsger W. Dijkstra, Wim H.J. Feijen, David Gries, Carel S. Scholten, and Netty van Gasteren. Wim Feijen is responsible for important details of the proof format.\n\nThe axioms are similar to those use by Dijkstra and Scholten in their monograph \"Predicate calculus and program semantics\" (Springer Verlag, 1990), but our order of presentation is slightly different.\n\nIn their monograph, Dijkstra and Scholten use the three inference rules Leibniz, Substitution, and Transitivity. However, Dijkstra/Scholten system is not a logic, as logicians use the word. Some of their manipulations are based on the meanings of the terms involved, and not on clearly presented syntactical rules of manipulation. The first attempt at making a real logic out of it appeared in \"A Logical Approach to Discrete Math\". However, inference rule Equanimity is missing there, and the definition of theorem is contorted to account for it. The introduction of Equanimity and its use in the proof format is due to Gries and Schneider. It is used, for example, in the proofs of soundness and completeness, and it will appear in the second edition of \"A Logical Approach to Discrete Math\".\n\nWe explain how the four inference rules are used in proofs, using the proof of formula_16. The logic symbols formula_17 and formula_18 indicate \"true\" and \"false,\" respectively, and formula_19 indicates \"not.\" The theorem numbers refer to theorems of \"A Logical Approach to Discrete Math\".\n\nformula_20\n\nFirst, lines formula_21–formula_22 show a use of inference rule Leibniz:\n\nformula_23\n\nis the conclusion of Leibniz, and its premise formula_24 is given on line formula_25. In the same way, the equality on lines formula_22–formula_27 are substantiated using Leibniz.\n\nThe \"hint\" on line formula_25 is supposed to give a premise of Leibniz, showing what substitution of equals for equals is being used. This premise is theorem formula_29 with the substitution formula_30, i.e.\n\nformula_31\n\nThis shows how inference rule Substitution is used within hints.\n\nFrom formula_32 and formula_33, we conclude by inference rule Transitivity that formula_34. This shows how Transitivity is used.\n\nFinally, note that line formula_27, formula_36, is a theorem, as indicated by the hint to its right. Hence, by inference rule Equanimity, we conclude that line formula_21 is also a theorem. And formula_21 is what we wanted to prove.\n\n"}
{"id": "1782993", "url": "https://en.wikipedia.org/wiki?curid=1782993", "title": "Fibrant object", "text": "Fibrant object\n\nIn mathematics, specifically in homotopy theory in the context of a model category \"M\", a fibrant object \"A\" of \"M\" is an object that has a fibration to the terminal object of the category.\n\nThe fibrant objects of a closed model category are characterized by having a right lifting property with respect to any trivial cofibration in the category. This property makes fibrant objects the \"correct\" objects on which to define homotopy groups. In the context of the theory of simplicial sets, the fibrant objects are known as Kan complexes after Daniel Kan. They are the Kan fibrations over a point.\n\nDually is the notion of cofibrant object, defined to be an object formula_1 such that the unique morphism formula_2 from the initial object to formula_1 is a cofibration.\n\n"}
{"id": "2435876", "url": "https://en.wikipedia.org/wiki?curid=2435876", "title": "Fictitious play", "text": "Fictitious play\n\nIn game theory, fictitious play is a learning rule first introduced by George W. Brown. In it, each player presumes that the opponents are playing stationary (possibly mixed) strategies. At each round, each player thus best responds to the empirical frequency of play of their opponent. Such a method is of course adequate if the opponent indeed uses a stationary strategy, while it is flawed if the opponent's strategy is non-stationary. The opponent's strategy may for example be conditioned on the fictitious player's last move.\n\nBrown first introduced fictitious play as an explanation for Nash equilibrium play. He imagined that a player would \"simulate\" play of the game in their mind and update their future play based on this simulation; hence the name \"fictitious\" play. In terms of current use, the name is a bit of a misnomer, since each play of the game actually occurs. The play is not exactly fictitious.\n\nIn fictitious play strict Nash equilibria are absorbing states. That is, if at any time period all the players play a Nash equilibrium, then they will do so for all subsequent rounds. (Fudenberg and Levine 1998, Proposition 2.1) In addition, if fictitious play converges to any distribution, those probabilities correspond to a Nash equilibrium of the underlying game. (Proposition 2.2)\nTherefore, the interesting question is, under what circumstances does fictitious play converge? The process will converge for a 2-person game if:\n\nFictitious play does not always converge, however. Shapley (1964) proved that in the game pictured here (a nonzero-sum version of Rock, Paper, Scissors), if the players start by choosing \"(a, B)\", the play will cycle indefinitely.\n\nBerger (2007) states that \"what modern game theorists describe as 'fictitious play' is not the learning process that George W. Brown defined in his 1951 paper\": Brown's \"original version differs in a subtle detail...\" in that modern usage involves the players updating their beliefs \"simultaneously\", whereas Brown described the players updating \"alternatingly\". Berger then uses Brown's original form to present a simple and intuitive proof of convergence in the case of two-player nondegenerate ordinal potential games.\n\nThe term \"fictitious\" had earlier been given another meaning in game theory. Von Neumann and Morgenstern [1944] defined a \"fictitious player\" as a player with only one strategy, added to an \"n\"-player game to turn it into a (\"n\" + 1)-player zero-sum game.\n\n\n"}
{"id": "9352497", "url": "https://en.wikipedia.org/wiki?curid=9352497", "title": "Flag (geometry)", "text": "Flag (geometry)\n\nIn (polyhedral) geometry, a flag is a sequence of faces of a polytope, each contained in the next, with exactly one face from each dimension. \n\nMore formally, a flag ψ of an \"n\"-polytope is a set {\"F\", \"F\", ..., \"F\"} such that \"F\" ≤ \"F\" (−1 ≤ \"i\" ≤ \"n\" − 1) and there is precisely one \"F\" in \"ψ\" for each \"i\", (−1 ≤ \"i\" ≤ \"n\"). Since, however, the minimal face \"F\" and the maximal face \"F\" must be in every flag, they are often omitted from the list of faces, as a shorthand. These latter two are called improper faces.\n\nFor example, a flag of a polyhedron comprises one vertex, one edge incident to that vertex, and one polygonal face incident to both, plus the two improper faces.\n\nA polytope may be regarded as regular if, and only if, its symmetry group is transitive on its flags. This definition excludes chiral polytopes.\n\nIn the more abstract setting of incidence geometry, which is a set having a symmetric and reflexive relation called \"incidence\" defined on its elements, a flag is a set of elements that are mutually incident. This level of abstraction generalizes both the polyhedral concept given above as well as the related flag concept from linear algebra.\n\nA flag is \"maximal\" if it is not contained in a larger flag. When all maximal flags of an incidence geometry have the same size, this common value is the \"rank\" of the geometry.\n\n"}
{"id": "26271144", "url": "https://en.wikipedia.org/wiki?curid=26271144", "title": "Functional renormalization group", "text": "Functional renormalization group\n\nIn theoretical physics, functional renormalization group (FRG) is an implementation of the renormalization group (RG) concept which is used in quantum and statistical field theory, especially when dealing with strongly interacting systems. The method combines functional methods of quantum field theory with the intuitive renormalization group idea of Kenneth G. Wilson. This technique allows to interpolate smoothly between the known microscopic laws and the complicated macroscopic phenomena in physical systems. In this sense, it bridges the transition from simplicity of microphysics to complexity of macrophysics. Figuratively speaking, FRG acts as a microscope with a variable resolution. One starts with a high-resolution picture of the known microphysical laws and subsequently decreases the resolution to obtain a coarse-grained picture of macroscopic collective phenomena. The method is nonperturbative, meaning that it does not rely on an expansion in a small coupling constant. Mathematically, FRG is based on an exact functional differential equation for a scale-dependent effective action.\n\nIn quantum field theory, the effective action formula_1 is an analogue of the classical action functional formula_2 and depends on the fields of a given theory. It includes all quantum and thermal fluctuations. Variation of formula_1 yields exact quantum field equations, for example for cosmology or the electrodynamics of superconductors. Mathematically, formula_1 is the generating functional of the one-particle irreducible Feynman diagrams. Interesting physics, as propagators and effective couplings for interactions, can be straightforwardly extracted from it. In a generic interacting field theory the effective action formula_1, however, is difficult to obtain. FRG provides a practical tool to calculate formula_1 employing the renormalization group concept.\n\nThe central object in FRG is a scale-dependent effective action functional formula_7 often called average action or flowing action. The dependence on the RG sliding scale formula_8 is introduced by adding a regulator (infrared cutoff) formula_9 to the full inverse propagator formula_10. Roughly speaking, the regulator formula_11 decouples slow modes with momenta formula_12 by giving them a large mass, while high momentum modes are not affected. Thus, formula_7 includes all quantum and statistical fluctuations with momenta formula_14. The flowing action formula_15 obeys the exact functional flow equation\n\nformula_16\n\nderived by Christof Wetterich and Tim R. Morris in 1993. Here formula_17 denotes a derivative with respect to the RG scale formula_8 at fixed values of the fields.\nThe functional differential equation for formula_7 must be supplemented with the initial condition formula_20, where the \"classical action\" formula_2 describes the physics at the microscopic ultraviolet scale formula_22. Importantly, in the infrared limit formula_23 the full effective action formula_24 is obtained. In the Wetterich equation formula_25 denotes a supertrace which sums over momenta, frequencies, internal indices, and fields (taking bosons with a plus and fermions with a minus sign). The exact flow equation for formula_15 has a one-loop structure. This is an important simplification compared to perturbation theory, where multi-loop diagrams must be included. The second functional derivative formula_10 is the full inverse field propagator modified by the presence of the regulator formula_11.\n\nThe renormalization group evolution of formula_15 can be illustrated in the theory space, which is a multi-dimensional space of all possible running couplings formula_30 allowed by the symmetries of the problem. As schematically shown in the figure, at the microscopic ultraviolet scale formula_22 one starts with the initial condition formula_32.\n\nAs the sliding scale formula_8 is lowered, the flowing action formula_15 evolves in the theory space according to the functional flow equation. The choice of the regulator formula_11 is not unique, which introduces some scheme dependence into the renormalization group flow. For this reason, different choices of the regulator formula_11 correspond to the different paths in the figure. At the infrared scale formula_37, however, the full effective action formula_38 is recovered for every choice of the cut-off formula_11, and all trajectories meet at the same point in the theory space.\n\nIn most cases of interest the Wetterich equation can only be solved approximately. Usually some type of expansion of formula_7 is performed, which is then truncated at finite order leading to a finite system of ordinary differential equations. Different systematic expansion schemes (such as the derivative expansion, vertex expansion, etc.) were developed. The choice of the suitable scheme should be physically motivated and depends on a given problem. The expansions do not necessarily involve a small parameter (like an interaction coupling constant) and thus they are, in general, of nonperturbative nature.\n\n\nContrary to the flow equation for the effective action, this scheme is formulated for the effective interaction\n\nformula_47\n\nwhich generates n-particle interaction vertices, amputated by the bare propagators formula_48;\nformula_49 is the \"standard\" generating functional for the n-particle Green functions.\n\nThe Wick ordering of effective interaction with respect to Green function formula_50 can be defined by\n\nformula_51.\n\nwhere formula_52 is the Laplacian in the field space. This operation is similar to Normal order and excludes from the interaction all possible terms, formed by a convolution of source fields with respective Green function D. Introducing some cutoff formula_44 the Polchinskii equation\n\nformula_54\n\nwhere\n\n<math>\\Delta _^{12}\\mathcal {V}_\\Lambda ^{(1)}\\mathcal {V}_\\Lambda ^{(2)}=\\frac{1}{2}\\left( {\\frac\n"}
{"id": "34728523", "url": "https://en.wikipedia.org/wiki?curid=34728523", "title": "Hans Wussing", "text": "Hans Wussing\n\nHans-Ludwig Wußing (October 15, 1927 in Waldheim – April 26, 2011 in Leipzig) was a German historian of mathematics and science.\n\nWussing graduated from high school, and from 1947 to 52 studied mathematics and physics at the University of Leipzig. Ernst Hölder was one of his teachers. In 1952 he took the state examination, and received his doctorate in 1957. His dissertation was on embedding finite groups. From 1956 to 1966 he was assistant at the Karl-Sudhoff Institute for the History of Medicine and Science at the University of Leipzig. He qualified as a professor there in 1966 with a ground-breaking work on the genesis of the abstract group concept. From 1966 to 1968 was Wußing lecturer, and from 1968 professor of history of mathematics and natural sciences. In 1969 his book \"Genesis of the Abstract Group Concept\" was published in German; it was translated by Abe Shenitzer and Hardy Grant in 1984. B.H. Newman wrote in Mathematical Reviews (see external link below) that Wussing's \"main thesis, ably defended and well documented, is that the roots of the abstract notion of a group do not lie, as frequently assumed, only in the theory of algebraic equations, but they are also to be found in the geometry and in the theory of numbers at the end of the 18th and the first half of the 19th centuries\". Newman comments that Wussings bibliography is \"oddly arranged\". Newman also notes that a broader perspective on the topic would require reading the works of George Abram Miller.\n\nPromoted from a department head at the Karl-Sudhoff Institute, he headed the institute from 1977 to 1982. In 1971 he became a corresponding member of the International Academy of the History of Science, and a regular member in 1981. In 1984 he became a full member of the Saxon Academy of Sciences in Leipzig. Wussing retired in 1992.\n\nWussing is the author of numerous scientific historical publications, the author of many mathematicians' biographies, and co-editor of several series of publications, including biographies in the Teubner Verlag, and several volumes in the series \"Klassiker der Exakten Wissenschaften\" (Ostwald's Classics of the Exact Sciences), in particular on Euler's work on functional theory, Gauss' diary, and Felix Klein's Erlangen program.\nIn 1993 he was awarded the Kenneth O. May Prize. Until 1998 he was Chairman of the Commission for the History of Science at the Saxon Academy of Sciences. He was also involved in the publication of Johann Christian Poggendorff's \"Biographical and Literary Pocket Dictionary of the History of Exact Sciences.\n\n\n"}
{"id": "13263", "url": "https://en.wikipedia.org/wiki?curid=13263", "title": "Hexadecimal", "text": "Hexadecimal\n\nIn mathematics and computing, hexadecimal (also base , or hex) is a positional numeral system with a radix, or base, of 16. It uses sixteen distinct symbols, most often the symbols 0–9 to represent values zero to nine, and A–F (or alternatively a–f) to represent values ten to fifteen.\n\nHexadecimal numerals are widely used by computer system designers and programmers, as they provide a more human-friendly representation of binary-coded values. Each hexadecimal digit represents four binary digits, also known as a nibble, which is half a byte. For example, a single byte can have values ranging from 0000 0000 to 1111 1111 in binary form, which can be more conveniently represented as 00 to FF in hexadecimal.\n\nIn mathematics, a subscript is typically used to specify the radix. For example the decimal value would be expressed in hexadecimal as . In programming, a number of notations are used to support hexadecimal representation, usually involving a prefix or suffix. The prefix codice_1 is used in C and related languages, which would denote this value by 0x.\n\nHexadecimal is used in the transfer encoding Base16, in which each byte of the plaintext is broken into two 4-bit values and represented by two hexadecimal digits.\n\nIn contexts where the base is not clear, hexadecimal numbers can be ambiguous and confused with numbers expressed in other bases. There are several conventions for expressing values unambiguously. A numerical subscript (itself written in decimal) can give the base explicitly: 159 is decimal 159; 159 is hexadecimal 159, which is equal to 345. Some authors prefer a text subscript, such as 159 and 159, or 159 and 159.\n\nIn linear text systems, such as those used in most computer programming environments, a variety of methods have arisen:\n\nThere is no universal convention to use lowercase or uppercase for the letter digits, and each is prevalent or preferred in particular environments by community standards or convention.\n\nThe use of the letters \"A\" through \"F\" to represent the digits above 9 was not universal in the early history of computers.\n\nThere are no traditional numerals to represent the quantities from ten to fifteen – letters are used as a substitute – and most European languages lack non-decimal names for the numerals above ten. Even though English has names for several non-decimal powers (\"pair\" for the first binary power, \"score\" for the first vigesimal power, \"dozen\", \"gross\" and \"great gross\" for the first three duodecimal powers), no English name describes the hexadecimal powers (decimal 16, 256, 4096, 65536, ... ). Some people read hexadecimal numbers digit by digit like a phone number, or using the NATO phonetic alphabet, the Joint Army/Navy Phonetic Alphabet, or a similar ad hoc system.\nSystems of counting on digits have been devised for both binary and hexadecimal.\nArthur C. Clarke suggested using each finger as an on/off bit, allowing finger counting from zero to 1023 on ten fingers. Another system for counting up to FF (255) is illustrated on the right.\n\nThe hexadecimal system can express negative numbers the same way as in decimal: −2A to represent −42 and so on.\n\nHexadecimal can also be used to express the exact bit patterns used in the processor, so a sequence of hexadecimal digits may represent a signed or even a floating point value. This way, the negative number −42 can be written as FFFF FFD6 in a 32-bit CPU register (in two's-complement), as C228 0000 in a 32-bit FPU register or C045 0000 0000 0000 in a 64-bit FPU register (in the IEEE floating-point standard).\n\nJust as decimal numbers can be represented in exponential notation, so too can hexadecimal numbers. By convention, the letter \"P\" (or \"p\", for \"power\") represents \"times two raised to the power of\", whereas \"E\" (or \"e\") serves a similar purpose in decimal as part of the E notation. The number after the \"P\" is \"decimal\" and represents the \"binary\" exponent.\n\nUsually the number is normalised so that the leading hexadecimal digit is 1 (unless the value is exactly 0).\n\nExample: 1.3DEp42 represents .\n\nHexadecimal exponential notation is required by the IEEE 754-2008 binary floating-point standard.\nThis notation can be used for floating-point literals in the C99 edition of the C programming language.\nUsing the \"%a\" or \"%A\" conversion specifiers, this notation can be produced by implementations of the \"printf\" family of functions following the C99 specification and\nSingle Unix Specification (IEEE Std 1003.1) POSIX standard.\n\nMost computers manipulate binary data, but it is difficult for humans to work with the large number of digits for even a relatively small binary number. Although most humans are familiar with the base 10 system, it is much easier to map binary to hexadecimal than to decimal because each hexadecimal digit maps to a whole number of bits (4).\nThis example converts 1111 to base ten. Since each position in a binary numeral can contain either a 1 or a 0, its value may be easily determined by its position from the right:\nTherefore:\n\nWith little practice, mapping 1111 to F in one step becomes easy: see table in Written representation. The advantage of using hexadecimal rather than decimal increases rapidly with the size of the number. When the number becomes large, conversion to decimal is very tedious. However, when mapping to hexadecimal, it is trivial to regard the binary string as 4-digit groups and map each to a single hexadecimal digit.\n\nThis example shows the conversion of a binary number to decimal, mapping each digit to the decimal value, and adding the results.\n\nCompare this to the conversion to hexadecimal, where each group of four digits can be considered independently, and converted directly:\n\nThe conversion from hexadecimal to binary is equally direct.\n\nAlthough quaternary (base 4) is little used, it can easily be converted to and from hexadecimal or binary. Each hexadecimal digit corresponds to a pair of quaternary digits and each quaternary digit corresponds to a pair of binary digits. In the above example 5 E B 5 2 = 11 32 23 11 02.\n\nThe octal (base 8) system can also be converted with relative ease, although not quite as trivially as with bases 2 and 4. Each octal digit corresponds to three binary digits, rather than four. Therefore we can convert between octal and hexadecimal via an intermediate conversion to binary followed by regrouping the binary digits in groups of either three or four.\n\nAs with all bases there is a simple algorithm for converting a representation of a number to hexadecimal by doing integer division and remainder operations in the source base. In theory, this is possible from any base, but for most humans only decimal and for most computers only binary (which can be converted by far more efficient methods) can be easily handled with this method.\n\nLet d be the number to represent in hexadecimal, and the series hh...hh be the hexadecimal digits representing the number.\n\n\n\"16\" may be replaced with any other base that may be desired.\n\nThe following is a JavaScript implementation of the above algorithm for converting any number to a hexadecimal in String representation. Its purpose is to illustrate the above algorithm. To work with data seriously, however, it is much more advisable to work with bitwise operators.\n\nIt is also possible to make the conversion by assigning each place in the source base the hexadecimal representation of its place value and then performing multiplication and addition to get the final representation.\nThat is, to convert the number B3AD to decimal one can split the hexadecimal number into its digits: B (11), 3 (3), A (10) and D (13), and then get the final result by multiplying each decimal representation by 16, where \"p\" is the corresponding hex digit position, counting from right to left, beginning with 0. In this case we have , which is 45997 base 10.\n\nMost modern computer systems with graphical user interfaces provide a built-in calculator utility, capable of performing conversions between various radices, in general including hexadecimal.\n\nIn Microsoft Windows, the Calculator utility can be set to Scientific mode (called Programmer mode in some versions), which allows conversions between radix 16 (hexadecimal), 10 (decimal), 8 (octal) and 2 (binary), the bases most commonly used by programmers. In Scientific Mode, the on-screen numeric keypad includes the hexadecimal digits A through F, which are active when \"Hex\" is selected. In hex mode, however, the Windows Calculator supports only integers.\n\nAs with other numeral systems, the hexadecimal system can be used to represent rational numbers, although repeating expansions are common since sixteen (10) has only a single prime factor (two):\n\nwhere an overline denotes a recurring pattern.\n\nFor any base, 0.1 (or \"1/10\") is always equivalent to one divided by the representation of that base value in its own number system. Thus, whether dividing one by two for binary or dividing one by sixteen for hexadecimal, both of these fractions are written as codice_48. Because the radix 16 is a perfect square (4), fractions expressed in hexadecimal have an odd period much more often than decimal ones, and there are no cyclic numbers (other than trivial single digits). Recurring digits are exhibited when the denominator in lowest terms has a prime factor not found in the radix; thus, when using hexadecimal notation, all fractions with denominators that are not a power of two result in an infinite string of recurring digits (such as thirds and fifths). This makes hexadecimal (and binary) less convenient than decimal for representing rational numbers since a larger proportion lie outside its range of finite representation.\n\nAll rational numbers finitely representable in hexadecimal are also finitely representable in decimal, duodecimal and sexagesimal: that is, any hexadecimal number with a finite number of digits has a finite number of digits when expressed in those other bases. Conversely, only a fraction of those finitely representable in the latter bases are finitely representable in hexadecimal. For example, decimal 0.1 corresponds to the infinite recurring representation 0.199999999999... in hexadecimal. However, hexadecimal is more efficient than bases 12 and 60 for representing fractions with powers of two in the denominator (e.g., decimal one sixteenth is 0.1 in hexadecimal, 0.09 in duodecimal, 0;3,45 in sexagesimal and 0.0625 in decimal).\n\nThe table below gives the expansions of some common irrational numbers in decimal and hexadecimal.\nPowers of two have very simple expansions in hexadecimal. The first sixteen powers of two are shown below.\n\nThe word \"hexadecimal\" is composed of \"hexa-\", derived from the Greek ἕξ (hex) for \"six\", and \"-decimal\", derived from the Latin for \"tenth\". Webster's Third New International online derives \"hexadecimal\" as an alteration of the all-Latin \"sexadecimal\" (which appears in the earlier Bendix documentation). The earliest date attested for \"hexadecimal\" in Merriam-Webster Collegiate online is 1954, placing it safely in the category of international scientific vocabulary (ISV). It is common in ISV to mix Greek and Latin combining forms freely. The word \"sexagesimal\" (for base 60) retains the Latin prefix. Donald Knuth has pointed out that the etymologically correct term is \"senidenary\" (or possibly, \"sedenary\"), from the Latin term for \"grouped by 16\". (The terms \"binary\", \"ternary\" and \"quaternary\" are from the same Latin construction, and the etymologically correct terms for \"decimal\" and \"octal\" arithmetic are \"denary\" and \"octonary\", respectively.) Alfred B. Taylor used \"senidenary\" in his mid-1800s work on alternative number bases, although he rejected base 16 because of its \"incommodious number of digits\". Schwartzman notes that the expected form from usual Latin phrasing would be \"sexadecimal\", but computer hackers would be tempted to shorten that word to \"sex\". The etymologically proper Greek term would be \"hexadecadic\" / \"ἑξαδεκαδικός\" / \"hexadekadikós\" (although in Modern Greek, \"decahexadic\" / \"δεκαεξαδικός\" / \"dekaexadikos\" is more commonly used).\n\nIn hexadecimal, numbers with nondecreasing digits are called plaindrones, those with nonincreasing digits are called nialpdromes, those with descending digits are called katadromes, and those with ascending digits are called metadromes.\n\nThe traditional Chinese units of measurement were base-16. For example, one jīn (斤) in the old system equals sixteen taels. The suanpan (Chinese abacus) can be used to perform hexadecimal calculations.\n\nAs with the duodecimal system, there have been occasional attempts to promote hexadecimal as the preferred numeral system. These attempts often propose specific pronunciation and symbols for the individual numerals. Some proposals unify standard measures so that they are multiples of 16.\n\nAn example of unified standard measures is hexadecimal time, which subdivides a day by 16 so that there are 16 \"hexhours\" in a day.\n\nBase16 (as a proper name without a space) can also refer to a binary to text encoding belonging to the same family as Base32, Base58, and Base64.\n\nIn this case, data is broken into 4-bit sequences, and each value (between 0 and 15 inclusively) is encoded using 16 symbols from the ASCII character set. Although any 16 symbols from the ASCII character set can be used, in practice the ASCII digits '0'-'9' and the letters 'A'-'F' (or the lowercase 'a'-'f') are always chosen in order to align with standard written notation for hexadecimal numbers.\n\nThere are several advantages of Base16 encoding:\n\nThe main disadvantages of Base16 encoding are:\n\nSupport for Base16 encoding is ubiquitous in modern computing. It is the basis for the W3C standard for URL Percent Encoding, where a character is replaced with a percent sign \"%\" and its Base16-encoded form. Most modern programming languages directly include support for formatting and parsing Base16-encoded numbers.\n\n"}
{"id": "2336224", "url": "https://en.wikipedia.org/wiki?curid=2336224", "title": "Hilbert's eighteenth problem", "text": "Hilbert's eighteenth problem\n\nHilbert's eighteenth problem is one of the 23 Hilbert problems set out in a celebrated list compiled in 1900 by mathematician David Hilbert. It asks three separate questions about lattices and sphere packing in Euclidean space.\n\nThe first part of the problem asks whether there are only finitely many essentially different space groups in formula_1-dimensional Euclidean space. This was answered affirmatively by Bieberbach.\n\nThe second part of the problem asks whether there exists a polyhedron which tiles 3-dimensional Euclidean space but is not the fundamental region of any space group; that is, which tiles but does not admit an isohedral (tile-transitive) tiling. Such tiles are now known as anisohedral. In asking the problem in three dimensions, Hilbert was probably assuming that no such tile exists in two dimensions; this assumption later turned out to be incorrect.\n\nThe first such tile in three dimensions was found by Karl Reinhardt in 1928. The first example in two dimensions was found by Heesch in 1935. The related einstein problem asks for a shape that can tile space but not with an infinite cyclic group of symmetries.\n\nThe third part of the problem asks for the densest sphere packing or packing of other specified shapes. Although it expressly includes shapes other than spheres, it is generally taken as equivalent to the Kepler conjecture.\n\nIn 1998 American mathematician Thomas Callister Hales gave a computer-aided proof of the Kepler conjecture. It shows that the most space-efficient way to pack spheres is in a pyramid shape.\n\n"}
{"id": "39648291", "url": "https://en.wikipedia.org/wiki?curid=39648291", "title": "Hironaka's example", "text": "Hironaka's example\n\nIn geometry, Hironaka's example is a non-Kähler complex manifold that is a deformation of Kähler manifolds found by . Hironaka's example can be used to show that several other plausible statements holding for smooth varieties of dimension at most 2 fail for smooth varieties of dimension at least 3.\n\nTake two smooth curves \"C\" and \"D\" in a smooth projective 3-fold \"P\", intersecting in two points \"c\" and \"d\" that are nodes for the reducible curve \"C\"∪\"D\". For some applications these should be chosen so that there is a fixed-point-free automorphism exchanging the curves \"C\" and \"D\" and also exchanging the points \"c\" and \"d\". Hironaka's example \"V\" is obtained by blowing up the curves \"C\" and \"D\", with \"C\" blown up first at the point \"c\" and \"D\" blown up first at the point \"d\". Then \"V\" has two smooth rational curves \"L\" and \"M\" lying over \"c\" and \"d\" such that \"L\"+\"M\" is algebraically equivalent to 0, so \"V\" cannot be projective.\n\nFor an explicit example of this configuration, take \"t\" to be a point of order 2 in an elliptic curve \"E\", take \"P\" to be \"E\"×\"E\"/(\"t\")×\"E\"/(\"t\"), take \"C\" and \"D\" to be the sets of points of the form (\"x\",\"x\",0) and (\"x\",0,\"x\"), so that \"c\" and \"d\" are the points (0,0,0) and (\"t\",0,0), and take the involution σ to be the one taking (\"x\",\"y\",\"z\") to (\"x\" + \"t\", \"z\",\"y\").\n\nHironaka's variety is a smooth 3-dimensional complete variety but is not projective as it has a non-trivial curve algebraically equivalent to 0. Any 2-dimensional smooth complete variety is projective, so 3 is the smallest possible dimension for such an example. There are plenty of 2-dimensional complex manifolds that are not algebraic, such as Hopf surfaces (non Kähler) and non-algebraic tori (Kähler).\n\nIn a projective variety, a nonzero effective cycle has non-zero degree so cannot be algebraically equivalent to 0. In Hironaka's example the effective cycle consisting of the two exceptional curves is algebraically equivalent to 0.\n\nIf one of the curves \"D\" in Hironaka's construction is allowed to vary in a family such that most curves of the family do not intersect \"D\", then one obtains a family of manifolds such that most are projective but one is not. Over the complex numbers this gives a deformation of smooth Kähler (in fact projective) varieties that is not Kähler. This family is trivial in the smooth category, so in particular there are Kähler and non-Kähler smooth compact 3-dimensional complex manifolds that are diffeomorphic.\n\nChoose \"C\" and \"D\" so that \"P\" has an automorphism σ of order 2 acting freely on \"P\" and exchanging \"C\" and \"D\", and also exchanging \"c\" and \"d\". Then the quotient of \"V\" by the action of σ is a smooth 3-dimensional algebraic space with an irreducible curve algebraically equivalent to 0. This means that the quotient is a smooth 3-dimensional algebraic space that is not a scheme.\n\nIf the previous construction is done with complex manifolds rather than algebraic spaces, it gives an example of a smooth 3-dimensional compact Moishezon manifold that is not an abstract variety. A Moishezon manifold of dimension at most 2 is necessarily projective, so 3 is the minimum possible dimension for this example.\n\nThis is essentially the same as the previous two examples. The quotient does exist as a scheme if every orbit is contained in an affine open subscheme; the counterexample above shows that this technical condition cannot be dropped.\n\nFor quasi-projective varieties, it is obvious that any finite subset is contained in an open affine subvariety. This property fails for Hironaka's example: a two-points set consisting of a point in each of the exceptional curves is not contained in any open affine subvariety.\n\nFor Hironaka's variety \"V\" over the complex numbers with an automorphism of order 2 as above, the Hilbert functor Hilb of closed subschemes is not representable by a scheme, essentially because the quotient by the group of order 2 does not exist as a scheme . In other words, this gives an example of a smooth complete variety whose Hilbert scheme does not exist. Grothendieck showed that the Hilbert scheme always exists for projective varieties.\n\nPick a non-trivial Z/2Z torsor \"B\" → \"A\"; for example in characteristic not 2 one could take \"A\" and \"B\" to be the affine line minus the origin with the map from \"B\" to \"A\" given by \"x\" → \"x\". Think of \"B\" as an open covering of \"U\" for the étale topology. If \"V\" is a complete scheme with a fixed point free action of a group of order 2, then descent data for the map \"V\" × \"B\" → \"B\" are given by a suitable isomorphism from \"V\"×\"C\" to itself, where \"C\" = \"B\"×\"B\" = \"B\" × Z/2Z. Such an isomorphism is given by the action of Z/2Z on \"V\" and \"C\". If this descent datum were effective then the fibers of the descent over \"U\" would give a quotient of \"V\" by the action of Z/2Z. So if this quotient does not exist as a scheme (as in the example above) then the descent data are ineffective. See .\n\nIf \"X\" is a scheme of finite type over a field there is a natural map from divisors to line bundles. If \"X\" is either projective or reduced then this map is surjective. Kleiman found an example of a non-reduced and non-projective \"X\" for which this map is not surjective as follows. Take Hironaka's example of a variety with two rational curves \"A\" and \"B\" such that \"A\"+\"B\" is numerically equivalent to 0. Then \"X\" is given by picking points \"a\" and \"b\" on \"A\" and \"B\" and introducing nilpotent elements at these points.\n\n"}
{"id": "746117", "url": "https://en.wikipedia.org/wiki?curid=746117", "title": "History of calculus", "text": "History of calculus\n\nCalculus, known in its early history as infinitesimal calculus, is a mathematical discipline focused on limits, functions, derivatives, integrals, and infinite series. Isaac Newton and Gottfried Wilhelm Leibniz independently discovered calculus in the mid-17th century. However, both inventors claimed that the other had stolen his work, and the Leibniz-Newton calculus controversy continued until the end of their lives.\n\nThe ancient period introduced some of the ideas that led to integral calculus, but does not seem to have developed these ideas in a rigorous and systematic way. Calculations of volumes and areas, one goal of integral calculus, can be found in the Egyptian Moscow papyrus (c. 1820 BC), but the formulas are only given for concrete numbers, some are only approximately true, and they are not derived by deductive reasoning. Babylonians may have discovered the trapezoidal rule while doing astronomical observations of Jupiter.\n\nFrom the age of Greek mathematics, Eudoxus (c. 408−355 BC) used the method of exhaustion, which foreshadows the concept of the limit, to calculate areas and volumes, while Archimedes (c. 287−212 BC) developed this idea further, inventing heuristics which resemble the methods of integral calculus. Greek mathematicians are also credited with a significant use of infinitesimals. Democritus is the first person recorded to consider seriously the division of objects into an infinite number of cross-sections, but his inability to rationalize discrete cross-sections with a cone's smooth slope prevented him from accepting the idea. At approximately the same time, Zeno of Elea discredited infinitesimals further by his articulation of the paradoxes which they create.\n\nArchimedes developed this method further, while also inventing heuristic methods which resemble modern day concepts somewhat in his \"The Quadrature of the Parabola\", \"The Method\", and \"On the Sphere and Cylinder\". It should not be thought that infinitesimals were put on a rigorous footing during this time, however. Only when it was supplemented by a proper geometric proof would Greek mathematicians accept a proposition as true. It was not until the 17th century that the method was formalized by Cavalieri as the method of Indivisibles and eventually incorporated by Newton into a general framework of integral calculus. Archimedes was the first to find the tangent to a curve other than a circle, in a method akin to differential calculus. While studying the spiral, he separated a point's motion into two components, one radial motion component and one circular motion component, and then continued to add the two component motions together, thereby finding the tangent to the curve. The pioneers of the calculus such as Isaac Barrow and Johann Bernoulli were diligent students of Archimedes; see for instance C. S. Roero (1983).\n\nThe method of exhaustion was reinvented in China by Liu Hui in the 4th century AD in order to find the area of a circle. In the 5th century AD, Zu Chongzhi established a method that would later be called Cavalieri's principle to find the volume of a sphere. In the Middle East, Alhazen derived a formula for the sum of fourth powers. He used the results to carry out what would now be called an integration, where the formulas for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid. In the 14th century, Indian mathematician Madhava of Sangamagrama and the Kerala school of astronomy and mathematics stated components of calculus such as the Taylor series and infinite series approximations. However, they were not able to combine many differing ideas under the two unifying themes of the derivative and the integral, show the connection between the two, and turn calculus into the powerful problem-solving tool we have today.\n\nThe mathematical study of continuity was revived in the 14th century by the Oxford Calculators and French collaborators such as Nicole Oresme. They proved the \"Merton mean speed theorem\": that a uniformly accelerated body travels the same distance as a body with uniform speed whose speed is half the final velocity of the accelerated body.\n\nIn the 17th century, European mathematicians Isaac Barrow, René Descartes, Pierre de Fermat, Blaise Pascal, John Wallis and others discussed the idea of a derivative. In particular, in \"Methodus ad disquirendam maximam et minima\" and in \"De tangentibus linearum curvarum\", Fermat developed an adequality method for determining maxima, minima, and tangents to various curves that was closely related to differentiation. Isaac Newton would later write that his own early ideas about calculus came directly from \"Fermat's way of drawing tangents.\"\n\nOn the integral side, Cavalieri developed his method of indivisibles in the 1630s and 1640s, providing a more modern form of the ancient Greek method of exhaustion, and computing Cavalieri's quadrature formula, the area under the curves \"x\" of higher degree, which had previously only been computed for the parabola, by Archimedes. Torricelli extended this work to other curves such as the cycloid, and then the formula was generalized to fractional and negative powers by Wallis in 1656. In a 1659 treatise, Fermat is credited with an ingenious trick for evaluating the integral of any power function directly. Fermat also obtained a technique for finding the centers of gravity of various plane and solid figures, which influenced further work in quadrature. James Gregory, influenced by Fermat's contributions both to tangency and to quadrature, was then able to prove a restricted version of the second fundamental theorem of calculus in the mid-17th century. The first full proof of the fundamental theorem of calculus was given by Isaac Barrow.\n\nOne prerequisite to the establishment of a calculus of functions of a real variable involved finding an antiderivative for the rational function formula_1 This problem can be phrased as quadrature of the rectangular hyperbola \"xy\" = 1. In 1647 Gregoire de Saint-Vincent noted that the required function \"F\" satisfied formula_2 so that a geometric sequence became, under \"F\", an arithmetic sequence. A. A. de Sarasa associated this feature with contemporary algorithms called \"logarithms\" that economized arithmetic by rendering multiplications into additions. So \"F\" was first known as the \"hyperbolic logarithm\". After Euler exploited e = 2.71828..., and \"F\" was identified as the inverse function of the exponential function, it became the natural logarithm, satisfying formula_3\n\nThe first proof of Rolle's theorem was given by Michel Rolle in 1691 using methods developed by the Dutch mathematician Johann van Waveren Hudde. The mean value theorem in its modern form was stated by Bernard Bolzano and Augustin-Louis Cauchy (1789–1857) also after the founding of modern calculus. Important contributions were also made by Barrow, Huygens, and many others.\n\nBefore Newton and Leibniz, the word “calculus” referred to any body of mathematics, but in the following years, \"calculus\" became a popular term for a field of mathematics based upon their insights. Newton and Leibniz, building on this work, independently developed the surrounding theory of infinitesimal calculus in the late 17th century. Also, Leibniz did a great deal of work with developing consistent and useful notation and concepts. Newton provided some of the most important applications to physics, especially of integral calculus. The purpose of this section is to examine Newton and Leibniz’s investigations into the developing field of infinitesimal calculus. Specific importance will be put on the justification and descriptive terms which they used in an attempt to understand calculus as they themselves conceived it.\n\nBy the middle of the 17th century, European mathematics had changed its primary repository of knowledge. In comparison to the last century which maintained Hellenistic mathematics as the starting point for research, Newton, Leibniz and their contemporaries increasingly looked towards the works of more modern thinkers. Europe had become home to a burgeoning mathematical community and with the advent of enhanced institutional and organizational bases a new level of organization and academic integration was being achieved. Importantly, however, the community lacked formalism; instead it consisted of a disordered mass of various methods, techniques, notations, theories, and paradoxes.\n\nNewton came to calculus as part of his investigations in physics and geometry. He viewed calculus as the scientific description of the generation of motion and magnitudes. In comparison, Leibniz focused on the tangent problem and came to believe that calculus was a metaphysical explanation of change. Importantly, the core of their insight was the formalization of the inverse properties between the integral and the differential of a function. This insight had been anticipated by their predecessors, but they were the first to conceive calculus as a system in which new rhetoric and descriptive terms were created. Their unique discoveries lay not only in their imagination, but also in their ability to synthesize the insights around them into a universal algorithmic process, thereby forming a new mathematical system.\n\nNewton completed no definitive publication formalizing his fluxional calculus; rather, many of his mathematical discoveries were transmitted through correspondence, smaller papers or as embedded aspects in his other definitive compilations, such as the \"Principia\" and \"Opticks\". Newton would begin his mathematical training as the chosen heir of Isaac Barrow in Cambridge. His aptitude was recognized early and he quickly learned the current theories. By 1664 Newton had made his first important contribution by advancing the binomial theorem, which he had extended to include fractional and negative exponents. Newton succeeded in expanding the applicability of the binomial theorem by applying the algebra of finite quantities in an analysis of infinite series. He showed a willingness to view infinite series not only as approximate devices, but also as alternative forms of expressing a term.\n\nMany of Newton's critical insights occurred during the plague years of 1665–1666 which he later described as, \"the prime of my age for invention and minded mathematics and [natural] philosophy more than at any time since.\" It was during his plague-induced isolation that the first written conception of fluxionary calculus was recorded in the unpublished \"De Analysi per Aequationes Numero Terminorum Infinitas\". In this paper, Newton determined the area under a curve by first calculating a momentary rate of change and then extrapolating the total area. He began by reasoning about an indefinitely small triangle whose area is a function of \"x\" and \"y\". He then reasoned that the infinitesimal increase in the abscissa will create a new formula where (importantly, \"o\" is the letter, not the digit 0). He then recalculated the area with the aid of the binomial theorem, removed all quantities containing the letter \"o\" and re-formed an algebraic expression for the area. Significantly, Newton would then “blot out” the quantities containing \"o\" because terms \"multiplied by it will be nothing in respect to the rest\".\n\nAt this point Newton had begun to realize the central property of inversion. He had created an expression for the area under a curve by considering a momentary increase at a point. In effect, the fundamental theorem of calculus was built into his calculations. While his new formulation offered incredible potential, Newton was well aware of its logical limitations at the time. He admits that \"errors are not to be disregarded in mathematics, no matter how small\" and that what he had achieved was “shortly explained rather than accurately demonstrated.\"\n\nIn an effort to give calculus a more rigorous explication and framework, Newton compiled in 1671 the \"Methodus Fluxionum et Serierum Infinitarum\". In this book, Newton's strict empiricism shaped and defined his fluxional calculus. He exploited instantaneous motion and infinitesimals informally. He used math as a methodological tool to explain the physical world. The base of Newton’s revised calculus became continuity; as such he redefined his calculations in terms of continual flowing motion. For Newton, variable magnitudes are not aggregates of infinitesimal elements, but are generated by the indisputable fact of motion. As with many of his works, Newton delayed publication. \"Methodus Fluxionum\" was not published until 1736.\n\nNewton attempted to avoid the use of the infinitesimal by forming calculations based on ratios of changes. In the \"Methodus Fluxionum\" he defined the rate of generated change as a fluxion, which he represented by a dotted letter, and the quantity generated he defined as a fluent. For example, if formula_4 and formula_5 are fluents, then formula_6 and formula_7 are their respective fluxions. This revised calculus of ratios continued to be developed and was maturely stated in the 1676 text \"De Quadratura Curvarum\" where Newton came to define the present day derivative as the ultimate ratio of change, which he defined as the ratio between evanescent increments (the ratio of fluxions) purely at the moment in question. Essentially, the ultimate ratio is the ratio as the increments vanish into nothingness. Importantly, Newton explained the existence of the ultimate ratio by appealing to motion;\n\n“For by the ultimate velocity is meant that, with which the body is moved, neither before it arrives at its last place, when the motion ceases nor after but at the very instant when it arrives... the ultimate ratio of evanescent quantities is to be understood, the ratio of quantities not before they vanish, not after, but with which they vanish”\n\nNewton developed his fluxional calculus in an attempt to evade the informal use of infinitesimals in his calculations.\n\nWhile Newton began development of his fluxional calculus in 1665–1666 his findings did not become widely circulated until later. In the intervening years Leibniz also strove to create his calculus. In comparison to Newton who came to math at an early age, Leibniz began his rigorous math studies with a mature intellect. He was a polymath, and his intellectual interests and achievements involved metaphysics, law, economics, politics, logic, and mathematics. In order to understand Leibniz’s reasoning in calculus his background should be kept in mind. Particularly, his metaphysics which described the universe as a Monadology, and his plans of creating a precise formal logic whereby, \"a general method in which all truths of the reason would be reduced to a kind of calculation.\"\n\nIn 1672 Leibniz met the mathematician Huygens who convinced Leibniz to dedicate significant time to the study of mathematics. By 1673 he had progressed to reading Pascal’s \"Traité des Sinus du Quarte Cercle\" and it was during his largely autodidactic research that Leibniz said \"a light turned on\" . Like Newton, Leibniz, saw the tangent as a ratio but declared it as simply the ratio between ordinates and abscissas. He continued this reasoning to argue that the integral was in fact the sum of the ordinates for infinitesimal intervals in the abscissa; in effect, the sum of an infinite number of rectangles. From these definitions the inverse relationship or differential became clear and Leibniz quickly realized the potential to form a whole new system of mathematics. Where Newton over the course of his career used several approaches in addition to an approach using infinitesimals, Leibniz made this the cornerstone of his notation and calculus.\n\nIn the manuscripts of 25 October to 11 November 1675, Leibniz recorded his discoveries and experiments with various forms of notation. He was acutely aware of the notational terms used and his earlier plans to form a precise logical symbolism became evident. Eventually, Leibniz denoted the infinitesimal increments of abscissas and ordinates \"dx\" and \"dy\", and the summation of infinitely many infinitesimally thin rectangles as a long s (∫ ), which became the present integral symbol formula_8.\n\nWhile Leibniz's notation is used by modern mathematics, his logical base was different from our current one. Leibniz embraced infinitesimals and wrote extensively so as, “not to make of the infinitely small a mystery, as had Pascal.” According to Gilles Deleuze, Leibniz's zeroes \"are nothings, but they are not absolute nothings, they are nothings respectively\" (quoting Leibniz' text \"Justification of the calculus of infinitesimals by the calculus of ordinary algebra\"). Alternatively, he defines them as, “less than any given quantity.” For Leibniz, the world was an aggregate of infinitesimal points and the lack of scientific proof for their existence did not trouble him. Infinitesimals to Leibniz were ideal quantities of a different type from appreciable numbers. The truth of continuity was proven by existence itself. For Leibniz the principle of continuity and thus the validity of his calculus was assured. Three hundred years after Leibniz's work, Abraham Robinson showed that using infinitesimal quantities in calculus could be given a solid foundation.\n\nThe rise of calculus stands out as a unique moment in mathematics. Calculus is the mathematics of motion and change, and as such, its invention required the creation of a new mathematical system. Importantly, Newton and Leibniz did not create the same calculus and they did not conceive of modern calculus. While they were both involved in the process of creating a mathematical system to deal with variable quantities their elementary base was different. For Newton, change was a variable quantity over time and for Leibniz it was the difference ranging over a sequence of infinitely close values. Notably, the descriptive terms each system created to describe change was different.\n\nHistorically, there was much debate over whether it was Newton or Leibniz who first \"invented\" calculus. This argument, the Leibniz and Newton calculus controversy, involving Leibniz, who was German, and the Englishman Newton, led to a rift in the European mathematical community lasting over a century. Leibniz was the first to publish his investigations; however, it is well established that Newton had started his work several years prior to Leibniz and had already developed a theory of tangents by the time Leibniz became interested in the question.\nIt is not known how much this may have influenced Leibniz. The initial accusations were made by students and supporters of the two great scientists at the turn of the century, but after 1711 both of them became personally involved, accusing each other of plagiarism.\n\nThe priority dispute had an effect of separating English-speaking mathematicians from those in the continental Europe for many years. Only in the 1820s, due to the efforts of the Analytical Society, did Leibnizian analytical calculus become accepted in England. Today, both Newton and Leibniz are given credit for independently developing the basics of calculus. It is Leibniz, however, who is credited with giving the new discipline the name it is known by today: \"calculus\". Newton's name for it was \"the science of fluents and fluxions\".\n\nThe work of both Newton and Leibniz is reflected in the notation used today. Newton introduced the notation formula_9 for the derivative of a function \"f\". Leibniz introduced the symbol formula_10 for the integral and wrote the derivative of a function \"y\" of the variable \"x\" as formula_11, both of which are still in use.\n\nSince the time of Leibniz and Newton, many mathematicians have contributed to the continuing development of calculus. One of the first and most complete works on both infinitesimal and integral calculus was written in 1748 by Maria Gaetana Agnesi.\n\nAntoine Arbogast (1800) was the first to separate the symbol of operation from that of quantity in a differential equation. Francois-Joseph Servois (1814) seem to have been the first to give correct rules on the subject. Charles James Hargreave (1848) applied these methods in his memoir on differential equations, and George Boole freely employed them. Hermann Grassmann and Hermann Hankel made great use of the theory, the former in studying equations, the latter in his theory of complex numbers.\n\nThe calculus of variations may be said to begin with a problem of Johann Bernoulli (1696). It immediately occupied the attention of Jakob Bernoulli but Leonhard Euler first elaborated the subject. His contributions began in 1733, and his \"Elementa Calculi Variationum\" gave to the science its name. Joseph Louis Lagrange contributed extensively to the theory, and Adrien-Marie Legendre (1786) laid down a method, not entirely satisfactory, for the discrimination of maxima and minima. To this discrimination Brunacci (1810), Carl Friedrich Gauss (1829), Siméon Denis Poisson (1831), Mikhail Vasilievich Ostrogradsky (1834), and Carl Gustav Jakob Jacobi (1837) have been among the contributors. An important general work is that of Sarrus (1842) which was condensed and improved by Augustin Louis Cauchy (1844). Other valuable treatises and memoirs have been written by Strauch (1849), Jellett (1850), Otto Hesse (1857), Alfred Clebsch (1858), and Carll (1885), but perhaps the most important work of the century is that of Karl Weierstrass. His course on the theory may be asserted to be the first to place calculus on a firm and rigorous foundation.\n\nNiels Henrik Abel seems to have been the first to consider in a general way the question as to what differential expressions can be integrated in a finite form by the aid of ordinary functions, an investigation extended by Liouville. Cauchy early undertook the general theory of determining definite integrals, and the subject has been prominent during the 19th century. Frullani integral, David Bierens de Haan's work on the theory and his elaborate tables,Lejeune Dirichlet's lectures embodied in Meyer's treatise, and numerous memoirs of Legendre, Poisson, Plana, Raabe, Sohncke, Schlömilch, Elliott, Leudesdorf, and Kronecker are among the noteworthy contributions.\n\nEulerian integrals were first studied by Euler and afterwards investigated by Legendre, by whom they were classed as Eulerian integrals of the first and second species, as follows:\n\nalthough these were not the exact forms of Euler's study.\n\nIf \"n\" is a positive integer, it follows that:\nbut the integral converges for all positive real formula_15 and defines an analytic continuation of the factorial function to all of the complex plane except for poles at zero and the negative integers. To it Legendre assigned the symbol formula_16, and it is now called the gamma function. Besides being analytic over positive reals ℝ,  formula_16 also enjoys the uniquely defining property that formula_18 is convex, which aesthetically justifies this analytic continuation of the factorial function over any other analytic continuation. To the subject Lejeune Dirichlet has contributed an important theorem (Liouville, 1839), which has been elaborated by Liouville, Catalan, Leslie Ellis, and others. On the evaluation of formula_19 and formula_20 Raabe (1843–44), Bauer (1859), and Gudermann (1845) have written. Legendre's great table appeared in 1816.\n\nThe application of the infinitesimal calculus to problems in physics and astronomy was contemporary with the origin of the science. All through the 18th century these applications were multiplied, until at its close Laplace and Lagrange had brought the whole range of the study of forces into the realm of analysis. To Lagrange (1773) we owe the introduction of the theory of the potential into dynamics, although the name \"potential function\" and the fundamental memoir of the subject are due to Green (1827, printed in 1828). The name \"potential\" is due to Gauss (1840), and the distinction between potential and potential function to Clausius. With its development are connected the names of Lejeune Dirichlet, Riemann, von Neumann, Heine, Kronecker, Lipschitz, Christoffel, Kirchhoff, Beltrami, and many of the leading physicists of the century.\n\nIt is impossible in this place to enter into the great variety of other applications of analysis to physical problems. Among them are the investigations of Euler on vibrating chords; Sophie Germain on elastic membranes; Poisson, Lamé, Saint-Venant, and Clebsch on the elasticity of three-dimensional bodies; Fourier on heat diffusion; Fresnel on light; Maxwell, Helmholtz, and Hertz on electricity; Hansen, Hill, and Gyldén on astronomy; Maxwell on spherical harmonics; Lord Rayleigh on acoustics; and the contributions of Lejeune Dirichlet, Weber, Kirchhoff, F. Neumann, Lord Kelvin, Clausius, Bjerknes, MacCullagh, and Fuhrmann to physics in general. The labors of Helmholtz should be especially mentioned, since he contributed to the theories of dynamics, electricity, etc., and brought his great analytical powers to bear on the fundamental axioms of mechanics as well as on those of pure mathematics.\n\nFurthermore, infinitesimal calculus was introduced into the social sciences, starting with Neoclassical economics. Today, it is a valuable tool in mainstream economics.\n\n\n\n"}
{"id": "8313563", "url": "https://en.wikipedia.org/wiki?curid=8313563", "title": "James A. D. W. Anderson", "text": "James A. D. W. Anderson\n\nJames Arthur Dean Wallace Anderson Known as James Anderson is an academic staff member in the School of Systems Engineering at the University of Reading, England. He is currently teaching compilers, algorithms, and computer algebra, and in the past he has taught programming and computer graphics.\n\nAnderson quickly gained publicity in December 2006 in the United Kingdom when the regional BBC South Today reported his claim of \"having solved a 1200 year old problem\", namely that of division by zero. However, commentators quickly responded that his ideas are just a variation of the standard IEEE 754 concept of NaN (Not a Number), which has been commonly employed on computers in floating point arithmetic for many years. Dr Anderson defended against the criticism of his claims on BBC Berkshire on 12 December 2006, saying, \"If anyone doubts me I can hit them over the head with a computer that does it.\"\n\nAnderson is a member of the British Computer Society, the British Machine Vision Association, Eurographics, and the British Society for the Philosophy of Science. He is also a teacher in the Computer Science department (School of Systems Engineering) at the University of Reading. He was\na psychology graduate who worked in the Electrical and Electronic Engineering departments at the University of Sussex and Plymouth Polytechnic (now the University of Plymouth). His doctorate is from the University of Reading for (in Anderson's words) \"developing a canonical description of the perspective transformations in whole numbered dimensions\".\n\nHe has written two papers on division by zero and has invented what he calls the \"Perspex machine\".\n\nAnderson claims that \"mathematical arithmetic is sociologically invalid\" and that IEEE floating-point arithmetic, with NaN, is also faulty.\n\nAnderson's transreal numbers were first mentioned in a 1997 publication, and made well-known on the Internet in 2006, but not accepted as useful by the mathematics community. These numbers are used in his concept of transreal arithmetic and the Perspex machine. According to Anderson, transreal numbers include all of the real numbers, plus three others: infinity (formula_1), negative infinity (formula_2) and \"nullity\" (formula_3), a numerical representation of a non-number that lies outside of the affinely extended real number line. (Nullity, confusingly, has an existing mathematical meaning.)\n\nAnderson intends the axioms of transreal arithmetic to complement the axioms of standard arithmetic; they are supposed to produce the same result as standard arithmetic for all calculations where standard arithmetic defines a result. In addition, they are intended to define a consistent numeric result for the calculations which are undefined in standard arithmetic, such as division by zero.\n\n\"Transreal arithmetic\" closely resembles IEEE floating point arithmetic, a floating point arithmetic commonly used on computers. IEEE floating point arithmetic, like transreal arithmetic, uses affine infinity (two separate infinities, one positive and one negative) rather than projective infinity (a single unsigned infinity, turning the number line into a loop).\n\nThe main difference is that IEEE arithmetic replaces the real (and transreal) number zero with positive and negative zero. (This is so that it can preserve the sign of a nonzero real number whose absolute value has been rounded down to zero. See also infinitesimal.) Division of any non-zero finite number by zero results in either positive or negative infinity.\n\nHowever, in IEEE arithmetic, division of zero by zero is still considered indeterminate. The reason for this is simple: A statement about the quotient of two numbers is understood in mathematics as another statement about multiplication. Specifically, if\n\nformula_4\n\nthis is understood as simply another way of saying that\n\nformula_5\n\nThus, if for some number formula_6\n\nformula_7\n\nthen this is just another way of saying that\n\nformula_8\n\nBut in fact this is true for all real numbers formula_6. And that is precisely the reason that mathematicians do not assign a single value to formula_10 but rather label it \"indeterminate\". Assigning a value to formula_10, even a newly fabricated \"number\", misses the point entirely.\n\nIn IEEE arithmetic, the value of formula_10 is therefore represented by the symbol Not a Number (NaN) (Not a Number). NaN is not meant to be a number, but rather an error message conveying the fact that the arithmetical operation the computer just attempted cannot be assigned a single number as an answer – even if formula_13 and formula_2 are considered numbers. Because formula_15 is an error message and not a number, it is not considered equal to anything, even itself. That is, the comparison formula_16 evaluates to false.\n\nHere are some identities in transreal arithmetic with the IEEE equivalents:\nThe main difference between transreal arithmetic and IEEE floating-point arithmetic is thus that nullity compares equal to nullity, whereas NaN does not compare equal to NaN.\n\nAnderson's analysis of the properties of transreal algebra is given in his paper on \"perspex machines\".\n\nDue to the more expansive definition of numbers in transreal arithmetic, several identities and theorems which apply to all numbers in standard arithmetic are not universal in transreal arithmetic. For instance, in transreal arithmetic, formula_17 is not true for all formula_18, since formula_19. That problem is addressed in ref. pg. 7. Similarly, it is not always the case in transreal arithmetic that a number can be cancelled with its reciprocal to yield formula_20. Cancelling zero with its reciprocal in fact yields nullity.\n\nExamining the axioms provided by Anderson, it is easy to see that any term which contains an occurrence of the constant formula_3 is provably equivalent to formula_3. Formally, let formula_23 be any term with a sub-term formula_3, then\nformula_25 is a theorem of the theory proposed by Anderson.\n\nAnderson's transreal arithmetic, and concept of \"nullity\" in particular, were introduced to the public by the BBC with its report in December 2006 where Anderson was featured on a BBC television segment teaching schoolchildren about his concept of \"nullity\". The report implied that Anderson had \"discovered\" the solution to division by zero, rather than simply attempting to formalize it. The report also suggested that Anderson was the first to solve this problem, when in fact the result of zero divided by zero has been expressed formally in a number of different ways (for example, NaN).\n\nThe BBC was criticized for irresponsible journalism, but the producers of the segment defended the BBC, stating that the report was a light-hearted look at a mathematical problem aimed at a mainstream, regional audience for BBC South Today rather than at a global audience of mathematicians. The BBC later posted a follow-up giving Anderson's response to many claims that the theory is flawed.\n\nAnderson has been trying to market his ideas for transreal arithmetic and \"Perspex machines\" to investors. He claims that his work can produce computers which run \"orders of magnitude faster than today's computers\". He has also claimed that it can help solve such problems as quantum gravity, the mind-body connection, consciousness and free will.\n\n\n\n"}
{"id": "56204268", "url": "https://en.wikipedia.org/wiki?curid=56204268", "title": "Johan Antony Barrau", "text": "Johan Antony Barrau\n\nJohan Antony Barrau (3 April 1873, Oisterwijk – 8 January 1953, Utrecht) was a Dutch mathematician, specializing in geometry.\n\nBarrau was educated at the Dutch Royal Naval College at Willemsoord and then at the University of Amsterdam. From 1891 to 1898, Barrau was an officer with the Royal Netherlands Navy, later with the Netherlands Marine Corps. However, he left the service and became a mathematics teacher at a Hogere Burgerschool in Dordrecht until 1900, then in Amsterdam. In 1907 he obtained his PhD at the University of Amsterdam under the supervision of Diederik Korteweg. From 1908 to 1913 Barrau was a mathematics professor at the Delft University of Technology. He was a professor of synthetic, analytical and descriptive differential geometry at the University of Groningen from 1913 to 1928.. From 1928 until his retirement at age 70, he was a professor at Utrecht University. He received the military service medal consisting of the Expedition Cross with the Atjeh clasp and was named Knight of the Order of the Netherlands Lion. Barrau published a textbook on analytical geometry and various articles in national and international journals.\n\nHe was an Invited Speaker of the ICM in 1920 at Strasbourg and in 1924 at Toronto.\n"}
{"id": "3168650", "url": "https://en.wikipedia.org/wiki?curid=3168650", "title": "Kaplan–Meier estimator", "text": "Kaplan–Meier estimator\n\nThe Kaplan–Meier estimator, also known as the product limit estimator, is a non-parametric statistic used to estimate the survival function from lifetime data. In medical research, it is often used to measure the fraction of patients living for a certain amount of time after treatment. In other fields, Kaplan–Meier estimators may be used to measure the length of time people remain unemployed after a job loss, the time-to-failure of machine parts, or how long fleshy fruits remain on plants before they are removed by frugivores. The estimator is named after Edward L. Kaplan and Paul Meier, who each submitted similar manuscripts to the \"Journal of the American Statistical Association\". The journal editor, John Tukey, convinced them to combine their work into one paper, which has been cited about 50,000 times since its publication.\n\nThe estimator is given by:\n\nwith formula_2 a time when at least one event happened, \"d\" the \"number of events\" (i.e., deaths) that happened at time formula_2 and formula_4 the \"individuals known to survive\" (have not yet had an event or been censored) at time formula_2.\n\nA plot of the Kaplan–Meier estimator is a series of declining horizontal steps which, with a large enough sample size, approaches the true survival function for that population. The value of the survival function between successive distinct sampled observations (\"clicks\") is assumed to be constant.\n\nAn important advantage of the Kaplan–Meier curve is that the method can take into account some types of censored data, particularly \"right-censoring\", which occurs if a patient withdraws from a study, is lost to follow-up, or is alive without event occurrence at last follow-up. On the plot, small vertical tick-marks indicate individual patients whose survival times have been right-censored. When no truncation or censoring occurs, the Kaplan–Meier curve is the complement of the empirical distribution function.\n\nIn medical statistics, a typical application might involve grouping patients into categories, for instance, those with Gene A profile and those with Gene B profile. In the graph, patients with Gene B die much more quickly than those with Gene A. After two years, about 80% of the Gene A patients survive, but less than half of patients with Gene B.\n\nIn order to generate a Kaplan–Meier estimator, at least two pieces of data are required for each patient (or each subject): the status at last observation (event occurrence or right-censored) and the time to event (or time to censoring). If the survival functions between two or more groups are to be compared, then a third piece of data is required: the group assignment of each subject.\n\nLet formula_6 be a random variable, which we think of as the time until an event of interest takes place. As indicated above, the goal is to estimate the survival function formula_7 underlying formula_8. Recall that this function is defined as\n\nformula_9,\n\nwhere formula_10.\n\nLet formula_11 be independent, identically distributed random variables, whose common distribution is that of formula_8: formula_13 is the random time when some event formula_14 happened. The data available for estimating formula_7 is not formula_16, but the list of pairs formula_17 where for formula_18, formula_19 is a fixed, deterministic integer, the censoring time of event formula_14 and formula_21. In particular, the information available about the timing of event formula_14 is whether the event happened before the fixed time formula_23 and if so, then the actual time of the event is also available. The challenge is to estimate formula_24 given this data.\n\nHere, we show two derivations of the Kaplan–Meier estimator. Both are based on rewriting the survival function in terms of what is sometimes called hazard, or mortality rates. However, before doing this it is worthwhile to consider a naive estimator.\n\nTo understand the power of the Kaplan–Meier estimator, it is worthwhile to first describe a naive estimator of the survival function.\n\nFix formula_25 and let formula_26. A basic argument shows that the following proposition holds:\n\nProposition 1: If the censoring time formula_27 of event formula_28 exceeds formula_29 (formula_30), then formula_31 if and only if formula_32.\n\nLet formula_28 be such that formula_30. It follows from the above proposition that\n\nformula_35\n\nLet formula_36 and consider only those formula_37, i.e. the events for which the outcome was not censored before time formula_29. Let formula_39 be the number of elements in formula_40. Note that the set formula_41 is not random and so neither is formula_42. Furthermore, formula_43 is a sequence of independent, identically distributed Bernoulli random variables with common parameter formula_44. Assuming that formula_45, this suggests to estimate formula_46 using\n\nformula_47,\n\nwhere the last equality follows because formula_48 implies formula_49.\n\nThe quality of this estimate is governed by the size of formula_50. This can be problematic when formula_50 is small, which happens, by definition, when a lot of the events are censored. A particularly unpleasant property of this estimator, that suggests that perhaps it is not the \"best\" estimator, is that it ignores all the observations whose censoring time precedes formula_29. Intuitively, these observations still contain information about formula_24: For example, when for many events with formula_54, formula_55 also holds, we can infer that events often happen early, which implies that formula_56 is large, which, through formula_57 means that formula_24 must be small. However, this information is ignored by this naive estimator. The question is then whether there exists an estimator that makes a better use of all the data. This is what the Kaplan–Meier estimator accomplishes. Note that the naive estimator cannot be improved when censoring does not take place; so whether an improvement is possible critically hinges upon whether censoring is in place.\n\nBy elementary calculations,\n\nformula_59\n\nwhere the one but last equality used that formula_8 is integer valued and for the last line we introduced\n\nformula_61.\n\nBy a recursive expansion of the equality formula_62, we get\n\nformula_63\n\nNote that here formula_64.\n\nThe Kaplan–Meier estimator can be seen as a \"plug-in estimator\" where each formula_65 is estimated based on the data and the estimator of formula_66 is obtained as a product of these estimates.\n\nIt remains to specify how formula_67 is to be estimated. By Proposition 1, for any formula_68 such that formula_69, formula_70 and formula_71 both hold. Hence, for any formula_72 such that formula_73,\n\nformula_74. By a similar reasoning that lead to the construction of the naive estimator above, we arrive at the estimator\n\nformula_75\n\n(think of estimating the numerator and denominator separately in the definition of the \"hazard rate\" formula_76). The Kaplan–Meier estimator is then given by\n\nformula_77.\n\nThe form of the estimator stated at the beginning of the article can be obtained by some further algebra. For this, write formula_78 where, using the actuarial science terminology, formula_79 is the number of known deaths at time formula_80, while formula_81 is the number of those persons who are alive at time formula_80.\n\nNote that if formula_83, formula_84. This implies that we can leave out from the product defining formula_85 all those terms where formula_83. Then, letting formula_87 be the times formula_80 when formula_89, formula_90 and formula_91, we arrive at the form of the Kaplan–Meier estimator given at the beginning of the article:\n\nformula_92.\n\nAs opposed to the naive estimator, this estimator can be seen to use the available information more effectively: In the special case mentioned beforehand, when there are many early events recorded, the estimator will multiply many terms with a value below one and will thus take into account that the survival probability cannot be large.\n\nKaplan–Meier estimator can be derived from maximum likelihood estimation of hazard function. More specifically given formula_93 as the number of events and formula_4 the total individuals at risk at time formula_95, discrete hazard rate formula_96 can be defined as the probability of an individual with an event at time formula_95. Then survival rate can be defined as:\n\nformula_98\n\nand the likelihood function for the hazard function up to time formula_95 is:\n\nformula_100\n\ntherefore the log likelihood will be:\n\nformula_101\n\nfinding the maximum of log likelihood with respect to formula_96 yields:\n\nformula_103\n\nwhere hat is used to denote maximum likelihood estimation. Given this result, we can write:\n\nformula_104\n\nThe Kaplan–Meier estimator is one of the most frequently used methods of survival analysis. The estimate may be useful to examine recovery rates, the probability of death, and the effectiveness of treatment. It is limited in its ability to estimate survival adjusted for covariates; parametric survival models and the Cox proportional hazards model may be useful to estimate covariate-adjusted survival.\n\nThe Kaplan–Meier estimator is a statistic, and several estimators are used to approximate its variance. One of the most common estimators is Greenwood's formula:\n\nwhere formula_106 is the number of cases and formula_107 is the total number of observations, for formula_108.\n"}
{"id": "10504570", "url": "https://en.wikipedia.org/wiki?curid=10504570", "title": "Lift (mathematics)", "text": "Lift (mathematics)\n\nA basic example in topology is lifting a path in one space to a path in a covering space. Consider, for instance, mapping opposite points on a sphere to the same point, a continuous map from the sphere covering the projective plane. A path in the projective plane is a continuous map from the unit interval, [0,1]. We can lift such a path to the sphere by choosing one of the two sphere points mapping to the first point on the path, then maintain continuity. In this case, each of the two starting points forces a unique path on the sphere, the lift of the path in the projective plane. Thus in the category of topological spaces with continuous maps as morphisms, we have\n\nLifts are ubiquitous; for example, the definition of fibrations (see homotopy lifting property) and the valuative criteria of separated and proper maps of schemes are formulated in terms of existence and (in the last case) uniqueness of certain lifts.\n\nIn algebraic topology and homological algebra, tensor product and the Hom functor are adjoint; however, they might not always lift to an exact sequence. This leads to the definition of the Ext functor and the Tor functor.\n\n"}
{"id": "30981930", "url": "https://en.wikipedia.org/wiki?curid=30981930", "title": "List of integrals of Gaussian functions", "text": "List of integrals of Gaussian functions\n\nIn these expressions,\n\nis the standard normal probability density function,\n\nis the corresponding cumulative distribution function (where erf is the error function) and\n\nis Owen's T function.\n\nOwen has an extensive list of Gaussian-type integrals; only a subset is given below.\n\nIn these integrals, \"n\"!! is the double factorial: for even \"n\" it is equal to the product of all even numbers from 2 to \"n\", and for odd \"n\" it is the product of all odd numbers from 1 to \"n\" ; additionally it is assumed that .\n\n"}
{"id": "9767106", "url": "https://en.wikipedia.org/wiki?curid=9767106", "title": "Low (computability)", "text": "Low (computability)\n\nIn computability theory, a Turing degree [\"X\"] is low if the Turing jump [\"X\"′] is 0′. A set is low if it has low degree. Since every set is computable from its jump, any low set is computable in 0′, but the jump of sets computable in 0′ can bound any degree r.e. in 0′ (Schoenfield Jump Inversion). \"X\" being low says that its jump \"X\"′ has the least possible degree in terms of Turing reducibility for the jump of a set.\n\nA degree is \"low n\" if its n'th jump is the n'th jump of 0. A set \"X\" is \"generalized low\" if it satisfies \"X\"′ ≡ \"X\" + 0′, that is: if its jump has the lowest degree possible. And a degree d is \"generalized low n\" if its n'th jump is the (n-1)'st jump of the join of d with 0′. More generally, properties of sets which describe their being computationally weak (when used as a Turing oracle) are referred to under the umbrella term \"lowness properties\".\n\nBy the Low basis theorem of Jockusch and Soare, any nonempty formula_1 class in formula_2 contains a set of low degree. This implies that, although low sets are computationally weak, they can still accomplish such feats as computing a completion of Peano Arithmetic. In practice, this allows a restriction on the computational power of objects needed for recursion theoretic constructions: for example, those used in the analyzing the proof-theoretic strength of Ramsey's theorem.\n\n\n"}
{"id": "20200", "url": "https://en.wikipedia.org/wiki?curid=20200", "title": "Management science", "text": "Management science\n\nManagement science (MS), is the broad interdisciplinary study of problem solving and decision making in human organizations, with strong links to management, economics, business, engineering, management consulting, and other sciences. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and accurate management decisions by arriving at optimal or near optimal solutions to complex decision problems. Management sciences help businesses to achieve goals using various scientific methods. \n\nThe field was initially an outgrowth of applied mathematics, where early challenges were problems relating to the optimization of systems which could be modeled linearly, i.e., determining the optima (maximum value of profit, assembly line performance, crop yield, bandwidth, etc. or minimum of loss, risk, costs, etc.) of some objective function. Today, management science encompasses any organizational activity for which the problem can be structured as a functional system so as to obtain a solution set with identifiable characteristics.\n\nManagement science is concerned with a number of different areas of study: One is developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems. The models used can often be represented mathematically, but sometimes computer-based, visual or verbal representations are used as well or instead. Another area is designing and developing new and better models of organizational excellence. \n\nManagement science research can be done on three levels:\n\nThe management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. The techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.\n\nIts origins can be traced to operations research, which made its debut during World War II when the Allied forces recruited scientists of various disciplines to assist with military operations. In these early applications, the scientists utilized simple mathematical models to make efficient use of limited technologies and resources. The application of these models within the corporate sector became known as management science. \n\nIn 1967 Stafford Beer characterized the field of management science as \"the business use of operations research\".\n\nSome of the fields that management science involves include: \n\n\nas well as many others.\n\nApplications of management science are abundant in industry as airlines, manufacturing companies, service organizations, military branches, and in government. The range of problems and issues to which management science has contributed insights and solutions is vast. It includes:.\n\nManagement science is also concerned with so-called \"soft-operational analysis\", which concerns methods for strategic planning, strategic decision support, and problem structuring methods (PSM). At this level of abstraction, mathematical modeling and simulation will not suffice. Therefore, during the past 30 years, a number of non-quantified modelling methods have been developed. These include morphological analysis and various forms of influence diagrams.\n\n"}
{"id": "27194034", "url": "https://en.wikipedia.org/wiki?curid=27194034", "title": "Math in Moscow", "text": "Math in Moscow\n\nMath in Moscow (MiM) is a one-semester study abroad program for North American and European undergraduates held at the Independent University of Moscow (IUM) in Moscow, Russia. The program consists mainly of math courses that are taught in English. The program was first offered in 2001, and since 2008 has been run jointly by the Independent University of Moscow, Moscow Center for Continuous Mathematical Education, and the Higher School of Economics (HSE). \n\nThe program has hosted over 200 participants, including students from Harvard, Princeton, MIT, Harvey Mudd, Berkeley, Cornell, Yale, McGill, Toronto and Montreal.\n\nThe MiM semester lasts fifteen weeks with fourteen weeks of teaching and one week of exams. Math courses are lectured by professors of the Independent University of Moscow and the Math Department of National Research University Higher School of Economics. The cultural elements of the program include organized trips to Saint Petersburg and to the Golden Ring towns of Vladimir and Suzdal. Students live in the dormitory of the Higher School of Economics.\n\nEach semester the American Mathematical Society offers up to five \"Math in Moscow\" scholarships provided by the National Science Foundation to US undergraduates, and the Canadian Mathematical Society offers one or two NSERC scholarships to Canadian students.\n\nThe program is often reviewed favorably by North American students and their departments.\n\nThe primary curriculum is entirely mathematical, drawing from every major field of mathematics. All courses are taught jointly with the Higher School of Economics, and are often attended by students from the HSE master's program. Likewise, Math in Moscow participants may attend open lectures and seminars at the Higher School of Economics. The Math in Moscow courses are formally divided into three groups according to the expected prerequisites, however admitted students may choose to attend whichever and as many courses as they wish. An incoming aptitude exam is administered to assist in advising students' course selections. \n\nAll courses expect at least a semester each of analysis and linear algebra as prerequisites. Courses at the first level require no more than this basic formal background, but are generally more intensive than their equivalents at North American universities, often taught from first-year graduate texts and presenting material typically covered only at a graduate level; intermediate courses correspond to senior-level offerings at scientifically-focused American and Canadian institutions; and advanced courses are graduate-level. \n\n\n\n\n\n\nIn addition to the mathematical curriculum, students are offered electives in Russian literature, Russia history, history of mathematics and science, and Russian language.\n\nThe courses deviate in structure from standard courses in the United States, Canada, and Europe. The Russian pedagogical tradition emphasizes developing the active participation of students. Classes are designed to encourage dialogues between the students and the teacher, which is more easily achieved in the program's small classes of two to ten students. Each math course runs three hours once a week. Roughly speaking, classes devote an hour and a half of lecture and an hour and a half of exercises, although structure deviates from course to course. Students may choose any number of courses, in practice between three and six from the mathematical curriculum. Courses are run jointly with the Higher School of Economics M.Sc program. Most of the Math in Moscow courses are given at the building of the Independent University of Moscow, located in the center of Moscow near the historic Arbat, some take place in the building of the Higher School of Economics. \n\n\n"}
{"id": "208502", "url": "https://en.wikipedia.org/wiki?curid=208502", "title": "Object-modeling technique", "text": "Object-modeling technique\n\nThe object-modeling technique (OMT) is an object modeling approach for software modeling and designing. It was developed around 1991 by Rumbaugh, Blaha, Premerlani, Eddy and Lorensen as a method to develop object-oriented systems and to support object-oriented programming. OMT describes object model or static structure of the system.\n\nOMT was developed as an approach to software development. The purposes of modeling according to Rumbaugh are:\n\nOMT has proposed three main types of models:\n\nOMT is a predecessor of the Unified Modeling Language (UML). Many OMT modeling elements are common to UML.\n\nFunctional Model in OMT:\nIn brief, a functional model in OMT defines the function of the whole internal processes in a model with the help of \"Data Flow Diagrams (DFDs)\". It details how processes are performed independently.\n\n\n\nThe model is defined by the organization’s vision, mission, and values, as well as sets of boundaries for the organization—what products or services it will deliver, what customers or markets it will target, and what supply and delivery channels it will use. While the business model includes high-level strategies and tactical direction for how the organization will implement the model, it also includes the annual goals that set the specific steps the organization intends to undertake in the next year and the measures for their expected accomplishment. Each of these is likely to be part of internal documentation that is available to the internal auditor.\n"}
{"id": "26628083", "url": "https://en.wikipedia.org/wiki?curid=26628083", "title": "Omega-categorical theory", "text": "Omega-categorical theory\n\nIn mathematical logic, an omega-categorical theory is a theory that has exactly one countably infinite model up to isomorphism. Omega-categoricity is the special case κ = formula_1 = ω of κ-categoricity, and omega-categorical theories are also referred to as ω-categorical. The notion is most important for countable first-order theories.\n\nMany conditions on a theory are equivalent to the property of omega-categoricity. In 1959 Erwin Engeler, Czesław Ryll-Nardzewski and Lars Svenonius, proved several independently. Despite this, the literature still widely refers to the Ryll-Nardzewski theorem as a name for these conditions. The conditions included with the theorem vary between authors.\n\nGiven a countable complete first-order theory \"T\" with infinite models, the following are equivalent:\n\n"}
{"id": "44903985", "url": "https://en.wikipedia.org/wiki?curid=44903985", "title": "Phenomenological model", "text": "Phenomenological model\n\nA phenomenological model is a scientific model that describes the empirical relationship of phenomena to each other, in a way which is consistent with fundamental theory, but is not directly derived from theory. In other words, a phenomenological model is not derived from first principles. A phenomenological model foregoes any attempt to explain why the variables interact the way they do, and simply attempts to describe the relationship, with the assumption that the relationship extends past the measured values. Regression analysis is sometimes used to create statistical models that serve as phenomenological models.\n\nPhenomenological models have been characterized as being completely independent of theories, though many phenomenological models, while failing to be derivable from a theory, incorporate principles and laws associated with theories. The liquid drop model of the atomic nucleus, for instance, portrays the nucleus as a liquid drop and describes it as having several properties (surface tension and charge, among others) originating in different theories (hydrodynamics and electrodynamics, respectively). Certain aspects of these theories—though usually not the complete theory—are then used to determine both the static and dynamical properties of the nucleus.\n"}
{"id": "23799", "url": "https://en.wikipedia.org/wiki?curid=23799", "title": "Power set", "text": "Power set\n\nIn mathematics, the power set (or powerset) of any set is the set of all subsets of , including the empty set and itself, variously denoted as (), 𝒫(), ℘() (using the \"Weierstrass p\"), , , or, identifying the powerset of with the set of all functions from to a given set of two elements, . In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set.\n\nAny subset of () is called a \"family of sets\" over .\n\nIf is the set , then the subsets of are\n\nand hence the power set of is .\n\nIf is a finite set with elements, then the number of subsets of is . This fact, which is the motivation for the notation , may be demonstrated simply as follows,\n\nCantor's diagonal argument shows that the power set of a set (whether infinite or not) always has strictly higher cardinality than the set itself (informally the power set must be larger than the original set). In particular, Cantor's theorem shows that the power set of a countably infinite set is uncountably infinite. The power set of the set of natural numbers can be put in a one-to-one correspondence with the set of real numbers (see Cardinality of the continuum).\n\nThe power set of a set , together with the operations of union, intersection and complement can be viewed as the prototypical example of a Boolean algebra. In fact, one can show that any \"finite\" Boolean algebra is isomorphic to the Boolean algebra of the power set of a finite set. For \"infinite\" Boolean algebras this is no longer true, but every infinite Boolean algebra can be represented as a subalgebra of a power set Boolean algebra (see Stone's representation theorem).\n\nThe power set of a set forms an abelian group when considered with the operation of symmetric difference (with the empty set as the identity element and each set being its own inverse) and a commutative monoid when considered with the operation of intersection. It can hence be shown (by proving the distributive laws) that the power set considered together with both of these operations forms a Boolean ring.\n\nIn set theory, is the set of all functions from to . As \"2\" can be defined as (see natural number), (i.e., ) is the set of all functions from to {0,1}. By identifying a function in with the corresponding preimage of , we see that there is a bijection between and (), where each function is the characteristic function of the subset in () with which it is identified. Hence and () could be considered identical set-theoretically. (Thus there are two distinct notational motivations for denoting the power set by : the fact that this function-representation of subsets makes it a special case of the notation and the property, mentioned above, that .)\n\nThis notion can be applied to the example above in which to see the isomorphism with the binary numbers\nfrom 0 to with being the number of elements in the set.\nIn , a \"1\" in the position corresponding to the location in the enumerated set indicates the presence of the element. So .\n\nFor the whole power set of we get:\nSuch bijective mapping of \"S\" to integers is arbitrary, so this representation of subsets of \"S\" is not unique, but the sort order of the enumerated set does not change its cardinality.\n\nHowever, such finite binary representation is only possible if \"S\" can be enumerated (this is possible even if \"S\" has an infinite cardinality, such as the set of integers or rationals, but not for example if \"S\" is the set of real numbers, in which we cannot enumerate all irrational numbers to assign them a defined finite location in an ordered set containing all irrational numbers).\n\nThe power set is closely related to the binomial theorem. The number of subsets with elements in the power set of a set with elements is given by the number of combinations, , also called binomial coefficients.\n\nFor example, the power set of a set with three elements, has:\n\nUsing this relationship we can compute formula_3 using the formula:\n\nformula_4\n\nTherefore one can deduce the following identity, assuming formula_5:\n\nformula_6\n\nIf is a finite set, there is a recursive algorithm to calculate ().\n\nDefine the operation }.\n\nIn English, return the set with the element added to each set in .\n\n\nIn other words, the power set of the empty set is the set containing the empty set and the power set of any other set is all the subsets of the set containing some specific element and all the subsets of the set not containing that specific element.\n\nThe set of subsets of of cardinality less than or equal to is sometimes denoted by or , and the set of subsets with cardinality strictly less than is sometimes denoted or . Similarly, the set of non-empty subsets of might be denoted by or .\n\nA set can be regarded as an algebra having no nontrivial operations or defining equations. From this perspective the idea of the power set of as the set of subsets of generalizes naturally to the subalgebras of an algebraic structure or algebra.\n\nNow the power set of a set, when ordered by inclusion, is always a complete atomic Boolean algebra, and every complete atomic Boolean algebra arises as the lattice of all subsets of some set. The generalization to arbitrary algebras is that the set of subalgebras of an algebra, again ordered by inclusion, is always an algebraic lattice, and every algebraic lattice arises as the lattice of subalgebras of some algebra. So in that regard subalgebras behave analogously to subsets.\n\nHowever, there are two important properties of subsets that do not carry over to subalgebras in general. First, although the subsets of a set form a set (as well as a lattice), in some classes it may not be possible to organize the subalgebras of an algebra as itself an algebra in that class, although they can always be organized as a lattice. Secondly, whereas the subsets of a set are in bijection with the functions from that set to the set {0,1} = 2, there is no guarantee that a class of algebras contains an algebra that can play the role of 2 in this way.\n\nCertain classes of algebras enjoy both of these properties. The first property is more common, the case of having both is relatively rare. One class that does have both is that of multigraphs. Given two multigraphs and , a homomorphism consists of two functions, one mapping vertices to vertices and the other mapping edges to edges. The set of homomorphisms from to can then be organized as the graph whose vertices and edges are respectively the vertex and edge functions appearing in that set. Furthermore, the subgraphs of a multigraph are in bijection with the graph homomorphisms from to the multigraph definable as the complete directed graph on two vertices (hence four edges, namely two self-loops and two more edges forming a cycle) augmented with a fifth edge, namely a second self-loop at one of the vertices. We can therefore organize the subgraphs of as the multigraph , called the power object of .\n\nWhat is special about a multigraph as an algebra is that its operations are unary. A multigraph has two sorts of elements forming a set of vertices and of edges, and has two unary operations giving the source (start) and target (end) vertices of each edge. An algebra all of whose operations are unary is called a presheaf. Every class of presheaves contains a presheaf that plays the role for subalgebras that 2 plays for subsets. Such a class is a special case of the more general notion of elementary topos as a category that is closed (and moreover cartesian closed) and has an object , called a subobject classifier. Although the term \"power object\" is sometimes used synonymously with exponential object , in topos theory is required to be .\n\nIn category theory and the theory of elementary topoi, the universal quantifier can be understood as the right adjoint of a functor between power sets, the inverse image functor of a function between sets; likewise, the existential quantifier is the left adjoint.\n\n\n\n"}
{"id": "25360385", "url": "https://en.wikipedia.org/wiki?curid=25360385", "title": "Primal ideal", "text": "Primal ideal\n\nIn mathematics, an element \"a\" of a commutative ring \"A\" is called (relatively) prime to an ideal \"Q\" if whenever \"ab\" is an element of \"Q\" then \"b\" is also an element of \"Q\".\n\nA proper ideal \"Q\" of a commutative ring \"A\" is said to be primal if the elements that are not prime to it form an ideal. \n"}
{"id": "38264067", "url": "https://en.wikipedia.org/wiki?curid=38264067", "title": "Quadratic set", "text": "Quadratic set\n\nIn mathematics, a quadratic set is a set of points in a projective space that bears the same essential incidence properties as a quadric (conic section in a projective plane, sphere or cone or hyperboloid in a projective space).\n\nLet formula_1 be a projective space. A quadratic set is a non-empty subset formula_2 of formula_3 for which the following two conditions hold: \n\nA quadratic set formula_2 is called non-degenerate if for every point formula_16, the set formula_17 is a hyperplane.\n\nA Pappian projective space is a projective space in which Pappus's hexagon theorem holds.\n\nThe following result, due to Francis Buekenhout, is an astonishing statement for finite projective spaces.\n\nOvals and ovoids are special quadratic sets:\nLet formula_29 be a projective space of dimension formula_28. A non-degenerate quadratic set formula_31 that does not contain lines is called ovoid (or oval in plane case).\n\nThe following equivalent definition of an oval/ovoid are more common:\n\nDefinition: (oval)\nA non-empty point set formula_32 of a projective plane is called \noval if the following properties are fulfilled:\nA line formula_4 is a \"exterior\" or \"tangent\" or \"secant\" line of the \noval if formula_39 or formula_40 or formula_41 respectively.\n\nFor \"finite\" planes the following theorem provides a more simple definition.\n\nTheorem: (oval in finite plane) Let be formula_42 a projective plane of order formula_43.\nA set formula_32 of points is an oval if formula_45 and if no three points\nof formula_32 are collinear.\n\nAccording to this theorem of Beniamino Segre, for \"Pappian\" projective planes of \"odd\" order the ovals are just conics:\nTheorem:\nLet be formula_42 a \"Pappian\" projective plane of \"odd\" order.\nAny oval in formula_42 is an oval \"conic\" (non-degenerate quadric).\n\nDefinition: (ovoid)\nA non-empty point set formula_31 of a projective space is called ovoid if the following properties are fulfilled:\n\nExample:\n\nFor \"finite\" projective spaces of dimension formula_43 over a field formula_59 we have:\nTheorem:\n\nCounterexamples (Tits–Suzuki ovoid) show that i.g. statement b) of the theorem above is not true for formula_66:\n\n\n"}
{"id": "701207", "url": "https://en.wikipedia.org/wiki?curid=701207", "title": "Radix", "text": "Radix\n\nIn mathematical numeral systems, the radix or base is the number of unique digits, including the digit zero, used to represent numbers in a positional numeral system. For example, for the decimal system (the most common system in use today) the radix is ten, because it uses the ten digits from 0 through 9.\n\nIn any standard positional numeral system, a number is conventionally written as with \"x\" as the string of digits and \"y\" as its base, although for base ten the subscript is usually assumed (and omitted, together with the pair of parentheses), as it is the most common way to express value. For example, (100) = 100 (in the decimal system) represents the number one hundred, while (100) (in the binary system with base 2) represents the number four.\n\n\"Radix\" is a Latin word for \"root\". \"Root\" can be considered a synonym for \"base\" in the arithmetical sense.\n\nIn the system with radix 13, for example, a string of digits such as 398 denotes the (decimal) number = 632. \n\nMore generally, in a system with radix \"b\" (), a string of digits denotes the number , where . In contrast to decimal, or radix 10, which has a ones' place, tens' place, hundreds' place, and so on, radix \"b\" would have a ones' place, then a \"b\"s' place, a \"b\"s' place, etc.\n\nCommonly used numeral systems include:\n\nThe octal and hexadecimal systems are often used in computing because of their ease as shorthand for binary. Every hexadecimal digit corresponds to a sequence of four binary digits, since sixteen is the fourth power of two; for example, hexadecimal 78 is binary . Similarly, every octal digit corresponds to a unique sequence of three binary digits, since eight is the cube of two.\n\nRadices are usually natural numbers. However, other positional systems are possible; e.g., golden ratio base (whose radix is a non-integer algebraic number), and negative base (whose radix is negative).\n\n\n"}
{"id": "1270875", "url": "https://en.wikipedia.org/wiki?curid=1270875", "title": "Random minimum spanning tree", "text": "Random minimum spanning tree\n\nIn mathematics, a random minimum spanning tree may be formed by assigning random weights from some distribution to the edges of an undirected graph, and then constructing the minimum spanning tree of the graph.\n\nWhen the given graph is a complete graph on vertices, and the edge weights have a continuous distribution function whose derivative at zero is , then the expected weight of its random minimum spanning trees is bounded by a constant, rather than growing as a function of . More precisely, this constant tends in the limit (as goes to infinity) to , where is the Riemann zeta function and is Apéry's constant. For instance, for edge weights that are uniformly distributed on the unit interval, the derivative is , and the limit is just .\n\nRandom minimum spanning trees of grid graphs may be used for invasion percolation models of liquid flow through a porous medium, and for maze generation.\n"}
{"id": "40966773", "url": "https://en.wikipedia.org/wiki?curid=40966773", "title": "Simplicial localization", "text": "Simplicial localization\n\nIn category theory, a branch of mathematics, the simplicial localization of a category \"C\" with respect to a class \"W\" of morphisms of \"C\" is a simplicial category \"LC\" whose formula_1 is the localization formula_2 of \"C\" with respect to \"W\"; that is, formula_3 for any objects \"x\", \"y\" in \"C\". The notion is due to Dwyer and Kan.\n\n"}
{"id": "455770", "url": "https://en.wikipedia.org/wiki?curid=455770", "title": "Spanning tree", "text": "Spanning tree\n\nIn the mathematical field of graph theory, a spanning tree \"T\" of an undirected graph \"G\" is a subgraph that is a tree which includes all of the vertices of \"G\", with minimum possible number of edges. In general, a graph may have several spanning trees, but a graph that is not connected will not contain a spanning tree (but see Spanning forests below). If all of the edges of \"G\" are also edges of a spanning tree \"T\" of \"G\", then \"G\" is a tree and is identical to \"T\" (that is, a tree has a unique spanning tree and it is itself).\n\nSeveral pathfinding algorithms, including Dijkstra's algorithm and the A* search algorithm, internally build a spanning tree as an intermediate step in solving the problem.\n\nIn order to minimize the cost of power networks, wiring connections, piping, automatic speech recognition, etc., people often use algorithms that gradually build a spanning tree (or many such trees) as intermediate steps in the process of finding the minimum spanning tree.\n\nThe Internet and many other telecommunications networks have transmission links that connect nodes together in a mesh topology that includes some loops.\nIn order to \"avoid bridge loops and \"routing loops\", many routing protocols designed for such networks—including the Spanning Tree Protocol, Open Shortest Path First, Link-state routing protocol, Augmented tree-based routing, etc.—require each router to remember a spanning tree.\n\nA special kind of spanning tree, the Xuong tree, is used in topological graph theory to find graph embeddings with maximum genus. A Xuong tree is a spanning tree such that, in the remaining graph, the number of connected components with an odd number of edges is as small as possible. A Xuong tree and an associated maximum-genus embedding can be found in polynomial time.\n\nA tree is a connected undirected graph with no cycles. It is a spanning tree of a graph \"G\" if it spans \"G\" (that is, it includes every vertex of \"G\") and is a subgraph of \"G\" (every edge in the tree belongs to \"G\"). A spanning tree of a connected graph \"G\" can also be defined as a maximal set of edges of \"G\" that contains no cycle, or as a minimal set of edges that connect all vertices.\n\nAdding just one edge to a spanning tree will create a cycle; such a cycle is called a fundamental cycle. There is a distinct fundamental cycle for each edge not in the spanning tree; thus, there is a one-to-one correspondence between fundamental cycles and edges not in the spanning tree. For a connected graph with \"V\" vertices, any spanning tree will have \"V\" − 1 edges, and thus, a graph of \"E\" edges and one of its spanning trees will have \"E\" − \"V\" + 1 fundamental cycles. For any given spanning tree the set of all \"E\" − \"V\" + 1 fundamental cycles forms a cycle basis, a basis for the cycle space.\n\nDual to the notion of a fundamental cycle is the notion of a fundamental cutset. By deleting just one edge of the spanning tree, the vertices are partitioned into two disjoint sets. The fundamental cutset is defined as the set of edges that must be removed from the graph \"G\" to accomplish the same partition. Thus, each spanning tree defines a set of \"V\" − 1 fundamental cutsets, one for each edge of the spanning tree.\n\nThe duality between fundamental cutsets and fundamental cycles is established by noting that cycle edges not in the spanning tree can only appear in the cutsets of the other edges in the cycle; and \"vice versa\": edges in a cutset can only appear in those cycles containing the edge corresponding to the cutset. This duality can also be expressed using the theory of matroids, according to which a spanning tree is a base of the graphic matroid, a fundamental cycle is the unique circuit within the set formed by adding one element to the base, and fundamental cutsets are defined in the same way from the dual matroid.\n\nIn graphs that are not connected, there can be no spanning tree, and one must consider spanning forests instead. Here there are two competing definitions:\nTo avoid confusion between these two definitions, suggest the term \"full spanning forest\" for a spanning forest with the same connectivity as the given graph, while instead call this kind of forest a \"maximal spanning forest\".\n\nThe number \"t\"(\"G\") of spanning trees of a connected graph is a well-studied\ninvariant.\n\nIn some cases, it is easy to calculate \"t\"(\"G\") directly:\n\nMore generally, for any graph \"G\", the number \"t\"(\"G\") can be calculated in polynomial time as the determinant of a matrix derived from the graph,\nusing Kirchhoff's matrix-tree theorem.\n\nSpecifically, to compute \"t\"(\"G\"), one constructs a square matrix in which the rows and columns are both indexed by the vertices of \"G\". The entry in row \"i\" and column \"j\" is one of three values:\nThe resulting matrix is singular, so its determinant is zero. However, deleting the row and column for an arbitrarily chosen vertex leads to a smaller matrix whose determinant is exactly \"t\"(\"G\").\n\nIf \"G\" is a graph or multigraph and \"e\" is an arbitrary edge of \"G\", then the number \"t\"(\"G\") of spanning trees of \"G\" satisfies the \"deletion-contraction recurrence\"\n\"t\"(\"G\") = \"t\"(\"G\" − \"e\") + \"t\"(\"G\"/\"e\"), where \"G\" − \"e\" is the multigraph obtained by deleting \"e\"\nand \"G\"/\"e\" is the contraction of \"G\" by \"e\". The term \"t\"(\"G\" − \"e\") in this formula counts the spanning trees of \"G\" that do not use edge \"e\", and the term \"t\"(\"G\"/\"e\") counts the spanning trees of \"G\" that use \"e\".\n\nIn this formula, if the given graph \"G\" is a multigraph, or if a contraction causes two vertices to be connected to each other by multiple edges,\nthen the redundant edges should not be removed, as that would lead to the wrong total. For instance a bond graph connecting two vertices by \"k\" edges has \"k\" different spanning trees, each consisting of a single one of these edges.\n\nThe Tutte polynomial of a graph can be defined as a sum, over the spanning trees of the graph, of terms computed from the \"internal activity\" and \"external activity\" of the tree. Its value at the arguments (1,1) is the number of spanning trees or, in a disconnected graph, the number of maximal spanning forests.\n\nThe Tutte polynomial can also be computed using a deletion-contraction recurrence, but its computational complexity is high: for many values of its arguments, computing it exactly is #P-complete, and it is also hard to approximate with a guaranteed approximation ratio. The point (1,1), at which it can be evaluated using Kirchhoff's theorem, is one of the few exceptions.\n\nA single spanning tree of a graph can be found in linear time by either depth-first search or breadth-first search. Both of these algorithms explore the given graph, starting from an arbitrary vertex \"v\", by looping through the neighbors of the vertices they discover and adding each unexplored neighbor to a data structure to be explored later. They differ in whether this data structure is a stack (in the case of depth-first search) or a queue (in the case of breadth-first search). In either case, one can form a spanning tree by connecting each vertex, other than the root vertex \"v\", to the vertex from which it was discovered. This tree is known as a depth-first search tree or a breadth-first search tree according to the graph exploration algorithm used to construct it. Depth-first search trees are a special case of a class of spanning trees called Trémaux trees, named after the 19th-century discoverer of depth-first search.\n\nSpanning trees are important in parallel and distributed computing, as a way of maintaining communications between a set of processors; see for instance the Spanning Tree Protocol used by OSI link layer devices or the Shout (protocol) for distributed computing. However, the depth-first and breadth-first methods for constructing spanning trees on sequential computers are not well suited for parallel and distributed computers. Instead, researchers have devised several more specialized algorithms for finding spanning trees in these models of computation.\n\nIn certain fields of graph theory it is often useful to find a minimum spanning tree of a weighted graph. Other optimization problems on spanning trees have also been studied, including the maximum spanning tree, the minimum tree that spans at least k vertices, the spanning tree with the fewest edges per vertex, the spanning tree with the largest number of leaves, the spanning tree with the fewest leaves (closely related to the Hamiltonian path problem), the minimum diameter spanning tree, and the minimum dilation spanning tree.\n\nOptimal spanning tree problems have also been studied for finite sets of points in a geometric space such as the Euclidean plane. For such an input, a spanning tree is again a tree that has as its vertices the given points. The quality of the tree is measured in the same way as in a graph, using the Euclidean distance between pairs of points as the weight for each edge. Thus, for instance, a Euclidean minimum spanning tree is the same as a graph minimum spanning tree in a complete graph with Euclidean edge weights. However, it is not necessary to construct this graph in order to solve the optimization problem; the Euclidean minimum spanning tree problem, for instance, can be solved more efficiently in \"O\"(\"n\" log \"n\") time by constructing the Delaunay triangulation and then applying a linear time planar graph minimum spanning tree algorithm to the resulting triangulation.\n\nA spanning tree chosen randomly from among all the spanning trees with equal probability is called a uniform spanning tree. Wilson's algorithm can be used to generate uniform spanning trees in polynomial time by a process of taking a random walk on the given graph and erasing the cycles created by this walk.\n\nAn alternative model for generating spanning trees randomly but not uniformly is the random minimal spanning tree. In this model, the edges of the graph are assigned random weights and then the minimum spanning tree of the weighted graph is constructed.\n\nBecause a graph may have exponentially many spanning trees, it is not possible to list them all in polynomial time. However, algorithms are known for listing all spanning trees in polynomial time per tree.\n\nEvery finite connected graph has a spanning tree. However, for infinite connected graphs, the existence of spanning trees is equivalent to the axiom of choice. An infinite graph is connected if each pair of its vertices forms the pair of endpoints of a finite path. As with finite graphs, a tree is a connected graph with no finite cycles, and a spanning tree can be defined either as a maximal acyclic set of edges or as a tree that contains every vertex.\n\nThe trees within a graph may be partially ordered by their subgraph relation, and any infinite chain in this partial order has an upper bound (the union of the trees in the chain). Zorn's lemma, one of many equivalent statements to the axiom of choice, requires that a partial order in which all chains are upper bounded have a maximal element; in the partial order on the trees of the graph, this maximal element must be a spanning tree. Therefore, if Zorn's lemma is assumed, every infinite connected graph has a spanning tree.\n\nIn the other direction, given a family of sets, it is possible to construct an infinite graph such that every spanning tree of the graph corresponds to a choice function of the family of sets. Therefore,\nif every infinite connected graph has a spanning tree, then the axiom of choice is true.\n\nThe idea of a spanning tree can be generalized to directed multigraphs. Given a vertex \"v\" on a directed multigraph \"G\", an \"oriented spanning tree\" \"T\" rooted at \"v\" is an acyclic subgraph of \"G\" in which every vertex other than \"v\" has outdegree 1. This definition is only satisfied when the “branches” of \"T\" point towards \"v\".\n\n"}
{"id": "6593472", "url": "https://en.wikipedia.org/wiki?curid=6593472", "title": "Structure theorem for Gaussian measures", "text": "Structure theorem for Gaussian measures\n\nIn mathematics, the structure theorem for Gaussian measures shows that the abstract Wiener space construction is essentially the only way to obtain a strictly positive Gaussian measure on a separable Banach space. It was proved in the 1970s by Kallianpur–Sato–Stefan and Dudley–Feldman–le Cam.\n\nThere is the earlier result due to H. Satô (1969) which proves that \"any Gaussian measure on a separable Banach space is an abstract Wiener measure in the sense of L. Gross\". The result by Dudley et al. generalizes this result to the setting of Gaussian measures on a general topological vector space.\n\nLet \"γ\" be a strictly positive Gaussian measure on a separable Banach space (\"E\", || ||). Then there exists a separable Hilbert space (\"H\", 〈 , 〉) and a map \"i\" : \"H\" → \"E\" such that \"i\" : \"H\" → \"E\" is an abstract Wiener space with \"γ\" = \"i\"(\"γ\"), where \"γ\" is the canonical Gaussian cylinder set measure on \"H\".\n"}
{"id": "1828474", "url": "https://en.wikipedia.org/wiki?curid=1828474", "title": "Subclass (set theory)", "text": "Subclass (set theory)\n\nIn set theory and its applications throughout mathematics, a subclass is a class contained in some other class in the same way that a subset is a set contained in some other set.\n\nThat is, given classes \"A\" and \"B\", \"A\" is a subclass of \"B\" if and only if every member of \"A\" is also a member of \"B\".\nIf \"A\" and \"B\" are sets, then of course \"A\" is also a subset of \"B\".\nIn fact, when using a definition of classes that requires them to be first-order definable, it's enough that \"B\" be a set; the axiom of specification essentially says that \"A\" must then also be a set.\n\nAs with subsets, the empty set is a subclass of every class, and any class is a subclass of itself. But additionally, every class is a subclass of the class of all sets. Accordingly, the subclass relation makes the collection of all classes into a Boolean lattice, which the subset relation does not do for the collection of all sets. Instead, the collection of all sets is an ideal in the collection of all classes. (Of course, the collection of all classes is something larger than even a class!)\n"}
{"id": "27865", "url": "https://en.wikipedia.org/wiki?curid=27865", "title": "Surface (topology)", "text": "Surface (topology)\n\nIn topology, a surface is a two-dimensional manifold. Some surfaces arise as the boundaries of three-dimensional solids; for example, the sphere is the boundary of the solid ball. Other surfaces arise as graphs of functions of two variables; see the figure at right. However, surfaces can also be defined abstractly, without reference to any ambient space. For example, the Klein bottle is a surface that cannot be embedded in three-dimensional Euclidean space.\n\nTopological surfaces are sometimes equipped with additional information, such as a Riemannian metric or a complex structure, that connects them to other disciplines within mathematics, such as differential geometry and complex analysis. The various mathematical notions of surface can be used to model surfaces in the physical world.\n\nIn mathematics, a surface is a geometrical shape that resembles a deformed plane. The most familiar examples arise as boundaries of solid objects in ordinary three-dimensional Euclidean space R, such as spheres. The exact definition of a surface may depend on the context. Typically, in algebraic geometry, a surface may cross itself (and may have other singularities), while, in topology and differential geometry, it may not.\n\nA surface is a two-dimensional space; this means that a moving point on a surface may move in two directions (it has two degrees of freedom). In other words, around almost every point, there is a \"coordinate patch\" on which a two-dimensional coordinate system is defined. For example, the surface of the Earth resembles (ideally) a two-dimensional sphere, and latitude and longitude provide two-dimensional coordinates on it (except at the poles and along the 180th meridian).\n\nThe concept of surface is widely used in physics, engineering, computer graphics, and many other disciplines, primarily in representing the surfaces of physical objects. For example, in analyzing the aerodynamic properties of an airplane, the central consideration is the flow of air along its surface.\n\nA \"(topological) surface\" is a topological space in which every point has an open neighbourhood homeomorphic to some open subset of the Euclidean plane E. Such a neighborhood, together with the corresponding homeomorphism, is known as a \"(coordinate) chart\". It is through this chart that the neighborhood inherits the standard coordinates on the Euclidean plane. These coordinates are known as \"local coordinates\" and these homeomorphisms lead us to describe surfaces as being \"locally Euclidean\".\n\nIn most writings on the subject, it is often assumed, explicitly or implicitly, that as a topological space a surface is also nonempty, second countable, and Hausdorff. It is also often assumed that the surfaces under consideration are connected.\n\nThe rest of this article will assume, unless specified otherwise, that a surface is nonempty, Hausdorff, second countable, and connected.\n\nMore generally, a \"(topological) surface with boundary\" is a Hausdorff topological space in which every point has an open neighbourhood homeomorphic to some open subset of the closure of the upper half-plane H in C. These homeomorphisms are also known as \"(coordinate) charts\". The boundary of the upper half-plane is the \"x\"-axis. A point on the surface mapped via a chart to the \"x\"-axis is termed a \"boundary point\". The collection of such points is known as the \"boundary\" of the surface which is necessarily a one-manifold, that is, the union of closed curves. On the other hand, a point mapped to above the \"x\"-axis is an \"interior point\". The collection of interior points is the \"interior\" of the surface which is always non-empty. The closed disk is a simple example of a surface with boundary. The boundary of the disc is a circle.\n\nThe term \"surface\" used without qualification refers to surfaces without boundary. In particular, a surface with empty boundary is a surface in the usual sense. A surface with empty boundary which is compact is known as a 'closed' surface. The two-dimensional sphere, the two-dimensional torus, and the real projective plane are examples of closed surfaces.\n\nThe Möbius strip is a surface on which the distinction between clockwise and counterclockwise can be defined locally, but not globally. In general, a surface is said to be \"orientable\" if it does not contain a homeomorphic copy of the Möbius strip; intuitively, it has two distinct \"sides\". For example, the sphere and torus are orientable, while the real projective plane is not (because the real projective plane with one point removed is homeomorphic to the open Möbius strip).\n\nIn differential and algebraic geometry, extra structure is added upon the topology of the surface. This added structures can be a smoothness structure (making it possible to define differentiable maps to and from the surface), a Riemannian metric (making it possible to define length and angles on the surface), a complex structure (making it possible to define holomorphic maps to and from the surface — in which case the surface is called a Riemann surface), or an algebraic structure (making it possible to detect singularities, such as self-intersections and cusps, that cannot be described solely in terms of the underlying topology).\n\nHistorically, surfaces were initially defined as subspaces of Euclidean spaces. Often, these surfaces were the locus of zeros of certain functions, usually polynomial functions. Such a definition considered the surface as part of a larger (Euclidean) space, and as such was termed \"extrinsic\".\n\nIn the previous section, a surface is defined as a topological space with certain properties, namely Hausdorff and locally Euclidean. This topological space is not considered a subspace of another space. In this sense, the definition given above, which is the definition that mathematicians use at present, is \"intrinsic\".\n\nA surface defined as intrinsic is not required to satisfy the added constraint of being a subspace of Euclidean space. It may seem possible for some surfaces defined intrinsically to not be surfaces in the extrinsic sense. However, the Whitney embedding theorem asserts every surface can in fact be embedded homeomorphically into Euclidean space, in fact into E: The extrinsic and intrinsic approaches turn out to be equivalent.\n\nIn fact, any compact surface that is either orientable or has a boundary can be embedded in E; on the other hand, the real projective plane, which is compact, non-orientable and without boundary, cannot be embedded into E (see Gramain). Steiner surfaces, including Boy's surface, the Roman surface and the cross-cap, are models of the real projective plane in E, but only the Boy surface is an immersed surface. All these models are singular at points where they intersect themselves.\n\nThe Alexander horned sphere is a well-known pathological embedding of the two-sphere into the three-sphere.\nThe chosen embedding (if any) of a surface into another space is regarded as extrinsic information; it is not essential to the surface itself. For example, a torus can be embedded into E in the \"standard\" manner (which looks like a bagel) or in a knotted manner (see figure). The two embedded tori are homeomorphic, but not isotopic: They are topologically equivalent, but their embeddings are not.\n\nThe image of a continuous, injective function from R to higher-dimensional R is said to be a parametric surface. Such an image is so-called because the \"x\"- and \"y\"- directions of the domain R are 2 variables that parametrize the image. A parametric surface need not be a topological surface. A surface of revolution can be viewed as a special kind of parametric surface.\n\nIf \"f\" is a smooth function from R to R whose gradient is nowhere zero, then the locus of zeros of \"f\" does define a surface, known as an \"implicit surface\". If the condition of non-vanishing gradient is dropped, then the zero locus may develop singularities.\nEach closed surface can be constructed from an oriented polygon with an even number of sides, called a fundamental polygon of the surface, by pairwise identification of its edges. For example, in each polygon below, attaching the sides with matching labels (\"A\" with \"A\", \"B\" with \"B\"), so that the arrows point in the same direction, yields the indicated surface.\n\nAny fundamental polygon can be written symbolically as follows. Begin at any vertex, and proceed around the perimeter of the polygon in either direction until returning to the starting vertex. During this traversal, record the label on each edge in order, with an exponent of -1 if the edge points opposite to the direction of traversal. The four models above, when traversed clockwise starting at the upper left, yield\n\nNote that the sphere and the projective plane can both be realized as quotients of the 2-gon, while the torus and Klein bottle require a 4-gon (square).\n\nThe expression thus derived from a fundamental polygon of a surface turns out to be the sole relation in a presentation of the fundamental group of the surface with the polygon edge labels as generators. This is a consequence of the Seifert–van Kampen theorem.\n\nGluing edges of polygons is a special kind of quotient space process. The quotient concept can be applied in greater generality to produce new or alternative constructions of surfaces. For example, the real projective plane can be obtained as the quotient of the sphere by identifying all pairs of opposite points on the sphere. Another example of a quotient is the connected sum.\n\nThe connected sum of two surfaces \"M\" and \"N\", denoted \"M\" # \"N\", is obtained by removing a disk from each of them and gluing them along the boundary components that result. The boundary of a disk is a circle, so these boundary components are circles. The Euler characteristic formula_5 of is the sum of the Euler characteristics of the summands, minus two:\n\nThe sphere S is an identity element for the connected sum, meaning that . This is because deleting a disk from the sphere leaves a disk, which simply replaces the disk deleted from \"M\" upon gluing.\n\nConnected summation with the torus T is also described as attaching a \"handle\" to the other summand \"M\". If \"M\" is orientable, then so is . The connected sum is associative, so the connected sum of a finite collection of surfaces is well-defined.\n\nThe connected sum of two real projective planes, , is the Klein bottle K. The connected sum of the real projective plane and the Klein bottle is homeomorphic to the connected sum of the real projective plane with the torus; in a formula, . Thus, the connected sum of three real projective planes is homeomorphic to the connected sum of the real projective plane with the torus. Any connected sum involving a real projective plane is nonorientable.\n\nA closed surface is a surface that is compact and without boundary. Examples are spaces like the sphere, the torus and the Klein bottle. Examples of non-closed surfaces are: an open disk, which is a sphere with a puncture; a cylinder, which is a sphere with two punctures; and the Möbius strip. As with any closed manifold, a surface embedded in Euclidean space that is closed with respect to the inherited Euclidean topology is \"not\" necessarily a closed surface; for example, a disk embedded in formula_7 that contains its boundary is a surface that is topologically closed, but not a closed surface.\n\nThe \"classification theorem of closed surfaces\" states that any connected closed surface is homeomorphic to some member of one of these three families:\n\nThe surfaces in the first two families are orientable. It is convenient to combine the two families by regarding the sphere as the connected sum of 0 tori. The number \"g\" of tori involved is called the \"genus\" of the surface. The sphere and the torus have Euler characteristics 2 and 0, respectively, and in general the Euler characteristic of the connected sum of \"g\" tori is .\n\nThe surfaces in the third family are nonorientable. The Euler characteristic of the real projective plane is 1, and in general the Euler characteristic of the connected sum of \"k\" of them is .\n\nIt follows that a closed surface is determined, up to homeomorphism, by two pieces of information: its Euler characteristic, and whether it is orientable or not. In other words, Euler characteristic and orientability completely classify closed surfaces up to homeomorphism.\n\nClosed surfaces with multiple connected components are classified by the class of each of their connected components, and thus one generally assumes that the surface is connected.\n\nRelating this classification to connected sums, the closed surfaces up to homeomorphism form a commutative monoid under the operation of connected sum, as indeed do manifolds of any fixed dimension. The identity is the sphere, while the real projective plane and the torus generate this monoid, with a single relation , which may also be written , since . This relation is sometimes known as ' after Walther von Dyck, who proved it in , and the triple cross surface is accordingly called '.\n\nGeometrically, connect-sum with a torus () adds a handle with both ends attached to the same side of the surface, while connect-sum with a Klein bottle () adds a handle with the two ends attached to opposite sides of an orientable surface; in the presence of a projective plane (), the surface is not orientable (there is no notion of side), so there is no difference between attaching a torus and attaching a Klein bottle, which explains the relation.\n\nCompact surfaces, possibly with boundary, are simply closed surfaces with a finite number of holes (open discs that have been removed). Thus, a connected compact surface is classified by the number of boundary components and the genus of the corresponding closed surface – equivalently, by the number of boundary components, the orientability, and Euler characteristic. The genus of a compact surface is defined as the genus of the corresponding closed surface.\n\nThis classification follows almost immediately from the classification of closed surfaces: removing an open disc from a closed surface yields a compact surface with a circle for boundary component, and removing \"k\" open discs yields a compact surface with \"k\" disjoint circles for boundary components. The precise locations of the holes are irrelevant, because the homeomorphism group acts \"k\"-transitively on any connected manifold of dimension at least 2.\n\nConversely, the boundary of a compact surface is a closed 1-manifold, and is therefore the disjoint union of a finite number of circles; filling these circles with disks (formally, taking the cone) yields a closed surface.\n\nThe unique compact orientable surface of genus \"g\" and with \"k\" boundary components is often denoted formula_10 for example in the study of the mapping class group.\n\nA Riemann surface is a complex 1-manifold. On a purely topological level, a Riemann surface is therefore also an orientable surface in the sense of this article. In fact, every compact orientable surface is realizable as a Riemann surface. Thus compact Riemann surfaces are characterized topologically by their genus: 0, 1, 2, ... On the other hand, the genus does not characterize the complex structure. For example, there uncountably many non-isomorphic compact Riemann surfaces of genus 1 (the elliptic curves).\n\nNon-compact surfaces are more difficult to classify. As a simple example, a non-compact surface can be obtained by puncturing (removing a finite set of points from) a closed manifold. On the other hand, any open subset of a compact surface is itself a non-compact surface; consider, for example, the complement of a Cantor set in the sphere, otherwise known as the Cantor tree surface. However, not every non-compact surface is a subset of a compact surface; two canonical counterexamples are the Jacob's ladder and the Loch Ness monster, which are non-compact surfaces with infinite genus.\n\nA non-compact surface M has a non-empty space of ends E(M), which informally speaking describes the ways that the surface \"goes off to infinity\". The space E(M) is always topologically equivalent to a closed subspace of the Cantor set. M may have a finite or countably infinite number N of handles, as well as a finite or countably infinite number N of projective planes. If both N and N are finite, then these two numbers, and the topological type of space of ends, classify the surface M up to topological equivalence. If either or both of N and N is infinite, then the topological type of M depends not only on these two numbers but also on how the infinite one(s) approach the space of ends. In general the topological type of M is determined by the four subspaces of E(M) that are limit points of infinitely many handles and infinitely many projective planes, limit points of only handles, limit points of only projective planes, and limit points of neither.\n\nIf one removes the assumption of second countability from the definition of a surface, there exist (necessarily non-compact) topological surfaces having no countable base for their topology. Perhaps the simplest example is the Cartesian product of the long line with the space of real numbers.\n\nAnother surface having no countable base for its topology, but \"not\" requiring the Axiom of Choice to prove its existence, is the Prüfer manifold, which can be described by simple equations that show it to be a real-analytic surface. The Prüfer manifold may be thought of as the upper half plane together with one additional \"tongue\" \"T\" hanging down from it directly below the point (\"x\",0), for each real \"x\".\n\nIn 1925, Tibor Radó proved the theorem all Riemann surfaces (i.e., one-dimensional complex manifolds) are necessarily second countable. By contrast, if one replaces the real numbers in the construction of the Prüfer surface by the complex numbers, one obtains a two-dimensional complex manifold (which is necessarily a 4-dimensional real manifold) with no countable base.\n\nThe classification of closed surfaces has been known since the 1860s, and today a number of proofs exist.\n\nTopological and combinatorial proofs in general rely on the difficult result that every compact 2-manifold is homeomorphic to a simplicial complex, which is of interest in its own right. The most common proof of the classification is , which brings every triangulated surface to a standard form. A simplified proof, which avoids a standard form, was discovered by John H. Conway circa 1992, which he called the \"Zero Irrelevancy Proof\" or \"ZIP proof\" and is presented in .\n\nA geometric proof, which yields a stronger geometric result, is the uniformization theorem. This was originally proven only for Riemann surfaces in the 1880s and 1900s by Felix Klein, Paul Koebe, and Henri Poincaré.\n\nPolyhedra, such as the boundary of a cube, are among the first surfaces encountered in geometry. It is also possible to define \"smooth surfaces\", in which each point has a neighborhood diffeomorphic to some open set in E. This elaboration allows calculus to be applied to surfaces to prove many results.\n\nTwo smooth surfaces are diffeomorphic if and only if they are homeomorphic. (The analogous result does not hold for higher-dimensional manifolds.) Thus closed surfaces are classified up to diffeomorphism by their Euler characteristic and orientability.\n\nSmooth surfaces equipped with Riemannian metrics are of foundational importance in differential geometry. A Riemannian metric endows a surface with notions of geodesic, distance, angle, and area. It also gives rise to Gaussian curvature, which describes how curved or bent the surface is at each point. Curvature is a rigid, geometric property, in that it is not preserved by general diffeomorphisms of the surface. However, the famous Gauss–Bonnet theorem for closed surfaces states that the integral of the Gaussian curvature \"K\" over the entire surface \"S\" is determined by the Euler characteristic:\nThis result exemplifies the deep relationship between the geometry and topology of surfaces (and, to a lesser extent, higher-dimensional manifolds).\n\nAnother way in which surfaces arise in geometry is by passing into the complex domain. A complex one-manifold is a smooth oriented surface, also called a Riemann surface. Any complex nonsingular algebraic curve viewed as a complex manifold is a Riemann surface.\n\nEvery closed orientable surface admits a complex structure. Complex structures on a closed oriented surface correspond to conformal equivalence classes of Riemannian metrics on the surface. One version of the uniformization theorem (due to Poincaré) states that any Riemannian metric on an oriented, closed surface is conformally equivalent to an essentially unique metric of constant curvature. This provides a starting point for one of the approaches to Teichmüller theory, which provides a finer classification of Riemann surfaces than the topological one by Euler characteristic alone.\n\nA \"complex surface\" is a complex two-manifold and thus a real four-manifold; it is not a surface in the sense of this article. Neither are algebraic curves defined over fields other than the complex numbers,\nnor are algebraic surfaces defined over fields other than the real numbers.\n\n\n\n"}
{"id": "9092737", "url": "https://en.wikipedia.org/wiki?curid=9092737", "title": "Third derivative", "text": "Third derivative\n\nIn calculus, a branch of mathematics, the third derivative is the rate at which the second derivative, or the rate of change of the rate of change, is changing, used to define aberrancy. The third derivative of a function formula_1 can be denoted by\n\nOther notations can be used, but the above are the most common.\n\nLet formula_3. Then formula_4, and formula_5. Therefore, the third derivative of \"f\"(\"x\") is, in this case,\n\nor, using Leibniz notation,\n\nNow for a more general definition. Let formula_8 be any function of \"x\". Then the third derivative of formula_8 is given by the following:\n\nThe third derivative is the rate at which the second derivative (\"<nowiki>f\"(x)</nowiki>\") is changing.\n\nIn differential geometry, the torsion of a curve — a fundamental property of curves in three dimensions — is computed using third derivatives of coordinate functions (or the position vector) describing the curve.\n\nIn physics, particularly kinematics, jerk is defined as the third derivative of the position function of an object. It is, essentially, the rate at which acceleration changes. In mathematical terms:\n\nwhere j(\"t\") is the jerk function with respect to time, and r(\"t\") is the position function of the object with respect to time.\n\nU.S. President Richard Nixon, when campaigning for a second term in office announced that the rate of increase of inflation was decreasing, which has been noted as \"the first time a sitting president used the third derivative to advance his case for reelection.\" Since inflation is itself a derivative—the rate at which the purchasing power of money decreases—then the rate of increase of inflation is the derivative of inflation, or the second derivative of the function of purchasing power of money with respect to time. Stating that a function is decreasing is equivalent to stating that its derivative is negative, so Nixon's statement is that the second derivative of inflation—or the third derivative of purchasing power—is negative.\n\nNixon's statement allowed for the rate of inflation to increase, however, so his statement was not as indicative of stable prices as it sounds.\n\n"}
{"id": "638899", "url": "https://en.wikipedia.org/wiki?curid=638899", "title": "Vertex (graph theory)", "text": "Vertex (graph theory)\n\nIn mathematics, and more specifically in graph theory, a vertex (plural vertices) or node is the fundamental unit of which graphs are formed: an undirected graph consists of a set of vertices and a set of edges (unordered pairs of vertices), while a directed graph consists of a set of vertices and a set of arcs (ordered pairs of vertices). In a diagram of a graph, a vertex is usually represented by a circle with a label, and an edge is represented by a line or arrow extending from one vertex to another.\n\nFrom the point of view of graph theory, vertices are treated as featureless and indivisible objects, although they may have additional structure depending on the application from which the graph arises; for instance, a semantic network is a graph in which the vertices represent concepts or classes of objects.\n\nThe two vertices forming an edge are said to be the endpoints of this edge, and the edge is said to be incident to the vertices. A vertex \"w\" is said to be adjacent to another vertex \"v\" if the graph contains an edge (\"v\",\"w\"). The neighborhood of a vertex \"v\" is an induced subgraph of the graph, formed by all vertices adjacent to \"v\".\n\nThe degree of a vertex, denoted 𝛿(v) in a graph is the number of edges incident to it. An isolated vertex is a vertex with degree zero; that is, a vertex that is not an endpoint of any edge (the example image illustrates one isolated vertex). A leaf vertex (also pendant vertex) is a vertex with degree one. In a directed graph, one can distinguish the outdegree (number of outgoing edges), denoted 𝛿(v), from the indegree (number of incoming edges), denoted 𝛿(v); a source vertex is a vertex with indegree zero, while a sink vertex is a vertex with outdegree zero. A simplicial vertex is one whose neighbors form a clique: every two neighbors are adjacent. A universal vertex is a vertex that is adjacent to every other vertex in the graph.\n\nA cut vertex is a vertex the removal of which would disconnect the remaining graph; a vertex separator is a collection of vertices the removal of which would disconnect the remaining graph into small pieces. A k-vertex-connected graph is a graph in which removing fewer than \"k\" vertices always leaves the remaining graph connected. An independent set is a set of vertices no two of which are adjacent, and a vertex cover is a set of vertices that includes at least one endpoint of each edge in the graph. The vertex space of a graph is a vector space having a set of basis vectors corresponding with the graph's vertices.\n\nA graph is vertex-transitive if it has symmetries that map any vertex to any other vertex. In the context of graph enumeration and graph isomorphism it is important to distinguish between labeled vertices and unlabeled vertices. A labeled vertex is a vertex that is associated with extra information that enables it to be distinguished from other labeled vertices; two graphs can be considered isomorphic only if the correspondence between their vertices pairs up vertices with equal labels. An unlabeled vertex is one that can be substituted for any other vertex based only on its adjacencies in the graph and not based on any additional information.\n\nVertices in graphs are analogous to, but not the same as, vertices of polyhedra: the skeleton of a polyhedron forms a graph, the vertices of which are the vertices of the polyhedron, but polyhedron vertices have additional structure (their geometric location) that is not assumed to be present in graph theory. The vertex figure of a vertex in a polyhedron is analogous to the neighborhood of a vertex in a graph.\n\n\n"}
{"id": "58126837", "url": "https://en.wikipedia.org/wiki?curid=58126837", "title": "Yael Tauman Kalai", "text": "Yael Tauman Kalai\n\nYael Tauman Kalai is a cryptographer and theoretical computer scientist who works as a Principal Researcher at Microsoft Research New England and as an Adjunct Professor at MIT in the Computer Science and Artificial Intelligence Lab.\n\nKalai graduated from the Hebrew University of Jerusalem in 1997. She worked with Adi Shamir at the Weizmann Institute of Science, earning a master's degree there in 2001, and then moved to the Massachusetts Institute of Technology, where she completed her PhD in 2006 with Shafi Goldwasser as her doctoral advisor. She did postdoctoral study at Microsoft Research and the Weizmann Institute before becoming a faculty member at the Georgia Institute of Technology. She took a permanent position at Microsoft Research in 2008.\n\nKalai is known for co-inventing ring signatures, which has become a key component of numerous systems such as Cryptonote and Monero (cryptocurrency). Subsequently, together with her advisor Shafi Goldwasser, she demonstrated an insecurity in the widely used Fiat–Shamir heuristic. Her work on delegating computation has applications to cloud computing.\n\nKalai was an invited speaker on mathematical aspects of computer science at the 2018 International Congress of Mathematicians.\n\nHer master's thesis introducing ring signatures won an outstanding master's thesis award and MIT PhD dissertation was awarded the George M. Sprowls Award for Outstanding PhD Thesis in Computer Science.\n\nShe was co-chair of the Theory of Cryptography Conference in 2017.\nKalai is the daughter of game theorist Yair Tauman. Her husband, Adam Tauman Kalai, also works at Microsoft Research.\n"}
{"id": "672202", "url": "https://en.wikipedia.org/wiki?curid=672202", "title": "Yang–Mills theory", "text": "Yang–Mills theory\n\nYang–Mills theory is a gauge theory based on the SU(\"N\") group, or more generally any compact, reductive Lie algebra. Yang–Mills theory seeks to describe the behavior of elementary particles using these non-abelian Lie groups and is at the core of the unification of the electromagnetic force and weak forces (i.e. U(1) × SU(2)) as well as quantum chromodynamics, the theory of the strong force (based on SU(3)). Thus it forms the basis of our understanding of the Standard Model of particle physics.\n\nIn a private correspondence, Wolfgang Pauli formulated in 1953 a six-dimensional theory of Einstein's field equations of general relativity, extending the five-dimensional theory of Kaluza, Klein, Fock and others to a higher-dimensional internal space. However, there is no evidence that Pauli developed the Lagrangian of a gauge field or the quantization of it. Because Pauli found that his theory \"leads to some rather unphysical shadow particles\", he refrained from publishing his results formally. Although Pauli did not publish his six-dimensional theory, he gave two talks about it in Zürich. Recent research shows that an extended Kaluza–Klein theory is in general not equivalent to Yang–Mills theory, as the former contains additional terms.\n\nIn early 1954, Chen Ning Yang and Robert Mills extended the concept of gauge theory for abelian groups, e.g. quantum electrodynamics, to nonabelian groups to provide an explanation for strong interactions. The idea by Yang–Mills was criticized by Pauli, as the quanta of the Yang–Mills field must be massless in order to maintain gauge invariance. The idea was set aside until 1960, when the concept of particles acquiring mass through symmetry breaking in massless theories was put forward, initially by Jeffrey Goldstone, Yoichiro Nambu, and Giovanni Jona-Lasinio.\n\nThis prompted a significant restart of Yang–Mills theory studies that proved successful in the formulation of both electroweak unification and quantum chromodynamics (QCD). The electroweak interaction is described by SU(2) × U(1) group while QCD is an SU(3) Yang–Mills theory. The electroweak theory is obtained by combining SU(2) with U(1), where quantum electrodynamics (QED) is described by a U(1) group, and is replaced in the unified electroweak theory by a U(1) group representing a weak hypercharge rather than electric charge. The massless bosons from the SU(2) × U(1) theory mix after spontaneous symmetry breaking to produce the 3 massive weak bosons, and the photon field. The Standard Model combines the strong interaction with the unified electroweak interaction (unifying the weak and electromagnetic interaction) through the symmetry group SU(2) × U(1) × SU(3). In the current epoch the strong interaction is not unified with the electroweak interaction, but from the observed running of the coupling constants it is believed they all converge to a single value at very high energies.\n\nPhenomenology at lower energies in quantum chromodynamics is not completely understood due to the difficulties of managing such a theory with a strong coupling. This may be the reason why confinement has not been theoretically proven, though it is a consistent experimental observation. Proof that QCD confines at low energy is a mathematical problem of great relevance, and an award has been proposed by the Clay Mathematics Institute for whoever is also able to show that the Yang–Mills theory has a mass gap and its existence.\n\nYang–Mills theories are a special example of gauge theory with a non-abelian symmetry group given by the Lagrangian\n\nwith the generators of the Lie algebra, indexed by , corresponding to the \"F\"-quantities (the curvature or field-strength form) satisfying\n\nwhere the are structure constants of the Lie algebra, and the covariant derivative defined as\n\nwhere is the identity matrix (matching the size of the generators), formula_4 is the vector potential, and \"g\" is the coupling constant. In four dimensions, the coupling constant \"g\" is a pure number and for a SU(\"N\") group one has formula_5\n\nThe relation\n\ncan be derived by the commutator\n\nThe field has the property of being self-interacting and equations of motion that one obtains are said to be semilinear, as nonlinearities are both with and without derivatives. This means that one can manage this theory only by perturbation theory, with small nonlinearities.\n\nNote that the transition between \"upper\" (\"contravariant\") and \"lower\" (\"covariant\") vector or tensor components is trivial for \"a\" indices (e.g. formula_8), whereas for μ and ν it is nontrivial, corresponding e.g. to the usual Lorentz signature, formula_9.\n\nFrom the given Lagrangian one can derive the equations of motion given by\n\nPutting formula_11, these can be rewritten as\n\nA Bianchi identity holds\n\nwhich is equivalent to the Jacobi identity\n\nsince formula_15. Define the dual strength tensor\nformula_16, then the Bianchi identity can be rewritten as\n\nA source formula_18 enters into the equations of motion as\n\nNote that the currents must properly change under gauge group transformations.\n\nWe give here some comments about the physical dimensions of the coupling. In \"D\" dimensions, the field scales as formula_20 and so the coupling must scale as formula_21. This implies that Yang–Mills theory is not renormalizable for dimensions greater than four. Furthermore, for \"D\" = 4, the coupling is dimensionless and both the field and the square of the coupling have the same dimensions of the field and the coupling of a massless quartic scalar field theory. So, these theories share the scale invariance at the classical level.\n\nA method of quantizing the Yang–Mills theory is by functional methods, i.e. path integrals. One introduces a generating functional for \"n\"-point functions as\n\nbut this integral has no meaning as it is because the potential vector can be arbitrarily chosen due to the gauge freedom. This problem was already known for quantum electrodynamics but here becomes more severe due to non-abelian properties of the gauge group. A way out has been given by Ludvig Faddeev and Victor Popov with the introduction of a ghost field (see Faddeev–Popov ghost) that has the property of being unphysical since, although it agrees with Fermi–Dirac statistics, it is a complex scalar field, which violates the spin–statistics theorem. So, we can write the generating functional as\n\nbeing\n\nfor the field,\n\nfor the gauge fixing and\n\nfor the ghost. This is the expression commonly used to derive Feynman's rules (see Feynman diagram). Here we have \"c\" for the ghost field while ξ fixes the gauge's choice for the quantization. Feynman's rules obtained from this functional are the following\n\nThese rules for Feynman diagrams can be obtained when the generating functional given above is rewritten as\n\nwith\n\nbeing the generating functional of the free theory. Expanding in \"g\" and computing the functional derivatives, we are able to obtain all the \"n\"-point functions with perturbation theory. Using LSZ reduction formula we get from the \"n\"-point functions the corresponding process amplitudes, cross sections and decay rates. The theory is renormalizable and corrections are finite at any order of perturbation theory.\n\nFor quantum electrodynamics the ghost field decouples because the gauge group is abelian. This can be seen from the coupling between the gauge field and the ghost field that is formula_29. For the abelian case, all the structure constants formula_30 are zero and so there is no coupling. In the non-abelian case, the ghost field appears as a useful way to rewrite the quantum field theory without physical consequences on the observables of the theory such as cross sections or decay rates.\n\nOne of the most important results obtained for Yang–Mills theory is asymptotic freedom. This result can be obtained by assuming that the coupling constant \"g\" is small (so small nonlinearities), as for high energies, and applying perturbation theory. The relevance of this result is due to the fact that a Yang–Mills theory that describes strong interaction and asymptotic freedom permits proper treatment of experimental results coming from deep inelastic scattering.\n\nTo obtain the behavior of the Yang–Mills theory at high energies, and so to prove asymptotic freedom, one applies perturbation theory assuming a small coupling. This is verified a posteriori in the ultraviolet limit. In the opposite limit, the infrared limit, the situation is the opposite, as the coupling is too large for perturbation theory to be reliable. Most of the difficulties that research meets is just managing the theory at low energies. That is the interesting case, being inherent to the description of hadronic matter and, more generally, to all the observed bound states of gluons and quarks and their confinement (see hadrons). The most used method to study the theory in this limit is to try to solve it on computers (see lattice gauge theory). In this case, large computational resources are needed to be sure the correct limit of infinite volume (smaller lattice spacing) is obtained. This is the limit the results must be compared with. Smaller spacing and larger coupling are not independent of each other, and larger computational resources are needed for each. As of today, the situation appears somewhat satisfactory for the hadronic spectrum and the computation of the gluon and ghost propagators, but the glueball and hybrids spectra are yet a questioned matter in view of the experimental observation of such exotic states. Indeed, the σ resonance is not seen in any of such lattice computations and contrasting interpretations have been put forward. This is a hotly debated issue.\n\nYang–Mills theories met with general acceptance in the physics community after Gerard 't Hooft, in 1972, worked out their renormalization, relying on a formulation of the problem worked out by his advisor Martinus Veltman. (Their work was recognized by the 1999 Nobel prize in physics.) Renormalizability is obtained even if the gauge bosons described by this theory are massive, as in the electroweak theory, provided the mass is only an \"acquired\" one, generated by the Higgs mechanism.\n\nThe mathematics of the Yang–Mills theory is a very active field of research, yielding e.g. invariants of differentiable structures on four-dimensional manifolds via work of Simon Donaldson. Furthermore, the field of Yang–Mills theories was included in the Clay Mathematics Institute's list of \"Millennium Prize Problems\". Here the prize-problem consists, especially, in a proof of the conjecture that the lowest excitations of a pure Yang–Mills theory (i.e. without matter fields) have a finite mass-gap with regard to the vacuum state. Another open problem, connected with this conjecture, is a proof of the confinement property in the presence of additional Fermion particles.\n\nIn physics the survey of Yang–Mills theories does not usually start from perturbation analysis or analytical methods, but more recently from systematic application of numerical methods to lattice gauge theories.\n\n\n\n\n"}
