{"id": "1354812", "url": "https://en.wikipedia.org/wiki?curid=1354812", "title": "A Million Random Digits with 100,000 Normal Deviates", "text": "A Million Random Digits with 100,000 Normal Deviates\n\nA Million Random Digits with 100,000 Normal Deviates is a random number book by the RAND Corporation, originally published in 1955. The book, consisting primarily of a random number table, was an important 20th century work in the field of statistics and random numbers. It was produced starting in 1947 by an electronic simulation of a roulette wheel attached to a computer, the results of which were then carefully filtered and tested before being used to generate the table. The RAND table was an important breakthrough in delivering random numbers, because such a large and carefully prepared table had never before been available. In addition to being available in book form, one could also order the digits on a series of punched cards.\n\nThe table is formatted as 400 pages, each containing 50 lines of 50 digits. Columns and lines are grouped in fives, and the lines are numbered 00000 through 19999. The standard normal deviates are another 200 pages (10 per line, lines 0000 through 9999), with each deviate given to three decimal places. There are 28 additional pages of front matter.\n\nThe main use of the tables was in statistics and the experimental design of scientific experiments, especially those that used the Monte Carlo method; in cryptography, they have also been used as \"nothing up my sleeve numbers,\" for example in the design of the Khafre cipher. The book was one of the last of a series of random number tables produced from the mid-1920s to the 1950s, after which the development of high-speed computers allowed faster operation through the generation of pseudorandom numbers rather than reading them from tables.\n\nThe book was reissued in 2001 () with a new foreword by RAND Executive Vice President Michael D. Rich. It has generated many humorous user reviews on Amazon.com.\n\nThe digits begin:\n10097 32533  76520 13586  34673 54876  80959 09117  39292 74945\n\n\n"}
{"id": "41893091", "url": "https://en.wikipedia.org/wiki?curid=41893091", "title": "Alessandro Faedo", "text": "Alessandro Faedo\n\nAlessandro Faedo (18 November 1913 – 15 June 2001) (also known as Alessandro Carlo Faedo or Sandro Faedo) was an Italian mathematician and politician, born in Chiampo. He is known for his work in numerical analysis, leading to the Faedo–Galerkin method: he was one of the pupils of Leonida Tonelli and, after his death, he succeeded him on the chair of mathematical analysis at the University of Pisa, becoming dean of the faculty of sciences and then rector and exerting a strong positive influence on the development of the university.\n\n\n\n\n\n"}
{"id": "49253005", "url": "https://en.wikipedia.org/wiki?curid=49253005", "title": "Alfred Foster (mathematician)", "text": "Alfred Foster (mathematician)\n\nAlfred Leon Foster (known as Alfred Foster) was an American mathematician. He was a professor at the University of California, Berkeley from 1934 until 1971. In 1932 he was an Invited Speaker at the ICM in Zürich.\n\nIn 1934 he accepted a regular position at Berkeley. At that time Griffith Evans was Head of the Mathematics Department and was charged by President Sproul with building a first-class mathematics center, which he did. Alfred Foster and Charles Morrey (who became the first department chairman after Evans' retirement) were Evans' first two appointments. Except for subsequent sabbatical leaves, spent most notably in Freiburg and Tübingen, Foster served continuously at Berkeley until his retirement at the then-mandatory age of 67 in 1971.\n\nFoster's Ph.D. dissertation and his first few papers were in the area of mathematical logic. Starting from this point, he soon focused his interest on the related theory of Boolean algebras and Boolean rings, and was thus led from logic to algebra. He extensively studied the role of duality in Boolean theory and subsequently developed a theory of n-ality for certain rings which played for n-valued logics the role of Boolean rings vis-a-vis Boolean algebras. The late Benjamin Bernstein of the Berkeley mathematics faculty was his collaborator in some of this research. This work culminated in his seminal paper “The theory of Boolean-like rings” appearing in 1946.\n\nFoster was married to Else Wagner; their marriage produced four children and eight grandchildren.\n\n"}
{"id": "6021605", "url": "https://en.wikipedia.org/wiki?curid=6021605", "title": "August Adler", "text": "August Adler\n\nAugust Adler (24 January 1863, Opava, Austrian Silesia – 17 October 1923, Vienna) was a Czech and Austrian mathematician noted for using the theory of inversion to provide an alternate proof of Mascheroni's compass and straightedge construction theorem.\n"}
{"id": "73242", "url": "https://en.wikipedia.org/wiki?curid=73242", "title": "Birthday problem", "text": "Birthday problem\n\nIn probability theory, the birthday problem or birthday paradox concerns the probability that, in a set of randomly chosen people, some pair of them will have the same birthday. By the pigeonhole principle, the probability reaches 100% when the number of people reaches 367 (since there are only 366 possible birthdays, including February 29). However, 99.9% probability is reached with just 70 people, and 50% probability with 23 people. These conclusions are based on the assumption that each day of the year (excluding February 29) is equally probable for a birthday.\n\nIt may well seem surprising that a group of just 23 individuals is required to reach a probability of 50% that two individuals in the group have the the same birthday: this result is perhaps made more plausible by considering that the comparisons of birthday will actually be made between every possible pair of individuals = 23 × 22/2 = 253 comparisons, which is well over half the number of days in year (183 at most), as opposed to fixing on one individual and comparing their birthday to everyone else's birthday. \n\nReal-world applications for the birthday paradox include a cryptographic attack called the birthday attack, which uses this probabilistic model to reduce the complexity of finding a collision for a hash function.\n\nThe history of the problem is obscure. W. W. Rouse Ball indicated (without citation) that it was first discussed by Harold Davenport. However, Richard von Mises proposed an earlier version of what is considered today to be the birthday problem. The problem was featured by Martin Gardner in his April 1957 \"Mathematical Games\" column in \"Scientific American\".\n\nThe problem is to compute an approximate probability that in a group of people at least two have the same birthday. For simplicity, variations in the distribution, such as leap years, twins, seasonal or weekday variations are disregarded, and it is assumed that all 365 possible birthdays are equally likely. (Real-life birthday distributions are not uniform, since not all dates are equally likely, but these irregularities have little effect on the analysis.)\n\nThe goal is to compute , the probability that at least two people in the room have the same birthday. However, it is simpler to calculate , the probability that no two people in the room have the same birthday. Then, because and are the only two possibilities and are also mutually exclusive, \n\nIn deference to widely published solutions concluding that 23 is the minimum number of people necessary to have a that is greater than 50%, the following calculation of will use 23 people as an example. If one numbers the 23 people from 1 to 23, the event that all 23 people have different birthdays is the same as the event that person 2 does not have the same birthday as person 1, and that person 3 does not have the same birthday as either person 1 or person 2, and so on, and finally that person 23 does not have the same birthday as any of persons 1 through 22. Let these events respectively be called \"Event 2\", \"Event 3\", and so on. One may also add an \"Event 1\", corresponding to the event of person 1 having a birthday, which occurs with probability 1. This conjunction of events may be computed using conditional probability: the probability of Event 2 is 364/365, as person 2 may have any birthday other than the birthday of person 1. Similarly, the probability of Event 3 given that Event 2 occurred is 363/365, as person 3 may have any of the birthdays not already taken by persons 1 and 2. This continues until finally the probability of Event 23 given that all preceding events occurred is 343/365. Finally, the principle of conditional probability implies that is equal to the product of these individual probabilities:\nThe terms of equation () can be collected to arrive at:\nEvaluating equation () gives \n\nTherefore,  (50.7297%).\n\nThis process can be generalized to a group of people, where is the probability of at least two of the people sharing a birthday. It is easier to first calculate the probability that all birthdays are \"different\". According to the pigeonhole principle, is zero when . When :\n\nwhere is the factorial operator, is the binomial coefficient and denotes permutation.\n\nThe equation expresses the fact that the first person has no one to share a birthday, the second person cannot have the same birthday as the first (), the third cannot have the same birthday as either of the first two (), and in general the th birthday cannot be the same as any of the preceding birthdays.\n\nThe event of at least two of the persons having the same birthday is complementary to all birthdays being different. Therefore, its probability is\n\nThe following table shows the probability for some other values of (this table ignores the existence of leap years, as described above, as well as assuming that each birthday is equally likely):\n\nThe Taylor series expansion of the exponential function (the constant )\n\nprovides a first-order approximation for for :\n\nTo apply this approximation to the first expression derived for , set . Thus,\n\nThen, replace with non-negative integers for each term in the formula of until , for example, when ,\n\nThe first expression derived for can be approximated as\n\nTherefore,\n\nAn even coarser approximation is given by\n\nwhich, as the graph illustrates, is still fairly accurate.\n\nAccording to the approximation, the same approach can be applied to any number of \"people\" and \"days\". If rather than 365 days there are , if there are persons, and if , then using the same approach as above we achieve the result that if is the probability that at least two out of people share the same birthday from a set of available days, then:\n\nThe probability of any two people not having the same birthday is . In a room containing \"n\" people, there are pairs of people, i.e. events. The probability of no two people sharing the same birthday can be approximated by assuming that these events are independent and hence by multiplying their probability together. In short can be multiplied by itself times, which gives us\n\nSince this is the probability of no one having the same birthday, then the probability of someone sharing a birthday is\n\nApplying the Poisson approximation for the binomial on the group of 23 people,\n\nso\n\nThe result is over 50% as previous descriptions.\n\nA good rule of thumb which can be used for mental calculation is the relation\n\nwhich can also be written as\n\nwhich works well for probabilities less than or equal to . In these equations, is the number of days in a year.\n\nFor instance, to estimate the number of people required for a chance of a shared birthday, we get\n\nWhich is not too far from the correct answer of 23.\n\nThis can also be approximated using the following formula for the \"number\" of people necessary to have at least a chance of matching:\nThis is a result of the good approximation that an event with probability will have a chance of occurring at least once if it is repeated times.\n\nThe white fields in this table show the number of hashes needed to achieve the given probability of collision (column) given a hash space of a certain size in bits (row). Using the birthday analogy: the \"hash space size\" resembles the \"available days\", the \"probability of collision\" resembles the \"probability of shared birthday\", and the \"required number of hashed elements\" resembles the \"required number of people in a group\". One could of course also use this chart to determine the minimum hash size required (given upper bounds on the hashes and probability of error), or the probability of collision (for fixed number of hashes and probability of error).\n\nFor comparison, to is the uncorrectable bit error rate of a typical hard disk. In theory, 128-bit hash functions, such as MD5, should stay within that range until about documents, even if its possible outputs are many more.\n\nThe argument below is adapted from an argument of Paul Halmos.\n\nAs stated above, the probability that no two birthdays coincide is\n\nAs in earlier paragraphs, interest lies in the smallest such that ; or equivalently, the smallest such that .\n\nUsing the inequality in the above expression we replace with . This yields\n\nTherefore, the expression above is not only an approximation, but also an upper bound of . The inequality\n\nimplies . Solving for gives\n\nNow, is approximately 505.997, which is barely below 506, the value of attained when . Therefore, 23 people suffice. Incidentally, solving for \"n\" gives the approximate formula of Frank H. Mathis cited above.\n\nThis derivation only shows that \"at most\" 23 people are needed to ensure a birthday match with even chance; it leaves open the possibility that is 22 or less could also work.\n\nGiven a year with days, the generalized birthday problem asks for the minimal number such that, in a set of randomly chosen people, the probability of a birthday coincidence is at least 50%. In other words, is the minimal integer such that\n\nThe classical birthday problem thus corresponds to determining . The first 99 values of are given here:\n\nA number of bounds and formulas for have been published.\nFor any , the number satisfies\n\nThese bounds are optimal in the sense that the sequence \ngets arbitrarily close to\nwhile it has\nas its maximum, taken for .\n\nThe bounds are sufficiently tight to give the exact value of in 99% of all cases, for example . In general, it follows from these bounds that always equals either\nwhere denotes the ceiling function.\nThe formula\n\nholds for 73% of all integers . The formula\n\nholds for almost all , i.e., for a set of integers with asymptotic density 1.\n\nThe formula\n\nholds for all , but it is conjectured that there are infinitely many counterexamples to this formula.\n\nThe formula\n\nholds for all , and it is conjectured that this formula holds for all .\n\nThe birthday problem can be generalized as follows:\n\nThe generic results can be derived using the same arguments given above.\n\nConversely, if denotes the number of random integers drawn from to obtain a probability that at least two numbers are the same, then\n\nThe birthday problem in this more generic sense applies to hash functions: the expected number of -bit hashes that can be generated before getting a collision is not , but rather only . This is exploited by birthday attacks on cryptographic hash functions and is the reason why a small number of collisions in a hash table are, for all practical purposes, inevitable.\n\nThe theory behind the birthday problem was used by Zoe Schnabel under the name of capture-recapture statistics to estimate the size of fish population in lakes.\n\nThe basic problem considers all trials to be of one \"type\". The birthday problem has been generalized to consider an arbitrary number of types. In the simplest extension there are two types of people, say men and women, and the problem becomes characterizing the probability of a shared birthday between at least one man and one woman. (Shared birthdays between two men or two women do not count.) The probability of no shared birthdays here is\n\nwhere and are Stirling numbers of the second kind. Consequently, the desired probability is .\n\nThis variation of the birthday problem is interesting because there is not a unique solution for the total number of people . For example, the usual 50% probability value is realized for both a 32-member group of 16 men and 16 women and a 49-member group of 43 women and 6 men.\n\nFor a fixed probability :\n\nTaking the above formula for we have:\n\nNote: some values falling outside the bounds have been colored to show that the approximation is not always exact.\n\nA related question is, as people enter a room one at a time, which one is most likely to be the first to have the same birthday as someone already in the room? That is, for what is maximum? The answer is 20—if there is a prize for first match, the best position in line is 20th.\n\nNote that in the birthday problem, neither of the two people is chosen in advance. By way of contrast, the probability that someone in a room of other people has the same birthday as a particular person (for example, you), is given by\n\nand for general by\n\nIn the standard case of substituting gives about 6.1%, which is less than 1 chance in 16. For a greater than 50% chance that one person in a roomful of people has the same birthday as \"you\", would need to be at least 253. Note that this number is significantly higher than : the reason is that it is likely that there are some birthday matches among the other people in the room.\n\nIt is not a coincidence that ; a similar approximate pattern can be found using a number of possibilities different from 365, or a target probability different from 50%.\n\nAnother generalization is to ask what is the probability of finding at least one pair in a group of people with birthdays within calendar days of each other's, if there are equally likely birthdays.\n\nThe number of people required so that the probability that some pair will have a birthday separated by days or fewer will be higher than 50% is:\n\nThus in a group of just seven random people, it is more likely than not that two of them will have a birthday within a week of each other.\n\nThe probability that the th integer randomly chosen from will repeat at least one previous choice equals above. The expected total number of times a selection will repeat a previous selection as such integers are chosen equals\n\nIn an alternative formulation of the birthday problem, one asks the \"average\" number of people required to find a pair with the same birthday. If we consider the probability function Pr[ people have at least one shared birthday], this \"average\" is determining the Mean of the distribution, as opposed to the customary formulation which determines the Median. The problem is relevant to several hashing algorithms analyzed by Donald Knuth in his book \"The Art of Computer Programming\". It may be shown that if one samples uniformly, with replacement, from a population of size , the number of trials required for the first repeated sampling of \"some\" individual has expected value , where\n\nThe function\n\nhas been studied by Srinivasa Ramanujan and has asymptotic expansion:\n\nWith days in a year, the average number of people required to find a pair with the same birthday is , somewhat more than 23, the number required for a 50% chance. In the best case, two people will suffice; at worst, the maximum possible number of people is needed; but on average, only 25 people are required.\n\nAn informal demonstration of the problem can be made from the list of Prime Ministers of Australia, of which there have been 29 , in which Paul Keating, the 24th Prime Minister, and Edmund Barton, the first Prime Minister, share the same birthday, 18 January.\n\nIn the 2014 FIFA World Cup, each of the 32 squads had 23 players. An analysis of the official squad lists suggested that 16 squads had pairs of players sharing birthdays, and of these 5 squads had two pairs: Argentina, France, Iran, South Korea and Switzerland each had two pairs, and Australia, Bosnia and Herzegovina, Brazil, Cameroon, Colombia, Honduras, Netherlands, Nigeria, Russia, Spain and USA each with one pair.\n\nVoracek, Tran and Formann showed that the majority of people markedly overestimate the number of people that is necessary to achieve a given probability of people having the same birthday, and markedly underestimate the probability of people having the same birthday when a specific sample size is given. Further results were that psychology students and women did better on the task than casino visitors/personnel or men, but were less confident about their estimates.\n\nA related problem is the partition problem, a variant of the knapsack problem from operations research. Some weights are put on a balance scale; each weight is an integer number of grams randomly chosen between one gram and one million grams (one tonne). The question is whether one can usually (that is, with probability close to 1) transfer the weights between the left and right arms to balance the scale. (In case the sum of all the weights is an odd number of grams, a discrepancy of one gram is allowed.) If there are only two or three weights, the answer is very clearly no; although there are some combinations which work, the majority of randomly selected combinations of three weights do not. If there are very many weights, the answer is clearly yes. The question is, how many are just sufficient? That is, what is the number of weights such that it is equally likely for it to be possible to balance them as it is to be impossible?\n\nSome people's intuition is that the answer is above . Most people's intuition is that it is in the thousands or tens of thousands, while others feel it should at least be in the hundreds. The correct answer is 23.\n\nThe reason is that the correct comparison is to the number of partitions of the weights into left and right. There are different partitions for weights, and the left sum minus the right sum can be thought of as a new random quantity for each partition. The distribution of the sum of weights is approximately Gaussian, with a peak at and width , so that when is approximately equal to the transition occurs. 2 is about 4 million, while the width of the distribution is only 5 million.\n\nArthur C. Clarke's novel \"A Fall of Moondust\", published in 1961, contains a section where the main characters, trapped underground for an indefinite amount of time, are celebrating a birthday and find themselves discussing the validity of the birthday problem. As stated by a physicist passenger: \"If you have a group of more than twenty-four people, the odds are better than even that two of them have the same birthday.\" Eventually, out of 22 present, it is revealed that two characters share the same birthday, May 23.\n\n\n"}
{"id": "554087", "url": "https://en.wikipedia.org/wiki?curid=554087", "title": "Causal system", "text": "Causal system\n\nIn control theory, a causal system (also known as a physical or nonanticipative system) is a system where the output depends on past and\ncurrent inputs but not future inputs—i.e., the output formula_1 depends on only the input formula_2 for values of formula_3.\n\nThe idea that the output of a function at any time depends only on past and present values of input is defined by the property commonly referred to as causality. A system that has \"some\" dependence on input values from the future (in addition to possible dependence on past or current input values) is termed a non-causal or acausal system, and a system that depends \"solely\" on future input values is an anticausal system. Note that some authors have defined an anticausal system as one that depends solely on future \"and present\" input values or, more simply, as a system that does not depend on past input values.\n\nClassically, nature or physical reality has been considered to be a causal system. Physics involving special relativity or general relativity require more careful definitions of causality, as described elaborately in Causality (physics).\n\nThe causality of systems also plays an important role in digital signal processing, where filters are constructed so that they are causal, sometimes by altering a non-causal formulation to remove the lack of causality so that it is realizable. For more information, see causal filter.\n\nFor a causal system, the impulse response of the system must use only the present and past values of the input to determine the output. This requirement is a necessary and sufficient condition for a system to be causal, regardless of linearity. Note that similar rules apply to either discrete or continuous cases. By this definition of requiring no future input values, systems must be causal to process signals in real time.\n\nDefinition 1: A system mapping formula_4 to formula_5 is causal if and only if, for any pair of input signals formula_6 and formula_7 such that\nthe corresponding outputs satisfy\n\nDefinition 2: Suppose formula_10 is the impulse response of any system formula_11 described by a linear constant coefficient differential equation. The system formula_11 is causal if and only if\notherwise it is non-causal.\n\nThe following examples are for systems with an input formula_4 and output formula_5.\n\n\n\n\n"}
{"id": "42127", "url": "https://en.wikipedia.org/wiki?curid=42127", "title": "Christiaan Huygens", "text": "Christiaan Huygens\n\nChristiaan Huygens ( ; ; ; 14 April 1629 – 8 July 1695) was a Dutch physicist, mathematician, astronomer and inventor, who is widely regarded as one of the greatest scientists of all time and a major figure in the scientific revolution. In physics, Huygens made groundbreaking contributions in optics and mechanics, while as an astronomer he is chiefly known for his studies of the rings of Saturn and the discovery of its moon Titan. As an inventor, he improved the design of the telescope with the invention of the Huygenian eyepiece. His most famous invention, however, was the invention of the pendulum clock in 1656, which was a breakthrough in timekeeping and became the most accurate timekeeper for almost 300 years. Because he was the first to use mathematical formulae to describe the laws of physics, Huygens has been called the first theoretical physicist and the founder of mathematical physics.\n\nIn 1659, Huygens was the first to derive the now standard formula for the centripetal force in his work \"De vi centrifuga\". The formula played a central role in classical mechanics and became known as the second of Newton's laws of motion. Huygens was also the first to formulate the correct laws of elastic collision in his work \"De motu corporum ex percussione\", but his findings were not published until 1703, after his death. In the field of optics, he is best known for his wave theory of light, which he proposed in 1678 and described in 1690 in his \"Treatise on Light\", which is regarded as the first mathematical theory of light. His theory was initially rejected in favor of Isaac Newton's corpuscular theory of light, until Augustin-Jean Fresnel adopted Huygens' principle in 1818 and showed that it could explain the rectilinear propagation and diffraction effects of light. Today this principle is known as the Huygens–Fresnel principle.\n\nHuygens invented the pendulum clock in 1656, which he patented the following year. In addition to this invention, his research in horology resulted in an extensive analysis of the pendulum in his 1673 book \"Horologium Oscillatorium\", which is regarded as one of the most important 17th-century works in mechanics. While the first part of the book contains descriptions of clock designs, most of the book is an analysis of pendulum motion and a theory of curves. In 1655, Huygens began grinding lenses with his brother Constantijn in order to build telescopes to conduct astronomical research. He designed a 50-power refracting telescope with which he discovered that the ring of Saturn was \"a thin, flat ring, nowhere touching, and inclined to the ecliptic.\" It was with this telescope that he also discovered the first of Saturn's moons, Titan. He eventually developed in 1662 what is now called the Huygenian eyepiece, a telescope with two lenses, which diminished the amount of dispersion.\n\nAs a mathematician, Huygens was a pioneer on probability and wrote his first treatise on probability theory in 1657 with the work \"Van Rekeningh in Spelen van Gluck\". Frans van Schooten, who was the private tutor of Huygens, translated the work as \"De ratiociniis in ludo aleae\" (\"On Reasoning in Games of Chance\"). The work is a systematic treatise on probability and deals with games of chance and in particular the problem of points. The modern concept of probability grew out of the use of expectation values by Huygens and Blaise Pascal (who encouraged him to write the work).\n\nThe last years of Huygens, who never married, were characterized by loneliness and depression. As a rationalist, he refused to believe in an immanent supreme being, and could not accept the Christian faith of his upbringing. Although Huygens did not believe in such a supernatural being, he did hypothesize on the possibility of extraterrestrial life in his \"Cosmotheoros\", which was published shortly before his death in 1695. He speculated that extraterrestrial life was possible on planets similar to Earth and wrote that the availability of water in liquid form was a necessity for life.\n\nChristiaan Huygens was born on 14 April 1629 in The Hague, into a rich and influential Dutch family, the second son of Constantijn Huygens. Christiaan was named after his paternal grandfather. His mother was Suzanna van Baerle. She died in 1637, shortly after the birth of Huygens' sister. The couple had five children: Constantijn (1628), Christiaan (1629), Lodewijk (1631), Philips (1632) and Suzanna (1637).\n\nConstantijn Huygens was a diplomat and advisor to the House of Orange, and also a poet and musician. His friends included Galileo Galilei, Marin Mersenne and René Descartes. Huygens was educated at home until turning sixteen years old. He liked to play with miniatures of mills and other machines. His father gave him a liberal education: he studied languages and music, history and geography, mathematics, logic and rhetoric, but also dancing, fencing and horse riding.\n\nIn 1644 Huygens had as his mathematical tutor Jan Jansz de Jonge Stampioen, who set the 15-year-old a demanding reading list on contemporary science. Descartes was impressed by his skills in geometry.\n\nHis father sent Huygens to study law and mathematics at the University of Leiden, where he studied from May 1645 to March 1647. Frans van Schooten was an academic at Leiden from 1646, and also a private tutor to Huygens and his elder brother, replacing Stampioen on the advice of Descartes. Van Schooten brought his mathematical education up to date, in particular introducing him to the work of Fermat on differential geometry.\n\nAfter two years, from March 1647, Huygens continued his studies at the newly founded Orange College, in Breda, where his father was a curator: the change occurred because of a duel between his brother Lodewijk and another student. Constantijn Huygens was closely involved in the new College, which lasted only to 1669; the rector was André Rivet. Christiaan Huygens lived at the home of the jurist Johann Henryk Dauber, and had mathematics classes with the English lecturer John Pell. He completed his studies in August 1649. He then had a stint as a diplomat on a mission with Henry, Duke of Nassau. It took him to Bentheim, then Flensburg. He took off for Denmark, visited Copenhagen and Helsingør, and hoped to cross the Øresund to visit Descartes in Stockholm. It was not to be.\n\nWhile his father Constantijn had wished his son Christiaan to be a diplomat, it also was not to be. In political terms, the First Stadtholderless Period that began in 1650 meant that the House of Orange was not in power, removing Constantijn's influence. Further, he realised that his son had no interest in such a career.\n\nHuygens generally wrote in French or Latin. While still a college student at Leiden he began a correspondence with the intelligencer Mersenne, who died quite soon afterwards in 1648. Mersenne wrote to Constantijn on his son's talent for mathematics, and flatteringly compared him to Archimedes (3 January 1647). The letters show the early interests of Huygens in mathematics. In October 1646 there is the suspension bridge, and the demonstration that a catenary is not a parabola. In 1647/8 they cover the claim of Grégoire de Saint-Vincent to squaring the circle; rectification of the ellipse; projectiles, and the vibrating string. Some of Mersenne's concerns at the time, such as the cycloid (he sent Evangelista Torricelli's treatise on the curve), the centre of oscillation, and the gravitational constant, were matters Huygens only took seriously towards the end of the 17th century. Mersenne had also written on musical theory. Huygens preferred meantone temperament; he innovated in 31 equal temperament, which was not itself a new idea but known to Francisco de Salinas, using logarithms to investigate it further and show its close relation to the meantone system.\n\nIn 1654, Huygens returned to his father's house in The Hague, and was able to devote himself entirely to research. The family had another house, not far away at Hofwijck, and he spent time there during the summer. His scholarly life did not allow him to escape bouts of depression.\nSubsequently, Huygens developed a broad range of correspondents, though picking up the threads after 1648 was hampered by the five-year \"Fronde\" in France. Visiting Paris in 1655, Huygens called on Ismael Boulliau to introduce himself. Then Boulliau took him to see Claude Mylon. The Parisian group of savants that had gathered around Mersenne held together into the 1650s, and Mylon, who had assumed the secretarial role, took some trouble from then on to keep Huygens in touch. Through Pierre de Carcavi Huygens corresponded in 1656 with Pierre de Fermat, whom he admired greatly, though this side of idolatry. The experience was bittersweet and even puzzling, since it became clear that Fermat had dropped out of the research mainstream, and his priority claims could probably not be made good in some cases. Besides, Huygens was looking by then to apply mathematics, while Fermat's concerns ran to purer topics.\n\nHuygens was often slow to publish his results and discoveries. In the early days his mentor Frans van Schooten was cautious for the sake of his reputation.\n\nThe first work Huygens put in print was \"Theoremata de quadratura\" (1651) in the field of quadrature. It included material discussed with Mersenne some years before, such as the fallacious nature of the squaring of the circle by Grégoire de Saint-Vincent. His preferred methods were those of Archimedes and Fermat. Quadrature was a live issue in the 1650s, and through Mylon, Huygens intervened in the discussion of the mathematics of Thomas Hobbes. Persisting in trying to explain the errors Hobbes had fallen into, he made an international reputation.\nHuygens studied spherical lenses from a theoretical point of view in 1652–3, obtaining results that remained unpublished until Isaac Barrow (1669). His aim was to understand telescopes. He began grinding his own lenses in 1655, collaborating with his brother Constantijn. He designed in 1662 what is now called the Huygenian eyepiece, with two lenses, as a telescope ocular. Lenses were also a common interest through which Huygens could meet socially in the 1660s with Baruch Spinoza, who ground them professionally. They had rather different outlooks on science, Spinoza being the more committed Cartesian, and some of their discussion survives in correspondence. He encountered the work of Antoni van Leeuwenhoek, another lens grinder, in the field of microscopy which interested his father.\n\nHuygens wrote the first treatise on probability theory, \"De ratiociniis in ludo aleae\" (\"On Reasoning in Games of Chance\", 1657). He had been told of recent work in the field by Fermat, Blaise Pascal and Girard Desargues two years earlier, in Paris. Frans van Schooten translated the original Dutch manuscript \"Van Rekeningh in Spelen van Geluck\" into Latin and published it in his \"Exercitationum mathematicarum\". It deals with games of chance, in particular the problem of points. Huygens took as intuitive his appeals to concepts of a \"fair game\" and equitable contract, and used them set up a theory of expected values. In 1662 Sir Robert Moray sent Huygens John Graunt's life table, and in time Huygens and his brother Lodewijk worked on life expectancy.\n\nOn 3 May 1661, Huygens observed the planet Mercury transit over the Sun, using the telescope of instrument maker Richard Reeve in London, together with astronomer Thomas Streete and Reeve. Streete then debated the published record of the transit of Hevelius, a controversy mediated by Henry Oldenburg. Huygens passed to Hevelius a manuscript of Jeremiah Horrocks on the transit of Venus, 1639, which thereby was printed for the first time in 1662. In that year Huygens, who played the harpsichord, took an interest in music, and Simon Stevin's theories on it; he showed very little concern to publish his theories on consonance, some of which were lost for centuries. The Royal Society of London elected him a Fellow in 1663.\n\nThe Montmor Academy was the form the old Mersenne circle took after the mid-1650s. Huygens took part in its debates, and supported its \"dissident\" faction who favoured experimental demonstration to curtail fruitless discussion, and opposed amateurish attitudes. During 1663 he made what was his third visit to Paris; the Montmor Academy closed down, and Huygens took the chance to advocate a more Baconian programme in science. In 1666 he moved to Paris and earned a position at Louis XIV's new French Academy of Sciences.\n\nIn Paris Huygens had an important patron and correspondent in Jean-Baptiste Colbert. However, his relationship with the Academy was not always easy, and in 1670 Huygens, seriously ill, chose Francis Vernon to carry out a donation of his papers to the Royal Society in London, should he die. Then the Franco-Dutch War took place (1672–8). England's part in it (1672–4) is thought to have damaged his relationship with the Royal Society. Robert Hooke for the Royal Society lacked the urbanity to handle the situation, in 1673.\n\nDenis Papin was assistant to Huygens from 1671. One of their projects, which did not bear fruit directly, was the gunpowder engine. Papin moved to England in 1678, and continued to work in this area. Using the Paris Observatory (completed in 1672), Huygens made further astronomical observations. In 1678 he introduced Nicolaas Hartsoeker to French scientists such as Nicolas Malebranche and Giovanni Cassini.\n\nIt was in Paris, also, that Huygens met the young diplomat Gottfried Leibniz, there in 1672 on a vain mission to meet Arnauld de Pomponne, the French Foreign Minister. At this time Leibniz was working on a calculating machine, and he moved on to London in early 1673 with diplomats from Mainz; but from March 1673 Leibniz was tutored in mathematics by Huygens. Huygens taught him analytical geometry; an extensive correspondence ensued, in which Huygens showed reluctance to accept the advantages of infinitesimal calculus.\n\nHuygens moved back to The Hague in 1681 after suffering serious depressive illness. In 1684, he published \"Astroscopia Compendiaria\" on his new tubeless aerial telescope. He attempted to return to France in 1685 but the revocation of the Edict of Nantes precluded this move. His father died in 1687, and he inherited Hofwijck, which he made his home the following year.\n\nOn his third visit to England, in 1689, Huygens met Isaac Newton on 12 June. They spoke about Iceland spar, and subsequently corresponded about resisted motion.\n\nHuygens observed the acoustical phenomenon now known as flanging in 1693. He died in The Hague on 8 July 1695, and was buried in the Grote Kerk.\n\nHuygens never married.\n\nHuygens has been called the leading European natural philosopher between Descartes and Newton. He adhered to the tenets of the mechanical philosophy of his time. In particular he sought explanations of the force of gravity that avoided action at a distance.\n\nIn common with Robert Boyle and Jacques Rohault, Huygens adhered to what has been called, more explicitly, \"experimentally oriented corpuscular-mechanical\" natural philosophy. In the analysis of the Scientific Revolution this appears as a mainstream position, at least from the founding of the Royal Society to the emergence of Newton, and was sometimes labelled \"Baconian\", while not being inductivist or identifying with the views of Francis Bacon in a simple-minded way. After his first visit to England in 1661, when he attended a meeting of the Gresham College group in April and learned directly about Boyle's air pump experiments, Huygens spent time in late 1661 and early 1662 replicating the work. It proved a long process, brought to the surface an experimental issue (\"anomalous suspension\") and the theoretical issue of \"horror vacui\", and ended in July 1663 as Huygens became a Fellow of the Royal Society. It has been said that Huygens finally accepted Boyle's view of the void, as against the Cartesian denial of it; and also (in \"Leviathan and the Air Pump\") that the replication of results trailed off messily.\n\nNewton's influence on John Locke was mediated by Huygens, who assured Locke that Newton's mathematics was sound, leading to Locke's acceptance of a \"corpuscular-mechanical\" physics.\n\nThe general approach of the mechanical philosophers was to postulate theories of the kind now called \"contact action\". Huygens adopted this method, but not without seeing its difficulties and failures. Leibniz, his student in Paris, abandoned the theory. Seeing the universe this way made the theory of collisions central to physics. The requirements of the mechanical philosophy, in the view of Huygens, were stringent. Matter in motion made up the universe, and only explanations in those terms could be truly intelligible. While he was influenced by the Cartesian approach, he was less doctrinaire. He studied elastic collisions in the 1650s but delayed publication for over a decade.\n\nHuygens concluded quite early that Descartes's laws for the elastic collision of two bodies must be wrong, and he formulated the correct laws. An important step was his recognition of the Galilean invariance of the problems. His views then took many years to be circulated. He passed them on in person to William Brouncker and Christopher Wren in London, in 1661. What Spinoza wrote to Henry Oldenburg about them, in 1666 which was during the Second Anglo-Dutch War, was guarded. Huygens had actually worked them out in a manuscript \"De motu corporum ex percussione\" in the period 1652–6. The war ended in 1667, and Huygens announced his results to the Royal Society in 1668. He published them in the \"Journal des sçavans\" in 1669.\n\nHuygens stated what is now known as the second of Newton's laws of motion in a quadratic form. In 1659 he derived the now standard formula for the centripetal force, exerted on an object describing a circular motion, for instance by the string to which it is attached. In modern notation:\n\nwith \"m\" the mass of the object, \"v\" the velocity and \"r\" the radius. The publication of the general formula for this force in 1673 was a significant step in studying orbits in astronomy. It enabled the transition from Kepler's third law of planetary motion, to the inverse square law of gravitation. The interpretation of Newton's work on gravitation by Huygens differed, however, from that of Newtonians such as Roger Cotes; he did not insist on the \"a priori\" attitude of Descartes, but neither would he accept aspects of gravitational attractions that were not attributable in principle to contact of particles.\n\nThe approach used by Huygens also missed some central notions of mathematical physics, which were not lost on others. His work on pendulums came very close to the theory of simple harmonic motion; but the topic was covered fully for the first time by Newton, in Book II of his \"Principia Mathematica\" (1687). In 1678 Leibniz picked out of Huygens's work on collisions the idea of conservation law that Huygens had left implicit.\n\nHuygens is remembered especially for his wave theory of light, which he first communicated in 1678 to the Paris Académie des sciences. It was published in 1690 in his \"Traité de la lumière\" (Treatise on light), making it the first mathematical theory of light. He refers to Ignace-Gaston Pardies, whose manuscript on optics helped him on his wave theory.\n\nHuygens assumes that the speed of light is finite, as had been shown in an experiment by Olaus Roemer in 1679, but which Huygens is presumed to have already believed. The challenge for the wave theory of light at that time was to explain geometrical optics, as most physical optics phenomena (such as diffraction) had not been observed or appreciated as issues. It posits light radiating wavefronts with the common notion of light rays depicting propagation normal to those wavefronts. Propagation of the wavefronts is then explained as the result of spherical waves being emitted at every point along the wave front (the Huygens–Fresnel principle). It assumed an omnipresent ether, with transmission through perfectly elastic particles, a revision of the view of Descartes. The nature of light was therefore a longitudinal wave.\n\nHuygens had experimented in 1672 with double refraction (birefringence) in Icelandic spar (calcite), a phenomenon discovered in 1669 by Rasmus Bartholin. At first he could not elucidate what he found. He later explained it with his wave front theory and concept of evolutes. He also developed ideas on caustics. Newton in his \"Opticks\" of 1704 proposed instead a corpuscular theory of light. The theory of Huygens was not widely accepted, one strong objection being that longitudinal waves have only a single polarization which cannot explain the observed birefringence. However the 1801 interference experiments of Thomas Young and François Arago 's 1819 detection of the Poisson spot could not be explained through any particle theory, reviving the ideas of Huygens and wave models. In 1821 Fresnel was able to explain birefringence as a result of light being not a longitudinal (as had been assumed) but actually a transverse wave. The thus-named Huygens–Fresnel principle was the basis for the advancement of physical optics, explaining all aspects of light propagation. It was only understanding the detailed interaction of light with atoms that awaited quantum mechanics and the discovery of the photon.\nHuygens investigated the use of lenses in projectors. He is credited as the inventor of the magic lantern, described in correspondence of 1659. There are others to whom such a lantern device has been attributed, such as Giambattista della Porta, and Cornelis Drebbel: the point at issue is the use of a lens for better projection. Athanasius Kircher has also been credited for that.\n\nHuygens designed more accurate clocks than were available at the time. In 1656, inspired by earlier research into pendulums by Galileo Galilei, he invented the pendulum clock, which was a breakthrough in timekeeping and became the most accurate timekeeper for the next 275 years until the 1930s. Huygens contracted the construction of his clock designs to Salomon Coster in The Hague, with a local patent (\"octroy\"). He was less successful elsewhere: Pierre Séguier refused him any French rights, Simon Douw of Rotterdam copied the design in 1658, and Ahasuerus Fromanteel also, in London. The oldest known Huygens-style pendulum clock is dated 1657 and can be seen at the Museum Boerhaave in Leiden.\n\nHuygens motivation for inventing the pendulum clock was to create an accurate marine chronometer that could be used to find longitude by celestial navigation during sea voyages. Exploiting the invention at sea proved troublesome, however, because the rocking motion of the ship disturbed the motion of the pendulum. In 1660 Lodewijk Huygens made a trial on a voyage to Spain, and reported that heavy weather made the clock useless. Alexander Bruce elbowed into the field in 1662, and Huygens called in Sir Robert Moray and the Royal Society to mediate and preserve some of his rights. Trials continued into the 1660s, the best news coming from a Royal Navy captain Robert Holmes operating against the Dutch possessions in 1664. Lisa Jardine doubts that Holmes reported the results of the trial accurately, and Samuel Pepys expressed his doubts at the time: \"The said master\" [i.e. the captain of Holmes' ship] \"affirmed, that the vulgar reckoning proved as near as that of the watches, which\" [the clocks], \"added he, had varied from one another unequally, sometimes backward, sometimes forward, to 4, 6, 7, 3, 5 minutes; as also that they had been corrected by the usual account.\" One for the French Academy on an expedition to Cayenne ended badly. Jean Richer suggested correction for the figure of the Earth. By the time of the Dutch East India Company expedition of 1686 to the Cape of Good Hope, Huygens was able to supply the correction retrospectively.\n\nIn 1673 Huygens published \"Horologium Oscillatorium sive de motu pendulorum\", his major work on pendulums and horology. It had been observed by Mersenne and others that pendulums are not quite isochronous: their period depends on their width of swing, with wide swings taking slightly longer than narrow swings.\n\nHuygens analyzed this problem by finding the curve down which a mass will slide under the influence of gravity in the same amount of time, regardless of its starting point; the so-called tautochrone problem. By geometrical methods which were an early use of calculus, he showed it to be a cycloid, rather than the circular arc of a pendulum's bob, and therefore that pendulums are not isochronous. He also solved a problem posed by Mersenne: how to calculate the period of a pendulum made of an arbitrarily shaped swinging rigid body. This involved discovering the center of oscillation and its reciprocal relationship with the pivot point. In the same work, he analysed the conical pendulum, consisting of a weight on a cord moving in a circle, using the concept of centrifugal force.\nHuygens was the first to derive the formula for the period of an ideal mathematical pendulum (with massless rod or cord and length much longer than its swing), in modern notation:\n\nwith \"T\" the period, \"l\" the length of the pendulum and \"g\" the gravitational acceleration. By his study of the oscillation period of compound pendulums Huygens made pivotal contributions to the development of the concept of moment of inertia.\n\nHuygens also observed coupled oscillations: two of his pendulum clocks mounted next to each other on the same support often became synchronized, swinging in opposite directions. He reported the results by letter to the Royal Society, and it is referred to as \"an odd kind of sympathy\" in the Society's minutes. This concept is now known as entrainment.\n\nHuygens developed a balance spring watch in the same period as, though independently of, Robert Hooke. Controversy over the priority persisted for centuries. A Huygens watch employed a spiral balance spring; but he used this form of spring initially only because the balance in his first watch rotated more than one and a half turns. He later used spiral springs in more conventional watches, made for him by Thuret in Paris from around 1675.\n\nSuch springs were essential in modern watches with a detached lever escapement because they can be adjusted for isochronism. Watches in the time of Huygens and Hooke, however, employed the very undetached verge escapement. It interfered with the isochronal properties of any form of balance spring, spiral or otherwise.\n\nIn February 2006, a long-lost copy of Hooke's handwritten notes from several decades of Royal Society meetings was discovered in a cupboard in Hampshire, England. The balance-spring priority controversy appears, by the evidence contained in those notes, to be settled in favour of Hooke's claim.\n\nIn 1675, Huygens patented a pocket watch. The watches which were made in Paris from c. 1675 and following the Huygens plan are notable for lacking a fusee for equalizing the mainspring torque. The implication is that Huygens thought that his spiral spring would isochronise the balance, in the same way that he thought that the cycloidally shaped suspension curbs on his clocks would isochronise the pendulum.\n\nIn 1655, Huygens proposed that Saturn was surrounded by a solid ring, \"a thin, flat ring, nowhere touching, and inclined to the ecliptic.\" Using a 50 power refracting telescope that he designed himself, Huygens also discovered the first of Saturn's moons, Titan. In the same year he observed and sketched the Orion Nebula. His drawing, the first such known of the Orion nebula, was published in \"Systema Saturnium\" in 1659. Using his modern telescope he succeeded in subdividing the nebula into different stars. The brighter interior now bears the name of the \"Huygenian region\" in his honour. He also discovered several interstellar nebulae and some double stars.\n\nIn 1659, Huygens was the first to observe a surface feature on another planet, Syrtis Major, a volcanic plain on Mars. He used repeated observations of the movement of this feature over the course of a number of days to estimate the length of day on Mars, which he did quite accurately to 24 1/2 hours. This figure is only a few minutes off of the actual length of the Martian day of 24 hours, 37 minutes.\n\nShortly before his death in 1695, Huygens completed \"Cosmotheoros\", published posthumously in 1698. In it he speculated on the existence of extraterrestrial life, on other planets, which he imagined was similar to that on Earth. Such speculations were not uncommon at the time, justified by Copernicanism or the plenitude principle. But Huygens went into greater detail, though without the benefit of understanding Newton's laws of gravitation, or the fact that the atmospheres on other planets are composed of different gases. The work, translated into English in its year of publication, has been seen as in the fanciful tradition of Francis Godwin, John Wilkins and Cyrano de Bergerac, and fundamentally Utopian; and also to owe in its concept of planet to cosmography in the sense of Peter Heylin.\n\nHuygens wrote that availability of water in liquid form was essential for life and that the properties of water must vary from planet to planet to suit the temperature range. He took his observations of dark and bright spots on the surfaces of Mars and Jupiter to be evidence of water and ice on those planets. He argued that extraterrestrial life is neither confirmed nor denied by the Bible, and questioned why God would create the other planets if they were not to serve a greater purpose than that of being admired from Earth. Huygens postulated that the great distance between the planets signified that God had not intended for beings on one to know about the beings on the others, and had not foreseen how much humans would advance in scientific knowledge.\n\nIt was also in this book that Huygens published his method for estimating stellar distances. He made a series of smaller holes in a screen facing the sun, until he estimated the light was of the same intensity as that of the star Sirius. He then calculated that the angle of this hole was formula_3th the diameter of the Sun, and thus it was about 30,000 times as far away, on the (incorrect) assumption that Sirius is as luminous as our sun. The subject of photometry remained in its infancy until the time of Pierre Bouguer and Johann Heinrich Lambert.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20405001", "url": "https://en.wikipedia.org/wiki?curid=20405001", "title": "Controlled invariant subspace", "text": "Controlled invariant subspace\n\nIn control theory, a controlled invariant subspace of the state space representation of some system is a subspace such that, if the state of the system is initially in the subspace, it is possible to control the system so that the state is in the subspace at all times. This concept was introduced by Giuseppe Basile and Giovanni Marro .\n\nConsider a linear system described by the differential equation\nHere, x(\"t\") ∈ R denotes the state of the system and u(\"t\") ∈ R is the input. The matrices \"A\" and \"B\" have size \"n\" × \"n\" and \"n\" × \"p\" respectively.\n\nA subspace \"V\" ⊂ R is a \"controlled invariant subspace\" if for any x(0) ∈ \"V\", there is an input u(\"t\") such that x(\"t\") ∈ \"V\" for all nonnegative \"t\".\n\nA subspace \"V\" ⊂ R is a controlled invariant subspace if and only if \"AV\" ⊂ \"V\" + Im \"B\". If \"V\" is a controlled invariant subspace, then there exists a matrix \"K\" such that the input u(\"t\") = \"K\"x(\"t\") keeps the state within \"V\"; this is a simple feedback control .\n\n"}
{"id": "1079448", "url": "https://en.wikipedia.org/wiki?curid=1079448", "title": "Dirac operator", "text": "Dirac operator\n\nIn mathematics and quantum mechanics, a Dirac operator is a differential operator that is a formal square root, or half-iterate, of a second-order operator such as a Laplacian. The original case which concerned Paul Dirac was to factorise formally an operator for Minkowski space, to get a form of quantum theory compatible with special relativity; to get the relevant Laplacian as a product of first-order operators he introduced spinors. \n\nIn general, let \"D\" be a first-order differential operator acting on a vector bundle \"V\" over a Riemannian manifold \"M\". If \nwhere ∆ is the Laplacian of \"V\", then \"D\" is called a Dirac operator.\n\nIn high-energy physics, this requirement is often relaxed: only the second-order part of \"D\" must equal the Laplacian.\n\nExample 1: \"D\" = −\"i\" ∂ is a Dirac operator on the tangent bundle over a line.\n\nExample 2: We now consider a simple bundle of importance in physics: The configuration space of a particle with spin confined to a plane, which is also the base manifold. It is represented by a wavefunction \nwhere \"x\" and \"y\" are the usual coordinate functions on R. \"χ\" specifies the probability amplitude for the particle to be in the spin-up state, and similarly for \"η\". The so-called spin-Dirac operator can then be written\nwhere \"σ\" are the Pauli matrices. Note that the anticommutation relations for the Pauli matrices make the proof of the above defining property trivial. Those relations define the notion of a Clifford algebra.\n\nSolutions to the Dirac equation for spinor fields are often called \"harmonic spinors\".\n\nExample 3: Feynman's Dirac operator describes the propagation of a free fermion in three dimensions and is elegantly written \nusing the Feynman slash notation.\n\nExample 4: Another Dirac operator arises in Clifford analysis. In euclidean \"n\"-space this is\nwhere {\"e\": \"j\" = 1, ..., \"n\"} is an orthonormal basis for euclidean \"n\"-space, and R is considered to be embedded in a Clifford algebra. \n\nThis is a special case of the Atiyah–Singer–Dirac operator acting on sections of a spinor bundle.\n\nExample 5: For a spin manifold, \"M\", the Atiyah–Singer–Dirac operator is locally defined as follows: For and \"e\"(\"x\"), ..., \"e\"(\"x\") a local orthonormal basis for the tangent space of \"M\" at \"x\", the Atiyah–Singer–Dirac operator is\nwhere formula_7 is a lifting of the Levi-Civita connection on \"M\" to the spinor bundle over \"M\".\n\nIn Clifford analysis, the operator acting on spinor valued functions defined by \nis sometimes called Dirac operator in \"k\" Clifford variables. In the notation, \"S\" is the space of spinors, formula_9 are \"n\"-dimensional variables and formula_10 is the Dirac operator in the \"i\"-th variable. This is a common generalization of the Dirac operator () and the Dolbeault operator (, \"k\" arbitrary). It is an invariant differential operator, invariant under the action of the group . The resolution of \"D\" is known only in some special cases.\n\n"}
{"id": "25354354", "url": "https://en.wikipedia.org/wiki?curid=25354354", "title": "Double recursion", "text": "Double recursion\n\nIn recursive function theory, double recursion is an extension of primitive recursion which allows the definition of non-primitive recursive functions like the Ackermann function.\n\nRaphael M. Robinson called functions of two natural number variables \"G\"(\"n\", \"x\") double recursive with respect to \"given functions\", if\n\nRobinson goes on to provide a specific double recursive function (originally defined by Rózsa Péter)\nwhere the \"given functions\" are primitive recursive, but \"G\" is not primitive recursive. In fact, this is precisely the function now known as the Ackermann function.\n\n"}
{"id": "44253927", "url": "https://en.wikipedia.org/wiki?curid=44253927", "title": "Dynamic secrets", "text": "Dynamic secrets\n\nDynamic Secrets is a novel key management scheme for secure communications. It was proposed by Sheng Xiao, Weibo Gong, and Don Towsley. The first academic publication had been nominated for INFOCOM 2010 best paper award. Later a monograph was published by Springer to extend this scheme to a framework.\n\nDynamic secrets can be applied to all bi-directional communication systems and some single-directional communication systems to improve their communication security. There are three main benefits:\n\n1. the encryption and authentication keys are rapidly and automatically updated for any pair of communication devices\n\n2. the key update process binds to the communication process and incurs negligible computing and bandwidth cost\n\n3. Use a cloned key in either authentication or encrypted communication is guaranteed to be detected. the detection has no false alarms and does not cost any computing / networking resources. (dynamic secrets automatically break the secure communication when the clone key and the legitimate key co-exist. however, in order to find out who is the attacker, it takes further actions and consumes computing power and network bandwidth.)\n"}
{"id": "32183016", "url": "https://en.wikipedia.org/wiki?curid=32183016", "title": "Ear decomposition", "text": "Ear decomposition\n\nIn graph theory, an ear of an undirected graph \"G\" is a path \"P\" where the two endpoints of the path may coincide, but where otherwise no repetition of edges or vertices is allowed, so every internal vertex of \"P\" has degree two in \"P\". An ear decomposition of an undirected graph \"G\" is a partition of its set of edges into a sequence of ears, such that the one or two endpoints of each ear belong to earlier ears in the sequence and such that the internal vertices of each ear do not belong to any earlier ear. Additionally, in most cases the first ear in the sequence must be a cycle. An open ear decomposition or a proper ear decomposition is an ear decomposition in which the two endpoints of each ear after the first are distinct from each other.\n\nEar decompositions may be used to characterize several important graph classes, and as part of efficient graph algorithms. They may also be generalized from graphs to matroids.\n\nSeveral important classes of graphs may be characterized as the graphs having certain types of ear decompositions.\n\nA graph is \"k\"-vertex-connected if the removal of any (\"k\" − 1) vertices leaves a connected subgraph, and \"k\"-edge-connected if the removal of any (\"k\" − 1) edges leaves a connected subgraph.\n\nThe following result is due to :\n\nThe following result is due to :\n\nIn both cases the number of ears is necessarily equal to the circuit rank of the given graph. Robbins introduced the ear decomposition of 2-edge-connected graphs as a tool for proving the Robbins theorem, that these are exactly the graphs that may be given a strongly connected orientation. Because of the pioneering work of Whitney and Robbins on ear decompositions, an ear decomposition is sometimes also called a Whitney–Robbins synthesis .\n\nA non-separating ear decomposition is an open ear decomposition such that, for each vertex \"v\" with only one exception, \"v\" has a neighbor whose first appearance in the decomposition is in a later ear than the first appearance of \"v\". This type of ear decomposition may be used to generalize Whitney's result:\nIf such a decomposition exists, it can be chosen with respect to a particular edge \"uv\" of \"G\" in such a way that \"u\" is in the first ear, \"v\" is the new vertex in the last ear with more than one edge, and \"uv\" is a single-edge ear.\nThis result was first stated explicitly by , but as describes, it is equivalent to a result in the 1971 Ph.D. thesis of Lee Mondshein. Structures closely related to non-separating ear decompositions of maximal planar graphs, called canonical orderings, are also a standard tool in graph drawing.\n\nThe above definitions can also be applied to directed graphs. An ear would then be a directed path where all internal vertices have indegree and outdegree equal to 1. A directed graph is strongly connected if it contains a directed path from every vertex to every other vertex. Then we have the following theorem:\nSimilarly, a directed graph is biconnected if, for every two vertices, there exists a simple cycle in the graph containing both of them. Then\n\nAn ear decomposition is \"odd\" if each of its ears uses an odd number of edges. A factor-critical graph is a graph with an odd number of vertices, such that for each vertex \"v\", if \"v\" is removed from the graph then the remaining vertices have a perfect matching. found that:\nMore generally, a result of makes it possible to find in any graph \"G\" the ear decomposition with the fewest even ears.\n\nA \"tree\" ear decomposition is a proper ear decomposition in which the first ear is a single edge and for each subsequent ear formula_5, there is a single ear formula_6, formula_7, such that both endpoints of formula_5 lie on formula_6 . A \"nested\" ear decomposition is a tree ear decomposition such that, within each ear formula_6, the set of pairs of endpoints of other ears formula_5 that lie within formula_6 form a set of nested intervals. A series-parallel graph is a graph with two designated terminals \"s\" and \"t\" that can be formed recursively by combining smaller series-parallel graphs in one of two ways: series composition (identifying one terminal from one smaller graph with one terminal from the other smaller graph, and keeping the other two terminals as the terminals of the combined graph) and parallel composition (identifying both pairs of terminals from the two smaller graphs).\n\nThe following result is due to :\nMoreover, any open ear decomposition of a 2-vertex-connected series-parallel graph must be nested. The result may be extended to series-parallel graphs that are not 2-vertex-connected by using open ear decompositions that start with a path between the two terminals.\n\nThe concept of an ear decomposition can be extended from graphs to matroids. An ear decomposition of a matroid is defined to be a sequence of circuits of the matroid, with two properties:\nWhen applied to the graphic matroid of a graph \"G\", this definition of an ear decomposition coincides with the definition of a proper ear decomposition of \"G\": improper decompositions are excluded by the requirement that each circuit include at least one edge that also belongs to previous circuits. Using this definition, a matroid may be defined as factor-critical when it has an ear decomposition in which each circuit in the sequence has an odd number of new elements .\n\nOn classical computers, ear decompositions of 2-edge-connected graphs and open ear decompositions of 2-vertex-connected graphs may be found by greedy algorithms that find each ear one at a time. A simple greedy approach that computes at the same time ear decompositions, open ear decompositions, st-numberings and -orientations in linear time (if exist) is given in . The approach is based on computing a special ear decomposition named chain decomposition by one path-generating rule. shows that non-separating ear decompositions may also be constructed in linear time.\n\n, , and provided efficient parallel algorithms for constructing ear decompositions of various types. For instance, to find an ear decomposition of a 2-edge-connected graph, the algorithm of proceeds according to the following steps:\nThese algorithms may be used as subroutines for other problems including testing connectivity, recognizing series-parallel graphs, and constructing \"st\"-numberings of graphs (an important subroutine in planarity testing).\n\nAn ear decomposition of a given matroid, with the additional constraint that every ear contains the same fixed element of the matroid, may be found in polynomial time given access to an independence oracle for the matroid .\n\n"}
{"id": "244649", "url": "https://en.wikipedia.org/wiki?curid=244649", "title": "Emanuel Lasker", "text": "Emanuel Lasker\n\nEmanuel Lasker (December 24, 1868 – January 11, 1941) was a German chess player, mathematician, and philosopher who was World Chess Champion for 27 years (from 1894 to 1921). In his prime, Lasker was one of the most dominant champions, and he is still generally regarded as one of the strongest players ever.\n\nHis contemporaries used to say that Lasker used a \"psychological\" approach to the game, and even that he sometimes deliberately played inferior moves to confuse opponents. Recent analysis, however, indicates that he was ahead of his time and used a more flexible approach than his contemporaries, which mystified many of them. Lasker knew contemporary analyses of openings well but disagreed with many of them. He published chess magazines and five chess books, but later players and commentators found it difficult to draw lessons from his methods.\n\nLasker made contributions to the development of other games. He was a first-class contract bridge player and wrote about bridge, Go, and his own invention, Lasca. His books about games presented a problem that is still considered notable in the mathematical analysis of card games. Lasker was also a research mathematician who was known for his contributions to commutative algebra, which included proving the primary decomposition of the ideals of polynomial rings. His philosophical works and a drama that he co-wrote, however, received little attention.\n\nEmanuel Lasker was born on December 24, 1868 at Berlinchen in Neumark (now Barlinek in Poland), the son of a Jewish cantor. At the age of eleven he was sent to Berlin to study mathematics, where he lived with his brother Berthold, eight years his senior, who taught him how to play chess. According to the website Chessmetrics, Berthold was among the world's top ten players in the early 1890s. To supplement their income Emanuel Lasker played chess and card games for small stakes, especially at the Café Kaiserhof.\n\nLasker shot up through the chess rankings in 1889, when he won the Café Kaiserhof's annual Winter tournament 1888/89 and the \"Hauptturnier A\" (\"second division\" tournament) at the sixth DSB Congress (German Chess Federation's congress) held in Breslau. Winning the Hauptturnier earned Lasker the title of \"master\". The candidates were divided into two groups of ten. The top four in each group competed in a final. Lasker won his section, with 2½ points more than his nearest rival. However, scores were reset to 0 for the final. With two rounds to go, Lasker trailed the leader, Viennese amateur von Feierfeil, by 1½ points. Lasker won both of his final games, while von Feierfeil lost in the penultimate round (being mated in 121 moves after the position was reconstructed incorrectly following an adjournment) and drew in the last round. The two players were now tied. Lasker won a playoff and garnered the master title. This enabled him to play in master-level tournaments and thus launched his chess career.\n\nLasker finished second in an international tournament at Amsterdam, ahead of some well-known masters, including Isidore Gunsberg (assessed as the second strongest player in the world at that time by Chessmetrics). In 1890 he finished third in Graz, then shared first prize with his brother Berthold in a tournament in Berlin. In spring 1892, he won two tournaments in London, the second and stronger of these without losing a game. At New York City 1893, he won all thirteen games, one of the few times in chess history that a player has achieved a perfect score in a significant tournament.\nHis record in matches was equally impressive: at Berlin in 1890 he drew a short play-off match against his brother Berthold; and won all his other matches from 1889 to 1893, mostly against top-class opponents: Curt von Bardeleben (1889; ranked 9th best player in the world by Chessmetrics at that time), Jacques Mieses (1889; ranked 11th), Henry Edward Bird (1890; then 60 years old; ranked 29th), Berthold Englisch (1890; ranked 18th), Joseph Henry Blackburne (1892, without losing a game; Blackburne was aged 51 then, but still 9th in the world), Jackson Showalter (1892–93; 22nd) and Celso Golmayo Zúpide (1893; 29th). Chessmetrics calculates that Emanuel Lasker became the world's strongest player in mid-1890, and that he was in the top ten from the very beginning of his recorded career in 1889.\n\nIn 1892 Lasker founded the first of his chess magazines, \"The London Chess Fortnightly\", which was published from August 15, 1892 to July 30, 1893. In the second quarter of 1893 there was a gap of ten weeks between issues, allegedly because of problems with the printer. Shortly after its last issue Lasker traveled to the US, where he spent the next two years.\n\nLasker challenged Siegbert Tarrasch, who had won three consecutive strong international tournaments (Breslau 1889, Manchester 1890, and Dresden 1892), to a match. Tarrasch haughtily declined, stating that Lasker should first prove his mettle by attempting to win one or two major international events.\n\nRebuffed by Tarrasch, Lasker challenged the reigning World Champion Wilhelm Steinitz to a match for the title. Initially Lasker wanted to play for US $5,000 a side and a match was agreed at stakes of $3,000 a side, but Steinitz agreed to a series of reductions when Lasker found it difficult to raise the money. The final figure was $2,000, which was less than for some of Steinitz' earlier matches (the final combined stake of $4,000 would be worth over $495,000 at 2006 values). Although this was publicly praised as an act of sportsmanship on Steinitz' part, Steinitz may have desperately needed the money. The match was played in 1894, at venues in New York, Philadelphia, and Montreal. Steinitz had previously declared he would win without doubt, so it came as a shock when Lasker won the first game. Steinitz responded by winning the second, and maintained the balance through the sixth. However, Lasker won all the games from the seventh to the eleventh, and Steinitz asked for a week's rest. When the match resumed, Steinitz looked in better shape and won the 13th and 14th games. Lasker struck back in the 15th and 16th, and Steinitz did not compensate for his losses in the middle of the match. Hence Lasker won convincingly with ten wins, five losses and four draws. Lasker thus became the second formally recognized World Chess Champion, and confirmed his title by beating Steinitz even more convincingly in their re-match in 1896–97 (ten wins, two losses, and five draws).\n\nInfluential players and journalists belittled the 1894 match both before and after it took place. Lasker's difficulty in getting backing may have been caused by hostile pre-match comments from Gunsberg and Leopold Hoffer, who had long been a bitter enemy of Steinitz. One of the complaints was that Lasker had never played the other two members of the top four, Siegbert Tarrasch and Mikhail Chigorin – although Tarrasch had rejected a challenge from Lasker in 1892, publicly telling him to go and win an international tournament first. After the match, some commentators, notably Tarrasch, said Lasker had won mainly because Steinitz was old (58 in 1894).\n\nEmanuel Lasker answered these criticisms by creating an even more impressive playing record. Before World War I broke out, his most serious \"setbacks\" were third place at Hastings 1895 (where he may have been suffering from the after-effects of typhoid fever), a tie for second at Cambridge Springs 1904, and a tie for first at the Chigorin Memorial in St Petersburg 1909. He won first prizes at very strong tournaments in St Petersburg (1895–96, \"Quadrangular\"), Nuremberg (1896), London (1899), Paris (1900) and St Petersburg (1914), where he overcame a 1½-point deficit to finish ahead of the rising stars, Capablanca and Alexander Alekhine, who later became the next two World Champions. For decades chess writers have reported that Tsar Nicholas II of Russia conferred the title of \"Grandmaster of Chess\" upon each of the five finalists at St Petersburg 1914 (Lasker, Capablanca, Alekhine, Tarrasch and Marshall), but chess historian Edward Winter has questioned this, stating that the earliest known sources supporting this story were published in 1940 and 1942.\n\nLasker's match record was as impressive between his 1896–97 re-match with Steinitz and 1914: he won all but one of his normal matches, and three of those were convincing defenses of his title. He first faced Marshall in the World Chess Championship 1907, when despite his aggressive style, Marshall could not win a single game, losing eight and drawing seven (final score: 11½−3½).\n\nHe then played Tarrasch in the World Chess Championship 1908, first at Düsseldorf then at Munich. Tarrasch firmly believed the game of chess was governed by a precise set of principles. For him the strength of a chess move was in its logic, not in its efficiency. Because of his stubborn principles he considered Lasker as a who won his games only thanks to dubious tricks, while Lasker mocked the arrogance of Tarrasch who, in his opinion, shone more in salons than at the chessboard. At the opening ceremony, Tarrasch refused to talk to Lasker, only saying: \"Mr. Lasker, I have only three words to say to you: check and mate!\"\nLasker gave a brilliant answer on the chessboard, winning four of the first five games, and playing a type of chess Tarrasch could not understand. For example, in the second game after 19 moves arose a situation (see diagram) in which Lasker was a pawn down, with a and . At this point it appeared Tarrasch was winning, but 20 moves later he was forced to resign. Lasker eventually won by 10½−5½ (eight wins, five draws, and three losses). Tarrasch claimed the wet weather was the cause of his defeat.\n\nIn 1909 Lasker drew a short match (two wins, two losses) against Dawid Janowski, an all-out attacking Polish expatriate. Several months later they played a longer match in Paris, and chess historians still debate whether this was for the World Chess Championship. Understanding Janowski's style, Lasker chose to defend solidly so that Janowski unleashed his attacks too soon and left himself vulnerable. Lasker easily won the match 8–2 (seven wins, two draws, one loss). This victory was convincing for everyone but Janowski, who asked for a revenge match. Lasker accepted and they played a World Chess Championship match in Berlin in November–December 1910. Lasker crushed his opponent, winning 9½−1½ (eight wins, three draws, no losses). Janowski did not understand Lasker's moves, and after his first three losses he declared to Edward Lasker, \"Your homonym plays so stupidly that I cannot even look at the chessboard when he thinks. I am afraid I will not do anything good in this match.\"\n\nBetween his two matches against Janowski, Lasker arranged another World Chess Championship in January–February 1910 against Carl Schlechter. Schlechter was a modest gentleman, who was generally unlikely to win the major chess tournaments by his peaceful inclination, his lack of aggressiveness and his willingness to accept most draw offers from his opponents (about 80% of his games finished by a draw). According to Isaak and Vladimir Linder, the match was originally to be a 30 game affair and Schlecter would have to win by a two game margin. They note that the Austrian chess historian Micheal Ehn has established, based on the Viennese sources, that Lasker agreed to forgo the plus two provision in view of the match being subsequently reduced to only 10 games. For proof Ehn quoted Schlechter's comment printed in Allgemeine Sportzeitung (ASZ) of December 9, 1909 \"There will be ten games in all. The winner on points will receive the title of world champion. If the points are equal, the decision will be made by the arbiter.\"\nAt the beginning, Lasker tried to attack but Schlechter had no difficulty defending, so that the first four games finished in draws. In the fifth game Lasker had a big advantage, but committed a blunder that cost him the game. Hence at the middle of the match Schlechter was one point ahead. The next four games were drawn, despite fierce play from both players. In the sixth Schlechter managed to draw a game being a pawn down. In the seventh Lasker nearly lost because of a beautiful exchange sacrifice from Schlechter. In the ninth only a blunder from Lasker allowed Schlechter to draw a lost ending. The score before the last game was thus 5–4 for Schlechter. In the tenth game Schlechter tried to win tactically and took a big advantage, but he missed a clear win at the 35th move, continued to take increasing risks and finished by losing. Hence the match was a draw and Lasker remained World Champion.\n\nIn 1911 Lasker received a challenge for a world title match against the rising star José Raúl Capablanca. Lasker was unwilling to play the traditional \"first to win ten games\" type of match in the semi-tropical conditions of Havana, especially as drawn games were becoming more frequent and the match might last for over six months. He therefore made a counter-proposal: if neither player had a lead of at least two games by the end of the match, it should be considered a draw; the match should be limited to the best of thirty games, counting draws; except that if either player won six games \"and\" led by at least two games before thirty games were completed, he should be declared the winner; the champion should decide the venue and stakes, and should have the exclusive right to publish the games; the challenger should deposit a forfeit of US $2,000 (equivalent to over $194,000 in 2006 values); the time limit should be twelve moves per hour; play should be limited to two sessions of 2½ hours each per day, five days a week. Capablanca objected to the time limit, the short playing times, the thirty-game limit, and especially the requirement that he must win by two games to claim the title, which he regarded as unfair. Lasker took offence at the terms in which Capablanca criticized the two-game lead condition and broke off negotiations, and until 1914 Lasker and Capablanca were not on speaking terms. However, at the 1914 St. Petersburg tournament, Capablanca proposed a set of rules for the conduct of World Championship matches, which were accepted by all the leading players, including Lasker.\n\nLate in 1912 Lasker entered into negotiations for a world title match with Akiba Rubinstein, whose tournament record for the previous few years had been on a par with Lasker's and a little ahead of Capablanca's. The two players agreed to play a match if Rubinstein could raise the funds, but Rubinstein had few rich friends to back him and the match was never played. This situation demonstrated some of the flaws inherent in the championship system then being used. The start of World War I in summer 1914 put an end to hopes that Lasker would play either Rubinstein or Capablanca for the World Championship in the near future.\nThroughout World War I (1914–1918) Lasker played in only two serious chess events. He convincingly won (5½−½) a non-title match against Tarrasch in 1916. In September–October 1918, shortly before the armistice, he won a quadrangular (four-player) tournament, half a point ahead of Rubinstein.\n\nDespite his superb playing results, chess was not Lasker's only interest. His parents recognized his intellectual talents, especially for mathematics, and sent the adolescent Emanuel to study in Berlin (where he found he also had a talent for chess). Lasker gained his abitur (high school graduation certificate) at Landsberg an der Warthe, now a Polish town named Gorzów Wielkopolski but then part of Prussia. He then studied mathematics and philosophy at the universities in Berlin, Göttingen (where David Hilbert was one of his doctoral advisors) and Heidelberg.\n\nIn 1895 Lasker published two mathematical articles in \"Nature\". On the advice of David Hilbert he registered for doctoral studies at Erlangen during 1900–1902. In 1901 he presented his doctoral thesis \"Über Reihen auf der Convergenzgrenze\" (\"On Series at Convergence Boundaries\") at Erlangen and in the same year it was published by the Royal Society. He was awarded a doctorate in mathematics in 1902. His most significant mathematical article, in 1905, published a theorem of which Emmy Noether developed a more generalized form, which is now regarded as of fundamental importance to modern algebra and algebraic geometry.\n\nLasker held short-term positions as a mathematics lecturer at Tulane University in New Orleans (1893) and Victoria University in Manchester (1901; Victoria University was one of the \"parents\" of the current University of Manchester). However, he was unable to secure a longer-term position, and pursued his scholarly interests independently.\n\nIn 1906 Lasker published a booklet titled \"Kampf\" (\"Struggle\"), in which he attempted to create a general theory of all competitive activities, including chess, business and war. He produced two other books which are generally categorized as philosophy, \"Das Begreifen der Welt\" (\"Comprehending the World\"; 1913) and \"Die Philosophie des Unvollendbar\" (sic; \"The Philosophy of the Unattainable\"; 1918).\n\nIn 1896–97 Lasker published his book \"Common Sense in Chess\", based on lectures he had given in London in 1895.\n\nIn 1903, Lasker played in Ostend against Mikhail Chigorin, a six-game match that was sponsored by the wealthy lawyer and industrialist Isaac Rice in order to test the Rice Gambit. Lasker narrowly lost the match. Three years later Lasker became secretary of the Rice Gambit Association, founded by Rice in order to promote the Rice Gambit, and in 1907 Lasker quoted with approval Rice's views on the convergence of chess and military strategy.\n\nIn November 1904, Lasker founded \"Lasker's Chess Magazine\", which ran until 1909.\n\nFor a short time in 1906 Emanuel Lasker was interested in the strategy game Go, but soon returned to chess. He was introduced to the game by his namesake Edward Lasker, who wrote a successful book \"Go and Go-Moku\" in 1934.\n\nAt the age of 42, in July 1911, Lasker married Martha Cohn (née Bamberger), a rich widow who was a year older than Lasker and already a grandmother. They lived in Berlin. Martha Cohn wrote popular stories under the pseudonym \"L. Marco\".\n\nDuring World War I, Lasker invested all of his savings in German war bonds, which lost nearly their entire value with the wartime and post-war inflation. During the war, he wrote a pamphlet which claimed that civilization would be in danger if Germany lost the war.\n\nIn January 1920 Lasker and José Raúl Capablanca signed an agreement to play a World Championship match in 1921, noting that Capablanca was not free to play in 1920. Because of the delay, Lasker insisted on a final clause that allowed him to play anyone else for the championship in 1920, that nullified the contract with Capablanca if Lasker lost a title match in 1920, and that stipulated that if Lasker resigned the title Capablanca should become World Champion. Lasker had previously included in his agreement before World War I to play Akiba Rubinstein for the title a similar clause that if he resigned the title, it should become Rubinstein's.\n\nA report in the \"American Chess Bulletin\" (July–August 1920 issue) said that Lasker had resigned the world title in favor of Capablanca because the conditions of the match were unpopular in the chess world. The \"American Chess Bulletin\" speculated that the conditions were not sufficiently unpopular to warrant resignation of the title, and that Lasker's real concern was that there was not enough financial backing to justify his devoting nine months to the match. When Lasker resigned the title in favor of Capablanca he was unaware that enthusiasts in Havana had just raised $20,000 to fund the match provided it was played there. When Capablanca learned of Lasker's resignation he went to the Netherlands, where Lasker was living at the time, to inform him that Havana would finance the match. In August 1920 Lasker agreed to play in Havana, but insisted that he was the challenger as Capablanca was now the champion. Capablanca signed an agreement that accepted this point, and soon afterwards published a letter confirming this. Lasker also stated that, if he beat Capablanca, he would resign the title so that younger masters could compete for it.\n\nThe match was played in March–April 1921. After four draws, the fifth game saw Lasker blunder with Black in an equal ending. Capablanca's solid style allowed him to easily draw the next four games, without taking any risks. In the tenth game, Lasker as White played a position with an isolated queen pawn but failed to create the necessary activity and Capablanca reached a superior ending, which he duly won. The eleventh and fourteenth games were also won by Capablanca, and Lasker resigned the match.\n\nReuben Fine and Harry Golombek attributed this to Lasker's being in mysteriously poor form. On the other hand, Vladimir Kramnik thought that Lasker played quite well and the match was an \"even and fascinating fight\" until Lasker blundered in the last game, and explained that Capablanca was 20 years younger, a slightly stronger player, and had more recent competitive practice.\n\nLasker was in his early 50s when he lost the world championship to Capablanca, and he retired from serious match play afterwards; his only other match was a short exhibition against Frank James Marshall in 1940, which Lasker lost. After winning the Moravská Ostrava 1923 chess tournament (without a single loss), the New York 1924 chess tournament (1½ points ahead of Capablanca) and finishing second at Moscow in 1925 (1½ points behind Efim Bogoljubow, ½ point ahead of Capablanca), he effectively retired from serious chess.\nDuring the Moscow 1925 chess tournament, Emanuel Lasker received a telegram informing him that the drama written by himself and his brother Berthold, \"Vom Menschen die Geschichte\" (\"History of Mankind\"), had been accepted for performance at the Lessing theatre in Berlin. Emanuel Lasker was so distracted by this news that he lost badly to Carlos Torre the same day. The play, however, was not a success.\n\nIn 1926, Lasker wrote \"Lehrbuch des Schachspiels\", which he re-wrote in English in 1927 as \"Lasker's Manual of Chess\". He also wrote books on other games of mental skill: \"Encyclopedia of Games\" (1929) and \"Das verständige Kartenspiel\" (means \"Sensible Card Play\"; 1929; English translation in the same year), both of which posed a problem in the mathematical analysis of card games; \"Brettspiele der Völker\" (\"Board Games of the Nations\"; 1931), which includes 30 pages about Go and a section about a game he had invented in 1911, Lasca.\n\nIn 1930, Lasker was a special correspondent for Dutch and German newspapers reporting on the Culbertson-Buller bridge match during which he became a registered teacher of the Culbertson system. He became an expert bridge player, representing Germany at international events in the early 1930s, and wrote \"Das Bridgespiel\" (\"The Game of Bridge\") in 1931.\n\nIn October 1928 Emanuel Lasker's brother Berthold died.\n\nIn spring 1933 Adolf Hitler started a campaign of discrimination and intimidation against Jews, depriving them of their property and citizenship. Lasker and his wife Martha, who were both Jewish, were forced to leave Germany in the same year. After a short stay in England, in 1935 they were invited to live in the USSR by Nikolai Krylenko, the Commissar of Justice who was responsible for the Moscow show trials and, in his other capacity as Sports Minister, was an enthusiastic supporter of chess. In the USSR, Lasker renounced his German citizenship and received Soviet citizenship. He took permanent residence in Moscow, and was given a post at Moscow's Institute for Mathematics and a post of trainer of the USSR national team.\nLasker returned to competitive chess to make some money, finishing fifth in Zürich 1934 and third in Moscow 1935 (undefeated, ½ point behind Mikhail Botvinnik and Salo Flohr; ahead of Capablanca, Rudolf Spielmann and several Soviet masters), sixth in Moscow 1936 and seventh equal in Nottingham 1936. His performance in Moscow 1935 at age 66 was hailed as \"a biological miracle.\"\n\nJoseph Stalin's Great Purge started at about the same time the Laskers arrived in the USSR.\n\nIn August 1937, Martha and Emanuel Lasker decided to leave the Soviet Union, and they moved, via the Netherlands, to the United States (first Chicago, next New York) in October 1937. The next year Emanuel Lasker's patron, Krylenko, was removed from his office and executed. Meanwhile, in the United States Lasker tried to support himself by giving chess and bridge lectures and exhibitions, as he was now too old for serious competition. In 1940 he published his last book, \"The Community of the Future\", in which he proposed solutions for serious political problems, including anti-Semitism and unemployment.\n\nLasker died of a kidney infection in New York on January 11, 1941, at the age of 72, as a charity patient at the Mount Sinai Hospital. He was buried at historic Beth Olom Cemetery, Queens, New York. His wife Martha and his sister, Mrs. Lotta Hirschberg, survived him.\n\nLasker was considered to have a \"psychological\" method of play in which he considered the subjective qualities of his opponent, in addition to the objective requirements of his position on the board. Richard Réti published a lengthy analysis of Lasker's play in which he concluded that Lasker deliberately played inferior moves that he knew would make his opponent uncomfortable. W. H. K. Pollock commented, \"It is no easy matter to reply correctly to Lasker's bad moves.\"\n\nLasker himself denied the claim that he deliberately played bad moves, and most modern writers agree. According to Grandmaster Andrew Soltis and International Master John L. Watson, the features that made his play mysterious to contemporaries now appear regularly in modern play: the g2–g4 \"Spike\" attack against the Dragon Sicilian; sacrifices to gain positional advantage; playing the \"practical\" move rather than trying to find the best move; counterattacking and complicating the game before a disadvantage became serious. Former World Champion Vladimir Kramnik said, \"He realized that different types of advantage could be interchangeable: tactical edge could be converted into strategic advantage and vice versa\", which mystified contemporaries who were just becoming used to the theories of Steinitz as codified by Siegbert Tarrasch.\n\nMax Euwe opined that the real reason behind Lasker's success was his \"exceptional defensive technique\" and that \"almost all there is to say about defensive chess can be demonstrated by examples from the games of Steinitz and Lasker\", with the former exemplifying passive defence and the latter an active defence.\n\nThe famous win against José Raúl Capablanca at St. Petersburg in 1914, which Lasker needed in order to retain any chance of catching up with Capablanca, is sometimes offered as evidence of his \"psychological\" approach. Reuben Fine describes Lasker's choice of opening, the Exchange Variation of the Ruy Lopez, as \"innocuous but psychologically potent\". However, an analysis of Lasker's use of this variation throughout his career concludes that he had excellent results with it as White against top-class opponents, and sometimes used it in \"must-win\" situations. Luděk Pachman writes that Lasker's choice presented his opponent with a dilemma: with only a ½ point lead, Capablanca would have wanted to play safe; but the Exchange Variation's pawn structure gives White an endgame advantage, and Black must use his aggressively in the middlegame to nullify this. In Kramnik's opinion, Lasker's play in this game demonstrated deep positional understanding, rather than psychology.\n\nFine reckoned Lasker paid little attention to the openings, but Capablanca thought Lasker knew the openings very well but disagreed with a lot of contemporary opening analysis. In fact before the 1894 world title match, Lasker studied the openings thoroughly, especially Steinitz's favorite lines. He played primarily e4 openings, particularly the Ruy Lopez. He opened with 1.d4 relatively rarely, although his d4 games had a higher winning percentage than his e4 ones. With the Black pieces, he mainly answered 1.e4 with the French Defense and 1.d4 with the Queen's Gambit. Lasker also used the Sicilian Defense fairly often. In Capablanca's opinion, no player surpassed Lasker in the ability to assess a position quickly and accurately, in terms of who had the better prospects of winning and what strategy each side should adopt. Capablanca also wrote that Lasker was so adaptable that he played in no definite style, and that he was both a tenacious defender and a very efficient finisher of his own attacks.\n\nLasker followed Steinitz's principles, and both demonstrated a completely different chess paradigm than the “romantic” mentality before them. Thanks to Steinitz and Lasker, positional players gradually became common (Tarrasch, Schlechter, and Rubinstein stand out.) But, while Steinitz created a new school of chess thought, Lasker’s talents were far harder for the masses to grasp; hence there was no Lasker school.\n\nIn addition to his enormous chess skill, Lasker was said to have an excellent competitive temperament: his rival Siegbert Tarrasch once said, \"Lasker occasionally loses a game, but he never loses his head.\" Lasker enjoyed the need to adapt to varying styles and to the shifting fortunes of tournaments. Although very strong in matches, he was even stronger in tournaments. For over 20 years, he always finished ahead of the younger Capablanca: at St. Petersburg 1914, New York 1924, Moscow 1925, and Moscow 1935. Only in 1936 (15 years after their match), when Lasker was 67, did Capablanca finish ahead of him.\n\nIn 1964, \"Chessworld\" magazine published an article in which future World Champion Bobby Fischer listed the ten greatest players in history. Fischer did not include Lasker in the list, deriding him as a \"coffee-house player [who] knew nothing about openings and didn't understand positional chess\". In a poll of the world's leading players taken some time after Fischer's list appeared, Tal, Korchnoi, and Robert Byrne all said that Lasker was the greatest player ever. Both Pal Benko and Byrne stated that Fischer later reconsidered and said that Lasker was a great player.\n\nStatistical ranking systems place Lasker high among the greatest players of all time. The book \"Warriors of the Mind\" places him sixth, behind Garry Kasparov, Anatoly Karpov, Fischer, Mikhail Botvinnik and Capablanca. In his 1978 book \"The Rating of Chessplayers, Past and Present\", Arpad Elo gave retrospective ratings to players based on their performance over the best five-year span of their career. He concluded that Lasker was the joint second strongest player of those surveyed (tied with Botvinnik and behind Capablanca). The most up-to-date system, Chessmetrics, is rather sensitive to the length of the periods being compared, and ranks Lasker between fifth and second strongest of all time for peak periods ranging in length from one to twenty years. Its author, the statistician Jeff Sonas, concluded that only Kasparov and Karpov surpassed Lasker's long-term dominance of the game. By Chessmetrics' reckoning, Lasker was the number 1 player in 292 different months—a total of over 24 years. His first No. 1 rank was in June 1890, and his last in December 1926—a span of 36½ years. Chessmetrics also considers him the strongest 67-year-old in history: in December 1935, at age 67 years and 0 months, his rating was 2691 (number 7 in the world), well above second-place Viktor Korchnoi's rating at that age (2660, number 39 in the world, in March 1998).\n\nLasker founded no school of players who played in a similar style. Max Euwe, World Champion 1935–1937 and a prolific writer of chess manuals, who had a lifetime 0–3 score against Lasker, said, \"It is not possible to learn much from him. One can only stand and wonder.\" However, Lasker's pragmatic, combative approach had a great influence on Soviet players like Mikhail Tal and Viktor Korchnoi.\n\nThere are several \"Lasker Variations\" in the chess openings, including Lasker's Defense to the Queen's Gambit, Lasker's Defense to the Evans Gambit (which effectively ended the use of this gambit in tournament play until a revival in the 1990s), and the Lasker Variation in the McCutcheon Variation of the French Defense.\n\nOne of Lasker's most famous games is Lasker–Bauer, Amsterdam 1889, in which he sacrificed both bishops in a maneuver later repeated in a number of games. Similar sacrifices had already been played by Cecil Valentine De Vere and John Owen, but these were not in major events and Lasker probably had not seen them.\n\nLasker was shocked by the poverty in which Wilhelm Steinitz died and did not intend to die in similar circumstances. He became notorious for demanding high fees for playing matches and tournaments, and he argued that players should own the copyright in their games rather than let publishers get all the profits. These demands initially angered editors and other players, but helped to pave the way for the rise of full-time chess professionals who earn most of their living from playing, writing and teaching. Copyright in chess games had been contentious at least as far back as the mid-1840s, and Steinitz and Lasker vigorously asserted that players should own the copyright and wrote copyright clauses into their match contracts. However, Lasker's demands that challengers should raise large purses prevented or delayed some eagerly awaited World Championship matches—for example Frank James Marshall challenged him in 1904 to a match for the World Championship but could not raise the stakes demanded by Lasker until 1907. This problem continued throughout the reign of his successor Capablanca.\n\nSome of the controversial conditions that Lasker insisted on for championship matches led Capablanca to attempt twice (1914 and 1922) to publish rules for such matches, to which other top players readily agreed.\n\nLasker was also a mathematician. In his 1905 article on commutative algebra, Lasker introduced the theory of primary decomposition of ideals, which has influence in the theory of Noetherian rings. Rings having the \"primary decomposition property\" are called \"Laskerian rings\" in his honor.\n\nHis attempt to create a general theory of all competitive activities were followed by more consistent efforts from von Neumann on game theory, and his later writings about card games presented a significant issue in the mathematical analysis of card games.\n\nHowever, his dramatic and philosophical works have never been highly regarded.\n\nLasker was a good friend of Albert Einstein, who wrote the introduction to the posthumous biography \"Emanuel Lasker, The Life of a Chess Master\" from Dr. Jacques Hannak (1952). In this preface Einstein express his satisfaction at having met Lasker, writing:\nPoet Else Lasker-Schüler was his sister-in-law. Edward Lasker, born in Kempen (Kępno), Greater Poland (then Prussia), the German-American chess master, engineer, and author, claimed that he was distantly related to Emanuel Lasker. They both played in the great New York 1924 chess tournament.\n\n\n\n\n\nIn Michael Chabon's alternate history mystery novel, \"The Yiddish Policemen's Union\", the murdered man, Mendel Shpilman (born during the 1960s), being a chess enthusiast, uses the name \"Emanuel Lasker\" as an alias. The reference is clearly understood by the protagonist, Detective Meyer Landsman, because he has also studied chess.\n\nThe following table gives Lasker's placings and scores in tournaments. The first \"Score\" column gives the number of points on the total possible. In the second \"Score\" column, \"+\" indicates the number of won games, \"−\" the number of losses, and \"=\" the number of draws.\n\nHere are Lasker's results in matches. The first \"Score\" column gives the number of points on the total possible. In the second \"Score\" column, \"+\" indicates the number of won games, \"−\" the number of losses, and \"=\" the number of draws.\n\n\n\n"}
{"id": "10538912", "url": "https://en.wikipedia.org/wiki?curid=10538912", "title": "Flat topology", "text": "Flat topology\n\nIn mathematics, the flat topology is a Grothendieck topology used in algebraic geometry. It is used to define the theory of flat cohomology; it also plays a fundamental role in the theory of descent (faithfully flat descent). The term \"flat\" here comes from flat modules.\n\nThere are several slightly different flat topologies, the most common of which are the fppf topology and the fpqc topology. \"fppf\" stands for ', and in this topology, a morphism of affine schemes is a covering morphism if it is faithfully flat and of finite presentation. \"fpqc\" stands for ', and in this topology, a morphism of affine schemes is a covering morphism if it is faithfully flat. In both categories, a covering family is defined be a family which is a cover on Zariski open subsets. In the fpqc topology, any faithfully flat and quasi-compact morphism is a cover. These topologies are closely related to descent. The \"pure\" faithfully flat topology without any further finiteness conditions such as quasi compactness or finite presentation is not used much as is not subcanonical; in other words, representable functors need not be sheaves.\n\nUnfortunately the terminology for flat topologies is not standardized. Some authors use the term \"topology\" for a pretopology, and there are several slightly different pretopologies sometimes called the fppf or fpqc (pre)topology, which sometimes give the same topology.\n\nFlat cohomology was introduced by Grothendieck in about 1960.\n\nLet \"X\" be an affine scheme. We define an fppf cover of \"X\" to be a finite and jointly surjective family of morphisms\n\nwith each \"X\" affine and each \"φ\" flat, finitely presented. This generates a pretopology: for \"X\" arbitrary, we define an fppf cover of \"X\" to be a family\n\nwhich is an fppf cover after base changing to an open affine subscheme of \"X\". This pretopology generates a topology called the \"fppf topology\". (This is not the same as the topology we would get if we started with arbitrary \"X\" and \"X\" and took covering families to be jointly surjective families of flat, finitely presented morphisms.) We write \"Fppf\" for the category of schemes with the fppf topology.\n\nThe small fppf site of X is the category \"O\"(\"X\") whose objects are schemes \"U\" with a fixed morphism \"U\" → \"X\" which is part of some covering family. (This does not imply that the morphism is flat, finitely presented.) The morphisms are morphisms of schemes compatible with the fixed maps to \"X\". The large fppf site of X is the category \"Fppf/X\", that is, the category of schemes with a fixed map to \"X\", considered with the fppf topology.\n\n\"Fppf\" is an abbreviation for \"fidèlement plate de présentation finie\", that is, \"faithfully flat and of finite presentation\". Every surjective family of flat and finitely presented morphisms is a covering family for this topology, hence the name. The definition of the fppf pretopology can also be given with an extra quasi-finiteness condition; it follows from Corollary 17.16.2 in \nEGA IV that this gives the same topology.\n\nLet \"X\" be an affine scheme. We define an fpqc cover of \"X\" to be a finite and jointly surjective family of morphisms {\"u\" : \"X\" → \"X\"} with each \"X\" affine and each \"u\" flat. This generates a pretopology: For \"X\" arbitrary, we define an fpqc cover of \"X\" to be a family {\"u\" : \"X\" → \"X\"} which is an fpqc cover after base changing to an open affine subscheme of \"X\". This pretopology generates a topology called the \"fpqc topology\". (This is not the same as the topology we would get if we started with arbitrary \"X\" and \"X\" and took covering families to be jointly surjective families of flat morphisms.) We write \"Fpqc\" for the category of schemes with the fpqc topology.\n\nThe small fpqc site of X is the category \"O\"(\"X\") whose objects are schemes \"U\" with a fixed morphism \"U\" → \"X\" which is part of some covering family. The morphisms are morphisms of schemes compatible with the fixed maps to \"X\". The large fpqc site of X is the category \"Fpqc/X\", that is, the category of schemes with a fixed map to \"X\", considered with the fpqc topology.\n\n\"Fpqc\" is an abbreviation for \"fidèlement plate quasi-compacte\", that is, \"faithfully flat and quasi-compact\". Every surjective family of flat and quasi-compact morphisms is a covering family for this topology, hence the name.\n\nThe procedure for defining the cohomology groups is the standard one: cohomology is defined as the sequence of derived functors of the functor taking the sections of a sheaf of abelian groups.\n\nWhile such groups have a number of applications, they are not in general easy to compute, except in cases where they reduce to other theories, such as the étale cohomology.\n\nThe following example shows why the \"faithfully flat topology\" without any finiteness conditions does not behave well. Suppose \"X\" is the affine line over an algebraically closed field \"k\". For each closed point \"x\" of \"X\" we can consider the local ring \"R\" at this point, which is a discrete valuation ring whose spectrum has one closed point and one open (generic) point. We glue these spectra together by identifying their open points to get a scheme \"Y\". There is a natural map from \"Y\" to \"X\". The affine line \"X\" is covered by the sets Spec(\"R\") which are open in the faithfully flat topology, and each of these sets has a natural map to \"Y\", and these maps are the same on intersections. However they cannot be combined to give a map from \"X\" to \"Y\", because the underlying spaces of \"X\" and \"Y\" have different topologies.\n\n\n\n"}
{"id": "225611", "url": "https://en.wikipedia.org/wiki?curid=225611", "title": "Floradora", "text": "Floradora\n\n\"Floradora\", also called Keyword, was a doubly enciphered diplomatic code used by the Germans during the Second World War. The Allies used tabulating equipment, created by IBM, to break the code over period of more than a year in 1941 and 1942.\n\n"}
{"id": "364377", "url": "https://en.wikipedia.org/wiki?curid=364377", "title": "Functional equation", "text": "Functional equation\n\nIn mathematics, a functional equation is any equation in which the unknown represents a function.\nOften, the equation relates the value of a function (or functions) at some point with its values at other points. For instance, properties of functions can be determined by considering the types of functional equations they satisfy. The term \"functional equation\" usually refers to equations that cannot be simply reduced to algebraic equations.\n\n\n\n\nExponentiating,\n\n\nBut if we wrote \"ƒ\"(\"a\", \"b\") instead of \"a\" ○ \"b\" then the associative law would look more like what one conventionally thinks of as a functional equation,\n\nOne feature that all of the examples listed above share in common is that, in each case, two or more known functions (sometimes multiplication by a constant, sometimes addition of two variables, sometimes the identity function) are inside the argument of the unknown functions to be solved for.\n\nWhen it comes to asking for \"all\" solutions, it may be the case that conditions from mathematical analysis should be applied; for example, in the case of the \"Cauchy equation\" mentioned above, the solutions that are continuous functions are the 'reasonable' ones, while other solutions that are not likely to have practical application can be constructed (by using a Hamel basis for the real numbers as vector space over the rational numbers). The Bohr–Mollerup theorem is another well-known example.\n\nSolving functional equations can be very difficult, but there are some common methods of solving them. For example, in dynamic programming a variety of successive approximation methods are used to solve Bellman's functional equation, including methods based on fixed point iterations.\n\nA main method of solving elementary functional equations is substitution. It is often useful to prove surjectivity or injectivity and prove oddness or evenness, if possible. It is also useful to guess possible solutions. Induction is a useful technique to use when the function is only defined for rational or integer values.\n\nA discussion of involutory functions is topical. For example, consider the function\nComposing with itself gives Babbage's functional equation (1820),\nSeveral other functions also satisfy the functional equation\nincluding\nwhich includes the previous three as special cases or limits.\n\nExample 1. Find all functions that satisfy\nfor all , assuming \"ƒ\" is a real-valued function.\n\nLet  =  = 0, \n\nSo \"ƒ\"(0) = 0 and \"ƒ\"(0) = 0.\n\nNow, let  = −,\nA square of a real number is nonnegative, and a sum of nonnegative numbers is zero iff both numbers are 0.\n\nSo \"ƒ\"(x) = 0 for all and is the only solution.\n\n\n\n"}
{"id": "22833268", "url": "https://en.wikipedia.org/wiki?curid=22833268", "title": "Geometric design", "text": "Geometric design\n\nGeometric design (GD) is a branch of computational geometry. It deals with the construction and representation of free-form curves, surfaces, or volumes and is closely related to geometric modeling. Core problems are curve and surface modelling and representation. GD studies especially the construction and manipulation of curves and surfaces given by a set of points using polynomial, rational, piecewise polynomial, or piecewise rational methods. The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level-set method.\n\nApplication areas include shipbuilding, aircraft, and automotive industries, as well as architectural design. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by shipbuilders of 1960s.\n\nGeometric models can be built for objects of any dimension in any geometric space. Both 2D and 3D geometric models are extensively used in computer graphics. 2D models are important in computer typography and technical drawing. 3D models are central to computer-aided design and manufacturing, and many applied technical fields such as geology and medical image processing.\n\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an algorithm. They are also contrasted with digital images and volumetric models; and with implicit mathematical models such as the zero set of an arbitrary polynomial. However, the distinction is often blurred: for instance, geometric shapes can be represented by objects; a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, the modeling of fractal objects often requires a combination of geometric and procedural techniques.\n\nGeometric problems originating in architecture can lead to interesting research and results in geometry processing, computer-aided geometric design, and discrete differential geometry.\n\n\n"}
{"id": "4577462", "url": "https://en.wikipedia.org/wiki?curid=4577462", "title": "Geometric modeling", "text": "Geometric modeling\n\nGeometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes.\n\nThe shapes studied in geometric modeling are mostly two- or three-dimensional, although many of its tools and principles can be applied to sets of any finite dimension. Today most geometric modeling is done with computers and for computer-based applications. Two-dimensional models are important in computer typography and technical drawing. Three-dimensional models are central to computer-aided design and manufacturing (CAD/CAM), and widely used in many applied technical fields such as civil and mechanical engineering, architecture, geology and medical image processing.\n\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an opaque algorithm that generates its appearance. They are also contrasted with digital images and volumetric models which represent the shape as a subset of a fine regular partition of space; and with fractal models that give an infinitely recursive definition of the shape. However, these distinctions are often blurred: for instance, a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, a fractal model yields a parametric or implicit model when its recursive definition is truncated to a finite depth.\n\nNotable awards of the area are the John A. Gregory Memorial Award and the Bézier award.\n\n\nGeneral textbooks:\nFor multi-resolution (multiple level of detail) geometric modeling :\nSubdivision methods (such as subdivision surfaces):\n\n"}
{"id": "49270505", "url": "https://en.wikipedia.org/wiki?curid=49270505", "title": "Hafnian", "text": "Hafnian\n\nIn mathematics, the hafnian of an adjacency matrix of a graph is the number of perfect matchings in the graph. It was so named by Eduardo R. Caianiello \"to mark the fruitful period of stay in Copenhagen (Hafnia in Latin).\"\n\nThe hafnian of a 2n × 2n symmetric matrix is computed as\n\nwhere formula_2 is the symmetric group on [\"2n\"].\n\nEquivalently,\nwhere formula_4 is the set of all 1-factors (perfect matchings) on the complete graph formula_5, namely the set of all formula_6 ways to partition the set formula_7 into formula_8 subsets of size formula_9.\n"}
{"id": "2801004", "url": "https://en.wikipedia.org/wiki?curid=2801004", "title": "Handle decompositions of 3-manifolds", "text": "Handle decompositions of 3-manifolds\n\nIn mathematics, a handle decomposition of a 3-manifold allows simplification of the original 3-manifold into pieces which are easier to study.\n\nAn important method used to decompose into handlebodies is the Heegaard splitting, which gives us a decomposition in two handlebodies of equal genus.\n\nAs an example: lens spaces are orientable 3-spaces, and allow decomposition into two solid-tori which are genus-one-handlebodies. The genus one non-orientable space is a space which is the union of two solid Klein bottles and corresponds to the twisted product of the 2-sphere and the 1-sphere: formula_1.\n\nEach orientable 3-manifold is the union of exactly two orientable handlebodies; meanwhile, each non-orientable one needs three orientable handlebodies.\n\nThe minimal genus of the glueing boundary determines what is known as the Heegaard genus. For non-orientable spaces an interesting invariant is the tri-genus.\n\n"}
{"id": "2976061", "url": "https://en.wikipedia.org/wiki?curid=2976061", "title": "Harnack's inequality", "text": "Harnack's inequality\n\nIn mathematics, Harnack's inequality is an inequality relating the values of a positive harmonic function at two points, introduced by . , and generalized Harnack's inequality to solutions of elliptic or parabolic partial differential equations. Perelman's solution of the Poincaré conjecture uses a version of the Harnack inequality, found by , for the Ricci flow. Harnack's inequality is used to prove Harnack's theorem about the convergence of sequences of harmonic functions. Harnack's inequality can also be used to show the interior regularity of weak solutions of partial differential equations.\n\nHarnack's inequality applies to a non-negative function \"f\" defined on a closed ball in R with radius \"R\" and centre \"x\". It states that, if \"f\" is continuous on the closed ball and harmonic on its interior, then for every point \"x\" with |\"x\" − \"x\"| = \"r\" < \"R\",\n\nIn the plane R (\"n\" = 2) the inequality can be written:\n\nFor general domains formula_3 in formula_4 the inequality can be stated as follows: If formula_5 is a bounded domain with formula_6, then there is a constant formula_7 such that \n\nfor every twice differentiable, harmonic and nonnegative function formula_9. The constant formula_7 is independent of formula_11; it depends only on the domains formula_3 and formula_5.\n\nBy Poisson's formula\n\nwhere \"ω\" is the area of the unit sphere in R and \"r\" = |\"x\" − \"x\"|.\n\nSince\n\nthe kernel in the integrand satisfies\n\nHarnack's inequality follows by substituting this inequality in the above integral and using the fact that the average of a harmonic function over a sphere equals its value at the center of the sphere:\n\nFor elliptic partial differential equations, Harnack's inequality states that the supremum of a positive solution in some connected open region is bounded by some constant times the infimum, possibly with an added term containing a functional norm of the data:\n\nThe constant depends on the ellipticity of the equation and the connected open region.\n\nThere is a version of Harnack's inequality for linear parabolic PDEs such as heat equation.\n\nLet formula_19 be a smooth (bounded) domain in formula_20 and consider the linear elliptic operator\n\nwith smooth and bounded coefficients and a positive definite matrix formula_22. Suppose that formula_23 is a solution of\n\nsuch that\n\nLet formula_27 be compactly contained in formula_19 and choose formula_29. Then there exists a constant \"C\" > 0 (depending only on \"K\", formula_30 and the coefficients of formula_31) such that, for each formula_32,\n\n\n"}
{"id": "2336114", "url": "https://en.wikipedia.org/wiki?curid=2336114", "title": "Hilbert's twenty-second problem", "text": "Hilbert's twenty-second problem\n\nHilbert's twenty-second problem is the penultimate entry in the celebrated list of 23 Hilbert problems compiled in 1900 by David Hilbert. It entails the uniformization of analytic relations by means of automorphic functions.\n\nThe entirety of the original problem statement is as follows:\nAs Poincaré was the first to prove, it is always possible to reduce any algebraic relation between two variables to uniformity by the use of automorphic functions of one variable. That is, if any algebraic equation in two variables be given, there can always be found for these variables two such single valued automorphic functions of a single variable that their substitution renders the given algebraic equation an identity. The generalization of this fundamental theorem to any analytic non-algebraic relations whatever between two variables has likewise been attempted with success by Poincaré, though by a way entirely different from that which served him in the special problem first mentioned. From Poincaré's proof of the possibility of reducing to uniformity an arbitrary analytic relation between two variables, however, it does not become apparent whether the resolving functions can be determined to meet certain additional conditions. Namely, it is not shown whether the two single valued functions of the one new variable can be so chosen that, while this variable traverses the regular domain of those functions, the totality of all regular points of the given analytic field are actually reached and represented. On the contrary it seems to be the case, from Poincaré's investigations, that there are beside the branch points certain others, in general infinitely many other discrete exceptional points of the analytic field, that can be reached only by making the new variable approach certain limiting points of the functions. In view of the fundamental importance of Poincaré's formulation of the question it seems to me that an elucidation and resolution of this difficulty is extremely desirable.\n\nIn conjunction with this problem comes up the problem of reducing to uniformity an algebraic or any other analytic relation among three or more complex variables—a problem which is known to be solvable in many particular cases. Toward the solution of this the recent investigations of Picard on algebraic functions of two variables are to be regarded as welcome and important preliminary studies.\n\nKoebe proved the general uniformization theorem that if a Riemann surface is homeomorphic to an open subset of the complex sphere (or equivalently if every Jordan curve separates it), then it is conformally equivalent to an open subset of the complex sphere.\n\nThis problem is currently open. Some progress has been made by Griffith and Bers.\n"}
{"id": "19597108", "url": "https://en.wikipedia.org/wiki?curid=19597108", "title": "Hobby–Rice theorem", "text": "Hobby–Rice theorem\n\nIn mathematics, and in particular the necklace splitting problem, the Hobby–Rice theorem is a result that is useful in establishing the existence of certain solutions. It was proved in 1965 by Charles R. Hobby and John R. Rice; a simplified proof was given in 1976 by A. Pinkus.\n\nGiven an integer \"k\", define a \"partition\" of the interval [0,1] as a sequence of numbers which divide the interval to formula_1 subintervals:\n\nDefine a \"signed partition\" as a partition in which each subinterval formula_3 has an associated sign formula_4:\n\nThe Hobby-Rice theorem says that for every \"k\" continuously integrable functions:\n\nthere exists a signed partition of [0,1] such that:\n\n(in other words: for each of the \"k\" functions, its integral over the positive subintervals equals its integral over the negative subintervals).\n\nThe theorem was used by Noga Alon in the context of necklace splitting in 1987.\n\nSuppose the interval [0,1] is a cake. There are \"k\" partners and each of the \"k\" functions is a value-density function of one partner. We want to divide the cake to two parts such that \"all\" partners agree that the parts have the same value. The Hobby-Rice theorem implies that this can be done with \"k\" cuts.\n"}
{"id": "4594282", "url": "https://en.wikipedia.org/wiki?curid=4594282", "title": "Holomorphic separability", "text": "Holomorphic separability\n\nIn mathematics in complex analysis, the concept of holomorphic separability is a measure of the richness of the set of holomorphic functions on a complex manifold or complex-analytic space.\n\nA complex manifold or complex space formula_1 is said to be holomorphically separable, if whenever \"x\" ≠ \"y\" are two points in formula_1, there exists a holomorphic function formula_3, such that \"f\"(\"x\") ≠ \"f\"(\"y\").\n\nOften one says the holomorphic functions \"separate points\".\n\n"}
{"id": "8139434", "url": "https://en.wikipedia.org/wiki?curid=8139434", "title": "International Journal of Mathematics and Mathematical Sciences", "text": "International Journal of Mathematics and Mathematical Sciences\n\nThe International Journal of Mathematics and Mathematical Sciences is a biweekly peer-reviewed mathematics journal. It was established in 1978 by Lokenath Debnath.\n\n"}
{"id": "1001293", "url": "https://en.wikipedia.org/wiki?curid=1001293", "title": "Irreducibility (mathematics)", "text": "Irreducibility (mathematics)\n\nIn mathematics, the concept of irreducibility is used in several ways.\n\n"}
{"id": "4221033", "url": "https://en.wikipedia.org/wiki?curid=4221033", "title": "K-theory (physics)", "text": "K-theory (physics)\n\nIn string theory, K-theory classification refers to a conjectured application of K-theory (in abstract algebra and algebraic topology) to superstrings, to classify the allowed Ramond–Ramond field strengths as well as the charges of stable D-branes.\n\nIn condensed matter physics K-theory has also found important applications, specially in the topological classification of topological insulators, superconductors and stable Fermi surfaces (, ).\n\nThis conjecture, applied to D-brane charges, was first proposed by . It was popularized by who demonstrated that in type IIB string theory arises naturally from Ashoke Sen's realization of arbitrary D-brane configurations as stacks of D9 and anti-D9-branes after tachyon condensation.\n\nSuch stacks of branes are inconsistent in a non-torsion Neveu–Schwarz (NS) 3-form background, which, as was highlighted by , complicates the extension of the K-theory classification to such cases. suggested a solution to this problem: D-branes are in general classified by a twisted K-theory, that had earlier been defined by .\n\nThe K-theory classification of D-branes has had numerous applications. For example, used it to argue that there are eight species of orientifold one-plane. applied the K-theory classification to derive new consistency conditions for flux compactifications. K-theory has also been used to conjecture a formula for the topologies of T-dual manifolds by . Recently K-theory has been conjectured to classify the spinors in compactifications on generalized complex manifolds.\n\nDespite these successes, RR fluxes are not quite classified by K-theory. argued that the K-theory classification is incompatible with S-duality in IIB string theory.\n\nIn addition, if one attempts to classify fluxes on a compact ten-dimensional spacetime, then a complication arises due to the self-duality of the RR fluxes. The duality uses the Hodge star, which depends on the metric and so is continuously valued and in particular is generically irrational. Thus not all of the RR fluxes, which are interpreted as the Chern characters in K-theory, can be rational. However Chern characters are always rational, and so the K-theory classification must be replaced. One needs to choose a half of the fluxes to quantize, or a \"polarization\" in the geometric quantization-inspired language of Diaconescu, Moore, and Witten and later of . Alternately one may use the K-theory of a 9-dimensional time slice as has been done by .\n\nIn the classical limit of type II string theory, which is type II supergravity, the Ramond–Ramond field strengths are differential forms. In the quantum theory the well-definedness of the partition functions of D-branes implies that the RR field strengths obey Dirac quantization conditions when spacetime is compact, or when a spatial slice is compact and one considers only the (magnetic) components of the field strength which lie along the spatial directions. This led twentieth century physicists to classify RR field strengths using cohomology with integral coefficients.\n\nHowever some authors have argued that the cohomology of spacetime with integral coefficients is too big. For example, in the presence of Neveu–Schwarz H-flux or non-spin cycles some RR fluxes dictate the presence of D-branes. In the former case this is a consequence of the supergravity equation of motion which states that the product of a RR flux with the NS 3-form is a D-brane charge density. Thus the set of topologically distinct RR field strengths that can exist in brane-free configurations is only a subset of the cohomology with integral coefficients.\n\nThis subset is still too big, because some of these classes are related by large gauge transformations. In QED there are large gauge transformations which add integral multiples of two pi to Wilson loops. The p-form potentials in type II supergravity theories also enjoy these large gauge transformations, but due to the presence of Chern-Simons terms in the supergravity actions these large gauge transformations transform not only the p-form potentials but also simultaneously the (p+3)-form field strengths. Thus to obtain the space of inequivalent field strengths from the forementioned subset of integral cohomology we must quotient by these large gauge transformations.\n\nThe Atiyah–Hirzebruch spectral sequence constructs twisted K-theory, with a twist given by the NS 3-form field strength, as a quotient of a subset of the cohomology with integral coefficients. In the classical limit, which corresponds to working with rational coefficients, this is precisely the quotient of a subset described above in supergravity. The quantum corrections come from torsion classes and contain mod 2 torsion corrections due to the Freed-Witten anomaly.\n\nThus twisted K-theory classifies the subset of RR field strengths that can exist in the absence of D-branes quotiented by large gauge transformations. Daniel Freed has attempted to extend this classification to include also the RR potentials using differential K-theory.\n\nK-theory classifies D-branes in noncompact spacetimes, intuitively in spacetimes in which we are not concerned about the flux sourced by the brane having nowhere to go. While the K-theory of a 10d spacetime classifies D-branes as subsets of that spacetime, if the spacetime is the product of time and a fixed 9-manifold then K-theory also classifies the conserved D-brane charges on each 9-dimensional spatial slice. While we were required to forget about RR potentials to obtain the K-theory classification of RR field strengths, we are required to forget about RR field strengths to obtain the K-theory classification of D-branes.\n\nAs has been stressed by Petr Hořava, the K-theory classification of D-branes is independent of, and in some ways stronger than, the classification of BPS states. K-theory appears to classify stable D-branes missed by supersymmetry based classifications.\n\nFor example, D-branes with torsion charges, that is with charges in the order N cyclic group formula_1, attract each other and so can never be BPS. In fact, N such branes can decay, whereas no superposition of branes that satisfy a Bogomolny bound may ever decay. However the charge of such branes is conserved modulo N, and this is captured by the K-theory classification but not by a BPS classification. Such torsion branes have been applied, for example, to model Douglas-Shenker strings in supersymmetric U(N) gauge theories.\n\nAshoke Sen has conjectured that, in the absence of a topologically nontrivial NS 3-form flux, all IIB brane configurations can be obtained from stacks of spacefilling D9 and anti D9 branes via tachyon condensation. The topology of the resulting branes is encoded in the topology of the gauge bundle on the stack of the spacefilling branes. The topology of the gauge bundle of a stack of D9s and anti D9s can be decomposed into a gauge bundle on the D9's and another bundle on the anti D9's. Tachyon condensation transforms such a pair of bundles to another pair in which the same bundle is direct summed with each component in the pair. Thus the tachyon condensation invariant quantity, that is, the charge which is conserved by the tachyon condensation process, is not a pair of bundles but rather the equivalence class of a pair of bundles under direct sums of the same bundle on both sides of the pair. This is precisely the usual construction of topological K-theory. Thus the gauge bundles on stacks of D9's and anti-D9's are classified by topological K-theory. If Sen's conjecture is right, all D-brane configurations in type IIB are then classified by K-theory. Petr Horava has extended this conjecture to type IIA using D8-branes.\n\nWhile the tachyon condensation picture of the K-theory classification classifies D-branes as subsets of a 10-dimensional spacetime with no NS 3-form flux, the Maldacena, Moore, Seiberg picture classifies stable D-branes with finite mass as subsets of a 9-dimensional spatial slice of spacetime.\n\nThe central observation is that D-branes are not classified by integral homology because Dp-branes wrapping certain cycles suffer from a Freed-Witten anomaly, which is cancelled by the insertion of D(p-2)-branes and sometimes D(p-4)-branes that end on the afflicted Dp-brane. These inserted branes may either continue to infinity, in which case the composite object has an infinite mass, or else they may end on an anti-Dp-brane, in which case the total Dp-brane charge is zero. In either case, one may wish to remove the anomalous Dp-branes from the spectrum, leaving only a subset of the original integral cohomology.\n\nThe inserted branes are unstable. To see this, imagine that they extend in time away (into the past) from the anomalous brane. This corresponds to a process in which the inserted branes decay via a Dp-brane that forms, wraps the forementioned cycle and then disappears. MMS refer to this process as an instanton, although really it need not be instantonic.\n\nThe conserved charges are thus the nonanomolous subset quotiented by the unstable insertions. This is precisely the Atiyah-Hirzebruch spectral sequence construction of twisted K-theory as a set.\n\nDiaconescu, Moore, and Witten have pointed out that the twisted K-theory classification is not compatible with the S-duality covariance of type IIB string theory. For example, consider the constraint on the Ramond–Ramond 3-form field strength G in the Atiyah-Hirzebruch spectral sequence (AHSS):\n\nwhere d=Sq+H is the first nontrivial differential in the AHSS, Sq is the third Steenrod square and the last equality follows from the fact that the nth Steenrod square acting on any n-form x is xformula_3x.\n\nThe above equation is not invariant under S-duality, which exchanges G and H. Instead Diaconescu, Moore, and Witten have proposed the following S-duality covariant extension\n\nwhere P is an unknown characteristic class that depends only on the topology, and in particular not on the fluxes. have found a constraint on P using the E gauge theory approach to M-theory pioneered by Diaconescu, Moore, and Witten.\n\nThus D-branes in IIB are not classified by twisted K-theory after all, but some unknown S-duality-covariant object that inevitably also classifies both fundamental strings and NS5-branes.\n\nHowever the MMS prescription for calculating twisted K-theory is easily S-covariantized, as the Freed-Witten anomalies respect S-duality. Thus the S-covariantized form of the MMS construction may be applied to construct the S-covariantized twisted K-theory, as a set, without knowing having any geometric description for just what this strange covariant object is. This program has been carried out in a number of papers, such as and , and was also applied to the classification of fluxes by . use this approach to prove Diaconescu, Moore, and Witten's conjectured constraint on the 3-fluxes, and they show that there is an additional term equal to the D3-brane charge. shows that the Klebanov-Strassler cascade of Seiberg dualities consists of a series of S-dual MMS instantons, one for each Seiberg duality. The group, formula_1 of universality classes of the formula_6 supersymmetric gauge theory is then shown to agree with the S-dual twisted K-theory and not with the original twisted K-theory.\n\nSome authors have proposed radically different solutions to this puzzle. For example, propose that instead of twisted K-theory, II string theory configurations should be classified by elliptic cohomology.\n\nProminent researchers in this area include Edward Witten, Peter Bouwknegt, Angel Uranga, Emanuel Diaconescu, Gregory Moore, Anton Kapustin, Jonathan Rosenberg, Ruben Minasian, Amihay Hanany, Hisham Sati, Nathan Seiberg, Juan Maldacena, Daniel Freed, and Igor Kriz.\n\n\n\nAn excellent introduction to the K-theory classification of D-branes in 10 dimensions via Ashoke Sen's conjecture is the original paper \"D-branes and K-theory\" by Edward Witten; there is also an extensive review by .\n\nA very comprehensible introduction to the twisted K-theory classification of conserved D-brane charges on a 9-dimensional timeslice in the presence of Neveu–Schwarz flux is .\n\n"}
{"id": "3047554", "url": "https://en.wikipedia.org/wiki?curid=3047554", "title": "Kelly criterion", "text": "Kelly criterion\n\nIn probability theory and intertemporal portfolio choice, the Kelly criterion, Kelly strategy, Kelly formula, or Kelly bet is a formula used to determine the optimal size of a series of bets in order to maximise the logarithm of wealth. In most gambling scenarios, and some investing scenarios under some simplifying assumptions, the Kelly strategy will do better than any essentially different strategy in the long run (that is, over a span of time in which the observed fraction of bets that are successful equals the probability that any given bet will be successful). It was described by J. L. Kelly, Jr, a researcher at Bell Labs, in 1956. The practical use of the formula has been demonstrated.\n\nThe Kelly Criterion is to bet a predetermined fraction of assets and can be counterintuitive. In one study, each participant was given $25 and asked to bet on a coin that would land heads 60% of the time. Participants had 30 minutes to play, so could place about 300 bets, and the prizes were capped at $250. Behavior was far from optimal. \"Remarkably, 28% of the participants went bust, and the average payout was just $91. Only 21% of the participants reached the maximum. 18 of the 61 participants bet everything on one toss, while two-thirds gambled on tails at some stage in the experiment.\" Using the Kelly criterion and based on the odds in the experiment, the right approach would be to bet 20% of the pot on each throw (see first example below). If losing, the size of the bet gets cut; if winning, the stake increases.\n\nAlthough the Kelly strategy's promise of doing better than any other strategy in the long run seems compelling, some economists have argued strenuously against it, mainly because an individual's specific investing constraints may override the desire for optimal growth rate. The conventional alternative is expected utility theory which says bets should be sized to maximize the expected utility of the outcome (to an individual with logarithmic utility, the Kelly bet maximizes expected utility, so there is no conflict; moreover, Kelly's original paper clearly states the need for a utility function in the case of gambling games which are played finitely many times). Even Kelly supporters usually argue for fractional Kelly (betting a fixed fraction of the amount recommended by Kelly) for a variety of practical reasons, such as wishing to reduce volatility, or protecting against non-deterministic errors in their advantage (edge) calculations.\n\nIn recent years, Kelly has become a part of mainstream investment theory and the claim has been made that well-known successful investors including Warren Buffett and Bill Gross use Kelly methods. William Poundstone wrote an extensive popular account of the history of Kelly betting.\n\nThe second-order Taylor polynomial can be used as a good approximation of the main criterion. Primarily, it is useful for stock investment, where the fraction devoted to investment is based on simple characteristics that can be easily estimated from existing historical data – expected value and variance. This approximation leads to results that are robust and offer similar results as the original criterion.\n\nFor simple bets with two outcomes, one involving losing the entire amount bet, and the other involving winning the bet amount multiplied by the payoff odds, the Kelly bet is:\n\nwhere:\n\n\nAs an example, if a gamble has a 60% chance of winning (\"p\" = 0.60, \"q\" = 0.40), and the gambler receives 1-to-1 odds on a winning bet (\"b\" = 1), then the gambler should bet 20% of the bankroll at each opportunity (\"f\"* = 0.20), in order to maximize the long-run growth rate of the bankroll.\n\nIf the edge is negative (\"b\" < \"q\" / \"p\") the formula gives a negative result, indicating that the gambler should take the other side of the bet. For example, in standard American roulette, the bettor is offered an even money payoff (b = 1) on red, when there are 18 red numbers and 20 non-red numbers on the wheel (p = 18/38). The Kelly bet is -1/19, meaning the gambler should bet one-nineteenth of their bankroll that red will \"not\" come up. Unfortunately, the casino doesn't allow betting \"against\" something coming up, so a Kelly gambler cannot place a bet.\n\nThe top of the first fraction is the expected net winnings from a $1 bet, since the two outcomes are that you either win $\"b\" with probability \"p\", or lose the $1 wagered, i.e. win $-1, with probability \"q\". Hence:\n\nFor even-money bets (i.e. when \"b\" = 1), the first formula can be simplified to:\nSince q = 1-p, this simplifies further to\n\nA more general problem relevant for investment decisions is the following:\n\n1. The probability of success is formula_5.\n\n2. If you succeed, the value of your investment increases from formula_6 to formula_7.\n\n3. If you fail (for which the probability is formula_8) the value of your investment decreases from formula_6 to formula_10. (Note that the previous description above assumes that \"a\" is 1).\n\nIn this case, the Kelly criterion turns out to be the relatively simple expression\n\nNote that this reduces to the original expression for the special case above (formula_12) for formula_13.\n\nClearly, in order to decide in favor of investing at least a small amount formula_14, you must have\n\nwhich obviously is nothing more than the fact that your expected profit must exceed the expected loss for the investment to make any sense.\n\nThe general result clarifies why leveraging (taking a loan to invest) decreases the optimal fraction to be invested, as in that case formula_16. Obviously, no matter how large the probability of success, formula_5, is, if formula_18 is sufficiently large, the optimal fraction to invest is zero. Thus, using too much margin is not a good investment strategy, no matter how good an investor you are.\n\nHeuristic proofs of the Kelly criterion are straightforward.\nFor a symbolic verification with Python and SymPy one would set the derivative y'(x) of the expected value of the logarithmic bankroll y(x) to 0 and solve for \"x\":\n\nThe Kelly criterion maximises the expectation of the logarithm of wealth (the expectation value of a function is given by the sum of the probabilities of particular outcomes multiplied by the value of the function in the event of that outcome). We start with 1 unit of wealth and bet a fraction formula_19 of that wealth on an outcome that occurs with probability formula_20 and offers odds of formula_21. The probability of winning is formula_20, and in that case the wealth is equal to formula_23. The probability of losing is formula_24, and in that case the wealth is equal to formula_25. Therefore our expectation value for log wealth formula_26 is given by: \n\nformula_27\n\nTo find the value of formula_19 for which the expectation value is maximised, we differentiate the above expression and set this equal to zero. This gives:\n\nformula_29\n\nRearranging this equation for formula_19 gives the Kelly criterion:\n\nformula_31\n\nFor a rigorous and general proof, see Kelly's original paper or some of the other references listed below. Some corrections have been published.\n\nWe give the following non-rigorous argument for the case b = 1 (a 50:50 \"even money\" bet) to show the general idea and provide some insights.\n\nWhen \"b\" = 1, the Kelly bettor bets 2\"p\" - 1 times initial wealth, \"W\", as shown above. If they win, they have 2\"pW\". If they lose, they have 2(1 - \"p\")\"W\". Suppose they make \"N\" bets like this, and win \"K\" of them. The order of the wins and losses doesn't matter, so they will have:\n\nSuppose another bettor bets a different amount, (2\"p\" - 1 + formula_33)W for some positive or negative formula_33. They will have (2p + formula_33)\"W\" after a win and [2(1 - \"p\")- formula_33]\"W\" after a loss. After the same wins and losses as the Kelly bettor, they will have:\n\nTake the derivative of this with respect to formula_33 and get:\n\nThe turning point of the original function occurs when this derivative equals zero, which occurs at:\n\nwhich implies:\n\nbut:\n\nso in the long run, final wealth is maximized by setting formula_33 to zero, which means following the Kelly strategy.\n\nThis illustrates that Kelly has both a deterministic and a stochastic component. If one knows K and N and wishes to pick a constant fraction of wealth to bet each time (otherwise one could cheat and, for example, bet zero after the K win knowing that the rest of the bets will lose), one will end up with the most money if one bets:\n\neach time. This is true whether \"N\" is small or large. The \"long run\" part of Kelly is necessary because K is not known in advance, just that as \"N\" gets large, \"K\" will approach \"pN\". Someone who bets more than Kelly can do better if for a stretch; someone who bets less than Kelly can do better if for a stretch, but in the long run, Kelly always wins.\n\nThe heuristic proof for the general case proceeds as follows.\n\nIn a single trial, if you invest the fraction formula_45 of your capital, if your strategy succeeds, your capital at the end of the trial increases by the factor formula_46, and, likewise, if the strategy fails, you end up having your capital decreased by the factor formula_47. Thus at the end of formula_48 trials (with formula_49 successes and formula_50 failures ), the starting capital of $1 yields\n\nMaximizing formula_52, and consequently formula_53, with respect to formula_45 leads to the desired result\n\nFor a more detailed discussion of this formula for the general case, see. There, it can be seen that the substitution of formula_5 for the ratio of the number of \"successes\" to the number of trials implies that the number of trials must be very large, since formula_5 is defined as the limit of this ratio as the number of trials goes to infinity. In brief, betting formula_58 each time will likely maximize the wealth growth rate only in the case where the number of trials is very large, and formula_5 and formula_60 are the same for each trial. In practice, this is a matter of playing the same game over and over, where the probability of winning and the payoff odds are always the same. In the heuristic proof above, formula_49 successes and formula_50 failures are highly likely only for very large formula_48.\n\nIn a 1738 article, Daniel Bernoulli suggested that, when one has a choice of bets or investments, one should choose that with the highest geometric mean of outcomes. This is mathematically equivalent to the Kelly criterion, although the motivation is entirely different (Bernoulli wanted to resolve the St. Petersburg paradox).\n\nThe Bernoulli article was not translated into English until 1954, but the work was well-known among mathematicians and economists.\n\nKelly's criterion may be generalized \nwhere formula_67 are the pay-off odds. formula_68, is the dividend rate where formula_69 is the track take or tax, formula_70 is the revenue rate after deduction of the track take when k-th horse wins. The fraction of the bettor's funds to bet on k-th horse is formula_71. Kelly's criterion for gambling with multiple mutually exclusive outcomes gives an algorithm for finding the optimal set formula_72 of outcomes on which it is reasonable to bet and it gives explicit formula for finding the optimal fractions formula_73 of bettor's wealth to be bet on the outcomes included in the optimal set formula_72. \nThe algorithm for the optimal set of outcomes consists of four steps.\n\nStep 1 Calculate the expected revenue rate for all possible (or only for several of the most promising) outcomes:\nformula_75\n\nStep 2 Reorder the outcomes so that the new sequence formula_76 is non-increasing. Thus formula_77 will be the best bet.\n\nStep 3 Set formula_78 (the empty set), formula_79, formula_80. Thus the best bet formula_81 will be considered first.\n\nStep 4 Repeat:\n\nIf formula_82 then insert k-th outcome into the set: formula_83, recalculate formula_84 according to the formula: \nformula_85 and then set formula_86,\n\nElse set formula_87 and then stop the repetition.\n\nIf the optimal set formula_72 is empty then do not bet at all. If the set formula_72 of optimal outcomes is not empty then the optimal fraction formula_73 to bet on k-th outcome may be calculated from this formula: formula_91.\n\nOne may prove that \nwhere the right hand-side is the reserve rate. Therefore the requirement formula_82 may be interpreted as follows: k-th outcome is included in the set formula_72 of optimal outcomes if and only if its expected revenue rate is greater than the reserve rate. The formula for the optimal fraction formula_73 may be interpreted as the excess of the expected revenue rate of k-th horse over the reserve rate divided by the revenue after deduction of the track take when k-th horse wins or as the excess of the probability of k-th horse winning over the reserve rate divided by revenue after deduction of the track take when k-th horse wins. The binary growth exponent is\nand the doubling time is\n\nThis method of selection of optimal bets may be applied also when probabilities formula_64 are known only for several most promising outcomes, while the remaining outcomes have no chance to win. In this case it must be that formula_99 and formula_100.\n\nConsidering a single asset (stock, index fund, etc.) and a risk-free rate, it is easy to obtain the optimal fraction to invest through geometric Brownian motion. \n\nThe value of a lognormally distributed asset formula_101 at time formula_102 (formula_103) is\n\nfrom the solution of the geometric Brownian motion where formula_105 is a Wiener process, and formula_106 (percentage drift) and formula_107 (the percentage volatility) are constants. Taking expectations of the logarithm:\n\nThen the expected log return formula_109 is\n\nFor a portfolio made of an asset formula_101 and a bond paying risk-free rate formula_112 with fraction formula_45 invested in formula_101 and formula_115 in the bond, the expected rate of return formula_116 is given by\n\nSolving formula_118 we obtain\n\nformula_120 is the fraction that maximizes the return, and so, is the Kelly fraction.\n\nThorp arrived at the same result but through a different derivation.\n\nRemember that formula_121 is different from the asset log return formula_109. Confusing this is a common mistake made by websites and articles talking about the Kelly Criterion.\n\nConsider a market with formula_123 correlated stocks formula_124 with stochastic returns formula_125, formula_126 and a riskless bond with \nreturn formula_127. An investor puts a fraction formula_128 of their capital in formula_124 and the rest is invested in the bond. Without loss of generality, assume that investor's starting capital is equal to 1. \nAccording to the Kelly criterion one should maximize\n<br>\nExpanding this with the Taylor series around formula_131 we obtain \n<br>\n<br>\nThus we reduce the optimization problem to quadratic programming and the unconstrained solution\nis \n<br>\nwhere formula_134 and formula_135 are the vector of means and the matrix of second mixed noncentral moments of the excess returns.\nThere is also a numerical algorithm for the fractional Kelly strategies and for the optimal solution under no leverage and no short selling constraints.\n\n"}
{"id": "1018951", "url": "https://en.wikipedia.org/wiki?curid=1018951", "title": "List of convexity topics", "text": "List of convexity topics\n\nThis is a list of convexity topics, by Wikipedia page.\n"}
{"id": "23128927", "url": "https://en.wikipedia.org/wiki?curid=23128927", "title": "Locally catenative sequence", "text": "Locally catenative sequence\n\nIn mathematics, a locally catenative sequence is a sequence of words in which each word can be constructed as the concatenation of previous words in the sequence.\n\nFormally, an infinite sequence of words \"w\"(\"n\") is locally catenative if, for some positive integers \"k\" and \"i\"...\"i\":\n\nSome authors use a slightly different definition in which encodings of previous words are allowed in the concatenation.\n\nThe sequence of Fibonacci words \"S\"(\"n\") is locally catenative because\n\nThe sequence of Thue–Morse words \"T\"(\"n\") is not locally catenative by the first definition. However, it is locally catenative by the second definition because\nwhere the encoding \"μ\" replaces 0 with 1 and 1 with 0.\n"}
{"id": "17523721", "url": "https://en.wikipedia.org/wiki?curid=17523721", "title": "Looman–Menchoff theorem", "text": "Looman–Menchoff theorem\n\nIn the mathematical field of complex analysis, the Looman–Menchoff theorem states that a continuous complex-valued function defined in an open set of the complex plane is holomorphic if and only if it satisfies the Cauchy–Riemann equations. It is thus a generalization of a theorem by Édouard Goursat, which instead of assuming the continuity of \"f\", assumes its Fréchet differentiability when regarded as a function from a subset of R to R.\n\nA complete statement of the theorem is as follows:\n\n\nLooman pointed out that the function given by \"f\"(\"z\") = exp(−\"z\") for \"z\" ≠ 0, \"f\"(0) = 0 satisfies the Cauchy–Riemann equations everywhere but is not analytic (or even continuous) at \"z\" = 0. This shows that the function \"f\" must be assumed continuous in the theorem.\n\nThe function given by \"f\"(\"z\") = \"z\"/|\"z\"| for \"z\" ≠ 0, \"f\"(0) = 0 is continuous everywhere and satisfies the Cauchy–Riemann equations at \"z\" = 0, but is not analytic at \"z\" = 0 (or anywhere else). This shows that a naive generalization of Looman–Menchoff-like theorem to a single point, is false.\n\n\n"}
{"id": "1189560", "url": "https://en.wikipedia.org/wiki?curid=1189560", "title": "Loop (topology)", "text": "Loop (topology)\n\nA loop in mathematics, in a topological space \"X\" is a continuous function \"f\" from the unit interval \"I\" = [0,1] to \"X\" such that \"f\"(0) = \"f\"(1). In other words, it is a path whose initial point is equal to the terminal point.\n\nA loop may also be seen as a continuous map \"f\" from the pointed unit circle \"S\" into \"X\", because \"S\" may be regarded as a quotient of \"I\" under the identification of 0 with 1.\n\nThe set of all loops in \"X\" forms a space called the loop space of \"X\".\n\n"}
{"id": "55143215", "url": "https://en.wikipedia.org/wiki?curid=55143215", "title": "Lou van den Dries", "text": "Lou van den Dries\n\nLaurentius Petrus Dignus \"Lou\" van den Dries (born May 26, 1951) is a Dutch mathematician working in model theory. He is a professor of mathematics at the University of Illinois at Urbana–Champaign.\n\nHe completed his PhD at Utrecht University in 1978 under the supervision of Dirk van Dalen with a dissertation entitled \"Model Theory of Fields\".\n\nVan den Dries is most known for his seminal work in o-minimality. He has also made contributions to the model theory of -adic fields, valued fields, and finite fields, and to the study of transseries. With Alex Wilkie, he improved Gromov's theorem on groups of polynomial growth using nonstandard methods.\n\nVan den Dries has been a corresponding member of the Royal Netherlands Academy of Arts and Sciences since 1993. He was an invited speaker at the 1990 International Congress of Mathematicians in Kyoto and delivered the Tarski Lectures at the University of California, Berkeley in 2017. He was awarded the Shoenfield Prize from the Association for Symbolic Logic in 2016 for his chapter \"Lectures on the Model Theory of Valued Fields\" in \"Model Theory in Algebra, Analysis and Arithmetic\", edited by Dugald Macpherson and Carlo Toffalori. Van den Dries was jointly awarded the 2018 Karp Prize with Matthias Aschenbrenner and Joris van der Hoeven \"for their work in model theory, especially on asymptotic differential algebra and the model theory of transseries.\"\n\nHis doctoral students include Matthias Aschenbrenner.\n\n"}
{"id": "19272910", "url": "https://en.wikipedia.org/wiki?curid=19272910", "title": "Many antennas", "text": "Many antennas\n\nMany antennas is a smart antenna technique which overcomes the performance limitation of single user multiple-input multiple-output (MIMO) techniques. In cellular communication, the maximum number of considered antennas for downlink is 2 and 4 to support 3GPP Long Term Evolution (LTE) and IMT Advanced requirements, respectively. Since the available spectrum band will probably be limited while the data rate requirement will continuously increase beyond IMT-A to support the mobile multimedia services, it is highly probable that the number of transmit antennas at the base station must be increased to 8–64 or more. The installation of many antennas at single base stations introduced many challenges and required development of several high technologies: a new SDMA engine, a new beamforming algorithm and a new antenna array.\n\n\nThe table summarizes the recent history of multiple antenna techniques in cellular communications. The table includes the future prediction as well for IMT-A and beyond.\n\n"}
{"id": "22408365", "url": "https://en.wikipedia.org/wiki?curid=22408365", "title": "Mechanical counter", "text": "Mechanical counter\n\nMechanical counters are digital counters built using mechanical components. Long before electronics became common, mechanical devices were used to count events. They typically consist of a series of disks mounted on an axle, with the digits zero through nine marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were used as odometers for bicycles and cars and in tape recorders and fuel dispensers and to control manufacturing processes. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter. Mechanical counters can be made into electromechanical counters, that count electrical impulses, by adding a small solenoid.\n\nAn odometer for measuring distance was first described by Vitruvius around 27 and 23 BC, although the actual inventor may have been Archimedes of Syracuse (c. 287 BC – c. 212 BC). It was based on chariot wheels turning 400 times in one Roman mile. For each revolution a pin on the axle engaged a 400 tooth cogwheel, thus turning it one complete revolution per mile. This engaged another gear with holes along the circumference, where pebbles (\"calculus\") were located, that were to drop one by one into a box. The distance traveled would thus be given simply by counting the number of pebbles.\n\nThe odometer was also independently invented in ancient China, possibly by the profuse inventor and early scientist Zhang Heng (78 AD – 139 AD) of the Han Dynasty (202 BC–220 AD). By the 3rd century (during the Three Kingdoms Period), the Chinese had termed the device as the 'jì lĭ gŭ chē' (記里鼓車), or 'li-recording drum carriage' Chinese texts of the 3rd century tell of the mechanical carriage's functions, and as one li is traversed, a mechanical-driven wooden figure strikes a drum, and when ten li is traversed, another wooden figure would strike a gong or a bell with its mechanical-operated arm. \n\n\n"}
{"id": "1864042", "url": "https://en.wikipedia.org/wiki?curid=1864042", "title": "Monochromatic triangle", "text": "Monochromatic triangle\n\nIn graph theory and theoretical computer science, the monochromatic triangle problem is an algorithmic problem on graphs,\nin which the goal is to partition the edges of a given graph into two triangle-free subgraphs. It is NP-complete but fixed-parameter tractable on graphs of bounded treewidth.\n\nThe monochromatic triangle problem takes as input an n-node undirected graph G(V,E) with node set V and edge set E.\nThe output is a Boolean value, true if the edge set E of G can be partitioned into two disjoint sets E1 and E2, such that both of the two subgraphs G1(V,E1) and G2(V,E2) are triangle-free graphs, and false otherwise. This decision problem is NP-complete.\n\nThe problem may be generalized to triangle-free edge coloring, finding an assignment of colors to the edges of a graph so that no triangle has all three edges given the same color. The monochromatic triangle problem is the special case of triangle-free edge-coloring when there are exactly two colors available. If there exists a two-color triangle-free edge coloring, then the edges of each color form the two sets E1 and E2 of the monochromatic triangle problem. Conversely, if the monochromatic triangle problem has a solution, we can use one color for E1 and a second color for E2 to obtain a triangle-free edge coloring.\n\nBy Ramsey's theorem, for any finite number \"k\" of colors, there exists a number \"n\" such that complete graphs of \"n\" or more vertices do not have triangle-free edge colorings with \"k\" colors. For \"k\" = 2, the corresponding value of \"n\" is 6. I.e., the answer to the monochromatic triangle problem on the complete graph \"K\" is no.\n\nIt is straightforward to express the monochromatic triangle problem in the monadic second-order logic of graphs (MSO), by a logical formula that asserts the existence of a partition of the edges into two subsets such that there do not exist three mutually adjacent vertices whose edges all belong to the same side of the partition. It follows from Courcelle's theorem that the monochromatic triangle problem is fixed-parameter tractable on graphs of bounded treewidth. More precisely, there is an algorithm for solving the problem whose running time is the number of vertices of the input graph multiplied by a quickly-growing but computable function of the treewidth.\n"}
{"id": "43170363", "url": "https://en.wikipedia.org/wiki?curid=43170363", "title": "Planarization", "text": "Planarization\n\nIn graph drawing, planarization is a method of extending drawing methods from planar graphs to graphs that are not planar, by embedding the non-planar graphs within a larger planar graph.\n\nPlanarization may be performed by using any method to find a drawing (with crossings) for the given graph, and then replacing each crossing point by a new artificial vertex, causing each crossed edge to be subdivided into a path. The original graph will be represented as a immersion minor of its planarization.\n\nIn incremental planarization, the planarization process is split into two stages. First, a large planar subgraph is found within the given graph. Then, the remaining edges that are not already part of this subgraph are added back one at a time, and routed through an embedding of the planar subgraph. When one of these edges crosses an already-embedded edge, the two edges that cross are replaced by two-edge paths, with a new artificial vertex that represents the crossing point placed at the middle of both paths. In some case a third local optimization stage is added to the planarization process, in which edges with many crossings are removed and re-added in an attempt to improve the planarization.\n\nUsing incremental planarization for graph drawing is most effective when the first step of the process finds as large a planar graph as possible. Unfortunately, finding the planar subgraph with the maximum possible number of edges (the \"maximum planar subgraph\" problem) is NP-hard, and MaxSNP-hard, implying that there probably does not exist a polynomial time algorithm that solves the problem exactly or that approximates it arbitrarily well.\n\nIn an \"n\"-vertex connected graph, the largest planar subgraph has at most 3\"n\" − 6 edges, and any spanning tree forms a planar subgraph with \"n\" − 1 edges. Thus, it is easy to approximate the maximum planar subgraph within an approximation ratio of one-third, simply by finding a spanning tree. A better approximation ratio, 9/4, is known, based on a method for finding a large partial 2-tree as a subgraph of the given graph. Alternatively, if it is expected that the planar subgraph will include almost all of the edges of the given graph, leaving only a small number \"k\" of non-planar edges for the incremental planarization process, then one can solve the problem exactly by using a fixed-parameter tractable algorithm whose running time is linear in the graph size but non-polynomial in the parameter \"k\". The problem may also be solved exactly by a branch and cut algorithm, with no guarantees on running time, but with good performance in practice. This parameter \"k\" is known as the skewness of the graph.\n\nThere has also been some study of a related problem, finding the largest planar induced subgraph of a given graph. Again, this is NP-hard, but fixed-parameter tractable when all but a few vertices belong to the induced subgraph. proved a tight bound of 3\"n\"/(Δ + 1) on the size of the largest planar induced subgraph, as a function of \"n\", the number of vertices in the given graph, and Δ, its maximum degree; their proof leads to a polynomial time algorithm for finding an induced subgraph of this size.\n\nOnce a large planar subgraph has been found, the incremental planarization process continues by considering the remaining edges one by one. As it does so, it maintains a planarization of the subgraph formed by the edges that have already been considered. It adds each new edge to a planar embedding of this subgraph, forming a drawing with crossings, and then replaces each crossing point with a new artificial vertex subdividing the two edges that cross. In some versions of this procedure, the order for adding edges is arbitrary, but it is also possible to choose the ordering to be a random permutation, running the same algorithm several times and returning the best planarization that it finds.\n\nIn the simplest form of this process, the planar embedding of the planarized subgraph is not allowed to change while new edges are added. In order to add each new edge in a way that minimizes the number of crossings it forms, one can use a shortest path algorithm in the dual graph of the current embedding, in order to find the shortest sequence of faces of the embedding and edges to be crossed that connects the endpoints of the new edge to each other. This process takes polynomial time per edge.\n\nFixing the embedding of the planarized subgraph is not necessarily optimal in terms of the number of crossings that result. In fact, there exist graphs that are formed by adding one edge to a planar subgraph, where the optimal drawing has only two crossings but where fixing the planar embedding of the subgraph forces a linear number of crossings to be created. As a compromise between finding the optimal planarization of a planar subgraph plus one edge, and keeping a fixed embedding, it is possible to search over all embeddings of the planarized subgraph and find the one that minimizes the number of crossings formed by the new edge.\n"}
{"id": "57164571", "url": "https://en.wikipedia.org/wiki?curid=57164571", "title": "Richard C. DiPrima Prize", "text": "Richard C. DiPrima Prize\n\nThe Richard C. DiPrima Prize is awarded every two years by the Society for Industrial and Applied Mathematics to an early career researcher who has done outstanding research in applied mathematics. First awarded in 1988, it honors the memory of Richard C. DiPrima, a former president of SIAM who also served for many years as a member of its council and board of trustees, as vice president for programs, and as a dedicated and committed member.\n\nThe recipients of the Richard C. DiPrima Prize are:\n\n"}
{"id": "21182177", "url": "https://en.wikipedia.org/wiki?curid=21182177", "title": "Set TSP problem", "text": "Set TSP problem\n\nIn combinatorial optimization, the set TSP, also known as the generalized TSP, group TSP, One-of-a-Set TSP, Multiple Choice TSP or Covering Salesman Problem, is a generalization of the Traveling salesman problem (TSP), whereby it is required to find a shortest tour in a graph which visits all specified subsets of the vertices of a graph. The subsets of vertices must be disjoint. The ordinary TSP is a special case of the set TSP when all subsets to be visited are singletons. Therefore, the set TSP is also NP-hard.\n\nThere is a direct transformation for an instance of the set TSP to an instance of the standard asymmetric TSP. The idea is to first create disjoint sets and then assign a directed cycle to each set. The salesman, when visiting a vertex in some set, then walks around the cycle for free. To not use the cycle would ultimately be very costly.\n\nThe Set TSP has a lot of interesting applications in several path planning problems. For example, a two vehicle cooperative routing problem could be transformed into a set TSP, tight lower bounds to the Dubins TSP and generalized Dubins path problem could be computed by solving a Set TSP.\n\nThe one-dimensional cutting stock problem as applied in the paper / plastic film industries, involves cutting jumbo rolls into smaller ones. This is done by generating cutting patterns typically to minimise waste. Once such a solution has been produced, one may seek to minimise the knife changes, by re-sequencing the patterns (up and down in the figure), or moving rolls left or right within each pattern. These moves do not affect the waste of the solution.\n\nIn the above figure, patterns (width no more than 198) are rows; knife changes are indicated by the small white circles; for example, patterns 2-3-4 have a roll of size 42.5 on the left - the corresponding knife does not have to move. Each pattern represents a TSP set, one of whose permutations must be visited. For instance, for the last pattern, which contains two repeated sizes (twice each), there are 5! / (2! × 2!) = 30 permutations. The number of possible solutions to the above instance is 12! × (5!) × (6!) × (7!) / ((2!) × (3!)) ≈ 5.3 × 10. The above solution contains 39 knife changes, and has been obtained by a heuristic; it is not known whether this is optimal. Transformations into the regular TSP, as described in would involve a TSP with 5,520 nodes.\n"}
{"id": "11800092", "url": "https://en.wikipedia.org/wiki?curid=11800092", "title": "Shannon wavelet", "text": "Shannon wavelet\n\nIn functional analysis, a Shannon wavelet may be either of real or complex type. \nSignal analysis by ideal bandpass filters defines a decomposition known as Shannon wavelets (or sinc wavelets). The Haar and sinc systems are Fourier duals of each other.\n\nThe Fourier transform of the Shannon mother wavelet is given by:\n\nwhere the (normalised) gate function is defined by\n\nThe analytical expression of the real Shannon wavelet can be found by taking the inverse Fourier transform:\n\nor alternatively as\n\nwhere\n\nis the usual sinc function that appears in Shannon sampling theorem.\n\nThis wavelet belongs to the formula_6-class of differentiability, but it decreases slowly at infinity and has no bounded support, since band-limited signals cannot be time-limited.\n\nThe scaling function for the Shannon MRA (or \"Sinc\"-MRA) is given by the sample function:\n\nIn the case of complex continuous wavelet, the Shannon wavelet is defined by\n\n"}
{"id": "1562127", "url": "https://en.wikipedia.org/wiki?curid=1562127", "title": "Shekel function", "text": "Shekel function\n\nShekel function is a multidimensional, multimodal, continuous, deterministic function commonly used as a test function for testing optimization techniques.\n\nThe mathematical form of a function in formula_1 dimensions with formula_2 maxima is:\n\nformula_3\n\nor, similarly,\n\nformula_4\n\nShekel, J. 1971. \"Test Functions for Multimodal Search Techniques.\" \"Fifth Annual Princeton Conference on Information Science and Systems\".\n\n"}
{"id": "39033720", "url": "https://en.wikipedia.org/wiki?curid=39033720", "title": "Stochastic cellular automaton", "text": "Stochastic cellular automaton\n\nStochastic cellular automata or 'probabilistic cellular automata' (PCA) or 'random cellular automata' or locally interacting Markov chains are an important extension of cellular automaton. Cellular automata are a discrete-time dynamical system of interacting entities, whose state is discrete.\n\nThe state of the collection of entities is updated at each discrete time according to some simple homogeneous rule. All entities' states are updated in parallel or synchronously. Stochastic Cellular Automata are CA whose updating rule is a stochastic one, which means the new entities' states are chosen according to some probability distributions. It is a discrete-time random dynamical system. From the spatial interaction between the entities, despite the simplicity of the updating rules, complex behaviour may emerge like self-organization. As mathematical object, it may be considered in the framework of stochastic processes as an interacting particle system in discrete-time.\nSee \nfor a more detailed introduction.\n\nAs discrete-time Markov process, PCA are defined on a product space formula_1 (cartesian product) where formula_2\nis a finite or infinite graph, like formula_3 and where formula_4 is a finite space, like for instance\nformula_5 or formula_6. The transition probability has a product form\nformula_7 where \nformula_8 and formula_9 is a probability distribution on formula_10.\nIn general some locality is required formula_11 where \nformula_12 with formula_13 a finite neighbourhood of k. See for a more detailed introduction following the probability theory's point of view.\n\nThere is a version of the majority cellular automaton with probabilistic updating rules. See the Toom's rule.\n\nPCA may be used to simulate the Ising model of ferromagnetism in statistical mechanics.\nSome categories of models were studied from a statistical mechanics point of view.\n\nThere is a strong connection\nbetween probabilistic cellular automata and the cellular Potts model in particular when it is implemented in parallel.\n\nThe Galves-Locherbach model is an example of a generalized PCA with a non Markovian aspect.\n\n"}
{"id": "53993", "url": "https://en.wikipedia.org/wiki?curid=53993", "title": "Sylow theorems", "text": "Sylow theorems\n\nIn mathematics, specifically in the field of finite group theory, the Sylow theorems are a collection of theorems named after the Norwegian mathematician Peter Ludwig Sylow (1872) that give detailed information about the number of subgroups of fixed order that a given finite group contains. The Sylow theorems form a fundamental part of finite group theory and have very important applications in the classification of finite simple groups.\n\nFor a prime number \"p\", a Sylow \"p\"-subgroup (sometimes \"p\"-Sylow subgroup) of a group \"G\" is a maximal \"p\"-subgroup of \"G\", i.e., a subgroup of \"G\" that is a \"p\"-group (so that the order of every group element is a power of \"p\") that is not a proper subgroup of any other \"p\"-subgroup of \"G\". The set of all Sylow \"p\"-subgroups for a given prime \"p\" is sometimes written Syl(\"G\").\n\nThe Sylow theorems assert a partial converse to Lagrange's theorem. Lagrange's theorem states that for any finite group \"G\" the order (number of elements) of every subgroup of \"G\" divides the order of \"G\". The Sylow theorems state that for every prime factor \"p\" of the order of a finite group \"G\", there exists a Sylow \"p\"-subgroup of \"G\" of order \"p\", the highest power of \"p\" that divides the order of \"G\". Moreover, every subgroup of order \"p\" is a Sylow \"p\"-subgroup of \"G\", and the Sylow \"p\"-subgroups of a group (for a given prime \"p\") are conjugate to each other. Furthermore, the number of Sylow \"p\"-subgroups of a group for a given prime \"p\" is congruent to \n\nCollections of subgroups that are each maximal in one sense or another are common in group theory. The surprising result here is that in the case of Syl(\"G\"), all members are actually isomorphic to each other and have the largest possible order: if |\"G\"| = \"pm\" with \"n\" > 0 where \"p\" does not divide \"m\", then every Sylow \"p\"-subgroup \"P\" has order |\"P\"| = \"p\". That is, \"P\" is a \"p\"-group and \"gcd\"(|\"G\" : \"P\"|, \"p\") = 1. These properties can be exploited to further analyze the structure of \"G\".\n\nThe following theorems were first proposed and proven by Ludwig Sylow in 1872, and published in \"Mathematische Annalen\".\n\nTheorem 1: For every prime factor \"p\" with multiplicity \"n\" of the order of a finite group \"G\", there exists a Sylow \"p\"-subgroup of \"G\", of order \"p\".\n\nThe following weaker version of theorem 1 was first proved by Augustin-Louis Cauchy, and is known as Cauchy's theorem.\n\nCorollary: Given a finite group \"G\" and a prime number \"p\" dividing the order of \"G\", then there exists an element (and hence a subgroup) of order \"p\" in \"G\".\n\nTheorem 2: Given a finite group \"G\" and a prime number \"p\", all Sylow \"p\"-subgroups of \"G\" are conjugate to each other, i.e. if \"H\" and \"K\" are Sylow \"p\"-subgroups of \"G\", then there exists an element \"g\" in \"G\" with \"g\"\"Hg\" = \"K\".\n\nTheorem 3: Let \"p\" be a prime factor with multiplicity \"n\" of the order of a finite group \"G\", so that the order of \"G\" can be written as , where and \"p\" does not divide \"m\". Let \"n\" be the number of Sylow \"p\"-subgroups of \"G\". Then the following hold:\n\nThe Sylow theorems imply that for a prime number \"p\" every Sylow \"p\"-subgroup is of the same order, \"p\". Conversely, if a subgroup has order \"p\", then it is a Sylow \"p\"-subgroup, and so is isomorphic to every other Sylow \"p\"-subgroup. Due to the maximality condition, if \"H\" is any \"p\"-subgroup of \"G\", then \"H\" is a subgroup of a \"p\"-subgroup of order \"p\".\n\nA very important consequence of Theorem 3 is that the condition \"n\" = 1 is equivalent to saying that the Sylow \"p\"-subgroup of \"G\" is a normal subgroup\n(there are groups that have normal subgroups but no normal Sylow subgroups, such as \"S\").\n\nThere is an analogue of the Sylow theorems for infinite groups. We define a Sylow \"p\"-subgroup in an infinite group to be a \"p\"-subgroup (that is, every element in it has \"p\"-power order) that is maximal for inclusion among all \"p\"-subgroups in the group. Such subgroups exist by Zorn's lemma.\n\nTheorem: If \"K\" is a Sylow \"p\"-subgroup of \"G\", and \"n\" = |Cl(\"K\")| is finite, then every Sylow \"p\"-subgroup is conjugate to \"K\", and \"n\" ≡ 1 (mod \"p\"), where Cl(\"K\") denotes the conjugacy class of \"K\".\n\nA simple illustration of Sylow subgroups and the Sylow theorems are the dihedral group of the \"n\"-gon, \"D\". For \"n\" odd, 2 = 2 is the highest power of 2 dividing the order, and thus subgroups of order 2 are Sylow subgroups. These are the groups generated by a reflection, of which there are \"n,\" and they are all conjugate under rotations; geometrically the axes of symmetry pass through a vertex and a side.\nBy contrast, if \"n\" is even, then 4 divides the order of the group, and the subgroups of order 2 are no longer Sylow subgroups, and in fact they fall into two conjugacy classes, geometrically according to whether they pass through two vertices or two faces. These are related by an outer automorphism, which can be represented by rotation through π/\"n\", half the minimal rotation in the dihedral group.\nAnother example are the Sylow p-subgroups of \"GL\"(\"F\"), where \"p\" and \"q\" are primes ≥ 3 and \"p\" ≡ 1 (mod \"q\") , which are all abelian. The order of \"GL\"(\"F\") is (\"q\" - 1)(\"q\" - \"q\") = (\"q\")(\"q\" + 1)(\"q\" - 1). Since \"q\" = pm + 1, the order of \"GL\"(\"F\") = p m'. Thus by Theorem 1, the order of the Sylow p-subgroups is \"p\".\n\nOne such subgroup \"P\", is the set of diagonal matrices formula_1, \"x\" is any primitive root of \"F\". Since the order of \"F\" is \"q\" - 1, its primitive roots have order \"q - 1\", which implies that \"x\" or \"x\" and all its powers have an order which is a power of \"p\". So, \"P\" is a subgroup where all its elements have orders which are powers of \"p\". There are \"p\" choices for both \"a\" and \"b\", making |\"P\"| = \"p\". This means \"P\" is a Sylow p-subgroup, which is abelian, as all diagonal matrices commute, and because Theorem 2 states that all Sylow p-subgroups are conjugate to each other, the Sylow p-subgroups of \"GL\"(\"F\") are all abelian.\n\nSince Sylow's theorem ensures the existence of p-subgroups of a finite group, its worthwhile to study groups of prime power order more closely. Most of the examples use Sylow's theorem to prove that a group of a particular order is not simple. For groups of small order, the congruence condition of Sylow's theorem is often sufficient to force the existence of a normal subgroup. \n\nSome non-prime numbers \"n\" are such that every group of order \"n\" is cyclic. One can show that \"n\" = 15 is such a number using the Sylow theorems: Let \"G\" be a group of order 15 = 3 · 5 and \"n\" be the number of Sylow 3-subgroups. Then \"n\" formula_2 5 and \"n\" ≡ 1 (mod 3). The only value satisfying these constraints is 1; therefore, there is only one subgroup of order 3, and it must be normal (since it has no distinct conjugates). Similarly, \"n\" must divide 3, and \"n\" must equal 1 (mod 5); thus it must also have a single normal subgroup of order 5. Since 3 and 5 are coprime, the intersection of these two subgroups is trivial, and so \"G\" must be the internal direct product of groups of order 3 and 5, that is the cyclic group of order 15. Thus, there is only one group of order 15 (up to isomorphism).\n\nA more complex example involves the order of the smallest simple group that is not cyclic. Burnside's \"p q\" theorem states that if the order of a group is the product of one or two prime powers, then it is solvable, and so the group is not simple, or is of prime order and is cyclic. This rules out every group up to order 30 .\n\nIf \"G\" is simple, and |\"G\"| = 30, then \"n\" must divide 10 ( = 2 · 5), and \"n\" must equal 1 (mod 3). Therefore, \"n\" = 10, since neither 4 nor 7 divides 10, and if \"n\" = 1 then, as above, \"G\" would have a normal subgroup of order 3, and could not be simple. \"G\" then has 10 distinct cyclic subgroups of order 3, each of which has 2 elements of order 3 (plus the identity). This means \"G\" has at least 20 distinct elements of order 3.\n\nAs well, \"n\" = 6, since \"n\" must divide 6 ( = 2 · 3), and \"n\" must equal 1 (mod 5). So \"G\" also has 24 distinct elements of order 5. But the order of \"G\" is only 30, so a simple group of order 30 cannot exist.\n\nNext, suppose |\"G\"| = 42 = 2 · 3 · 7. Here \"n\" must divide 6 ( = 2 · 3) and \"n\" must equal 1 (mod 7), so \"n\" = 1. So, as before, \"G\" can not be simple.\n\nOn the other hand, for |\"G\"| = 60 = 2 · 3 · 5, then \"n\" = 10 and \"n\" = 6 is perfectly possible. And in fact, the smallest simple non-cyclic group is \"A\", the alternating group over 5 elements. It has order 60, and has 24 cyclic permutations of order 5, and 20 of order 3.\n\nPart of Wilson's theorem states that\n\nfor every prime \"p\". One may easily prove this theorem by Sylow's third theorem. Indeed, \nobserve that the number \"n\" of Sylow's \"p\"-subgroups \nin the symmetric group \"S\" is (\"p\" − 2)!. On the other hand, \"n\" ≡ 1 (mod \"p\"). Hence, (\"p\" − 2)! ≡ 1 (mod \"p\"). So, (\"p\" − 1)! ≡ −1 (mod \"p\").\n\nFrattini's argument shows that a Sylow subgroup of a normal subgroup provides a factorization of a finite group. A slight generalization known as Burnside's fusion theorem states that if \"G\" is a finite group with Sylow \"p\"-subgroup \"P\" and two subsets \"A\" and \"B\" normalized by \"P\", then \"A\" and \"B\" are \"G\"-conjugate if and only if they are \"N\"(\"P\")-conjugate. The proof is a simple application of Sylow's theorem: If \"B\"=\"A\", then the normalizer of \"B\" contains not only \"P\" but also \"P\" (since \"P\" is contained in the normalizer of \"A\"). By Sylow's theorem \"P\" and \"P\" are conjugate not only in \"G\", but in the normalizer of \"B\". Hence \"gh\" normalizes \"P\" for some \"h\" that normalizes \"B\", and then \"A\" = \"B\" = \"B\", so that \"A\" and \"B\" are \"N\"(\"P\")-conjugate. Burnside's fusion theorem can be used to give a more powerful factorization called a semidirect product: if \"G\" is a finite group whose Sylow \"p\"-subgroup \"P\" is contained in the center of its normalizer, then \"G\" has a normal subgroup \"K\" of order coprime to \"P\", \"G\" = \"PK\" and \"P\"∩\"K\" = {1}, that is, \"G\" is \"p\"-nilpotent.\n\nLess trivial applications of the Sylow theorems include the focal subgroup theorem, which studies the control a Sylow \"p\"-subgroup of the derived subgroup has on the structure of the entire group. This control is exploited at several stages of the classification of finite simple groups, and for instance defines the case divisions used in the Alperin–Brauer–Gorenstein theorem classifying finite simple groups whose Sylow 2-subgroup is a quasi-dihedral group. These rely on J. L. Alperin's strengthening of the conjugacy portion of Sylow's theorem to control what sorts of elements are used in the conjugation.\n\nThe Sylow theorems have been proved in a number of ways, and the history of the proofs themselves is the subject of many papers including , , , , and to some extent .\n\nOne proof of the Sylow theorems exploits the notion of group action in various creative ways. The group \"G\" acts on itself or on the set of its \"p\"-subgroups in various ways, and each such action can be exploited to prove one of the Sylow theorems. The following proofs are based on combinatorial arguments of . In the following, we use \"a\" formula_2 \"b\" as notation for \"a divides b\" and \"a\" formula_5 \"b\" for the negation of this statement.\n\nProof: Let |\"G\"| = \"pm = pu\" such that \"p\" does not divide \"u\", and let Ω denote the set of subsets of \"G\" of size \"p\". \"G\" acts on Ω by left multiplication. The orbits \"G\"ω = {\"g\"ω | \"g\" ∈ \"G\"} of the ω ∈ Ω are the equivalence classes under the action of \"G\".\n\nFor any ω ∈ Ω consider its stabilizer subgroup \"G\" = {\"g\" ∈ \"G\" | \"g\"ω = ω}. For any fixed element α ∈ ω the function [\"g\" ↦ \"g\"α] maps \"G\" to ω injectively: for any two \"g\", \"h\" ∈ \"G\" we have that \"g\"α = \"h\"α implies \"g\" = \"h\", because α ∈ ω ⊆ \"G\" means that one may cancel on the right. Therefore, \" p\" = |ω| ≥ |\"G\"|.\n\nOn the other hand,\n\nand no power of \"p\" remains in any of the factors inside the product on the right. Hence \"ν\"(|Ω|) = \"ν\"(\"m\") = \"r\".\nLet \"R\" ⊆ Ω be a complete representation of all the equivalence classes under the action of \"G\". Then,\nThus, there exists an element ω ∈ \"R\" such that \"s\" := \"ν\"(|\"G\"ω|) ≤ \"ν\"(|Ω|) = \"r\". Hence |\"G\"ω| = \"pv\" where \"p\" does not divide \"v\". By the orbit-stabilizer theorem we have |\"G\"| = |\"G\"| / |\"G\"ω| = \"pu/v\". Therefore, \"p\" formula_2 |\"G\"|, so \"p\" ≤ |\"G\"| and \"G\"\" is the desired subgroup.\n\nProof: Write Ω as a disjoint sum of its orbits under \"G\". Any element \"x\" ∈ Ω not fixed by \"G\" will lie in an orbit of order |\"G\"|/|\"G\"| (where \"G\" denotes the stabilizer), which is a multiple of \"p\" by assumption. The result follows immediately.\n\nTheorem 2: If \"H\" is a \"p\"-subgroup of \"G\" and \"P\" is a Sylow \"p\"-subgroup of \"G\", then there exists an element \"g\" in \"G\" such that \"g\"\"Hg\" ≤ \"P\". In particular, all Sylow \"p\"-subgroups of \"G\" are conjugate to each other (and therefore isomorphic), that is, if \"H\" and \"K\" are Sylow \"p\"-subgroups of \"G\", then there exists an element \"g\" in \"G\" with \"g\"\"Hg\" = \"K\".\n\nProof: Let Ω be the set of left cosets of \"P\" in \"G\" and let \"H\" act on Ω by left multiplication. Applying the Lemma to \"H\" on Ω, we see that |Ω| ≡ |Ω| = [\"G\" : \"P\"] (mod \"p\"). Now \"p\" formula_5 [\"G\" : \"P\"] by definition so \"p\" formula_5 |Ω|, hence in particular |Ω| ≠ 0 so there exists some \"gP\" ∈ Ω. It follows that for some \"g\" ∈ \"G\" and ∀ \"h\" ∈ \"H\" we have \"hgP\" = \"gP\" so \"g\"\"HgP\" = \"P\" and therefore \"g\"\"Hg\" ≤ \"P\". Now if \"H\" is a Sylow \"p\"-subgroup, |\"H\"| = |\"P\"| = |\"gPg\"| so that \"H\" = \"gPg\" for some \"g\" ∈ \"G\".\n\nTheorem 3: Let \"q\" denote the order of any Sylow \"p\"-subgroup of a finite group \"G\". Then \"n\" = |\"G\" : \"N\"(\"P\")|, \"n\" formula_2 |\"G\"|/\"q\" and \"n\" ≡ 1 (mod \"p\").\n\nProof: Let \"G\" act on \"P\", a Sylow p-subgroup, by conjugation. By the orbit-stabilizer theorem, \"n\" = [\"G\" : \"Stab\"(\"P\")]. \"Stab\"(\"P\") = { \"g\" ∈ \"G\" | \"gPg\" = \"P\" } = \"N\" (\"P\"), the normalizer of \"P\" in \"G\". Thus, \"n\" = |\"G\" : \"N\"(\"P\")|, and it follows that this number is a divisor of |\"G\"|/\"q\". Let Ω be the set of all Sylow \"p\"-subgroups of \"G\", and let \"P\" act on Ω by conjugation. Let \"Q\" ∈ Ω and observe that then \"Q\" = \"xQx\" for all \"x\" ∈ \"P\" so that \"P\" ≤ \"N\"(\"Q\"). By Theorem 2, \"P\" and \"Q\" are conjugate in \"N\"(\"Q\") in particular, and \"Q\" is normal in \"N\"(\"Q\"), so then \"P\" = \"Q\". It follows that Ω = {\"P\"} so that, by the Lemma, |Ω| ≡ |Ω| = 1 (mod \"p\").\n\nThe problem of finding a Sylow subgroup of a given group is an important problem in computational group theory.\n\nOne proof of the existence of Sylow \"p\"-subgroups is constructive: if \"H\" is a \"p\"-subgroup of \"G\" and the index [\"G\":\"H\"] is divisible by \"p\", then the normalizer \"N\" = \"N\"(\"H\") of \"H\" in \"G\" is also such that [\"N\" : \"H\"] is divisible by \"p\". In other words, a polycyclic generating system of a Sylow \"p\"-subgroup can be found by starting from any \"p\"-subgroup \"H\" (including the identity) and taking elements of \"p\"-power order contained in the normalizer of \"H\" but not in \"H\" itself. The algorithmic version of this (and many improvements) is described in textbook form in , including the algorithm described in . These versions are still used in the GAP computer algebra system.\n\nIn permutation groups, it has been proven in (; ) that a Sylow \"p\"-subgroup and its normalizer can be found in polynomial time of the input (the degree of the group times the number of generators). These algorithms are described in textbook form in , and are now becoming practical as the constructive recognition of finite simple groups becomes a reality. In particular, versions of this algorithm are used in the Magma computer algebra system.\n\n\n\n"}
{"id": "394508", "url": "https://en.wikipedia.org/wiki?curid=394508", "title": "Séminaire de Géométrie Algébrique du Bois Marie", "text": "Séminaire de Géométrie Algébrique du Bois Marie\n\nIn mathematics, the Séminaire de Géométrie Algébrique du Bois Marie (SGA) was an influential seminar run by Alexander Grothendieck. It was a unique phenomenon of research and publication outside of the main mathematical journals that ran from 1960 to 1969 at the IHÉS near Paris. (The name came from the small wood on the estate in Bures-sur-Yvette where the IHÉS was located from 1962.) The seminar notes were eventually published in twelve volumes, all except one in the Springer Lecture Notes in Mathematics series.\n\nThe material has a reputation of being hard to read for a number of reasons. More elementary or foundational parts were relegated to the EGA series of Grothendieck and Jean Dieudonné, causing long strings of logical dependencies in the statements. The style is very abstract and makes heavy use of category theory. Moreover, an attempt was made to achieve maximally general statements, while assuming that the reader is aware of the motivations and concrete examples.\n\nThe original notes to SGA were published in fascicles by the IHÉS, most of which went through two or three revisions. These were published as the seminar proceeded, beginning in the early 60's and continuing through most of the decade. They can still be found in large math libraries, but distribution was limited. In the late 60's and early 70's, the original seminar notes were comprehensively revised and rewritten to take into account later developments. In addition, a new volume, SGA 4½, was compiled by Pierre Deligne and published in 1977; it contains simplified and new results by Deligne within the scope of SGA4 as well as some material from SGA5, which had not yet appeared at that time. The revised notes, except for SGA2, were published by Springer in its \"Lecture Notes in Mathematics\" series.\n\nAfter a dispute with Springer, Grothendieck refused the permission for reprints of the series. While these later revisions were more widely distributed than the original fascicles, they are still uncommon outside of libraries.\n\nReferences to SGA typically mean the later, revised editions and not the original fascicles; some of the originals were labelled by capital letters, thus for example S.G.A.D. = SGA3 and S.G.A.A. = SGA4.\n\nThe volumes of the SGA series are the following:\n\n\nIn the 1990s it became obvious that the lack of availability of the SGA was becoming more and more of a problem to researchers and graduate students in algebraic geometry: not only are the copies in book form too few for the growing number of researchers, but they are also difficult to read because of the way they are typeset (on an electric typewriter, with mathematical formulae written by hand). Thus, under the impetus of various mathematicians from several countries, a project was formed of re-publishing SGA in a more widely available electronic format and using LaTeX for typesetting; also, various notes are to be added to correct for minor mistakes or obscurities. The result should be published by the Société Mathématique de France. Legal permission to reprint the works was obtained from every author except Alexander Grothendieck himself, who could not be contacted; it was decided to proceed without his explicit agreement on the grounds that his refusal for the SGA to be re-published by Springer-Verlag was an objection against Springer and not one of principle.\n\nAs a first step, the entire work was scanned and made available on-line (see the links section below) by Frank Calegari, Jim Borger and William Stein. The job of typesetting the text anew and proofreading it was then distributed among dozens of volunteers (most of them junior French mathematicians, because of the required fluency in French and knowledge of algebraic geometry), starting with SGA1 in late 2001.\n\nThe coordinating editor for the work on SGA1 was Bas Edixhoven from University of Leiden (at the time University of Rennes): the first version was available on the arXiv.org e-print archive on June 20, 2002, and the proof-read version was uploaded on January 4, 2004, and later published in book form by the Société Mathématique de France. Work on SGA2 was started in 2004 with Yves Laszlo as coordinating editor. The LaTeX source file is available on the arXiv.org e-print archive; SGA2 appeared in print in late 2005 by the Société Mathématique de France (see https://web.archive.org/web/20091130171320/http://smf.emath.fr/Publications/DocumentsMathematiques/).\n\nLaszlo has also edited SGA4 and recently Philippe Gille and Patrick Polo have uploaded TeXed version of SGA3. In January 2010, however, Grothendieck requested that work cease on republishing SGA. In late 2014 work on republishing SGA resumed and it was restored to the Grothendieck circle site.\n\n\n\n\n\n\n\n"}
{"id": "308891", "url": "https://en.wikipedia.org/wiki?curid=308891", "title": "Tag system", "text": "Tag system\n\nA tag system is a deterministic computational model published by Emil Leon Post in 1943 as a simple form of a Post canonical system. A tag system may also be viewed as an abstract machine, called a Post tag machine (not to be confused with Post–Turing machines)—briefly, a finite-state machine whose only tape is a FIFO queue of unbounded length, such that in each transition the machine reads the symbol at the head of the queue, deletes a constant number of symbols from the head, and appends to the tail a symbol-string that depends solely on the first symbol read in this transition. \n\nBecause all of the indicated operations are performed in a single transition, a tag machine strictly has only one state.\n\nA \"tag system\" is a triplet (\"m\", \"A\", \"P\"), where\n\n\nA \"halting word\" is a word that either begins with the halting symbol or whose length is less than \"m\".\n\nA transformation \"t\" (called the \"tag operation\") is defined on the set of non-halting words, such that if \"x\" denotes the leftmost symbol of a word \"S\", then \"t\"(\"S\") is the result of deleting the leftmost \"m\" symbols of \"S\" and appending the word \"P(x)\" on the right. Thus, the system processes the m-symbol head into a tail of variable length, but the generated tail depends solely on the first symbol of the head. \n\nA \"computation\" by a tag system is a finite sequence of words produced by iterating the transformation \"t\", starting with an initially given word and halting when a halting word is produced. (By this definition, a computation is not considered to exist unless a halting word is produced in finitely-many iterations. Alternative definitions allow nonhalting computations, for example by using a special subset of the alphabet to identify words that encode output.)\n\nThe term \"m-tag system\" is often used to emphasise the deletion number. Definitions vary somewhat in the literature (cf References), the one presented here being that of Rogozhin.\n\nThe use of a halting symbol in the above definition allows the output of a computation to be encoded in the final word alone, whereas otherwise the output would be encoded in the entire sequence of words produced by iterating the tag operation.\n\nA common alternative definition uses no halting symbol and treats all words of length less than \"m\" as halting words. Another definition is the original one used by Post 1943 (described in the historical note below), in which the only halting word is the empty string.\n\nThis is merely to illustrate a simple 2-tag system that uses a halting symbol.\n\nThis simple 2-tag system is adapted from [De Mol, 2008]. It uses no halting symbol, but halts on any word of length less than 2, and computes a slightly modified version of the Collatz sequence.\n\nIn the original Collatz sequence, the successor of n is either (for even \"n\") or 3\"n\" + 1 (for odd n). The value 3\"n\" + 1 is clearly even for odd \"n\", hence the next term after 3\"n\" + 1 is surely . In the sequence computed by the tag system below we skip this intermediate step, hence the successor of \"n\" is for odd \"n\".\n\nIn this tag system, a positive integer \"n\" is represented by the word aa...a with \"n\" a's.\n\nFor each \"m\" > 1, the set of \"m\"-tag systems is Turing-complete; i.e., for each \"m\" > 1, it is the case that for any given Turing machine T, there is an \"m\"-tag system that emulates T. In particular, a 2-tag system can be constructed to emulate a Universal Turing machine, as was done by Wang 1963 and by Cocke & Minsky 1964.\n\nConversely, a Turing machine can be shown to be a Universal Turing Machine by proving that it can emulate a Turing-complete class of \"m\"-tag systems. For example, Rogozhin 1996 proved the universality of the class of 2-tag systems with alphabet {\"a\", ..., \"a\", H} and corresponding productions {\"aaW\", ..., \"aaW\", \"aa\", H}, where the \"W\" are nonempty words; he then proved the universality of a very small (4-state, 6-symbol) Turing machine by showing that it can simulate this class of tag systems.\n\nThis version of the halting problem is among the simplest, most-easily described undecidable decision problems:\n\nGiven an arbitrary positive integer \"n\" and a list of \"n\"+1 arbitrary words \"P\",\"P\"...,\"P\",\"Q\" on the alphabet {1,2...,\"n\"}, does repeated application of the tag operation \"t\": \"ijX\" → \"XP\" eventually convert \"Q\" into a word of length less than 2? That is, does the sequence \"Q\", \"t\"(\"Q\"), \"t\"(\"Q\"), \"t\"(\"Q\"), ... terminate?\n\nThe above definition differs from that of Post 1943, whose tag systems use no halting symbol, but rather halt only on the empty word, with the tag operation \"t\" being defined as follows:\n\n\nThe above remark concerning the Turing-completeness of the set of \"m\"-tag systems, for any \"m\" > 1, applies also to these tag systems as originally defined by Post.\n\nAccording to a footnote in Post 1943, B. P. Gill suggested the name for an earlier variant of the problem in which the first \"m\" symbols are left untouched, but rather a check mark indicating the current position moves to the right by \"m\" symbols every step. The name for the problem of determining whether or not the check mark ever touches the end of the sequence was then dubbed the \"problem of tag\", referring to the children's game of tag.\n\nA cyclic tag system is a modification of the original tag system. The alphabet consists of only two symbols, 0 and 1, and the production rules comprise a list of productions considered sequentially, cycling back to the beginning of the list after considering the \"last\" production on the list. For each production, the leftmost symbol of the word is examined—if the symbol is 1, the current production is appended to the right end of the word; if the symbol is 0, no characters are appended to the word; in either case, the leftmost symbol is then deleted. The system halts if and when the word becomes empty.\n\nCyclic tag systems were created by Matthew Cook and were used in Cook's demonstration that the Rule 110 cellular automaton is universal. A key part of the demonstration was that cyclic tag systems can emulate a Turing-complete class of tag systems.\n\nAn \"m\"-tag system with alphabet {\"a\", ..., \"a\"} and corresponding productions {\"P\", ..., \"P\"} is emulated by a cyclic tag system with \"m*n\" productions (\"Q\", ..., \"Q\", -, -, ..., -), where all but the first \"n\" productions are the empty string (denoted by '-'). The \"Q\" are encodings of the respective \"P\", obtained by replacing each symbol of the tag system alphabet by a length-\"n\" binary string as follows (these are to be applied also to the initial word of a tag system computation):\n\nThat is, \"a\" is encoded as a binary string with a 1 in the k position from the left, and 0's elsewhere. Successive lines of a tag system computation will then occur encoded as every (\"m*n\") line of its emulation by the cyclic tag system.\n\nThis is a very small example to illustrate the emulation technique.\nEvery sixth line (marked by '*') produced by the cyclic tag system is the encoding of a corresponding line of the tag system computation, until the emulated halt is reached.\n\n\n\n"}
{"id": "321671", "url": "https://en.wikipedia.org/wiki?curid=321671", "title": "Tessellation", "text": "Tessellation\n\nA tessellation of a flat surface is the tiling of a plane using one or more geometric shapes, called tiles, with no overlaps and no gaps. In mathematics, tessellations can be generalized to higher dimensions and a variety of geometries.\n\nA periodic tiling has a repeating pattern. Some special kinds include regular tilings with regular polygonal tiles all of the same shape, and semiregular tilings with regular tiles of more than one shape and with every corner identically arranged. The patterns formed by periodic tilings can be categorized into 17 wallpaper groups. A tiling that lacks a repeating pattern is called \"non-periodic\". An aperiodic tiling uses a small set of tile shapes that cannot form a repeating pattern. In the geometry of higher dimensions, a space-filling or honeycomb is also called a \"tessellation of space\".\n\nA real physical tessellation is a tiling made of materials such as cemented ceramic squares or hexagons. Such tilings may be decorative patterns, or may have functions such as providing durable and water-resistant pavement, floor or wall coverings. Historically, tessellations were used in Ancient Rome and in Islamic art such as in the decorative geometric tiling of the Alhambra palace. In the twentieth century, the work of M. C. Escher often made use of tessellations, both in ordinary Euclidean geometry and in hyperbolic geometry, for artistic effect. Tessellations are sometimes employed for decorative effect in quilting. Tessellations form a class of patterns in nature, for example in the arrays of hexagonal cells found in honeycombs.\n\nTessellations were used by the Sumerians (about 4000 BC) in building wall decorations formed by patterns of clay tiles.\n\nDecorative mosaic tilings made of small squared blocks called tesserae were widely employed in classical antiquity, sometimes displaying geometric patterns.\n\nIn 1619 Johannes Kepler made an early documented study of tessellations. He wrote about regular and semiregular tessellations in his \"Harmonices Mundi\"; he was possibly the first to explore and to explain the hexagonal structures of honeycomb and snowflakes.\n\nSome two hundred years later in 1891, the Russian crystallographer Yevgraf Fyodorov proved that every periodic tiling of the plane features one of seventeen different groups of isometries. Fyodorov's work marked the unofficial beginning of the mathematical study of tessellations. Other prominent contributors include Aleksei Shubnikov and Nikolai Belov (1964), and Heinrich Heesch and Otto Kienzle (1963).\n\nIn Latin, \"tessella\" is a small cubical piece of clay, stone or glass used to make mosaics. The word \"tessella\" means \"small square\" (from \"tessera\", square, which in turn is from the Greek word τέσσερα for \"four\"). It corresponds to the everyday term \"tiling\", which refers to applications of tessellations, often made of glazed clay.\n\nTessellation in two dimensions, also called planar tiling, is a topic in geometry that studies how shapes, known as \"tiles\", can be arranged to fill a plane without any gaps, according to a given set of rules. These rules can be varied. Common ones are that there must be no gaps between tiles, and that no corner of one tile can lie along the edge of another. The tessellations created by bonded brickwork do not obey this rule. Among those that do, a regular tessellation has both identical regular tiles and identical regular corners or vertices, having the same angle between adjacent edges for every tile. There are only three shapes that can form such regular tessellations: the equilateral triangle, square, and regular hexagon. Any one of these three shapes can be duplicated infinitely to fill a plane with no gaps.\n\nMany other types of tessellation are possible under different constraints. For example, there are eight types of semi-regular tessellation, made with more than one kind of regular polygon but still having the same arrangement of polygons at every corner. Irregular tessellations can also be made from other shapes such as pentagons, polyominoes and in fact almost any kind of geometric shape. The artist M. C. Escher is famous for making tessellations with irregular interlocking tiles, shaped like animals and other natural objects. If suitable contrasting colours are chosen for the tiles of differing shape, striking patterns are formed, and these can be used to decorate physical surfaces such as church floors.\n\nMore formally, a tessellation or tiling is a cover of the Euclidean plane by a countable number of closed sets, called \"tiles\", such that the tiles intersect only on their boundaries. These tiles may be polygons or any other shapes. Many tessellations are formed from a finite number of prototiles in which all tiles in the tessellation are congruent to the given prototiles. If a geometric shape can be used as a prototile to create a tessellation, the shape is said to \"tessellate\" or to \"tile the plane\". The Conway criterion is a sufficient but not necessary set of rules for deciding if a given shape tiles the plane periodically without reflections: some tiles fail the criterion but still tile the plane. No general rule has been found for determining if a given shape can tile the plane or not, which means there are many unsolved problems concerning tessellations.\n\nMathematically, tessellations can be extended to spaces other than the Euclidean plane. The Swiss geometer Ludwig Schläfli pioneered this by defining \"polyschemes\", which mathematicians nowadays call polytopes. These are the analogues to polygons and polyhedra in spaces with more dimensions. He further defined the Schläfli symbol notation to make it easy to describe polytopes. For example, the Schläfli symbol for an equilateral triangle is {3}, while that for a square is {4}. The Schläfli notation makes it possible to describe tilings compactly. For example, a tiling of regular hexagons has three six-sided polygons at each vertex, so its Schläfli symbol is {6,3}.\n\nOther methods also exist for describing polygonal tilings. When the tessellation is made of regular polygons, the most common notation is the vertex configuration, which is simply a list of the number of sides of the polygons around a vertex. The square tiling has a vertex configuration of 4.4.4.4, or 4. The tiling of regular hexagons is noted 6.6.6, or 6.\n\nMathematicians use some technical terms when discussing tilings. An \"edge\" is the intersection between two bordering tiles; it is often a straight line. A \"vertex\" is the point of intersection of three or more bordering tiles. Using these terms, an \"isogonal\" or vertex-transitive tiling is a tiling where every vertex point is identical; that is, the arrangement of polygons about each vertex is the same. The fundamental region is a shape such as a rectangle that is repeated to form the tessellation. For example, a regular tessellation of the plane with squares has a meeting of four squares at every vertex.\n\nThe sides of the polygons are not necessarily identical to the edges of the tiles. An edge-to-edge tiling is any polygonal tessellation where adjacent tiles only share one full side, i.e., no tile shares a partial side or more than one side with any other tile. In an edge-to-edge tiling, the sides of the polygons and the edges of the tiles are the same. The familiar \"brick wall\" tiling is not edge-to-edge because the long side of each rectangular brick is shared with two bordering bricks.\n\nA \"normal tiling\" is a tessellation for which every tile is topologically equivalent to a disk, the intersection of any two tiles is a single connected set or the empty set, and all tiles are uniformly bounded. This means that a single circumscribing radius and a single inscribing radius can be used for all the tiles in the whole tiling; the condition disallows tiles that are pathologically long or thin.\n\nA monohedral tiling is a tessellation in which all tiles are congruent; it has only one prototile. A particularly interesting type of monohedral tessellation is the spiral monohedral tiling. The first spiral monohedral tiling was discovered by Heinz Voderberg in 1936; the Voderberg tiling has a unit tile that is a nonconvex enneagon. The Hirschhorn tiling, published by Michael D. Hirschhorn and D. C. Hunt in 1985, is a pentagon tiling using irregular pentagons: regular pentagons cannot tile the Euclidean plane as the internal angle of a regular pentagon, , is not a divisor of 2.\n\nAn isohedral tiling is a special variation of a monohedral tiling in which all tiles belong to the same transitivity class, that is, all tiles are transforms of the same prototile under the symmetry group of the tiling. If a prototile admits a tiling, but no such tiling is isohedral, then the prototile is called anisohedral and forms anisohedral tilings.\n\nA regular tessellation is a highly symmetric, edge-to-edge tiling made up of regular polygons, all of the same shape. There are only three regular tessellations: those made up of equilateral triangles, squares, or regular hexagons. All three of these tilings are isogonal and monohedral.\n\nA semi-regular (or Archimedean) tessellation uses more than one type of regular polygon in an isogonal arrangement. There are eight semi-regular tilings (or nine if the mirror-image pair of tilings counts as two). These can be described by their vertex configuration; for example, a semi-regular tiling using squares and regular octagons has the vertex configuration 4.8 (each vertex has one square and two octagons). Many non-edge-to-edge tilings of the Euclidean plane are possible, including the family of Pythagorean tilings, tessellations that use two (parameterised) sizes of square, each square touching four squares of the other size. An edge tessellation is one in which each tile can be reflected over an edge to take up the position of a neighbouring tile, such as in an array of equilateral or isosceles triangles.\n\nTilings with translational symmetry in two independent directions can be categorized by wallpaper groups, of which 17 exist. It has been claimed that all seventeen of these groups are represented in the Alhambra palace in Granada, Spain. Though this is disputed, the variety and sophistication of the Alhambra tilings have surprised modern researchers. Of the three regular tilings two are in the \"p6m\" wallpaper group and one is in \"p4m\". Tilings in 2D with translational symmetry in just one direction can be categorized by the seven frieze groups describing the possible frieze patterns. Orbifold notation can be used to describe wallpaper groups of the Euclidean plane.\n\nPenrose tilings, which use two different quadrilateral prototiles, are the best known example of tiles that forcibly create non-periodic patterns. They belong to a general class of aperiodic tilings, which use tiles that cannot tessellate periodically. The recursive process of substitution tiling is a method of generating aperiodic tilings. One class that can be generated in this way is the rep-tiles; these tilings have surprising self-replicating properties. Pinwheel tilings are non-periodic, using a rep-tile construction; the tiles appear in infinitely many orientations. It might be thought that a non-periodic pattern would be entirely without symmetry, but this is not so. Aperiodic tilings, while lacking in translational symmetry, do have symmetries of other types, by infinite repetition of any bounded patch of the tiling and in certain finite groups of rotations or reflections of those patches. A substitution rule, such as can be used to generate some Penrose patterns using assemblies of tiles called rhombs, illustrates scaling symmetry. A Fibonacci word can be used to build an aperiodic tiling, and to study quasicrystals, which are structures with aperiodic order.\n\nWang tiles are squares coloured on each edge, and placed so that abutting edges of adjacent tiles have the same colour; hence they are sometimes called Wang dominoes. A suitable set of Wang dominoes can tile the plane, but only aperiodically. This is known because any Turing machine can be represented as a set of Wang dominoes that tile the plane if and only if the Turing machine does not halt. Since the halting problem is undecidable, the problem of deciding whether a Wang domino set can tile the plane is also undecidable.\n\nTruchet tiles are square tiles decorated with patterns so they do not have rotational symmetry; in 1704, Sébastien Truchet used a square tile split into two triangles of contrasting colours. These can tile the plane either periodically or randomly.\n\nSometimes the colour of a tile is understood as part of the tiling; at other times arbitrary colours may be applied later. When discussing a tiling that is displayed in colours, to avoid ambiguity one needs to specify whether the colours are part of the tiling or just part of its illustration. This affects whether tiles with the same shape but different colours are considered identical, which in turn affects questions of symmetry. The four colour theorem states that for every tessellation of a normal Euclidean plane, with a set of four available colours, each tile can be coloured in one colour such that no tiles of equal colour meet at a curve of positive length. The colouring guaranteed by the four colour theorem does not generally respect the symmetries of the tessellation. To produce a colouring which does, it is necessary to treat the colours as part of the tessellation. Here, as many as seven colours may be needed, as in the picture at right.\n\nNext to the various tilings by regular polygons, tilings by other polygons have also been studied.\n\nAny triangle or quadrilateral (even non-convex) can be used as a prototile to form a monohedral tessellation, often in more than one way. Copies of an arbitrary quadrilateral can form a tessellation with translational symmetry and 2-fold rotational symmetry with centres at the midpoints of all sides. For an asymmetric quadrilateral this tiling belongs to wallpaper group p2. As fundamental domain we have the quadrilateral. Equivalently, we can construct a parallelogram subtended by a minimal set of translation vectors, starting from a rotational centre. We can divide this by one diagonal, and take one half (a triangle) as fundamental domain. Such a triangle has the same area as the quadrilateral and can be constructed from it by cutting and pasting.\n\nIf only one shape of tile is allowed, tilings exists with convex \"N\"-gons for \"N\" equal to 3, 4, 5 and 6. For , see Pentagonal tiling, for , see Hexagonal tiling,for , see Heptagonal tiling and for , see octagonal tiling.\n\nFor results on tiling the plane with polyominoes, see Polyomino § Uses of polyominoes.\n\nVoronoi or Dirichlet tilings are tessellations where each tile is defined as the set of points closest to one of the points in a discrete set of defining points. (Think of geographical regions where each region is defined as all the points closest to a given city or post office.) The \"Voronoi cell\" for each defining point is a convex polygon. The Delaunay triangulation is a tessellation that is the dual graph of a Voronoi tessellation. Delaunay triangulations are useful in numerical simulation, in part because among all possible triangulations of the defining points, Delaunay triangulations maximize the minimum of the angles formed by the edges. Voronoi tilings with randomly placed points can be used to construct random tilings of the plane.\n\nTessellation can be extended to three dimensions. Certain polyhedra can be stacked in a regular crystal pattern to fill (or tile) three-dimensional space, including the cube (the only Platonic polyhedron to do so), the rhombic dodecahedron, the truncated octahedron, and triangular, quadrilateral, and hexagonal prisms, among others. Any polyhedron that fits this criterion is known as a plesiohedron, and may possess between 4 and 38 faces. Naturally occurring rhombic dodecahedra are found as crystals of andradite (a kind of garnet) and fluorite.\n\nA Schwarz triangle is a spherical triangle that can be used to tile a sphere.\n\nTessellations in three or more dimensions are called honeycombs. In three dimensions there is just one regular honeycomb, which has eight cubes at each polyhedron vertex. Similarly, in three dimensions there is just one quasiregular honeycomb, which has eight tetrahedra and six octahedra at each polyhedron vertex. However, there are many possible semiregular honeycombs in three dimensions. Uniform polyhedra can be constructed using the Wythoff construction.\n\nThe Schmitt-Conway biprism is a convex polyhedron with the property of tiling space only aperiodically.\n\nIt is possible to tessellate in non-Euclidean geometries such as hyperbolic geometry. A uniform tiling in the hyperbolic plane (which may be regular, quasiregular or semiregular) is an edge-to-edge filling of the hyperbolic plane, with regular polygons as faces; these are vertex-transitive (transitive on its vertices), and isogonal (there is an isometry mapping any vertex onto any other).\n\nA uniform honeycomb in hyperbolic space is a uniform tessellation of uniform polyhedral cells. In 3-dimensional hyperbolic space there are nine Coxeter group families of compact convex uniform honeycombs, generated as Wythoff constructions, and represented by permutations of rings of the Coxeter diagrams for each family.\n\nIn architecture, tessellations have been used to create decorative motifs since ancient times. Mosaic tilings often had geometric patterns. Later civilisations also used larger tiles, either plain or individually decorated. Some of the most decorative were the Moorish wall tilings of Islamic architecture, using Girih and Zellige tiles in buildings such as the Alhambra and La Mezquita.\n\nTessellations frequently appeared in the graphic art of M. C. Escher; he was inspired by the Moorish use of symmetry in places such as the Alhambra when he visited Spain in 1936. Escher made four \"Circle Limit\" drawings of tilings that use hyperbolic geometry. For his woodcut \"Circle Limit IV\" (1960), Escher prepared a pencil and ink study showing the required geometry. Escher explained that \"No single component of all the series, which from infinitely far away rise like rockets perpendicularly from the limit and are at last lost in it, ever reaches the boundary line.\"\n\nTessellated designs often appear on textiles, whether woven, stitched in or printed. Tessellation patterns have been used to design interlocking motifs of patch shapes in quilts.\n\nTessellations are also a main genre in origami (paper folding), where pleats are used to connect molecules such as twist folds together in a repeating fashion.\n\nTessellation is used in manufacturing industry to reduce the wastage of material (yield losses) such as sheet metal when cutting out shapes for objects like car doors or drinks cans.\n\nTessellation is apparent in the mudcrack-like cracking of thin films – with a degree of self-organisation being observed using micro and nanotechnologies.\n\nThe honeycomb provides a well-known example of tessellation in nature with its hexagonal cells.\n\nIn botany, the term \"tessellate\" describes a checkered pattern, for example on a flower petal, tree bark, or fruit. Flowers including the fritillary and some species of \"Colchicum\" are characteristically tessellate.\n\nMany patterns in nature are formed by cracks in sheets of materials. These patterns can be described by Gilbert tessellations, also known as random crack networks. The Gilbert tessellation is a mathematical model for the formation of mudcracks, needle-like crystals, and similar structures. The model, named after Edgar Gilbert, allows cracks to form starting from randomly scattered over the plane; each crack propagates in two opposite directions along a line through the initiation point, its slope chosen at random, creating a tessellation of irregular convex polygons. Basaltic lava flows often display columnar jointing as a result of contraction forces causing cracks as the lava cools. The extensive crack networks that develop often produce hexagonal columns of lava. One example of such an array of columns is the Giant's Causeway in Northern Ireland. Tessellated pavement, a characteristic example of which is found at Eaglehawk Neck on the Tasman Peninsula of Tasmania, is a rare sedimentary rock formation where the rock has fractured into rectangular blocks.\n\nOther natural patterns occur in foams; these are packed according to Plateau's laws, which require minimal surfaces. Such foams present a problem in how to pack cells as tightly as possible: in 1887, Lord Kelvin proposed a packing using only one solid, the bitruncated cubic honeycomb with very slightly curved faces. In 1993, Denis Weaire and Robert Phelan proposed the Weaire–Phelan structure, which uses less surface area to separate cells of equal volume than Kelvin's foam.\n\nTessellations have given rise to many types of tiling puzzle, from traditional jigsaw puzzles (with irregular pieces of wood or cardboard) and the tangram to more modern puzzles which often have a mathematical basis. For example, polyiamonds and polyominoes are figures of regular triangles and squares, often used in tiling puzzles. Authors such as Henry Dudeney and Martin Gardner have made many uses of tessellation in recreational mathematics. For example, Dudeney invented the hinged dissection, while Gardner wrote about the rep-tile, a shape that can be dissected into smaller copies of the same shape. Inspired by Gardner's articles in Scientific American, the amateur mathematician Marjorie Rice found four new tessellations with pentagons. Squaring the square is the problem of tiling an integral square (one whose sides have integer length) using only other integral squares. An extension is squaring the plane, tiling it by squares whose sizes are all natural numbers without repetitions; James and Frederick Henle proved that this was possible.\n\n\n"}
{"id": "46493377", "url": "https://en.wikipedia.org/wiki?curid=46493377", "title": "The Algorithm Auction", "text": "The Algorithm Auction\n\nThe Algorithm Auction is the world’s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.\n\nFive lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid’s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.\n\nTwo other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.\n\nExhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktivist Richard Jones.\n\nSebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.\n\nFollowing the auction, the Museum of Modern Art held a salon titled \"The Way of the Algorithm\" highlighting algorithms as \"a ubiquitous and indispensable component of our lives.\"\n"}
{"id": "16958307", "url": "https://en.wikipedia.org/wiki?curid=16958307", "title": "Trifocal tensor", "text": "Trifocal tensor\n\nIn computer vision, the trifocal tensor (also tritensor) is a 3×3×3 array of numbers\n(i.e., a tensor) that incorporates all projective geometric relationships\namong three views. It relates the coordinates of corresponding points or lines in three views, being\nindependent of the scene structure and depending only on the relative motion (i.e., pose) among the\nthree views and their intrinsic calibration parameters. Hence, the trifocal tensor can be considered as\nthe generalization of the fundamental matrix in three views.\nIt is noted that despite that the tensor is made up of 27 elements, only 18 of them are actually independent.\n\nThe tensor can also be seen as a collection of three rank-two 3 x 3 matrices\nformula_1 known as its \"correlation slices\".\nAssuming that the projection matrices of three views are\nformula_2, \nformula_3 and formula_4,\nthe correlation slices of the corresponding tensor can be expressed in closed form as\nformula_5,\nwhere \nformula_6\nare respectively the \"i\" columns of the camera matrices.\nIn practice, however, the tensor is estimated from point and line matches across the three views.\n\nOne of the most important properties of the trifocal tensor is that it gives rise to linear relationships between\nlines and points in three images. More specifically, for triplets of corresponding points\nformula_7\nand any corresponding lines\nformula_8 through\nthem, the following \"trilinear constraints\" hold:\n\nwhere formula_14 denotes the skew-symmetric cross product matrix.\n\nGiven the trifocal tensor of three views and a pair of matched points in two views, it is possible to determine the\nlocation of the point in the third view without any further information. This is known as \"point transfer\" and a\nsimilar result holds for lines.\n\n\n"}
{"id": "27967069", "url": "https://en.wikipedia.org/wiki?curid=27967069", "title": "Uniform matroid", "text": "Uniform matroid\n\nIn mathematics, a uniform matroid is a matroid in which every permutation of the elements is a symmetry.\n\nThe uniform matroid formula_1 is defined over a set of formula_2 elements. A subset of the elements is independent if and only if it contains at most formula_3 elements. A subset is a basis if it has exactly formula_3 elements, and it is a circuit if it has exactly formula_5 elements. The rank of a subset formula_6 is formula_7 and the rank of the matroid is formula_3.\n\nA matroid of rank formula_3 is uniform if and only if all of its circuits have exactly formula_5 elements.\n\nThe matroid formula_11 is called the formula_2-point line.\n\nThe dual matroid of the uniform matroid formula_1 is another uniform matroid formula_14. A uniform matroid is self-dual if and only if formula_15.\n\nEvery minor of a uniform matroid is uniform. Restricting a uniform matroid formula_1 by one element (as long as formula_17) produces the matroid\nformula_18 and contracting it by one element (as long as formula_19) produces the matroid formula_20.\n\nThe uniform matroid formula_1 may be represented as the matroid of affinely independent subsets of formula_2 points in general position in formula_3-dimensional Euclidean space, or as the matroid of linearly independent subsets of formula_2 vectors in general position in an formula_25-dimensional real vector space.\n\nEvery uniform matroid may also be realized in projective spaces and vector spaces over all sufficiently large finite fields. However, the field must be large enough to include enough independent vectors. For instance, the formula_2-point line formula_11 can be realized only over finite fields of formula_28 or more elements (because otherwise the projective line over that field would have fewer than formula_2 points): formula_30 is not a binary matroid, formula_31 is not a ternary matroid, etc. For this reason, uniform matroids play an important role in Rota's conjecture concerning the forbidden minor characterization of the matroids that can be realized over finite fields.\n\nThe problem of finding the minimum-weight basis of a weighted uniform matroid is well-studied in computer science as the selection problem. It may be solved in linear time.\n\nAny algorithm that tests whether a given matroid is uniform, given access to the matroid via an independence oracle, must perform an exponential number of oracle queries, and therefore cannot take polynomial time.\n\nUnless formula_32, a uniform matroid formula_1 is connected: it is not the direct sum of two smaller matroids.\nThe direct sum of a family of uniform matroids (not necessarily all with the same parameters) is called a partition matroid.\n\nEvery uniform matroid is a paving matroid, a transversal matroid and a strict gammoid.\n\nNot every uniform matroid is graphic, and the uniform matroids provide the smallest example of a non-graphic matroid, formula_30. The uniform matroid formula_35 is the graphic matroid of an formula_2-edge dipole graph, and the dual uniform matroid formula_37 is the graphic matroid of its dual graph, the formula_2-edge cycle graph. formula_39 is the graphic matroid of a graph with formula_2 self-loops, and formula_41 is the graphic matroid of an formula_2-edge forest. Other than these examples, every uniform matroid formula_1 with formula_44 contains formula_30 as a minor and therefore is not graphic.\n\nThe formula_2-point line provides an example of a Sylvester matroid, a matroid in which every line contains three or more points.\n\n"}
