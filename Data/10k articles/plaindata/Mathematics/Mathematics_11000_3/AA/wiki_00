{"id": "5560456", "url": "https://en.wikipedia.org/wiki?curid=5560456", "title": "234 (number)", "text": "234 (number)\n\n234 (two hundred [and] thirty-four) is the integer following 233 and preceding 235.\n\n234 is a practical number.\nThere are 234 ways of grouping six children into rings of at least two children with one child at the center of each ring.\n"}
{"id": "16110989", "url": "https://en.wikipedia.org/wiki?curid=16110989", "title": "Affix grammar", "text": "Affix grammar\n\nAn affix grammar is a kind of formal grammar; it is used to describe the syntax of languages, mainly computer languages, using an approach based on how natural language is typically described.\n\nThe grammatical rules of an affix grammar are those of a context-free grammar, except that certain parts in the nonterminals (the affixes) are used as arguments. If the same affix occurs multiple times in a rule, its value must agree, i.e. it must be the same everywhere. In some types of affix grammar, more complex relationships between affix values are possible.\n\nWe can describe an extremely simple fragment of English in the following manner:\n\nThis context-free grammar describes simple sentences such as\n\nWith more nouns and verbs, and more rules to introduce other parts of speech, a large range of English sentences can be described; so this is a promising approach for describing the syntax of English.\n\nHowever, the given grammar also describes sentences such as\n\nThese sentences are wrong: in English, subject and verb have a grammatical number, which must agree.\n\nAn affix grammar can express this directly:\n\nThis grammar only describes correct English sentences, although it could be argued that\nis still incorrect and should instead read\n\nThis, too, can be incorporated using affixes, if the means of describing the relationships between different affix values are powerful enough. As remarked above, these means depend on the type of affix grammar chosen.\n\nIn the simplest type of affix grammar, affixes can only take values from a finite domain, and affix values can only be related through agreement, as in the example.\nApplied in this way, affixes increase compactness of grammars, but do not add expressive power.\n\nAnother approach is to allow affixes to take arbitrary strings as values and allow concatenations of affixes to be used in rules. The ranges of allowable values for affixes can be described with context-free grammar rules. This produces the formalism of two-level grammars, also known as \"Van Wijngaarden grammars\" or \"2VW\" grammars. These have been successfully used to describe complicated languages, in particular, the syntax of the Algol 68 programming language. However, it turns out that, even though affix values can only be manipulated with string concatenation, this formalism is Turing complete; hence, even the most basic questions about the language described by an arbitrary 2VW grammar are undecidable in general.\n\nExtended Affix Grammars, developed in the 1980s, are a more restricted version of the same idea. They were mainly applied to describe the grammar of natural language, e.g. English.\n\nAnother possibility is to allow the values of affixes to be computed by code written in some programming language. Two basic approaches have been used:\n"}
{"id": "230982", "url": "https://en.wikipedia.org/wiki?curid=230982", "title": "Akra–Bazzi method", "text": "Akra–Bazzi method\n\nIn computer science, the Akra–Bazzi method, or Akra–Bazzi theorem, is used to analyze the asymptotic behavior of the mathematical recurrences that appear in the analysis of divide and conquer algorithms where the sub-problems have substantially different sizes. It is a generalization of the master theorem for divide-and-conquer recurrences, which assumes that the sub-problems have equal size. It is named after mathematicians Mohamad Akra and Louay Bazzi.\n\nThe Akra–Bazzi method applies to recurrence formulas of the form\n\nThe conditions for usage are:\n\n\nThe asymptotic behavior of formula_13 is found by determining the value of formula_14 for which formula_15 and plugging that value into the equation\n\n(see Θ). Intuitively, formula_17 represents a small perturbation in the index of formula_18. By noting that formula_19 and that the absolute value of formula_20 is always between 0 and 1, formula_17 can be used to ignore the floor function in the index. Similarly, one can also ignore the ceiling function. For example, formula_22 and formula_23 will, as per the Akra–Bazzi theorem, have the same asymptotic behavior.\n\nSuppose formula_24 is defined as 1 for integers formula_25 and formula_26 for integers formula_27. In applying the Akra–Bazzi method, the first step is to find the value of formula_14 for which formula_29. In this example, formula_30. Then, using the formula, the asymptotic behavior can be determined as follows:\n\nThe Akra–Bazzi method is more useful than most other techniques for determining asymptotic behavior because it covers such a wide variety of cases. Its primary application is the approximation of the runtime of many divide-and-conquer algorithms. For example, in the merge sort, the number of comparisons required in the worst case, which is roughly proportional to its runtime, is given recursively as formula_32 and \n\nfor integers formula_34, and can thus be computed using the Akra–Bazzi method to be formula_35.\n\n\nO Método de Akra-Bazzi na Resolução de Equações de Recorrência "}
{"id": "56615271", "url": "https://en.wikipedia.org/wiki?curid=56615271", "title": "Aparna Higgins", "text": "Aparna Higgins\n\nAparna W. Higgins is a mathematician known for her encouragement of undergraduate mathematicians to participate in mathematical research. Higgins originally specialized in universal algebra, but her more recent research concerns graph theory, including graph pebbling and line graphs. She is a professor of mathematics at the University of Dayton.\n\nHiggins is originally from Mumbai, India, and did her undergraduate studies at the University of Mumbai, graduating in 1978. She completed her Ph.D. in 1983 at the University of Notre Dame; her dissertation, \"Heterogeneous Algebras Associated with Non-Indexed Algebras, a Representation Theorem on Weak Automorphisms of Universal Algebras\", was supervised by Abraham Goetz.\n\nIn 2009 she became director of Project NExT, after the previous director, T. Christine Stevens, stepped down; this project is an initiative of the Mathematical Association of America to provide career guidance to new doctorates in mathematics.\n\nHiggins is married to Bill Higgins, a mathematics professor at Wittenberg University, and the two regularly take their sabbaticals together in California.\n\nHiggins won a Distinguished Teaching Award from the Mathematical Association of America in 1995, for her contributions to undergraduate research. In 2005 she was one of three winners of the Deborah and Franklin Haimo Award for Distinguished College or University Teaching of Mathematics of the Mathematical Association of America.\n"}
{"id": "2887375", "url": "https://en.wikipedia.org/wiki?curid=2887375", "title": "Apodicticity", "text": "Apodicticity\n\n\"Apodictic\" or \"apodeictic\" (, \"capable of demonstration\") is an adjectival expression from Aristotelean logic that refers to propositions that are demonstrably, necessarily or self-evidently the case. Apodicticity or apodixis is the corresponding abstract noun, referring to logical certainty.\n\nApodictic propositions contrast with assertoric propositions, which merely assert that something is (or is not) the case, and with problematic propositions, which assert only the possibility of something being true. Apodictic judgments are clearly provable or logically certain. For instance, \"Two plus two equals four\" is apodictic. \"Chicago is larger than Omaha\" is assertoric. \"A corporation could be wealthier than a country\" is problematic. In Aristotelian logic, \"apodictic\" is opposed to \"dialectic,\" as scientific proof is opposed to philosophical reasoning. Kant contrasted \"apodictic\" with \"problematic\" and \"assertoric\" in the \"Critique of Pure Reason\", on page A70/B95.\n\nHans Reichenbach, one of the founders of logical positivism, offered a modified version of Immanuel Kant's a priorism by distinguishing between \"apodictic a priorism\" and \"constitutive a priorism\".\n\n"}
{"id": "19410107", "url": "https://en.wikipedia.org/wiki?curid=19410107", "title": "Arithmeum", "text": "Arithmeum\n\nThe Arithmeum is a mathematics museum owned by the Forschungsinstitut für Diskrete Mathematik (Research Institute for Discrete Mathematics) at the University of Bonn.\n\nIt was founded by the Director of the Institute, Bernhard Korte, who contributed his private collection of calculating machines.\n\nThe building's steel-glass facade - located at Lennéstrasse 2 - is meant to represent the \"transparency of science\".\n\nThe permanent exhibit \"Calculating in Olden and Modern Times\" () shows the progression of mechanical calculating machines through 1,200 pieces.\n\nIt holds the very large,(4000 pieces), IJzebrand Schuitema (1929-2013) 400 year collection of slide rules.\n\nThere are also exhibits on very-large-scale integrated (VLSI) logic chips, historical arithmetic books dating back to Johannes Gutenberg's times, and the relationship between art and science.\n\n"}
{"id": "3801097", "url": "https://en.wikipedia.org/wiki?curid=3801097", "title": "Asynchronous system", "text": "Asynchronous system\n\nThe primary focus of this article is asynchronous control in digital electronic systems. In a synchronous system, operations (instructions, calculations, logic, etc.) are coordinated by one, or more, centralized clock signals. An asynchronous digital system, in contrast, has no global clock. Asynchronous systems do not depend on strict arrival times of signals or messages for reliable operation. Coordination is achieved via events such as: packet arrival, changes (transitions) of signals, handshake protocols, and other methods.\n\nAsynchronous systems – much like object-oriented software – are typically constructed out of modular 'hardware objects', each with well-defined communication interfaces. These modules may operate at variable speeds, whether due to data-dependent processing, dynamic voltage scaling, or process variation. The modules can then be combined together to form a correct working system, without reference to a global clock signal. Typically, low power is obtained since components are activated only on demand. Furthermore, several asynchronous styles have been shown to accommodate clocked interfaces, and thereby support mixed-timing design. Hence, asynchronous systems match well the need for correct-by-construction methodologies in assembling large-scale heterogeneous and scalable systems.\n\nThere is a large spectrum of asynchronous design styles, with tradeoffs between robustness and performance (and other parameters such as power). The choice of design style depends on the application target: reliability/ease-of-design vs. speed. The most robust designs use 'delay-insensitive circuits', whose operation is correct regardless\nof gate and wire delays; however, only limited useful systems can be designed with this style. Slightly less robust, but much more useful, are quasi-delay-insensitive circuits (also known as speed-independent circuits), such as delay-insensitive minterm synthesis, which operate correctly regardless of gate delays; however, wires at each fanout point must be tuned for roughly equal delays. Less robust but faster circuits, requiring simple localized one-sided timing constraints, include controllers using fundamental-mode operation (i.e. with setup/hold requirements on when new inputs can be received), and bundled datapaths using matched delays (see below). At the extreme, high-performance \"timed circuits\" have been proposed, which use tight two-side timing constraints, where the clock can still be avoided but careful physical delay tuning is required, such as for some high-speed pipeline applications.\n\nAsynchronous communication is typically performed on channels. Communication is used both to synchronize operations of the concurrent system as well as to pass data. A simple channel typically consists of two wires: a request and an acknowledge. In a '4-phase handshaking protocol' (or return-to-zero), the request is asserted by the sender component, and the receiver responds by asserting the acknowledge; then both signals are de-asserted in turn. In a '2-phase handshaking protocol' (or transition-signalling), the requester simply toggles the value on the request wire (once), and the receiver responds by toggling the value on the acknowledge wire. Channels can also be extended to communicate data.\n\nAsynchronous datapaths are typically encoded using several schemes. Robust schemes use two wires or 'rails' for each bit, called 'dual-rail encoding'. In this case, first rail is asserted to transmit a 0 value, or the second rail is asserted to transmit a 1 value. The asserted rail is then reset to zero before the next data value is transmitted, thereby indicating 'no data' or a 'spacer' state. A less robust, but widely used and practical scheme, is called 'single-rail bundled data'. Here, a single-rail (i.e. synchronous-style) function block can be used, with an accompanying worst-case matched delay. After valid data inputs arrive, a request signal is asserted as the input to the matched delay. When the matched delay produces a 'done' output, the block guaranteed to have completed computation. While this scheme has timing constraints, they are simple, localized (unlike in synchronous systems), and one-sided, hence are usually easy to validate.\n\nThe literature in this field exists in a variety of conference and journal proceedings. The leading symposium is the IEEE Async Symposium (International Symposium on Asynchronous Circuits and Systems), founded in 1994. A variety of asynchronous papers have also been published since the mid-1980s in such conferences as IEEE/ACM Design Automation Conference, IEEE International Conference on Computer Design, IEEE/ACM International Conference on Computer-Aided Design, International Solid-State Circuits Conference, and Advanced Research in VLSI, as well as in leading journals such as IEEE Transactions on VLSI Systems, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, and Transactions on Distributed Computing.\n\n\nAdapted from Steve Nowick's column in the ACM SIGDA e-newsletter by Igor Markov <br>\nOriginal text is available at https://web.archive.org/web/20060624073502/http://www.sigda.org/newsletter/2006/eNews_060115.html\n\n"}
{"id": "4827724", "url": "https://en.wikipedia.org/wiki?curid=4827724", "title": "Banach function algebra", "text": "Banach function algebra\n\nIn functional analysis a Banach function algebra on a compact Hausdorff space \"X\" is unital subalgebra, \"A\" of the commutative C*-algebra \"C(X)\" of all continuous, complex valued functions from \"X\", together with a norm on \"A\" which makes it a Banach algebra.\n\nA function algebra is said to vanish at a point p if f(p) = 0 for all formula_1. A function algebra separates points if for each distinct pair of points formula_2, there is a function formula_1 such that formula_4.\n\nFor every formula_5 define formula_6. Then formula_7\nis a non-zero homomorphism (character) on formula_8.\n\nTheorem: A Banach function algebra is semisimple (that is its Jacobson radical is equal to zero) and each commutative unital, semisimple Banach algebra is isomorphic (via the Gelfand transform) to a Banach function algebra on its character space (the space of algebra homomorphisms from \"A\" into the complex numbers given the relative weak* topology).\n\nIf the norm on formula_8 is the uniform norm (or sup-norm) on formula_10, then formula_8 is called\na uniform algebra. Uniform algebras are an important special case of Banach function algebras.\n"}
{"id": "30485319", "url": "https://en.wikipedia.org/wiki?curid=30485319", "title": "Biot–Tolstoy–Medwin diffraction model", "text": "Biot–Tolstoy–Medwin diffraction model\n\nIn applied mathematics, the Biot–Tolstoy–Medwin (BTM) diffraction model describes edge diffraction. Unlike the uniform theory of diffraction (UTD), BTM does not make the high frequency assumption (in which edge lengths and distances from source and receiver are much larger than the wavelength). BTM sees use in acoustic simulations.\n\nThe impulse response according to BTM is given as follows:\n\nThe general expression for sound pressure is given by the convolution integral\n\nwhere formula_2 represents the source signal, and formula_3 represents the impulse response at the receiver position. The BTM gives the latter in terms of\n\n\nas an integral over edge positions formula_5\n\nwhere the summation is over the four possible choices of the two signs, formula_13 and formula_14 are the distances from the point formula_5 to the source and receiver respectively, and formula_16 is the Dirac delta function.\n\nwhere\n\n\n"}
{"id": "6598775", "url": "https://en.wikipedia.org/wiki?curid=6598775", "title": "Branching theorem", "text": "Branching theorem\n\nIn mathematics, the branching theorem is a theorem about Riemann surfaces. Intuitively, it states that every non-constant holomorphic function is locally a polynomial.\n\nLet formula_1 and formula_2 be Riemann surfaces, and let formula_3 be a non-constant holomorphic map. Fix a point formula_4 and set formula_5. Then there exist formula_6 and charts formula_7 on formula_1 and formula_9 on formula_2 such that\n\nThis theorem gives rise to several definitions:\n"}
{"id": "1432664", "url": "https://en.wikipedia.org/wiki?curid=1432664", "title": "Choice function", "text": "Choice function\n\nA choice function (selector, selection) is a mathematical function \"f\" that is defined on some collection \"X\" of nonempty sets and assigns to each set \"S\" in that collection some element \"f\"(\"S\") of \"S\". In other words, \"f\" is a choice function for \"X\" if and only if it belongs to the direct product of \"X\".\n\nLet \"X\" = { {1,4,7}, {9}, {2,7} }. Then the function that assigns 7 to the set {1,4,7}, 9 to {9}, and 2 to {2,7} is a choice function on \"X\".\n\nErnst Zermelo (1904) introduced choice functions as well as the axiom of choice (AC) and proved the well-ordering theorem, which states that every set can be well-ordered. AC states that every set of nonempty sets has a choice function. A weaker form of AC, the axiom of countable choice (AC) states that every countable set of nonempty sets has a choice function. However, in the absence of either AC or AC, some sets can still be shown to have a choice function.\n\n\nA function formula_8 is said to be a selection of a multivalued map φ:\"A\" → \"B\" (that is, a function formula_9 from \"A\" to the power set formula_10), if\n\nThe existence of more regular choice functions, namely continuous or measurable selections is important in the theory of differential inclusions, optimal control, and mathematical economics.\n\nNicolas Bourbaki used epsilon calculus for their foundations that had a formula_12 symbol that could be interpreted as choosing an object (if one existed) that satisfies a given proposition. So if formula_13 is a predicate, then formula_14 is the object that satisfies formula_15 (if one exists, otherwise it returns an arbitrary object). Hence we may obtain quantifiers from the choice function, for example formula_16 was equivalent to formula_17.\n\nHowever, Bourbaki's choice operator is stronger than usual: it's a \"global\" choice operator. That is, it implies the axiom of global choice. Hilbert realized this when introducing epsilon calculus.\n\n"}
{"id": "6497220", "url": "https://en.wikipedia.org/wiki?curid=6497220", "title": "Computational complexity of mathematical operations", "text": "Computational complexity of mathematical operations\n\nThe following tables list the computational complexity of various algorithms for common mathematical operations.\n\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.\n\nNote: Due to the variety of multiplication algorithms, \"M\"(\"n\") below stands in for the complexity of the chosen multiplication algorithm.\n\nMany of the methods in this section are given in Borwein & Borwein.\n\nThe elementary functions are constructed by composing arithmetic operations, the exponential function (exp), the natural logarithm (log), trigonometric functions (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are analytic and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.\n\nBelow, the size \"n\" refers to the number of digits of precision at which the function is to be evaluated.\n\nIt is not known whether O(\"M\"(\"n\") log \"n\") is the optimal complexity for elementary functions. The best known lower bound is the trivial bound Ω(\"M\"(\"n\")).\n\nThis table gives the complexity of computing approximations to the given constants to \"n\" correct digits.\nAlgorithms for number theoretical calculations are studied in computational number theory.\n\nThe following complexity figures assume that arithmetic with individual elements has complexity \"O\"(1), as is the case with fixed-precision floating-point arithmetic or operations on a finite field.\n\nIn 2005, Henry Cohn, Robert Kleinberg, Balázs Szegedy, and Chris Umans showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.\n\nBecause of the possibility of blockwise inverting a matrix, where an inversion of an matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of operations, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.\n\n"}
{"id": "30879232", "url": "https://en.wikipedia.org/wiki?curid=30879232", "title": "Consensus–expectations gap", "text": "Consensus–expectations gap\n\nA consensus–expectations gap is a gap between what a group of decision-makers are expected to agree on, and what they are actually able to agree on. The expression was first used by Asle Toje in the book \"The European Union as a small power : after the post-Cold War\". The term owes to Christopher Hill's capability–expectations gap between what the European Communities had been talked up to do and what the collective was actually able to deliver. Hill saw the capability–expectations gap as having three primary components, namely, the ability to agree, resource availability and the instruments at its disposal. The 'consensus–expectations gap' focuses on one of Hill's variables: the ability to agree.\n"}
{"id": "23674225", "url": "https://en.wikipedia.org/wiki?curid=23674225", "title": "Convex bipartite graph", "text": "Convex bipartite graph\n\nIn the mathematical field of graph theory, a convex bipartite graph is a bipartite graph with specific properties.\nA bipartite graph, (\"U\" ∪ \"V\", \"E\"), is said to be convex over the vertex set \"U\" if \"U\" can be enumerated such that for all \"v\" ∈ \"V\" the vertices adjacent to \"v\" are consecutive.\n\nConvexity over \"V\" is defined analogously. A bipartite graph (\"U\" ∪ \"V\", \"E\") that is convex over both \"U\" and \"V\" is said to be biconvex or doubly convex.\n\nLet \"G\" = (\"U\" ∪ \"V\", \"E\") be a bipartite graph, i.e., the vertex set is \"U\" ∪ \"V\" where \"U\" ∩ \"V\" = ∅.\nLet \"N\"(\"v\") denote the neighborhood of a vertex \"v\" ∈ \"V\". \nThe graph \"G\" is convex over \"U\" if and only if there exists a bijective mapping, \"f\": \"U\" → {1, …, |\"U\"|}, such that for all \"v\" ∈ \"V\",\nfor any two vertices \"x\",\"y\" ∈ \"N\"(\"v\") ⊆ \"U\" there does not exist a \"z\" ∉ \"N\"(\"v\") such that \"f\"(\"x\") < \"f\"(\"z\") < \"f\"(\"y\").\n\n\n"}
{"id": "15054768", "url": "https://en.wikipedia.org/wiki?curid=15054768", "title": "Degasperis–Procesi equation", "text": "Degasperis–Procesi equation\n\nIn mathematical physics, the Degasperis–Procesi equation\n\nis one of only two exactly solvable equations in the following family of third-order, non-linear, dispersive PDEs:\n\nwhere formula_3 and \"b\" are real parameters (\"b\"=3 for the Degasperis–Procesi equation). It was discovered by Degasperis and Procesi in a search for integrable equations similar in form to the Camassa–Holm equation, which is the other integrable equation in this family (corresponding to \"b\"=2); that those two equations are the only integrable cases has been verified using a variety of different integrability tests. Although discovered solely because of its mathematical properties, the Degasperis–Procesi equation (with formula_4) has later been found to play a similar role in water wave theory as the Camassa–Holm equation.\n\nAmong the solutions of the Degasperis–Procesi equation (in the special case formula_5) are the so-called multipeakon solutions, which are functions of the form\n\nwhere the functions formula_7 and formula_8 satisfy\n\nThese ODEs can be solved explicitly in terms of elementary functions, using inverse spectral methods.\n\nWhen formula_4 the soliton solutions of the Degasperis–Procesi equation are smooth; they converge to peakons in the limit as formula_3 tends to zero.\n\nThe Degasperis–Procesi equation (with formula_5) is formally equivalent to the (nonlocal) hyperbolic conservation law\n\nwhere formula_14, and where the star denotes convolution with respect to \"x\".\nIn this formulation, it admits weak solutions with a very low degree of regularity, even discontinuous ones (shock waves). In contrast, the corresponding formulation of the Camassa–Holm equation contains a convolution involving both formula_15 and formula_16, which only makes sense if \"u\" lies in the Sobolev space formula_17 with respect to \"x\". By the Sobolev embedding theorem, this means in particular that the weak solutions of the Camassa–Holm equation must be continuous with respect to \"x\".\n\n"}
{"id": "35243831", "url": "https://en.wikipedia.org/wiki?curid=35243831", "title": "Dominant functor", "text": "Dominant functor\n\nIn category theory, an abstract branch of mathematics, a dominant functor is a functor \"F\" : \"C\" → \"D\" in which every object of \"D\" is a retract of an object of the form \"F\"(\"x\") for some object \"X\" of \"C\".\n"}
{"id": "1040475", "url": "https://en.wikipedia.org/wiki?curid=1040475", "title": "Ehrenfeucht–Fraïssé game", "text": "Ehrenfeucht–Fraïssé game\n\nIn the mathematical discipline of model theory, the Ehrenfeucht–Fraïssé game (also called back-and-forth games)\nis a technique for determining whether two structures \nare elementarily equivalent. The main application of Ehrenfeucht–Fraïssé games is in proving the inexpressibility of certain properties in first-order logic. Indeed, Ehrenfeucht–Fraïssé games provide a complete methodology for proving inexpressibility results for first-order logic. In this role, these games are of particular importance in finite model theory and its applications in computer science (specifically Computer Aided Verification and database theory), since Ehrenfeucht–Fraïssé games are one of the few techniques from model theory that remain valid in the context of finite models. Other widely used techniques for proving inexpressibility results, such as the compactness theorem, do not work in finite models.\n\nEhrenfeucht–Fraïssé like games can also be defined for other logics, such as fixpoint logics and pebble games for finite variable logics; extensions are powerful enough to characterise definability in existential second-order logic.\n\nThe main idea behind the game is that we have two structures, and two players (defined below). One of the players wants to show that the two structures are different, whereas the other player wants to show that they are elementarily equivalent (satisfy the same first-order sentences). The game is played in turns and rounds; A round proceeds as follows: First the first player (Spoiler) chooses any element from one of the structures, and the other player chooses an element from the other structure. The other player's task is to always pick an element that is \"similar\" to the one that Spoiler chose. The second player (Duplicator) wins if there exists an isomorphism between the elements chosen in the two different structures.\n\nThe game lasts for a fixed number of steps (formula_1) (an ordinal, but usually a finite number or formula_2).\n\nSuppose that we are given two structures formula_3 \nand formula_4, each with no function symbols and the same set of relation symbols, \nand a fixed natural number \"n\". We can then define the Ehrenfeucht–Fraïssé \ngame formula_5 to be a game between two players, Spoiler and Duplicator, \nplayed as follows:\n\nFor each \"n\" we define a relation formula_35 if Duplicator wins the \"n\"-move game formula_5. These are all equivalence relations on the class of structures with the given relation symbols. The intersection of all these relations is again an equivalence relation formula_37.\n\nIt is easy to prove that if Duplicator wins this game for all \"n\", that is, formula_37, then formula_3 and formula_4 are elementarily equivalent. If the set of relation symbols being considered is finite, the converse is also true.\n\nThe back-and-forth method used in the Ehrenfeucht–Fraïssé game to verify elementary equivalence was given by Roland Fraïssé \nin his thesis;\nit was formulated as a game by Andrzej Ehrenfeucht. The names Spoiler and Duplicator are due to Joel Spencer. Other usual names are Eloise [sic] and Abelard (and often denoted by formula_41 and formula_42) after Heloise and Abelard, a naming scheme introduced by Wilfrid Hodges in his book \"Model Theory\", or alternatively Eve and Adam.\n\nChapter 1 of Poizat's model theory text contains an introduction to the Ehrenfeucht–Fraïssé game, and so do Chapters 6, 7, and 13 of Rosenstein's book on linear orders. A simple example of the Ehrenfeucht–Fraïssé game is given in one of Ivars Peterson's MathTrek columns .\n\nPhokion Kolaitis' slides and Neil Immerman's book chapter on Ehrenfeucht–Fraïssé games discuss applications in computer science, the methodology theorem for proving inexpressibility results, and several simple inexpressibility proofs using this methodology.\n\n"}
{"id": "1609861", "url": "https://en.wikipedia.org/wiki?curid=1609861", "title": "Graph labeling", "text": "Graph labeling\n\nIn the mathematical discipline of graph theory, a graph labeling is the assignment of labels, traditionally represented by integers, to the edges or vertices, or both, of a graph.\n\nFormally, given a graph , a vertex labeling is a function of \"V\" to a set of \"labels\". A graph with such a function defined is called a vertex-labeled graph. Likewise, an edge labeling is a function of \"E\" to a set of labels. In this case, the graph is called an edge-labeled graph.\n\nWhen the edge labels are members of an ordered set (e.g., the real numbers), it may be called a weighted graph.\n\nWhen used without qualification, the term labeled graph generally refers to a vertex-labeled graph with all labels distinct. Such a graph may equivalently be labeled by the consecutive integers }, where is the number of vertices in the graph. For many applications, the edges or vertices are given labels that are meaningful in the associated domain. For example, the edges may be assigned weights representing the \"cost\" of traversing between the incident vertices.\n\nIn the above definition a graph is understood to be a finite undirected simple graph. However, the notion of labeling may be applied to all extensions and generalizations of graphs. For example, in automata theory and formal language theory it is convenient to consider labeled multigraphs, i.e., a pair of vertices may be connected by several labeled edges.\n\nMost graph labelings trace their origins to labelings presented by Alex Rosa in his 1967 paper. Rosa identified three types of labelings, which he called α-, β-, and ρ-labelings. β-labelings were later renamed \"graceful\" by S. W. Golomb and the name has been popular since.\n\nA graph is known as graceful when its vertices are labeled from 0 to , the size of the graph, and this labeling induces an edge labeling from 1 to . For any edge \"e\", the label of \"e\" is the positive difference between the two vertices incident with \"e\". In other words, if \"e\" is incident with vertices labeled \"i\" and \"j\", \"e\" will be labeled . Thus, a graph is graceful if and only if there exists an injection that induces a bijection from \"E\" to the positive integers up to .\n\nIn his original paper, Rosa proved that all eulerian graphs with size equivalent to 1 or 2 (mod 4) are not graceful. Whether or not certain families of graphs are graceful is an area of graph theory under extensive study. Arguably, the largest unproven conjecture in graph labeling is the Ringel–Kotzig conjecture, which hypothesizes that all trees are graceful. This has been proven for all paths, caterpillars, and many other infinite families of trees. Kotzig himself has called the effort to prove the conjecture a \"disease.\"\n\nAn \"edge-graceful labeling\" on a simple graph (no loops or multiple edges) on \"p\" vertices and \"q\" edges is a labelling of the edges by distinct integers in } such that the labeling on the vertices induced by labeling a vertex with the sum of the incident edges taken modulo \"p\" assigns all values from 0 to to the vertices. A graph \"G\" is said to be \"edge-graceful\" if it admits an edge-graceful labeling.\n\nEdge-graceful labelings were first introduced by S. Lo in 1985.\n\nA necessary condition for a graph to be edge-graceful is \"Lo's condition\":\n\nA \"harmonious labeling\" on a graph \"G\" is an injection from the vertices of \"G\" to the group of integers modulo \"k\", where \"k\" is the number of edges of \"G\", that induces a bijection between the edges of \"G\" and the numbers modulo \"k\" by taking the edge label for an edge (\"x\", \"y\") to be the sum of the labels of the two vertices \"x\", \"y\" (mod \"k\"). A \"harmonious graph\" is one that has a harmonious labeling. Odd cycles are harmonious, as is the Petersen graph. It is conjectured that trees are all harmonious if one vertex label is allowed to be reused.\nThe seven-page book graph provides an example of a graph that is not harmonious.\n\nA \"graph coloring\" is a subclass of graph labelings. A \"vertex coloring\" assigns different labels to adjacent vertices; an \"edge colouring\" assigns different labels to adjacent edges.\n\nA \"lucky labeling\" of a graph \"G\" is an assignment of positive integers to the vertices of \"G\" such that if \"S\"(\"v\") denotes the sum of the labels on the neighbours of \"v\", then \"S\" is a vertex coloring of \"G\". The \"lucky number\" of \"G\" is the least \"k\" such that \"G\" has a lucky labeling with the integers }.\n\n"}
{"id": "24575820", "url": "https://en.wikipedia.org/wiki?curid=24575820", "title": "Idealizer", "text": "Idealizer\n\nIn abstract algebra, the idealizer of a subsemigroup \"T\" of a semigroup \"S\" is the largest subsemigroup of \"S\" in which \"T\" is an ideal. Such an idealizer is given by\n\nIn ring theory, if \"A\" is an additive subgroup of a ring \"R\", then formula_2 (defined in the multiplicative semigroup of \"R\") is the largest subring of \"R\" in which \"A\" is a two-sided ideal.\n\nIn Lie algebra, if \"L\" is a Lie ring (or Lie algebra) with Lie product [\"x\",\"y\"], and \"S\" is an additive subgroup of \"L\", then the set\n\nis classically called the normalizer of \"S\", however it is apparent that this set is actually the Lie ring equivalent of the idealizer. It is not necessary to mention that [\"S\",\"r\"]⊆\"S\", because anticommutativity of the Lie product causes [\"s\",\"r\"] = −[\"r\",\"s\"]∈\"S\". The Lie \"normalizer\" of \"S\" is the largest subring of \"S\" in which \"S\" is a Lie ideal.\n\nOften, when right or left ideals are the additive subgroups of \"R\" of interest, the idealizer is defined more simply by taking advantage of the fact that multiplication by ring elements is already absorbed on one side. Explicitly,\nif \"T\" is a right ideal, or\nif \"L\" is a left ideal.\n\nIn commutative algebra, the idealizer is related to a more general construction. Given a commutative ring \"R\", and given two subsets \"A\" and \"B\" of an \"R\" module \"M\", the conductor or transporter is given by \nIn terms of this conductor notation, an additive subgroup \"B\" of \"R\" has idealizer \n\nWhen \"A\" and \"B\" are ideals of \"R\", the conductor is part of the structure of the residuated lattice of ideals of \"R\".\n\nThe multiplier algebra \"M\"(\"A\") of a \"C\"-algebra \"A\" is isomorphic to the idealizer of \"π\"(\"A\") where \"π\" is any faithful nondegenerate representation of \"A\" on a Hilbert space \"H\".\n\n"}
{"id": "14962", "url": "https://en.wikipedia.org/wiki?curid=14962", "title": "Identity element", "text": "Identity element\n\nIn mathematics, an identity element or neutral element is a special type of element of a set with respect to a binary operation on that set, which leaves other elements unchanged when combined with them. This concept is used in algebraic structures such as groups and rings. The term \"identity element\" is often shortened to \"identity\" (as will be done in this article) when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.\n\nLet be a set  with a binary operation ∗ on it. Then an element  of  is called a left identity if for all  in , and a right identity if for all  in . If is both a left identity and a right identity, then it is called a two-sided identity, or simply an identity.\n\nAn identity with respect to addition is called an additive identity (often denoted as 0) and an identity with respect to multiplication is called a multiplicative identity (often denoted as 1). These need not be ordinary addition and multiplication, but rather arbitrary operations. The distinction is used most often for sets that support both binary operations, such as rings and fields. The multiplicative identity is often called unity in the latter context (a ring with unity). This should not be confused with a unit in ring theory, which is any element having a multiplicative inverse. Unity itself is necessarily a unit.\n\nAs the last example (a semigroup) shows, it is possible for to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if is a left identity and is a right identity then . In particular, there can never be more than one two-sided identity. If there were two, and , then would have to be equal to both and .\n\nIt is also quite possible for to have \"no\" identity element. A common example of this is the cross product of vectors; in this case, the absence of an identity element is related to the fact that the direction of any nonzero cross product is always orthogonal to any element multiplied – so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive semigroup of positive natural numbers.\n\n\n"}
{"id": "1038048", "url": "https://en.wikipedia.org/wiki?curid=1038048", "title": "Iterated logarithm", "text": "Iterated logarithm\n\nIn computer science, the iterated logarithm of \"n\", written  \"n\" (usually read \"log star\"), is the number of times the logarithm function must be iteratively applied before the result is less than or equal to 1. The simplest formal definition is the result of this recurrence relation:\n\nOn the positive real numbers, the continuous super-logarithm (inverse tetration) is essentially equivalent:\nbut on the negative real numbers, log-star is 0, whereas formula_3 for positive \"x\", so the two functions differ for negative arguments.\nIn computer science, is often used to indicate the binary iterated logarithm, which iterates the binary logarithm instead. The iterated logarithm accepts any positive real number and yields an integer. Graphically, it can be understood as the number of \"zig-zags\" needed in Figure 1 to reach the interval [0, 1] on the \"x\"-axis.\n\nMathematically, the iterated logarithm is well-defined not only for base 2 and base \"e\", but for any base greater than formula_4.\n\nThe iterated logarithm is useful in analysis of algorithms and computational complexity, appearing in the time and space complexity bounds of some algorithms such as:\n\n\nThe iterated logarithm grows at an extremely slow rate, much slower than the logarithm itself. For all values of \"n\" relevant to counting the running times of algorithms implemented in practice (i.e., \"n\" ≤ 2, which is far more than the estimated number of atoms in the known universe), the iterated logarithm with base 2 has a value no more than 5.\n\nHigher bases give smaller iterated logarithms. Indeed, the only function commonly used in complexity theory that grows more slowly is the inverse Ackermann function.\n\nThe iterated logarithm is closely related to the generalized logarithm function used in symmetric level-index arithmetic. It is also proportional to the additive persistence of a number, the number of times someone must replace the number by the sum of its digits before reaching its digital root.\n\nSanthanam shows that DTIME and NTIME are distinct up to formula_5\n"}
{"id": "14829789", "url": "https://en.wikipedia.org/wiki?curid=14829789", "title": "Johann Friedrich Hennert", "text": "Johann Friedrich Hennert\n\nJohann Friedrich Hennert (19 October 1733 – 30 March 1813) was German-born and lectured in mathematics and physics at the University of Utrecht. He was a significant student of Leonhard Euler. He was known for his inclination towards the British school of philosophy.\n\nHennert held the chair of mathematics at the University of Utrecht until 1805.\n\nHennert was an important figure in the history of Dutch mathematics. He wrote a number of textbooks on differential calculus.\n\nJan van Swinden was one of his most important students.\n\n"}
{"id": "3578468", "url": "https://en.wikipedia.org/wiki?curid=3578468", "title": "John M. Ball", "text": "John M. Ball\n\nSir John Macleod Ball (born 1948) is Sedleian Professor of Natural Philosophy at the University of Oxford. He was the President of the International Mathematical Union from 2003–06 and a Fellow of Queen's College, Oxford. He was educated at the University of Cambridge and Sussex University, and prior to taking up his Oxford post was a professor of mathematics at Heriot-Watt University in Edinburgh.\n\nBall's research interests include elasticity, the calculus of variations, and infinite-dimensional dynamical systems. He was knighted in the New Year Honours list for 2006 \"for services to Science\". He is a member of the Norwegian Academy of Science and Letters and a fellow of the American Mathematical Society.\n\nHe was a member of the first Abel Prize Committee in 2002 and for the Fields Medal Committee in 1998. From 1996 - 1998 he was president of the London Mathematical Society, and from 2003 - 2006 he was president of the International Mathematical Union, IMU. In October 2011 he was elected on the Executive Board of ICSU for a three-year period starting January 2012. Ball is listed as an ISI highly cited researcher.\n\nAlong with Stuart S. Antman he won the Theodore von Kármán Prize in 1999. In 2018 he received the King Faisal International Prize in Mathematics.\n\nBall received an Honorary Doctorate from Heriot-Watt University in 1998.\n\nHe was elected a Fellow of The Royal Society of Edinburgh in 1980.\n\nBall is married to Sedhar Chozam (Lady Sedhar Ball), a Tibetan-born actress. He has three children.\n\n"}
{"id": "16988587", "url": "https://en.wikipedia.org/wiki?curid=16988587", "title": "Klaus Wagner", "text": "Klaus Wagner\n\nKlaus Wagner (March 31, 1910 – February 6, 2000) was a German mathematician. He studied topology at the University of Cologne under the supervision of Karl Dörge, who had been a student of Issai Schur. Wagner received his Ph.D. in 1937, and taught at Cologne for many years himself. In 1970, he moved to the University of Duisburg, where he remained until his retirement in 1978.\n\nWagner was honored in 1990 by a festschrift on graph theory, and in June 2000, following Wagner's death, the University of Cologne hosted a Festkolloquium in his memory.\n\nWagner is known for his contributions to graph theory and particularly the theory of graph minors, graphs that can be formed from a larger graph by contracting and removing edges.\n\nWagner's theorem characterizes the planar graphs as exactly those graphs that do not have as a minor either a complete graph \"K\" on five vertices or a complete bipartite graph \"K\" with three vertices on each side of its bipartition. That is, these two graphs are the only minor-minimal non-planar graphs. It is closely related to, but should be distinguished from, Kuratowski's theorem, which states that the planar graphs are exactly those graphs that do not contain as a subgraph a subdivision of \"K\" or \"K\".\n\nAnother result of his, also known as Wagner's theorem, is that a four-connected graph is planar if and only if it has no \"K\" minor. This implies a characterization of the graphs with no \"K\" minor as being constructed from planar graphs and Wagner graph (an eight-vertex Möbius ladder) by clique-sums, operations that glue together subgraphs at cliques of up to three vertices and then possibly remove edges from those cliques. This characterization was used by Wagner to show that the case \"k\" = 5 of the Hadwiger conjecture on the chromatic number of \"K\"-minor-free graphs is equivalent to the four color theorem. Analogous characterizations of other families of graphs in terms of the summands of their clique-sum decompositions have since become standard in graph minor theory.\n\nWagner conjectured in the 1930s (although this conjecture was not published until later) that in any infinite set of graphs, one graph is isomorphic to a minor of another. The truth of this conjecture implies that any family of graphs closed under the operation of taking minors (as planar graphs are) can automatically be characterized by finitely many forbidden minors analogously to Wagner's theorem characterizing the planar graphs. Neil Robertson and Paul Seymour finally published a proof of Wagner's conjecture in 2004 and it is now known as the Robertson–Seymour theorem.\n"}
{"id": "214137", "url": "https://en.wikipedia.org/wiki?curid=214137", "title": "Linear form", "text": "Linear form\n\nIn linear algebra, a linear functional or linear form (also called a one-form or covector) is a linear map from a vector space to its field of scalars. In ℝ, if vectors are represented as column vectors, then linear functionals are represented as row vectors, and their action on vectors is given by the dot product, or the matrix product with the row vector on the left and the column vector on the right.  In general, if \"V\" is a vector space over a field \"k\", then a linear functional \"f\" is a function from \"V\" to \"k\" that is linear:\n\nThe set of all linear functionals from \"V\" to \"k\", Hom(\"V\",\"k\"), forms a vector space over \"k\" with the addition of the operations of addition and scalar multiplication (defined pointwise).  This space is called the dual space of \"V\", or sometimes the algebraic dual space, to distinguish it from the continuous dual space.  It is often written \"V\", \"V′\", or \"V\" when the field \"k\" is understood.\n\nIf \"V\" is a topological vector space, the space of continuous linear functionals — the \"continuous dual\" — is often simply called the dual space.  If \"V\" is a Banach space, then so is its (continuous) dual.  To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the \"algebraic dual space\".  In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.\n\nSuppose that vectors in the real coordinate space R are represented as column vectors\n\nFor each row vector [\"a\" … \"a\"] there is a linear functional \"f\" defined by\nand each linear functional can be expressed in this form.\n\nThis can be interpreted as either the matrix product or the dot product of the row vector [\"a\" ... \"a\"] and the column vector formula_7:\n\nLinear functionals first appeared in functional analysis, the study of vector spaces of functions.  A typical example of a linear functional is integration: the linear transformation defined by the Riemann integral\n\nis a linear functional from the vector space C[\"a\", \"b\"] of continuous functions on the interval [\"a\", \"b\"] to the real numbers. The linearity of follows from the standard facts about the integral:\n\nLet \"P\" denote the vector space of real-valued polynomial functions of degree ≤\"n\" defined on an interval [\"a\", \"b\"].  If \"c\" ∈ [\"a\", \"b\"], then let be the evaluation functional\nThe mapping \"f\" → \"f\"(\"c\") is linear since\n\nIf \"x\", ..., \"x\" are distinct points in , then the evaluation functionals form a basis of the dual space of \"P\".  ( proves this last fact using Lagrange interpolation.)\n\nThe integration functional defined above defines a linear functional on the subspace of polynomials of degree . If are distinct points in , then there are coefficients for which\n\nfor all . This forms the foundation of the theory of numerical quadrature.\n\nThis follows from the fact that the linear functionals defined above form a basis of the dual space of .\n\nLinear functionals are particularly important in quantum mechanics.  Quantum mechanical systems are represented by Hilbert spaces, which are anti–isomorphic to their own dual spaces.  A state of a quantum mechanical system can be identified with a linear functional.  For more information see bra–ket notation.\n\nIn the theory of generalized functions, certain kinds of generalized functions called distributions can be realized as linear functionals on spaces of test functions.\n\n\nIn finite dimensions, a linear functional can be visualized in terms of its level sets.  In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel hyperplanes.  This method of visualizing linear functionals is sometimes introduced in general relativity texts, such as \"Gravitation\" by .\n\nEvery non-degenerate bilinear form on a finite-dimensional vector space \"V\" induces an isomorphism such that \n\nwhere the bilinear form on \"V\" is denoted (for instance, in Euclidean space is the dot product of \"v\" and \"w\").\n\nThe inverse isomorphism is , where \"v\" is the unique element of \"V\" such that\n\nThe above defined vector is said to be the dual vector of .\n\nIn an infinite dimensional Hilbert space, analogous results hold by the Riesz representation theorem.  There is a mapping into the \"continuous dual space\" \"V\".  However, this mapping is antilinear rather than linear.\n\nLet the vector space \"V\" have a basis formula_16, not necessarily orthogonal.  Then the dual space \"V*\" has a basis formula_17 called the dual basis defined by the special property that\n\nOr, more succinctly,\n\nwhere δ is the Kronecker delta.  Here the superscripts of the basis functionals are not exponents but are instead contravariant indices.\n\nA linear functional formula_20 belonging to the dual space formula_21 can be expressed as a linear combination of basis functionals, with coefficients (\"components\") \"u\", \n\nThen, applying the functional formula_20 to a basis vector \"e\" yields\n\ndue to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.  Then\n\nSo each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.\n\nWhen the space \"V\" carries an inner product, then it is possible to write explicitly a formula for the dual basis of a given basis.  Let \"V\" have (not necessarily orthogonal) basis formula_26.  In three dimensions (), the dual basis can be written explicitly\nfor \"i\" = 1, 2, 3, where \"ε\" is the Levi-Civita symbol and formula_28 the inner product (or dot product) on \"V\".\n\nIn higher dimensions, this generalizes as follows\nwhere formula_30 is the Hodge star operator.\n\n\n"}
{"id": "49076898", "url": "https://en.wikipedia.org/wiki?curid=49076898", "title": "Liquidity at risk", "text": "Liquidity at risk\n\nThe Liquidity-at-Risk (short: LaR) is a quantity to measure financial risks and is the maximum net liquidity drain relative to the expected liquidity position which should not be exceeded at a given confidence level (e.g. 95%). The LaR is analog to the Value-at-Risk (VaR) where a quantile of the EBIT-distribution is considered, however it does take stochastic cash flows into account.\n\nStatistical measures for financial risk are not intuitive. Increasing the confidence level (e.g. from 99.0% to 99.9%) does not capture very rare events with possibly high impact. The only way around is to use extreme value theory for modelling the distribution tails. In other words: Statistical liquidity risk modelling approaches do not provide certainty in terms of a reliable lower limit for future liquidity.\n\n"}
{"id": "20250230", "url": "https://en.wikipedia.org/wiki?curid=20250230", "title": "Low and high hierarchies", "text": "Low and high hierarchies\n\nIn the computational complexity theory, the low hierarchy and high hierarchy of complexity levels were introduced in 1983 by Uwe Schöning to describe the internal structure of the complexity class NP.\n\nLater these hierarchies were extended to sets outside NP.\n\nThe framework of high/low hierarchies makes sense only under the assumption that P is not NP. On the other hand, if the low hierarchy consists of at least two levels, then P is not NP.\n\nIt is not known whether these hierarchies cover all NP.\n"}
{"id": "3390080", "url": "https://en.wikipedia.org/wiki?curid=3390080", "title": "Margolus–Levitin theorem", "text": "Margolus–Levitin theorem\n\nThe Margolus–Levitin theorem, named for Norman Margolus and Lev B. Levitin, gives a fundamental limit on quantum computation (strictly speaking on all forms on computation). The processing rate cannot be higher than 6 × 10 operations per second per joule of energy. Or stating the bound for one bit:\n\n\n"}
{"id": "25750436", "url": "https://en.wikipedia.org/wiki?curid=25750436", "title": "Mathematical sciences", "text": "Mathematical sciences\n\nThe mathematical sciences are a group of areas of study that includes, in addition to mathematics, those academic disciplines that are primarily mathematical in nature but may not be universally considered subfields of mathematics proper.\n\nStatistics, for e.g., is mathematical in its methods but grew out of scientific observations which merged with inverse probability and grew through applications in the social sciences, some areas of physics and biometrics to become its own separate, though closely allied field. Computer science, computational science, population genetics, operations research, cryptology, econometrics, theoretical physics, chemical reaction network theory and actuarial science are other fields that may be considered part of mathematical sciences.\n\nSome institutions offer degrees in mathematical sciences (e.g. the United States Military Academy, Stanford University, and University of Khartoum) or applied mathematical sciences (e.g. the University of Rhode Island).\n\n\n"}
{"id": "32364992", "url": "https://en.wikipedia.org/wiki?curid=32364992", "title": "Mode of a linear field", "text": "Mode of a linear field\n\nIn physics, a vector field is linear if it is a solution of a set of linear equations \"E\". For instance, in physics, the electromagnetic field in vacuum, defined in the usual (3 + 1)-dimensional space \"S\", obeys Maxwell's equations. A linear combination of electromagnetic fields, with constant, real coefficients, is a new field which obeys Maxwell's equations.\n\nThe solutions of the linear equations are represented in a real vector space \"M\". A radius of \"M\", which represents proportional solutions, is called a \"mode\".\n\nA norm may be defined. For instance, in electromagnetism, it is usually the energy of the solution \"assuming that there is no other field in S\". From the norm are defined the orthogonality and the scalar product of solutions. The orthogonality of solutions extends to the corresponding modes.\n"}
{"id": "11118768", "url": "https://en.wikipedia.org/wiki?curid=11118768", "title": "Parabolic induction", "text": "Parabolic induction\n\nIn mathematics, parabolic induction is a method of constructing representations of a reductive group from representations of its parabolic subgroups. \n\nIf \"G\" is a reductive algebraic group and formula_1 is the Langlands decomposition of a parabolic subgroup \"P\", then parabolic induction consists of taking a representation of formula_2, extending it to \"P\" by letting \"N\" act trivially, and inducing the result from \"P\" to \"G\".\n\nThere are some generalizations of parabolic induction using cohomology, such as cohomological parabolic induction and Deligne–Lusztig theory.\n\nThe \"philosophy of cusp forms\" was a slogan of Harish-Chandra, expressing his idea of a kind of reverse engineering of automorphic form theory, from the point of view of representation theory. The discrete group Γ fundamental to the classical theory disappears, superficially. What remains is the basic idea that representations in general are to be constructed by parabolic induction of cuspidal representations. A similar philosophy was enunciated by Israel Gelfand, and the philosophy is a precursor of the Langlands program. A consequence for thinking about representation theory is that cuspidal representations are the fundamental class of objects, from which other representations may be constructed by procedures of induction.\n\nAccording to Nolan Wallach\n\nPut in the simplest terms the \"philosophy of cusp forms\" says that for each Γ-conjugacy classes of Q-rational parabolic subgroups one should construct automorphic functions (from objects from spaces of lower dimensions) whose constant terms are zero for other conjugacy classes and the constant terms for [an] element of the given class give all constant terms for this parabolic subgroup. This is almost possible and leads to a description of all automorphic forms in terms of these constructs and cusp forms. The construction that does this is the Eisenstein series.\n\n"}
{"id": "55213021", "url": "https://en.wikipedia.org/wiki?curid=55213021", "title": "Parsimonious reduction", "text": "Parsimonious reduction\n\nIn computational complexity theory and game complexity, a parsimonious reduction is a transformation from one problem to another (a reduction) that preserves the number of solutions.\n\nMore formally, \nparsimonious reductions are defined for problems in nondeterministic complexity classes such as NP,\nwhich are defined by a \"verifier\" algorithm whose input is a pair of strings (an instance of the problem and a candidate solution) and whose output is true when the candidate solution is a valid solution of the instance.\nFor example, in the Boolean satisfiability problem, the instance might be a Boolean expression and the candidate solution might be a truth assignment to its variables; a valid truth assignment is one that makes the expression evaluate to true.\nA parsimonious reduction from one problem \"X\" of this type to another problem \"Y\" is an algorithmic transformation from instances of \"X\" to instances of \"Y\"\nsuch that the number of solutions of an instance of \"X\" equals the number of solutions of the transformed instance of \"Y\".\n\nJust as many-one reductions are important for proving NP-completeness,\nparsimonious reductions are important for proving completeness for counting complexity classes such as ♯P. Because parsimonious reductions preserve the property of having a unique solution, they are also used in game complexity, to show the hardness of puzzles such as sudoku where the uniqueness of the solution is an important part of the definition of the puzzle.\n\nSpecific types of parsimonious reductions may be defined by the computational complexity or other properties of the transformation algorithm.\nFor instance, a \"polynomial-time parsimonious reduction\" is one in which the transformation algorithm takes polynomial time. These are the types of reduction used to prove ♯P-completeness. In parameterized complexity, \"fpt parsimonious reductions\" are used; these are parsimonious reductions whose transformation is a fixed-parameter tractable algorithm and that map bounded parameter values to bounded parameter values by a computable function.\n\nPolynomial-time parsimonious reductions are a special case of a more general class of reductions for counting problems, the polynomial-time counting reductions.\n"}
{"id": "328252", "url": "https://en.wikipedia.org/wiki?curid=328252", "title": "Pick's theorem", "text": "Pick's theorem\n\nGiven a simple polygon constructed on a grid of equal-distanced points (i.e., points with integer coordinates) such that all the polygon's vertices are grid points, Pick's theorem provides a simple formula for calculating the area of this polygon in terms of the number of \"lattice points in the interior\" located in the polygon and the number of \"lattice points on the boundary\" placed on the polygon's perimeter:\n\nIn the example shown, we have interior points and boundary points, so the area is  = 7 +  − 1 = 7 + 4 − 1 = 10 square units.\n\nNote that the theorem as stated above is only valid for \"simple\" polygons, i.e., ones that consist of a single piece and do not contain holes. For a polygon that has holes, with a boundary in the form of simple closed curves, the slightly more complicated formula gives the area.\n\nThe result was first described by Georg Alexander Pick in 1899. The Reeve tetrahedron shows that there is no analogue of Pick's theorem in three dimensions that expresses the volume of a polytope by counting its interior and boundary points. However, there is a generalization in higher dimensions via Ehrhart polynomials. The formula also generalizes to surfaces of polyhedra.\n\nConsider a polygon and a triangle , with one edge in common with . Assume Pick's theorem is true for both and separately; we want to show that it is also true for the polygon obtained by adding to . Since and share an edge, all the boundary points along the edge in common are merged to interior points, except for the two endpoints of the edge, which are merged to boundary points. So, calling the number of boundary points in common , we have\n\nand\n\nFrom the above follows\n\nand\n\nSince we are assuming the theorem for and for separately,\n\nTherefore, if the theorem is true for polygons constructed from triangles, the theorem is also true for polygons constructed from triangles. For general polytopes, it is well known that they can always be triangulated. That this is true in dimension 2 is an easy fact. To finish the proof by mathematical induction, it remains to show that the theorem is true for triangles. The verification for this case can be done in these short steps:\n\n\nThe last step uses the fact that if the theorem is true for the polygon and for the triangle , then it's also true for ; this can be seen by a calculation very much similar to the one shown above.\n\n\n"}
{"id": "33297737", "url": "https://en.wikipedia.org/wiki?curid=33297737", "title": "Radial set", "text": "Radial set\n\nIn mathematics, given a linear space formula_1, a set formula_2 is radial at the point formula_3 if for every formula_4 there exists a formula_5 such that for every formula_6, formula_7. Geometrically, this means formula_8 is radial at formula_9 if for every formula_4 a line segment emanating from formula_9 in the direction of formula_12 lies in formula_8, where the length of the line segment is required to be non-zero but can depend on formula_12.\n\nThe set of all points at which formula_2 is radial is equal to the algebraic interior. The points at which a set is radial are often referred to as internal points.\n\nA set formula_2 is absorbing if and only if it is radial at 0. Some authors use the term \"radial\" as a synonym for \"absorbing\", i. e. they call a set radial if it is radial at 0.\n"}
{"id": "21118619", "url": "https://en.wikipedia.org/wiki?curid=21118619", "title": "Rayleigh's method of dimensional analysis", "text": "Rayleigh's method of dimensional analysis\n\nRayleigh's method of dimensional analysis is a conceptual tool used in physics, chemistry, and engineering. This form of dimensional analysis expresses a functional relationship of some variables in the form of an exponential equation. It was named after Lord Rayleigh.\n\nThe method involves the following steps:\n\n\nDrawback – It doesn't provide any information regarding number of dimensionless groups to be obtained as a result of dimension analysis\n\n"}
{"id": "4412890", "url": "https://en.wikipedia.org/wiki?curid=4412890", "title": "Receiver (information theory)", "text": "Receiver (information theory)\n\nThe receiver in information theory is the receiving end of a communication channel. It receives decoded messages/information from the sender, who first encoded them. Sometimes the receiver is modeled so as to include the decoder. Real-world receivers like radio receivers or telephones can not be expected to receive as much information as predicted by the noisy channel coding theorem.\n"}
{"id": "19271448", "url": "https://en.wikipedia.org/wiki?curid=19271448", "title": "Road coloring theorem", "text": "Road coloring theorem\n\nIn graph theory the road coloring theorem, known until recently as the road coloring conjecture, deals with synchronized instructions. The issue involves whether by using such instructions, one can reach or locate an object or destination from any other point within a network (which might be a representation of city streets or a maze). In the real world, this phenomenon would be as if you called a friend to ask for directions to his house, and he gave you a set of directions that worked no matter where you started from. This theorem also has implications in symbolic dynamics.\n\nThe theorem was first conjectured by . It was proved by .\n\nThe image to the right shows a directed graph on eight vertices in which each vertex has out-degree 2. (Each vertex in this case also has in-degree 2, but that is not necessary for a synchronizing coloring to exist.) The edges of this graph have been colored red and blue to create a synchronizing coloring.\n\nFor example, consider the vertex marked in yellow. No matter where in the graph you start, if you traverse all nine edges in the walk \"blue-red-red—blue-red-red—blue-red-red\", you will end up at the yellow vertex. Similarly, if you traverse all nine edges in the walk \"blue-blue-red—blue-blue-red—blue-blue-red\", you will always end up at the vertex marked in green, no matter where you started.\n\nThe road coloring theorem states that for a certain category of directed graphs, it is always possible to create such a coloring.\n\nLet \"G\" be a finite, strongly connected, directed graph where all the vertices have the same out-degree \"k\". Let \"A\" be the alphabet containing the letters 1, ..., \"k\". A \"synchronizing coloring\" (also known as a \"collapsible coloring\") in \"G\" is a labeling of the edges in \"G\" with letters from \"A\" such that (1) each vertex has exactly one outgoing edge with a given label and (2) for every vertex \"v\" in the graph, there exists a word \"w\" over \"A\" such that all paths in \"G\" corresponding to \"w\" terminate at \"v\".\n\nThe terminology \"synchronizing coloring\" is due to the relation between this notion and that of a synchronizing word in finite automata theory.\n\nFor such a coloring to exist at all, it is necessary that \"G\" be aperiodic. The road coloring theorem states that aperiodicity is also \"sufficient\" for such a coloring to exist. Therefore, the road coloring problem can be stated briefly as:\n\nPrevious partial or special-case results include the following:\n\n\n\n"}
{"id": "73415", "url": "https://en.wikipedia.org/wiki?curid=73415", "title": "Sieve of Eratosthenes", "text": "Sieve of Eratosthenes\n\nIn mathematics, the sieve of Eratosthenes is a simple, ancient algorithm for finding all prime numbers up to any given limit.\n\nIt does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, . The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.\n\nThe earliest known reference to the sieve (, \"kóskinon Eratosthénous\") is in Nicomachus of Gerasa's \"Introduction to Arithmetic\", which describes it and attributes it to Eratosthenes of Cyrene, a Greek mathematician.\n\nOne of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in arithmetic progressions.\n\nA prime number is a natural number that has exactly two distinct natural number divisors: 1 and itself.\n\nTo find all the prime numbers less than or equal to a given integer by Eratosthenes' method:\n\n\nThe main idea here is that every value given to will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).\n\nAs a refinement, it is sufficient to mark the numbers in step 3 starting from , as all the smaller multiples of will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when is greater than . \n\nAnother refinement is to initially list odd numbers only, , and count in increments of from in step 3, thus marking only odd multiples of . This actually appears in the original algorithm. This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of are generated that are coprime with those small primes, in the first place.\n\nTo find all the prime numbers less than or equal to 30, proceed as follows.\n\nFirst, generate a list of integers from 2 to 30:\n\nThe first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list):\n\nThe next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list):\n\nThe next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5):\n\nThe next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30:\n\nThe sieve of Eratosthenes can be expressed in pseudocode, as follows:\n\nThis algorithm produces all primes not greater than . It includes a common optimization, which is to start enumerating the multiples of each prime from . The time complexity of this algorithm is , provided the array update is an operation, as is usually the case.\n\nAs Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements. For large , the range of primes may not fit in memory; worse, even for moderate , its cache use is highly suboptimal. The algorithm walks through the entire array , exhibiting almost no locality of reference.\n\nA solution to these problems is offered by \"segmented\" sieves, where only portions of the range are sieved at a time. These have been known since the 1970s, and work as follows:\n\n\nIf is chosen to be , the space complexity of the algorithm is , while the time complexity is the same as that of the regular sieve.\n\nFor ranges with upper limit so large that the sieving primes below as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the sieve of Sorenson can be used instead.\n\nAn incremental formulation of the sieve generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime are generated directly by counting up from the square of the prime in increments of (or for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the dataflow paradigm as\n\nPrimes can also be produced by iteratively sieving out the composites through [[Trial division|divisibility testing]] by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical [[Analysis of algorithms|complexity]] than that of the sieve of Eratosthenes in generating ranges of primes.\n\nWhen testing each prime, the \"optimal\" trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes \"for free\", between the composites. The widely known 1975 [[functional programming|functional]] sieve code by [[David Turner (computer scientist)|David Turner]] is often presented as an example of the sieve of Eratosthenes but is actually a sub-optimal trial division sieve.\n\nThe work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or\nwhere is the sieving range in this and all further analysis.\n\nBy rearranging Mertens' second theorem, this is equal to as approaches infinity, where M is the Meissel–Mertens constant of about ...\n\nThe optimization of starting at the square of each prime and only culling for primes less than the square root changes the \"\" in the above expression to (or ) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first primes is and the prime number theorem says that is approximately , then the sum of primes to is , and therefore the sum of base primes to is expressed as a factor of . The extra offset of two per base prime is , where is the prime-counting function in this case, or ; expressing this as a factor of as are the other terms, this is . Combining all of this, the expression for the number of optimized operations without wheel factorization is\n\nFor the wheel factorization cases, there is a further offset of the operations not done of\nwhere is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is\nand it can easily be determined that this wheel factor is\nas is the fraction of remaining candidates for the highest wheel prime, , and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.\n\nCombining all of the above analysis, the total number of operations for a sieving range up to including wheel factorization for primes up to is approximately\n\nTo show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):\n\nThe above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (10) and above.\n\nThe sieve of Eratosthenes is a popular way to benchmark computer performance. As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the time complexity of calculating all primes below in the random access machine model is operations, a direct consequence of the fact that the prime harmonic series asymptotically approaches . It has an exponential time complexity with regard to input size, though, which makes it a pseudo-polynomial algorithm. The basic algorithm requires of memory.\n\nThe bit complexity of the algorithm is bit operations with a memory requirement of .\n\nThe normally implemented page segmented version has the same operational complexity of as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size .\n\nA special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses operations and bits of memory.\n\nTo show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):\n\nThe above shows that the estimate is not very accurate even for maximum practical ranges of about 10. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.\n\nOne should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared sieve of Atkin if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 10 at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.\n\nUsing big O notation is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve has an performance, but its basic implementation requires either a \"one large array\" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.\n\nEuler's proof of the zeta product formula contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once. The same sieve was rediscovered and observed to take linear time by . It, too, starts with a list of numbers from 2 to in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:\nHere the example is shown starting from odds, after the first step of the algorithm. Thus, on the th step all the remaining multiples of the th prime are removed from the list, which will thereafter contain only numbers coprime with the first primes (cf. wheel factorization), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.\n\nThus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime. In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.\n\nNote that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is , , , , ..., , ..., so care must be taken dealing with this.\n\n\n"}
{"id": "586694", "url": "https://en.wikipedia.org/wiki?curid=586694", "title": "Signed number representations", "text": "Signed number representations\n\nIn computing, signed number representations are required to encode negative numbers in binary number systems.\n\nIn mathematics, negative numbers in any base are represented by prefixing them with a minus (\"−\") sign. However, in computer hardware, numbers are represented only as sequences of bits, without extra symbols. The four best-known methods of extending the binary numeral system to represent signed numbers are: sign-and-magnitude, ones' complement, two's complement, and offset binary. Some of the alternative methods use implicit instead of explicit signs, such as negative binary, using the base −2. Corresponding methods can be devised for other bases, whether positive, negative, fractional, or other elaborations on such themes.\n\nThere is no definitive criterion by which any of the representations is universally superior. The representation used in most current computing devices is two's complement, although the Unisys ClearPath Dorado series mainframes use ones' complement.\n\nThe early days of digital computing were marked by a lot of competing ideas about both hardware technology and mathematics technology (numbering systems). One of the great debates was the format of negative numbers, with some of the era's most expert people having very strong and different opinions. One camp supported two's complement, the system that is dominant today. Another camp supported ones' complement, where any positive value is made into its negative equivalent by inverting all of the bits in a word. A third group supported \"sign & magnitude\" (sign-magnitude), where a value is changed from positive to negative simply by toggling the word's sign (high-order) bit.\n\nThere were arguments for and against each of the systems. Sign & magnitude allowed for easier tracing of memory dumps (a common process 40 years ago) as small numeric values use fewer 1 bits. Internally, these systems did ones' complement math so numbers would have to be converted to ones' complement values when they were transmitted from a register to the math unit and then converted back to sign-magnitude when the result was transmitted back to the register. The electronics required more gates than the other systemsa key concern when the cost and packaging of discrete transistors was critical. IBM was one of the early supporters of sign-magnitude, with their 704, 709 and 709x series computers being perhaps the best known systems to use it.\n\nOnes' complement allowed for somewhat simpler hardware designs as there was no need to convert values when passed to and from the math unit. But it also shared an undesirable characteristic with sign-magnitudethe ability to represent negative zero (−0). Negative zero behaves exactly like positive zero; when used as an operand in any calculation, the result will be the same whether an operand is positive or negative zero. The disadvantage, however, is that the existence of two forms of the same value necessitates two rather than a single comparison when checking for equality with zero. Ones' complement subtraction can also result in an end-around borrow (described below). It can be argued that this makes the addition/subtraction logic more complicated or that it makes it simpler as a subtraction requires simply inverting the bits of the second operand as it is passed to the adder. The PDP-1, CDC 160 series, CDC 3000 series, CDC 6000 series, UNIVAC 1100 series, and the LINC computer use ones' complement representation.\n\nTwo's complement is the easiest to implement in hardware, which may be the ultimate reason for its widespread popularity. Processors on the early mainframes often consisted of thousands of transistorseliminating a significant number of transistors was a significant cost savings. Mainframes such as the IBM System/360, the GE-600 series, and the PDP-6 and PDP-10 use two's complement, as did minicomputers such as the PDP-5 and PDP-8 and the PDP-11 and VAX. The architects of the early integrated circuit-based CPUs (Intel 8080, etc.) chose to use two's complement math. As IC technology advanced, virtually all adopted two's complement technology. x86, m68k, Power Architecture, MIPS, SPARC, ARM, Itanium, PA-RISC, and DEC Alpha processors are all two's complement.\n\nThis representation is also called \"sign–magnitude\" or \"sign and magnitude\" representation. In this approach, a number's sign is represented with a sign bit: setting that bit (often the most significant bit) to 0 for a positive number or positive zero, and setting it to 1 for a negative number or negative zero. The remaining bits in the number indicate the magnitude (or absolute value). Hence, in a byte with only seven bits (apart from the sign bit), the magnitude can range from 0000000 (0) to 1111111 (127). Thus numbers ranging from −127 to +127 can be represented once the sign bit (the eighth bit) is added. For example, −43 encoded in an eight-bit byte is 10101011 while 43 is 00101011. A consequence of using signed magnitude representation is that there are two ways to represent zero, 00000000 (0) and 10000000 (−0).\n\nThis approach is directly comparable to the common way of showing a sign (placing a \"+\" or \"−\" next to the number's magnitude). Some early binary computers (e.g., IBM 7090) use this representation, perhaps because of its natural relation to common usage. Signed magnitude is the most common way of representing the significand in floating point values.\n\nAlternatively, a system known as ones' complement can be used to represent negative numbers. The ones' complement form of a negative binary number is the bitwise NOT applied to it, i.e. the \"complement\" of its positive counterpart. Like sign-and-magnitude representation, ones' complement has two representations of 0: 00000000 (+0) and 11111111 (−0).\n\nAs an example, the ones' complement form of 00101011 (43) becomes 11010100 (−43). The range of signed numbers using ones' complement is represented by to and ±0. A conventional eight-bit byte is −127 to +127 with zero being either 00000000 (+0) or 11111111 (−0).\n\nTo add two numbers represented in this system, one does a conventional binary addition, but it is then necessary to do an \"end-around carry\": that is, add any resulting carry back into the resulting sum. To see why this is necessary, consider the following example showing the case of the addition of −1 (11111110) to +2 (00000010):\n\nIn the previous example, the first binary addition gives 00000000, which is incorrect. The correct result (00000001) only appears when the carry is added back in. \n\nA remark on terminology: The system is referred to as \"ones' complement\" because the negation of a positive value x (represented as the bitwise NOT of x) can also be formed by subtracting x from the ones' complement representation of zero that is a long sequence of ones (−0). Two's complement arithmetic, on the other hand, forms the negation of x by subtracting x from a single large power of two that is congruent to +0. Therefore, ones' complement and two's complement representations of the same negative value will differ by one.\n\nNote that the ones' complement representation of a negative number can be obtained from the sign-magnitude representation merely by bitwise complementing the magnitude.\n\nThe problems of multiple representations of 0 and the need for the end-around carry are circumvented by a system called two's complement. In two's complement, negative numbers are represented by the bit pattern which is one greater (in an unsigned sense) than the ones' complement of the positive value.\n\nIn two's-complement, there is only one zero, represented as 00000000. Negating a number (whether negative or positive) is done by inverting all the bits and then adding one to that result. This actually reflects the ring structure on all integers modulo 2: formula_1. Addition of a pair of two's-complement integers is the same as addition of a pair of unsigned numbers (except for detection of overflow, if that is done); the same is true for subtraction and even for \"N\" lowest significant bits of a product (value of multiplication). For instance, a two's-complement addition of 127 and −128 gives the same binary bit pattern as an unsigned addition of 127 and 128, as can be seen from the 8-bit two's complement table.\n\nAn easier method to get the negation of a number in two's complement is as follows:\n\nMethod two:\n\n\nExample: for +2, which is 00000010 in binary (the ~ character is the C bitwise NOT operator, so ~X means \"invert all the bits in X\"):\n\n\nOffset binary, also called excess-K or biased representation, uses a pre-specified number K as a biasing value. A value is represented by the unsigned number which is K greater than the intended value. Thus 0 is represented by K, and −K is represented by the all-zeros bit pattern. This can be seen as a slight modification and generalization of the aforementioned two's-complement, which is virtually the representation with negated most significant bit.\n\nBiased representations are now primarily used for the exponent of floating-point numbers. The IEEE floating-point standard defines the exponent field of a single-precision (32-bit) number as an 8-bit excess-127 field. The double-precision (64-bit) exponent field is an 11-bit excess-1023 field; see exponent bias. It also had use for binary-coded decimal numbers as excess-3.\n\nIn conventional binary number systems, the base, or radix, is 2; thus the rightmost bit represents 2, the next bit represents 2, the next bit 2, and so on. However, a binary number system with base −2 is also possible.\nThe rightmost bit represents , the next bit represents , the next bit and so on, with alternating sign. The numbers that can be represented with four bits are shown in the comparison table below.\n\nThe range of numbers that can be represented is asymmetric. If the word has an even number of bits, the magnitude of the largest negative number that can be represented is twice as large as the largest positive number that can be represented, and vice versa if the word has an odd number of bits.\n\nThe following table shows the positive and negative integers that can be represented using four bits.\n\nSame table, as viewed from \"given these binary bits, what is the number as interpreted by the representation system\":\n\nGoogle's Protocol Buffers \"zig-zag encoding\" is a system similar to sign-and-magnitude, but uses the least significant bit to represent the sign and has a single representation of zero. This allows a variable-length quantity encoding intended for nonnegative (unsigned) integers to be used efficiently for signed integers.\n\nAnother approach is to give each digit a sign, yielding the signed-digit representation. For instance, in 1726, John Colson advocated reducing expressions to \"small numbers\", numerals 1, 2, 3, 4, and 5. In 1840, Augustin Cauchy also expressed preference for such modified decimal numbers to reduce errors in computation.\n\n\n"}
{"id": "396513", "url": "https://en.wikipedia.org/wiki?curid=396513", "title": "Simulation preorder", "text": "Simulation preorder\n\nIn theoretical computer science a simulation preorder is a relation between state transition systems associating systems which behave in the same way in the sense that one system \"simulates\" the other.\n\nIntuitively, a system simulates another system if it can match all of its moves.\n\nThe basic definition relates states within one transition system, but this is easily adapted to relate two separate transition systems by building a system consisting of the disjoint union of the corresponding components.\n\nGiven a labelled state transition system (S, Λ, →), a \"simulation\" relation is a binary relation R over S (i.e. R ⊆ S × S) such that for every pair of elements (p,q) ∈ R, for all α ∈ Λ, and for all p' ∈ S, \n\nimplies that there is a q' ∈ S such that\n\nand (p',q') ∈ R.\n\nEquivalently, in terms of relational composition:\n\nGiven two states p and q in S, q \"simulates\" p, written p ≤ q if there is a simulation R such that (p, q) ∈ R. The relation ≤ is a preorder, and is usually called the \"simulation preorder\". It is the largest simulation relation over a given transition system.\n\nTwo states \"p\" and \"q\" are said to be \"similar\", written p ≤≥ q, if \"p\" simulates \"q\" and \"q\" simulates \"p\". Similarity is an equivalence relation, but it is coarser than bisimilarity.\n\nWhen comparing two different transition systems (S', Λ', →') and (S\", Λ\", →\"), the basic notions of simulation and similarity can be used by forming the disjoint composition of the two machines, (S, Λ, →) with S = S' ∐ S\", Λ = Λ' ∪ Λ\" and → = →' ∪ →\", where ∐ is the disjoint union operator between sets.\n\n\n"}
{"id": "2628049", "url": "https://en.wikipedia.org/wiki?curid=2628049", "title": "Sphericity", "text": "Sphericity\n\nSphericity is the measure of how closely the shape of an object approaches that of a mathematically perfect sphere. For example, the sphericity of the balls inside a ball bearing determines the quality of the bearing, such as the load it can bear or the speed at which it can turn without failing. Sphericity is a specific example of a compactness measure of a shape. Defined by Wadell in 1935, the sphericity, formula_1, of a particle is: the ratio of the surface area of a sphere (with the same volume as the given particle) to the surface area of the particle:\n\nwhere formula_3 is volume of the particle and formula_4 is the surface area of the particle. The sphericity of a sphere is unity by definition and, by the isoperimetric inequality, any particle which is not a sphere will have sphericity less than 1.\n\nSphericity applies in three dimensions; its analogue in two dimensions, such as the cross sectional circles along a cylindrical object such as a shaft, is called roundness.\n\nThe sphericity, formula_1, of an oblate spheroid (similar to the shape of the planet Earth) is:\n\nwhere \"a\" and \"b\" are the semi-major and semi-minor axes respectively.\n\nHakon Wadell defined sphericity as the surface area of a \nsphere of the same volume as the particle divided by the actual surface area of the particle. \n\nFirst we need to write surface area of the sphere, formula_7 in terms of the volume of the particle, formula_3\n\ntherefore\n\nhence we define formula_11 as:\n\n\n"}
{"id": "48751362", "url": "https://en.wikipedia.org/wiki?curid=48751362", "title": "Stoer–Wagner algorithm", "text": "Stoer–Wagner algorithm\n\nIn graph theory, the Stoer–Wagner algorithm is a recursive algorithm to solve the minimum cut problem in undirected weighted graphs with non-negative weights. It was proposed by Mechthild Stoer and Frank Wagner in 1995. The essential idea of this algorithm is to shrink the graph by merging the most intensive vertices, until the graph only contains two combined vertex sets. At each phase, the algorithm finds the minimum formula_1-formula_2 cut for two vertices formula_1 and formula_2 chosen as its will. Then the algorithm shrinks the edge between formula_1 and formula_2 to search for non formula_1-formula_2 cuts. The minimum cut found in all phases will be the minimum weighted cut of the graph.\n\nA cut is a partition of the vertices of a graph into two disjoint subsets. A minimum cut is a cut for which the size or weight of the cut is not larger than the size of any other cut. For an unweighted graph, the minimum cut would simply be the cut with the least edges. For a weighted graph, the sum of all edges' weight on the cut determines whether it is a minimum cut. In practice, the minimum cut problem is always discussed with the maximum flow problem, to explore the maximum capacity of a network, since the minimum cut is a bottleneck in a graph or network.\n\nLet formula_9 be a weighted undirected graph. Let formula_10 be a global min-cut of formula_11. Suppose that formula_12. If exactly one of formula_1 or formula_2 is in formula_15, then formula_10 is also a formula_1-formula_2 min-cut of formula_11.\n\nThis algorithm starts by finding a s-t min-cut formula_10 of formula_11, for the two vertices formula_22. For the pair of formula_23, it has two different situations: formula_10 is a global min-cut of formula_11; or they belong to the same side of the global min-cut of formula_11. Therefore, the global min-cut can be found by checking the graph formula_27, which is the graph after the merging of vertices formula_1 and formula_2. During the merging, if formula_1 and formula_2 are connected by an edge then this edge disappears. If s and t both have edges to some vertex v, then the weight of the edge from the new vertex st to v is formula_32. The algorithm is described as:\n formula_34\n\n while formula_39\nThe algorithm works in phases. In the MinimumCutPhase, the subset formula_36 of the graphs vertices grows starting with an arbitrary single vertex until formula_36 is equal to formula_43. In each step, the vertex which is outside of formula_36, but most tightly connected with formula_36 is added to the set formula_36. This procedure can be formally shown as: add vertex formula_47 such that formula_48, where formula_49 is the sum of the weights of all the edges between formula_36 and formula_51. So, in a single phase, a pair of vertices formula_1 and formula_2 , and a min formula_54 cut formula_55 is determined. After one phase of the MinimumCutPhase, the two vertices are merged as a new vertex, and edges from the two vertices to a remaining vertex are replaced by an edge weighted by the sum of the weights of the previous two edges. Edges joining the merged nodes are removed. If there is a minimum cut of formula_11 separating formula_1 and formula_2, the formula_55 is a minimum cut of formula_11. If not, then the minimum cut of formula_11 must have formula_1 and formula_2 on a same side. Therefore, the algorithm would merge them as one node. In addition, the MinimumCut would record and update the global minimum cut after each MinimumCutPhase. After formula_64 phases, the minimum cut can be determined.\n\nThe graph in step 1 shows the original graph formula_11 and randomly selects node 2 as the starting node for this algorithm. In the MinimumCutPhase, set formula_36 only has node 2, the heaviest edge is edge (2,3), so node 3 is added into set formula_36. Next, set formula_36 contains node 2 and node 3, the heaviest edge is (3,4), thus node 4 is added to set formula_36. By following this procedure, the last two nodes are node 5 and node 1, which are formula_1 and formula_2 in this phase. By merging them, the new graph is as shown in step 2. In this phase, the weight of cut is 5, which is the summation of edges (1,2) and (1,5). Right now, the first loop of MinimumCut is completed.\n\nIn step 2, starting from node 2, the heaviest edge is (2,15), thus node 15 is put in set formula_36. The next heaviest edges is (2,3) or (15,6), we choose (15,6) thus node 6 is added to the set. Then we compare edge (2,3) and (6,7) and choose node 3 to put in set formula_36. The last two nodes are node 7 and node 8. Therefore, merge edge (7,8). The minimum cut is 5, so remain the minimum as 5.\n\nThe following steps repeat the same operations on the merged graph, until there is only one edge in the graph, as shown in step 7. The global minimum cut has edge (2,3) and edge (6,7), which is detected in step 5.\n\nTo prove the correctness of this algorithm, we need to prove that MinimumCutPhase is in fact a minimum formula_54 cut of the graph, where s and t are the two vertices last added in the phase. Therefore, a lemma is shown below:Lemma 1: MinimumCutPhase returns a minimum formula_54-cut of formula_11.We prove this by induction on the set of active vertices. Let formula_77 be an arbitrary formula_54 cut, and formula_79 be the cut of the phase. We must show that formula_80. Observe that a single run of MinimumCutPhase gives us a permutation of all the vertices in the graph (where formula_81 is the first and formula_1 and formula_2 are the two vertices added last in the phase). So, we say that the vertex formula_84 is active if formula_85, the vertex before formula_84 in the ordering of vertices produced by MinimumCutPhase is in formula_87 or vice versa, which is to say, they are on opposite sides of the cut. We define formula_88 as the set of vertices added to formula_36 before formula_84 and formula_91 to be the cut of the set formula_92induced by formula_55. For all the active vertex formula_84:formula_95Let formula_96 be the first active vertex. By the definition of these two quantities, the formula_97 and formula_98 are equivalent. formula_99 is simply all vertices added to formula_36 before formula_96, and the edges between these vertices and formula_96 are the edges that cross the cut formula_55. Therefore, as shown above, for the active vertex formula_104 and formula_84 (formula_84 is added to formula_36 before formula_104):formula_109formula_110 by induction, formula_111formula_112 since formula_113 contributes to formula_114 but not to formula_115 (and other edges are of non-negative weights)Thus, since formula_2 is always an active vertex since the last cut of the phase separates formula_1 from formula_2 by definition, for any active vertex formula_2:formula_120Therefore, the cut of the phase is at most as heavy as formula_55.\n\nThe running time of the algorithm MinimumCut is equal to the added running time of the formula_122 runs of MinimumCutPhase, which is called on graphs with decreasing number of vertices and edges.\n\nFor the MinimumCutPhase, a single run of it needs at most formula_123 time.\n\nTherefore, the overall running time should be the product of two phase complexity, which is formula_124[2].\n\nFor the further improvement, the key is to make it easy to select the next vertex to be added to the set formula_36, the most tightly connected vertex. During execution of a phase, all vertices that are not in formula_36 reside in a priority queue based on a key field. The key of a vertex formula_43 is the sum of the weights of the edges connecting it to the current formula_36, that is, formula_129. Whenever a vertex formula_84 is added to formula_36 we have to perform an update of the queue. formula_84 has to be deleted from the queue, and the key of every vertex formula_133 not in formula_36, connected to formula_84 has to be increased by the weight of the edge formula_136, if it exists. As this is done exactly once for every edge, overall we have to perform formula_137 ExtractMax and formula_138 IncreaseKey operations. By using the Fibonacci heap we can perform an ExtractMax operation in formula_139 amortized time and an IncreaseKey operation in formula_140 amortized time. Thus, the time we need for this key step that dominates the rest of the phase, is formula_123.\n\n// Adjacency matrix implementation of Stoer–Wagner min cut algorithm.\n// Running time:\n// O(|V|^3)\n// INPUT: \n// - graph, constructed using AddEdge()\n// OUTPUT:\n// - (min cut value, nodes in half of min cut)\n\n\nusing namespace std;\n\ntypedef vector<int> VI;\ntypedef vector<VI> VVI;\n\nconst int INF = 1000000000;\n\npair<int, VI> GetMinCut(VVI &weights) \n\nconst int maxn = 550; \nconst int inf = 1000000000; \nint n, r; \nint edge[maxn][maxn], dist[maxn]; \nbool vis[maxn], bin[maxn]; \nvoid init() \nint contract( int &s, int &t ) // Find s,t \nint Stoer_Wagner() \n"}
{"id": "7423338", "url": "https://en.wikipedia.org/wiki?curid=7423338", "title": "Stolarsky mean", "text": "Stolarsky mean\n\nIn mathematics, the Stolarsky mean of two positive real numbers \"x\", \"y\" is defined as: \n\nIt is derived from the mean value theorem, which states that a secant line, cutting the graph of a differentiable function formula_2 at formula_3 and formula_4, has the same slope as a line tangent to the graph at some point formula_5 in the interval formula_6.\n\nThe Stolarsky mean is obtained by\nwhen choosing formula_9.\n\n\nOne can generalize the mean to \"n\" + 1 variables by considering the mean value theorem for divided differences for the \"n\"th derivative.\nOne obtains\n\n"}
{"id": "23402810", "url": "https://en.wikipedia.org/wiki?curid=23402810", "title": "Studies in Applied Mathematics", "text": "Studies in Applied Mathematics\n\nThe journal Studies in Applied Mathematics is published by Wiley–Blackwell on behalf of the Massachusetts Institute of Technology.\n\nIt features scholarly articles on mathematical applications in allied fields, notably computer science, mechanics, astrophysics, geophysics, and high-energy physics. \nIts pedigree came from the \"MIT Journal of Mathematics and Physics\" which was founded by the MIT Mathematics Department in 1920. The Journal changed to its present name in 1969.\n\nThe journal was edited from 1969 by David Benney of the Department of Mathematics, Massachusetts Institute of Technology.\n\nAccording to ISI Journal Citation Reports, in 2009 it ranked 34th among the 202 journals in the Applied Mathematics category.\n\n"}
{"id": "43063523", "url": "https://en.wikipedia.org/wiki?curid=43063523", "title": "Subhamiltonian graph", "text": "Subhamiltonian graph\n\nIn graph theory and graph drawing, a subhamiltonian graph is a subgraph of a planar Hamiltonian graph.\n\nA graph \"G\" is subhamiltonian if \"G\" is a subgraph of another graph aug(\"G\") on the same vertex set, such that aug(\"G\") is planar and contains a Hamiltonian cycle. For this to be true, \"G\" itself must be planar, and additionally it must be possible to add edges to \"G\", preserving planarity, \nin order to create a cycle in the augmented graph that passes through each vertex exactly once. The graph aug(\"G\") is called a Hamiltonian augmentation of \"G\".\n\nIt would be equivalent to define \"G\" to be subhamiltonian if \"G\" is a subgraph of a Hamiltonian planar graph, without requiring this larger graph to have the same vertex set. That is, for this alternative definition, it should be possible to add both vertices and edges to \"G\" to create a Hamiltonian cycle. However, if a graph can be made Hamiltonian by the addition of vertices and edges it can also be made Hamiltonian by the addition of edges alone, so this extra freedom does not change the definition.\nIn a subhamiltonian graph, a subhamiltonian cycle is a cyclic sequence of vertices such that adding an edge between each consecutive pair of vertices in the sequence preserves the planarity of the graph. A graph is subhamiltonian if and only if it has a subhamiltonian cycle.\n\nThe class of subhamiltonian graphs (but not this name for them) was introduced by , who proved that these are exactly the graphs with two-page book embeddings. Subhamiltonian graphs and Hamiltonian augmentations have also been applied in graph drawing to problems involving embedding graphs onto universal point sets, simultaneous embedding of multiple graphs, and layered graph drawing.\n\nSome classes of planar graphs are necessarily Hamiltonian, and therefore also subhamiltonian; these include the 4-connected planar graphs and the Halin graphs.\n\nEvery planar graph with maximum degree at most four is subhamiltonian, as is every planar graph with no separating triangles.\nIf the edges of an arbitrary planar graph are subdivided into paths of length two, the resulting subdivided graph is always subhamiltonian.\n"}
{"id": "42625595", "url": "https://en.wikipedia.org/wiki?curid=42625595", "title": "Summability kernel", "text": "Summability kernel\n\nIn mathematics, a summability kernel is a family or sequence of periodic integrable functions satisfying a certain set of properties, listed below. Certain kernels, such as the Fejér kernel, are particularly useful in Fourier analysis. Summability kernels are related to approximation of the identity; definitions of an approximation of identity vary, but sometimes the definition of an approximation of the identity is taken to be the same as for a summability kernel.\n\nLet formula_1. A summability kernel is a sequence formula_2 in formula_3 that satisfies\n\nNote that if formula_9 for all formula_10, i.e. formula_2 is a positive summability kernel, then the second requirement follows automatically from the first.\n\nIf instead we take the convention formula_12, the first equation becomes formula_13, and the upper limit of integration on the third equation should be extended to formula_14.\n\nWe can also consider formula_15 rather than formula_16; then we integrate (1) and (2) over formula_15, and (3) over formula_18.\n\n\nLet formula_2 be a summability kernel, and formula_20 denote the convolution operation.\n"}
{"id": "6500531", "url": "https://en.wikipedia.org/wiki?curid=6500531", "title": "Surrogate model", "text": "Surrogate model\n\nA surrogate model is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of design variables. For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ..). For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete. As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis and \"what-if\" analysis become impossible since they require thousands or even millions of simulation evaluations.\n\nOne way of alleviating this burden is by constructing approximation models, known as surrogate models, response surface models, \"metamodels\" or \"emulators\", that mimic the behavior of the simulation model as closely as possible while being computationally cheap(er) to evaluate. Surrogate models are constructed using a data-driven, bottom-up approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important. A model is constructed based on modeling the response of the simulator to a limited number of intelligently chosen data points. This approach is also known as behavioral modeling or black-box modeling, though the terminology is not always consistent. When only a single design variable is involved, the process is known as curve fitting.\n\nThough using surrogate models in lieu of experiments and simulations in engineering design is more common, surrogate modelling may be used in many other areas of science where there are expensive experiments and/or function evaluations.\n\nThe scientific challenge of surrogate modeling is the generation of a surrogate that is as accurate as possible, using as few simulation evaluations as possible. The process comprises three major steps which may be interleaved iteratively:\n\n\nThe accuracy of the surrogate depends on the number and location of samples (expensive experiments or simulations) in the design space. Various design of experiments (DOE) techniques cater to different sources of errors, in particular, errors due to noise in the data or errors due to an improper surrogate model.\n\nThe most popular surrogate models are polynomial response surfaces, kriging, Gradient-Enhanced Kriging (GEK), radial basis function, support vector machines, space mapping, and artificial neural networks. For some problems, the nature of true function is not known a priori so it is not clear which surrogate model will be most accurate. In addition, there is no consensus on how to obtain the most reliable estimates of the accuracy of a given surrogate.\nMany other problems have known physics properties. In these cases, physics-based surrogates such as space-mapping based models are the most efficient.\n\nA recent survey of surrogate-assisted evolutionary optimization techniques can be found in.\n\nSpanning two decades of development and engineering applications, Rayas-Sanchez reviews aggressive space mapping exploiting surrogate models. Recently, Razavi et al. has published a state-of-the-art review of surrogate models used in water resources management field. \n\nRecently proposed comparison-based surrogate models (e.g. ranking support vector machine) for evolutionary algorithms, such as CMA-ES, allow to preserve some invariance properties of surrogate-assisted optimizers:\n\n\nAn important distinction can be made between two different applications of surrogate models: design optimization and design space approximation (also known as emulation).\n\nIn surrogate model based optimization an initial surrogate is constructed using some of the available budget of expensive experiments and/or simulations. The remaining experiments/simulations are run for designs which the surrogate model predicts may have promising performance. The process usually takes the form of the following search/update procedure.\n\n\nDepending on the type of surrogate used and the complexity of the problem, the process may converge on a local or global optimum, or perhaps none at all.\n\nIn design space approximation, one is not interested in finding the optimal parameter vector but rather in the global behavior of the system. Here the surrogate is tuned to mimic the underlying model as closely as needed over the complete design space. Such surrogates are a useful, cheap way to gain insight into the global behavior of the system. Optimization can still occur as a post processing step, although with no update procedure (see above) the optimum found cannot be validated.\n\n\n\n"}
{"id": "25162434", "url": "https://en.wikipedia.org/wiki?curid=25162434", "title": "Tertiary ideal", "text": "Tertiary ideal\n\nIn mathematics, a tertiary ideal is an (two-sided) ideal in a (perhaps noncommutative) ring that cannot be expressed as a nontrivial intersection of a right fractional ideal with another ideal. Tertiary ideals generalize primary ideals to the case of noncommutative rings. Although primary decompositions do not exist in general for ideals in noncommutative rings, tertiary decompositions do, at least if the ring is Noetherian.\n\nEvery primary ideal is tertiary. Tertiary ideals and primary ideals coincide for commutative rings. To any (two-sided) ideal, a tertiary ideal can be associated called the tertiary radical, defined as\n\nThen \"t\"(\"I\") always contains \"I\".\n\nIf \"R\" is a (not necessarily commutative) Noetherian ring and \"I\" a right ideal in \"R\", then \"I\" has a unique irredundant decomposition into tertiary ideals\n\n\n"}
{"id": "2647057", "url": "https://en.wikipedia.org/wiki?curid=2647057", "title": "Trinomial", "text": "Trinomial\n\nIn elementary algebra, a trinomial is a polynomial consisting of three terms or monomials.\n\n\nA trinomial equation is a polynomial equation involving three terms. An example is the equation formula_15 studied by Johann Heinrich Lambert in the 18th century.\n\n"}
{"id": "25815583", "url": "https://en.wikipedia.org/wiki?curid=25815583", "title": "Worley noise", "text": "Worley noise\n\nWorley noise is a noise function introduced by Steven Worley in 1996. In computer graphics it is used to create procedural textures, that is textures that are created automatically in arbitrary precision and don't have to be drawn by hand. Worley noise comes close to simulating textures of stone, water, or cell noise.\n\nThe basic idea is to take random points in space (2- or 3-dimensional) and then for every point in space take the distance to the \"n\"th-closest point (e.g. the second-closest point) as some kind of color information.\nMore precisely:\n\nTypical implementations, in three dimensions, divide the space into cubes. A fixed number of positions are generated for each cube. In the case of three dimensions, nine cubes' points need to be generated, to be sure to find the closest.\n\n\n\n"}
