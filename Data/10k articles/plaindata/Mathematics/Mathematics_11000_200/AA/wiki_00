{"id": "174108", "url": "https://en.wikipedia.org/wiki?curid=174108", "title": "Abc conjecture", "text": "Abc conjecture\n\nThe \"abc\" conjecture (also known as the Oesterlé–Masser conjecture) is a conjecture in number theory, first proposed by and . It is stated in terms of three positive integers, \"a\", \"b\" and \"c\" (hence the name) that are relatively prime and satisfy \"a\" + \"b\" = \"c\". If \"d\" denotes the product of the distinct prime factors of \"abc\", the conjecture essentially states that \"d\" is usually not much smaller than \"c\". In other words: if \"a\" and \"b\" are composed from large powers of primes, then \"c\" is usually not divisible by large powers of primes. The precise statement is given below.\n\nThe \"abc\" conjecture originated as the outcome of attempts by Oesterlé and Masser to understand the Szpiro conjecture about elliptic curves. The latter conjecture has more geometric structures involved in its statement in comparison with the \"abc\" conjecture.\n\nThe \"abc\" conjecture and its versions express, in concentrated form, some fundamental feature of various problems in Diophantine geometry. A number of famous conjectures and theorems in number theory would follow immediately from the \"abc\" conjecture or its versions. described the \"abc\" conjecture as \"the most important unsolved problem in Diophantine analysis\".\n\nVarious proofs of abc have been claimed but so far none is accepted by the mathematical community. \n\nBefore we state the conjecture we introduce the notion of the radical of an integer: for a positive integer \"n\", the radical of \"n\", denoted rad(\"n\"), is the product of the distinct prime factors of \"n\". For example\n\nIf \"a\", \"b\", and \"c\" are coprime positive integers such that \"a\" + \"b\" = \"c\", it turns out that \"usually\" \"c\" < rad(\"abc\"). The \"abc conjecture\" deals with the exceptions. Specifically, it states that:\n\nAn equivalent formulation states that:\n\nA third equivalent formulation of the conjecture involves the \"quality\" \"q\"(\"a\", \"b\", \"c\") of the triple (\"a\", \"b\", \"c\"), defined as\n\nFor example,\n\nA typical triple (\"a\", \"b\", \"c\") of coprime positive integers with \"a\" + \"b\" = \"c\" will have \"c\" < rad(\"abc\"), i.e. \"q\"(\"a\", \"b\", \"c\") < 1. Triples with \"q\" > 1 such as in the second example are rather special, they consist of numbers divisible by high powers of small prime numbers.\n\nWhereas it is known that there are infinitely many triples (\"a\", \"b\", \"c\") of coprime positive integers with \"a\" + \"b\" = \"c\" such that \"q\"(\"a\", \"b\", \"c\") > 1, the conjecture predicts that only finitely many of those have \"q\" > 1.01 or \"q\" > 1.001 or even \"q\" > 1.0001, etc. In particular, if the conjecture is true then there must exist a triple (\"a\", \"b\", \"c\") which achieves the maximal possible quality \"q\"(\"a\", \"b\", \"c\") .\n\nThe condition that \"ε\" > 0 is necessary as there exist infinitely many triples \"a\", \"b\", \"c\" with rad(\"abc\") < \"c\". For example let:\n\nFirst we note that \"b\" is divisible by 9:\n\nUsing this fact we calculate:\n\nBy replacing the exponent 6\"n\" by other exponents forcing \"b\" to have larger square factors, the ratio between the radical and \"c\" can be made arbitrarily small. Specifically, let \"p\" > 2 be a prime and consider:\n\nNow we claim that \"b\" is divisible by \"p\":\n\nThe last step uses the fact that \"p\" divides 2-1. This follows from Fermat's little theorem, which shows that, for \"p\">2, 2=\"pk\"+1 for some integer \"k\". Raising both sides to the power of \"p\" then shows that 2=\"p\"(...)+1.\n\nAnd now with a similar calculation as above we have:\n\nA list of the highest-quality triples (triples with a particularly small radical relative to \"c\") is given below; the highest quality, 1.6299, was found by Eric Reyssat for\n\nThe \"abc\" conjecture has a large number of consequences. These include both known results (some of which have been proven separately since the conjecture has been stated) and conjectures for which it gives a conditional proof. While an earlier proof of the conjecture would have been more significant in terms of consequences, the \"abc\" conjecture itself remains of interest for the other conjectures it would prove, together with its numerous links with deep questions in number theory.\n\nThe abc conjecture implies that \"c\" can be bounded above by a near-linear function of the radical of \"abc\". However, exponential bounds are known. Specifically, the following bounds have been proven:\n\nIn these bounds, \"K\" is a constant that does not depend on \"a\", \"b\", or \"c\", and \"K\" and \"K\" are constants that depend on \"ε\" (in an effectively computable way) but not on \"a\", \"b\", or \"c\". The bounds apply to any triple for which \"c\" > 2.\n\nIn 2006, the Mathematics Department of Leiden University in the Netherlands, together with the Dutch Kennislink science institute, launched the ABC@Home project, a grid computing system, which aims to discover additional triples \"a\", \"b\", \"c\" with rad(\"abc\") < \"c\". Although no finite set of examples or counterexamples can resolve the \"abc\" conjecture, it is hoped that patterns in the triples discovered by this project will lead to insights about the conjecture and about number theory more generally.\n\nAs of May 2014, ABC@Home had found 23.8 million triples.\n\nNote: the \"quality\" \"q\"(\"a\", \"b\", \"c\") of the triple (\"a\", \"b\", \"c\") is defined above.\n\nThe \"abc\" conjecture is an integer analogue of the Mason–Stothers theorem for polynomials.\n\nA strengthening, proposed by , states that in the \"abc\" conjecture one can replace rad(\"abc\") by\n\nwhere \"ω\" is the total number of distinct primes dividing \"a\", \"b\" and \"c\".\n\nAndrew Granville noticed that the minimum of the function formula_16 over formula_17 occurs when formula_18\n\nThis incited to propose a sharper form of the \"abc\" conjecture, namely:\nwith \"κ\" an absolute constant. After some computational experiments he found that a value of formula_20 was admissible for \"κ\".\n\nThis version is called \"explicit \"abc\" conjecture\".\n\nwhere Ω(\"n\") is the total number of prime factors of \"n\" and\n\nwhere Θ(\"n\") is the number of integers up to \"n\" divisible only by primes dividing \"n\".\n\nLet \"k\" = rad(\"abc\"). They conjectured there is a constant \"C\" such that\n\nholds whereas there is a constant \"C\" such that\n\nholds infinitely often.\n\nLucien Szpiro proposed a solution in 2007, but it was found to be incorrect shortly afterwards.\n\nIn August 2012 Shinichi Mochizuki claimed a proof of the abc conjecture.\nHe released a series of four preprints developing a new theory called Inter-universal Teichmüller theory (IUTT) which is then applied to prove several famous conjectures in number theory, including the abc conjecture but also Szpiro's conjecture, the hyperbolic Vojta's conjecture.\nThe papers have not been accepted by the mathematical community as providing a proof of abc. This is not only because of their impenetrability,\nbut also because at least one specific point in the argument has been identified as a gap by some other experts.\nThough a small circle of mathematicians have vouched for the correctness of the proof,\nand have attempted to communicate their understanding via workshops on inter-universal Teichmüller theory, this has failed to convince the number theory community at large.\nIn March 2018, Peter Scholze and Jakob Stix visited Kyoto for discussions with Mochizuki;\n\"see the History section of the article on IUTT for details\".\nWhile they did not resolve the differences, they brought them into clearer focus.\nScholze and Stix concluded that the gap was \"so severe that … small modifications will not rescue the proof strategy\";\nMochizuki claimed that they misunderstood vital aspects of the theory and made invalid simplifications.\n\n\n"}
{"id": "57378495", "url": "https://en.wikipedia.org/wiki?curid=57378495", "title": "Ana Cannas da Silva", "text": "Ana Cannas da Silva\n\nAna M. L. G. Cannas da Silva (born 1968) is a Portuguese mathematician specializing in symplectic geometry and geometric topology. She works in Switzerland as a senior scientist in mathematics at ETH Zurich.\n\nCannas was born in Lisbon. After studying at St. John de Britto College, she earned a licenciatura in mathematics in 1990 from the Instituto Superior Técnico in the University of Lisbon. She then went to the Massachusetts Institute of Technology for graduate studies, earning a master's degree in 1994 and completing her Ph.D. in 1996. Her dissertation, \"Multiplicity Formulas for Orbifolds\", was supervised by Victor Guillemin.\n\nAfter a temporary position as Morrey Assistant Professor at the University of California, Berkeley, Cannas returned to the Instituto Superior Técnico as a faculty member in 1997. She took a second position as a senior lecturer and research scholar in mathematics at Princeton University in 2006, keeping at the same time her position at the Instituto Superior Técnico. In 2011 she moved from Princeton and the Instituto Superior Técnico to ETH Zurich.\n\nIn 2009, the alumni of St. John de Britto College awarded Cannas their José Carlos Belchior Prize in honor of her achievements as an alumna of the school.\n\nCannas is the author or coauthor of:\n"}
{"id": "19291343", "url": "https://en.wikipedia.org/wiki?curid=19291343", "title": "Askey–Gasper inequality", "text": "Askey–Gasper inequality\n\nIn mathematics, the Askey–Gasper inequality is an inequality for Jacobi polynomials proved by and used in the proof of the Bieberbach conjecture.\n\nIt states that if , and then\n\nwhere\n\nis a Jacobi polynomial.\n\nThe case when can also be written as\n\nIn this form, with a non-negative integer, the inequality was used by Louis de Branges in his proof of the Bieberbach conjecture.\n\n gave a short proof of this inequality, by combining the identity\n\nwith the Clausen inequality.\n\n give some generalizations of the Askey–Gasper inequality to basic hypergeometric series.\n\n\n"}
{"id": "36886954", "url": "https://en.wikipedia.org/wiki?curid=36886954", "title": "Bipartite matroid", "text": "Bipartite matroid\n\nIn mathematics, a bipartite matroid is a matroid all of whose circuits have even size.\n\nA uniform matroid formula_1 is bipartite if and only if formula_2 is an odd number, because the circuits in such a matroid have size formula_3.\n\nEulerian matroids were defined by as a generalization of the bipartite graphs, graphs in which every cycle has even size. A graphic matroid is bipartite if and only if it comes from a bipartite graph.\n\nAn Eulerian graph is one in which all vertices have even degree; Eulerian graphs may be disconnected. For planar graphs, the properties of being bipartite and Eulerian are dual: a planar graph is bipartite if and only if its dual graph is Eulerian. As Welsh showed, this duality extends to binary matroids: a binary matroid is bipartite if and only if its dual matroid is an Eulerian matroid, a matroid that can be partitioned into disjoint circuits.\n\nFor matroids that are not binary, the duality between Eulerian and bipartite matroids may break down. For instance, the uniform matroid formula_4 is non-bipartite but its dual formula_5 is Eulerian, as it can be partitioned into two 3-cycles. The self-dual uniform matroid formula_6 is bipartite but not Eulerian.\n\nIt is possible to test in polynomial time whether a given binary matroid is bipartite. However, any algorithm that tests whether a given matroid is Eulerian, given access to the matroid via an independence oracle, must perform an exponential number of oracle queries, and therefore cannot take polynomial time.\n"}
{"id": "4072976", "url": "https://en.wikipedia.org/wiki?curid=4072976", "title": "Collineation", "text": "Collineation\n\nIn projective geometry, a collineation is a one-to-one and onto map (a bijection) from one projective space to another, or from a projective space to itself, such that the images of collinear points are themselves collinear. A collineation is thus an \"isomorphism\" between projective spaces, or an automorphism from a projective space to itself. Some authors restrict the definition of collineation to the case where it is an automorphism. The set of all collineations of a space to itself form a group, called the collineation group.\n\nSimply, a collineation is a one-to-one map from one projective space to another, or from a projective space to itself, such that the images of collinear points are themselves collinear. One may formalize this using various ways of presenting a projective space. Also, the case of the projective line is special, and hence generally treated differently.\n\nFor a projective space defined in terms of linear algebra (as the projectivization of a vector space), a collineation is a map between the projective spaces that is order-preserving with respect to inclusion of subspaces.\n\nFormally, let \"V\" be a vector space over a field \"K\" and \"W\" a vector space over a field \"L\". Consider the projective spaces \"PG\"(\"V\") and \"PG\"(\"W\"), consisting of the vector lines of \"V\" and \"W\". \nCall \"D\"(\"V\") and \"D\"(\"W\") the set of subspaces of \"V\" and \"W\" respectively. A collineation from \"PG\"(\"V\") to \"PG\"(\"W\") is a map α : \"D\"(\"V\") → \"D\"(\"W\"), such that:\n\nGiven a projective space defined axiomatically in terms of an incidence structure (a set of points \"P,\" lines \"L,\" and an incidence relation \"I\" specifying which points lie on which lines, satisfying certain axioms), a collineation between projective spaces thus defined then being a bijective function \"f\" between the sets of points and a bijective function \"g\" between the set of lines, preserving the incidence relation.\n\nEvery projective space of dimension greater than or equal to three is isomorphic to the projectivization of a linear space over a division ring, so in these dimensions this definition is no more general than the linear-algebraic one above, but in dimension two there are other projective planes, namely the non-Desarguesian planes, and this definition permits one to define collineations in such projective planes.\n\nFor dimension one, the set of points lying on a single projective line defines a projective space, and the resulting notion of collineation is just any bijection of the set.\n\nFor a projective space of dimension one (a projective line; the projectivization of a vector space of dimension two), all points are collinear, so the collineation group is exactly the symmetric group of the points of the projective line. This is different from the behavior in higher dimensions, and thus one gives a more restrictive definition, specified so that the fundamental theorem of projective geometry holds.\n\nIn this definition, when \"V\" has dimension two, a collineation from \"PG\"(\"V\") to \"PG\"(\"W\") is a map , such that:\nThis last requirement ensures that collineations are all semilinear maps.\n\nThe main examples of collineations are projective linear transformations (also known as homographies) and automorphic collineations. For projective spaces coming from a linear space, the fundamental theorem of projective geometry states that all collineations are a combination of these, as described below.\n\nProjective linear transformations (homographies) are collineations (planes in a vector space correspond to lines in the associated projective space, and linear transformations map planes to planes, so projective linear transformations map lines to lines), but in general not all collineations are projective linear transformations. PGL is in general a proper subgroup of the collineation group.\n\nAn is a map that, in coordinates, is a field automorphism applied to the coordinates.\n\nIf the geometric dimension of a pappian projective space is at least 2, then every collineation is the product of a homography (a projective linear transformation) and an automorphic collineation. More precisely, the collineation group is the projective semilinear group, which is the semidirect product of homographies by automorphic collineations.\n\nIn particular, the collineations of are exactly the homographies, as R has no nontrivial automorphisms (that is, Gal(R/Q) is trivial).\n\nSuppose \"φ\" is a nonsingular semilinear map from \"V\" to \"W\", with the dimension of \"V\" at least three. Define by saying that for all \"Z\" in \"D\"(\"V\"). As \"φ\" is semilinear, one easily checks that this map is properly defined, and further more, as \"φ\" is not singular, it is bijective. It is obvious now that \"α\" is a collineation. We say that \"α\" is induced by \"φ\".\n\nThe fundamental theorem of projective geometry states the converse:\n\nSuppose \"V\" is a vector space over a field \"K\" with dimension at least three, \"W\" is a vector space over a field \"L\", and \"α\" is a collineation from PG(\"V\") to PG(\"W\"). This implies \"K\" and \"L\" are isomorphic fields, \"V\" and \"W\" have the same dimension, and there is a semilinear map \"φ\" such that \"φ\" induces \"α\".\n\nFor , the collineation group is the projective semilinear group, PΓL – this is PGL, twisted by field automorphisms; formally, the semidirect product , where \"k\" is the prime field for \"K\".\n\nThus for \"K\" a prime field (formula_2 or formula_3), we have , but for \"K\" not a prime field (such as formula_4 for or formula_5), the projective linear group is in general a proper subgroup of the collineation group, which can be thought of as \"transformations preserving a projective \"semi\"-linear structure\". Correspondingly, the quotient group corresponds to \"choices of linear structure\", with the identity (base point) being the existing linear structure. Given a projective space without an identification as the projectivization of a linear space, there is no natural isomorphism between the collineation group and PΓL, and the choice of a linear structure (realization as projectivization of a linear space) corresponds to a choice of subgroup , these choices forming a torsor over Gal(\"K\"/\"k\").\n\nThe idea of a line was abstracted to a ternary relation determined by collinearity (points lying on a single line). According to Wilhelm Blaschke it was August Möbius that first abstracted this essence of geometrical transformation:\nContemporary mathematicians view geometry as an incidence structure with an automorphism group consisting of mappings of the underlying space that preserve incidence. Such a mapping permutes the lines of the incidence structure, and the notion of collineation persists.\n\nAs mentioned by Blaschke and Klein, Michel Chasles preferred the term \"homography\" to \"collineation\". A distinction between the terms arose when the distinction was clarified between the real projective plane and the complex projective line. Since there are no non-trivial field automorphisms of the real number field, all the collineations are homographies in the real projective plane., however due to the field automorphism complex conjugation, not all collineations of the complex projective line are homographies. In applications such as computer vision where the underlying field is the real number field, \"homography\" and \"collineation\" can be used interchangeably.\n\nThe operation of taking the complex conjugate in the complex plane amounts to a reflection in the real line. With the notation \"z\" for the conjugate of \"z\", an anti-homography is given by\nThus an anti-homography is the composition of conjugation with an homography, and so is an example of a collineation which is not an homography. For example, geometrically, the mapping formula_7 amounts to circle inversion. The transformations of inversive geometry of the plane are frequently described as the collection of all homographies and anti-homographies of the complex plane.\n\n"}
{"id": "741875", "url": "https://en.wikipedia.org/wiki?curid=741875", "title": "Combinatorial proof", "text": "Combinatorial proof\n\nIn mathematics, the term combinatorial proof is often used to mean either of two types of mathematical proof:\n\nThe term \"combinatorial proof\" may also be used more broadly to refer to any kind of elementary proof in combinatorics. However, as writes in his review of (a book about combinatorial proofs), these two simple techniques are enough to prove many theorems in combinatorics and number theory.\n\nAn archetypal double counting proof is for the well known formula for the number formula_1 of \"k\"-combinations (i.e., subsets of size \"k\") of an \"n\"-element set:\nHere a direct bijective proof is not possible: because the right-hand side of the identity is a fraction, there is no set \"obviously\" counted by it (it even takes some thought to see that the denominator always evenly divides the numerator). However its numerator counts the Cartesian product of \"k\" finite sets of sizes \"n\", , ..., , while its denominator counts the permutations of a \"k\"-element set (the set most obviously counted by the denominator would be another Cartesian product \"k\" finite sets; if desired one could map permutations to that set by an explicit bijection). Now take \"S\" to be the set of sequences of \"k\" elements selected from our \"n\"-element set without repetition. On one hand, there is an easy bijection of \"S\" with the Cartesian product corresponding to the numerator formula_3, and on the other hand there is a bijection from the set \"C\" of pairs of a \"k\"-combination and a permutation \"σ\" of \"k\" to \"S\", by taking the elements of \"C\" in increasing order, and then permuting this sequence by \"σ\" to obtain an element of \"S\". The two ways of counting give the equation\nand after division by \"k\"! this leads to the stated formula for formula_1. In general, if the counting formula involves a division, a similar double counting argument (if it exists) gives the most straightforward combinatorial proof of the identity, but double counting arguments are not limited to situations where the formula is of this form.\n\n gives an example of a combinatorial enumeration problem (counting the number of sequences of \"k\" subsets \"S\", \"S\", ... \"S\", that can be formed from a set of \"n\" items such that the subsets have an empty common intersection) with two different proofs for its solution. The first proof, which is not combinatorial, uses mathematical induction and generating functions to find that the number of sequences of this type is (2 −1). The second proof is based on the observation that there are 2 −1 proper subsets of the set {1, 2, ..., \"k\"}, and (2 −1) functions from the set {1, 2, ..., \"n\"} to the family of proper subsets of {1, 2, ..., \"k\"}. The sequences to be counted can be placed in one-to-one correspondence with these functions, where the function formed from a given sequence of subsets maps each element \"i\" to the set {\"j\" | \"i\" ∈ \"S\"}.\n\nStanley writes, “Not only is the above combinatorial proof much shorter than our previous proof, but also it makes the reason for the simple answer completely transparent. It is often the case, as occurred here, that the first proof to come to mind turns out to be laborious and inelegant, but that the final answer suggests a simple combinatorial proof.” Due both to their frequent greater elegance than non-combinatorial proofs and the greater insight they provide into the structures they describe, Stanley formulates a general principle that combinatorial proofs are to be preferred over other proofs, and lists as exercises many problems of finding combinatorial proofs for mathematical facts known to be true through other means.\n\nStanley does not clearly distinguish between bijective and double counting proofs, and gives examples of both kinds, but the difference between the two types of combinatorial proof can be seen in an example provided by , of proofs for Cayley's formula stating that there are \"n\" different trees that can be formed from a given set of \"n\" nodes. Aigner and Ziegler list four proofs of this theorem, the first of which is bijective and the last of which is a double counting argument. They also mention but do not describe the details of a fifth bijective proof.\n\nThe most natural way to find a bijective proof of this formula would be to find a bijection between \"n\"-node trees and some collection of objects that has \"n\" members, such as the sequences of \"n\" − 2 values each in the range from 1 to \"n\". Such a bijection can be obtained using the Prüfer sequence of each tree. Any tree can be uniquely encoded into a Prüfer sequence, and any Prüfer sequence can be uniquely decoded into a tree; these two results together provide a bijective proof of Cayley's formula.\n\nAn alternative bijective proof, given by Aigner and Ziegler and credited by them to André Joyal, involves a bijection between, on the one hand, \"n\"-node trees with two designated nodes (that may be the same as each other), and on the other hand, \"n\"-node directed pseudoforests. If there are \"T\" \"n\"-node trees, then there are \"n\"\"T\" trees with two designated nodes. And a pseudoforest may be determined by specifying, for each of its nodes, the endpoint of the edge extending outwards from that node; there are \"n\" possible choices for the endpoint of a single edge (allowing self-loops) and therefore \"n\" possible pseudoforests. By finding a bijection between trees with two labeled nodes and pseudoforests, Joyal's proof shows that \"T\" = \"n\".\n\nFinally, the fourth proof of Cayley's formula presented by Aigner and Ziegler is a double counting proof due to Jim Pitman. In this proof, Pitman considers the sequences of directed edges that may be added to an \"n\"-node empty graph to form from it a single rooted tree, and counts the number of such sequences in two different ways. By showing how to derive a sequence of this type by choosing a tree, a root for the tree, and an ordering for the edges in the tree, he shows that there are \"Tn\"! possible sequences of this type. And by counting the number of ways in which a partial sequence can be extended by a single edge, he shows that there are \"n\"\"n\"! possible sequences. Equating these two different formulas for the size of the same set of edge sequences and cancelling the common factor of \"n\"! leads to Cayley's formula.\n\n"}
{"id": "3941020", "url": "https://en.wikipedia.org/wiki?curid=3941020", "title": "Converse relation", "text": "Converse relation\n\nIn mathematics, the converse relation, or transpose, of a binary relation is the relation that occurs when the order of the elements is switched in the relation. For example, the converse of the relation 'child of' is the relation 'parent of'. In formal terms, if and are sets and is a relation from to , then is the relation defined so that if and only if . In set-builder notation, }.\n\nThe notation is analogous with that for an inverse function. Although many functions do not have an inverse, every relation does have a unique converse. The unary operation that maps a relation to the converse relation is an involution, so it induces the structure of a semigroup with involution on the binary relations on a set, or, more generally, induces a dagger category on the category of relations as detailed below. As a unary operation, taking the converse (sometimes called conversion or transposition) commutes with the order-related operations of the calculus of relations, that is it commutes with union, intersection, and complement.\n\nThe converse relation is also called the or transpose relation— the latter in view of its similarity with the transpose of a matrix. It has also been called the opposite or dual of the original relation, or the inverse of the original relation. Other notations for the converse relation include , formula_1, , or .\n\nFor the usual (maybe strict or partial) order relations, the converse is the naively expected \"opposite\" order, for examples, formula_2\n\nA relation may be represented by a logical matrix such as\n\nThen the converse relation is represented by its transpose matrix:\n\nThe converse of kinship relations are named: \"\"A\" is a child of \"B\" has converse \"B\" is a parent of \"A\". \"A\" is a nephew or niece of \"B\" has converse \"B\" is an uncle or aunt of \"A\". The relation \"A\" is a sibling of \"B\"\" is its own converse, since it is a symmetric relation.\n\nIn set theory, one presumes a universe \"U\" of discourse, and a fundamental relation of set membership \"x\" ∈ \"A\" when \"A\" is a subset of \"U\". The power set of all subsets of \"U\" is the domain of the converse formula_5\n\nIn the monoid of binary endorelations on a set (with the binary operation on relations being the composition of relations), the converse relation does not satisfy the definition of an inverse from group theory, i.e. if \"L\" is an arbitrary relation on \"X\", then formula_6 does \"not\" equal the identity relation on \"X\" in general. The converse relation does satisfy the (weaker) axioms of a semigroup with involution: formula_7 and formula_8.\n\nSince one may generally consider relations between different sets (which form a category rather than a monoid, namely the category of relations Rel), in this context the converse relation conforms to the axioms of a dagger category (aka category with involution). A relation equal to its converse is a symmetric relation; in the language of dagger categories, it is self-adjoint.\n\nFurthermore, the semigroup of endorelations on a set is also a partially ordered structure (with inclusion of relations as sets), and actually an involutive quantale. Similarly, the category of heterogeneous relations, Rel is also an ordered category.\n\nIn the calculus of relations, \"conversion\" (the unary operation of taking the converse relation) commutes with other binary operations of union and intersection. Conversion also commutes with unary operation of complementation as well as with taking suprema and infima. Conversion is also compatible with the ordering of relations by inclusion.\n\nIf a relation is reflexive, irreflexive, symmetric, antisymmetric, asymmetric, transitive, total, trichotomous, a partial order, total order, strict weak order, total preorder (weak order), or an equivalence relation, its converse is too.\n\nIf \"I\" represents the identity relation, then a relation \"R\" may have an inverse as follows:\n\nA function is invertible if and only if its converse relation is a function, in which case the converse relation is the inverse function.\n\nThe converse relation of a function formula_11 is the relation formula_12 defined by formula_13.\n\nThis is not necessarily a function: One necessary condition is that \"f\" be injective, since else formula_14 is multi-valued. This condition is sufficient for formula_14 being a partial function, and it is clear that formula_14 then is a (total) function if and only if \"f\" is surjective. In that case, i.e. if \"f\" is bijective, formula_14 may be called the inverse function of \"f\".\n\nFor example, the function formula_18 has the inverse function formula_19.\n\nHowever, the function formula_20 has the inverse relation formula_21, which is not a function, being multi-valued.\n\n"}
{"id": "10330610", "url": "https://en.wikipedia.org/wiki?curid=10330610", "title": "Crystal base", "text": "Crystal base\n\nIn algebra, a crystal base or canonical base is a base of a representation, such that generators of a quantum group or semisimple Lie algebra have a particularly simple action on it. Crystal bases were introduced by and (under the name of canonical bases).\n\nAs a consequence of the defining relations for the quantum group formula_1, formula_1 can be regarded as a Hopf algebra over formula_3, the field of all rational functions of an indeterminate \"q\" over formula_4.\n\nFor simple root formula_5 and non-negative integer formula_6, define \n\nIn an integrable module formula_8, and for weight formula_9, a vector formula_10 (i.e. a vector formula_11 in formula_8 with weight formula_9) can be uniquely decomposed into the sums\n\nwhere formula_15, formula_16, formula_17 only if formula_18, and formula_19 only if formula_20. \n\nLinear mappings formula_21 can be defined on formula_22 by\n\nLet formula_25 be the integral domain of all rational functions in formula_3 which are regular at formula_27 (\"i.e.\" a rational function formula_28 is an element of formula_25 if and only if there exist polynomials formula_30 and formula_31 in the polynomial ring formula_32 such that formula_33, and formula_34). A crystal base for formula_8 is an ordered pair formula_36, such that\n\n\nTo put this into a more informal setting, the actions of formula_54 and formula_55 are generally singular at formula_27 on an integrable module formula_8. The linear mappings formula_58 and formula_59 on the module are introduced so that the actions of formula_60 and formula_61 are regular at formula_27 on the module. There exists a formula_3-basis of weight vectors formula_64 for formula_8, with respect to which the actions of formula_58 and formula_59 are regular at formula_27 for all \"i\". The module is then restricted to the free formula_25-module generated by the basis, and the basis vectors, the formula_25-submodule and the actions of formula_58 and formula_59 are evaluated at formula_27. Furthermore, the basis can be chosen such that at formula_27, for all formula_75, formula_58 and formula_59 are represented by mutual transposes, and map basis vectors to basis vectors or 0.\n\nA crystal base can be represented by a directed graph with labelled edges. Each vertex of the graph represents an element of the formula_78-basis formula_41 of formula_43, and a directed edge, labelled by \"i\", and directed from vertex formula_81 to vertex formula_82, represents that formula_83 (and, equivalently, that formula_84), where formula_85 is the basis element represented by formula_81, and formula_87 is the basis element represented by formula_82. The graph completely determines the actions of formula_58 and formula_59 at formula_27. If an integrable module has a crystal base, then the module is irreducible if and only if the graph representing the crystal base is connected (a graph is called \"connected\" if the set of vertices cannot be partitioned into the union of nontrivial disjoint subsets formula_92 and formula_93 such that there are no edges joining any vertex in formula_92 to any vertex in formula_93).\n\nFor any integrable module with a crystal base, the weight spectrum for the crystal base is the same as the weight spectrum for the module, and therefore the weight spectrum for the crystal base is the same as the weight spectrum for the corresponding module of the appropriate Kac–Moody algebra. The multiplicities of the weights in the crystal base are also the same as their multiplicities in the corresponding module of the appropriate Kac–Moody algebra.\n\nIt is a theorem of Kashiwara that every integrable highest weight module has a crystal base. Similarly, every integrable lowest weight module has a crystal base.\n\nLet formula_8 be an integrable module with crystal base formula_36 and formula_98 be an integrable module with crystal base formula_99. For crystal bases, the coproduct formula_100, given by \n\nis adopted. The integrable module formula_102 has crystal base formula_103, where formula_104. For a basis vector formula_105, define \n\nThe actions of formula_58 and formula_59 on formula_110 are given by\n\nThe decomposition of the product two integrable highest weight modules into irreducible submodules is determined by the decomposition of the graph of the crystal base into its connected components (i.e. the highest weights of the submodules are determined, and the multiplicity of each highest weight is determined).\n\n"}
{"id": "6469973", "url": "https://en.wikipedia.org/wiki?curid=6469973", "title": "De Gua's theorem", "text": "De Gua's theorem\n\nDe Gua's theorem is a three-dimensional analog of the Pythagorean theorem and named after Jean Paul de Gua de Malves.\n\nIf a tetrahedron has a right-angle corner (like the corner of a cube), then the square of the area of the face opposite the right-angle corner is the sum of the squares of the areas of the other three faces.\n\nThe Pythagorean theorem and de Gua's theorem are special cases (\"n\" = 2, 3) of a general theorem about \"n\"-simplices with a right-angle corner. This, in turn, is a special case of a yet more general theorem by Donald R. Conant and William A. Beyer, which can be stated as follows.\n\nLet \"U\" be a measurable subset of a \"k\"-dimensional affine subspace of formula_2 (so formula_3). For any subset formula_4 with exactly \"k\" elements, let formula_5 be the orthogonal projection of \"U\" onto the linear span of formula_6, where formula_7 and formula_8 is the standard basis for formula_2. Then\n\nwhere formula_11 is the \"k\"-dimensional volume of \"U\" and the sum is over all subsets formula_4 with exactly \"k\" elements.\n\nDe Gua's theorem and its generalisation (above) to \"n\"-simplices with right-angle corners correspond to the special case where \"k\" = \"n\"−1 and \"U\" is an (\"n\"−1)-simplex in formula_2 with vertices on the co-ordinate axes. For example, suppose \"n\" = 3, \"k\" = 2 and \"U\" is the triangle formula_14 in formula_15 with vertices \"A\", \"B\" and \"C\" lying on the formula_16-, formula_17- and formula_18-axes, respectively. The subsets formula_19 of formula_20 with exactly 2 elements are formula_21, formula_22 and formula_23. By definition, formula_24 is the orthogonal projection of formula_25 onto the formula_26-plane, so formula_24 is the triangle formula_28 with vertices \"O\", \"B\" and \"C\", where \"O\" is the origin of formula_15. Similarly, formula_30 and formula_31, so the Conant–Beyer theorem says\n\nwhich is de Gua's theorem.\n\nJean Paul de Gua de Malves (1713–85) published the theorem in 1783, but around the same time a slightly more general version was published by another French mathematician, Charles de Tinseau d'Amondans (1746–1818), as well. However the theorem had also been known much earlier to Johann Faulhaber (1580–1635) and René Descartes (1596–1650).\n\n\n"}
{"id": "42878233", "url": "https://en.wikipedia.org/wiki?curid=42878233", "title": "Drafting film", "text": "Drafting film\n\nDrafting film is a sturdier and more dimensionally stable substitute for drafting paper sometimes used for technical drawings, especially architectural drawings, and for art layout drawings, replacing drafting linen for these purposes. Nowadays it is almost invariably made of transparent biaxially oriented polyethylene terephthalate, which should last several centuries under normal storage conditions, with one or two translucent matte surfaces provided by a coating. However, some older drafting films are cellulose acetate, which degrades in only a few decades due to the vinegar syndrome. Uncoated films are preferred for archival, because there is then no possibility that the coating material could deteriorate over time or react with other materials.\n\nUncoated films require the use of an etching ink that penetrates the surface of the film.\n\nBecause non-etching ink doesn't penetrate the surface of the film the way it penetrates paper, it is often possible to remove inked lines from drafting film when drawing on it by hand; an abrasive-free vinyl drafting eraser is the preferred tool for this, although a scalpel works too. For the same reason, graphite on the surface tends to smear; there are polyester pencil drafting leads available as a substitute for graphite for drawing on drafting film. Different matte coatings (generally consisting of amorphous silica particles dispersed in a resin) are better suited for graphite or for polyester leads.\n\nManual drawing on drafting film requires special care with cleanliness, because oil from the illustrator's hands can form skid patches on the film's nonporous surface, where ink and pencil will not adhere to the matte surface, while drafting paper will absorb the oil.\n\nDimensional stability is important for scale drawings that don't have explicit dimensions, such as maps, because measurements may be taken directly from the drawing, so drawings that expand and shrink with temperature and humidity can introduce great imprecision into these measurements; some undimensioned drawing standards require the use of drafting film for such drawings.\n"}
{"id": "5827598", "url": "https://en.wikipedia.org/wiki?curid=5827598", "title": "Evelyn Boyd Granville", "text": "Evelyn Boyd Granville\n\nEvelyn Boyd Granville (born May 1, 1924) was the second African-American woman to receive a Ph.D. in mathematics from an American University; she earned it in 1949 from Yale University (she attended Smith College before Yale). She performed pioneering work in the field of computing.\n\nEvelyn Boyd was born in Washington, D.C.; her father worked odd jobs due to the Great Depression but separated from her mother when Boyd was young. Boyd and her older sister were raised by her mother and aunt, who both worked at the Bureau of Engraving and Printing. She was valedictorian at Dunbar High School, which at that time was a segregated but academically competitive school for black students in Washington.\n\nWith financial support from her aunt and, later, a small partial scholarship from Phi Delta Kappa, Boyd entered Smith College in the fall of 1941. She majored in mathematics and physics, but also took a keen interest in astronomy. She was elected to Phi Beta Kappa and to Sigma Xi and graduated summa cum laude in 1945. Encouraged by a graduate scholarship from the Smith Student Aid Society of Smith College, she applied to graduate programs in mathematics and was accepted by both Yale University and the University of Michigan; she chose Yale because of the financial aid they offered. There she studied functional analysis under the supervision of Einar Hille, finishing her doctorate in 1949. Her dissertation was \"On Laguerre Series in the Complex Domain\".\n\nFollowing graduate school, Boyd went to New York University Institute for Mathematics and performed research and teaching there. After, in 1950, she took a teaching position at Fisk University, a college for black students in Nashville, Tennessee (more prestigious postings being unavailable to black women). Two of her students there, Vivienne Malone-Mayes and Etta Zuber Falconer, went on to earn doctorates in mathematics of their own. But by 1952 she left academia and returned to Washington with a position at the Diamond Ordnance Fuze Laboratories. In January 1956, she moved to IBM as a computer programmer; when IBM received a NASA contract, she moved to Vanguard Computing Center in Washington, D.C. \n\nBoyd moved from Washington to New York City in 1957. In 1960, after marrying Reverend G. Mansfield Collins, Boyd moved to Los Angeles. There she worked for the U.S. Space Technology Laboratories, which became the North American Aviation Space and Information Systems Division in 1962. She worked on various projects for the Apollo program, including celestial mechanics, trajectory computation, and \"digital computer techniques\".\n\nForced to move because of a restructuring at IBM, she took a position at California State University, Los Angeles in 1967 as a full professor of mathematics.\nAfter retiring from CSULA in 1984 she taught at Texas College in Tyler, Texas for four years, and then in 1990 joined the faculty of the University of Texas at Tyler as the Sam A. Lindsey Professor of mathematics. There she developed elementary school math enrichment programs. Since 1967, Granville has remained a strong advocate for women's education in tech.\n\nIn 1951, Granville and two African American colleagues were denied entrance to a regional meeting of the Mathematical Association of America (MAA), because it was held at a whites-only hotel. The MAA and the American Mathematical Society (AMS) subsequently changed their practices, under pressure from Lee Lorch, to improve their inclusivity.\n\nBoyd married Reverend Gamaliel Mansifeld Collins in 1960. In 1967, Boyd and Collins divorced. She married realtor Edward V. Granville in 1970. The two moved to Tyler, Texas in 1984.\n\nIn 1989, she was awarded an honorary doctorate by Smith College, the first one given by an American institution to an African-American woman mathematician.\n\nShe was appointed to the Sam A. Lindsey Chair of the University of Texas at Tyler (1990-1991).\n\nIn 1998, Granville was honoured by the National Academy of Engineering.\n\nIn 1999, the United States National Academy of Sciences inducted her into its Portrait Collection of African-Americans in Science.\n\nIn 2000, she was awarded the Wilbur Lucius Cross Medal, the Yale Graduate School Alumni Association's highest honour.\n\nIn 2001, she was cited in the Virginia state senate's Joint Resolution No. 377, \"Designating February 25 as \"African-American Scientist and Inventor Day.\"\"\n\nIn 2006 she was awarded an honorary degree by Spelman College.\n\nIn 2016, technology firm New Relic's \"Mount Codemore\" initiative named her as one of \"four giants of women’s contributions to science and technology\".\n\n\n\n"}
{"id": "365026", "url": "https://en.wikipedia.org/wiki?curid=365026", "title": "Finite intersection property", "text": "Finite intersection property\n\nIn general topology, a branch of mathematics, a collection \"A\" of subsets of a set \"X\" is said to have the finite intersection property (FIP) if the intersection over any finite subcollection of \"A\" is nonempty. It has the strong finite intersection property (SFIP) if the intersection over any finite subcollection of \"A\" is infinite.\n\nA centered system of sets is a collection of sets with the finite intersection property.\n\nLet formula_1 be a set with formula_2 a family of subsets of formula_1 indexed by an arbitrary set formula_4. Then the collection formula_5 has the finite intersection property (FIP), if any finite subcollection defined by formula_6 has non-empty intersection, that is formula_7 is a nonempty subset of formula_1.\n\nClearly the empty set cannot belong to any collection with the finite intersection property. The condition is trivially satisfied if the intersection over the entire collection is nonempty (in particular, if the collection itself is empty), and it is also trivially satisfied if the collection is nested, meaning that the collection is totally ordered by inclusion (equivalently, for any finite subcollection, a particular element of the subcollection is contained in all the other elements of the subcollection), e.g. the nested sequence of intervals (0, 1/\"n\"). These are not the only possibilities however. For example, if \"X\" = (0, 1) and for each positive integer \"i\", \"X\" is the set of elements of \"X\" having a decimal expansion with digit 0 in the \"i\"'th decimal place, then any finite intersection is nonempty (just take 0 in those finitely many places and 1 in the rest), but the intersection of all \"X\" for \"i\" ≥ 1 is empty, since no element of (0, 1) has all zero digits.\n\nThe finite intersection property is useful in formulating an alternative definition of compactness: a space is compact if and only if every collection of closed sets having the finite intersection property has nonempty intersection. This formulation of compactness is used in some proofs of Tychonoff's theorem and the uncountability of the real numbers (see next section)\n\nTheorem. Let \"X\" be a non-empty compact Hausdorff space that satisfies the property that no one-point set is open. Then \"X\" is uncountable.\n\nProof. We will show that if \"U\" ⊆ \"X\" is nonempty and open, and if \"x\" is a point of \"X\", then there is a neighbourhood \"V\" ⊂ \"U\" whose closure doesn’t contain \"x\" (\"x\" may or may not be in \"U\"). Choose \"y\" in \"U\" different from \"x\" (if \"x\" is in \"U\", then there must exist such a \"y\" for otherwise \"U\" would be an open one point set; if \"x\" isn’t in \"U\", this is possible since \"U\" is nonempty). Then by the Hausdorff condition, choose disjoint neighbourhoods \"W\" and \"K\" of \"x\" and \"y\" respectively. Then \"K\" ∩ \"U\" will be a neighbourhood of \"y\" contained in \"U\" whose closure doesn’t contain \"x\" as desired.\n\nNow suppose \"f\" : N → \"X\" is a bijection, and let {\"x\" : \"i\" ∈ N} denote the image of \"f\". Let \"X\" be the first open set and choose a neighbourhood \"U\" ⊂ \"X\" whose closure doesn’t contain \"x\". Secondly, choose a neighbourhood \"U\" ⊂ \"U\" whose closure doesn’t contain \"x\". Continue this process whereby choosing a neighbourhood \"U\" ⊂ \"U\" whose closure doesn’t contain \"x\". Then the collection {\"U\" : \"i\" ∈ N} satisfies the finite intersection property and hence the intersection of their closures is nonempty (by the compactness of \"X\"). Therefore, there is a point \"x\" in this intersection. No \"x\" can belong to this intersection because \"x\" doesn’t belong to the closure of \"U\". This means that \"x\" is not equal to \"x\" for all \"i\" and \"f\" is not surjective; a contradiction. Therefore, \"X\" is uncountable.\n\nAll the conditions in the statement of the theorem are necessary:\n\n1. We cannot eliminate the Hausdorff condition; a countable set with the indiscrete topology is compact, has more than one point, and satisfies the property that no one point sets are open, but is not uncountable.\n\n2. We cannot eliminate the compactness condition as the set of all rational numbers shows.\n\n3. We cannot eliminate the condition that one point sets cannot be open as a finite space as the discrete topology shows.\n\nCorollary. Every closed interval [\"a\", \"b\"] with \"a\" < \"b\" is uncountable. Therefore, R is uncountable.\n\nCorollary. Every perfect, locally compact Hausdorff space is uncountable.\n\nProof. Let \"X\" be a perfect, compact, Hausdorff space, then the theorem immediately implies that \"X\" is uncountable. If \"X\" is a perfect, locally compact Hausdorff space which is not compact, then the one-point compactification of \"X\" is a perfect, compact Hausdorff space. Therefore, the one point compactification of \"X\" is uncountable. Since removing a point from an uncountable set still leaves an uncountable set, \"X\" is uncountable as well.\n\nA filter has the finite intersection property by definition.\n\nLet \"X\" be nonempty, \"F\" ⊆ 2, \"F\" having the finite intersection property. Then there exists an \"F\"′ ultrafilter (in 2) such that \"F\" ⊆ \"F\"′.\n\nSee details and proof in . This result is known as ultrafilter lemma.\n\nA family of sets \"A\" has the strong finite intersection property (sfip), if every finite subfamily of \"A\" has infinite intersection.\n"}
{"id": "404343", "url": "https://en.wikipedia.org/wiki?curid=404343", "title": "Fixed-point lemma for normal functions", "text": "Fixed-point lemma for normal functions\n\nThe fixed-point lemma for normal functions is a basic result in axiomatic set theory stating that any normal function has arbitrarily large fixed points (Levy 1979: p. 117). It was first proved by Oswald Veblen in 1908.\n\nA normal function is a class function \"f\" from the class Ord of ordinal numbers to itself such that:\nIt can be shown that if \"f\" is normal then \"f\" commutes with suprema; for any nonempty set \"A\" of ordinals,\nIndeed, if sup \"A\" is a successor ordinal then sup \"A\" is an element of \"A\" and the equality follows from the increasing property of \"f\". If sup \"A\" is a limit ordinal then the equality follows from the continuous property of \"f\".\n\nA fixed point of a normal function is an ordinal β such that \"f\"(β) = β.\n\nThe fixed point lemma states that the class of fixed points of any normal function is nonempty and in fact is unbounded: given any ordinal α, there exists an ordinal β such that β ≥ α and \"f\"(β) = β.\n\nThe continuity of the normal function implies the class of fixed points is closed (the supremum of any subset of the class of fixed points is again a fixed point). Thus the fixed point lemma is equivalent to the statement that the fixed points of a normal function form a closed and unbounded class.\n\nThe first step of the proof is to verify that \"f\"(γ) ≥ γ for all ordinals γ and that \"f\" commutes with suprema. Given these results, inductively define an increasing sequence <α> (\"n\" < ω) by setting α = α, and α = \"f\"(α) for \"n\" ∈ ω. Let β = sup {α : \"n\" ∈ ω}, so β ≥ α. Moreover, because \"f\" commutes with suprema, \nThe last equality follows from the fact that the sequence <α> increases. formula_1\n\nAs an aside, it can be demonstrated that the β found in this way is the smallest fixed point greater than or equal to α.\n\nThe function \"f\" : Ord → Ord, \"f\"(α) = ω is normal (see initial ordinal). Thus, there exists an ordinal θ such that θ = ω. In fact, the lemma shows that there is a closed, unbounded class of such θ.\n\n"}
{"id": "36271791", "url": "https://en.wikipedia.org/wiki?curid=36271791", "title": "Fundamental increment lemma", "text": "Fundamental increment lemma\n\nIn single-variable differential calculus, the fundamental increment lemma is an immediate consequence of the definition of the derivative \"f\"(\"a\") of a function \"f\" at a point \"a\":\nThe lemma asserts that the existence of this derivative implies the existence of a function formula_2 such that \nfor sufficiently small but non-zero \"h\". For a proof, it suffices to define\nand verify this formula_2 meets the requirements.\n\nIn that the existence of formula_2 uniquely characterises the number formula_7, the fundamental increment lemma can be said to characterise the differentiability of single-variable functions. For this reason, a generalisation of the lemma can be used in the definition of differentiability in multivariable calculus. In particular, suppose \"f\" maps some subset of formula_8 to formula_9. Then \"f\" is said to be differentiable at a if there is a linear function \nand a function \nsuch that \nfor non-zero h sufficiently close to 0. In this case, \"M\" is the unique derivative (or total derivative, to distinguish from the directional and partial derivatives) of \"f\" at a. Notably, \"M\" is given by the Jacobian matrix of \"f\" evaluated at a.\n\n\n"}
{"id": "473996", "url": "https://en.wikipedia.org/wiki?curid=473996", "title": "Gaston Tarry", "text": "Gaston Tarry\n\nGaston Tarry (27 September 1843 – 21 June 1913) was a French mathematician. Born in Villefranche de Rouergue, Aveyron, he studied mathematics at high school before joining the civil service in Algeria.\n\nHe pursued mathematics as an amateur, his most famous achievement being his confirmation in 1901 of Leonhard Euler's conjecture that no 6×6 Graeco-Latin square was possible.\n\n\n"}
{"id": "563694", "url": "https://en.wikipedia.org/wiki?curid=563694", "title": "Gell-Mann matrices", "text": "Gell-Mann matrices\n\nThe Gell-Mann matrices, developed by Murray Gell-Mann, are a set of eight linearly independent 3×3 traceless Hermitian matrices used in the study of the strong interaction in particle physics.\nThey span the Lie algebra of the SU(3) group in the defining representation.\n\nand formula_1. \n\nThese matrices are traceless, Hermitian (so they can generate unitary matrix group elements through exponentiation), and obey the extra trace orthonormality relation. These properties were chosen by Gell-Mann because they then naturally generalize the Pauli matrices for SU(2) to SU(3), which formed the basis for Gell-Mann's quark model. Gell-Mann's generalization further extends to general SU(\"n\"). For their connection to the standard basis of Lie algebras, see the Weyl–Cartan basis.\n\nIn mathematics, orthonormality typically implies a norm which has a value of unity (1). Gell-Mann matrices, however, are normalized to a value of 2. Thus, the trace of the pairwise product results in the ortho-normalization condition \n\nwhere formula_3 is the Kronecker delta.\n\nThis is so the embedded Pauli matrices corresponding to the three embedded subalgebras of \"SU\"(2) are conventionally normalized. In this three-dimensional matrix representation, the Cartan subalgebra is the set of linear combinations (with real coefficients) of the two matrices formula_4 and formula_5, which commute with each other.\n\nThere are three independent SU(2) subalgebras:\n\nwhere the and are linear combinations of formula_4 and formula_5. The SU(2) Casimirs of these subalgebras mutually commute. \n\nHowever, any unitary similarity transformation of these subalgebras will yield SU(2) subalgebras. There is an uncountable number of such transformations.\n\nThe 8 infinitesimal generators of the Lie algebra are indexed by satisfy the commutation relations\n\nThe structure constants formula_12 are completely antisymmetric in the three indices, generalizing the antisymmetry of the Levi-Civita symbol formula_13 of . They have values\n\nIn general, they evaluate to zero, unless they contain an odd count of indices from the set {2,5,7}, corresponding to the antisymmetric (imaginary) s.\n\nSince the eight matrices and the identity are a complete trace-orthogonal set spanning all 3×3 matrices, it is straightforward to find two Fierz completeness relations, (Li & Cheng, 4.134), analogous to that satisfied by the Pauli matrices. Namely, using the dot to sum over all adjoint indices and utilizing Greek indices for the matrices' row/column indices, the following identities hold,\nand \nOne may prefer the recast version, resulting from a linear combination of the above,\n\nA particular choice of matrices is called a group representation, because any element of SU(3) can be written in the form formula_18, where the eight formula_19 are real numbers and a sum over the index is implied. Given one representation, an equivalent one may be obtained by an arbitrary unitary similarity transformation, since that leaves the commutator unchanged.\n\nThe matrices can be realized as a representation of the infinitesimal generators of the special unitary group called SU(3). The Lie algebra of this group (a real Lie algebra in fact) has dimension eight and therefore it has some set with eight linearly independent generators, which can be written as formula_20, with \"i\" taking values from 1 to 8. \n\nThe squared sum of the Gell-Mann matrices gives the quadratic Casimir operator, a group invariant, \nThere is another, independent, cubic Casimir operator, as well.\n\nThese matrices serve to study the internal (color) rotations of the gluon fields associated with the coloured quarks of quantum chromodynamics (cf. colors of the gluon). A gauge color rotation is a spacetime-dependent SU(3) group element \nformula_22, where summation over the eight indices is implied.\n\n\n"}
{"id": "39224451", "url": "https://en.wikipedia.org/wiki?curid=39224451", "title": "General covariant transformations", "text": "General covariant transformations\n\nIn physics, general covariant transformations are symmetries of gravitation theory on a world manifold formula_1. They are gauge transformations whose parameter functions are vector fields on formula_1. From the physical viewpoint, general covariant transformations are treated as particular (holonomic) reference frame transformations in general relativity. In mathematics, general covariant transformations are defined as particular automorphisms of so-called natural fiber bundles. \n\nLet formula_3 be a fibered manifold with local fibered coordinates formula_4. Every automorphism of formula_5 is projected onto a diffeomorphism of its base formula_1. However, the converse is not true. A diffeomorphism of formula_1 need not give rise to an automorphism of formula_5.\n\nIn particular, an infinitesimal generator of a one-parameter Lie group of automorphisms of formula_5 is a projectable vector field\n\non formula_5. This vector field is projected onto a vector field formula_12 on formula_1, whose flow is a one-parameter group of diffeomorphisms of formula_1. Conversely, let formula_15 be a vector field on formula_1. There is a problem of constructing its lift to a projectable vector field on formula_5 projected onto formula_18. Such a lift always exists, but it need not be canonical. Given a connection formula_19 on formula_5, every vector field formula_18 on formula_1 gives rise to the horizontal vector field\n\non formula_5. This horizontal lift formula_25 yields a monomorphism of the formula_26-module of vector fields on formula_1 to the formula_28-module of vector fields on formula_5, but this monomorphisms is not a Lie algebra morphism, unless formula_19 is flat.\n\nHowever, there is a category of above mentioned natural bundles formula_31 which admit the functorial lift formula_32 onto formula_33 of any vector field formula_18 on formula_1 such that formula_36 is a Lie algebra monomorphism\n\nThis functorial lift formula_32 is an infinitesimal general covariant transformation of formula_33.\n\nIn a general setting, one considers a monomorphism formula_40 of a group of diffeomorphisms of formula_1 to a group of bundle automorphisms of a natural bundle formula_31. Automorphisms formula_43 are called the general covariant transformations of formula_33. For instance, no vertical automorphism of formula_33 is a general covariant transformation.\n\nNatural bundles are exemplified by tensor bundles. For instance, the tangent bundle formula_46 of formula_1 is a natural bundle. Every diffeomorphism formula_48 of formula_1 gives rise to the tangent automorphism formula_50 of formula_46 which is a general covariant transformation of formula_46. With respect to the holonomic coordinates formula_53 on formula_46, this transformation reads\n\nA frame bundle formula_56 of linear tangent frames in formula_46 also is a natural bundle. General covariant transformations constitute a subgroup of holonomic automorphisms of formula_56. All bundles associated with a frame bundle are natural. However, there are natural bundles which are not associated with formula_56.\n\n\n"}
{"id": "31696675", "url": "https://en.wikipedia.org/wiki?curid=31696675", "title": "Geomagnetic pole", "text": "Geomagnetic pole\n\nThe geomagnetic poles are antipodal points where the axis of a best-fitting dipole intersects the surface of Earth. This theoretical dipole is equivalent to a powerful bar magnet at the center of Earth and comes closer than any other model to accounting for the magnetic field observed at Earth's surface. In contrast, the magnetic poles of the actual Earth are not antipodal; that is, the line on which they lie does not pass through Earth's center.\n\nOwing to motion of fluid in the Earth's outer core, the actual magnetic poles are constantly moving. However, over thousands of years their direction averages to the Earth's rotation axis. On the order of once every half a million years, the poles reverse (north switches place with south).\n\nAs a first-order approximation, the Earth's magnetic field can be modeled as a simple dipole (like a bar magnet), tilted about 9.6° with respect to the Earth's rotation axis (which defines the Geographic North and Geographic South Poles) and centered at the Earth's center. The North and South Geomagnetic Poles are the antipodal points where the axis of this theoretical dipole intersects the Earth's surface, thus unlike the magnetic poles they always have an equal degree of latitude and supplementary degrees of longitude respectively (2017: Lat. 80.5°N, 80.5°S; Long. 72.8°W, 107.2°E). If the Earth's magnetic field were a perfect dipole then the field lines would be vertical to the surface at the Geomagnetic Poles, and they would coincide with the North and South magnetic poles. However, the approximation is imperfect, and so the Magnetic and Geomagnetic Poles lie some distance apart.\n\nLike the North Magnetic Pole, the North Geomagnetic Pole attracts the north pole of a bar magnet and so is in a physical sense actually a magnetic \"south\" pole. It is the center of the 'open' magnetic field lines which connect to the interplanetary magnetic field and provide a direct route for the solar wind to reach the ionosphere. it was located at approximately , on Ellesmere Island, Nunavut, Canada.\n\nThe South Geomagnetic Pole is the point where the axis of this best-fitting tilted dipole intersects the Earth's surface in the southern hemisphere. As of 2005 it was calculated to be located at 79.74°S 108.22°E, near the Vostok Station. \n\nBecause the Earth's actual magnetic field is not an exact dipole, the (calculated) North and South Geomagnetic Poles do not coincide with the North and South Magnetic Poles. If the Earth's magnetic fields were exactly dipolar, the north pole of a magnetic compass needle would point directly at the North Geomagnetic Pole. In practice it does not because the geomagnetic field that originates in the core has a more complex non-dipolar part, and magnetic anomalies in the Earth's crust also contribute to the local field. \n\nThe locations of geomagnetic poles are calculated by the International Geomagnetic Reference Field, a statistical fit to measurements of the Earth's field by satellites and in geomagnetic observatories. The Geomagnetic Poles are wandering for the same reason the Magnetic Poles wander.\n\nThe geomagnetic poles move over time because the geomagnetic field is produced by motion of the molten iron alloys in the Earth's outer core (see geodynamo). Over the past 150 years the poles have moved westward at a rate of 0.05° to 0.1° per year, with little net north or south motion.\n\nOver several thousand years, the average location of the geomagnetic poles coincides with the geographical poles. Paleomagnetists have long relied on the \"Geocentric axial dipole (GAD) hypothesis\", which states that, aside from during geomagnetic reversals, the time-averaged position of the geomagnetic poles has always coincided with the geographic poles. There is considerable paleomagnetic evidence supporting this hypothesis.\n\nOver the life of the Earth, the orientation of Earth's magnetic field has reversed many times, with geomagnetic north becoming geomagnetic south and vice versa – an event known as a geomagnetic reversal. Evidence of geomagnetic reversals can be seen at mid-ocean ridges where tectonic plates move apart. As magma seeps out of the mantle and solidifies to become new ocean floor, the magnetic minerals in it are magnetized in the direction of the magnetic field. Thus, starting at the most recently formed ocean floor, one can read out the direction of the magnetic field in previous times as one moves farther away to older ocean floor.\n\n\n\n"}
{"id": "243343", "url": "https://en.wikipedia.org/wiki?curid=243343", "title": "George Green (mathematician)", "text": "George Green (mathematician)\n\nGeorge Green (14 July 1793 – 31 May 1841) was a British mathematical physicist who wrote \"An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\" (Green, 1828). The essay introduced several important concepts, among them a theorem similar to the modern Green's theorem, the idea of potential functions as currently used in physics, and the concept of what are now called Green's functions. Green was the first person to create a mathematical theory of electricity and magnetism and his theory formed the foundation for the work of other scientists such as James Clerk Maxwell, William Thomson, and others. His work on potential theory ran parallel to that of Carl Friedrich Gauss.\n\nGreen's life story is remarkable in that he was almost entirely self-taught. He received only about one year of formal schooling as a child, between the ages of 8 and 9.\n\nGreen was born and lived for most of his life in the English town of Sneinton, Nottinghamshire, now part of the city of Nottingham. His father, also named George, was a baker who had built and owned a brick windmill used to grind grain.\n\nIn his youth, Green was described as having a frail constitution and a dislike for doing work in his father's bakery. He had no choice in the matter, however, and as was common for the time he likely began working daily to earn his living at the age of five.\n\nRoughly 25–50% of children in Nottingham received any schooling in this period. The majority of schools were Sunday schools, run by the Church, and children would typically attend for one or two years only.\nRecognizing the young Green's above average intellect, and being in a strong financial situation due to his successful bakery, his father enrolled him in March 1801 at Robert Goodacre's Academy in Upper Parliament Street. Robert Goodacre was a well-known science populariser and educator of the time. He published \"Essay on the Education of Youth\", in which he wrote that he did not \"study the interest of the boy but the embryo Man\". To a non-specialist, he would have seemed deeply knowledgeable in science and mathematics, but a close inspection of his essay and curriculum revealed that the extent of his mathematical teachings was limited to algebra, trigonometry and logarithms. Thus, Green's later mathematical contributions, which exhibited knowledge of very modern developments in mathematics, could not have resulted from his tenure at the Robert Goodacre Academy. He stayed for only four terms (one school year), and it was speculated by his contemporaries that he probably exhausted all they had to teach him.\n\nIn 1773 George's father moved to Nottingham, which at the time had a reputation for being a pleasant town with open spaces and wide roads. By 1831, however, the population had increased nearly five times, in part due to the budding industrial revolution, and the city became known as one of the worst slums in England. There were frequent riots by starving workers, often associated with special hostility towards bakers and millers on the suspicion that they were hiding grain to drive up food prices.\n\nFor these reasons, in 1807, George Green senior bought a plot of land in Sneinton. On this plot of land he built a \"brick wind corn mill\", now referred to as Green's Windmill. It was technologically impressive for its time, but required nearly twenty-four-hour maintenance, which was to become George Green's burden for the next twenty years.\n\nJust as with baking, Green found the responsibilities of operating the mill annoying and tedious. Grain from the fields was arriving continuously at the mill's doorstep, and the sails of the windmill had to be constantly adjusted to the windspeed, both to prevent damage in high winds, and to maximise rotational speed in low winds. The millstones that would continuously grind against each other, could wear down or cause a fire if they ran out of grain to grind. Every month the stones, which weighed over a ton, would have to be replaced or repaired.\n\nIn 1823 Green formed a relationship with Jane Smith, the daughter of William Smith, hired by Green Senior as mill manager. Although Green and Jane Smith never married, Jane eventually became known as Jane Green and the couple had seven children together; all but the first had Green as a baptismal name. The youngest child was born 13 months before Green's death. Green provided for his common-law wife and children in his will.\n\nWhen Green was thirty, he became a member of the Nottingham Subscription Library. This library exists today, and was likely one of the only sources of Green's advanced mathematical knowledge. Unlike more conventional libraries, the subscription library was exclusive to a hundred or so subscribers, and the first on the list of subscribers was the Duke of Newcastle. This library catered to requests for specialised books and journals that satisfied the particular interests of their subscribers.\n\nIn 1828, Green published \"An Essay on the Application of Mathematical Analysis to the Theories of Electricity and Magnetism\", which is the essay he is most famous for today. It was published privately at the author's expense, because he thought it would be presumptuous for a person like himself, with no formal education in mathematics, to submit the paper to an established journal. When Green published his \"Essay\", it was sold on a subscription basis to 51 people, most of whom were friends and probably could not understand it.\n\nThe wealthy landowner and mathematician Sir Edward Bromhead bought a copy and encouraged Green to do further work in mathematics. Not believing the offer was sincere, Green did not contact Bromhead for two years.\n\nBy the time Green's father died in 1829, the senior Green had become one of the gentry due to his considerable accumulated wealth and land owned, roughly half of which he left to his son and the other half to his daughter. The young Green, now thirty-six years old, consequently was able to use this wealth to abandon his miller duties and pursue mathematical studies.\n\nMembers of the Nottingham Subscription Library who knew Green repeatedly insisted that he obtain a proper University education. In particular, one of the library's most prestigious subscribers was Sir Edward Bromhead, with whom Green shared many correspondences; he insisted that Green go to Cambridge.\n\nIn 1832, aged nearly forty, Green was admitted as an undergraduate at Gonville and Caius College, Cambridge. He was particularly insecure about his lack of knowledge of Greek and Latin, which were prerequisites, but it turned out not to be as hard for him to learn these as he had envisaged, as the degree of expected mastery was not as high as he had expected. In the mathematics examinations, he won the first-year mathematical prize. He graduated with a BA in 1838 as a 4th Wrangler (the 4th highest scoring student in his graduating class, coming after James Joseph Sylvester who scored 2nd).\n\nFollowing his graduation, Green was elected a fellow of the Cambridge Philosophical Society. Even without his stellar academic standing, the Society had already read and made note of his Essay and three other publications, so Green was welcomed.\n\nThe next two years provided an unparalleled opportunity for Green to read, write, and discuss his scientific ideas. In this short time he published an additional six publications with applications to hydrodynamics, sound, and optics.\n\nIn his final years at Cambridge, Green became rather ill, and in 1840 he returned to Sneinton, only to die a year later. There are rumours that at Cambridge, Green had \"succumbed to alcohol\", and some of his earlier supporters, such as Sir Edward Bromhead, tried to distance themselves from him.\n\nGreen's work was not well known in the mathematical community during his lifetime. Besides Green himself, the first mathematician to quote his 1828 work was the Briton Robert Murphy (1806–1843) in his 1833 work. In 1845, four years after Green's death, Green's work was rediscovered by the young William Thomson (then aged 21), later known as Lord Kelvin, who popularised it for future mathematicians. According to the book \"George Green\" by D.M. Cannell, William Thomson noticed Murphy's citation of Green's 1828 essay but found it difficult to locate Green's 1828 work; he finally got some copies of Green's 1828 work from William Hopkins in 1845.\n\nIn 1871 N. M. Ferrers assembled \"The Mathematical Papers of the late George Green\" for publication.\n\nGreen's work on the motion of waves in a canal (resulting in what is known as Green's law) anticipates the WKB approximation of quantum mechanics, while his research on light-waves and the properties of the ether produced what is now known as the Cauchy-Green tensor. Green's theorem and functions were important tools in classical mechanics, and were revised by Schwinger's 1948 work on electrodynamics that led to his 1965 Nobel prize (shared with Feynman and Tomonaga). Green's functions later also proved useful in analysing superconductivity. On a visit to Nottingham in 1930, Albert Einstein commented that Green had been 20 years ahead of his time. The theoretical physicist Julian Schwinger who used Green's functions in his ground-breaking works, published a tribute entitled \"The Greening of Quantum Field Theory: George and I\" in 1993.\n\nThe George Green Library at the University of Nottingham is named after him, and houses the majority of the university's science and engineering Collection. \"The George Green Institute for Electromagnetics Research\", a research group in the University of Nottingham engineering department, is also named after him. In 1986, Green's Windmill was restored to working order. It now serves both as a working example of a 19th-century windmill and as a museum and science centre dedicated to Green.\n\nWestminster Abbey has a memorial stone for Green in the nave adjoining the graves of Sir Isaac Newton and Lord Kelvin.\n\nHis work and influence on nineteenth century applied physics had been largely forgotten until the publication of his biography by Mary Cannell in 1993.\n\nIt is unclear to historians exactly where Green obtained information on current developments in mathematics, as Nottingham had little in the way of intellectual resources. What is even more mysterious is that Green had used \"the Mathematical Analysis,\" a form of calculus derived from Leibniz that was virtually unheard of, or even actively discouraged, in England at the time (due to Leibniz being a contemporary of Newton who had his own methods that were championed in England). This form of calculus, and the developments of mathematicians such as Laplace, Lacroix and Poisson were not taught even at Cambridge, let alone Nottingham, and yet Green had not only heard of these developments, but also improved upon them.\n\nIt is speculated that only one person educated in mathematics, John Toplis, headmaster of Nottingham High School 1806–1819, graduate from Cambridge and an enthusiast of French mathematics, is known to have lived in Nottingham at the time.\n\n\n\n"}
{"id": "666526", "url": "https://en.wikipedia.org/wiki?curid=666526", "title": "Greedoid", "text": "Greedoid\n\nIn combinatorics, a greedoid is a type of set system. It arises from the notion of the matroid, which was originally introduced by Whitney in 1935 to study planar graphs and was later used by Edmonds to characterize a class of optimization problems that can be solved by greedy algorithms. Around 1980, Korte and Lovász introduced the greedoid to further generalize this characterization of greedy algorithms; hence the name greedoid. Besides mathematical optimization, greedoids have also been connected to graph theory, language theory, poset theory, and other areas of mathematics.\n\nA set system (\"F\", E) is a collection \"F\" of subsets of a ground set E (i.e. \"F\" is a subset of the power set of E). When considering a greedoid, a member of \"F\" is called a feasible set. When considering a matroid, a feasible set is also known as an \"independent set\".\n\nAn accessible set system (\"F\", E) is a set system in which every nonempty feasible set X contains an element x such that X\\{x} is feasible. This implies that any nonempty, finite, accessible set system necessarily contains the empty set ∅.\n\nA greedoid (\"F\", E) is an accessible set system that satisfies the \"exchange property\":\n\n\nA basis of a greedoid is a maximal feasible set, meaning it is a feasible set but not contained in any other one. A basis of a subset X of E is a maximal feasible set contained in X.\n\nThe rank of a greedoid is the size of a basis. \nBy the exchange property, all bases have the same size.\nThus, the rank function is well defined. The rank of a subset X of E is the size of a basis of X. Just as with matroids, greedoids have a cryptomorphism in terms of rank functions.\nA function formula_1 is the rank function of a greedoid on the ground set E if and only if formula_2 is subcardinal, monotonic, and locally semimodular, that is, for any formula_3 and any formula_4 we have\n\nMost classes of greedoids have many equivalent definitions in terms of set system, language, poset, simplicial complex, and so on. The following description takes the traditional route of listing only a couple of the more well-known characterizations.\n\nAn interval greedoid (\"F\", E) is a greedoid that satisfies the \"Interval Property\":\n\n\nEquivalently, an interval greedoid is a greedoid such that the union of any two feasible sets is feasible if it is contained in another feasible set.\n\nAn antimatroid (\"F\", E) is a greedoid that satisfies the \"Interval Property without Upper Bounds\":\n\n\nEquivalently, an antimatroid is (i) a greedoid with a unique basis; or (ii) an accessible set system closed under union. It is easy to see that an antimatroid is also an interval greedoid.\n\nA matroid (\"F\", E) is a non-empty greedoid that satisfies the \"Interval Property without Lower Bounds\":\n\n\nIt is easy to see that a matroid is also an interval greedoid.\n\n\nIn general, a greedy algorithm is just an iterative process in which a \"locally best choice\", usually an input of minimum weight, is chosen each round until all available choices have been exhausted.\nIn order to describe a greedoid-based condition in which a greedy algorithm is optimal, we need some more common terminologies in greedoid theory.\nWithout loss of generality, we consider a greedoid G = (\"F\", E) with E finite.\n\nA subset X of E is rank feasible if the largest intersection of X with any feasible set has size equal to the rank of X.\nIn a matroid, every subset of E is rank feasible.\nBut the equality does not hold for greedoids in general.\n\nA function w: E → ℝ is \"R\"-compatible if {x ∈ E: w(x) ≥ c} is rank feasible for all real numbers c.\n\nAn objective function f: 2 → ℝ is linear over a set S if, for all X ⊆ S, we have f(X) = Σ w(x) for some weight function w: S → ℜ.\n\nProposition. A greedy algorithm is optimal for every R-compatible linear objective function over a greedoid.\n\nThe intuition behind this proposition is that, during the iterative process, each optimal exchange of minimum weight is made possible by the exchange property, and optimal results are obtainable from the feasible sets in the underlying greedoid. This result guarantees the optimality of many well-known algorithms. For example, a minimum spanning tree of a weighted graph may be obtained using Kruskal's algorithm, which is a greedy algorithm for the cycle matroid. Prim's algorithm can be explained by taking the vertex search greedoid instead.\n\n\n\n"}
{"id": "41226", "url": "https://en.wikipedia.org/wiki?curid=41226", "title": "Hamming code", "text": "Hamming code\n\nIn telecommunication, Hamming codes are a family of linear error-correcting codes. Hamming codes can detect up to two-bit errors or correct one-bit errors without detection of uncorrected errors. By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three.\nRichard Hamming invented Hamming codes in 1950 as a way of automatically correcting errors introduced by punched card readers. In his original paper, Hamming elaborated his general idea, but specifically focused on the Hamming(7,4) code which adds three parity bits to four bits of data.\n\nIn mathematical terms, Hamming codes are a class of binary linear codes. For each integer there is a code with block length and message length . Hence the rate of Hamming codes is , which is the highest possible for codes with minimum distance of three (i.e., the minimal number of bit changes needed to go from any code word to any other code word is three) and block length . The parity-check matrix of a Hamming code is constructed by listing all columns of length that are non-zero, which means that the dual code of the Hamming code is the shortened Hadamard code. The parity-check matrix has the property that any two columns are pairwise linearly independent.\n\nDue to the limited redundancy that Hamming codes add to the data, they can only detect and correct errors when the error rate is low. This is the case in computer memory (ECC memory), where bit errors are extremely rare and Hamming codes are widely used. In this context, an extended Hamming code having one extra parity bit is often used. Extended Hamming codes achieve a Hamming distance of four, which allows the decoder to distinguish between when at most one one-bit error occurs and when any two-bit errors occur. In this sense, extended Hamming codes are single-error correcting and double-error detecting, abbreviated as SECDED.\n\nRichard Hamming, the inventor of Hamming codes, worked at Bell Labs in the late 1940s on the Bell Model V computer, an electromechanical relay-based machine with cycle times in seconds. Input was fed in on punched paper tape, seven-eighths of an inch wide which had up to six holes per row. During weekdays, when errors in the relays were detected, the machine would stop and flash lights so that the operators could correct the problem. During after-hours periods and on weekends, when there were no operators, the machine simply moved on to the next job.\n\nHamming worked on weekends, and grew increasingly frustrated with having to restart his programs from scratch due to detected errors. In a taped interview Hamming said, \"And so I said, 'Damn it, if the machine can detect an error, why can't it locate the position of the error and correct it?'\". Over the next few years, he worked on the problem of error-correction, developing an increasingly powerful array of algorithms. In 1950, he published what is now known as Hamming Code, which remains in use today in applications such as ECC memory.\n\nA number of simple error-detecting codes were used before Hamming codes, but none were as effective as Hamming codes in the same overhead of space.\n\nParity adds a single bit that indicates whether the number of ones (bit-positions with values of one) in the preceding data was even or odd. If an odd number of bits is changed in transmission, the message will change parity and the error can be detected at this point; however, the bit that changed may have been the parity bit itself. The most common convention is that a parity value of one indicates that there is an odd number of ones in the data, and a parity value of zero indicates that there is an even number of ones. If the number of bits changed is even, the check bit will be valid and the error will not be detected.\n\nMoreover, parity does not indicate which bit contained the error, even when it can detect it. The data must be discarded entirely and re-transmitted from scratch. On a noisy transmission medium, a successful transmission could take a long time or may never occur. However, while the quality of parity checking is poor, since it uses only a single bit, this method results in the least overhead.\n\nA two-out-of-five code is an encoding scheme which uses five bits consisting of exactly three 0s and two 1s. This provides ten possible combinations, enough to represent the digits 0–9. This scheme can detect all single bit-errors, all odd numbered bit-errors and some even numbered bit-errors (for example the flipping of both 1-bits). However it still cannot correct any of these errors.\n\nAnother code in use at the time repeated every data bit multiple times in order to ensure that it was sent correctly. For instance, if the data bit to be sent is a 1, an \"repetition code\" will send 111. If the three bits received are not identical, an error occurred during transmission. If the channel is clean enough, most of the time only one bit will change in each triple. Therefore, 001, 010, and 100 each correspond to a 0 bit, while 110, 101, and 011 correspond to a 1 bit, with the greater quantity of digits that are the same ('0' or a '1') indicating what the data bit should be. A code with this ability to reconstruct the original message in the presence of errors is known as an \"error-correcting\" code. This triple repetition code is a Hamming code with since there are two parity bits, and data bit.\n\nSuch codes cannot correctly repair all errors, however. In our example, if the channel flips two bits and the receiver gets 001, the system will detect the error, but conclude that the original bit is 0, which is incorrect. If we increase the size of the bit string to four, we can detect all two-bit errors but cannot correct them, (the quantity of parity bits is even) at five bits, we can correct all two-bit errors, but not all three-bit errors.\n\nMoreover, increasing the size of the parity bit string is inefficient, reducing throughput by three times in our original case, and the efficiency drops drastically as we increase the number of times each bit is duplicated in order to detect and correct more errors.\n\nIf more error-correcting bits are included with a message, and if those bits can be arranged such that different incorrect bits produce different error results, then bad bits could be identified. In a seven-bit message, there are seven possible single bit errors, so three error control bits could potentially specify not only that an error occurred but also which bit caused the error.\n\nHamming studied the existing coding schemes, including two-of-five, and generalized their concepts. To start with, he developed a nomenclature to describe the system, including the number of data bits and error-correction bits in a block. For instance, parity includes a single bit for any data word, so assuming ASCII words with seven bits, Hamming described this as an \"(8,7)\" code, with eight bits in total, of which seven are data. The repetition example would be \"(3,1)\", following the same logic. The code rate is the second number divided by the first, for our repetition example, 1/3.\n\nHamming also noticed the problems with flipping two or more bits, and described this as the \"distance\" (it is now called the \"Hamming distance\", after him). Parity has a distance of 2, so one bit flip can be detected, but not corrected and any two bit flips will be invisible. The (3,1) repetition has a distance of 3, as three bits need to be flipped in the same triple to obtain another code word with no visible errors. It can correct one-bit errors or detect but not correct two-bit errors. A (4,1) repetition (each bit is repeated four times) has a distance of 4, so flipping three bits can be detected, but not corrected. When three bits flip in the same group there can be situations where attempting to correct will produce the wrong code word. In general, a code with distance \"k\" can detect but not correct errors.\n\nHamming was interested in two problems at once: increasing the distance as much as possible, while at the same time increasing the code rate as much as possible. During the 1940s he developed several encoding schemes that were dramatic improvements on existing codes. The key to all of his systems was to have the parity bits overlap, such that they managed to check each other as well as the data.\n\nThe following general algorithm generates a single-error correcting (SEC) code for any number of bits. The main idea is to choose the error-correcting bits such that the index-XOR (the XOR of all the bit positions containing a 1) is 0. We use positions 1, 10, 100, etc (in binary) as the error-correcting bits, which guarantees it is possible to set the error-correcting bits so that the index-XOR of the whole message is 0. If the receiver receives a string with index-XOR 0, they can conclude there were no corruptions, and otherwise, the index-XOR indicates the index of the corrupted bit.\n\nThe following steps implement this algorithm:\n\n\nThe form of the parity is irrelevant. Even parity is mathematically simpler, but there is no difference in practice.\n\nThis general rule can be shown visually:\n\nShown are only 20 encoded bits (5 parity, 15 data) but the pattern continues indefinitely. The key thing about Hamming Codes that can be seen from visual inspection is that any given bit is included in a unique set of parity bits. To check for errors, check all of the parity bits. The pattern of errors, called the error syndrome, identifies the bit in error. If all parity bits are correct, there is no error. Otherwise, the sum of the positions of the erroneous parity bits identifies the erroneous bit. For example, if the parity bits in positions 1, 2 and 8 indicate an error, then bit 1+2+8=11 is in error. If only one parity bit indicates an error, the parity bit itself is in error.\n\nAs you can see, if you have parity bits, it can cover bits from 1 up to formula_1. If we subtract out the parity bits, we are left with formula_2 bits we can use for the data. As varies, we get all the possible Hamming codes:\n\nHamming codes have a minimum distance of 3, which means that the decoder can detect and correct a single error, but it cannot distinguish a double bit error of some codeword from a single bit error of a different codeword. Thus, some double-bit errors will be incorrectly decoded as if they were single bit errors and therefore go undetected, unless no correction is attempted.\n\nTo remedy this shortcoming, Hamming codes can be extended by an extra parity bit. This way, it is possible to increase the minimum distance of the Hamming code to 4, which allows the decoder to distinguish between single bit errors and two-bit errors. Thus the decoder can detect and correct a single error and at the same time detect (but not correct) a double error.\n\nIf the decoder does not attempt to correct errors, it can reliably detect triple bit errors. If the decoder does correct errors, some triple errors will be mistaken for single errors and \"corrected\" to the wrong value. Error correction is therefore a trade-off between certainty (the ability to reliably detect triple bit errors) and resiliency (the ability to keep functioning in the face of single bit errors).\n\nThis extended Hamming code is popular in computer memory systems, where it is known as \"SECDED\" (abbreviated from \"single error correction, double error detection\"). Particularly popular is the (72,64) code, a truncated (127,120) Hamming code plus an additional parity bit, which has the same space overhead as a (9,8) parity code.\n\nIn 1950, Hamming introduced the [7,4] Hamming code. It encodes four data bits into seven bits by adding three parity bits. It can detect and correct single-bit errors. With the addition of an overall parity bit, it can also detect (but not correct) double-bit errors.\n\nThe matrix \nformula_3 is called a (canonical) generator matrix of a linear (\"n\",\"k\") code,\n\nand formula_4 is called a parity-check matrix.\n\nThis is the construction of G and H in standard (or systematic) form. Regardless of form, G and H for linear block codes must satisfy\n\nformula_5, an all-zeros matrix.\n\nSince [7, 4, 3] = [\"n\", \"k\", \"d\"] = [2 − 1, 2−1−\"m\", \"m\"]. The parity-check matrix H of a Hamming code is constructed by listing all columns of length \"m\" that are pair-wise independent.\n\nThus H is a matrix whose left side is all of the nonzero n-tuples where order of the n-tuples in the columns of matrix does not matter. The right hand side is just the (\"n\" − \"k\")-identity matrix.\n\nSo G can be obtained from H by taking the transpose of the left hand side of H with the identity k-identity matrix on the left hand side of G.\n\nThe code generator matrix formula_6 and the parity-check matrix formula_7 are:\n\nformula_8\n\nand\n\nformula_9\n\nFinally, these matrices can be mutated into equivalent non-systematic codes by the following operations:\n\n\n\nFrom the above matrix we have 2 = 2 = 16 codewords.\nLet formula_10 be a row vector of binary data bits, formula_11. The codeword formula_12 for any of the 16 possible data vectors formula_13 is given by the standard matrix product formula_14 where the summing operation is done modulo-2.\n\nFor example, let formula_15. Using the generator matrix formula_16 from above, we have (after applying modulo 2, to the sum),\n\nformula_17\n\nThe [7,4] Hamming code can easily be extended to an [8,4] code by adding an extra parity bit on top of the (7,4) encoded word (see Hamming(7,4)).\nThis can be summed up with the revised matrices:\n\nand\n\nNote that H is not in standard form. To obtain G, elementary row operations can be used to obtain an equivalent matrix to H in systematic form:\n\nFor example, the first row in this matrix is the sum of the second and third rows of H in non-systematic form. Using the systematic construction for Hamming codes from above, the matrix A is apparent and the systematic form of G is written as\n\nThe non-systematic form of G can be row reduced (using elementary row operations) to match this matrix.\n\nThe addition of the fourth row effectively computes the sum of all the codeword bits (data and parity) as the fourth parity bit.\n\nFor example, 1011 is encoded (using the non-systematic form of G at the start of this section) into 01100110 where blue digits are data; red digits are parity bits from the [7,4] Hamming code; and the green digit is the parity bit added by the [8,4] code.\nThe green digit makes the parity of the [7,4] codewords even.\n\nFinally, it can be shown that the minimum distance has increased from 3, in the [7,4] code, to 4 in the [8,4] code. Therefore, the code can be defined as [8,4] Hamming code. \n\n\n"}
{"id": "7842900", "url": "https://en.wikipedia.org/wiki?curid=7842900", "title": "Heinrich Suter", "text": "Heinrich Suter\n\nHeinrich Suter (4 January 1848 in Hedingen – 17 March 1922 in Dornach) was a historian of science specializing in Islamic mathematics and astronomy.\n\nAfter graduation from the \"Industrie Schule\" at Zürich, Suter studied in Berlin (1869/70) and at ETH Zürich and the University of Zürich. He received in 1871 from the University of Zürich his Promotierung (Ph.D.) with dissertation \"Geschichte der mathematischen Wissenschaften von den ältesten Zeiten bis Ende des 16. Jahrhunderts\". His dissertation was published in 1872 as a book and was subsequently translated into Russian.\n\nIn 1874 Suter began teaching as a vicar at the Gymnasium in Schaffhausen, then taught from 1876 to 1886 in Aarau, and finally from 1886 until his retirement in 1916 in Zürich.\n\nSuter in his early forties learned Arabic and acquired some knowledge of Syriac, Persian and Turkish. He studied the history of mathematics and astronomy in the Islamic societies. In Moritz Cantor's \"Abhandlungen zur Geschichte der Mathematik“ were published in 1892 Suter's translation of the mathematically related entries in the \"Kitāb al-Fihrist\" of Ibn al-Nadim and in 1893 Suter's translation of the mathematical parts of the catalog of the \"Khedivial Library\" in Cairo. One of his most important works is his work, commissioned by the Royal Danish Academy of Sciences, on the astronomical tables of Al-Khwarizmi.\n\nIn 1904 Suter was an Invited Speaker of the ICM in Heidelberg.\n\n\n\nBM. – \"Bibliotheca Mathematica\", ZM. – \"Zeitschrift für Mathematik und Physik\", ZDMG – \"Zeitschrift der Deutschen Morgenländischen Gesellschaft\", SE – \"Sitzungsberichte der phys.-med. Sozietät Erlangen\", OLZ – \"Orientalistische Literatur-Zeitung\".\n\n\n"}
{"id": "662181", "url": "https://en.wikipedia.org/wiki?curid=662181", "title": "Irrational base discrete weighted transform", "text": "Irrational base discrete weighted transform\n\nIn mathematics, the irrational base discrete weighted transform (IBDWT) is a variant of the fast Fourier transform using an irrational base; it was developed by Richard Crandall (Reed College), Barry Fagin (Dartmouth College) and Joshua Doenias (NeXT Software) in the early 1990s using Mathematica.\n\nThe IBDWT is used in the Great Internet Mersenne Prime Search's client Prime95 to perform FFT multiplication, as well as in other programs implementing Lucas-Lehmer test, such as CUDALucas and Glucas.\n\n"}
{"id": "15390784", "url": "https://en.wikipedia.org/wiki?curid=15390784", "title": "Isochron", "text": "Isochron\n\nIn the mathematical theory of dynamical systems, an isochron is a set of initial conditions for the system that all lead to the same long-term behaviour.\n\nConsider the ordinary differential equation for a solution formula_1 evolving in time:\n\nThis ordinary differential equation (ODE) needs two initial conditions at, say, time formula_3. Denote the initial conditions by formula_4 and formula_5 where formula_6 and formula_7 are some parameters. The following argument shows that the isochrons for this system are here the straight lines formula_8.\n\nThe general solution of the above ODE is\n\nNow, as time increases, formula_10, the exponential terms decays very quickly to zero (exponential decay). Thus \"all\" solutions of the ODE quickly approach formula_11. That is, \"all\" solutions with the same formula_12 have the same long term evolution. The exponential decay of the formula_13 term brings together a host of solutions to share the same long term evolution. Find the isochrons by answering which initial conditions have the same formula_12.\n\nAt the initial time formula_3 we have formula_16 and formula_17. Algebraically eliminate the immaterial constant formula_18 from these two equations to deduce that all initial conditions formula_19 have the same formula_12, hence the same long term evolution, and hence form an isochron.\n\nLet's turn to a more interesting application of the notion of isochrons. Isochrons arise when trying to forecast predictions from models of dynamical systems. Consider the toy system of two coupled ordinary differential equations\n\nA marvellous mathematical trick is the normal form (mathematics) transformation. Here the coordinate transformation near the origin\n\nto new variables formula_23 transforms the dynamics to the separated form\n\nHence, near the origin, formula_25 decays to zero exponentially quickly as its equation is formula_26. So the long term evolution is determined solely by formula_27: the formula_27 equation is the model. \n\nLet us use the formula_27 equation to predict the future. Given some initial values formula_30 of the original variables: what initial value should we use for formula_31? Answer: the formula_32 that has the same long term evolution. In the normal form above, formula_27 evolves independently of formula_25. So all initial conditions with the same formula_27, but different formula_25, have the same long term evolution. Fix formula_27 and vary formula_25 gives the curving isochrons in the formula_39 plane. For example, very near the origin the isochrons of the above system are approximately the lines formula_40. Find which isochron the initial values formula_30 lie on: that isochron is characterised by some formula_32; the initial condition that gives the correct forecast from the model for all time is then formula_43.\n\nYou may find such normal form transformations for relatively simple systems of ordinary differential equations, both deterministic and stochastic, via an interactive web site.\n"}
{"id": "222348", "url": "https://en.wikipedia.org/wiki?curid=222348", "title": "Kamal (navigation)", "text": "Kamal (navigation)\n\nA kamal is a celestial navigation device that determines latitude. The invention of the kamal allowed for the earliest known latitude sailing, and was thus the earliest step towards the use of quantitative methods in navigation. It originated with Arab navigators of the late 9th century, and was employed in the Indian Ocean from the 10th century. It was adopted by Indian navigators soon after, and then adopted by Chinese navigators some time before the 16th century.\n\nThe kamal consists of a rectangular wooden card about , to which a string with several equally spaced knots is attached through a hole in the middle of the card. The kamal is used by placing one end of the string in the teeth while the other end is held away from the body roughly parallel to the ground. The card is then moved along the string, positioned so the lower edge is even with the horizon, and the upper edge is occluding a target star, typically Polaris because its angle to the horizon does not change with longitude or time. The angle can then be measured by counting the number of knots from the teeth to the card, or a particular knot can be tied into the string if travelling to a known latitude.\n\nThe knots were typically tied to measure angles of one finger-width. When held at arm's length, the width of a finger measures an angle that remains fairly similar from person to person. This was widely used (and still is today) for rough angle measurements, an angle known as \"issabah إصبع\" for 'finger' in Arabic, or a \"chih\" in Chinese. By modern measure, this is about 1 degree, 36 minutes, and 25 seconds, or just over 1.5 degrees. It is equal to the arcsine of the ratio of the width of the finger to the length of the arm.\n\nDue to the limited width of the card, the kamal was only really useful for measuring Polaris in equatorial latitudes, where Polaris remains close to the horizon. This fact may explain why it was not common in Europe. For these higher-latitude needs somewhat more complex devices based on the same principle were used, notably the cross-staff and backstaff.\n\nThe kamal is still a tool recommended for use in sea kayaking. In such an application, it can be used for estimating distances to land. The distance can be calculated from the formula \n\nwhere formula_2 is the distance to the object, formula_3 is the size of the object observed, formula_4 is the distance from the kamal to the observer's eye, and formula_5 is the apparent size of the object observed.\n\n"}
{"id": "25887069", "url": "https://en.wikipedia.org/wiki?curid=25887069", "title": "Karlsruhe metric", "text": "Karlsruhe metric\n\nIn metric geometry, the Karlsruhe metric is a measure of distance that assumes travel is only possible along rays through the origin and circular arcs centered at the origin.The name alludes to the layout of the city of Karlsruhe, which has radial streets and circular avenues around a central point. This metric is also called Moscow metric.\n\nThe Karlsruhe distance between two points formula_1 is given as\n\nformula_2\n\nwhere formula_3 are the polar coordinates of formula_4 and formula_5 is the angular distance between the two points.\n\n\n"}
{"id": "18657553", "url": "https://en.wikipedia.org/wiki?curid=18657553", "title": "Lattice-based cryptography", "text": "Lattice-based cryptography\n\nLattice-based cryptography is the generic term for constructions of cryptographic primitives that involve lattices, either in the construction itself or in the security proof. Lattice-based constructions are currently important candidates for post-quantum cryptography. Unlike more widely used and known public-key schemes such as the RSA, Diffie-Hellman or Elliptic-Curve cryptosystems, which are easily attacked by a quantum computer, some lattice-based constructions appear to be resistant to attack by both classical and quantum computers. Furthermore, many lattice-based constructions are known to be secure under the assumption that certain well-studied computational lattice problems cannot be solved efficiently.\n\nIn 1996, Miklós Ajtai introduced the first lattice-based cryptographic construction whose security could be based on the hardness of well-studied lattice problems. Fundamentally, Ajtai's result was a worst-case to average-case reduction. I.e., he showed that a certain average-case lattice problem, known as Short Integer Solutions (SIS), is at least as hard to solve as a worst-case lattice problem. He then showed a cryptographic hash function whose security is equivalent to the computational hardness of SIS.\n\nIn 1998, Jeffrey Hoffstein (), Jill Pipher, and Joseph H. Silverman introduced a lattice-based public-key encryption scheme, known as NTRU. However, their scheme is not known to be at least as hard as solving a worst-case lattice problem.\n\nThe first lattice-based public-key encryption scheme with provable security was introduced by Oded Regev in 2005, together with the Learning with Errors problem (LWE). Since then, much follow-up work has focused on improving Regev's security proof and improving the efficiency of the original scheme. Much more work has been devoted to constructing additional cryptographic primitives based on LWE and related problems. In 2009, Craig Gentry introduced the first fully homomorphic encryption scheme, which was based on a lattice problem.\n\nA lattice formula_1 is the set of all integer linear combinations of basis vectors formula_2. I.e., \nformula_3\nFor example, formula_4 is a lattice, generated by the standard orthonormal basis for formula_5. Crucially, the basis for a lattice is not unique. For example, the vectors formula_6, formula_7, and formula_8 form an alternative basis for formula_9.\n\nThe most important lattice-based computational problem is the Shortest Vector Problem (SVP or sometimes GapSVP), which asks us to approximate the minimal Euclidean length of a non-zero lattice vector. This problem is thought to be hard to solve efficiently, even with approximation factors that are polynomial in formula_10, and even with a quantum computer. Many (though not all) lattice-based cryptographic constructions are known to be secure if SVP is in fact hard in this regime.\n\nOn June 25, 2009, Craig Gentry using lattice-based cryptography showed the first fully homomorphic encryption scheme as announced by IBM. His scheme supports evaluations of arbitrary depth circuits.\n\n\n\n\nLattice-based cryptographic constructions are the leading candidates for public-key post-quantum cryptography. Indeed, the main alternatives to lattice-based cryptography are schemes based on the hardness of factoring and related problems and schemes based on the hardness of the discrete logarithm and related problems. However, both factoring and the discrete logarithm are known to be solvable in polynomial time on a quantum computer. Furthermore, algorithms for factorization tend to yield algorithms for discrete logarithm, and vice versa. This further motivates the study of constructions based on alternative assumptions, such as the hardness of lattice problems.\n\nMany lattice-based cryptographic schemes are known to be secure assuming the \"worst-case\" hardness of certain lattice problems. I.e., if there exists an algorithm that can efficiently break the cryptographic scheme with non-negligible probability, then there exists an efficient algorithm that solves a certain lattice problem on any input. In contrast, cryptographic schemes based on, e.g., factoring would be broken if factoring were hard \"on an average input,<nowiki>\"</nowiki> even if factoring were in fact hard in the worst case. However, for the more efficient and practical lattice-based constructions (such as schemes based on NTRU and even schemes based on LWE with more aggressive parameters), such worst-case hardness results are not known. For some schemes, worst-case hardness results are known only for certain structured lattices.\n\nFor many cryptographic primitives, the only known constructions are based on lattices or closely related objects. These primitives include fully homomorphic encryption, indistinguishability obfuscation, cryptographic multilinear maps, and functional encryption.\n\n\n"}
{"id": "28726137", "url": "https://en.wikipedia.org/wiki?curid=28726137", "title": "Leelavati Award", "text": "Leelavati Award\n\nThe Leelavati Award is an award for outstanding contribution to public outreach in mathematics. It is named after the 12th-century mathematical treatise \"Lilavati\" devoted to arithmetic and algebra written by the Indian mathematician Bhāskara II, also known as Bhaskara Achārya. In the book the author posed, in verse form, a series of problems in (elementary) arithmetic to one Leelavati (perhaps his daughter) and followed them up with hints to solutions. This work appears to have been the main source of learning arithmetic and algebra in medieval India. The work was also translated into Persian and was influential in West Asia.\n\nThe Leelavati Prize was handed out for the first time at the closing ceremony of the International Congress of Mathematicians (ICM) 2010 in Hyderabad, India. Established by the Executive Organising Committee (EOC) of the ICM with the endorsement of the IMU Executive Committee (EC), the Leelavati Prize started out to be a one-time international award for outstanding public outreach work for mathematics. The award was so well received at the conferenece and in the mathematical press that the IMU EC decided to turn the prize into a recurring four-yearly award and the award ceremony a regular feature of every ICM closing ceremony.\n\nThe Leelavati prize is not intended to reward mathematical research but rather outreach activities in the broadest possible sense. It carries a cash prize of 1,000,000 Indian Rupees together with a citation and is sponsored by Infosys.\n\nWinners:\n\n\n"}
{"id": "2868583", "url": "https://en.wikipedia.org/wiki?curid=2868583", "title": "List of planar symmetry groups", "text": "List of planar symmetry groups\n\nThis article summarizes the classes of discrete symmetry groups of the Euclidean plane. The symmetry groups are named here by three naming schemes: International notation, orbifold notation, and Coxeter notation.\nThere are three kinds of symmetry groups of the plane:\n\nThere are two families of discrete two-dimensional point groups, and they are specified with parameter \"n\", which is the order of the group of the rotations in the group.\n\nThe 7 frieze groups, the two-dimensional line groups, with a direction of periodicity are given with five notational names. The Schönflies notation is given as infinite limits of 7 dihedral groups. The yellow regions represent the infinite fundamental domain in each.\n\nThe 17 wallpaper groups, with finite fundamental domains, are given by International notation, orbifold notation, and Coxeter notation, classified by the 5 Bravais lattices in the plane: square, oblique (parallelogrammatic), hexagonal (equilateral triangular), rectangular (centered rhombic), and rhombic (centered rectangular).\n\nThe \"p1\" and \"p2\" groups, with no reflectional symmetry, are repeated in all classes. The related pure reflectional Coxeter group are given with all classes except oblique.\n\n\n\n"}
{"id": "1449083", "url": "https://en.wikipedia.org/wiki?curid=1449083", "title": "Loop (graph theory)", "text": "Loop (graph theory)\n\nIn graph theory, a loop (also called a self-loop or a \"buckle\") is an edge that connects a vertex to itself. A simple graph contains no loops.\n\nDepending on the context, a graph or a multigraph may be defined so as to either allow or disallow the presence of loops (often in concert with allowing or disallowing multiple edges between the same vertices):\n\n\nIn a graph with one vertex, all edges must be loops. Such a graph is called a bouquet.\n\nFor an undirected graph, the degree of a vertex is equal to the number of adjacent vertices.\n\nA special case is a loop, which adds two to the degree. This can be understood by letting each connection of the loop edge count as its own adjacent vertex. In other words, a vertex with a loop \"sees\" itself as an adjacent vertex from \"both\" ends of the edge thus adding two, not one, to the degree.\n\nFor a directed graph, a loop adds one to the in degree and one to the out degree.\n\nLoops in Graph Theory\n\nLoops in Topology\n\n"}
{"id": "43794390", "url": "https://en.wikipedia.org/wiki?curid=43794390", "title": "Mathematical theory", "text": "Mathematical theory\n\nA mathematical theory is a subfield of mathematics that is an area of mathematical research. A theory can be a body of knowledge, and so in this sense a \"mathematical theory\" refers to an area of mathematical research. This is distinct from the idea of mathematical models. Branches of mathematics like group theory and number theory are examples of this.\n\n\n"}
{"id": "1251423", "url": "https://en.wikipedia.org/wiki?curid=1251423", "title": "Maude system", "text": "Maude system\n\nThe Maude system is an implementation of rewriting logic developed at SRI International. It is similar in its general approach to Joseph Goguen's OBJ3 implementation of equational logic, but based on rewriting logic rather than order-sorted equational logic, and with a heavy emphasis on powerful metaprogramming based on reflection.\n\nMaude is free software, and tutorials are available online.\n\nMaude modules (rewrite theories) consists of a term-language plus sets of equations and rewrite-rules. Terms in a rewrite theory are constructed using operators (functions taking 0 or more arguments of some sort, which return a term of a specific sort). Operators taking 0 arguments are considered constants, and one constructs their term-language by these simple constructs.\n\nThis rewrite theory specifies all the natural numbers. The sort saying is introduced, that there exists a sort called Nat (short for natural numbers), and below is the specification of how these terms are constructed. The operator s in Example 1 is the successor function representing the next natural number in the sequence of natural numbers i.e. s(N) := N + 1. s(0) is meant to represent the natural number 1 and so on. 0 is a constant, it takes no input parameter(s) but returns a Nat.\n\nIn Example 2 the + sign is introduced, meant to represent addition of natural numbers. Its definition is almost identical to the previous one, with input and output sorts, but there is a difference: its operator has underscores on each side. Maude lets the user specify whether or not operators are infix, postfix or prefix (default), this is done using underscores as place fillers for the input terms. So the + operator takes its input on each side making it an infix operator. Unlike our previous operator s which takes its input terms after the operator (prefix).\n\nThe three stars are Maude's rest-of-line-comments and lets the parser know that it can ignore the rest of the line (not part of the program), with parenthesis meaning section comments:\n\nThe extended Nat module also holds two variables and two sets of equations.\n\nWhen Maude \"executes\", it rewrites terms according to our specifications. And this is done with the statement\n\nor the shorter form\n\nFor this last statement to be used it is important that nothing is ambiguous. A small example from our rewrite theory representing the natural numbers:\n\nUsing the two equations in the rewrite theory NAT Maude was able to reduce our term into the desired term, the representation of the number two i.e. the second successor of 0. At that point no other application of the two equations were possible, so Maude halts. Maude rewrites terms according to the equations whenever there is a match between the closed terms that one tries to rewrite (or reduce) and the left hand side of an equation in our equation-set. A match in this context is a substitution of the variables in the left hand side of an equation which leaves it identical to the term that one tries to rewrite/reduce.\n\nTo illustrate it further one can look at the left hand side of the equations as Maude executes, reducing/rewriting the term.\n\ncan be applied to the term:\n\nsince the substitution:\n\nmakes them identical, and therefore it can be reduced/rewritten using this equation. After this equation has been applied to the term, one is left with the term:\n\nIf one takes a close look at that term, they will see that it has a fitting substitution with matches the first equation, at least parts of the term matches the first equation:\n\nsubstitution:\n\nThe second substitution and rewrite step rewrites an inner-term (the whole term does not match any of the equations but the inner term does). Then one ends up with our resulting term s(s(0)), and it can seem like a lot of hassle to add 1 + 1, but hopefully you will soon see the strength of this approach.\n\nAnother thing worth mentioning is that reduction/rewriting up to this point has taken something very important for granted, which is:\n\nThis cannot be taken for granted, and for a rewrite theory to be sane, one has to ensure that equational application is confluent and terminating. To prove that term-rewriting terminates is not possible in every instance, which we know from studying the halting problem. To be able to prove that term-rewriting (with regards to the equations) terminates, one can usually create some mapping between terms and the natural numbers, and show that application of the equations reduces the associated value of the term. Induction then proves that it is a halting process since the event of finding smaller natural numbers is a halting process. Naturally equation sets that can cause a term-rewrite to contain cycles will not be terminating. To prove confluence is another important aspect since a rewrite theory lacking this ability will be rather flawed. To prove that the equation set is confluent one has to prove that any application of the equations to any belonging term will lead to the same resulting term (termination is a prerequisite). Details for how to prove termination or confluence will not be given here, it just had to be mentioned, since this is where equations and rewrite-rules differ, which is the next topic of this short overview.\n\nUp to this point rewriting and reduction have been more or less the same thing, our first rewrite theory had no rewrite rules, still we rewrote terms, so it is time to illustrate what rewrite rules are, and how they differ from the equations we have seen so far (they do not differ much from equations naturally since we talk about the two concepts almost interchangeably).\n\nThe module presented so far NAT which represented the natural numbers and addition on its elements, is considered a functional module/rewrite theory, since it contains no rewrite rules. Such modules are often encapsuled with a fmod and endfm (instead of mod endm) to illustrate this fact. A functional module and its set of equations must be confluent and terminating since they build up the part of a rewrite theory that always should compute the same result, this will be clear once the rewrite rules come into play.\n\nThis module introduces two new sorts, and a set of rewrite rules. We also include our previous module, to illustrate how equations and rewrite rules differ. The rewrite rules is thought of as a set of legal state changes, so while equations hold the same meaning on both sides of the equality sign, rewrite rules do not (rewrite rules use a => token instead of an equality sign). You are still the same person after you're married (this is open for debate), but something has changed, your marital status at least. So this is illustrated by a rewrite rule, not an equation. Rewrite rules do not have to be confluent and terminating so it does matter a great deal what rules are chosen to rewrite the term. The rules are applied at \"random\" by the Maude system, meaning that you can not be sure that one rule is applied before another rule and so on. If an equation can be applied to the term, it will always be applied before any rewrite rule.\n\nA small example is in order:\n\nHere we tell the Maude system to rewrite our input term according to the rewrite theory, and we tell it to stop after 3 rewrite steps (remember that rewrite rules do not have to be terminating or confluent so an upper bound is not a bad idea), we just want see what sort of state we end up in after randomly choosing 3 matching rules. And as you can see the state this person ends up in might look a bit strange. (When you're married at the age of one you kind of stick out a bit in kindergarten I guess). It also says 4 rewrite steps, although we specifically stated an upper limit of 3 rewrite steps, this is because rewrite steps that are the result of applying equations does not count (they do not change the term, at least if the equations are sane). In this example one of the equations of the NAT module has been used to reduce the term 0 + s(0) to s(0), which accounts for the 4'th rewrite step.\n\nTo make this rewrite theory a bit less morbid, we can alter some of our rewrite rules a bit, and make them conditional rewrite rules, which basically means they have to fulfill some criteria to be applied to the term (other than just matching the left hand side of the rewrite rule).\n\nIt seems reasonable that you cannot die once you're dead, and you cannot marry as long as you are married. The leaning toothpicks (/\\) are supposed to resemble a logical AND which means that both criteria have to be fulfilled to be able to apply the rule (apart from term matching). Equations can also be made conditional in a similar manner.\n\nMaude sets out to solve a different set of problems than ordinary imperative languages like C, Java or Perl. It is a formal reasoning tool, which can help us verify that things are \"as they should\", and show us why they are not if this is the case. In other words, Maude lets us define formally what we mean by some concept in a very abstract manner (not concerning ourselves with how the structure is internally represented and so on), but we can describe what is thought to be the equal concerning our theory (equations) and what state changes it can go through (rewrite rules). This is extremely useful to validate security protocols and critical code. The Maude system has proved flaws in cryptography protocols by just specifying what the system can do (in a manner similar to the PERSON rewrite theory), and by looking for unwanted situations (states or terms that should not be possible to reach) the protocol can be shown to contain bugs, not programming bugs but situations happen that are hard to predict just by walking down the \"happy path\" as most developers do.\n\nWe can use Maude's built-in search to look for unwanted states, or it can be used to show that no such states can be reached.\n\nA small example from our PERSON module once again.\n\nHere the Natural numbers have a more familiar look (the basic Maude modules prelude.maude has been loaded, that can be done with the command \"in prelude\", or 1 can be replaced by s(0) and 2 by s(s(0)) if you don't want to load the default Maude-modules), naturally Maude has built-in support for ordinary structures like integers and floats and so on. The natural numbers are still members of the built-in sort Nat. We state that we want to search for a transition using one rewrite rule (=>1), which rewrites the first term into the other. The result of the investigation is not shocking but still, sometimes proving the obvious is all we can do. If we let Maude use more than one step we should see a different result:\n\nTo see what led us in that direction we can use the built in command show path which lets us know what rule applications led us to the resulting term. The token (=>+) means one or more rule application.\n\nAs we can see the rule application \"birthday\" followed by \"las-vegas\" got us where we wanted. Since all rewrite theories or modules with many possible rule applications will generate a huge tree of possible states to search for with the search command, this approach is not always the solution. We also have the ability to control what rule applications should be attempted at each step using meta-programming, due to the reflective property or rewriting logic.\n\n\n"}
{"id": "4449765", "url": "https://en.wikipedia.org/wiki?curid=4449765", "title": "Moritz Pasch", "text": "Moritz Pasch\n\nMoritz Pasch (8 November 1843, Breslau, Prussia (now Wrocław, Poland) – 20 September 1930, Bad Homburg, Germany) was a German mathematician of Jewish ancestry specializing in the foundations of geometry. He completed his Ph.D. at the University of Breslau at only 22 years of age. He taught at the University of Giessen, where he is known to have supervised 30 doctorates.\n\nIn 1882, Pasch published a book, \"Vorlesungen über neuere Geometrie\", calling for the grounding of Euclidean geometry in more precise primitive notions and axioms, and for greater care in the deductive methods employed to develop the subject. He drew attention to a number of heretofore unnoted tacit assumptions in Euclid's \"Elements\". He then argued that mathematical reasoning should not invoke the physical interpretation of the primitive terms, but should instead rely solely on formal manipulations justified by axioms. This book is the point of departure for:\n\nPasch is perhaps best remembered for Pasch's axiom:\n\nGiven three noncollinear points \"a, b, c\" and a line \"X\" not containing any of these points, if \"X\" includes a point between \"a\" and \"b\", then \"X\" also includes one and only one of the following: a point between \"a\" and \"c\", or a point between \"b\" and \"c\".\n\nIn other words, if a line crosses one side of a triangle, that line must also cross one of the two remaining sides of the same triangle.\n\n\n"}
{"id": "28081151", "url": "https://en.wikipedia.org/wiki?curid=28081151", "title": "Multiple representations (mathematics education)", "text": "Multiple representations (mathematics education)\n\nMultiple representations are ways to symbolize, to describe and to refer to the same mathematical entity. They are used to understand, to develop, and to communicate different mathematical features of the same object or operation, as well as connections between different properties. Multiple representations include graphs and diagrams, tables and grids, formulas, symbols, words, gestures, software code, videos, concrete models, physical and virtual manipulatives, pictures, and sounds. Representations are thinking tools for doing mathematics.\n\nThe use of multiple representations supports and requires tasks that involve decision-making and other problem-solving skills. The choice of which representation to use, the task of making representations given other representations, and the understanding of how changes in one representation affect others are examples of such mathematically sophisticated activities. Estimation, another complex task, can strongly benefit from multiple representations \n\nCurricula that support starting from conceptual understanding, then developing procedural fluency, for example, AIMS Foundation Activities, frequently use multiple representations.\n\nSupporting student use of multiple representations may lead to more open-ended problems, or at least accepting multiple methods of solutions and forms of answers. Project-based learning units, such as WebQuests, typically call for several representations. \n\nSome representations, such as pictures, videos and manipulatives, can motivate because of their richness, possibilities of play, technologies involved, or connections with interesting areas of life. Tasks that involve multiple representations can sustain intrinsic motivation in mathematics by supporting higher-order thinking and problem solving.\n\nMultiple representations may also remove some of the gender biases that exist in math classrooms. Explaining probability solely and only through baseball statistics may potentially alienate students who have no interest in sports. When showing a tie to real-life applications, teachers should choose representations that are varied and of interest to all genders and cultures.\n\nTasks that involve construction, use, and interpretation of multiple representations can lend themselves to rubric assessment and to other assessment types suitable for open-ended activities. For example, tapping into visualization for math problem solving manifests multiple representations. These multiple representations arise when each student uses their knowledge base, and experience to create a visualization of the problem domain on the way toward a solution. Since visualization can be categorized into two main areas, schematic or pictorial, most students will provide on or the other or sometimes both methods to represent the problem domain.\n\nComparison of the different visualization tools created by each student is an excellent example of multiple representations. Further, the instructor may glean from these examples elements which they incorporate into their grading rubric. In this way, it is the students that provide the examples and standards against which scoring is done. This crucial factor places each student on equal footing and links them directly with their performance in class.\n\nStudents with special needs may be weaker in their use of some of the representations. For these students, it may be especially important to use multiple representations for two purposes. First, including representations that currently work well for the student ensures the understanding of the current mathematical topic. Second, connections among multiple representations within the same topic strengthens overall skills in using all representations, even those currently problematic.\n\nIt is also helpful to ESL/ELL (English as a Second Language/English Language Learners) to use multiple representations. The more you can bring a concept to \"life\" in a visual way, the more likely the students will grasp what you are talking about. This is also important with younger students who may have not had a lot of experience/prior knowledge on the topics we are teaching.\n\nUsing multiple representations can help differentiate instruction by addressing different learning styles.\n\nVisual representations, manipulatives, gestures, and to some degree grids, can support qualitative reasoning about mathematics. Instead of only emphasizing computational skills, multiple representations can help students make the conceptual shift to the meaning and use of, and to develop algebraic thinking. By focusing more on the conceptual representations of algebraic problems students will become more capable problem solvers.\n\nNational Council of Teachers of Mathematics has a standard dealing with multiple representations. In part, it reads \n\"Instructional programs should enable all students to do the following:\n\nWhile there are many representations used in mathematics, the secondary curricula heavily favor numbers (often in tables), formulas, graphs and words.\n\nSeveral curricula use extensively developed systems of manipulatives and the corresponding representations. For example, Cuisinaire rods, Montessori beads , and Algebra Tiles , Base-10 blocks, counters\n\nUse of computer tools to create and to share mathematical representations opens several possibilities. It allows to link multiple representations dynamically. For example, changing a formula can instantly change the graph, the table of values, and the text read-out for the function represented in all these ways. Technology use can increase accuracy and speed of data collection and allow real-time visualization and experimentation. It also supports collaboration.\n\nComputer tools may be intrinsically interesting and motivating to students, and provide a familiar and comforting context students already use in their daily life.\n\nSpreadsheet software such as Excel, OpenOffice.org, Google Documents, is widely used in many industries, and showing students the use of applications can make math more realistic. Most spreadsheet programs provide dynamic links among formulas, grids and several types of graphs.\n\nCarnegie Learning curriculum is an example of emphasis on \nmultiple representations and use of computer tools. More specifically, Carnegie learning focuses the student not only on solving the real life scenarios presented in the text, but also promotes literacy through sentence writing and explanations of student thinking. In conjunction with the scenario based text Carnegie Learning provides a web based tutoring program called the \"Cognitive Tutor\" which uses data collected from each question a student answers to direct the student to areas where they need more help.\n\nGeoGebra is free software dynamically linking geometric constructions, graphs, formulas, and grids. It can be used in a browser and is light enough for older or low-end computers.\n\nProject Interactivate has many activities linking visual, verbal and numeric representations. There are currently 159 different activities available, in many areas of math, including numbers and operations, probability, geometry, algebra, statistics and modeling.\n\nAnother helpful tool for mathematicians, scientists, engineers is LaTeX. It is a typsetting program that allows you to create tables, figures, graphs etc. in order to give a precise visual of the problem being worked on. \n\nThere are concerns that technology for working with multiple representations can become a distraction from mathematical content, and an end in itself .\n\nCare should be taken that informal representations do not prevent students from progressing toward formal, symbolic mathematics .\n"}
{"id": "1569373", "url": "https://en.wikipedia.org/wiki?curid=1569373", "title": "Peter Simons (academic)", "text": "Peter Simons (academic)\n\nPeter M. Simons, (born 23 March 1950) is a retired professor of philosophy at Trinity College Dublin.\n\nSimons studied at the University of Manchester, and has held teaching posts at the University of Bolton, the University of Salzburg, where he is Honorary Professor of Philosophy, and the University of Leeds. He has been President of the European Society for Analytic Philosophy and is current director of the Franz Brentano Foundation.\n\nHis research interests include metaphysics and ontology, the history of logic, the history of Central European Philosophy, particularly in Austria and Poland in the 19th and 20th centuries, and the application of metaphysics to engineering and other non-philosophical disciplines. He is the author of two books and over 200 articles. He is currently working on a project supported by the British Academy to chart the metaphysics of quantity.\n\n\n\n"}
{"id": "1487868", "url": "https://en.wikipedia.org/wiki?curid=1487868", "title": "Poincaré metric", "text": "Poincaré metric\n\nIn mathematics, the Poincaré metric, named after Henri Poincaré, is the metric tensor describing a two-dimensional surface of constant negative curvature. It is the natural metric commonly used in a variety of calculations in hyperbolic geometry or Riemann surfaces.\n\nThere are three equivalent representations commonly used in two-dimensional hyperbolic geometry. One is the Poincaré half-plane model, defining a model of hyperbolic space on the upper half-plane. The Poincaré disk model defines a model for hyperbolic space on the unit disk. The disk and the upper half plane are related by a conformal map, and isometries are given by Möbius transformations. A third representation is on the punctured disk, where relations for q-analogues are sometimes expressed. These various forms are reviewed below.\n\nA metric on the complex plane may be generally expressed in the form\n\nwhere λ is a real, positive function of formula_2 and formula_3. The length of a curve γ in the complex plane is thus given by \n\nThe area of a subset of the complex plane is given by\n\nwhere formula_6 is the exterior product used to construct the volume form. The determinant of the metric is equal to formula_7, so the square root of the determinant is formula_8. The Euclidean volume form on the plane is formula_9 and so one has\n\nA function formula_11 is said to be the potential of the metric if\n\nThe Laplace–Beltrami operator is given by\n\nThe Gaussian curvature of the metric is given by\n\nThis curvature is one-half of the Ricci scalar curvature.\n\nIsometries preserve angles and arc-lengths. On Riemann surfaces, isometries are identical to changes of coordinate: that is, both the Laplace–Beltrami operator and the curvature are invariant under isometries. Thus, for example, let \"S\" be a Riemann surface with metric formula_15 and \"T\" be a Riemann surface with metric formula_16. Then a map\n\nwith formula_18 is an isometry if and only if it is conformal and if\n\nHere, the requirement that the map is conformal is nothing more than the statement\n\nthat is,\n\nThe Poincaré metric tensor in the Poincaré half-plane model is given on the upper half-plane H as\n\nwhere we write formula_23\nThis metric tensor is invariant under the action of SL(2,R). That is, if we write\n\nfor formula_25 then we can work out that\n\nfor formula_27\n\nAnother interesting form of the metric can be given in terms of the cross-ratio. Given any four points formula_28 and formula_29 in the compactified complex plane formula_30 the cross-ratio is defined by\n\nThen the metric is given by\n\nHere, formula_33 and formula_34 are the endpoints, on the real number line, of the geodesic joining formula_35 and formula_36. These are numbered so that formula_35 lies in between formula_33 and formula_36.\n\nThe geodesics for this metric tensor are circular arcs perpendicular to the real axis (half-circles whose origin is on the real axis) and straight vertical lines ending on the real axis.\n\nThe upper half plane can be mapped conformally to the unit disk with the Möbius transformation\n\nwhere \"w\" is the point on the unit disk that corresponds to the point \"z\" in the upper half plane. In this mapping, the constant \"z\" can be any point in the upper half plane; it will be mapped to the center of the disk. The real axis formula_41 maps to the edge of the unit disk formula_42 The constant real number formula_43 can be used to rotate the disk by an arbitrary fixed amount.\n\nThe canonical mapping is\n\nwhich takes \"i\" to the center of the disk, and \"0\" to the bottom of the disk.\n\nThe Poincaré metric tensor in the Poincaré disk model is given on the open unit disk \n\nby\n\nThe volume element is given by\n\nThe Poincaré metric is given by\n\nfor formula_49\n\nThe geodesics for this metric tensor are circular arcs whose endpoints are orthogonal to the boundary of the disk.\n\nA second common mapping of the upper half-plane to a disk is the q-mapping\n\nwhere \"q\" is the nome and τ is the half-period ratio:\nIn the notation of the previous sections, τ is the coordinate in the upper half-plane formula_52. The mapping is to the punctured disk, because the value \"q\"=0 is not in the image of the map.\n\nThe Poincaré metric on the upper half-plane induces a metric on the q-disk\n\nThe potential of the metric is\n\nThe Poincaré metric is distance-decreasing on harmonic functions. This is an extension of the Schwarz lemma, called the Schwarz–Ahlfors–Pick theorem.\n\n\n"}
{"id": "7365499", "url": "https://en.wikipedia.org/wiki?curid=7365499", "title": "Powertype (UML)", "text": "Powertype (UML)\n\nIn the Unified Modeling Language 1.x, powertype is a keyword for a specific UML stereotype, and applies to a class or dependency. Powertype shows a classifier whose instances (objects) are children of the given parent.\n\nIn UML 2.x, a powertype is a metaclass whose instances are subclasses of a given class. The stereotype has been removed and the powertype is now indicated by typing the generalization set.\n\nIn the UML Superstructure 2.4.1 Specification Document the following definition is given:\n\nReferences:\n"}
{"id": "409348", "url": "https://en.wikipedia.org/wiki?curid=409348", "title": "Primorial prime", "text": "Primorial prime\n\nIn mathematics, a primorial prime is a prime number of the form \"p\"# ± 1, where \"p\"# is the primorial of \"p\" (the product of the first \"n\" primes).\n\nPrimality tests show that\n\nThe first term of the second sequence is 0, because \"p\"# = 1 is the empty product, and thus \"p\"# + 1 = 2, which is prime. Similarly, the first term of the first sequence is not 1, as \"p\"# = 2, and 2 − 1 = 1 is not prime.\n\nThe first few primorial primes are\n\n, the largest known primorial prime is 1098133# − 1 (\"n\" = 85586) with 476,311 digits, found by the PrimeGrid project.\n\nEuclid's proof of the infinitude of the prime numbers is commonly misinterpreted as defining the primorial primes, in the following manner:\n\n\n"}
{"id": "31010487", "url": "https://en.wikipedia.org/wiki?curid=31010487", "title": "Rabatment of the rectangle", "text": "Rabatment of the rectangle\n\nRabatment of the rectangle is a compositional technique used as an aid for the placement of objects or the division of space within a rectangular frame, or as an aid for the study of art.\n\nEvery rectangle contains two implied squares, each consisting of a short side of the rectangle, an equal length along each longer side, and an imaginary fourth line parallel to the short side. The process of mentally rotating the short sides onto the long ones is called \"rabatment\", and often the imaginary fourth line is called \"the rabatment\".\n\nAlso known as rebatement and rabattement, \"rabatment\" means the rotation of a plane into another plane about their line of intersection, as in closing an open hinge. In two dimensions, it means to rotate a line about a point until the line coincides with another sharing the same point. The term is used in geometry, art and architecture.\n\nThere is no absolute explanation of the mechanism of this method, but there are various theories. One argument is that squares are such a simple, primal geometric shape that the brain automatically looks for them, mentally completing this rabatment whether it is made explicit or not. When a composition uses elements of the scene to match, the square feels complete in itself, producing a feeling of harmony.\n\nRenaissance artists used rabatment as a foundation to art and architectural works, but the rabatment can be observed in art taken from almost any period.\n\nAs one of many composition techniques, rabatment of the rectangle can be used to inform the positioning of elements within the rectangle. There is no hard and fast rule regarding such positioning; a composition can have a sense of dynamic unrest or a sense of equilibrium relative to important lines such as ones taken from rabatment or from the rule of thirds, or from nodal points such as the \"eyes of a rectangle\"—the four intersections derived from the rule of thirds. Primary image elements can be positioned within one of the two rabatment squares to define the center of interest, and secondary image elements can be placed outside of a rabatment square.\n\nThe concept of rabatment can be applied to rectangles of any proportion. For rectangles with a 3:2 ratio (as in 35mm film in still photography), it happens that the rabatment lines are exactly matched to the rule of thirds lines.\n\nIn a horizontally-aligned rectangle, there is one implied square for the left side and one for the right; for a vertically-aligned rectangle, there are upper and lower squares. If the long sides of the rectangle are exactly twice the length of the short, this line is right in the middle. With longer-proportioned rectangles, the squares don't overlap, but with shorter-proportioned ones, they do. In Western cultures that read left to right, attention is often focused inside the left-hand rabatment, or on the line it forms at the right-hand side of the image.\n\nWhen rabatment is used with one side of a golden rectangle, and then iteratively applied to the left-over rectangle, the resulting \"whirling rectangles\" describe the golden spiral.\n\n"}
{"id": "10151936", "url": "https://en.wikipedia.org/wiki?curid=10151936", "title": "Random regular graph", "text": "Random regular graph\n\nA random \"r\"-regular graph is a graph selected from formula_1, which denotes the probability space of all \"r\"-regular graphs on \"n\" vertices, where 3 ≤ \"r\" < \"n\" and \"nr\" is even. It is therefore a particular kind of random graph, but the regularity restriction significantly alters the properties that will hold, since most graphs are not regular.\n\nAs with more general random graphs, it is possible to prove that certain properties of random \"r\"-regular graphs hold asymptotically almost surely. In particular, for formula_2, a random \"r\"-regular graph of large size is asymptotically almost surely \"r\"-connected. In other words, although \"r\"-regular graphs with connectivity less than \"r\" exist, the probability of selecting such a graph tends to 0 as \"n\" increases. \n\nIf formula_3 > 0 is a positive constant, and \"d\" is the least integer satisfying\n\nformula_4\n\nthen, asymptotically almost surely, a random \"r\"-regular graph has diameter at most \"d\". There is also a (more complex) lower bound on the diameter of \"r\"-regular graphs, so that almost all \"r\"-regular graphs (of the same size) have almost the same diameter.\n\nThe distribution of the number of short cycles is also known: for fixed \"m\" ≥ 3, let \"Y\",\"Y\",…,\"Y\" be the number of cycles of lengths up to \"m\". Then the \"Y\" are asymptotically independent Poisson random variables with means\n\nformula_5\n\nIt is non-trivial to implement the random selection of \"r\"-regular graphs efficiently and in an unbiased way, since most graphs are not regular. The \"pairing model\" (also \"configuration model\") is a method which takes \"nr\" points, and partitions them into \"n\" buckets with \"r\" points in each of them. Taking a random matching of the \"nr\" points, and then contracting the \"r\" points in each bucket into a single vertex, yields an \"r\"-regular graph or multigraph. If this object has no multiple edges or loops (i.e. it is a graph), then it is the required result. If not, a restart is required.\n\nA refinement of this method was developed by Brendan McKay and Nicholas Wormald.\n"}
{"id": "11418033", "url": "https://en.wikipedia.org/wiki?curid=11418033", "title": "Robbins constant", "text": "Robbins constant\n\nIn geometry, the Robbins' constant, named after David P. Robbins, is the average distance between two points selected at random within a unit cube. Robbins' constant can be expressed as \n\nIts numerical value is approximately \n"}
{"id": "18727637", "url": "https://en.wikipedia.org/wiki?curid=18727637", "title": "Room square", "text": "Room square\n\nA Room square, named after Thomas Gerald Room, is an \"n\" × \"n\" array filled with \"n\" + 1 different symbols in such a way that:\n\nAn example, a Room square of order seven, if the set of symbols is integers from 0 to 7:\n\nIt is known that a Room square (or squares) exist if and only if \"n\" is odd but not 3 or 5.\n\nThe order-7 Room square was used by Robert Richard Anstice to provide additional solutions to Kirkman's schoolgirl problem in the mid-19th century, and Anstice also constructed an infinite family of Room squares, but his constructions did not attract attention. Thomas Gerald Room reinvented Room squares in a note published in 1955, and they came to be named after him. In his original paper on the subject, Room observed that \"n\" must be odd and unequal to 3 or 5, but it was not shown that these conditions are both necessary and sufficient until the work of W. D. Wallis in 1973.\n\nPre-dating Room's paper, Room squares had been used by the directors of duplicate bridge tournaments in the construction of the tournaments. In this application they are known as Howell rotations. The columns of the square represent tables, each of which holds a deal of the cards that is played by each pair of teams that meet at that table. The rows of the square represent rounds of the tournament, and the numbers within the cells of the square represent the teams that are scheduled to play each other at the table and round represented by that cell.\n\nArchbold and Johnson used Room squares to construct experimental designs.\n\nThere are connections between Room squares and other mathematical objects including quasigroups, Latin squares, graph factorizations, and Steiner triple systems.\n\n\n"}
{"id": "27838", "url": "https://en.wikipedia.org/wiki?curid=27838", "title": "Sequence", "text": "Sequence\n\nIn mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed. Like a set, it contains members (also called \"elements\", or \"terms\"). The number of elements (possibly infinite) is called the \"length\" of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order matters. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first \"n\" natural numbers (for a sequence of finite length \"n\"). The position of an element in a sequence is its \"rank\" or \"index\"; it is the natural number from which the element is the image. It depends on the context or a specific convention, if the first element has index 0 or 1. When a symbol has been chosen for denoting a sequence, the \"n\"th element of the sequence is denoted by this symbol with \"n\" as subscript; for example, the \"n\"th element of the Fibonacci sequence is generally denoted \"F\".\n\nFor example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be \"finite\", as in these examples, or \"infinite\", such as the sequence of all even positive integers (2, 4, 6, ...). In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.\nA sequence can be thought of as a list of elements with a particular order. Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. In particular, sequences are the basis for series, which are important in differential equations and analysis. Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.\n\nThere are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. One way to specify a sequence is to list the elements. For example, the first four odd numbers form the sequence (1, 3, 5, 7). This notation can be used for infinite sequences as well. For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...). Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements. Other ways to denote a sequence are discussed after the examples.\n\nThe prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves. Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...). The prime numbers are widely used in mathematics and specifically in number theory.\n\nThe Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements. The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).\n\nFor a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.\n\nOther examples of sequences include ones made up of rational numbers, real numbers, and complex numbers. The sequence (.9, .99, .999, .9999, ...) approaches the number 1. In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion. For instance, is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). A related sequence is the sequence of decimal digits of , i.e. (3, 1, 4, 1, 5, 9, ...). This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.\n\nOther notations can be useful for sequences whose pattern cannot be easily guessed, or for sequences that do not have a pattern such as the digits of . One such notation is to write down a general formula for computing the \"n\"th term as a function of \"n\", enclose it in parentheses, and include a subscript indicating the range of values that \"n\" can take. For example, in this notation the sequence of even numbers could be written as formula_1. The sequence of squares could be written as formula_2. The variable \"n\" is called an index, and the set of values that it can take is called the index set.\n\nIt is often useful to combine this notation with the technique of treating the elements of a sequence as variables. This yields expressions like formula_3, which denotes a sequence whose \"n\"th element is given by the variable formula_4. For example:\nNote that we can consider multiple sequences at the same time by using different variables; e.g. formula_6 could be a different sequence than formula_3. We can even consider a sequence of sequences: formula_8 denotes a sequence whose \"m\"th term is the sequence formula_9.\n\nAn alternative to writing the domain of a sequence in the subscript is to indicate the range of values that the index can take by listing its highest and lowest legal values. For example, the notation formula_10 denotes the ten-term sequence of squares formula_11. The limits formula_12 and formula_13 are allowed, but they do not represent valid values for the index, only the supremum or infimum of such values, respectively. For example, the sequence formula_14 is the same as the sequence formula_3, and does not contain an additional term \"at infinity\". The sequence formula_16 is a bi-infinite sequence, and can also be written as formula_17.\n\nIn cases where the set of indexing numbers is understood, the subscripts and superscripts are often left off. That is, one simply writes formula_18 for an arbitrary sequence. Often, the index \"k\" is understood to run from 1 to ∞. However, sequences are frequently indexed starting from zero, as in\nIn some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred. In these cases the index set may be implied by a listing of the first few abstract elements. For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.\n\n\nMoreover, the subscripts and superscripts could have been left off in the third, fourth, and fifth notations, if the indexing set was understood to be the natural numbers. Note that in the second and third bullets, there is a well-defined sequence formula_25, but it is not the same as the sequence denoted by the expression.\n\nSequences whose elements are related to the previous elements in a straightforward way are often defined using recursion. This is in contrast to the definition of sequences of elements as functions of their positions.\n\nTo define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it. In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule. The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions. Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.\n\nThe Fibonacci sequence can be defined using a recursive rule along with two initial elements. The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.\nThe first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34. A more complicated example of a sequence that is defined recursively is Recaman's sequence. We can define Recaman's sequence by\n\nNot all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated. For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).\n\nMany sequences have the property that each element of a sequence can be computed from the previous element. In this case, there is some function \"f\" such that for all \"n\", formula_32.\n\nThere are many different notions of sequences in mathematics, some of which (\"e.g.\", exact sequence) are not covered by the definitions and notations introduced below.\n\nFor the purposes of this article, we define a sequence to be a function whose domain is an interval of integers. This definition covers several different uses of the word \"sequence\", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions). However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers. The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice. In some contexts, to shorten exposition, the codomain of the sequence is fixed by context, for example by requiring it to be the set R of real numbers, the set C of complex numbers, or a topological space.\n\nAlthough sequences are a type of function, they are usually distinguished notationally from functions in that the input is written as a subscript rather than in parentheses, i.e. \"a\" rather than \"f\"(\"n\"). There are terminological differences as well: the value of a sequence at the input 1 is called the \"first element\" of the sequence, the value at 2 is called the \"second element\", etc. Also, while a function abstracted from its input is usually denoted by a single letter, e.g. \"f\", a sequence abstracted from its input is usually written by a notation such as formula_33, or just as formula_34. Here \"A\" is the domain, or index set, of the sequence.\n\nSequences and their limits (see below) are important concepts for studying topological spaces. An important generalization of sequences is the concept of nets. A net is a function from a (possibly uncountable) directed set to a topological space. The notational conventions for sequences normally apply to nets as well.\n\nThe length of a sequence is defined as the number of terms in the sequence.\n\nA sequence of a finite length \"n\" is also called an \"n\"-tuple. Finite sequences include the empty sequence ( ) that has no elements.\nNormally, the term \"infinite sequence\" refers to a sequence that is infinite in one direction, and finite in the other—the sequence has a first element, but no final element. Such a sequence is called a singly infinite sequence or a one-sided infinite sequence when disambiguation is necessary. In contrast, a sequence that is infinite in both directions—i.e. that has neither a first nor a final element—is called a bi-infinite sequence, two-way infinite sequence, or doubly infinite sequence. A function from the set Z of \"all\" integers into a set, such as for instance the sequence of all even integers ( …, −4, −2, 0, 2, 4, 6, 8… ), is bi-infinite. This sequence could be denoted formula_35.\n\nA sequence is said to be \"monotonically increasing\", if each term is greater than or equal to the one before it. For example, the sequence formula_36 is monotonically increasing if and only if \"a\" formula_37 \"a\" for all \"n\" ∈ N. If each consecutive term is strictly greater than (>) the previous term then the sequence is called strictly monotonically increasing. A sequence is monotonically decreasing, if each consecutive term is less than or equal to the previous one, and strictly monotonically decreasing, if each is strictly less than the previous. If a sequence is either increasing or decreasing it is called a monotone sequence. This is a special case of the more general notion of a monotonic function.\n\nThe terms nondecreasing and nonincreasing are often used in place of \"increasing\" and \"decreasing\" in order to avoid any possible confusion with \"strictly increasing\" and \"strictly decreasing\", respectively.\n\nIf the sequence of real numbers (\"a\") is such that all the terms are less than some real number \"M\", then the sequence is said to be bounded from above. In other words, this means that there exists \"M\" such that for all \"n\", \"a\" ≤ \"M\". Any such \"M\" is called an \"upper bound\". Likewise, if, for some real \"m\", \"a\" ≥ \"m\" for all \"n\" greater than some \"N\", then the sequence is bounded from below and any such \"m\" is called a \"lower bound\". If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.\n\nA subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements. For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...). The positions of some elements change when other elements are deleted. However, the relative positions are preserved.\n\nFormally, a subsequence of the sequence formula_3 is any sequence of the form formula_39, where formula_40 is a strictly increasing sequence of positive integers.\n\nSome other types of sequences that are easy to define include:\n\nAn important property of a sequence is \"convergence\". If a sequence converges, it converges to a particular value known as the \"limit\". If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.\n\nInformally, a sequence has a limit if the elements of the sequence become closer and closer to some value formula_41 (called the limit of the sequence), and they become and remain \"arbitrarily\" close to formula_41, meaning that given a real number formula_43 greater than zero, all but a finite number of the elements of the sequence have a distance from formula_41 less than formula_43. \n\nFor example, the sequence formula_46 shown to the right converges to the value 0. On the other hand, the sequences formula_47 (which begins 1, 8, 27, …) and formula_48 (which begins -1, 1, -1, 1, …) are both divergent. \n\nIf a sequence converges, then the value it converges to is unique. This value is called the limit of the sequence. The limit of a convergent sequence formula_34 is normally denoted formula_50. If formula_34 is a divergent sequence, then the expression formula_50 is meaningless.\n\nA sequence of real numbers formula_34 converges to a real number formula_41 if, for all formula_55, there exists a natural number formula_56 such that for all formula_57 we have formula_58\n\nIf formula_34 is a sequence of complex numbers rather than a sequence of real numbers, this last formula can still be used to define convergence, with the provision that formula_60 denotes the complex modulus, i.e. formula_61. If formula_34 is a sequence of points in a metric space, then the formula can be used to define convergence, if the expression formula_63 is replaced by the expression formula_64, which denotes the distance between formula_4 and formula_41.\n\nIf formula_34 and formula_68 are convergent sequences, then the following limits exist, and can be computed as follows:\n\n\nMoreover:\n\nA Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is \"Cauchy characterization of convergence for sequences\":\nIn contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined by\n\"x\" = 1 and \"x\" = \nis Cauchy, but has no rational limit, cf. . More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.\n\nMetric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.\n\nIn calculus, it is common to define notation for sequences which do not converge in the sense discussed above, but which instead become and remain arbitrarily large, or become and remain arbitrarily negative. If formula_4 becomes arbitrarily large as formula_87, we write\nIn this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is .\n\nIf formula_4 becomes arbitrarily negative (i.e. negative and large in magnitude) as formula_87, we write\nand say that the sequence diverges or converges to negative infinity.\n\nA series is, informally speaking, the sum of the terms of a sequence. That is, it is an expression of the form formula_92 or formula_93, where formula_34 is a sequence of real or complex numbers. The partial sums of a series are the expressions resulting from replacing the infinity symbol with a finite number, i.e. the \"N\"th partial sum of the series formula_92 is the number\nThe partial sums themselves form a sequence formula_97, which is called the sequence of partial sums of the series formula_92. If the sequence of partial sums converges, then we say that the series formula_92 is convergent, and the limit formula_100 is called the value of the series. The same notation is used to denote a series and its value, i.e. we write formula_101.\n\nSequences play an important role in topology, especially in the study of metric spaces. For instance:\n\nSequences can be generalized to nets or filters. These generalizations allow one to extend some of the above theorems to spaces without metrics.\n\nThe topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.\n\nMore formally, given a sequence of spaces formula_102, the product space\n\nis defined as the set of all sequences formula_104 such that for each \"i\", formula_105 is an element of formula_106. The canonical projections are the maps \"p\" : \"X\" → \"X\" defined by the equation formula_107. Then the product topology on \"X\" is defined to be the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections \"p\" are continuous. The product topology is sometimes called the Tychonoff topology.\n\nIn analysis, when talking about sequences, one will generally consider sequences of the form\nwhich is to say, infinite sequences of elements indexed by natural numbers.\n\nIt may be convenient to have the sequence start with an index different from 1 or 0. For example, the sequence defined by \"x\" = 1/log(\"n\") would be defined only for \"n\" ≥ 2. When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given \"N\".\n\nThe most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers. This type can be generalized to sequences of elements of some vector space. In analysis, the vector spaces considered are often function spaces. Even more generally, one can study sequences with elements in some topological space.\n\nA sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.\n\nThe most important sequences spaces in analysis are the ℓ spaces, consisting of the \"p\"-power summable sequences, with the \"p\"-norm. These are special cases of L spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted \"c\" and \"c\", with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.\n\nSequences over a field may also be viewed as vectors in a vector space. Specifically, the set of \"F\"-valued sequences (where \"F\" is a field) is a function space (in fact, a product space) of \"F\"-valued functions over the set of natural numbers.\n\nAbstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.\n\nIf \"A\" is a set, the free monoid over \"A\" (denoted \"A\", also called Kleene star of \"A\") is a monoid containing all the finite sequences (or strings) of zero or more elements of \"A\", with the binary operation of concatenation. The free semigroup \"A\" is the subsemigroup of \"A\" containing all elements except the empty sequence.\n\nIn the context of group theory, a sequence\nof groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:\n\nNote that the sequence of groups and homomorphisms may be either finite or infinite.\n\nA similar definition can be made for certain other algebraic structures. For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.\n\nIn homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations. Spectral sequences are a generalization of exact sequences, and since their introduction by , they have become an important research tool, particularly in homotopy theory.\n\nAn ordinal-indexed sequence is a generalization of a sequence. If α is a limit ordinal and \"X\" is a set, an α-indexed sequence of elements of \"X\" is a function from α to \"X\". In this terminology an ω-indexed sequence is an ordinary sequence.\n\nAutomata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ. Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a \"word\" (or input word). The sequence of states encountered by the automaton when processing a word is called a \"run\". A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter. This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.\n\nInfinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science. They are often referred to simply as \"sequences\" or \"streams\", as opposed to finite \"strings\". Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}). The set \"C\" = {0, 1} of all infinite binary sequences is sometimes called the Cantor space.\n\nAn infinite binary sequence can represent a formal language (a set of strings) by setting the \"n\" th bit of the sequence to 1 if and only if the \"n\" th string (in shortlex order) is in the language. This representation is useful in the diagonalization method for proofs.\n\n\n\n\n\n\n"}
{"id": "36278557", "url": "https://en.wikipedia.org/wiki?curid=36278557", "title": "Singular integral operators of convolution type", "text": "Singular integral operators of convolution type\n\nIn mathematics, singular integral operators of convolution type are the singular integral operators that arise on R and T through convolution by distributions; equivalently they are the singular integral operators that commute with translations. The classical examples in harmonic analysis are the harmonic conjugation operator on the circle, the Hilbert transform on the circle and the real line, the Beurling transform in the complex plane and the Riesz transforms in Euclidean space. The continuity of these operators on \"L\" is evident because the Fourier transform converts them into multiplication operators. Continuity on \"L\" spaces was first established by Marcel Riesz. The classical techniques include the use of Poisson integrals, interpolation theory and the Hardy–Littlewood maximal function. For more general operators, fundamental new techniques, introduced by Alberto Calderón and Antoni Zygmund in 1952, were developed by a number of authors to give general criteria for continuity on \"L\" spaces. This article explains the theory for the classical operators and sketches the subsequent general theory.\n\nThe theory for \"L\" functions is particularly simple on the circle. If \"f\" ∈ \"L\"(T), then it has a Fourier series expansion\n\nHardy space H(T) consists of the functions for which the negative coefficients vanish, \"a\" = 0 for \"n\" < 0. These are precisely the square-integrable functions that arise as boundary values of holomorphic functions in the open unit disk. Indeed, \"f\" is the boundary value of the function\n\nin the sense that the functions\n\ndefined by the restriction of \"F\" to the concentric circles |\"z\"| = \"r\", satisfy\n\nThe orthogonal projection \"P\" of \"L\"(T) onto H(T) is called the Szegő projection. It is a bounded operator on \"L\"(T) with operator norm 1. By Cauchy's theorem\n\nthen \"T\" defines a bounded operator on L for 1 < \"p\" < ∞ and a continuous operator from L into functions of weak type L.\n\nIn fact by the Marcinkiewicz interpolation argument and duality, it suffices to check that if \"f\" is smooth of compact support then\n\nTake a Calderón−Zygmund decomposition of \"f\" as above\n\nwith intervals \"J\" and with α = λμ, where μ > 0. Then\n\nThe term for \"g\" can be estimated using Chebychev's inequality:\n\nIf \"J\"* is defined to be the interval with the same centre as \"J\" but twice the length, the term for \"b\" can be broken up into two parts:\n\nThe second term is easy to estimate:\n\nTo estimate the first term note that\n\nThus by Chebychev's inequality:\n\nBy construction the integral of \"b\" over \"J\" is zero. Thus, if \"y\" is the midpoint of \"J\", then by Hörmander's condition:\n\nHence\n\nCombining the three estimates gives\n\nThe constant is minimized by taking\n\nThe Markinciewicz interpolation argument extends the bounds to any L with 1 < \"p\" < 2 as follows. Given \"a\" > 0, write\n\nwhere \"f\" = \"f\" if |\"f\"| < \"a\" and 0 otherwise and \"f\" = \"f\" if |\"f\"| ≥ \"a\" and 0 otherwise. Then by Chebychev's inequality and the weak type L inequality above\n\nHence\n\nContinuity of the norms can be shown by a more refined argument or follows from the Riesz–Thorin interpolation theorem.\n\n"}
{"id": "10274608", "url": "https://en.wikipedia.org/wiki?curid=10274608", "title": "Stress majorization", "text": "Stress majorization\n\nStress majorization is an optimization strategy used in multidimensional scaling (MDS) where, for a set of \"n\" \"m\"-dimensional data items, a configuration \"X\" of \"n\" points in \"r(«m)\"-dimensional space is sought that minimizes the so-called \"stress\" function formula_1. Usually \"r\" is 2 or 3, i.e. the \"(\"n\" x \"r\")\" matrix \"X\" lists points in 2- or 3-dimensional Euclidean space so that the result may be visualised (i.e. an MDS plot). The function formula_2 is a cost or loss function that measures the squared differences between ideal (formula_3-dimensional) distances and actual distances in \"r\"-dimensional space. It is defined as:\n\nwhere formula_5 is a weight for the measurement between a pair of points formula_6, formula_7 is the euclidean distance between formula_8 and formula_9 and formula_10 is the ideal distance between the points (their separation) in the formula_3-dimensional data space. Note that formula_12 can be used to specify a degree of confidence in the similarity between points (e.g. 0 can be specified if there is no information for a particular pair).\n\nA configuration formula_13 which minimizes formula_1 gives a plot in which points that are close together correspond to points that are also close together in the original formula_3-dimensional data space.\n\nThere are many ways that formula_16 could be minimized. For example, Kruskal recommended an iterative steepest descent approach. However, a significantly better (in terms of guarantees on, and rate of, convergence) method for minimizing stress was introduced by Jan de Leeuw. De Leeuw's \"iterative majorization\" method at each step minimizes a simple convex function which both bounds formula_2 from above and touches the surface of formula_2 at a point formula_19, called the \"supporting point\". In convex analysis such a function is called a \"majorizing\" function. This iterative majorization process is also referred to as the SMACOF algorithm (\"Scaling by MAjorizing a COmplicated Function\").\n\nThe stress function formula_2 can be expanded as follows:\n\nNote that the first term is a constant formula_22 and the second term is quadratic in X (i.e. for the Hessian matrix V the second term is equivalent to trformula_23) and therefore relatively easily solved. The third term is bounded by:\n\nwhere formula_25 has:\n\nand formula_28 for formula_29\n\nand formula_30.\n\nProof of this inequality is by the Cauchy-Schwarz inequality, see Borg (pp. 152–153).\n\nThus, we have a simple quadratic function formula_31 that majorizes stress:\n\nThe iterative minimization procedure is then:\n\n\nThis algorithm has been shown to decrease stress monotonically (see de Leeuw).\n\nStress majorization and algorithms similar to SMACOF also have application in the field of graph drawing. That is, one can find a reasonably aesthetically appealing layout for a network or graph by minimizing a stress function over the positions of the nodes in the graph. In this case, the formula_10 are usually set to the graph-theoretic distances between nodes \"i\" and \"j\" and the weights formula_12 are taken to be formula_39. Here, formula_40 is chosen as a trade-off between preserving long- or short-range ideal distances. Good results have been shown for formula_41.\n"}
{"id": "21950759", "url": "https://en.wikipedia.org/wiki?curid=21950759", "title": "Wiles's proof of Fermat's Last Theorem", "text": "Wiles's proof of Fermat's Last Theorem\n\nWiles's proof of Fermat's Last Theorem is a proof by British mathematician Andrew Wiles of a special case of the modularity theorem for elliptic curves. Together with Ribet's theorem, it provides a proof for Fermat's Last Theorem. Both Fermat's Last Theorem and the modularity theorem were almost universally considered inaccessible to proof by contemporaneous mathematicians, meaning that they were believed to be impossible to prove using current knowledge.\n\nWiles first announced his proof on Wednesday 23 June 1993 at a lecture in Cambridge entitled \"Modular Forms, Elliptic Curves and Galois Representations\". However, in September 1993 the proof was found to contain an error. One year later on Monday 19 September 1994, in what he would call \"the most important moment of [his] working life\", Wiles stumbled upon a revelation that allowed him to correct the proof to the satisfaction of the mathematical community. The corrected proof was published in 1995.\n\nWiles' proof uses many techniques from algebraic geometry and number theory, and has many ramifications in these branches of mathematics. It also uses standard constructions of modern algebraic geometry, such as the category of schemes and Iwasawa theory, and other 20th-century techniques which were not available to Fermat.\n\nTogether, the two papers which contain the proof are 129 pages long, and consumed over seven years of Wiles's research time. John Coates described the proof as one of the highest achievements of number theory, and John Conway called it the proof of the [20th] century. Wiles' path to proving Fermat's Last Theorem, by way of proving the modularity theorem for the special case of semistable elliptic curves, established powerful modularity lifting techniques and opened up entire new approaches to numerous other problems. For solving Fermat's Last Theorem, he was knighted, and received other honours such as the 2016 Abel Prize. When announcing that Wiles had won the Abel Prize, the Norwegian Academy of Science and Letters described his achievement as a \"stunning proof\".\n\nFermat's Last Theorem, formulated in 1637, states that no three distinct positive integers \"a\", \"b\", and \"c\" can satisfy the equation\nif \"n\" is an integer greater than two (\"n\" > 2).\n\nOver time, this simple assertion became one of the most famous unproved claims in mathematics. Between its publication and Andrew Wiles' eventual solution over 350 years later, many mathematicians and amateurs attempted to prove this statement, either for all values of n > 2, or for specific cases. It spurred the development of entire new areas within number theory. Proofs were eventually found for all values of n up to around 4 million, first by hand, and later by computer. But no general proof was found that would be valid for all possible values of \"n\", nor even a hint how such a proof could be undertaken.\n\nSeparately from anything related to Fermat's Last Theorem, in the 1950s and 1960s Japanese mathematician Goro Shimura, drawing on ideas posed by Yutaka Taniyama, conjectured that a connection might exist between elliptic curves and modular forms. These were mathematical objects with no known connection between them. Taniyama and Shimura posed the question whether, unknown to mathematicians, the two kinds of object were actually identical mathematical objects, just seen in different ways.\n\nThey conjectured that every rational elliptic curve is also modular. This became known as the Taniyama–Shimura conjecture. In the West, this conjecture became well known through a 1967 paper by André Weil, who gave conceptual evidence for it; thus, it is sometimes called the Taniyama–Shimura–Weil conjecture.\n\nBy around 1980, much evidence had been accumulated to form conjectures about elliptic curves, and many papers had been written which examined the consequences if the conjecture was true, but the actual conjecture itself was unproven and generally considered inaccessible - meaning that mathematicians believed a proof of the conjecture was probably impossible using current knowledge. \n\nFor decades, the conjecture remained an important but unsolved problem in mathematics. Around 50 years after first being proposed, the conjecture was finally proven and renamed the modularity theorem, largely as a result of Andrew Wiles' work described below.\n\nOn yet another separate branch of development, in the late 1960s, Yves Hellegouarch came up with the idea of associating solutions (\"a\",\"b\",\"c\") of Fermat's equation with a completely different mathematical object: an elliptic curve. The curve consists of all points in the plane whose coordinates (\"x\", \"y\") satisfy the relation\nSuch an elliptic curve would enjoy very special properties, which are due to the appearance of high powers of integers in its equation and the fact that \"a\" + \"b\" = \"c\" is an \"n\"th power as well.\n\nIn 1982–1985, Gerhard Frey called attention to the unusual properties of this same curve, now called a Frey curve. He showed that it was likely that the curve could link Fermat and Taniyama, since any counterexample to Fermat's Last Theorem would probably also imply that an elliptic curve existed that was not modular.\n\nIn plain English, Frey had shown that there were good reasons to believe that any set of numbers (\"a\", \"b\", \"c\", \"n\") capable of disproving Fermat's Last Theorem, could also (probably) be used to disprove the Taniyama–Shimura–Weil conjecture. Therefore, if the Taniyama–Shimura–Weil conjecture were true, no set of numbers capable of disproving Fermat could exist, so Fermat's Last Theorem would have to be true as well.\n\nIf the link identified by Frey could be proven, then in turn, it would mean that a proof or disproof of either of Fermat's Last Theorem or the Taniyama–Shimura–Weil conjecture would simultaneously prove or disprove the other.\n\nTo complete this link, it was necessary to show that Frey's intuition was correct: that a Frey curve, if it existed, could not be modular. \n\nIn 1985, Jean-Pierre Serre provided a partial proof that a Frey curve could not be modular. Serre did not provide a complete proof of his proposal; the missing part (which Serre had noticed early on) became known as the epsilon conjecture or ε-conjecture (it is now known as Ribet's theorem). Serre's main interest was in an even more ambitious conjecture, Serre's conjecture on modular Galois representations, which would imply the Taniyama–Shimura–Weil conjecture. However his partial proof came close to confirming the link between Fermat and Taniyama.\n\nIn the summer of 1986, Ken Ribet succeeded in proving the epsilon conjecture, now known as Ribet's theorem. His article was published in 1990. In doing so, Ribet finally proved the link between the two theorems by confirming as Frey had suggested, that a proof of the Taniyama–Shimura–Weil conjecture for the kinds of elliptic curves Frey had identified, together with Ribet's theorem, would also prove Fermat's Last Theorem:\n\nFollowing the developments related to the Frey Curve, and its link to both Fermat and Taniyama, a proof of Fermat's Last Theorem would follow from a proof of the Taniyama–Shimura–Weil conjecture — or at least a proof of the conjecture for the kinds of elliptic curves that included Frey's equation (known as semistable elliptic curves).\n\nHowever, despite the progress made by Serre and Ribet, this approach to Fermat was widely considered unusable as well, since almost all mathematicians saw the Taniyama–Shimura–Weil conjecture itself as completely inaccessible to proof with current knowledge. For example, Wiles's ex-supervisor John Coates stated that it seemed \"impossible to actually prove\", and Ken Ribet considered himself \"one of the vast majority of people who believed [it] was completely inaccessible\".\n\nHearing of Ribet's 1986 proof of the epsilon conjecture, English mathematician Andrew Wiles, who had studied elliptic curves and had a childhood fascination with Fermat, decided to begin working in secret towards a proof of the Taniyama–Shimura–Weil conjecture, since it was now professionally justifiable as well as because of the enticing goal of proving such a long-standing problem.\n\nRibet later commented that \"Andrew Wiles was probably one of the few people on earth who had the audacity to dream that you can actually go and prove [it].\" \n\nWiles initially presented his proof in 1993. It was finally accepted as correct, and published, in 1995, following the correction of a subtle error in one part of his original paper. His work was extended to a full proof of the modularity theorem over the following 6 years by others, who built on Wiles's work.\n\nDuring 21–23 June 1993 Wiles announced and presented his proof of the Taniyama–Shimura conjecture for semi-stable elliptic curves, and hence of Fermat's Last Theorem, over the course of three lectures delivered at the Isaac Newton Institute for Mathematical Sciences in Cambridge, England. There was a relatively large amount of press coverage afterwards.\n\nAfter the announcement, Nick Katz was appointed as one of the referees to review Wiles's manuscript. In the course of his review, he asked Wiles a series of clarifying questions that led Wiles to recognise that the proof contained a gap. There was an error in one critical portion of the proof which gave a bound for the order of a particular group: the Euler system used to extend Kolyvagin and Flach's method was incomplete. The error would not have rendered his work worthless – each part of Wiles's work was highly significant and innovative by itself, as were the many developments and techniques he had created in the course of his work, and only one part was affected. Without this part proved, however, there was no actual proof of Fermat's Last Theorem.\n\nWiles spent almost a year trying to repair his proof, initially by himself and then in collaboration with his former student Richard Taylor, without success. By the end of 1993, rumours had spread that under scrutiny, Wiles' proof had failed, but how seriously was not known. Mathematicians were beginning to pressure Wiles to disclose his work whether or not complete, so that the wider community could explore and use whatever he had managed to accomplish. But instead of being fixed, the problem, which had originally seemed minor, now seemed very significant, far more serious, and less easy to resolve.\n\nWiles states that on the morning of 19 September 1994, he was on the verge of giving up and was almost resigned to accepting that he had failed, and to publishing his work so that others could build on it and find the error. He states that he was having a final look to try and understand the fundamental reasons why his approach could not be made to work, when he had a sudden insight that the specific reason why the Kolyvagin–Flach approach would not work directly, also meant that his original attempts using Iwasawa theory could be made to work if he strengthened it using his experience gained from the Kolyvagin–Flach approach since then. Each was inadequate by itself, but fixing one approach with tools from the other would resolve the issue and produce a class number formula (CNF) valid for all cases that were not already proven by his refereed paper:\n\nOn 6 October Wiles asked three colleagues (including Faltings) to review his new proof, and on 24 October 1994 Wiles submitted two manuscripts, \"Modular elliptic curves and Fermat's Last Theorem\" and \"Ring theoretic properties of certain Hecke algebras\", the second of which Wiles had written with Taylor and proved that certain conditions were met which were needed to justify the corrected step in the main paper.\n\nThe two papers were vetted and finally published as the entirety of the May 1995 issue of the \"Annals of Mathematics\". The new proof was widely analysed, and became accepted as likely correct in its major components. These papers established the modularity theorem for semistable elliptic curves, the last step in proving Fermat's Last Theorem, 358 years after it was conjectured.\n\nFermat claimed to \"...have discovered a truly marvelous proof of this, which this margin is too narrow to contain\". Wiles's proof is very complex, and incorporates the work of so many other specialists that it was suggested in 1994 that only a small number of people were capable of fully understanding at that time all the details of what he had done. The complexity of Wiles's proof motivated a 10-day conference at Boston University; the resulting book of conference proceedings aimed to make the full range of required topics accessible to graduate students in number theory.\n\nAs noted above, Wiles proved the Taniyama–Shimura–Weil conjecture for the special case of semistable elliptic curves, rather than for all elliptic curves. Over the following years, Christophe Breuil, Brian Conrad, Fred Diamond, and Richard Taylor (sometimes abbreviated as \"BCDT\") carried the work further, ultimately proving the Taniyama–Shimura–Weil conjecture for all elliptic curves in a 2001 paper. Now proved, the conjecture became known as the modularity theorem.\n\nIn 2005, Dutch computer scientist Jan Bergstra posed the problem of formalizing Wiles' proof in such a way that it could be verified by computer.\n\nWiles used proof by contradiction, in which one assumes the opposite of what is to be proved, and show if that were true, it would create a contradiction. The contradiction shows that the assumption must have been incorrect.\n\nThe proof falls roughly in two parts. In the first part, Wiles proves a general result about \"lifts\", known as the \"modularity lifting theorem\". This first part allows him to prove results about elliptic curves by converting them to problems about Galois representations of elliptic curves. He then uses this result to prove that all semi-stable curves are modular, by proving that the \"Galois representations\" of these curves are modular, instead.\n\nWiles opted to attempt to match elliptic curves to a countable set of modular forms. He found that this direct approach was not working, so he transformed the problem by instead matching the Galois representations of the elliptic curves to modular forms. Wiles denotes this matching (or mapping) that, more specifically, is a ring homomorphism:\nformula_4 is a deformation ring and formula_5 is a Hecke ring.\n\nWiles had the insight that in many cases this ring homomorphism could be a ring isomorphism (Conjecture 2.16 in Chapter 2, §3 of the 1995 paper). He realised that the map between formula_4 and formula_5 is an isomorphism if and only if two abelian groups occurring in the theory are finite and have the same cardinality. This is sometimes referred to as the \"numerical criterion\". Given this result, Fermat's Last Theorem is reduced to the statement that two groups have the same order. Much of the text of the proof leads into topics and theorems related to ring theory and commutation theory. Wiles's goal was to verify that the map formula_8 is an isomorphism and ultimately that formula_9. In treating deformations, Wiles defined four cases, with the flat deformation case requiring more effort to prove and treated in a separate article in the same volume entitled \"Ring-theoretic properties of certain Hecke algebras\".\n\nGerd Faltings, in his bulletin, gives the following commutative diagram (p. 745):\n\nor ultimately that formula_9, indicating a complete intersection. Since Wiles could not show that formula_9 directly, he did so through formula_12 and formula_13 via lifts.\n\nIn order to perform this matching, Wiles had to create a class number formula (CNF). He first attempted to use horizontal Iwasawa theory but that part of his work had an unresolved issue such that he could not create a CNF. At the end of the summer of 1991, he learned about an Euler system recently developed by Victor Kolyvagin and Matthias Flach that seemed \"tailor made\" for the inductive part of his proof, which could be used to create a CNF, and so Wiles set his Iwasawa work aside and began working to extend Kolyvagin and Flach's work instead, in order to create the CNF his proof would require. By the spring of 1993, his work had covered all but a few families of elliptic curves, and in early 1993, Wiles was confident enough of his nearing success to let one trusted colleague into his secret. Since his work relied extensively on using the Kolyvagin–Flach approach, which was new to mathematics and to Wiles, and which he had also extended, in January 1993 he asked his Princeton colleague, Nick Katz, to help him review his work for subtle errors. Their conclusion at the time was that the techniques Wiles used seemed to work correctly.\n\nWiles' use of Kolyvagin–Flach would later be found to be the point of failure in the original proof submission, and he eventually had to revert to Iwasawa theory and a collaboration with Richard Taylor to fix it. In May 1993, while reading a paper by Mazur, Wiles had the insight that the 3/5 switch would resolve the final issues and would then cover all elliptic curves. (See Chapter 5 of the paper for this 3/5 switch.)\n\nGiven an elliptic curve \"E\" over the field Q of rational numbers formula_14, for every prime power formula_15, there exists a homomorphism from the absolute Galois group\nto\nthe group of invertible 2 by 2 matrices whose entries are integers (formula_18). This is because formula_19, the points of \"E\" over formula_20, form an abelian group, on which formula_21 acts; the subgroup of elements \"x\" such that formula_22 is just formula_23, and an automorphism of this group is a matrix of the type described.\n\nLess obvious is that given a modular form of a certain special type, a Hecke eigenform with eigenvalues in Q, one also gets a homomorphism from the absolute Galois group\nThis goes back to Eichler and Shimura. The idea is that the Galois group acts first on the modular curve on which the modular form is defined, thence on the Jacobian variety of the curve, and finally on the points of formula_15 power order on that Jacobian. The resulting representation is not usually 2-dimensional, but the Hecke operators cut out a 2-dimensional piece. It is easy to demonstrate that these representations come from some elliptic curve but the converse is the difficult part to prove.\n\nInstead of trying to go directly from the elliptic curve to the modular form, one can first pass to the (formula_18) representation for some \"ℓ\" and \"n\", and from that to the modular form. In the case \"ℓ\" = 3 and \"n\" = 1, results of the Langlands–Tunnell theorem show that the (mod 3) representation of any elliptic curve over Q comes from a modular form. The basic strategy is to use induction on \"n\" to show that this is true for \"ℓ\" = 3 and any \"n\", that ultimately there is a single modular form that works for all n. To do this, one uses a counting argument, comparing the number of ways in which one can lift a (formula_18) Galois representation to (formula_28) and the number of ways in which one can lift a (formula_29) modular form. An essential point is to impose a sufficient set of conditions on the Galois representation; otherwise, there will be too many lifts and most will not be modular. These conditions should be satisfied for the representations coming from modular forms and those coming from elliptic curves. If the original (mod 3) representation has an image which is too small, one runs into trouble with the lifting argument, and in this case, there is a final trick, which has since taken on a life of its own with the subsequent work on the Serre Modularity Conjecture. The idea involves the interplay between the (mod 3) and (mod 5) representations. (Again, see Chapter 5 of the Wiles paper for this 3/5 switch.)\n\nIn his 108-page article published in 1995, Wiles divides the subject matter up into the following chapters (preceded here by page numbers):\n\nGerd Faltings subsequently provided some simplifications to the 1995 proof, primarily in switching from geometric constructions to rather simpler algebraic ones. The book of the Cornell conference also contained simplifications to the original proof.\n\nWiles's paper is over 100 pages long and often uses the specialised symbols and notations of group theory, algebraic geometry, commutative algebra, and Galois theory. The mathematicians who helped to lay the groundwork for Wiles often created new specialised concepts and technical jargon.\n\nAmong the introductory presentations are an email which Ribet sent in 1993; Hesselink's quick review of top-level issues, which gives just the elementary algebra and avoids abstract algebra; or Daney's web page, which provides a set of his own notes and lists the current books available on the subject. Weston attempts to provide a handy map of some of the relationships between the subjects. F. Q. Gouvêa's 1994 article \"A Marvelous Proof\", which reviews some of the required topics, won a Lester R. Ford award from the Mathematical Association of America. Faltings' 5-page technical bulletin on the matter is a quick and technical review of the proof for the non-specialist. For those in search of a commercially available book to guide them, he recommended that those familiar with abstract algebra read Hellegouarch, then read the Cornell book, which is claimed to be accessible to \"a graduate student in number theory\". The Cornell book does not cover the entirety of the Wiles proof.\n\n\n\n"}
{"id": "24879634", "url": "https://en.wikipedia.org/wiki?curid=24879634", "title": "Witness (mathematics)", "text": "Witness (mathematics)\n\nIn mathematical logic, a witness is a specific value \"t\" to be substituted for variable \"x\" of an existential statement of the form ∃\"x\" \"φ\"(\"x\") such that \"φ\"(\"t\") is true.\n\nFor example, a theory \"T\" of arithmetic is said to be inconsistent if there exists a proof in \"T\" of the formula \"0 = 1\". The formula I(\"T\"), which says that \"T\" is inconsistent, is thus an existential formula. A witness for the inconsistency of \"T\" is a particular proof of \"0 = 1\" in \"T\".\n\nBoolos, Burgess, and Jeffrey (2002:81) define the notion of a witness with the example, in which \"S\" is an \"n\"-place relation on natural numbers, \"R\" is an \"n\"-place recursive relation, and ↔ indicates logical equivalence (if and only if):\nIn this particular example, B-B-J have defined \"s\" to be \"(positively) recursively semidecidable\", or simply \"semirecursive\".\n\nIn predicate calculus, a Henkin witness for a sentence formula_1 in a theory \"T\" is a term \"c\" such that \"T\" proves \"φ\"(\"c\") (Hinman 2005:196). The use of such witnesses is a key technique in the proof of Gödel's completeness theorem presented by Leon Henkin in 1949.\n\nThe notion of witness leads to the more general idea of game semantics. In the case of sentence formula_1 the winning strategy for the verifier is to pick a witness for formula_3. For more complex formulas involving universal quantifiers, the existence of a winning strategy for the verifier depends on the existence of appropriate Skolem functions. For example, if \"S\" denotes formula_4 then an equisatisfiable statement for \"S\" is formula_5. The Skolem function \"f\" (if it exists) actually codifies a winning strategy for the verifier of \"S\" by returning a witness for the existential sub-formula for every choice of \"x\" the falsifier might make.\n\n\n"}
{"id": "4222668", "url": "https://en.wikipedia.org/wiki?curid=4222668", "title": "Witt's theorem", "text": "Witt's theorem\n\nIn mathematics, Witt's theorem, named after Ernst Witt, is a basic result in the algebraic theory of quadratic forms: any isometry between two subspaces of a nonsingular quadratic space over a field \"k\" may be extended to an isometry of the whole space. An analogous statement holds also for skew-symmetric, Hermitian and skew-Hermitian bilinear forms over arbitrary fields. The theorem applies to classification of quadratic forms over \"k\" and in particular allows one to define the Witt group \"W\"(\"k\") which describes the \"stable\" theory of quadratic forms over the field \"k\".\n\nLet be a finite-dimensional vector space over a field \"k\" of characteristic different from 2 together with a non-degenerate symmetric or skew-symmetric bilinear form. If is an isometry between two subspaces of \"V\" then \"f\" extends to an isometry of \"V\".\n\nWitt's theorem implies that the dimension of a maximal totally isotropic subspace (null space) of \"V\" is an invariant, called the index or of \"b\", and moreover, that the isometry group of acts transitively on the set of maximal isotropic subspaces. This fact plays an important role in the structure theory and representation theory of the isometry group and in the theory of reductive dual pairs.\n\nLet , , be three quadratic spaces over a field \"k\". Assume that\n\nThen the quadratic spaces and are isometric:\n\nIn other words, the direct summand appearing in both sides of an isomorphism between quadratic spaces may be \"cancelled\".\n\nLet be a quadratic space over a field \"k\". Then\nit admits a Witt decomposition:\n\nwhere is the radical of \"q\", is an anisotropic quadratic space and is a split quadratic space. Moreover, the anisotropic summand, termed the core form, and the hyperbolic summand in a Witt decomposition of are determined uniquely up to isomorphism.\n\nQuadratic forms with the same core form are said to be \"similar\" or Witt equivalent.\n\n"}
{"id": "4062502", "url": "https://en.wikipedia.org/wiki?curid=4062502", "title": "Zero-product property", "text": "Zero-product property\n\nIn algebra, the zero-product property states that the product of two nonzero elements is nonzero. In other words, it is the following assertion:\n\nIf formula_1, then formula_2 or formula_3.\n\nThe zero-product property is also known as the rule of zero product, the null factor law, the multiplication property of zero or the nonexistence of nontrivial zero divisors. All of the number systems studied in elementary mathematics — the integers formula_4, the rational numbers formula_5, the real numbers formula_6, and the complex numbers formula_7 — satisfy the zero-product property. In general, a ring which satisfies the zero-product property is called a domain.\n\nSuppose formula_8 is an algebraic structure. We might ask, does formula_8 have the zero-product property? In order for this question to have meaning, formula_8 must have both additive structure and multiplicative structure. Usually one assumes that formula_8 is a ring, though it could be something else, e.g., the nonnegative integers formula_12.\n\nNote that if formula_8 satisfies the zero-product property, and if formula_14 is a subset of formula_8, then formula_14 also satisfies the zero product property: if formula_17 and formula_18 are elements of formula_14 such that formula_20, then either formula_2 or formula_3 because formula_17 and formula_18 can also be considered as elements of formula_8.\n\n\n\n\nSuppose formula_51 and formula_52 are univariate polynomials with real coefficients, and formula_53 is a real number such that formula_54. (Actually, we may allow the coefficients and formula_53 to come from any integral domain.) By the zero-product property, it follows that either formula_56 or formula_57. In other words, the roots of formula_58 are precisely the roots of formula_51 together with the roots of formula_52.\n\nThus, one can use factorization to find the roots of a polynomial. For example, the polynomial formula_61 factorizes as formula_62; hence, its roots are precisely 3, 1, and -2.\n\nIn general, suppose formula_63 is an integral domain and formula_64 is a monic univariate polynomial of degree formula_65 with coefficients in formula_63. Suppose also that formula_64 has formula_68 distinct roots formula_69. It follows (but we do not prove here) that formula_64 factorizes as formula_71. By the zero-product property, it follows that formula_72 are the \"only\" roots of formula_64: any root of formula_64 must be a root of formula_75 for some formula_76. In particular, formula_64 has at most formula_68 distinct roots.\n\nIf however formula_63 is not an integral domain, then the conclusion need not hold. For example, the cubic polynomial formula_80 has six roots in formula_31 (though it has only three roots in formula_4).\n\n\n\n"}
